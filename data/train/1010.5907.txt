{
  "article_text": [
    "multiple optimised parameter estimation and data compression ( moped ;  @xcite ) is a patented algorithm for the compression of data and the speeding up of the evaluation of likelihood functions in astronomical data analysis and beyond .",
    "it becomes particularly useful when the noise covariance matrix is dependent upon the parameters of the model and so must be calculated and inverted at each likelihood evaluation .",
    "however , such benefits come with limitations . since moped only guarantees maintaining the fisher matrix of the likelihood at a chosen point , multimodal and some degenerate distributions will present a problem . in this paper we report on some of the limitations of the application of the moped algorithm . in the cases where moped does accurately represent the likelihood function",
    ", however , its compression of the data and consequent much faster likelihood evaluation does provide orders of magnitude improvement in runtime . in  @xcite ,",
    "the authors demonstrate the method by analysing the spectra of galaxies and in  @xcite they illustrate the benefits of moped for estimation of the cmb power spectrum .",
    "the problem of `` badly '' behaved likelihoods was found by  @xcite for the problem of light transit analysis ; nonetheless , the authors present a solution that still allows moped to provide a large speed increase .",
    "we begin by introducing moped in section  2 and define the original and moped likelihood functions , along with comments on the potential speed benefits of moped . in section",
    "3 we introduce an astrophysical scenario where we found that moped did not accurately portray the true likelihood function . in section  4",
    "we expand upon this scenario to another where moped is found to work and to two other scenarios where it does not .",
    "we present a discussion of the criteria under which we believe moped will accurately represent the likelihood in section  5 , as well as a discussion of an implementation of the solution provided by  @xcite .",
    "full details of the moped method are given in  @xcite , here we will only present a limited introduction .",
    "we begin by defining our data as a vector , @xmath0 .",
    "our model describes @xmath0 by a signal plus random noise , @xmath1 where the signal is given by a vector @xmath2 that is a function of the set of parameters @xmath3 defining our model , and the true parameters are given by @xmath4 .",
    "the noise is assumed to be gaussian with zero mean and noise covariance matrix @xmath5 , where the angle brackets indicate an ensemble average over noise realisations ( in general this matrix may also be a function of the parameters @xmath6 ) .",
    "the full likelihood for @xmath7 data points in @xmath0 is given by @xmath8^{\\textrm{t } } \\mathcal{n}(\\btheta)^{-1 } [ { \\bf x}-{\\bf u}(\\btheta)]\\right\\}}.\\end{aligned}\\ ] ] at each point , then , this requires the calculation of the determinant and inverse of an @xmath9 matrix . both scale as @xmath10 ,",
    "so even for smaller datasets this can become cumbersome .",
    "moped allows one to eliminate the need for this matrix inversion by compressing the @xmath7 data points in @xmath0 into @xmath11 data values , one for each parameters of the model .",
    "additionally , moped creates the compressed data values such that they are independent and have unit variance , further simplifying the likelihood calculation on them to an @xmath12 operation .",
    "typically , @xmath13 so this gives us a significant increase in speed .",
    "a single compression is done on the data , @xmath0 , and then again for each point in parameter space where we wish to compute the likelihood .",
    "the compression is done by generating a set of weighting vectors , @xmath14 ( @xmath15 ) , from which we can generate a set of moped components from the theoretical model and data , @xmath16 note that the weighting vectors must be computed at some assumed fiducial set of parameter values , @xmath17 .",
    "the only choice that will truly maintain the likelihood peak is when the fiducial parameters are the true parameters , but obviously we will not know these in advance for real analysis situations .",
    "thus , we can choose our fiducial model to be anywhere and iterate the procedure , taking our likelihood peak in one iteration as the fiducial model for the next iteration .",
    "this process will converge very quickly , and may not even be necessary in some instances . for our later examples , since we do know the true parameters we will use these as the fiducial ( @xmath18 ) in order to remove this as a source of confusion ( all equations , however , are written for the more general case ) .",
    "note that the true parameters , @xmath4 , will not necessarily coincide with the peak @xmath19 of the original likelihood or the peak @xmath20 of the moped likelihood ( see below ) .",
    "the weighting vectors must be generated in some order so that each subsequent vector ( after the first ) can be made orthogonal to all previous ones .",
    "we begin by writing the derivative of the model with respect to the @xmath21th parameter as @xmath22 .",
    "this gives us a solution for the first weighting vector , properly normalised , of @xmath23 the first compressed value is @xmath24 and will weight up the data combination most sensitive to the first parameter .",
    "the subsequent weighting vectors are made orthogonal by subtracting out parts that are parallel to previous vectors , and are normalized .",
    "the resulting formula for the remaining weighting vectors is @xmath25 @xmath26 where @xmath27 .",
    "weighting vectors generated with equations   and   form an orthnomal set with respect to the noise covariance matrix so that @xmath28 this means that the noise covariance matrix of the compressed values @xmath29 is the identity , which significantly simplifies the likelihood calculation .",
    "the new likelihood function is given by @xmath30 where @xmath31 represents the compressed data and @xmath32 represents the compressed signal .",
    "this is a much easier likelihood to calculate and is time - limited by the generation of a new signal template instead of the inversion of the noise covariance matrix .",
    "the peak value of the moped likelihood function is not guaranteed to be unique as there may be other points in the original parameter space that map to the same point in the compressed parameter space ; this is a characteristic that we will investigate .",
    "moped implicity assumes that the covariance matrix , @xmath33 , is independent of the parameters . with this assumption ,",
    "a full likelihood calculation with @xmath7 data points would require only an @xmath34 operation at each point in parameter space ( or @xmath35 if @xmath33 is diagonal ) . in moped",
    ", however , the compression of the theoretical data is an @xmath36 linear operation followed by an @xmath12 likelihood calculation .",
    "thus , one loses on speed if @xmath33 is diagonal but gains by a factor of @xmath37 otherwise . for the data sets we will analyze , @xmath38 .",
    "we begin , though , by assuming a diagonal @xmath33 for simplicity , recognizing that this will cause a speed reduction but that it is a necessary step before addressing a more complex noise model .",
    "one can iterate the parameter estimation procedure if necessary , taking the maximum likelihood point found as the new fiducial and re - analyzing ( perhaps with tighter prior constraints ) ; this procedure is recommended for moped in  @xcite , but is not always found to be necessary .",
    "moped has the additional benefit that the weighting vectors , @xmath39 , need only to be computed once provided the fiducial model parameters are kept constant over the analysis of different data sets . computed compressed parameters , @xmath40 ,",
    "can also be stored for re - use and require less memory than storing the entire theoretical data set .",
    "in order to demonstrate some of the limitations of the applicability of the moped algorithm , we will consider a few test cases .",
    "these originate in the context of gravitational wave data analysis for the _ laser interferometer space antenna _",
    "( _ lisa _ ) since it is in this scenario that we first discovered such cases of failure .",
    "the full problem is seven - dimensional parameter estimation , but we have fixed most of these variables to their known true values in the simulated data set in order to create a lower - dimensional problem that is simpler to analyse .",
    "we consider the case of a sine - gaussian burst signal present in the lisa detector .",
    "the short duration of the burst with respect to the motion of lisa allows us to use the static approximation to the response . in frequency space ,",
    "the waveform is described by  ( @xcite ) @xmath41 here @xmath42 is the dimensionless amplitude factor ; @xmath43 is the width of the gaussian envelope of the burst measured in cycles ; @xmath44 is the central frequency of the oscillation being modulated by the gaussian envelope ; and @xmath45 is the central time of arrival of the burst .",
    "this waveform is further modulated by the sky position of the burst source , @xmath46 and @xmath47 , and the burst polarisation , @xmath48 , as they project onto the detector .",
    "the one - sided noise power spectral density of the lisa detector is given by  ( @xcite ) @xmath49 where @xmath50s is the light travel time along one arm of the lisa constellation , @xmath51hz@xmath52 is the proof mass acceleration noise and @xmath53hz@xmath52 is the shot noise .",
    "this is independent of the signal parameters and all frequencies are independent of each other , so the noise covariance matrix is constant and diagonal .",
    "this less computationally expensive example allows us to show some interesting examples .",
    "we begin by taking the one - dimensional case where the only unknown parameter of the model is the central frequency of the oscillation , @xmath44 .",
    "we set @xmath54 and @xmath55s ; we then analyze a @xmath56s segment of lisa data , beginning at @xmath57s , sampled at a @xmath58s cadence .",
    "for this example , the data was generated with random noise ( following the lisa noise power spectrum ) at an snr of @xmath59 with @xmath60hz ( thus @xmath61hz for moped ) .",
    "the prior range on the central frequency goes from @xmath62hz to @xmath63hz .",
    "@xmath64 samples uniformly spaced in @xmath44 were taken and their likelihoods calculated using both the original and moped likelihood functions .",
    "the log - likelihoods are shown in figure  [ fig : likecomp ] .",
    "note that the absolute magnitudes are not important but the relative values within each plot are meaningful .",
    "both the original and moped likelihoods have a peak close to the input value @xmath65 .     for the chosen template.,width=312 ]",
    "we see , however , that in going from the original to moped log - likelihoods , the latter also has a second peak of equal height at an incorrect @xmath44 . to see where this peak comes from",
    ", we look at the values of the compressed parameter @xmath66 as it varies with respect to @xmath44 as shown in figure  [ fig : yf_vs_f ] .",
    "the true compressed value peak occurs at @xmath67hz , where @xmath68 .",
    "however , we see that there is another frequency that yields this exact same value of @xmath66 ; it is at this frequency that the second , incorrect peak occurs .        by creating a mapping from @xmath44 to @xmath66 that is not one - to - one , moped has created the possibility for a second solution that is indistinguishable in likelihood from the correct one .",
    "this is a very serious problem for parameter estimation .",
    "interestingly , we find that even when moped fails in a one - parameter case , adding a second parameter may actually rectify the problem , although not necessarily . if we now allow the width of the burst , @xmath43 , to be a variable parameter , there are now two orthognal moped weighting vectors that need to be calculated .",
    "this gives us two compressed parameters for each pair of @xmath44 and @xmath43 .",
    "each of these may have its own unphysical degeneracies , but in order to give an unphysical mode of equal likelihood to the true peak , these degeneracies will need to coincide . in figure  [ fig : ytruecontours ] , we plot the contours in @xmath69 space of where @xmath70 as @xmath6 ranges over @xmath44 and @xmath43 values .",
    "we can clearly see the degeneracies present in either variable , but since these only overlap at the one location , near to where the true peak is , there is no unphysical second mode in the moped likelihood .     and @xmath71 as they vary over @xmath44 and @xmath43 .",
    "the one intersection is the true maximum likelihood peak.,width=312 ]    hence , when we plot the original and moped log - likelihoods in figure  [ fig : fqlikes ] , although the behaviour away from the peak has changed , the peak itself remains in the same location and there is no second mode .",
    "adding more parameters , however , does not always improve the situation .",
    "we now consider the case where @xmath43 is once again fixed to its true value and we instead make the polarisation of the burst , @xmath48 , a variable parameter .",
    "there are degeneracies in both of these parameters and in figure  [ fig : ytruecontours3 ] we plot the contours in @xmath72-space where the compressed values are each equal to the value at the maximum moped likelihood point .",
    "these two will necessarily intersect at the maximum likelihood solution , near the true value ( @xmath73 hz and @xmath74 rad ) , but a second intersection is also apparent .",
    "this second intersection will have the same likelihood as the maximum and be another mode of the solution .",
    "however , as we can see in figure  [ fig : fpslikes ] in the left plot , this is not a mode of the original likelihood function .",
    "moped has , in this case , created a second mode of equal likelihood to the true peak .     and",
    "@xmath75 values as they vary as functions of @xmath44 and @xmath48.,width=312 ]        for an even more extreme scenario , we now fix to the true @xmath48 and allow the time of arrival of the burst @xmath45 to vary ( we also define @xmath76 ) . in this scenario , the contours in @xmath77-space where @xmath70 are much more complicated .",
    "thus , we have many more intersections of the two contours than just at the likelihood peak near the true values and moped creates many alternative modes of likelihood equal to that of the original peak .",
    "this is very problematic for parameter estimation . in figure",
    "[ fig : ytruecontours2 ] we plot these contours so the multiple intersections are apparent .",
    "figure  [ fig : ftlikes ] shows the original and moped log - likelihoods , where we can see the single peak becoming a set of peaks .     and",
    "@xmath75 values as they vary as functions of @xmath44 and @xmath45 .",
    "we can see many intersections here other than the true peak.,width=312 ]",
    "what we can determine from the previous two sections is a general rule for when moped will generate additional peaks in the likelihood function equal in magnitude to the true one . for an @xmath11-dimensional model , if we consider the @xmath78-dimensional hyper - surfaces where @xmath70 , then any point where these @xmath11 hyper - surfaces intersect will yield a set of @xmath6-parameter values with likelihood equal to that at the peak near the true values .",
    "we expect that there will be at least one intersection at the likelihood peak corresponding to approximately the true solution .",
    "however , we have shown that other peaks can exist as well .",
    "the set of intersections of contour surfaces will determine where these additional peaks are located .",
    "this degeneracy will interact with the model s intrinsic degeneracy , as any degenerate parameters will yield the same compressed values for different original parameter values .",
    "unfortunately , there is no simple way to find these contours other than by mapping out the @xmath79 values , which is equivalent in procedure to mapping the moped likelihood surface .",
    "the benefit comes when this procedure is significantly faster than mapping the original likelihood surface .",
    "the mapping of @xmath79 can even be performed before data is obtained or used , if the fiducial model is chosen in advance ; this allows us to analyse properties of the moped compression before applying it to data analysis . if the moped likelihood is mapped and there is only one contour intersection , then we can rely on the moped algorithm and will have saved a considerable amount of time , since moped has been demonstrated to provide speed - ups of a factor of up to @xmath80 in  @xcite .",
    "however , if there are multiple intersections then it is necessary to map the original likelihood to know if they are due to degeneracy in the model or were created erroneously by moped . in this latter case",
    ", the time spent finding the moped likelihood surface can be much less than that which will be needed to map the original likelihood , so relatively little time will have been wasted . if any model degeneracies are known in advance , then we can expect to see them in the moped likelihood and will not need to find the original likelihood on their account .",
    "one possible way of determining the validity of degenerate peaks in the moped likelihood function is to compare the original likelihoods of the peak parameter values with each other . by using the maximum moped likelihood point found in each mode and evaluating the original likelihood at this point",
    ", we can determine which one is correct .",
    "the correct peak and any degeneracy in the original likelihood function will yield similar values to one another , but a false peak in the moped likelihood will have a much lower value in the original likelihood and can be ruled out .",
    "this means that a bayesian evidence calculation can not be obtained from using the moped likelihood ; however , the algorithm was not designed to be able to provide this .",
    "the solution for this problem presented in  @xcite is to use multiple fiducial models to create multiple sets of weighting vectors .",
    "the log - likelihood is then averaged across these choices .",
    "each different fiducial will create a set of likelihood peaks that include the true peaks and any extraneous ones .",
    "however , the only peaks that will be consistent between fiducials are the correct ones .",
    "therefore , the averaging maintains the true peaks but decreases the likelihood at incorrect values .",
    "this was tested with 20 random fiducials for the two - parameter models presented and was found to leave only the true peak at the maximum likelihood value .",
    "other , incorrect , peaks are still present , but at log - likelihood values five or more units below the true peak .",
    "when applied to the full seven parameter model , however , the snr threshold for signal recovery is increased significantly , from @xmath81 to @xmath82 .",
    "the moped algorithm for reducing the computational expense of likelihood functions can , in some examples , be extremely useful and provide orders of magnitude of improvement .",
    "however , as we have shown , this is not always the case and moped can produce erroneous peaks in the likelihood that impede parameter estimation .",
    "it is important to identify whether or not moped has accurately portrayed the likelihood function before using the results it provides .",
    "some solutions to this problem have been presented and tested ,",
    "pg s phd is funded by the gates cambridge trust .",
    "feroz f , gair j , graff p , hobson m p , & lasenby a n , cqg , * 27 * 7 pp .",
    "075010 ( 2010 ) , arxiv:0911.0288 [ gr - qc ] .",
    "gupta s & heavens a f , mnras * 334 * 167 - 172 ( 2002 ) , arxiv : astro - ph/0108315 .",
    "heavens a f , jimenez r , & lahav o , mnras * 317 * 965 - 972 ( 2000 ) , arxiv : astro - ph/9911102 .",
    "protopapas p , jimenez r , & alcock a , mnras * 362 * 460 - 468 ( 2005 ) , arxiv : astro - ph/0502301 ."
  ],
  "abstract_text": [
    "<S> we investigate the use of the multiple optimised parameter estimation and data compression algorithm ( moped ) for data compression and faster evaluation of likelihood functions . since moped only guarantees maintaining the fisher matrix of the likelihood at a chosen point , multimodal and some degenerate distributions will present a problem . </S>",
    "<S> we present examples of scenarios in which moped does faithfully represent the true likelihood but also cases in which it does not . through these examples , we aim to define a set of criteria for which moped will accurately represent the likelihood and hence may be used to obtain a significant reduction in the time needed to calculate it . </S>",
    "<S> these criteria may involve the evaluation of the full likelihood function for comparison .    </S>",
    "<S> [ firstpage ]    methods : data analysis  methods : statistical </S>"
  ]
}