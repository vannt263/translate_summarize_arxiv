{
  "article_text": [
    "consider the large and possibly sparse matrix eigenproblem @xmath1 with @xmath2 , the 2-norm @xmath3 and the eigenvalues labeled as @xmath4 for a given target @xmath5 .",
    "we are interested in the eigenvalue @xmath6 closest to the target @xmath0 and/or the associated eigenvector @xmath7 .",
    "we denote @xmath8 by @xmath9 for simplicity .",
    "a number of numerical methods @xcite are available for solving this kind of problems .",
    "the residual arnoldi ( ra ) method and shift - invert residual arnoldi ( sira ) method are new ones that have their origins in the jacobi  davidson ( jd ) method @xcite .",
    "ra was initially proposed by van der vorst and stewart in 2001 ; see @xcite .",
    "the methods were then studied and developed by lee @xcite and lee and stewart @xcite .",
    "we briefly describe ra now .    given a starting vector @xmath10 with @xmath11 ,",
    "suppose an orthonormal @xmath12 has been constructed by the arnoldi process .",
    "then the columns of @xmath13 form a basis of the @xmath14-dimensional krylov subspace @xmath15 , and the next basis vector @xmath16 is obtained by orthogonalizing @xmath17 against @xmath13 .",
    "let @xmath18 be the candidate ritz pair of @xmath19 for a desired eigenpair of @xmath19 with respect to @xmath20 , and define the residual @xmath21 .",
    "then the ra method orthogonalizes @xmath22 against @xmath13 to get the next basis vector , which , in exact arithmetic , is just @xmath16 obtained by the arnoldi process @xcite",
    ". so the arnoldi method is mathematically equivalent to the ra method .",
    "however , van der vorst and stewart discovered a striking phenomenon that @xmath22 in the ra method may allow much larger errors or perturbations than @xmath17 in the arnoldi method .",
    "the shift - invert arnoldi ( sia ) method is the arnoldi method applied to the shift - invert matrix @xmath23 and finds a few eigenvalues nearest to @xmath0 and the associated eigenvectors .",
    "it computes @xmath16 by orthogonalizing @xmath24 against @xmath13 , whose columns now form a basis of @xmath25 .",
    "so at step @xmath14 one has to solve the linear system @xmath26 for @xmath27 .",
    "the sira method @xcite is an alternative of the ra method applied to @xmath28 . at each step",
    "one has to solve the linear system @xmath29 for @xmath27 , where @xmath30 is the residual of the current approximate eigenpair @xmath31 obtained by sira .",
    "then the sira method computes the next basis vector @xmath16 by orthogonalizing @xmath27 against @xmath13 .",
    "a mathematical difference between sia and sira is that the sia method computes ritz pairs of the shift - invert @xmath28 with respect to @xmath25 and recovers an approximation to @xmath9 , while the sira method computes the ritz pairs of the original @xmath19 with respect to the same @xmath25 and gets an approximation to @xmath9 .",
    "so sia and sira generally obtain different approximations to @xmath9 with respect to the same subspace @xmath25 .",
    "however , for large ( [ inneriteration ] ) , only iterative solvers are generally viable .",
    "this leads to the inexact sira , an inner - outer iterative method , built - up by outer iteration as the eigensolver and inner iteration as the solver of ( [ inneriteration ] ) .",
    "inexact eigensolvers have attracted much attention over the last two decades , and among them inexact sia type methods @xcite are closely related to the work in the current paper .",
    "central concerns on all inexact eigensolvers are how the accuracy of inner iterations ensures and affects the convergence of outer iterations and how to choose the accuracy requirements of inner iterations so that each inexact eigensolver mimics its corresponding exact counterpart very well in the sense that the two eigensolvers use almost the same or very comparable outer iterations to achieve the convergence .",
    "the jd method with fixed or variable targets @xcite is a very popular inexact eigensolver , in which a correction equation ( inner linear system ) is solved iteratively at each outer iteration ; see , e.g. , @xcite and more recent @xcite .",
    "hitherto , however , there has been no result on the accuracy requirement of inner iterations involved in the standard jd method .",
    "existing work only focuses on the simplified ( or single - vector ) jd method without subspace acceleration .",
    "one hopes that the results on the accuracy requirement of inner iterations developed for the simplified jd may help understand the standard jd . nevertheless , such treatment may be too inaccurate and far from the essence of the standard jd .",
    "as is well known , the standard jd is much more complicated than the simplified jd , and the convergence of its outer iterations is much more involved ; see @xcite and also @xcite for details .",
    "therefore , the standard jd method lacks a general theory on inner iterations , and a rigorous and insightful analysis is necessary and very appealing .    for the inexact sia method ,",
    "simoncini @xcite has established a relaxation theory on the accuracy requirements of inner iterations of ( [ siainner ] ) as @xmath14 increases .",
    "she proved that the accuracy of approximate solution of ( [ siainner ] ) should be very high initially and is relaxed as the approximate eigenpairs start converging .",
    "freitag and spence @xcite have extended simoncini s relaxation theory to the inexact implicitly restarted arnoldi method .",
    "xue and elman  @xcite have made a refined analysis on the relaxation strategy .",
    "so it may be very costly to implement the inexact sia type methods .    for the sira method ,",
    "it has been reported by lee @xcite and lee and stewart @xcite that when the accuracy of approximate solutions of ( [ inneriteration ] ) is low or modest at each step , the method may still work well .",
    "lee and stewart @xcite have made some analysis on the ra and sira methods but they did not derive any quantitative and explicit bounds for the accuracy requirements of inner iterations .    in this paper , we take a different approach from that in @xcite to giving a rigorous one - step analysis of the inexact sira method and establish a general and quantitative theory of the accuracy requirements of inner iterations .",
    "our analysis approach applies to the jd method with the fixed target @xmath0 as well .",
    "we first show that the exact sira and jd methods are mathematically equivalent .",
    "we then focus on a detailed quantitative analysis of the inexact sira and jd methods .",
    "let @xmath32 be the relative error of the approximate solution of the inner linear system .",
    "we prove that a modestly small @xmath32 , e.g. , @xmath33 $ ] , is generally enough to make the inexact sira and jd use almost the same outer iterations as the exact ones to achieve the convergence . as a result ,",
    "one only needs to solve all inner linear systems with low or modest accuracy in the inexact sira and the jd methods , and both methods are expected to be considerably more effective than the inexact sia method .",
    "we should point out that our work is locally an one step analysis .",
    "a global analysis involving subspaces accumulating all previous perturbations is much harder and seems impossible .",
    "actually , an one step local analysis is typical in the field of inexact eigensolvers , and it indeed sheds lights on the behavior of the inexact solvers .    the paper is organized as follows . in section  [ sec : sira_vs_jd ] , we review the sira and jd methods and show the equivalence of two exact versions . in section  [ sec : eps ] , we derive some relationships between @xmath32 and subspace expansions and show that the inexact sira and methods behave very similar when their respective inner linear systems are solved with the same accuracy . in section  [ sec : teps ] , we consider subspace improvement and the selection of @xmath32 and prove that the inexact sira mimics the exact sira very well when @xmath32 is modestly small at all steps . in section  [ issue ] , we consider some practical issues and design practical stopping criteria for inner solves in the inexact sira and jd . in section  [ numer ] ,",
    "we report numerical experiments to confirm our theory and the considerable superiority of the inexact sira and jd algorithms to the inexact sia algorithm .",
    "meanwhile , we show that the inexact sira and jd are similarly effective . finally , we conclude the paper and point out future work in section  [ concl ] .    throughout the paper ,",
    "denote by @xmath34 the 2-norm of a vector or matrix , by @xmath35 the identity matrix with the order clear from the context , by the superscript @xmath36 the conjugate transpose of a vector or matrix , and by @xmath37 the condition number of a nonsingular matrix @xmath38 .",
    "we measure the distance between a nonzero vector @xmath39 and a subspace @xmath40 by @xmath41 where @xmath42 is the orthogonal projector onto @xmath40 and the columns of @xmath43 form an orthonormal basis of the orthogonal complement of @xmath40 .",
    "algorithms  [ alg : sira][alg : jd ] describe the sira algorithm and the jd algorithm with the fixed target @xmath0 , respectively ( for brevity we drop iteration subscript ) . comparing them , we observe that the only seemingly differences between them are the linear systems to be solved ( step 4 ) and the expansion vectors to be orthogonalized against the initial subspace @xmath40 .",
    "in fact , they are equivalent , as the following theorem shows .",
    "[ equiva ] for the same initial @xmath40 , if @xmath44 , then the sira method and the jd method are mathematically equivalent when inner linear systems ( [ lssira ] ) and ( [ ls_jd ] ) are solved exactly .    for the same initial @xmath45 ,",
    "the two methods share the same @xmath46 , @xmath47 and @xmath39 , leading to the same @xmath48 and @xmath49 .",
    "let @xmath50 and @xmath51 be the exact solutions of ( [ lssira ] ) and ( [ ls_jd ] ) , respectively . since @xmath23 , we get @xmath52 from ( [ ls_jd ] ) , we have @xmath53 where @xmath54 . premultiplying two sides of ( [ ls_jd_0 ] ) by @xmath28 , we obtain @xmath55 since @xmath56 , we get @xmath57 .",
    "since @xmath58 , we have @xmath59 .",
    "so from ( [ u_sira ] ) and ( [ u_jd ] ) , we get @xmath60 note that @xmath61 and @xmath62 ( after normalization ) are the subspace expansion vectors in sira and jd , respectively .",
    "the two methods generate the same subspace in the next iteration and @xmath31 obtained by them are thus identical .    from ( [ ls_jd_0 ] ) , define @xmath63 where @xmath64 then ( [ ls_jd_0 ] ) and thus ( [ ls_jd ] ) become @xmath65 whose solution is @xmath66 and is the same as @xmath51 up to the sign @xmath67 .",
    "so mathematically , hereafter we use ( [ mjd ] ) as the inner linear system in the jd method .",
    "since @xmath68 approximates the eigenvalue @xmath69 of @xmath28 , @xmath70 approximates @xmath71 .",
    "so @xmath72 is a residual associated with the desired eigenpair @xmath9 , just like @xmath48 in ( [ lssira ] ) .",
    "we observe that ( [ lssira ] ) and ( [ ls_jd_1 ] ) fall into the category of @xmath73 where specifically @xmath74 and @xmath75 in sira and @xmath76 and @xmath75 in jd .",
    "the exact solution @xmath27 of ( [ alpha12 ] ) is @xmath77 since @xmath78 , the ( unnormalized ) subspace expansion vector is @xmath79 .",
    "let @xmath80 be an approximate solution of ( [ ls_unified ] ) , whose relative error is defined by @xmath81 then we can write @xmath82 with @xmath83 the normalized error direction vector .",
    "so we get @xmath84 where @xmath85 define @xmath86 which are the normalized subspace expansion vectors in the inexact and exact methods , respectively .",
    "we measure the difference between @xmath87 and @xmath88 by the relative error @xmath89 or by @xmath90 .",
    "two quantities @xmath91 and @xmath90 are two valid measures for the difference . next we establish a relationship between @xmath92 and @xmath93 , which will be used in proving our final result in this paper .",
    "[ lemma1 ] with the notations defined above , it holds that @xmath94    let @xmath95 be an orthonormal basis of the orthogonal complement of @xmath96 with respect to @xmath97 .",
    "since @xmath98 , by definition ( [ sinedef ] ) we get @xmath99 from ( [ iptu ] ) we have @xmath100 . substituting it into ( [ sin_tv_v ] )",
    "gives @xmath101    in order to make the inexact sira method mimic the sira method well , we must require that @xmath102 approximates @xmath103 with certain accuracy , i.e. , @xmath92 suitably small , so that the two expanded subspaces have comparable quality .",
    "we will come back to this key point and estimate @xmath92 quantitatively in section [ sec : teps ] .    in what follows",
    "we establish an important relationship between @xmath32 and @xmath91 , and based on it we analyze how @xmath32 varies with @xmath104 and @xmath105 for a given @xmath92 .",
    "[ thm4 ] let @xmath39 be the current approximate eigenvector and @xmath106 with @xmath107 in ( [ alpha12 ] ) .",
    "we have @xmath108    by definition ( [ fperp ] ) , we have @xmath109 from ( [ iptu ] ) , we get @xmath110 by ( [ u_unified ] ) , we substitute @xmath111 into the above , giving @xmath112 decompose @xmath39 into the orthogonal direct sum @xmath113 with @xmath114 and @xmath115 . then we get @xmath116 where @xmath117 . making use of @xmath118 and @xmath119",
    ", we obtain @xmath120 therefore , combining the last relation with ( [ eps_teps_relation ] ) establishes ( [ eps_teps_relation_general ] ) .",
    "observe that the linear system @xmath121 , which is also the one in the inverse power method at each step , falls into the form of ( [ ls_unified ] ) by taking @xmath122 and @xmath123 .",
    "for this case , from ( [ eps_teps_relation_general ] ) we have @xmath124 we comment that ( i ) @xmath125 is moderate as @xmath83 is a general vector and ( ii ) @xmath126 if @xmath39 is a reasonably good approximation to @xmath127 and in the worst case @xmath128 . in case that @xmath125 is small",
    ", @xmath32 becomes big for a fixed small @xmath91 , that is , linear system ( [ ls_unified ] ) is allowed to be solved with less accuracy .",
    "so a small @xmath125 is a lucky event .",
    "we can use this theorem to further illustrate why it is bad to solve @xmath121 iteratively . for a fixed small @xmath91 , ( [ eps_teps_relation_original ] )",
    "tells us that @xmath32 should become smaller as @xmath129 as the algorithms converge . as a result",
    ", we have to solve inner linear systems with higher accuracy as @xmath39 becomes more accurate . more generally , this is the case when @xmath130 is not small and typically of @xmath131 .",
    "therefore , for @xmath132 and more general @xmath133 , the resulting method and sia type methods are similar and no winner in theory .",
    "they are common in that they all require to solve inner linear systems accurately for some steps and they are different in that the former solves inner linear systems with poor accuracy initially and then with increasing accuracy as the algorithm converges , while the latter ones solve inner linear systems with high accuracy in some initial outer iterations and then with decreasing accuracy as the algorithms converge",
    ".    based on ( [ eps_teps_relation_general ] ) , it is natural for us to maximize its upper bound with respect to @xmath133 for a fixed @xmath91 .",
    "this will make @xmath32 is as small as possible , so that we pay least computational efforts to solve ( [ ls_unified ] ) .",
    "this amounts to minimizing @xmath130 .",
    "as is well known , the optimal @xmath133 is @xmath134 such @xmath106 corresponds to the choice @xmath76 and @xmath75 in ( [ ls_unified ] ) , exactly leading to linear system ( [ ls_jd_1 ] ) in the jd method .",
    "therefore , in the sense of minimizing @xmath130 , the jd method is the best .",
    "if we take @xmath135 , which is the approximation to @xmath69 in sira , by letting @xmath74 and @xmath75 , then ( [ ls_unified ] ) becomes @xmath136 which is exactly the linear system in the sira method . in each of jd and sira",
    ", @xmath130 is the residual norm of an approximate eigenpair @xmath137 of @xmath28 .    in what follows ,",
    "we denote @xmath32 by @xmath138 and @xmath139 in the sira and jd methods , respectively . to derive our final and key relationships between @xmath140 and @xmath91 , we need the following lemma , which is direct from theorem 6.1 of @xcite and establishes a close and compact relationship between @xmath141 and the residual norm @xmath130 .    [",
    "lem : upper_bound_sin_y_x ] suppose @xmath142 is a simple desired eigenpair of @xmath143 and let @xmath144 be unitary",
    ". then @xmath145{\\mathbf{b}}\\left[\\begin{array}{cc}{\\mathbf{x } } & { \\mathbf{x}}_\\perp \\end{array}\\right]= \\left[\\begin{array}{cc}\\frac{1}{\\lambda-\\sigma } & { \\mathbf{c}}^h \\\\ \\mathbf{0 } & { \\mathbf{l}}\\end{array}\\right],\\ ] ] where @xmath146 and @xmath147 .",
    "let @xmath137 be an approximation to @xmath142 , assume that @xmath133 is not an eigenvalue of @xmath148 and define @xmath149 then @xmath150    combining ( [ upper_bound_sin_y_x ] ) with theorem  [ thm4 ] , we obtain one of our main results .",
    "[ thm5 ] assume that @xmath133 is an approximation to @xmath69 and is not an eigenvalue of @xmath148 .",
    "then @xmath151 in particular , for @xmath135 and @xmath152 , which correspond to the sira and jd methods , respectively , assume that each of them is not an eigenvalue of @xmath148 .",
    "then it holds that @xmath153 and @xmath154    this theorem shows that once @xmath92 is known we can a - priori determine the accuracy requirements @xmath138 and @xmath139 on approximate solutions of inner linear systems ( [ lssira ] ) and ( [ ls_jd ] ) .",
    "it is important to observe from ( [ upper_bound_eps ] ) that @xmath155 if @xmath133 is well separated from the eigenvalues of @xmath28 other than @xmath69 and @xmath28 is normal or mildly non - normal and @xmath125 is not small . for @xmath125 small , noting that bound ( [ upper_bound_eps ] ) is compact , we are lucky to have a bigger @xmath32 , i.e. , to solve the inner linear system with less accuracy . if @xmath156 is considerably smaller than @xmath157 , then @xmath32 may be bigger than @xmath91 considerably and we are likely lucky to solve the inner linear system with less accuracy .    for the @xmath133 s in the sira and jd methods , by continuity",
    "the corresponding two @xmath156 s are close .",
    "therefore , for a given @xmath91 , we have essentially the same upper bounds for @xmath138 and @xmath139 .",
    "this means that we need to solve the corresponding inner linear systems ( [ lssira ] ) and ( [ ls_jd ] ) in the sira and jd methods with essentially the same accuracy @xmath32 . in other words ,",
    "the sira and jd methods behave very similar when ( [ lssira ] ) and ( [ ls_jd ] ) are solved with the same accuracy .",
    "in this section , we first focus on the fundamental problem of how to select @xmath91 to make the inexact sira and jd mimic the exact sira very well from the current step to the next one . then we show how to achieve our ultimate goal : the determination of @xmath32 .",
    "recall that the subspace expansion vectors are @xmath103 and @xmath102 for the exact sira and the inexact sira or jd ; see ( [ tv_and_v ] ) .",
    "define @xmath158 $ ] , @xmath159 and @xmath160 $ ] , @xmath161 . in order to make the inexact sira method mimic the exact sira method very well",
    ", we must require that the two expanded subspaces @xmath162 and @xmath163 have almost the same quality , namely , @xmath164 , whose quantitative meaning will be clear later .",
    "[ thm : sin_v_xp_div_sin_tv_xp ] with the notations above , assume @xmath165 with @xmath166 . )",
    "that @xmath167 and the exact sira , sia and jd methods terminate prematurely if @xmath168 . in this case",
    ", @xmath162 is an invariant subspace of @xmath19 and we stop subspace expansion .",
    "we will exclude this rare case .",
    "] then we have @xmath169 suppose @xmath170 is acute . if @xmath171 , we have @xmath172    since @xmath173 by @xmath174 we obtain @xmath175 which proves ( [ sin_v_xp ] ) .",
    "similarly , we have @xmath176 hence , from ( [ sin_v_xp ] ) and ( [ sin_tv_xp ] ) , we get ( [ sin_v_xp_div_sin_tv_xp ] ) .",
    "exploiting the trigonometric identity @xmath177 the angle triangle inequality @xmath178 and the monotonic increasing property of the @xmath179 function in the first quadrant , we get @xmath180    from ( [ sin_v_xp_div_sin_tv_xp ] ) , ( [ triangle ] ) and ( [ sin_tv_v_final ] ) , we obtain @xmath181 from which it follows that ( [ tau ] ) holds .    from ( [ sin_v_xp ] )",
    ", we see that @xmath182 is exactly one step subspace improvement when @xmath40 is expanded to @xmath162 .",
    "( [ tau ] ) shows that , to make @xmath183 , @xmath184 should be small .",
    "meanwhile , ( [ tau ] ) also indicates that a very small @xmath184 can not improve the bounds essentially .",
    "actually , for our purpose , a fairly small @xmath184 , e.g. , @xmath185 , is enough since we have @xmath186 and the lower and upper bounds are very near and differ marginally .",
    "therefore , @xmath163 and @xmath162 are of almost the same quality for approximating @xmath127 . as a result",
    ", it is expected that the inexact sira or jd computes new approximation over @xmath163 to the desired @xmath9 that has almost the same accuracy as that obtained by the exact sira over @xmath162 .",
    "more precisely , the accuracy of the approximate eigenpair by the exact sira and that by the inexact sira or jd are generally the same within roughly a multiple @xmath187 $ ] ( this assertion can be justified from the results in @xcite ) .",
    "so how near the constant @xmath188 is to one is insignificant , the inexact sira and jd generally mimic the exact sira very well when @xmath184 is fairly small .",
    "concisely , we may well draw the conclusion that @xmath185 makes the inexact sira mimic the exact sira very well , that is , the exact and inexact sira methods use almost the same outer iterations to achieve the convergence .",
    "next we discuss the selection of @xmath92 .",
    "once @xmath92 is available , in principle we can exploit compact bounds ( [ eps_s ] ) and ( [ eps_j ] ) to determine the accuracy requirements @xmath138 and @xmath139 on inner iterations in the sira and jd .    from the definition of @xmath184",
    ", we have @xmath189 as theorem  [ thm : sin_v_xp_div_sin_tv_xp ] requires @xmath190 , we must have @xmath191 .",
    "but @xmath192 is not available and a - priori , so we can only use a reasonable estimate on @xmath182 in ( [ teps ] ) . in the following , we will look into @xmath182 and show that it is actually independent of the quality of the approximate eigenvector @xmath39 , i.e. , @xmath141 , and the subspace quality , i.e. , @xmath193 .",
    "this means that @xmath182 stays around some constant during outer iterations .",
    "then we analyze its size , which is shown to be problem dependent and stay around some certain constant during outer iterations . based on these results",
    ", we can propose a general practical selection of @xmath92 .",
    "obviously , in order to achieve a given @xmath184 , the smaller @xmath182 is , the smaller @xmath91 must be and the more accurately we need to solve the inner linear system .",
    "we now investigate @xmath194 and show that it is bounded independently of @xmath141 and @xmath193 , so is @xmath195 . from ( [ expand ] ) and ( [ tv_and_v ] ) , it is known that @xmath103 and @xmath196 are in the same direction .",
    "therefore , from decomposition ( [ decompose_y ] ) of @xmath39 , we have @xmath197 note that @xmath198 and @xmath199 . so @xmath200 combining ( [ cosvxp ] ) and ( [ bybound ] ) , we have @xmath201 a seemingly trivial bound .",
    "however , the proof clearly shows that our derivation is general and does not miss anything essential .",
    "we are not able to make the bound essentially sharper and more elegant as the inequalities used in the proof can not be sharpened generally .",
    "nevertheless , this is enough for our purpose .",
    "a key implication is that the bound is independent of @xmath141 and @xmath193 , so @xmath194 is expected to be around some constant during outer iterations , so is @xmath195 .",
    "it is possible to estimate @xmath182 in some important cases . for the starting vector @xmath10",
    ", it is known that the exact sira , sia and jd methods work on the standard krylov subspaces @xmath202 and @xmath203 .",
    "here we have temporarily added iteration subscripts and assume that the current iteration step is @xmath14 .",
    "it is direct from ( [ sin_v_xp_div_sin_tv_xp ] ) to get @xmath204 where the @xmath205 are exact subspace expansion vectors and @xmath206at steps @xmath207 .",
    "for the krylov subspaces @xmath208 and @xmath209 , there have been some estimates on @xmath210 in @xcite . for @xmath28",
    "is diagonalizable , suppose all the @xmath211 and @xmath0 are real and @xmath69 is also the algebraically largest eigenvalue of @xmath28 , and define @xmath212 then it is shown in @xcite that @xmath213 where @xmath214 is a certain constant only depending on @xmath10 and the conditioning of the eigensystem of @xmath28 .",
    "so , ignoring the constant factor @xmath214 , we see the product @xmath215 converges to zero at least as rapidly as @xmath216 as we have argued , all the @xmath217 , @xmath207 , stay around a certain constant",
    ". so basically , each step subspace improvement @xmath218 , behaves like and is no more than the factor @xmath219 the average convergence factor for one step .",
    "returning to our notation , we see the size of @xmath182 crucially depends on the eigenvalue distribution .",
    "the better @xmath69 is separated from the other eigenvalues of @xmath28 , the smaller @xmath182 is .",
    "conversely , if @xmath69 is poorly separated from the others , @xmath182 may be near to one . for more complicated complex eigenvalues and/or @xmath0 ,",
    "quantitative results are obtained for @xmath210 and similar conclusions are drawn in @xcite .",
    "however , we should point that these estimates may be conservative and also only predict linear convergence . in practice ,",
    "a slightly superlinear convergence may occur sometimes , as has been observed in @xcite .    for @xmath185 ,",
    "if @xmath220 $ ] , then by ( [ teps ] ) we have @xmath221 $ ] .",
    "such @xmath182 means that @xmath69 is well separated from the other eigenvalues of @xmath28 and the exact sira generally converges fast . in practice , however , for a given @xmath91 we do not know the value of @xmath184 produced by @xmath91 as @xmath222 and its bound are not known . for a given @xmath91 ,",
    "if we are unlucky to get a @xmath184 not small like @xmath223 , the inexact sira may use more outer iterations than the exact sira .",
    "suppose we select @xmath224 .",
    "then if each @xmath225 , we get @xmath185 . for this case , we have a very good subspace @xmath226 for @xmath227 since @xmath228 , so the exact sira generally converges very fast ! for a real - world problem ,",
    "however , one should not expect that @xmath69 is generally so well separated from the other eigenvalues that the convergence can be so rapid .",
    "therefore , we generally expect that @xmath229 $ ] makes @xmath230 , so that the inexact sira and jd mimic the exact sira very well .",
    "summarizing the above , we propose taking @xmath231.\\label{tildeepsilon}\\ ] ]    our ultimate goal is to determine @xmath138 and @xmath139 for the inexact sira and jd .",
    "compact bounds ( [ eps_s ] ) and ( [ eps_j ] ) show that they are generally of @xmath232 . however , it is impossible to compute the bounds cheaply and accurately",
    ". we will consider their practical estimates on @xmath138 and @xmath139 in section  [ issue ] , where we demonstrate that these estimates are cheaply obtainable .",
    "due to the storage requirement and computational cost , algorithms  [ alg : sira][alg : jd ] will be impractical for large steps of outer iterations . to be practical , it is necessary to restart them for difficult problems .",
    "let @xmath233 be the maximum of outer iterations allowed .",
    "if the basic sira and jd algorithms do not converge , then we simply update @xmath10 and restart them .",
    "we call the resulting restarted algorithms algorithms  34 , respectively .    in implementations",
    ", we adopt the following strategy to update @xmath10 . for outer iteration steps",
    "@xmath234 during the current cycle , suppose @xmath235 is the candidate for approximating the desired eigenpair @xmath236 of @xmath19 at the @xmath237-th outer iteration",
    ". then we take @xmath238 as the updated starting vector in the next cycle .",
    "such a restarting strategy guarantees that we use the _ best _ candidate ritz vector in the sense of ( [ revector ] ) to restart the algorithms .    in what follows",
    "we consider some practical issues and design practical stopping criteria for inner iterations in the ( non - restarted and restarted ) inexact sira and jd algorithms .    given @xmath91 ,",
    "since @xmath148 is not available , it is impossible to compute @xmath239 and @xmath240 in ( [ eps_s ] ) and ( [ eps_j ] ) .",
    "also , we can not compute @xmath241 in ( [ eps_s ] ) and ( [ eps_j ] ) . in practice",
    ", we simply replace the insignificant factor @xmath241 by one , which makes @xmath138 and @xmath139 as small as possible , so that the inexact sira and jd algorithms are the safest to mimic the exact sira .",
    "we replace @xmath157 by @xmath242 in the inexact sira and jd , respectively . for @xmath239",
    ", we can exploit the spectrum information of @xmath46 to estimate it .",
    "let @xmath243 be the other eigenvalues ( ritz values ) of @xmath46 other than @xmath47 .",
    "then we use the estimate @xmath244 note that it is very expensive to compute @xmath68 but @xmath245 .",
    "so we simply use @xmath246 to estimate @xmath247 . with these estimates and taking the equalities in compact bounds ( [ eps_s ] ) and ( [ eps_j ] )",
    ", we get @xmath248 it might be possible to have @xmath249 for a given @xmath92 .",
    "this would make @xmath250 no accuracy as an approximation to @xmath27 . as a remedy , from now on we set @xmath251 for @xmath252",
    ", we simply set @xmath253 .",
    "note that @xmath254 is a - priori and uncomputable .",
    "we are not able to determine whether it is below @xmath32 or not .",
    "however , it is easy to verify that @xmath255 and @xmath256 where @xmath257 and @xmath258 , the restriction of @xmath28 to the orthogonal complement of @xmath259 .",
    "alternatively , based on the above two relations , in practice we require that inner solves stop when the a - posteriori computable relative residual norms @xmath260 and @xmath261 for the inexact sira and jd , respectively .",
    "_ in @xcite , a - priori accuracy requirements have been determined for inner iterations in sia type methods . in computation ,",
    "a - posteriori residuals are intuitive , and are probably the only practical way to approximate the a - priori residuals . here , by the above lower and upper bounds ( [ prioris ] ) and ( [ priorij ] ) that relate the a - posteriori relative residuals to the a - priori errors of approximate solutions , we have simply demonstrated that ( [ stopcrit ] ) and ( [ stopcritjd ] ) are reasonable stopping criteria for inner solves .",
    "we see that the a - priori errors and the a - posteriori errors are definitely comparable once the linear systems are not ill conditioned .",
    "we report numerical experiments to confirm our theory .",
    "our aims are mainly three - fold : ( i ) regarding outer iterations , for fairly small @xmath262 and @xmath263 , the ( non - restarted and restarted ) inexact sira and jd behave very like the ( non - restarted and restarted ) exact sira .",
    "even a bigger @xmath264 often works very well .",
    "( ii ) regarding inner iterations and overall efficiency , the inexact sira and jd algorithms are considerably more efficient than the inexact sia . (",
    "iii ) sira and jd are similarly effective .",
    "all the numerical experiments were performed on an intel ( r ) core ( tm)2 quad cpu q9400 @xmath265ghz with main memory 2 gb using matlab 7.8.0 with the machine precision @xmath266 under the microsoft windows xp operating system .    at the @xmath14th step of the inexact sira or jd method , we have @xmath267 .",
    "let @xmath268 be the eigenpairs of @xmath269 , which are ordered as @xmath270 we use the ritz pair @xmath271 to approximate the desired eigenpair @xmath236 of @xmath19 , and the associated residual is @xmath272 .",
    "we stop the algorithms if @xmath273 in the inexact sira and jd , we stop inner solves when ( [ stopcrit ] ) and ( [ stopcritjd ] ) are satisfied , respectively , and denote by sira(@xmath92 ) and jd(@xmath92 ) the inexact sira and jd algorithms with the given parameter @xmath92 .",
    "we use the following stopping criteria for inner iterations in the exact sira and sia algorithms and the inexact sia algorithm .",
    "* for the exact  sira algorithm , we require the approximate solution @xmath274 to satisfy @xmath275 * for the inexact sia algorithm , we take the same outer iteration tolerance @xmath276 , and use the stopping criterion ( 3.14 ) in @xcite for inner solve , where @xmath277 and the steps @xmath14 suitably bigger than the number of outer iterations used by the exact sira so as to ensure the convergence of the inexact sia with the same accuracy . for the restarted inexact sia ,",
    "we take @xmath14 the maximum outer iterations @xmath278 allowed for each cycle .    in the numerical experiments",
    ", we always take the zero vector as an initial approximate solution to each inner linear system and solve it by the right - preconditioned gmres(30 ) method .",
    "outer iterations start with the normalized vector @xmath279 . for the correction equation in the jd method",
    ", we use @xmath280 the restriction of @xmath281 to the orthogonal complement of @xmath282 , as a preconditioner , which is suggested in @xcite .",
    "@xmath283 means the inverse of @xmath284 restricted to the orthogonal complement of @xmath282 . here",
    "@xmath285 is some preconditioner used for all the inner linear systems involved in the algorithms tested except jd .",
    "we use the matlab function @xmath286=ilu(a - sigma*speye(n),setup)$ ] to compute the sparse incomplete lu factorization of @xmath287 with a given dropping tolerance @xmath288 .",
    "we then take @xmath289 .",
    "van der vorst @xcite shows how to use @xmath284 as a left preconditioner for ( [ ls_jd ] ) .",
    "it can also be used a right preconditioner for ( [ ls_jd ] ) in the same spirit . adapted from @xcite",
    ", we briefly describe how to do so .",
    "suppose that a krylov solver for ( [ ls_jd ] ) with right - preconditioning starts with zero vector as an initial guess to the solution .",
    "then the starting vector for the krylov solver is @xmath290 , which is in the subspace orthogonal to @xmath291 , and all iteration vectors for the krylov solver are in that subspace .",
    "we compute @xmath292 for a vector @xmath293 supplied by the krylov solver at each inner iteration .",
    "let @xmath294 and note that @xmath295 .",
    "then it follows that @xmath296 where @xmath297 .",
    "equivalently , @xmath298 .",
    "again , using @xmath295 , we have @xmath299 , i.e. , @xmath300 .",
    "therefore , we can compute @xmath292 by @xmath301    in all the tables below , we denote by @xmath302 the number of outer iterations to achieve the convergence , by @xmath303 the total number of inner iterations , i.e. , the products of the matrix @xmath304 by vectors used by the krylov solver , by @xmath305 the times of @xmath306 , by @xmath307 the total cpu time of solving the small eigenproblems , by @xmath308 the total cpu time of generating the orthonormal basis @xmath309 and forming the projection matrix @xmath46 , by @xmath310 the time of constructing the preconditioner and by @xmath311 the total cpu time of the krylov solver for solving right - preconditioned inner linear systems .",
    "we point out that the ( inexact and exact ) sira and jd methods must form the projection matrices explicitly while sia does not and it gives its projection matrix as a byproduct when generating the orthonormal basis of @xmath309 . as a result , for the same dimension of subspace , @xmath308 for sia is smaller than that for sira and jd .",
    "this will be confirmed clearly in later numerical experiments , and we will not mention this observation later . for examples 13",
    "we test algorithms  [ alg : sira][alg : jd ] , the inexact sia and exact sira ; for example 4 we test these algorithms and the restarted algorithms  34 as well as the restarted inexact sia .",
    "* example 1 .",
    "* this problem is a large nonsymmetric standard eigenvalue problem of cry10000 of @xmath312 that arises from the stability analysis of a crystal growth problem from @xcite .",
    "we are interested in the eigenvalue nearest to @xmath313 .",
    "the computed eigenvalue is @xmath314 .",
    "the preconditioner @xmath315 is obtained by the sparse incomplete lu factorization of @xmath316 with @xmath317 .",
    "table  [ tab_cry10000 ] reports the results obtained , and the left and right parts of figure  [ fig_cry10000 ] depict the convergence curve of @xmath318 versus @xmath302 and the curve of @xmath303 versus @xmath302 for the algorithms , respectively .    .",
    "left : relative outer residual norms versus outer iterations .",
    "right : the numbers of inner iterations versus outer iterations.__,height=219 ]    . left : relative outer residual norms versus outer iterations . right : the numbers of inner iterations versus outer iterations.__,height=219 ]    .__example 1 .",
    "cry10000 with @xmath313 ( the unit of @xmath319 is @xmath320 second ) .",
    "_ _ [ cols=\">,>,>,>,>,>,>,>,>,>\",options=\"header \" , ]     it is seen from table  [ tab_dw8192_restart ] and the left part of figure  [ fig_dw8192_restart ] that all the algorithms other than sira(@xmath321 ) and jd(@xmath321 ) solved the problem very successfully with no more than three restarts used and the convergence processes were very smooth .",
    "the restarted inexact sia behaved like the restarted exact sira well but not so well as the restarted sira and jd with @xmath322 , which behaved very like the restarted exact sira in the first two restarts and almost converged to our prescribed convergence accuracy at the second restart .",
    "we also find that , compared with table  [ tab_dw8192_restart ] , the restarted sira(@xmath263 ) , jd(@xmath263 ) and exact sira performed excellently since @xmath302 s used by them were very near to the ones used by their corresponding non - restarted versions , respectively . for the restarted sira(@xmath321 ) and jd(@xmath321 ) , the case that @xmath306 occurred at @xmath323 of outer iterations .",
    "they did not mimic the exact sira well and used considerably more outer iterations than the inexact sira and jd with @xmath324 and @xmath322 .",
    "so @xmath325 is not a good choice for the restarted inexact sira and jd for this example , though @xmath303 and the total computing time are not so considerably more than those used by the algorithms with @xmath326 .    regarding the overall performance , for given @xmath324 and @xmath322 , the restarted sira and jd algorithms performed very similarly and were about more than twice as fast as the restarted inexact sia , in terms of both @xmath303 and the total computing time ( actually @xmath311 now ) .",
    "during the last cycle , the restarted inexact sira(@xmath263 ) and jd(@xmath263 ) had already achieved the convergence at the tenth and eighth outer iteration , respectively .",
    "so we stopped the algorithm at that step and actually solved only about a third of twenty - nine inner linear systems needed to solve in each of the previous cycles .",
    "as a result , the number of inner iterations needed in the last circle was also about a third of that needed in each of the first three cycles .",
    "this is the reason why , in the right part of figure  [ fig_dw8192_restart ] , the curves for the restarted sira(@xmath263 ) and jd(@xmath263 ) had a drastic decrease at last restart .",
    "as is expected , the restarted inexact sira and jd algorithms used almost constant inner iterations for the same @xmath91 per restart , while the inexact sia used fewer and fewer inner iterations as outer iterations converged .",
    "the figure clearly shows that the restarted inexact sia used much more inner iterations than the restarted sira(@xmath263 ) and jd(@xmath263 ) at each of the first three cycles .",
    "we see from tables  [ tab_dw8192][tab_dw8192_restart ] that for this example the dominant cost is still paid to the solutions of preconditioned inner linear systems but unlike examples 13 the construction of preconditioners is very cheap and negligible , compared with @xmath311 .    in summary , it is seen from all the numerical experiments that both @xmath327 and @xmath311 are reasonable measures of overall performance of sira , jd and sia algorithms .",
    "we have tested some other problems .",
    "we have also tested the algorithms when tuning is applied to our preconditioner @xmath315 @xcite . all of them have shown that the inexact sira and jd mimic the inexact sia and the exact sira very well for @xmath328 and use much fewer inner iterations than the inexact sia . as far as the overall efficiency is concerned , sira(@xmath321 ) and jd(@xmath321 ) may work well and often use comparably inner iterations than sira(@xmath329 ) and jd(@xmath329 ) , but they are likely to need considerably more outer iterations and can not mimic the exact sira well .",
    "therefore , for the robust and general purpose , we propose using @xmath330 $ ] in practice .",
    "we have found that the tuned preconditioning has no advantage over the usual preconditioning and is often inferior to the latter for the linear systems involved in the inexact sira , jd and sia algorithms .",
    "for example , we have found that for example 3 the tuned preconditioning used about three times more inner iterations than the usual preconditioning .",
    "we have quantitatively analyzed the convergence of the sira and and jd methods over one step and proved that one only needs to solve all the inner linear systems involved in them with low or modest accuracy .",
    "based on the theory established , we have designed practical stopping criteria for inner iterations of the inexact sira and jd .",
    "numerical experiments have illustrated that our theory works very well and the non - restarted and restarted inexact sira and jd algorithms behave very like the non - restarted and restarted exact sira algorithms . meanwhile , we have confirmed that the inexact sira and jd algorithms are similarly effective and both of them are much more efficient than the inexact sia algorithms .",
    "it is known that the ( inexact ) jd method with variable shifts is used more commonly . the analysis approach proposed in this paper",
    "may be extended to analyze the accuracy requirements of inner iterations in the jd method with variable shifts and a rigorous general theory may be expected .",
    "this work is in progress .",
    "since the harmonic projection may be more suitable to solve the interior eigenvalue problem , it is very significant to consider the harmonic version of sira .",
    "moreover , it is known that the standard projection , i.e. , the rayleigh  ritz method , and its harmonic version may have convergence problem when computing eigenvectors @xcite .",
    "so it is worthwhile and appealing to use the refined rayleigh ",
    "ritz procedure @xcite and the refined harmonic version @xcite for solving the large eigenproblem considered in this paper .",
    "these constitute our future work ."
  ],
  "abstract_text": [
    "<S> using a new analysis approach , we establish a general convergence theory of the shift - invert residual arnoldi ( sira ) method for computing a simple eigenvalue nearest to a given target @xmath0 and the associated eigenvector . in sira , </S>",
    "<S> a subspace expansion vector at each step is obtained by solving a certain inner linear system . </S>",
    "<S> we prove that the inexact sira method mimics the exact sira well , that is , the former uses almost the same outer iterations to achieve the convergence as the latter does if all the inner linear systems are iteratively solved with _ low _ or _ modest _ accuracy during outer iterations . based on the theory , we design practical stopping criteria for inner solves . </S>",
    "<S> our analysis is on one step expansion of subspace and the approach applies to the jacobi  davidson ( jd ) method with the fixed target @xmath0 as well , and a similar general convergence theory is obtained for it . </S>",
    "<S> numerical experiments confirm our theory and demonstrate that the inexact sira and jd are similarly effective and are considerably superior to the inexact sia .    </S>",
    "<S> * keywords . </S>",
    "<S> * subspace expansion , expansion vector , inexact , low or modest accuracy , the sira method , the jd method , inner iteration , outer iteration .    </S>",
    "<S> * ams subject classifications . </S>",
    "<S> * 65f15 , 15a18 , 65f10 . </S>"
  ]
}