{
  "article_text": [
    "in recent years , photometric redshift ( often abbreviated as photo - z ) estimation has become a vital and widespread tool in the astronomical repertoire .",
    "large amounts of photometric data are produced by sky surveys such as the sloan digital sky survey ( * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* sdss ) , but spectroscopic measurements are more scarce due to the longer observing times required .",
    "therefore it is important to accurately estimate the redshift  and thus the distance  of objects based just on their photometry .",
    "there are two distinct approaches to photo - z estimation  the empirical approach starts from a training set with known redshifts and uses a machine learning method to perform the estimation @xcite , while the template - based approach fits a spectral template using synthetic photometry .",
    "the former approach is generally more accurate @xcite , but the latter does not require an extensive training set , and is thus more flexible",
    ".    within the family of template - based photometric redshift estimation methods , there are two main branches : those that search for the maximum likelihood , best - fitting spectral energy distribution ( sed ) template and record the redshift of this single sed @xcite , and bayesian methods that aim to reproduce the full posterior redshift probability distribution based on the observations @xcite .",
    "the latter method provides a more detailed answer , but it is computationally more expensive , and it does need more input information in the form of a prior ( or , with a flat prior , the peak of the probability distribution gives the same result as the maximum likelihood method ) .",
    "large photometric catalogs are most often stored in relational databases ( e.g. _ _ skyserver _ _ for sdss ) , and cross - matching tools that connect such databases also use the relational model @xcite .",
    "it would be advantageous to have a photo - z implementation that can work directly on data in the servers , and on results from cross - matches , by integrating into the well - established sql language .",
    "there are various software available that utilize either the template - based or the empirical approach to perform photometric redshift estimation .",
    "however , to the best of our knowledge , none of the public codes have the ability to perform the computations within the database itself .",
    "thus , they require moving the photometric catalogs outside of the database , and the results also have to be loaded into the db separately . this is a cumbersome process , especially when the amount of data is in the pb range , as will be the case with upcoming surveys such as the lsst @xcite and pan - starrs @xcite .",
    "the problem is also acute when on - demand photo - z would be needed to quickly process cross - match results .",
    "additionally , existing template - based photometric redshift estimation codes use a predefined set of broad - band photometric filters , and compute synthetic magnitudes in all of these bands before the actual photo - z calculations .",
    "this approach is not ideal for heterogeneous observations , such as the hubble source catalog @xcite , where each object can have a different corresponding filter set , and the total number of filters is much larger than the number of available filters for an object .",
    "also , this issue can become especially prominent if we consider that filter transmission curves can change over time ",
    "if this time - dependence is to be properly taken into account , precomputing synthetic magnitudes is simply not an option .",
    "we sought to address these issues with our own implementation , named photo - z - sql .",
    "we decided to adopt the template - based photo - z approach , since sufficiently large training sets are usually not available when dealing with cross - match results and variable filter sets .",
    "the code has been developed in the c # language , which can be run stand - alone , but can also be integrated into microsoft sql server as a set of user - defined functions .",
    "microsoft sql server is a commercial db server application , which has been chosen because it enables using a general - purpose computer language via clr integration ( see @xcite for another astronomy - related use case ) , and also because it is used by e.g. the sdss and pan - starrs collaborations .",
    "the database integration has the added benefit of utilizing the computational capabilities of db servers , which often far exceed those of common workstations .",
    "parallel execution is supported , taking advantage of modern multi - core systems .",
    "the objective - oriented power of c # also leads to flexibility in defining and providing priors , templates and filters .",
    "furthermore , we use lazy evaluation for synthetic magnitudes , computing them only when they are needed , and caching them to be reused .",
    "our code implements both the maximum likelihood and the bayesian template - based photo - z method as separate functions , following the standard approaches in the literature @xcite . this section details the calculations performed by our algorithm .      the maximum likelihood method can be summarized with the following formulae :    @xmath0    @xmath1    where @xmath2 and @xmath3 are the redshift and the template sed , @xmath4 is a factor that scales the total flux of the template spectrum , @xmath5 is the measured broad - band magnitude vector of an object , @xmath6 is the synthetic magnitude vector of template @xmath3 at redshift @xmath2 , and finally , @xmath7 is the covariance matrix of magnitude errors between filters .",
    "the fitting process involves iterating over a set of redshifts and templates , but not over @xmath4 , since for a given @xmath2 and @xmath3 the @xmath4 value that yields the lowest @xmath8 can be determined algebraically .",
    "@xmath9 denote the set of best - fitting parameters , corresponding to the lowest @xmath8 value found during the iteration .",
    "@xmath10 is the photometric redshift estimate .",
    "most applications assume that magnitude errors are uncorrelated , in addition to being gaussian with a standard deviation matching the estimated measurement error for the given object . under the uncorrelated assumption ,",
    "the @xmath7 covariance matrix takes the following form :    @xmath11    where @xmath12 is the estimated magnitude error vector of the object , and @xmath13 is the kroenecker delta .",
    "the expression for @xmath8 can thus be simplified under this assumption :    @xmath14    however , in many cases this assumption may simply not hold @xcite . for this reason",
    ", our code does implement using the full @xmath7 covariance matrix , which , to our knowledge , is a missing feature in other public codes .      using the notation introduced in sec .",
    "[ sec : ml ] , the bayesian approach starts from the following expression :    @xmath15    @xmath16 is the probability of a redshift and a template sed  scaled to a flux  given the data , i.e. the measured magnitudes and magnitude errors ( optionally taking the covariance between filters into account ) .",
    "@xmath17 is the prior probability of the given template sed at the given redshift .",
    "@xmath18 is the likelihood that the given template and redshift produce the data .",
    "the denominator contains a factor that normalizes the total probability to @xmath19 , but in practice this is not computed , and instead the final probability distribution is normalized .",
    "we note that we use the @xmath20 term as shorthand for integrating over the model parameters that describe a template sed , @xmath21 does not represent a real number .",
    "the likelihood term can be formulated in the following way :    @xmath22    where @xmath8 is the expression defined in eq .",
    "[ eq : chi2 ] . again , assuming uncorrelated magnitude errors , the simplified version in eq .",
    "[ eq : chi2simple ] can be applied , but our application does support using the full @xmath7 covariance matrix .",
    "the method aims to extract the posterior redshift probability distribution , @xmath23 , which can be obtained by integrating out the nuisance parameters in eq .",
    "[ eq : bayes ] :    @xmath24    in the actual implementation , the integrals are discretized , replaced by a summation over a predetermined set of template parameters and a range in flux .",
    "also , the redshift is only evaluated at the points of a predetermined grid .",
    "the exact form of prior to use has to be chosen from the list currently available ( see sec .  [",
    "sec : sqlinterface ] and sec .",
    "[ sec : configurations ] ) , but further priors could be easily added to the code .",
    "the output result is the @xmath23 posterior redshift probability distribution , normalized to an integral probability of @xmath19 on the given redshift grid .",
    "microsoft sql server provides a unique opportunity among relational database management systems in its common language runtime ( clr ) integration feature .",
    "compiled assemblies produced by any common language infrastructure ( cli ) programming language , which include c++ , c # and f # , can be loaded into a db on a server , thus granting access to code with arbitrarily complex functionality .",
    "the main difficulty in sql - clr integration pertains to the sql interface of the code , which has to translate between storage types of the programming language and the types available in sql , and additionally it has to conform to the limitations of user - defined functions and user - defined stored procedures in sql .",
    "conversions between basic types such as integers , floating - point numbers and strings are fairly trivial , but more complex containers do need to be implemented on a case - by - case basis , relying on the binary storage types of sql , or potentially on user - defined types .",
    "the publicly available _ sqlarray _",
    "library @xcite is a good example of this process , providing variable - length array functionality in sql .",
    "most algorithms need to store data , maintain a state between different execution steps . in a sql interface , functions called in subsequent queries",
    "would normally only be able to share data by storing it on a disk , in db tables using sql types .",
    "however , in our implementation we bypass this limitation , and maintain a state in - memory without needing to convert to sql types ( see below ) .",
    "the photo - z - sql library has been coded in c#. the interface consists of a set of user - defined sql functions which can be used to first configure the photo - z calculation engine , and then perform the computations themselves .",
    "parallel execution is fully supported to utilize the multi - core processors of typical db servers .",
    "we provide install and uninstall scripts to make the installation process straightforward .",
    "the remainder of this section gives more details about our implementation and interface in relation to the database integration .",
    "the data needs of the algorithm are the following :    * observed fluxes / magnitudes and errors for every object ( perhaps with a @xcite extinction map value ) , * the transmission curves corresponding to the broad - band filters , * the spectral templates that are considered in the fit , * information about the prior , * additional configuration details such as the resolution in redshift and luminosity space .",
    "in addition to the fluxes / magnitudes , the filter set may also change from object to object , but the rest of the data are static in a single estimation run , and thus can be pre - loaded and stored in the db server .",
    "additionally , the synthetic magnitude cache corresponding to the given configuration also has to be stored .",
    "there is a way to perform this storage in - memory , without moving data into temporary tables : a static c # class that uses the singleton pattern will retain its state in microsoft sql server between function calls .",
    "therefore we created a singleton wrapper class that stores the photo - z configuration and synthetic magnitudes .",
    "currently only a single , global configuration can exist in a db , but in the future this can easily be extended to allow separate configurations , e.g. linked to a user guid .",
    "the db server needs access to filter and template curves .",
    "while this could be solved by locally loading the required data into tables , it would put an extra burden on users to create them , and to ensure that the photo - z code accesses the data correctly .",
    "instead , we decided to use the spectrum services and filter profile services of the virtual observatory ( vo )",
    ". users can choose from the existing templates and filters , or they can upload their own .",
    "this solution has the added benefit that the identifiers used in the virtual observatory can uniquely link the fluxes / magnitudes of objects to corresponding filters .",
    "the filter curves are also cached , thus they have to be downloaded from the vo only once .",
    "the per - object data is expected to come from tables in the local db server , supplied to the sql functions of the photo - z - sql library . in order to allow variable - length array inputs of fluxes / magnitudes ( and their errors ) , we used the _ sqlarray _",
    "library @xcite , which can also be installed into a db server via a simple script .",
    "the overall setup of photo - z - sql is illustrated in fig .",
    "[ fig : setup ] .          interaction with the photo - z - sql code is achieved with a collection of user - defined sql functions that are called from the db .",
    "there are two types of functions , config functions that set up the photo - z configuration , and compute functions that perform the photo - z estimation itself .",
    "when functions require a filter or template , they can be specified either with their vo integer identifier , or a complete url address .",
    "these two options correspond to two versions of the given function , with a _ i d or _ url suffix in the function name , respectively . here",
    "we briefly list the main functions and their role , and an example query of a complete setup is provided in [ sec : queryexample ] .",
    "* * config.setuptemplatelist * specifies the list of sed templates to use in the photo - z estimation , along with the resolution in redshift and luminosity .",
    "either the number of steps around the best - fitting luminosity can be given , or alternatively a range of physical luminosities ( _ luminosityspecified suffix ) .",
    "* * config.setupextinctionlaw * specifies the reference spectrum , @xmath25 dust parameter and extinction law to use when correcting for galactic extinction ( see sec .",
    "[ sec : extinction ] for more details ) . *",
    "* config.removeextinctionlaw * removes the data associated with the extinction law ",
    "galactic extinction correction is no longer applied .",
    "* * config.setupflatprior * specifies that a flat prior should be used ( this is the default prior ) . * * config.setupbenitezhdfprior * specifies that the hdf - n prior of @xcite should be used ( see sec .",
    "[ sec : configurations ] for more details ) . *",
    "* config.setupabsolutemagnitudelimitprior * specifies a maximum limit in absolute magnitude as the prior , in the provided reference filter .",
    "cosmological parameters are also needed to define the distance",
    " redshift relation . * * config.setuptemplatetypeprior * specifies a prior that assigns a given probability to each of the used sed templates . *",
    "* config.removeinitialization * removes all data connected to the photo - z configuration , including cached filters , templates and synthetic magnitudes . * * compute.photozminchisqr * performs maximum likelihood photo - z estimation , returning a single scalar @xmath10 value ( see sec .",
    "[ sec : ml ] ) .",
    "the prior is not used here , and luminosity is only evaluated at the best - fitting value .",
    "magnitudes ( or fluxes ) and their errors , and corresponding filter identifiers are needed .",
    "an extinction map value can be specified , and the fit itself can be done in either magnitude or flux space .",
    "also , an extra uncorrelated variance term can be added to the observational errors ( see sec .  [",
    "sec : configurations ] ) .",
    "* * compute.photozbayesian * performs bayesian photo - z estimation , returning the entire posterior redshift probability distribution within the specified coverage as a sql table ( see sec .",
    "[ sec : bayesian ] for more details ) .",
    "input parameters are the same as in compute.photozminchisqr .",
    "in this section , we detail some additional the features of the implementation that could be noteworthy to users .",
    "the computation of synthetic magnitudes is relatively expensive , as it involves integrating a filter curve with an sed curve .",
    "this has to be done  ideally only once  for every filter and sed pairing , at every considered redshift .",
    "available photo - z software generally solve this issue by precomputing the entire synthetic magnitude table before the actual photo - z computations . as we noted in sec .",
    "[ sec : intro ] , when the selection of filters changes between objects , or there is a large number of filters from which only a few are available at a time , or especially when filter time - dependence is to be taken into account , this choice is far from optimal .    in our implementation , we instead chose to set up a synthetic magnitude cache . the memory addresses of the filters serve as keys in a hash table , while the corresponding values are arrays of synthetic magnitudes .",
    "the arrays are indexed by the given template parameters and redshift .",
    "whenever a synthetic magnitude is needed , it is retrieved from the cache if available , or computed and then stored in the cache if not . therefore values are computed only when necessary , and only once .    with our approach , there is no requirement to know beforehand which filters will be needed . while it does include an additional hash table lookup whenever a synthetic magnitude is accessed , that is an inexpensive operation .",
    "additionally , all functions working on the cache have been programmed to support parallel access .",
    "currently , the cache has to be cleared by the user via a function call , but it would be possible to implement e.g. the deletion of old synthetic magnitudes after a time or cache size limit has been passed .      all of our photo - z algorithms have been programmed to be able to perform computations using either magnitudes or fluxes .",
    "when fluxes are used , the additive @xmath26 flux scaling parameter is replaced by a multiplicative @xmath27 parameter , but the best - fitting value of @xmath27 can still be determined algebraically for a given template sed and redshift ( see eq .",
    "[ eq : chi2 ] and sec .",
    "[ sec : ml ] ) .",
    "currently , our code supports using the ab magnitude system @xcite , and the sdss @xmath28 magnitude system @xcite , but additional systems could be readily included .",
    "functions are available for converting magnitudes and magnitude errors into fluxes and flux errors , and vice versa  conversions between magnitude systems also use this mechanism .",
    "ideally , all photometric magnitude measurements should conform to the theoretical flux zeropoints of the given magnitude system , which , for example , is @xmath29 for the ab magnitude system @xcite .",
    "however , there are cases when this assumption is incorrect , and the actual zeropoint of a broad - band filter is slightly different @xcite .    to mitigate this issue",
    ", we implemented an optional zeropoint calibration algorithm which uses objects with known redshifts .",
    "the uncorrelated assumption is adopted here ( see eq .",
    "[ eq : uncorrelated ] ) , and the best - fitting template is determined at the given redshift using eq .",
    "[ eq : chi2simple ] and eq .",
    "[ eq : argmin ] for every object @xmath30 in the @xmath31 training set .",
    "then , for each of the broad - band filters , indexed by @xmath32 , a similar algebraic minimization is performed to find the filter - dependent zeropoint shift in magnitude , @xmath33 :    @xmath34    since the zeropoint shift can change the best - fitting template @xmath35 and the corresponding @xmath36 , the two minimization steps are repeated iteratively  with the latest @xmath33 shift applied to the @xmath37 measured magnitudes  until a chosen zeropoint precision is achieved .",
    "there is an additional , optional refinement to the calibration that can be applied after the iteration .",
    "if our assumptions are correct , and the data are well - described by the template spectra , the residuals scaled by the estimated photometric errors should follow the standard normal distribution",
    ". this will not be the case if the photometric errors are underestimated in a band , but the errors could be scaled by a factor @xmath38 so that the standard deviation of the scaled residual distribution is @xmath19 :    @xmath39    @xmath40    here @xmath41 is the number of objects in the training set , and the error scaling factor is linked to a given @xmath32 filter .",
    "thus , problematic passbands with ill - estimated errors or a higher proportion of badly matching templates can be downweighted during the actual photo - z estimation .",
    "we note that the same calibration algorithm can also be performed using fluxes , in that case the @xmath42 zeropoint flux multiplier is estimated for each filter .",
    "we provide a built - in implementation for correcting broad - band magnitudes for galactic extinction using the ir dust map of @xcite .",
    "at present , two different extinction models are supported by the code .",
    "the first computes eq .",
    "b2 of @xcite and assumes the @xcite extinction law in the optical and the @xcite law in the uv and ir wavelength ranges .",
    "the second calculates eq .",
    "a1 of @xcite while assuming the @xcite extinction law .",
    "the reference galaxy spectrum used in the computation can be specified freely , but it should have coverage in the entire wavelength range of the broad - band filters for which the correction is applied .",
    "to date , there have been two large publications dealing with the comparison of photometric redshift estimation methods , `` photo - z accuracy testing '' or _ phat _ by @xcite , and `` a critical assessment of photometric redshift methods : a candels investigation '' by @xcite , which we will refer to as _",
    "capr_. we use the public datasets of these articles to demonstrate the performance of our code .",
    "we adopted some of the more successful approaches in the literature in our setups @xcite .",
    "we use two different sets of galaxy template seds , the hubble udf set of @xcite , denoted by _ bpz _ , and the cosmos set of @xcite , indicated by _",
    "lp_. in the case of the _ bpz _ set , following @xcite we linearly interpolated @xmath43 galaxies between each of the @xmath44 neighboring templates to generate a total of @xmath45 seds .",
    "the wavelength coverage of these templates ends at @xmath46 , after which they were linearly extrapolated . for the _ lp _ set , adopting the choices of @xcite we used the _ le phare _ code @xcite to add emission lines of different fluxes , and to apply a selection of extinction laws with different parameters to the templates , yielding a total of @xmath47 seds .",
    "the bayesian estimation method was selected .",
    "the resolution of the redshift grid was taken to be @xmath48 .",
    "we use two different priors , a simple flat prior ( indicated by _ flat _ ) , and the apparent @xmath49-band magnitude , galaxy type and redshift prior of @xcite , empirically calibrated on hdf - n data ( denoted by _ hdf _ ) .",
    "the latter prior has been adapted to the _ lp _ template set based on galaxy type , distributing the total probability of a given type evenly among its instances . since a measured @xmath49-band magnitude may not be available , the synthetic @xmath49-band magnitude corresponding to the given parameters is used as a proxy .",
    "we found that adding an independent magnitude variance term to the measured magnitude variance can improve the estimation results .",
    "this extra error term can represent uncertainties in galactic extinction , or in the template seds themselves , and it prevents single filters with very low errors from placing too stringent limits on the fitted templates .",
    "thus , as an additional refinement , we may add an extra @xmath48 mag of error when using the _ lp _ set , or @xmath50 mag in the case of the _ bpz _ set ( shown by the tag _ err _ ) .",
    "the exact values were chosen based on the redshift estimation results of the _ phat _ dataset , but have not been fine - tuned .    the combination of the two template sets , two priors , and whether or not extra error is added yields a total of @xmath44 different configurations that we present .",
    "the phat1 dataset @xcite contains @xmath51 galaxies that have a spectroscopic redshift , with magnitude measurements in @xmath52 different broad - band filters that span the wavelength range between @xmath53 and @xmath54 ( here we do not use the extra @xmath55 irac filters ) .",
    "the measures that we report are the @xmath56 bias and @xmath57 standard deviation  excluding outliers  of the normalized redshift error , @xmath58 , and also the @xmath59 percentage of outliers .",
    "outliers are defined as having @xmath60 .",
    "@xmath10 is chosen to be the highest - probability redshift in the posterior redshift distribution defined in eq .",
    "[ eq : bayesfinal ] .",
    "the results are presented in tab .",
    "[ tab : phatresults ] , while the spectroscopic ",
    "photometric redshift scatterplots are shown in fig .",
    "[ fig : phatscatter ] . applying the _ hdf _",
    "prior is slightly beneficial for the smaller _ bpz _ template set , and somewhat detrimental in the case of the detailed _ lp _ sed set . adding the extra error term reduces the estimation bias considerably while also slightly reducing the scatter , however , the outlier rate is marginally increased .",
    "all in all , our results are comparable to those of the better - performing methods in table 5 of @xcite , whose typical values were : @xmath61 , @xmath62 and @xmath63 .",
    "we note that the _ phat _ dataset is not large enough to warrant performing the calibration detailed in sec .",
    "[ sec : calibration ] .",
    "additionally , the @xcite dust map value corresponding to the galaxies was not published , therefore galactic extinction has not been taken into account .",
    ".numerical results for the _ phat _ dataset .",
    "the @xmath56 average bias and @xmath64 standard deviation of the @xmath58 normalized redshift estimation error , and the @xmath59 outlier rate with @xmath60 outliers , for each configuration we ran . [ cols=\"^,^,^,^\",options=\"header \" , ]      photometric redshift as a function of the @xmath65 spectroscopic redshift , for the calibration tests we ran on the _ capr _ dataset .",
    "the text in the top left corner of each panel indicates the given setup .",
    "outlying galaxies with @xmath60 are shown in red , non - outlying galaxies in blue . ]",
    "in this article , we presented a new c # photometric redshift estimation code , photo - z - sql .",
    "we detailed the photo - z approaches implemented by the code , and listed the most important available features .",
    "we demonstrated the performance of our code on two public datasets that had been used for photo - z method comparison , _ phat _ and _ capr _ @xcite .",
    "our photo - z estimation results are on par with those of the better performers in the literature , as expected from our adopted configurations , e.g. choice of template seds matching @xcite .",
    "while we do not introduce anything inherently new from a photo - z estimation point of view , merely adopt some of the more successful methods in the literature , our implementation does possess the following important technical advantages :    * capability to be directly integrated into sql database servers , eliminating the need to move photometric data outside the db , and giving users and administrators an integrated platform for photo - z computations . *",
    "dynamic , on - demand handling of the set of broad - band photometric filters , which is a requirement for heterogeneous databases such as the hubble source catalog @xcite .",
    "* ability to utilize the full covariance matrix between filters .",
    "we should note that , while from a modeling standpoint it is better not to neglect the covariance between filters , in practice it may be difficult to appropriately determine the covariance matrix when no direct measurements are available .",
    "an empirical approach similar to the one in sec .",
    "[ sec : calibration ] might be adopted , but that solution will depend on the adopted template set and calibration sample .",
    "the implementation is actively in development , especially regarding priors , calibration techniques and the sql interface .",
    "the planned applications include the assembly of a photo - z table for the hubble source catalog @xcite , and integration into the sdss _ skyserver _ , or the database cross - matching platform _ _ skyquery _ _ @xcite .",
    "the main unsolved challenge in running photo - z on heterogeneous , cross - matched data is the handling of different aperture sizes used in different catalogs .",
    "the differing apertures will result in a per - object magnitude shift between photometric bands , dependent on the apparent size of the object on the sky , and on what portion of it is covered by the apertures . without going to the source images , these magnitude shifts can not be transformed out  unless there are multiple available aperture sizes for the same band , and some form of profile fitting can be utilized . more research will be needed on how strongly this issue affects photo - z results , and on how to mitigate the effects .",
    "another avenue for continuing this research could be the implementation of an empirical photo - z technique in c # , to be integrated into microsoft sql server . while the interface for specifying the training set and the learning step in sql would presumably be more difficult to implement and to use than the interface of the template - based method , in principle there are no technical obstacles since a pre - loaded state can be stored in a static c # class .",
    "furthermore , the 2016 version of microsoft sql server will provide support for r , potentially making it straightforward to perform empirical photo - z calculations directly in a db .",
    "the photo - z - sql code is available for download at https://github.com/beckrob/photo-z-sql .",
    "the realization of this work was supported by the hungarian nkfi nn grant 114560 .",
    "rb was   supported through the new national excellence program of the ministry of human capacities , hungary .",
    "the first example sets up a photo - z configuration .",
    "first it clears the potentially existing photo - z setup , specifies that @xmath66 in the input data denotes a missing value , and chooses a flat prior .",
    "then , it creates a string with vo template identifiers corresponding to the _ lp _ template set ( see sec .",
    "[ sec : configurations ] ) , and parses it into a _ sqlarray",
    "finally , this template list is given to the photo - z code , with a redshift coverage between @xmath67 and a linear step size of @xmath48 .",
    "@xmath68 steps will be taken in luminosity space around the best - fitting luminosity .",
    "--to get a sqlarray int array of template ids , a string --has to be set up with this format : ' [ 511,512,( ... ),1151 ] ' declare @idstring varchar(max ) = ' [ ' declare @id int = 511 while @id < 1151 begin     set @idstring = @idstring + cast(@id as varchar(max ) ) + ' , '     set @id = @id + 1 end set @idstring = @idstring + cast(@id as varchar(max ) ) + ' ] '        the second example performs maximum likelihood photo - z estimation on a sample of sdss data , using the @xmath69 sdss filters .",
    "the fit is performed in magnitude space , with no additional extinction correction , and with a @xmath48 mag independent error term added .",
    "select top 100 gal.objid ,        photozsql.[compute].photozminchisqr_id (             sqlarray.floatarraymax.vector_5(gal.dered_u ,                                             gal.dered_g ,                                             gal.dered_r ,                                             gal.dered_i ,                                             gal.dered_z ) ,             sqlarray.floatarraymax.vector_5(gal.modelmagerr_u ,                                             gal.modelmagerr_g ,                                             gal.modelmagerr_r ,                                             gal.modelmagerr_i ,                                             gal.modelmagerr_z ) ,             1,@filteridarray,0.0,0,0.01 ) as zphot from dr12.galaxy as gal ....",
    "select top 100 gal.objid,pz . * from dr12.galaxy as gal cross apply photozsql.[compute].photozbayesian_id (             sqlarray.floatarraymax.vector_5(gal.dered_u ,                                             gal.dered_g ,                                             gal.dered_r ,                                             gal.dered_i ,                                             gal.dered_z ) ,             sqlarray.floatarraymax.vector_5(gal.modelmagerr_u ,                                             gal.modelmagerr_g ,                                             gal.modelmagerr_r ,                                             gal.modelmagerr_i ,                                             gal.modelmagerr_z ) ,             1,@filteridarray,0.0,0,0.01 ) as pz ...."
  ],
  "abstract_text": [
    "<S> we present a flexible template - based photometric redshift estimation framework , implemented in c # , that can be seamlessly integrated into a sql database ( or db ) server and executed on - demand in sql . </S>",
    "<S> the db integration eliminates the need to move large photometric datasets outside a database for redshift estimation , and utilizes the computational capabilities of db hardware . </S>",
    "<S> the code is able to perform both maximum likelihood and bayesian estimation , and can handle inputs of variable photometric filter sets and corresponding broad - band magnitudes . </S>",
    "<S> it is possible to take into account the full covariance matrix between filters , and filter zero points can be empirically calibrated using measurements with given redshifts . </S>",
    "<S> the list of spectral templates and the prior can be specified flexibly , and the expensive synthetic magnitude computations are done via lazy evaluation , coupled with a caching of results . </S>",
    "<S> parallel execution is fully supported . for large upcoming photometric surveys such as the lsst </S>",
    "<S> , the ability to perform in - place photo - z calculation would be a significant advantage . also , the efficient handling of variable filter sets is a necessity for heterogeneous databases , for example the hubble source catalog , and for cross - match services such as skyquery . </S>",
    "<S> we illustrate the performance of our code on two reference photo - z datasets , phat and capr / candels . </S>",
    "<S> the code is available for download at https://github.com/beckrob/photo-z-sql .    </S>",
    "<S> galaxies : distances and redshifts , techniques : photometric , astronomical databases : miscellaneous </S>"
  ]
}