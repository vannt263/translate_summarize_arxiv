{
  "article_text": [
    "in the classical multi - armed bandit problem , a decision - maker repeatedly chooses from among a finite set of actions .",
    "each action generates a random reward drawn independently from a probability distribution associated with the action .",
    "the decision - maker is uncertain about these reward distributions , but learns about them as rewards are observed .",
    "strong performance requires striking a balance between _ exploring _ poorly understood actions and _ exploiting _ previously acquired knowledge to attain high rewards .",
    "because selecting one action generates no information pertinent to other actions , effective algorithms must sample every action many times",
    ".    there has been significant interest in addressing problems with more complex _ information structures _ ,",
    "in which sampling one action can inform the decision - maker s assessment of other actions .",
    "effective algorithms must take advantage of the information structure to learn more efficiently .",
    "recent work has extended popular algorithms for the classical multi - armed bandit problem , such as _ upper confidence bound _ ( ucb ) algorithms and _ thompson sampling _ , to address such contexts .    in some cases , such as classical and linear bandit problems ,",
    "strong performance guarantees have been established for ucb algorithms and thompson sampling . however , as we will demonstrate through simple analytic examples , these algorithms can perform very poorly when faced with more complex information structures .",
    "the shortcoming lies in the fact that these algorithms do not adequately assess the information gain from selecting an action .    in this paper",
    ", we propose a new algorithm  _ information - directed sampling _ ( ids )  that preserves numerous guarantees of thompson sampling for problems with simple information structures while offering strong performance in the face of more complex problems that daunt alternatives like thompson sampling or ucb algorithms .",
    "ids quantifies the amount learned by selecting an action through an information theoretic measure : the mutual information between the true optimal action and the next observation . each action is sampled in a manner that minimizes the ratio between squared expected single - period regret and this measure of information gain .",
    "as we will show through simple analytic examples , the way in which ids assesses information gain allows it to dramatically outperform ucb algorithms and thompson sampling .",
    "further , by leveraging the tools of our recent information theoretic analysis of thompson sampling @xcite , we establish an expected regret bound for ids that applies across a very general class of models and scales with the entropy of the optimal action distribution . we also specialize this bound to several classes of online optimization problems , including problems with full feedback , linear optimization problems with bandit feedback , and combinatorial problems with semi - bandit feedback , in each case establishing that bounds are order optimal up to a poly - logarithmic factor .",
    "we benchmark the performance of ids through simulations of the widely studied bernoulli , gaussian , and linear bandit problems , for which ucb algorithms and thompson sampling are known to be very effective .",
    "we find that even in these settings , ids outperforms ucb algorithms , thompson sampling , and the knowledge gradient algorithm .",
    "this is particularly surprising for bernoulli bandit problems , where thompson sampling and ucb algorithms are known to be asymptotically optimal in the sense proposed by @xcite .",
    "ids is a stationary randomized policy . _",
    "randomized _ because the ratio between expected single - period regret and our measure of information gain can be smaller for a randomized action than for any deterministic action .",
    "_ stationary _ because action probabilities are selected based only on the current posterior distribution ; this is as opposed to ucb algorithms , for example , which selects actions in a manner that depends on the current time period .",
    "it is natural to wonder whether randomization plays a fundamental role .",
    "our approach to bounding expected regret can be applied to other policies and scales with a policy - dependent statistic we call the _",
    "information ratio_. we establish that randomization is essential to our results because the information ratio can become arbitrarily large for any stationary deterministic policy .",
    "ids solves a single - period optimization problem as a proxy to an intractable multi - period problem .",
    "solution of this single - period problem can itself be computationally demanding , especially in cases where the number of actions is enormous or mutual information is difficult to evaluate . to carry out computational experiments",
    ", we develop numerical methods for particular classes of online optimization problems .",
    "we also propose mean - based ids  an approximate form of ids that is suitable for some problems with bandit feedback , satisfies our regret bounds for such problems , employs an alternative information measure , and can sometimes facilitate design of more efficient numerical methods .",
    "more broadly , we hope that our development and analysis of ids facilitate the future design of efficient algorithms that capture its benefits .",
    "it is worth noting that the problem formulation we work with , which is presented in section [ sec : formulation ] , is very general , encompassing not only problems with bandit feedback , but also a broad array of information structures for which observations can offer information about rewards of arbitrary subsets of actions or factors that influence these rewards .",
    "because ids and our analysis accommodate this level of generality , they can be specialized to problems that in the past have been studied individually , such as those involving pricing and assortment optimization ( see , e.g. , @xcite ) , though in each case , developing an efficient version of ids may require innovation .",
    "ucb algorithms are the primary approach considered in the segment of the stochastic multi - armed bandit literature that treats problems with dependent arms .",
    "ucb algorithms have been applied to problems where the mapping from action to expected reward is a linear @xcite , generalized linear @xcite , or sparse linear @xcite model ; is sampled from a gaussian process @xcite or has small norm in a reproducing kernel hilbert space @xcite ; or is a smooth ( e.g. lipschitz continuous ) model @xcite .",
    "recently , an algorithm known as thompson sampling has received a great deal of interest . @xcite provided the first analysis for linear contextual bandit problems .",
    "@xcite consider a more general class of models , and show that standard analysis of upper confidence bound algorithms leads to bounds on the expected regret of thompson sampling .",
    "very recent work of @xcite provides asymptotic frequentist bounds on the growth rate of regret for problems with dependent arms . both ucb algorithms and thompson sampling",
    "have been applied to other types of problems , like reinforcement learning @xcite and monte carlo tree search @xcite .",
    "we will describe both ucb algorithms and thompson sampling in more detail in section [ sec : beyond ucb ] .    in one of the first papers on multi - armed bandit problems with dependent arms , @xcite consider a general model in which the reward distribution associated with each action depends on a common unknown parameter .",
    "when the parameter space is finite , they provide a lower bound on the asymptotic growth rate of the regret of any admisible policy as the time horizon tends to infinity and show that this bound is attainable .",
    "these results were later extended by @xcite and @xcite to apply to the adaptive control of markov chains and to problems with infinite parameter spaces .",
    "these papers provide results of fundemental importance , but seem to have been overlooked by much of the recent literature .",
    "two other papers @xcite have used the mutual information between the optimal action and the next observation to guide action selection .",
    "each focuses on optimization of expensive - to - evaluate black - box functions . here , _",
    "black  box _ indicates the absence of strong structural assumptions such as convexity and that the algorithm only has access to function evaluations , while _ expensive - to - evaluate _ indicates that the cost of evaluation warrants investing considerable effort to determine where to evaluate .",
    "these papers focus on settings with low - dimensional continuous action spaces , and with a gaussian process prior over the objective function , reflecting the belief that `` smoother '' objective functions are more plausible than others .",
    "this approach is often called `` bayesian optimization '' in the machine learning community @xcite .",
    "both @xcite and @xcite propose selecting each sample to maximize the mutual information between the next observation and the optimal solution .",
    "several features distinguish our work from that of @xcite and @xcite .",
    "first , these papers focus on pure exploration problems : the objective is simply to learn about the optimal solution  not to attain high cumulative reward .",
    "second , and more importantly , they focus only on problems with gaussian process priors and continuous action spaces . for such problems , simpler approaches like ucb algorithms @xcite , probability of improvement @xcite , and expected improvement @xcite are already extremely effective . as noted by @xcite , each of these algorithms simply chooses points with `` _ potentially _ high values of the objective function : whether because the prediction is high , the uncertainty is great , or both . ''",
    "by contrast , a major motivation of our work is that a richer information measure is needed to address problems with more complicated information structures .",
    "finally , we provide a variety of general theoretical guarantees for information - directed sampling , whereas @xcite and @xcite propose their algorithms as heuristics without guarantees .",
    "appendix [ subsec : pure exploration ] shows that our theoretical guarantees extend to pure exploration problems .    the knowledge gradient ( kg ) algorithm uses a different measure of information to guide action selection : the algorithm computes the impact of a single observation on the quality of the decision made by a _",
    "greedy _ algorithm , which simply selects the action with highest posterior expected reward .",
    "this measure was proposed by @xcite and studied further by @xcite and @xcite .",
    "kg seems natural since it explicitly seeks information that improves decision quality .",
    "computational studies suggest that for problems with gaussian priors , gaussian rewards , and relatively short time horizons , kg performs very well .",
    "however , even in some simple settings , kg may not converge to optimality .",
    "in fact , it may select a suboptimal action in _ every _ period , even as the time horizon tends to infinity .",
    "ids also measures the information provided by a single observation , but our results imply it converges to optimality . in appendix",
    "[ sec : value of info ] , we define kg more formally , and provide some insight into why it can fail to converge to optimality .",
    "our work also connects to a much larger literature on bayesian experimental design ( see @xcite for a review ) .",
    "recent work has demonstrated the effectiveness of _ greedy _ or _ myopic _ policies that always maximize a measure of the information gain from the next sample .",
    "@xcite and @xcite consider problem settings in which this greedy policy is optimal .",
    "another recent line of work @xcite shows that measures of information gain sometimes satisfy a decreasing returns property known as adaptive sub - modularity , implying the greedy policy is competitive with the optimal policy .",
    "our algorithm also only considers the information gain due to the _ next sample _",
    ", even though the goal is to acquire information over many periods .",
    "our results establish that the manner in which ids encourages information gain leads to an effective algorithm , even for the different objective of maximizing cumulative reward .",
    "we consider a general probabilistic , or bayesian , formulation in which uncertain quantities are modeled as random variables .",
    "the decision  maker sequentially chooses actions @xmath0 from a finite action set @xmath1 and observes the corresponding outcomes @xmath2 .",
    "conditional on the true outcome distribution @xmath3 , the random variables @xmath4 are drawn i.i.d according to @xmath5 for each action @xmath6 . in particular , we assume that for any @xmath7 and @xmath8 , @xmath9 .",
    "the true outcome distribution @xmath3 is itself randomly drawn from the family @xmath10 of distributions .",
    "the agent associates a reward @xmath11 with each outcome @xmath12 , where the reward function @xmath13 is fixed and known .",
    "uncertainty about @xmath3 induces uncertainty about the true optimal action , which we denote by @xmath14 $ ] .",
    "the @xmath15period _ regret _ of the sequence of actions @xmath16 is the random variable , @xmath17,\\ ] ] which measures the cumulative difference between the reward earned by an algorithm that always chooses the optimal action and actual accumulated reward up to time @xmath15 . in this paper",
    "we study expected regret @xmath18 = { \\mathbb{e}}\\left[\\sum_{t=1}^{t }   \\left [ r(y_t(a^ * ) ) - r(y_t(a_t ) ) \\right ] \\right],\\ ] ] where the expectation is taken over the randomness in the actions @xmath19 and the outcomes @xmath20 , and over the prior distribution over @xmath3 .",
    "this measure of performance is commonly called _",
    "bayesian regret _ or _",
    "bayes risk_.    [ [ filtrations - and - randomized - policies . ] ] filtrations and randomized policies .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    actions are chosen based on the history of past observations and possibly some external source of randomness . to represent this external source of randomness more formally ,",
    "we introduce an i.i.d .",
    "sequence of random variables @xmath21 that are jointly independent of the outcomes @xmath22 and @xmath3 .",
    "we define all random variables with respect to a probability space @xmath23 .",
    "fix the filtration @xmath24 where @xmath25 is the sigma  algebra generated by @xmath26 .",
    "the action @xmath19 is measurable with respect to the sigma  algebra generated by @xmath27 .",
    "that is , given the history of past observations , @xmath19 is random only through its dependence on @xmath28 .",
    "the objective is to choose actions in a manner that minimizes expected regret . for this purpose",
    ", it s useful to think of the actions as being chosen by a _ randomized policy _",
    "@xmath29 , which is an @xmath30predictable sequence @xmath31 .",
    "an action is chosen at time @xmath32 by randomizing according to @xmath33 , which specifies a probability distribution over @xmath1 .",
    "we denote the set of probability distributions over @xmath1 by @xmath34 .",
    "we explicitly display the dependence of regret on the policy @xmath29 , letting @xmath35 $ ] denote the expected value given by when the actions @xmath36 are chosen according to @xmath29 .",
    "[ [ further - notation . ] ] further notation .",
    "+ + + + + + + + + + + + + + + + +    we set @xmath37 to be the posterior distribution of @xmath38 . for two probability measures @xmath39 and @xmath40 over a common measurable space , if @xmath39 is absolutely continuous with respect to @xmath40 , the _ kullback - leibler divergence _ between @xmath39 and @xmath40 is @xmath41 where @xmath42 is the radon  nikodym derivative of @xmath39 with respect to @xmath40 . for a probability distribution @xmath39 over a finite set @xmath43 , the _",
    "shannon entropy _ of @xmath39 is defined as @xmath44 .",
    "the _ mutual information _ under the posterior distribution between two random variables @xmath45 , and @xmath46 , denoted by @xmath47 is the kullback - leibler divergence between the joint posterior distribution of @xmath48 and @xmath49 and the product of the marginal distributions .",
    "note that @xmath50 is a random variable because of its dependence on the conditional probability measure @xmath51 .    to reduce notation",
    ", we define the _ information gain _ from an action @xmath52 to be @xmath53 .",
    "as shown for example in lemma 5.5.6 of @xcite , this is equal to the expected reduction in entropy of the posterior distribution of @xmath38 due to observing @xmath54 : @xmath55,\\ ] ] which plays a crucial role in our results .",
    "let @xmath56 $ ] denote the expected instantaneous regret of action @xmath52 at time @xmath32 .",
    "we use overloaded notation for @xmath57 and @xmath58 . for an action sampling distribution @xmath59",
    ", @xmath60 denotes the expected information gain when actions are selected according to @xmath61 , and @xmath62 is defined analogously .",
    "ids explicitly balances between having low expected regret in the current period and acquiring new information about which action is optimal .",
    "it does this by minimizing over all action sampling distributions @xmath59 the ratio between the square of expected regret @xmath63 and information gain @xmath64 about the optimal action @xmath38 .",
    "in particular , the policy @xmath65 is defined by : @xmath66    we call @xmath67 the _ information ratio _ of an action sampling distribution @xmath29 and @xmath68 the _ minimal information ratio_. our analysis will provide a number of bounds on the minimal information ratio , which is key to information - directed sampling s theoretical guarantees .",
    "for example , we show that when rewards are bounded in @xmath69 $ ] , @xmath70 .",
    "bounds of this form show that , in any period , the algorithm s regret can only be large if it s expected to acquire a lot of information about which action is optimal .",
    "equivalently , it shows that the `` cost '' per bit of information acquired can not be too large .",
    "note that the solution to may be a randomized policy , and we will show in section [ sec : randomization ] that this is essential to our results .",
    "however , as we will show in the next subsection , there is always an action sampling distribution minimizing that has support over at most two actions .",
    "we now investigate the structure of the optimization problem and present an efficient algorithm for solving it .",
    "suppose that there are @xmath71 actions , and that the posterior expected regret and information gain are stored in the vectors @xmath72 and @xmath73 .",
    "this subsection treats @xmath74 and @xmath75 as known , but otherwise arbitrary non - negative vectors .",
    "we will discuss methods for computing these quantities in later sections .",
    "assume @xmath76 , as otherwise the optimal action is known with certainty .",
    "we will focus on the following optimization problem : @xmath77 here @xmath78 and the set of feasible solutions is the probability simplex in @xmath79 .",
    "the following result establishes that is a convex optimization problem , and surprisingly , has an optimal solution with at most two non - zero components .",
    "therefore , while ids is a randomized policy , it randomizes over at most two actions .",
    "[ prop : support at most 2 ] the function @xmath80 is convex on @xmath81 .",
    "moreover , there is an optimal solution @xmath82 to with @xmath83 .",
    "algorithm [ alg : chooseaction ] uses proposition [ prop : support at most 2 ] to choose an action in a manner that minimizes . for a problem with @xmath84 actions , the algorithm requires inputs @xmath85 and @xmath86 specifying respectively the expected regret and information gain of each action .",
    "the sampling distribution that minimizes is computed by looping over all pairs of actions @xmath87 and finding the optimal probability of playing @xmath88 instead of @xmath89 . finding this probability is particularly efficient because the objective function is convex .",
    "golden section search , for example , provides a very efficient method for optimizing a convex function over @xmath69 $ ] .",
    "in addition , in this case , any solution to @xmath90 ^ 2 / \\left [ q ' g_i + ( 1-q')g_j \\right ]   = 0 $ ] is given by the solution to a quadratic equation , and therefore can be expressed in closed form .",
    "@xmath91     +   + * select action * : +      here we introduce an approximate form of ids that is suitable for some problems with bandit feedback , satisfies our regret bounds for such problems , and can sometimes facilitate design of more efficient numerical methods .",
    "we will derive this policy by investigating the structure of the mutual information @xmath92 , and considering a modified information measure .",
    "let @xmath93 denote the posterior predictive distribution at an action @xmath52 , and let @xmath94 denote the posterior predictive distribution conditional on the event that @xmath95 is the optimal action .",
    "crucial to our results is the following fact , which is a consequence of standard properties of mutual information is a general random variable can be found in the appendix of @xcite . ] : @xmath96.\\ ] ] that is , the mutual information between @xmath38 and @xmath54 is the expected kullback - leibler divergence between the posterior predictive distribution @xmath97 and the predictive distribution conditioned on the identity of the optimal action @xmath98 .",
    "our analysis provides theoretical guarantees for an algorithm that uses a simpler measure of divergence : the squared divergence `` in mean '' between @xmath99 and @xmath97 , @xmath100 -\\underset{y\\sim p_{t , a}}{{\\mathbb{e } } } \\left [ r(y ) \\right ] \\right)^2\\\\ & = & \\left ( { \\mathbb{e}}\\left[r(y_{t}(a))\\vert { \\mathcal{f}_{t-1 } } , a^*=a^ * \\right ]   - { \\mathbb{e}}\\left[r(y_{t}(a))\\vert { \\mathcal{f}_{t-1}}\\right ] \\right)^2 .\\end{aligned}\\ ] ] define @xmath101,\\ ] ] which replaces the kullback - leibler divergence in with squared divergence in mean .",
    "we introduce the policy @xmath102 where @xmath103",
    "this section establishes regret bounds for information - directed sampling that scale with the entropy of the optimal action distribution .",
    "recall that we have defined the information ratio of an action sampling distribution to be @xmath104 the _ minimal _ information ratio is @xmath105 which is the smallest possible ratio between squared expected regret and expected information gain .",
    "the next proposition shows that bounds on a policy s information ratio imply bounds on expected regret .",
    "[ prop : general regret bound ] fix a deterministic @xmath106 and a policy @xmath107 such that @xmath108 almost surely for each @xmath109 .",
    "then , @xmath110 \\leq \\sqrt{\\lambda h(\\alpha_{1 } ) t } .\\ ] ]",
    "this proposition relies on a worst case bound on the minimal information ratio of the form @xmath111 , and we will provide several bounds of that form later in this section .",
    "the next proposition establishes regret bounds that depend on the average value of @xmath112 instead of a worst case bound .",
    "[ prop : regret bound average information ratio ] for any @xmath113 , @xmath114 \\leq \\sqrt{\\left ( \\frac{1}{t}\\mathbb{e}\\sum_{t=1}^{t } \\psi_{t}^ { * }   \\right ) h(\\alpha_{1 } ) t } .\\ ] ]",
    "we now establish upper bounds on the minimal information ratio @xmath112 in several important settings , which yields explicit regret bounds when combined with proposition [ prop : general regret bound ] .",
    "these bounds show that , in any period , the algorithm s expected regret can only be large if it s expected to acquire a lot of information about which action is optimal . in this sense",
    ", it effectively balances between exploration and exploitation in _ every _ period .",
    "the bounds on the minimal information ratio @xmath112 also help to clarify the role it plays in our results : it roughly captures the extent to which sampling some actions allows the decision maker to make inferences about _ other _ actions . in the worst case ,",
    "the ratio depends on the number of actions , reflecting the fact that actions could provide no information about others . for problems with full information ,",
    "the minimal information ratio is bounded by a numerical constant , reflecting that sampling one action perfectly reveals the rewards that would have been earned by selecting any other action .",
    "the problems of online linear optimization under `` bandit feedback '' and under `` semi  bandit feedback '' lie between these two extremes , and the ratio provides a natural measure of each problem s information structure . in each case , our bounds reflect that ids is able to automatically exploit this structure .",
    "the proofs of these worst ",
    "case bounds follow from our recent analysis of thompson sampling , and the implied regret bounds are the same as those established for thompson sampling . in particular , since @xmath115 where @xmath116 is the thompson sampling policy , it is enough to bound @xmath117 .",
    "several such bounds were provided in @xcite .",
    "is exactly equal to the term @xmath118 that is bounded in @xcite . ]",
    "while the analysis is similar in the cases considered here , ids outperforms thompson sampling in simulation , and , as we will highlight in the next section , is sometimes provably much more informationally efficient .    for each problem setting",
    ", we will compare our upper bounds on expected regret with known lower bounds .",
    "some of these lower bounds were developed and stated in an adversarial framework , but were proved using the _",
    "probabilistic method _ ; authors fixed a family of distributions @xmath10 and an initial distribution over @xmath3 and lower bounded the expected regret under this environment of any algorithm .",
    "this provides lower bounds on @xmath119 $ ] in our framework .    to simplify the exposition ,",
    "our results are stated under the assumption that rewards are uniformly bounded .",
    "this effectively controls the worst - case variance of the reward distribution , and as shown in the appendix of @xcite , our results can be extended to the case where reward distributions are sub - gaussian .",
    "[ assum : bounded rewards ] @xmath120      the next proposition shows that @xmath112 is never larger than @xmath121 .",
    "that is , there is always an action sampling distribution @xmath59 such that @xmath122 . in the next section",
    ", we will will show that under different information structures the ratio between regret and information gain can be much smaller , which leads to stronger theoretical guarantees .",
    "[ prop : worst case bound ] for any @xmath123 , @xmath124 almost surely .",
    "combining proposition [ prop : worst case bound ] with proposition [ prop : general regret bound ] shows that @xmath125   \\leq \\sqrt{\\frac{1}{2 } |{\\mathcal{a}}| h(\\alpha_{1})t}$ ] and @xmath126    \\leq \\sqrt{\\frac{1}{2 } |{\\mathcal{a}}| h(\\alpha_{1})t}$ ] .",
    "our focus in this paper is on problems with _",
    "partial feedback_. for such problems , what the decision maker observes depends on the actions selected , which leads to a tension between exploration and exploitation .",
    "problems with full information arise as an extreme point of our formulation where the outcome @xmath54 is perfectly revealed by observing @xmath127 for some @xmath128 ; what is learned does not depend on the selected action .",
    "the next proposition shows that under full information , the minimal information ratio is bounded by @xmath129 .",
    "[ prop : full information ] suppose for each @xmath130 there is a random variable @xmath131 such that for each @xmath132 , @xmath133 .",
    "then for all @xmath130 , @xmath134 almost surely .    combining this result with proposition [ prop : general regret bound ] shows @xmath135 \\leq \\sqrt{\\frac{1}{2 } h(\\alpha_{1 } ) t}$ ] .",
    "further , a worst  case bound on the entropy of @xmath136 shows that @xmath135 \\leq \\sqrt{\\frac{1}{2 } \\log ( |{\\mathcal{a}}| ) t}$ ] .",
    "@xcite show this bound is order optimal , in the sense that for any time horizon @xmath15 and number of actions @xmath137 there exists a prior distribution over @xmath3 under which @xmath138 \\geq c_{0 } \\sqrt{\\log ( |{\\mathcal{a}}| ) t}$ ] where @xmath139 is a numerical constant that does not depend on @xmath140 or @xmath15 .",
    "the bound here improves upon this worst case bound since @xmath141 can be much smaller than @xmath142 when the prior distribution is informative .",
    "the stochastic linear bandit problem has been widely studied ( e.g. @xcite ) and is one of the most important examples of a multi - armed bandit problem with `` correlated arms . '' in this setting , each action is associated with a finite dimensional feature vector , and the mean reward generated by an action is the inner product between its known feature vector and some unknown parameter vector . because of this structure , observations from taking one action allow the decision  maker to make inferences about other actions .",
    "the next proposition bounds the minimal information ratio for such problems .",
    "[ prop : linear ] if @xmath143 and for each @xmath144 there exists @xmath145 such that for all @xmath6 @xmath146 = a^t \\theta_p,\\ ] ] then for all @xmath130 , @xmath147 almost surely .",
    "this result shows that @xmath148 \\leq \\sqrt{\\frac{1}{2 } h(\\alpha_{1})d t } \\leq \\sqrt{\\frac{1}{2 } \\log(|{\\mathcal{a}}|)d t}$ ] for linear bandit problems .",
    "again , @xcite show this bound is order optimal , in the sense that for any time horizon @xmath15 and dimension @xmath149 if the actions set is @xmath150 , there exists a prior distribution over @xmath3 such that @xmath138 \\geq c_{0 } \\sqrt{\\log ( |{\\mathcal{a}}| ) d t}$ ] where @xmath139 is a constant the is independent of @xmath149 and @xmath15 .",
    "the bound here improves upon this worst case bound since @xmath141 can be much smaller than @xmath142 when the prior distribution in informative .      to motivate the information structure studied here , consider a simple resource allocation problem",
    "there are @xmath149 possible projects , but the decision  maker can allocate resources to at most @xmath151 of them at a time . at time @xmath32 , project @xmath152 yields a random reward @xmath153 , and the reward from selecting a subset of projects @xmath154 is @xmath155 . in the linear bandit formulation of this problem , upon choosing a subset of projects @xmath52 the agent would only observe the overall reward @xmath156 .",
    "it may be natural instead to assume that the outcome of each selected project @xmath157 is observed .",
    "this type of observation structure is sometimes called `` semi ",
    "bandit '' feedback @xcite",
    ".    a naive application of proposition [ prop : linear ] to address this problem would show @xmath158 .",
    "the next proposition shows that since the entire parameter vector @xmath157 is observed upon selecting action @xmath52 , we can provide an improved bound on the information ratio .",
    "[ prop : semi bandit ] suppose @xmath159 , and that there are random variables @xmath160 such that @xmath161 assume that the random variables @xmath162 are independent conditioned on @xmath163 and @xmath164 $ ] almost surely for each @xmath165 .",
    "then for all @xmath130 , @xmath166 almost surely .",
    "in this problem , there are as many as @xmath167 actions , but because ids exploits the structure relating actions to one another , its regret is only polynomial in @xmath168 and @xmath149 . in particular",
    ", combining proposition [ prop : semi bandit ] with proposition [ prop : general regret bound ] shows @xmath135 \\leq \\frac{1}{m}\\sqrt{\\frac{d}{2 } h(\\alpha_{1})t } $ ] . since @xmath169 this also yields a bound of order @xmath170 . as shown by @xcite ,",
    "the lower bound is @xmath171 which is @xmath168 times larger than in our formulation .",
    "the lower bound stated in their paper is therefore of order @xmath172 .",
    "they do nt provide a complete proof of their result , but note that it follows from standard lower bounds in the bandit literature . in the proof of theorem 5 in that paper",
    ", they construct an example in which the decision maker plays @xmath168 bandit games in parallel , each with @xmath173 actions . using that example , and the standard bandit lower bound ( see theorem 3.5 of @xcite ) , the agent s regret from each component",
    "must be at least @xmath174 , and hence her overall expected regret is lower bounded by a term of order @xmath175 . ] for this problem is of order @xmath176 , so our bound is order optimal up to a @xmath177 factor .",
    "ids is a _ randomized policy _ , and it s natural to wonder whether this randomization plays a fundamental role . here , we will show through a simple example that no non - randomized policy can have a uniformly bounded information ratio .",
    "for this reason , randomization is essential for our theoretical results .",
    "ids is also a _ stationary policy _ , because action probabilities are selected based only on the current posterior distribution and not the current time period .",
    "more broadly , randomization seems to be crucial to the algorithm s convergence , as most stationary deterministic policies would eventually settle on playing only a single action .",
    "consider a problem with two actions @xmath178 .",
    "rewards from @xmath179 follow a bernoulli distribution with mean 1/2 and this is known apriori .",
    "the distribution of rewards from @xmath180 is bernoulli(3/4 ) with probability @xmath181 and is bernoulli(1/4 ) with probability @xmath182 . consider the limit as @xmath183 .",
    "then , it is almost certain that action 1 is optimal .",
    "the expected regret from choosing action 1 is therefore almost zero , but the information gain from choosing action 1 is exactly zero .",
    "meanwhile , the regret from choosing action 2 is almost 1/4 , but the information gain is very small ( certainly no larger than @xmath141 ) .",
    "hence , for neither choice of action allows for a bounded information ratio @xmath184 . nevertheless , by mixing between the two actions ids is able to attain low expected regret , positive expected information gain , and a bounded ratio between the two .",
    "upper confidence bound algorithms ( ucb ) and thompson sampling are two of the most popular approaches to balancing between exploration and exploitation . in some cases , ucb algorithms and thompson sampling are empirically effective , and have strong theoretical guarantees .",
    "specific ucb algorithms and thompson sampling are known to be asymptotically efficient for multi  armed bandit problems with independent arms @xcite and satisfy strong regret bounds for some problems with dependent arms @xcite .",
    "both classes of algorithms experiment with all actions that could still _ plausibly _ be optimal given the observed data .",
    "this guarantees actions are not prematurely discarded , but also that samples are not `` wasted '' on clearly suboptimal actions . while this is enough to guarantee strong performance",
    "is some settings , we will show that these algorithms can perform very poorly when faced with more complex information structures .",
    "we demonstrate this through several examples - each of which is designed to be simple and transparent . to set the stage for our discussion",
    ", we now introduce ucb algorithms and thompson sampling .",
    "[ [ thompson - sampling . ] ] thompson sampling .",
    "+ + + + + + + + + + + + + + + + + +    the thompson sampling algorithm simply samples actions according to the posterior probability they are optimal .",
    "in particular , actions are chosen randomly at time @xmath32 according to the sampling distribution @xmath185 . by definition",
    ", this means that for each @xmath6 , @xmath186 .",
    "this algorithm is sometimes called _ probability matching _ because the action selection distribution is _ matched _ to the posterior distribution of the optimal action .",
    "note that thompson sampling draws actions only from the support of the posterior distribution of @xmath38 .",
    "that is , it never selects an action @xmath52 if @xmath187 .",
    "put differently , this implies that it only selects actions that are optimal under some @xmath188 .",
    "[ [ ucb - algorithms . ] ] ucb algorithms .",
    "+ + + + + + + + + + + + + + +    upper confidence bound algorithms provide a simple method for balancing between exploration and exploitation .",
    "actions are selected through two steps .",
    "first , for each action @xmath6 an upper confidence bound @xmath189 is constructed .",
    "then , the algorithm selects an action @xmath190 with maximal upper confidence bound . the upper confidence bound @xmath189 represents the greatest mean reward value that is statistically plausible . in particular",
    ", @xmath189 is typically constructed so that @xmath191\\leq b_{t}(a)\\ ] ] with high probability , but that @xmath192 $ ] as data about action @xmath52 accumulates .    like thompson sampling , many ucb algorithms only select actions that are optimal under some @xmath188 .",
    "for example , consider a ucb algorithm that constructs at each time @xmath32 a confidence set @xmath193 containing the set of distributions that are statistically plausible given observed data .",
    "upper confidence bounds are defined to be @xmath194,\\ ] ] which is the highest expected reward attainable under one of the plausible distributions .",
    "many optimistic algorithms take this form ( see for example @xcite ) . any action @xmath195 must be optimal under one of the outcome distributions @xmath196 .",
    "an alternative method involves choosing @xmath189 to be a particular quantile of the posterior distribution of the action s mean reward under @xmath3 @xcite . in each of the examples we construct",
    ", such an algorithm always chooses actions from the support of @xmath38 unless the quantiles are so low that @xmath197 $ ] .",
    "let @xmath198 and suppose that @xmath3 is drawn uniformly at random from a finite set @xmath199 of alternatives .",
    "consider a problem with bandit - feedback @xmath200 . under @xmath201 , the reward of action",
    "@xmath202 is @xmath203 action @xmath204 is known to never yield the maximal reward , and is therefore never selected by thompson sampling or any sensible ucb algorithm . instead",
    ", these algorithms will select among @xmath205 , ruling out only a single action at a time until a reward 1 is earned and the optimal action is identified .",
    "their expected regret therefore grows linearly in @xmath206 .",
    "information - directed sampling is able to recognize that much more is learned by drawing action @xmath204 than by selecting one of the other arms .",
    "in fact , selecting action @xmath207 immediately identifies the optimal action .",
    "ids will select this action , learn which action is optimal , and select that action in all future periods .",
    "its regret is independent of @xmath206 .",
    "consider a linear bandit problem where @xmath143 and the reward from an action @xmath6 is @xmath208 .",
    "the true parameter @xmath209 is known to be drawn uniformly at random from the set of 1sparse vectors @xmath210 . for simplicity ,",
    "assume @xmath211 for some @xmath212 .",
    "the action set is taken to be the set of vectors in @xmath213 normalized to be a unit vector in the @xmath214 norm : @xmath215 .",
    "we will show that the expected number of time steps for thompson sampling ( or a ucb algorithm ) to identify the optimal action grows linearly with @xmath149 , whereas ids requires only @xmath216 time steps .    when an action @xmath52 is selected and @xmath217 is observed , each @xmath218 with @xmath219 is ruled out .",
    "let @xmath220 denote the set of all parameters in @xmath221 that are consistent with the rewards observed up to time @xmath32 and let @xmath222 denote the corresponding set of possible positive components .    for this problem , @xmath223 .",
    "that is , if @xmath209 were known , choosing the action @xmath209 would yield the highest possible reward .",
    "thompson sampling and ucb algorithms only choose actions from the support of @xmath38 and therefore will only sample actions @xmath6 that , like @xmath38 , have only a single positive component . unless that is also the positive component of @xmath209 ,",
    "the algorithm will observe a reward of zero and rule out only one possible value for @xmath209 . in the worst case",
    ", the algorithm requires @xmath149 samples to identify the optimal action .",
    "now , consider an application of information - directed sampling to this problem .",
    "the algorithm essentially performs binary search : it selects @xmath132 with @xmath224 for half of the components @xmath225 and @xmath226 for the other half as well as for any @xmath227 .",
    "after just @xmath216 time steps the true support of the parameter vector @xmath209 is identified .    to see why this is the case , first note that all parameters in @xmath220 are equally likely and hence the expected reward of an action @xmath52 is @xmath228 .",
    "since @xmath229 and @xmath230 for each @xmath132 , every action whose positive components are in @xmath231 yields the highest possible expected reward of @xmath232 .",
    "therefore , binary search minimizes expected regret in period @xmath32 for this problem . at the same time",
    ", binary search is assured to rule out half of the parameter vectors in @xmath220 at each time @xmath32 .",
    "this is the largest possible expected reduction , and also leads to the largest possible information gain about @xmath233 . since binary search both minimizes expected regret in period @xmath32 and uniquely maximizes expected information gain in period @xmath32 , it is the sampling strategy followed by ids .    in this setting",
    "we can explicitly calculate the information ratio of each policy , and the difference between them highlights the advantages of information - directed sampling .",
    "we have @xmath234 where @xmath235 if @xmath236 as @xmath237 .",
    "when the dimension @xmath149 is large , @xmath238 is much smaller .",
    "consider the problem of repeatedly recommending an assortment of products to a customer .",
    "the customer has unknown type @xmath239 where @xmath240 .",
    "each product is geared toward customers of a particular type , and the assortment @xmath241 of @xmath168 products offered is characterized by the vector of product types @xmath242 .",
    "we model customer responses through a random utility model in which customers are apriori more likely to derive high value from a product geared toward their type . when offered an assortment of products @xmath52 , the customer associates with the @xmath88th product utility @xmath243 , where @xmath244 follows an extreme  value distribution and @xmath245 is a known constant .",
    "this is a standard multinomial logit discrete choice model .",
    "the probability a customer of type @xmath246 chooses product @xmath88 is given by @xmath247 when an assortment @xmath52 is offered at time @xmath32 , the customer makes a choice @xmath248 and leaves a review @xmath249 indicating the utility derived from the product , both of which are observed by the recommendation system .",
    "the reward to the recommendation system is the normalized utility of the customer @xmath250 .",
    "if the type @xmath251 of the customer were known , then the optimal recommendation would be @xmath252 , which consists only of products targeted at the customer s type .",
    "therefore , both thompson sampling and ucb algorithms would only offer assortments consisting of a single type of product .",
    "because of this , each type of algorithm requires order @xmath253 samples to learn the customer s true type .",
    "ids will instead offer a _",
    "diverse _ assortment of products to the customer , allowing it to learn much more quickly .    to make the presentation more transparent ,",
    "suppose that @xmath251 is drawn uniformly at random from @xmath254 and consider the behavior of each type of algorithm in the limiting case where @xmath255 . in this regime ,",
    "the probability a customer chooses a product of type @xmath251 if it available tends to 1 , and the review @xmath249 tends to @xmath256 , an indicator for whether the chosen product had type @xmath251 .",
    "the initial assortment offered by ids will consist of @xmath168 different and previously untested product types .",
    "such an assortment maximizes both the algorithm s expected reward in the next period and the algorithm s information gain , since it has the highest probability of containing a product of type @xmath251 .",
    "the customer s response almost perfectly indicates whether one of those items was of type @xmath251 .",
    "the algorithm continues offering assortments containing @xmath168 unique , untested , product types until a review near @xmath257 is received . with extremely high probability , this takes at most @xmath258 time periods . by diversifying the @xmath168 products in the assortment",
    ", the algorithm learns a factor of @xmath168 times faster .    as in the previous example",
    ", we can explicitly calculate the information ratio of each policy , and the difference between them highlights the advantages of ids .",
    "the information ratio of ids is more that @xmath168 times smaller : @xmath259",
    "this section presents computational experiments evaluating the effectiveness of information - directed sampling , and comparing its performance to other algorithms . in section [ sec : beyond ucb ] , we showed that popular approaches like ucb algorithms and thompson sampling can perform very poorly when faced with complicated information structures , and for this reason are sometimes dramatically outperformed by ids . in this section ,",
    "we focus instead on simpler settings where current approaches are extremely effective .",
    "we find that even for these simple and widely studied settings , information - directed sampling displays performance exceeding state of the art .",
    "for each experiment , the algorithm used to implement ids is presented in appendix [ sec : computation 2 ] .",
    "our first experiment involves a multi - armed bandit problem with independent arms and binary rewards .",
    "the mean reward of each arm is drawn from @xmath260 , which is the uniform distribution , and the means of separate arms are independent .",
    "figure [ fig : bernoulli_regret_plot ] and table [ table : bernoulli ] present the results of 1000 independent trials of an experiment with 10 arms and a time horizon of 1000 .",
    "we compared the performance of ids to that of six other algorithms , and found that it had the lowest average regret of 18.16 .",
    "the famous ucb1 algorithm of @xcite selects the action @xmath52 which maximizes the upper confidence bound @xmath261 where @xmath262 is the empirical average reward from samples of action @xmath52 and @xmath263 is the number of samples of action @xmath52 up to time @xmath32 .",
    "the average regret of this algorithm is 131.3 , which is dramatically larger than that of ids .",
    "for this reason ucb1 is omitted from figure [ fig : bernoulli_regret_plot ] .",
    "the confidence bounds of ucb1 are constructed to facilitate theoretical analysis . for practical performance @xcite",
    "proposed using an algorithm called ucb - tuned .",
    "this algorithm selects the action @xmath52 which maximizes the upper confidence bound @xmath264 , where @xmath265 is an upper bound on the variance of the reward distribution at action @xmath52 .",
    "while this method dramatically outperforms ucb1 , it is still outperformed by ids .",
    "the moss algorithm of @xcite is similar to ucb1 and ucb  tuned , but uses slightly different confidence bounds .",
    "it is known to satisfy regret bounds for this problem that are minimax optimal up to a numerical constant factor .    in previous numerical experiments @xcite , thompson sampling and bayes ucb exhibited state - of - the - art performance for this problem .",
    "each also satisfies strong theoretical guarantees , and is known to be asymptotically optimal in the sense defined by @xcite .",
    "unsurprisingly , they are the closest competitors to ids .",
    "the bayes ucb algorithm , studied in @xcite , constructs upper confidence bounds based on the quantiles of the posterior distribution : at time step @xmath32 the upper confidence bound at an action is the @xmath266 quantile of the posterior distribution of that action .    a somewhat different approach is the knowledge gradient ( kg ) policy of @xcite , which uses a one - step lookahead approximation to the value of information to guide experimentation . for reasons described in appendix [ sec : value of info ] , kg does not explore sufficiently to identify the optimal arm in this problem , and therefore its regret grows linearly with time . because kg explores very little , its realized regret is highly variable , as depicted in table [ table : bernoulli ] . in 100 out of the 1000 trials ,",
    "the regret of kg was lower than .62 , reflecting that the best arm was almost always chosen . in the worst 100 out of the 1000 trials ,",
    "the regret of kg was larger than 161.971 . it should be noted that kg is particularly poorly suited to problems with discrete observations and long time horizons .",
    "it can perform very well in other types of experiments .",
    "finally , as displayed in figure [ fig : bernoulli_regret_plot ] , our results indicate that the the variation of ids @xmath267 presented in subsection [ subsec : ids in mean ] has extremely similar performance to standard ids for this problem .",
    "it s worth pointing out that , although gittins indices characterize the bayes optimal policy for infinite horizon discounted problems , the finite horizon formulation considered here is computationally intractable @xcite . a similar index policy @xcite designed for finite horizon problems could be applied as a heuristic in this setting . however , a specialized algorithm is required to compute these indices , and this procedure is extremely computationally intensive in our setting due to the long time horizon . since the goal of this paper is not to develop the best possible policy for bandit problems with independent arms , we have opted to only compare ids against a variety of simple and widely studied benchmarks .",
    "& & & & & & & & & & & mean regret & 51.595 & 18.16 & 28.504 & 23.086 & 131.3 & 36.081 & 46.592 standard error & 2.1782 & 0.5523 & 0.4647 & 0.4305 & 0.6124 & 0.3297 & 0.3231 quantile .1 & 0.62316 & 4.4098 & 13.584 & 8.8376 & 104.76 & 24.08 & 36.377 quantile .25 & 2.7314 & 8.0054 & 18.243 & 12.825 & 118.66 & 29.184 & 39.968 quantile .5 & 13.593 & 13.537 & 25.537 & 20.548 & 132.62 & 35.119 & 45.111 quantile .75 & 83.932 & 21.673 & 35.222 & 30.737 & 145.22 & 41.629 & 50.752 quantile .9 & 161.97 & 36.728 & 46.969 & 40.909 & 154.91 & 48.424 & 57.373    .49     .49       our second experiment treats a different multi - armed bandit problem with independent arms .",
    "the reward value at each action @xmath52 follows a gaussian distribution @xmath268 .",
    "the mean @xmath269 is drawn from a gaussian prior , and the means of different reward distributions are drawn independently .",
    "we ran 1000 simulation trials of a problem with 10 arms .",
    "the results are displayed in figure [ fig : gaussian_regret_plot ] and table [ table : gaussian ] .",
    "for this problem , we again compare thompson sampling , bayes ucb , kg , and ids .",
    "we simulate the mean - based variant information - directed sampling @xmath267 presented in subsection [ subsec : ids in mean ] , as it offers some computational advantages .",
    "& & & & & & mean regret & 57.53 & 62.271 & 66.836 & 53.66 & 155.95 & 52.711 standard error & 3.4395 & 0.96969 & 0.96614 & 1.6605 & 1.2142 & 1.5906 quantile .1 & 16.199 & 34.282 & 37.005 & 23.356 & 106.47 & 23.387 quantile .25 & 20.431 & 42.589 & 47.483 & 28.471 & 130.28 & 29.529 quantile .5 & 25.474 & 56.828 & 61.082 & 38.051 & 155.53 & 40.922 quantile .75 & 35.465 & 73.431 & 79.867 & 55.198 & 180.8 & 59.563 quantile .9 & 120.53 & 98.383 & 99.93 & 96.904 & 205.49 & 85.585 quantile .95 & 279.03 & 111.34 & 120.66 & 148.30 & 220.48 & 124.99     in|>p.3 in| & * 10 * & * 25 * & * 50 * & * 75 * & * 100 * & * 250 * & * 500 * & * 750 * & * 1000 * * regret of ids * & 9.79 & 15.71 & 21.10 & 24.56 & 27.13 & 36.52 & 44.65 & 49.85 & 53.66 * regret of kg * & 9.08 & 15.26 & 19.94 & 23.79 & 25.52 & 34.62 & 44.32 & 50.63 & 62.87    r0.5        we also simulated the gpucb of @xcite .",
    "this algorithm maximizes the upper confidence bound @xmath270 where @xmath271 and @xmath272 are the posterior mean and standard deviation of @xmath273 .",
    "they provide regret bounds that hold with probability at least @xmath274 when @xmath275 this value of @xmath276 is far too large for practical performance , at least in this problem setting .",
    "the average regret of gpucb in the definition of @xmath276 , as this choice leads to a lower value of @xmath276 and stronger performance .",
    "] is 156.6 , which is roughly three times that of information - directed sampling .",
    "for this reason , we considered a tuned version of gpucb that sets @xmath277 .",
    "we ran 1000 trials of many different values of @xmath246 to find the value @xmath278 with the lowest average regret for this problem . this tuned version of gpucb had average regret of 52.7 , which is roughly equivalent to ids .",
    "the work on knowledge gradient ( kg ) focuses almost entirely on problems with gaussian reward distributions and gaussian priors .",
    "we find kg performs better in this experiment than it did in the bernoulli setting , and its average regret is competitive with that of ids .    as in the bernoulli setting , kg s realized regret is highly variable .",
    "the median regret of kg is the lowest of any algorithm , but in 50 of the 1000 trials its regret exceeded 279  seemingly reflecting that the algorithm did not explore enough to identify the best action .",
    "kg may is particularly effective over short time spans .",
    "unlike information - directed sampling , kg takes the time horizon @xmath15 as an input , and explores less aggressively when there are fewer time periods remaining . table [ table : varying time horizon ]",
    "compares the regret of kg and ids over different time horizons . even though ids does not know the time horizon , it is competitive with kg even over short horizons .",
    "we are hopeful that ids could be modified to exploit fixed and known time horizons even more effectively .      the previous subsections present numerical examples in which information - directed sampling outperforms bayes ucb and thompson sampling for some problems with independent arms .",
    "this is surprising since each of these algorithms is known , in a sense we will soon formalize , to be asymptotically optimal for these problems .",
    "this section presents simulation results over a much longer time horizon that suggest ids scales in the same asymptotically optimal way .",
    "we consider again a problem with binary rewards and independent actions .",
    "the action @xmath279 yields in each time period a reward that is 1 with probability @xmath280 and 0 otherwise .",
    "the seminal work of @xcite provides the following asymptotic lower bound on regret of any policy @xmath29 :    @xmath281}{\\log t } \\geq \\frac{\\sum_{a\\neq a^*}(\\theta_{a^*}-\\theta_{a } ) } { \\sum_{a\\neq a^ * } { d_{\\rm kl}}(\\theta_{a^ * } \\ , || \\ , \\theta_{a } ) } : = c(\\theta)\\ ] ]    r0.5        note that we have conditioned on the parameter vector @xmath282 , indicating that this is a frequentist lower bound . nevertheless ,",
    "when applied with an independent uniform prior over @xmath282 , both bayes ucb and thompson sampling are known to attain this lower bound @xcite .",
    "our next numerical experiment fixes a problem with three actions and with @xmath283 we compare algorithms over a 10,000 time periods . due to the expense of running this experiment , we were only able to execute 200 independent trials .",
    "each algorithm uses a uniform prior over @xmath282 .",
    "our results , along with the asymptotic lower bound of @xmath284 , are presented in figure [ fig : lai robbins plot ] .",
    "our final numerical experiment treats a linear bandit problem .",
    "each action @xmath285 is defined by a 5 dimensional feature vector .",
    "the reward of action @xmath52 at time @xmath32 is @xmath286 where @xmath287 is drawn from a multivariate gaussian prior distribution , and @xmath288 is independent gaussian noise . in each period ,",
    "only the reward of the selected action is observed . in our experiment ,",
    "the action set @xmath1 contains 30 actions , each with features drawn uniformly at random from @xmath289 $ ] .",
    "the results displayed in figure [ fig : linear ] are averaged over 1000 independent trials .",
    "we compare the regret of six algorithms .",
    "three of these - gp - ucb , thompson sampling , and ids - satisfy strong regret bounds for this problem .",
    "both gp - ucb and thompson sampling are significantly outperformed by ids .",
    "we also include bayes ucb @xcite and a version of gp - ucb that was tuned , as in subsection [ subsec : independent normal experiment ] , to minimize its average regret .",
    "each of these displays performance that is competitive with that of ids .",
    "these algorithms are heuristics , in the sense that the way their confidence bounds are constructed differ significantly from those of linear ucb algorithms that are known to satisfy theoretical guarantees .",
    "as discussed in subsection [ subsec : independent normal experiment ] , unlike ids , kg takes the time horizon @xmath15 as an input , and explores less aggressively when there are fewer time periods remaining .",
    "table [ table : linear kg varying time horizon ] compares ids to kg over several different time horzions .",
    "even though ids does not exploit knowledge of the time horizon , it is competitve with kg over short time horizons .     *",
    "25 * & * 50 * & * 75 * & * 100 * & * 250 * * regret of ids * & 5.81 & 10.10 & 14.75 & 18.07 & 20.80 & 31.63 * regret of kg * & 5.72 & 9.99 & 14.33 & 18.47 & 22.09 & 37.37",
    "this paper has proposed information - directed sampling  a new algorithm for online optimization problems in which a decision maker must learn from partial feedback .",
    "we establish a general regret bound for the algorithm , and specialize this bound to several widely studied problem classes .",
    "we show that it sometimes greatly outperforms other popular approaches , which do nt carefully measure the information provided by sampling actions . finally ,",
    "for some simple and widely studied classes of multi - armed bandit problems we demonstrate simulation performance surpassing popular approaches .",
    "many important open questions remain , however .",
    "ids solves a single - period optimization problem as a proxy to an intractable multi - period problem .",
    "solution of this single - period problem can itself be computationally demanding , especially in cases where the number of actions is enormous or mutual information is difficult to evaluate .",
    "an important direction for future research concerns the development of computationally elegant procedures to implement ids in important cases .",
    "even when the algorithm can not be directly implemented , however , one may hope to develop simple algorithms that capture its main benefits .",
    "proposition [ prop : general regret bound ] shows that any algorithm with small information ratio satisfies strong regret bounds .",
    "thompson sampling is a very tractable algorithm that , we conjecture , sometimes has nearly minimal information ratio .",
    "perhaps simple schemes with small information ratio could be developed for other important problem classes , like the sparse linear bandit problem .",
    "in addition to computational considerations , a number of statistical questions remain open .",
    "one question raised is whether ids attains the lower bound of @xcite for some bandit problems with independent arms . beyond the empirical evidence presented in subsection [ subsec : asymptotic ] ,",
    "there are some theoretical reasons to conjecture this is true .",
    "next , a more precise understanding of problem s _ information complexity _ remains an important open question for the field .",
    "our regret bound depends on the problem s information complexity through a term we call the information ratio , but it s unclear if or when this is the right measure .",
    "finally , it may be possible to derive lower bounds using the same information theoretic style of argument used in the derivation of our upper bounds .",
    "this section presents a number of ways in which the results and ideas discussed throughout this paper can be extended .",
    "we will consider the use of algorithms like information - directed sampling for pure  exploration problems , a form of information - directed sampling that aims to acquire information about @xmath3 instead of @xmath38 , and a version of information directed - sampling that uses a tuning parameter to control how aggressively the algorithm explores . in each case , new theoretical guarantees can be easily established by leveraging our analysis of information - directed sampling .",
    "consider the problem of adaptively gathering observations @xmath290 so as to minimize the expected loss of the best decision at time @xmath15 , @xmath291.\\ ] ] recall that we have defined @xmath292 $ ] to be the expected regret of action @xmath52 at time @xmath32 .",
    "this is a `` pure exploration problem , '' in the sense that one is interested only in the terminal regret and not in the algorithm s cumulative regret .",
    "however , the next proposition shows that bounds on the algorithm s cumulative expected regret imply bounds on @xmath293 $ ] .",
    "[ prop : terminal regret from cumulative regret ] if actions are selected according to a policy @xmath29 , then @xmath294   \\leq \\frac{{\\mathbb{e}}\\left [ { \\rm regret}\\left ( t , \\pi \\right )   \\right]}{t}.\\ ] ]    by the tower property of conditional expectation , @xmath295 = \\delta_{t}(a)$ ] .",
    "therefore , jensen s inequality shows @xmath296 \\leq \\min_{a\\in { \\mathcal{a } } } \\delta_{t}(a ) \\leq \\delta_{t}(\\pi_t ) $ ] .",
    "taking expectations and iterating this relation shows that @xmath297 \\leq { \\mathbb{e}}\\left [ \\min_{a \\in { \\mathcal{a } } } \\delta_{t}(a ) \\right ] \\leq   { \\mathbb{e}}\\left [ \\delta_{t}(\\pi_{t } ) \\right ] \\hspace{10pt } \\forall t \\in \\{1, ... ,t\\}.\\ ] ] the result follows by summing both sides of over @xmath298 and dividing each by @xmath15 .",
    "information - directed sampling is designed to have low cumulative regret , and therefore balances between acquiring information and taking actions with low expected regret . for pure exploration problems ,",
    "it s natural instead to consider an algorithm that always acquires as much information about @xmath38 as possible .",
    "the next proposition provides a theoretical guarantee for an algorithm of this form .",
    "the proof of this result combines our analysis of information - directed sampling with the analysis used to prove proposition [ prop : terminal regret from cumulative regret ] , and is provided in appendix [ sec : proof of extensions ] .",
    "[ prop : pure exploration ids ] if actions are selected so that @xmath299 and @xmath300 almost surely for each @xmath301 , then @xmath294 \\leq \\sqrt{\\frac{\\lambda h(\\alpha_{1 } ) } { t}}.\\ ] ]      information - directed sampling optimizes a single - period objective that balances earning high immediate reward and acquiring information .",
    "information is quantified using the mutual information between the true optimal action @xmath38 and the algorithm s next observation @xmath54 . in this subsection",
    ", we will consider an algorithm that instead quantifies the amount learned through selecting an action @xmath52 using the mutual information @xmath302 between the algorithm s next observation and the true outcome distribution @xmath3 .",
    "such an algorithm could actively seek information that is irrelevant to future decisions .",
    "however , in some cases , such an algorithm can be computationally simple while offering statistically efficiency .",
    "we introduce a modified form of the information ratio @xmath303 which replaces the expected information gain about @xmath38 , @xmath304 , with the expected information gain about @xmath3 .    for any action sampling distribution @xmath305 , @xmath306 furthermore , if @xmath10 is finite , and there is some @xmath106 and policy @xmath307 satisfying @xmath308 almost surely , then @xmath309 \\leq \\sqrt { \\lambda h(p^ * )   t}.\\ ] ]    equation relies on the inequality @xmath310 , which itself follows from the data processing inequality of mutual information because @xmath38 is a function of @xmath3 .",
    "the proof of the second part of the proposition is almost identical to the proof of proposition [ prop : general regret bound ] , and is omitted .",
    "we have provided several bounds on the information ratio of @xmath311 of the form @xmath312 . by this proposition ,",
    "such bounds imply that if @xmath313 satisfies @xmath314 then , @xmath315 , and the regret bound applies .      in this section , we present an alternative form of information - directed sampling that depends on a tuning parameter @xmath106 .",
    "as @xmath316 varies , the algorithm strikes a different balanace between exploration and exploration .",
    "the following proposition provides regret bounds for this algorithm provided @xmath316 is sufficiently large .",
    "fix any @xmath106 such that @xmath300 almost surely for each @xmath317 . if @xmath318 is defined so that @xmath319 then @xmath320 \\leq \\sqrt { \\lambda h(\\alpha ) t}.\\ ] ]    we have that @xmath321 where ( a ) follows since @xmath322 is feasible for the optimization problem , and ( b ) follows since @xmath323 since @xmath324 , it must be the case that @xmath325 .",
    "the result then follows by applying proposition [ prop : general regret bound ] .",
    "here we will present an implementation of information - directed sampling for the setting described in subsection [ subsec : bernoulli experiment ] . consider a multi - armed bandit problem with binary rewards and @xmath206 actions denoted by @xmath326 .",
    "the mean reward @xmath327 of each arm @xmath328 is drawn from a beta prior distribution , and the means of separate arms are drawn independently .    because the beta distribution is a conjugate prior for the bernoulli distribution ,",
    "the posterior distribution of each @xmath327 is a beta distribution .",
    "the parameters @xmath329 of this distribution can be updated easily .",
    "let @xmath330 and @xmath331 denote respectively the pdf and cdf of the posterior distribution of @xmath327 .",
    "the posterior probability that @xmath332 can be written as @xmath333 \\overline{f}(x)dx\\ ] ] where @xmath334 .",
    "algorithm [ alg : bernoulliids ] uses this expression to compute the posterior probability @xmath335 that an action @xmath336 is optimal .",
    "to compute the information gain @xmath337 of action @xmath89 , we use equation .",
    "let @xmath338 $ ] denote the expected value of @xmath339 given that action @xmath88 is optimal .",
    "step 18 computes the information gain @xmath337 of action @xmath202 as the expected kullback leibler divergence between a bernoulli distribution with mean @xmath340 and the posterior distribution at action @xmath89 , which is bernoulli with parameter @xmath341 .    finally , the algorithm computes the expected reward of the optimal action @xmath342 $ ] and uses that to compute the expected regret of action @xmath89 : @xmath343 = \\rho^ * - \\frac{\\beta^1_i } { ( \\beta^1_i+\\beta^2_i)}.\\ ] ]    practical implementations of this algorithm can approximate each definite integral by evaluating the integrand at a discrete grid of points in @xmath344 $ ] .",
    "the values of @xmath345 and @xmath346 can be computed and stored for each value of @xmath347 in this grid . in each period , the posterior distribution of only a single action is updated , and hence these values need to be updated for only one action each period .",
    "steps 14 - 21 are the most computationally intensive part of the algorithm .",
    "the computational cost of these steps scales as @xmath348 where @xmath206 is the number of actions and @xmath253 is the number of points used in the discretization of [ 0,1 ] .",
    "algorithm [ alg : gaussianids ] presents an implementation of mean - based information - directed sampling for the setting described in subsection [ subsec : independent normal experiment ] .",
    "consider a multi - armed bandit problem with @xmath206 independent arms denoted by @xmath326 .",
    "the mean reward @xmath327 of each arm @xmath328 is drawn from a gaussian prior distribution , and the means of separate arms are drawn independently .",
    "rewards are corrupted by zero mean guassian noise with known variance .",
    "algorithm [ alg : gaussianids ] is extremely similar to algorithm [ alg : bernoulliids ] .",
    "one clear difference is that this form of information - directed sampling uses the mean - based variation of ids described in subsection [ subsec : ids in mean ] .",
    "in addition , the guassian distribution has a special structure that simplifies the computation of @xmath349.\\ ] ] in particular , the computation of @xmath340 in the algorithm uses the following closed form expression for the expected value of a truncated normal distribution :    @xmath350 = \\mu_j - \\sigma_j \\phi\\left ( \\frac{x-\\mu_j}{\\sigma_j } \\right ) / \\phi\\left ( \\frac{x-\\mu_j}{\\sigma_j } \\right)=\\mu_j - \\sigma^{2}_{j } f_j(x ) /f_j(x ) , \\ ] ] if @xmath351 .",
    "* initialize * : input posterior parameters : + @xmath352 , @xmath353 + * calculate optimal action probabilities * : +   +    * initialize * : input posterior parameters : + @xmath354 , @xmath355 + * calculate optimal action probabilities * : +   +   +      this section provides an implementation of mean - based information - directed sampling for the problem of linear optimization under bandit feedback . consider a problem where the action set @xmath1 is a finite subset of @xmath356 , and whenever an action @xmath52 is sampled only the resulting reward @xmath200 is observed .",
    "there is an unknown parameter @xmath357 such that for each @xmath132 the expected reward of @xmath52 is @xmath208 . in subsection [ subsec : ids in mean ] we introduced the term @xmath101\\ ] ] where @xmath358 -\\underset{y\\sim p_{t , a}}{{\\mathbb{e } } } \\left [ r(y ) \\right ] \\right)^2\\\\ & = & \\left ( { \\mathbb{e}}\\left[r(y_{t}(a))\\vert { \\mathcal{f}_{t-1 } } , a^*=a^ * \\right ]   - { \\mathbb{e}}\\left[r(y_{t}(a))\\vert { \\mathcal{f}_{t-1}}\\right ] \\right)^2 .\\end{aligned}\\ ] ] we will show that for this problem @xmath359 takes on a particularly simple form , and will present an algorithm that leverages this . write @xmath360 $ ] and write @xmath361.$ ] define @xmath362\\right ) \\left ( x-{\\mathbb{e}}\\left[x \\vert { \\mathcal{f}_{t-1}}\\right]\\right)^t    \\vert { \\mathcal{f}_{t-1}}\\right]\\ ] ] to be the posterior covariance of a random variable @xmath363 .",
    "then , @xmath364 [ \\mu_{t}^{(a^*)}- \\mu_{t}]^t \\right)a\\ ] ] and therefore @xmath365 where @xmath366 [ \\mu_{t}^{(a^*)}- \\mu_{t}]^t = { \\rm cov}_{t}\\left ( \\mu_{t}^{(a^*)}\\right).\\ ] ] is exactly the posterior covariance matrix of @xmath367 .",
    "algorithm [ alg : linearids ] presents a simulation based procedure that computes @xmath368 and @xmath369 and selects an action according to the distribution @xmath370 .",
    "this algorithm requires the ability to generate a large number of samples , denoted by @xmath371 in the algorithm , from the posterior distribution of @xmath209 , which is denoted by @xmath372 in the algorithm .",
    "the action set @xmath373 is represented by a matrix @xmath374 where the @xmath88th row of @xmath375 is the action feature vector @xmath376 .",
    "the algorithm directly approximates the matrix @xmath377 that appears in equation .",
    "it does this by sampling parameters from the posterior distribution of @xmath209 , and , for each action @xmath52 , tracking the number of times @xmath52 was optimal and the sample average of parameters under which @xmath52 was optimal . from these samples",
    ", it can also compute an estimated vector @xmath378 of the mean reward from each action and an estimate @xmath379 of the expected reward from the optimal action @xmath38 .",
    "* initialize * : input @xmath380 , @xmath371 and posterior distribution @xmath381 .    *",
    "perform monte carlo * : +   +",
    "like ids , the _ knowledge  gradient _ policy @xcite selects actions by optimizing a single period objective that encourages earning high expected reward and acquiring a lot of information .",
    "define @xmath382\\ ] ] to be the expected reward earned by selecting the best estimated action at time @xmath32 .",
    "define the `` kg factor '' @xmath383 -   v_{t}\\ ] ] to be the expected improvement in decision quality due to sampling action @xmath52 and observing @xmath54 . for a problem with time horizon @xmath15 , the knowledge gradient ( kg )",
    "policy selects an action in time period @xmath32 by solving the maximization problem : @xmath384 + ( t - t ) v^{kg}_{t , a }   \\right\\}.\\ ] ]    the measure of information @xmath385 used by knowledge gradient is extremely natural .",
    "information is valuable only if it improves the quality of the agent s decisions .",
    "unfortunately , as highlighted by the next example , algorithms based on this measure may fail even in extremely simple cases .",
    "consider a problem with two actions @xmath386 and binary rewards .",
    "action 1 is known to yield a reward of @xmath387 with probability @xmath388 and a reward of zero otherwise .",
    "action @xmath389 yields a reward of @xmath387 with unknown probability @xmath390 .",
    "recall that the mean of a random variable with distribution @xmath391 is @xmath392 .",
    "if @xmath393 , then @xmath394 , since after a single sample of action @xmath389 , the posterior mean could never be higher than @xmath395 .",
    "in particular , a single sample could never be influential enough to change which action has the highest posterior mean .",
    "for this reason , the kg decision rule selects @xmath179 in the first period .",
    "since nothing is learned from the resulting observation , it will continue selecting action 1 in all subsequent periods .",
    "even as the time horizon @xmath15 tends to infinity , the kg policy would never select action 2 .",
    "its cumulative regret over @xmath15 time periods is therefore equal to @xmath396 -.51 \\right)$ ] , which grows linearly with @xmath15 .",
    "[ [ many - step - lookahead . ] ] many step lookahead .",
    "+ + + + + + + + + + + + + + + + + + + +    as noted by @xcite , the algorithm s poor performance in this case is due to the increasing returns to information . a single sample of action @xmath180 provides no value , even though sampling the action several times could be quite valuable .",
    "to address the possibility of increasing returns to information @xcite propose a modified form of kg that considers the value of sampling a single alternative many times .",
    "unfortunately , this approach also does not work in general .",
    "as shown the next example , even for problems with independent arms , the value of perfectly observing a single action could be exactly zero , even if there is value to combining information from multiple actions .",
    "consider a problem with two actions .",
    "the reward of each action @xmath397 is @xmath280 , but the parameters @xmath398 and @xmath399 are unknown .",
    "they are distributed independently according to a prior distribution with @xmath400 = .5\\\\   \\mathbb{p}\\left ( \\theta_2 = .7 \\right)= 1- \\mathbb{p}\\left ( \\theta_2 = .5 \\right ) & \\implies &   { \\mathbb{e}}[\\theta_2 ] = .6 \\end{aligned}\\ ] ] the value of observing either @xmath398 or @xmath399 alone is zero , since choosing action @xmath389 is ( weakly ) optimal regardless of what is observed .",
    "no realization of @xmath398 could exceed @xmath401 $ ] and @xmath399 is never less than @xmath402 $ ] .",
    "nevertheless , @xmath403 and @xmath404 , so sampling either action provides information about the optimum .",
    "first , we show the function @xmath80 is convex on @xmath81 .",
    "as shown in chapter 3 of @xcite , @xmath405 is convex over @xmath406 .",
    "the function @xmath407 is affine .",
    "since convexity is preserved under composition with an affine function , the function @xmath408 is convex .",
    "we now prove the second claim .",
    "consider the optimization problems @xmath409 where @xmath410 and @xmath411 denotes the optimal objective value for the minimization problem .",
    "the set of optimal solutions to and correspond . note that @xmath412 but for any feasible @xmath29 , @xmath413 since @xmath414 .",
    "therefore , any optimal solution @xmath415 to is an optimal solutions to and satisfies @xmath416 .",
    "similarly , if @xmath417 then simple algebra shows that @xmath418 and hence that @xmath29 is an optimal solution to    we will now show that there is a minimizer of @xmath419 with at most two nonzero components , which implies the same is true of @xmath420 .",
    "fix a minimizer @xmath421 of @xmath419 .",
    "differentiating of @xmath422 with respect to @xmath29 at @xmath423 yields @xmath424 where @xmath425 is the expected instantaneous regret of the sampling distribution @xmath421 .",
    "let @xmath426 denote the smallest partial derivative of @xmath427 at @xmath421 .",
    "it must be the case that any @xmath88 with @xmath428 satisfies @xmath429 , as otherwise transferring probability from action @xmath328 could lead to strictly lower cost .",
    "this shows that @xmath430 let @xmath431 be the indices such that @xmath432 ordered so that @xmath433 then we can choose a @xmath434 $ ] so that @xmath435 by equation , this implies as well that @xmath436 and hence that the sampling distribution that plays @xmath437 with probability @xmath438 and @xmath439 otherwise has the same instantaneous expected regret and the same expected information gain as @xmath421 .",
    "that is , starting with a general sampling distribution @xmath421 that maximizes @xmath422 , we showed there is a sampling distribution with support over at most two actions attains the same objective value and hence that also maximizes @xmath422 .",
    "the following fact expresses the mutual information between @xmath38 and @xmath54 as the as the expected reduction in the entropy of @xmath38 due to observing @xmath54 .",
    "[ fact : mutual information to entropy ] ( lemma 5.5.6 of @xcite ) @xmath440\\ ] ]    [ lem : bound on information gain ] if actions are selected according to a policy @xmath441 , @xmath442    @xmath443 = { \\mathbb{e}}\\sum_{t=1}^{t } \\left ( h(\\alpha_{t } ) - h(\\alpha_{t+1 } ) \\right)= h(\\alpha_{1 } ) - h(\\alpha_{t+1 } ) \\leq h(\\alpha_{1}),\\ ] ] where the first equality relies on fact [ fact : mutual information to entropy ] and the tower property of conditional expectation and the final inequality follows from the non - negativity of entropy .      by definition , if @xmath108 , then @xmath444 .",
    "therefore , @xmath445= { \\mathbb{e}}\\sum_{t=1}^{t } \\delta_{t}(\\pi_t ) \\leq \\sqrt{\\lambda } { \\mathbb{e}}\\sum_{t=1}^{t } \\sqrt{g_{t}\\left(\\pi_t \\right ) } \\overset{(a)}{\\leq } \\sqrt{\\lambda t } \\sqrt{{\\mathbb{e}}\\sum_{t=1}^{t } g_{t}\\left(\\pi_t \\right ) } \\overset{(b)}{\\leq } \\sqrt{\\lambda h(\\alpha_1 ) t } .\\end{aligned}\\ ] ] inequality ( a ) follows from hlder s inequality and ( b ) follows from lemma [ lem : bound on information gain ] .      by the definition of @xmath446 given in equation and the definition of @xmath112 given in equation , @xmath447 .",
    "this implies @xmath448 = { \\mathbb{e}}\\sum_{t=1}^{t } \\delta_{t } \\left (   \\pi_{t}^{{\\rm ids } } \\right ) & = & { \\mathbb{e}}\\sum_{t=1}^{t } \\sqrt{\\psi_{t}^ * } \\sqrt{g_{t}\\left(\\pi_{t}^{{\\rm ids}}\\right ) } \\\\ & \\overset{(a)}{\\leq } & \\sqrt{{\\mathbb{e}}\\sum_{t=1}^{t } \\psi_{t}^ * } \\sqrt{{\\mathbb{e}}\\sum_{t=1}^{t } g_{t}\\left(\\pi_{t}^{{\\rm ids}}\\right)}\\\\   & \\overset{(b)}{\\leq } & \\sqrt { h(\\alpha_1 ) }   \\sqrt{{\\mathbb{e}}\\sum_{t=1}^{t } \\psi_{t}^ * }   \\\\ & = &   \\sqrt{\\left(\\frac{1}{t } { \\mathbb{e}}\\sum_{t=1}^{t } \\psi_{t}^ * \\right ) h(\\alpha_1 ) t},\\end{aligned}\\ ] ] where ( a ) follows from holder s inequality and ( b ) follows from lemma [ lem : bound on information gain ] .",
    "here we leverage the tools of our very recent analysis of thompson sampling @xcite to provide bounds on the information ratio of ids . since @xmath449",
    ", the bounds on @xmath450 provided by @xcite immediately yield bounds on the minimal information ratio .",
    "the bounds for problems with bandit feedback also apply to mean - based ids , and some new analysis is required to show this .",
    "the following fact lower bounds the kullback ",
    "leibler divergence between two bounded random variables in terms of the difference between their means .",
    "it follows easily from an application of pinsker s inequality .",
    "[ fact : dme to dkl ] for any distributions @xmath39 and @xmath40 such that that @xmath39 is absolutely continuous with respect to @xmath40 , and any random variable @xmath451 such that @xmath452 , @xmath453 - { \\mathbb{e}}_{q } \\left [ x \\right ]   \\leq \\sqrt{\\frac{1}{2 } { d_{\\rm kl}}\\left ( p || q \\right ) } , \\ ] ] where @xmath454 and @xmath455 denote the expectation operators under @xmath39 and @xmath40 .",
    "a proof of this result can be found in the appendix of @xcite .",
    "because of assumption [ assum : bounded rewards ] , this fact shows @xmath456    the following corollary of this fact allows us to bound the information ratio by considering the information gain in mean @xmath457 instead of @xmath458 .    [ cor : info ratio to info ratio in mean ] for any action sampling distribution @xmath59 , @xmath459    by corollary [ cor : info ratio to info ratio in mean ] , bounds on the information ratio of @xmath311 and @xmath460 follow by bounding @xmath461 since @xmath462 the following proposition presents a simplified form of the right hand side of this equation , which we will use in our subsequent analysis .    [ prop : modified info ratio for ts ] @xmath463 \\right)^2}{{\\underset{\\underset { a^ * \\sim \\alpha_t } { a \\sim \\alpha_t}}{{\\mathbb{e}}}}\\left [ { d_{\\rm me}}\\left ( p_{t , a}(\\cdot",
    "| a^ * ) \\ , , \\ , p_{t , a } \\right)^2 \\right]}\\ ] ]    the proof essentially follows by plugging in the definitions of @xmath464 and @xmath465 in subsection [ subsec : ids in mean ] , the definition @xmath466 , and the definition @xmath467 of thompson sampling .",
    "first , @xmath468 = \\underset{a\\sim \\alpha_{t}}{{\\mathbb{e}}}\\left [ g^{\\rm me}_{t}(a)\\right]={\\underset{\\underset { a^ * \\sim \\alpha_t } { a \\sim \\alpha_t}}{{\\mathbb{e}}}}\\left [ { d_{\\rm me}}\\left ( p_{t , a}(\\cdot | a^ * ) \\ , , \\ , p_{t , a } \\right)^2\\right].\\ ] ] then , @xmath469 & = & { \\mathbb{e}}\\left[r(y_{t}(a^ * ) ) \\vert { \\mathcal{f}_{t-1}}\\right ] -   { \\underset{a \\sim \\alpha_t}{{\\mathbb{e}}}}\\left [   { \\mathbb{e}}\\left [ r(y_{t}(a ) ) \\vert { \\mathcal{f}_{t-1}}\\right]\\right ]   \\\\ & \\overset{(a)}{= } & { \\underset{a \\sim \\alpha_t}{{\\mathbb{e}}}}\\left [    { \\mathbb{e}}\\left [ r(y_{t}(a ) )   \\vert { \\mathcal{f}_{t-1 } } , a^ * = a \\right ] - { \\mathbb{e}}\\left [ r(y_{t}(a ) )   \\vert { \\mathcal{f}_{t-1}}\\right ]   \\right ] \\\\ & = & { \\underset{a \\sim \\alpha_t}{{\\mathbb{e}}}}\\left [   { d_{\\rm me}}\\left ( p_{t , a}(\\cdot | a ) \\ , , \\ ,",
    "p_{t , a } \\right ) \\right ] \\end{aligned}\\ ] ] where ( a ) follows by the tower property of expectation .",
    "we bound the numerator of by @xmath140 times its denominator .",
    "this is sufficient because of . by the cauchy ",
    "schwarz inequality , @xmath470 the result then follows by squaring both sides .",
    "the result is implied by proposition 4 in @xcite , which shows @xmath471 .",
    "the term @xmath472 in that paper is exactly @xmath473 .",
    "the proof of this result is the generalization of the proof of proposition [ prop : worst case bound ] .",
    "that result followed through an appropriate application of the cauchy ",
    "schwartz inequality .",
    "the next fact provides an analogous result for matrices . for any rank @xmath474 matrix @xmath475 with singular values @xmath476 ,",
    "let @xmath477 denote respectively the nuclear norm , frobenius norm and trace of @xmath478 .",
    "[ fact : trace frobenius inequality ] for any matrix @xmath479 , @xmath480    a proof of this fact can be found in the appendix of @xcite .",
    "we now prove proposition [ prop : linear ] .",
    "write @xmath481 and define @xmath482 by @xmath483 for each @xmath484",
    ". then @xmath485 = \\rm{trace}(m),\\ ] ] and @xmath486= \\| m \\|_{\\rm f}^2.\\ ] ] this shows , by fact [ fact : trace frobenius inequality ] and proposition [ prop : modified info ratio for ts ] that @xmath487 we now show @xmath488 .",
    "define @xmath489 \\\\ \\mu^j & = & \\mathbb{e}\\left [ \\theta_{p^*}| { \\mathcal{f}_{t-1 } } , a^*=a_j \\right ] .",
    "\\end{aligned}\\ ] ] then , by the linearity of the expectation operator , @xmath490 .",
    "therefore , @xmath491 and @xmath492\\left[\\begin{array}{cccc } \\sqrt{\\alpha_{t}(a_1)}a_{1 } & \\cdots & \\cdots & \\sqrt{\\alpha_{t}(a_k)}a_{k}\\end{array}\\right].\\ ] ] since @xmath478 is the product of a @xmath206 by @xmath149 matrix and a @xmath149 by @xmath206 matrix , it has rank at most @xmath149 .",
    "proposition 6 of @xcite , shows @xmath493 almost surely for any @xmath123 .",
    "note that the term @xmath472 in that paper is exactly @xmath494 .",
    "the result then follows since @xmath495",
    "with probability 1 , @xmath496 then , @xmath497 \\overset{(a)}{\\leq } \\frac{1}{t } { \\mathbb{e}}\\left [ \\sum_{t=1}^{t } \\underset{a\\in{\\mathcal{a}}}{\\min } \\ , \\delta_{t}(a ) \\right ] & \\leq & \\frac{\\sqrt{\\lambda}}{t }   { \\mathbb{e}}\\left[\\sum_{t=1}^{t } \\sqrt{\\underset{a\\in{\\mathcal{a}}}{\\max } \\ , g_{t}(a ) } \\right ] \\\\ & \\overset{(b)}{\\leq } &   \\sqrt{\\frac{\\lambda}{t } }   \\sqrt { { \\mathbb{e}}\\sum_{t=1}^{t } \\underset{a\\in{\\mathcal{a}}}{\\max } \\ , g_{t}(a ) } \\\\ & \\overset{(c)}{\\leq } & \\sqrt { \\frac{\\lambda h(\\alpha_{1})}{t}},\\end{aligned}\\ ] ] where ( a ) follows by equation in the proof of proposition [ prop : terminal regret from cumulative regret ] , ( b ) follows by the cauchy  schwartz inequality , and ( c ) follows by lemma [ lem : bound on information gain ] ."
  ],
  "abstract_text": [
    "<S> we propose _ information - directed sampling _  a new algorithm for online optimization problems in which a decision - maker must balance between exploration and exploitation while learning from partial feedback . </S>",
    "<S> each action is sampled in a manner that minimizes the ratio between squared expected single - period regret and a measure of information gain : the mutual information between the optimal action and the next observation .    </S>",
    "<S> we establish an expected regret bound for information - directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution . </S>",
    "<S> for the widely studied bernoulli , gaussian , and linear bandit problems , we demonstrate simulation performance surpassing popular approaches , including upper confidence bound algorithms , thompson sampling , and the knowledge gradient algorithm . </S>",
    "<S> further , we present simple analytic examples illustrating that , due to the way it measures information gain , information - directed sampling can dramatically outperform upper confidence bound algorithms and thompson sampling . </S>"
  ]
}