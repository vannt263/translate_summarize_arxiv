{
  "article_text": [
    "astronomy has witnessed an ever - increasing deluge of data over the past decade .",
    "future surveys will gather very large amounts of data daily that will require on - the - fly analysis to limit the amount of data that can be stored or analyzed , and allow timely discoveries of candidates to be followed - up : for instance euclid ( * ? ? ?",
    "* gb per day ) , wfirst ( * ? ? ?",
    "* per day ) , or the large synoptic survey telescope ( lsst , * ? ? ?",
    "* tb per day ) .",
    "the evolution of the type , volume , cadence of the astronomical data requires the advent of robust methods that will enable a maximal rate of extraction of information from the data .",
    "this means that for a given problem , one needs to make sure all the relevant information has is made available in the first step , followed by the use of a suitable method that is able to narrow down on the important aspects of the data .",
    "this can be both for feature selection as well as for noise filtering .    in machine learning parlance ,",
    "the tasks required can be designated as classification tasks ( derive a discrete value : star vs galaxy for instance ) or regression tasks ( derive a continuous value : photometric redshift for instance . )",
    "methods for classification or regression usually are of two kinds : physically motivated , or empirical .",
    "physically motivated methods use templates built from previously observed data , like star or galaxy spectral energy distributions ( seds ) for instance in the case of determining photometric redshifts",
    ". they also attempt at including as much knowledge as we have of the processes involved in the problem at stake , such as prior information .",
    "physically motivated methods seem more appropriate than empirical , however the very fact that they require a good knowledge of the physics involved might be a important limitation . indeed , our knowledge of a number of processes involved in shaping the seds of galaxies for instance is still quite limited ( e.g. * ? ? ?",
    "* and references therein ) : whether is it about the processes driving star formation , dust attenuation laws , initial mass function , star formation history , agn contribution etc  hence choices need to be made : either make a number of assumptions , in order to reduce the number of free parameters . or , one can decide to be more inclusive and add more parameters , but at the cost of potential degeneracies .",
    "physically motivated methods are also usually limited in the way they treat the correlation between the properties of the objects , because the only correlations taken into account are those included in the models used , and might not reflect all of the information contained in the data .    on the other hand ,",
    "empirical methods require few or no assumptions about the physics of the problem .",
    "the goal of an empirical method is to build a decision function from the data themselves .",
    "the quality of the generalization of the results to a full dataset depends of course on the representativity of the training sample .",
    "we note however that the question of the generalization also applies to physically motivated methods , as they are also validated on the training data .",
    "depending on the methods used , transformations may need to be effected before the method is applied ; for example in the case of support vector machines ( svm , e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) linear separability is presumed , thereby requiring non - linearly separable or regressible data to be kernel transformed to enable this .",
    "a challenge with empirical methods is that with growing numbers of input parameters , it becomes prohibitive in terms of cpu time to use all of them .",
    "it can also be counter - productive to feed the machine learning tool with all of these parameters , as some of them might either be too noisy , or not bring any relevant information to the specific problem to tackle .",
    "moreover , high dimensional problems suffer from the bane of being over fit by machine learning methods @xcite , thereby yielding high - dimensional non - optimal solutions .",
    "this requires sub - selection of relevant variables from a large @xmath5dimensional space ( with @xmath6 potentially close to 1000 ) .",
    "this task itself can also be achieved using machine learning tools . here",
    "we present , to our knowledge , the first application to astronomy of the combination of two machine learning techniques : genetic algorithms ( ga , e.g. * ? ? ?",
    "* ) for selecting relevant features followed by svm to build a decision / reward function .",
    "ga alone have already been used in astronomy for instance to study the orbits of exoplanets , and the seds of young stellar objects @xcite , the analysis of snia data @xcite , the study of star formation and metal enrichment histories of resolved stellar system @xcite , the detection of globular clusters from hst imaging @xcite , or photometric redshifts estimation @xcite . on the other hand , svm",
    "have been extensively used to solve a number of problems such as object classification @xcite , the identification of red variables sources @xcite , photometric redshift estimation @xcite , morphological classification @xcite , and parameters estimation for gaia spectrophotometry @xcite .",
    "we note also that the combination of ga and svm has already been used in a number of fields such as cancer classification ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) , chemistry ( e.g. * ? ? ?",
    "* ) , bankruptcy prediction ( e.g. * ? ? ?",
    "* ) etc  we do not attempt here to provide the most optimized results from this combination of methods .",
    "rather , we present a proof - of - concept that shows that ga and svm yield remarkable results when combined together as opposed to using the svm as a standalone tool .    in this paper , we focus on two tasks frequently seen in large surveys : star / galaxy separation and determination of photometric redshifts of distant galaxies . star / galaxy separation is a classification problem that has been usually constrained purely from the morphology of the objects ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "on the other hand , a number of studies used the color information with template fitting to separate stars from galaxies ( e.g. * ? ? ? * ) . on top of these two common approaches ,",
    "good results have been obtained by feeding morphology and colors to machine learning techniques ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "our goal here is to extend the star / galaxy separation to faint magnitudes where morphology is not as reliable .",
    "we apply here the combination of ga with svm to star / galaxy separation in the pan - starrs1 ( ps1 ) medium deep survey @xcite .    on the other hand ,",
    "determining photometric redshifts is a well - studied problem of regression that has been dealt with using a variety of methods , including template fitting ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , cross correlation functions @xcite , random forests @xcite , neural networks @xcite , polynomial fitting @xcite , symbolic regression @xcite .",
    "we apply the ga - svm to the zcosmos bright sample @xcite .",
    "the outline of this paper is as follows . in sect .",
    "[ sec_data ] we present the data we apply our methods to , and also the training sets we use : ps1 , and zcosmos bright . in sect .",
    "[ sec_methods ] we briefly describe our methods ; we include a more detailed description of svm in an appendix .",
    "we present our results in sect .",
    "[ sec_results ] , and finally list our conclusions .",
    "we perform star / galaxy separation in ps1 medium deep data ( see sect . [ sec_ps1 ] ) , using a training set built from cosmos acs imaging ( see sect .",
    "[ sec_cosmos_sg ] ) .",
    "we then derive photometric redshifts for the zcosmos bright sample ( see sect . [ sec_cosmos_z ] ) .      the pan - starrs1 survey @xcite surveyed 3/4 of the northern hemisphere ( @xmath7deg ) , in 5 filters : @xmath8 , @xmath9 , @xmath10 , @xmath11 , and @xmath12 @xcite .",
    "in addition , ps1 has obtained deeper imaging in 10 fields ( amounting to 80 deg@xmath13 ) called as the medium deep survey @xcite .",
    "we use here data in the md04 field , which overlaps the cosmos survey @xcite .",
    "we use our own reduction of the ps1 data in the medium deep survey .",
    "we use the image stacks generated by the image processing pipeline @xcite , and also the cfht @xmath14 band data obtained by e. magnier as follow up of the medium deep fields .",
    "we have at hand 6 bands : @xmath15 , @xmath8 , @xmath9 , @xmath10 , @xmath11 , and @xmath12 .",
    "we perform photometry using the following steps ; we consider the ps1 _ skycell _ as the smallest entity : _",
    "i ) _ resample ( using ` swarp ` , * ? ? ?",
    "* ) the u band images to the ps1 resolution , and register all images ; _ ii ) _ for each band fit the psf to a moffat function , and match that psf to the worse psf in each skycell ; _ iii ) _ using these psf - matched images , we derive a @xmath16 image @xcite ; _ iv ) _ we perform photometry using the ` sextractor ` dual mode @xcite , detecting objects in the @xmath16 image and measuring the fluxes in the psf matched images : the kron - like apertures are defined from the @xmath16 image and hence are the same over all bands ; _ v ) _ for each detection we measure ` spread_model ` in each band on the original , non psf - matched images .",
    "we consider here kron magnitudes , which are designed to contain @xmath17 of the source light , regardless of being a point source star or an extended galaxy .    ` spread_model `",
    "@xcite is a discriminant between the local psf , measured with ` psfex ` @xcite and a circular exponential model of scale length @xmath18 convolved with that psf .",
    "this discriminant has been shown to perform better than sextractor previous morphological classifier , ` class_star ` .",
    "we use the star / galaxy classification from @xcite derived from high spatial resolution hubble space telescope ( hst ) imaging ( in the @xmath19 band ) as our training set for star / galaxy classification .",
    "they separate stars from galaxy based on their ` sextractor ` ` mag_auto ` and ` mu_max ` ( peak surface brightness above surface level ) . by comparing the derived stellar counts with the models of @xcite",
    ", @xcite showed that the stellar counts are in excellent agreement with the models for @xmath20 .",
    "we perform here star / galaxy separation down to @xmath21 .",
    "we use data taken as part of the cosmos survey @xcite .",
    "we focus here on objects with spectroscopic redshifts obtained during the zcosmos bright campaign ( * ? ? ?",
    "* @xmath22 ) .",
    "we use the photometry obtained in 25 bands by various groups in broad optical and near - infrared bands : ( * ? ? ?",
    "* @xmath23,@xmath24,@xmath25,@xmath26 , @xmath27 , @xmath28,@xmath29,@xmath30 ) ; narrow bands : ( * ? ? ?",
    "* @xmath31 ) and ( * ? ? ? * @xmath32 ) ; intermediate bands ( * ? ? ?",
    "* @xmath33 , @xmath34 , @xmath35 , @xmath36 , @xmath37 , @xmath38 , @xmath39 , @xmath40 , @xmath41 , @xmath42 , @xmath43 , @xmath44 ) .",
    "we also use irac1 and irac2 photometry @xcite .",
    "we consider here only objects with good redshift determination ( confidence class : 3.5 and 4.5),@xmath45 ( which yields 5,807 objects ) and measured magnitudes for all the bands listed above , which finally leaves us with 5093 objects .",
    "we do not use objects with other confidence class values , as the spectroscopic redshifts can be erroneous in at least 15% of cases ( ilbert , private communication ) .",
    "we present here the two machine learning methods we use in this work : genetic algorithms are used to select the relevant features , and support vector machines are used to predict the property of interest using these features .",
    "genetic algorithms ( ga , e.g. * ? ? ?",
    "* ) apply the basic premise of genetics to the evolution of solution sets of a problem , until they reach optimality .",
    "the definition of optimality itself is debateable , however , the general idea is to use a reward function to direct the evolution .",
    "that is , we evolve solution sets of parameters ( or genes ) known as organisms through several generations , according to some pre - defined evolutionary reward function .",
    "the reward function may be a goodness of fit , or a function thereof that may take into account more than just the goodness of fit .",
    "for example , one may choose to determine subsets of parameters which optimize @xmath16 or likelihood , akaike information criteria , energy functions , entropy , etc .",
    "the lengths of the first generation of organisms is chosen to range from the minimum expected dimensionality of the parameter space ( which can be 1 ) to the maximum expected dimensionality of the covariance .    the fittest organisms in a particular generation",
    "are then given higher probabilities of being chosen to be the parents for successive generations using a roulette selection method based on their fitnesses .",
    "parents are then cross - bred using a customized procedure - in our method we first choose the lengths of the children to be based on a tapered distribution where the taper depends on the fitnesses of the parents . that way the length of the child is closer to that of the fitter parent .",
    "the child is then populated using the genes of both parents , where genes that are present in both parents are given twice the weight as genes that are only present in one parent .",
    "the idea is that the child should , with a greater probability , contain genes that are present in both parents as opposed to the ones contained in only one of them .",
    "this process iteratively produces fitter children in successive generations and is terminated when no further improvement in the average organism quality is seen .",
    "like in biological genetics , we also introduce a mutation in the genes with constant probability .",
    "the genes which are chosen to be mutated , are replaced with any of the genes that are not part of either parent , with uniform probability of choosing from the remaining genes .",
    "this allows for genes that are not part of the current gene pool to be expressed .",
    "the conceptual simplicity of genetic algorithms combined with their evolutionary analogy as applied to some of the hardest multi - parameter global optimization problems makes them highly sought after .",
    "support vector machines ( svm ) ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) is a machine learning algorithm that constructs a maximum margin hyperplane to separate linearly separable patterns .",
    "svm is especially efficient in high dimensional parameter spaces , where separating two classes of objects is a hard problem , and performs with best case complexity @xmath46 .",
    "where the data is not linearly separable , a kernel transformation can be applied to map the original parameter space to a higher dimensional feature space where the data becomes linearly separable . for both problems we attempt to solve in this paper",
    ", we use a gaussian radial basis function kernel , defined as    @xmath47    another advantage of using svm is that there are established guarantees of their performance which has been well documented in literature .",
    "also , the final classification plane is not affected by local minima in the classification or regression statistic , which other methods based on least squares , or maximum likelihood may not guarantee . for",
    "a detailed description of svm please refer to @xcite .",
    "we use here the ` python ` implementation within the ` scikit - learn ` @xcite module .",
    "we combine the ga and svm to select the optimal set of parameters that enable one to classify objects or derive photometric redshifts . in either case , we first gather all input parameters , and build color combinations from all available magnitudes . we also consider here transformations of the parameters , namely their logarithm and exponential on top of their catalog values that we name linear afterwards , in order to capture non - linear dependencies on the parameters .",
    "note that any transformation could be used , we limited ourselves to @xmath48 and @xmath49 for sake of simplicity for this first application .",
    "this way , the rate of change of the dependent parameter as a function of the independent parameter in question is more accurately captured , and dependencies on multiple transformations of the same variable can also be captured . to eliminate scaling issues in transformed spaces",
    ", we transform all independent parameters to between -1 and 1 .",
    "the optimization is then performed the following two steps iteratively until the end criterion or convergence criterion is met : selection of the relevant set of features in the first , and automatic optimization of svm parameters to yield optimal classification / regression given the parameters . for svm classification , the true positive rate is used as the fitness function . for svm regression , a custom fitness function based on the problem at hand is chosen .",
    "for example , for svm regressions on the photometric redshift we use    @xmath50    once the fitness of all organisms has been evaluated , a new generation of same size as that of the parent generation is created using roulette selection .",
    "the ga then runs until it reaches a pre - defined stopping criterion .",
    "we use here the posterior distribution of the parameters .",
    "we stop the ga when all parameters have been used at least 10 times .",
    "we also use the posterior distribution to choose the optimal set of parameters .",
    "various schemes can be defined . for our application , we restrict ourselves to characterizing the posterior distribution by its mean @xmath51 and standard deviation @xmath52 ( see figure [ fig_ga_posterior ] ) .",
    "we consider here all parameters that appear more that @xmath53 or @xmath54 times in the posterior distribution , depending on which our results change significantly .",
    "for instance , in the case of photometric redshifts the mean of the posterior distribution is @xmath55 , and the standard deviation @xmath56 , we keep all parameters that occur more than 24 times in the posterior distribution when using the @xmath53 criterion .",
    "svm are not \" black boxes  , but come with a well defined formalism , and free parameters to be adjusted . for this application",
    ", we use the @xmath57svm version of the algorithm which allows to control the fractional error and the lower limit on the fraction of support vectors used .",
    "we use here @xmath58 . in the case of classification ( star / galaxy separation ) , we are then left with only one free parameter , the inverse of the width of the gaussian kernel , @xmath59 ( eq . [ eq_rbf ] ) . in the case of regression ( photometric redshifts ) , we have an additional free parameter , the trade off parameter @xmath60 ( see appendix [ app_svm ] ) .    if not used with caution , machine learning methods can lead to over - fitting : the decision function is biased by the training sample and will not perform well on other samples . in order to avoid over - fitting and optimize the value of @xmath59 and @xmath60 , we perform 10-fold cross - validation",
    ": we divide the sample in 10 subsets ; we then perform classification or regression for each subset after training on the 9 other subsets .",
    "we perform this cross validation for a grid of @xmath59 and @xmath60 values . in the case of svm",
    ", the overfitting can be measured by the fraction of objects used as support vectors , @xmath61 .",
    "if @xmath62 , most of the training sample is used as support vectors , will lead to poor generalization . for each iteration , we also get @xmath63 , in order to include it on our cost function . for both applications ,",
    "we minimize a custom cost function that optimizes the quality of the classification or regression , and @xmath63 .",
    "we use as inputs to the ga feature selection step all magnitudes available for the ps1 dataset : @xmath15 , @xmath8 , @xmath64 , @xmath10 , @xmath11 , and @xmath12 ; the ` spread_model ` values derived from each of these bands ; the ellipticity measured on the @xmath16 image , all colors .",
    "we also included a few quantities determined by running the code ` lephare ` on these data : photometric redshift , and ratios of the minimum @xmath16 using galaxy , star , and quasar templates : @xmath65 , @xmath66 , and @xmath67 . including the transformations of these parameters",
    "yields 96 input parameters to the ga - svm feature selection procedure .",
    "the parameters selected by the ga with occurence larger than @xmath53 times in the posterior distribution are listed in table [ tab_sg_param ] .",
    "we also indicate the parameters whose occurence is larger than @xmath54times .",
    "the 15 selected parameters include ` spread_model ` derived in @xmath8 , @xmath9 and @xmath10 , but are dominated by colors ( 7 of 15 ) . using the parameters occuring more than @xmath54 yields similar results , however with significant overfitting .",
    "ccc @xmath15 & log & @xmath68 + @xmath9 & lin & @xmath68 + @xmath9 & log & @xmath69 + ` spread_model_g ` & exp & @xmath68 + ` spread_model_r ` & lin & @xmath69 + ` spread_model_r ` & log & @xmath69 + ` spread_model_i ` & exp & @xmath69 + @xmath70 & lin & @xmath68 + @xmath71 & lin & @xmath68 + @xmath72 & lin & @xmath69 + @xmath73 & log & @xmath68 + @xmath74 & exp & @xmath68 + @xmath75 & lin & @xmath68 + @xmath76 & exp & @xmath68 + @xmath77 & lin & @xmath68 +    in order to quantify the quality of the star / galaxy separation , we use following definitions for the completeness @xmath78 and the purity @xmath79 :    @xmath80    where @xmath81 is the number of objects of class @xmath82 correctly classified , and @xmath83 is the number of objects of class @xmath82 misclassified .",
    "the same definition holds for the star completeness and purity .",
    "based on these definitions , in the case of star / galaxy separation , we use as cost function :    @xmath84    in other words , we choose here to optimize the average completeness and purity for stars and galaxies , and also penalize high @xmath63 , which amounts to penalize high fractional errors , and overfitting as well .",
    "other optimization schemes can be adopted .",
    "we perform the optimization over the only svm free parameter here , the inverse of the width of the gaussian kernel , @xmath59 .",
    "we use a grid search using a @xmath48-spaced binning for @xmath85 .",
    "we show in fig .",
    "[ fig_sg_sep ] the ` spread_model ` derived in the ps1 @xmath86 band , as a function of @xmath10 .",
    "[ fig_sg_sep ] shows that ` spread_model ` enables to recover a star sequence down to @xmath87 . at fainter magnitudes morphology alone",
    "is not able to separate accurately stars from galaxies at the ps1 angular resolution .",
    "the colors coding shows the result of the ga - svm classification .",
    "we classify objects down to @xmath21 .",
    "we choose this limit because the completeness of the ps1 data drops significantly beyond 24.5 , and also because the training set we use is valid down to @xmath88 .",
    "1 suggests that the ga - svm is able to recover the classification at bright magnitudes , but also to extend it at the faint end .",
    "we list in table [ tab_sg_performance ] the percentage of objects classified correctly .",
    "our method correctly classifies 97% of the objects down to @xmath21 . we can compare our results to those from the ps1 photometric classification server @xcite , who used svm on bright objects using ps1 photometry .",
    "they obtained 84.9% of stars correctly classified down to @xmath89 , and 97% of galaxies down to @xmath90 .",
    "our method enables to improve upon those , as we get 88.6% of stars correctly classified , and 99.3% of galaxies correctly classified in the same magnitude range .",
    "we examine in more details in fig .",
    "[ fig_sg_sep_color_mag ] and [ fig_sg_sep_color_color ] the colors of the objects .",
    "these figures show that the bulk of stars that are misclassified as galaxies are at the faint end @xmath91 , and that these objects are in regions where the colors of stars and galaxies are similar .",
    "galaxies misclassified as stars are brighter @xmath92 but again are in regions where colors of stars and galaxies overlap .",
    "there are also a handful of very bright stars misclassified as galaxies , in a domain where the galaxy sampling is very poor .",
    "finally , we classify as galaxies a few stars showing colors at the outskirt of the color distributions",
    ". these objects might be either misclassified by the acs photometry , or the color might be significantly impacted by photometric scatter .",
    "we show in fig .",
    "[ fig_sg_quality ] the completeness ( left ) and purity ( right ) of our classifications as a function of @xmath10 .",
    "cccc all types & 97.4 & 98.1 & 92.7 + galaxies & 99.8 & 99.2 & 94.5 + stars & 74.5 & 88.5 & 75.9    we derive the completeness and purity for each cross - validation subset , and show in fig .",
    "[ fig_sg_quality ] the average , and as error bars the standard deviation .",
    "most of the features of our classification seen in fig .",
    "[ fig_sg_quality ] are due to the fact that the training sample is unbalanced at the bright end and the faint end : at the bright end , stars outnumber the galaxies , and the other way around at the faint end . at the bright end ( @xmath93 , which is also within the saturation regime ) ,",
    "the completeness is higher for stars than for galaxies , while noisy because of small statistics .",
    "some bright galaxies are misclassified as stars . at the faint end ( @xmath94 ) , the star completeness decreases , as some stars are classified as galaxies . the impression given by fig .",
    "[ fig_sg_quality ] is striking , as the star completeness decreases to 0 at @xmath95 .",
    "note however that the stars represent only 3% of the overall population at this flux level @xcite .",
    "the purity shows a similar behaviour at the bright end . at the faint end",
    "however the stars purity is larger than 0.85 for @xmath96 . for galaxies , both completeness and purity are lower than 0.8 at the bright end ( @xmath97 ) , however the number of galaxies is small at these magnitudes . at @xmath98 ,",
    "completeness and purity are independent of magnitude down to @xmath21 and larger than 0.95 .",
    "we compare this results to the classification obtained with ` spread_model ` derived in the @xmath10 band .",
    "we determine a single cut in ` spread_model_i ` using its distribution for reference stars and galaxies .",
    "our cut is the value of ` spread_model_i ` such that @xmath99 .",
    "we show in fig .",
    "[ fig_sg_quality ] the completeness and purity obtained with ` spread_model_i ` as dotted lines . for @xmath100 , the results from ` spread_model_i ` and our method are similar , as at the ps1 resolution , point sources and extended objects are well discriminated in this magnitude range . at @xmath101 , our baseline method performs better , in particular for galaxies , which is expected as we add color information . while the galaxies purity is similar for both methods , the completeness obtained with ` spread_model_i ` drops to 0.75 at @xmath102 , but our methods yields a completeness consistent with 1 down to this magnitude . for stars ,",
    "the purity obtained with the two methods are similar .",
    "on the other hand , the completeness we obtain drops faster than that obtained with ` spread_model_i ` . in order to see whether we can improve our baseline method",
    ", we optimize the svm parameters independently in two magnitude range : @xmath103 and @xmath1 . as galaxies at the faint end outnumber stars by several orders of magnitude , we add an extra free parameter for the optimization at @xmath1 , which attempts at correcting this sampling issue . in practice",
    "we use all stars available , but only a fraction of the galaxies available , from 1 to 10 times the number of stars .",
    "the results obtained are shown in solid lines on fig .",
    "[ fig_sg_quality ] .",
    "the results for galaxies are virtually unchanged compared to our baseline method . for stars , we are able to improve at the faint end , where the purity is better than that obtained with ` spread_model_i ` for @xmath104 .    as a final check , we also derive the star / galaxy separation by using svm only , without selecting the inputs with the ga .",
    "the results are only marginally different .",
    "we note however that a parameter space with lower dimensions is less prone to overfitting with machine learning methods . on the other hand ,",
    "even with similar results , a svm - based star / galaxy separation with smaller number of inputs parameters is more likely to generalize properly .",
    "we used 978 input parameters as inputs for the ga - svm optimization procedure : all magnitudes and colors available from the cosmos dataset and the transformations of these parameters , as described in sect .",
    "[ sec_optimization ] .",
    "hereafter we consider the parameters that appear at least @xmath105 times in the posterior distribution , we are left with 131 parameters .",
    "the parameters retained by the ga - svm are dominated by colors ( 82% , 108/131 ) , and in particular colors involving intermediate and narrow bands ( 71% , 93/131 ) . using the parameters that appear @xmath69 times in the ga posterior distribution yields 45 parameters , with similar proportions of colors and intermediate and narrow bands .",
    "this is in line with the conclusions of studies using sed fitting methods which show that including narrow and intermediate bands improves significantly the estimation of photometric redshifts ( e.g. * ? ? ?",
    "we quantify the errors on photometric redshifts as @xmath106 , use as a measure of global accuracy @xmath52 the normalized median absolute deviation , defined as @xmath107 .",
    "we define the following cost function for the svm optimization : @xmath108        we perform the optimization over the 2 svm free parameters available here , @xmath59 , and the trade off parameter @xmath109 we use a grid search using a log - spaced binning for @xmath85 and @xmath110 .",
    "we compare in fig .",
    "[ fig_zs_zp ] spectroscopic and photometric redshifts .",
    "we obtain an overall accuracy of 0.013 yields similar results with an accuracy of @xmath111 .",
    "the percentage of outliers , defined as objects with @xmath112 , is below 1% .",
    "the average error ( bias ) is equivalent to 0 ; our results do not show any significant bias as a function of redshift . at high redshifts ( @xmath113 )",
    "the spectroscopic sampling is small , and so the model is less constrained . using the parameters that appear @xmath69 times in the ga - svm posterior distribution yields similar results ( @xmath114 ) , which shows that by using 3 times less parameters , the same accuracy can be achieved .    on a similar sample , @xcite , using a sed fitting method , obtain an overall accuracy of 0.007 . while our results are slightly worse at face value , we note that we use here only 2 free parameters for the photometric redshift optimization , and one model to derive the photometric redshifts ( the one from svm ) . on the other hand @xcite rely on 21 sed templates , 30 zero point offsets ( one per band ) , and an extra parameter describing the amount @xmath115 of internal dust attenuation .",
    "we also explicitly avoid overfitting , and doing so guarantees the potential for generalization of these results .",
    "our tests show that we can obtain an overall accuracy of @xmath116 , however this comes at the price of significant overfitting ( the support vectors are made up from the whole sample . )        as above , we also derive the photometric redshifts by using svm only , without selecting the inputs with the ga . in this case",
    "the results are much worse , yielding large errors : @xmath117 .",
    "this shows that the combination of ga and svm yields better results than svm alone .",
    "we also test whether we can derive empirical error estimates for each objects using svm .",
    "we use a variant of the @xmath118fold cross validation , so - called `` inverse '' .",
    "the usual @xmath118fold cross validation consists of dividing the sample into @xmath119 subsamples ( we use @xmath120 here ) ; for each subsample predictions are made using the svm trained on the union of the other @xmath121 subsamples . to derive error estimates",
    ", we use the inverse @xmath118fold cross validation such that we train the svm on one subsample , and predict photometric redshifts for the union of the other @xmath121 subsamples .",
    "we have then @xmath121 estimates of @xmath122 .",
    "we derive an empirical error estimate @xmath123 which is the standard deviation of these estimates .",
    "we show in fig .",
    "[ fig_zp_err ] the normalized distribution of the ratio @xmath124 .",
    "if our empirical estimate were an accurate measurement of the actual error , this distribution should be a normal one .",
    "the comparison with a normal distribution shows that this is indeed the case , which suggests that our error estimates are accurate .",
    "we present a new combination of two machine learning methods that we apply to two common problems in astronomy , star / galaxy separation and photometric redshift estimation .",
    "we use genetic algorithms to select relevant features , and support vector machines to estimate the quantity of interest using the selected features .",
    "we show that the combination of these two methods yields remarkable results , and offers an interesting opportunity for future large surveys which will gather large amount of data . in the case of star / galaxy separation ,",
    "the improvements over existing methods are a consequence of adding more information , while for photometric redshifts , it is rather the selection of the input information fed to the machine learning methods .",
    "this shows that the combination of ga and svm is very efficient in the case of problems with large dimensions .",
    "we first apply the ga - svm method to star / galaxy separation in the ps1 medium deep survey .",
    "our baseline method correctly classifies 97% of objects , in particular virtually all galaxies .",
    "our results improve upon the new sextractor morphological classifier , ` spread_model ` , which is expected as we added color information compared to morphology only .",
    "we show how these results can be further improved for stars by training separately bright and faint objects , and taking into account the respective number of stars and galaxies to avoid being dominated by one population .",
    "we then apply the ga - svm method to photometric redshift estimation for the zcosmos bright sample .",
    "we obtain an accuracy of 0.013 , which compares well with results from sed fitting , as we are using only 2 free parameters .",
    "we also show that we can derive accurate error estimates for the photometric redshifts .",
    "we present here a proof - of - concept of a new method that can be modified or improved depending on the problem at stake .",
    "for instance , one can substitute another machine learning tool to svm ( such as random forests for instance ) to derive the quantity of interest .",
    "furthermore the criterion used to select the final number of features from the ga posterior distribution can be also be optimized beyond the one we use here .",
    "all these tools will enable to use as much information as possible in an efficient way for future large surveys .",
    "the pan - starrs1 surveys ( ps1 ) have been made possible through contributions of the institute for astronomy , the university of hawaii , the pan - starrs project office , the max - planck society and its participating institutes , the max planck institute for astronomy , heidelberg and the max planck institute for extraterrestrial physics , garching , the johns hopkins university , durham university , the university of edinburgh , queen s university belfast , the harvard - smithsonian center for astrophysics , the las cumbres observatory global telescope network incorporated , the national central university of taiwan , the space telescope science institute , the national aeronautics and space administration under grant no .",
    "nnx08ar22 g issued through the planetary science division of the nasa science mission directorate , the national science foundation under grant no .",
    "ast-1238877 , the university of maryland , and eotvos lorand university ( elte ) and the los alamos national laboratory .",
    "we provide here a short description of the svm .",
    "more detailed presentations of the formalism can be found elsewhere ( e.g. * ? ? ? * ; * ? ? ?",
    "* ; * ? ? ?",
    "for the sake of brevity , we present here only the equations relevant to regression with svm . equations for classification are similar , except a few differences that we mention whenever necessary .",
    "the training data usually consists of a number objects with input parameters , @xmath125 , of any dimension , and the known values of the quantity of interest , @xmath126 .",
    "the goal of svm is to find a function @xmath127 such as @xmath128 which yields @xmath126 with a maximal error @xmath129 , and such as @xmath127 is as flat as possible . in other words ,",
    "the amplitude of the slope @xmath130 has to be minimal .",
    "one way to achieve this is to minimize the norm @xmath131 with the condition @xmath132 .",
    "the margin in that case is @xmath133 . in other words ,",
    "svm attempt to regress with the largest possible margin .    in a number of problems",
    ", the data can not be separated using a fix , hard margin .",
    "it is then useful to allow some points to be misclassified .",
    "one uses a \" soft margin  , which enables to allow some errors in the results .",
    "the modified minimization reads :          eq .",
    "[ eq_sv ] shows that the solution of the minimization problem is a linear combination of a number of input data points . in other words ,",
    "the solution is based on a number of support vectors , the number of training samples where @xmath141 .",
    "the above equations use the actual values of the data , assuming that the separation can be performed linearly . for most high dimension problems ,",
    "this assumption is not valid any more .",
    "the fact that only a scalar product between the support vectors and the input data is required enables to use the so - called \" kernel trick  .",
    "the idea behind the trick is that one can use functions which satisfy a number of conditions to map the input space to another where the separation can be performed linearly .",
    ". [ eq_sv ] then becomes :        finally , a slightly modified version of the algorithm ( @xmath57svr ) allows to determine @xmath129 and control the number of support vector .",
    "a parameter @xmath144 $ ] is introduced such that eq .",
    "[ eq_svm_min ] becomes        in the regression case described here , the free parameters for @xmath57svr are the trade off parameter @xmath60 and all the kernel parameters ( assuming that one fixes @xmath57 , which allows control of the error and the fraction of support vectors ) . in the classification case ( @xmath57svc ) ,",
    "the free parameters are only those from the kernel which is used , as @xmath57 replaces the trade off parameter @xmath60 , and again , one would usually fix @xmath57 ."
  ],
  "abstract_text": [
    "<S> we apply a combination of a genetic algorithms ( ga ) and support vector machines ( svm ) machine learning algorithm to solve two important problems faced by the astronomical community : star / galaxy separation , and photometric redshift estimation of galaxies in survey catalogs . </S>",
    "<S> we use the ga to select the relevant features in the first step , followed by optimization of svm parameters in the second step to obtain an optimal set of parameters to classify or regress , in process of which we avoid over - fitting . </S>",
    "<S> we apply our method to star / galaxy separation in pan - starrs1 data . </S>",
    "<S> we show that our method correctly classifies 98% of objects down to @xmath0 , with a completeness ( or true positive rate ) of 99% for galaxies , and 88% for stars . by combining colors with morphology , our star / classification method yields better results than the new sextractor classifier ` spread_model ` in particular at the faint end ( @xmath1 ) . </S>",
    "<S> we also use our method to derive photometric redshifts for galaxies in the cosmos bright multi - wavelength dataset down to an error in @xmath2 of @xmath3 , which compares well with estimates from sed fitting on the same data ( @xmath4 ) while making a significantly smaller number of assumptions . </S>"
  ]
}