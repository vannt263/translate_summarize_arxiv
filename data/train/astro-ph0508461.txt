{
  "article_text": [
    "the higher - level inference problem of allowing the data to decide the set of parameters to be used in fitting is known as _ model selection_. in the bayesian framework , the key model selection statistic is the _",
    "bayesian evidence _",
    "( jeffreys 1961 ; mackay 2003 ) , being the average likelihood of a model over its prior parameter space . the evidence can be used to assign probabilities to models and to robustly establish whether the data require additional parameters .",
    "while the use of bayesian methods is common practice in cosmological parameter estimation , its natural extension to model selection has lagged behind .",
    "work in this area has been hampered by difficulties in calculating the evidence to high enough accuracy to distinguish between the models of interest .",
    "the issue of model selection has been raised in some recent papers ( marshall , hobson & slosar 2003 ; niarchou , jaffe & pogosian 2004 ; saini , weller & bridle 2004 ; bassett , corasaniti & kunz 2004 ) , and information criterion based approximate methods introduced in liddle ( 2004 ) .",
    "semi - analytic approximations such as the laplace approximation , which works well only for gaussian likelihoods , and the savage",
    " dickey density ratio which works for more general likelihood functions but requires the models being compared to be nested , are methods that were recently exploited by trotta ( 2005 ) .",
    "a more general and accurate numerical method is thermodynamic integration , but beltrn et al .",
    "( 2005 ) found that in realistic applications around @xmath0 likelihood evaluations were needed per model to obtain good accuracy , making it a supercomputer - class problem in cosmology where likelihood evaluations are computationally costly .",
    "in this paper we present a new algorithm which , for the first time , combines accuracy , general applicability , and computational feasibility .",
    "it is based on the method of _ nested sampling _ , proposed by skilling ( 2004 ) , in which the multi - dimensional integral of the likelihood of the data over parameter space is performed using monte carlo sampling , working through the prior volume to the regions of high likelihood .",
    "using bayes theorem , the probability that a model ( hypothesis : @xmath1 ) is true in light of observed data ( @xmath2 ) is given by @xmath3 it shows how our prior knowledge @xmath4 is modified in the presence of data .",
    "the posterior probability of the parameters ( @xmath5 ) of a model in light of data is given by @xmath6 where @xmath7 is the likelihood of the data given the model and its parameters , and @xmath8 is the prior on the parameters .",
    "this is the relevant quantity for parameter estimation within a model , for which the denominator @xmath9 is of no consequence .",
    "@xmath9 is however the _ evidence _ for the model @xmath1 , the key quantity of interest for the purpose of model selection ( jeffreys 1961 ; mackay 2003 ; gregory 2005 ) .",
    "normalizing the posterior @xmath10 marginalized over @xmath5 to unity , it is given by @xmath11 the prior also being normalized to unity .",
    "the evidence for a given model is thus the normalizing constant that sets the area under the posterior @xmath10 to unity . in the now - standard markov chain monte carlo method for tracing the posterior ( gilks et al .",
    "1996 ; lewis & bridle 2002 ) , the posterior is reflected in the binned number density of accumulated samples .",
    "the evidence found in this way would not generally be accurate as the algorithm would sample the peaks of the probability distribution well , but would under - sample the tails which might occupy a large volume of the prior .",
    "when comparing two different models using bayes theorem , the ratio of posterior probabilities of the two models would be the ratio of their evidences ( called the bayes factor ) multiplied by the ratio of any prior probabilities that we may wish to assign to these models ( eq .  1 ) .",
    "it can be seen from eq .",
    "( [ evdef ] ) that while more complex models will generally result in better fits to the data , the evidence , being proportional to the volume occupied by the posterior relative to that occupied by the prior , automatically implements occam s razor .",
    "it favours simpler models with greater predictive power provided they give a good fit the data , quantifying the tension between model simplicity and the ability to fit to the data in the bayesian sense .",
    "jeffreys ( 1961 ) provides a useful guide to what constitutes a significant difference between two models : @xmath12 is substantial , @xmath13 is strong , and @xmath14 is decisive . for reference , a @xmath15 of @xmath16 corresponds to odds of about 1 in 13 .",
    "while for parameter fitting the priors become irrelevant once the data are good enough , for model selection some dependence on the prior ranges always remains however good the data .",
    "the dependence on prior parameter ranges is a part of bayesian reasoning , and priors should be chosen to reflect our state of knowledge about the parameters before the data came along .",
    "the bayesian evidence is unbiased , as opposed to approximations such as the information criteria .",
    "perhaps the most important application of model selection is in assessing the need for a new parameter , describing some new physical effect proposed to influence the data .",
    "frequentist - style approaches are commonplace , where one accepts the parameter on the basis of a better fit , corresponding to an improvement in @xmath17 by some chosen threshold ( leading to phrases such as ` two - sigma detection ' ) .",
    "such approaches are non - bayesian : the evidence shows that the size of the threshold depends both on the properties of the dataset and on the priors , and in fact the more powerful the dataset the higher the threshold that must be set ( trotta 2005 ) .",
    "further , as the bayesian evidence provides a rank - ordered list of models , the need to choose a threshold is avoided ( though one must still decide how large a difference in evidence is needed for a robust conclusion ) .",
    "the main purpose of this paper is to present an algorithm for evidence computation with widespread applications .",
    "however as a specific application we examine the need for extra parameters against the simplest viable cosmological model , a @xmath18cdm model with harrison  zeldovich initial spectrum , whose five parameters are the baryon density @xmath19 , cold dark matter density @xmath20 , the hubble parameter @xmath21 ( or the ratio @xmath22 of the approximate sound horizon at decoupling to its angular diameter distance ) , the optical depth @xmath23 , and the amplitude @xmath24 of primordial perturbations .",
    "we study the case for two additional parameters , the scalar spectral index and the dark energy equation of state ( assumed constant ) .",
    "nested sampling ( skilling 2004 ) is a scheme to trace the variation of the likelihood function with prior mass , with the effects of topology , dimensionality and everything else implicitly built into it .",
    "it breaks up the prior volume into a large number of ` equal mass ' points and orders them by likelihood . rewriting eq .",
    "( [ evdef ] ) in the notation of skilling ( 2004 ) , with @xmath25 as the fraction of total prior mass such that @xmath26 and the likelihood @xmath27 , the equation for the evidence becomes @xmath28 thus the problem of calculating the evidence has become a one - dimensional integral , in which the integrand is positive and decreasing .",
    "suppose we can evaluate the likelihood as @xmath29 , where the @xmath30 are a sequence of decreasing values , such that @xmath31 as shown schematically in figure  [ nested ] .",
    "then the evidence can be estimated by any numerical method , for example the trapezoid rule @xmath32    the nested sampling algorithm achieves the above summation in the following way :    1 .",
    "sample @xmath33 points randomly from within the prior , and evaluate their likelihoods .",
    "initially we will have the full prior range available , i.e. @xmath34 .",
    "2 .   select the point with the lowest likelihood @xmath35 .",
    "the prior volume corresponding to this point , @xmath30 , can be estimated probabilistically .",
    "the average volume decrease is given as @xmath36 where @xmath37 is the expectation value of the largest of @xmath33 random numbers from uniform(0,1 ) , which is @xmath38 .",
    "3 .   increment the evidence by @xmath39 .",
    "4 .   discard the lowest likelihood point and replace it with a new point , which is uniformly distributed within the remaining prior volume @xmath40 .",
    "the new point must satisfy the hard constraint on likelihood of @xmath41 .",
    "5 .   repeat steps 24 , until the evidence has been estimated to some desired accuracy .",
    "the method thus works its way up the likelihood surface , through nested surfaces of equal likelihood .",
    "after @xmath42 steps the prior mass remaining shrinks to @xmath43 .",
    "the process is terminated when some stopping criterion is satisfied , and a final amount of @xmath44 due to the @xmath45 remaining sample points is added to the thus far accumulated evidence .    thus a multi - dimensional integral is performed using monte carlo sampling , imposing a hard constraint in likelihood on samples that are uniformly distributed in the prior , implying a regularized probabilistic progression through the prior volume . besides implementing and testing this scheme in the cosmological context",
    ", our main contribution lies in developing a general strategy to sample new points efficiently .      the prior space to sample from reduces by a constant factor of n/(n+1 ) every time the lowest likelihood point is discarded . the most challenging task in implementing the algorithm is to sample uniformly from the remaining prior volume , without creating too large an overhead in likelihood evaluations even when the remaining volume of prior space may be very small .",
    "the new point must be uncorrelated to the existing points , but we can still use the set of existing points as a guide .",
    "we find the covariance of the live points , rotate our coordinates to the principal axes , and create an ellipsoid which just touches the maximum coordinate values of the existing points .",
    "to allow for the iso - likelihood contours not being exactly elliptical , these limits are expanded by a constant enlargement factor , aiming to include the full volume with likelihood exceeding the current limit ( if this is not done new points will be biased towards the centre , thus overestimating the evidence ) .",
    "new points are then selected uniformly within the expanded ellipse until one has a likelihood exceeding the old minimum , which then replaces the discarded lowest - likelihood point .",
    "[ cols=\"^,^,^,^,^,^ \" , ]     the two algorithm parameters to be chosen are the number of points @xmath33 and the enlargement factor of the ellipsoid .",
    "figure 2 shows evidence verses @xmath33 , for a flat harrison  zeldovich model with a cosmological constant .",
    "the mean evidence values and standard deviations obtained from 4 repetitions are shown .",
    "these are shown for two different values of the enlargement factor . when the enlargement factor is large enough ( here 1.5 ) the evidence obtained with 100 sample points agrees with that obtained with 500 , while when the enlargement factor is not large enough , the evidences obtained with small n are systematically biased high .",
    "similar tests done on multi - dimensional gaussian likelihood functions , for which the expected evidence can be found analytically , indicated the same . based on such tests we choose to work with @xmath33 of 300 , and enlargement factors of 1.5 for the 5d model ( this corresponds to a 50% increase in the range of each parameter ) , 1.7 for 6d and 1.8 for 7d models .",
    "these choices are conservative ( smaller values would reduce the computing time ) , and were made in order to ensure evidences that are accurate and free of systematics . in our cosmological applications",
    "we have also computed evidences with larger enlargement factors and found that they remained unchanged .",
    "the typical acceptance rate in finding a new point during the course of the algorithm was found to be roughly constant at @xmath46 for an enlargement factor of 1.5 , and lower for larger enlargement factors , after an initial period of almost @xmath47 acceptance , as expected .",
    "figure 3a shows the accumulated evidence , the evidence contributed by the remaining points at each stage , and their sum , against prior volume remaining , again for a flat harrison  zeldovich model with a cosmological constant .",
    "the @xmath25 at which the calculation can be terminated will depend on the details of the problem ( e.g.  dimensionality , priors etc . ) .",
    "we define a parameter @xmath48 as the maximum possible fractional amount that the remaining points could increase the evidence by : @xmath49 where @xmath50 is the maximum likelihood of the current set of sample points .",
    "figure 3b zooms into a late part of the solid curve , now plotting it against the value of the parameter @xmath48 .",
    "the calculation need only be carried out until the standard error on the mean evidence , computed for a certain number of repetitions , drops below the desired accuracy . an uncertainty in @xmath51 of 0.1",
    "would be the highest conceivable accuracy one might wish , and with 8 repetitions this happens when @xmath52 a few .",
    "this takes us to quite small @xmath25 , of order @xmath53 in our actual cosmological simulations .",
    "we have calculated the evidences of four different cosmological models : 1 ) a flat , harrison  zeldovich model with a cosmological constant ( @xmath18cdm+hz ) , 2 ) the same as 1 , except allowing the tilt of the primordial perturbation spectrum @xmath54 to vary ( @xmath18cdm+@xmath54 ) , 3 ) the same as 1 , except allowing the equation of state of the dark energy to take alternative values to @xmath55 ( @xmath56+hz ) , and finally 4 ) allowing both @xmath54 and @xmath56 to vary ( @xmath57 ) .",
    "the prior ranges for the other parameters were fixed at @xmath58 , @xmath59 , @xmath60 , @xmath61 , and @xmath62 .",
    "the data sets we use are cmb tt and te anisotropy power spectrum data from the wmap experiment ( bennett et al .",
    "2003 ; spergel et al .  2003 ; kogut et al .",
    "2003 ) , together with higher @xmath63 cmb temperature power spectrum data from vsa ( dickinson et al .",
    "2004 ) , cbi ( pearson et al .",
    "2003 ) and acbar ( kuo et al .",
    "2004 ) , matter power spectrum data from sdss ( tegmark et al .",
    "2004 ) and 2dfgrs ( percival et al .",
    "2001 ) , and supernovae apparent magnitude  redshift data from riess et al .",
    "( 2004 ) .",
    "results are shown in table 1 .",
    "for the spectral tilt , evidences have been found for two different prior ranges , as an additional test of the method . for a prior range twice the size of the original in @xmath64 the evidence",
    "is expected to change by @xmath65 at most and that difference is recovered .",
    "the first 2 rows show the priors on the additional parameters of the model , or their constant values if they were fixed .",
    "the 3rd row shows the enlargement factor ( e.f ) we used for the model .",
    "the 4th row shows the total number of likelihood evaluations needed to compute the mean @xmath66 evidence to an accuracy @xmath67 , and the 5th row shows the mean @xmath51 and the standard error in that mean computed from 8 repetitions of the calculation , normalized to the @xmath18cdm+hz evidence .",
    "the @xmath18cdm+hz model has the highest evidence , and as such is the preferred fit to the data .",
    "hence we do not find any indication of a need to introduce parameters beyond the base set of 5 , in agreement with the conclusions of liddle ( 2004 ) and trotta ( 2005 ) .",
    "however , the difference between the @xmath51 of the higher - dimensional models and the base model is not large enough to significantly exclude any of those models at present .",
    "we introduce the nested sampling algorithm for the computation of bayesian evidences for cosmological model selection .",
    "we find that this new algorithm uniquely combines accuracy , general applicability and computational feasibility .",
    "it is able to attain an accuracy ( standard error in the mean @xmath66 evidence ) of 0.1 in o(@xmath68 ) likelihood evaluations .",
    "it is therefore much more efficient than thermodynamic integration , which is the only other method that shares the general applicability of nested sampling .",
    "nested sampling also leads to a good estimate of the posterior probability density of the parameters of the model for free , which we will discuss in a forthcoming paper .",
    "we also plan to make a public release of the code in the near future .    using nested sampling",
    "we have computed the evidence for the simplest cosmological model , with a base set of 5 parameters , which provides a good fit to current cosmological data .",
    "we have computed the evidence of models with additional parameters  the scalar spectral tilt , a constant dark energy equation of state parameter , and both of these together .",
    "we find that current data offer no indication of a need to add extra parameters to the base model , which has the highest evidence amongst the models considered .",
    "the authors were supported by pparc .",
    "we thank martin kunz , sam leach , peter thomas and especially john skilling for helpful advice and comments .",
    "the analysis was performed on the uk national cosmology supercomputer ( cosmos ) in cambridge .",
    "beltran , m. , garcia - bellido , j. , lesgourgues , j. , liddle , a. r. , & slosar , a. 2005 , phys .",
    "d , 71 , 063532 bennett , c. l. , et al . 2003 , apjs , 148 , 1 [ wmap collaboration ] dickinson , c. et al .",
    "2004 , mnras , 353 , 732 [ vsa collaboration ] gilks w. r. , richardson s. , & spiegelhalter d. j. ( eds . ) 1996 , _ markov chain monte carlo in practice _ , chapman & hall gregory , p. 2005 , _ bayesian logical data analysis for the physical sciences _ ,",
    "cambridge university press jeffreys , h. 1961 , _ theory of probability _ , oxford university press kogut , a. , et al .",
    "2003 , apjs , 148 , 161 [ wmap collaboration ] kuo , c. j. , et al .",
    "2004 , apj , 600 , 32 [ acbar collaboration ] lewis , a. , & bridle , s. , 2002 , phys .",
    "d , 66 , 103511 liddle , a. r. 2004 , mnras , 351 , l49 mackay , d. j. c. 2003 , _ information theory , inference and learning algorithms _ , cambridge university press marshall , p. j. , hobson , m. p. , & slosar a. 2003 , mnras , 346 , 489 niarchou , n. , jaffe , a. h. , & pogosian , l. 2004 , phys .",
    "d , 69 , 063515 pearson , t. j. , et al .",
    "2003 , apj , 591 , 556 [ cbi collaboration ] percival , w. j. , et al . 2001 ,",
    "mnras , 327 , 1297 ; 2002 , 337 , 1068 [ 2dfgrs collaboration ] riess , a. g. , et al . 2004 , apj , 607 , 665 [ supernova search team collaboration ] saini , t. d. , weller , j. , & bridle , s. l. , 2004 , mnras , 348 , 603 skilling , j. 2004 , ` nested sampling for general bayesian computation ' , unpublished , available from : http://www.inference.phy.cam.ac.uk / bayesys/. spergel , d. n. , et al .",
    "2003 , apjs , 148 , 175 [ wmap collaboration ] tegmark , m. , et al .",
    "2004 , apj , 606 , 702 [ sdss collaboration ] trotta , r. 2005 , astro - ph/0504022 ."
  ],
  "abstract_text": [
    "<S> the abundance of new cosmological data becoming available means that a wider range of cosmological models are testable than ever before . </S>",
    "<S> however , an important distinction must be made between _ parameter fitting _ and _ model selection_. while parameter fitting simply determines how well a model fits the data , model selection statistics , such as the bayesian evidence , are now necessary to choose between these different models , and in particular to assess the need for new parameters . </S>",
    "<S> we implement a new evidence algorithm known as _ nested sampling _ , which combines accuracy , generality of application and computational feasibility , and apply it to some cosmological datasets and models . </S>",
    "<S> we find that a five - parameter model with harrison  zeldovich initial spectrum is currently preferred . </S>"
  ]
}