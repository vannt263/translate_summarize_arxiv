{
  "article_text": [
    "we consider the following sequence space model @xmath4 where @xmath5 are the coefficients of a signal and the noise @xmath6 has a diagonal covariance matrix @xmath7 .",
    "this heterogeneous model may appear in several frameworks where the variance is fluctuating , for example in heterogeneous regression , coloured noise , fractional brownian motion models or statistical inverse problems , for which the general literature is quite exhaustive @xcite .",
    "the goal is to estimate the unknown parameter @xmath8 by using the observations @xmath9 .",
    "model selection is a core problem in statistics .",
    "one of the main reference in the field dates back to the aic criterion @xcite , but there has been a huge amount of papers on this subject ( e.g. , @xcite ) .",
    "model selection is usually linked to the choice of a penalty and its precise choice is the main difficulty in model selection both from a theoretical and a practical perspective .",
    "there is a close relationship between model selection and thresholding procedures , which is addressed e.g. in @xcite .",
    "the idea is that the search for a `` good penalty '' in model selection is indeed very much related to the choice of a `` good threshold '' in wavelet procedures .",
    "there exists also a fascinating connection between the false discovery rate control ( fdr ) and both thresholding and model selection , as studied in @xcite , which will become apparent later in our paper .",
    "our main modeling assumption is that the parameter @xmath3 of interest is sparse .",
    "sparsity is one of the leading paradigms nowadays and signals with a sparse representation in some basis ( for example wavelets ) or functions with sparse coefficients appear in many scientific fields ( see @xcite among many others ) .    in this paper",
    ", we consider the sequence space model with heterogeneous errors .",
    "our goal is then to select among a family of models the best possible one , by use of a data - driven selection rule . in particular",
    ", one has to deal with the special heterogeneous nature of the observations , and the choice of the penalty must reflect this .",
    "the heterogenous case is much more involved than the direct ( homogeneous ) model .",
    "indeed , there is no more symmetry inside the stochastic process that one needs to control , since each empirical coefficient has its own variance . the problem and",
    "the penalty do not only depend on the number of coefficients that one selects , but also on their position .",
    "this also appears in the minimax bounds where the coefficients in the least favourable model will go to the larger variances . by a careful and explicit choice of the penalty , however , we are able to select the correct coefficients and get a sharp non - asymptotic control of the risk of our procedure .",
    "results are also obtained for full model selection and a fdr - type control on a family of thresholds . in the case of known sparsity @xmath10",
    ", we consider a non - adaptive threshold estimator and obtain a minimax upper bound .",
    "this estimator exactly attains the lower bound and is then minimax .",
    "using our model selection approach , the procedure is almost minimax ( up to a factor 2 ) .",
    "moreover , the procedure is fully adaptive .",
    "indeed , the sparsity @xmath10 is unknown and we obtain an explicit penalty , valid in the mathematical proofs and directly applicable in simulations .",
    "the paper is organized as follows . in the following subsection [ sec : exa ] ,",
    "we give examples of problems where our heterogeneous model appears .",
    "section [ sec : sel ] contains the data - driven procedure and a general result . in section [ sec : spa ] , we consider the sparsity assumptions and obtain theorems for the full subset selection and thresholding procedures .",
    "section [ sec : low ] and [ sec : upp ] are concerned with minimax lower and upper bounds . in section [ sec",
    ": num ] , we present numerical results for the finite - sample properties of the methods .",
    "consider first a model of heterogeneous regression @xmath11 where @xmath12 are i.i.d .",
    "standard gaussian , but their variance are fluctuating depending on the design points @xmath13 and @xmath14 is some spiky unknown function . in this model @xmath15 . by spiky function",
    "we mean that @xmath16 is zero apart from a small subset of all design points @xmath13 .",
    "these signals are frequently encountered in applications ( though rarely modeled in theoretical statistics ) , e.g. when measuring absorption spectra in physical chemistry ( i.e. rare well - localised and strong signals ) or jumps in log returns of asset prices ( i.e. log - price increments which fluctuate at low levels except when larger shocks occur ) .",
    "often in applications coloured noise models are adequate .",
    "let us consider here the problem of estimating an unknown function observed with a noise defined by some fractional brownian motion , @xmath17,\\ ] ] where @xmath14 is an unknown @xmath18periodic function in @xmath19 , @xmath20=0 , @xmath21 is the noise level and @xmath22 is a fractional brownian motion , defined by ( see @xcite ) , @xmath23 where @xmath24 is a brownian motion , @xmath25 , @xmath26 is the gamma function . the fractional brownian motion also appears in econometric applications to model the long - memory phenomena , e.g. in @xcite .",
    "the model ( [ mod ] ) is close to the standard gaussian white noise model , which corresponds to the case @xmath27 . here",
    ", the behaviour of the noise is different .",
    "we are not interested in the fractional brownian motion itself , but we want to estimate the unknown function @xmath14 based on the noisy data @xmath28 , as in @xcite .",
    "a very important point is linked with the definition of the fractional integration operator . in this framework , if the function @xmath14 is supposed to be @xmath18periodic , then the natural way is to consider the periodic version of fractional integration ( given in ( [ frac ] ) ) , such that @xmath29 and thus ( see p.135 in @xcite ) , @xmath30    by integration and projection on the cosine ( or sine ) basis and using ( [ eigen ] ) , one obtains the sequence space model ( as in @xcite ) , @xmath31 where @xmath32 are independent with @xmath33 , where @xmath34 and @xmath35 .",
    "consider the following framework of a general inverse problem @xmath36 where @xmath37 is a known injective compact linear bounded operator , @xmath14 an unknown @xmath38-dimensional function , @xmath39 is a gaussian white noise and @xmath40 the noise level .",
    "we will use here the framework of singular values decomposition ( svd ) , see e.g. @xcite .",
    "denote by @xmath41 the eigenfunctions of the operator @xmath42 associated with the strictly positive eigenvalues @xmath43 . remark that any function @xmath14 may be decomposed in this orthonormal basis as @xmath44 , where @xmath45 .",
    "let @xmath46 be the normalized image basis @xmath47 by projection and division by the singular values , we may obtain the empirical coefficients @xmath48 we then obtain a model in the sequence space ( see @xcite ) @xmath49 with @xmath33 and @xmath50 .",
    "we consider the sequence space model for coefficients of an unknown @xmath51-function @xmath14 with respect to an orthornormal system @xmath52 .",
    "the estimator over an arbitrary large , but finite index set @xmath53 is then defined by @xmath54 where @xmath55 the empirical version of @xmath14 is defined as @xmath56 we write @xmath57 and @xmath58 for the cardinality of @xmath53 .",
    "let us write @xmath59 for the covariance matrix of the @xmath60 restricted to the indices @xmath61 for which @xmath62 , i.e. @xmath63 with @xmath64 .",
    "by @xmath65 we denote the operator norm , i.e. the largest absolute eigenvalue .    the random elements @xmath66 take values in the sample space @xmath67 .",
    "we now consider an arbitrary family @xmath68 of borel - measurable data - driven subset selection rules .",
    "define an estimator by minimizing in the family @xmath69 the penalized empirical risk : @xmath70 with the penalty @xmath71 where @xmath72 denotes the @xmath73-th largest value among @xmath74 and @xmath75 .",
    "remark that @xmath76 is defined in an equivalent way by @xmath77 where @xmath78 then , define the data - driven estimator @xmath79 the next lemma shows that one has an explicit risk hull , a concept introduced in full detail in @xcite .",
    "[ th : hull ] the function @xmath80 with the penalty from is a risk hull , i.e. we have @xmath81    recall @xmath58 and introduce the stochastic term @xmath82 remark that @xmath83 such that @xmath84 follows from @xmath85 let us write @xmath86 and let @xmath87 denote the inverse rank of @xmath88 in @xmath89 ( e.g. , @xmath90 if @xmath91 such that @xmath92 note that for any enumeration @xmath93 of @xmath94 by monotonicity : @xmath95 holds .",
    "we therefore obtain with the inverse order statistics @xmath96 and @xmath97 ( i.e. @xmath98 etc . ) of @xmath99 and @xmath100 , respectively , @xmath101 { \\leqslant}\\operatorname{{\\mathbf e}}\\big[\\sum_{j=1}^n\\sigma_{(j)}^2\\big(\\zeta_{(j)}^2 - 2(\\log(ne / j)+j^{-1}\\log_+(n{\\lvert \\sigma \\rvert}))\\big)_+ \\big].\\ ] ] it remains to evaluate @xmath102 $ ] .",
    "we obtain by independence , @xmath103 and by the mill ratio inequality @xmath104 @xmath105 this implies for any @xmath106 @xmath107=\\int_p^\\infty p(\\zeta_{(j)}^2>\\kappa)\\,d\\kappa{\\leqslant}2j^{-1}p^{-j/2}\\exp(j\\log(ne / j)-jp/2).\\ ] ] we conclude @xmath108\\ ] ] @xmath109 @xmath110 where @xmath111 and the supremum is attained at @xmath112 with value @xmath113 .",
    "[ th : oracle ] let @xmath76 be the data - driven rule defined in ( [ hstar ] ) . for any @xmath114",
    ", we have @xmath115+\\omega_\\delta,\\ ] ] where @xmath116    in view of lemma [ th : hull ] , @xmath117 is a risk hull , and therefore we have @xmath118 on the other hand , since @xmath76 minimizes @xmath119 we have @xmath120.\\ ] ] in order to combine the inequalities ( [ ep1 ] ) and ( [ ep2 ] ) , we rewrite @xmath121 in terms of @xmath122 @xmath123 therefore , using this equation and ( [ ep1 ] , [ ep2 ] ) , we obtain @xmath124+{\\lvert f \\rvert}^2 + \\sqrt{2}\\min\\big(\\tfrac1n,{\\lvert \\sigma \\rvert}\\big ) + 2\\mathbf{e}_f \\ , \\sum_{\\lambda \\in \\lambda } h_\\lambda^\\star f_\\lambda \\xi_\\lambda \\\\ & + \\mathbf{e}_f\\,\\biggl[\\sum_{\\lambda \\in \\lambda } h_\\lambda^\\star \\xi_\\lambda^2-pen(h^\\star)\\biggr ] .",
    "\\end{split}\\ ] ] remark now that for any deterministic index set @xmath125 @xmath126 this implies for @xmath127 @xmath128 then , by the general inequality @xmath129 for @xmath130 we obtain @xmath131 note that @xmath132 since @xmath133 . by ( [ cauch ] ) and ( [ vari_1 ] ) we obtain @xmath134 in a similar way , we obtain @xmath135 note that @xmath136 since @xmath137 . using ( [ cauch2 ] ) and ( [ vari_2 ] ) one has @xmath138 note also that , since @xmath139 , we have @xmath140 insertion of ( [ var - theta ] ) and ( [ var - theta2 ] ) into yields @xmath141 by using the risk hull as in lemma [ th : hull ] , one obtains @xmath142 { \\leqslant}\\sqrt{2}\\min\\big(\\tfrac1n,{\\lvert \\sigma \\rvert}\\big).\\ ] ] inserting ( [ var - theta ] ) , ( [ var - theta2 ] ) and ( [ hull2 ] ) into ( [ ep3 ] ) yields @xmath143+{\\lvert f \\rvert}^2 + \\sqrt{2}\\min\\big(\\tfrac1n,{\\lvert \\sigma \\rvert}\\big)+ \\tfrac{2}{\\delta}\\sum_{\\lambda \\in \\lambda}\\min(f_\\lambda^2,\\sigma^2_{\\lambda})\\\\   & \\quad + \\sqrt{2}\\min\\big(\\tfrac1n,{\\lvert \\sigma \\rvert}\\big)+ \\frac{\\delta}{2 } \\mathbf{e}_f\\|\\hat { f}(h^\\star ) -f\\|^2.\\end{aligned}\\ ] ] using ( [ fin1 ] ) we obtain , @xmath144 + 2\\sqrt{2}\\min\\big(\\tfrac1n,{\\lvert \\sigma \\rvert}\\big)+\\frac{2}{\\delta}\\sum_{\\lambda \\in \\lambda}\\min(f_\\lambda^2,\\sigma^2_{\\lambda}).\\ ] ] finally , we let the bias explicitly appear in @xmath145 and the result follows from @xmath146 for @xmath147 $ ] .",
    "let us consider the intuitive version of sparsity by assuming a small proportion of nonzero coefficients ( cf .",
    "@xcite ) , i.e. the family @xmath148 where @xmath149 denotes the maximal proportion of nonzero coefficients .    throughout",
    ", we assume that this proportion @xmath10 is such that asymptotically @xmath150      the goal here is to study the accuracy of the full model selection over the whole family of estimators .",
    "each coefficient may be chosen to be inside or outside the model .",
    "let us consider the case where @xmath151 denotes all deterministic subset selections , @xmath152    [ th : ms ] let @xmath76 be the data - driven rule defined in ( [ hstar ] ) with @xmath151 as in ( [ hms ] ) .",
    "we have , for @xmath153 , uniformly over @xmath154 , @xmath155    in particular , if @xmath156 ( i.e. , any polynomial growth for @xmath157 is admissible ) and @xmath158 , then we obtain @xmath159    for @xmath154 the right - hand side in theorem [ th : oracle ] can be bounded by considering the oracle @xmath160 such that @xmath161+\\omega_\\delta & { \\leqslant}(1+\\delta)2pen(h^f)+\\omega_\\delta.\\end{aligned}\\ ] ] we will use the following inequality , as @xmath162 , @xmath163 by comparison with the integral . since @xmath164 , we obtain that @xmath165 @xmath166 as @xmath153 . on the other hand , we have @xmath167 we use @xmath168 which shows @xmath169 choosing @xmath170 such that @xmath171 , e.g. @xmath172 , we thus find , as @xmath173 , @xmath174 using theorem [ th : oracle ] , equation ( [ omega_del ] ) we have ( [ bound1 ] ) .",
    "moreover , using the bounds on @xmath175 and @xmath157 we obtain ( [ bound1b ] ) .",
    "consider now a family of threshold estimators .",
    "the problem is to study the data - driven selection of the threshold .",
    "let us consider the case where @xmath151 denotes the threshold selection rules with arbitrary threshold values @xmath176 @xmath177 note that @xmath69 consists of @xmath58 different subset selection rules only and can be implemented efficiently using the order statistics of @xmath178 .",
    "[ th : tr ] let @xmath76 be the data - driven rules defined in ( [ hstar ] ) with @xmath151 as in ( [ htr ] ) .",
    "if @xmath179 , then we have , for @xmath153 , uniformly over @xmath154 @xmath180 assuming for @xmath181 the growth bounds @xmath182 with a second condition always checked if @xmath183 , this inequality simplifies to @xmath184    let us now evaluate the right - hand side of the oracle inequality in theorem [ th : oracle ] for the threshold selection rules with arbitrary threshold values @xmath176 defined in ( [ htr ] ) .",
    "given an oracle parameter @xmath185 ( to be determined below ) , we set @xmath186 .",
    "we obtain with @xmath187 denoting the ( inverse ) rank of the coefficient with index @xmath188 among @xmath189 @xmath190\\\\ & { \\leqslant}\\mathbf{e}_f\\big [ \\sum_{\\lambda\\in\\lambda}\\big({\\bf 1}({\\lvert x_\\lambda \\rvert}{\\leqslant}\\tau_\\lambda)f_\\lambda^2- { \\bf 1}({\\lvert x_\\lambda \\rvert}>\\tau_\\lambda ) ( x_\\lambda^2-f_\\lambda^2)\\\\ & \\qquad\\qquad + 4\\sigma_\\lambda^2 { \\bf 1}({\\lvert x_\\lambda \\rvert}>\\tau_\\lambda)(\\log(en / r_\\lambda ) + r_\\lambda^{-1}\\log_+(n{\\lvert \\sigma \\rvert}))\\big)\\big].\\end{aligned}\\ ] ]    let us first show that @xmath191 $ ] is always non - negative . by symmetry",
    "@xmath192 has the same law as @xmath193 . defining the function @xmath194 , we check by considering the different cases that @xmath195 holds .",
    "we conclude @xmath196 & = \\tfrac12 \\operatorname{{\\mathbf e}}_f[g(\\xi_\\lambda)+g(-\\xi_\\lambda)]{\\geqslant}0.\\end{aligned}\\ ] ] hence , the term with a minus sign in can be discarded for an upper bound .",
    "let us now consider the coefficients that contain a signal part ( i.e. with @xmath197 )",
    ". the following inequality will be helpful to obtain a bound independent of the size of @xmath198 .",
    "let us denote by @xmath199 the corresponding inverse rank within @xmath200 . with @xmath201 on the event @xmath202 we obtain @xmath203 where for the last inequality we have used that for @xmath204 distinct values @xmath205 the expression is maximal in the case @xmath206 .",
    "the general identity @xmath207=c+\\int_c^\\infty p(z{\\geqslant}z)dz$ ] applied to @xmath208 and deterministic @xmath209 yields @xmath210&{\\leqslant}c_\\lambda+\\int_{c_\\lambda}^\\infty p({\\lvert \\xi_\\lambda \\rvert}{\\geqslant}\\sqrt{z}-\\tau_\\lambda)\\,dz { \\leqslant}c_\\lambda+2e^{-(\\sqrt{c_\\lambda}-\\tau_\\lambda)^2/(2\\sigma_\\lambda^2)}.\\end{aligned}\\ ] ]    in order to ensure @xmath211 whenever @xmath197 , we are lead to choose @xmath212 in the sequel we bound @xmath213 simply by @xmath214 in the case @xmath197 . then using again the bound on sums of logarithms ( [ sumlog ] ) and @xmath215 as well as the concavity of @xmath216 for bounding the sum of exponentials",
    ", we obtain that over the signal part satisfies @xmath217\\\\ & { \\leqslant}\\sum_{\\lambda\\in\\lambda , f_\\lambda\\not=0}(c_\\lambda+2e^{-(\\sqrt{c_\\lambda}-\\tau_\\lambda)^2/(2\\sigma_\\lambda^2 ) } ) { \\leqslant}n\\gamma_n(c_n{\\lvert \\sigma_{h_f } \\rvert}+2 e^{-(c_n-(t^0)^2)/2}),\\,\\end{aligned}\\ ] ] where @xmath218    owing to @xmath219 we even have @xmath217 \\nonumber\\\\ & { \\leqslant}{\\lvert \\sigma_{h_f } \\rvert}n\\gamma_n c_n(1+o(1)).\\label{sig2}\\end{aligned}\\ ] ]    on the other hand , for the non - signal part @xmath220 , we introduce @xmath221 and we use the large deviation bound : @xmath222=n p({\\lvert \\xi_\\lambda \\rvert}>\\tau_\\lambda){\\leqslant}2n(t^0)^{-1}e^{-(t^0)^2/2}. \\ ] ] again by considering worst case permutations instead of the ranks , using ( [ sumlog ] ) and by jensen s inequality for the concave functions @xmath223 we infer : @xmath224\\\\ & { \\leqslant}4{\\lvert \\sigma \\rvert}\\mathbf{e}_f\\left[\\sum_{\\lambda\\in\\lambda } { \\bf 1}({\\lvert \\xi_\\lambda \\rvert}>\\tau_\\lambda)(\\log(en / r_\\lambda)+r_\\lambda^{-1}\\log_+(n{\\lvert \\sigma \\rvert}))\\right]\\\\ & { \\leqslant}4{\\lvert \\sigma \\rvert}\\mathbf{e}\\left[\\sum_{j=1}^{n_\\tau}(\\log(en / j)+j^{-1}\\log_+(n{\\lvert \\sigma \\rvert}))\\right]\\\\ & { \\leqslant}4{\\lvert \\sigma \\rvert}\\mathbf{e}\\left[(n_\\tau\\log(en / n_\\tau)+\\log(n_\\tau)\\log_+(n{\\lvert \\sigma \\rvert}))\\right ] ( 1+o(1))\\\\ & { \\leqslant}4{\\lvert \\sigma \\rvert } ( 2n ( t^0)^{-1 } e^{-(t^0)^2/2}(1+t_0 ^ 2/2)+(\\log n-(t^0)^2/2)\\log_+(n{\\lvert \\sigma \\rvert}))(1+o(1))\\\\ & { \\leqslant}2{\\lvert \\sigma \\rvert}(2n e^{-(t^0)^2/2}t^0+(2\\log n-(t^0)^2)\\log_+(n{\\lvert \\sigma \\rvert}))(1+o(1)).\\end{aligned}\\ ] ] for the @xmath225 chosen , the total bound over is thus , by ( [ sig2 ] ) , ( [ eq42 ] ) and by definition of @xmath226 in ( [ c_n ] ) , @xmath227 this yields the asserted general bound and inserting the bound for @xmath228 gives directly the second bound .      _",
    "heterogeneous case .",
    "_ one may compare the method and its accuracy with other results in related frameworks .",
    "for example , @xcite considers a very close framework of model selection in inverse problems by using the svd approach .",
    "this results in a noise @xmath229 which is heterogeneous and diagonal .",
    "@xcite study the related topic of inverse problems and wavelet vaguelette decomposition ( wvd ) , built on @xcite .",
    "the framework in @xcite is more general than ours .",
    "however , this leads to less precise results . in all their results @xcite",
    ", there exist universal constants which are not really controlled .",
    "this is even more important for the constants inside the method , for example in the penalty .",
    "our method contains an explicit penalty .",
    "it is used in the mathematical results and also in simulations without additional tuning .",
    "a possible extension of our method to the dependent wvd case does not seem straight - forward .",
    "_ homogeneous case .",
    "_ let us compare with other work for the homogeneous setting @xmath230 .",
    "there exist a lot of results in this framework , see e.g. @xcite .",
    "again those results contain universal constants , not only in the mathematical results , but even inside the methods .",
    "for example , constants in front of the penalty , but also inside the fdr technique , with an hyper - parameter @xmath231 which has to be tuned .",
    "the perhaps closest paper to our work is @xcite in the homogeneous case .",
    "our penalty is analogous to `` twice the optimal '' penalty considered in @xcite .",
    "this is due to difficulties in the heterogenous case , where the stochastic process that one needs to control is much more involved in this setting .",
    "indeed , there is no more symmetry inside this stochastic process , since each empirical coefficient has its own variance . the problem and",
    "the penalty do not only depend on the number of coefficients that one selects , but also on their position .",
    "this leads to a result @xmath232 , where one gets a constant @xmath233 in @xcite .",
    "the potential loss of the factor 2 in the heterogeneous framework might possibly be avoidable in theory , but in simulations the results seem comparably less sensitive to this factor than to other modifications , e.g. to how many data points , among the @xmath204 non - zero coefficients , are close to the critical threshold level , which defines some kind of effective sparsity of the problem ( often muss less than @xmath204 ) .",
    "this effect is not treated in the theoretical setup in all of the fdr - related studies , where implicitly a worst case scenario of the coefficients magnitude is understood .",
    "[ th : lower ] for any estimator @xmath234 based on @xmath235 observations we have the minimax lower bound @xmath236 { \\geqslant}\\sup_{\\alpha_n\\in s_\\lambda(n\\gamma_n , c_n ) } 2\\big(1+o(1)\\big)\\big(\\sum_{\\lambda\\in\\lambda } \\sigma_\\lambda ^2\\alpha_{\\lambda , n}\\log(\\alpha_{\\lambda , n}^{-1})\\big)\\ ] ] for some @xmath237 where @xmath238^\\lambda\\,|\\,\\sum_\\lambda\\alpha_\\lambda{\\leqslant}r(1-c)\\}$ ] denotes the intersection of @xmath239-times the @xmath235-dimensional unit cube with @xmath240-times the @xmath235-simplex and where @xmath241 as @xmath153 .    distributing mass uniformly over the @xmath242 indices with largest values @xmath243 yields the lower bound , as @xmath153 , @xmath236 { \\geqslant}2n\\gamma_n\\log(\\gamma_n^{-1})\\big(1+o(1)\\big)\\frac 1{r_n}\\sum_{i=1}^{r_n } \\sigma_{(i ) } ^2\\ ] ] in terms of the inverse order statistics @xmath244 , provided @xmath245 ( i.e. , @xmath242 must be somewhat larger than @xmath204 ) .",
    "note that for polynomial growth @xmath246 , @xmath247 , the lower bound is , as @xmath153 , @xmath236 { \\geqslant}2\\big(1+o(1)\\big){\\lvert \\sigma \\rvert}n\\gamma_n\\log(\\gamma_n^{-1}).\\ ] ]    the lower bound is a kind of weighted entropy .",
    "in contrast to the upper bounds above the minimax ( and the bayes ) lower bound does not involve the quantity @xmath214 , individual to each unknown @xmath14 . in the proof for this heterogeneous model ,",
    "conceptually we need to allow for a high complexity of the class @xmath248 , leading to the entropy factor @xmath249 , and to put more prior probability on coefficients with larger variance , which explains the abstract weighted entropy expression .",
    "consider for each coefficient @xmath250 the following bayesian prior , which turns out to be asymptotically least favorable : @xmath251 with some @xmath252 . without loss of generality",
    "we may assume @xmath253 so slowly that @xmath254 .",
    "introducing the number of non - zero entries @xmath255 and writing @xmath256 for the joint law of prior and observations , we deduce by chebyshev inequality @xmath257 the property @xmath258 then implies that the bayes - optimal risk , derived below , will be an asymptotic minimax lower bound over @xmath248 .",
    "we need to calculate the bayes risk and find the posterior law of @xmath259 for each coordinate @xmath61 : @xmath260 since we deal with quadratic loss , the bayes estimator @xmath261 equals the conditional expectation @xmath262 $ ] and the bayes risk the expectation of the conditional variance , which is calculated as @xmath263=\\operatorname{{\\mathbf e}}[f_\\lambda ^2]-\\operatorname{{\\mathbf e}}[\\operatorname{{\\mathbf e}}[f_\\lambda |x_\\lambda ] ^2 ] = \\mu_{\\lambda , n}^2\\big(\\alpha_{\\lambda , n}-\\int \\frac{\\alpha_{\\lambda , n}^2{\\varphi}_{\\mu_{\\lambda , n},\\sigma_\\lambda ^2}(x)^2 } { ( 1-\\alpha_{\\lambda , n}){\\varphi}_{0,\\sigma_\\lambda ^2}(x)+ \\alpha_{\\lambda , n}{\\varphi}_{\\mu_{\\lambda , n},\\sigma_\\lambda ^2}(x)}\\,dx\\big).\\ ] ] the integral can be transformed into an expectation with respect to @xmath264 and bounded by jensen s inequality : @xmath265\\\\ & \\qquad { \\leqslant}\\alpha_{\\lambda , n } \\big(1+\\alpha_{\\lambda , n}^{-1}(1-\\alpha_{\\lambda , n})\\operatorname{{\\mathbf e}}[\\exp(\\sigma_\\lambda ^{-1}z-\\mu_{\\lambda , n}^2/(2\\sigma_\\lambda ^2))]\\big)^{-1 } \\\\ & \\qquad = \\alpha_{\\lambda , n } \\big(1+\\alpha_{\\lambda , n}^{-1}(1-\\alpha_{\\lambda , n})\\exp((1-\\mu_{\\lambda , n}^2)/(2\\sigma_\\lambda ^2))\\big)^{-1}.\\end{aligned}\\ ] ]    since @xmath266 uniformly , we just select @xmath267 such that @xmath263{\\geqslant}2\\sigma_\\lambda ^2\\alpha_{\\lambda , n}(1-(\\log c_n^{-1})^{-1/2})\\log(\\alpha_{\\lambda , n}^{-1 } ) ( 1-((1+(1-\\alpha_{\\lambda , n})\\alpha_{\\lambda ,",
    "n}^{-(\\log c_n^{-1})^{-1/2}}e^{1/(2\\sigma_\\lambda ^2)}))^{-1}).\\ ] ] noting @xmath268 uniformly over @xmath188 , the overall bayes risk is hence uniformly lower bounded by @xmath269 the supremum at @xmath235 is attained for @xmath270 where @xmath271 is such that @xmath272 holds , provided @xmath273 for all @xmath188 . the latter condition is fulfilled if @xmath274 .",
    "alternatively , we may write @xmath275 and the entropy expression becomes @xmath276 where the @xmath277 $ ] sum up to one : @xmath278 . from this representation",
    "we immediately infer the lower bound @xmath279 using the uniform weights @xmath280 .",
    "note that for polynomial growth @xmath246 , @xmath247 , and for @xmath281 , we have @xmath282 and the lower bound is indeed @xmath236 { \\geqslant}2\\big(1+o(1)\\big){\\lvert \\sigma \\rvert}n\\gamma_n\\log(\\gamma_n^{-1}).\\ ] ]",
    "consider now the setting where the sparsity @xmath10 is known and a correctly tuned threshold estimator is applied in order to identify the unknown positions of the significant non - zero coefficients @xmath283 .",
    "[ th : upper ] consider the threshold estimator defined coordinate - wise by @xmath284 and @xmath285 chosen such that @xmath286 .",
    "then , as @xmath153 , @xmath236{\\leqslant}2n\\gamma_n\\beta_n(1+o(1))\\ ] ] holds .",
    "this implies that , as @xmath153 , @xmath236{\\leqslant}2n\\gamma_n\\log(\\gamma_n^{-1}){\\lvert \\sigma \\rvert}(1+o(1)),\\ ] ] which is minimax optimal for at most polynomial growth in @xmath287 by the lower bound in theorem [ th : lower ] .    for faster growth than polynomial",
    ", we might well have @xmath288 .",
    "so , in general the upper bound matches exactly the lower bound with respect to the term @xmath289 , while the influence of the heterogeneous noise depends on the specific case . however , this procedure is non - adaptive since the threshold relies on the knowledge of the sparsity @xmath10 .    introduce the threshold value @xmath290 and note @xmath291 .",
    "we can split the error as follows : @xmath292=f_\\lambda^2\\operatorname{{\\mathbb p}}((\\xi_\\lambda+f_\\lambda/\\sigma_\\lambda)^2{\\leqslant}\\tau_{\\lambda , n}^2)+\\operatorname{{\\mathbf e}}[\\sigma_\\lambda^2\\xi_\\lambda^2{\\bf 1}_{\\{(\\xi_\\lambda+f_\\lambda/\\sigma_\\lambda)^2>\\tau_{\\lambda , n}^2\\}}]=:i+ii.\\ ] ] for @xmath293 term i is estimated by @xmath294 together with a symmetric argument for @xmath295 and a direct bound for @xmath296 , we thus obtain a bound for general @xmath283 : @xmath297 since for @xmath298 we have @xmath299 , we consider @xmath300 and infer @xmath301 inserting the choice of the thresholds , we conclude @xmath302    for term ii and @xmath197 the immediate estimate @xmath303 suffices , while for @xmath220 we integrate out explicitly and obtain : @xmath304=\\sigma_\\lambda^22(\\tau_{\\lambda , n}+1)e^{-\\tau_{\\lambda , n}^2/2 } = 2\\sigma_\\lambda^2\\sqrt{2\\log(\\alpha_{\\lambda , n}^{-1})}\\alpha_{\\lambda , n}(1+\\tau_{\\lambda , n}^{-1}).\\ ] ] the overall risk of our estimator is therefore bounded by @xmath305{\\leqslant}\\sum_{\\lambda : f_\\lambda\\not=0}\\big(2\\sigma_\\lambda^2\\log(\\alpha_{\\lambda , n}^{-1})(1+o(1 ) ) + \\sigma_\\lambda^2\\big)+\\sum_{\\lambda : f_\\lambda=0 } 2\\sigma_\\lambda^2\\sqrt{2\\log(\\alpha_{\\lambda , n}^{-1})}\\alpha_{\\lambda , n}(1+o(1))\\\\ & { \\leqslant}(2+o(1 ) ) \\big(\\sum_{\\lambda : f_\\lambda\\not=0}\\log(\\alpha_{\\lambda , n}^{-1})\\sigma_\\lambda^2 + \\sqrt 2\\max_\\lambda\\big((\\log(\\alpha_{\\lambda , n}^{-1}))^{-1/2}\\alpha_{\\lambda , n}\\big)\\sum_{\\lambda : f_\\lambda=0 } \\sigma_\\lambda^2\\log(\\alpha_{\\lambda , n}^{-1})\\big).\\end{aligned}\\ ] ] choosing @xmath306 , with @xmath285 satisfying @xmath286 , minimises the last bound ( asymptotically ) and yields @xmath307{\\leqslant}(2+o(1))n\\gamma_n\\beta_n\\ ] ] because by @xmath308 the second term is of smaller order .",
    "the last result is a direct consequence .",
    "indeed , we always have @xmath309 by bounding @xmath310 , which is minimax optimal for at most polynomial growth in @xmath287 by the lower bound in theorem [ th : lower ] .",
    "( blue ) , observations @xmath311 ( green in full subset , green / yellow in adaptive threshold , magenta not taken ) and universal / sparse thresholds ( black ) ( parameter values : @xmath312 , @xmath313 , @xmath314 for @xmath315 ) . , width=529,height=264 ]    in figure [ fig1 ] a typical realisation of the coefficients @xmath283 is shown in blue with 50 non - zero coefficients chosen uniformly on @xmath316 $ ] and increasing noise level @xmath314 for @xmath317 .",
    "the inner black diagonal lines indicate the sparse threshold ( with oracle value of @xmath10 ) and the outer diagonal lines the universal threshold .",
    "the non - blue points depict noisy observations @xmath193 .",
    "observations included in the adaptive full subset selection estimator are coloured green , while those included for the adaptive threshold estimator are the union of green and yellow points ( in fact , for this sample the adaptive thresholding selects all full subset selected points ) , the discarded observations are in magenta",
    ".        we have run 1000 monte carlo experiments for the parameters @xmath312 , @xmath314 in the sparse ( @xmath318 ) and dense ( @xmath313 ) case . in figure [ fig2 ]",
    "the first 100 relative errors are plotted for the different estimation procedures in the dense case .",
    "the errors are taken as a quotient with the sample - wise oracle threshold value applied to the renormalised @xmath319 .",
    "therefore only the full subset selection can sometimes have relative errors less than one .",
    "table [ tab1 ] lists the relative monte carlo errors for the two cases .",
    "the last column reports the relative error of the oracle procedure with @xmath320 that discards all observations @xmath193 with @xmath220 ( not noticing the model selection complexity ) .",
    "the simulation results are quite stable for variations of the setup .",
    "altogether the thresholding works globally well .",
    "the ( approximate ) full subset selection procedure ( see below for the greedy algorithm used ) is slightly worse and exhibits a higher variability , but is still pretty good . by construction , in the dense case",
    "the oracle sparse threshold works better than the universal threshold , while the universal threshold works better in very sparse situations .",
    "the reason why the sparse threshold even with a theoretical oracle choice of @xmath10 does not work so well is that the entire theoretical analysis is based upon potentially most difficult signal - to - noise ratios , that is coefficients @xmath283 of the size of the threshold or the noise level . here , however , the effective sparsity is larger ( i.e. , effective @xmath10 is smaller ) because the uniformly generated non - zero coefficients can be relatively small especially at indices with high noise level , see also figure [ fig1 ] .",
    "let us briefly describe how the adaptive full subset selection procedure has been implemented .",
    "the formula attributes to each selected coefficient @xmath193 the individual penalty @xmath321 with the inverse rank @xmath322 of @xmath323 .",
    "due to @xmath324 all coefficients with @xmath325 are included into @xmath326 in an initial step .",
    "then , iteratively @xmath327 is extended to @xmath328 by including all coefficients with @xmath329 the iteration stops when no further coefficients can be included .",
    "the estimator @xmath330 at this stage definitely contains all coefficients also taken by @xmath331 . in a second iteration",
    "we now add in a more greedy way coefficients that will decrease the total penalized empirical risk . including a new coefficient @xmath332 ,",
    "adds to the penalized empirical risk the ( positive or negative ) value @xmath333 here , @xmath334 is to be understood as the rank at @xmath335 when setting @xmath336 .",
    "consequently , the second iteration extends @xmath330 each time by one coefficient @xmath332 for which the displayed formula gives a negative value until no further reduction of the total penalized empirical risk is obtainable .",
    "this second greedy optimisation does not necessarily yield the optimal full subset selection solution , but most often in practice it yields a coefficient selection @xmath331 with a significantly smaller penalized empirical risk than the adaptive threshold procedure .",
    "the numerical complexity of the algorithm is of order @xmath337 due to the second iteration in contrast to the exponential order @xmath338 when scanning all possible subsets .",
    "a more refined analysis of our procedure would be interesting , but might have minor statistical impact in view of the good results for the straight - forward adaptive thresholding scheme .",
    "the authors would like to thank iain johnstone , debashis paul and thorsten dickhaus for interesting discussions .",
    "m. rei gratefully acknowledges financial support from the dfg via research unit for1735 _ structural inference in statistics_.                                            massart p. ( 2007 ) . _",
    "concentration inequalities and model selection .",
    "_ lectures from the 33rd summer school on probability theory held in saint - flour , july 6 - 23 , 2003 .",
    "lecture notes in mathematics , springer , berlin ."
  ],
  "abstract_text": [
    "<S> we consider a gaussian sequence space model @xmath0 where @xmath1 has a diagonal covariance matrix @xmath2 . </S>",
    "<S> we consider the situation where the parameter vector @xmath3 is sparse . </S>",
    "<S> our goal is to estimate the unknown parameter by a model selection approach . </S>",
    "<S> the heterogenous case is much more involved than the direct model . </S>",
    "<S> indeed , there is no more symmetry inside the stochastic process that one needs to control since each empirical coefficient has its own variance . the problem and </S>",
    "<S> the penalty do not only depend on the number of coefficients that one selects , but also on their position . </S>",
    "<S> this appears also in the minimax bounds where the worst coefficients will go to the larger variances . </S>",
    "<S> however , with a careful and explicit choice of the penalty we are able to select the correct coefficients and get a sharp non - asymptotic control of the risk of our procedure . </S>",
    "<S> some simulation results are provided . </S>"
  ]
}