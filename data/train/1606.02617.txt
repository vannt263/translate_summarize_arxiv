{
  "article_text": [
    "since the introduction of the k nearest neighbor ( knn ) method by fix and hodges in 1951 @xcite a lot of different variants of it have appeared in order to make it suitable to different scenarios .",
    "the most notable improvements were done in terms of adaptive distance metrics @xcite@xcite@xcite , fast access via space partitioning ( packed r * trees @xcite , kd - trees@xcite , x - trees@xcite , spy - tec @xcite ) , knowledge base ( prototype ) pruning ( @xcite,@xcite ) or classification based on sensitive distributed data ( @xcite,@xcite,@xcite ) .",
    "an overview over the state - of - the - art in nearest neighbor techniques is given in @xcite .",
    "the continuing richness of investigation work into nearest neighbor can be explained with the omnipresence of cbr ( case based reasoning ) type of problems @xcite or just from the practical point of view of its massive parallelizability or simply populartiy .",
    "after all nearest neighbor is conceptually easy to understand - many students get to learn the nearest neighbor as the first classifier .",
    "all of the beforehand mentioned advances evolve around the question of how to optimize the knn s distance measure for retrieving .",
    "the method s apparent laziness might be the reason why a fast preoptimization of @xmath0 has not been paid a lot attention to .",
    "the proper choice of @xmath0 is an important factor for achieving maximum performance of a knn .",
    "however , as we will show , conventional @xmath0 optimization via cross - validation or bootstrapping is slow and promises potential for being sped up .",
    "therefore , we will devote this paper to the concept of fast @xmath0 optimization .",
    "there is work addressing this issue in an alternative way by introducing _",
    "incremental _ knn classifiers based on different types of trees . this class of nearest neighbors attempts to eliminate the influence of k by choosing the right @xmath0 ad hoc .",
    "the rationale behind this method is that for classification tasks the exact majority of a specific class label is not necessarily interesting .",
    "it is only interesting when nearest neighbor is used as a density estimator . in other cases",
    "it is generally enough to feel safe about which class label rules the nearest set .",
    "this class of nearest neighbor starts by polling a minimal amount of nearest neighbors .",
    "then it analyzes the labels and if the retrieved collection of labels is indecisive it will poll more nearest neighbors until it considers the collection decisive enough .",
    "naturally , the method does not scale to small values of @xmath0 , because e.g. a @xmath1 will never be indecisive .",
    "this is a problem because we know from experiments that small @xmath0 are often optimal .",
    "incremental nearest neighbors have their strength in very large databases where typical queries do not need to compute all relative distances . during a lifetime of a large database some distances",
    "might not be computed at all .",
    "the lazy distance computation make incremental nearest neighbor methods ideal candidates for real time tasks operating on large volatile data .",
    "however , in a cross validation setup which needs to compute all distances incremental knns can not play out their strengths . because of the restrictions and intended use we exempted incremental methods from investigation in this paper .",
    "we organize this paper in three parts .",
    "in the first part we will study the options to make the estimation of @xmath0 as fast as possible , in the second part we will experimentally compare the result with the conventional approach and in the last part we will draw a conclusion .",
    "the knn is commonly considered a classifier from the area of supervised learning theory . in this theory",
    "there exists a training function @xmath2 that delivers a model @xmath3 based on a set of options @xmath4 , a matrix of example values @xmath5 and a vector of labels @xmath6 of respective size ( @xmath7 ) . in turn , the model @xmath3 is used in a classification function @xmath8 that is presented with a matrix of new examples @xmath9 and its task is to deliver a vector of new labels @xmath10 ( @xmath11 ) . @xmath2 and @xmath8 must be related but there is no restriction on what the model can be . it can be a set of complex items , a matrix , a vector or just a single value , indeed .    from the perspective of a human the purpose of a model",
    "is to make predictions about the future . in order to be able to do this",
    "the human brain requires a simplification of the world . only with the simplification of the world to a reduced number of variables and rules",
    "it can compute future states faster than they occur . in case of knn the model @xmath3",
    "consists of the data and of the smoothing parameter @xmath0 .",
    "this means , that the function @xmath2 is an identity function between model and parameters .",
    "this conflicts with the common notion of a model because the production of models commonly implies reduction . however , the collected data already are a reduction of the world ! from a practical point of view",
    "they can be considered as representative model states of the world and the data in the model is used to predict the class variable of a new vector before it is actually recorded .",
    "this fits perfectly well with the original notion of models .",
    "however , the model is only fixed , when @xmath0 is fixed .",
    "according to the framework , fixing the model ( getting the right value for @xmath0 ) is the job of the function @xmath2 .",
    "many training techniques in machine learning use optimization strategies developed for real numbers and open parameter spaces .",
    "this is not suitable for @xmath0 as it is an integer value and has known left and right limits : an ideal candidate for full search .",
    "most frameworks for pattern recognition offer macro optimization for the remaing model parameters that express themself in the initial training options @xmath4 .",
    "the structural compatibility of the knn with the pattern recognition frameworks macro optimization functions seduces the users very often to macro optimize @xmath0 .",
    "the consequence of this is that knn must compute distances repetitively as it can not assume that specific vectors will simply exchange their role between training and testing in the future . in case of knn",
    "this is exceptionally regrettable .",
    "what does macro optimization mean for the computational complexity ?",
    "here we assume - but without loose of generality - that the dataset @xmath5 is of size @xmath12 ( @xmath12-rows in matrix @xmath5 ) and can be exactly divided into @xmath13 equally sized partitions ready to be rearranged into @xmath13 different train and test setups . since everything is being recomputed the computational complexity for this kind of cross - validation of @xmath0 for a brute knn is @xmath14 .",
    "@xmath15 is the size of the tested range of @xmath0 and since we are considering full search we accept that @xmath15 depends on the size @xmath12 of the dataset and the number of folds @xmath13 .",
    "this means that the @xmath16 .",
    "the scan of nearest sets yields a partial complexity of @xmath17 .",
    "hence , for full search the total complexity is @xmath18 .    in order to reduce this high complexity",
    "it is necessary to optimize knn within the train function @xmath2 as it has all necessary information about the relationships among the examples and the labels .",
    "this means that @xmath19 should become @xmath20 where @xmath21 is a vector of partition indices of the kind @xmath22 .",
    "now , the train function @xmath2 can utilize the fact that no new data will arrive during the training and all possible distance requests can be computed in advance .",
    "distances within the same partition need not be computed as they will be never requested .",
    "these fields can be set to infinity ( alternatively they can be filled up with the largest value found in the matrix + 1 ) .",
    "the lower triangle is symmetrical to the upper triangle of the matrix because vectors between two points have the same norm .",
    "the distance matrix @xmath23 has a structure as shown in figure [ fig : initial_matrix ] .",
    "the size of @xmath23 is @xmath24 .",
    "by @xmath25 we mean the component _ distance _ and by @xmath26 we mean the associated _",
    "label_.    alone this redesign causes the complexity of the distance computations to get reduced to @xmath27 .",
    "however this benefit is achieved at the expense of higher memory use .",
    "the brute knn has a space complexity of @xmath28 , now the space complexity has risen to @xmath29 .",
    "the next step is to sort the vectors horizontally according to their distance .",
    "although collecting the @xmath0 best solutions would be faster for a single run it means for a range of @xmath0 that you effectively obtain the insertion sort .",
    "since there exist faster sorting algorithms we choose to sort but by using a different algorithm .",
    "the fastest algorithm for doing so is the quick sort .",
    "its average complexity is @xmath30 . in the worst case scenario",
    "the sorting complexity of this method is @xmath31 . in that case",
    "@xmath12 rows will be sorted with @xmath31 .",
    "this means that the worst time complexity so far is @xmath32 and average case is @xmath33 .    in order to obtain nearest neighbors for each vector indexed by the row",
    "a counting matrix @xmath34 is initialized with zeros .",
    "@xmath35 is the number of symbols or classes . for each row in @xmath23 and @xmath36 and for the columns @xmath37 in @xmath23 the counters for the specific class label",
    "is increased .",
    "more precisely , for every row @xmath38 and for every tested @xmath0 the counters @xmath39 are updated by @xmath40 .",
    "the complexity of this operation is @xmath41 . for the overall method this adds up to @xmath42 . in parallel the level of correct classification",
    "must be computed because after every modification of @xmath36 the state for the smaller @xmath0 is lost .",
    "therefore a matrix @xmath43 for recording the number of correct classfications is required .",
    "how is this number computed ? at every round @xmath0 of the nearest neighbor candidate computations @xmath44 contains in each of its rows a vector that tells how many labels of specific kind are in the nearest neighbors set .",
    "the classification label is @xmath45 .",
    "the complexity for this operation is @xmath46 .",
    "this simple method is ambiguous by nature , as there can be many labels that are represented by the same amount of vectors in a nearby set .",
    "computer implementations prefer to return the symbol with the smalest coding .",
    "however , it is possible to have a shadow matrix @xmath47 that is the sum of the distances observed for each class label in the set so far .",
    "the rule for computing @xmath48 is the same as for @xmath36 with the difference that instead of adding ones to the matrix you add distances . when symbol frequency is ambiguous ( argmax returns more than one value ) it is possible to use @xmath48 to find which samples are closer overall . because of the specific interest into fast @xmath0 optimization the simple argmax processing is used .",
    "now , every @xmath49 is compared for equality with @xmath50 ( ground truth ) and the binary result is added to @xmath51 .",
    "the @xmath52 will return @xmath13 best @xmath0 .",
    "@xmath53 is obtained by averaging .",
    "considering all parts of the algorithm together the overall complexity is @xmath54",
    "the method for fast @xmath0 computation ( autoknn ) was tested against three other algorithms from the ann library 1.1@xcite : brute , kd - tree and bd - tree knn with default settings .",
    "the autoknn and its competitors performed a complete cross validation run on the ad , diabetes , gene , glass , heart , heartc , horse , ionosphere , iris , mushrooms , soybean , statlog australian , statlog german , statlog heart , statlog sat , statlog segment statlog shuttle , statlog vehicle , thyroid , waveform and wine datasets with 3 , 5 , 10 and 20 folds .",
    "the goal of the experiment was the measurement of the time required to complete the full course of testing different @xmath0 .",
    "the data was separated into stratified partitions which were used in different configurations in order to obtain a training and a testing set .",
    "the autoknn computes the classification results for all @xmath0 while the other algorithms are bound to use a logarithmic search . by logarithmic search",
    "the following schema is meant : @xmath55 .",
    "this schema is practically motivated and rational under the assumption that the influence of additional labels on the result diminishes with higher values of @xmath0 .",
    "practical consideration is primarily test time .",
    "example : while autoknn required 15,35s for a complete scan based on the _ ad _ dataset , on same data exact brute knn needed 353,7s in logarithmic mode and 19595,7s in full mode .",
    "the use of the logarithmic mode makes results with exactly the same values impossible .",
    "however , the differences in resulting @xmath0 and thus in accuracy were absolutely negligible so that the results are directly comparable nonetheless",
    ".    we added the experiment times for all databases up to a total for each cross validation size .",
    "the results are shown in the figure and the table under [ fig : result ] .",
    "time measurements were performed on a amd phenom ii 965 with 8 gb of ram with a linux 2.6.35 kernel .",
    "the algorithms are implemented in c / c++ and were compiled with gcc 4.4.5 with o3 option .",
    "only core algorithm operation was measured and all time for additional i / o was ignored . for best comparability , ann library sources were statically included .",
    "the nearest neighbor approach is considered user friendly and is frequently used for data mining , classification and regression tasks .",
    "it is embedded into many automatic environments that make use of knn s flexibility .",
    "although knn has been used , analyzed and advanced for almost six decades a repeating question can not be answered by current literature : what is the fastest way to estimate the right value for @xmath0 and what are the expenses for doing so .",
    "the approach chosen here is to move the @xmath0 esimation away from the meta framework right into the training function @xmath2 .",
    "the advantage of this is that additional information about the data can be made .",
    "this additional information allows to precompute the distances among all vectors without waste and to reuse them numerous times . from this design change which is known to practitioners but not discussed in literature a reduction in time complexity",
    "can be observed from @xmath56 to @xmath57 in average case .",
    "the experiments show , that this has significant impact on the speed of the @xmath0 estimation task .",
    "the comparison between kd - tree knn and the proposed approach proves moreover that having a better time complexity saves practically more time than an efficient distance measure for this task .",
    "the cost of this improvement is a higher space complexity ( now @xmath29 ) . in order to esimtate the practical impact of this complexity exchange we studied the contents of the uci repository @xcite .",
    "the uci should be a reasonable crossover of the problems people face in real life .    out of 162 datasets we found that 90% of them have less than 50k examples , 80% of them have less than 10k examples and half of the uci s datasets has less than 1000 examples ( for exact distribution see fig .",
    "[ fig : uci ] ) .",
    "these sizes can be easily handled on higher class commodity computers .",
    "this leads to the conclusion that turning in space complexity for time complexity is a good choice most of the time .",
    "future implementations should offer an integrated @xmath0 searching .",
    "the results also show that the so found values for @xmath0 can be transfered not only to other exact knn but also to approximate knn working on kd - tree and bd - tree models .",
    "this work has been made possible through the funding of the paren ( pattern recognition and engineering @xcite ) project by the bmbf ( federal ministry of education and research , germany ) .",
    "00 fix , e. , hodges , j.l . _ discriminatory analysis , nonparametric discrimination : consistency properties_. technical report 4 , usaf school of aviation medicine , randolph field , texas , 1951 .",
    "carlotta domeniconi , dimitrios gunopulos , jing peng .",
    "_ adaptive metric nearest neighbor classification_. cvpr , vol . 1 ,",
    "pp.1517 ( 2000 )    jing peng , douglas r. heisterkamp , h. k. dai .",
    "_ adaptive kernel metric nearest neighbor classification_. icpr , vol .",
    "3 , pp.30033 ( 2002 )    t. deselaers , r. paredes , e. vidal , h. ney . _",
    "learning weighted distances for relevance feedback in image retrieval_. icpr 2008 , tampa , florida , usa ( 08/12/2008 )    i. kamel , c. faloutsos .",
    "_ on packing r - trees_. proceedings of the 2nd conference on information and knowledge management ( cikm ) , washington dc ( 1993 )    thomas h. cormen , charles e. leiserson , ronald l. rivest .",
    "_ introduction to algorithms_. ch .",
    "10 , mit press and mcgraw - hill ( 2009 )    s. berchtold , d. keim , h .-",
    "_ the x - tree : an index structure for high - dimensional data_. in proceedings of 22th international conference on very large databases ( vldb96 ) , pp 2839 , morgan kaufmann ( 1996 )    dong - ho lee , hyoung - joo kim .",
    "_ an efficient technique for nearest - neighbor query processing on the spy - tec_. ieee transactions on knowledge and data engineering , vol .",
    "15 , no . 6 , pp . 1472 - 1486 , ieee educational activities department , piscataway , nj , usa ( 2003 )    jose salvador - sanchez , filiberto pla , francesco j. ferri . _ prototype selection for the nearest - neighbor rule through proximity graphs_. prl , vol . 18 , no . 6 , pp .",
    "507 - 513 ( 1997 )    u. lipowezky .",
    "_ selection of the optimal prototype subset for 1-nn classification _ prl , vol .",
    "907 - 918 ( 1998 )    keith price bibliography - nearest neighbor literature overview    david w. aha . _",
    "the omnipresence of case - based reasoning in science and application_. journal on knowledge - based systems , vol 11 , pp .",
    "261 - 273 ( 1998 )    young , b. , bhatnagar , r. : secure k - nn algorithm for distributed databases .",
    "university of cincinnati ( 2006 )    shaneck , m. , yongdae , k. , kumar , v. : privacy preserving nearest neighbor search . dept . of computer science minneapolis ( 2006 )    kantarcoglu , m. , clifton ,",
    "c. : privately computing a distributed @xmath0-nn classifier .",
    "lncs , vol .",
    "3202 , 279290 ( 2004 )    mount , d. m. : approximate nearest neighbor library 1.1 , department of computer science and institute for advanced computer studies , university of maryland ( 2010 )    uci machine learning repository :    pattern recognition and engineering ( paren ) , dfki , bmbf project : ( 2008 - 2010 )    janick v. frasch and aleksander lodwich and faisal shafait and thomas m. breuel _ a bayes - true data generator for evaluation of supervised and unsupervised learning methods_. pattern recognition letters , volume 32:11 , p.1523 - 1531 , 2011 , doi : 10.1016/j.patrec.2011.04.010",
    "the following diagrams are results from knn based on natural and synthetic datasets .",
    "synthetic datasets were obtained using wgks @xcite the standard deviation was estimated based on a 10x cross - validation .",
    "the diagrams are non linear . sections of little change are compressed , hence x - axis are discontinuous ."
  ],
  "abstract_text": [
    "<S> the k nearest neighbors ( knn ) method has received much attention in the past decades , where some theoretical bounds on its performance were identified and where practical optimizations were proposed for making it work fairly well in high dimensional spaces and on large datasets . from countless experiments of the past </S>",
    "<S> it became widely accepted that the value of @xmath0 has a significant impact on the performance of this method . </S>",
    "<S> however , the efficient optimization of this parameter has not received so much attention in literature . </S>",
    "<S> today , the most common approach is to cross - validate or bootstrap this value for all values in question . </S>",
    "<S> this approach forces distances to be recomputed many times , even if efficient methods are used . hence , estimating the optimal @xmath0 can become expensive even on modern systems . </S>",
    "<S> frequently , this circumstance leads to a sparse manual search of @xmath0 . in this paper </S>",
    "<S> we want to point out that a systematic and thorough estimation of the parameter @xmath0 can be performed efficiently . </S>",
    "<S> the discussed approach relies on large matrices , but we want to argue , that in practice a higher space complexity is often much less of a problem than repetitive distance computations . </S>"
  ]
}