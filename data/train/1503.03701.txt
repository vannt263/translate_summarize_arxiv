{
  "article_text": [
    "recently , a new breed of topic models , dubbed counting grids ( cg ) @xcite , has been shown to have advantages in unsupervised learning over previous topic models , while at the same time providing a natural representation for visualization and user interface design @xcite .",
    "cg models are _ generative _ models based on a grid of word distributions , which can best be thought of as the grounds for a massive venn diagram of documents .",
    "the intersections among multiple documents ( bags of words ) create little intersection units with a very small number of words in them ( or rather , a very sparse distribution of the words ) .",
    "the grid arrangement of these sparse distributions , which we will refer to here as _ microtopics _ , facilitates fast cumulative sum based inference and learning algorithms that chop up the documents into much smaller constitutive pieces than what traditional topic models typically do .",
    "for example , fig .",
    "[ fig : fig0 ] shows a small part of such a grid with a few representative words with greatest probability from each microtopic .",
    "each of the science magazine abstracts used to train this grid is assumed to have been generated from a group of microtopics found in a single 4 @xmath0 4 window with equal weight given to all component microtopics .",
    "thus , each microtopic can be 16 times sparser than the set of documents grouped into the window .",
    "a document may share a window with another very similar document , but it is also mapped so that it only partially overlaps with a window that is the source for a set of slightly less related documents .",
    "the varying window overlap literally results in a varying overlap in document themes .",
    "this modeling assumption results in a trained grid where nearby microtopics tend to be related to each other as they are often used together to generate a document .",
    "consider , e.g. , the lower right 4@xmath04 window in fig .",
    "[ fig : fig0 ] . the word distributions in these 16 cells are such that a variety of science papers on evidence of ancient life on earth could be generated by sampling words from there .",
    "( note that each cell , though of very low entropy , contains a distribution over the entire vocabulary . ) in the posterior distribution , this window is by far the most likely source for an article on a bizarre microorganism that produced nitrogen in cretaceous oceans . in the 4@xmath04 window",
    "two cells to the left of this example we find mapped a variety of articles on even more ancient events on earth , e.g. on how sulfur isotopes reveal a deep mantle storage of ancient crust .",
    "but there we also start to see words which increase the fit for articles that describe similar events on other planets .",
    "further movement to the left gets us away from the earth and into astronomy .    to demonstrate the refinement of the microtopics compared to topics from a typical topic model , the color labeling of the grid was created so as to reflect the kullback-leibler ( kl ) divergence of the individual microtopics to the topics trained on the same data through latent dirichlet allocation ( lda ) .",
    "the lda topics , hand - labeled after unsupervised training , correspond to fairly broad topics , while the cg represents the data as a group of slowly evolving microtopics .",
    "for example , all the yellow coded microtopics map to the `` physics '' lda topic , but they occupy a contiguous area in which from left to right the focus slowly shifts from electromagnetism and particle physics to material science . furthermore , it is interesting to see the microtopics that occupy the boundaries between coarser topics that lda model found , capturing the links among astronomy , physics and biology .",
    "it is immediately evident that the 2d cgs can have great use in data visualization , though the model can be trained for arbitrary dimensionality @xcite .",
    "these models combine topic modeling and data embedding ideas in a way that facilitates intuitive regularization controls and allows creation of much larger sets of organized sparse topics .",
    "furthermore , they lend them selves to elegant visualization and browsing strategies , and we encourage the reader to see the example http://research.microsoft.com/en-us/um/people/jojic/cgbrowser.zip",
    ".    however , the existing em algorithm for cg learning is prone to local minima problems which occasionally lead to under performance @xcite .",
    "in addition , no direct testing of the microtopic coherence has been performed to date , which makes it unclear if they are meaningful outside their windowed grouping .",
    "after all , a variety of sophisticated topic models have been developed and tested by the research community , but lda seems to still beat them often in practice .",
    "e.g. , [ 16,17 ] raise doubts that various reported perplexity improvements over the basic lda model are meaningful as they are sensitive to smoothing constants in the model , and also fail to translate to improvements in human judgement of topic quality . in fact , lda usually outperforms more complex models on tasks that involve human judgement , which may be the main reason why practitioners of data science prefer this basic model to others @xcite .",
    "here we develop hierarchical versions of cg models , which in our experiments produced embeddings of considerably higher quality .",
    "we show that layering into deeper architectures primarily aids in avoiding bad local minima , rather than increasing representational capacity : the trained hierarchical model can be collapsed into an original counting grid form but with a much higher likelihood compared to the grids fit to the same data using em with random restarts .",
    "the better data fit then translates into quantitatively better summaries of the data , as shown in numerical experiments as well as human evaluations of microtopics obtained through crowdsourcing .",
    "[ fig : fig0 ]",
    "[ [ the - ccg - grids ] ] the ( c)cg grids @xcite : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the basic counting grid @xmath1 @xcite is a set of distributions on the @xmath2-dimensional toroidal discrete grid @xmath3 indexed by @xmath4 . the grids in this paper are bi - dimensional and typically from @xmath5 to @xmath6 in size .",
    "the index @xmath7 indexes a particular word in the vocabulary @xmath8 $ ] .",
    "thus , @xmath9 is the probability of the word @xmath7 at the @xmath2-dimensional discrete location @xmath10 , and @xmath11 at every location on the grid .",
    "the model generates bags of words , each represented by a list of words @xmath12 with each word @xmath13 taking an integer value between @xmath14 and @xmath15 .",
    "the modeling assumption in the basic cg model is that each bag is generated from the distributions in a single window @xmath16 of a preset size , e.g. , @xmath17 .",
    "a bag can be generated by first picking a window at a @xmath2-dimensional location @xmath18 , denoted as @xmath19 , then generating each of the @xmath20 words by sampling a location @xmath21 for a particular microtopic @xmath22 uniformly within the window , and finally by sampling from that microtopic .",
    "because the conditional distribution @xmath23 is a preset uniform distribution over the grid locations inside the window placed at location @xmath18 , the variable @xmath21 can be summed out@xcite , and the generation can directly use the grouped histograms @xmath24 where @xmath25 is the area of the window , e.g. 25 when 5@xmath05 windows are used .",
    "in other words , the position of the window @xmath18 in the grid is a latent variable given which we can write the probability of the bag as @xmath26 as the grid is toroidal , a window can start at any position and there is as many @xmath27 distributions as there are @xmath28 distributions .",
    "the former will have a considerably higher entropy as they are averages of many @xmath28 distributions .",
    "although the basic cg model is essentially a simple mixture assuming the existence of a single source ( one window ) for all the features in one bag , it can have a very large number of ( highly related ) choices @xmath27 to choose from .",
    "topic models @xcite , on the other hand , are admixtures that capture word co - occurrence statistics by using a much smaller number of topics that can be more freely combined to explain a single document .",
    "componential counting grids ( ccg ) @xcite combine these ideas , allowing multiple groups of broader topics @xmath27 to be mixed to explain a single document .",
    "the entropic @xmath27 distributions are still made of sparse microtopics @xmath28 in the same way as in cg so that the ccg model can have a much larger number of topics than an lda model without overtraining .",
    "more precisely , each word @xmath13 can be generated from a different window , placed at location @xmath29 , but the choice of the window follows the same prior distributions @xmath30 for all words . within the window at location @xmath29",
    "the word comes from a particular grid location @xmath31 , and from that grid distribution the word is assumed to have been generated .",
    "the probability of a bag is now @xmath32 in a well - fit ccg model , each data point has an inferred @xmath33 distribution that usually hits multiple places in the grid , while in a cg , each data point tends to have a rather peaky posterior location distribution because the model is a mixture .",
    "both models can be learned efficiently using the em algorithm because the inference of the hidden variables , as well as updates of @xmath28 and @xmath27 can be performed using summed area tables @xcite , and are thus considerably faster than most of the sophisticated sampling procedures used to train other topic models",
    ". an intriguing property of these models is that even on a @xmath34 grid with @xmath35 microtopics @xmath28 and just as many grouped topics @xmath27 , there is no room for too many independent groups . with a window size @xmath36 , for example , we can place only @xmath37 windows without overlap , and the remaining windows are overlapping the pieces of these 16 . the ratio between grid and window size is referred to as the _ capacity _ of the model , and the training set size necessary to avoid overtraining the model only needs to be 1 - 2 orders of magnitude above the capacity number .",
    "thus a grid of 1024 microtopics may very well be trainable with thousands of data points , rather than 100s of thousands that traditional topic models usually require for that many topics .",
    "[ fig : dig ]    [ [ raw - image - embedding - using - ccgs ] ] raw image embedding using ( c)cgs : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in previous applications of cg models to computer vision , images were represented as spatially disordered bags of features .",
    "we experimented with embedding raw images with full spatial information preserved , and we present this here as we feel that the image data helps in illuminating the benefits of hierarchical learning .",
    "an image described by a full intensity function @xmath38 could be considered as a set of words , each word being an image location @xmath39 . for a @xmath40 image",
    ", we have a vocabulary of size @xmath41 .",
    "the number of repetitions of word @xmath42 is then set to be proportional to the intensity i(x , y ) .",
    "( in case of color images , the number of features is simply tripled with each color channel treated in this way ) . in other words , an unwrapped image is considered to be a word ( location ) histogram . @xmath28 and @xmath27 distributions",
    "can then also be seen as images , as they provide weights for different image locations .",
    "if we tile the image representations of these distributions we get additional insight into cgs as an embedding method .",
    "[ fig : dig ] shows a portion of a @xmath43 grid trained on 2000 mnist digits assuming a @xmath44 window averaging . to illustrate the generative model , in c )",
    "we show the partial window sums for two overlapping windows over @xmath28 .",
    "the green and blue areas form a window that generates a version of digit 3 , which can be seen at the top left of this portion of the @xmath27 grid ( panel b ) ) .",
    "the blue and red , on the other hand , combine into a window that represent a digit 2 at the position ( 3,3 ) in panel b ) .",
    "partial sums for green , blue and red areas are shown in c ) and these partial sums , color coded and overlapped are also illustrated in d ) .",
    "careful observation of b ) or the full grid in the appendix , demonstrates the slow deformation of digits from one to another in the @xmath27 distributions .",
    "the appendix has additional examples of image dataset embedding , including rendered 3d head models and images of bold eagles retrieved by internet search .",
    "the cg @xmath28 distributions shown here look like little strokes , while @xmath27 distributions are full digits .",
    "the ccg model , on the other hand , combines multiple @xmath27 distributions to represent a single image , and so @xmath27 looks like a grid of strokes fig .",
    "[ fig : digits2]a , while @xmath28 distributions are even sparser .",
    "[ [ hierarchical - grids ] ] hierarchical grids : + + + + + + + + + + + + + + + + + + +    by learning a model in which microtopics join forces with their neighbors to explain the data , ( c-)cg models tend to exhibit high degrees of relatedness of nearby topics . as we slowly move away from one microtopic , the meaning of the topics we go over gradually shifts to related narrowly defined topics as illustrated by fig . [",
    "fig : fig0 ] ; this makes these grids attractive to hci applications .",
    "but this also means that simple learning algorithms can be prone to local minima , as random initializations of the em learning sometimes result in grouping certain related topics into large chunks , and sometime breaking these same chunks into multiple ones with more potential for suboptimal microtopics along boundaries .",
    "to illustrate this , in fig .",
    "[ fig : digits2]a we show a @xmath43 grid of strokes @xmath27 ( eq . [ eq : h ] ) learned from 2000 mnist digits using a ccg model assuming a @xmath45 window averaging .",
    "nearby features @xmath27 are highly related to each other as they are the result of adding up features in overlapping windows over @xmath28 ( which is not shown ) .",
    "ccg is an admixture model , and so each digit indexed by @xmath46 has a relatively rich posterior distribution @xmath47 over the locations in the grid that point to different strokes @xmath27 .",
    "fig : digits2 ] , we show one of the main principal components of variation in @xmath48 as an image of the size of the grid . for three peaks there , we also show @xmath27-features at those locations .",
    "the combination of these three sparse features creates a longer contiguous stroke , which indicates that this longer stroke is often found in the data .",
    "thus , the separation of these features across three distant parts of the map is likely a result of a local minimum in basic em training . to transfer this reasoning to text models ,",
    "consider the 5th cell in the first row in fig .",
    "[ fig : fig0 ] with words hiv , aids , and the blue cell in the middle of the last column with words selection , adaptive .",
    "the separation of these two things in faraway locations may very well be a result of a local minimum , which could be detected if location posteriors exhibit correlation .",
    "this illustration points to an idea on how to build better models .",
    "the distribution over locations @xmath18 that a data point @xmath46 maps to ( a posteriori ) could be considered a new representation of the data point ( digit in this case ) , with the mapped grid locations considered as features , and the posterior probabilities for these locations considered as feature counts .",
    "thus another layer of a generative model can be added to generate the locations in the grid below , fig .",
    "[ fig : gm]c - d",
    ". it is particularly useful to use another microtopic grid model as this added layer , because of the inherent relatedness of the nearby locations in the grid",
    ". the layer above can thus be either another admixture grid model ( componential counting grid - ccg ) , or a mixture ( cg ) , and this layering can be continued to create a deep model .",
    "as cg is a mixture model , it terminates the layering : its posterior distributions are peaky and thus uncorrelated .",
    "however , an arbitrary number of ccgs can be stacked on top of each other in this manner , terminating on top with a cg layer to form a hierarchical cg ( hcg ) model , or terminating in a ccg layer to form a hierarchical ccg ( hccg ) model . in each layer , the pointers to features below are grouped , which should result in creating a contiguous longer stroke as discussed above in a grid cell that contains a combination of pointers to the lower layers .        for the sake of brevity",
    ", we only derive the hcg learning algorithm with a single intermediate ccg layer . the extension to hccg and higher order hierarchies",
    "is reported in the appendix .",
    "variational inference and learning procedure for counting grid - based models utilizes cumulative sums and is only slower than training an individual ( c)cg layer by a factor proportional to the number of layers . the graphical model for hcg",
    "is shown in fig .",
    "[ fig : gm]c , where location variables pointing to grids in different layers have the same name , @xmath18 but carry a disambiguating superscript . _ to avoid superscripts in the equations below , we renamed the cg s location variable from @xmath49 to @xmath50 and dropped the superscript ",
    "@xmath51 in the layer above_. the bottom ccg layer follows @xmath52",
    "@xmath53 the latter is a pre - set distribution over the grid locations , uniform inside @xmath54 . instead of the prior @xmath55 the locations",
    "are generated from a top layer cg , indexed by @xmath50 ( @xmath56 in the figure ) , @xmath57 this equation also shows that the lower - levels grid locations act as observations in the higher level .",
    "we use the fully factorized variational posterior @xmath58 to write the negative free energy @xmath59 bounding the non - constant part of the loglikelihood of the data as @xmath60 we maximize @xmath59 with the em algorithm which iterates e- and m - steps until convergence .",
    "e : @xmath61 the m step re - estimates the model parameters using these updatedposteriors : @xmath62 \\nonumber \\\\",
    "\\pi_{{\\tiny cg},{{\\bf i}}}({{\\bf l } } ) \\hspace{-0.3 cm } & \\propto & \\hspace{-0.1 cm } \\hat{\\pi}_{{\\tiny cg},{{\\bf i}}}({{\\bf l } } ) \\cdot \\sum_{t , n } q^t(\\ell_n = { { \\bf l}})\\hspace{-0.cm}\\cdot\\hspace{-0.2 cm } \\sum_{{{\\bf k}}| { { \\bf i}}\\in   w_{{\\bf k } } } \\hspace{-0.2cm}\\frac { q^t ( k_n = { { \\bf i}})}{\\hat{h}_{{\\tiny cg},{{\\bf i}}}({{\\bf l } } ) }   \\nonumber \\end{aligned}\\ ] ] where the last ( cg ) update is performed analogous with @xcite .",
    "interestingly , training these hierarchical models stage by stage , reminiscent of deep models where such incremental learning was practically useful @xcite .",
    "+ although it has been shown that a deep neural network can be compressed into a shallow broader one through post training @xcite , the stacked ( c-)cg models can be collapsed mathematically . in this sense we can view hcg and hccg as _ hierarchical learning algorithms _ for cg and ccg , which are easier to visualize than deeper models .",
    "for example , for hcg in fig .",
    "[ fig : gm]c - d , it is straightforward to see that the following grid defined over the original features @xmath63 , @xmath64 can be used as a single layer grid that describes the same data distribution as the two - layer model are the grouped microtopics in the window @xmath65 - eq .",
    "[ eq : h ] ] . however , the grids estimated from the hierarchical models should be more compact as the scattered groups of features are progressively merged in each new layer .",
    "_ learning in hierarchical models is thus more gradual and results in better local maxima , and we show below that the results are far superior to regular em learning of the collapsed cg or ccg models . _",
    "in all the experiments we used models with two extra layers , although , in some experiments , we found that three levels worked slightly better . in general , the optimal number of layers will depend on the particular application .",
    "[ [ likelihood - comparison ] ] likelihood comparison : + + + + + + + + + + + + + + + + + + + + + +    in the first experiment we compared the local maxima on models learned using the ( full ) mnist data set .",
    "the two layer hcg model was first pre - trained stage - wise as , e.g. , @xcite , by training the higher level on the posterior distribution from the lower level as the input .",
    "then , the model was refined by further variational em training .",
    "the procedure is repeated 20 times with different random initializations to produce twenty hierarchical models . as discussed above",
    ", these models can be collapsed to a cg model by integrating out intermediate layers ( [ eq : collapse ] ) .",
    "these models were then compared with twenty models learned by directly learning cg models through previously published standard em learning algorithm starting from twenty random initializations . despite being collapsible to the same mathematical form ,",
    "the hcg models consistently produced higher likelihood than the cg models directly learned using the standard method .",
    "_ in fact , each cg model created by collapsing one of the learned hcg models had log likelihood at least two standard deviations above the highest log likelihood learned by basic em ( p - value @xmath66 ) . _",
    "both learning approaches used the computation time equivalent to 1000 iterations of standard em , which was more than enough for convergence .",
    "[ [ document - classification ] ] document classification : + + + + + + + + + + + + + + + + + + + + + + + +    next we ran test to see if the increased likelihood obtainable with a better learning algorithm translates into increased quality of representation when posterior distributions for individual text documents are considered as features in classification tasks .",
    "we considered the 20-newsgroup dataset ( 20n ) and the mastercook dataset ( mc ) composed by 4000 recipes divided in 15 classes .",
    "previous work @xcite reduced 20-newsgroup dataset into subsets with varying similarities and we considered the hardest subset composed by posts from the very similar newsgroups ` comp.os.ms-windows ` , ` comp.windows.x ` and ` comp.graphics ` .",
    "we considered the same complexities as in @xcite , using 10-fold cross validation and classified test document using maximum likelihood .",
    "results for both datasets are shown in tab . [",
    "tab : doccl ] .",
    ".document classification .",
    "when bold , hierarchical grids outperformed the basic grids with statistical significance ( hcg p - value @xmath672.01e-4 , hccg p - values @xmath68 1e-3 ) .",
    "`` linsvm '' stands for linear support vector machines which we reported as baseline . [ cols=\"<,^,^,^,^,^\",options=\"header \" , ]     [ tab : doccl ]        [ [ evaluation - of - microtopic - quality - using - quantitative - measures - related - to - the - use - in - visualization - and - indexing ] ] evaluation of microtopic quality using quantitative measures related to the use in visualization and indexing : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we evaluated the coherence and the clarity of the microtopics comparing the collapsed ( 2 layers ) hierarchical grids - hcg and hccg with regular grids @xcite , latent dirichlet allocation ( lda ) @xcite , the correlated topic model ( ctm ) @xcite which allows to learn a large set of correlated topics and few non - parametric topic models @xcite .",
    "+ generative models are often evaluated in terms of perplexity .",
    "however different models , even different learning algorithms applied to the same model , are very difficult to compare @xcite and better perplexity does not always indicate better quality of topics as judged by human evaluators @xcite . on the other hand , the subjective evaluation of topic quality",
    "is highly related to measures that have to do with data indexing , e.g. quality of word combinations when used for information retrieval .",
    "thus we start with a novel evaluation procedure for topic models which is strongly related to information indexing and then show that we obtain similar evaluation results when we use human judgement . in the following experiments",
    ", we considered a corpus @xmath69 composed of science magazine reports and scientific articles from the last 20 years .",
    "this is a very diverse corpus similar to the one used in @xcite . as preprocessing step , we removed stop - words and applied the porters stemmer algorithm @xcite .",
    "we considered grids of size @xmath70 and @xmath43 fixing the window size to @xmath71 .",
    "( previous literature showed that counting grids are only sensitive to the ratio between grid and window area , as long as windows are sufficiently big . ) we varied number of topics for lda and ctm in @xmath72 .",
    "for each complexity we trained 5 models starting with different random initializations and we averaged the results . in each repetition",
    ", we considered a random third of this corpus , for total of roughly @xmath73 documents , @xmath74 different words and more than @xmath75 tokens",
    ".    to evaluate ( micro)topics , we repetitively sampled _",
    "k_-tuples of words and checked for consistency , diversity and clarity of the indexed content . in the following , we describe the procedure used for evaluating grids .",
    "an equivalent procedure was used to evaluate other topic models for comparison .",
    "+ to pick a tuple @xmath76 of @xmath77 words , we sampled a grid location @xmath78 .",
    "then , we repetitively sampled the microtopic @xmath79 to obtain the words in the tuple @xmath80 .",
    "we did not allow repetitions of words in the tuple .",
    "we considered @xmath81 different @xmath82-tuples , not allowing repeated tuples .",
    "+ then we checked for consistency , diversity and clarity of content indexed by each tuple .",
    "the * consistency * is quantified in terms of the average number of documents from the dataset that contained _ all _ words in @xmath76 .",
    "the * diversity * of indexed content is illustrated through the cumulative graph of acquired unique documents as more and more @xmath77-tuples are sampled and used to retrieve documents containing them .",
    "as this last curve depends on the sample order , we further repeated the process 5 times for a total of 25k different samples .",
    "finally the * clarity * @xcite , measures the ambiguity of a query with respect to a collection of documents and it has been used to identify ineffective queries , on average , without relevance information .",
    "formally , the query clarity is measured as the entropy between the n - tuple and the language model @xmath83 ( unigram distributions ) as @xmath84 where @xmath85 .",
    "we estimated the likelihood of an individual document model generating the tuple @xmath86 and obtain @xmath87 using uniform prior probabilities for documents that contains a word in the tuple , and a zero prior for the rest .",
    "finally , to estimate @xmath88 we employed montecarlo sampling .",
    "+ results are illustrated in fig.[fig : results ] and should be appreciated by looking at all three measures together , as some can be over - optimized at the expense of others .",
    "the diversity curve that consistently grows as more tuples are sampled indicates that the sampled tuples belong to different subsets of the data , and are thus discriminative in segmenting the data into different clusters .",
    "the average tuple consistency , on the other hand , demonstrates that the sampled tuples do occur in large chunks of the data , demonstrating that the induced clusters are of significant size .",
    "the clarity measure shows that the clusters made of texts retrieved using different tuples have clear differentiation from the rest of the dataset in usage of all the words in the dictionary .",
    "we report results for the @xmath89 grids and the best result of lda and ctm which peaked respectively at 80 and 60 topics .",
    "results for other grid sizes can be found in the additional material ; they are stable across complexities with slightly better performances for larger grids .",
    "+ all grid models show good consistency of words selected as they are optimized so that documents words map into overlapping windows . through positioning and intersection of many related documents",
    "the words end up being arranged in a fine - grained manner so as to reflect their higher - order co - occurrence statistics .",
    "hierarchical learning greatly improved the results despite the fact that hccg and hcg can be reduced to ( c)cgs through marginalization ( [ eq : collapse ] ) .",
    "+ overall hccg strongly outperformed all the methods , especially with a total gain of 0.5 bits on clarity , which is around third of the score for lda / ctm . despite allowing for correlated topics that enable ctm to learn larger topic models , ctm trails lda in these graphs as topics were over expanded .",
    "we also considered non - parametric topic models such as `` dilan '' @xcite and the hierarchical dirichlet process @xcite but their best results were poor and we did not reported them in the figure .",
    "to get an idea , both models only indexed 25% of the content after 5000 2-tuples samples and had a clarity lower of 0.7 - 1.2 bits than other topic models .",
    "[ fig : intrusion ]    [ [ human - judgments - of - topic - coherence ] ] human judgments of topic coherence : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we next tested the quality of the inferred topics . topic coherence is often measured based on co - occurrence of the top @xmath90 words per topic . while good as a quick sanity check of a single learned model , when this measure is used to compare models , it will favor models that lock onto top themes and distribute the rest of the words in the tails of the topic distributions .",
    "the lda models usually have a large drop off in topic coherence when the number of topics is increased to force the model to address more correlations in the data . indeed ,",
    "using this measure , lda topics outperform cg topics in case of small models . but",
    "as the number of topics grows , the microtopics trained by hcg significantly outperform both lda and cg ( see the appendix ) .",
    "a more interesting measure of topic quality , which not only depends on individual topic coherence but also on meaningful separation of different topics , requires human evaluation of _ word intrusions_. in a word intrusion task @xcite , six randomly ordered words are presented to a human subject who then guesses which word is an outlier . in the original procedure a target topic is randomly selected and then the five words with _",
    "highest _ probability are picked .",
    "then , an intruder is added to this set .",
    "it is selected at random from the low probability words of the target topic that have high probability in some other topic . finally the six words are shuffled and presented to the subject .",
    "if the target topic shows a lack of coherence or distinction from the intruding topic , the subject will often fail to correctly identify the intruder .",
    "this task is again geared towards only getting the top words right in a topic model and ignoring the rest of the distribution , which makes it unsuitable to comparison with microtopic models which attempt to extract much more correlation from the data .",
    "thus instead of picking the top words from each topic , we sampled the words from the target topic to create the in - group . after sampling the location of a microtopic from the grid @xmath78 , we picked three randomly chosen words from @xmath79 or from the small groups of microtopics in the window of size 2@xmath02 , and 3@xmath03 around @xmath78 ( the latter is equivalent to computing the window distributions @xmath27 using windows of smaller size than the ones used in training and should give us the indication if the granularity assumed in the window size was exaggerated : if it is then averaging of nearby topics should significantly reduce the noise due to forced topic splitting ) .",
    "for each of these groups we choose the intruder word using the standard procedure .",
    "if in this harder task humans can identify intruders better for microtopic models than for lda models , this would indicate that the microtopics are not simply random subsamples of broader topics captured in @xmath27 and similar in entropy to lda topics .",
    "they would be a meaningful breakup of broad topics into finer ones .",
    "we compared lda ( known to performed better than ctm on intrusion tasks @xcite ) , hcg , and hccg , on randomly crawled 10k wikipedia articles and used amazon mechanical turk ( 24000 completed tasks from 345 different people ) .",
    "the trained grids were of size 32 @xmath0 32 and the windows 5 @xmath0 5 .",
    "the optimal lda size was chosen using likelihood crossvalidation over the range of complexities as in the previous experiments ( the peak performance there was at 80 topics ) .",
    "results are shown in fig.[fig : intrusion ] as a function of the euclidean distance on the grid of the intruder word from the topic .",
    "hccg outperformed lda ( p - values for the 3 tasks 1.20e-11 , 1.88e-5 , 2.97e-05 ) and hcg (",
    "p - values for the 3 tasks 3.97e-18 , 1.01e-11 , 3.14e-19 ) indicating that learning microtopics is possible with a good algorithm .",
    "overall , _ users were able to solve correctly 71% of hccg problems and only 58% of lda problems_. interestingly , the performance of hccg and hcg does not seem to depend on the distance of the intruder word : even picking intruder word from a very close location rather than from a far away one lead to no additional confusion for the user .",
    "this shows that hccg chops up the data into meaningful microtopics which are then combined into a large number of groups @xmath27 that do not over broaden the scope .",
    "hccg and hcg also outperformed respectively cg and ccg ( see the appendix ) .",
    "[ [ learning - to - separate - mixed - digits . ] ] learning to separate mixed digits .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    finally , we show that an hccg model can be used to perform a task that eludes most unsupervised _ and _ supervised models .",
    "we created a set of 10000 @xmath91 images , each containing two different mnist digits overlapped , fig .",
    "[ fig : digitmix ] .",
    "we trained an hccg model consisting of five @xmath89 layers on this data stagewise by feeding @xmath92 from one layer to the next .",
    "windows of size 5 @xmath0 5 were used in all layers . from layer to layer , the new representations of the image consist of growing combinations of low level features @xmath27 from the bottom layer ( sparseness of which is similar to fig .",
    "[ fig : digits2]a ) .",
    "the hierarchical grouping is further encouraged by simply smoothing @xmath93 with a @xmath94 gaussian kernel with deviation of 0.75 , before feeding it to the next layer ( this is motivated by the fact that nearby features in @xmath27 are related and so if two distant locations should be grouped , so should those locations neighbors ) . once the model is collapsed to a single hccg grid the components no longer look like short strokes but like whole digits , mostly free of overlap : the model has learned to approximately separate the images into constitutive digits .",
    "reasoning on overlapping digits even eludes deep neural networks trained in a supervised manner , but here we did not use the information about which two digits are present in each of the training images .",
    "we show that with new learning algorithms based on a hierarchy of ccg models , possibly terminated on the top with a cg , it is possible to learn large grids of sparse related microtopics from relatively small datasets .",
    "these microtopics correspond to intersections of multiple documents , and are considerably narrower than what traditional topic models can achieve without overtraining on the same data . yet , these microtopics are well formed , as both the numerical measures of consistency , diversity and clarity and the user study on 345 mechanical turkers show .",
    "another approach to capturing sparse intersections of broader topics is through product of expert models , e.g. rbms @xcite , which consist of relatively broad topics but model the data through intersections rather than admixing .",
    "rbms are also often stacked into deep structures . in future work",
    "it would be interesting to compare these models , though the tasks we used here would have to be somewhat changed to focus on the intersection modeling , rather than the topic coherence ( as this is not what rbm topics are optimized for ) .",
    "hccg and hcg models have a clear advantage in that it is easy to visualize how the data is represented , which is useful both to end users in hci applications , and to machine learning experts during model development and debugging .",
    "another parallel between the stacks of ccgs and other deep models is that the uniform connectivity of units is directly enforced through window constraints , rather than encouraged by dropout .",
    "finally , in this specific context we illustrate a broader phenomenon that requires more methodical and broader treatment by the machine learning community .",
    "a more complex ( deeper ) model showed here large advantages in terms of training likelihood , but these advantages were _ not _ due to the expanded parameter space , because the resulting model is equivalent to a collapsed single layer model .",
    "rather than being a reflection of increased representational abilities of the model , better likelihoods were thus the result of better fitting algorithm that consists of training a deep model ( and then collapsing it into a simpler but equivalent parameterization ) .",
    "similar phenomena are likely regularly encountered elsewhere in machine learning , but not always recognized as such , as in the absence of the full knowledge of the extrema of the fitting criterion , an increase in performance is often inappropriately ascribed to better modeling rather than better model fitting ."
  ],
  "abstract_text": [
    "<S> the counting grid is a grid of _ microtopics _ , sparse word / feature distributions . </S>",
    "<S> the generative model associated with the grid does not use these microtopics individually , but in predefined groups which can only be ( ad)mixed as such . </S>",
    "<S> each allowed group corresponds to one of all possible overlapping rectangular windows into the grid . </S>",
    "<S> the capacity of the model is controlled by the ratio of the grid size and the window size . </S>",
    "<S> this paper builds upon the basic counting grid model and it shows that hierarchical reasoning helps avoid bad local minima , produces better classification accuracy and , most interestingly , allows for extraction of large numbers of coherent microtopics even from small datasets . </S>",
    "<S> we evaluate this in terms of consistency , diversity and clarity of the indexed content , as well as in a user study on word intrusion tasks . </S>",
    "<S> we demonstrate that these models work well as a technique for embedding raw images and discuss interesting parallels between hierarchical cg models and other deep architectures . </S>"
  ]
}