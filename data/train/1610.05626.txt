{
  "article_text": [
    "the weak galerkin ( wg ) finite element methods ( fem ) refer to a new class of finite element discretizations for solving partial differential equations ( pde ) . in the wg methods ,",
    "classical differential operators are replaced by generalized differential operators as distributions . unlike the classical fem that impose continuity in the approximation space ,",
    "the wg methods enforce the continuity weakly in the formulation using generalized weak derivatives and parameter - free stabilizers .",
    "the wg methods are naturally extended from the standard fem , and are more advantageous over fem in several aspects .",
    "for instance , high order wg spaces are usually more convenient to construct than conforming fem spaces since there is no continuity requirement on the approximation spaces .",
    "also , the relaxation of the continuity requirement enables easy implementation of wg methods on polygonal meshes .",
    "the first wg method was introduced in @xcite for the second - order elliptic equation , in which the @xmath0 finite elements such as raviart thomas elements are used to approximate weak gradients . later in @xcite , wg methods following the stabilization approach were introduced , which can be applied on polygonal meshes .",
    "this new stabilized wg discretization has been applied to many classical pde models , such as elliptic interface problems @xcite , the maxwell equation @xcite , brinkman equation @xcite , and biharmonic equation @xcite .",
    "it is well known that superconvergence is an important and desirable mathematical property of numerical methods for solving pde .",
    "superconvergence phenomenon means the convergence rate at certain points is higher than the optimal global convergence rate of numerical solutions . due to its wide application",
    ", superconvergence has been extensively studied in the past decades , see for example @xcite .",
    "there are also some literature on superconvergence analysis for wg methods .",
    "for instance , in @xcite , the error estimate revealed a superconvergence for the wg approximation ( without stabilization terms ) on simplicial meshes . in @xcite ,",
    "superconvergence of the wg methods with stabilizers are obtained by @xmath1 projection methods .",
    "one goal of this article is to analyze the supercloseness property of a class of wg methods with generalized stabilizers . unlike the stabilizer introduced in @xcite ,",
    "there is a fine - tune parameter in our new stabilizer , and it reduces to the standard stabilizer when the parameter @xmath2 .",
    "we will show that this new parameter plays a critical role in the analysis for supercloseness . to be more specific ,",
    "we show that the new wg solutions are superclose to a lagrange type interpolation of the exact solution .",
    "another focus of this article is to develop an efficient post - processing technique of wg methods which leads to a better approximation of the gradient of solution .",
    "we adopt the polynomial preserving recovery ( ppr ) technique @xcite in our post - processing .",
    "the main idea of ppr is to construct a higher - order polynomial locally around each node based on current numerical solution . unlike the standard fem approximation which is a continuous function ,",
    "wg solution is discontinuous across the boundary of elements ; hence , there can be multiple values associated with a single node .",
    "consequently , we will need to introduce an appropriate weighted average to unify these values before applying the standard ppr scheme .",
    "the analysis of superconvergence of ppr scheme relies heavily on the aforementioned supercloseness property .",
    "the rest of the paper is organized as follows . in section 2",
    ", we introduce the definition of weak functions / derivatives , and present the wg method for the model second order elliptic equation . in section 3 , we describe a lagrange type interpolation operator which is used in the supercloseness analysis . in section 4 ,",
    "we present the error estimation for supercloseness .",
    "section 5 is devoted to the construction of the ppr operator for wg solutions . in section 6 ,",
    "we present the superconvergence analysis for ppr scheme . in section 7 , we provide some numerical experiments .",
    "in this paper , we consider the following second - order elliptic problem with homogeneous dirichlet boundary condition as a model problem : @xmath3 where @xmath4 is an open rectangular domain or a union of rectangular domains .",
    "the weak formulation for ( [ model ] ) can be written as : find @xmath5 such that @xmath6 where @xmath7 is the @xmath1-inner product , and @xmath8 is a subspace of sobolev space @xmath9 ^ 2\\}$ ] with vanishing boundary value .",
    "let @xmath10 be a shape - regular rectangular mesh of domain @xmath11 .",
    "for each element @xmath12 , denote by @xmath13 the diameter of @xmath14 .",
    "the mesh size of @xmath15 is defined as @xmath16 .",
    "denote by @xmath17 the set of all edges in @xmath18 and @xmath19 the set of all interior edges in @xmath18 .",
    "let @xmath20 be a set of polynomials that the degrees of @xmath21 and @xmath22 are no more than @xmath23 , and let @xmath24 define the space of weak functions on every element @xmath14 by @xmath25 note that @xmath26 and @xmath27 are completely independent .",
    "@xcite denote by @xmath28 the weak gradient of @xmath29 as a linear functional of the sobolev space @xmath30 ^ 2 : \\nabla\\cdot \\bq\\in l^2(t)\\}$ ] .",
    "that is the action on any function @xmath31 is given by @xmath32 where @xmath33 is the unit outward normal vector on @xmath34 .",
    "next we define the space @xmath35 to be @xmath36^t,\\end{aligned}\\ ] ] where @xmath37 is a set of polynomials whose degrees of @xmath21 and @xmath22 are no more than @xmath38 and @xmath39 , respectively .",
    "the discrete weak gradient operator of @xmath29 , denoted by @xmath40 , is the unique function in @xmath35 , satisfying @xmath41 where @xmath33 is the unit outward normal vector on @xmath34 .",
    "let @xmath42 and @xmath43 be the global wg spaces of weak functions and weak gradients as follows @xmath44 note that any weak function @xmath45 in @xmath42 has a single - valued component @xmath27 on each edge @xmath46 .",
    "let @xmath47 be the subspace of @xmath42 with vanishing boundary value on @xmath48 .    for each @xmath49 ,",
    "the discrete weak gradient @xmath50 is computed piecewisely using ( [ weak_grad ] ) on each element @xmath51 , i.e. , @xmath52 for simplicity , we drop the subscript @xmath23 from the notation @xmath53 in the rest of the paper .",
    "define the following bilinear forms @xmath54 where @xmath55 .",
    "the functional @xmath56 defined by @xmath57 is a norm on the space @xmath47 .",
    "moreover , @xmath58    it is easy to see that @xmath59 is a semi - norm in @xmath47 .",
    "hence , it suffices to show that @xmath60 whenever @xmath61 . using and we have @xmath62 that is @xmath63 on each @xmath64 , and",
    "@xmath65 on each @xmath46 .",
    "it follows from @xmath65 that @xmath66 for any @xmath67 and @xmath33 is the outward normal of @xmath34 .",
    "thus @xmath68 on each @xmath64 , and @xmath26 is a constant on each @xmath14 . together with @xmath65 , we conclude that @xmath45 is a constant on the global domain @xmath11 .",
    "the fact @xmath69 implies @xmath60 . as a result",
    ", @xmath59 is a norm in space @xmath47 .    for any @xmath70",
    ", it follows from the definition of weak gradient , the trace inequality , the inverse inequality , and the assumption @xmath71 that @xmath72 we obtain ( [ prop_norm_1 ] ) .",
    "the inequality ( [ prop_norm_2 ] ) follows from that @xmath73 is small and @xmath74 .",
    "we consider the following weak galerkin method : find @xmath75 such that @xmath76 where @xmath77 .",
    "the difference between the wg method and the classical wg method in @xcite is that the stabilizer contains a fine - tune parameter @xmath78 .",
    "later on , it will be shown that the parameter @xmath78 plays an important role in the supercloseness analysis in section 4 .",
    "numerical experiments in section 7 also demonstrate this feature .",
    "this section introduces an interpolation operator that will be used later in the superconvergence analysis .",
    "let @xmath79 be @xmath80 lobatto points on the reference interval @xmath81 $ ] , which are @xmath80 zeros of the lobatto polynomial @xmath82 .",
    "we define a lagrange interpolation operator @xmath83 such that @xmath84 where @xmath85 , @xmath86 are the lagrange interpolation associated with lobatto points @xmath87",
    ". the following properties of @xmath85 can be easily verified : @xmath88 we also recall an interpolation error representation in @xcite .",
    "[ interlemma ] let @xmath89 , we have the following error equation @xmath90 where c is a constant , @xmath82 is the lobatto polynomial with order @xmath80 , and @xmath91    as shown in lemma [ interlemma ] , the interpolation operator @xmath92 preserves polynomials of degree up to @xmath23 .",
    "we composite the interpolation operators ( [ inter_opera_1d ] ) in @xmath21- and @xmath22- directions to obtain an interpolation operator in the two dimensional domain @xmath93 such that @xmath94 where @xmath95 are the interpolation operators in @xmath21- , @xmath22- directions , respectively . from , it is easy to prove @xmath96 . by lemma [ interlemma ]",
    "we have the following estimates .",
    "@xcite there exists a constant @xmath97 such that for any @xmath98 , the following inequality holds true @xmath99    the definition of @xmath100 given in ( [ weak_grad ] ) and the fact that @xmath101 yield the following lemma .    the interpolation operator defined in ( [ interplation ] )",
    "satisfies @xmath102 where @xmath55 .",
    "in this section , we derive an error estimate for @xmath103 , where @xmath104 is the solution of the wg method ( [ wg_algorithm ] ) and @xmath105 is the interpolation of the exact solution of problem ( [ model ] ) .",
    "[ order_est ] let @xmath98 be the solution of ( [ model ] ) , and @xmath106 be the solution of wg method ( [ wg_algorithm ] ) .",
    "the following error estimate holds @xmath107    since @xmath108 , then @xmath109 is well - defined .",
    "multiplying both sides of ( [ model ] ) by @xmath26 , and using integration by parts , we have @xmath110 here we use the facts that the normal component @xmath111 of the flux is continuous on all interior edges and @xmath112 .    from ( [ wg_algorithm ] ) , ( [ continuous_prop ] ) , ( [ prop_u ] ) , the cauchy - schwarz inequality , ( [ estimates_inter ] ) , ( [ prop_norm_1 ] ) , the property of interpolation operator @xmath113 , and @xmath114 we obtain @xmath115 here , we have used the fact that @xmath116 .",
    "the estimate shows that the wg solution @xmath104 is superclose to the interpolation @xmath105 when @xmath117 .",
    "it reaches the maximum rate of convergence when @xmath118 . further increasing the value of @xmath78 will not improve the rate of convergence .",
    "in this section , we introduce a gradient recovery operator @xmath119 onto space @xmath120 , with @xmath121 , on the rectangular mesh @xmath10 . for a wg solution @xmath104 in ( [ wg_algorithm ] )",
    ", we define @xmath122 on the following three types of mesh nodes @xcite : vertex , edge node , and internal node , see fig .",
    "[ fig : three_nodes ] .",
    "we define a patch @xmath123 for every vertex @xmath124 by @xmath125 be the union of the elements in the first layer around @xmath124",
    ". there can be two types of vertices .",
    "the first type is the interior vertex @xmath126 , and the other one is the boundary vertex @xmath127 , see fig .",
    "[ fig : two_vertiese ] for an illustration .    @xmath128@xmath128    before we introduce the ppr scheme , we need to clarify some notations .",
    "* @xmath129 : all nodes in @xmath130 .",
    "they could be vertices , edge nodes , or internal nodes .",
    "* @xmath131 : all mesh nodes in @xmath132 . * @xmath133 : @xmath134 is the set of all mesh nodes in @xmath135 . here , @xmath136 is the number of the nodes . for the linear element",
    "all nodes are vertices . for quadratic and higher - order elements , there are vertices , edge nodes , and internal nodes . * @xmath137 : all interior vertices in @xmath11 .",
    "* @xmath138 : all interior vertices in @xmath139 .",
    "* @xmath140 : @xmath141 is the set of all interior vertices in @xmath135 . denoted by @xmath142 the number of nodes in @xmath143 .      in order to obtain the recovered gradient @xmath145",
    ", we need to use the values of @xmath104 at mesh nodes in @xmath133 to get an approximation @xmath146 in the least - square sense . however , on a vertex or an edge node , the wg solution @xmath104 may have more than one value , as illustrated in fig .",
    "[ fig : edge_node ] . as a result , we must redefine the value of @xmath104 at those nodes .",
    "@xmath147    for any node @xmath148 , denote by @xmath149 the possible values for @xmath104 at @xmath150 where @xmath151 is the number of these values .",
    "note that @xmath152 might be the value of the interior part @xmath153 or the boundary part @xmath154 of the weak function @xmath155 at point @xmath150 .",
    "we define a function @xmath144 such that the value of @xmath144 at @xmath150 is given by @xmath156 moreover , we require @xmath157 to be a function satisfying @xmath158 where @xmath85 is the lagrange basis associated with @xmath150 .",
    "it can be proved that the function @xmath144 satisfies the following lemma .",
    "[ jump_comb ] given @xmath159 , let @xmath144 be defined as ( [ refor_uh])-([req_baru_1 ] ) .",
    "assume that @xmath160 is an interior vertex , @xmath161 is the patch for @xmath150 , and @xmath134 is the set of the nodes in @xmath162 , where @xmath136 is the number of the elements in @xmath133 .",
    "then for @xmath163 , @xmath164 , the following properties hold .",
    "a.   @xmath165 can be written as the jump of @xmath104 at @xmath166 , if @xmath167 is a vertex or an edge node on @xmath34 , b.   @xmath168 , if @xmath167 is an internal mesh node in @xmath14 .    without loss of generality , we consider an interior vertex @xmath169 .",
    "assume that @xmath170 , @xmath171 are the values of @xmath104 at @xmath169 , see the left plot in fig .",
    "[ fig : edge_node ] .",
    "let @xmath172 and @xmath173 .",
    "then , we have @xmath174 this shows that @xmath175 consists of the jump of @xmath104 at @xmath169 .",
    "furthermore , it can be written as @xmath176 where @xmath153 and @xmath154 share the edge @xmath177 and @xmath169 lies on the edge @xmath177 .    for boundary vertices and edge nodes ,",
    "the proof is similar . for internal nodes ,",
    "the property @xmath178 follows directly from the definition of @xmath144 .",
    "recall that the function @xmath144 is defined to have a unifed value at each node .",
    "therefore we can apply ppr scheme to construct the gradient recovery operator @xmath119 .",
    "we consider the following four cases .    * case 1 .",
    "* for each interior vertex @xmath179 , we fit a polynomial in @xmath180 to the redefined wg solution @xmath181 by the least - square method .",
    "let @xmath182 be the local coordinates with respect to the origin @xmath150 .",
    "the fitting polynomial is defined as @xmath183 where @xmath184 with @xmath185 and @xmath186 , and @xmath187 is the number of the basis of @xmath180 . by the least - square method , the vector @xmath188 can be solved from @xmath189 where @xmath190 and @xmath191 where @xmath192 is the coordinates of @xmath166 in the reference domain .",
    "define @xmath122 at the point @xmath150 as @xmath193    * case 2 .",
    "* for a boundary vertex @xmath194 , we define @xmath195 where @xmath142 is the number of interior vertices in @xmath140 and @xmath196 is the local coordinates of @xmath150 with @xmath166 be the origin .",
    "* case 3 . * for an edge node @xmath150 which lies on an edge between vertices @xmath169 and @xmath197 , we define @xmath198 where @xmath199 and @xmath200 are the coordinates of @xmath201 with respect to the origins @xmath169 and @xmath197 , respectively . the weight @xmath78 is determined by the ratio of the distances of @xmath150 to @xmath169 and @xmath197 , that is @xmath202 , see fig .",
    "[ fig : internal_nodes ] ( a ) .",
    "* case 4 . * for an internal node @xmath150 which lies in an element formed by vertices @xmath169 , @xmath197 , ... , @xmath203 , we define @xmath204 where @xmath196 is the local coordinates of @xmath201 with respect to the origin @xmath166 . the weight @xmath205 is determined by the space ratio of the opposite patch to @xmath166 , that is @xmath206 , and @xmath207 , see fig .",
    "[ fig : internal_nodes ] ( b ) .    for any @xmath106",
    ", @xmath122 is defined as the linear combination of the values of @xmath122 at the interior vertex . for @xmath208 , we define @xmath209 by @xmath210 where @xmath211 is the interpolation operator given in ( [ interplation ] ) .",
    "in this section , we report several properties of the operator @xmath119 , and analyze the superconvergence between @xmath212 and @xmath122 .",
    "the following lemma can be directly verified following the same procedure as lemma 3.10 in @xcite .",
    "[ lem3.10 ] let @xmath160 be an interior vertex with the patch @xmath161 , and let @xmath213 be the least square polynomial of the function @xmath214 in the patch @xmath161 . then there is a constant @xmath97 such that @xmath215    by the definition given in subsection [ ppr - operator ] , @xmath119 is a polynomial - preserving operator which satisfies the following lemma .    the gradient recovery operator @xmath119 satisfies the following properties @xmath216 where @xmath97 is a constant and @xmath157 satisfying ( [ refor_uh])-([req_baru_1 ] ) is the redefined function of @xmath104 .",
    "the following lemma provides an important tool in establishing our main result .    for @xmath106 ,",
    "the following property holds true @xmath217 where @xmath157 satisfying ( [ refor_uh])-([req_baru_1 ] ) is the redefined function of @xmath104 .",
    "we will prove in three steps .",
    "* step 1*. for any @xmath64 , recall that @xmath218 denotes the set of the interior vertices in @xmath219 . then , from the definition of @xmath119 , we have @xmath220 using lemma [ lem3.10 ] , we have @xmath221 it follows that @xmath222    * step 2*. define the auxiliary function @xmath223 as @xmath224 for any interior vertex @xmath160 , it follows from the definition of @xmath144 and @xmath153 that @xmath223 is a piecewise polynomial on @xmath161 .",
    "then from the triangle inequality we have @xmath225 it follows from ( [ prop_norm_1 ] ) that @xmath226    * step 3*. we shall prove @xmath227 .",
    "let @xmath228 , where @xmath229 are the lagrange bases .",
    "let @xmath230 be the affine function for @xmath231 on the reference domain . note that @xmath232 is uniformly bounded ,",
    "then we obtain @xmath233 let @xmath234 and @xmath235_e$ ] be the jump of @xmath104 over @xmath177 . from lemma [ jump_comb ] , we know that the values of @xmath236 on the mesh nodes on @xmath237 is the combination of the jump of @xmath104 on edges @xmath238 , the values of @xmath236 on the internal mesh nodes in @xmath239 are zeros . using the inverse inequality @xmath240 and ( [ prop_norm_2 ] ) , we obtain @xmath241_e|^2_{\\infty , e } \\leq c\\sum_{e\\in\\mathcal{e}(t_1)}h^{-1}|[u_h]_e|^2_{0,e } \\\\ & & \\leq c\\sum_{t\\in \\mathcal{t}_h(t_1)}h^{-1}\\langle u_0-u_b , u_0-u_b\\rangle_{\\partial t } \\leq   c\\sum_{t\\in \\mathcal{t}_h(t_1)}\\3baru_h\\3bar^2_t,\\end{aligned}\\ ] ] where @xmath242 . for other three elements @xmath243",
    ", the proof can be finished similarly . finally , combining , , and",
    ", we have @xmath244    now we are ready to state our main result for superconvergence .",
    "let @xmath98 be the solution of ( [ model ] ) and @xmath106 be the solution of ( [ wg_algorithm ] ) .",
    "let @xmath245 be the recovered gradient by ppr introduced in section 5.3 .",
    "then we have the following error estimate @xmath246    it follows from ( [ inq_gh_3 ] ) , ( [ g_hi_h ] ) , ( [ inq_gh_2 ] ) , ( [ inq_gh_4 ] ) , and ( [ superclose ] ) that @xmath247 which completes the proof .",
    "the estimate shows that the gradient recovery @xmath122 is superconvergent to @xmath212 when @xmath71 . as the value of @xmath78 increases , the convergence rate will also increase , and it reach the maximum rate of convergence @xmath80 when @xmath248 .",
    "in this section , we present some numerical examples to demonstrate the convergence of wg methods and the ppr recovery .",
    "we test our algorithm for the @xmath249 and @xmath250 elements , and choose different stabilizing parameters in our numerical algorithms for comparison .",
    "we focus on @xmath251 , the error between the wg solution and its lagrange interpolation in the energy norm , and @xmath252 , the error between the ppr recovered gradient and the true gradient in the @xmath1 norm .      in this example",
    ", we consider the problem ( [ model ] ) in the unit square @xmath254 , and use a family of uniform cartesian meshes .",
    "the weak galerkin space is given by @xmath255 the discrete weak gradient @xmath28 on each element @xmath64 is defined as @xmath256^t.\\ ] ] the right - hand side function @xmath257 is chosen such that the exact solution is @xmath258 tables [ sinsink=1uni ] and [ sinsink=1unippr ] report the convergence rates of @xmath251 and @xmath252 , respectively .",
    "different values of the stabilizing parameter @xmath78 have been tested . here",
    "the parameter @xmath259 denotes the number of rectangles in each direction .",
    "table [ sinsink=1uni ] clearly demonstrates that the convergence rate is @xmath260 , which is consistent with the error estimates .",
    "table [ sinsink=1unippr ] indicates the superconvergence behavior of the ppr recovery .",
    "we note that for @xmath261 , the numerical results seem to be even better than our theoretical analysis .",
    "we note that for @xmath253 , our superconvergence analysis requires the exact solution to be in @xmath263 .",
    "data in tables [ multik=1uni]-[multik=1unippr ] demonstrate that the convergence orders perfectly match or are even better than orders in our theoretical analysis . for higher order approximation @xmath264 , to get the analytical superconvergence order , we need the exact solution to be in @xmath265 . however , the exact solution here is barely in @xmath263 .",
    "hence , some superconvergence behavior does not exist , which is reflected in tables [ multik=2uni]-[multik=2unippr ] .    * a final remark*. the condition regarding @xmath78 is sharp in the supercloseness result ( 4.1 ) .",
    "as we can see from data in tables 7.1 , 7.3 , 7.5 , and 7.7 , the convergent rate follows loyally to the predicted @xmath266 . on the other hand",
    ", the condition regarding @xmath78 may not be necessary for our superconvergence result in theorem 6.4 as we can see from data in tables 7.2 , 7.4 , 7.6 , and 7.8 : when @xmath267 , the supercloseness lost but the superconvergence still exists , since the supercloseness result ( 4.1 ) is a sufficient condition for theorem 6.4 , not a necessary condition .",
    ", _ a superconvergence result for the approximate solution of the heat equation by a collocation method _ , the mathematical foundations of the finite element method with applications to partial differential equations , academic press , new york , 1972 , pp ."
  ],
  "abstract_text": [
    "<S> in this paper , we analyze the convergence and supercloseness properties of a class of weak galerkin ( wg ) finite element methods for solving second - order elliptic problems . </S>",
    "<S> it is shown that the wg solution is superclose to the lagrange type interpolation using lobatto points . </S>",
    "<S> this supercloseness behavior is obtained through some newly designed stabilization terms . a post - processing technique using the polynomial preserving recovery ( ppr ) </S>",
    "<S> is introduced for the wg approximation . </S>",
    "<S> superconvergence analysis is carried out for the ppr approximation . </S>",
    "<S> numerical examples are provided to verify our theoretical results .    </S>",
    "<S> supercloseness , superconvergence , polynomial preserving recovery , weak galerkin method .    </S>",
    "<S> primary , 65n30 , 65n15 , 65n12 ; secondary , 35j50 , 35b45 </S>"
  ]
}