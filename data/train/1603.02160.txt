{
  "article_text": [
    "a large class of popular and successful machine learning methods rely on kernels ( positive semidefinite functions ) , including support vector machines , kernel ridge regression , kernel pca @xcite , gaussian processes @xcite , and kernel - based hypothesis testing @xcite . a key component for many of these methods",
    "is that of estimating kernel mean embeddings and covariance operators of probability measures based on data .",
    "the use of simple empirical estimators has been challenged recently @xcite and alternative , better - behaved frequentist shrinkage strategies have been proposed . in this article , we develop a bayesian framework for estimation of kernel mean embeddings , recovering desirable shrinkage properties as well as allowing quantification of full posterior uncertainty .",
    "moreover , the developed framework has an additional extremely useful feature .",
    "namely , a persistent problem in kernel methods is that of kernel choice and hyperparameter selection , for which no general - purpose strategy exists . when a large dataset is available in a supervised setting , the standard approach is to use cross - validation",
    "however , in unsupervised learning and kernel - based hypothesis testing , cross - validation is not straightforward to apply and yet the choice of kernel is critically important . our framework gives a tractable closed - form marginal pseudolikelihood of the data allowing direct hyperparameter optimization as well as fully bayesian posterior inference through integrating over the kernel hyperparameters .",
    "we emphasise that this approach is fully unsupervised : it is based solely on the modelling of kernel mean embeddings  going beyond marginal likelihood based approaches in , e.g. , gaussian process regression  and is thus broadly applicable in situations , such as kernel - based hypothesis testing , where the hyperparameter choice has thus far been mainly driven by heuristics .    in section [ section : background ]",
    "we provide the necessary background on reproducing kernel hilbert spaces ( rkhs ) as well as describe some related works . in section [ section : bke ]",
    "we develop our bayesian kernel embedding model , showing a rigorous gaussian process prior formulation for an rkhs . in section [ section : bkl ]",
    "we show how to perform kernel learning and posterior inference with our model . in section [ section",
    ": experiments ] we empirically evaluate our model , arguing that our bayesian kernel learning ( bkl ) objective should be considered as a `` drop - in '' replacement for heuristic methods of choosing kernel hyperparameters currently in use , especially in unsupervised settings such as kernel - based testing . we close in section [ section : discussion ] with a discussion of various applications of our approach and future work .",
    "for any positive definite kernel function @xmath0 , there exists a unique reproducing kernel hilbert space ( rkhs ) @xmath1 .",
    "rkhs is an ( often infinite - dimensional ) space of functions @xmath2 where evaluation can be written as an inner product , and in particular @xmath3 for all @xmath4 .",
    "given a probability measure @xmath5 on @xmath6 , its kernel embedding into @xmath1 is defined as : @xmath7 embedding @xmath8 is an element of @xmath1 and serves as a representation of @xmath5 akin to a characteristic function .",
    "it represents expectations of rkhs functions in the form of an inner product @xmath9 . for a broad family of kernels termed _ characteristic _ @xcite ,",
    "every probability measure has a unique embedding  thus , such embeddings completely determine their probability measures and capture all of the moment information .",
    "this yields a framework for constructing nonparametric hypothesis tests for the two - sample problem and for independence , which are consistent against all alternatives @xcite  we review this framework in the next section .      given a kernel @xmath10 and probability measures @xmath5 and @xmath11 , the maximum mean discrepancy ( mmd ) between @xmath5 and @xmath11 @xcite",
    "is defined as the squared rkhs distance @xmath12 between their embeddings .",
    "a related quantity is the hilbert schmidt independence criterion ( hsic ) @xcite , a nonparametric dependence measure between random variables @xmath13 and @xmath14 on domains @xmath6 and @xmath15 respectively , defined as the squared rkhs distance @xmath16 between the embeddings of the joint distribution @xmath17 and of the product of the marginals @xmath18 with respect to a kernel @xmath19 on the product space .",
    "typically , @xmath20 factorises , i.e. @xmath21 . the empirical versions of mmd and hsic are used as test statistics for the two - sample ( @xmath22 vs. @xmath23 ) and independence ( @xmath24 vs. @xmath25 ) tests , respectively . with the help of the approximations to the asymptotic distribution under the null hypothesis ,",
    "corresponding p - values can be computed @xcite .",
    "in addition , the so - called `` witness function '' which is proportional to @xmath26 can be used to assess where the difference between the distributions arises .      for a set of i.i.d .",
    "samples @xmath27 , the kernel mean embedding is typically estimated by its empirical version @xmath28 from which various associated quantities , including the estimators of the squared rkhs distances between embeddings needed for kernel - based hypothesis tests , follow . as an empirical mean in an infinite - dimensional space ,",
    "is affected by stein s phenomenon , as overviewed by @xcite who also propose alternative shrinkage estimators similar to the well known james - stein estimator .",
    "improvements of test power using such shrinkage estimators are reported by @xcite .",
    "connections between the james - stein estimator and empirical bayes procedures are classical @xcite , and thus a natural question to consider is whether a bayesian formulation of the problem of kernel embedding estimation would yield similar shrinkage properties . in this paper",
    ", we will give a bayesian perspective of the problem of kernel embedding estimation .",
    "in particular , we will construct a flexible model for underlying probability measures based on gaussian measures in rkhss which allows derivation of a full posterior distribution of @xmath8 , recovering similar shrinkage properties to @xcite , as discussed in section [ section : relation_shrinkage ] .",
    "the model will give us a further advantage , however  as the marginal likelihood of the data given the kernel parameter can be derived leading to an informed choice of kernel parameters .      in supervised kernel methods like support vector machines ,",
    "leave - one - out or k - fold crossvalidation is an effective and widely used method for kernel selection , and the myriad papers on multiple kernel learning ( e.g.  @xcite ) assume that some loss function is available and thus focus on effective ways of learning combinations of kernels . in the related but distinct world of smoothing kernels and kernel density estimation , there are a variety of long - standing approaches to bandwidth selection , again based on a loss function ( in this case , mean integrated squared error is a popular choice @xcite , and there is even a formula giving the optimal smoothing parameter asymptotically , see @xcite ) but we are not aware of work linking this literature to methods based on positive definite / rkhs kernels we study here . separately",
    ", gaussian process learning can be undertaken by maximizing the marginal likelihood , which has a convenient closed form .",
    "this is noteworthy for its success and general applicability even for learning complicated combinations of kernels @xcite or rich kernel families @xcite .",
    "our approach has the same basic design as that of gaussian process learning , yet it is applicable to learning kernel embeddings , which falls outside the realm of supervised learning .",
    "as noted in @xcite , the choice of the kernel @xmath10 is critically important for the power of the tests presented in section  [ section : mmd - hsic - kpca ] .",
    "however , no general , theoretically - grounded approaches for kernel selection in this context exist .",
    "the difficulty is that , unlike in supervised kernel methods , a simple cross - validation approach for the kernel parameter selection is not possible .",
    "what would be an ideal objective function  asymptotic test power  can not be computed due to a complicated asymptotic null distribution . moreover , even if we were able to estimate the power by performing tests on `` training data '' for each of the individual candidate kernels , in order to account for multiple comparisons , this training data would have to be disjoint from the one on which the hypothesis test is performed , which is clearly wasteful of power and appropriate only in the type of large - scale settings discussed in @xcite .",
    "for these reasons , most users of kernel hypothesis tests in practice resort to using a parameterized kernel family such as squared exponential , and setting the lengthscale parameter based on the `` median heuristic . ''",
    "the exact origins of the median heuristic are unclear ( interestingly , it does not appear in the book that is most commonly cited as its source , @xcite ) but it may have been derived from @xcite and has precursors in classical work on bandwidth selection for kernel density estimation @xcite .",
    "note that there are two versions of the median heuristic in the literature : in both versions , given a set of observations @xmath27 we calculate @xmath29 and then one version ( e.g.  @xcite ) uses the gaussian rbf / squared exponential kernel parameterized as @xmath30 and the second version ( e.g.  @xcite ) uses the parameterization @xmath31 . some recent work has highlighted the situations in which the median heuristic can lead to poor performance @xcite .",
    "cases in which the median heuristic performs quite well and also cases in which it performs quite poorly are discussed in @xcite .",
    "we note that the median heuristic has also been used as a default value for supervised learning tasks ( e.g.  for the svm implementation in r package ` kernlab ` ) or when cross - validation is simply too expensive .    outside of kernel methods ,",
    "the same basic conundrum arises in spectral clustering in the choice of the parameters for the similarity graph ( * ? ? ?",
    "* section 8.1 ) and it is implicitly an issue in any unsupervised statistical method based on distances or dissimilarities , like the distance covariance ( which is in fact equivalent to hsic with a certain family of kernel functions @xcite ) , or even the choice of the number of neighbors @xmath10 in @xmath10-nearest neighbors algorithms .",
    "below , we will work with a parametric family of kernels @xmath32 .",
    "given a dataset @xmath33 of observations in @xmath34 for an unknown probability distribution @xmath5 , we wish to infer the kernel embedding @xmath35 for a given kernel @xmath36 in the parametric family .",
    "moreover , we wish to construct a model that will allow inference of the kernel hyperparameter @xmath37 as well .",
    "note that the two goals are related , since @xmath37 determines the space in which the embedding @xmath38 lies .",
    "when it is obvious from context , we suppress the dependence of the embeddings on the underlying measure @xmath5 , writing @xmath39 to emphasize the dependence on @xmath37 .",
    "similarly , we will use @xmath40 to denote the simple empirical estimator from eq .  , which depends on a fixed sample @xmath41 .",
    "our bayesian kernel embedding ( bke ) approach consists in specifying a prior on the kernel mean embedding @xmath42 and a likelihood function linking it to the observations through the empirical estimator @xmath40 .",
    "this will then allow us to infer the posterior distribution of the kernel mean embedding .",
    "the hyperparameter @xmath37 can itself have a prior , with the goal of learning a posterior distribution over the hyperparameter space .      a given hyperparameter @xmath37 ( which can itself have a prior distribution ) , parameterizes a kernel @xmath43 and a corresponding rkhs @xmath44 .",
    "while it is tempting to define a @xmath45 prior on @xmath42 , this is problematic since draws from such prior would almost surely fall outside @xmath1 @xcite . therefore , we define a gp prior over @xmath42 as follows : @xmath46 where @xmath47 is any finite measure on @xmath48 .",
    "this choice of @xmath49 ensures that @xmath50 with probability 1 by the _ nuclear dominance _",
    "@xcite of @xmath43 over @xmath49 for any stationary kernel @xmath36 and more broadly whenever @xmath51 . for completeness",
    ", we provide details of this construction in the appendix in section [ section : nuclear - dominance ] . since eq .",
    "is the convolution of a kernel with itself with respect to @xmath47 , for typical kernels @xmath43 , the resulting kernel @xmath49 can be thought of as a smoother version of @xmath43 .",
    "a particularly convenient choice for @xmath52 is to take @xmath47 to be proportional to a gaussian measure in which case @xmath53 can be computed analytically for a squared exponential kernel @xmath36 .",
    "the derivation is given in the appendix in section [ section : r - calculation ] , where we further show that if we set @xmath47 to be proportional to an isotropic gaussian measure with a large variance parameter , @xmath49 becomes very similar to a squared exponential kernel with lengthscale @xmath54 .",
    "we need a likelihood linking the kernel mean embedding @xmath42 to the observations @xmath41 .",
    "we define the likelihood via the empirical mean embedding estimator of eq .  , @xmath55 which depends on @xmath41 and @xmath37 .",
    "consider evaluating @xmath55 at some @xmath56 ( which need not be one of our observations ) .",
    "the result is a real number giving an empirical estimate of @xmath57 based on @xmath41 and @xmath37 .",
    "we link the empirical estimate , @xmath58 , to the corresponding modeled estimate , @xmath57 using a gaussian distribution with variance @xmath59 : @xmath60 our motivation for choosing this likelihood comes from the central limit theorem . for a fixed location @xmath61 , @xmath62 is an average of i.i.d .",
    "random variables so it satisfies : @xmath63).\\ ] ] we note that considering a heteroscedastic variance dependent on @xmath61 in would be a straightforward extension to our model , but we do not pursue this idea further here , i.e. while @xmath64 can depend both on @xmath37 and @xmath61 , we treat it as a single hyperparameter in the model .",
    "there are various ways to understand the construction of our hierarchical model .",
    "@xmath41 are drawn iid from @xmath5 , which we do not have access to .",
    "we could estimate @xmath5 directly ( e.g.  with a gaussian mixture model ) obtaining @xmath65 , and then estimate @xmath66 .",
    "but since density estimation is challenging in high dimensions , we posit a generative model for @xmath39 directly .    beginning at the top of the hierarchy , we have a fixed or random hyperparameter @xmath37 , which immediately defines @xmath43 and the corresponding rkhs @xmath44 .",
    "then , we introduce a gp prior over @xmath39 to ensure that @xmath67",
    ". a few realizations of @xmath39 drawn from our prior are shown in figure [ fig : prior - and - likelihood - illustrations ] ( a ) , for an illustrative one - dimensional example where the prior is a gaussian process with squared exponential kernel with lengthscale @xmath68 .",
    "small values of @xmath37 yield rough functions and large values of @xmath37 yield smooth functions .",
    "is a squared exponential kernel with lengthscale 0.1 .",
    "three draws of @xmath39 from the prior are shown in ( a ) .",
    "the empirical mean estimator @xmath55 , which is the link function for the likelihood , is shown in ( b ) with the observations shown as a rug plot . in ( c ) , the posterior mean embedding ( black line ) with uncertainty intervals ( gray lines ) is shown , as is the true mean embedding ( blue line ) based on the true data generating process ( a mixture of gaussians ) and the same @xmath43 . ]",
    "next , we need to define the likelihood , which links these draws from the prior to the observations @xmath41 .",
    "since @xmath39 is an infinite dimensional element in a hilbert space and @xmath69 we need to transform the observations so that we can put a probability distribution over them .",
    "we use the empirical estimate of the mean embedding @xmath55 as our link function .",
    "given a few observations , @xmath55 is shown in figure [ fig : prior - and - likelihood - illustrations ] ( b ) .",
    "our likelihood links @xmath55 to @xmath39 at the observation locations @xmath41 by assuming a squared loss function , i.e.  gaussian errors . as mentioned above",
    ", the motivation is the central limit theorem , but also the convenient conjugate form that a gaussian process with gaussian likelihood yields .",
    "a plot of the posterior over the mean embedding is shown in figure [ fig : prior - and - likelihood - illustrations ] ( c ) .",
    "a few points are worth noting : since the empirical estimator is already quite smooth ( notice its similarity to a kernel density estimate ) , the posterior mean embedding is only slightly smoother than the empirical mean embedding .",
    "notice that unlike kernel density estimation , there is no requirement that the kernel mean embedding be non - negative , thus explaining the posterior uncertainty intervals which are below zero .",
    "our original motivation for considering a bayesian model for kernel mean embeddings was to see whether there was a coherent bayesian formulation that corresponded to the shrinkage estimators in @xcite , while also enabling us to learn the hyperparameters .",
    "the first difficulty we faced was how to define a valid prior over the rkhs and a reasonable likelihood function .",
    "our choices are by no means definitive , and we hope to see further development in this area in the future .",
    "the second difficulty was that of developing a method for inferring hyperparameters , to which we turn in the next section .",
    "in this section we show how to perform learning and inference in the bayesian kernel embedding model introduced in the previous section .",
    "our model inherits various attractive properties from the gaussian process framework @xcite .",
    "first , we derive the posterior and posterior predictive distributions for the kernel mean embedding in closed form due to the conjugacy of our model , and show the relationship with previously proposed shrinkage estimators .",
    "we then derive the tractable marginal likelihood of the observations given the hyperparameters allowing for efficient map estimation or posterior inference for hyperparameters .      similarly to gp models ,",
    "the posterior mean of @xmath42 is available in closed form due to the conjugacy of gaussians .",
    "perhaps given our data we wish to infer @xmath42 at a new location @xmath70 . given a value of the hyperparameter @xmath37 we can calculate the posterior distribution of @xmath42 as well as the posterior predictive distribution @xmath71 .",
    "standard gp results @xcite yield the posterior distribution as : @xmath72^{\\top } \\;| \\;[\\widehat{\\mu_\\theta}(x_1 ) , \\ldots , \\widehat{\\mu_\\theta}(x_n)]^\\top , \\theta \\nonumber\\\\     & \\qquad\\sim \\mathcal{n}(r_\\theta ( r_\\theta+(\\tau^2/n ) i_n)^{-1}[\\widehat{\\mu_\\theta}(x_1 ) , \\ldots , \\widehat{\\mu_\\theta}(x_n)]^{\\top},\\nonumber \\\\     & \\qquad \\qquad \\qquad \\qquad r_\\theta - r_\\theta(r_\\theta+(\\tau^2/n ) i_n)^{-1}r_\\theta ) , \\label{eq : posterior - dist}\\end{aligned}\\ ] ] where @xmath73 is the @xmath74 matrix such that its @xmath75-th element is @xmath76 . the posterior predictive distribution at a new location @xmath77 is : @xmath78^\\top , \\theta \\nonumber\\\\      & \\qquad \\sim \\mathcal{n}(r_\\theta^{*\\top } ( r_\\theta+(\\tau^2/n ) i_n)^{-1}[\\widehat{\\mu_\\theta}(x_1 ) , \\ldots , \\widehat{\\mu_\\theta}(x_n)]^{\\top},\\nonumber\\\\      & \\qquad \\qquad \\qquad \\qquad r_\\theta^ { * * } - r_\\theta^{*\\top}(r_\\theta+(\\tau^2/n ) i_n)^{-1}r_\\theta^ * )      \\label{eq : posterior - f}\\end{aligned}\\ ] ] where @xmath79^\\top$ ] and @xmath80",
    ".    as in standard gp inference , the time complexity is @xmath81 due to the matrix inverses and the storage is @xmath82 to store the @xmath83 matrix @xmath84 .",
    "the spectral kernel mean shrinkage estimator ( s - kmse ) of @xcite for a fixed kernel @xmath10 is defined as : @xmath85 where @xmath86 is the empirical embedding , @xmath87 is the empirical covariance operator on @xmath1 , and @xmath88 is a regularization parameter .",
    "* proposition 12 ) shows that @xmath89 can be expressed as a weighted kernel mean @xmath90 , where @xmath91^\\top.\\end{aligned}\\ ] ] now , evaluating s - kmse at any point @xmath77 gives @xmath92^\\top,\\end{aligned}\\ ] ] where @xmath93^\\top$ ] .",
    "thus , the posterior mean in eq .",
    "recovers the s - kmse estimator @xcite , where the regularization parameter is related to the variance in the likelihood model , with a difference that in our case the kernel @xmath36 used to compute the empirical embedding is not the same as the kernel @xmath53 used to compute the kernel matrices .",
    "we note that our method has various advantages over the frequentist estimator @xmath89 : we have a closed - form uncertainty estimate , while we are not aware of a principled way of calculating the standard error of the frequentist estimators of embeddings .",
    "our model also leads to a method for learning the hyperparameters , which we discuss next .      in this section",
    "we focus on hyperparameter learning in our model . for the purposes of hyperparameter learning , we want to integrate out",
    "the kernel mean embedding @xmath42 and consider the probability of our observations @xmath41 given the hyperparameters @xmath37 . in order to link our generative model directly to the observations , we use a pseudolikelihood approach as discussed in detail below .",
    "we use the term pseudolikelihood because the model in this section will not correspond to the likelihood of the infinite dimensional empirical embedding ; rather it will rely on the evaluations of the empirical embedding at a finite set of points . let us fix a set of points @xmath94 in @xmath95 , with @xmath96 .",
    "these points are not treated as random , and the inference method we develop does not require any specific choice of @xmath97 .",
    "however , to ensure that there is a reasonable variability in the values of @xmath98 , these points should be placed in the high density regions of @xmath5 .",
    "the simplest approach is to use a small held out portion of the data ( with @xmath99 but @xmath96 ) .",
    "now , when we evaluate @xmath40 at these points , our modelling assumption from on vector @xmath100 $ ] can be written as @xmath101 however , as @xmath102 and all the terms @xmath103 are independent given @xmath42 , by cramr s decomposition theorem , this modelling assumption is for the mapping @xmath104 , given by @xmath105\\in\\mathbb{r}^{m},\\ ] ] equivalent to : @xmath106    applying the change of variable @xmath107 and using the generalization of the change - of - variables formula to non - square jacobian matrices as described in @xcite , we obtain a distribution for @xmath61 conditionally on @xmath42 and @xmath37 : @xmath108 , \\label{eq : like2}\\ ] ] where @xmath109_{ij}$ ] is an @xmath110 matrix , and @xmath111 & = & \\left(\\det\\left[j_\\theta(x)^{\\top}j_\\theta(x)\\right]\\right)^{1/2}\\nonumber\\\\   & = & \\left(\\det\\left[\\sum_{l=1}^{m}\\frac{\\partial k_\\theta(x , z_{l})}{\\partial x^{(i)}}\\frac{\\partial k_\\theta(x , z_{l})}{\\partial x^{(j)}}\\right]_{ij}\\right)^{1/2}\\nonumber\\\\   & = : & \\gamma_\\theta(x)\\;.\\label{eq : jacobian2}\\end{aligned}\\ ] ] the notation @xmath112 highlights the dependence on both @xmath37 and @xmath61 .",
    "an explicit calculation of @xmath112 for squared exponential kernels is described in section [ section : calculations ] .    by the conditional independence of @xmath113 given @xmath42 ,",
    "we obtain the pseudolikelihood of all @xmath114 observations : @xmath115 where @xmath116^{\\top}=\\text{vec}\\left\\{k_{\\theta,\\bf zx } \\right\\}\\in\\mathbb{r}^{mn}\\ ] ] and in the mean vector @xmath117^{\\top}$ ] , @xmath118 repeats @xmath114 times . under the prior , this mean vector",
    "has mean @xmath119 and covariance @xmath120 where @xmath121 is the @xmath122 matrix such that its @xmath75-th element is @xmath123 . combining this prior and the pseudolikelihood in",
    ", we have the marginal pseudolikelihood : @xmath124p(\\mu_\\theta|\\theta ) d\\mu_\\theta                            \\nonumber\\\\                                 & \\;=\\mathcal{n}\\left(\\phi_{{\\bf z}}({\\bf x});{\\bf 0},{\\bf 1}_{n}{\\bf 1}_{n}^{\\top}\\otimes r_{\\theta,{\\bf zz}}+\\tau^{2}i_{mn}\\right)\\prod_{i=1}^{n}\\gamma_\\theta(x_i ) .",
    "\\label{eq : integrate}\\end{aligned}\\ ] ]    while the marginal pseudolikelihood in eq .   involves a computation of the likelihood for an @xmath125-dimensional normal distribution , the kronecker structure of the covariance matrix allows efficient computation as described in appendix [ sec : kronecker ] .",
    "the complexity for calculating this likelihood is @xmath126 ( dominated by the inversion of @xmath127 ) .",
    "the jacobian term depends on the parametric form of @xmath43 , but a typical cost as shown in section [ section : calculations ] for the squared exponential kernel is @xmath128 . in this case , the computation of matrices @xmath121 and @xmath129 is @xmath130 and @xmath131 respectively .    just as in gp modeling , the marginal pseudolikelihood can be maximized directly for maximum likelihood ii ( also known as empirical bayes ) estimation , in which we look for a single best @xmath132 , or it can be used to construct an efficient mcmc sampler from the posterior of @xmath37 .      consider the isotropic squared exponential kernel with lengthscale matrix @xmath133 defined by @xmath134 in this case , we can analytically calculate @xmath135 , exact form is given in the appendix in section [ section : r - calculation ] .",
    "the partial derivatives of @xmath136 with respect to @xmath137 for @xmath138 can be easily derived as @xmath139 and therefore the jacobian from eq .",
    "is equal to @xmath140_{ij}\\right)^{1/2}\\;.\\end{aligned}\\ ] ] the computation of the matrix is @xmath141 and the determinant is @xmath142 .",
    "since we must calculate @xmath143 for each @xmath144 , the overall time complexity is @xmath128 .",
    "we demonstrate our approach on two synthetic datasets and one example on real data , focusing on two - sample testing with mmd and independence testing with hsic .",
    "first , we use our bayesian kernel embedding model and learn the kernel hyperparameters with maximum likelihood ii , optimizing the marginal likelihood .",
    "second , we take a fully bayesian approach to inference and learning with our model .",
    "finally , we apply the pc algorithm for causal structure discovery to a real dataset .",
    "the pc algorithm relies on a series of independence tests ; we use hsic with the lengthscales set with bayesian kernel learning .",
    "choosing lengthscales with the median heuristic is often a very bad idea . in the case of two sample testing",
    ", @xcite showed that mmd with the median heuristic failed to reject the null hypothesis when comparing samples from a grid of isotropic gaussians to samples from a grid of non - isotropic gaussians .",
    "we repeated this experiment by considering a distribution @xmath5 of a mixture of bivariate gaussians centered on a grid with diagonal covariance and unit variance and a distribution @xmath11 of a mixture of bivariate gaussians centered at the same locations but with rotated covariance matrices with a ratio @xmath145 of largest to smallest covariance eigenvalues .    as illustrated in figures [ fig : mixturegaussian](a ) and ( b ) , for small values of @xmath145 both distributions are very similar",
    "whereas the distinction between @xmath5 and @xmath11 becomes more apparent as @xmath145 increases . for different values of @xmath145 , we sample 100 observations from each mixture component , yielding 900 observations from @xmath5 and 900 observations from @xmath11 and then perform a two - sample test ( @xmath22 vs. @xmath23 ) using the mmd empirical estimate with an isotropic squared exponential kernel with one hyperparameter , the lengthscale . the type ii error ( i.e. probability that the test fails to reject the null hypothesis that @xmath146 at @xmath147 ) is shown in figure [ fig : mixturegaussian](c ) for differently skewed covariances ( @xmath145 from 0.5 to 15 ) when the median heuristic is chosen to select the kernel lengthscale or when using the bayesian kernel learning . in this example , the median heuristic picks a kernel with a large lengthscale , since the median distance between points is large . with",
    "this large lengthscale mmd always fails to reject at @xmath147 even for simple cases where @xmath145 is large .",
    "when we use bayesian kernel learning and optimize the marginal likelihood of eq .   for @xmath148 ( our results were not sensitive to the choice of this parameter , but in the fully bayesian case below we show that we can learn it ) we found the maximum marginal likelihood at a lengthscale of @xmath149 . with this choice of lengthscale",
    ", mmd correctly rejects the null hypothesis at @xmath147 even for very hard situations when @xmath150 .",
    "we observe that when @xmath145 is smaller than @xmath151 , the type ii error of mmd is very high for both choices of lengthscale , because the two distributions @xmath5 and @xmath11 are so similar that the test always retains the null hypothesis . in figure",
    "[ fig : mixturegaussian](d ) we illustrate the bkl marginal likelihood across a range of lengthscales .",
    "interestingly , there are multiple local optima and the median heuristic lies between the two main modes .",
    "the plot indicates that multiple scales may be of interest for this dataset , which makes sense given that the true data generating process is a mixture model .",
    "this insight can be incorporated into the bayesian kernel embedding framework by expanding our model , as discussed below . in figure",
    "[ fig : mixturegaussian](e ) we used the bke posterior to estimate the witness function @xmath152 .",
    "this function is large in magnitude in the locations where the two distributions differ .",
    "for ease of visualization we do not try to include posterior uncertainty intervals , but these are readily available from our model , and we show them for a 1-dimensional case below",
    ".    our model does not just provide a better way of choosing lengthscales .",
    "we can also use it in a fully bayesian context , where we place priors over the hyperparameters @xmath37 and @xmath64 , and then integrate them out to learn a posterior distribution over the mean embedding . switching to one dimension , we consider a distribution @xmath153 and a distribution @xmath154 .",
    "the densities are shown in figure [ fig : bayesian - witness](a ) .",
    "notice that the first two moments of these distributions are equal . to create a synthetic dataset we sampled @xmath114 observations from each distribution , and then combined them together into a sample of size @xmath155 , following the strategy in the previous experiment to learn a single lengthscale and kernel mean embedding for the combined dataset .",
    "we ran a hamiltonian monte carlo sampler ( hmc ) with nuts ( stan source code is in the appendix in section [ section : stan : model ] ) for the bayesian kernel embedding model with a squared exponential kernel , placing a @xmath156 prior on the lengthscale @xmath37 of the kernel and a @xmath156 prior on @xmath64 .",
    "we ran 4 chains for 400 iterations , discarding 200 iterations as warmup , with the chains starting at different random initial values .",
    "standard convergence and mixing diagnostics were good ( @xmath157 ) , so we considered the result to be 800 draws from the posterior distribution .",
    "recall that for fixed hyperparameters @xmath37 and @xmath64 we can obtain a posterior distribution over @xmath158 and @xmath159 .",
    "for each of our 800 draws , we drew a sample from these two distributions and then calculated the witness function as the difference , thus obtaining a random function drawn from the posterior distribution over @xmath160 ( where in practice we evaluate this function at a fine grid for plotting purposes ) .",
    "we thus obtained the full posterior distribution over the witness function , integrating over the kernel hyperparameter .",
    "we followed this procedure twice to create a dataset with @xmath161 and a dataset with @xmath162 . in figure",
    "[ fig : bayesian - witness](b ) we see that the witness function for the small dataset is not able to distinguish between the distributions as it rarely excludes 0 .",
    "( note that our model has the function 0 as its prior , which corresponds to the null hypothesis that the two distributions are equal .",
    "this could easily be changed to incorporate any relevant prior information . ) .",
    "as shown in figure [ fig : bayesian - witness](c ) , with more data the witness function is able to distinguish between the two distributions , mostly excluding 0 .     are drawn from distributions with equal means and variances .",
    "we then fit our bayesian kernel embedding model , with priors over the hyperparameters @xmath37 and @xmath64 to obtain a posterior over the witness function for two - sampling testing .",
    "the witness function indicates the model s posterior estimates of where the two distributions differ ( when the witness function is zero , it indicates no difference between the distributions ) .",
    "posterior means and 80% uncertainty intervals are shown . in",
    "( b ) the small sample size means that the model does not effectively distinguish between samples from a normal and a laplace distribution , while in ( c ) larger samples enable the model to find a clear difference , with much of the uncertainty envelope excluding 0 . ]",
    "finally , we consider the ozone dataset analyzed in @xcite , consisting of daily measurements of ozone concentration and eight related meteorological variables . following the approach in @xcite , we first pre",
    "- whiten the data to control for underlying temporal autocorrelation , then we use a combination of gaussian process regression followed by hsic to test for conditional independence .",
    "each time we run hsic , we set the kernel hyperparameters using bayesian kernel learning .",
    "the graphical model that we learn is shown in figure [ fig : ozone ] . the directed edge from the temperature variable to ozone",
    "is encouraging , as higher temperatures favor ozone formation through a variety of chemical processes which are not represented by variables in this dataset @xcite .",
    "note that this edge was not present in the graphical model in @xcite in which the median heuristic was used .",
    "we developed a framework for bayesian learning of kernel embeddings of probability measures .",
    "it is primarily designed for unsupervised settings , and in particular for kernel - based hypothesis testing . in these settings ,",
    "one relies critically on a good choice of kernel and our framework yields a new method , termed bayesian kernel learning , to inform this choice .",
    "we only explored learning the lengthscale of the squared exponential kernel , but our method extends to the case of richer kernels with more hyperparameters .",
    "we conceive of bayesian kernel learning as a drop - in replacement for selecting the kernel hyperparameters in settings where cross - validation is unavailable .",
    "a sampling - based bayesian approach is also demonstrated , enabling integration over kernel hyperparameters , and e.g. , obtaining the full posterior distribution over the witness function in two - sample testing .    while our method is designed for unsupervised settings ,",
    "there are various reasons it might be helpful in supervised settings or in applied bayesian modelling more generally . with the rise of large - scale kernel methods , it has become possible to apply , e.g.  svms or gps to very large datasets .",
    "but even with efficient methods , it can be very costly to run cross - validation over a large space of hyperparameters . in practice ,",
    "when , e.g.  large scale approximations based on random fourier features @xcite are used , we have not seen much attention paid to kernel learning ",
    "the features are often just one part of a complicated pipeline , so again the median heuristic is often employed . for these reasons , we think that the developed method for bayesian kernel learning would be a judicious alternative . moreover , it would be straightforward to develop scalable approximate versions of bayesian kernel learning itself .",
    "srf was supported by the erc ( fp7/617071 ) and epsrc ( ep / k009362/1 ) . thanks to wittawat jitkrittum , krikamol muandet , sayan mukherjee , jonas peters , aaditya ramdas , alex smola , and yee whye teh for helpful discussions .",
    "consider a dataset @xmath163 and suppose that there exists some unknown probability distribution @xmath5 for which the @xmath144 are i.i.d .",
    ": @xmath164 denote by @xmath42 the rkhs mean embedding element for a given kernel @xmath165 with hyperparameter @xmath166 and by @xmath167 the empirical mean embedding @xmath168    we posit as our model that @xmath42 has a gp prior with covariance @xmath53 , where @xmath169 where @xmath47 is a finite measure on @xmath170 thus ensuring that @xmath50 when drawn from the prior @xmath171 in addition , we model the link between the population mean embedding and the empirical mean embedding functions at a given location @xmath61 as follows@xmath172 where @xmath64 is another hyperparameter .",
    "the results in this section have appeared in the literature before , but as they are not well known or collected in one place , we have included them for completeness .",
    "a similar discussion appears in @xcite , but without the construction of explicit gp priors over the rkhss which we provide below .",
    "it is well known that the sample paths of a gp with kernel @xmath10 are almost surely outside rkhs @xmath1 , the result known as kallianpur s 0 - 1 law @xcite .",
    "it is easiest to demonstrate this by considering a mercer s expansion ( * ? ? ?",
    "* section 4.3 ) of kernel @xmath10 given by @xmath173 for the eigenvalue - eigenfunction pairs @xmath174 .",
    "then , a representation of @xmath175 is given by @xmath176 , where @xmath177 are independent and identically distributed standard normal random variables .",
    "however , @xmath178 so @xmath179 almost surely . this issue is often sidelined in the literature , cf .",
    "e.g. ( * ? ? ?",
    "* section 6.1 )  in gp regression , it is not necessary to ensure that the prior on the regression function is supported on @xmath1 ( the posterior mean will still lie in @xmath1 , however ) .",
    "however , since the object of our interest , kernel embedding , is by construction an element of @xmath1 - we opt for an approach where the prior is indeed specified over the correct space .",
    "fortunately , it is straightforward to construct a kernel @xmath180 such that the realizations from a gp with kernel @xmath180 are almost surely inside rkhs @xmath1 .",
    "for this , we will need notions of dominance and nuclear dominance for kernel functions .",
    "kernel @xmath10 is said to dominate kernel @xmath180 ( written @xmath181 ) if @xmath182 .",
    "* theorem 1.1 ) characterise dominance @xmath181 via the existence of a certain positive , continuous and self - adjoint operator @xmath183 for which @xmath184,k(\\cdot , x ' ) \\rangle_{{{\\mathcal{h}_k}}},\\qquad\\forall x , x'\\in{{\\mathcal{x}}}.\\ ] ] when @xmath185 is also a trace class operator , dominance is termed _ nuclear _ , and denoted @xmath186 . the following theorem from ( * ? ? ?",
    "* theorem 7.2 ) then fully characterises kernels that lead to valid gp priors over rkhs @xmath1 .",
    "let @xmath1 be separable and let @xmath187 .",
    "then @xmath188 has trajectories in @xmath1 with probability 1 if and only if @xmath186 .",
    "thus , we just need to specify a trace - class , positive , continuous and self - adjoint operator @xmath183 and compute @xmath189,k(\\cdot , x ' ) \\rangle_{{{\\mathcal{h}_k}}}$ ] .",
    "a convenient choice for a given bounded continuous kernel @xmath10 can be defined as follows .",
    "take the convolution operator @xmath190 with respect to a finite measure @xmath47 , defined as @xmath191(x)=\\int f(u)k(x , u)\\nu(du).\\ ] ] it is well known that the adjoint of @xmath192 is the inclusion of @xmath1 into @xmath193 ( * ? ? ?",
    "* section 4.3 ) .",
    "thus , we let @xmath194 , which is the ( uncentred ) covariance operator @xmath195 of @xmath47 . as a covariance operator , @xmath185 is then positive , continuous and self - adjoint .",
    "it is also trace - class in most cases of interest  and in particular whenever @xmath196 ( * ? ? ?",
    "* theorem 4.27 ) , and thus for every stationary kernel provided that @xmath47 is a finite measure .",
    "this leads to @xmath197,k(\\cdot , x ' ) \\rangle_{{{\\mathcal{h}_k}}}\\\\   { } & = & \\langle s_k^*[k(\\cdot , x)],s_k^*k(\\cdot , x ' ) \\rangle_{l^2(\\mathcal x ; \\nu)}\\\\   { } & = & \\int k(x , u)k(u , x')\\nu(du),\\end{aligned}\\ ] ] so @xmath180 can be simply computed as a convolution of @xmath10 with itself , and we can use @xmath188 as a prior over @xmath1 .      in this subsection , we derive the covariance function @xmath53 for squared exponential kernels . consider a squared exponential kernel on @xmath52 with full covariance matrix @xmath198 defined by @xmath199 while we have required in [ section : nuclear - dominance ] that @xmath47 is a finite measure for the covariance operator to be trace class when working with stationary kernels , let us for simplicity first consider the instructive case when @xmath47 is the lebesgue measure .",
    "then , we have @xmath200 note that @xmath201 then @xmath202 thus @xmath53 is proportional to another squared exponential kernel with covariance @xmath203 . for the special case where the covariance matrix @xmath198 is diagonal  let @xmath204 and @xmath205 we have @xmath206    now , take @xmath207 , i.e. , @xmath47 is a finite measure and is proportional to a gaussian measure on @xmath208 . in that case",
    ", we have @xmath209 from standard gaussian integration rules , it follows that @xmath210 where @xmath211 and @xmath212 .",
    "therefore @xmath213 thus , we see that @xmath53 has a nonstationary component that penalises the norm of @xmath214 .",
    "this is reminiscent of the well known locally stationary covariance functions @xcite .",
    "however , for large values of @xmath215 , the nonstationary component becomes negligible and @xmath53 reverts to being proportional to a standard squared exponential kernel with covariance @xmath203 , just like in the case of lebesgue measure .",
    "we note that any choice of @xmath216 gives a valid prior over @xmath217 . treating @xmath215 as another hyperparameter to be learned would be an interesting direction for future research .",
    "the marginal pseudolikelihood in eq .   requires computation of the likelihood for an @xmath125-dimensional normal distribution @xmath218 however , the kronecker product structure in the covariance matrix @xmath219 allows efficient computation .",
    "we denote with @xmath220 the eigendecomposition of the matrix @xmath121 with @xmath221 $ ] .",
    "note that @xmath222 is a rank - one matrix with the eigenvalue equal to @xmath114 .",
    "therefore @xmath223 has top @xmath224 eigenvalues equal to @xmath225 , @xmath226 , and the remaining @xmath227 all equal to @xmath64 .",
    "thus , the log - determinant is simply @xmath228+m\\log n+m(n-1)\\log\\tau^{2}.\\ ] ]    further , we need to compute @xmath229 . by completing @xmath230 to an orthonormal basis @xmath231 of @xmath232 and forming the corresponding matrix",
    "@xmath233 $ ] , and denoting by @xmath234 an @xmath74 matrix with @xmath235 and @xmath236 elsewhere , we have that @xmath237 we now simply need to apply kronecker identity @xmath238 , to obtain @xmath239_j^2}{n\\lambda_j+\\tau^2}+\\frac{1}{\\tau^2}\\sum_{i=2}^n\\sum_{j=1}^m \\left[q^\\top k_{\\theta,\\bf zx } b_i\\right]_j^2.\\end{aligned}\\ ] ]    for the first term , we have @xmath240_j^2}{n\\lambda_j+\\tau^2}=\\sum_{j=1}^m\\frac{\\left[q^\\top\\hat\\mu({\\bf z})\\right]_j^2}{\\lambda_j+\\tau^2/n}=\\sum_{j=1}^m\\frac{\\text{tr}\\left[\\hat\\mu({\\bf z})\\hat\\mu({\\bf z})^\\top q_jq_j^\\top\\right]}{\\lambda_j+\\tau^2/n}\\nonumber\\\\   &",
    "\\qquad = \\hat\\mu({\\bf z})^\\top \\left(r_{\\theta,{\\bf zz}}+(\\tau^2/n)i_m\\right)^{-1}\\hat\\mu({\\bf z}).\\end{aligned}\\ ] ]    and for the second term : @xmath241_{j}^{2 } & = & \\frac{1}{\\tau^{2}}\\sum_{j=1}^{m}\\sum_{i=2}^{n}\\left[q_{j}^{\\top}k_{\\theta,{\\bf zx}}b_{i}\\right]^{2}\\nonumber\\\\      & = &    \\frac{1}{\\tau^{2}}\\sum_{j=1}^{m}\\left\\ { \\left\\vert k_{\\theta,{\\bf xz}}q_{j}\\right\\vert ^{2}-n\\left(q_{j}^{\\top}\\hat{\\mu}({\\bf z})\\right)^{2}\\right\\ } \\nonumber\\\\      & = &    \\frac{1}{\\tau^{2}}\\left\\vert k_{\\theta,{\\bf xz}}\\right\\vert _ { f}^{2}-\\frac{n}{\\tau^{2}}\\left\\vert \\hat{\\mu}\\left({\\bf z}\\right)\\right\\vert ^{2}.\\end{aligned}\\ ] ]    altogether , the log - likehood is given by @xmath242\\\\   & & \\quad\\quad + \\,\\hat{\\mu}({\\bf z})^{\\top}\\left(r_{\\theta,{\\bf zz}}+(\\tau^{2}/n)i_{m}\\right)^{-1}\\hat{\\mu}({\\bf z})\\nonumber\\\\   & & \\quad\\quad\\quad + \\,\\frac{1}{\\tau^{2}}\\left\\vert k_{\\theta,{\\bf xz}}\\right\\vert _",
    "{ f}^{2}-\\frac{n}{\\tau^{2}}\\left\\vert \\hat{\\mu}\\left({\\bf z}\\right)\\right\\vert ^{2}\\nonumber\\\\   & & \\quad\\quad\\quad\\quad+\\,m\\log n+m(n-1)\\log\\tau^{2}+mn\\log(2\\pi)\\biggr\\}\\nonumber.\\end{aligned}\\ ] ]",
    ".... functions {    //",
    "phi should be m x n    real kron_multi_normal(matrix k , matrix r , matrix q1,vector e1,int m , int n , real sigma2 ) {      vector[m*n ] e ;      matrix[m , m ] q2 ;      vector[m ] e2 ;      vector[m ] ones ;      vector[m*n ] mv2 ;      real mvp ;      real logdet ;      q2 < -   eigenvectors_sym(r ) ;      e2 < - eigenvalues_sym(r ) ;      for(j in 1:m ) {        ones[j ] < - 1 ;        for(i in 1:n )          e[(j-1)*n + i ] < - 1/(e1[i ] * e2[j ] + sigma2 ) ;      }      mv2 < - to_vector((transpose(q2 ) * transpose(k ) ) * q1 ) ;      mvp < - sum(mv2 .",
    "* mv2 ) ;      logdet < - sum(log(e2 . *",
    "( ones * n ) + ones * sigma2 ) ) + m * ( n-1 ) * log(sigma2 ) ;        return ( - .5 * logdet - .5 * mvp ) ;    } }      transformed data {    matrix[n , m ] xu_dist2 ;    matrix[m , m ] u_dist2 ;    matrix[n , n ] ones ;    vector[n ] zeros ;    matrix[n , n ] q1 ;    vector[n ] e1 ;       for ( i in 1:n ) {      zeros[i ] < - 0 ;      e1[i ] < - 0 ;      for ( j in 1:n )         ones[i , j ] < - 1 ;          for(j in 1:m )        xu_dist2[i , j ] < - square(x[i ] - u[j ] ) ;    }    for(i in 1:m ) {      for(j in 1:m )        u_dist2[i , j ] < - square(u[i ] - u[j ] ) ;    }    e1[1 ]",
    "< - n ;    q1 < - eigenvectors_sym(ones ) ; }    parameters {    real < lower=0 > lengthscale ;    real < lower=0 > sigma2 ; } transformed parameters {    matrix[m , m ] r ;                                  matrix[n , m ] j ;                                  matrix[n , m ] k ;     //",
    "r < - lengthscale * sqrt(pi ( ) ) *     r < - exp(- u_dist2/(4*lengthscale^2 ) ) ;      k < - exp(- xu_dist2/(2*lengthscale^2 ) ) ;     j < - k .",
    "* xu_dist2 / lengthscale^4 ; }"
  ],
  "abstract_text": [
    "<S> kernel methods are one of the mainstays of machine learning , but the problem of kernel learning remains challenging , with only a few heuristics and very little theory . </S>",
    "<S> this is of particular importance in methods based on estimation of kernel mean embeddings of probability measures . for characteristic kernels , which include </S>",
    "<S> most commonly used ones , the kernel mean embedding uniquely determines its probability measure , so it can be used to design a powerful statistical testing framework , which includes nonparametric two - sample and independence tests . in practice , however , the performance of these tests can be very sensitive to the choice of kernel and its lengthscale parameters . to address this central issue </S>",
    "<S> , we propose a new probabilistic model for kernel mean embeddings , the bayesian kernel embedding model , combining a gaussian process prior over the reproducing kernel hilbert space containing the mean embedding with a conjugate likelihood function , thus yielding a closed form posterior over the mean embedding . </S>",
    "<S> the posterior mean of our model is closely related to recently proposed shrinkage estimators for kernel mean embeddings , while the posterior uncertainty is a new , interesting feature with various possible applications . </S>",
    "<S> critically for the purposes of kernel learning , our model gives a simple , closed form marginal pseudolikelihood of the observed data given the kernel hyperparameters . </S>",
    "<S> this marginal pseudolikelihood can either be optimized to inform the hyperparameter choice or fully bayesian inference can be used . </S>"
  ]
}