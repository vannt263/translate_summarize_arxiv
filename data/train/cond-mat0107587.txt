{
  "article_text": [
    "recent analyses of extracellular recordings performed in two motor areas of behaving monkeys have tried to clarify how information about movements is trasmitted and received from higher to lower stages of processing , and to identify distinct roles of the two areas in the planning and execution of movements @xcite .",
    "although this study failed to produce clearcut results , it remains interesting to try and understand , from a more theoretical point of view , how information about multi - dimensional correlates of neural activity may be transmitted from the input to the output of a simple network .",
    "in fact , a theoretical study is still lacking , which explores how the coding of stimuli with continuous as well as discrete dimensions is transferred across a network .    in a previous report @xcite the mutual information between the activity ( ` firing rates ' ) of a finite population of @xmath0 units ( ` neurons ' ) and a set of correlates , which have both a discrete and a continuous angular dimension , has been evaluated analytically in the limit of large noise .",
    "this parametrization of the correlates can be applied to movements performed in a given direction and classified according to different `` types '' ; yet it is equally applicable to other correlates , like visual stimuli characterized by an orientation and a discrete feature ( colour , shape , etc .. ) , or in general to any correlate which can be identified by an angle and a `` type '' . in this study , we extend the analysis performed for one population , to consider two interconnected areas , and we evaluate the mutual information between the firing rates of a finite population of @xmath1 output neurons and a set of continuous+discrete stimuli , given that the rate distribution in input is known . in input , a threshold nonlinearity has been shown to lower the information about the stimuli in a simple manner , which can be expressed as a renormalization of the noise @xcite .",
    "how does the information in the output depend on the same nonlinearity ?",
    "how does it depend on the noise in the output units ? is the power to discriminate among discrete stimuli more robust to transmission down one set of random synapses , than the information about a continuously varying parameter ?",
    "we address these issues by calculating the mutual information , using the replica trick and under the assumption of replica symmetry ( see for example @xcite ) .",
    "saddle point equations are solved numerically .",
    "we analyze how the information trasmission depends on the parameters of the model , i.e. the level of output and input noise , on the ratio between the two population sizes , as well as on the tuning curve with respect to the continuous correlate , and on number of discrete correlates .",
    "the input - output transfer function is a crucial element in the model .",
    "the binary and the sigmoidal functions used in many earlier theoretical and simulation studies @xcite fail to describe accurately current - to - frequency transduction in real neurons .",
    "such trasduction is well captured instead , away from saturation , by a threshold - linear function @xcite .",
    "such a function combines the threshold of real neurons , the linear behaviour typical of pyramidal neurons above threshold , and the accessibility to a full analytical treatment @xcite , as demonstrated here , too . for the sake of analytical feasibility ,",
    "however , we take the input units to be purely linear . therefore it should be kept in mind , in considering the final results that the threshold nonlinearity is only applied to the output units .",
    "in analogy to the model studied in @xcite we consider a set of @xmath0 input units which fire to an external continuous+discrete stimulus , parametrized by an angle @xmath3 and a discrete variable @xmath4 , with a gaussian distribution : @xmath5 ; \\label{dist}\\ ] ] @xmath6 is the firing rate in one trial of the @xmath7 input neuron , while the mean of the distribution , @xmath8 is written : @xmath9 @xmath10 where @xmath11 is a quenched random variable distributed between @xmath12 and @xmath13 , @xmath14 is the preferred direction for neuron @xmath15 . according to eq.([tuning_tot ] )",
    "neurons fire at an average firing rate which modulates with @xmath3 with amplitude @xmath16 , or takes a fixed value @xmath17 , independently of @xmath3 , with amplitude @xmath18 .",
    "we assume that quenched variables are uncorrelated and identically distributed across units and across the @xmath19 discrete correlates : @xmath20^{nk } \\label{ro_eps}\\ ] ] @xmath21^n=\\frac{1}{(2\\pi)^n}.\\ ] ]    in @xcite it has been shown that a cosinusoidal shaped function as in eq.([tuning_tot ] ) is able to capture the main features of directional tuning of real neurons in motor cortex .",
    "moreover it has been shown that the presence of negative firing rates in the distribution ( [ dist ] ) , which is not biologically plausible , does not alter information values , with respect to a more realistic choice for the firing distribution , in that it leads to the same curves except for a renormalization of the noise .",
    "output neurons are activated by input neurons via uncorrelated gaussian connection weights @xmath22 .",
    "each output neuron performs a linear summation of the inputs ; the outcome is distorted by a gaussian distributed noise @xmath23 and then thresholded , as in the following : @xmath24^+;\\,\\,\\,\\,i=1 .. m , j=1 ..",
    "n \\label{output}\\ ] ]    in eq.([output ] ) @xmath25 is a threshold term , @xmath26 is a ( 0,1 ) binary variable , with mean @xmath27 , which expresses the sparsity or dilution of the connectivity matrix , and @xmath28 @xmath29 @xmath30 @xmath31^+=x\\theta(x).\\ ] ]",
    "we aim at estimating the mutual information between the output patterns of activity and the continuous+discrete stimuli : @xmath32 @xmath33 where the distribution @xmath34 is determined by the threshold linear relationship ( [ output ] ) , @xmath35 is given in eq.([dist ] ) and @xmath36 is a short notation for the average across the quenched variables @xmath37,@xmath38,@xmath39,@xmath40 and on the noise @xmath41 .",
    "we assume that the stimuli are equally likely : @xmath42 .",
    "eq.([info ] ) can be written as : @xmath43 with : @xmath44 @xmath45 \\right\\rangle_{\\varepsilon,\\vartheta^0,c , j,\\delta}. \\label{outent}\\end{aligned}\\ ] ]    the analytical evaluation of the _ equivocation _ @xmath46 can be performed inserting eq.([xi_eta ] ) in the expression ( [ equiv ] ) , and using the replica trick to get rid of the logarithm : @xmath47    to take into account the threshold - linear relation ( [ output ] ) we consider the following equalities : @xmath48    inserting eq.([anna ] ) in eq.([replica ] ) one obtains : @xmath49 -1\\right ) .",
    "\\label{replica2}\\end{aligned}\\ ] ]    the average across the quenched disorder @xmath27,@xmath50,@xmath51 in eq.([replica2 ] ) can be performed in a very similar way as shown in @xcite : using the integral representation for each @xmath51 function , gaussian integration across @xmath50,@xmath51 is standard ; the average on @xmath27 can be performed assuming large the number @xmath0 of input neurons .",
    "the final outcome for the _ equivocation _ reads : @xmath52^m -1\\right ) ,   \\label{replica3 } \\end{aligned}\\ ] ] where we have put @xmath53 .",
    "integration on @xmath54 is straightforward .",
    "integration on @xmath55 can be performed introducing @xmath56 auxiliary variables @xmath57 via @xmath51 functions expressed in their integral representation . considering the expression ( [ dist ] ) for the input distribution and with some rearrangement of the terms",
    "the final result can be expressed as : @xmath58^m-1\\right ) , \\nonumber\\end{aligned}\\ ] ] where : @xmath59    the evaluation of the entropy of the responses @xmath60 , eq.([outent ] ) , can be carried out in a very similar way , introducing replicas in the continuous+discrete stimulus space .",
    "the final result reads : @xmath61^{n+1 } \\left\\langle   e^{-\\sum_{\\alpha,\\beta}(\\delta_{\\alpha\\beta}-\\sigma^{-1}_{\\alpha\\beta } ) \\tilde{\\eta}(\\vartheta_\\alpha , s_\\alpha)\\tilde{\\eta}(\\vartheta_\\beta , s_\\beta)/2\\sigma^2 } \\right\\rangle_{\\varepsilon,\\vartheta^0}^n\\right.\\label{outent2}\\\\ & & \\left.e^{-\\frac{m}{2}tr \\ln g } \\left[\\int_{-\\infty}^0   \\prod_\\alpha \\frac{d\\xi^\\alpha}{\\sqrt{2\\pi } } e^{-\\sum_{\\alpha,\\beta}(\\xi^\\alpha-\\xi_0 ) ( g^{-1}_{\\alpha\\beta}/2)(\\xi^\\beta-\\xi_0)}+\\int^{\\infty}_0    \\frac{d\\xi}{(2\\pi)^{\\frac{n+1}{2 } } } e^{-\\sum_{\\alpha,\\beta } ( g^{-1}_{\\alpha\\beta}/2)(\\xi^-\\xi_0)^2}\\right]^m-1\\right ) .",
    "\\nonumber\\end{aligned}\\ ] ]",
    "the integrals in eq.([equiv2]),([outent2 ] ) can not be solved without resorting to an approximation . in analogy to what is used in @xcite , we use a saddle - point approximation ( which in general would be valid in the limit @xmath62 ) and we assume replica symmetry @xcite in the parameters @xmath63 , @xmath64 .",
    "this allows to explicitely invert and diagonalize the matrices @xmath65,@xmath66 : @xmath67 the assumption of replica symmetry seems to have more subtle implications in the present situation .",
    "these will be discussed below .    in replica symmetry",
    "the mutual information can be expressed as follows : @xmath68}\\right.\\nonumber\\\\ & & \\left .- e^{n\\left[(n+1)z_0^b\\tilde{z}_0^b - n(n+1)z_1^b\\tilde{z}_1^b- \\frac{r}{2}\\left(tr\\ln g(z_0^b , z_1^b)+f(z_0^b , z_1^b)\\right)-\\frac{1}{2}tr\\ln\\sigma(\\tilde{z}_0^b,\\tilde{z}_1^b)- h^b(\\tilde{z}_0^b,\\tilde{z}_1^b)\\right]}\\right\\ } , \\label{info_lim}\\end{aligned}\\ ] ] with @xmath69 ; \\label{f}\\ ] ] @xmath70 ; \\label{ha}\\ ] ] @xmath71^{n+1 } \\left\\langle   e^{-\\sum_{\\alpha,\\beta}\\left(\\delta_{\\alpha\\beta}-\\sigma^{-1}_{\\alpha\\beta}\\right )   \\tilde{\\eta}(\\vartheta_\\alpha , s_\\alpha)\\tilde{\\eta}(\\vartheta_\\beta , s_\\beta)/2\\sigma^2 } \\right\\rangle_{\\varepsilon,\\vartheta^0}^n\\right ] .",
    "\\label{hb}\\ ] ]    we have set @xmath72 and @xmath73,@xmath74,@xmath75,@xmath76 are the solutions of the saddle point equations : @xmath77;\\nonumber\\\\ z_1^{a , b}&=&-\\frac{1}{n}\\frac{\\partial}{\\partial\\tilde{z}_1}\\left[\\frac{1}{2 } tr\\ln\\sigma(\\tilde{z}_0,\\tilde{z}_1)+h^{a , b}(\\tilde{z}_0,\\tilde{z}_1)\\right];\\nonumber\\\\ \\tilde{z}_0^{a , b}&=&\\frac{\\partial}{\\partial z_0}\\frac{r}{2}\\left [ tr\\ln g(z_0,z_1)+f(z_0,z_1)\\right];\\nonumber\\\\ \\tilde{z}_1^{a , b}&=&-\\frac{1}{n}\\frac{\\partial}{\\partial z_1}\\frac{r}{2}\\left [ tr\\ln g(z_0,z_1)+f(z_0,z_1)\\right].\\end{aligned}\\ ] ]    all the equations must be evaluated in the limit @xmath78 .",
    "it is easy to check that all terms in the exponent in eq.([info_lim ] ) are order @xmath79 .",
    "in fact , since when @xmath78 only one replica remains , one has : @xmath80_{|_{n=0}}.\\end{aligned}\\ ] ]    therefore , from the saddle point equations , @xmath74 are order @xmath79 and @xmath81 is also order @xmath79 : @xmath82    since @xmath83 , it is easy to check by explicit evaluation that , when @xmath78 , all the @xmath84 diagonal terms among the matrix elements @xmath85 are order @xmath79 and all the @xmath86 out - of - diagonal terms are order @xmath13",
    ". then all terms in the exponent of eqs.([ha]),([hb ] ) are order @xmath79 , and we can expand the exponentials , which allows us to perform the quenched averages across @xmath87 . considering the expression of @xmath88 , eq.([tuning_tot ] ) , one obtains : @xmath89\\right ) ; \\label{ha_hb}\\end{aligned}\\ ] ] @xmath90 ^ 2\\rangle_{\\varepsilon,\\vartheta^0}\\nonumber\\\\                & = & ( \\eta^0)^2\\left[(a_2+\\alpha^2 - 2\\alpha a_1)\\langle\\varepsilon^2\\rangle_{\\varepsilon}+\\alpha^2 + 2\\alpha(a_1-\\alpha)\\langle\\varepsilon\\rangle_{\\varepsilon}\\right ] ; \\label{lambda1}\\end{aligned}\\ ] ] @xmath91 ^ 2   \\langle\\tilde{\\eta}(\\vartheta_1,s_1)\\tilde{\\eta}(\\vartheta_2,s_2 ) \\rangle_{\\varepsilon,\\vartheta^0}\\nonumber\\\\                & = & ( \\eta^0)^2\\left[(a_1-\\alpha)^2\\left(\\frac{k-1}{k } \\langle\\varepsilon\\rangle_{\\varepsilon}^2+\\frac{1}{k}\\langle\\varepsilon^2 \\rangle_{\\varepsilon}\\right)+\\alpha^2 + 2\\alpha(a_1-\\alpha ) \\langle\\varepsilon\\rangle_{\\varepsilon}\\right ] ; \\label{lambda2}\\end{aligned}\\ ] ] @xmath92    a similar expansion in @xmath79 for @xmath93 and for @xmath94 allows to derive explicitely the saddle point equations : @xmath95 \\lambda_\\eta^1+\\frac{1}{\\left(1 + 2\\sigma^2\\tilde{z}_1^b\\right)^2}\\lambda_\\eta^2;\\nonumber\\\\ \\tilde{z}_1^{a , b}&=&-c\\sigma^2_j\\frac{r}{2}\\left\\{\\sigma\\left(\\frac{\\xi^0}{\\sqrt{p+q}}\\right)\\frac{\\xi^0}{\\left(p+q\\right)^{\\frac{3}{2}}}-\\frac{1}{p}{{\\rm erf}}\\left(\\frac{\\xi^0}{\\sqrt{p+q}}\\right)\\right.\\nonumber\\\\ & & \\left.+\\int_{-\\infty}^{\\infty } dt \\left[1+\\ln\\left({{\\rm erf}}\\left(-\\frac{\\xi^0-t\\sqrt{q}}{\\sqrt{p}}\\right)\\right)\\right]\\sigma\\left(\\frac{\\xi^0-t\\sqrt{q}}{\\sqrt{p}}\\right)\\frac{1}{p^\\frac{3}{2}}\\left[\\xi^0-t\\frac{q+p}{\\sqrt{q}}\\right]\\right \\};\\nonumber\\\\ \\label{saddlepoint}\\end{aligned}\\ ] ] where : @xmath96 @xmath97    from the expression of @xmath73 in eq.([saddlepoint ] ) , it is easy to verify that the dependence on @xmath98 in eq.([ha_hb ] ) , which might affect the information in eq.([info_lim ] ) , cancels out with the products @xmath99,@xmath100 which should contribute to the information in the limit @xmath78 ( see eq.([info_lim ] ) ) . therefore , since @xmath73 is known and @xmath75 depends only on @xmath76 , the mutual information can be expressed as a function of @xmath75,@xmath76 , which in turn are to be determined self - consistently by the saddle point equations .",
    "the average information per input cell can be written , finally : @xmath101 + \\gamma_2^b(\\tilde{z}_1^b)-\\gamma_2^a(\\tilde{z}_1^a)\\right\\ } , \\label{final_info}\\ ] ] with @xmath102;\\end{aligned}\\ ] ] @xmath103 @xmath104.\\ ] ]    the expression for the mutual information only contains terms linear in either @xmath0 or @xmath1 .",
    "since the last of the saddle - point equations , ( [ saddlepoint ] ) , contains @xmath105 , if one fixes @xmath0 and increases @xmath1 the information grows non - linearly , because the position of the saddle point varies .",
    "it turns out that , as shown below , the growth is only very weakly sublinear , at least when @xmath106 .",
    "analogously , fixing @xmath1 and varying @xmath0 we would find a non - linearity due to the @xmath105-dependence of the saddle point .",
    "if @xmath105 is fixed and @xmath0 and @xmath1 grow together , the information rises purely linearly .    what our analytical treatment misses out , however , is the nonlinearity required to appear as the mutual information approaches its ceiling , the entropy of the stimulus set .",
    "the approach to this saturating value was described at the input stage @xcite , where also the initial linear rise ( in @xmath0 ) was obtained in the large noise limit @xcite .",
    "therefore , our saddle point method is in same sense similar to taking a large ( input ) noise limit , @xmath107 , to its leading ( order @xmath108 ) term .",
    "it is possible that the saddle point method could be extended , to account also for successive terms in a large noise expansion .",
    "this would probably require integrating out the fluctuations around the saddle point , but by carefully analysing the relation of different replicas to different values of the quenched variables .",
    "we leave this possible extension to future work . the present calculation , therefore ,",
    "although employing a saddle point method which is usually applicable for large @xmath0 and @xmath1 , should be considered effectively as yielding the initial linear rise in the mutual information , the one observed with @xmath1 small .",
    "eq.([saddlepoint ] ) for @xmath76 has been solved numerically using a matlab code .",
    "convergence to self - consistency has been found already after @xmath109 iterations with an error lower than @xmath110 .",
    "fig.[fig1 ] shows the mutual information as a function of the output population size , for an input population size equal to @xmath111 cells .",
    "this is contrasted with the information in the input units , about exactly the same set of correlates , calculated as in @xcite , by keeping only the leading ( linear ) term in @xmath0 .",
    "in fact , in @xcite the mutual information carried by a finite population of neurons firing according to eq.([dist ] ) had been evaluated analytically , in the limit of large noise , by means of an expansion in @xmath112 . to linear order in @xmath0",
    "the analytical expression for the information carried by @xmath0 input neurons reads : @xmath113 where @xmath114 , @xmath115 are defined , again , as in eqs.([lambda1 ] ) , ( [ lambda2 ] ) . in analogy to what had been done in @xcite",
    "we have set @xmath116 . as evident from the graph ,",
    "also the output information is essentially linear up to a value of @xmath117 , and quasi - linear even for @xmath118 .",
    "it should be remined , again , that our saddle point method only takes into account the term linear in @xmath0 in the information _ input _ units carry about the stimulus .",
    "it is not possible , therefore , for eq.([final_info ] ) to reproduce the saturation in the mutual information as it approaches the entropy of the stimulus set ( which is finite , if one considers only discrete stimuli ) .",
    "the nearly linear behaviour in @xmath1 thus reflects the linear behaviour in @xmath0 induced , in the intermediate quantity ( the information available at the input stage ) , by our saddle point approximate evaluation .    as it is clear from the comparison in fig.[fig1 ] ,",
    "when the two populations of units are affected by the same noise the input information is considerably higher than the output one .",
    "this is expected , since output and input noise sum up while influencing the firing of output neurons , but also because the input distribution is taken to be a pure gaussian , while the output rates are affected by a threshold .",
    "if the input - output tranformation were linear and the output noise much smaller than the input one , one would expect that output and input units would carry the same amount of information .",
    "briefly , in a linear network with zero output noise one has : @xmath119    considering eqs.([xi_eta]),([dist ] ) , an _ effective _ expression for the distribution @xmath120 can be obtained by direct integration of the @xmath51 functions @xmath121 via their integral representation , on @xmath122 : @xmath123 @xmath124 @xmath125    this distribution is then used to evaluate both the equivocation , eq.([equiv ] ) , and the entropy of the responses , eq.([outent ] ) .",
    "we do not report the calculation , that is straightforward and analogous to the one reported in @xcite .",
    "the final result , which is valid for a finite population size @xmath1 , and up to the linear approximation in @xmath126 , is analogous to eq.([input ] ) : @xmath127 thus , we expect that taking the limits @xmath128 and @xmath129 simultaneously in eq.([final_info ] ) , we should get to the same result : the output information should equal the input one when @xmath130 grows large .    from eq.([final_info ] ) it is easy to show that : @xmath131;\\ ] ] when @xmath132 one obtains exactly the linear limit , eq.([asymptot ] ) .",
    "we have verified this analytical limit by studying numerically the approach to the asymptotic value of the mutual information .",
    "fig.[fig3 ] shows the dependence of output information on the output noise @xmath133 , for 4 different choices of the ( reciprocal of the ) threshold , @xmath134 .",
    "a large value , @xmath135 , implies linear output units . as expected , the output information , which always grows for decreasing values of the output noise , for @xmath135 approaches asymptotically the input information .",
    "for increasing values of the output noise , the information vanishes with a typical sigmoid curve , with its point of inflection when the output matches the input noise .",
    "we have then examined how the information in output ( compared to the input ) depends on the number @xmath19 of discrete correlates and on the width of the tuning function ( [ tuning2 ] ) , parametrized by @xmath136 , with respect to the continuous correlate .",
    "fig.[fig4 ] shows a comparison between input and output information for a sample of 10 cells , as a function of @xmath19 .",
    "both curves quickly reach an asymptotic value , obtained by setting @xmath137 in eq.([lambda2 ] ) for @xmath138 .",
    "the relative information loss in output is roughly constant with @xmath19 .",
    "a comparison is shown with the case where correlates are purely discrete , which is obtained by setting @xmath139 in eq.([tuning2 ] ) .",
    "the curves exhibit a similar behaviour , even if the rise with @xmath19 is steeper , and the asymptotic values are higher .",
    "this may be surprising , but it is in fact a consequence of the specific model we have considered , eq.([tuning_tot ] ) , where a unit has the same tuning curve to each of the discrete correlates , only varying its amplitude with respect to a value constant in the angle . as @xmath137 ,",
    "most of the mutual information is about the discrete correlates , and the tuning to the continuous dimension , present for @xmath140 , effectively adds noise to the discrimination among discrete cases , noise which is not present for @xmath139 .    with respect to the continuous dimension",
    ", the selectivity of the input units can be increased by varying the power @xmath136 of the cosine from 0 ( no selectivity ) through 1 ( very distributed encoding , as for the discrete correlates ) to higher values ( progressively narrower tuning functions ) .",
    "fig.[fig5 ] reports the resulting behaviour of the information in input and in output , for the case @xmath141 ( only a continuous correlate ) and @xmath142 ( continuous+discrete correlates ) .",
    "increasing selectivity implies a `` sparser '' @xcite representation of the angle , the continuous variable , and hence less information , on average .",
    "however if the correlate is purely continuous there is an initial increase , before reaching the optimal sparseness . it should be kept in mind , again , that the asymptotic equality of the @xmath141 and @xmath142 cases is a consequence of the specific model , eq.([tuning_tot ] ) , which assigns the same preferred angle to each discrete correlate .",
    "the resolution with which the continuous dimension can be discriminated does not , within this model , improve with larger @xmath19 , while the added contribution , of being able to discriminate among discrete correlates , decreases in relative importance as the tuning becomes sharper .",
    "figures [ fig4 ] and [ fig5 ] show that , as long as the output noise is non zero and the threshold is finite , information is lost going from input to output , but the information loss does not appear to depend on the structure and on the dimensionality of the correlate .    note that , while the purely continuous case has been easily obtained by setting @xmath141 in the expression of @xmath143 , eq.([lambda2 ] ) , for the purely discrete case it is enough to set @xmath139 .",
    "we have attempted to clarify how information about multi - dimensional stimuli , with both a continuous and a discrete dimension , is transmitted from a population of units with a known coding scheme , down to the next stage of processing .",
    "previous studies had focused on the mutual information between input and output units in a two - layer threshold - linear network either with learning @xcite or with simple random connection weights @xcite .",
    "more recent investigations have tried to quantify the efficiency of a population of units in coding a set of discrete @xcite or continuous @xcite correlates .",
    "the analysis in @xcite has been then generalized to the more realistic case of multi - dimensional continuous+discrete correlates @xcite .",
    "this work correlates with both research streams , in an effort to define a unique conceptual framework for population coding .",
    "the main difference with the second group of studies is obviously the presence of the network linking input to output units .",
    "the main difference with the first two papers , instead , is the analysis of a distinct mutual information quantity : not between input and output units , but between correlates ( `` stimuli '' ) and output units . in @xcite",
    "it had been argued , for a number @xmath19 of purely discrete correlates , that the information _ about _ the stimuli reduces to the information about the `` reference '' neural activity when @xmath137 .",
    "the reference activity is simply the mean response to a given stimulus when the information is measured from the variable , noisy responses around that means ; or it can be taken to be the stored pattern of activity , when the retrieval of such patterns is considered , as in @xcite .",
    "true , the information about the stimuli saturates at the entropy of the stimulus set , but for @xmath137 this entropy diverges , only the linear term in @xmath0 is relevant @xcite , and the two quantities , information about the stimuli and information about the reference activity , coincide .",
    "our present saddle point calculation is only able to capture , effectively , the mutual information which is linear in the number of input units , as mentioned above .",
    "it fails to describe the approach to the saturating value , the entropy of the set of correlates , be this finite or infinite .",
    "therefore , ours is close to a calculation of the information about a reference activity - in our case , the activity of the input units .",
    "the remaining difference is that we can take into account , albeit solely in the linear term , the dependence on @xmath19 ( through the equation for @xmath138 , eq.([lambda2 ] ) ) , without having to take the further limit @xmath137 .    due to the presence of a threshold and of a non zero output noise",
    "the information in output is lower than that in input , and we have shown analytically that in the limit of a noiseless , linear input - output transfer function the ouptput information tends asymptotically to the input one .",
    "we have not , however , introduced a threshold in the input units , which would be necessary for a fair comparison . in an independent line of research , recent work @xcite",
    "has also quantified the contribution to the mutual information , in a different model , of cubic and higher order non - linearities in the transfer function , by means of a diagrammatic expansion in a noise parameter . in @xcite",
    "it has been shown that the effect of a threshold in the input units on the input information results merely in a renormalization of the noise .",
    "the resulting effect on the output information remains to be explored , possibly with similar methods .",
    "considering mixed continuous and discrete dimensions in our stimulus set , we had been wondering whether the information loss in output depended on the presence or absence of discrete or continuous dimensions in the stimulus structure .",
    "we have shown that for a fixed , finite level of noise this loss dose not depend significantly on the structure of the stimulus , but solely on the relative magnitude of input and output noise , and on the position of the output threshold .",
    "a recent work has shown that the interplay between short and long range connectivities in the hopfield model leads to a deformation of the phase diagram with the appearence of novel phases @xcite .",
    "it would be interesting to introduce short and long range connections in our model , and to examine how the coding efficiency of output neurons depends on the interaction between short and long range connections",
    ". this will be the object of future investigations ."
  ],
  "abstract_text": [
    "<S> in a previous report we have evaluated analytically the mutual information between the firing rates of @xmath0 independent units and a set of multi - dimensional continuous+discrete stimuli , for a finite population size and in the limit of large noise . here , we extend the analysis to the case of two interconnected populations , where input units activate output ones via gaussian weights and a threshold linear transfer function . </S>",
    "<S> we evaluate the information carried by a population of @xmath1 output units , again about continuous+discrete correlates . </S>",
    "<S> the mutual information is evaluated solving saddle point equations under the assumption of replica symmetry , a method which , by taking into account only the term linear in @xmath0 of the input information , is equivalent to assuming the noise to be large . within this limitation , we analyze the dependence of the information on the ratio @xmath2 , on the selectivity of the input units and on the level of the output noise . </S>",
    "<S> we show analytically , and confirm numerically , that in the limit of a linear transfer function and of a small ratio between output and input noise , the output information approaches asymptotically the information carried in input . </S>",
    "<S> finally , we show that the information loss in output does not depend much on the structure of the stimulus , whether purely continuous , purely discrete or mixed , but only on the position of the threshold nonlinearity , and on the ratio between input and output noise . </S>"
  ]
}