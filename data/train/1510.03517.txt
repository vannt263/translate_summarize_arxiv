{
  "article_text": [
    "determining the optimal well locations and controls in an oil field is a challenging task .",
    "the decision is hard since the reservoir performance is affected by geological , engineering , economical and other parameters @xcite .",
    "optimization algorithms provide a systematic way to solve this problem . by using optimization algorithms , a quality solution",
    "can be achieved automatically and hence reduce the risk in decision - making .",
    "well placement and control optimization generally are computationally expensive and nonconvex , and not every optimization algorithm is appropriate for these problems .",
    "therefore , finding and applying algorithms that are efficient and robust is one of most important tasks in solving well placement and control optimization problems .    in this work ,",
    "we introduce and apply the multilevel coordinate search ( mcs ) algorithm for the problems of optimizing well placement , well control , and joint placement with control .",
    "mcs , introduced by @xcite , is a global optimization algorithm and is designed to handle the complex topography and multimodality of the multidimensional nonlinear objective functions without requiring excessive computing resources .",
    "rios @xcite completed a systematic comparison using a test set of 502 problems and found that mcs outperforms the other 21 derivative - free algorithms tested ( see table [ tab : solver ] ) .",
    "though mcs has shown its superiority in benchmark and real world problems @xcite , to the best of our knowledge , it has not been applied to the optimization of oil field development .",
    "we compare mcs , generalized pattern search ( gps ) , particle swarm optimization ( pso ) , and covariance matrix adaptive evolution strategy ( cma - es ) in four typical test cases from the field of optimal reservoir production development .",
    "our results demonstrate that mcs is strongly competitive and outperforms the other algorithms in most cases .",
    "lll solver & version & language + asa & 26.30 & c + bobyqa & 2009 & fortran + cma - es & 3.26beta & matlab + dakota / direct & 4.2 & c++ + dakota / ea & 4.2 & c++ + dakota / pattern & 4.2 & c++ + dakota / solis - wets & 4.2 & c++ + dfo & 2.0 & fortran + fminsearch & 1.1.6.2 & matlab + global & 1.0 & matlab + hopspack & 2.0 & c++ + imfil & 1.01 & matlab + mcs & 2.0 & matlab + newuoa & 2004 & fortran + nomad & 3.3 & c++ + pswarm & 1.3 & matlab + sid - psm & 1.1 & matlab + snobfit & 2.1 & matlab + tomlab / glccluster & 7.3 & matlab + tomlab / lgo & 7.3 & matlab + tomlab / multimin & 7.3 & matlab + tomlab / oqnlp & 7.3 & matlab +    oil field development optimization has two main sub - problems : well placement optimization , and well control optimization .",
    "these two problems are often treated separately @xcite .",
    "recently , there has been increasing focus on optimizing well placement and control jointly @xcite .",
    "well placement problems aim to optimize the locations of injection and production wells .",
    "the location of each vertical well is parametrized by its plane coordinates @xmath0 , which are usually integers in the reservoir simulator .",
    "well control problems focus on optimizing production scheduling .",
    "the optimization variables are often the time - varying bottom hole pressures ( bhps ) or the flow rates for each well .",
    "the joint problem optimizes well placement and control parameters simultaneously .",
    "thus , the joint problems are more complex and challenging with an increase in the number and type of variables @xcite .    in the past",
    ", a number of algorithms have been devised and analysed for both separate and joint problem of well placement and control optimization .",
    "these algorithms fall into two categories : gradient - based methods and derivative - free methods .",
    "applications of gradient - based methods to oil field problems have been presented in many papers @xcite .",
    "these methods take advantage of the gradient information to guide their search .",
    "the gradient of the objective function can be obtained by using adjoint - based techniques @xcite , or may be approximated by using numerical methods such as finite differences @xcite .",
    "the adjoint method , developed in the 1970s @xcite , is widely used for assisted history matching @xcite and well production optimization @xcite .",
    "gradient based methods have some drawbacks for the well placement and control problem ; these problems are nonconvex and generally contain multiple optima .",
    "for some problems , particularly well placement , the optimization surface can be very rough , which results in discontinuous gradients @xcite .",
    "however , the gradient - based methods are often the most efficient methods especially for the optimal well control problem @xcite .    for the joint well placement and control optimization problem ,",
    "two procedures are proposed and studied .",
    "the first one is a simultaneous procedure , which optimizes over all well locations and control parameters simultaneously .",
    "the second one is a sequential procedure , that decouples the joint problem into the well placement optimization subproblem and the well control placement optimization subproblem .",
    "the simultaneous procedure ensures that the best solution exists somewhere in the search space .",
    "but it may be difficult to find the global optima because the search space may be very large and rough .",
    "the sequential procedure divides the optimization variables into two smaller groups and optimizes separately . for each subproblem ,",
    "the search space is smaller than the simultaneous one , but it can not ensure the best solution exists in the search space because the optimal location depends on how the well is operated and vice - versa .",
    "there are several papers @xcite which demonstrate that the simultaneous procedure is superior to the sequential approach . in @xcite , however , they found that a sequential procedure was competitive and even preferable to the simultaneous approach in some cases . to test this further , we do a further investigation of the effectiveness of these two procedures using a joint placement and control optimization example .    many black - box ,",
    "derivative - free methods have been used in oil field problems @xcite .",
    "typical algorithms include genetic algorithms ( ga ) @xcite , particle swarm optimization ( pso ) @xcite , generalized pattern search ( gps ) @xcite , covariance matrix adaptation strategy ( cma - es ) @xcite and hybrid approaches @xcite .",
    "these algorithms can be classified as either deterministic or stochastic , and provide global or local search .",
    "the stochastic / global approaches have also been hybridized with deterministic / local search techniques .",
    "these algorithms only require the value of objective function and involve no explicit gradient calculations . for smooth objective functions ,",
    "the derivative - free methods generally need more function evaluations to converge to a solution than the gradient - based ones",
    ". however , most of the derivative - free algorithms parallelize naturally and easily , which make their efficiency satisfactory @xcite , and indeed these methods are less likely to become trapped in local optima .",
    "we are particularly interested in using the multilevel coordinate search ( mcs ) algorithm for the following reasons : 1 ) it combines a global search with a local search , which leads to a quicker convergence than many methods that operate only at the global level .",
    "2 ) it is an intermediate between heuristic methods that find the global optimum only with high probability and methods that guarantee to find a optimum with a required accuracy .",
    "3 ) it does not need analytic or numerical derivatives .",
    "4 ) it is guaranteed to converge if the objective is continuous in the neighbourhood of a global minimizer , no additional smoothness properties are required .",
    "5 ) the algorithm parameters in mcs have a clear meaning and are easy to choose .",
    "6 ) it has proved itself in benchmark tests and many real world problems @xcite . based on these features , we believe that mcs has great potential to solve oil field optimization problems , which are nonconvex , nonlinear , and contain many local optima and discontinuities .    in this paper , we apply mcs to optimization problems of varying complexity in terms of the number and type of optimization variables , the dimension and size of the reservoir models , and the number of wells .",
    "we investigate the effect of the algorithmic parameters ( initialization list type , number of levels , and the effect of local search ) on the optimization results .",
    "we propose a detailed comparison between mcs and three other popular derivative - free algorithms ( gps , pso , and cma - es ) .",
    "this paper is organized as follows : section [ sec:2_problem ] describes the formulation of the optimization problems .",
    "sec:3_solver ] gives an overview of the optimization algorithms considered . in section [ sec:4_exam_resu ]",
    "we describe our numerical experiments and the corresponding results . finally , in section [ sec:6_concl ] we provide some concluding remarks .",
    "the objective functions for general oil field development optimization problems are often the net present value ( npv ) or cumulative oil production @xcite .",
    "we use npv as the objective function for all our work .",
    "npv accounts for revenue from the oil and gas produced , and for the cost of handling water production and injection .",
    "the npv is defined as @xmath1,\\ ] ] where @xmath2 , @xmath3 , @xmath4 and @xmath5 are the flow rates of the gas , oil , water produced and water injected for well @xmath6 at time step @xmath7 , respectively ; @xmath8 and @xmath9 are the gas and oil revenue ; @xmath10 and @xmath11 are the costs of produced water and the costs of injected water .",
    "@xmath12 is total number of time steps , @xmath13 is the time at the end of @xmath7th time step , @xmath14 is @xmath7th time step size , @xmath15 provides the appropriate normalization for @xmath13 , e.g. , @xmath16 days , and @xmath17 is the fractional discount rate",
    ".    the quantities @xmath2 , @xmath3 , @xmath4 are functions of the dynamic state variables @xmath18 ( e.g. , pressure , saturation ) , the geological and fluid variables @xmath19 ( e.g. , permeability , porosity , viscosity and density of each phase , etc . ) , the well placement vector @xmath20 , and the control vector @xmath21 . hence equation ( [ eq : npv ] )",
    "can be written as @xmath22 where @xmath23 is a nonlinear response to the input variables ",
    "@xmath23 depends on the output from the numerical solution of a system of nonlinear pdes describing the reservoir response .    for the well placement and control optimization problem",
    ", we wish to maximize the net present value @xmath23 by adjusting the placement vector @xmath20 and the well control vector @xmath21 subject to the constraints @xmath24 @xmath25 @xmath26 @xmath27 @xmath28 equation ( [ eq : constr1 ] ) represents the reservoir simulation equations .",
    "this constraint ensures that the governing reservoir flow equations are satisfied .",
    "equation ( [ eq : constr2 ] ) describes the nonlinear constraint functions ( e.g. , distance between wells , well water cuts , facility constraints such as field - level injection limits , etc . ) .",
    "equations ( [ eq : constr3][eq : constr5 ] ) are the equality , inequality , and bound constraints of well placement vector and well control vector , respectively .",
    "these constraints are related to , for example , the capacity limitations of the wells and/or facilities .    due to the complexity of the well placement and control optimization problem , the objective function",
    "is not convex , it may have many minima , maxima , and saddlepoints .",
    "furthermore , the objective surface is very rough , it is therefore hard to extract gradient information .    to illustrate this ,",
    "a two - dimensional heterogeneous reservoir model is selected .",
    "the model uses 50@xmath2950 grid blocks with four producers , one in each of the four corners .",
    "we optimize the location of one injection well .",
    "the npv surface map is generated numerically by putting a single injection well at each grid block and computing the npv of production from the reservoir .",
    "the npv surface map and contour map are given in fig .",
    "[ fig : npvmap ] . from the figure",
    "we can see that the npv surface is noisy , and includes at least five local optima .",
    "this optimization problem has only two variables ; it can be regarded as the simplest problem in well placement and control optimization . for more complex problems ,",
    "the objective surface has far more roughness with more local optima . @xcite and @xcite also discuss the roughness of the npv surface .      in the well placement optimization problem",
    ", we seek to determine the optimal locations for a specified number of vertical wells , or the optimal trajectories for a specified number of 3-d angled wells .",
    "the optimization problem studied here can be expressed as    @xmath30    for vertical wells , the location of each well is given by its plane coordinates @xmath0 .",
    "thus the total number of variables for well placement vector @xmath20 is @xmath31 , where @xmath32 is the total number of wells , and @xmath33 and @xmath34 are the number of production and injection wells , respectively .",
    "the placement of each of 3-d angled well is parameterized by 6 variables , @xmath35 , @xmath36 , @xmath37 , @xmath38 , @xmath39 , and @xmath40 @xcite .",
    "the coordinates of the well heel are given by @xmath35 and @xmath36 , and the depth of the well heel is given by @xmath37 .",
    "@xmath38 is the length of the well .",
    "@xmath39 is the angle of the well in the @xmath35-@xmath36 plane , and @xmath40 is the angle of the well makes with the horizontal plane .",
    "for this 3-d angled well placement optimization problem , the total number of variables is @xmath41 .",
    "the parameterizations for vertical wells and 3-d angled wells are illustrated in fig .",
    "[ fig : wellpara ] .    as in section [ sec:21_genproblem ] ,",
    "the general oilfield development optimization problem includes a set of constraints . for our well placement and control optimization problem , we assume only bound constraints are imposed explicitly .",
    "the governing reservoir flow equations , equation ( [ eq : constr1 ] ) , are always satisfied since we use a reservoir simulator to calculate the objective function value .",
    "some complex constraints , for example , the minimum distance between wells may be naturally enforced since , if two wells are very close , the npv will generally not be high .",
    "alternatively , one can impose the minimum distance as explicit constraint as in @xcite .",
    "the effect of imposing such constraints explicitly or implicitly on the optimization needs further research . in this paper",
    "we choose not to explore this topic .",
    "we also require that wells are not outside the reservoir boundary . for reservoirs with irregular boundaries ,",
    "this is a nonlinear constraint .",
    "if a vertical well is placed outside the reservoir , or the 3-d angled well is drilled at inactive gridblocks , it can not produce oil .",
    "this leads to a low npv and will be eliminated by the optimization algorithm .",
    "some complex constraints can be transformed to bound constraints .",
    "the placement of a 3-d angled well can be parameterized either by @xmath42 or by @xmath43 , where @xmath44 and @xmath45 are the heel point and the toe point of well , respectively @xcite . using the latter approach ,",
    "the constraints of the maximum and minimum well length can be written as @xmath46 which are two nonlinear inequality constraints .",
    "for the first approach , the maximum and minimum well length can be stated as bound constraints .",
    "choosing how to enforce constraints requires some experience , and are one of the main difficulties in formulating and solving optimization problems .",
    "most of the optimization algorithms considered in this paper were originally designed for either the unconstrained optimization problem or problems with bound constraints only .",
    "while constraints can decrease the size of the search space , treating constraints explicitly does often burden the computation .",
    "the coordinates of each well are real variables in actual oilfields , but are usually treated as integers ( i.e. gridblock locations ) in reservoir simulators .",
    "the well coordinate variables in our work are continuous but will be rounded before we pass them to the simulator to evaluate the objective function .",
    "this leads a discontinuity in npv as the well moves across the boundary between two gridblocks .",
    "optimization becomes problematic when there are discontinuities in objective function surface while using gradient - based algorithms . the derivative - free algorithms considered in this paper , mcs , gps , pso , and cma - es ,",
    "do not need this gradient information and hence , can be used in this case . however , restricting the real locations to integer values does introduce an error in the optimized location .",
    "the error depends on the gridblock size , and is acceptable in an engineering sense . @xcite and @xcite give ways to deal with this discontinuity in the npv while optimizing well placement .      the well control optimization problem aims to determine the optimal time - varying well setting for each of the production and injection wells .",
    "the optimization problem can be stated as follows :    @xmath47    where @xmath48 denotes the well control vector .",
    "each well can be controlled either by well rate or by bottom hole pressure ( bhp ) .",
    "the time - varying well controls are represented by piecewise functions in time with @xmath12 intervals .",
    "the number of variables for this problem is @xmath49 .    as in well placement",
    "optimization problem , only bound constraints are considered explicitly here .",
    "the joint problem optimizes both well locations and controls .",
    "the optimization problem is defined as follows : @xmath50 where @xmath51 and @xmath48 denote the well placement and control vectors .",
    "two procedures are commonly used for joint well placement and control optimization  a simultaneous procedure or a sequential procedure .",
    "the simultaneous procedure optimizes well locations and controls simultaneously , hence the number of optimization variables , @xmath52 for vertical wells and @xmath53 for 3-d angled wells , is larger than the individual problems , which makes the optimization more difficult .    in the sequential procedure",
    ", well placement is optimized first using some reasonable control scheme .",
    "the controls are then optimized for the wells using the best locations found in the first stage .",
    "these two stages may be repeated .",
    "the sequential procedure decouples the joint problem into two separate subproblems , and the difficulty for each subproblem is decreased .",
    "the number of optimization variables for the well placement stage is either @xmath31 or @xmath41 ( for vertical or angled well ) , and is @xmath49 for the well control stage .",
    "the joint problem is worth studying because the optimal location of each well depends on how the well is operated and vice - versa @xcite .",
    "furthermore , @xcite point out that the results obtained also depend directly on the specified reservoir life time , which makes the joint optimization problem even more complicated .",
    "we use a predefined reservoir life for examples in this paper .",
    "mcs , first proposed by huyer and neumaier @xcite , was inspired by direct @xcite .",
    "mcs is a branching scheme which searches by recursively splitting hyperrectangles . like",
    "direct , mcs is a mathematical programming approach which provides a systematic search within the bound constraints ( the bounds can be infinite for mcs ) .",
    "mcs builds upon direct by introducing a multilevel mechanism which allows a balanced global and local search .",
    "direct has no local search capability .",
    "levels are assigned as an increasing function of the number of times a box has been split .",
    "the global search portion of the algorithm is accomplished by splitting boxes that have not been searched often  those with a low level . within a level",
    "the boxes with the best function values are selected to complete a local search .",
    "the local search builds a quadratic model , determines a promising search direction and performs a line search .",
    "this allows for quicker convergence while the global part of the algorithm identifies a region near the global optimum .",
    "mcs allows for a more irregular splitting than direct , giving preference to regions with low function values .",
    "convergence to a global optima is guaranteed as the number of levels goes to infinity if the objective function is continuous around the global optimizer .",
    "@xcite reports that mcs works well in problems where the global optimum can be constrained by finite bound constraints .",
    "@xcite report very good performance in the early search phase with a small budget of objective function evaluations .",
    "mcs provides numerous heuristic enhancements over direct .",
    "consider a @xmath54-dimensional bound constrained minimization problem @xmath55 where @xmath56 denotes the objective function ; @xmath57 $ ] denotes the optimization variables ; @xmath58 $ ] and @xmath59 $ ] are the lower and upper bound , respectively .",
    "the pseudocode of the basic steps of mcs are described in alg .",
    "[ algomcs ] .",
    "a complete description of the algorithm is quite complex and can be found in @xcite .",
    "for the experiments in this paper , we used the implementation of mcs from @xcite .",
    "the algorithm was originally designed to minimize a function @xmath56 . to maximize a function @xmath56 we simplify minimize @xmath60 . during the initialization portion of the algorithm",
    ", mcs accepts an initialization list which is used to produce an initial set of boxes partitioning the search space . for @xmath6th coordinate , an initialization list stores @xmath61 points @xmath62 $ ] ( @xmath63 ) .",
    "users can incorporate their knowledge and experience to choose points with a high possibility of obtaining good solution .",
    "mcs continues to process and split boxes until some boxes reach level @xmath64 .",
    "a box can be split either by rank or by expected gain , depending on the relationship between the box level @xmath65 and number of split times at each coordinate .",
    "if a box has been split many times and reached a high level , mcs selects the coordinate which has been split the fewest times and splits along this coordinate ( split by rank ) . if the level of a box is not high , mcs splits along a coordinate with the maximum expected gain according to a quadratic model obtained by fitting function values ( split by expect gain ) .",
    "the parameter @xmath64 controls the precision of the global search phase before any local search would be attempted .",
    "mcs also has the option to turn off the local search phase .",
    "we provide a simple example to see how mcs works .",
    "we consider the objective function @xmath66 , which is a six - hump camel function with 2 unknowns .",
    "the bounds for the 2 unknowns are @xmath67 $ ] and @xmath68 $ ] .",
    "the global minimizer for this function is @xmath69 $ ] and the global minimum value is @xmath70 .",
    "we choose the default parameter settings for mcs : @xmath71 for @xmath72 , the initialization list @xmath73 , @xmath74 , @xmath75 , a maximum number of level @xmath76 , and we turn the local search phase on .",
    "[ fig : mcs_procedure ] shows a loop of mcs for this problem .",
    "[ fig : subfig : mcs_init ] presents the boxes after the initialization procedure ( lines 14 of alg .",
    "[ algomcs ] ) . by using the default initialization list ,",
    "mcs first splits the root box along the @xmath35-coordinate at the midpoint , the two boundary points , and the points between determined by the golden ratio .",
    "then mcs chooses the new box that has the highest estimated variability and splits it along the @xmath36-coordinate .",
    "we note that the initialization list can also be specified by the user .",
    "different initialization lists results in a different split of the boxes .",
    "one other commonly used initialization list is @xmath77 , @xmath74 , @xmath78 . by using this initialization list ,",
    "the boxes after the initialization procedure are shown in fig .",
    "[ fig : mcs_algoinit2 ] .",
    "the initialization list can also be generated automatically with the aid of line searches .",
    "starting from the point @xmath79|$ ] , for @xmath6th coordinate , we complete line searches along this coordinate to find up to @xmath80 minima within @xmath81 function evaluations .",
    "all local minimizers found by line searches are put into the initialization list .",
    "if the number of minima is less than @xmath80 , then we use the points closest to @xmath82 and @xmath83 obtained by line searches to supplement the initialization list .",
    "the best point is taken as the start point for line searches for the next coordinate .",
    "we choose @xmath84 and @xmath85 as recommend by @xcite for all examples in this paper . with this setting",
    ", mcs needs up to @xmath86 function evaluations to generate the initialization list .",
    "this stage will slow down the convergence of the optimization in the early stages , however , such an initialization list can ultimately improve the performance of mcs significantly .",
    "hence , to get the advantages of the line search , the total number of function evaluations needs set to a number much larger than @xmath86 . after the initialization procedure",
    ", the search space will be further split until one of the boxes reaches the maximum level @xmath64 .",
    "[ fig : subfig : mcs_split ] shows the boxes after the splitting procedure . as mentioned above",
    ", @xmath64 decides the depth to which mcs explores a region and hence controls the precision of the global search phase .",
    "if @xmath87 , then the boxes obtained are shown in fig .",
    "[ fig : mcs_split2 ] .",
    "once a box reaches the maximum level , a local search starts from its base point .",
    "[ fig : subfig : mcs_ls ] shows the points evaluated by the local search .",
    "the local search stops if the maximum number of steps in local search is reached , or the stopping criteria , @xmath88 , where @xmath56 and @xmath89 are the current and the smallest values of objective function , respectively , is triggered .",
    "the maximum number of steps is 50 , and @xmath90 , by default .",
    "after that , mcs will cycle back to the split procedure .",
    "mcs was originally designed for either unconstrained optimization problems or problems with bound constraints only . in general",
    ", there are many natural ways to attempt to extend unconstrained optimization algorithms to handle constraints .",
    "for example , the penalty function method @xcite , lagrange multiplier method @xcite , and so on . to the best of our knowledge , there has been no such paper which has addressed this topic for mcs .",
    ", @xmath74 , @xmath78.,scaledwidth=45.0% ]    .,scaledwidth=45.0% ]      in order to analyse the sensitivity of the parameters in the mcs algorithm , we apply mcs , with 7 different settings of the parameters , to our examples .",
    "the 7 settings used are :    * mcs-1 : mcs with its default settings from @xcite .",
    "a simple initialization list is used consisting of midpoints and boundary points , i.e. @xmath91 . the number of levels is chosen as @xmath92 , where @xmath54 is the dimension of the problem .",
    "the maximal number of visits in the local search is 50 , and the acceptable relative accuracy for local search is @xmath90 .",
    "* mcs-2 : mcs with the initialization list @xmath93 . unlike the initialization list in mcs-1 ,",
    "the points here are uniformly spaced but do not include the boundary points .",
    "the other settings are same as in mcs-1 . * mcs-3 : mcs with an auto - generated initialization list . in mcs-3",
    ", we first perform a sequence of line searches along all coordinate directions to generate the initialization list .",
    "the other settings are same as in mcs-1 .",
    "* mcs-4 : mcs with the initialization list @xmath94 . unlike the initialization list in mcs-1",
    ", we use an user defined initial guess @xmath95 instead of the midpoints .",
    "the other settings are same as in mcs-1 . *",
    "mcs-5 : mcs with the initialization list @xmath96 .",
    "unlike the initialization list in mcs-2 , we use an user defined initial guess @xmath95 instead of the midpoints .",
    "the other settings are same as in mcs-2 .",
    "* mcs-6 : mcs with a larger maximum number of levels , @xmath97 .",
    "this is chosen to attempt to improve the global search phase .",
    "the other settings are same as in mcs-4 . *",
    "mcs-7 : mcs without the local search phase . in mcs-7 , we set the maximal number of visits in the local search to 0 .",
    "the other settings are same as in mcs-4 .      for comparison three algorithms",
    " generalized pattern search ( gps ) , particle swarm optimization ( pso ) and covariance matrix adaptation evolution strategy ( cma - es )  are used .    generalized pattern search",
    "( gps ) @xcite is a deterministic local search algorithm .",
    "it does not require gradients and hence , it can be used on problems that are not continuous or differentiable . for the parameter settings ,",
    "we use a @xmath98 positive spanning set , where @xmath54 is the dimension of the search space .",
    "the expansion factor is set to 2 , and the contraction is set to 0.5 @xcite .",
    "particle swarm optimization ( pso ) @xcite is a population - based stochastic search method .",
    "pso s search mechanism mimics the social behavior of biological organisms such as a flock of birds .",
    "pso can search a very large space of candidate solutions , which reduces the chance of getting trapped at an unsatisfactory local optimum .",
    "the performance of pso depends on the values assigned to the algorithm parameters .",
    "following the work of @xcite , our implementation of pso uses the population size of 50 , and the weighting parameters @xmath99 , @xmath100 , and @xmath101 .",
    "the best parameters are usually problem dependent .",
    "further tuning , for a specific problem , will likely yield superior performance . further discussion on this issue can be found in @xcite and @xcite . the covariance matrix adaptation strategy ( cma - es ) @xcite is a population - based stochastic optimization algorithm . unlike a genetic algorithm ( ga ) ,",
    "pso , and other classic population - based stochastic search algorithms , candidate solutions of cma - es are sampled from a probability distribution which are updated iteratively . for cma - es ,",
    "we use the settings from @xcite ( see table [ tab : cmaes_para ] ) .",
    "in fact , according to their work , cma - es doses not require significant parameter tuning for its application .",
    "cc parameter & value + @xmath102 & @xmath103 + @xmath104 & @xmath105 + @xmath106 & @xmath107 + @xmath108 & @xmath109 + @xmath110 & @xmath111 + @xmath112 & @xmath113 + @xmath114 & @xmath115 +    pso and cma - es are stochastic algorithms and the result of each trial is different .",
    "thus , in order to assess the overall performance of these algorithms , we run each of the algorithms many times for each test example .",
    "the objective function evaluations for the well placement and control optimization problems require the evaluation of a numerical oil reservoir simulator . the cost of the total optimization process is completely dominated by the cost of each simulation evaluation .",
    "the time spent in the nuts and bolts of the optimization algorithm itself can be neglected . as a result",
    "we choose to use the number of simulation runs as our performance indicator to compare the optimization strategies .",
    "this is a widely used measure in the well placement and control optimization literature @xcite .",
    "it is also worth mentioning that , gps and cma - es need an initial guess to start the optimization processes .",
    "pso can generate an initial population automatically . in our four examples",
    ", we use a physically reasonable initial guess for gps , pso , and cma - es . for mcs , the initial point is determined by the initialization list .",
    "either the initialization list includes the initial guess ( mcs-4 , mcs-5 ) or an ordinary initialization list is used ( mcs-1 , mcs-2 , mcs-3 ) .      for the joint well placement and control optimization problem , we consider both a simultaneous procedure and a sequential procedure .",
    "the simultaneous procedure optimizes over the well locations and controls simultaneously . to solve the joint problem with a simultaneous approach we consider the 7 different configurations of mcs , and also gps , pso , and cma - es .",
    "the sequential procedure divides the optimization process into a well placement optimization stage and a well control optimization stage .",
    "each stage is an independent optimization problem and can be optimized using the same or different algorithms .",
    "we label the approaches used for the sequential procedure in the form _",
    "algorithm1-algorithm2 _ , where algorithm1 denotes the algorithm used for the well placement optimization stage and algorithm2 denotes the algorithm used for the well control optimization stage",
    "many such combinations are possible .",
    "the combinations considered in this paper are divided into 3 groups .",
    "the first group includes mcs - mcs , gps - gps , pso - pso , and cma - es - cma - es , which use the same algorithm in both the well placement stage and the well control stage .",
    "the second group includes mcs - gps , mcs - pso , and mcs - cma - es , which use mcs for the well placement stage .",
    "the third group gps - mcs , pso - mcs , and cma - es - mcs uses mcs for the well control stage .",
    "the first example uses the punq - s3 model , which is a small reservoir model based on an actual north sea reservoir @xcite .",
    "the model uses @xmath116 grid blocks with @xmath117 m and 1761 active grid blocks .",
    "the simulation model involves a three - phase gas - oil - water flow .",
    "the field initially contains 6 production wells and no injection wells due to the strong aquifer .",
    "[ fig : punqs3 ] shows the depth of the top face and permeability field , together with the initial well locations of punq - s3 model .",
    "the reservoir production time is 20 years , the bottom hole pressure of each well is fixed at 200 bar .    we seek to optimize the well locations of all 6 wells .",
    "the formulation of the well placement optimization problem is given in section [ sec:22placementop ] .",
    "the objective function is the net present value ( npv ) .",
    "the simulator used to predict the production dynamics ( the flow rates of the gas , oil , water produced and water injected ) is eclipse @xcite , a commercial reservoir simulator from schlumberger ltd .",
    "the economic parameters used to calculate npv are given in table [ tab : eco_set_1 ] .",
    "every well has two positional variables which gives a total of 12 optimization variables .",
    "only bound constraints are considered in this example .",
    "we force @xmath118 and @xmath119 for all 6 wells .",
    "the optimization problem was solved by using gps , pso , cma - es , and all 7 configurations of mcs .",
    "the maximum number of simulation runs is set to 600 .",
    "cc parameter & value + gas revenue ( usd/@xmath120 ) & 0.5 + oil revenue ( usd/@xmath120 ) & 500.0 + water - production cost ( usd/@xmath120 ) & 80.0 + annual discount rate & 0 +      the results of example 1 are shown in table [ tab : e1_result ] . in this table , the final npv after 600 simulation runs for each algorithm is given .",
    "moreover , for pso and cma - es , the maximum , minimum , mean , median , and standard deviation of npv are given . from the table",
    "we can see that gps obtains the highest npv value after the 600 simulation runs .",
    "though the maximum npv for pso and cma - es are slightly higher than mcs in some cases , mcs generally performs better than pso and cma - es when compared to the mean and the median npv .",
    "plots of the npv for the four algorithms versus the number of simulation runs are shown in fig .",
    "[ fig : e1_algo ] . note that for pso and cma - es , 10 trials are performed and the solid lines depict the median npv over all 10 trials .",
    "since gps and mcs are deterministic algorithms , only one trial is performed .    from fig .",
    "[ fig : e1_algo ] we can see that mcs showed excellent convergence speed at the early stage of optimization ( simulation runs @xmath121200 ) . gps converges slower than mcs at an early stage , but eventually gps obtains the highest npv .",
    "the two stochastic algorithms , pso and cma - es , showed slow convergence speed at an early stage of the optimization process .",
    "since pso and cma - es are stochastic algorithms , the performance is different for each trial .",
    "[ fig : e1_psocmaes ] shows the range of npv found amongst the trials of pso and cma - es . in this figure ,",
    "the areas between the maximum and minimum npv are shaded for pso and cma - es .",
    "it is clear that the npv obtained by pso and cma - es has a high variation for this example .",
    "this suggests that when solving this problem by pso or cma - es , a single trial has a high risk to obtain an unsatisfactory npv .",
    "the 7 different mcs configurations are also tested with this example .",
    "detailed results are shown in fig .",
    "[ fig : e1_mcs ] .",
    "these algorithms are divided into 3 groups .",
    "the first group ( mcs-1 , mcs-2 , mcs-3 , mcs-4 , and mcs-5 ) uses different initialization lists .",
    "this allows us to check the impact of the initialization list .",
    "the second group ( mcs-4 , mcs-6 ) uses a different maximum number of levels , @xmath64 .",
    "the higher @xmath64 , the better the global search ability .",
    "the third group ( mcs-4 , mcs-7 ) is used to analyse the role of local search in mcs .    from the first group",
    "mcs-3 ultimately achieves the highest npv , followed by mcs-4 and mcs-2 , and then mcs-5 and mcs-1 .",
    "the ultimate difference in npv between the seven configurations is small ( about 5% ) .",
    "starting from a relatively low npv , mcs-1 obtains a npv slightly smaller than mcs-4 .",
    "the npvs obtained by mcs-2 and mcs-5 are similar .",
    "this shows that mcs with a good initial guess in the initialization list has an advantage over the others , but the advantage is very slight with a large computational budget .",
    "mcs-4 and mcs-5 , starts the optimization with a relatively high npv , and has a significant advantage over mcs-1 and mcs-2 when the computational budget is limited .    comparing mcs-1 and mcs-2",
    ", mcs-2 converges faster than mcs-1 , and obtains higher npv than mcs-1 ultimately .",
    "this indicates that the uniformly spaced initialization list without boundary points is more suitable . to explore this ,",
    "we normalize the search space to the @xmath122$]-interval and map the initialization lists and the global optima to the normalized search space in fig .",
    "[ fig : e1_ila ] .",
    "it is clear that the optimal solution is aligned better with initialization list ii ( mcs-2 ) , which explains its better performance for this problem .",
    "the optimal solution is not known a priori in most cases , so although a suitable initialization list can improve the performance of mcs , it is difficult to choose between mcs-1 and mcs-2 a priori . for mcs-4 and mcs-5 ,",
    "the difference in the ultimate npv is small .",
    "this indicates that with a good initial guess , the importance of the initialization list decreases .",
    "the second group , mcs-4 and mcs-6 , compares the performance of mcs with different specified maximum levels @xmath64 .",
    "mcs with a larger number of maximum levels , namely @xmath97 ultimately obtains a higher npv than @xmath92 .",
    "the performance of mcs with and without local search are compared in the third group with mcs-4 and mcs-7 .",
    "the convergence speed of mcs without local search is severely decreased and the maximum npv found is reduced .    the initial well locations and the optimized well locations are shown in fig .",
    "[ fig : e1_loc ] . in this example",
    ", the initial locations are chosen from reasonable positions given by industry ",
    "locations used in actual production @xcite .",
    "as we can see , the wells are drilled around the gas cap .",
    "the optimized well locations are still located around the gas cap .",
    "this is reasonable from a petroleum engineering perspective since the gas cap can keep the pressure up and drive oil to the well bores .",
    "[ fig : e1_fopt ] shows the cumulative gas , oil , and water production using the initial well locations and optimized locations versus time .",
    "it is clear that the optimized well locations can produce more oil and less water .",
    "the cumulative gas for optimized well locations is lower , this can keep the reservoir pressure higher and drive more oil .",
    "this example uses the egg model which has been used in numerous papers related to well placement and control optimization @xcite .",
    "the model uses @xmath123 grid cells of which 18,553 cells are active .",
    "the details of the geological and fluid parameter settings of egg model can be found in @xcite .",
    "[ fig : eggmodel ] shows the reservoir model , displaying the permeability and the default placement of wells . note that the model was modified slightly to make it more suitable for production with horizontal wells and angled wells .",
    "the grid block size is set to @xmath124 , and the net to gross thickness ratio is set to 0.2 .",
    "we optimize the placement of 12 3-d angled wells ( 8 producers and 4 injectors ) for this example .",
    "the total number of variables is 72 .",
    "we use the default well placement setting from @xcite as the initial guess for our optimization problem .",
    "that is , the initial @xmath35 and @xmath36 are obtained from the default egg model as shown in fig .",
    "[ fig : eggmodel ] .",
    "the initial @xmath37 is set to 1 which means the heel of each well lies in the top layer .",
    "the initial well length , @xmath38 , is set to 60 m. initially , we choose @xmath125 and @xmath126 , that is each well is vertical initially .",
    "the constraints for this problem include :    1 .",
    "@xmath35 and @xmath36 , the coordinates of each well , are between 1 to 60 ; 2 .",
    "@xmath37 , the depth of the well heel , is between 1 to 7 ; 3 .",
    "@xmath38 , the length of the well , is between 50 to 300 m ; 4 .",
    "the angle @xmath39 lies between 0 to @xmath127 ; 5 .",
    "the angle @xmath40 of each well lies between 0 and @xmath128 .",
    "the reservoir simulation time is 20 years .",
    "all wells are controlled by bottom hole pressure : 395 bar for production wells and 410 bar for injection wells .",
    "the objective function is the npv and the related economic parameters are given in table [ tab : eco_egg ] .",
    "cc parameter & value + base drill cost ( usd / well ) & 25 m + drilling cost ( usd / m ) & 50,000 + gas revenue ( usd/@xmath120 ) & 0.5 + oil revenue ( usd/@xmath120 ) & 500.0 + water - production cost ( usd/@xmath120 ) & 80.0 + water - injection cost ( usd/@xmath120 ) & 80.0 + annual discount rate & 0 +      the results of example 2 are shown in table [ tab : e22_result ] . in this table ,",
    "the ultimate npv after 10000 simulation runs for each algorithm is given .",
    "moreover , for pso and cma - es , the maximum , minimum , mean , median , and standard deviation of npv are shown . from the table",
    "we can see that unlike the simple example 1 , pso eventually outperforms the other algorithms .",
    "this is because the search space for this problem is much larger than the simple examples , and pso has good ability explore the entire search space .",
    "mcs outperforms the other algorithms with a limited computational budget and without the variability inherent in pso .",
    "plots of the npv for the four algorithms versus the number of simulation runs are shown in fig .",
    "[ fig : uncwop_algo ] .",
    "note that for pso and cma - es , 5 trials are performed and the solid lines depict the median npv over all 5 trials . from the figure we can see that mcs showed excellent convergence speed at the early stage of optimization ( simulation runs @xmath1212000 ) .",
    "this is useful for optimization given a limited computational budget .",
    "[ fig : uncwop_psocmaes ] shows the range of npv found amongst the trials of pso and cma - es . for pso and cma - es ,",
    "most trials converge to a npv between 8 to 9@xmath129usd .",
    "pso is able to find a much higher npv ( about 11@xmath129usd ) .",
    "this shows that the problem is hard and most algorithms have only converged to a local optima .",
    "it also shows that pso potentially has a better ability for this type of hard problem .",
    "[ fig : e22_mcs ] shows the performance of the different configurations of mcs . using the default initialization list , as in mcs-1 and mcs-2 gives an unreasonable initial guess . mcs-4 and",
    "mcs-5 use a reasonable initial guess leading to a much improved performance . despite",
    "this mcs-2 eventually obtains the highest npv . in fig .",
    "[ fig : e22_mcs_initanalysis ] we normalize the search space to the [ 0,1]-interval and map the all optimization candidates ( denoted by circles ) and the optimal solutions ( denoted by crosses ) of mcs-1 and mcs-2 to the normalized search space . from the figure",
    "we can see that 46 out of 72 optimization variables in the optimal solution obtained by mcs-1 after 10000 function evaluations are still located in the positions defined by the initialization list .",
    "this occurs for 23 out of 72 variables for mcs-2 . for this problem ,",
    "an uniformly spaced initialization list , not containing any boundary points , is more suitable than an initialization list with boundary points .",
    "mcs-3 uses line search to generate the initialization list automatically .",
    "the optimization starts with a very low npv , but eventually obtains a high npv .",
    "this configuration is highly recommended for problems where the users can not provide a good initial guess .",
    "mcs-6 , using a larger maximum number of levels ( @xmath97 ) , performs better than mcs-4 with @xmath92 . for a large scale optimization problem ,",
    "a larger maximum number of levels is recommended .",
    "mcs-7 , which turns off the local search phase , performs a little worse than mcs-4 .    the final oil saturation distribution for the initial well placement and the optimal well placement are shown in fig .",
    "[ fig : e22_well ] .",
    "all wells are vertical for the initial well placement . for the optimal well placement obtained by the optimization algorithm ,",
    "some wells are angled . from the final oil saturation distribution",
    ", we can see that the well placement obtained by the optimization algorithm gives a larger sweep area , and obtains higher production performance eventually .",
    "this example from @xcite uses a single - layer reservoir model with @xmath130 uniform grid blocks with @xmath131 m and @xmath132 m .",
    "the model consists of four production wells and one injection well .",
    "the wells form a five - spot well pattern .",
    "we consider an oil - water two phase flow in this model .",
    "the permeability field and well placements are shown in fig .",
    "[ fig : e1_perm ] .",
    "there are two high permeability zones and two low permeability zones in the model .",
    "the permeabilities are @xmath133 and @xmath134 , respectively .",
    "detailed reservoir information is given in table [ tab : e2_rpara ] .",
    "cc parameter & value + reservoir grid & @xmath135 + grid size ( m ) & @xmath136 + porosity & 0.2 + net - to - gross ratio & 0.2 + initial oil saturation & 0.8 + initial pressure ( @xmath137 ) & 200 + oil viscosity ( @xmath138 ) & 0.42 + water viscosity ( @xmath138 ) & 1.7 +    the reservoir lifetime is set to 720 days . with a fixed injection rate of @xmath139 for well inj-01 , we seek to optimize the liquid rates of four production wells .",
    "two variations of this optimization problem are considered .",
    "* case 1 : each well is produced under a liquid rate throughout its lifetime .",
    "this gives 4 optimization variables in total .",
    "* case 2 : the liquid rate for each well is updated every 90 days ( 8 control periods ) .",
    "this gives 32 optimization variables in total .",
    "the objective function we use for this example is npv and the corresponding economic parameters are the same as in example 1 and are given in table [ tab : eco_set_1 ] .",
    "only bound constraints are considered and the detailed optimization parameters are given in table [ tab : e2_opara ] .",
    "ccc parameter & case 1 & case 2 + variables & 4 & 32 + initial rate ( pro-01 , pro-03 ) ( @xmath140 ) & 20 & 20 + initial rate ( pro-02 , pro-04 ) ( @xmath140 ) & 40 & 40 + minimum rate ( @xmath140 ) & 0 & 0 + maximum rate ( pro-01 , pro-03 ) ( @xmath140 ) & 40 & 40 + maximum rate ( pro-02 , pro-04 ) ( @xmath140 ) & 80 & 80 +      the results for the two cases of example 3 are shown in table [ tab : e20_result ] and table [ tab : e23_result ] .",
    "case 1 uses a maximum of 400 simulations to optimize 4 variables and case 2 uses a maximum of 3200 simulations to optimize 32 variables .    the initial guess for the optimal control strategy for gps , pso , and cma - es is the point half way between the lower and upper bounds . coincidentally ,",
    "mcs with its default settings also uses the middle value as its start point .",
    "so for this example , mcs-1 and mcs-4 are identical , and mcs-2 and mcs-5 are identical .",
    "hence , we omit mcs-4 and mcs-5 while analyzing the results . from table [",
    "tab : e20_result ] we can see that for case 1 , all algorithms gps , pso , cma - es and mcs ( except for configuration mcs-7 ) are able to obtain a high npv value at the end of the optimization , and the ultimate difference between the algorithms is small . the mean and median npv found by pso is slightly smaller than the other algorithms . for case 2 , similar conclusions can be drawn from table [ tab : e23_result ] .",
    "after 3200 simulation runs , gps obtains the highest npv .",
    "cma - es and mcs ( again except for configuration mcs-7 ) are in the middle , while pso performs the worst .    plots of the npv of the four algorithms versus the number of simulation runs are shown in fig .  [ fig : e2_algo ] . as in example 1",
    ", 10 trials are performed for pso and cma - es , and the solid lines depict the median npv over all 10 trials of these two algorithms .    from fig .",
    "[ fig : e2_algo ] we can see that , the final npv obtained by mcs is not the highest over all algorithms tested , however , once again mcs outperforms when the number of simulation runs is limited and is not affected by the variability of the other algorithms .",
    "when the number of simulation runs is limited to 15% of the final number of simulation runs ( 60 simulation runs for case 1 and 480 simulation runs for case 2 ) , the npv obtained by each algorithm is given in table [ tab : e2_result_limited ] .",
    "note that in this table we use the median npv of 10 trials for pso and cma - es .",
    "we use the median instead of the mean because it is less sensitive to outliers in the data .",
    "when the total number of simulation runs is limited , mcs showed significant advantages over pso , gps , and cma - es . again mcs-7 provides poor results  showing the importance of the local search feature within mcs .",
    "this table shows the potential of mcs with a low computational budget .",
    "as we progress from case 1 to case 2 , the number of optimization variables increases from 4 to 32 . the performance of gps with a low number of simulation runs decreases .",
    "in case 2 , the maximum npv found by gps is less than the other 3 algorithms when the number of simulation runs is limited to 1000 .",
    "after 1000 simulation runs , gps is able to find a higher npv than pso .",
    "the early stage of the optimization process mainly reflects the global search phase , and the later stage of the optimization process includes the effect of the local search phase for the algorithms tested . in case 2 , it is clear that pso performs better than gps at an early stage , but gps outperforms later .",
    "overall , mcs , which includes both a global search phase and a local search phase , showed a better convergence rate than gps and pso .",
    "[ fig : e2_psocmaes ] shows the range of npv for the trials for pso and cma - es for example 3 . in this figure ,",
    "the areas between the maximum and minimum npv are shaded for pso and cma - es . from this figure",
    "we can see that for case 1 , the range of the best npv is large initially and then the range decreases for both pso and cma - es .",
    "cma - es has a small variability near convergence . for case 2 , with a larger number of optimization variables than case 1",
    ", the range of npv does not decrease for pso .",
    "each trial falls into a local optima and has a difficult time to escape .",
    "the range for cma - es decreases to near zero .",
    "this indicates that for pso and cma - es , a large computational budget can decrease the performance variability for this example .",
    "compared to cma - es , pso more easily falls into a local optima for problems with a large number of optimization variables .",
    "as in example 1 , we tested different mcs configurations and divided them into 3 groups to do further analysis .",
    "the results are shown in fig .",
    "[ fig : e2_0_mcs ] and fig .",
    "[ fig : e2_3_mcs ] .    fig .",
    "[ fig : subfig : e2_0_mcs_init ] and fig .",
    "[ fig : subfig : e2_3_mcs_init ] compare the performance of mcs with different initialization lists for the two cases of example 3 .",
    "for case 1 , the convergence rate of mcs-2 is the fastest , followed by mcs-3 and mcs-1 . mcs-2 and mcs-3 ultimately obtain the highest npv . for case 2 ,",
    "mcs-1 and mcs-2 give a similar convergence rate at an early stage in the optimization process , then mcs-1 falls behind mcs-2 .",
    "mcs-3 shows a very slow rate of convergence at an early stage of the optimization process , but it obtains the highest npv finally .",
    "mcs-3 generates the initialization list by using a line search .",
    "this takes a few additional simulation runs before the splitting and local search steps .",
    "this explains the slow convergence initially .",
    "the effect of the maximum number of levels is shown in fig .",
    "[ fig : subfig : e2_0_mcs_smax ] and fig .",
    "[ fig : subfig : e2_3_mcs_smax ] . for case 1 , using @xmath92 ( mcs-1 ) performs similarly to using @xmath97 ( mcs-6 ) . for case 2 , which has 32 variables , using @xmath92 ( mcs-1 ) converges slightly faster than using @xmath97 ( mcs-6 ) , and finally obtains a higher npv .",
    "this indicates that a small number of levels is enough for these cases .",
    "[ fig : subfig : e2_0_mcs_ls ] and fig .",
    "[ fig : subfig : e2_3_mcs_ls ] show that local search plays an important role in mcs , without it the convergence speed decreases significantly .    fig .",
    "[ fig : opt_control ] presents the optimum controls for wells pro-01 and pro-02 under different control frequencies .",
    "we omit the results for well pro-03 and pro-04 because the reservoir is symmetric .",
    "the optimum controls become more like a bang - bang solution for all wells with an increase in the number of control steps .",
    "it is worth noting that the optimum controls for case 1 are significantly different that those for case 2 .",
    "this reflects the different production strategies for wells using a static rate compared to using dynamic well controls in water flooding reservoirs .",
    "+        this example use a 2d reservoir model with the permeability and porosity fields taken from the third layer of the spe10 benchmark model @xcite .",
    "it consists of @xmath141 grid cells and the size of each grid cell is @xmath142 .",
    "we consider an oil - water two phase flow in this model and the initial oil saturation is 0.8 .",
    "[ fig : both_spe10 ] shows the permeability and porosity fields of the model .",
    "the optimization problem is to place four wells in the reservoir , including two production wells ( p1 , p2 ) and two injection wells ( i1 , i2 ) .",
    "all wells are controlled via bhp that is updated every two years .",
    "the production period for this example is 10 years .",
    "thus , there are two location variables and five control variables per well and 28 variables in total .",
    "only bound constraints are considered in this example .",
    "the economic and optimization parameters are summarized in table [ tab : e3_epara ] and table [ tab : e3_opara ] respectively .",
    "both the simultaneous procedure and the sequential procedure are used for this example .",
    "cc parameter & value + oil revenue ( @xmath143 ) & 503.2 + water - production cost ( @xmath143 ) & 75.5 + water - injection cost ( @xmath143 ) & 50.3 + annual discount rate & 0 +    ccccc parameter & p1 & p2 & i1 & i2 + initial location & ( 60,25 ) & ( 1,25 ) & ( 30,1 ) & ( 30,50 ) + initial bhp ( @xmath137 ) & 175 & 175 & 362.5 & 362.5 + minimum bhp ( @xmath137 ) & 100 & 100 & 275 & 275 + maximum bhp ( @xmath137 ) & 250 & 250 & 450 & 450 +      the simultaneous procedure optimizes over the well locations and controls simultaneously . for this problem",
    ", we optimize the locations and control parameters of the 4 wells .",
    "each well has 2 location variables and 5 control variables , thus there are 28 variables in total .",
    "given this problem s complexity , we set the maximum number of simulation runs for this example to be 10000 .",
    "the maximum , minimum , mean , median , and standard deviation of npv for each algorithm is given in table [ tab : e3_result ] . from the table",
    "we can see that mcs-1 obtains the highest npv value after 10000 simulation runs .",
    "the average npv for pso and cma - es are in the middle , while gps performs the worst .",
    "plots of the npv of the four algorithms versus the number of simulation runs are shown in fig .",
    "[ fig : e3_algo ] .        fig .",
    "[ fig : e3_algo ] shows that mcs converges fastest , followed by cma - es , pso , and gps in that order . unlike example 1 and 2",
    ", the convergence speed of gps is slowest among all algorithms .",
    "the npv of gps has a jump at about 4000 simulation runs .",
    "it appears that at this point gps jumps from a local optima .",
    "[ fig : e3_psocmaes ] shows the range of npv for the trials of pso and cma - es . in this figure ,",
    "the areas between the maximum and minimum npv are shaded for pso and cma - es .",
    "it is clear that the npv obtained by pso and cma - es has a high variation for this example .",
    "as with examples 1 , 2 , and 3 , we use 7 mcs configurations , and the results are compared within the 3 groups in fig .",
    "[ fig : e3_mcs ] . fig .",
    "[ fig : subfig : e3_mcs_init ] shows the performance of mcs with different initialization lists .",
    "we use a semilog plot to make this figure clearer .",
    "the initialization list with a good initial guess ( mcs-4 and mcs-5 ) , starts its search from a relatively high npv , but obtains a npv lower than mcs-1 , which uses the default initialization list with a boundary point .",
    "for this example , mcs-1 , mcs-2 , and mcs-3 recover quite quickly from the bad initial guess , and are able to converge more quickly .",
    "we can see that for this example , the initialization list without boundary points ( mcs-2 ) performs unsatisfactorily both in terms of the convergence rate and the final npv .",
    "this is because the optimal solution for this example lies near the boundary , as shown in fig .",
    "[ fig : e3_ila ] . using a line search to generate",
    "the initialization list ( mcs-3 ) ultimately obtains the highest npv for this example .",
    "[ fig : subfig : e3_mcs_smax ] shows the performance of mcs with different numbers of maximum levels . using @xmath97 ( mcs-6 ) outperforms choosing @xmath92 ( mcs-4 ) for this example . the performance of mcs with and without local search is shown in fig .  [ fig : subfig : e3_mcs_ls ] .",
    "mcs without local search ( mcs-7 ) is clearly inferior .",
    "the sequential procedure decouples the joint problem into two separate subproblems . for the well placement optimization subproblem ,",
    "we optimize the locations of the 4 wells under an assumed control scheme .",
    "this gives 8 optimization variables .",
    "for the well control optimization subproblem , we optimize the well controls of the 4 wells with assumed well locations .",
    "each well has 5 control steps , this gives 20 optimization variables in total .",
    "the maximum number of simulation runs for each well placement optimization stage is 60 , while for each well control optimization stage the maximum number of simulation runs is set to 140 .",
    "and the maximum number of simulation runs for the problem in total is 5000 .",
    "this allows us to iterate 25 times between the well placement and well control optimization phases .",
    "based on the results of the previous section we use mcs-1 in the sequential procedure .",
    "the maximum , minimum , mean , median , and standard deviation of npv for each algorithm combination is given in table [ tab : e3_result_seq ] .",
    "plots of the npv versus the number of simulation runs for each approach are shown in fig .",
    "[ fig : e3_seq ] . from table [ tab :",
    "e3_result_seq ] and fig .",
    "[ fig : e3_seq ] we see that mcs - mcs converges faster than the other combinations and obtained the highest npv value at the end of the optimization .",
    "gps - mcs converges slowly at the early stage , but it ultimately obtains the second highest npv .",
    "the combinations which contain stochastic algorithms , especially cma - es , perform unsatisfactorily for this example .",
    ".results of the sequential procedure for example 4 .",
    "values shown are npv in @xmath144 usd . [ cols=\"<,<,<,<,<,<,<\",options=\"header \" , ]     we also compare the optimal npv obtained using the simultaneous and the sequential procedures using beanplots in fig .",
    "[ fig : seqvssim ] .",
    "a beanplot @xcite promotes visual comparison of univariate data between groups . in a beanplot ,",
    "the individual observations are shown as small lines in a one - dimensional scatter plot .",
    "in addition , the estimated density of the distributions is visible and the mean ( bold line ) and median ( marker ` + ' ) are shown .        from fig .",
    "[ fig : seqvssim ] we can see that , with the simultaneous procedure , the final npv values obtained by all algorithms are less than @xmath145usd . with the sequential procedure , mcs - mcs , gps - mcs , and mcs - pso can obtain a npv value higher than @xmath145usd .",
    "indeed the simultaneous algorithm becomes trapped in a sub - optimal solution . in fig .",
    "[ fig : e3_local ] we show the npv obtained around the candidate solution in each of the 28 dimensions by sampling at @xmath146 for @xmath147 . for the well location variables we choose @xmath148 and for the well control variables we set @xmath149 to 1% of the range of the @xmath6th variable .",
    "we see that in most of directions the npv remains constant .",
    "the npv is lower in a few directions . and only gives an improved npv in a few directions .",
    "hence the algorithms have difficulty finding an ascent direction .        the optimal well placement and the final oil saturation distribution obtained with the simultaneous and sequential procedures are shown in fig .  [ fig : e3_soil ] .",
    "the corresponding optimal controls of each well are given in table [ tab : e3_control ] .",
    "the optimal well locations obtained by the simultaneous procedure are significantly different from the locations obtained by the sequential procedure . from the final oil saturation distribution",
    ", we can see that , the locations obtained by the sequential procedure give a larger sweep area .",
    "the optimal controls obtained by the simultaneous procedure are similar to those found by the sequential algorithm .    in theory , for a joint well placement and control optimization problem , the simultaneous procedure is able to find the global optima , but this is not guaranteed for the sequential procedure since the optimal location of each well depends on how the well is operated and vice - versa .",
    "the simultaneous procedure , with a larger number of optimization variables , makes the joint problem more difficult .",
    "it requires a higher computational budget and has a higher risk of falling into a local optima and achieving a suboptimal solution , especially for a larger scale problem . the sequential procedure , decouples a hard joint problem into two easier subproblems , and hopes to approach the global optima iteratively . in general , the sequential procedure is worth considering in practice .",
    "our test results show that mcs is strongly competitive with existing algorithms for well placement , well control , and joint optimization problems . in all 4 examples ,",
    "mcs offers good convergence speed , especially when the number of simulation runs is limited .",
    "moreover , mcs does not suffer from the inherent variability of the stochastic algorithms .",
    "based on the results of the examples , for placement and control optimization we suggest a mcs configuration which uses a line search to generate the initialization list .",
    "the number of levels @xmath92 is enough for most problems but a higher @xmath64 should be used for difficult problems .",
    "local search is an important part of mcs , and is highly recommended .",
    "in this paper , we applied the multilevel coordinate search algorithm for four typical oil field development optimization problems .",
    "the problems include well placement optimization , well control optimization , and joint optimization of well placement and control .",
    "the performance of mcs has been compared with generalized pattern search , particle swarm optimization , and covariance matrix adaptation evolution strategy through several case studies including both synthetic and real reservoirs .",
    "the results presented here demonstrate that the mcs algorithm is strongly competitive , and outperforms the other algorithms in most cases , especially for the joint optimization problem .",
    "mcs has significant advantages in solving optimization problems with a limited number of simulation runs .",
    "in addition , mcs does not suffer from the inherent variability of the stochastic approaches .    for joint well placement and well control optimization problem ,",
    "both the simultaneous procedure and the sequential procedure were considered . in our example",
    ", the sequential procedure finds the best solution .",
    "although the simultaneous procedure can theoretically obtain the global optima , the sequential procedure is worth considering in practice .",
    "the sequential procedure decouples a difficult joint problem to two easier separate subproblem , decreases the number of optimization variables , make the problem easier to solve and decreases the risk of the algorithm falling into a local optima . among all algorithm combinations considered in this paper , mcs - mcs showed best performance both in terms of convergence speed and final npv value in the sequential procedure .",
    "mcs has shown its potential in our work , but more research is needed .",
    "future work includes applying the mcs algorithm to realistic large - scale oil field cases .",
    "this will involve an extension of mcs to handle linearly and nonlinearly constrained problems , possibly by a penalty approach .",
    "the authors acknowledge funding from the natural sciences and engineering research council of canada ( nserc ) discovery grant program , the national science and technology major project of the ministry of science and technology of china ( 2011zx05011 - 002 ) , the foundation for outstanding young scientist in shandong province ( grant no .",
    "bs2014nj011 ) , and the program of china scholarships council ( no . 201406450017 ) .",
    "alqahtani , g.  d. , alzahabi , a. , spinner , t. , soliman , m.  y. , 2014 . a computational comparison between optimization techniques for wells placement problem : mathematical formulations , genetic algorithms and very fast simulated annealing . journal of materials science and chemical engineering 02  ( 10 ) , 5973 .",
    "asadollahi , m. , nvdal , g. , dadashpour , m. , kleppe , j. , feb . 2014 .",
    "production optimization using derivative free methods applied to brugge field case .",
    "journal of petroleum science and engineering 114 , 2237 .",
    "ciaurri , d.  e. , mukerji , t. , durlofsky , l.  j. , 2011 .",
    "derivative - free optimization for oil field operations . in : yang ,",
    "x .- s . , koziel , s. ( eds . ) , computational optimization and applications in engineering and industry .",
    "359 in studies in computational intelligence .",
    "springer berlin heidelberg , pp .",
    "1955 .",
    "emerick , a.  a. , silva , e. , messer , b. , almeida , l.  f. , szwarcman , d. , pacheco , m. a.  c. , vellasco , m. m. b.  r. , 2009 .",
    "well placement optimization using a genetic algorithm with nonlinear constraints . in : spe reservoir simulation symposium .",
    "society of petroleum engineers .",
    "fonseca , r.  m. , stordal , a.  s. , leeuwenburgh , o. , van  den hof , p. m.  j. , jansen , j.  d. , 2014 .",
    "robust ensemble - based multi - objective optimization . in :",
    "ecmor xiv-14th european conference on the mathematics of oil recovery .",
    "forouzanfar , f. , li , g. , reynolds , a.  c. , 2010 .",
    "a two - stage well placement optimization method based on adjoint gradient . in : spe annual technical conference and exhibition .",
    "society of petroleum engineers .",
    "forouzanfar , f. , poquioma , w.  e. , reynolds , a.  c. , 2015 . a covariance matrix adaptation algorithm for simultaneous estimation of optimal placement and control of production and water injection wells . in : spe reservoir simulation symposium .",
    "society of petroleum engineers ( spe ) .",
    "forouzanfar , f. , reynolds , a.  c. , jul . 2014 .",
    "joint optimization of number of wells , well locations and controls using a gradient - based algorithm .",
    "chemical engineering research and design 92  ( 7 ) , 13151328 .",
    "forouzanfar , f. , reynolds , a.  c. , li , g. , may 2012 .",
    "optimization of the well locations and completions for vertical and horizontal wells using a derivative - free optimization algorithm .",
    "journal of petroleum science and engineering 8687 , 272288 .",
    "handels , m. , zandvliet , m. , brouwer , r. , jansen , j.  d. , 2007 .",
    "adjoint - based well - placement optimization under production constraints . in : spe reservoir simulation symposium .",
    "society of petroleum engineers .",
    "hansen , n. , kern , s. , jan .",
    "2004 . evaluating the cma evolution strategy on multimodal test functions . in : yao , x. , burke , e.  k. ( eds . ) ,",
    "parallel problem solving from nature - ppsn viii .",
    "3242 in lecture notes in computer science .",
    "springer berlin heidelberg , pp .",
    "282291 .",
    "isebor , o.  j. , durlofsky , l.  j. , ciaurri , d.  e. , aug .",
    "a derivative - free methodology with local and global search for the constrained joint optimization of well locations and controls .",
    "computational geosciences 18  ( 3 - 4 ) , 463482 .",
    "jansen , j.  d. , fonseca , r.  m. , kahrobaei , s. , siraj , m.  m. , van  essen , g.  m. , van  den hof , p. m.  j. , nov .",
    "2014 . the egg model  a geological ensemble for reservoir simulation .",
    "geoscience data journal 1  ( 2 ) , 192195 .              lambot , s. , javaux , m. , hupet , f. , vanclooster , m. , nov .",
    "2002 . a global multilevel coordinate search procedure for estimating the unsaturated soil hydraulic properties .",
    "water resources research 38  ( 11 ) , 615 .",
    "li , l. , jafarpour , b. , mohammad - khaninezhad , m.  r. , nov .",
    "a simultaneous perturbation stochastic approximation algorithm for coupled well placement and control optimization under geologic uncertainty .",
    "computational geosciences 17  ( 1 ) , 167188 .",
    "onwunalu , j.  e. , durlofsky , l.  j. , 2009 .",
    "development and application of a new well pattern optimization algorithm for optimizing large scale field development . in :",
    "spe annual technical conference and exhibition .",
    "society of petroleum engineers ( spe ) .",
    "shakhsi - niaei , m. , iranmanesh , s.  h. , torabi , s.  a. , jun .",
    "optimal planning of oil and gas development projects considering long - term production and transmission .",
    "computers & chemical engineering 65 , 6780 .",
    "siraj , m.  m. , van  den hof , p.  m. , jansen , j.  d. , 2015 . model and economic uncertainties in balancing short - term and long - term objectives in water - flooding optimization . in : spe reservoir simulation symposium .",
    "society of petroleum engineers .",
    "tavallali , m.  s. , karimi , i.  a. , teo , k.  m. , baxendale , d. , ayatollahi , s. , aug .",
    "optimal producer well placement and production planning in an oil reservoir .",
    "computers & chemical engineering 55 , 109125 .",
    "zakirov , i. , aanonsen , s.  i. , zakirov , e.  s. , palatnik , b.  m. , 1996 . optimizing reservoir performance by automatic allocation of well rates . in : 5th european conference on the mathematics of oil recovery .",
    "zhou , k. , hou , j. , zhang , x. , du , q. , kang , x. , jiang , s. , aug .",
    "2013 . optimal control of polymer flooding based on simultaneous perturbation stochastic approximation method guided by finite difference gradient .",
    "computers & chemical engineering 55 , 4049 ."
  ],
  "abstract_text": [
    "<S> determining optimal well placements and controls are two important tasks in oil field development . </S>",
    "<S> these problems are computationally expensive , nonconvex , and contain multiple optima . </S>",
    "<S> the practical solution of these problems require efficient and robust algorithms . in this paper , </S>",
    "<S> the multilevel coordinate search ( mcs ) algorithm is applied for well placement and control optimization problems . </S>",
    "<S> mcs is a derivative - free algorithm that combines global and local search . </S>",
    "<S> both synthetic and real oil fields are considered . </S>",
    "<S> the performance of mcs is compared to generalized pattern search ( gps ) , particle swarm optimization ( pso ) , and covariance matrix adaptive evolution strategy ( cma - es ) algorithms . </S>",
    "<S> results show that the mcs algorithm is strongly competitive , and outperforms for the joint optimization problem and with a limited computational budget . </S>",
    "<S> the effect of parameter settings for mcs are compared for the test examples . for the joint optimization problem </S>",
    "<S> we compare the performance of the simultaneous and sequential procedures and show the utility of the latter .    </S>",
    "<S> well placement , well control , joint optimization , multilevel coordinate search , derivative - free optimization , reservoir simulation - based optimization </S>"
  ]
}