{
  "article_text": [
    "in regression learning problems , we are given data @xmath13 in @xmath14 where @xmath15 is a bounded subset of @xmath16 and @xmath17 is a bounded subset of @xmath18 .",
    "we assume this data is chosen iid ( independently and identically distributed ) according to an unknown probability distribution @xmath19 .",
    "we say that @xmath10 is a ` position ' , and @xmath20 is a ` label ' .",
    "these data points may be , for example , images of people s faces in pixel space with a person s age as the corresponding label , or auto - regressive time series data ( @xcite , @xcite ) .",
    "the output of a learning algorithm is a decision function @xmath21 .",
    "even though we only know @xmath5 data points from distribution @xmath19 , we hope to construct @xmath22 which will be able to _ generalize _ to unobserved points in the distribution .",
    "this means we would like @xmath22 to predict the value of @xmath20 for any given value of @xmath23 .",
    "since we want our function @xmath22 to fit the data accurately and also have this generalization ability , we refer to vapnik s structural risk minimization ( srm ) principle ( @xcite , @xcite ) . in srm , we limit our choice of functions @xmath22 so they are chosen from a class @xmath24 , of finite capacity ( i.e. finite vc dimension ) .",
    "otherwise , we can not hope to choose a function @xmath22 which has generalization ability - we would overfit the data .",
    "one convenient way to implement srm is to let @xmath24 be a ball within a reproducing kernel hilbert space ( rkhs ) @xmath25 , with norm @xmath26 . in this form",
    ", we have an ivanov regularization problem ; one can show that the solution is always the minimizer of a corresponding tikhonov regularization problem .",
    "algorithms for classification and regression solve this tikhonov regularization problem , so that the decision function is given by @xmath27 ( @xcite , @xcite ) , where @xmath28 @xmath12 is called the regularized risk functional .",
    "note that we define our rkhs @xmath25 so that @xmath29 iff @xmath30 is finite .",
    "thus , minimizing over @xmath29 is equivalent to minimizing over all functions @xmath22 . the first term in @xmath12",
    "is called the empirical risk , and @xmath31 is a pre - determined loss function .",
    "we will generally use the least squares loss function @xmath32 , but a similar analysis can be performed for other loss functions . the second term is called the regularization term , and @xmath33 is called the regularization parameter ; one always takes @xmath34 . here",
    ", @xmath33 can be viewed as the trade - off between accuracy and generalization .",
    "if @xmath33 is very small , we are minimizing the empirical risk , increasing the accuracy of our model to the data , and possibly overfitting .",
    "if @xmath33 is very large , our algorithm will generalize at the expense of accuracy . in a sense , @xmath33 controls the capacity of the function class from which @xmath22 is chosen : the larger @xmath33 is , the smaller the radius of the ball @xmath24 in @xmath25 . in practice ,",
    "@xmath33 is often chosen empirically , perhaps to minimize the leave - one - out error on a training set .",
    "another interpretation of this functional is through the eyes of algorithmic stability , as described by bousquet and elisseeff ( @xcite ) . here",
    ", the regularization term prevents the algorithm from being sensitive to the replacement of one data point . in either case ,",
    "the regularization problem is well - posed only when @xmath33 is strictly greater than 0 .",
    "we assume that the labels @xmath20 are ` noisy ' , in the sense that there is a marginal distribution @xmath35 for each @xmath10 .",
    "we denote the expectation value of the label y for position x as @xmath36 , and we denote the marginal distribution along the @xmath10-axis as @xmath37 .",
    "( this is the distribution of @xmath19 after integrating over the @xmath20 values . )    for the case when @xmath38 , we show convergence of @xmath27 to a function @xmath8 as the regularization term vanishes , provided @xmath39 ; i.e. we need to find conditions on the simultaneous convergence @xmath38 and @xmath40 so that @xmath41 . here , the function @xmath8 is defined by : @xmath42 ^",
    "2|\\mthx\\right ) d\\mu(\\mthx).\\end{aligned}\\ ] ] in other words , @xmath43 is the minimizer of the _ ` actual risk'_. since we are using the least squares loss function , this minimizer is simply the expectation of @xmath20 for each @xmath10 ; @xmath44 .",
    "we assume that we have chosen a rkhs which is large enough to contain @xmath45 .",
    "in other words , @xmath46 . this is not an exceedingly strong assumption ; in fact , many popular kernels ( e.g. gaussian kernels ) can produce rkhs of arbitrarily high vc dimension .",
    "although @xmath45 may not be in @xmath25 for every case , @xmath45 will be in @xmath25 for most smooth processes which have bounded noise , as long as we implement a sufficiently powerful rkhs .",
    "for the fixed @xmath5 case , we may express label @xmath20 for position @xmath10 as the random variable @xmath47 , where @xmath48 is a deterministic function assumed to be in @xmath25 , and @xmath49 is random noise with some probability distribution , with @xmath49 and @xmath50 independent if @xmath51 .",
    "we denote the vector of noise values as @xmath52 . in order to force the noise to vanish",
    ", we will assume the noise is generated by a fixed random process generating noise with norm bounded by @xmath53 almost surely , and we will only shrink its amplitude .",
    "since the noise is generated by this fixed process , the theorem will hold whenever the noise is bounded , and thus , if the noise is bounded almost surely , the theorem will hold almost surely . using",
    "the least squared loss and making the noise explicit , our algorithm becomes : @xmath54 for @xmath34 , the minimizer of @xmath12 is unique , because @xmath55 is strictly convex .",
    "since the noise is random , @xmath27 is still a random variable .",
    "our goal is to show ` stability ' for this algorithm , i.e. we need to find a set of conditions on the simultaneous convergence @xmath56 which allows @xmath57 , where @xmath4 is the element of @xmath25 with minimal norm that has zero empirical risk when noise is not present .",
    "( since we assume that @xmath58 , @xmath59 itself minimizes @xmath12 when @xmath60 . since many functions in @xmath25 vanish at all the @xmath61 , there may be infinitely many functions with zero empirical risk ; our algorithm will converge to the one with the smallest rkhs norm . )",
    "intuitively , this stability analysis demonstrates that there s no inherent error in our algorithm when noise or regularization is present , and that a small amount of noise or regularization can not dramatically disrupt the algorithm s output .",
    "this type of stability is different from the ` algorithmic stability ' of devroye(@xcite ) .",
    "algorithmic stability measures the variability of an algorithm s output as the data set changes .",
    "our type of stability determines whether the algorithm s output changes dramatically when noise or regularization is present .",
    "algorithmic stability is a property of one particular algorithm for one particular distribution of data .",
    "our stability is not - the distribution changes as noise is removed , and the algorithm changes as the regularization term shrinks .",
    "we actually use algorithmic stability to help us show stability of our algorithm in this sense .    theorem 1 states that the regularized least squares regression algorithm is stable as the number of data points increases to infinity .",
    "theorem 2 states that the regularized least squares regression algorithm is stable for a fixed n point data set .",
    "+ * main algorithm * ( regularized least squares regression ) : + for a data set @xmath62 , where @xmath63 , @xmath64 , @xmath65 @xmath66    denote by @xmath45 the function @xmath36 .",
    "denote by @xmath67 the data set @xmath13 .",
    "if @xmath68 and if @xmath69 is chosen to depend on @xmath5 such that @xmath70 and @xmath71 as @xmath6 , then we have convergence of the _ main algorithm _ : @xmath72    here , ` @xmath73 ' denotes convergence in probability .",
    "assume we are given @xmath5 fixed positions @xmath74 .",
    "suppose that for each @xmath75 , the labels are given by @xmath76 , where the @xmath77 s are independent random variables with @xmath78 almost surely .",
    "denote by @xmath79 the data set @xmath13 .",
    "define @xmath4 by : + * ( i ) * @xmath80 for @xmath81 + * ( ii ) * @xmath82 is minimal , among all functions which satisfy * ( i)*. + if @xmath83 is chosen to depend on @xmath84 such that @xmath85 as @xmath86 and @xmath87 , then we have convergence of the _ main algorithm _",
    "almost surely : @xmath88     + section 2 contains a short review of rkhs .",
    "section 3 and 4 contain the proofs of theorems 1 and 2 .",
    "@xmath25 is a real reproducing kernel hilbert space ( rkhs ) if @xmath25 has the following properties :    * @xmath25 _ is a hilbert space .",
    "_ @xmath25 is a complete , inner product , real vector space of functions @xmath89 .",
    "we denote @xmath25 s inner product by @xmath90 , and @xmath25 s norm by @xmath26 . *",
    "_ reproducing property .",
    "_ there exists a bilinear form @xmath91 such that @xmath92 , we have @xmath93 and @xmath94 for any @xmath95 .",
    "this @xmath96 is called the ` reproducing kernel ' of the rkhs.@xcite@xcite@xcite .",
    "we sometimes denote @xmath97 by @xmath98 . * _ spanning property .",
    "_ @xmath99    since @xmath25 is a real hilbert space , @xmath100 for all @xmath101 .",
    "it follows that @xmath102 , i.e. @xmath96 is symmetric in its two arguments . an equivalent definition of an rkhs is a hilbert space of functions @xmath21 such that all evaluation functionals @xmath103 , are continuous . given @xmath104 , the associated @xmath105 gram matrix @xmath106 has entries @xmath107 where @xmath96 is the reproducing kernel for the rkhs @xmath25 .",
    "the gram matrix is always a positive semi - definite matrix .",
    "the representer theorem transforms the minimization of our functional @xmath108 into an optimization problem over only @xmath5 numbers .",
    "this advantage is the main reason why scientists take @xmath24 to be a ball in a rkhs @xmath25 .",
    "we present a corollary of this theorem below . + * corollary of the representer theorem * ( kimeldorf , wahba)@xcite ) _ the function @xmath109 can be represented in the form @xmath110 .",
    "this is true for any arbitrary loss function v. _ + ( this corollary is a specific case of the full representer theorem @xcite . ) + having described the basic facts about rkhs , we now continue with the proofs of theorems 1 and 2 .",
    "for the _ main algorithm _ above , we are increasing the size of the data set @xmath67 as @xmath5 increases .",
    "we need to show convergence @xmath111 , where @xmath112 and @xmath6 .",
    "that is , we need to show @xmath113 we can break up the distance @xmath114 into two contributions .",
    "the first contribution is called ` variance ' , and it is due to the finite number of randomly chosen noisy data points .",
    "the variance vanishes with arbitrarily high probability as the number of data points increases , even if the noise does not vanish .",
    "the second contribution is the ` bias ' due to the restriction we place on our hypothesis space , i.e. the fact that @xmath22 is chosen from with a ball of a rkhs .",
    "this term vanishes as the ball gets larger , i.e. when @xmath115 gets smaller .",
    "@xmath116    where @xmath117    where @xmath118 , and    @xmath119 , where @xmath120 ^ 2|\\mthx)d\\mu(\\mthx ) + \\lambda_n \\|f\\|_h^2.$ ]    lemma 1.1 below describes a method for proving that the minimizers of two convex functions are close in @xmath25 .",
    "+ * lemma 1.1 . *    since @xmath121 and @xmath122 are minimizers of @xmath123 and @xmath124 respectively , and using the closeness condition * ( a ) * :    @xmath125 + @xmath126 + so , @xmath127    now , @xmath128 .    back to the proof of theorem 1 .",
    "we proceed one term at a time .",
    "+ * variance term * we will choose more general versions of @xmath129 and @xmath130 temporarily .",
    "@xmath131 that is , we assume that the loss function @xmath132 is lipschitz continuous , or ` sigma - admissible ' @xcite .",
    "the least squares loss has @xmath133 , since @xmath134 .    we need to verify the conditions * ( a ) * and * ( b ) * in order to use lemma 1.1 . to verify the closeness property * ( a ) * for our functionals @xmath135 and @xmath136 : @xmath137",
    "there are many available upper bounds for the right side of equation ( [ rempractual ] ) , including vapnik s vc bound , which relies on the vc dimension of the class of allowed decision functions @xmath24",
    "( @xcite , @xcite ) .",
    "the particular bound we utilize for this paper was constructed by bousquet and elisseeff @xcite , and it is based on ` algorithmic stability ' . in general , bounds of this quantity",
    "are probabilistic , and are based on some capacity measure of the algorithm or space of functions @xmath24 .",
    "this particular bound relies on the sigma - admissibility of the loss function v , and mcdiarmid s concentration of measure inequality .",
    "+ @xmath138 is a training sample @xmath139 .",
    "that is , we replace the @xmath141 training point in @xmath138 by a new data point in order to obtain @xmath142 .",
    "+ the algorithm @xmath143 is _ uniformly @xmath144-stable with respect to loss function _ @xmath145 if : @xmath146 + basically , this algorithmic stability measures how much the algorithm s output could possibly change , as measured by the loss function @xmath147 , when we replace one data point . +",
    "* algorithmic stability theorem * ( bousquet and elisseeff , @xcite ) _ if we are given a uniformly @xmath144-stable algorithm with respect to loss function @xmath147 , which outputs functions bounded by the constant @xmath148 ( i.e. @xmath149 ) , then for any @xmath150 , _ @xmath151    * algorithmic stability of tikhonov learning algorithms * ( bousquet and elisseeff , @xcite ) _ the _ main algorithm _ is uniformly @xmath144-stable , with @xmath152 . here , @xmath153 is an upper bound on the sigma - admissibility constant @xmath154 , and @xmath155 is an upper bound on the diagonal elements of the gram matrix , that is , @xmath156 . _ + returning to the proof of theorem 1 , we now know the right side of ( [ rempractual ] ) is bounded by @xmath157 ( for large values of @xmath5 ) with probability at least @xmath158 , where : @xmath159    we now have the closeness condition * ( a ) * of lemma 1.1 satisfied with probability at least @xmath158 , i.e. @xmath160 , with probability at least @xmath158 , where @xmath161 is given in ( [ thm1p ] ) .",
    "now we verify condition * ( b)*. we need to show that @xmath162 implies that @xmath163 for every @xmath22 .",
    "let s define a function @xmath164 so that @xmath165 in what follows .",
    "@xmath166 d % \\mu(\\mthx ) % \\\\ & & + \\int h^2(\\mthx ) d \\mu(\\mthx ) + \\lambda_n % \\|\\hat{f}_{\\lambda_n}\\|_\\h^2 + % 2\\lambda_n ( \\hat{f}_{\\lambda_n},h)_\\h + \\lambda_n\\|h\\|_\\h^2\\\\   & = & \\hat{l}_{\\lambda_n}(\\hat{f}_{\\lambda_n } ) + \\big [ 2 \\int\\e [ ( \\hat{f}_{\\lambda_n}(\\mthx)-y)h(\\mthx)|\\mthx]d\\mu(\\mthx ) + \\ ; 2\\lambda_n ( \\hat{f}_{\\lambda_n},h)_\\h \\big ] \\\\ & & + \\int   h^2(\\mthx ) d \\mu(\\mthx)+ \\lambda_n\\|h\\|_\\h^2\\end{aligned}\\ ] ]    the terms in the brackets are linear in @xmath164 .",
    "remember that @xmath167 is the minimizer of @xmath168 , and thus the linear terms must be zero .",
    "( if the linear terms are non - zero , we can reverse the sign of @xmath164 and contradict @xmath167 as the minimizer . )",
    "the last two terms are always positive .",
    "@xmath169 now we can see that * ( b ) * holds : @xmath170 in this case , @xmath171    since both the conditions * ( a ) * and * ( b ) * are satisfied , lemma 1.1 produces @xmath172 where @xmath161 is given in ( [ thm1p ] ) .",
    "we are done with the variance term .",
    "+ * bias term * we will prove that the bias term vanishes using the spectral theorem .",
    "define the function @xmath173 in what follows .",
    "now , @xmath174d\\mu(\\mthx ) + \\lambda_n % ( h+f_\\e , h+f_\\e)_\\h \\\\ & = & \\int ( k_\\mthx , h)_\\h^2 d\\mu(\\mthx ) + \\int \\e[(f_\\e(\\mthx ) -",
    "y)^2|\\mthx]d\\mu(\\mthx)+ \\lambda_n \\|h + f_\\e\\|^2_\\h\\end{aligned}\\ ] ]    the minimizer of @xmath130 again must have first variational derivative equal to 0 .",
    "using fubini s theorem , we find :    @xmath175\\bigg|_{\\gamma=0}\\\\ % & = & \\frac{\\partial}{\\partial \\gamma } \\bigg [ \\int % ( k_\\mthx , h)_\\h^2d\\mu(\\mthx ) + 2\\gamma \\left(\\int ( k_\\mthx , h)_\\h % k_\\mthx d \\mu(\\mthx),g\\right)_\\h + \\gamma^2 % \\int(k_\\mthx , g)_\\h^2d\\mu(\\mthx ) \\\\&&+ \\lambda_n % \\left[(h+f_\\e , h+f_\\e)_h + 2\\gamma ( h+f_\\e , g)_\\h + % \\gamma^2 ( g , g)_\\h\\right]\\bigg]\\bigg|_{\\gamma=0}\\\\ & = & 2\\left(\\int ( k_\\mthx , h)k_\\mthx d \\mu(\\mthx ) + \\lambda_n ( h+f_\\e),g\\right)_\\h\\end{aligned}\\ ] ]    if @xmath176 , the above expression must be zero for all @xmath177 , thus : @xmath178    let s define a new operator @xmath179 .",
    "+ @xmath180 + one can check that @xmath179 is self - adjoint since @xmath181 . for an operator @xmath182 from one hilbert space @xmath183 , to another , @xmath184 , the operator norm of @xmath182 is defined by @xmath185 .",
    "our operator @xmath179 is bounded , since by cauchy - schwarz ,",
    "@xmath186    we are going to use the spectral theorem next , but first let us review a few facts from functional analysis about this theorem ( @xcite ) .",
    "the spectral theorem allows one to define functions of a bounded self - adjoint operator on a hilbert space @xmath187 .",
    "if the function is a polynomial , e.g. @xmath188 , then it is clear how to define the corresponding operator @xmath189 .",
    "the spectral theorem extends the correspondence @xmath190 to all continuous functions ( in fact , to all bounded borel functions ) .",
    "moreover , one has @xmath191 . because @xmath192 is a real function , the operator @xmath193 provided by the spectral theorem is also self - adjoint .",
    "in addition , for each @xmath194 , we have a measure @xmath195 on @xmath196 such that @xmath197 .",
    "the measure @xmath195 is concentrated on that part of the spectrum @xmath196 along which @xmath22 has a nonzero component . in particular ,",
    "if @xmath198 , then @xmath22 is an eigenvector of @xmath199 with eigenvalue 0 , and @xmath195 is a @xmath200-measure concentrated on @xmath201 .",
    "if on the contrary , @xmath202 , then @xmath203 .    now , using the definition of the spectral measure @xmath204 for the operator @xmath179 and the function @xmath43 , we have from ( [ thm1 t ] ) : @xmath205 since @xmath206 is positive semidefinite , @xmath179 is a positive operator and thus has non - negative spectrum only .",
    "one can see that @xmath207 is empty , i.e. take any function @xmath208 such that @xmath209 .",
    "then , @xmath210 ; thus , @xmath208 must be zero almost everywhere .",
    "it follows that @xmath201 is a set of measure zero for @xmath204 . as @xmath211 , @xmath112 , and",
    "the function @xmath212 converges to 0 pointwise on @xmath213 , and thus almost everywhere with respect to @xmath204 ; since this function is bounded by 1 , we can again use the dominated convergence theorem to say that the integral vanishes as @xmath6 .",
    "one can not give a more explicit bound for this term without more information about the relationship between @xmath214 and @xmath25 . in any case , we have convergence of the bias term to 0 .",
    "now , we can complete the proof of theorem 1 .",
    "for any @xmath215 , we must be able to show that @xmath216 .",
    "so , let us choose an arbitrary fixed value @xmath217 .",
    "the bias term vanishes as @xmath6 and @xmath70 , so there must exist an @xmath218 so that for @xmath219 , @xmath115 is sufficiently small , so that the bias term is bounded by @xmath220 .",
    "thus , we consider the bias term bounded by @xmath220 in the limit as @xmath6 ; since this term does not depend on the data , the bound clearly holds with probability 1 .",
    "now , we must choose @xmath221 so that the variance term is bounded by @xmath220 . using the bound in ( [ thm1variancebd ] ) , we choose @xmath222 .",
    "the corresponding probability @xmath161 is then given by ( @xmath223 ) ,    @xmath224    we need @xmath161 to vanish as @xmath6 ; this is satisfied if @xmath225 as @xmath211 .",
    "also , there must exist an @xmath218 such that for @xmath219 , we have @xmath226 ; we need this in order to use the algorithmic stability theorem .",
    "thus , theorem 1 is proved .",
    "this section contains the proof of theorem 2 .",
    "first , some notation . the positions @xmath74 will be considered fixed throughout this section .",
    "+ @xmath227 + the ` evaluation operator ' @xmath228 evaluates a function @xmath22 at each position @xmath61 in the data set .",
    "note that @xmath228 ` loses information ' about a function @xmath22 by evaluating it at only @xmath5 points .",
    "that is , @xmath229 is a nontrivial subspace of @xmath25 .",
    "the adjoint @xmath230 of the operator @xmath228 is given by @xmath231 .",
    "one can show that @xmath228 is a bounded operator , with @xmath232 the operator @xmath233 is automatically positive and self - adjoint .",
    "we will later use the spectral theorem on the bounded self - adjoint operator @xmath233 .",
    "we start the proof of theorem 2 with the following lemma . + * lemma 2.1 * : _ the following characterizations of @xmath4 are equivalent _ :",
    "@xmath4 satisfies : + * ( i ) * @xmath80 for @xmath81 , and + * ( ii ) * @xmath234 that satisfy @xmath235 2 .",
    "@xmath4 satisfies : @xmath236 3 .",
    "@xmath4 satisfies : + * ( i ) * @xmath80 for @xmath81 , and + * ( iii ) * @xmath237 we have @xmath238 .",
    "we will show 1 .",
    "@xmath239 2 .",
    "@xmath239 3 .",
    "@xmath239 1 .",
    "@xmath240 2 .",
    "first , we show that the function described in 1 .",
    "is unique . from the reproducing property ,",
    "we know that @xmath4 has nonzero components along each of the @xmath241 s for which @xmath242 .",
    "since @xmath25 is a hilbert space , we can always decompose @xmath4 into a component @xmath243 within the span of the @xmath241 s and a component @xmath244 orthogonal to each @xmath241 ( where @xmath245 ) .",
    "now , @xmath246 .",
    "thus , if @xmath247 , then @xmath4 no longer has minimal norm and contradicts property * ( ii)*. the component of @xmath4 along each of the @xmath241 s is determined by the value of @xmath2 .",
    "so , functions @xmath22 that satisfy both * ( i ) * and * ( ii ) * can be written @xmath248 for the fixed values of @xmath249 , @xmath81 .",
    "in particular , the @xmath249 s must satisfy : @xmath250 thus , the function described in 1 .",
    "is unique .",
    "it is straightforward to see that the function described in 2 .",
    "is exactly the function described in 1 .",
    "evaluating the right side of ( [ lemma2.1 ] ) at @xmath251 , we obtain @xmath252 moreover , the function described in 2 .",
    "lies entirely within the span of the @xmath241 s",
    ". therefore it obeys * ( i ) * and * ( ii ) * and we have 1 .",
    "@xmath240 2 .",
    "@xmath239 3 .",
    "because 2 .",
    "@xmath239 1 . , *",
    "( i ) * is satisfied .",
    "we just need to show * ( iii)*. + for any @xmath253 @xmath254    \\3 .",
    "@xmath239 1 . here , *",
    "( i ) * is automatic , so we need to check * ( ii)*. + @xmath255i=1, .. ,n@xmath256    now , @xmath257    thus we have @xmath258 , so lemma 2.1 is proved .    back to the proof of theorem 2 .",
    "the functional @xmath259 in the _ main algorithm _ expressed in terms of @xmath228 becomes : @xmath260    the minimizer of @xmath259 must satisfy @xmath261 . in other words ,",
    "the first variational derivative of @xmath259 is 0 at its minimizer .",
    "recalling that @xmath262 , this minimization problem becomes : @xmath263 \\big|_{\\gamma=0}\\\\ % & = & \\frac{\\partial}{\\partial \\gamma } \\big [ \\frac{1}{n}(p f - p % \\overline{f } - \\frac{1}{t}\\mathbf{b},p f - p \\overline{f } - % \\frac{1}{t}\\mathbf{b})_{\\ell_2 } + 2\\gamma\\frac{1}{n}(p h , p % f - p\\overline{f}-\\frac{1}{t}\\mathbf{b})_{\\ell_2 } + % \\\\ & & \\gamma^2\\frac{1}{n}(p h , p h)_{\\ell_2 } + \\lambda_t ( f , f)_\\h % + 2\\gamma\\lambda_t(f , h)_\\h + \\gamma^2\\lambda_t ( h , h)_\\h \\big ] \\big|_{\\gamma=0}\\\\ & = & 2\\frac{1}{n}(p h , p f - p \\overline{f } -\\frac{1}{t}\\mathbf{b})_{\\ell_2 } + 2\\lambda_t(f , h)_\\h \\\\&= & 2\\big(h,\\frac{1}{n}p^*p f - \\frac{1}{n}p^ * p \\overline{f } - \\frac{1}{n}p^*\\big(\\frac{1}{t}\\mathbf{b}\\big ) + \\lambda_t f\\big)_\\h .\\end{aligned}\\ ] ]",
    "this must be true for any function @xmath164 , so @xmath264 , implying @xmath265 it follows that @xmath266    in order to show stability , we bound the two terms on the right of ( [ thm2terms ] ) , and construct these bounds so they vanish as @xmath267 .",
    "that is , we need to bound the norms above . to accomplish this",
    ", we will use the spectral theorem on the bounded self - adjoint operator @xmath268 .    to bound the first term in equation ( [ thm2terms ] ) , recall that the operator obtained from the function @xmath269 of the self - adjoint operator",
    "@xmath233 is self - adjoint .",
    "also , since @xmath233 is a positive operator , the spectrum @xmath270 of the operator @xmath233 is concentrated on @xmath271 . using the spectral measure @xmath272 on the spectrum @xmath270",
    ", we find : @xmath273 ^ 2 \\overline{f } , \\overline{f})_\\h .",
    "% & = & ( ( \\frac{1}{n}p^ * p+\\lambda_t)^{-2 } \\lambda_t^2\\overline{f},\\overline{f})_\\h \\\\ \\\\&= & \\int_{spec(p^ * p ) } \\left(\\frac{\\lambda_t}{\\frac{1}{n}z + \\lambda_t}\\right)^2 d \\nu_{\\overline{f};p^*p}(z)\\end{aligned}\\ ] ] because @xmath274 , we have @xmath275 for all @xmath276 . by lemma 2.1 , we know that @xmath277 , and since @xmath278 , we know that @xmath279 . since @xmath280 for all @xmath281 , it then follows from the dominated convergence theorem that @xmath282 .",
    "one can not give a more explicit bound for this first term ; it would require more specific knowledge of the relationship between @xmath214 and @xmath25 . in any case",
    ", we have achieved our goal in showing that the first term of ( [ thm2terms ] ) vanishes as @xmath283 .",
    "we need the second term in equation ( [ thm2terms ] ) to vanish also .",
    "recall that for operator @xmath284 , it is true that @xmath285 , and that a continuous real function of a self adjoint operator such as @xmath233 is self adjoint .",
    "@xmath286 we use the spectral theorem for the bounded self - adjoint operator @xmath287 , namely the fact @xmath288 where @xmath289 here .",
    "the maximum value of @xmath290 occurs at @xmath291 , and it is @xmath292 .",
    "thus , @xmath293 as long as we design @xmath294 so that @xmath295 , then we have the desired convergence of this term to 0 .",
    "we are done with the second term of equation ( [ thm2terms ] ) .",
    "theorem 2 is proven .",
    "we have proved stability for the regularized least squares regression algorithm , for the sense in which inverse problems are examined . we have shown stability for this algorithm in two cases : the case when the number of data points @xmath5 is a constant , and the case where @xmath6 .",
    "it is important that our algorithm is stable in this sense , because we do not want any inherent error in the algorithm s output .",
    "neither a small amount of noise in the data nor a small amount of regularization should drastically influence the algorithm s output .",
    "we hope that the reader will gain more from our result than the knowledge that regularized least squares regression is stable in the inverse operator sense .",
    "we have found the particular methods introduced in the proofs of theorem 1 and theorem 2 useful for various learning problems , especially those which require the convexity of learning functionals or convergence of learning algorithms .",
    "namely , we demonstrate two methods for showing that the minimizers of two learning functionals are close : use of the spectral theorem , and the technique of lemma 1.1 , which can both be generally applied to other learning algorithms .",
    "the author would like to express infinite gratitude to ingrid daubechies ."
  ],
  "abstract_text": [
    "<S> we discuss stability for a class of learning algorithms with respect to noisy labels . </S>",
    "<S> the algorithms we consider are for regression , and they involve the minimization of regularized risk functionals , such as @xmath0 . </S>",
    "<S> we shall call the algorithm ` stable ' if , when @xmath1 is a noisy version of @xmath2 for some function @xmath3 , the output of the algorithm converges to @xmath4 as the regularization term and noise simultaneously vanish . </S>",
    "<S> we consider two flavors of this problem , one where a data set of @xmath5 points remains fixed , and the other where @xmath6 . for the case </S>",
    "<S> where @xmath7 , we give conditions for convergence to @xmath8 ( the function which is the expectation of @xmath9 for each @xmath10 ) , as @xmath11 . for the fixed @xmath5 case , we describe the limiting non - noisy , non - regularized function @xmath4 , and give conditions for convergence . in the process , we develop a set of tools for dealing with functionals such as @xmath12 , which are applicable to many other problems in learning theory .    </S>",
    "<S>     march 7 , 2003     + </S>"
  ]
}