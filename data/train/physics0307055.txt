{
  "article_text": [
    "the inference of physical quantities from data generated either by experiment or by numerical simulation is a ubiquitous and often cumbersome task .",
    "whether the data is corrupted by noise , hampered by finite resolution or tied up in correlations , in principle it should always be possible to improve the analysis by taking into account , in addition to the information contained in the data , whatever other knowledge one might have about the physical quantities to be inferred or about how the data was generated . the way to link this prior information with the new information in the data",
    "is found in bayesian probability theory .",
    "bayesian methods are increasingly popular in physics @xcite .",
    "they are essential whenever repeating the experiment many times in order to reduce the measurement uncertainty is either too expensive or time consuming . this is a common situation in astronomy and astrophysics @xcite , and also in large laboratory experiments as in fusion @xcite and in high energy physics @xcite .",
    "other typical uses in physics arise in spectrum restoration , in ill - posed inversion problems @xcite and when separating a signal from an unknown background @xcite .",
    "applications include mass spectrometry @xcite , rutherford backscattering @xcite and nuclear magnetic resonance @xcite .    from a general point of view",
    "the problem of inductive inference is to update from a prior probability distribution to a posterior distribution when new information becomes available .",
    "the challenge is to develop updating methods that are systematic and objective .",
    "two methods have been found which are of very broad applicability : one is based on bayes theorem and the other is based on the maximization of entropy .",
    "the choice between these two updating methods is dictated by the nature of the information being processed .    when we want to update our beliefs about the values of certain quantities @xmath0 on the basis of the observed values of other quantities @xmath1  the data  and of some known relation between @xmath0 and @xmath1 we must use bayes theorem .",
    "the updated or posterior distribution is @xmath2 ; the relation between @xmath1 and @xmath0 is supplied by a known model @xmath3 ; the previous knowledge about @xmath0 is codified both into the prior  probability @xmath4 and also in the likelihood  distribution @xmath3 .    the selection of the prior @xmath4 is a controversial issue which has generated an enormous literature @xcite .",
    "the difficulty is that it is not clear how to carry out an objective translation of our previous beliefs about @xmath0 into a distribution @xmath4 .",
    "one reasonable attitude is to admit subjectivity and recognize that different individuals may start from the same information and legitimately end with different translations . in simple cases experience and physical intuition",
    "have led to a considerable measure of success , but we are often confronted with new complex situations involving perhaps parameter spaces of high dimensionality where we have neither a previous experience nor a reliable intuition .",
    "on the other hand , there are special cases where some degree of objectivity can be attained .",
    "for example , requirements of invariance can go a long way towards the complete specification of a prior .",
    "considerable effort has been spent seeking an objective characterization of that elusive state of knowledge that presumably reflects complete ignorance .",
    "although there are convincing arguments against the existence of such non - informative priors @xcite , the search has had the merit of suggesting connections with the notion of entropy @xcite including two proposals for entropic priors  @xcite .",
    "this brings us to the second method of processing information .",
    "bayes theorem follows from the product rule for joint probabilities , @xmath5 , and therefore its applicability is restricted to situations where assertions concerning the joint values of the data @xmath1 and the parameters @xmath0 are meaningful .",
    "but there are situations where the available information is of a different nature and involves assertions about the probabilities themselves .",
    "such information , which includes but is not limited to assertions about expected values , can not be processed using bayes theorem .",
    "the method of maximum entropy ( me ) is designed for updating from a prior probability distribution to a posterior distribution when the information to be processed takes the form of a constraint on the family of acceptable posterior distributions @xcite .",
    "the early and less satisfactory justification of the me method followed from interpreting entropy , through the shannon axioms , as a measure of the amount of uncertainty in a probability distribution @xcite .",
    "objections to this approach are that the shannon axioms refer to probabilities of discrete variables , the entropy of continuous variables is not defined , and that the use of entropy as the unique measure of uncertainty remained questionable .",
    "other so - called entropies could and , indeed , were introduced . ultimately , the real problem is that shannon was not concerned with inductive inference .",
    "he was not trying to update probability distributions but was instead analyzing the capacity of communication channels .",
    "shannon s entropy makes no reference to prior distributions .",
    "considerations such as these motivated several attempts to justify the me method directly as a method of inductive inference without invoking questionable measures of uncertainty @xcite .",
    "the concept of relative entropy is then introduced as a tool for consistent reasoning which , in the special case of uniform priors , reduces to the usual entropy .",
    "there is no need for an interpretation in terms of heat , disorder , or uncertainty , or even in terms of an amount of information .",
    "perhaps this is the explanation of why the search for the meaning of entropy has turned out to be so elusive : strictly , _",
    "entropy needs no interpretation_. in section 2 , as background for the rest of the paper , we present a brief outline of one such ` no - interpretation ' approach inspired by @xcite .",
    "in this paper we use entropic arguments to translate prior information into a prior distribution . rather than seeking a totally non - informative prior , we make use of information that we do in fact have .",
    "remarkably , it turns out that the very conditions that allow us to contemplate using bayes theorem  namely , knowledge of a likelihood function , @xmath3  already constitute valuable prior information . in this sense",
    "one can assert that the search for completely non - informative priors is misplaced : if we do not know the likelihood , then prior distributions are not needed anyway . the prior thus obtained is an entropic prior .",
    "the name and the first proposal of a prior of this kind is due to skilling @xcite for the case of discrete distributions .",
    "the generalization to the continuous case and further elaborations by rodrguez @xcite constitute a second proposal .",
    "it is essential for the successful use of any prior , and of entropic priors in particular , to be aware of what information they contain and , crucially , what information they do not contain .",
    "no prior can be expected to succeed unless all the information relevant to the problem at hand has been taken into account .",
    "it is quite likely that most practical problems that were encountered with entropic priors in the past can be traced to a failure to identify and incorporate all the relevant information .",
    "the information that has , in this paper , been translated into the entropic prior is that contained in the likelihood .",
    "entropic priors discussed here apply to a situation where all we know about the quantities @xmath0 is that they appear as parameters in the likelihood @xmath3 , _ and nothing else_. generalizations are , of course , possible .",
    "sometimes we are aware of additional relevant information beyond what is contained in the likelihood and it can easily be incorporated into a modified entropic prior .",
    "other times we might be guilty of overlooking additional information we already have .",
    "indeed , we would not be willing to spend valuable effort in the determination of a parameter @xmath0 unless we suspected that knowledge of @xmath0 has important implications elsewhere .",
    "typically we know something about the physical significance and the physical meaning of @xmath0 .",
    "it is clear that in these cases we know considerably more than just that @xmath0 is a parameter appearing in the likelihood .",
    "we might even conceive of several different experiments , @xmath6 , each yielding different sets of data @xmath7 related to @xmath0 by different likelihood functions @xmath8 .",
    "it is sometimes objected that one s prior knowledge about @xmath0 should not depend on which experiment one decides to use to measure it , but this objection is misplaced : the mere fact that @xmath0 is measurable through one or another experiment is additional information which , if relevant , should be taken into account .    another family of problems that can be tackled as a rather straightforward extension of the ideas described here involve choosing which likelihood distribution from among several competing candidates is responsible for generating the data . indeed , it is clear that any systematic approach to model selection requires as a prerequisite the capability to process in an objective way the information implicit in each of those likelihoods . except for some brief remarks in the final section , all these further developments , valuable as they might be , will be addressed elsewhere .",
    "our contribution includes a derivation of an entropic prior ( section 3 ) following the same principles of me inference that have been successful in statistical mechanics .",
    "in fact , our whole approach is guided by intuition gained from applications of me  to statistical mechanics .",
    "preliminary steps along this direction were taken in @xcite where a problem with the important case of experiments that can be indefinitely repeated had already been identified but not fully resolved .",
    "this problem , re - examined in section 4 , is interpreted as a symptom that important relevant information has been overlooked .",
    "the complete resolution , which hinges on identifying and incorporating this additional information , is given in sections 5 and 6 .",
    "the actual way in which me is used in the derivation , in analogy to standard applications in statistical mechanics , turns out to be important because it clarifies what it is that has been derived and how to use it : ours is , in effect , a third proposal for an entropic prior . in section 7",
    "we discuss in detail the important example of a gaussian likelihood and finally , in section 8 , we summarize and comment on the differences among the three versions of entropic prior and on possible further developments .",
    "the goal is to update beliefs about @xmath9 which are codified in the prior probability distribution @xmath10 to a posterior distribution @xmath11 when new information in the form of a constraint becomes available .",
    "( the constraints can , but need not , be linear . )",
    "the selection is carried out by ranking the probability distributions according to increasing _",
    "preference_. one feature we impose on the ranking scheme is transitivity : if distribution @xmath12 is preferred over distribution @xmath13 , and @xmath13 is preferred over @xmath14 , then @xmath12 is preferred over @xmath14 .",
    "such transitive rankings are implemented by assigning to each @xmath15 a real number @xmath16 $ ] called the entropy of @xmath17 in such a way that if @xmath12 is preferred over @xmath13 , then @xmath18>s[p_{2}]$ ] .",
    "the selected @xmath17 will be that which maximizes @xmath16 $ ] .",
    "thus the method involves entropies which are real numbers and entropies that should be maximized .",
    "these are features imposed by design ; they are dictated by the function that the me method is supposed to perform .",
    "next we determine the functional form of @xmath16 $ ] .",
    "this is the rule that defines the ranking scheme . _",
    "the purpose of the rule is to do induction .",
    "_ we want to extrapolate , to generalize from those special cases where we know what the preferred distribution should be to the much larger number of cases where we do not .",
    "thus , in order to be an inductive rule @xmath16 $ ] must have wide applicability ; we will assume that _ the same rule applies to all cases_. there is no justification for this universality except for the usual pragmatic justification of induction : we must be inclined to generalize lest we become paralyzed into not generalizing at all .",
    "but then , we should remain cautious and keep in mind that in many instances induction just fails .",
    "the argument goes as follows @xcite .",
    "if a general theory exists , then it must apply to special cases .",
    "furthermore , if in a certain special case the preferred distribution is known , then this knowledge can be used to constrain the form of @xmath16 $ ] .",
    "finally , if enough special cases are known , then @xmath16 $ ] will be completely determined .",
    "the known special cases are called the axioms  of me . as we will see below the axioms",
    "reflect the conviction that one should not change one s mind frivolously , that whatever was learned in the past is important",
    ". the chosen posterior distribution should coincide with the prior as closely as possible and one should only update those aspects of one s beliefs for which corrective new evidence has been supplied .",
    "the three axioms are listed below .",
    "* axiom 1 : locality*. _ local information has local effects .",
    "_ we do not revise the relative probabilities @xmath19 with @xmath1 and @xmath20 within a certain domain @xmath21 unless the newly provided information refers explicitly to the domain @xmath22 .",
    "the power of this axiom stems from the arbitrariness in the choice of @xmath22 .",
    "the consequence of the axiom is that non - overlapping domains of @xmath1 contribute additively to the entropy : @xmath16=\\int dy\\,f(p(y))$ ] where @xmath23 is some unknown function .",
    "* axiom 2 : coordinate invariance .",
    "* _ the ranking should not depend on the system of coordinates .",
    "_ the coordinates that label the points @xmath1 are arbitrary ; they carry no information .",
    "the consequence of this axiom is that @xmath16=\\int dy\\,p(y)f(p(y)/m(y))$ ] involves coordinate invariants such as @xmath24 and @xmath25 , where the density @xmath10 and the function @xmath26 are , at this point , unknown .",
    "next we make a second use of the locality axiom to enforce objectivity .",
    "we allow domain @xmath22 to extend over the whole space @xmath27 and assert that _ when there is no new information there is no reason to change one s mind .",
    "_ when there are no constraints the selected posterior distribution should coincide with the prior distribution .",
    "this eliminates the arbitrariness in the density @xmath10 : up to normalization @xmath10 is the prior distribution .",
    "* axiom 3 :  consistency for independent subsystems*. _ when a system is composed of independent subsystems it should not matter whether the inference procedure treats them separately or jointly .",
    "_ if @xmath28 , and the subsystem priors @xmath29 and @xmath30 are respectively upgraded to @xmath31 and @xmath32 , then the prior for the whole system @xmath33 should be upgraded to @xmath34 .",
    "this axiom restricts the function @xmath26 to be a logarithm .",
    "( the fact that the logarithm applies also when the subsystems are not independent follows from our inductive hypothesis that the ranking scheme has universal applicability . )    the overall consequence of these axioms @xcite is that probability distributions @xmath11 should be ranked relative to the prior @xmath10 according to their ( relative ) entropy @xcite , @xmath35=-\\int dy\\,p(y)\\log\\frac{p(y)}{m(y)}. \\label{s[p]}%\\ ] ] the derivation has singled out @xmath36 $ ] as _ the unique entropy to be used in inductive inference_. other expressions , such as @xmath37 $ ] , or @xmath36+s[m , p]$ ] , or even expressions that do not involve the logarithm , may be useful for other purposes , but they do not constitute an induction : they are not a generalization from the simple cases described in the axioms .",
    "we end this section with two comments on the prior density @xmath10 .",
    "first , @xmath36 $ ] may be infinitely negative when @xmath10 vanishes within some region @xmath22 .",
    "in other words , the me method confers an overwhelming preference on those distributions @xmath11 that vanish whenever @xmath10 does .",
    "is this a problem ?",
    "not really .",
    "a similar problem  also arises in the context of bayes theorem .",
    "a vanishing prior represents a tremendously serious prejudice because no amount of data to the contrary would allow us to revise it .",
    "the solution in both cases is to recognize that unless we are absolutely certain that @xmath1 could not possibly lie within @xmath22 then we should not have assigned @xmath38 in the first place . assigning a very low but non",
    "zero prior represents a safer and less prejudiced representation of one s beliefs and/or doubts both in the context of bayesian and of me inference .",
    "second , choosing the prior density @xmath10 can be tricky . when there is no information leading us to prefer one microstate of a physical system over another we might as well assign equal prior probability to each state .",
    "thus it is reasonable to identify @xmath10 with the density of states and the invariant @xmath39 is the number of microstates in @xmath40 .",
    "this is the basis for statistical mechanics .",
    "other examples of relevance to physics arise when there is no reason to prefer one region of the space @xmath27 over another",
    ". then we should assign the same prior probability to regions of the same volume ,  and we can choose @xmath41 to be the volume of a region @xmath42 in the space @xmath27 . notice that because of the presence of the prior @xmath10 not all subjectivity has been eliminated and laplace s principle of insufficient reason still plays an important role , albeit in a somewhat modified form .",
    "just as with bayes theorem , what is objective here is the manner in which information is processed , not the initial probability assignments .",
    "in this section we follow @xcite closely .",
    "we use the me method to derive a prior @xmath4 for use in bayes theorem , @xmath43 the selection of a preferred distribution using the me method demands that the space in which the search will be conducted be specified . being a consequence of the product rule for joint probabilities , bayes theorem requires that assertions such as ` @xmath1 and @xmath0 ' be meaningful and that the ` probability of @xmath1 and @xmath0 ' be well defined .",
    "therefore we must focus our attention on @xmath44 rather than @xmath4 ; the relevant universe of discourse is neither @xmath45 , the space of all @xmath0s , nor the data space @xmath27 , but the product @xmath46 .",
    "this point , first made by rodrguez @xcite , is central to the argument .",
    "our derivation and the final result , however , differ from his in several respects @xcite .    to rank distributions in the space @xmath46 we must decide on a prior @xmath47 . at this starting point absolutely nothing",
    "is known about the variables @xmath0 , in particular , they have no physical meaning , and no relation between @xmath1 and @xmath0 is known .",
    "the @xmath0s are totally arbitrary .",
    "therefore the prior must be a product @xmath48 of the separate priors in the spaces @xmath27 and @xmath45 .",
    "indeed , the distribution that maximizes the relative entropy @xmath49=-\\int dy\\,d\\theta\\,p(y,\\theta)\\,\\log\\frac{p(y,\\theta ) } { m(y)\\mu(\\theta)},\\ ] ] when no constraints are imposed is @xmath50 ; it is such that data about @xmath1 tells us absolutely nothing about @xmath0 .    in what follows",
    "we assume that @xmath10 is known .",
    "we consider this an important part of understanding what data it is that has been collected . in section 7",
    "we will suggest a reasonable @xmath10 for the special case of a gaussian likelihood .",
    "the prior @xmath51 remains unspecified .",
    "next we incorporate the crucial piece of information from which the parameters @xmath0 derive their physical meaning and which establishes the relation between @xmath0 and @xmath1 : the likelihood function @xmath3 is known .",
    "this has two consequences : first , the joint distribution @xmath44 is constrained to be of the form @xmath52 .",
    "notice that this constraint is not in the form that is most usual for applications of the me method : it is not an expectation value .",
    "note also that the only information we are using about the quantities @xmath0 is that they appear as parameters in the likelihood @xmath3 , _",
    "nothing else_. in many situations of experimental interest there exists additional relevant information beyond what is contained in the likelihood ; such information should be included as additional constraints in the maximization of the relative entropy @xmath53 .",
    "second , now that a bare minimum is known about @xmath0 , namely that each @xmath0 represents a probability distribution , there is a natural but still subjective choice for @xmath51 . as discussed in @xcite , except for an overall multiplicative constant , there is a unique riemannian metric that adequately reflects the fact that the points in a space of probability distributions are not ` structureless ' , but happen to be probability distributions ; this is the fisher - rao metric . within the finite - dimensional subspace defined by the constraint ",
    "the known @xmath3  the natural metric on @xmath45 is @xmath54 , where the unique @xmath55 induced by the family of distributions @xmath3 is @xmath56 accordingly we choose @xmath57 , where @xmath58 is the determinant of @xmath55 .",
    "having identified the prior measure and the constraints , we allow the me method to take over .",
    "the preferred distribution @xmath44 is chosen by varying @xmath59 to maximize @xmath60   &   = -\\int dy\\,d\\theta\\,\\pi(\\theta)p(y|\\theta)\\,\\log \\frac{\\pi(\\theta)p(y|\\theta)}{g^{1/2}(\\theta)m(y)}\\label{sigma[pi]}\\\\ &   = -\\int d\\theta\\,\\pi(\\theta)\\log\\frac{\\pi(\\theta)}{g^{1/2}(\\theta)}+\\int d\\theta\\,\\pi(\\theta)s(\\theta)~,\\nonumber\\end{aligned}\\ ] ] where @xmath61 is the entropy of the likelihood , @xmath62 @xmath63 writing the lagrange multiplier that enforces @xmath64 as @xmath65 , and assuming @xmath3 is normalized yields @xmath66 therefore the probability that the value of @xmath0 should lie within the small volume @xmath67 is @xmath68 this entropic prior is our first main result .",
    "it tells us that the preferred value of @xmath0 is that which maximizes the entropy @xmath61 because this maximizes the scalar probability density @xmath69 .",
    "it also tells us the degree to which values of @xmath0 away from the maximum are ruled out ; in many cases the preference for the me distribution can be overwhelming .",
    "note also that the density @xmath69 is a scalar function and the presence of the jacobian factor @xmath70 makes eq.([main ] ) manifestly invariant under changes of the coordinates @xmath0 in the space @xmath45 .",
    "we can claim a partial success .",
    "the ingredients that have been used are precisely those that led us to consider using bayes theorem in the first place .",
    "the information contained in the model  by which we mean that the data space @xmath27 , its measure @xmath10 , and the conditional distribution @xmath3  has been translated into a prior @xmath4 .",
    "the success is partial because it has been achieved for the special case of the fixed data space @xmath27 of those experiments which can not conceivably be repeated .",
    "a more complete treatment requires that we address the important case of experiments that can be repeated indefinitely .",
    "experiments need not be repeatable but sometimes they are .",
    "let us assume that successive repetitions are possible and that they happen to be independent .",
    "suppose , to be specific , that the experiment is performed twice so that the space of data @xmath71 consists of the possible outcomes @xmath72 and @xmath73 .",
    "suppose further that @xmath0 is not a random  variable ; the value of @xmath0 is fixed but unknown . then the joint distribution in the space",
    "@xmath74 is @xmath75 and the appropriate @xmath53 entropy is @xmath76=-\\int dy_{1}\\,dy_{2}\\,d\\theta\\,p(y_{1},y_{2},\\theta ) \\,\\log\\frac{p(y_{1},y_{2},\\theta)}{\\left [   g^{(2)}(\\theta)\\right ] ^{1/2}\\,m(y_{1})m(y_{2})},\\ ] ] where @xmath77 is the determinant of the fisher - rao metric for @xmath78 . from eq.([fisher metric ] ) it follows that @xmath79 so that @xmath80 , @xmath81 being the dimension of @xmath0 . maximizing @xmath82 $ ] subject to @xmath83",
    "we get @xmath84 where @xmath85 is the entropy of @xmath86 , and @xmath87 .",
    "the generalization to @xmath88 repetitions of the experiment , with data space @xmath89 , is immediate , @xmath90 this is clearly wrong : the dependence of @xmath91 on the amount @xmath88 of data would lead us to a perpetual revision of the prior as more data is collected .",
    "the absurdity of this situation becomes manifest when we consider the case of large @xmath88 .",
    "then the exponential preference for the value of @xmath0 that maximizes @xmath61 becomes so pronounced that no amount of data to the contrary can successfully overcome its effect .",
    "the data becomes irrelevant , and the more data we have , the more irrelevant it becomes .",
    "repeatable experiments present us with a problem .",
    "one possible attitude is to blame the me method : it gives nonsense and can not be trusted .",
    "as with all inductive methods this is , of course , a logical possibility .",
    "a second , more constructive approach , is to always be prepared to question the results of me calculations on the basis that there is no guarantee that all the information relevant to the situation at hand has been taken into account .",
    "the problem is not a failure of the me method but a failure to include all the relevant information .    that this is indeed the case",
    "can be seen as follows : when we say an experiment can be repeated twice , @xmath92 , we actually know more than just @xmath93 .",
    "we also know that forgetting or discarding the value of say @xmath73 , yields an experiment that is totally indistinguishable from the single , @xmath94 , experiment .",
    "this _ additional _ information is quantitatively expressed by @xmath95 , or equivalently @xmath96 which leads to @xmath97 .",
    "in the general case we get the manifestly reasonable result @xmath98 the challenge then is to identify a constraint that codifies this information within each space @xmath99 .",
    "the problem with the prior @xmath101 in eq.([pi(n ) ] ) is that it expresses an overwhelming preference for the value @xmath102 of @xmath0 that maximizes the entropy @xmath61 . indeed , as @xmath103 we have @xmath104 leading to @xmath105 which is manifestly incorrect .",
    "this suggests that a better prior would be obtained by maximizing the entropy @xmath106 of distributions on the space space @xmath99 subject to an additional constraint on the numerical value @xmath107 of the expected entropy @xmath108 .",
    "it is not that we happen to know the numerical value @xmath107 of @xmath109 .",
    "in fact we do not .",
    "it is rather that we recognize that information about @xmath107 is relevant in the sense that if @xmath107 were known the problem above would not arise .",
    "naturally , additional effort will be required to obtain the needed value of @xmath107 .",
    "the logic of the previous paragraph may sound unfamiliar and further comments may be helpful . when justifying the use of the me method to obtain , say , the canonical boltzmann - gibbs distribution ( @xmath110 ) it has been common to say something like we seek the minimally biased ( _ i.e. _ maximum entropy ) distribution that codifies the information we do possess ( the expected energy ) and nothing else .",
    "many authors find this justification objectionable . indeed , they might argue , for example , that the spectrum of black body radiation is what it is independently of whatever information happens to be available to us .",
    "we prefer to phrase our objection differently : in most realistic situations the expected value of the energy is not a quantity we happen to know .",
    "nevertheless , it is still true that maximizing entropy subject to a constraint on this ( unknown ) expected energy leads to correct predictions .",
    "therefore , the justification behind imposing a constraint on the expected energy can not be that this is a quantity that happens to be known  because it is not  but rather that the _ _  _ _ expected energy is the quantity that _ should _ be _ _ known . even if unknown , we recognize it as the crucial relevant information without which no successful predictions can be made .",
    "therefore we proceed as if this crucial information were available and produce a formalism that contains the temperature as a free parameter that will later have to be obtained from the experiment itself . in other words ,",
    "the temperature ( or expected energy ) is one additional parameter to be inferred from the data .",
    "the entropy on the space @xmath99 is @xmath111   &   = -\\int\\,d\\theta\\,dy^{(n)}\\,\\pi(\\theta)p(y^{(n)}%    ) } \\nonumber\\\\ &   = -\\int\\,d\\theta\\,\\pi(\\theta)\\log\\frac{\\pi(\\theta)}{g^{1/2}(\\theta)\\,}+n\\int d\\theta\\,\\pi(\\theta)s(\\theta)\\end{aligned}\\ ] ] where @xmath61 given by eq.([stheta ] ) .",
    "( a constant factor of @xmath112 associated to the fisher - rao measure @xmath113 has been omitted .",
    "it would eventually be absorbed into the normalization of @xmath4 . ) to obtain the prior @xmath4 we maximize @xmath106 subject to constraints on @xmath108 and that @xmath114 be normalized , @xmath115   = 0~. \\label{varying sigma n}%\\ ] ] this gives , @xmath116 therefore , @xmath117",
    "~.\\ ] ] the undesired dependence on @xmath88 is eliminated if in each space @xmath118 the lagrange multipliers @xmath119 are chosen so that @xmath120 is a constant independent of @xmath88 .",
    "the resulting entropic prior , @xmath121 satisfies eq.([constr on pi ] ) .",
    "this is our second main result .",
    "the prior @xmath122 codifies information contained in the likelihood function , plus information about the expected value of the entropy of the likelihood implicit in the hyper - parameter @xmath100 , @xmath123 with @xmath124 is given by @xmath125    the next and final step is figure out which @xmath100 applies to the particular experimental situation under consideration .",
    "the natural way to proceed is to invoke bayes theorem @xmath126 the choice of a prior @xmath127 for @xmath100 itself is addressed in the next section .",
    "if we were truly interested in the actual @xmath100 , we could marginalize over @xmath0 to obtain @xmath128 but our interest in the value of @xmath100 is only indirect ; @xmath100 is a necessary but annoying technical complication along the way to the real goal which is inferring @xmath0 .",
    "marginalizing over @xmath100 , we get @xmath129 where @xmath130 this is the answer we sought : the effective prior for @xmath0 , the averaged @xmath131 , is independent of the actual data @xmath132 , as it should .",
    "the last step is the assignment of @xmath127 .",
    "to remain consistent with the spirit of this paper , namely using me to obtain priors , the prior for @xmath100 must itself be an entropic prior .",
    "the motivation behind discussing entropic priors is that we wish to consider information included in the likelihood function .",
    "since @xmath3 refers to @xmath0 but makes no reference to any hyper - parameters it is quite clear that @xmath100 should not be treated like the other @xmath0s .",
    "the relation between @xmath100 and the data @xmath1 is indirect : @xmath100 is related to @xmath0 , and @xmath0 is related to @xmath1 .",
    "once @xmath0 is given , the data @xmath1 becomes irrelevant , it contains no further information about @xmath100 .",
    "the whole significance of @xmath100 is derived purely from its appearance in @xmath133 , eq.([main 2 ] ) .",
    "therefore , the relevant universe of discourse is @xmath134 with @xmath135 .",
    "we focus our attention on the joint distribution @xmath136 and we obtain @xmath127 by maximizing the entropy @xmath137=-\\int d\\alpha\\,d\\theta\\,\\,\\pi(\\alpha,\\theta)\\log\\frac { \\pi(\\alpha,\\theta)}{\\gamma^{1/2}(\\alpha)\\,g^{1/2}(\\theta)}\\ , \\label{sigma prime}%\\ ] ] where @xmath138 is determined below . since no reference is made to repeatable experiments in @xmath89 there is no need for any further constraints except for normalization .",
    "the fisher - rao measure @xmath138 in eq.([sigma prime ] ) is @xmath139   ^{2}.\\ ] ] using eqs.([main 2]),([sbar ] ) and ( [ zeta(alpha ) ] ) we get @xmath140   ^{2}=(\\delta s)^{2},\\ ] ] but @xmath141   ^{2}=(\\delta s)^{2}.\\ ] ] therefore , @xmath142 the interpretation is straightforward : the distance between @xmath143 or , in words , the local entropy uncertainty @xmath144 is the distance per unit change in @xmath100 .    to maximize @xmath145 rewrite it as @xmath137=-\\int d\\alpha\\,\\pi(\\alpha)\\log\\frac{\\pi(\\alpha)}% { \\gamma^{1/2}}+\\int d\\alpha\\,\\pi(\\alpha)\\,s(\\alpha),\\ ] ] where @xmath146 is given by @xmath147 then , varying with respect to @xmath127 gives @xmath148 this is our third main result .",
    "it completes our derivation of the actual prior for @xmath0 : the averaged @xmath131 in eq.([main 3 ] ) codifies information contained in the likelihood function , plus the insight that for repeatable experiments , information about the expected likelihood entropy , even if unavailable , is relevant .",
    "we argued above that the hyper - parameter @xmath100 should not be treated in the same way as the other parameters @xmath0 because the likelihood @xmath149 refers only to @xmath0s and not to @xmath100 .",
    "nonetheless , it may still be worthwhile to discuss briefly what would happen if @xmath100  were treated as one of the @xmath0s . in this case",
    ", the entropic prior @xmath150 would be determined by focusing our attention on the joint distribution @xmath151 where the last two factors on the right are assumed known .",
    "the assumed universe of discourse would be @xmath152 .",
    "a straightforward application of the me method would , as before , run into trouble with an unwanted @xmath88 dependence which would require the introduction of a new constraint on the appropriate expected entropy .",
    "thus , the entropic prior for @xmath100 would involve a second hyper - parameter @xmath153",
    ". the unknown @xmath153 would itself require its own entropic prior , involving yet a third hyper - parameter @xmath154 , and so on .",
    "there would be an endless chain of hyper - parameters @xcite . in any practical calculation",
    ", the chain would have to be truncated .",
    "whether the predictions about @xmath0 depend on where and how the truncation is carried out remains to be studied .",
    "but , fortunately , this is not necessary : @xmath100 is not like the other @xmath0s .",
    "consider data @xmath155 that are scattered around an unknown value @xmath156 , @xmath157 with @xmath158 and @xmath159 the goal is to estimate the parameters @xmath160 on the basis of the data @xmath132 and the information implicit in the model : the data space @xmath27 , the measure @xmath10 ( discussed below ) , and the gaussian likelihood , @xmath161   ~. \\label{gaussian likelihood}%\\ ] ]    in section 3 we asserted that knowing the measure @xmath10 is part of knowing what data has been collected .",
    "therefore , nothing can be said about @xmath10 without further specification of the experimental situation .",
    "it turns out , however , that in many physical situations where the data happen to be distributed according to eq.([gaussian likelihood ] ) the underlying space @xmath27 is sufficiently symmetric , _",
    "i.e. _ , invariant under translations , that we can assume @xmath162 this is physically reasonable .",
    "gaussian distributions arise when the measured value of @xmath1 is the sum of a large number of microscopic  contributions and the details of how the individual contributions are themselves distributed are washed out in the macroscopic  sum .",
    "the macroscopically relevant features are just those that distinguish one gaussian from another , namely , the mean @xmath156 and the variance @xmath163 .",
    "this is the physical basis behind the central limit theorem .",
    "but if microscopic details are irrelevant it should be possible to understand the situation from a purely macroscopic point of view : it should be possible to obtain the gaussian distribution as the preferred one among all those with the given @xmath156 and @xmath163 , and this is , indeed , the case : setting @xmath164 in @xmath36 $ ] , eq.([s[p ] ] ) , and maximizing subject to constraints on the mean and variance yields eq.([gaussian likelihood ] ) .    from eqs .",
    "( [ stheta ] ) and ( [ gaussian likelihood ] ) the entropy of the likelihood is @xmath165",
    "\\quad \\text{where}\\quad\\sigma_{0}\\overset{\\operatorname*{def}}{=}\\left (   \\frac { e}{2\\pi}\\right )   ^{1/2}\\frac{1}{m}~,\\ ] ] and the corresponding fisher - rao measure , from eq.([fisher metric ] ) is @xmath166{cc}% 1/\\sigma^{2 } & 0\\\\ 0 & 2/\\sigma^{2}% \\end{array } \\right\\vert = \\frac{2}{\\sigma^{4}}~.\\ ] ]    note that both @xmath167 and @xmath168 are independent of @xmath156 .",
    "this means that if we were concerned with the simpler problem of estimating @xmath156  in a situation where @xmath53 happens to be known , then the entropic prior , in any of the versions eq.([main ] ) , ( [ main 2 ] ) , or ( [ main 3 ] ) , is a constant independent of @xmath156 . in other words , when @xmath53 is known , the bayesian estimate of @xmath156 using entropic priors coincides with the maximum likelihood estimate , _",
    "i.e. _ , by the popular procedure of minimizing @xmath169    returning to the more interesting case of unknown @xmath53 , the @xmath170-dependent entropic prior , eq.([main 2 ] ) is @xmath171 @xmath172 is improper in both @xmath156 and @xmath53 ; normalization requires  the introduction of high and low cutoffs for both @xmath156 and @xmath173 . the fact that without cutoffs the model is not well defined is an indication that more relevant information is being requested : the cutoffs constitute relevant information that must be taken into account .",
    "( the logic parallels that which led to the introduction of @xmath100 in section 5 . )",
    "the case of unknown cutoff values is important and we intend to explore it in detail in future work .",
    "the basic idea is that specifying cutoffs is an integral part of defining the model , and therefore the choice of cutoffs can be tackled as a problem of model selection . in the remainder of this section , however , we will assume that the information about cutoffs is already available .",
    "it is convenient to write the range of @xmath156 as @xmath174 and to define the @xmath53 cutoffs in terms of dimensionless quantities @xmath175 and",
    "@xmath176 ; @xmath53 extends from @xmath177 to @xmath178",
    ". then @xmath124 and @xmath172 are given by @xmath179 and @xmath180 notice that in the special case of @xmath181 , the prior over @xmath53 reduces to @xmath182 which is called the jeffreys prior and is usually introduced by the requirement of invariance under scale transformations , @xmath183",
    ".    writing @xmath184 , the prior for @xmath100 can be obtained from eq.([gamma(alpha ) ] ) , @xmath185 and from eqs.([main 3 ] ) and ( [ s(alpha ) ] ) , @xmath186   ~ , \\label{pi(alpha)gaussian}%\\ ] ] where the normalization @xmath187 has been suitably redefined .",
    "eqs.([gammagaussian ] ) and ( [ pi(alpha)gaussian ] ) simplify considerably when we take the limit @xmath188 .",
    "clearly the same result is obtained whether we let @xmath189 while keeping @xmath175 fixed , or letting @xmath190 while keeping @xmath176 fixed , or even allowing @xmath191 and @xmath190 simultaneously .",
    "the resulting @xmath192 and @xmath127 are@xmath193 and @xmath194{cc}% \\frac{1}{\\left (   1-\\alpha\\right )   ^{2}}\\exp\\left [   { \\frac{1}{\\alpha-1}}\\right ] & \\text{for\\quad}\\alpha<1\\\\ 0 & \\text{for\\quad}\\alpha\\geq1 \\end{array } \\right .   \\label{limit pigaussian}%\\ ] ] where @xmath127 is normalized .",
    "this is shown in fig .",
    "[ fig . 1 ] .",
    "[ ptb ]    fig1.eps    @xmath127 reaches its maximum value at @xmath195 . since @xmath196 for @xmath197 the expected value of @xmath100 and all higher moments diverge",
    "this suggests that replacing the unknown @xmath100 in the prior @xmath122 by any given numerical value @xmath198 is probably not a good approximation .",
    "as explained in section 5 , since @xmath100 is unknown , the effective prior for @xmath199 is obtained marginalizing @xmath200 over @xmath100 , eq.([main 3 ] ) . since @xmath201 for @xmath202 as @xmath188",
    "we can safely take the limit @xmath189 or @xmath203 .",
    "conversely , since @xmath204 for @xmath205 we can not take @xmath190 or @xmath206 .",
    "the limit @xmath203 while keeping @xmath207 fixed gives , @xmath208{cc}% \\frac{1}{\\delta\\mu\\sigma_{l}}\\frac{\\exp\\left [   { \\frac{1}{\\alpha-1 } } \\right ]   } { 1-\\alpha}\\left (   \\frac{\\sigma}{\\sigma_{l}}\\right )   ^{\\alpha-2 } & \\text{for\\quad}\\alpha<1\\\\ 0 & \\text{for\\quad}\\alpha\\geq1 .",
    "\\end{array } \\right .",
    "\\label{pi(theta , alpha)g}%\\ ] ]    the averaged prior for @xmath156 and @xmath53 is @xmath209   } { 1-\\alpha}\\left (   \\frac{\\sigma}{\\sigma_{l}}\\right )   ^{\\alpha } d\\alpha~,\\ ] ] which integrates to @xmath210 where @xmath211 is a modified bessel function of the second kind .",
    "this is the entropic prior for the gaussian model .",
    "the function @xmath212 is shown in fig .",
    "2 ] as a function of @xmath213 .",
    "[ tb ]    fig2.eps    @xmath214 has an integrable singularity as @xmath215 where it behaves as @xmath216 since @xmath207 is a lower cutoff the region of large @xmath217 is more relevant .",
    "the leading asymptotic behavior is given by@xmath218    finally , we turn to bayes theorem , eq.([entropic bayes ] ) , with the prior ( [ main 5 ] ) to obtain estimators for @xmath156 and @xmath53 . for large @xmath88",
    "the results are independent of the prior and the estimators coincide with the standard maximum likelihood results .",
    "the case when @xmath88 is not so large is the more interesting one . as estimators we can take the expected values @xmath219 and @xmath220 over the posterior ( [ entropic bayes ] ) .",
    "the integrations can be performed numerically and are not particularly illuminating .",
    "alternatively , one can follow standard practice and marginalize eq.([entropic bayes ] ) over @xmath53 to obtain the distribution @xmath221 and calculate the estimator @xmath222 from @xmath223 and its error bar @xmath224 from @xmath225 when @xmath221 happens to be a gaussian these estimators coincide with the expected values @xmath219 and @xmath220 .",
    "the final result for @xmath222 is very simple . for any value of @xmath88",
    "we have @xmath226 the estimator @xmath222 is the sample average .",
    "the result for @xmath224 is not as elegant but , of course , for large @xmath88 it asymptotically reduces to @xmath227",
    "in this paper the method of maximum relative entropy has been used to translate the information contained in the known form of the likelihood into a prior distribution for bayesian inference .",
    "the argument follows closely the analogous me  methods that have been so successful in statistical mechanics . for experiments that can not be repeated the resulting entropic prior  is formally identical with the einstein fluctuation formula . for repeatable experiments , however , the expected value of the entropy of the likelihood  represented in terms of a lagrange multiplier @xmath100  turns out to be relevant information that must be included in the analysis . as an illustration",
    "the important case of a gaussian likelihood was treated in detail .",
    "it may be useful to comment briefly on the differences between our entropic prior and the versions previously proposed by skilling and by rodrguez .",
    "perhaps the main difference with skilling s prior is that , unlike ours , its use is not restricted to probability distributions but is intended for generic positive additive distributions  including , for example , the distributions of intensities in images @xcite .",
    "one problem here is that of justifying the applicability of the me method in such a general context . our impulse to generalize is a dangerous one",
    "; we may get away with indulging it occasionally but overindulgence will certainly lead to error . in any case , our argument in section 3 , which consists in maximizing the entropy @xmath53 subject to a constraint @xmath5 , makes no sense in the case of generic positive additive distributions for which there is no available product rule .",
    "a more specific problem arises from the fact that skilling s entropy is not , in general , dimensionless and the hyper - parameter @xmath100 is vaguely interpreted some sort of cutoff carrying the appropriate corrective units .",
    "some of the difficulties , which led skilling to seek an alternative approach , were identified in @xcite .",
    "rodrguez s approach is closer to ours .",
    "his prior applies to probability distributions and appears to be derived from a me principle @xcite .",
    "one difference , perhaps a minor one , is his treatment of the underlying measure @xmath10 . for us",
    "@xmath10 is not arbitrary ; knowing @xmath10 is part of knowing what data has been collected . for him",
    "@xmath10 is just an initial guess and he suggests setting @xmath228 for some value @xmath229 .",
    "the more important difference , however , is that the number of observed data @xmath230 is deliberately and explicitly left unspecified .",
    "the space @xmath231 over which distributions are defined , and therefore the distributions themselves , also remain unspecified .",
    "it is not clear what the maximization of an entropy over such unspecified spaces could possibly mean but a hyper - parameter @xmath100 is eventually introduced and it is interpreted as a virtual number of observations supporting the initial guess @xmath229 .",
    "he proposes that @xmath100 be considered as one more among the parameters @xmath0 to be inferred .",
    "as mentioned earlier this leads to the introduction of an endless chain of additional hyper - parameters .",
    "there are several directions in which the ideas of this paper can be further extended .",
    "first , we emphasize once again that the entropic priors discussed here apply to a situation where all we know about the quantities @xmath0 is that they appear as parameters in the likelihood @xmath3 , _ and nothing else_. in many situations of experimental interest there exists additional relevant information beyond what is contained in the likelihood .",
    "such information should be included as additional constraints in the maximization of the relative entropy @xmath53 in eq.([varying sigma n ] ) .",
    "the resulting modified entropic prior would provide a better representation of our state of knowledge prior to the acquisition of the data . indeed , the advantage of the bayesian approach over the usual method of maximum likelihood is the possibility of including additional relevant information by replacing a flat prior by an appropriately more informative prior",
    ". there is nothing to prevent us from performing a similar improvement and going beyond the bare  entropic priors discussed in this paper .",
    "two kinds of additional information that are easy to include are restrictions on the range of the parameters @xmath0 and information about the known expected values of some variables @xmath232 .",
    "steps in this direction were taken in section 5 , where @xmath232 is the likelihood entropy , and in section 7 where high and low cutoffs on the range of the gaussian parameters were introduced .",
    "second , in the introduction we mentioned the interesting possibility of analyzing data @xmath7 from different experiments , @xmath6 , related to @xmath0 by different likelihood functions @xmath233 .",
    "clearly this can be analyzed as a single combined experiment with likelihood @xmath234 to which all our previous results apply .",
    "as we stated earlier , the mere fact that @xmath0 is measurable through one or another experiment is additional relevant information that can be taken into account .",
    "third , we also mentioned that problems of model selection can be tackled as an extension of the ideas described in this paper .",
    "on the basis of data @xmath1 we want to select one model among several competing candidates labeled by @xmath235 with likelihood distributions given by @xmath236 . the answer , _",
    "i.e. _ , the probability of model @xmath237 given the data @xmath1 , is given by bayes theorem , @xmath238 this is exact .",
    "the problem is solved , at least in principle , once an entropic prior for @xmath239 is assigned .",
    "however , the remaining practical problems associated with carrying out the actual numerical calculations could , of course , still be quite formidable .",
    "finally , we end with a word of caution . as in all instances of inductive inference",
    "there is the possibility that predictions based on the me method could be wrong because not all the information relevant to the problem at hand was taken into account .",
    "this potential problem is not peculiar to the me method , it is a problem shared by all methods of induction .",
    "nevertheless , we are confident that the rewards of extending the benefits of an inductive method singled out by requirements of objectivity , the me method , beyond its traditional territory of statistical mechanics and into that of data analysis will be enormous .",
    "* acknowledgments- * many of our comments and arguments have been inspired by carlos c. rodrguez , volker dose , and rainer fisher through insightful questions and discussions which we gratefully acknowledge .",
    "a. c. also acknowledges the hospitality of the max - planck - institut fr plasmaphysik during the two extended visits when most of this work was carried out .      for a recent review see v. _ _ dose , bayesian inference in physics : case studies , rep .",
    "phys . , accepted for publication ( 2003 ) ; for a pedagogical introduction",
    "see d. s. sivia , data analysis , a bayesian tutorial  ( oxford university press , oxford , 1996 ) .",
    "r.  preuss et al . , phys .",
    "* 73 * , 732 ( 1994 ) ; r.  preuss , w.  hanke and w.  von der linden , phys .",
    "lett .  * 75 * , 1344 ( 1995 ) ; r.  preuss , w.  hanke , c.  grber , and h.g .",
    "evertz , phys .",
    "* 79 * , 1122 ( 1997 ) .",
    "e. t. jaynes , ieee trans .",
    "cybern . vol . *",
    "ssc-4 * , 227 ( 1968 ) ; j. m. bernardo , j. roy .",
    "b * 41 * , 113 ( 1979 ) ; a. zellner , `` bayesian methods and entropy in economics and econometrics '' in _ maximum entropy and bayesian methods _ , edited by w. t. grandy jr . and",
    "l. h. schick ( kluwer , dordrecht , 1991 ) .",
    "j. skilling , `` classic maximum entropy '' in _ maximum entropy and bayesian methods _ , j. skilling ( ed . ) ( kluwer , dordrecht , 1989 ) ; `` quantified maximum entropy '' in _ maximum entropy and bayesian methods _ , p. f. fougre ( ed . ) ( kluwer , dordrecht , 1990 ) .",
    "c. c. rodrguez , the metrics generated by the kullback number  in _ maximum entropy and bayesian methods _",
    ", j. skilling ( ed . ) ( kluwer , dordrecht , 1989 ) ; objective bayesianism and geometry  in _ maximum entropy and bayesian methods _ , p. f. fougre ( ed . ) ( kluwer , dordrecht , 1990 ) ; entropic priors  in _ maximum entropy and bayesian methods _ , edited by w. t. grandy jr . and",
    "l. h. schick ( kluwer , dordrecht , 1991 ) ; bayesian robustness : a new look from geometry  in _ maximum entropy and bayesian methods _ , g. r. heidbreder ( ed . ) ( kluwer , dordrecht , 1996 ) .    on terminology",
    ": the terms ` prior ' and ` posterior ' are normally used in the context of bayes theorem ; we retain the same terminology when using me because we are concerned with the similar goal of processing information to update from a prior to a posterior .",
    "the method of me  is usually understood in the restricted sense that one updates from a prior distribution that happens to be uniform .",
    "here we adopt a broader meaning that includes updates from arbitrary priors and which involves the maximization of relative entropy .",
    "indeed since all entropies are relative to some prior the qualifier ` relative ' is not needed and will henceforth be omitted .    c. e. shannon , bell systems tech .",
    "journal * 27 * , 379 , 623 ( 1948 ) ; c. e. shannon and w. weaver , _ the mathematical theory of communication _",
    "( univ . of illinois press , urbana , 1949 ) ; n. wiener , _ cybernetics _",
    "( mit press , cambridge , 1948 ) ; l. brillouin , _ science and information theory _ , ( academic press , new york , 1956 ) ; s. kullback , _ information theory and statistics _",
    "( wiley , new york , 1959 ) .    e. t. jaynes , information theory and statistical mechanics  phys . rev . * 106 * , 620 and * 108 * , 171 ( 1957 ) ; r. d. rosenkrantz ( ed . ) , _",
    "e. t. jaynes : papers on probability , statistics and statistical physics _",
    "( reidel , dordrecht , 1983 ) ; e. t. jaynes , _ probability theory : the logic of science _ ( cambridge university press , cambridge , 2003 )",
    ".    j. e. shore and r. w. johnson , axiomatic derivation of the principle of maximum entropy and the principle of minimum cross - entropy ,  ieee trans .",
    "theory * it-26 * , 26 ( 1980 ) ; y. tikochinsky , n. z. tishby and r. d. levine , phys .",
    "lett . * 52 * , 1357 ( 1984 ) and phys . rev . *",
    "a30 * , 2638 ( 1984 ) ; i. csiszar , ann . stat .",
    "* 19 * , 2032 ( 1991 ) .",
    "c. c. rodrguez , see section 3 of `` are we cruising a hypothesis space ? '' in _ maximum entropy and bayesian methods _",
    ", ed . by w. von der linden , v. dose , r. fischer and r. preuss ( kluwer , dordrecht , 1999 ) .",
    "c. c. rodrguez : ` entropic priors for discrete probabilistic networks and for mixtures of gaussian models ' . in : _",
    "bayesian inference and maximum entropy methods in science and engineering _ , ed . by r. l. fry , aip conf .",
    "* 617 * , 410 ( 2002 ) ( online at arxiv.org/abs/physics/0201016 ) .",
    "a. caticha , ` maximum entropy , fluctuations and priors ' , in _ bayesian methods and maximum entropy in science and engineering _ , ed . by a. mohammad - djafari , aip conf",
    ". proc . * 568 * , 94 ( 2001 ) ( online at arxiv.org/abs/math-ph/0008017 ) .",
    "the number and the wording of our axioms differs from skilling s because we concentrate on the specific problem of ranking probability distributions while he was concerned with ranking general positive additive distributions .",
    "proofs , which are easily constructed following shore and johnson @xcite and skilling @xcite , will be presented elsewhere .",
    "s. amari , _ differential - geometrical methods in statistics _",
    "( springer - verlag , 1985 ) ; for a brief derivation see a. caticha , change , time and information geometry ,  in _ bayesian methods and maximum entropy in science and engineering _ , ed . by a. mohammad - djafari , aip conf .",
    "proc . * 568 * , 72 ( 2001 ) ( online at arxiv.org/abs/math-ph/0008018 ) .",
    "j. skilling and s. sibisi , priors on measures  in _ maximum - entropy and bayesian methods _ , k. m. hanson and r. n. silver ( eds . ) ( kluwer , dordrecht , 1996 ) ; j. skilling , massive inference and maximum entropy  in _ maximum - entropy and bayesian methods _ , g. j. erickson , j. t. ryckert and c. r. smith ( eds . ) ( kluwer , dordrecht , 1998 ) ."
  ],
  "abstract_text": [
    "<S> the problem of assigning probability distributions which objectively reflect the prior information available about experiments is one of the major stumbling blocks in the use of bayesian methods of data analysis . in this paper </S>",
    "<S> the method of maximum ( relative ) entropy ( me ) is used to translate the information contained in the known form of the likelihood into a prior distribution for bayesian inference . </S>",
    "<S> the argument is inspired and guided by intuition gained from the successful use of me methods in statistical mechanics . for experiments that can not be repeated the resulting entropic prior  is formally identical with the einstein fluctuation formula . </S>",
    "<S> for repeatable experiments , however , the expected value of the entropy of the likelihood turns out to be relevant information that must be included in the analysis . </S>",
    "<S> the important case of a gaussian likelihood is treated in detail . </S>"
  ]
}