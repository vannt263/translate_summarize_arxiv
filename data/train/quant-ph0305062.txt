{
  "article_text": [
    "we are going to analyze discrete probability distributions @xmath1 , which consist of non  negative numbers summing to unity .",
    "@xmath2 . to characterize quantitatively such probability vectors",
    "one uses shannon ( information ) entropy @xcite @xmath3 where we adopt the convention that @xmath4 , if necessary .",
    "the shannon entropy is distinguished by several unique properties @xcite , but it is often convenient to introduce generalized _ rnyi entropies _ @xcite parametrized by a continuous parameter @xmath5 , @xmath6 .",
    "\\label{psalph1}\\ ] ] the rnyi entropies are well defined for positive @xmath7 , but is is not difficult to show that for any probability distribution @xmath8 one has @xmath9 . for consistency the shannon entropy @xmath10",
    "will thus be denoted by @xmath11 .",
    "this method of generalizing the shannon entropy is by far not the only one  for reviews of other generalizations see books by kapur @xcite and arndt @xcite .    in this work",
    "we discuss relations between rnyi entropies of different orders , and in particular between @xmath11 , @xmath12 and @xmath13 .",
    "physical motivation for such a study is twofold .",
    "first , we may not know the entire vector @xmath8 , but only a few of its @xmath14 norms , so knowing the renyi entropies , say @xmath12 and @xmath13 we want to estimate the unknown shannon entropy @xmath11 .",
    "such a situation occurs if one studies an @xmath0 dimensional quantum mechanical mixed state @xmath15 according to the scheme recently proposed by p. horodecki et al . @xcite and measures directly the traces tr@xmath16 for @xmath17 . if @xmath18 the entire spectrum of @xmath15 remains unknown , and it is not possible to find its von neumann entropy , @xmath19 , ( i.e. the shannon entropy of the spectrum ) , but the generalized renyi entropies @xmath20 , including the linear entropy , which is a function of @xmath12 may be readily obtained .",
    "similar problems arise in many different branches of physics , for instance by the study of the statistics between particles created in high  energy collisions @xcite . measuring probabilities of that two independent collisions give rise to the same distribution of particles allows one to obtain the rnyi entropy @xmath12 , but not directly the shannon entropy @xmath11 .    another reason to study relations between @xmath11 and @xmath12 has been provided by the work by pipek and varga @xcite .",
    "they assumed that both these quantities are known , and observed that their difference , @xmath21 called _ structural entropy _ , provides an important characterization of the analyzed probability vector @xmath8 .",
    "for instance , an increase of the structural entropy charactering an eigenstate of a tight binding model indicates the anderson transition .",
    "several other applications of structural entropy include also quantum chemistry and statistical analysis of quantum spectra , ( see @xcite and references therein ) .",
    "the von neumann entropy of a mixed state obtained by partial trace of a bi  partite pure state , @xmath22 , measures the degree of entanglement of the pure state @xmath23 . alternatively one can measure the entanglement by generalized entropies ( see e.g. @xcite ) , so relations between entropies analyzed in this work provide bounds between different measures of entanglement .",
    "this very point has recently been discussed in the paper by wei et al .",
    "@xcite , which provides an additional motivation for the present work .",
    "this paper is ogranized as follows . in section 2",
    "the basic properties of the rnyi entropies are reviewed . in section 3",
    "we present recent results of topse and harremos @xcite , which allow us to propose lower and upper bounds on the shannon entropy obtained out of the rnyi entropies of order two and three , provided the length @xmath0 of the vector is known .",
    "they are derived in section 4 , while in section 5 we propose and analyze en estimation of the shannon entropy .",
    "consider a random variable @xmath24 attaining not more than @xmath0 different values with probabilities @xmath25 , @xmath26 .",
    "such discrete probability distribution @xmath27 may be cahracterized by the shannon entropy ( [ shann1 ] ) or generalized rnyi entropies ( [ psalph1 ] ) .",
    "all generalized entropies @xmath28 vary from zero for a certain event ( the distribution @xmath29 ) to @xmath30 , for the uniform distribution , ( the distribution @xmath31 ) . for the distributions with @xmath32 equal elements , @xmath33 ,",
    "the entropies admit intermediate values , @xmath34 .",
    "the rnyi entropy @xmath28 converges to the shannon entropy in the limit @xmath35 .",
    "it is also useful to express the shannon entropy as the limit of the derivative , @xmath36 }          { \\partial q}. \\label{psderiv}\\ ] ]    some special cases of @xmath37 are of special interest .",
    "for @xmath38 we have @xmath39 $ ] .",
    "the rnyi entropy of order two , called _ extension entropy _",
    "@xcite , is closely related to the _ inverse participation ratio _ , @xmath40 .",
    "\\label{ipr}\\ ] ] this quantity characterizes the `` effective number of different events '' which the stochastic variable may admit , and varies from unity for @xmath41 , to @xmath0 for the uniform distribution @xmath42 .",
    "another quantity @xmath43 , called _ index of coincidence _",
    "@xcite , in quantum mechanical problems is called _ purity _ , since the larger @xmath44 the more pure , the state it describes .",
    "the quantity @xmath45 is called _ linear entropy _ since in analogy to shannon entropy it achieves its maximum for the uniform distribution @xmath42 .    in the case",
    "@xmath46 the rnyi entropy is a function of the number @xmath47 of positive components of the vector , @xmath48 . in the limit @xmath49",
    "we obtain a quantity analogous to the chebyshev norm : @xmath50 , where @xmath51 is the largest component of @xmath8 .",
    "the rnyi entropy ( [ psalph1 ] ) is a sum of @xmath0 terms so for any finite @xmath0 the function of @xmath28 on @xmath5 is differentiable .",
    "the functional dependence of the rnyi entropy on its parameter was investigated in detail by back and schlgl @xcite ) . making use of the fact that the function @xmath52 is convex for @xmath53 and concave for @xmath54",
    "they have proved several inequalities called _ rnyi information _ was analyzed , so the direction of the inequalities derived there is inverted .",
    "] , which we recall in the case @xmath55 , @xmath56    the first inequality ( [ diff1 ] ) means that the rnyi entropy is a non increasing function of its parameter , @xmath57 and this statement is valid also for infinite probability vectors and the cases of nondifferentiable @xmath28 @xcite .",
    "hence the structural entropy @xmath21 is non  negative @xcite .",
    "inequality ( [ diff4 ] ) implies that the dependence of the rnyi entropy on its parameter is convex . ,",
    "in which consequences of this error are pointed out . ] thus knowing the @xmath58 and @xmath59entropies one obtains by linear interpolation an upper bound for the shannon entropy @xmath60 this relation gives us an upper bound for the structural entropy @xmath61 valid for any vector @xmath62 of a finite length @xmath0 .    in an analogous way , if the rnyi entropies of order @xmath59 and @xmath63 are known , the linear extrapolation provides a lower bound for the shannon entropy @xmath64 which combined with ( [ ren02 ] ) , allows one to write down a simple estimation @xmath65 , ( see fig 3.a ) , @xmath66 . \\label{ren023}\\ ] ]    making use of the inequality ( [ diff2 ] ) we obtain the relation @xmath67 which is equivalent to the statement that the @xmath14norm is a non  increasing function , @xmath68 .",
    "this result provides another upper bound , @xmath69 .",
    "although it is not applicable for the shannon entropy , for which @xmath70 so the inequality becomes trivial , but it gives an usefull bound on @xmath28 with @xmath71 by the limiting value @xmath72 , @xmath73    in further sections of this work we shall discuss possibilities of finding more precise bounds and estimations for the shannon entropy , provided the dimension @xmath0 of the probability vector is known .",
    "for any value @xmath74 the generalized entropy @xmath37 is equal zero for certain events described by the distribution @xmath41 , and achieves its maximum for the uniform distribution , @xmath75 .    to investigate further relations between the r ' enyi entropies of different order we have chosen to analyze the case of @xmath76 dimensional vectors @xmath8 .",
    "the space of all possible probability vectors , plotted in the the plane @xmath77 forms an equilateral triangle of side @xmath78 measured in the euclidean distance .",
    "its three corners : @xmath79 , @xmath80 and @xmath81 represent certain events , while the center of the triangle corresponds to the uniform distribution @xmath82 .",
    "[ fig : ren1 ] shows sets of points characterized by the same rnyi entropy of order @xmath5 , which may be called iso - entropy curves . independently of the value of",
    "the parameter @xmath5 the generalized entropy attains its minimum , @xmath83 , at the corners of the triangle , while the maximum @xmath84 is achieved at the point @xmath82 at the center of the triangle .",
    "as shown in fig .",
    "[ fig : ren1]a the maximum is rather flat for @xmath85 .",
    "the case shown in this panel resembles the limiting case @xmath86 , for which the entropy reflects the number of events which may occur : it vanish at the corners of the triangle , is equal to @xmath87 at its sides and equals to @xmath88 for any point inside the triangle .",
    "the other example , @xmath89 , presented in fig .",
    "[ fig : ren1]d . is similar to the limiting case @xmath72 , for which the iso - entropy curves are perpendicular to the lines joining @xmath82 with the corners .",
    "[ htbp ]    -0.2 cm .",
    "the generalized rnyi entropy is constant along the curves plotted for ( a ) @xmath85 , ( b ) @xmath70 ( shannon entropy ) , ( c ) @xmath38 ( euclidean circles  distance @xmath90 ) , and ( d ) @xmath89 .",
    "dotted lines form the triangle @xmath91 .",
    ", title=\"fig:\",width=359 ]    superimposing some of the above pictures on one graph allows one to understand further relations between the rnyi entropies .",
    "the generalized entropies are correlated ; e.g. for the distributions @xmath92 the entropies are equal to @xmath34 independently of the value of @xmath5 .    the problem ,",
    "which values the entropy @xmath37 may admit , provided @xmath93 is given , has been solved by harremos and topse @xcite . for any distribution @xmath94 they proved a simple ( but not very sharp ) upper bound on @xmath11 by @xmath12 , @xmath95 where the lower bound is a special case of ( [ renmonot ] ) .",
    "moreover , they showed that the set @xmath96 of possible probability distributions plotted in the plane @xmath37 versus @xmath93 is not convex ( see fig .",
    "2 ) , and its boundaries are formed of arcs corresponding to the interpolating probability distributions @xmath97 . \\label{reninterp}\\ ] ] more precisely , for any probability distribution @xmath27 consisting of @xmath0 components and arbitrary @xmath98 the following bounds hold @xcite @xmath99 where @xmath100 is a function of the known value of @xmath101 and the natural number @xmath32 is selected by the inequality @xmath102 .",
    "the above results , crucial for the main body of this work , are easy to understand .",
    "let us discuss the simplest nontrivial case with @xmath76 .",
    "the two dimensional simplex of probability distributions may be divided into @xmath103 identical parts , equivalent to the triangle @xmath91 , as shown in fig .",
    "three sides of the triangle are formed of the interpolating distributions @xmath104 , @xmath105 and @xmath106 and these distinguished probabability distributions are extreme in a sense that they lead to the bounds ( [ renibound ] ) .",
    "the bounds between @xmath11 and @xmath12 for @xmath76 are presented in fig .",
    "to obtain them it is sufficient to travel along the sides of the triangle @xmath91 , computing @xmath11 and @xmath12 at each point and to plot the data obtained in the plane @xmath11 versus @xmath12 .",
    "more formally , the upper boundary of the set @xmath96 consist of one arc derived from the family of distributions @xmath107 ; for any value of @xmath100 we compute @xmath108 , invert it to obtain @xmath109 and plot @xmath110 . in the case",
    "@xmath76 the upper bounds plotted in fig .",
    "2a , c and e arise from the hypotenuse @xmath111 of the triangle @xmath91 from fig .",
    "[ fig : ren1].b and c.    in the similar way the lower bound may be derived from the distributions @xmath112 for @xmath113 .",
    "it consists of @xmath114 arcs forming an cascade @xmath115 .",
    "note that the distributions @xmath92 are represented in each plot by the points @xmath116 , which connect the neighbouring arcs . for @xmath76",
    "the lower bound consists of two arcs , corresponding to the adjacent sides @xmath117 and @xmath118 of @xmath91 .",
    "the shape of the set @xmath96 requires a comment .",
    "the @xmath119 dimensional simplex  the set of all @xmath0points probability distributions is convex and any of its projections onto a plane forms a convex set .",
    "however , its image at the plane @xmath37 versus @xmath93 needs not to be convex , since the transformations @xmath120 and @xmath121 are nonlinear .",
    "the boundaries of @xmath96 are obtained as the image of an appropriately chosen path on the boundary of the simplex . in the case",
    "considered it is the path @xmath122 , independently of the values of @xmath5 and @xmath123 .",
    "observe that the general structure of the set @xmath96 does not depend on @xmath123 .",
    "however , the larger difference @xmath124 , the larger area of the set : the less information on @xmath28 is provided by @xmath125 .    [ htbp ]    -0.2 cm   and @xmath126 at the rnyi entropies plane @xmath11 and @xmath37 : @xmath38 ( a , b ) ; @xmath127 ( c , d ) and @xmath128 ( e , f ) .",
    "thin dotted lines in each panel represent the monotonicity lower bounds ( [ renmonot ] ) while bold dotted curves in panel ( a ) and ( b ) denote upper bound ( [ ps02inf ] ) .",
    ", title=\"fig:\",width=359 ]    let us emphasize in this point that results presented in @xcite do not close the issue of finding bounds and relations between different entropies .",
    "results analogous to ( [ renibound ] ) for a more general class of entropy functions were recently obtained by berry and sanders @xcite . a more precise lower bound for shannon entropy , quadaratic in terms of index of coincidence ( purity )",
    "was found by topse @xcite .",
    "results ( [ renibound ] ) allow us to obtain bounds for a value of the entropy @xmath37 , provided the value @xmath93 is known .",
    "let us first assume that the entropy @xmath12 is known and we want to extract some information on @xmath11 .",
    "we start computing the rnyi entropy of order two , @xmath129 \\",
    "\\label{s2aaa}\\ ] ] and invert it to obtain @xmath130    in this way we receive sharp upper bounds for @xmath131 @xmath132 , \\label{interaa2}\\ ] ] which for @xmath35 reduces to @xmath133 with @xmath100 given by ( [ aaa12 ] ) .    to obtain analogous lower bound we find @xmath32 such that @xmath134 and compute @xmath135 .",
    "also this relation may be easily inverted providing @xmath136 .",
    "thus we arrive at a lower bound for the rnyi entropy @xmath137 \\ , \\label{inter12d}\\ ] ] and in particular case , for the shannon entropy @xmath138 with @xmath139 and @xmath140 .",
    "let us now assume , we know the value of the rnyi entropy @xmath13 .",
    "as in ( [ s2aaa ] ) we compute @xmath141 , and invert it finding @xmath142 $ ] as the largest ( real ) root of the polynomial @xmath143 then the upper bound valid for @xmath144 is given by the same formula ( [ interaa2 ] ) with @xmath100 given by the root of ( [ interaa3 ] ) instead of ( [ aaa12 ] ) .",
    "for @xmath145 one obtains then the upper bound for the shannon entropy @xmath146 with @xmath100 determined by ( [ interaa3 ] ) .    to get the lower",
    "bound we look for @xmath147 such that @xmath148 .",
    "the relation @xmath149 may be inverted explicitly for @xmath150 providing @xmath151/3}}$ ] . for @xmath152 @xmath100",
    "is given by the only root of the polynomial @xmath153 = 0 .",
    "\\label{interaa4}\\ ] ] in the interval @xmath154 $ ] .",
    "the lower bound for the rnyi entropies has the same form as ( [ inter12d ] ) and gives for the shannon entropy @xmath155 with @xmath156 and @xmath157 .",
    "in previous section we obtained two upper bounds for the shannon entropy : ( [ inter12d ] ) stemming from the rnyi entropy @xmath12 , and ( [ ren23 ] ) obtained from @xmath13 .",
    "the latter is in general a worse one at @xmath127 we have less information on @xmath11 , than knowing it at @xmath38 . ] , but it allows for a linear extrapolation , which gives @xmath158 our numerical results allow us to advance the following    * conjecture . *",
    "_ for any probability distribution @xmath62 the bound @xmath159 holds .",
    "_    in the same way one may try to extrapolate lower bounds defining @xmath160 . for certain probability vectors this quantity may give a useful approximation for the shannon entropy .",
    "interestingly , a relation analogous to ( [ conjec ] ) , @xmath161 is not true : it is violated e.g. if @xmath162 .    making use of the rigorous bound ( [ ren23 ] ) and the conjecture ( [ conjec ] ) we may suggest to estimate the unknown value of the shannon entropy by the mean value @xmath163 = h^u_{12}({\\vec x})+h_{2}(\\vec{x})-   \\frac{1}{2 } [ h^u_{13}({\\vec x})+h_{3}(\\vec{x } ) ] , \\label{estim1}\\ ] ] which is obtained by combined methods based as well on the lower as well as on the upper bounds . observe that this explicitly computable quantity involves only rnyi entropies @xmath12 and @xmath13 and the dimension @xmath0 .",
    "this estimation may be improved noting that the rigorous lower bounds ( [ ren23 ] ) and ( [ shann12d ] ) are not equivalent . since for some distributions the latter bound gives better ( higher ) results , we may improve ( [ estim1 ] ) writing @xmath164 if the value of the zero - entropy , @xmath86 , is known , one may replace the upper bound @xmath165 used above by the minimum , @xmath166 .",
    "[ htbp ]    -0.2 cm   for a random probability distribution of size @xmath167 ( solid line ) .",
    "dense dotted curves represent @xmath12 bounds : upper ( [ interaa2 ] ) and lower ( [ inter12d ] ) , while faint dotted curve denote analogous bounds based on @xmath13 .",
    "straight dashed  dotted lines show lower ( [ ren23 ] ) and upper ( [ up23 ] ) extrapolations . the exact value of @xmath11 is denoted by @xmath168 , while the estimation ( [ estim2 ] ) by the middle @xmath169 . , title=\"fig:\",width=359 ]    figure 3 shows the bounds and the estimations described above for a randomly chosen probability distributions with @xmath167 components .",
    "the overall quality of the proposed estimations for the shannon entropy may be judged from fig .",
    "4 , which shows the histogram of the deviations @xmath170 and @xmath171 for a sample of @xmath172 probability vectors generated randomly according to the statistical ( fisher  rao ) measure on the @xmath119 dimensional simplex @xcite .",
    "this measure has a simple geometric interpretation : is suffices to consider a unit vector @xmath173 distributed uniformly on the sphere @xmath174 , and to define the probability vector @xmath175 .",
    "numerical results obtained in this way allow us to conclude that the proposed estimation ( [ estim2 ] ) provides a useful all - purpose approximation of the shannon entropy",
    ". note that the precision of this approximation decreases with the length @xmath0 of the probability vector .    to judge about possible application of the estimate @xmath176 in the analysis of physical data",
    ", one should perform analogous numerical simulations with random vectors @xmath62 generated according to a specific probability distribution adjusted to a given physical problem .",
    "[ htbp ]    -0.2 cm : @xmath168 denotes the probability density of error @xmath170 of the estimation ( [ estim2 ] ) , while @xmath177 denotes results for the error @xmath171 of the lower bound ( [ ren23 ] ) for a sample of @xmath172 random probability vectors of size @xmath178 . , title=\"fig:\",width=359 ]",
    "in this work we considered the problem of finding the bounds and extrapolations for the shannon ( entropy ) , provided some of the renyi entropies are known . in general , generalized entropies of integer order @xmath38 and @xmath127 are easiest to calculate , and they are sufficient to obtain bounds ( [ ren02 ] ) and ( [ ren23 ] ) for the shannon entropy .",
    "the quality of the bound may be improved if the length @xmath0 of the probability vector is known",
    ". then an explicit extrapolation ( [ estim2 ] ) allows us to estimate the actual value of the entropy @xmath10 .",
    "note that the bounds and extrapolations discussed may be easily rewritten in terms of a non - extensive entropy @xmath179\\ \\label{phcalpha}\\ ] ] used by havrda and charvat @xcite and daroczy @xcite , which became often used in statistical physics after the seminal work of tsallis @xcite .",
    "in particular , the linear entropy @xmath180 is just the nonextensive entropy of order two , @xmath181 , and the bounds between @xmath28 and @xmath11 imply analogous relations between @xmath182 and @xmath11 .",
    "in fact the plot presented in @xcite shows bounds between von neumann entropy and the linear entropy and they follow directly from relation ( [ renibound ] ) proven in @xcite .",
    "the issue of comparing the rnyi entropies @xmath86 , @xmath11 and @xmath12 is closely related to the problem of describing the degree of chaos of an analyzed classical dynamical system by the topological entropy @xmath183 , the kolmogorov ",
    "sinai ( metric ) entropy @xmath184 and the correlation entropy @xmath185 .",
    "these dynamical entropies are defined as the _ rate _ of the increase of the rnyi entropies in time @xcite , but since the length of the probability vector is not finite the @xmath0dependent bounds discussed in this work are not applicable .",
    "the same concerns comparison of generalized fractal dimensions @xmath186 of fractal measures : the box ",
    "counting dimension @xmath187 , the information dimension @xmath188 and the correlation dimension @xmath90 , which also form a non  increasing function of the rnyi parameter @xmath5 @xcite , are defined by the limit @xmath189 .",
    "the generalized entropies may also be used to characterize localization properties of continuous probability distributions .",
    "for instance , any pure quantum state may be represented in the phase space by the husimi distribution .",
    "its localization can be measured by the _ wehrl entropy _ defined as the continuous ( boltzmann  gibbs ) entropy of the husimi distribution @xcite . in an analogous way",
    "one may define generalized rnyi ",
    "wehrl entropies @xcite , and above results may be used to obtain similar bounds for the wehrl entropy .",
    "it is a pleasure to thank p. harremos and f. topse for explaining us their results prior to publication and several fruitful comments .",
    "i am also thankful to i. bengtsson , a.biaas , a. ostruszka , f. mintert , w. munro and w. somczyski for inspiring discussions and d. berry , b. sanders and i. varga for helpful correspondence",
    ". financial support by komitet bada naukowych in warsaw under the grant 2p03b-072  19 and by a research grant of the volkswagen stiftung is gratefully acknowledged .",
    "( [ diff4 ] ) implies that @xmath190 is a convex function of @xmath5 .",
    "however , it does not imply that the rnyi entropy @xmath28 is a convex function of @xmath5 .",
    "for instance , the rnyi entropy for a @xmath191 probability vector @xmath192 is not a convex function of @xmath5 .",
    "i am deeply obliged to christian schaffner for drawing my attention to this fact .",
    "therefore , equations ( [ ren02 ] ) , ( [ ren002 ] ) , and ( [ ren23 ] ) are not satisfied and conjecture ( [ conjec ] ) can not hold .",
    "furthermore , equations ( [ ren023 ] ) , ( [ estim1 ] ) , ( [ estim2 ] ) may only be used to extrapolate the unknown value of the shannon entropy , from the available data on @xmath12 and @xmath13 . on the other hand",
    ", we would like to mention that the lack of convexity of @xmath28 in @xmath5 does not influence the bounds between particular values of the rnyi entropy presented in sections 3 and 4 of the present paper .",
    "i. varga and j. pipek , on rnyi entropies characterizing the shape and the extension of the phase space representation of quantum wave functions in disordered systems , arxiv preprint cond - mat/0204041 ( 2002 ) ."
  ],
  "abstract_text": [
    "<S> relations between shannon entropy and rnyi entropies of integer order are discussed . for any @xmath0point discrete probability distribution for which the rnyi entropies of order two and three are known , </S>",
    "<S> we provide an lower and an upper bound for the shannon entropy . </S>",
    "<S> the average of both bounds provide an explicit extrapolation for this quantity . </S>",
    "<S> these results imply relations between the von neumann entropy of a mixed quantum state , its linear entropy and traces .    </S>",
    "<S> ver . </S>",
    "<S> 2 with corrigendum added ,  february 17 , 2005 </S>"
  ]
}