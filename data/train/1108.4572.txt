{
  "article_text": [
    "in designing wearable products , such as garments or head gears , one of the most important objectives is to make the products to fit the humans comfortably . in order to accommodate the human shape variabilities ,",
    "different sizes are usually created .",
    "traditionally , this is achieved by considering a few key dimensions of the human body . to design a sizing system ,",
    "anthropometric measurement data of these dimensions are collected and tabulated .",
    "then , a griding of the dimensions is formed to create the sizes  @xcite  @xcite .",
    "although traditional anthropometry has a long history and has accumulated a vast amount of data , it is limited by its tools ( mainly tape measure and caliper ) ; the sparse dimensional measurements do not provide sufficient shape information . in many applications , designers need full 3d models as representative shapes for the sizes  @xcite . these models are sometimes called manikins when the full body is concerned , or head forms in case of the head and face are concerned . for the purpose of generality , we call them _ design models _ in this paper .",
    "these 3d models provide the overall shape of the human body .",
    "they need to be chosen carefully to ensure that the manufactured items fit the target population .    without 3d information ,",
    "design models are often created by artists who sculpt out the 3d forms , interpolating the measurement dimensions using their experience in creating human shapes  @xcite .",
    "these approaches are labour - intensive and do not create accurate human models .    3d anthropometric data , obtained using 3d imaging technologies , provide detailed shape information .",
    "in addition , traditional measurements can also be extracted from the 3d models .",
    "therefore , 3d anthropometric data offers an opportunity to improve the quality of the design models and , at the same time , maintain the simplicity of the traditional design schemes where key body dimensions are used .",
    "despite the fact that 3d anthropometric databases have been available for two decades , surprisingly little research has been reported concerning using the 3d data to improve the design models .",
    "robinette  @xcite , ball  ( * ? ? ?",
    "* chapter 7 ) , and meunier et al .",
    "@xcite are recent attempts in this direction .",
    "however , all of these methods rely heavily on manual interaction and do not give a systematic way to optimize the design models such that the sizing system provide maximal accommodation .    in this paper",
    ", we present a method for automatically creating a set of design models based on a 3d anthropometric database and optimally computing the design models that represent the underlying target population .",
    "starting with a 3d anthropometric database @xmath0 that represents the target population for the product to be designed , the method requires as input the ordered set of @xmath1 measurements @xmath2 on the human body that need to be considered during the design phase .",
    "for instance , when designing glasses , the set @xmath2 to be considered are the width of the face and the width of the bridge of the nose .",
    "furthermore , the method requires as input an ordered set of @xmath1 tolerances @xmath3 ; one tolerance for each measurement in @xmath2 . in the example with the glasses",
    ", we may know that the glasses to be designed can be adjusted by @xmath4 to fit faces with different widths and by @xmath5 to fit noses with different widths of the bridge . in this case",
    ", @xmath3 contains the tolerances @xmath6 and @xmath7 .",
    "if the product is non - adjustable , the tolerances represent the measurement intervals that the sizes cover . with this information ,",
    "the method finds a set of design models that optimally represent the population acquired in the database for the specific purpose of designing a product that depends on the measurements @xmath2 and that allows adjustments along the tolerances @xmath3 .",
    "the method proceeds by mapping each of the subjects in the database to a point in @xmath8 and by solving a discrete covering problem in this space .",
    "we use the subjects in the database directly instead of learning their underlying probability distribution since the type of distribution is usually unknown .",
    "the design models created using this approach are expected to have higher data accuracy and completeness than traditionally produced design models because we compute the best design models from a database that represents the target population .",
    "sections  [ conversion ] ,  [ cover_box ] , and  [ fixed_box ] discuss how to convert the problem to a discrete covering problem and how to solve the covering problem . to obtain the design models",
    ", the method employs the full procrustes mean  @xcite as outlined in section  [ gpa ] .",
    "the design models represent the given database .",
    "if the database is large , we expect good design models .",
    "if the database contains few subjects , it is possible to assume and fit a probability distribution satisfied by the population and to extrapolate subjects from the database based on that distribution . in case of a small database , this approach may provide more accurate results .",
    "section  [ extrapolate ] discusses this option .",
    "section  [ experiments ] gives design examples .",
    "some prior work has focused on operating in a parameter space based on traditional sparse anthropometric measurements . if only sparse anthropometric measurements are known , it is not straight forward to find 3d design models because computing a 3d model based on a small set of measurements is an under constrained problem . in this work ,",
    "the shape information is provided by a 3d anthropometric database .",
    "this data allows to compute design models by constraining them to lie within the shape space spanned by the database .    in this work ,",
    "we use the civilian american and european surface anthropometry resource ( caesar ) database  @xcite to compute a sizing system .",
    "this database contains human shapes in a similar posture .",
    "furthermore , each shape contains a set of 73 anthropometric landmarks . we exploit this information to parameterize the models",
    "mcculloch et al .",
    "@xcite used the traditional sparse anthropometric measurements to automatically create an apparel sizing system that has good fit . for a fixed set of measurements on the body , a fixed number of sizes , and a fixed percentage of the population that needs to be fitted by the sizing system , they optimize the fit of the sizing system",
    "the fit for a human body is defined by a weighted distance function between the measurements on the body and the measurements used for the sizing system . optimizing the fit amounts to solving a non - linear optimization system .",
    "this system is hard to solve and suboptimal solutions may be obtained .",
    "this approach can operate in multi - dimensional spaces .",
    "it is not straight forward to obtain design models from the resulting sizing system because computing a design model from a sparse set of measurements is an under constrained problem .",
    "mochimaru and kouchi  @xcite represented each model in a database of human shapes using a set of manually placed landmark positions .",
    "they proposed an approach to find representative three - dimensional body shapes .",
    "the approach first reduces the dimensionality of the data using multi - dimensional scaling , and then uses principal component analysis ( pca ) to find representative shapes .",
    "mochimaru and kouchi showed that this approach is suitable to find representative shapes of a human foot .",
    "while this approach is fully automatic , it assumes that the distortion introduced by multi - dimensional scaling is small .",
    "while this may be true for low - dimensional data , it is not in general true when three - dimensional measurements of high resolution are considered .",
    "hence , there is no guarantee that the design models optimally represent the population .",
    "recently , three - dimensional measurements of high resolution have been used to aid in the design process .",
    "robinette  @xcite studied different ways to align a database of 3d laser scans of heads for helmet design .",
    "the goal is to find alignments with minimum variability of shape .",
    "this way , one can design helmets that offer maximum protection .",
    "meunier et al .",
    "@xcite used a database of 3d laser scans of heads to objectively assess the fit of a given helmet .",
    "this assessment strategy is useful to analyze existing designs , but it does not produce design models .",
    "kouchi and mochimaru  @xcite proposed an approach to design spectacle frames based on database of human face shapes .",
    "the approach proceeds by analyzing the morphological similarities of the faces in the database and by dividing the faces into four groups . for each group",
    ", a representative form is found automatically . in this work ,",
    "a spectacle frame was designed for each representative shape , and it was shown that a good fit was achieved .",
    "guo et al .",
    "@xcite proposed a similar approach to construct design models for helmet design using a database of magnetic resonance images of heads .",
    "the approach proceeds by automatically dividing the head shapes into groups and by constructing one design model per group .",
    "although the methods by kouchi and mochimaru and guo et al . require little manual work , the methods are specific to spectacle and helmet design , respectively , and can not easily be extended to the design of other gear or garments .",
    "furthermore , the division of heads into groups is not guaranteed to produce design models that optimally represent a target population .",
    "meunier et al .",
    "@xcite propose to parameterize a set of head scans and to perform pca on the parameterized scans .",
    "they then use the first two principal components to manually design a set of three design models . while this approach aims to exploit the information provided by a parameterized database of head shapes ,",
    "the approach is heuristic and assumes that the data follows a gaussian distribution .",
    "furthermore , manually picking the design models based on a learned distribution is hard in a high - dimensional space without reducing the dimensionality of the data because humans can not easily visualize high - dimensional spaces .",
    "hence , if the aim is to consider four or more principal components , the approach by meunier et al . becomes very difficult to use .",
    "we propose a fully - automatic method to compute design models that represent a given 3d anthropometric database well .",
    "since the approach computes the design models automatically , it can operate in high - dimensional spaces . unlike the method by mcculloch et al .",
    "@xcite , our method does not rely on solving a non - linear optimization system . instead , we model the fit explicitly using a set of tolerances ( one tolerance along each dimension ) that explains by how much the garment or gear can be adjusted along each dimension . these tolerances depend directly on the design and the materials that are used in a specific application . in this way",
    ", our method finds optimal design models for a specific task such as helmet design . to our knowledge , this is the first method that simultaneously considers the optimal fit accommodation and design models .",
    "the proposed approach proceeds by first parameterizing all of the @xmath9 subjects present in the database @xmath0 . in general",
    ", finding accurate point - to - point correspondences between a set of shapes is a hard problem  @xcite .",
    "however , in our application , we know that the shapes are human body shapes , which allows the use of template - based approaches to parameterize the database .",
    "first , consider the case where the scans are assumed to be in similar posture .",
    "this assumption is common in our application because in a typical 3d anthropometry survey , the human subjects are asked to maintain a standard posture . in this case",
    ", we can proceed by first using ben azouz et al.s  @xcite approach to automatically predict landmarks on the scans followed by xi et al.s approach  @xcite to parameterize the models .",
    "ben azouz et al.s approach proceeds by learning the locations of a set of anthropometric landmarks from a database of human models using a markov random field ( mrf ) , and by using probabilistic inference on the learned mrf to automatically predict the landmarks on a newly available scan .",
    "xi et al.s approach  @xcite exploits the anthropometric landmarks to fit a template model to the scans .",
    "the method proceeds in two steps .",
    "first , it computes a radial basis function that maps the anthropometric landmarks on the template mesh to the corresponding landmarks on the scan .",
    "this function is used to deform the template mesh .",
    "second , the method further deforms the template to fit the scan using a fine fitting approach as in allen et al .",
    "@xcite .",
    "second , consider the general case where the postures of the scans vary . in this case , we can again proceed by first predicting landmarks on the scans automatically and by using a template - based deformation to parameterize the models . when using this method  @xcite , the locations of the landmarks are learned and predicted in an isometry - invariant space and the template fitting",
    "approach uses a skeleton - based deformation to allow for posture variation .    in this work ,",
    "we use the caesar database .",
    "the models of this database are in similar posture and for each model , we know a set of 73 manually placed anthropometric landmarks .",
    "hence , we use the anthropometric landmarks as input to xi et al.s approach  @xcite to parameterize the models .",
    "once the database is parameterized , each subject @xmath10 of the database is represented by a triangular mesh .",
    "we can now measure corresponding distances on all of the bodies .",
    "this paper considers euclidean distances .",
    "however , the techniques outlined in the following extend to arbitrary distances , such as geodesics .",
    "the approach computes the ordered set @xmath2 of @xmath1 distances that are meaningful for the design of a specific garment or gear for each of the @xmath9 bodies in the database . for each body ,",
    "the @xmath1 measurements in order can be viewed as a point @xmath11 in the _ parameter space _ @xmath12 .",
    "the entire database is then represented as a set @xmath13 of @xmath9 points @xmath14 in @xmath15 .",
    "the designer can specify a range of fit along each of the @xmath1 dimensions that were measured on the bodies .",
    "this range specifies how much a garment stretches or by how much gear can be adjusted in the given direction .",
    "the ordered set @xmath3 of @xmath1 ranges in order defines the side lengths of a @xmath1-dimensional box in @xmath15 .",
    "let @xmath16 denote the side length in dimension @xmath17 and let @xmath18 denote the @xmath1-dimensional box with side lengths @xmath16 centered at the origin .",
    "the problem of computing a sizing system that fits the given population can now be expressed as a covering problem in @xmath15 .",
    "that is , we aim to cover all of the points in @xmath13 with translated copies of @xmath18 .",
    "sections  [ cover_box ] and  [ fixed_box ] discuss how to solve this problem .",
    "once the points in @xmath13 are covered by a set of translated copies of @xmath18 , we aim to convert each box @xmath19 back into a body shape that represents the points covered by @xmath19 .",
    "this is achieved by computing the full procrustes mean shape  @xcite of the body shapes corresponding to the points covered by @xmath19 .",
    "section  [ gpa ] discusses this step in detail .",
    "this section discusses the problem of finding the minimum number of translated copies of @xmath18 that cover all of the points in @xmath13 .",
    "we first analyze how many translated copies of @xmath18 need to be considered by the algorithm .",
    "when considering only one distance dimension , the box @xmath18 becomes a line segment of fixed length .",
    "note that two line segments that are combinatorially equivalent ( i.e. they cover the same points of @xmath13 ) do not add anything to the space of solutions .",
    "therefore , the algorithm only needs to consider translated line segments that are combinatorially different from each other .",
    "since the set covered by a translated copy of @xmath18 only changes if a point of @xmath11 either enters or leaves the set , the algorithm only needs to consider the translated line segments with endpoints @xmath11 . since a line segment has two endpoints and since there are @xmath9 points in @xmath13 , we need to consider @xmath20 boxes . extending this argument to @xmath1 distance dimensions",
    "yields that at most @xmath21 translated copies of @xmath18 need to be considered .",
    "the problem of finding the minimum number of boxes among the set of at most @xmath21 translated copies of @xmath18 that cover all of the points in @xmath13 can now be viewed as a set cover problem .",
    "a history of this problem can be found in vazirani  ( * ? ? ?",
    "* chapter 2 ) .",
    "this problem is np - hard  @xcite and it is therefore impractical to find an optimal solution .",
    "hence , we aim to find an approximation to the solution that has bounded error .",
    "a solution is a @xmath22-approximation of the optimal solution if it uses @xmath23 boxes with @xmath24 for any @xmath25 , where @xmath26 is the number of boxes used by the optimal solution . in the following ,",
    "we discuss how to compute a @xmath22-approximation  @xcite .",
    "we then discuss a more efficient greedy algorithm to find a set of boxes that is a @xmath27-approximation of the optimal solution .",
    "this section summarizes the approach by hochbaum and maass  @xcite to find a @xmath22-approximation to the covering problem .",
    "the approach proceeds by dividing the parameter space into a regular grid with width @xmath16 along each dimension . for a given input parameter @xmath28 related to @xmath29 @xmath30 , the approach partitions the parameter space into slabs of @xmath28 grid cells along each dimension .",
    "note that @xmath31 partitions are possible because the locations of the slabs can be shifted @xmath28 times along each dimension .",
    "for each of the partitions , the algorithm computes the union of optimal solutions in the sets of @xmath32 grid cells created by the partition .",
    "the algorithm takes @xmath33 time . for a detailed analysis ,",
    "refer to  @xcite .",
    "this is only practical for problems with few points in low dimensions .",
    "hence , we only implemented this approach for @xmath34 .",
    "this section summarizes an efficient algorithm to find a solution to the covering problem .",
    "unfortunately , the solution is not guaranteed to be a @xmath22-approximation of the optimal solution .",
    "the approach proceeds by considering @xmath35 translated copies of @xmath18 .",
    "we denote these boxes @xmath36 . for each box @xmath19 ,",
    "we compute the set of points of @xmath13 covered by @xmath19 .",
    "this yields a collection of @xmath35 sets .",
    "our goal is to find a small subset of the sets that covers all of the points in @xmath13 .",
    "we solve the problem using a greedy approach by repeatedly selecting the set covering the maximum number of uncovered points until all points are covered .",
    "finding the set of points covered by one box @xmath19 takes @xmath37 time .",
    "hence , finding all of the sets takes @xmath38 time . we store the following information . for each point @xmath11 ,",
    "we store which boxes contain @xmath11 , for each box @xmath19 , we store which points are contained in @xmath19 , and we store for each box @xmath19 the number of uncovered points in @xmath19 .",
    "the set that covers the maximum number of uncovered points can now be found in @xmath39 time .",
    "furthermore , using the stored information , we can remove one point from all of the boxes @xmath19 in @xmath39 time .",
    "since there are a total of @xmath9 points , the algorithm takes @xmath40 time .",
    "hence , this approach takes @xmath38 time .    as discussed above ,",
    "if we consider all of the combinatorially different boxes @xmath19 , @xmath35 is at most @xmath21 .",
    "when all combinatorially different boxes are considered , it can be proven that this greedy approach is a @xmath41-approximation of the optimal solution  ( * ? ? ?",
    "* chapter 2 ) .",
    "hence , we can find a @xmath41-approximation of the optimal solution in @xmath42 time .",
    "denote the set of all combinatorially different boxes by @xmath43 . in practice , we reduce the number of boxes further by considering only the translated copies of @xmath18 with centers in @xmath13 . denote the set of these boxes by @xmath44 .",
    "this way , we obtain @xmath45 and reduce the running time to @xmath46 at the cost of not considering all of the combinatorially different boxes . to analyze the approximation ratio of this solution , consider the case @xmath47 , where the boxes become line segments .",
    "assume that the optimal solution picks a line segment @xmath48 not included in @xmath44 .",
    "recall that @xmath48 is in the set @xmath43 .",
    "all of the elements covered by @xmath48 can be covered by two line segments in @xmath44 ; namely , the line segment centered at the leftmost point in @xmath48 and the line segment centered at the rightmost point in @xmath48 .",
    "this argument can be generalized to @xmath1-dimensional space as follows .",
    "a box @xmath48 picked by the optimal solution that is not included in @xmath44 can be covered by @xmath49 boxes in @xmath44 .",
    "hence , the optimal solution using @xmath44 needs at most @xmath49 times the number of boxes picked by the optimal solution using @xmath43 . since we solve the problem using the greedy algorithm on @xmath44 and since the greedy algorithm is known to compute a @xmath41-approximation of the optimal solution  ( * ? ? ?",
    "* chapter 2 ) , our approach is guaranteed to compute a @xmath27-approximation of the optimal solution .",
    "in some applications such as the design of garments , it is not desirable to produce sizes for an entire population . instead , one wishes to produce a fixed number @xmath26 of sizes that fits the largest portion of the population .",
    "for instance , a company that manufactures t - shirts may wish to manufacture three sizes ( small , medium , and large ) in a way that the sizes fit the largest possible portion of the population .    in parameter space",
    ", this means that we do not wish to cover all of the points in @xmath13 .",
    "instead , the goal is to cover the maximum number of points in @xmath13 with a fixed number @xmath26 of boxes .",
    "this problem is also np - hard since a polynomial - time algorithm to solve this problem would give a polynomial - time algorithm to solve problem considered in section  [ cover_box ] .",
    "hence , we solve this problem using a greedy approach .",
    "unfortunately , the solution is not guaranteed to be a @xmath22-approximation of the optimal solution . however , by a similar argument to the one in the previous section , we can show that the approach computes a @xmath27-approximation of the optimal solution .",
    "the greedy approach proceeds as in section  [ greedy_cover ] .",
    "the only difference is that we stop after the first @xmath26 boxes are selected . using an analysis similar to the one in section  [ greedy_cover ] ,",
    "it can be shown that this algorithm takes @xmath38 time , where @xmath35 is the number of boxes considered by the algorithm .",
    "recall that @xmath50 in theory and that we set @xmath45 in practice .",
    "once the algorithm selected a set of @xmath26 boxes @xmath51 to represent the given measurements , we aim to convert each box @xmath19 back into a body shape that represents the points covered by @xmath19 .",
    "this is achieved by finding the parameterized body shapes corresponding to points in @xmath13 covered by @xmath19 and by computing the full procrustes mean of these shapes  @xcite . to compute the full procrustes mean of @xmath52 shapes",
    ", we repeatedly compute the average of the @xmath52 shapes and align each of the @xmath52 shapes to the average shape using a rigid transformation .",
    "if the box @xmath19 contains a sufficient number of points and if the mean of these points is close to the center of @xmath19 , the procrustes mean is a good representative of @xmath19 .",
    "otherwise , the procrustes mean of these shapes will not yield a good representation of the shapes covered by @xmath19 .",
    "if this situation occurs , the approach can be modified by sampling a set of points @xmath53 in @xmath19 and by finding the shapes @xmath54 corresponding to these points as outlined in section  [ extrapolate ] .",
    "we can then find a design model by computing the full procrustes mean of the shapes @xmath54 .",
    "this approach yields a set of @xmath26 body shapes @xmath55 corresponding to the boxes @xmath19 . the shapes @xmath55 can now be used for the design of garments or gear .",
    "note that the shapes @xmath55 are not simply scaled versions of each other since each shape @xmath55 is derived from a different set of true body scans .",
    "we use each shape @xmath55 as a design model .",
    "the design models computed in the previous sections represent the given database .",
    "if the database contains few subjects , the coverage of the target population by the computed design models may be small . in this case , extrapolating subjects from the database may provide more accurate results . in order to extrapolate subjects from the database , we need to assume that the target population obeys a specific probability density . in this section ,",
    "we assume that the data follows a gaussian distribution .",
    "we discuss how to extrapolate subjects from the database to improve the coverage of our method .",
    "we use the approach by allen et al .",
    "@xcite to compute a new shape @xmath54 based on a new point @xmath53 in @xmath15 .",
    "since the database is parameterized and follows a gaussian distribution , we can perform pca of the data . in pca space ,",
    "each shape @xmath10 is represented by a vector @xmath56 of pca weights .",
    "pca yields a mean shape @xmath57 and a matrix @xmath58 that can be used to compute a new shape @xmath54 based on a new vector of pca weights @xmath59 as @xmath60 . recall that we aim to create a new shape based on a new point @xmath53 in @xmath15 . to achieve this goal",
    ", we use the database to learn a linear mapping from @xmath11 to @xmath61 .",
    "this mapping is called _ feature analysis _ and described in detail by allen et al .",
    "feature analysis yields a matrix @xmath18 that can be used to compute a new vector of pca weights @xmath59 based on a new point @xmath53 as @xmath62 .",
    "this model allows to compute a shape @xmath54 based on a new point @xmath53 as @xmath63 .",
    "it remains to outline how to find new points @xmath53 .",
    "we assume that the points @xmath64 can be modeled by a gaussian distribution .",
    "we learn the distribution from the given database using maximum likelihood estimation .",
    "we then sample a set of points from this distribution and find the corresponding shapes using feature analysis .",
    "we use these new subjects along with the given database to find the design models .",
    "another way to find new points @xmath53 is to sample the surface of a fixed equal probability density of the learned gaussian distribution ( this surface is an ellipsoid ) .",
    "this approach is useful to increase boundary coverage .",
    "we use as database 50 faces containing 6957 triangles each from the caesar database  @xcite .",
    "figure  [ extra_faces ] shows three faces that were obtained using feature analysis .",
    "we demonstrate the proposed approach for the design of glasses and for the design of helmets .",
    "the first example shows the concept of the presented approaches using a small database of faces .",
    "the second example gives an evaluation of the greedy approaches using a large database of heads .",
    "when designing glasses , one may wish to measure the width of the face and the width of the bridge of the nose .",
    "using these two measurements yields @xmath65 .",
    "figure  [ measurements ] shows the measurements on one face in the database .",
    "the first dimension measures the euclidean distance between the blue points and the second dimension measures the euclidean distance between the red points .",
    "for this example , we use 50 faces from the caesar database  @xcite .",
    "we choose a small database since this allows to illustrate the result of the @xmath22-approximation .",
    "in our example , we aim to design glasses that can be adjusted by @xmath66 in the first dimension and by @xmath5 in the second dimension .",
    "this defines the box @xmath18 .",
    "figure  [ approx ] shows the result when covering @xmath13 using a @xmath22-approximation with @xmath67 .",
    "the figure shows the points @xmath13 as grey triangles and the centers of the boxes @xmath19 as black squares .",
    "furthermore , for each box , the figure shows a screen shot of the corresponding procrustes mean shape .",
    "these face shapes can be used to create the sizes for the glasses .",
    "figure  [ greedy ] shows the result of a greedy covering of @xmath13 .",
    "recall that this is a @xmath68-approximation of the optimal solution .",
    "the symbols used in the figure are identical to the ones in figure  [ approx ] .",
    "figure  [ greedy](a ) shows the result when we aim to cover the entire population .",
    "we can see that in this example , we only require one extra shape when covering with the greedy algorithm than when covering using a @xmath22-approximation with @xmath67 .",
    "figure  [ greedy](b ) shows the result of greedily covering a large subset of @xmath13 with three boxes .",
    "we can see that the selected boxes cover the parameter space well .    [",
    "cols=\"^,^ \" , ]      when designing a helmet , the three most crucial measurements are the head width , the head depth , and the face hight  @xcite . using these three measurements yields @xmath69 .",
    "figure  [ measurements_helmet ] shows the measurements on one head in the database .",
    "the first dimension measures the euclidean distance between the red points , the second dimension measures the euclidean distance between the green points , and the third dimension measures the euclidean distance between the blue points .",
    "for this example , we conduct an evaluation .",
    "we use 1500 heads from the caesar database  @xcite to compute the design models and we then test the quality of fit using 500 different heads from the caesar database .",
    "in our example , we aim to design helmets that can be adjusted by @xmath70 in the first dimension , by @xmath71 in the second dimension , and by @xmath72 in the third dimension .",
    "this defines the box @xmath18 .",
    "figures  [ greedy_head_complete ] and  [ greedy_head ] show the results of a greedy covering of @xmath13 .",
    "the figures show the points @xmath13 as black points and the centers of the boxes @xmath19 as red points .",
    "furthermore , for each box , the figures show a screen shot of the corresponding procrustes mean shape .",
    "the coordinate axes are shown in the colour of the corresponding dimension ( see figure  [ measurements_helmet ] ) .",
    "figure  [ greedy_head_complete ] shows the result when we aim to cover the entire population .",
    "this covering requires eight design models .",
    "figure  [ greedy_head ] shows the result of greedily covering a large subset of @xmath13 with three boxes .",
    "the boxes corresponding to the three design models cover a large subset of @xmath13 .",
    "note that for the data used in this example , it is not easy to manually find the best locations of the boxes since we use three measurements and since it is hard to optimally place points manually in three - dimensional space .",
    "+    we use 500 different head shapes to compute the quality of fit of the computed design models as follows .",
    "we compute the points in @xmath15 corresponding to the 500 head shapes and we compute how many of these points are covered by at least one of the computed boxes . the eight design models computed using the greedy covering algorithm shown in figure  [ greedy_head_complete ] cover @xmath73 of all the shapes .",
    "the three design models computed by greedily covering the largeset subset of @xmath13 using three boxes shown in figure  [ greedy_head ] cover @xmath74 of all the shapes .",
    "this shows that when using the three design models shown in figure  [ greedy_head ] to design three sizes of a helmet , we expect that @xmath74 of all adults find that at least one of these three helmets fits them .",
    "this paper presented a novel approach to generate design models for the design of gear or garments .",
    "the approach makes use of the widely available anthropometric databases to find design models that represent a large portion of the population .",
    "we find the design models by solving a covering problem in a low - dimensional parameter space .",
    "note that in this paper , we use translated boxes of the same size to cover the parameter space .",
    "this seems the most intuitive shape with which a designer may wish to cover the parameter space . to solve this problem",
    ", we presented a slow @xmath22-approximation and a fast @xmath27-approximation of the optimal solution .",
    "if the aim is to cover the parameter space with translated balls or ellipsoids of the same size , all of the algorithms in this paper can be adapted to this scenario .",
    "if the aim is to cover the parameter space with boxes , balls , or ellipsoids of different size or orientation , the algorithms presented in sections  [ greedy_cover ] and  [ fixed_box ] can be adapted to this scenario .",
    "the resulting design models are only as good as the given anthropometric database .",
    "this paper discussed the option of improving the coverage of the target population by extrapolating models from the database before computing the design models .",
    "we thank pengcheng xi for providing us with the data .",
    "this work has partially been funded by the cluster of excellence _ multimodal computing and interaction _ within the excellence initiative of the german federal government ."
  ],
  "abstract_text": [
    "<S> when designing a product that needs to fit the human shape , designers often use a small set of 3d models , called _ design models _ , either in physical or digital form , as representative shapes to cover the shape variabilities of the population for which the products are designed . until recently </S>",
    "<S> , the process of creating these models has been an art involving manual interaction and empirical guesswork . </S>",
    "<S> the availability of the 3d anthropometric databases provides an opportunity to create design models optimally . in this paper </S>",
    "<S> , we propose a novel way to use 3d anthropometric databases to generate design models that represent a given population for design applications such as the sizing of garments and gear . </S>",
    "<S> we generate the representative shapes by solving a covering problem in a parameter space . </S>",
    "<S> well - known techniques in computational geometry are used to solve this problem . </S>",
    "<S> we demonstrate the method using examples in designing glasses and helmets . </S>"
  ]
}