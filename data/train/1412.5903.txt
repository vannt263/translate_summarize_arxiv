{
  "article_text": [
    "in this work we tackle the problem of _ unconstrained text recognition _  recognising text in natural images without restricting the words to a fixed lexicon or dictionary .",
    "usually this problem is decomposed into a word detection stage followed by a word recognition stage .",
    "the word detection stage generates bounding boxes around words in an image , while the word recognition stage takes the content of these bounding boxes and recognises the text within .",
    "this paper focuses on the text recognition stage , developing a model based on deep convolutional neural networks ( cnns ) ( @xcite ) .",
    "previous methods using cnns for word recognition ( discussed in more detail in section section  [ sec : related ] ) has either constrained ( @xcite ) or heavily weighted ( @xcite ) the recognition results to be from a dictionary of known words .",
    "this works very well when training and testing are limited to a fixed vocabulary , but does not generalise to where previously unseen or non - language based text must be recognised  for example for generic alpha - numeric strings such as number plates or phone numbers .    the shift of focus towards a model which performs accurately without a fixed dictionary increases the complexity of the text recognition problem .",
    "to solve this , we propose a novel cnn architecture ( figure  [ fig : path ] ) employing a _ conditional random field _",
    "( crf ) whose unary terms are outputs of a cnn character predictor , which are position - dependent , and whose higher order terms are outputs of a cnn n - gram predictor , which are position - independent .",
    "the recognition result is then obtained by finding the character sequence that maximises the crf score , enforcing the consistency of the individual predictions .",
    "the crf model builds on our previous work where we explored dictionary - based recognition ( @xcite ) for two scenarios : the first was to train a different cnn character classifier for each position in the word being recognised , using the whole image of the word as input to each classifier ( an idea also expored by  @xcite ) ; the second was to construct a cnn predictor to detect the n - grams contained in the word , effectively encoding the text as a bag - of - n - grams .",
    "the dictionary - free joint model proposed here is trained by defining a structured output learning problem , and back - propagating the corresponding _ structured output loss_. this formulation results in multi - task learning of both the character and n - gram predictors , and additionally learns how to combine their representations in the crf , resulting in more accurate text recognition .",
    "the result is a highly flexible text recognition system that achieves excellent unconstrained text recognition performance as well as state - of - the - art recognition performance when using standard dictionary constraints .",
    "while performance is measured on real images as contained in standard text recognition benchmarks , all results are obtained by training the model  _ purely on synthetic data_. the model is evaluated on this synthetic data as well in order to study its performance under different scenarios .",
    "section  [ sec : related ] outlines work related to ours .",
    "section  [ sec : characters ] reviews the character sequence model and section  [ sec : ngrams ] the bag - of - n - grams model .",
    "section  [ sec : joint ] shows how these predictors can be combined to form a joint crf model and formulates the training of the latter as structured - output learning .",
    "section  [ sec : eval ] evaluates these models extensively and section  [ sec : conclusion ] summarises our findings .",
    "we concentrate here on text recognition methods , recognising from a cropped image of a single word , rather than the text detection stages of scene text recognition ( ` text spotting ' ) that generate the word detections .",
    "traditional text recognition methods are based on sequential character classification , finding characters by sliding window methods ( @xcite , after which a word prediction is made by integrating character classifier predictions in a left - to - right manner .",
    "the character classifiers include random ferns ( @xcite ) in @xcite , and cnns in  @xcite . both  @xcite and  @xcite use a small fixed lexicon as a language model to constrain word recognition .",
    "more recent works such as  @xcite make use of over - segmentation methods , guided by a supervised classifier , to generate candidate character proposals in a single - word image , which are subsequently classified as true or false positives .",
    "for example , photoocr  ( @xcite ) uses binarization and a sliding window classifier to generate candidate character regions , with words recognised through a beam search driven by classifier scores and static n - gram language model , followed by a re - ranking using a dictionary of 100k words .",
    "@xcite uses the convolutional nature of cnns to generate response maps for characters and bigrams which are integrated to score lexicon words .",
    "in contrast to these approaches based on character classification , the work by  @xcite instead uses the notion of holistic word recognition .",
    "@xcite still rely on explicit character classifiers , but construct a graph to infer the word , pooling together the full word evidence .",
    "@xcite use aggregated fisher vectors  ( @xcite ) and a structured svm framework to create a joint word - image and text embedding . @xcite and more recently @xcite also formluate joint embedding spaces , achieving impressive results with minimal training data .",
    "@xcite use whole word - image features to recognize words by comparing to simple black - and - white font - renderings of lexicon words . in our own previous work ( @xcite )",
    "we use large cnns acting on the full word image region to perform 90k - way classification to a dictionary word .",
    "it should be noted that all the methods make use of strong static language models , either relying on a constrained dictionary or re - ranking mechanism .",
    "@xcite had great success using a cnn with multiple position - sensitive character classifier outputs ( closely related to the character sequence model in section [ sec : characters ] ) to perform street number recognition .",
    "this model was extended to captcha sequences ( up to 8 characters long ) where they demonstrated impressive performance using synthetic training data for a synthetic problem ( where the generative model is known ) , but we show that synthetic training data can be used for a real - world data problem ( where the generative model is unknown ) .",
    "there have been previous uses of graphical models with back - propagated loss functions for neural networks , such as the early text recognition work of @xcite to combine character classifier results on image segmentations .",
    "another example is the recent work of @xcite for human pose estimation , where an mrf - like model over the distribution of spatial locations for each body part is constructed , incorporating a single round of message - passing .",
    "we now review the component cnn models , originally presented in our tech report @xcite , that form the basis of our joint model in section  [ sec : joint ] .      [ cols=\"^,^ \" , ]",
    "in this paper we have introduced a new formulation for word recognition , designed to be used identically in language and non - language scenarios . by modelling character positions and the presence of common n - grams ,",
    "we can define a joint graphical model .",
    "this can be trained effectively by back propagating structured output loss , and results in a more accurate word recognition system than predicting characters alone .",
    "we show impressive results for unconstrained text recognition with the ability to generalise recognition to previously unseen words , and match state - of - the - art accuracy when comparing in lexicon constrained scenarios ."
  ],
  "abstract_text": [
    "<S> we develop a representation suitable for the unconstrained recognition of words in natural images , where unconstrained means that there is no fixed lexicon and words have unknown length .    to this end </S>",
    "<S> we propose a convolutional neural network ( cnn ) based architecture which incorporates a conditional random field ( crf ) graphical model , taking the whole word image as a single input . </S>",
    "<S> the unaries of the crf are provided by a cnn that predicts characters at each position of the output , while higher order terms are provided by another cnn that detects the presence of n - grams . </S>",
    "<S> we show that this entire model ( crf , character predictor , n - gram predictor ) can be jointly optimised by back - propagating the structured output loss , essentially requiring the system to perform multi - task learning , and training requires only synthetically generated data . </S>",
    "<S> the resulting model is a more accurate system on standard real - world text recognition benchmarks than character prediction alone , setting a benchmark for systems that have not been trained on a particular lexicon . </S>",
    "<S> in addition , our model achieves state - of - the - art accuracy in lexicon - constrained scenarios , without being specifically modelled for constrained recognition . to test the generalisation of our model </S>",
    "<S> , we also perform experiments with random alpha - numeric strings to evaluate the method when no visual language model is applicable . </S>"
  ]
}