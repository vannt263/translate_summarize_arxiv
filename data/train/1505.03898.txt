{
  "article_text": [
    "analog - to - digital conversions and succeeding signal processing stages , quantization is an important issue .",
    "the extreme quantization scheme is that we only acquire one - bit for each measurement .",
    "this scheme only needs a single comparator and has many benefits in hardware implementation such as low power and a high rate .",
    "suppose that we have a linear measurement system @xmath0 for a signal @xmath1 .",
    "then the analog measurement is @xmath2 , and the one - bit quantized observation is only its sign , i.e. , @xmath3 .",
    "we set the sign of a non - negative number as 1 and that of a negative number as -1 .",
    "then the signal recovery problem related to one - bit measurements can be formulated as finding a signal @xmath4 from the signs of a set of measurements , i.e. , @xmath5 with @xmath6 let @xmath7 $ ] and @xmath8^t$ ] denote the measurement systems and the measurements respectively .",
    "it is easy to notice that signals with the same direction but different magnitudes have the same one - bit measurements with the same measurement systems , i.e. , the magnitude of the signal is lost in this quantization .",
    "therefore , we have to make an additional assumption on the magnitude of @xmath4 . without loss of generality , we assume @xmath9 .",
    "then the meaning of one - bit signal recovery can be explained as finding the subset of the unit sphere @xmath9 partitioned by random hyperplanes . in general ,",
    "when the number of hyperplanes becomes larger , the feasible set becomes smaller , and the recovery result becomes more accurate .",
    "however , there may still be infinite many points in the subset , and we need additional assumptions on the signal to make it unique .",
    "the compressive sensing ( cs ,  @xcite ) tells us that if the signal is sparse , we may exactly recover the signal with much fewer measurements than the dimension of the signal  @xcite .",
    "this technique has been successfully applied in many fields .",
    "however , quantization is rarely considered in these applications .    motivated by the advantages of one - bit quantization and cs , _ one - bit compressive sensing ( 1bit - cs ) _",
    "is proposed in  @xcite and has attracted many attentions in recent years .",
    "1bit - cs tries to recover a sparse signal from the signs of a small number of measurements . here",
    "the number of measurements can be larger than the dimension of the signal , which is different from regular cs .",
    "same as in regular cs problems , the fundamental assumption for 1bit - cs is that the true signal is sparse , i.e. , only a few components of the signal are non - zero . then , 1bit - cs is to find the sparsest solution in the feasible set , i.e. , @xmath10 where @xmath11 counts the number of non - zero components .",
    "this problem is non - convex because of the @xmath12-norm in the objective and the constraint @xmath13 .",
    "there are several algorithms that approximately solve ( [ 1bit ] ) or its variants . see  @xcite .    in  ( [ 1bit ] ) , we require that @xmath14 holds for all measurements with the assumption that there is no noise in the measurements",
    ". however , in real applications , there is always noise in the measuring process , i.e. , @xmath15 with @xmath16 .",
    "when the noise is small and @xmath17 , we can still recover the true signal accurately .",
    "the robustness to small noise is one of the advantages of 1bit - cs . however ,",
    "when the noise @xmath18 is large , we may have @xmath19 .",
    "in addition , there could be sign flips on components of @xmath20 during the transmission .",
    "note that sign changes because of noise happen with a higher probability when the magnitudes of true analog measurements are small , while sign flips during the transmission happen randomly among the measurements . with this difference in mind",
    ", the methods to deal with these two types of sign changes will also be different .    with noise or / and sign flips , the feasible set of ( [ 1bit ] )",
    "excludes the true signal and can become empty . to deal with noise and sign flips ,",
    "soft loss functions are used to replace the hard constraint , and it leads to robust 1bit - cs models . the first robust model is given by @xcite .",
    "it utilizes the following hinge loss to measure the sign changes , @xmath21 in the same paper , the squared hinge loss is also considered .",
    "the attempt in @xcite considers the following _ linear loss _",
    ", @xmath22 via minimizing the hinge or the linear loss , some robust 1bit - cs models and corresponding algorithms are proposed in @xcite and so on .",
    "these models will be reviewed in section ii . with these robust models ,",
    "1bit - cs becomes more attractive .",
    "for example , it is shown in @xcite and @xcite that under some conditions , signal recovery based on one - bit measurements is even better than conventional methods for nonlinear distortions and heavy noise .    in 1bit - cs",
    ", we only have sign information @xmath23 , and hence recovering @xmath4 can be regarded as a binary classification problem . in the binary classification field ,",
    "the hinge loss is widely used , e.g. , it is the loss function used in the classical support vector machine ( svm ,  @xcite ) . in  @xcite and other literature , it is shown that the hinge loss enjoys many good properties for classification such as classification - calibration , bayes consistency , and so on . in traditional classification tasks ,",
    "the linear loss is rarely considered .",
    "recently , it is found in  @xcite that applying the linear loss in svm is equal to the classical kernel rule @xcite , which enjoys computational effectiveness yet lacks of accuracy in many tasks .",
    "however , according to the experiments in @xcite and @xcite , the linear loss is quite suitable for 1bit - cs , compared with the hinge loss .",
    "this unusual phenomena that the linear loss performs better than the hinge loss motives us to investigate the properties of 1bit - cs .",
    "we will apply a _ pinball loss _ to establish recovery models for 1bit - cs .",
    "statistically , the pinball loss is closely related to the concept of quantile ; see @xcite for regression , and @xcite for classification . in this paper , we use the following definition for the pinball loss : @xmath24 the pinball loss is characterized by the parameter @xmath25 , and it is convex when @xmath26 .",
    "the hinge loss and the linear loss can be viewed as particular pinball loss functions with @xmath27 and @xmath28 , respectively . in other words",
    ", @xmath29 provides a bridge from the hinge loss to the linear loss .",
    "the hinge loss is a good choice for regular classification tasks ; and the linear loss shows good performance in 1bit - cs .",
    "hence , it is expected that a suitable trade - off between them can achieve better performance in 1bit - cs .    in this paper",
    ", we will discuss two models with the pinball loss minimization .",
    "first , based on the model given by @xcite , we propose a new model consisting of the pinball loss minimization , a @xmath12-norm constraint , and the @xmath30-norm unit sphere constraint .",
    "this problem is non - convex because of the @xmath12 norm and @xmath30 norm constraints . in order to solve this problem ,",
    "_ pinball iterative hard thresholding ( piht ) _ is established and evaluated by numerical experiments .",
    "second , we propose a convex model which contains the pinball loss , a @xmath31-norm regularization term , and the @xmath30-norm ball constraint .",
    "this model considers both the @xmath31-norm and the @xmath30-norm .",
    "so we name it as _ elastic - net pin - svm ( ep - svm)_. when @xmath32 , it reduces to the model given by @xcite . to effectively solve ep - svm ,",
    "its dual problem is derived , and a dual coordinate ascent algorithm is given .",
    "this algorithm is shown to converge to a global optimum , and its effectiveness is illustrated by numerical experiments .",
    "the rest of this paper is organized as follows .",
    "a brief review on the existing 1bit - cs methods is given in section ii .",
    "section iii introduces the pinball loss and proposes a pinball loss model with a @xmath12-norm constraint for 1bit - cs . in section iv",
    ", the elastic - net pin - svm is discussed , and an algorithm is provided to solve it .",
    "both proposed methods are evaluated on numerical experiments in section v , showing the performance of the pinball loss in 1bit - cs .",
    "a conclusion is given to end this paper in section vi .",
    "1bit - cs was introduced in 2008 by  @xcite , and since then , it has attracted lots of attentions . since the original model   is hard to minimize because of the @xmath12-norm , which is nonsmooth and non - convex .",
    "one alternative way is to minimize the convex hull of @xmath12 , i.e , the @xmath31-norm , and obtain the following 1bit - cs model : @xmath33 this model is given by  @xcite , and an efficient heuristic is established in @xcite .    due to",
    "the fact that the unit sphere is non - convex , ( [ 1bit - l1 ] ) is still a non - convex problem . in order to pursue the convexity ,",
    "the non - convex sphere constraint @xmath9 is replaced by a convex constraint on the measurements in  @xcite , and a convex model is established as follows : @xmath34 where @xmath35 is a given positive constant .",
    "note that ( [ 1bit - l1-convex ] ) can be reformulated as a linear programming problem because the second constraint @xmath36 becomes @xmath37 if the first constraint is satisfied .",
    "however , the solution of ( [ 1bit - l1-convex ] ) is not necessarily located on the unit sphere , hence one needs to project the solution onto the unit sphere .",
    "in fact , the solution is independent of @xmath35 after projected onto the unit sphere .    as we mentioned before",
    ", these models work only when there is no noise or the noise is too small to change the binary measurements , i.e. , there is no sign changes in @xmath20 . in real applications ,",
    "noise in the measurements is unavoidable , and there could be sign flips on @xmath20 during the transmission .",
    "noise or / and sign flips can make 1bit - cs problems ( [ 1bit - l1 ] ) and infeasible , and even feasible , the true signal is not in the feasible set . in other words ,",
    "the related classification problem is non - separable , and even separable , the classifier is not accurate . to deal with noise and sign flips",
    ", one can use a soft loss function instead of the hard constraint . since models with soft loss functions",
    "can tolerate the existence of noise and sign flips , they are called robust 1bit - cs models . in @xcite ,",
    "the following robust model is introduced : @xmath38 where @xmath39 is the number of non - zero components in the true signal .",
    "_ binary iterative hard thresholding with a one - sided @xmath31-norm _ ( biht ) is proposed to solve it approximately .",
    "the one - sided @xmath31-norm is related to the hinge loss function in classical l1-svm  @xcite , whose statistical property in classification has been well studied and understood in @xcite and @xcite .",
    "similar to the link between l1-svm and l2-svm , can be modified into the following problem via replacing the one - sided @xmath31-norm with a one - sided @xmath30-norm : @xmath40 for which binary iterative hard thresholding with a one - sided @xmath30-norm ( biht-@xmath30 , @xcite ) is proposed .",
    "modifications of biht / biht-@xmath30 for sign flips are proposed by  @xcite to improve their robustness to sign flips . however",
    ", this modification can not improve their robustness to sign changes because of noise in the measuring process .",
    "there are several ways to deal with sign changes because of noise , e.g. , @xcite uses maximum likelihood estimation ; @xcite uses a logistic function .",
    "note that both problems   and   are non - convex , and the algorithms biht / biht-@xmath30 only approximately solve the problems . the convex model for robust 1bit - cs using the linear loss proposed in  @xcite",
    "is : @xmath41 where @xmath35 is a given positive constant .",
    "the unit sphere constraint @xmath9 is relaxed to the unit ball constraint @xmath42 , and the sparsity constraint @xmath43 is replaced by the @xmath31 constraint @xmath44 .",
    "moreover , the one - sided @xmath31-norm is replaced by a linear loss to avoid the trivial zero solution , and minimizing the linear loss can be explained as maximizing the correlation between @xmath45 and @xmath46 .",
    "one can equivalently put the @xmath31-norm in the objective function instead of in the constraint .",
    "the corresponding problem is given by @xcite : @xmath47 where @xmath48 is the regularization parameter for the @xmath31-norm .",
    "in the rest of this paper , we call ( [ plan ] ) _ plan s model _ and ( [ zhang ] ) _ the passive model_. the latter comes from the name of the algorithm for ( [ zhang ] ) in  @xcite . both problems   and   are convex , and there is a closed - form solution for ( [ zhang ] ) .",
    "in robust 1bit - cs models , the loss function plays an important role . according to the experiments in @xcite ,",
    "plan s model and the passive model , which both minimize the linear loss , perform much better than biht / biht-@xmath30 .",
    "however , the linear loss is quite rare in other classification tasks . to the best of our knowledge , among the existing classification methods , only the classical kernel rule  @xcite , which enjoys computational effectiveness yet has bad classification accuracy generally ,",
    "could be regarded as a support vector machine with the linear loss .",
    "this connection is recently discussed in @xcite .    to improve the performance from the one - sided @xmath31-norm and the linear loss , we in this paper consider the pinball loss , which is defined in ( [ pinball ] ) .",
    "note that there are other equivalent formulations to define the pinball loss in @xcite and @xcite .",
    "the parameter @xmath25 is a key parameter for the pinball loss , and the one - sided @xmath31-norm and the linear loss correspond to the cases @xmath49 and @xmath32 , respectively .    if @xmath50 , @xmath51 is convex .",
    "thus , according to theorem 2 of @xcite , one can verify that it is classification - calibrated , i.e. , the function which minimizes the risk induced from @xmath52 has the same sign as the bayes rule .",
    "furthermore , if @xmath25 is non - negative , it is proved in @xcite that minimizing the pinball loss results in the bayes rule . however , when @xmath53 , the pinball loss is not consistent to the bayes rule because @xmath52 with a negative @xmath25 is not lower - bounded .",
    "thus , in most classification problems , the performance of a negative @xmath25 is not good , especially when @xmath25 is around @xmath54 .",
    "however the experiments on 1bit - cs conflict with the common sense : @xmath28 leads to much better results than @xmath27 , implying that 1bit - cs has some special properties and motivating us to investigate the pinball loss with @xmath55 $ ] .      at this section ,",
    "we replace the one - sided @xmath31 norm in   with the pinball loss and expect that a better performance could be achieved .",
    "specifically , we establish the following model , @xmath56 besides that a different loss function is considered , we also consider a bias term @xmath57 in the loss .",
    "one can change @xmath58 based on the measurement systems and even choose different @xmath58 s for different measurements .",
    "however , for the sake of simplicity , we choose the same @xmath58 for all measurements .",
    "minimization of a classification loss related to @xmath59 makes data locate on the half - planes @xmath60 .",
    "the margin between the two half - planes is given by @xmath61 . in 1bit - cs , @xmath62 is fixed to be @xmath63 , that means the margin is @xmath64 .",
    "pursuing a large margin between two classes is helpful , especially when the data are noise - corrupted . in most svm classifiers ,",
    "@xmath58 is set to be one . in biht ,",
    "@xmath65 and the loss function becomes the one - sided @xmath31-norm of @xmath66 .",
    "we will show the effect of @xmath58 in robust 1bit - cs after introducing the algorithm for  .    replacing the subgradient of the one - sided @xmath31 norm in biht with that of the pinball loss , we obtain  _ pinball iterative hard thresholding ( piht ) _ for ( [ piht ] ) .",
    "the algorithm is summarized in algorithm [ piht - algorithm ] , where @xmath67 stands for the best @xmath39-term approximation used in biht @xcite .",
    "set @xmath68 , @xmath69 , and @xmath70 +    return @xmath71    it is not hard to verify that @xmath72 gives a subgradient of @xmath73 , which is parallel to lemma 5 in @xcite .",
    "same as biht , the convergence of piht can not be guaranteed neither .",
    "the user needs to give a maximum number of iterations .",
    "though biht lacks of convergence analysis , it shows good performance in noiseless 1bit - cs and is widely applied ; see , e.g. , @xcite .",
    "we in this stage give a simple example to investigate the performance of the pinball loss for different @xmath25 and @xmath58 values .",
    "_ experiment 1 .",
    "we randomly generate a 1000-dimensional 10-sparse vector @xmath74 , i.e. , there are 10 non - zero components in @xmath75 . the non - zero components are randomly selected and their values follow the standard gaussian distribution .",
    "we take 500 binary measurements with @xmath76 drawing from the standard gaussian distribution as well . here , we consider the noise - free case and 10% of the measurements are flipped . _    _ algorithm [ piht - algorithm ] is used to recover the signal , and the result is denoted as @xmath77 .",
    "the step - size @xmath78 is chosen as suggested in  @xcite and fixed .",
    "the average recovery error @xmath79 of @xmath80 runs is used to measure the recovery performance . in fig.[fig - example-1 ] , the average recovery errors for different @xmath25 and @xmath58 values are plotted .",
    "the performance corresponding to the one - sided @xmath31-norm and the linear loss is marked .",
    "generally , we can conclude that piht with a suitable negative @xmath25 improves the performance of biht .",
    "the performance of piht is not very sensitive to @xmath58 when @xmath81 , and we suggest @xmath82 , which coincides with the loss used in most svm classifiers . _      from fig.[fig - example-1 ] , we observe that a significantly better performance could be achieved by piht with a negative @xmath25 .",
    "we do not claim which @xmath25 value is the best , but one can find that the recovery performance is not monotonous with respect to @xmath25 . generally , the pinball loss with a negative @xmath25 performs better than the hinge loss , i.e. , the pinball loss with @xmath27 .",
    "this conflicts with the observation from many other classification tasks and motivates us to investigate special properties of 1bit - cs .",
    "consider @xmath83 calculated by ( [ subgradient ] ) for @xmath84 .",
    "when @xmath27 , i.e. , the hinge loss is minimized , @xmath85 is not zero only for the observations satisfying @xmath86 ,",
    "i.e. , these observations are not strictly correctly classified .",
    "here we consider 1bit - cs as a binary classification problem , and by strictly , we mean that the analog measurement is not near zero . because the hinge loss minimization is to minimize the summation of the distances to the decision boundary for the measurements that are not strictly correctly classified , the measurements that are strictly correctly classified do not contribute in the optimal solution .",
    "if we let @xmath87 as in  , then the true signal is optimal when there is no noise or sign flips because the objective value is lower bounded by 0 , and the objective value for the true signal is 0 . when there are sign changes in the measurements , then the objective value for the true signal is not 0 any more , and only the measurements with inconsistent signs , i.e. , the sign of @xmath45 is different from that of the @xmath88 , contribute to the optimal solution for  .",
    "thus many measurements are useless in determining the optimal solution .",
    "the idea behind the linear loss and piht is to draw information from not only the incorrectly classified data , but also from the correctly classified ones .",
    "for example , when @xmath89 , @xmath83 calculated by ( [ subgradient ] ) encourages a larger @xmath90 when @xmath91 as well . following this way , all measurements contribute to the final result , and the influence of sign flips and noise is weakened .",
    "if we can detect the measurements with sign changes accurately , we can remove or replace them with the opposite values . in  @xcite , _ adaptive outlier pursuit ( aop ) _ is designed to detect the sign flips during the transmission . via adaptively detecting the measurements with sign flips , the performance of biht for 1bit - cs is significantly improved .",
    "we can also combine aop and piht to improve the performance of piht .",
    "the new method is called aop - piht .",
    "because during the iterations , aop detects the sign flips more and more accurately , the effect of @xmath25 should be decreased .",
    "we heuristically set @xmath92 in aop - piht , where @xmath93 is the initial @xmath25 value for the pinball loss and @xmath94 is the counter of the outer loop in aop - piht .",
    "we compare the performances of biht , piht , aop - biht , and aop - piht in the following two experiments .    _ experiment 2 .",
    "we randomly generate a 1000-dimensional 15-sparse vector @xmath74 in the same way as in experiment 1 .",
    "we compare piht , biht , aop - biht , and aop - piht for recovering the signal with different numbers of binary measurements ( @xmath95 ) .",
    "again there is no noise in the measurements but 10% of the signs are flipped .",
    "the average recovery errors of these four methods are shown in fig.[fig - aop ] .",
    "_    from fig .",
    "[ fig - aop ] , we can see that piht has a better performance than biht for all @xmath96 before aop is applied and both algorithms have very similar performances after aop is applied .",
    "aop is able to improve the performances of both algorithms , because it is able to detect most sign flips , and after the sign flips are corrected , the measurements are more accurate .",
    "however , before aop is applied , piht is more robust than biht .    as mentioned in the previous sections ,",
    "there are mainly two different sources of sign changes . though aop is able to detect random sign flips , sign changes because of noise are more difficult to detect .",
    "the next experiment shows that piht is able to deal with sign changes mainly because of noise . in the following experiment ,",
    "the performances of biht , piht , aop - biht , and aop - piht are evaluated for two cases .",
    "firstly , we consider the case with noise only , and then we consider the case with both noise and sign flips .    _",
    "experiment 3 .",
    "we randomly generate a 1000-dimensional 15-sparse vector @xmath74 in the same way as in experiment 1 .",
    "then we take 800 analog measurements and add noise with different signal to noise ratio ( snr ) values ( @xmath97 ) before the quantization .",
    "first we consider the case without sign flips , then we flip 10% of the measurements after the quantization . in fig.[fig - aop - noise",
    "] , the average recovery errors of biht , piht , aop - biht , and aop - piht are displayed by green dotted , blue dashed , black dot - dashed , and red solid line , respectively .",
    "_    when noise is the main source of sign changes , e.g. , no sign flips in fig.[fig - aop - noise : a ] and low snrs in fig.[fig - aop - noise : b ] , piht has the best performance among these four algorithms .",
    "aop - biht and aop - piht have similar performances , and their performances are worse than that of piht , i.e. , aop reduces the performance of piht when noise is the main source of sign changes .",
    "however , when sign changes happen mainly because of random sign flips , e.g. , fig.[fig - aop ] and high snrs in fig.[fig - aop - noise : b ] , aop - piht has a better performance than piht .",
    "it confirms that the two different sources of sign changes have to consider differently and aop is only suitable when the number of random sign flips is large .",
    "the main purpose of this paper is to introduce the pinball loss for 1bit - cs .",
    "we leave modifications based on the pinball loss for the further work . in general , many advanced techniques for biht are also applicable for piht . since minimizing the pinball loss instead of",
    "the hinge loss could improve performances , one can naturally expect that modifications on piht , e.g. , aop - piht , will achieve better performances .",
    "in the above section , we replaced the one - sided @xmath31 norm in biht with the pinball loss and established piht for robust 1bit - cs .",
    "numerical experiments illustrate that piht performs better than biht . however , problem   that piht solves is non - convex , and there is no guarantee that piht converges to the global optimum of  . in this section ,",
    "we propose a convex model using the pinball loss and derive an algorithm to solve it .",
    "specifically , the convex problem is @xmath98 here @xmath48 is a parameter to balance the regularization term @xmath99 and the data loss term .",
    "we call   as an _ elastic - net pin - svm ( ep - svm ) _ because it involves both @xmath31-norm and @xmath30-norm .",
    "when @xmath28 , the pinball loss becomes the linear loss , and   reduces to the passive model ( [ zhang ] ) , for which there is a closed - form solution  @xcite .",
    "according to the experience on other classification tasks and the performance in fig.[fig - example-1 ] , we expect that a suitably selected @xmath25 may improve the performance .",
    "however , for all @xmath25 greater than @xmath54 , analytic solutions are not available and we need an efficient algorithm",
    ". we will introduce a coordinate ascent method to solve  .      in order to obtain the dual problem of",
    ", we reformulate   into the following problem @xmath100 where @xmath101 is the indicator function defined as @xmath102 the corresponding lagrangian is @xmath103 then we can minimize @xmath104 with respect to the primal variables @xmath105 and obtain the dual problem of ( [ elastic - net - primal-1 ] ) as below , @xmath106    assume that we solve the dual problem and obtain optimal @xmath107 and @xmath108 , then we can find the optimal @xmath109 for   as follows :    1 .   if @xmath110 , i.e. , @xmath111 , the optimal @xmath109 can be obtained as @xmath112 2 .",
    "if @xmath113 , i.e. , @xmath114 .",
    "the optimal @xmath109 may not be unique , and all @xmath109 that satisfies + [ sol_con ] @xmath115 + are optimal .",
    "* remark 1 : * if @xmath116 , we have @xmath117 . then it is easy to check that @xmath118 is optimal .",
    "this generalizes the result for the passive model  ( * ? ? ?",
    "* lemma 1 ) . when @xmath32 , there is no constraints  - , and any @xmath109 satisfying  - is optimal .",
    "let us define two hypercubes @xmath119 if @xmath120 , then the optimal @xmath109 will always be on the unit sphere .",
    "even if @xmath121 , we may still have @xmath111 when @xmath57 , and the optimal @xmath109 is on the unit sphere .",
    "however , if @xmath87 , the optimal dual objective is @xmath122 when @xmath123 , and the the primal objective becomes zero when @xmath124 , so @xmath118 is optimal for the primal problem .    in order to get the optimal @xmath109 on the unit sphere , we can choose smaller @xmath48 because smaller @xmath48 implies smaller @xmath125 and @xmath126 becomes smaller .      in the dual problem  ,",
    "the constraints are separable and we can apply the coordinate ascent method efficiently .",
    "the subproblems are :    \\1 ) @xmath127-subproblem : @xmath128 is separable with respect to @xmath129 , so @xmath127 can be computed in parallel via @xmath130    \\2 ) @xmath131-subproblem : let us consider @xmath132 .",
    "then it becomes the following optimization problem on @xmath133 : @xmath134 denote @xmath135 .",
    "problem   becomes @xmath136 the optimal solution @xmath137 can be calculated analytically as following :    * if @xmath138 , the objective function is non - increasing .",
    "we have that @xmath139 is optimal , i.e. , @xmath140 . *",
    "if @xmath141 , we have @xmath142 where @xmath143 with @xmath144    summarizing the previous discussion , we give the dual coordinate ascent method for ( [ elastic - net - primal ] ) in algorithm [ ep - svm - algorithm ] .    set @xmath145 ; + calculate @xmath135 +    when @xmath28 , the algorithm ends in one iteration because @xmath146 for all @xmath147 .",
    "thus , there is an analytical solution for the passive model . for other @xmath25 values",
    ", there is no analytical solution .",
    "however , the next theorem states that the output @xmath109 of algorithm  [ ep - svm - algorithm ] is optimal for  .",
    "[ thm-2 ] the dual coordinate ascent for ep - svm ( algorithm  [ ep - svm - algorithm ] ) converges to an optimal solution of  .",
    "the optimality condition for   is    * if @xmath13 , there exists @xmath148 satisfying @xmath149 & \\mbox { if } c - y_i({{\\bf u}}_i^t{{\\bf x}})=0\\\\ = -\\tau / m & \\mbox { if } c - y_i({{\\bf u}}_i^t{{\\bf x}})<0 \\end{array } \\right.\\end{aligned}\\ ] ] such that @xmath150 . *",
    "if @xmath151 , there exists @xmath148 satisfying @xmath152 & \\mbox { if } c - y_i({{\\bf u}}_i^t{{\\bf x}})=0\\\\ = -\\tau / m & \\mbox { if } c - y_i({{\\bf u}}_i^t{{\\bf x}})<0 \\end{array } \\right.\\end{aligned}\\ ] ] such that @xmath153 .",
    "then we show that the output of algorithm  [ ep - svm - algorithm ] satisfies the optimality condition . + * case 1 * ( @xmath154 ) : we have @xmath155 and the algorithm shows that @xmath156 and @xmath157 are coordinate maximum of  .",
    "thus we have    * if @xmath158 , we have @xmath159 . * if @xmath160 , we have @xmath161 . * if @xmath162 , we have @xmath163 . *",
    "if @xmath164 , we have @xmath165 . * if @xmath166 , we have @xmath167 . *",
    "if @xmath168 , we have @xmath169 .    therefore , @xmath108 satisfy  , and we have @xmath170 which means that @xmath109 is optimal . +",
    "* case 2 * ( @xmath171 ) : we have @xmath172 , and @xmath156 and @xmath157 being coordinate maximum of   tells us that    * if @xmath158 , we have @xmath173 . * if @xmath160 , we have @xmath174 . *",
    "if @xmath162 , we have @xmath175 . * if @xmath164 , i.e. , @xmath176 , we have @xmath165 . * if @xmath177 , i.e. , @xmath178 , we have @xmath167 . *",
    "if @xmath179 , i.e. , @xmath180 , we have @xmath169 .",
    "therefore , for any @xmath109 satisfying  , @xmath108 satisfies  , and we have @xmath181 which means that @xmath109 is optimal .",
    "* remark 2 : * both the proof of theorem  [ thm-2 ] and algorithm  [ ep - svm - algorithm ] suggest that if @xmath182 for all @xmath147 , then @xmath117 , and ep - svm reduces to the passive model no matter what @xmath25 is .",
    "it happens because @xmath183 if @xmath184 .",
    "therefore , we choose @xmath58 to be much smaller than most @xmath185 . in all the experiments in this paper , @xmath76 has the same dimension ( @xmath186 ) , and they are generated in the same way .",
    "so we choose the same @xmath58 .    in practice , we can set a maximum number of iterations @xmath187 and choose @xmath188 as the stopping criterion . here",
    "@xmath189 is a small positive number . in the following experiments",
    ", we set @xmath190 and @xmath191 .",
    "though the passive model has an analytical solution , the linear loss is not a good classification loss in regular classification problems and 1bit - cs .",
    "thus we choose the pinball loss with @xmath192 . in order to evaluate the improvement using ep - svm with different @xmath25 values",
    ", we consider an experiment similar to experiment 1 .",
    "_ experiment 4 .",
    "we randomly generate a 1000-dimensional 15-sparse vector @xmath74 in the same way as in experiment 1 .",
    "then we take 300 binary measurements and flip 10% of them .",
    "ep - svm ( [ elastic - net - primal ] ) with different @xmath25 and @xmath58 values are evaluated .",
    "for the regularization coefficient @xmath48 , we choose @xmath193 , as suggested in @xcite .",
    "the experiments are repeated 100 times and the average recovery error is plotted in fig.[fig - example-2 ] . _",
    "this experiment has similar results as experiment 1 .",
    "the recovery error is not monotonous with respect to @xmath25 , and a suitable @xmath25 value , e.g. , @xmath194 in this experiment , leads to a better result .",
    "the performances of different @xmath58 values in fig.[fig - example-2:b ] suggest that @xmath82 is a good choice for this measurement system .",
    "it is possible that different @xmath48 values are suitable for different @xmath25 values . in the following , we set @xmath195 and consider the performances with different @xmath196 and @xmath25 values .",
    "the corresponding average recovery error is shown by a contour map in fig.[fig - example-3 ] .",
    "the curves represent the level sets , and the colors stand for the recovery error . generally , when @xmath28 , the suitable @xmath196 is around @xmath63 , which is also the suggestion of @xcite . if a larger @xmath25 is used , the corresponding suitable @xmath196 is smaller .",
    "the relationship between @xmath25 and @xmath48 is problem - dependent . for practical use , we suggest @xmath197 and @xmath198 for ep - svm . in the numerical experiments",
    ", we will evaluate other parameter values as well .",
    "[ c]@xmath25 [ c]@xmath196   and @xmath48 values , where @xmath195 .",
    "the experiment parameters are as the same as those in fig.[fig - example-2 ] .",
    ", title=\"fig:\",scaledwidth=30.0% ]",
    "in the previous sections , we introduced the pinball loss for robust 1bit - cs and established two models and two corresponding algorithms .",
    "several simple experiments illustrate that the pinball loss minimization helps us improve the recovery performance for 1bit - cs . in this section ,",
    "we further evaluate the performance of the pinball loss in more experiments with different noise levels and different numbers of measurements . to highlight the main purpose of this paper ,",
    "i.e. , using a new loss function for 1bit - cs , we do not consider advanced techniques such as aop .",
    "as shown in fig.[fig - aop ] , suitably applying those techniques for pinball loss minimization can further improve the performance .",
    "first , assume that the sparsity is known in advance .",
    "then  ( [ biht1 ] ) and ( [ piht ] ) are applicable to recover the signal .",
    "we solve them by biht and algorithm [ piht - algorithm ] , respectively .",
    "note that there is no stopping criterion for both biht and piht .",
    "we set the maximum number of iterations to @xmath199 for both of them .",
    "though the experiment shown in fig.[fig - example-1 ] implies that piht with @xmath200 is a good choice .",
    "we also evaluate the performance for @xmath201 and @xmath202 .",
    "the data are generated following the same way of experiments 14 : we have @xmath96 one - bit measurements and try to recover a @xmath203 dimensional signal with @xmath39-sparsity .",
    "the sign flip ratio is @xmath204 and the snr in measurements is @xmath205 .",
    "all the results below is the average value for repeating 100 times .",
    "the experiments are done with matlab 2013a on core i5 - 1.80ghz , 4.0 gb .",
    "the source code for algorithm [ piht - algorithm ] and algorithm [ ep - svm - algorithm ] can be found on the authors homepages .    to test the performance of biht and piht for different numbers of measurements , we select @xmath206 , @xmath207 , @xmath208 , @xmath209 and vary @xmath96 from @xmath80 to @xmath210 .",
    "fig.[fig - piht ] displays the performances for biht and piht with different @xmath25 values . compared with biht , using a negative @xmath25 improves the performance significantly with similar computational time . the good performance of @xmath200 is again confirmed .    in the previous experiment",
    ", we assumed that there is no noise and considered only sign flips .",
    "next we consider the performance of piht for different snrs with a fixed sign flip ratio .",
    "the average recovery error is shown by fig.[fig - piht - noise ] .",
    "the performances again suggest @xmath211 for this measurement system .",
    "[ c]signal - to - noise ratio @xmath205 [ c]recovery error   and @xmath209 .",
    "the result of biht is shown by blue dashed line , piht with @xmath201 is shown by green dotted line , piht with @xmath200 by red solid line , and piht with @xmath212 by black dot - dashed line .",
    ", title=\"fig : \" ]    when the sparsity of the true signal is not known in advance yet we have an estimation of the sparsity , we may still apply biht and piht with this estimation .",
    "however , the performance will be reduced , as shown in the next experiment . in order to test the performances of piht with different estimations of the sparsity of the signal",
    ", we fix the sparsity of the true signal as @xmath213 , but use different @xmath39 values in biht and piht . in fig.[fig - piht - k ] , we can observe that if the estimation on the sparsity is accurate , piht gives good recovery performance .",
    "but if the gap between the estimation and the real sparsity is large , the performance of piht becomes bad .",
    "this experiment shows that an accurate estimation is necessary for piht .",
    "[ c]sparsity parameter in biht and piht [ c]recovery error , other parameters for this experiment are @xmath214 and @xmath207 .",
    "the result of biht is shown by blue dashed line , piht with @xmath201 is shown by green dotted line , piht with @xmath200 by red solid line , and piht with @xmath212 by black dot - dashed line .",
    ", title=\"fig : \" ]      in general , the sparsity of the true signal is not known or the true signal is only approximately sparse . in these cases ,",
    "the performance of piht is reduced if the estimation is not correct , and we can consider plan s model , the passive model , or the proposed ep - svm .",
    "plan s model and the passive model use the same loss function , and they have similar recovery errors , according to the numerical study in  @xcite .",
    "since there is an efficient algorithm for the passive model , we in this paper only compare ep - svm with the passive model .    in the passive model and ep - svm , there is a regularization coefficient @xmath48 .",
    "as suggested by @xcite , we set @xmath193 for the passive model . for ep - svm ,",
    "the suitable @xmath48 value depends on @xmath25 , as illustrated by fig.[fig - example-3 ] .",
    "heuristically , we let @xmath195 , where @xmath196 is given below ,    [ cols=\">,>,>,>,>,>,>,>\",options=\"header \" , ]     let @xmath215 and vary the number of measurements from @xmath216 to @xmath217 . the average recovery errors and the computational time for different @xmath25 values are listed in table [ table - epsvm-1 ] .",
    "the smallest average recovery error for each @xmath96 is highlighted in bold font .",
    "the results imply that @xmath218 is a promising choice .",
    "though there is no analytic solution when @xmath192 , the computational time in table  [ table - epsvm-1 ] shows that algorithm  [ ep - svm - algorithm ] is very fast .",
    "furthermore , we test the performance of algorithm  [ ep - svm - algorithm ] for noise - corrupted data ( @xmath219 ) , and the corresponding results are reported in table  [ table - epsvm-2 ] .    from the results listed in table [ table - epsvm-1 ] and [ table - epsvm-2 ]",
    ", we observe that minimizing the pinball loss can improve the accuracy of the linear loss .",
    "the computational time is monotonically increasing with respect to @xmath25 , and generally algorithm  [ ep - svm - algorithm ] is efficient . in practice , we suggest @xmath197 and @xmath220 for ep - svm . in the following , we use this parameter set for different sparsity levels and different snrs .",
    "the results are shown in fig.[fig - epsvm ] , from which one can observe the improvement of pinball loss minimization .",
    "in 1bit - cs , one recovers a signal from a set of sign measurements .",
    "it can also be regarded as a binary classification problem , for which the hinge loss enjoys many good properties . however , the linear loss performs better than the hinge loss in robust 1bit - cs .",
    "thus , a trade - off between them is expected to share their good properties and improve the recovery performance for 1bit - cs .",
    "we introduce the pinball loss , which is a trade - off between the hinge loss and the linear loss .",
    "we proposed two model for minimizing the pinball loss and the two algorithms to solve them .",
    "piht improves the performance of the biht proposed in  @xcite and is suitable for the case when the sparsity of the true signal is given .",
    "ep - svm generalizes the passive model proposed in  @xcite and is suitable for the case when the sparsity of the true signal is not given .",
    "a fast dual coordinate ascent algorithm is proposed to solve ep - svm , and its convergence is proved .",
    "the numerical experiments demonstrate that the pinball loss , as a trade - off between the hinge loss and the linear loss , improves the existing 1bit - cs models with better performances . in the future",
    ", we will investigate other advanced methods based on the pinball loss minimization .",
    "the related statistical properties in view of learning are also interesting for study .",
    "j.  n. laska , z.  wen , w.  yin , and r.  g. baraniuk , `` trust , but verify : fast and accurate signal recovery from 1-bit compressive measurements , '' _ ieee transactions on signal processing _ ,",
    "59 , no .  11 ,",
    "pp . 52895301 , 2011 .",
    "l.  jacques , j.  n. laska , p.  t. boufounos , and r.  g. baraniuk , `` robust 1-bit compressive sensing via binary stable embeddings of sparse vectors , '' _ ieee transactions on information theory _ , vol .",
    "59 , no .  4 , pp . 20822102 , 2013 .",
    "y.  plan and r.  vershynin , `` robust 1-bit compressed sensing and sparse logistic regression : a convex programming approach , '' _ ieee transactions on information theory _ , vol .",
    "59 , no .  1 ,",
    "pp . 482494 , 2013 .",
    "p.  t. boufounos , `` reconstruction of sparse signals from distorted randomized measurements , '' in _ proceedings of 2010 ieee international conference on acoustics speech and signal processing ( icassp ) , _ pp .",
    "39984001 .",
    "j.  fang , y.  shen , h.  li , and z.  ren , `` sparse signal recovery from one - bit quantized data : an iterative reweighted algorithm , '' _ signal processing _ , vol .",
    "102 , no .  0 , pp . 201  206 , 2014 .",
    "a.  movahed , a.  panahi , and g.  durisi , `` a robust rfpi - based 1-bit compressive sensing reconstruction algorithm , '' in _ proceedings of 2012 ieee information theory workshop ( itw ) _ , 2012 , pp .",
    "567571 .",
    "d.  lee , t.  sasaki , t.  yamada , k.  akabane , y.  yamaguchi , and k.  uehara , `` spectrum sensing for networked system using 1-bit compressed sensing with partial random circulant measurement matrices , '' in _ proceedings of the 75th ieee vehicular technology conference ( vtc spring ) _ , 2012 , pp .",
    "15 .    c.  e. luo ,",
    "`` a low power self - capacitive touch sensing analog front end with sparse multi - touch detection , '' in _ proceedings of 2014 ieee international conference on acoustics , speech and signal processing ( icassp ) _ , 2014 , pp ."
  ],
  "abstract_text": [
    "<S> the one - bit quantization can be implemented by one single comparator , which operates at low power and a high rate . </S>",
    "<S> hence one - bit compressive sensing ( _ 1bit - cs _ ) becomes very attractive in signal processing . when the measurements are corrupted by noise during signal acquisition and transmission , 1bit - cs is usually modeled as minimizing a loss function with a sparsity constraint . </S>",
    "<S> the existing loss functions include the hinge loss and the linear loss . </S>",
    "<S> though 1bit - cs can be regarded as a binary classification problem because a one - bit measurement only provides the sign information , the choice of the hinge loss over the linear loss in binary classification is not true for 1bit - cs . </S>",
    "<S> many experiments show that the linear loss performs better than the hinge loss for 1bit - cs . </S>",
    "<S> motivated by this observation , we consider the pinball loss , which provides a bridge between the hinge loss and the linear loss . using this bridge , two 1bit - cs models and two corresponding algorithms </S>",
    "<S> are proposed . </S>",
    "<S> pinball loss iterative hard thresholding improves the performance of the binary iterative hard theresholding proposed in  @xcite and is suitable for the case when the sparsity of the true signal is given . </S>",
    "<S> elastic - net pinball support vector machine generalizes the passive model proposed in  @xcite and is suitable for the case when the sparsity of the true signal is not given . </S>",
    "<S> a fast dual coordinate ascent algorithm is proposed to solve the elastic - net pinball support vector machine problem , and its convergence is proved . </S>",
    "<S> the numerical experiments demonstrate that the pinball loss , as a trade - off between the hinge loss and the linear loss , improves the existing 1bit - cs models with better performances .    compressive sensing , one - bit , classification , pinball loss , dual coordinate ascent </S>"
  ]
}