{
  "article_text": [
    "robot grasping is far from a solved problem .",
    "one challenge still being addressed is that of computing a suitable grasp pose , given image observations of an object .",
    "however , issuing commands to align a robot gripper with that precise pose is highly challenging in practice , due to the uncertainty in gripper pose which can arise from noisy measurements from joint encoders , deformation of kinematic links , and inaccurate calibration between the camera and the robot .",
    "consider attempting a grasp on the object in figure [ fig : summary_a ]",
    ". given perfect control of a robot arm , the maximum grasp quality across the object could be targeted ( peak of blue function ) . however ,",
    "if the gripper misses its target , then in this case it will achieve a very poor grasp ; to the left , there are unstable regions , and to the right , there is a part of the object which would block the grasp and cause a collision .    to solve this , rather than predicting a single grasp",
    ", we propose to learn a _ grasp function _ , which computes a grasp quality score over all possible grasp poses , given a certain level of discretisation . here",
    "lies our key novelty , and this allows for the gripper s pose uncertainty to then be marginalised out , by smoothing the grasp function with a function representing this uncertainty , to yield a _ robust grasp function_. in this way , the final grasp pose will target an area of the object which no longer lies directly next to areas of poor grasp quality , as shown in the right of figure [ fig : summary_a ] .    to generate the grasp function",
    ", we train a convolutional neural network ( cnn ) to predict the grasp score for every pose of a parallel - jaw gripper , with respect to an observed depth image of an object .",
    "this is visualised in figure [ fig : summary_b ] , whereby each line indicates a gripper pose . to satisfy the need of cnns for large volumes of training data , we generate training pairs in simulation , by rendering synthetic depth images of 3d meshes , and use a physics simulator to predict the quality of grasps over the range of gripper poses .",
    "whilst this uncertainty could be incorporated directly into the simulation , learning the grasp function enables different arms to operate with just one set of simulations , and also allows for incorporation of dynamic uncertainty if particular arm configurations are known to have different uncertainties .",
    "furthermore , the grasp function could have broader use in the context of grasp planning , when obstacles or arm kinematics may prevent the achievement of some poses .",
    "in recent years , deep learning @xcite and its computer vision counterpart cnns @xcite have revolutionised the fields of object recognition @xcite , object segmentation @xcite , and local feature learning @xcite .",
    "consequently , these methods have also shown to be successful in robotics applications .",
    "robot localisation is moving away from using hand - engineered features @xcite and towards deep learning features @xcite , active recognition has achieved state - of - the - art performance by deep learning camera control @xcite , deep reinforcement learning is enabling end - to - end training for robot arm control @xcite , and even autonomous driving has been tackled by similar learning - based approaches @xcite",
    ".    for determining object grasp poses from images , use of hand - engineered features still performs well in complex cases such as dense clutter @xcite or multi - fingered grasping @xcite .",
    "however , in simpler cases , grasp pose detection via cnns has achieved state - of - the - art solutions .",
    "one approach to this has been to train on manually - labelled datasets , where human labellers have determined the location of a suitable grasp point on an image of an object .",
    "the cornell grasping dataset @xcite for example , consists of rgb and depth images with a parallel - jaw gripper pose defined in image coordinates . in @xcite ,",
    "a cnn is trained on this dataset by combining both rgb and depth data into a single network , and predicting the probability that a particular pose will be graspable , by passing the corresponding image patch through the network .",
    "this method was speeded up in @xcite by passing the entire image through the network rather than individual patches , eliminating the time - consuming need to process multiple patches for each frame .",
    "one challenge with deep learning is the need for a very large volume of training data , and the use of manually - labelled images is therefore not suitable for larger - scale training",
    ". one alternative approach , which we also adopt , has been to generate training data in simulation , and attempt to minimise the gap between synthetic data and real data .",
    "a popular example is the graspit !",
    "simulator @xcite , which processes 3d meshes of objects and computes the stability of a grasp based upon the grasp wrench space .",
    "whilst these methods do not incorporate dynamic effects which are typically involved in real grasping , prediction of static grasps can be achieved to a high accuracy by close analysis of the object shape . in @xcite ,",
    "this simulation was used to predict the suitability of a rgbd patch for finger locations in multi - fingered grasping , together with the suitability of each type of hand configuration .",
    "the work of @xcite used a similar static grasp metric and considered uncertainty in gripper pose , object pose , and frictional coefficients . in @xcite , grasping in clutter was achieved by using a static stability heuristic , based on a partial reconstruction of the objects .",
    "static metrics for generating training data have their limitations though , due to the ignorance of motion as the object is lifted from the surface . in @xcite",
    ", it has been shown that dynamic physics simulations offer a more accurate prediction of grasp quality than the standard static metrics .",
    "furthermore , @xcite illustrated how good - quality grasps predicted by physics simulations are highly correlated with those predicted by human labellers .",
    "one final approach to generating training data for deep learning is to do so with real - world experiments on a real robot .",
    "it was shown in @xcite that a reinforcement learning approach can achieve effective results by testing grasps on a real platform , although training time was several weeks and thus the scalability is of great limitation .",
    "@xcite then scaled this up from weeks to months , and also used multiple robots in parallel to learn a form of visual servoing , to predict when a moving gripper is in a suitable pose to grasp the object currently between the gripper s jaws .",
    "we avoid these approaches due to their lack of scalability and flexibility to challenges more complex than simple parallel - jaw grasping , and investigate how well a simulation can model real - world behaviour .    for parallel - jaw grasping as in our work",
    ", all the prior solutions fall into one of three categories : regressing the optimum grasp from an entire image , regressing the optimum grasp from a patch , or assigning a grasp quality score to patch . in this paper , we present the first work , to our knowledge , which effectively follows the third approach , but does so directly from a single image without processing individual patches , and hence achieving real - time grasping .",
    "this prediction of grasp quality scores over the entire image then allows us to incorporate gripper pose uncertainty during online operation , by smoothing the grasp quality score distribution with this uncertainty .",
    "the task is to grasp a single isolated object with a parallel - jaw gripper , by observing a depth image of the object from a single view and computing an optimum pose for the gripper to be sent to , as depicted in figure [ fig : gripper_pose ] .",
    "we follow the trend in this setup @xcite by constraining the gripper to a perpendicular orientation with respect to a flat table surface upon which the object is resting .",
    "grasps are then executed at a constant height such that the gripper s tips are 1 mm above the surface at the lowest point in the grasp .",
    "the remaining parameters to be learned which define the grasp pose , are therefore the translational position on the surface , and the rotation of the gripper with respect to the surface s normal .        given that the target gripper pose is computed through image observations , we first define these free parameters of the target pose in terms of image coordinates . in our experiments , we rigidly mount a depth camera onto the wrist of the robot arm , although this could instead be achieved by additional apparatus as in @xcite . the camera is positioned at a fixed height from the surface , and with viewing direction parallel to the surface s normal . by calibrating this camera with respect to the robot ,",
    "a target pose in the robot frame can therefore be computed and executed . in these image",
    "coordinates , we denote the target pose as @xmath0 , where @xmath1 and @xmath2 are the horizontal and vertical translations of the gripper s centroid relative to the centre of the image , and @xmath3 is the rotation of the gripper about the image s @xmath4-axis , as shown in figure [ fig : gripper_pose ] .",
    "given the static transformation between the camera frame and the gripper frame , we can transform a target pose in image coordinates to a target pose in the robot s frame by a sequence of transformations :    @xmath5    where @xmath6 is the target gripper pose in the robot frame , @xmath7 is the transformation between the robot frame and the starting gripper frame ( when the image was captured ) , @xmath8 is the calibrated transformation between the gripper frame and the camera frame , and @xmath9 is the transformation between 2d image coordinates and the 3d camera frame .    to make training and inference tractable , the space of gripper poses defined by @xmath10 is now discretised into space @xmath11 , such that each element @xmath12 represents a unique gripper pose ( one line in figure [ fig:4c ] ) .",
    "choice in the granularity of this discretisation is a compromise between precision in target pose prediction , and tractability of training .",
    "we chose a discretisation of 1 cm in translation on the table surface , corresponding to a 14 pixels in the @xmath13 images , and @xmath14 in rotation .",
    "the task now becomes to predict , from an observed depth image , a single grasp quality score for each of these 8712 possible grasp poses in @xmath11 , to yield the overall grasp function @xmath15 .",
    "prediction of these grasp quality scores is achieved by training a cnn to take as input a single depth image , and to output a score for each grasp pose .",
    "cnns require a huge amount of training data to fully exploit their capacity to learn complex functions , whilst avoiding overfitting . as such , real - world experiments are not scalable enough to generate the required extent of data to learn the grasp function . instead , we generate all our training data in simulation , by rendering depth images using opengl s depth buffer , and using a physics engine to simulate grasps .",
    "we capitalise on the recent modelnet dataset @xcite , a large - scale collection of 3d mesh models representing a range of common objects which has been collated specifically for deep learning experimentation .",
    "the use of mesh models is particularly suitable for our application , for two reasons .",
    "first , synthetic depth images can be easily rendered from meshes , which are much more realistic than synthetic rgb images , as these often struggle to model illumination and texture with sufficient realism .",
    "second , mesh models allow us to directly attempt grasps over the entirety of the model using physics simulation .",
    "we use the dynamic animation and robotics toolkit ( dart ) @xcite for physics simulation .",
    "dart was chosen over other static simulators such as with graspit !",
    "@xcite , due to the importance of dynamics modelling in predicting the behaviour of real - world grasps , which typically involve movement after a grasp is executed .",
    "dart was also chosen because of its suitability for robotics applications from its hard constraints imposed on the resolving of forces , its support of mesh models for collision detection , and its intuitive use of generalised coordinates for kinematics modelling .",
    "whilst dart is slower than physics engines more suited to computer graphics , such as bullet , its greater accuracy in modelling precise , real - world behaviour is imperative for an application such as grasp simulation .",
    "however , a range of alternative physics simulators also exist which could be interchangeable for the physics simulation @xcite .",
    "using dart , we constructed a simple parallel - jaw gripper , consisting of a `` hand '' which is free to move kinematically , and two `` fingers '' which are controlled dynamically by torque control on a revolute joint .",
    "we then constructed a flat surface , upon which each object mesh is placed . to execute a grasp , the hand is positioned in a specified pose on the surface ,",
    "after which a constant force is applied to the finger joints for a fixed time .",
    "subsequently , the hand is raised upwards by 20 cm at a constant speed of 0.1 m / s .",
    "the dimensions of the gripper were matched to that of our kinova mico arm , with a fixed distance of 10 cm between fingers .",
    "figure [ fig : dart_plane ] demonstrates the simulation world for a executing a single grasp attempt .",
    "due to imperfections in the physics simulator , physical values including the applied finger torque , the coefficients of friction , and the object density , were all manually tweaked to yield an acceptable level of grasping realism over a range of objects . whilst this would be an obvious flaw",
    "if these properties were being modelled , in our case the important output is the relative grasp quality over different poses  not the absolute magnitude of torque required .          to simulate a depth image of the object model , images",
    "were rendered using opengl s depth buffer . a noise model , inspired by @xcite ,",
    "was then applied to simulate a true image from the primesense carmine 1.09 camera used in our experiments .",
    "this noise model consists of two gaussian components : random shifting of pixels in a local region to simulate noise in the localisation of each depth measurement , and further random noise to simulate noise in the depth measurements themselves . for a ground truth depth of @xmath16 at pixel location @xmath17 , the depth after applying the model",
    "is defined as :    @xmath18    where @xmath19 is the standard deviation of noise in pixel localisation ( set to 1 pixel in our experiments ) , and @xmath20 is the standard deviation of noise in depth estimation ( set to 1.5 mm in our experiments ) . this noisy image is then used as input for training the cnn .",
    "figure [ fig:4b ] illustrates the effect of applying this noise model to a synthetic depth image .",
    "a ground truth grasp function is then calculated for every training object , by computing the grasp quality score @xmath15 for every discrete pose @xmath21 .",
    "each object was placed at a position in the centre of the camera image , and grasp attempts were executed for each pose . if the object was successfully lifted fully off the surface to a height of 20 cm , then that attempt was labelled with a score of 1 .",
    "each pose was assigned five grasp attempts , with each defined by a uniformly random draw from the continuous space of poses encompassing that discrete pose .",
    "the overall score for the pose was then the average over all five attempts . adding this small random noise allows a more informative score to be assigned to the pose from a range of 0 to 1 in 0.2 intervals , rather than a less informative score of either 0 or 1 .",
    "figure [ fig:4c ] presents a visualisation of these scores across all poses for one training object .",
    "as can be seen , higher scores are assigned to those areas of the image corresponding to stable areas for grasping .",
    "0.23     0.23     0.23     0.23     objects were selected from the modelnet database @xcite for training , covering a range of shapes such as _ airplane _ , _ chair _ , _ dish _ and _",
    "hammer_. for each model , rather than placing it upright in its standard pose , a random orientation was chosen .",
    "this is because rather than learning about object identity or semantics , pure grasping is more concerned with learning an understanding of object shape , and enriching the training images with observations from a wide range of unusual shapes is much more important than learning pose - specific grasps .",
    "furthermore , we randomly scaled each object model to within a range of appropriate dimensions for graspable objects , with the maximum dimension constrained to being between 5 cm and 20 cm .",
    "to learn a mapping between a depth image and the predicted grasp score @xmath22 , we train a cnn to output a score for every pose @xmath23 over the image .",
    "recent developments in deep learning now allow for networks which can learn very complex functions with high - dimensional outputs .",
    "we exploit this to enable prediction of a grasp score as a distribution over poses , rather than , for example , just predicting the single pose with the maximum score , as is typical with traditional learning - based grasping solutions .",
    "however , whilst deep networks can be trained for direct regression @xcite , their performance is superior when trained for classification , and so we form the training problem as one of classifying each pose in terms of its score .",
    "we retain the discretisation of scores already existing due to attempting five grasps per pose , and train the network to predict which of the six possible scores 0 , 0.2 , 0.4 , 0.6 , 0.8 , 1.0 , the pose should be assigned to . to achieve this ,",
    "a network structure similar to that of alexnet @xcite is adopted , with five convolutional layers adjoined by max pooling , and two fully - connected layers .",
    "the network takes as input a depth image , and then outputs a single value for each of the six scores .",
    "figure [ fig : cnn ] illustrates the structure of the network , and how the output corresponds to the learned grasp function .",
    "the network is trained by attempting to reduce the difference , across all images and all poses , between the ground truth score class @xmath24 ( @xmath25 ) , and the score class predicted by the network @xmath26 .",
    "this is achieved by minimising the following loss function :    @xmath27    here , @xmath28 is the number of training images in a mini - batch , @xmath29 is the number of poses output by the network ( @xmath30 ) , and @xmath31 is the number of possible scores which the pose can be classified as (= 6 ) . @xmath32",
    "therefore represents the ground truth score value for the @xmath33 image and the @xmath34 gripper pose , and @xmath35 is a length-@xmath31 vector corresponding to the output from the cnn for this pose .",
    "the indicator function @xmath36 is equal to 1 iff the score @xmath32 is equal to @xmath37 , and equal to 0 otherwise .",
    "as is standard in neural network classification , the softmax function is used here rather than the l1 or l2 norm , because only the relative values of the scores are important , not their absolute values .",
    "this is defined by :    @xmath38    training was done with gradient descent on mini - batches , using the tensorflow library @xcite .",
    "during grasp execution on a real robot , the depth image is first preprocessed to remove pixels which have zero values , as a consequence of imperfect sensing .",
    "this is achieved by replacing those pixels with the nearest non - zero pixel , or if there are several within a given radius , the average of these .",
    "figure [ fig : inpainting ] illustrates this effect .",
    "0.48        0.48     [ cols=\"^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^ \" , ]",
    "in this work , we have developed a method for predicting a grasp quality score over all grasp poses , which we call the _",
    "grasp function_. we investigated generating synthetic training data using physics simulation and depth image simulation , and using a cnn to map a depth image onto this grasp function .",
    "after convolving this grasp function with the gripper s pose uncertainty , we have shown that the pose corresponding to the maximum of this smoothed function is superior to the maximum of the original grasp function , both in synthetic and real - world experiments .",
    "the use of physics simulators and synthetic depth images has great capacity for extending this work to more complex tasks which also require large training datasets .",
    "for example , increasing the number of degrees of freedom in gripper pose , to incorporate height and angle - of - attack , would enable a greater range of grasps to be executed .",
    "however , with data generation already taking a week to complete , this would require a more refined selection of which simulations to process , and future work will investigate using active learning methods to enable the required scalability .",
    "research presented in this paper has been supported by dyson technology ltd .",
    "z. wu , s. song , a. khosla , f. yu , l. zhang , x. tang and j. xiao , in 3d shapenets : a deep representation for volumetric shapes , in proc .",
    "cvpr 2015 e. johns , s. leutenegger and a. davison , pairwise decomposition of image sequences for active multi - view recognition , in proc .",
    "cvpr 2016 a. miller and p. k. allen , graspit ! : a versatile simulator for robotic grasping , in ieee robotics and automation magazine , vol . 11 , no . 4 , 2004 y. lecun , y. bengio and g. hinton , deep learning , in nature , vol . 521 ,",
    "7553 , 2015 a. krizhevsky , iiya sutskever and g. hinton , imagenet classification with deep convolutional neural networks , in proc . nips 2012 i. lenz , h. lee and a. saxena , deep learning for detecting robotic grasps , in trans .",
    "ijrr , 2014 y. jiang , s. moseson and a. saxena , efficient grasping from rgbd images : learning using a new rectangle , in proc .",
    "icra 2011 j. redmon and a. angelova , real - time grasp detection using convolutional neural networks , in proc .",
    "icra 2015 j. varley , j. weisz , j. weiss and p. allen , generating multi - fingered robotic grasps via deep learning , in proc .",
    "iros 2015 j. mahler , f. t. pokomy , b. hou , m. roderick , m. laskey , m. aubry , k. kohlhoff , t. kroeger , j. kuffner and k. goldbert , dex - net 1.0 : a cloud - based network of 3d objects for robust grasp planning using a multi - armed bandit model with correlated rewards , in proc .",
    "icra 2016 d. kappler , j. bohg and s. schaal , leveraging big data for grasp planning , in proc .",
    "icra 2015 l. pinto and a. gupta , supersizing self - supervision : learning to grasp from 50k tries and 700 robot hours , in proc .",
    "icra , 2016 t. bohg , a. morales , t. asfour and d. kragic , data - drive grasp synthes : a survey , in trans .",
    "robotics , vol .",
    "2 t. stouraitis , u. hillenbrand and m. a. roa , functional power grasps transferred through warping and replanning , in proc .",
    "icra 2015 j. weisz and p. k. allen , pose error robust grasping from contact wrench space metrics , in proc .",
    "icra 2012 j. kim , k. iwamoto , j. j. kuffner , y. ota and n. s. pollard , physically - based grasp quality evaluation under uncertainty , in proc .",
    "icra 2012 m. kopicki , r. detry and j. l. wyatt , one shot learning and generation of dexterous grasps for novel objects , in trans .",
    "ijrr , 2015 t. erez , y. tassa and e. todorov , simulation tootls for model - based robotics : comparison of bullet , havok , mujoco , ode and physx , in proc .",
    "icra 2015 a. handa , t. whelan , j. mcdonald and a. j. davison , a benchmark for rgb - d visual odometry , 3d reconstruction and slam , in proc .",
    "icra 2014 a. doumanoglou , v. balntas , r. kouskouridas and t - k .",
    "kim , siamese regression networks with efficient mid - level feature extraction for 3d object pose estimation , in arxiv:1607.02257 , 2016 http://tensorflow.org sergey levine , peter pastor , alex krizhevsky and deirdre quillen , learning hand - eye coordination for robotic grasping with deep learning and large - scale data collection , in proc .",
    "iser 2016 v. balntas , e. johns , l. tang and k. mikolajczyk , pn - net : conjoined triple deep network for learning local image descriptors , in arxiv:1601.05030 , 2016 c. chen , a. seff , a. kornhauser and j. xiao , deepdriving : learning affordance for direct perception in autonomous driving , in proc .",
    "iccv 2015 c. finn , x. yu tan , y. duan , t. darrell , s. levine and p. abbeel , in proc .",
    "icra 2015 http://dartsim.github.io k. he , x. zhang , s. ren and j. sun , deep residual learning for image recognition , in proc .",
    "cvpr 2016 j. long , e shelhamer and t. darrell , fully convolutional networks for semantic segmentation , in proc .",
    "cvpr 2015 e. johns and g .- z .",
    "yang , generative methods for long - term place recognition in dynamic scenes , in trans .",
    "ijrr 2014 e. boularias , j. a. bagnell and a. stentz , learning to manipulate unknown objects in clutter by reinforcement , in proc .",
    "aaai 2015 n. sunderhauf , s. shirazi , f. dayoub , b. upcroft and m. milford , on the performance of convnet features for place recognition , in proc .",
    "iros 2015 m. gualtieri , a. ten pas , k. saenko and r. platt , high precision grasp pose detection in dense clutter , in arxiv:1603.01564 , 2016"
  ],
  "abstract_text": [
    "<S> this paper presents a new method for parallel - jaw grasping of isolated objects from depth images , under large gripper pose uncertainty . whilst most approaches aim to predict the single best grasp pose from an image , </S>",
    "<S> our method first predicts a score for every possible grasp pose , which we denote the _ grasp function_. with this , it is possible to achieve grasping robust to the gripper s pose uncertainty , by smoothing the grasp function with the pose uncertainty function . </S>",
    "<S> therefore , if the single best pose is adjacent to a region of poor grasp quality , that pose will no longer be chosen , and instead a pose will be chosen which is surrounded by a region of high grasp quality . to learn this function , we train a convolutional neural network which takes as input a single depth image of an object , and outputs a score for each grasp pose across the image . </S>",
    "<S> training data for this is generated by use of physics simulation and depth image simulation with 3d object meshes , to enable acquisition of sufficient data without requiring exhaustive real - world experiments . </S>",
    "<S> we evaluate with both synthetic and real experiments , and show that the learned grasp score is more robust to gripper pose uncertainty than when this uncertainty is not accounted for . </S>"
  ]
}