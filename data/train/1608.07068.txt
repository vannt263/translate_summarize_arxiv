{
  "article_text": [
    "generating a natural language description of the visual contents of a video is one of the holy grails in computer vision .",
    "recently , thanks to breakthroughs in deep learning  @xcite and recurrent neural networks ( rnn ) , many attempts @xcite have been made to jointly model videos and their corresponding sentence descriptions .",
    "this task is often referred to as video captioning . here , we focus on a much more challenging task : _ video title generation_. a great video title compactly describes the most salient event as well as catches people s attention ( e.g. , `` bmx rider gets hit by scooter at park '' in fig .  [",
    "fig : teaser]-top ) .",
    "in contrast , video captioning generates a sentence to describe a video as a whole ( e.g. , `` a man riding on bike '' in fig .",
    "[ fig : teaser]-bottom ) .",
    "video captioning has many potential applications such as helping the visually impaired to interpret the world .",
    "we believe that video title generation can further enable artificial intelligence systems to communicate more naturally by describing the most salient event in a long and continuous visual observation .",
    "video title generation poses two main challenges for existing video captioning methods @xcite .",
    "first of all , most video captioning methods assume that every video is trimmed into a 10 - 25 seconds short clip in both training and testing",
    ". however , the majority of videos on the web are untrimmed , such as user - generated videos ( ugvs ) which are typically 1 - 2 minutes long .",
    "the task of video title generation is to learn from untrimmed video and title pairs to generate a title for an unseen untrimmed video . in training",
    ", the first challenge is to temporally align a title to the most salient event , i.e. the video highlight ( red box in fig .",
    "[ fig : teaser ] ) in the untrimmed video .",
    "most video captioning methods , which ignore this challenge , are likely to learn an imprecise association between words and frequently observed visual evidence in the whole video .",
    "yao et al .",
    "@xcite recently propose a novel soft - attention mechanism to softly select visual observation for each word .",
    "however , we found that the learned per - word attention is prone to imprecise associations given untrimmed videos .",
    "hence , it is important to make video title generators `` highlight sensitive '' . as a second challenge ,",
    "title sentences are extremely diverse ( e.g. , each word appears in only @xmath1 sentences on average in our dataset ) .",
    "note that the two latest movie description datasets @xcite also share the same challenge of diverse sentences . on these datasets , state - of - the - art methods @xcite have reported fairly low performance .",
    "hence , it is important to `` increase the number of sentences '' for training a more reliable language model .",
    "we propose two generally applicable methods to address these challenges .",
    "* highlight sensitive captioner . *",
    "we combine a highlight detector with video captioners  @xcite to train models that can jointly generate titles and locate highlights .",
    "the highlights annotated in training can be used to further improve the highlight detector . as a result ,",
    "our `` highlight sensitive '' captioner learns to generate title sentences specifically describing the highlight moment in a video .",
    "* sentence augmentation . * to encourage the generation of more diverse titles",
    ", we augment the training set with sentence - only examples that do not come with corresponding videos .",
    "our intuition is to learn a better language model from additional sentences . in order to allow state - of - the - art video captioners to train with additional sentence - only examples ,",
    "we introduce the idea of `` dummy video observation '' . in short ,",
    "we associate all augmented sentences to the same dummy video observation in training so that the same training procedures in most state - of - the - art methods ( e.g. , @xcite ) can be used to train with additional augmented sentences .",
    "this method enables any video captioner to be improved by observing additional sentence - only examples , which are abundant on the web .    to facilitate the study of our task",
    ", we collected a challenging large - scale `` video title in the wild '' ( vtw ) dataset with the following properties : * highly open - domain . *",
    "our dataset consists of @xmath0 automatically crawled ugvs as opposed to self - recorded single domain videos @xcite .",
    "* untrimmed videos . *",
    "each video is on an average 1.5 minutes ( 45 seconds median duration ) and contains a highlight event which makes this video interesting .",
    "note that our videos are almost 5 - 10 times longer than clips in @xcite .",
    "our highlight sensitive captioner precisely addresses the unknown highlight challenge .    * diverse sentences . * each video in our dataset",
    "is associated with one title sentence .",
    "the vocabulary is very diverse , since on average each word only appears in @xmath1 sentences in vtw , compared to @xmath2 sentences in @xcite .",
    "our sentence augmentation method directly addresses the diverse sentences challenge .",
    "* description .",
    "* besides titles , our dataset also provides accompanying description sentences with more detailed information about each video .",
    "these sentences differ from the multiple sentences in @xcite , since our description may refer to non - visual information of the video .",
    "we show in our experiments that they can be treated as augmented sentences to improve video title generation performance .",
    "we address video title generation with the following contributions .",
    "( 1 ) we propose a novel highlight sensitive method to adapt two state - of - the - art video captioners @xcite to video title generation .",
    "our method significantly outperforms @xcite in meteor and cider .",
    "( 2 ) our highlight sensitive method improves highlight detection performance from @xmath3 to @xmath4 map .",
    "( 3 ) we propose a novel sentence augmentation method to train state - of - the - art video captioners with additional sentence - only examples .",
    "this method significantly outperforms @xcite in meteor and cider .",
    "( 4 ) we show that sentence augmentation can be applied on another video captioning dataset ( m - vad  @xcite ) to further improve the captioning performance in meteor .",
    "( 5 ) by combining both methods , we achieve the best video title generation performance of @xmath5 in meteor and @xmath6 in cider .",
    "( 6 ) finally , we collected one of the first large - scale  video title in the wild \" ( vtw ) dataset to benchmark the video title generation task .",
    "the dataset will be released for research usage .",
    "* video captioning .",
    "* early work on video captioning @xcite typically perform a two - stage procedure . in the first stage , classifiers are used to detect objects , actions , and scenes . in the second stage , a model combining visual confidences with a language model is used to estimate the most likely combination of subject , verb , object , and scene .",
    "then , a sentence is generated according to a predefined template .",
    "these methods require a few manual engineered components such as the content to be classified and the template . hence , the generated sentences are often not as diverse as sentences used in natural human description .    recently",
    ", image captioning methods @xcite begin to adopt the convolutional neural networks ( cnn ) and recurrent neural networks ( rnn ) approaches .",
    "they learn models directly from a large number of image and sentence pairs .",
    "the cnn replaces the predefined features to generate a powerful distributed visual representation .",
    "the rnn takes the cnn features as input and learns to decode it into a sentence .",
    "these are combined into a large network that can be jointly trained to directly map an image to a sentence .",
    "similarly , recent video captioning methods adopt a similar approach .",
    "venugopalan et al .",
    "@xcite map a video into a fix dimension feature by average - pooling cnn features of many frames and then use a rnn to generate a sentence .",
    "however , this method discards the temporal information of the video .",
    "rohrbach et al .",
    "@xcite propose to combine different rnn architectures with multiple cnn classifiers for classifying verbs ( actions ) , objects , and places .",
    "lisa anne hendricks et al .",
    "@xcite propose to utilize unpaired data for training to generate image captions and video descriptions . to capture temporal information in a video , venugopalan et al .",
    "@xcite propose to use rnn to encode a sequence of cnn features extracted from frames following the temporal order .",
    "this direct video - encoding and sentence - decoding approach outperforms @xcite significantly .",
    "concurrently , yao et al .",
    "@xcite proposes to model the temporal structure of visual features in two ways .",
    "first , it designs a 3d cnn based on dense trajectory - like features @xcite to capture local temporal structure",
    ". then , it incorporates a soft - attention mechanism to select temporal - specific video observations for generating each word .",
    "our proposed highlight sensitive method can be considered as a hard - attention mechanism to select a video segment ( i.e. , a highlight ) for generating the sentence . in our experiments",
    ", we find that our highlight sensitive method further improves @xcite .",
    "instead of rnn for encoding or decoding , xu et al .",
    "@xcite propose to embed both video and sentence to a joint space .",
    "most recently , pan et al .",
    "@xcite further propose a novel framework to jointly perform visual - semantic embedding and learn a rnn model for video captioning .",
    "pan et al .",
    "@xcite propose a novel hierarchical rnn to exploit video temporal structure in a longer range .",
    "yu et al .",
    "@xcite propose a novel hierarchical framework containing a sentence generator and a paragraph generator . despite many new advances in video captioning , video title generation has not been well studied .",
    "* video highlight detection .",
    "* most early highlight detection works focus on broadcasting sport videos  @xcite .",
    "recently , a few methods have been proposed to detect highlights in generic personal videos .",
    "sun et al .",
    "@xcite automatically harvest user preference to learn a model for identifying highlights in each domain .",
    "instead of generating a video title , song et al .",
    "@xcite utilize video titles to summarize each video .",
    "the method requires additional images to be retrieved by title search for learning visual concepts .",
    "there are also a few fully unsupervised approaches .",
    "zhao and xing  @xcite propose a quasi - real time method to generate short summaries .",
    "yang et al .",
    "@xcite propose a recurrent auto - encoder to extract video highlights .",
    "our video title generation method is one of the first to combine explicit highlight detection ( not soft - attention ) with sentence generation .",
    "* video captioning datasets . *",
    "a number of video captioning datasets  @xcite have been introduced .",
    "chen and dolan  @xcite collect one of the first multiple - sentence video description datasets with 1967 youtube videos .",
    "the duration of each clip is between 10 and 25 seconds , typically depicting a single activity or a short sequence .",
    "it requires significant human effort to build this dataset , since all @xmath7 sentences are labeled by crowdsourced annotators .",
    "on the other hand , we collect our dataset with a large number of video and sentence pairs fully automatically .",
    "rohrbach et al .",
    "@xcite collect a movie dataset with @xmath8 sentences from audio transcripts and video snippets in 72 hd movies .",
    "it also takes significant human effort to build this dataset , since each sentence is manually aligned to the movie .",
    "torabi et al .",
    "@xcite collect a movie dataset with @xmath9 sentences from audio transcripts and video snippets in 96 hd movies .",
    "they introduce an automatic descriptive video service ( dvs ) segmentation and alignment method for movies . hence , similar to our automatically collected dataset , they can scale up the collection of a dvs - derived dataset with minimal human intervention .",
    "jun xu et al .",
    "@xcite collect a large video description dataset by 257 popular queries from a commercial video search engine , with 118 videos for each query .",
    "we compare the sentences in our dataset with two movie description datasets in sec .",
    "[ sec.datacomp ] and find that our vocabularies are fairly different ( see @xcite ) . in this sense , our dataset is complementary to theirs .",
    "however , both datasets are not suitable for evaluating video title generation , since they consist of short clips with 6 - 10 seconds and selecting the most salient event in the video is not critical .",
    "our goal is to automatically generate a title sentence for a video , where the title should compactly describe the most salient event in the video .",
    "this task is similar to video captioning , since both tasks generate a sentence given a video .",
    "however , most video captioning methods focus on generating _ a relevant sentence _ given a 6 - 10 seconds short clip .",
    "in contrast , video title generation aims to produce a title sentence describing the most salient event given a typical 1 minute user - generated video ( ugv ) .",
    "hence , video title generation is an important extension of generic video captioning to understand a large number of ugvs on the web .",
    "to study video title generation , we have collected a new  video titles in the wild \" ( vtw ) dataset that consists of ugvs .",
    "we first introduce the dataset and discuss its unique properties and the challenges for video title generation .",
    "then , our proposed methods will be introduced in sec .",
    "[ sec.m ] .",
    "everyday , a vast amount of ugvs are uploaded to video sharing websites . to facilitate web surfers to view the interesting ones ,",
    "many online communities curate a set of interesting ugvs .",
    "we program a web crawler to harvest ugvs from these communities .",
    "for this paper , we have collected @xmath0 open - domain videos with 1.5 minutes duration on average ( 45 seconds median duration ) .",
    "we also crawl the following curated meta information about each video ( see fig .",
    "[ fig : data ] ) : _ title : _ a single and concise sentence produced by an editor , which we use as ground truth for training and testing ; _ description : _ 1 - 3 longer sentences which are different from titles , as they may not be relevant to the salient event , or may not be relevant to the visual contents ; _ others : _ tags , places , dates and category .",
    "this data is automatically collected from well established online communities that post 10 - 20 new videos per day .",
    "we do not conduct any further curation of the videos or sentences so the data can be considered `` in the wild '' .    *",
    "unknown highlight in ugvs.*[sec.dhl ] we now describe how title generation is related to highlight in ugvs .",
    "these ugvs are on an average @xmath10 minutes long which is 5 - 10 times longer than clips in video captioning datasets @xcite .",
    "intuitively , the title should be describing a segment of the video corresponding to the highlight ( i.e. , the salient event ) . to confirm this intuition ,",
    "we manually label title - specific highlights ( i.e. , compact video segments well described by the titles ) in a subset of videos .",
    "we found that the median highlight duration is about @xmath11 seconds .",
    "moreover , the non - highlight part of the video might not be precisely described by the title . in our dataset ,",
    "the temporal location and extent of the highlight in most videos are unknown .",
    "this creates a challenge for a standard video captioner to learn the correct association between words in titles and video observations . in sec .",
    "[ sec.thl ] , we propose a novel highlight - sensitive method to jointly locate highlights and generate titles for addressing this challenge .      [",
    "cols=\"^,^,^,^,^,^\",options=\"header \" , ]     [ table.vcperf ]      * highlight sensitive captioner . * when we apply our method on s2vt  @xcite , hl-1 significantly outperforms vanilla and hl consistently improves over hl-1 ( the better b@1 - 4 , meteor @xmath5 , and cider @xmath12 in table .",
    "[ table.vcperf ] ) .",
    "when we apply our method on sa  @xcite , the similar trend appears and hl achieves the better meteor @xmath13 and cider @xmath12 than both of the vanilla and the hl-1 .",
    "moreover , the updated highlight detector ( see technical report  @xcite for details ) achieves the best @xmath4 map as compared to the initial @xmath3 map .",
    "we also found that training considering highlight temporal location is important , since vanilla - gt - hl does not outperform vanilla .",
    "we further use the vanilla model on s2vt to automatically select highlight clips .",
    "then , we train a highlight - sensitive captioner based on these selected highlight clips as hl-0 .",
    "it achieves meteor @xmath14 and cider @xmath15 which is only slightly inferior to hl on s2vt .",
    "it shows that our method trained without highlight supervision also outperforms vanilla .",
    "* sentence augmentation . * on vtw ,",
    "when we apply our method on s2vt  @xcite , vanilla+desc .",
    "does not consistently improve accuracy ; however , both web aug . and",
    "desc . aug .",
    "improve accuracy significantly as compared to vanilla ( table .",
    "[ table.vcperf ] ) .",
    "when we apply our method on sa  @xcite , the similar trend appears and web aug . achieves the best meteor @xmath16 and cider @xmath17 .",
    "seconds duration .",
    "frames in the red box are manually selected from the detected highlight for illustration .",
    "note that our sentence in the last row has low meteor , but was judged by human to be better than the baseline .",
    ", title=\"fig:\",scaledwidth=95.0% ]   seconds duration .",
    "frames in the red box are manually selected from the detected highlight for illustration .",
    "note that our sentence in the last row has low meteor , but was judged by human to be better than the baseline .",
    ", title=\"fig:\",scaledwidth=95.0% ]   seconds duration .",
    "frames in the red box are manually selected from the detected highlight for illustration .",
    "note that our sentence in the last row has low meteor , but was judged by human to be better than the baseline .",
    ", title=\"fig:\",scaledwidth=95.0% ]   seconds duration .",
    "frames in the red box are manually selected from the detected highlight for illustration .",
    "note that our sentence in the last row has low meteor , but was judged by human to be better than the baseline . , title=\"fig:\",scaledwidth=95.0% ]   seconds duration .",
    "frames in the red box are manually selected from the detected highlight for illustration .",
    "note that our sentence in the last row has low meteor , but was judged by human to be better than the baseline .",
    ", title=\"fig:\",scaledwidth=95.0% ]   seconds duration .",
    "frames in the red box are manually selected from the detected highlight for illustration .",
    "note that our sentence in the last row has low meteor , but was judged by human to be better than the baseline .",
    ", title=\"fig:\",scaledwidth=95.0% ]    * our full method . * on vtw dataset , hl with web aug . on both s2vt and sa outperform their own variants ( last row in table .",
    "[ table.vcperf ] ) , especially in cider which gives higher value when a rare n - gram is predicted correctly .",
    "our best accuracy is achieved by combining hl with web aug .",
    "we also ask human judges to compare sentences generated by our hl+web aug . on s2vt method and the s2vt baseline ( vanilla ) on half of the testing videos ( see technical report  @xcite for details ) .",
    "human judges decide that @xmath18 of our sentences are on par or better than the baseline sentences .",
    "we show the detected highlights and generated video titles in fig .",
    "[ fig : ty ] .",
    "note that our sentence in the last row of fig .",
    "[ fig : ty ] has low meteor , but was judged by human to be better than the baseline .",
    "* setence augmentation on m - vad . *",
    "since s2vt outperforms sa in meteor and cider on vtw , we evaluate the performance of s2vt+web aug . on the m - vad dataset  @xcite .",
    "our method achieves @xmath19 in meteor as compared to @xmath20 of the s2vt baseline and @xmath21 reported in @xcite .",
    "this shows its great potential to improve video captioning accuracy across different datasets .",
    "we introduce video title generation , a much more challenging task than video captioning . we propose to extend state - of - the - art video captioners for generating video titles .",
    "to evaluate our methods , we harvest the large - scale `` video title in the wild '' ( vtw ) dataset . on vtw",
    ", our proposed methods consistently improve title prediction accuracy , and the best performance is achieved by applying both methods .",
    "finally , on the m - vad  @xcite , our sentence augmentation method ( meteor @xmath19 ) outperforms the s2vt baseline ( @xmath21 in  @xcite ) .",
    "* acknowledgements . *",
    "we thank microsoft research asia , most 103 - 2218-e-007 - 025 , most 104 - 3115-e-007 - 005 , novatek fellowship , and panasonic for their support .",
    "we also thank shih - han chou , heng hsu , and i - hsin lee for their collaboration .",
    "guadarrama , s. , krishnamoorthy , n. , malkarnenkar , g. , venugopalan , s. , mooney , r. , darrell , t. , saenko , k. : youtube2text : recognizing and describing arbitrary activities using semantic hierarchies and zero - shot recognition . in : iccv .",
    "( 2013 )        barbu , a. , bridge , e. , burchill , z. , coroian , d. , dickinson , s. , fidler , s. , michaux , a. , mussman , s. , narayanaswamy , s. , salvi , d. , schmidt , l. , shangguan , j. , siskind , j.m . , waggoner , j. , wang , s. , wei , j. , yin , y. , zhang , z. : video in sentences out . in : uai .",
    "( 2012 )      donahue , j. , hendricks , l.a . , guadarrama , s. , rohrbach , m. , venugopalan , s. , saenko , k. , darrell , t. : long - term recurrent convolutional networks for visual recognition and description . in : cvpr .",
    "( 2015 )        mao , j. , wei , x. , yang , y. , wang , j. , huang , z. , yuille , a.l . :",
    "learning like a child : fast novel visual concept learning from sentence descriptions of images . in : proceedings of the ieee international conference on computer vision .",
    "( 2015 ) 25332541                                                              abadi , m. , agarwal , a. , barham , p. , brevdo , e. , chen , z. , citro , c. , corrado , g.s .",
    ", davis , a. , dean , j. , devin , m. , ghemawat , s. , goodfellow , i. , harp , a. , irving , g. , isard , m. , jia , y. , jozefowicz , r. , kaiser , l. , kudlur , m. , levenberg , j. , man , d. , monga , r. , moore , s. , murray , d. , olah , c. , schuster , m. , shlens , j. , steiner , b. , sutskever , i. , talwar , k. , tucker , p. , vanhoucke , v. , vasudevan , v. , vigas , f. , vinyals , o. , warden , p. , wattenberg , m. , wicke , m. , yu , y. , zheng , x. : : large - scale machine learning on heterogeneous systems ( 2015 ) software available from tensorflow.org ."
  ],
  "abstract_text": [
    "<S> a great video title describes the most salient event compactly and captures the viewer s attention . </S>",
    "<S> in contrast , video captioning tends to generate sentences that describe the video as a whole . </S>",
    "<S> although generating a video title automatically is a very useful task , it is much less addressed than video captioning . </S>",
    "<S> we address video title generation for the first time by proposing two methods that extend state - of - the - art video captioners to this new task . </S>",
    "<S> first , we make video captioners _ highlight sensitive _ by priming them with a highlight detector . </S>",
    "<S> our framework allows for jointly training a model for title generation and video highlight localization . </S>",
    "<S> second , we induce high sentence diversity in video captioners , so that the generated titles are also diverse and catchy . </S>",
    "<S> this means that a large number of sentences might be required to learn the sentence structure of titles . </S>",
    "<S> hence , we propose a novel _ sentence augmentation _ method to train a captioner with additional sentence - only examples that come without corresponding videos . </S>",
    "<S> we collected a large - scale _ video titles in the wild _ </S>",
    "<S> ( vtw ) dataset of @xmath0 automatically crawled user - generated videos and titles . </S>",
    "<S> on vtw , our methods consistently improve title prediction accuracy , and achieve the best performance in both automatic and human evaluation . </S>",
    "<S> finally , our sentence augmentation method also outperforms the baselines on the m - vad dataset . </S>"
  ]
}