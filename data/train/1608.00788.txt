{
  "article_text": [
    "online evaluation using interleaving is an increasingly popular paradigm in ranker evaluation and has been found to be efficient and sensitive @xcite .",
    "here efficient means that relatively little click feedback is required to reliably distinguish rankers , and sensitive means that interleaving can distinguish between rankers with very similar retrieval quality .",
    "in addition to these requirements an important criterion for evaluating interleaving methods is that they be unbiased , in the sense that they do not systematically favour certain rankers independently of their actual quality .",
    "multileaving methods were recently introduced @xcite and potentially offer substantial improvements over interleaving in terms of efficiency , since they can compare sets of rankers of arbitrary size at each comparison .",
    "they were also found to be similarly sensitive to interleaving @xcite .",
    "section  [ sec : related ] describes related work , including two state of the art methods , team draft multileave ( tdm ) and probabilistic multileave ( pm ) .",
    "we observe that tdm does not scale well as the number of rankers in the comparison set increases , limiting its efficiency .",
    "we show that pm can fail to properly account for ranker similarities , introducing bias .",
    "section  [ sec : sosm ] then describes our proposed solution and section  [ sec : experiment ] provides experimental results demonstrating the new algorithm s superiority .",
    "our contributions are to produce a new multileaving method which outperforms current methods and to identify that pm can be biased .",
    "multileaving consists of two stages .",
    "first we create the multileaved list by sampling from the individual ranked lists , and then we credit rankers based on user clicks on the multileaved list and thereby infer a preference ordering of the rankers .",
    "three multileaving methods have been proposed , namely , team draft multileave ( tdm ) @xcite , optimised multileave @xcite and probabilistic multileave ( pm ) @xcite .",
    "we describe the two best performing methods , tdm and pm .",
    "tdm creates the multileaved list in rounds . in each round a random ordering of the rankers is decided and the top document that has not yet appeared in the multileaving is drawn from each ranker .",
    "this process is then repeated until the multileaved list is of sufficient length . in the credit inference stage of tdm ,",
    "rankers are credited for each click on a document drawn from the corresponding ranker .",
    "this credit does not consider the position of the document in the ranker s retrieved list . a matrix , @xmath0 , of pair - wise preferences between pairs of rankers",
    "is then inferred based on which ranker was given more credit .",
    "since rankers are only credited for clicks on documents drawn from the corresponding ranker , tdm does not scale well to comparing more rankers than there are documents in the multileaved list .",
    "since users of search engines typically only inspect the first results page , the multileaved list is effectively only of length 10 .",
    "we are often interested in comparing significantly more than just 10 rankers , so there is a need for multileaving methods which scale better to larger comparison sets .",
    "pm also creates the multileaved list in rounds . in each round a random ordering of the rankers is decided .",
    "then , a document is probabilistically selected from the ranker , where the probability of drawing a document @xmath1 from ranker @xmath2 is determined solely by the document s rank and is given by equation  [ eq1 ] , where @xmath3 is the rank of document @xmath1 in ranker @xmath2 , and @xmath4 is the set of documents ranked by @xmath2 .",
    "@xmath5 note that when a document is drawn , the document is removed from all the rankers retrieved lists . in the next round , the probabilities of the remaining documents are recalculated according to equation  [ eq1 ] , where the rankings , @xmath3 , are now determined in the absence of previously chosen documents .    in the credit inference stage , pm considers all possible assignments of documents to rankers that could have occurred , and weight each assignment based on its probability .",
    "an assignment , @xmath6 , has probability , @xmath7 , given by @xmath8 where @xmath9 , is the length of the multileaved list , @xmath10 , is the probability of drawing document @xmath11 from the assigned ranker , @xmath12 and is given by equation  [ eq1 ] , and @xmath13 is given by @xmath14 . for an assignment , @xmath6 , ranker @xmath2",
    "is given credit , @xmath15 equal to the number of assigned documents clicked on .",
    "the total credit , @xmath16 , assigned to ranker @xmath2 , is given by @xmath17 , where @xmath18 is the set of all possible assignments .",
    "a matrix , @xmath0 , of pair - wise preferences between pairs of rankers is then inferred based on which ranker was given more credit .",
    "pm can be biased since rankers can benefit from the presence of documents contributed by similar rankers , and these rankers will therefore perform better according to pm than they actually do in practice . to illustrate this problem ,",
    "consider the following simple example : three rankers and their corresponding retrieved lists : @xmath19 @xmath20 , and @xmath21 .",
    "the possible multileavings of length two , are @xmath22 and @xmath23 .",
    "the former multileaving occurs with probability 0.3704 , and the latter occurs with probability 0.6296 .",
    "assume that @xmath24 and @xmath25 are both relevant and always clicked on , i.e. all three rankers have equal performance . even though the rankers are equally good , @xmath26 will lose to the other two rankers with probability @xmath27 due to the fact that when the multileaving @xmath22 occurs , @xmath26 is given more credit in the credit inference stage of pm and if @xmath23 occurs , @xmath28 and @xmath29 are given more credit .",
    "thus , for pm , the presence of similar rankers introduces bias and distorts the outcome of comparisons .",
    "similar rankers benefit because their assignments are weighted higher .",
    "we propose a multileaving method called sample - only scored multileave ( sosm ) which scales well with the number of rankers being compared , without introducing bias .",
    "the main difference relative to pm is that the score attributed to each ranker _ only _ depends on how each ranker ranks the sample of documents contained in the multileaving , _ not _ how each ranker ranks documents in their original retrieved lists . in this way , if a document is preferentially sampled , it will not disproportionally disadvantage other rankers provided they rank the sample well .",
    "the process of creating the multileaved list in sosm is identical to that in tdm described in section  [ sec : related ] .    to infer preferences",
    ", each ranker ranks the documents in the multileaved list such that @xmath30 denotes the order of document @xmath1 in the multileaved list according to ranker @xmath2 . letting @xmath31 denote the documents of the multileaved list",
    ", the score of document @xmath1 for ranker @xmath2 is given in equation  [ eq2 ] .",
    "letting @xmath32 denote the clicked documents , ranker @xmath2 is credited with @xmath33 , where @xmath34 a matrix , @xmath0 , of preferences between pairs of rankers is then inferred based on which ranker was given more credit . note the similarity between our scoring function in equation  [ eq2 ] , and that of equation  [ eq1 ] used by pm .",
    "the only difference is that in the denominator we only sum over the documents contained in the multileaved list .",
    "sosm scales well with the number of rankers being compared , as verified experimentally in section  [ sec : experiment ] , and is also unbiased .",
    "it is simple to verify that sosm is unbiased , according to the definition of bias given in @xcite .",
    "additionally , we verify the unbiasedness of sosm experimentally in section  [ sec : experiment ] .",
    "in our problem setup , we are given a set of rankers @xmath35 whose performance we want to evaluate on a dataset using click feedback @xcite .",
    "multileaving methods output an @xmath36 matrix @xmath0 after each comparison , where @xmath37 is 1 if the multileaving method inferred a preference for ranker @xmath38 over @xmath2 , 0 if it inferred a preference for ranker @xmath2 over @xmath38 and @xmath39 if no preference was inferred between the rankers .",
    "we then define @xmath40 such that @xmath41 is the average over @xmath42 multileaved comparisons of @xmath37 .",
    "the mean ndcg@10 for held out queries in the dataset is assumed to be ground truth .",
    "we define an @xmath36 preference matrix @xmath43 in which @xmath44 is @xmath45 if ranker @xmath38 has a higher ndcg@10 than ranker @xmath2 , 0 if @xmath2 has a higher score than @xmath38 , and @xmath39 if the two rankers have the same score .    for a given pair of rankers , we consider the multileaving method to have made an error after @xmath42 comparisons if @xmath46 and @xmath44 are not equal .",
    "we wish to minimize the percentage of errors made , @xmath47    for these experiments we compare feature rankers from the mslr - web30k @xcite , ylr1 and ylr2 @xcite datasets .",
    "feature rankers can be rankers like pagerank or the bm25 score of the body of a document .",
    "feature rankers were also used for the experimental setup in @xcite .",
    "we use a simulated user setup . for each iteration",
    "we randomly sample with replacement a query from the pool of queries of the dataset .",
    "the rankers being compared are then multileaved , and clicks on the multileaved list are generated from three different probabilistic user models : the perfect , navigational and informational click models as described in @xcite .    for each dataset the queries",
    "are split into a training and test set . for a given run on @xmath48 rankers , we randomly sample @xmath48 feature rankers from the given dataset and compute ndcg@10 for each of these rankers on the test query set . these ndcg@10 scores are used to create a ground truth preference matrix @xmath43 against which we can measure the percentage errors @xmath49 defined in equation  [ eq : groundtruth ] . for each iteration",
    "we then randomly sample a query from the training set , multileave the @xmath48 rankers using each multileaving method , and compute @xmath49 for each method .",
    "we then investigate how @xmath49 develops at each iteration .",
    "additionally we show the percentage errors of the ndcg@10 score computed only from the queries so far used for multileaving .",
    "this serves as a lower bound on the error that can reasonably be obtained . for pm",
    "we fix the sample size parameter at 10,000 as in @xcite .",
    "* findings : *    table  [ table : results ] enumerates the percentage error after 2,000 and 10,000 iterations when multileaving 5 , 40 or 100 rankers for the three click models . in almost all cases",
    "sosm is superior .",
    "the two exceptions occur when comparing only 5 rankers . in this case , tdm is marginally better than sosm for the informational click model , and equivalent for the perfect click model . for 40 or 100 rankers ,",
    "sosm substantially outperforms tdm and pm .",
    "for example , with 100 rankers and the informational click model , the error rates are reduced by 50% from 32% to 16% after 10,000 iterations .    [ cols=\"^,^,^,^,<,^,^,<\",options=\"header \" , ]     [ table : results ]    figure  [ fig : clickmodels ] shows how the performances of the multileaving methods are affected by the click model used .",
    "for all three click models , sosm outperforms both tdm and pm .",
    "this is most pronounced for the navigational model , where the percentage error for tdm and pm is 50% greater than that of sosm ( 30% compared to 20% ) .",
    "figure  [ fig : datasets ] shows the sensitivity of the multileaving methods to different choices of dataset .",
    "we repeat the experiment from figure  [ fig : clickmodels](b ) using two other datasets .",
    "all three algorithms ( tdm , pm , sosm ) perform similarly across datasets . in all cases",
    ", sosm exhibits superior performance .",
    "+    figure  [ fig : varyrankers ] shows how the performances of the multileaving methods vary with the number of rankers being compared .",
    "we show the percentage error after 2,000 iterations , as a function of the number @xmath48 of rankers that are multileaved . for a given @xmath48 , a random subset of rankers",
    "is selected and multileaved for 2,000 iterations .",
    "the same subset is used for pm , tdm and sosm .",
    "this is repeated 25 times , each time with a different random subset of @xmath48 rankers .",
    "the results in figure  [ fig : varyrankers ] are the average of these 25 runs .",
    "we observe that for sosm and pm the error remains relatively stable as the number of rankers increases .",
    "however , for tdm the error is increasing and we observe that its performance becomes worse than pm for large numbers of rankers .",
    "this is due to the fact that during the scoring phase , tdm is unable to assign credit to more than the 10 rankers from which the multileaved documents originated .",
    "figure  [ fig : bias ] tests if the multileaving methods are biased . in section  [ sec : related ]",
    ", we showed that pm could exhibit bias under certain conditions .",
    "for this experiment we use a random click model , i.e. clicks are random and independent of document relevance . in this case , we expect that the elements of the pairwise preference matrix should converge to 0.5 , i.e. there is no observed preference between rankers @xmath50 and @xmath51 . in this case ,",
    "an error is declared if the value of @xmath46 deviates from 0.5 by more than 0.03 .",
    "figure  [ fig : bias ] shows that pm exhibits very strong bias , with error rates of about 60% .",
    "tdm s behaviour is much better , but after 2000 iterations some bias is still present . in contrast , the percentage error decreases much quicker in sosm , and is almost zero after just 2000 iterations .",
    "we identified and experimentally verified weaknesses in the scalability of tdm and the unbiasedness of pm .",
    "we then proposed a new algorithm , sosm , that corrects these problems .",
    "experimental results using simulated users ( perfect , navigational , informational click models ) , on three different datasets confirmed that ( i ) sosm scales well with the number of rankers to be multileaved , ( ii ) is unbiased , and ( iii ) has significantly less error than prior methods . in some cases error rates",
    "were reduced by half .",
    "the residual error needs investigating but is likely to be partly due to ( i ) establishing a ground truth based on ndcg@10 , which is not used as the scoring function in equation  [ eq2 ] , and ( ii ) the ground truth data is computed on `` test '' data that is not used during the multileave experiments .",
    "a.  schuth , r .- j .",
    "bruintjes , f.  bttner , j.  van doorn , c.  groenland , h.  oosterhuis , c .-",
    "n . tran , b.  veeling , j.  van  der velde , r.  wechsler , et  al .",
    "probabilistic multileave for online retrieval evaluation .",
    "_ sigir _ , pages 955958 , 2015 ."
  ],
  "abstract_text": [
    "<S> online ranker evaluation is a key challenge in information retrieval . </S>",
    "<S> an important task in the online evaluation of rankers is using implicit user feedback for inferring preferences between rankers . </S>",
    "<S> interleaving methods have been found to be efficient and sensitive , i.e. they can quickly detect even small differences in quality . </S>",
    "<S> it has recently been shown that multileaving methods exhibit similar sensitivity but can be more efficient than interleaving methods . </S>",
    "<S> this paper presents empirical results demonstrating that existing multileaving methods either do not scale well with the number of rankers , or , more problematically , can produce results which substantially differ from evaluation measures like ndcg . </S>",
    "<S> the latter problem is caused by the fact that they do not correctly account for the similarities that can occur between rankers being multileaved . </S>",
    "<S> we propose a new multileaving method for handling this problem and demonstrate that it substantially outperforms existing methods , in some cases reducing errors by as much as 50% . </S>"
  ]
}