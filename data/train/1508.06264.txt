{
  "article_text": [
    "in different pattern classification problems , various performances are employed to evaluate the classifiers , including classification accuracy ( acc ) , f1 score , matthews correlation coefficient ( mcc ) , area under the receiver operating characteristic ( roc ) curve ( auc ) and recall - precision break even point ( rp - bep ) of recall - precision curve . due to the nonlinear and nonsmooth nature of many performance measures ,",
    "it is difficult to optimize them directly to learn an optimal classifier . to solve this problem ,",
    "joachims @xcite proposed a support vector machine learning method for multivariate performance measures ( svm@xmath0 ) .",
    "this other method has been applied to optimized some nonlinear multivariate performance measures to learn linear classifiers successfully .",
    "however , it is limited to the learning of linear classifiers .",
    "when data samples of different classes can not be separated by a linear boundary , it is suggested to employ the kernel trick to map the data samples to a nonlinear high - dimensional data space so that a linear boundary could be learned @xcite .",
    "joachims and yu @xcite also extended the svm@xmath0 to its kernel version to handle the nonlinearly distributed data .",
    "one important shortage of this method lies on the choosing of an optimal kernel function with its corresponding parameter . in @xcite ,",
    "the rbf - kernel is used to classification problems on some data sets without any justification , but it is highly doubt if this kernel is suitable for other data sets .",
    "moreover , how the optimal parameter of the kernel function possibly influences the results significantly .",
    "one possible way to solve this problem is to conduct an exhausting linear search or a cross validation in the kernel function and parameter space by using the training set , which is very time - consuming and also makes the learned classifier over - fitting to the training samples .    to solve this problem , we assume that the desired kernel can be obtained by the linear combination of some candidate kernel functions with different kernel parameters",
    ". the optimal kernel is parameterized by the linear combination weights associated with different kernels .",
    "this framework is called multi - kernel learning ( mkl ) since we explore the nonlinear kernel spaces of multiple kernels @xcite . to learn the kernel weights",
    ", we cast the mkl problem with the multivariate performance measures problem , and proposed an unified learning problem for both mkl and multivariate performance measures problems .",
    "for the first time , we propose the problem of learning an optimal kernel for multivariate performance measures , and a novel solution for this problem by learning kernel in multiple kernel spaces simultaneously with optimizing multivariate performance measures .",
    "the rest parts of this paper are organized as follows : in section [ sec : method ] , we introduce the novel method by formulating the problem first , optimizing it then , and developing an iterative algorithm finally , in section [ sec : exp ] , the proposed method is evaluated on some benchmark data sets , and in section [ sec : con ] , the paper is concluded .",
    "we assume we have a training data set with @xmath1 training samples , and the training samples are organized in an training matrix @xmath2\\in \\mathbb{r}^{d\\times n}$ ] , where the @xmath3-th column @xmath4 is the @xmath5-dimensional feature vector of the @xmath3-th training sample .",
    "moreover , we also organize the class labels in a class label vector @xmath6^\\top\\in \\{+1,-1\\}^{n}$ ] , where @xmath7 is the binary class label of the @xmath3-th training sample . under the framework of kernel learning @xcite",
    ", an sample vector can be mapped into a high dimensional nonlinear hilbert space , via a implicit mapping function @xmath8 , where @xmath9 is the dimension of the hilbert space .",
    "the mapping function is explored by a kernel function , which is defined as the dot - produce of the mapping of two samples @xmath4 and @xmath10 , as @xmath11 . in the multi - kernel learning framework",
    ", we may have several such hilbert spaces available and there corresponding nonlinear mapping functions are denoted as @xmath12 , where @xmath13 is the number of hilbert spaces , @xmath14 is the nonlinear mapping function of the @xmath15-th mapping function , and @xmath16 is the dimension of the @xmath15-th hilbert space .",
    "we also define the kernel function for the @xmath15-th hilbert space as @xmath17",
    ". we weight and concatenate the mapping function to form a longer vector in a more general hilbert space , @xmath18^\\top \\in \\mathbb{r}^{d ' } $ ] where @xmath19 is the nonnegative weight for the @xmath15-th hilbert space , @xmath20^\\top \\in \\mathbb{r}_+^m$ ] is the weight vector , and @xmath21 is the dimension of the general hilbert space . its corresponding kernel function is given as    @xmath22    it can be seen that the kernel function is also a weighted linear combination of the @xmath13 kernel functions of the @xmath13 hilbert spaces .",
    "we map all the samples to the hilbert spaces , and organize the mapping results in a @xmath23 matrix as @xmath24\\in \\mathbb{r}^{d'\\times n}$ ] .",
    "we can also apply the kernel function to the matrix and obtain the @xmath25 kernel matrix @xmath26 , where @xmath27\\in \\mathbb{r}^{n\\times n}$ ] is the kernel matrix of the @xmath15-th hilbert space .",
    "we consider the problem of learning a hypotheses function @xmath28 which maps a tuple of @xmath1 samples organized in a data matrix @xmath29 to a label vector of @xmath1 labels @xmath30 . to this end",
    ", we first map the data matrix @xmath29 to the general hilbert space @xmath31 , and then apply a linear discriminant function of the following form    @xmath32    where @xmath33 is the parameter vector . actually , it is equal to the following prediction results ,    @xmath34    where @xmath35 is an element - wise @xmath36 operation function .    to avoid the over - fitting problem",
    ", we try to reduce the complexity of the hypotheses function parameter @xmath37 by minimizing the squared @xmath38 norm ,    @xmath39    we also want to reduce the prediction error of the hypotheses function on the training set . to measure the prediction error , a loss function can be applied to compare the true class label tuple @xmath30 against the output of the hypotheses function @xmath28 .",
    "the following optimization problem is obtained with a @xmath40 ,    @xmath41    instead of trying to optimize @xmath40 directly , we try to find its upper boundary and then minimize its upper boundary . given ( [ equ : h_w ] ) , we have the following inequalities ,    @xmath42    thus we have the upper boundary of @xmath40 , and the optimization problem in ( [ equ : delta1 ] ) can be relaxed to    @xmath43    we further relax the minimization of @xmath44 to the minimization of its upper boundary , which could be obtained by exploring the class label tuple space excluding @xmath30 , @xmath45 ,    @xmath46 \\end{aligned}\\ ] ]    thus we can translate the problem in ( [ equ : delta2 ] ) to ( [ equ : delta3 ] ) ,",
    "@xmath47 \\right \\}. \\end{aligned}\\ ] ]    it could be further relaxed by introducing a nonnegative slack variable @xmath48 to represent the upper boundary , so that the problem could be rewritten as    @xmath49    combining the problems in ( [ equ : h_w ] ) and ( [ equ : delta4 ] ) , and introducing constrains on @xmath50 to prevent negative kernel weights , the following overall optimization problem ,    @xmath51    where @xmath52 is a tradeoff parameter .      to optimize this problem",
    ", we give the primal lagrangian function as follows ,    @xmath53    where @xmath54 , @xmath55 , @xmath56 and @xmath57 are the lagrange multipliers .",
    "we argue the following dual optimization problem ,    @xmath58    by setting the digestives of the lagrange function with regard to @xmath37 and @xmath48 to zero , we have    @xmath59    by substituting these results and the kernel definition in ( [ equ : ker1 ] ) to ( [ equ : lag1 ] ) , we obtain the dual lagrangian function ,    @xmath60    this optimization problem is then transformed to    @xmath61    to solve this problem , we adopt an alternate optimization strategy .",
    "in an iterative algorithm , @xmath62 and @xmath50 with its lagrange multipliers @xmath63 and @xmath64 are optimized alternately .    *",
    "* optimizing @xmath62 * by fixing @xmath50 with its lagrange multipliers @xmath63 and @xmath64 , and only considering @xmath62 , the optimization problem in ( [ equ : lag2 ] ) is reduced to + @xmath65 + this problem can be solved as a quadratic programming problem . * * solving @xmath50 * by fixing @xmath62 , and only considering @xmath50 and its lagrange multipliers @xmath63 and @xmath64 , we have the following problem , + @xmath66 + this is the dual form of a constrained quadratic programming problem , and we can solve it as a constrained quadratic programming problem . *",
    "* updating @xmath67 * moreover , it should be noted that the construction of set @xmath67 is also a problem . to this end",
    ", we propose to construct @xmath67 sequentially in the iterative algorithm .",
    "we propose to construct @xmath67 by adding one new class label tuple to @xmath67 in each iteration according to updated @xmath37 and @xmath50 , + @xmath68 + where @xmath69^\\top\\in \\mathbb{r}^{n \\times 1}$ ] .",
    "then we can update @xmath67 by adding @xmath70 to it , + @xmath71      the iterative multi - kernel learning algorithm to optimize multivariate performance measure is summarized in algorithm [ alg : mkmpm ] .",
    "* input * : training sample feature matrix @xmath29 , and corresponding class label tuple @xmath30 ;    initialize @xmath72 and @xmath73 ;    initialize @xmath74 ;    obtain a predicted class label tuple @xmath70 as in ( [ equ : label1 ] ) by fixing @xmath75 and @xmath76 , and add it to @xmath67 as in ( [ equ : y ] ) ;    update @xmath77 by solving ( [ equ : alpha1 ] ) and fixing @xmath76 ;    update @xmath78 by solving ( [ equ : tau1 ] ) and fixing @xmath77 ;    * output * : output the learned @xmath79 and @xmath80 .",
    "in the first experiment , we perform the proposed to the problem of allergen prediction to optimized various prediction performance measures @xcite .      in this experiment",
    ", we used a dataset constructed by dang and lawrence @xcite .",
    "this dataset contains 42,977 protein sequences , 3,907 of them are allergens while the remaining 39,070 are non - allergens . to extract feature from each protein sequence",
    ", we used the bag - of - words method @xcite .",
    "firstly , the amino acid sequence of a protein is broken to some overlapping peptides with a small sliding window , and each peptides is treated as a word . to conduct the experiment , we perform the popular 10-fold cross validation .",
    "various performance measures are considered in this experiment .",
    "the multivariate performance measures are optimized on the training set and tested on the test set , including auc , rp - bep , acc , f score and mcc .",
    "+    we compare the proposed multi - kernel learning based multivariate performance measures optimization algorithm agains the original kernel version of svm@xmath0 , cutting - plane subspace pursuit ( cpsp ) algorithm @xcite .",
    "moreover , three different variations of svm@xmath0 are also compared as the state - of - the - art multivariate performance measures optimization methods , including the performance measure optimization method by classifier adaptation ( capo ) @xcite , the feature selection method for multivariate performance measures optimization ( fspo ) @xcite , and the non - decomposable loss functions optimization method ( ndlo ) @xcite .",
    "we used these methods to optimize the multivariate performances of auc of roc , pr - bep of recall - precision curve , acc , f score , and mcc respectively on the training set , and the test them on the test set .",
    "the boxplots of the corresponding performance measures of 10-fold cross validations are given in figure [ fig : allergen ] . from this figure",
    ", we can see clearly that the proposed multi - kernel based multivariate performance measure optimization method achieves the best results with regard to different performance measures .",
    "similar phenomenon can be observed in figure [ fig : allergen_mcc ] , and mklpo is the only algorithm which obtains a higher mcc median value than 0.900 . for other performance measures ,",
    "mklpo also optimize them to achieve the best performances measures on the test sets . among the compared algorithms , both cpsp and capo",
    "are improved by using kernel trickles .",
    "however , due to the limitation of single kernel , their performance are not necessarily superior to the linear models , fspo and ndlo . in most cases ,",
    "their performances are comparable to each other .      in this experiment",
    ", we test the proposed algorithm for the automatic assessment of rehabilitative speech treatment .      in this experiment",
    ", we use the dataset provided by tsanas et al .",
    "there are @xmath81 phonations in the data set .",
    "a speech expert is employed to assess the phonations , and label them as  acceptable \" or  unacceptable \" . among the 126 phonations ,",
    "42 is labeled as  acceptable \" while the remaining 84 is labeled as  unacceptable \" .",
    "each phonation is defined as a data sample in the problem of pattern classification , and  acceptable \" phonation is defined as positive sample , while  unacceptable \" phonation as negative sample . for the purpose of pattern classification ,",
    "we extract features from each of the phonations .",
    "to conduct the experiment , we also use the 10-fold cross validation .",
    "the multivariate performance measures are optimized on the training set and tested on the test set , including auc , rp - bep , acc , f score and mcc",
    ".       +    fig .",
    "[ fig : speech ] shows the boxplots of optimized multivariate performance measures of 10-fold cross validations by using rehabilitative speech treatment assessment data set .",
    "as can be seen , our mklpo algorithm significantly outperforms the other multivariate performance measures optimization algorithms in most cases . the performance difference is larger as the mcc is optimized as the desired multivariate performance measure .",
    "the capo algorithm outperforms other algorithms in most cases slightly besides the proposed mklpo algorithm .",
    "this result is consistent with the experiment results given in the previous section .",
    "recently a multivariate performance measures optimization method is proposed to estimate a given complex multivariate performance measure as a linear function .",
    "this method is based on kernel trick . however , it is difficult to choose a suitable kernel function with its corresponding parameter . to solve this problem , in this paper , we proposed the first multi - kernel learning based algorithm for the problem of optimization of multivariate performance measures .",
    "we build a unified objective function for the learning of both multiple kernel weight and classifier parameter for the purpose of multivariate performance measure .",
    "an iterative algorithm is developed to optimize the objective function .",
    "the experiment results on two different pattern classification problems show that the proposed algorithm outperforms the state - of - the - art multivariate performance measure optimization methods . in the future",
    ", we will also explore the potential of using the proposed methods to bioinformatics problems @xcite , integrated circuit design @xcite , multiple model big data analysis @xcite , software and network security @xcite , and power systems optimization @xcite .",
    "moreover , we will also improve the proposed method by regularizing the learning of classifier by graphs @xcite .",
    "t.  joachims , `` a support vector method for multivariate performance measures , '' in _ proceedings of the 22nd international conference on machine learning_.1em plus 0.5em minus 0.4emacm , 2005 , pp .",
    "377384 .",
    "y.  wang , p.  chen , and y.  jin , `` trajectory planning for an unmanned ground vehicle group using augmented particle swarm optimization in a dynamic environment , '' in _ systems , man and cybernetics , 2009 .",
    "ieee international conference on_.1em plus 0.5em minus 0.4emieee , 2009 , pp .",
    "43414346 .",
    "h.  wang and j.  wang , `` an effective image representation method using kernel classification , '' in _ 2014 ieee 26th international conference on tools with artificial intelligence ( ictai 2014 ) _ , 2014 , pp .",
    "853858 .",
    "j.  j .- y .",
    "wang , h.  bensmail , and x.  gao , `` joint learning and weighting of visual vocabulary for bag - of - feature based tissue classification , '' _ pattern recognition _ ,",
    "46 , no .  12 , pp . 32493255 , 2013 .",
    "n.  li , i.  tsang , and z .- h .",
    "zhou , `` efficient optimization of performance measures by classifier adaptation , '' _ ieee transactions on pattern analysis and machine intelligence _ ,",
    "35 , no .  6 , pp .",
    "13701382 , 2013 .",
    "m.  ranjbar , t.  lan , y.  wang , s.  n. robinovitch , z .-",
    "n . li , and g.  mori , `` optimizing nondecomposable loss functions in structured prediction , '' _ pattern analysis and machine intelligence , ieee transactions on _ , vol .",
    "35 , no .  4 , pp . 911924 , 2013 .",
    "a.  tsanas , m.  a. little , c.  fox , and l.  o. ramig , `` objective automatic assessment of rehabilitative speech treatment in parkinson s disease , '' _ ieee transactions on neural systems and rehabilitation engineering _ , vol .",
    "22 , no .  1 ,",
    "181  190 , 2014 .",
    "h.  chen and e.  ruckenstein , `` formation and degradation of multicomponent multicore micelles : insights from dissipative particle dynamics simulations , '' _ langmuir _ , vol .",
    "29 , no .  18 , pp .",
    "54285434 , 2013 .",
    "j.  wang , y.  li , q.  wang , x.  you , j.  man , c.  wang , and x.  gao , `` proclusensem : predicting membrane protein types by fusing different modes of pseudo amino acid composition , '' _ computers in biology and medicine _",
    "42 , no .  5 , pp . 564574 , 2012 .",
    "f.  yi , i.  moon , and y.  h. lee , `` three - dimensional counting of morphologically normal human red blood cells via digital holographic microscopy , '' _ journal of biomedical optics _ , vol .",
    "20 , no .  1 , pp . 016005016005 , 2015 .",
    "y.  wang , h .- c .",
    "han , j.  y. yang , m.  l. lindsey , and y.  jin , `` a conceptual cellular interaction model of left ventricular remodelling post - mi : dynamic network with exit - entry competition strategy , '' _ bmc systems biology _ , vol .  4 , no .",
    "suppl 1 , p.  s5 , 2010 .",
    "b.  peng , y.  liu , y.  zhou , l.  yang , g.  zhang , and y.  liu , `` modeling nanoparticle targeting to a vascular surface in shear flow through diffusive particle dynamics , '' _ nanoscale research letters _ , vol .",
    "10 , no .  1 ,",
    ", 2015 .",
    "y.  zhou , w.  hu , b.  peng , and y.  liu , `` biomarker binding on an antibody - functionalized biosensor surface : the influence of surface properties , electric field , and coating density , '' _ the journal of physical chemistry c _ , vol .",
    "118 , no .",
    "26 , pp . 1458614594 , 2014 .",
    "l.  zhang , j.  zhuge , y.  wang , r.  huang , c.  liu , d.  wu , z.  kang , d .- w .",
    "kim , and d.  park , `` new insights into oxide traps characterization in gate - all - around nanowire transistors with tin metal gates based on combined i g - i d rts technique , '' in _ vlsi technology , 2009 symposium on_.1em plus 0.5em minus 0.4emieee , 2009 , pp",
    ". 4647 .",
    "l.  zhang , r.  wang , j.  zhuge , r.  huang , d .- w .",
    "kim , d.  park , and y.  wang , `` impacts of non - negligible electron trapping / detrapping on the nbti characteristics in silicon nanowire transistors with tin metal gates , '' in _ 2008 ieee international electron devices meeting _",
    ", 2008 , pp . 14 .",
    "l.  zhang , m.  gungi , and p.  c. mcintyre , `` germanium channel p - mosfet with tio2/al2o3 bilayer high - k gate stacks and solutions for metal / tio2 interface stability , '' in _ silicon - germanium technology and device meeting ( istdm ) , 2012 international_.1em plus 0.5em minus 0.4emieee , 2012 , pp . 12 .",
    "l.  zhang , z.  kang , r.  wang , and r.  huang , `` a comprehensive study on schottky barrier nanowire transistors ( sb - nwts ) : principle , physical limits and parameter fluctuations , '' in _ solid - state and integrated - circuit technology , 2008 .",
    "icsict 2008 .",
    "9th international conference on_.1em plus 0.5em minus 0.4emieee , 2008 , pp .",
    "157160 .",
    "j.  zhuge , l.  zhang , r.  wang , r.  huang , d .- w .",
    "kim , d.  park , and y.  wang , `` random telegraph signal noise in gate - all - around silicon nanowire transistors featuring coulomb - blockade characteristics , '' _ applied physics letters _ , vol .",
    "94 , no .  8 , p. 083503",
    ", 2009 .",
    "z.  kang , l.  zhang , r.  wang , and r.  huang , `` investigations on the physical limitation and electrostatic improvement of a gate - all - around silicon nanowire transistor with schottky barrier source / drain , '' _ semiconductor science and technology _ , vol .  24 , no .  10 , p. 105001",
    ", 2009 .",
    "r.  wang , r.  huang , l.  zhang , h.  liu , d .- w .",
    "kim , d.  park , and y.  wang , `` experimental investigations on channel backscattering characteristics of gate - all - around silicon nanowire transistors from top - down approach , '' _ applied physics letters _",
    "93 , no .  8 , p. 083513",
    ", 2008 .",
    "j.  wang , y.  zhou , k.  duan , j.  j .- y .",
    "wang , and h.  bensmail , `` supervised cross - modal factor analysis for multiple modal data classification , '' in _ systems , man and cybernetics ( smc ) , 2015 ieee international conference on_.1em plus 0.5em minus 0.4emieee , 2015 .",
    "t.  li , x.  zhou , k.  brandstatter , d.  zhao , k.  wang , a.  rajendran , z.  zhang , and i.  raicu , `` zht : a light - weight reliable persistent dynamic scalable zero - hop distributed hash table , '' in _ parallel & distributed processing ( ipdps ) , 2013 ieee 27th international symposium on_.1em plus 0.5em minus 0.4emieee , 2013 , pp .",
    "775787 .",
    "t.  li , x.  zhou , k.  brandstatter , and i.  raicu , `` distributed key - value store on hpc and cloud systems , '' in _",
    "2nd greater chicago area system research workshop ( gcasr)_.1em plus 0.5em minus 0.4emciteseer , 2013 .",
    "k.  wang , a.  kulkarni , x.  zhou , m.  lang , and i.  raicu , `` using simulation to explore distributed key - value stores for exascale system services , '' in _",
    "2nd greater chicago area system research workshop ( gcasr ) _ , 2013 .",
    "k.  wang , m.  lang , x.  zhou , b.  mcclelland , k.  qiao , and i.  raicu , `` towards scalable distributed workload manager with monitoring - based weakly consistent resource stealing , '' in _",
    "hpdc 15 proceedings of the 24th international symposium on high - performance parallel and distributed computing _ , 2015 , pp .",
    "219222 .",
    "k.  wang , x.  zhou , h.  chen , m.  lang , and i.  raicu , `` next generation job management systems for extreme - scale ensemble computing , '' in _ proceedings of the 23rd international symposium on high - performance parallel and distributed computing _ , 2014 , pp .",
    "111114 .",
    "k.  wang , x.  zhou , t.  li , d.  zhao , m.  lang , and i.  raicu , `` optimizing load balancing and data - locality with data - aware scheduling , '' in _ big data ( big data ) , 2014 ieee international conference on _ , 2014 , pp .",
    "119128 .",
    "d.  zhao , z.  zhang , x.  zhou , t.  li , k.  wang , d.  kimpe , p.  carns , r.  ross , and i.  raicu , `` fusionfs : toward supporting data - intensive scientific applications on extreme - scale high - performance computing systems , '' in _ big data ( big data ) , 2014 ieee international conference on _ , 2014 , pp . 6170 .      q.  sun , w.  yu , n.  kochurov , q.  hao , and f.  hu , `` a multi - agent - based intelligent sensor and actuator network design for smart house and home automation , '' _ journal of sensor and actuator networks _ , vol .  2 , no .  3 , pp . 557588 , 2013 .",
    " , `` mobile target scenario recognition via low - cost pyroelectric sensing system : toward a context - enhanced accurate identification , '' _ systems , man , and cybernetics : systems , ieee transactions on _ , vol .  44 , no .  3 , pp .",
    "375384 , 2014 .",
    "s.  zhang , d.  caragea , and x.  ou , `` an empirical study on using the national vulnerability database to predict software vulnerabilities , '' _ lecture notes in computer science ( including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics ) _ , vol .",
    "6860 lncs , no .",
    "part 1 , pp .",
    "217231 , 2011 .",
    "s.  zhang , x.  zhang , and x.  ou , `` after we knew it : empirical study and modeling of cost - effectiveness of exploiting prevalent known vulnerabilities across iaas cloud , '' in _ proceedings of the 9th acm symposium on information , computer and communications security _ , 2014 , pp .",
    "317328 .",
    "s.  zhang , x.  ou , and j.  homer , `` effective network vulnerability assessment through model abstraction , '' in _ detection of intrusions and malware , and vulnerability assessment_.1em plus 0.5em minus 0.4em springer , 2011 , pp .",
    "h.  huang , s.  zhang , x.  ou , a.  prakash , and k.  sakallah , `` distilling critical attack graph surface iteratively through minimum - cost sat solving , '' in _ proceedings of the 27th annual computer security applications conference_.1em plus 0.5em minus 0.4emacm , 2011 , pp . 3140 .",
    "r.  zhuang , s.  zhang , a.  bardas , s.  a. deloach , x.  ou , and a.  singhal , `` investigating the application of moving target defenses to network security , '' in _ resilient control systems ( isrcs ) , 2013 6th international symposium on _ , 2013 , pp .",
    "162169 .",
    "wang , i.  almasri , and x.  gao , `` adaptive graph regularized nonnegative matrix factorization via feature selection , '' in _ pattern recognition ( icpr ) , 2012 21st international conference on _ , 2012 , pp .",
    "963966 .",
    "w.  shen and j.  wang , `` transaction costs - aware portfolio optimization via fast lowner - john ellipsoid approximation , '' in _ twenty - ninth aaai conference on artificial intelligence _ , 2015 , pp .",
    "1854  1860 .",
    "l.  wang , j.  engstrm , m.  gteman , and j.  isberg , `` constrained optimal control of a point absorber wave energy converter with linear generator , '' _ journal of renewable and sustainable energy _ ,",
    "vol .  7 , no .  4 , p. 043127",
    ", 2015 .",
    "x.  liu , j.  wang , m.  yin , b.  edwards , and p.  xu , `` supervised learning of sparse context reconstruction coefficients for data representation and classification , '' _ neural computing and applications _ , 2015 .",
    "j.  wang , y.  zhou , m.  yin , s.  chen , and b.  edwards , `` representing data by sparse combination of contextual data points for classification , '' in _ advances in neural networks ",
    "isnn 2015_.1em plus 0.5em minus 0.4emspringer , 2015 ."
  ],
  "abstract_text": [
    "<S> in this paper , we propose a multi - kernel classifier learning algorithm to optimize a given nonlinear and nonsmoonth multivariate classifier performance measure . </S>",
    "<S> moreover , to solve the problem of kernel function selection and kernel parameter tuning , we proposed to construct an optimal kernel by weighted linear combination of some candidate kernels . </S>",
    "<S> the learning of the classifier parameter and the kernel weight are unified in a single objective function considering to minimize the upper boundary of the given multivariate performance measure . </S>",
    "<S> the objective function is optimized with regard to classifier parameter and kernel weight alternately in an iterative algorithm by using cutting plane algorithm . </S>",
    "<S> the developed algorithm is evaluated on two different pattern classification methods with regard to various multivariate performance measure optimization problems . </S>",
    "<S> the experiment results show the proposed algorithm outperforms the competing methods .    </S>",
    "<S> pattern recognition , multiple kernel , multivariate performance measures , cutting plane algorithm </S>"
  ]
}