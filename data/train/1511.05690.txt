{
  "article_text": [
    "rings are algebraic structures in which two operations @xmath9 , generalizations of the standard @xmath10 and @xmath11 , are supported , and where the outcomes of the operations must be found in the set of interest ( _ e.g. _ , the ring @xmath12 on the set of integers states that adding or multiplying any two integers must yield an integer ) .",
    "importantly , the operations @xmath13 and @xmath14 must be invertible : for @xmath6 and @xmath7 in the ring , @xmath15 must be invertible to produce either @xmath6 ( which can be recovered as @xmath16 ) or @xmath7 ( which can be recovered as @xmath17 ) again ( the same is true for the operation @xmath10 , except the case when @xmath18 ) . the more general semirings do not necessarily include this inverse operation .",
    "for example , on the semiring @xmath2 , given only @xmath19 and the value of @xmath7 , it is not possible to retrieve the value of @xmath6 .",
    "the greater generality of semirings makes them important in geometry , optimization , and physics  @xcite .",
    "the seemingly pedantic distinction between semirings and rings becomes more pronounced when considering certain fast algorithms , which can be applied on rings but not on semirings .",
    "a key example of this is fast convolution with the fast fourier transform ( fft ) . on the ring @xmath12",
    ", fft can be used to perform convolution in @xmath20 steps ( superior to the @xmath21 required by a naive convolution algorithm ) ; however , one of the keys to the feasibility of fft convolution is the notion that the polynomial coefficients can be combined into a point representation of the polynomials , which is then operated upon , and then un - combined into the coefficients of the product polynomial ( essentially , operating on them while combined saves substantial time ) .",
    "for this reason , faster - than - naive algorithms for performing `` standard '' convolution ( _ i.e. _ , convolution on the ring @xmath12 ) are very well established numerical methods  @xcite , whereas the first algorithm with worst - case runtime in @xmath22 for max - convolution ( _ i.e. _ , convolution on the semiring @xmath0  @xcite ) was published fairly recently  @xcite and considered by many , including myself , to be a significant breakthrough . however , this sub - quadratic max - convolution algorithm has a runtime that is only slightly lower than quadratic when compared to the @xmath23 fast standard convolution methods mentioned above .",
    "the development of fast algorithms for max - convolution is considered important , because fft convolution - based dynamic programming algorithms can be used to perform fast sum - product statistical inference on sums of two or more random variables  @xcite , but when performing max - product ( _ i.e. _ , _ maximum a posteriori _ ) inference ( _ i.e. _ , computing the best configuration ) the resulting problem is a max convolution , which was previously limited to algorithms significantly slower than fft  @xcite ( with the exception of the binary case @xmath24 , wherein an @xmath23 _ maximum a posteriori _ algorithm based on sorting is possible  @xcite ) .",
    "reminiscent of the disparity between convolution over rings and convolution over semirings is the disparity between matrix multiplication over rings versus matrix multiplication over semirings : where naive matrix multiplication is in @xmath25 , fast matrix multiplication once again performs operations on combined elements and then un - combines them to achieve a runtime in @xmath26 with strassen s algorithm  @xcite , which has since been improved by other algorithms with the same strategy , such as with the @xmath27 coppersmith - winograd algorithm  @xcite , as well as newer variants of improved worst - case runtime  @xcite .",
    "however , the use of that un - combine step ( _ i.e. _ , inverting the @xmath14 operation in this case ) in the faster - than - naive algorithms prevents using such algorithms on semirings .",
    "for example , given an adjacency matrix corresponding to some graph , matrix multiplications on the semiring @xmath28 can be used to perform edge relaxations and find the shortest paths between any two vertices in the graph .",
    "this was initially conceived as a manner of multiplying the adjacency matrix by itself on the semiring @xmath3 to relax edges ( _ i.e. _ , when the path @xmath29 is more efficient than the direct edge @xmath30 , using @xmath31 as the new best distance , which replaces the direct edge ) , thereby making a new adjacency matrix containing all paths of length @xmath32 .",
    "this process could be repeated , with @xmath33 matrix multiplications on the semiring @xmath34 , thereby yielding all most efficient paths of length @xmath35 ( equivalent to the most efficient paths overall , since no optimal path would be longer then @xmath36 edges when each edge weight is in @xmath37 ) , resulting in an algorithm that runs in @xmath38  @xcite .",
    "from there it is trivial to achieve a speedup by computing the iterative matrix multiplication via the powers of two : @xmath39 matrix multiplication over the semiring @xmath3 solves the all - pairs shortest paths problem ( apsp ) in @xmath40 matrix multiplications ( or @xmath41 time ) ; however , existing fast matrix multiplication methods can not be applied to the @xmath3 semiring , and thus speedups substantially below @xmath25 ( the runtime required to solve the apsp problem with the floyd - warshall algorithm  @xcite ) are not achieved by simply utilizing fast matrix multiplication .",
    "in fact , it has been proven that restricting the available operations to @xmath11 and @xmath42 , implies @xmath43 such operations are necessary to solve the problem exactly  @xcite .",
    "the apsp problem is crucially important to many different fields , including applications where the connection to apsp is trivial ( _ e.g. _ , gps driving directions , routing network traffic ) and applications where the connection to apsp is nontrivial ( _ e.g. _ , a fast apsp solution being used within the sub - quadratic max - convolution algorithm of  @xcite ) .",
    "research has focused primarily on exploiting properties of particular graphs ( _ e.g. _ , using the fact that its adjacency matrix is sparse , that the graph is planar , _ etc . _ ) , or in more general cases , approaching the problem in a more combinatorial manner  @xcite .",
    "similar to the apsp problem , the problem of sorting all @xmath44 pairs from lists @xmath6 and @xmath7  @xcite exhibits a combinatorial nature similar to the max - convolution and apsp problems ; indeed , one method for performing max - convolution is to compute the top @xmath4 values in @xmath45 ( where @xmath46 and @xmath47 ) and then fill them in the appropriate indices @xmath48 in the max - convolution .",
    "however , despite its similarities to the two other problems , this particular problem poses a less clear parallel example where a fast algorithm is available for rings ( rather than semirings ) .",
    "all known exact approaches for this problem are in @xmath49 , the same as the cost of the naive algorithm ( which computes and sorts pairs )  @xcite .",
    "but the variant where only the top @xmath4 values are of interest is more complicated , since the top value is trivial ( it is the maximum element in @xmath6 plus the maximum element in @xmath7 ) and since the indices considered grow rapidly as @xmath4 increases : if the lists are first ordered so that @xmath50 and @xmath51 , then @xmath52 achieves the maximum , but either @xmath53 or @xmath54 achieves the second highest , and the third highest will be in @xmath55 . clearly , the values considered by this scheme grow in a combinatorial manner .",
    "this paper draws upon recent work by which fast ring - based algorithms ( _ i.e. _ , algorithms that are only appropriate for use on rings ) are used to approximate results on semirings ( to which those fast algorithms can not be applied ) via @xmath8-norm rings , a strategy previously outlined for the max - convolution problem . as noted above , until recently no known algorithms for max - convolution",
    "were even remotely as fast as fft convolution in practice . here , the method for achieving a numerical estimate of the max - convolution in @xmath20 time  @xcite , is reviewed .",
    "the exact max - convolution between two vectors @xmath6 and @xmath7 is defined as follows : @xmath56 where @xmath57 is a vector defined such that @xmath58 .",
    "thus , it is possible to see the problem by first filling in lists @xmath59 , and then the max - convolution result ( denoted @xmath60 above ) at index @xmath61 will be the maximum value found in list @xmath57 .",
    "of course , as described in this naive formulation , the runtime is still quadratic ; however it was previously noted that when the elements of @xmath6 and @xmath7 are nonnegative , the maximum over each vector @xmath57 could be found by exploiting the equivalence between the maximum and the chebyshev norm @xmath62 , and then using that to numerically approximate the chebyshev norm with a @xmath63-norm , where @xmath63 is a large numerical value : @xmath64 when @xmath65  @xcite .",
    "when using a fixed value of @xmath63 , @xmath57 can be expanded back into its constituent pieces @xmath66 thus , it is apparent that the max - convolution can be performed by taking every element of @xmath67 to the @xmath63 and taking every @xmath68 to the @xmath63 , convolving them regularly ( in @xmath69 time using fft convolution ) , and then taking every element in that convolution result to the power @xmath70 .",
    "thus , a fast approximation of max - convolution is computed in @xmath69 time ( and with a very fast runtime constant , because the algorithm can make use of efficient existing fft libraries ) .",
    "because of numerical instability when @xmath65 ( which is limited to underflow if the input problem is scaled so that all @xmath71 $ ] and @xmath72 $ ] ) , a piecewise approach that considered @xmath73 was used , and the highest numerically stable value of @xmath63 is used at each index .",
    "this is accomplished by computing a single fft - based max - convolution for each @xmath63 and then at each index @xmath61 in the result , finding the highest @xmath63 that produces a result at that index where @xmath74 ( where @xmath75 is the error threshold for the convolution algorithm ) . because @xmath76 is an upper bound of @xmath77 , when @xmath78 , then @xmath79 , and thus the algorithm does not suffer critical underflow  @xcite .",
    "this achieves a stable estimate with bounded error when @xmath80 , and thus the full procedure ( over all @xmath63 considered ) can be performed in @xmath81 .    when the inputs are @xmath82 $ ] , the worst - case relative error has been bounded  @xcite :    @xmath83    where @xmath63 is the largest @xmath84 that produces a numerically stable result . by dividing the estimate by the maximum possible value of @xmath85 , ( _ i.e. _ , dividing by @xmath86 ) the worst - case relative error",
    "can be decreased to @xmath87 .    but even more significantly",
    ", rather than simply use @xmath88 to approximate the maximum ,  @xcite also proposed a method for using the _ shape _ of the @xmath63 vs. @xmath89 curve to estimate @xmath90 .",
    "there are multiple ways to estimate @xmath90 from this curve , but one way that achieves a good balance between accuracy and efficiency is to model the norm sequence by representing the unique elements in @xmath57 as a multiset and then projecting down to a smaller number of elements :    @xmath91    where each @xmath92 is one of the @xmath93 unique values in @xmath57 and @xmath94 is the number of occurences of @xmath92 . from this perspective , it is possible to use empirically observed values of @xmath89 ( for a few different @xmath84 ) and then project the @xmath93 unique @xmath92 values and their respective counts @xmath94 down to a smaller number of @xmath95 unique @xmath96 values and their respective counts @xmath97 :    @xmath98    given @xmath99 norms from evenly spaced norms @xmath100 , the projection onto @xmath101 unique values @xmath102 has been shown to be zeros of the polynomial @xmath103 ( these zeros can be found by solving for the roots of the polynomial @xmath104 and then taking each root to the power @xmath70 ) , where @xmath105 , the coefficients of the polynomial , are defined by @xmath106 \\in null\\left ( \\left [ \\begin{array}{ccccc } \\| u^{(m ) } \\|_{p^*}^{p^ * } & \\| u^{(m ) } \\|_{2 { p^*}}^{2 { p^ * } } & \\| u^{(m ) } \\|_{3 { p^*}}^{3 { p^ * } } & \\cdots & \\| u^{(m ) } \\|_{(r+1 ) { p^*}}^{(r+1 ) { p^ * } } \\\\ \\| u^{(m ) } \\|_{2 { p^*}}^{2 { p^ * } } & \\| u^{(m ) } \\|_{3 { p^*}}^{3 { p^ * } } & \\| u^{(m ) } \\|_{4 { p^*}}^{4 { p^ * } } & \\cdots & \\| u^{(m ) } \\|_{(r+2 ) { p^*}}^{(r+2 ) { p^ * } } \\\\",
    "\\| u^{(m ) } \\|_{3 { p^*}}^{3 { p^ * } } & \\| u^{(m ) } \\|_{4 { p^*}}^{4 { p^ * } } & \\| u^{(m ) } \\|_{5 { p^*}}^{5 { p^ * } } & \\cdots & \\| u^{(m ) } \\|_{(r+3 ) { p^*}}^{(r+3 ) { p^ * } } \\\\ \\vdots & \\vdots & \\vdots & & \\vdots \\\\",
    "\\| u^{(m ) } \\|_{(r ) { p^*}}^{(r ) { p^ * } } & \\| u^{(m ) } \\|_{(r+1 ) { p^*}}^{(r+1 ) { p^ * } } & \\| u^{(m ) } \\|_{(r+2 ) { p^*}}^{(r+2 ) { p^ * } } & \\cdots & \\| u^{(m ) } \\|_{2 r { p^*}}^{2 r { p^ * } }",
    "\\\\ \\end{array } \\right ] \\right).\\ ] ] the maximum value in @xmath57 can thus be estimated as @xmath107 .",
    "when @xmath108 , this projection will be of the form @xmath109 for some vector @xmath110 with elements in @xmath111 $ ] and where at least one element equals @xmath112 ( w.l.o.g .",
    ", let @xmath113 ) . therefore , the worst - case relative error takes the form @xmath114 and will be maximized when the estimate @xmath115 attains a minimum , which corresponds to minimizing @xmath116 . aside from boundary points on the @xmath111 $ ] hypercube , those extrema will occur when @xmath117 or equivalently , @xmath118 because @xmath119 and @xmath113 , then the denominator @xmath120 , and it is therefore possible to exploit symmetry between the equations from two different partial derivatives @xmath121 ( where neither @xmath122 nor @xmath123 is @xmath112 , because @xmath124 is now a constant ) : @xmath125 therefore , a critical point for @xmath110 contains at most two unique values ( including @xmath124 ) .",
    "the value @xmath126 takes the form @xmath127 .",
    "the extrema can now be found by optimizing with respect to @xmath128 , which yields @xmath129 , for which @xmath130 .",
    "therefore , the worst - case relative error with the @xmath108 projection is bounded by @xmath131 .    with @xmath132",
    ", the projection likewise has a closed form ( described by @xcite ) , and empirical evidence suggests that the worst - case error will be achieved with three unique values in @xmath57 ( meaning there will likewise be three unique values in @xmath110 , including @xmath113 ) .",
    "although the ability to achieve the worst - case error for @xmath132 projection using three unique values has not been proven , if it were true , it would imply that the worst - case relative error for the @xmath132 projection is @xmath133 , meaning that it no longer depends on @xmath36 .",
    "essentially the projection methods use the shape of the @xmath63 vs. @xmath89 curve to estimate the maximum value in @xmath57 .",
    "furthermore , when only a constant number of @xmath63 are considered ( @xmath134 ) , numerical max - convolution can be approximated numerically in @xmath23 with a practical runtime only slightly slower than standard fft convolution .",
    "for the sake of simplicity , this manuscript abstracts the projection step into a black box function @xmath135 , which accepts a spectrum of norms of some vector @xmath57 and then uses them to estimate the maximum value in @xmath57 .",
    "this manuscript demonstrates that the fast max - convolution method described above generalizes as a strategy for all problems on semirings isomorphic to the semiring @xmath0 on nonnegative values . by exploiting the fact that a sequence of @xmath12 computations in different @xmath63-norms can be used to estimate the results on the semiring @xmath0",
    ", high - quality approximations can be achieved using off - the - shelf fast algorithms limited to rings .",
    "in addition to the max - convolution method previously described , the generalized approach is demonstrated on the two other difficult semiring problems : the apsp problem and finding the top @xmath4 values in @xmath44 . using a family of @xmath136 many @xmath63-norms for @xmath137",
    ", the ring @xmath12 defined for each @xmath63-norm can be solved using faster - than - naive algorithms ( which can be applied because operations are performed in a @xmath12 ring ) .",
    "then , for any index in the result , the sequence of results at each @xmath63-norm can be used to approximate the @xmath138 semiring . using this general approach ,",
    "it is possible to compute a high - quality approximation for the apsp problem in sub - cubic time and compute a high - quality approximation of the top @xmath4 values in @xmath44 ( including estimates of their indices @xmath139 , which can not be computed by any other known approximation strategies ) in @xmath140 .",
    "the outline of the approach presented here is as follows : @xmath141 is a naive algorithm of interest that inefficiently solves a problem on the semiring @xmath0 .",
    "@xmath142 is an identical algorithm on the ring @xmath12 .",
    "@xmath143 is an algorithm that produces an equivalent result to @xmath142 , but does so in a faster manner ( _ e.g. _ , fft convolution , strassen matrix multiplication , _ etc . _ ) .",
    "it is then demonstrated that , for a family of @xmath63 values , @xmath142 can be used to compute a sequence of @xmath63-norms , which can be used to approximate @xmath141 .",
    "the same approximation can therefore be achieved using @xmath143 instead of @xmath142 , since @xmath143 exhibits identical behavior to @xmath142 ( even if it internally performs computations in a manner completely different from @xmath142 ) .",
    "the semiring of interest defines @xmath0 on the real numbers .",
    "let @xmath144 be a collection of nonnegative real - valued inputs .",
    "let @xmath141 be a finite sequence of operations from the semiring of interest ( _ i.e. _ , @xmath145 and @xmath10 operations ) on the values in collection @xmath144 , which returns a new collection of those results @xmath146 .",
    "let @xmath142 be a finite sequence of operations created by replacing every @xmath145 operation in @xmath141 with a @xmath11 operation .",
    "where calling @xmath146 returned some collection , now @xmath147 will return a collection of identical shape ( _ i.e. _ , the return values can be thought of as tensors of the same dimension and shape ) .",
    "let @xmath143 be a finite sequence of operations where , for every valid input @xmath144 , @xmath148 returns a result numerically indistinguishable to calling @xmath147 . because many fast algorithms used for @xmath143 will have less numerical stability than their naive counterparts , stipulate that @xmath149 ( _ i.e. _",
    ", the result value at some index @xmath123 of the result tensor is numerically indistinguishable ) whenever the result @xmath150 has experienced neither critical underflow nor overflow . in summary , stipulate that @xmath149 whenever @xmath150 approaches neither zero nor infinity . if the means by which overflow occurs is limited ( this is achieved by scaling the inputs in @xmath111 $ ] , described later in this manuscript ) , then it is sufficient to stipulate that @xmath150 is accurate when sufficiently greater than zero . in other words , @xmath151 when @xmath152 , where @xmath75 is a value that depends on the numeric stability of @xmath143 .",
    "now consider that any sequence of operations creating an expression in @xmath141 can be distributed and rearranged into an equivalent expression : @xmath153 .",
    "therefore , any of the results computed by @xmath141 must be of the form @xmath154 , where the @xmath155 values are the results of finite numbers of products of the inputs .",
    "note that the ring @xmath12 possesses a similar property : @xmath156 , implying any results from @xmath142 or @xmath143 will be equal to an expression of the form @xmath157 where the @xmath155 values are the result of finite numbers of products on the inputs . because the @xmath158 values are products of combinations of the inputs , then calling @xmath159 ( where @xmath160 denotes taking each element of @xmath144 to the power @xmath63 ) will produce results of the form @xmath161 , because @xmath162 .",
    "when taking the input values to large powers @xmath63 , values larger than @xmath112 quickly become large and values smaller than @xmath112 quickly approach zero .",
    "for this reason , let @xmath163 then the error directly introduced by moving to the @xmath63-norm ring will be limited to underflow , since all inputs will be scaled to values in @xmath111 $ ] . for any given result at index @xmath123 , values of @xmath63 that result in critical levels of underflow will be excluded by verifying that @xmath164 . because overflow is no longer considered at index @xmath123 , and because critical underflow has been ruled out by the definitions of @xmath143 and @xmath75 chosen above , then it follows that @xmath165 . for example",
    ", when using a high - quality fft library for fast convolution of vectors with elements in @xmath111 $ ] and of length @xmath36 ( where @xmath36 is small enough to permit storing the vectors in ram on current computers ) , a result value at some index @xmath123 where fft convolution is greater than roughly @xmath166 indicates that fft convolution was stable to underflow . and",
    "since significant overflow can not have occurred , this result is numerically stable with respect to underflow and overflow , because overflow was eliminated by first scaling the problem  @xcite .",
    "so if @xmath147 performs the naive convolution between two vectors and @xmath148 performs the fft convolution between the same vectors , @xmath167 at some index @xmath123 implies that @xmath149 , where @xmath168 .",
    "first compute @xmath169 .",
    "then , for any given result index @xmath123 it is possible to produce points on the curve @xmath170 .",
    "this curve can be thought of as a sequence of @xmath63-norms taken to power @xmath63 , where the point paired with @xmath63 is of the form @xmath171    this curve can be used to compute @xmath172 , a numerical estimate of the true maximum at index @xmath123 . if the vector used to define result @xmath123 is denoted @xmath173 , then @xmath174    using this strategy , a faster - than - naive algorithm @xmath143 can be called a constant number of times ( @xmath175 ) to approximate @xmath141 . in the case of the fast numerical approach to max - convolution described in section  [ sec : introduction ] , @xmath141 corresponds to the naive @xmath21 max - convolution algorithm , which performs precisely the desired operations in the most naive manner on the semiring @xmath176 .",
    "@xmath142 corresponds to a naive @xmath21 standard convolution algorithm ; this naive standard convolution algorithm could be used @xmath177 times to perform the appropriate operations in @xmath178 space @xmath179 , and those @xmath63-norm results can later be aggregated to approximate the exact result from @xmath141 .",
    "however , no speedup is achieved when using the algorithm @xmath142 in this manner ; in fact , this process will almost certainly be slower , since the code of @xmath142 is nearly identical to @xmath141 and is called a small number of times for the different @xmath63 values , whereas @xmath141 is only called once . in this case , @xmath143 corresponds to an @xmath23 fft standard convolution algorithm , which is chosen exclusively based on its numerical equivalence to @xmath142 .",
    "for this reason , even if the steps in @xmath143 include operations other than @xmath10 or @xmath11 ( such as with fft convolution ) , its underlying equivalence to @xmath142 still affords much more efficient estimation of the various @xmath63-norms .",
    "these norms can in turn , permit numerical estimation of @xmath141 , even if there is no apparent direct connection between @xmath141 and @xmath143 .",
    "first , the proposed approach is demonstrated on a classic computer science problem , the apsp problem . in the most general case , when the adjacency matrix is dense ( _ i.e. _ , when all pairs of nodes in the graph are joined by an edge ) , sub - cubic runtimes have been achieved by complicated algorithms .",
    "the approach estimates the apsp resultant path lengths in @xmath180 steps , where @xmath181 is the cost of standard floating - point matrix multiplication ( @xmath181 is @xmath25 with naive matrix multiplication , @xmath26 with strassen s multiplication method , @xmath182 with the coppersmith - winograd algorithm , _ etc .",
    "the proposed method can be applied to graphs ( or directed graphs ) with nonnegative edge weights @xmath183 ( _ i.e. _ , where @xmath184 describes the adjacency matrix of the graph ) .",
    "the apsp problem seeks to find the shortest path between every pair of vertices : for any two vertices @xmath185 , the shortest distance computed in the apsp problem would consider all paths from @xmath186 to @xmath110 and find the one with shortest distance , including direct paths , paths that pass through a single other vertex @xmath6 , paths that pass through vertices @xmath6 then @xmath7 , _ etc .",
    "_ : @xmath187 .    here , the input consists of the collection of weights @xmath188 .",
    "define the naive algorithm to iteratively relax edges ( _ i.e. _ , it repeatedly finds shorter edges as it progresses ) by performing @xmath40 matrix multiplications on the semiring @xmath3 .",
    "aside from statically bounded looping instructions ( which could be unrolled for any particular problem ) , each of those matrix multiplications is constructed entirely of operations in the semiring @xmath34 . because the runtime is dominated by the @xmath3 matrix multiplications , it is possible to first simplify by letting @xmath141 be the naive matrix multiplication routine defined on the semiring ( algorithm  [ algorithm : naiveminmatrixmult ] ) .    on a graph with nonnegative edge weights ,",
    "it is trivial to create a bijective problem on @xmath0 , by letting @xmath189 : thus @xmath190 , indicating that @xmath11 operations have been converted to @xmath10 operations . likewise , since @xmath191 is a strictly decreasing function on @xmath37 , then @xmath192 has become @xmath145 in the transformed space ; an equivalent method @xmath193 can be made , which operates on the semiring @xmath0 .",
    "@xmath193 is then used to process transformed inputs ( algorithm  [ algorithm : naivemaxmatrixmult ] ) .",
    "it is now trivial to construct @xmath142 , which performs the same operations as @xmath141 , but where @xmath145 ( equivalent to @xmath192 in the original space ) is replaced by @xmath11 operations ( algorithm  [ algorithm : naivematrixmultplus ] ) .",
    "lastly , @xmath143 is constructed as an algorithm numerically equivalent to @xmath142 that achieves greater speed by using any sub - cubic matrix multiplication algorithm .",
    "therefore , if @xmath194 is called ( _ i.e. _ , @xmath143 is called on scaled inputs ) using a small collection of @xmath63 values , it is possible to create a sequence of norms for each cell @xmath195 in the result , and from that sequence it is possible to estimate the maximal path lengths @xmath196 on @xmath0 .",
    "these estimates can then be transformed back onto @xmath3 by using the inverse transformation @xmath197 .",
    "from there , it is clear that @xmath143 can be chosen as any fast matrix multiplication algorithm ( to that end , this manuscript uses the @xmath26 strassen matrix multiplication algorithm , as shown in algorithm  [ algorithm : strassen ] ) .",
    "the following strategy for max - matrix multiplication is proposed : first , compute the standard matrix multiplication @xmath198 \\cdot\\\\ \\left [ \\begin{array}{cccc } { \\left(y_{0,0}\\right)}^{p^ * } & { \\left(y_{0,1}\\right)}^{p^ * } & \\cdots & { \\left(y_{0,n-1}\\right)}^{p^*}\\\\ { \\left(y_{1,0}\\right)}^{p^ * } & { \\left(y_{1,1}\\right)}^{p^ * } & & { \\left(y_{1,n-1}\\right)}^{p^*}\\\\   \\vdots & & \\ddots & \\\\ { \\left(y_{n-1,0}\\right)}^{p^ * } & { \\left(y_{n-1,1}\\right)}^{p^ * } & & { \\left(y_{n-1,n-1}\\right)}^{p^*}\\\\ \\end{array } \\right]\\end{gathered}\\ ] ] for each @xmath84 ( via strassen s algorithm or any other fast matrix multiplication defined on the ring @xmath12 ) .",
    "then the estimate of the max - matrix multiplication at index @xmath195 is computed via the sequence made from vector norms to the @xmath63 : @xmath199 the min - matrix multiplication results can be computed by transforming back to the semiring @xmath3 , as described above .",
    "thus , by broadcasting into a small number of @xmath178 spaces , it is possible to approximate a single matrix multiplication on the semiring @xmath34 ( algorithm  [ algorithm : subcubicapproxmaxmatrixmult ] ) , and thereby approximate the apsp path lengths in @xmath40 fast matrix multiplications . in this case",
    "( using strassen multiplication ) , the overall runtime achieved is in @xmath200 , and is faster in practice than the @xmath25 floyd - warshall algorithm . as mentioned above",
    ", other fast matrix multiplication algorithms may be used in place of strassen s algorithm .",
    "for example , the coppersmith - winograd algorithm would make the overall runtime @xmath201 . ignoring the runtime constants ( which , it should be noted , can significantly slow down the more advanced faster - than - naive matrix multiplication algorithms in practice ) , when @xmath202 , @xmath203 , whereas @xmath204 .",
    "@xmath205 @xmath206    @xmath207 @xmath206    @xmath208 @xmath206    @xmath209 \\gets x'$ ]    @xmath210 \\gets y'$ ]    @xmath211 + @xmath212 + @xmath213 + @xmath214 + @xmath215 + @xmath216 + @xmath217 + @xmath218 $ ] + @xmath206    @xmath219 @xmath220    @xmath221    @xmath222 @xmath223 @xmath206      the fast rings approximation is also demonstrated on a second well - known computer science problem : sorting a list of all pairs @xmath5 where @xmath6 and @xmath7 are two @xmath36-length lists , is a classic problem in computer science for which no known algorithms achieve runtime superior to the naive @xmath224 approach ; furthermore , this naive @xmath224 approach ( algorithm  [ algorithm : naivexplusy ] ) is the fastest known approach that can also give the indices of @xmath6 and @xmath7 , by generating all tuples of the form @xmath225 and sorting them lexicographically .",
    "note that sorting all @xmath226 pairs @xmath44 would require @xmath224 steps and retrieving the top @xmath4 values from a max - heap would require inserting all @xmath226 values and then dequeuing the top @xmath4 in @xmath227 for an overall runtime in @xmath228 where @xmath229 .",
    "an approximate solution can be achieved by discretizing to integer values and then binning @xmath6 and @xmath7 ( the binned counts of @xmath230 values can be computed by convolving the binned @xmath6 and binned @xmath7 counts with fft convolution ) ; however , this approximation is sensitive to the discretization precision ( in both accuracy and runtime ) and yields only the sorted values @xmath5 and not the corresponding indices @xmath195  @xcite . here a novel numerical approximation to sorting @xmath230 is outlined using the strategies above .",
    "the proposed method can also be used to estimate the top @xmath4 values @xmath5 as well as estimate the indices @xmath195 that produce them . as before with the apsp problem , it is possible to draw an isomorphism between the semirings @xmath2 and @xmath0 : @xmath231 so that @xmath232 , indicating the @xmath11 operation has been converted to a @xmath10 operation .",
    "due to recent advances mentioned above regarding fast max - convolution , it is tempting to find a similarity ; however , max - convolution does not directly solve the top @xmath4 values in @xmath45 : given @xmath233 ( _ i.e. _ , the max - convolution between @xmath6 and @xmath7 ) the largest value in the max - convolution must be the largest value in @xmath45 : @xmath234 .",
    "but the second largest value in the max - convolution does not necessarily belong to the top @xmath4 values of @xmath45 , because the max - convolution at index @xmath61 gives the maximum value over all positive diagonals for which @xmath235 ; if the second largest value in @xmath45 has the same @xmath48 value as the first largest value chosen , then it will be obscured by first choice because @xmath236 , where @xmath57 is a vector that holds all elements @xmath45 along the positive diagonal @xmath48 , and @xmath60 only contains the maximum value of @xmath57 , not the second highest value , third highest value , _",
    "etc._.    it is trivial to see a naive sorting approach that simply performs @xmath237 ; but this sorting method is already obfuscated by the clever optimizations inherent to @xmath23 sorting algorithms . for this reason , a different algorithm @xmath141 is chosen ; this @xmath141 algorithm is equivalent to sorting all pairs @xmath45 , but the chosen definition of @xmath141 eschews the complexity of sophisticated sorting routines in favor of something simpler , although it is slower than the naive approach of generating all @xmath226 @xmath45 pairs and sorting them with an arbitrary @xmath23 algorithm . essentially , the algorithm is equivalent to finding the maximum value along each positive diagonal , and then the maximum value over those maximum values , which gives the next largest value in @xmath45 ( algorithm  [ algorithm : naivextimesy ] ) .",
    "the @xmath57 vectors correspond to the positive - sloping diagonals in the following matrix : @xmath238.\\ ] ] and so @xmath239 , @xmath240 , @xmath241 , @xmath242 .",
    "once the next highest remaining value is computed , that value is removed from future consideration by executing the line @xmath243 , which traverses through the list @xmath57 and removes the first value matching @xmath244 .",
    "overall , this algorithm runs in @xmath25 steps .",
    "@xmath245 @xmath246 @xmath247 @xmath248 , sorted[1 ] , \\ldots sorted[k-1 ] ~)$ ]    @xmath249 @xmath250    @xmath251 @xmath252 @xmath253 @xmath254 @xmath255 @xmath206    next , the algorithm @xmath142 is created from @xmath141 ( algorithm  [ algorithm : naivextimesyplus ] ) ; note that when it is desirable , only a subset of the @xmath145 operations may be replaced by @xmath11 operations . finally , a novel data structure motivated by this fast rings approximation is created for sequentially approximating and removing a maximum from a collection of norms , the normqueue .",
    "this novel data structure is paired with fast max - convolution to construct the faster - than - naive algorithm @xmath143 .",
    "this underscores that it is also possible to perform dynamic programming while in these various @xmath63-norm spaces , and therefore use the result of a computation on the semiring @xmath0 to subsequently alter the program flow .",
    "the normqueue works as follows : for some vector @xmath57 , assume a collection of norms to the @xmath63 is given @xmath256 from which it is possible to estimate the maximum value in @xmath257 .",
    "if the maximum element in @xmath57 occurred at index @xmath258 , then it is now possible to estimate the norms of the vector @xmath259 by subtracting the estimated maximum @xmath260 from each norm : @xmath261 proceeding inductively to iteratively estimate the maximum from the collection of norms and then removing the estimated maximum from those norms . note that the normqueue can be used with only @xmath136 norms , leading to approximate results , but an @xmath136 runtime to pop and reestimate the new max . while this data structure does accumulate error ( because small imperfections in early estimates of the maxima may have ripple effects on later estimates ) , the larger values experience less of this error because they are retrieved first ( limiting the ripple effect of errors accumulated to that point ) and because the norm sequence best summarizes larger values ( they are not the values that endure underflow ) .",
    "@xmath262 @xmath263    @xmath251 @xmath264 @xmath253 @xmath254 @xmath265 @xmath206    in order to derive some @xmath266 that is equivalent to but more optimized than @xmath267 , notice the fact that the @xmath268 vector computed by @xmath269 is equivalent to a standard convolution .",
    "one avenue of attack would be to compute @xmath268 for different @xmath84 ( denoted @xmath270 ) , and then aggregate them to approximate the maximum ( computed as the initial @xmath271 by @xmath272 ) . as noted above",
    ", the max - convolution will not solve this problem alone , since the max - convolution discards the second - highest value in each @xmath57 ( once again , operations on the semiring lose information in a manner that can not be undone ) . however in this case , it is desirable to keep the information in the @xmath63-norm , so that the second - highest value in each @xmath57 leaves some preserved signature .",
    "for this reason , the function @xmath143 is not defined ; instead , the @xmath63-norm aggregation is performed within the same function @xmath273 ; instead of @xmath273 calling @xmath143 ( as in the case of max - convolution where @xmath143 is fft and in max - matrix multiplication where @xmath143 is strassen or some other faster - than - naive matrix multiplication ) , here @xmath274 is described _ ab initio _ so that @xmath63-norm information from the different calls to @xmath143 can be shared ( algorithm  [ algorithm : fastxtimesy ] ) . by doing so",
    ", the method can exploit the fact that if @xmath275 , and so can recompute which value @xmath276 would take if its maximum ( @xmath244 ) were removed by simply subtracting out the term the maximum would contribute to the norms : @xmath277 . in practice , by using the normqueue .",
    "@xmath278    @xmath279 @xmath280 @xmath281    @xmath251 @xmath282 @xmath253    @xmath283 @xmath284 @xmath206    not only does this achieve a very fast approximation of the maximum @xmath4 values in @xmath285 , @xmath274 can be called twice and both calls can be used together to estimate the indices @xmath195 that correspond to each value @xmath285 : this is achieved by first noting that @xmath286 should give the same result regardless of the order of the elements in @xmath287 . thus ,",
    "if @xmath288 $ ] denotes the reverse of @xmath287 ( python notation ) , then @xmath289)$ ] .",
    "second , the value @xmath290 ( _ i.e. _ , the positive diagonal from which each next value is drawn ) can be trivially added to the return value by simply changing the line @xmath291 to @xmath292 . therefore , by computing @xmath293 and also computing @xmath294)$ ] , it is possible to get estimates for the positive diagonal from which each element was drawn .",
    "let the positive diagonal for a given result value ( from calling @xmath286 ) be denoted @xmath295 and let the negative diagonal from which that same result was drawn ( from calling @xmath294)$ ] ) be denoted @xmath296 .",
    "it can therefore be seen that @xmath297 , and so @xmath298 . once @xmath122 is computed",
    ", then @xmath299 .",
    "these estimates can subsequently be verified by comparing the approximation @xmath244 with the empirical value @xmath300 using the @xmath195 computed above ; when both are close , then the @xmath195 value is a reliable estimate .",
    "the ability to estimate the indices in this manner is significant , because an existing approximation that creates binned histograms for @xmath301 and @xmath302 and then uses those to compute the binned histogram of @xmath303 ( by convolving the @xmath301 and @xmath302 histograms with fft )  @xcite can not be used to estimate indices : doing so would require passing indices through the fft , which would require a set of integers to be stored for every value in the fft ( rather than a complex floating point value ) and those sets will take on several values as the fft recurses , including the full set @xmath304 . as a result ,",
    "passing indices through the fft can not yet be accomplished in @xmath22 time .",
    "here an elegant and standard @xmath25 algorithm , the floyd - warshall algorithm , for solving the apsp problem is compared against a simple , novel method that was created using the fast rings approximation strategy .",
    "this novel method ( made quickly and without a great expertise on the apsp problem ) achieves a fairly good approximation and outperforms the floyd - warshall algorithm , as shown in table  [ table : apsp - runtimes ] .",
    "note that even with a simple and fairly numerically unstable fast matrix multiplication algorithm ( a naive implementation of the strassen algorithm ) , the approximation is not only close to the exact value , it also often improves as the problem size increases , because there is an increased chance of finding an efficient path between two vertices .",
    "the index @xmath305 ( arbitrarily chosen as the first non - trivial index index @xmath306 necessarily has a distance of @xmath307 ) of a single @xmath308 problem yields an exact shortest path distance of @xmath309 and an approximate shortest path distance @xmath310 ( absolute error @xmath311 ) .",
    "r|cccccc @xmath36 & 16 & 32 & 64 & 128 & 256 & 512 +   + floyd - warshall runtime & 0.01155 & 0.09297 & 0.7717 & 5.956 & 49.43 & 378.9 + fast rings approximation runtime & 0.02527 & 0.09537 & 0.4572 & 2.594 & 15.31 & 103.5 + mse & 0.04587 & 0.05395 & 0.03049 & 0.02767 & 0.02228 & 0.01207 +      here the best known method ( which is the naive @xmath224 algorithm ) is demonstrated for finding the top @xmath4 values in @xmath312 , and compared to a novel @xmath313 algorithm based on the fast rings approximation . once again",
    ", the fast rings approximation achieves a superior runtime ( figure  [ figure : xplusy ] ) .",
    "although the error is significantly higher in this example ( because errors are accumulating in an iterative manner , which is not the case in max - convolution and the apsp problem ) , not only is the runtime superior , the space requirements are dramatically decreased ( from @xmath314 gb to memory usage in the low mbs due to a linear space requirement ) , because the matrix of all @xmath45 ( or , equivalently , @xmath44 in the original , un - transformed space ) is never actually computed .    .",
    "* for problems of different size ( @xmath315 and @xmath316 ) a single problem ( _ i.e. _ lists @xmath6 and @xmath7 ) is sampled from various distributions . on each problem , the naive @xmath224 approach is used to sort all possible @xmath226 pairs and the top @xmath317 values are compared with the top values estimated with the fast rings approximation in @xmath23 time ( using @xmath318 , _",
    "i.e. _ , @xmath319 ) . where the approximation is accurate , the indices @xmath195 that correspond to @xmath320 at every rank can be estimated with high fidelity ( _ e.g. _ , all @xmath321 top indices are correct for the uniform distributions with @xmath316 ) . when @xmath316 , the naive method required @xmath322 seconds and @xmath323 gb of ram , while fast approximation requires @xmath324 seconds and memory usage was insignificant in comparison.,width=480 ]",
    "this manuscript has proposed novel methods for two open problems ; although the methods themselves are certainly of interest , most exciting is that both were created without expertise even though both problems ( like max - convolution ) have been subject to years of hard work by the field .",
    "furthermore , the methods themselves are fairly simple ( as was the case for fast max - convolution ) ; other than code for * estimatemaxfromnormpowersequence * described in  @xcite and implemented in an efficient , vectorized manner with numpy , devising novel approximations is quite simple with this general strategy : just as there are many problems to which this strategy can be applied , there are many other variants of the strategy .",
    "the general idea changing the problem into a spectrum of rings , applying fast algorithms limited to rings , and then estimating the true result from the aggregated spectrum could also be paired with other soft - max functions that correspond well to the rings .",
    "even though the apsp approximation only estimates the path weights or distances of the shortest path ( as opposed to the path itself ) , there may be situations where branch and bound can be used to compute the paths in @xmath325 time given knowledge about the optimal path lengths ( _ i.e. _ , if the final paths are very efficient , then many edges can be excluded wherever the cumulative distance sufficiently exceeds the optimal path distance ) .",
    "approximation error can even be worked into this scheme by using a small buffer @xmath326 that prevents bounding unless the weight is more than @xmath326 greater than the approximation estimates .",
    "likewise , an approximation may also be suitable for cases from operations research where users are optimizing over graphs , and thus may need to repeatedly estimate the apsp path distances ( in this case , it may even be possible to perform local optimizations directly in the @xmath63-norm rings , which will be continuous and differentiable ) .",
    "there may also be a strategy similar to how indices can be estimated on the @xmath44 problem , which could permit estimation of the destination of each edge taken by matrix multiplication on @xmath0 .",
    "furthermore , the largest errors appear to occur when the shortest path between two vertices is long ; this is because the @xmath0 variant of the problem operates in a transformed space , and a total path weight of @xmath327 in the original space @xmath3 corresponds to @xmath328 in the transformed space .",
    "for this reason , it is promising to consider the application of scaling to keep the problem in a nicely bounded range or even the possibility of using an alternative transformation and then solving that problem on @xmath0 .",
    "in general , it may also be possible to solve @xmath3 problems in a similar manner without transforming to a @xmath0 formulation .    considering the @xmath44 method",
    ", there may be other uses for the normqueue data structure , the fast , approximate data structure for keeping track of the largest remaining values in a collection based on its norms ( rather than storing the values themselves ) .",
    "there may be other applications ( _ e.g. _ , in large - scale databases or web search ) where it is possible to cache a small collection of norms ( potentially with great efficiency via algorithms like fft convolution ) , but where caching the full list would be intractable .",
    "obvious applications would be similar to the @xmath44 problem , where a combinatorial effect makes caching results much more difficult , and where algorithms such as fft convolution can be used to compute the norms on some combination of variables an order of magnitude faster than if the norms were computed from scratch .",
    "this sequence of norms can also be updated online in other ways ( in addition to the @xmath329 operation employed ) , such as adding values to the queue ( by adding in @xmath330 to the values stored at each @xmath63 ) .    regarding the @xmath44 problem itself",
    ", it would be very interesting to see if the error of this preliminary algorithm could be improved by performing @xmath286 and @xmath331)$ ] simultaneously ( rather than serially , as was used for estimating the indices @xmath195 in this manuscript ) .",
    "if both instances proceed one index at a time , then an estimate of indices @xmath195 could be computed before calling @xmath329 ; if the estimated indices are accurate , then the exact value @xmath44 could replace the estimated value when updating the normqueue ( _ i.e. _ , when subtracting out the estimated max to the power @xmath63 from each @xmath332 ) , and such updates would introduce error much more slowly ( because , in an inductive manner , starting with high - quality estimates of the initial maxima would yield to more accurate updating of the normqueue , which would lead to higher quality estimates of subsequent maxima ) .",
    "this modularity of this fast rings approximation is a substantial benefit : in the likely case that future research discovers alternative methods for computing * estimatemaxfromnormpowersequence * ( which in this manuscript uses an @xmath132 projection ) , the accuracy or the speed - accuracy tradeoff of this method would immediately improve .",
    "future developments in the conjectured error bound for the @xmath132 projections will be of interest , as will error bounds for the @xmath333 and @xmath334 projections , for which closed - form polynomial roots can be computed . just as the error dramatically improves when changing from the @xmath335 approximation to the @xmath132 projection ( indeed , the error bound of the former depends on @xmath36 , the length of @xmath186 , whereas the relative error bound conjectured for the @xmath132 projection no longer depends on @xmath36 ) , using @xmath333 or even using different models of the norm may pose even greater advantages .",
    "it is also important to note that although @xmath333 or @xmath334 models will almost certainly be slightly slower , they may also require smaller @xmath177 sets to achieve the same error , thereby lowering the runtime again .",
    "this same modularity that lets new estimates of maxima be used easily also allows faster algorithms @xmath143 on the corresponding ring space ( _ e.g. _ , improved algorithms for matrix multiplication , convolution , _ etc .",
    "_ ) to be used wherever such an algorithm can be employed .",
    "this also means that sparse matrix multiplication could be easily paired with the apsp approximation ( although the advantages of the fast rings approximation will almost certainly be mitigated for such graphs ) . in cases where the dynamic range is large ,",
    "the strategy can also easily be used in the log - transform of the ring @xmath12 ( _ i.e. _ , @xmath336 ) to lower numerical error .",
    "high ( variable ) precision numbers could be used with this strategy ( introducing computational complexity in each arithmetic operation , but allowing for fewer @xmath63 to be used while attaining a high accuracy ) .",
    "it would even be possible to compute a small number ( @xmath337 or even @xmath136 ) of exact solutions to sub - problems ( _ e.g. _ , for the apsp problem , computing pairwise distances between a small number of vertex pairs with dijkstra s algorithm ) , and then use the relationship between those exact values and the approximate values to build a simple , affine model to correct numerical error ( like the affine model for correcting max - convolution results from  @xcite ) .",
    "this can be used to reduce bias and substantially lower the mse .",
    "like using @xmath63-norm rings , that strategy also generalizes to plenty of other problems . also reminiscent of the previous work on max - convolution",
    "is the ease of parallelizing the approach ( even coarse - grained parallelization ) , because the @xmath63 rings can often be solved in parallel ( _ e.g. _ , the matrix multiplications for each @xmath63 in the apsp problem could be trivially parallelized ) .",
    "the numerical error may also be reduced by using more precise alternatives to strassen s matrix multiplication algorithm , because strassen s algorithm is substantially less stable than naive matrix multiplication . fast but stable algorithms for",
    "matrix multiplication will decrease @xmath75 for that problem , thereby increasing the highest stable @xmath63 that can be used , and as a result substantially lowering the numerical error .",
    "it would also be interesting to investigate this approach on other problems on semirings ; the two problems discussed here ( _ i.e. _ , the apsp problem and finding the top @xmath4 values in @xmath44 ) were chosen arbitrarily .",
    "this approximation method would likely be of greatest utility on applications where little prior research exists ( in contrast with the apsp problem , for example ) .",
    "python code demonstrating these ideas is available at bitbucket.org/orserang/fast-semirings .",
    "i am grateful to mattias frnberg , julianus pfeuffer , xiao liang , marie hoffmann , knut reinert , and oliver kohlbacher for their fast and useful comments . o.s .",
    "acknowledges generous start - up funds from freie universitt berlin and the leibniz - institute for freshwater ecology and inland fisheries .",
    "d.  bremner , t.  m. chan , e.  d. demaine , j.  erickson , f.  hurtado , j.  iacono , s.  langerman , and p.  taslakian .",
    "necklaces , convolutions , and @xmath303 . in _ algorithms ",
    "esa 2006 _ , pages 160171 .",
    "springer , 2006 .",
    "j.  pfeuffer and o.  serang . a bounded @xmath8-norm approximation of max - convolution for sub - quadratic bayesian inference on additive factors . _ journal of machine learning research _ , 170 ( 36):0 139 , 2016 .                  r.  williams .",
    "faster all - pairs shortest paths via circuit complexity . in _ proceedings of the 46th annual acm symposium on theory of computing _ , stoc 14 , pages 664673 .",
    "acm , 2014 .",
    "isbn 978 - 1 - 4503 - 2710 - 7 ."
  ],
  "abstract_text": [
    "<S> important problems across multiple disciplines involve computations on the semiring @xmath0 ( or its equivalents , the negated version @xmath1 ) , the log - transformed version @xmath2 , or the negated log - transformed version @xmath3 ) : max - convolution , all - pairs shortest paths in a weighted graph , and finding the largest @xmath4 values in @xmath5 for two lists @xmath6 and @xmath7 . however , fast algorithms such as those enabling fft convolution , sub - cubic matrix multiplication , _ etc . _ , require inverse operations , and thus can not be computed on semirings . </S>",
    "<S> this manuscript generalizes recent advances on max - convolution : in this approach a small family of @xmath8-norm rings are used to efficiently approximate results on a nonnegative semiring . </S>",
    "<S> the general approach can be used to easily compute sub - cubic estimates of the all - pairs shortest paths in a graph with nonnegative edge weights and sub - quadratic estimates of the top @xmath4 values in @xmath5 when @xmath6 and @xmath7 are nonnegative . </S>",
    "<S> these methods are fast in practice and can benefit from coarse - grained parallelization . </S>"
  ]
}