{
  "article_text": [
    "the analysis of lifetime or failure time data has been of considerable interest in many branches of statistical applications . in many experiments , censoring is very common due to inherent limitations , or time and cost considerations on experiments .",
    "the data are said to be censored when , for observations , only a lower or upper bound on lifetime is available .",
    "thus , the problem of parameter estimation from censored samples is very important for real - data analysis . to obtain the parameter estimate ,",
    "some numerical optimization methods are required to find the mle .",
    "however , ordinary numerical methods such as the gauss - seidel iterative method and the newton - raphson gradient method may be very ineffective for complicated likelihood functions and these methods can be sensitive to the choice of starting values used . for censored sample problems , several approximations of the mle and the best linear unbiased estimate ( blue )",
    "have been studied instead of direct calculation of the mle . for example",
    ", the problem of parameter estimation from censored samples has been treated by several authors .",
    "@xcite has studied the mle and provided the blue for type - i and type - ii censored samples from an normal distribution .",
    "@xcite has derived the blue for a symmetrically type - ii censored sample from a laplace distribution for sample size up to @xmath0 .",
    "@xcite has given an approximation of the mle of the scale parameter of the rayleigh distribution with censoring .",
    "@xcite has given an approximation of the mle for a type - ii censored sample from an normal distribution .",
    "@xcite has given the blue for a type - ii censored sample from a laplace distribution .",
    "the blue needs the coefficients @xmath1 and @xmath2 , which were tabulated in @xcite , but the table is provided only for sample size up to @xmath0 . in addition , the approximate mle and the blue are not guaranteed to converge to the preferred mle .",
    "the methods above are also restricted only to type - i or type - ii ( symmetric ) censoring for sample size up to @xmath0 only .",
    "these aforementioned deficiencies can be overcome by the em algorithm . in many practical problems",
    ", however , the implementation of the ordinary em algorithm is very difficult .",
    "thus , @xcite proposed to use the mcem when the em algorithm is not available .",
    "however , the mcem algorithm presents a serious computational burden because in the e - step of the mcem algorithm , monte carlo random sampling is used to obtain the expected posterior log - likelihood .",
    "thus , it is natural to look for a better method .",
    "the proposed method using the quantile function instead of monte carlo random sampling has greater stability and also much faster convergence properties with smaller sample sizes .",
    "moreover , in many experiments , more general incomplete observations are often encountered along with the fully observed data , where incompleteness arises due to censoring , grouping , quantal responses , etc .",
    "one general form of an incomplete observation is of interval form .",
    "that is , a lifetime of a subject @xmath3 is specified as @xmath4 . in this paper , we deal with computing the mle for this general form of incomplete data using the em algorithm and its variants , mcem and qem .",
    "this interval form can handle right - censoring , left - censoring , quantal responses and fully - observed observations .",
    "the proposed method includes the aforementioned existing methods as a special case .",
    "this proposed method can also handle the data from intermittent inspection which are referred to as _ grouped data _ which provide only the number of failures in each inspection period . @xcite and",
    "@xcite have given an approximation of the mle under the exponential distribution only .",
    "@xcite described maximum likelihood methods , but the mle should be obtained by ordinary numerical methods .",
    "the proposed method enables us to obtain the mle through the em or qem sequences under a variety of distribution models .",
    "the rest of the paper is organized as follows . in section  [ sec : em ] , we introduce the basic concept of the em and mcem algorithms . in section  [ sec : qem ] , we present the quantile implementation of the em algorithm . in section  [ sec : model ] , we provide the likelihood construction with interval data and its em implementation issues . section  [ sec : parameter ] deals with the parameter estimation procedure of exponential , normal , laplace , rayleigh , and weibull distributions with interval data . in order to compare the performance of the proposed method with the em and mcem methods , monte carlo simulation study is presented in section  [ sec : simulation ] followed up with examples of various applications in section  [ sec : examples ] .",
    "this paper ends with concluding remarks in section  [ sec : conclusion ] .",
    "in this section , we give a brief introduction of the em and mcem algorithms .",
    "the em algorithm is a powerful computational technique for finding the mle of parametric models when there is no closed - form mle , or the data are incomplete .",
    "the em algorithm was introduced by @xcite to overcome the above difficulties . for more details about this em algorithm ,",
    "good references are @xcite , @xcite , @xcite , and @xcite .",
    "when the closed - form mle from the likelihood function is not available , numerical methods are required to find the maximizer ( _ i.e. _ , mle ) .",
    "however , ordinary numerical methods such as the gauss - seidel iterative method and the newton - raphson gradient method may be very ineffective for complicated likelihood functions and these methods can be sensitive to the choice of starting values used .",
    "in particular , if the likelihood function is flat near its maximum , the methods will stop before reaching the maximizer .",
    "these potential problems can be overcome by using the em algorithm .",
    "the em algorithm consists of two iterative steps : ( i ) expectation step ( e - step ) and ( ii ) maximization step ( m - step ) .",
    "the advantage of the em algorithm is that it solves a difficult incomplete - data problem by constructing two easy steps .",
    "the e - step of each iteration only needs to compute the conditional expectation of the log - likelihood with respect to the incomplete data given the observed data .",
    "the m - step of each iteration only needs to find the maximizer of this expected log - likelihood constructed in the e - step , which only involves handling `` complete - data '' log - likelihood function .",
    "thus , the em sequences repeatedly maximize the posterior log - likelihood function of the complete data given the incomplete data instead of maximizing the potentially complicated likelihood function of the incomplete data directly .",
    "an additional advantage of this method compared to other direct optimization techniques is that it is very simple and it converges reliably .",
    "in general , if it converges , it converges to a local maximum .",
    "hence in the case of the unimodal and concave likelihood function , the em sequences converge to the global maximizer from any starting value .",
    "we can employ this methodology for parameter estimation from interval data because interval data models are special cases of incomplete ( missing ) data models .    here , we give a brief introduction of the em and mcem algorithms .",
    "denote the vector of unknown parameters by @xmath5",
    ". then the complete - data likelihood is @xmath6 where @xmath7 and we denote the observed part of @xmath8 by @xmath9 and the incomplete ( missing ) part by @xmath10 . denote the estimate at the @xmath11-th em sequences by @xmath12 .",
    "the em algorithm consists of two distinct steps :    * ` compute ` @xmath13 + where @xmath14 . * ` find ` @xmath15 + which maximizes @xmath13 in @xmath16 .    in some problems ,",
    "the implementation of the e - step is difficult .",
    "@xcite propose to use the mcem to avoid this difficulty .",
    "the e - step is approximated by using monte carlo integration .",
    "simulating @xmath17 from the conditional distribution @xmath18 , we can approximate the expected posterior log - likelihood as follows : @xmath19 where @xmath20 .",
    "this method is called the monte carlo em ( mcem ) algorithm .",
    "major drawback to mcem is that it is very slow because it requires a large sample size in order to possess stable convergence properties .",
    "this problem can be overcome by the proposed method using the quantile function .",
    "the key idea underlying the quantile implementation of the em algorithm can be easily illustrated by the following example .",
    "the data in the example were first presented by @xcite and have since then been used very frequently for illustration in the reliability engineering and survival analysis literature including @xcite and @xcite .",
    "an experiment is conducted to determine the effect of a drug named 6-mercaptopurine ( 6-mp ) on leukemia remission times .",
    "a sample of size @xmath21 leukemia patients is treated with 6-mp and the times of remission are recorded .",
    "there are @xmath22 individuals for whom the remission time is fully observed , and the remission times for the remaining 12 individuals are randomly censored on the right . letting a plus ( + )",
    "denote a censored observation , the remission times ( in weeks ) are    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ xxx= xxx= xxx= xxx= xxx= xxx=",
    "xxx= xxx= xxx= xxx= xxx 6 6 6 @xmath23 7 @xmath24 10 @xmath25 @xmath26 13 16 + @xmath27@xmath28@xmath2922 23 @xmath30@xmath31@xmath31@xmath32@xmath33 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    using an exponential model , we can obtain the complete likelihood function and the conditional pdf @xmath34 @xmath35 where @xmath36 is a right - censoring time of test unit @xmath37 . using the above conditional pdf , we have the expected posterior log - likelihood @xmath38 then the monte carlo approximation of the expected posterior log - likelihood is given by @xmath39 where a random sample @xmath40 is from @xmath41 . in the monte carlo approximation , the term @xmath42 is approximated by @xmath43 this approximation can be improved by using the quantile function . for the conditional pdf @xmath44 ,",
    "the quantiles of @xmath45 , denoted by @xmath46 , are given by @xmath47 for @xmath48 .",
    "we can choose @xmath45 from any of the fractions , @xmath49 , @xmath50 , @xmath51 , etc . using the quantile function",
    ", we have the following approximation @xmath52 it is noteworthy that that a random sample @xmath40 in the monte carlo approximation is usually generated by using the above quantile function with @xmath45 from a random sample having a uniform distribution between @xmath53 and @xmath54 .    ]",
    "fig.[fig : qfunction ] presents the mcem and qem approximations of expected posterior log - likelihood functions for @xmath55  ( dashed curve ) , 100  ( dotted curve ) and 1000  ( dot - dashed curve ) at the first step ( @xmath56 ) , along with the exact expected posterior log - likelihood ( solid curve ) .",
    "the mcem and qem algorithms were run with starting value @xmath57 .",
    "as can be seen in the figure , the mcem and qem both successfully converge to the expected posterior log - likelihood as @xmath58 gets larger .",
    "note that the qem is much closer to the true expected posterior log - likelihood for smaller values of @xmath58 .    ) .",
    "[ fig : iteration ] ]    fig.[fig : iteration ] displays the iterations of the em and qem sequences in the example from the starting value @xmath57 .",
    "the horizontal solid lines indicate the mle ( @xmath59 ) .",
    "the figures clearly show that the qem is stable and converges very fast to the mle .",
    "even with very small quantile sizes , the qem outperforms the mcem .",
    "it should be noted that the qem with @xmath60 performs better than the mcem with @xmath61 .      another way to view",
    "the quantile implementation idea is by looking at the riemann - stieltjes integral .",
    "for simplicity of presentation , we consider the case where @xmath62 is one - dimensional .",
    "denote @xmath63 .",
    "let us consider a following riemann - stieltjes sum , @xmath64 in the limit as @xmath65 , we have @xmath66 using a change - of - variable integration technique with @xmath67 , we have @xmath68 hence the quantile approximation of the expectation posterior log - likelihood is a riemann - stieltjes sum which converges to the true integration . with @xmath69 , this sum",
    "is also known as the extended midpoint rule and it is accurate to the order of @xmath70 ; see @xcite . that is , @xmath71 where @xmath72 .    on the other hand ,",
    "the accuracy of the monte carlo approximation @xmath73 can be assessed as follows . by the central limit theorem , we have @xmath74 and this is accurate to the order of @xmath75 .",
    "it is immediate from the weak law of large numbers that @xmath76 . using this and ( [ eq : clt ] ) , we have @xmath77 from this , it is clear that the qem is much more accurate than the mcem .",
    "we can generalize the above result as follows .",
    "in the e - step , we replace the monte carlo approximation with the quantile approximation @xmath78 where @xmath79 with @xmath80 , and @xmath45 is any fraction . in this paper",
    ", we use @xmath81 .",
    "it should be noted that the approximation of the expected posterior log - likelihood in the proposed method can be viewed as being similar to a quasi - monte carlo approximation in the sense that the quasi - monte carlo approximation also uses a _ deterministic _ sequence rather than a _ random _ sample .",
    "in fact , @xcite shows that there exist such sequences in the normalized integration domain , which ensure an accuracy to the order of @xmath82 , where @xmath83 is the dimension of the integration space ( see * ? ? ? * ) .",
    "thus , using the quasi - monte carlo sequences in the normalized integration domain , one can improve the accuracy of the integration in the e - step of the mcem algorithm which leads to accuracy on the order of @xmath84 with @xmath85 .",
    "however , we should point that the proposed qem method leads to accuracy on the order of @xmath86 . therefore , although using the quasi - monte carlo approximation can improve the mcem , the inaccuracy in that case will be greater than that of the proposed qem method . also incorporating the quantiles from the proposed method into the m - step to obtain the mle is quite straightforward .",
    "on the other hand , if the quasi - monte carlo sequences in the normalized integration domain are used , it would be irrelevant to the use of its sequences in the m - step to obtain the maximizer .",
    "thus , the focus of the paper is to construct the em algorithm using the quantiles so that the mles can be straightforwardly obtained .",
    "in this section , we develop the likelihood functions which can be conveniently used for the em , mcem and qem algorithms .",
    "the general form of an incomplete observation is often of interval form .",
    "that is , the lifetime of a subject @xmath3 may not be observed exactly , but is known to fall in an interval : @xmath4 .",
    "this interval form includes censored , grouped , quantal - response , and fully - observed observations .",
    "for example , a lifetime is left - censored when @xmath87 and a lifetime is right - censored when @xmath88 .",
    "the lifetime is fully observed when @xmath89 .",
    "suppose that @xmath90 are observations on random variables which are independent and identically distributed and have a continuous distribution with the pdf @xmath91 and cdf @xmath92 . interval data from experiments can be conveniently represented by pairs @xmath93 with @xmath94 $ ] , @xmath95 where @xmath96 is an indicator variable and @xmath1 and @xmath2 are lower and upper ends of interval observations of test unit  @xmath37 , respectively . if @xmath97 , then the lifetime of the @xmath37-th test unit is fully observed .",
    "denote the observed part of @xmath7 by @xmath9 and the incomplete ( missing ) part by @xmath10 with @xmath98 .",
    "denote the vector of unknown distribution parameters by @xmath99 . then ignoring a normalizing constant",
    ", we have the complete - data likelihood function @xmath100 integrating @xmath101 with respect to @xmath62 , we obtain the observed - data likelihood @xmath102 where in general an empty product is taken to be one . using the @xmath93 notation , we have @xmath103 where @xmath104 and @xmath105 . here , although we provided the likelihood function for the interval - data case , it is easily extended to more general forms of incomplete data . for more details ,",
    "the reader is referred to @xcite and @xcite .",
    "thus , the goal is inference about @xmath16 given the complexity of the likelihood , and the em algorithm is a tool that can be used to accomplish this goal .",
    "then the issue here is how to implement the em algorithm when there are interval data in the sample . by treating the interval data as incomplete ( missing ) data ,",
    "it is possible to write the complete - data likelihood .",
    "this treatment allows one to fine the _ closed - form _ maximizer in the m - step . for convenience ,",
    "assume that all the data are of interval form with @xmath106 and @xmath107 .",
    "then the likelihood function in ( [ eq : likelihood1 ] ) can be rewritten as @xmath108 then the complete - data likelihood function corresponding to ( [ eq : likelihood2 ] ) becomes @xmath109 where the pdf of @xmath110 is given by @xmath111 for @xmath112 . using this",
    ", we have the following @xmath113-function in the e - step , @xmath114 it is worth looking at the integration when @xmath115 . for convenience , omitting the subject index @xmath37 and letting @xmath116 , we have @xmath117 it follows from the integration by parts that the above integral becomes @xmath118_a^{a+\\epsilon }   - \\int_{a}^{a+\\epsilon } \\frac{f'(z|\\boldsymbol{\\theta})}{f(z|\\boldsymbol{\\theta } ) }          \\cdot p_z(z |{\\boldsymbol{\\theta}^{(s)}})\\ , dz,\\ ] ] where @xmath119 applying lhospital rule to ( [ eq : qfunction1 ] ) with ( [ eq : qfunction2 ] ) , we obtain @xmath120 hence , for full observation , we simply use the interval @xmath121 $ ] notation which implies @xmath122 $ ] with the limit as @xmath123 .",
    "all kinds of data considered in this paper can be denoted by the interval - data form without the indicator variable , @xmath96 .    for notational convenience ,",
    "we let @xmath124 .",
    "then the complete - data likelihood function corresponding to ( [ eq : likelihood3 ] ) becomes @xmath125 where @xmath126 .",
    "from now , unless otherwise specified , @xmath62 refers to @xmath127 instead of @xmath128 .",
    "thus , we use ( [ eq : likelihood2 ] ) or ( [ eq : likelihood4 ] ) for the likelihood function or complete - data likelihood function instead of ( [ eq : likelihood0 ] ) or ( [ eq : likelihood1 ] ) , respectively .    for many distributions including weibull and laplace distributions ,",
    "it is extremely difficult or may be impossible to implement the em algorithm with interval data .",
    "this is because , during the e - step , the @xmath113-function does not integrate easily and this causes computational difficulties in the m - step . in order to avoid this problem",
    ", one can use mcem algorithm  @xcite which reduces the difficulty in the e - step through the use of a monte carlo integration . as aforementioned , although it can make some problems tractable , the mcem involves a serious computational burden and can often lead to unstable estimates .",
    "thus , we proposed a quantile implementation of the em algorithm which alleviates some of the computational burden of the mcem and leads to more stable estimates .    for stopping criterion for the em , mcem or qem algorithm , the algorithm stops if the changes are all relatively small compared to a given precision @xmath129 .",
    "as an example for normal distribution , the qem algorithm stops if @xmath130 and @xmath131 as well . in what follows , we obtain the em ( if available ) , mcem , and qem sequences for a variety of distributions , which maximize the likelihood function in ( [ eq : likelihood1 ] ) .",
    "in this section , we briefly provide the inferential procedure for the parameter estimation of exponential , normal , laplace , rayleigh , and weibull distributions from random samples in interval form . for the exponential and normal distributions , the ordinary em algorithm applies , so the mcem and qem are not needed . to compare the performance of the mcem and qem ,",
    "however , we include the exponential and normal distributions although the ordinary em algorithms are available .",
    "for the laplace distribution , the computation of the e - step is very complex , so either the mcem or the qem is more appropriate . for the rayleigh and weibull distributions ,",
    "the calculation of the integration in the e - step does not have a closed form .",
    "so , it is not feasible to use the ordinary em algorithm . as aforementioned ,",
    "it is noteworthy that the qem sequences are easily obtained by replacing a random sample @xmath132 in the mcem sequences with a quantile sample @xmath133 .",
    "we assume that @xmath110 are iid exponential random variables with the pdf given by @xmath134 . using ( [ eq : likelihood3 ] )",
    ", we have the complete - data log - likelihood of @xmath135 @xmath136 where the pdf of @xmath110 is given by @xmath137 for @xmath112 . when @xmath138 , the above random variable @xmath110 degenerates at @xmath139 .    *   + the @xmath140 function is given by @xmath141 where for @xmath107 @xmath142 = \\int_{a_i}^{b_i } \\!\\!\\ !",
    "z\\cdot p_{z_i}(z | \\lambda^{(s ) } ) \\ , dz       = \\frac { a_i\\exp(-\\lambda^{(s ) } a_i ) - b_i\\exp(-\\lambda^{(s ) } b_i ) }              { \\exp(-\\lambda^{(s ) } a_i ) - \\exp(-\\lambda^{(s ) } b_i ) } + \\frac{1}{\\lambda^{(s)}}.\\ ] ] when @xmath138 , we have @xmath143 .",
    "*   + differentiating @xmath144 with respect to @xmath145 and setting this to zero , we obtain @xmath146 solving for @xmath145 , we obtain the @xmath147st em sequence in the @xmath148    if we instead use the mcem ( or qem ) algorithm by simulating ( or quantiling ) @xmath149 from the truncated normal distribution @xmath150 , we then obtain the mcem ( or qem ) sequences @xmath151 where @xmath40 for @xmath152 are from the truncated exponential distribution @xmath153 defined in ( [ eq : expotruncation ] ) .",
    "it is of interest to consider the case where the data are right - censored . in this special case",
    ", the closed - form mle is known .",
    "if the data are fully observed ( _ i.e. _ , @xmath154 $ ] ) for @xmath155 , it is easily seen from lhospital rule that @xmath143 .",
    "if the observation is right - censored ( _ i.e. _ , @xmath156 $ ] ) for @xmath157 , we have @xmath158 . substituting these results into ( [ eq : emexpo ] ) leads to @xmath159 note that solving the stationary - point equation @xmath160 of ( [ eq : emexpo2 ] ) gives @xmath161 as expected , this result is the same as the well - known closed - form mle with the right - censored data .",
    "we assume that @xmath110 are iid normal random variables with parameter vector @xmath162 .",
    "then the complete - data log - likelihood is @xmath163 where the pdf of @xmath110 is given by @xmath164 for @xmath112 .",
    "similarly as before , if @xmath138 , then the random variable @xmath110 degenerates at @xmath165 .",
    "*   + denote the estimate of @xmath166 at the @xmath11-th em sequence by @xmath167 . ignoring constant terms , we have @xmath168 where @xmath169 $ ] and @xmath170 $ ] . using the following integral identities @xmath171 we have for @xmath107 @xmath172 when @xmath138 , we have @xmath173 and @xmath174 . *   + differentiating the expected log - likelihood @xmath13 with respect to @xmath175 and @xmath176 and solving for @xmath175 and @xmath176 , we obtain the em sequences @xmath177    if we instead use the mcem ( or qem ) algorithm by simulating ( or quantiling ) @xmath149 from the truncated normal distribution @xmath150 , we then obtain the mcem ( or qem ) sequences @xmath178 where @xmath40 are from the truncated normal distribution @xmath179 defined in ( [ eq : normaltruncation ] ) .",
    "we assume that @xmath110 are iid laplace random variables with parameter @xmath180 whose pdf is given by @xmath181 using ( [ eq : likelihood3 ] ) , we have the complete - data log - likelihood @xmath182 where the pdf of @xmath110 is given by @xmath183 for @xmath112 . similarly as before , if @xmath138 , then the random variable @xmath110 degenerates at @xmath139 .    *   + at the @xmath11-th step in the em sequence denoted by @xmath184",
    ", we have the expected log - likelihood @xmath185 the computation of the above integration part is very complex .",
    "we can overcome this difficulty by using mcem ( or qem ) approach .",
    "the approximate expected log - likelihood is @xmath186 where @xmath187 , and @xmath40 for @xmath152 are from @xmath188 defined in ( [ eq : laplacetruncation ] ) . *",
    "+ then we have the mcem ( or qem ) sequences @xmath189      let @xmath110 be iid rayleigh random variables with parameter @xmath190 whose pdf is given by @xmath191 then the complete - data log - likelihood is @xmath192 where the pdf of @xmath110 is given by @xmath193 for @xmath194 .",
    "similarly as before , if @xmath138 , then the random variable @xmath110 degenerates at @xmath139 .    *   + at the @xmath11-th step in the em sequence denoted by @xmath195 , we have the expected log - likelihood @xmath196 the calculation of the above integration part does not have a closed form . using the mcem",
    ", we have the approximate expected log - likelihood @xmath197 where @xmath198 and @xmath40 for @xmath152 are from @xmath199 defined in ( [ eq : rayleightruncation ] ) .",
    "*   + we then have the following mcem ( or qem ) sequences by differentiating @xmath200 : @xmath201      we assume that @xmath3 is iid weibull random variables with the pdf and cdf of @xmath3 given by @xmath202 and @xmath203 , respectively .    using ( [ eq : likelihood3 ] ) , we obtain the complete - data log - likelihood of @xmath204 : @xmath205 where the pdf of @xmath110 is given by @xmath206 for @xmath112 .",
    "similarly as before , if @xmath138 , then the random variable @xmath110 degenerates at @xmath139 .    *   + denote the estimate of @xmath166 at the @xmath11-th em sequence by @xmath207 .",
    "it follows from @xmath208 $ ] that @xmath209 where @xmath210 $ ] and @xmath211 $ ] .",
    "*   + differentiating @xmath144 with respect to @xmath145 and @xmath190 and setting this to zero , we obtain @xmath212 arranging for @xmath190 , we have the equation of @xmath190 @xmath213 the @xmath147st em sequence of @xmath190 is the solution of the above equation . after finding @xmath214",
    ", we obtain the @xmath147st em sequence of @xmath215 @xmath216    in this weibull case , it is extremely difficult or may be impossible to find the explicit expectations of @xmath217 $ ] and @xmath218 $ ] in the e - step , but the quantile function of the random variable @xmath110 at the @xmath11-th step can be easily obtained . from ( [ eq : fz ] )",
    ", we have @xmath219^{1/\\beta^{(s)}}.\\ ] ] using the above quantiles , we obtain the following qem algorithm .",
    "*   + denote the quantile approximation of @xmath140 by @xmath220 . then , we have @xmath221 *   + differentiating @xmath222 with respect to @xmath145 and @xmath190 and setting this to zero , we obtain @xmath223 arranging for @xmath190 , we have the equation of @xmath190 @xmath224 the @xmath147st qem sequence of @xmath190 is the solution of the above equation . after finding @xmath214 , we obtain the @xmath147st qem sequence of @xmath215 @xmath225    in the m - step , we need to estimate the shape parameter @xmath190 by numerically solving ( [ eq : eebeta ] ) , but this is only a one - dimensional root search and the uniqueness of this solution is guaranteed .",
    "lower and upper bounds for the root are explicitly obtained , so with these bounds we can find the root easily .",
    "we provide the proof of the uniqueness under quite reasonable conditions and give lower and upper bounds of @xmath190 in the appendix .",
    "in order to examine the performance of the proposed method , we use the monte carlo simulations with 5000 replications .",
    "we present the performance of this new method with the em and mcem estimators by comparing their estimated biases and the mean square errors ( mse ) .",
    "the biases are calculated by the sample average ( over 5000 replications ) of the differences between the method under consideration and the mle .",
    "the mse are also obtained by the sample variance of the differences between the method under consideration and the mle .",
    "l@rrr location & + estimator & & & +   + em & 1.342988@xmath226 & 1.779955@xmath227 &  + mcem + @xmath55 & 7.169276@xmath228 & 8.381887@xmath229 & 2.123573@xmath230 + @xmath231 & 2.223300@xmath228 & 8.170053@xmath232 & 2.178633@xmath233 + @xmath234 & 7.135135@xmath229 & 8.417492@xmath226 & 2.114590@xmath235 + @xmath236 & 2.265630@xmath229 & 8.433365@xmath235 & 2.110610@xmath226 + qem + @xmath55 & 2.511190@xmath228 & 2.558357@xmath226 & 6.957412@xmath235 + @xmath231 & 2.382535@xmath229 & 2.305853@xmath233 & 7.719289@xmath232 + @xmath234 & 2.349116@xmath232 & 2.432084@xmath237 & 7.318639@xmath228 + @xmath236 & 3.232357@xmath226 & 2.176558@xmath227 & 8.177841@xmath238 + scale & + estimator & & & +   + em & 3.706033@xmath226 & 1.143094@xmath227 &  + mcem + @xmath55 & 1.139404@xmath238 & 2.133204@xmath228 & 5.358577@xmath237 + @xmath231 & 3.540433@xmath228 & 2.069714@xmath229 & 5.522955@xmath230 + @xmath234 & 1.137090@xmath228 & 2.130881@xmath232 & 5.364419@xmath233 + @xmath236 & 3.621140@xmath229 & 2.150602@xmath226 & 5.315227@xmath235 + qem + @xmath55 & 5.580507@xmath228 & 1.272262@xmath232 & 8.984739@xmath233 + @xmath231 & 5.890585@xmath229 & 1.418158@xmath235 & 8.060413@xmath226 + @xmath234 & 6.315578@xmath232 & 1.644996@xmath230 & 6.948917@xmath229 + @xmath236 & 9.699447@xmath226 & 4.529568@xmath227 & 2.523627@xmath238 +    first , a random sample of size @xmath239 was drawn from an normal distribution with @xmath240 and @xmath241 and the largest five have been right - censored .",
    "all the algorithms were stopped after 10 iterations ( @xmath242 ) .",
    "the results are presented in table  [ mcem : simulation1 ] . to help compare the mse",
    ", we also find the simulated relative efficiency ( sre ) which is defined as @xmath243 from the result , the em is as efficient as the mle ( the mse of the em is almost zero ) .",
    "compared to the mcem , the qem has much smaller mse and much higher efficiency .",
    "for example with @xmath236 , the sre of the mcem is only @xmath244 for @xmath245 and @xmath246 for @xmath247 . on the other hand ,",
    "the sre of the qem is @xmath248 for @xmath245 and @xmath249 for @xmath247 . comparing the results in table  [ mcem : simulation1 ]",
    ", the qem with only @xmath60 performs better than the mcem with @xmath61 .",
    "next , we draw a random sample of size @xmath239 from a rayleigh distribution with @xmath250 with the largest five being right - censored .",
    "we compare the qem only with the mcem because the em is not available .",
    "the results are presented in table  [ mcem : simulation2 ] for rayleigh data .",
    "the results also show that the qem clearly outperforms the mcem .",
    ".estimated biases , mse , and sre of estimators under consideration with rayleigh data .",
    "[ cols=\"<,^ , > , > \" , ]",
    "in this paper , we have shown that the qem algorithm offers clear advantages over the mcem .",
    "it reduces the computational burden required when using the mcem because a much smaller size is required . unlike the mcem",
    ", the qem also possesses very stable convergence properties at each step .",
    "the qem algorithm provides a flexible and useful alternative when one is faced with a difficulty with the implementation of the ordinary em algorithm .",
    "a variety of examples of application were also illustrated using the proposed method .",
    "analogous to the approach of @xcite , the uniqueness of the solution of ( [ eq : eebeta ] ) can be proved as follows . for convenience , letting @xmath251 we rewrite ( [ eq : eebeta ] ) by @xmath252 the function @xmath253 is strictly decreasing from @xmath254 to @xmath53 on @xmath255 $ ] , while @xmath256 is increasing because it follows from the jensen s inequality that @xmath257 \\ge 0.\\ ] ] now , it suffices to show that @xmath258 for some @xmath190 .",
    "since @xmath259 where @xmath260 , we have @xmath258 for some @xmath190 unless @xmath261 for all @xmath37 and @xmath262 . this condition is extremely unrealistic in practice .",
    "next , we provide upper and lower bounds of @xmath190 .",
    "these bounds guarantee the unique solution in the interval and enable the root search algorithm to find the solution very stably and easily .",
    "since @xmath256 is increasing , we have @xmath263 that is , @xmath264 denote the above lower bound by @xmath265 .",
    "then , since @xmath256 is again increasing , we have @xmath266 , which leads to @xmath267              freireich , e.  j. , gehan , e. , frei , e. , schroeder , l.  r. , wolman , i.  j. , anbari , r. , burgert , e.  o. , mills , s.  d. , pinkel , d. , selawry , o.  s. , moon , j.  h. , gendel , b.  r. , spurr , c.  l. , storrs , r. , haurani , f. , hoogstraten , b. , and lee , s. ( 1963 ) . the effect of 6-mercaptopurine on the duration of steroid - induced remissions in acute leukemia : a model for evaluation of other potentially useful therapy . , * 21 * , 699716 ."
  ],
  "abstract_text": [
    "<S> the expectation - maximization  ( em ) algorithm is a powerful computational technique for finding the maximum likelihood estimates for parametric models when the data are not fully observed . </S>",
    "<S> the em is best suited for situations where the expectation in each e - step and the maximization in each m - step are straightforward . </S>",
    "<S> a difficulty with the implementation of the em algorithm is that each e - step requires the integration of the posterior log - likelihood function . </S>",
    "<S> this can be overcome by the monte carlo em  ( mcem ) algorithm . </S>",
    "<S> this mcem uses a random sample to estimate the integral in each e - step . but </S>",
    "<S> this mcem converges very slowly to the true integral , which causes computational burden and instability . in this paper </S>",
    "<S> we present a quantile implementation of the expectation - maximization  ( qem ) algorithm . </S>",
    "<S> this proposed method shows a faster convergence and greater stability . </S>",
    "<S> the performance of the proposed method and its applications are numerically illustrated through monte carlo simulations and several examples .    </S>",
    "<S> * keywords : * em algorithm , grouped data , incomplete data , interval data , maximum likelihood , mcem , missing data , quantile . </S>"
  ]
}