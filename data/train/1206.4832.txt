{
  "article_text": [
    "@xcite provided the concept of entropy as a measure of information , or more precisely , a measure of uncertainty given by probability distributions .",
    "@xcite was the first to introduce the concept of generalized measures of information when he proposed the first well - known generalization of shannon entropy , known as @xmath1-entropy or rnyi entropy , based on kolmogorov - nagumo averages .",
    "several other generalizations have also been studied in the literature  @xcite , and have been extensively used in physics , communication theory and other disciplines .",
    "one of the most recently studied generalized information measure is the nonextensive entropy , due to  @xcite , defined as @xmath2 where @xmath3 is the probability mass function of the discrete random variable on the set @xmath4 , and the @xmath0-logarithm is defined as @xmath5 , @xmath6 , @xmath7 .",
    "it is called nonextensive because of its pseudo - additive nature  @xcite . although this generalization had been introduced earlier in  @xcite , tsallis provided interpretations in the context of statistical mechanics .",
    "@xcite generalized the shannon - khinchin axioms to the nonextensive case .",
    "the most important characteristic of these generalized information measures is that , on maximization , they give rise to power - law distributions , while maximization of shannon entropy gives rise to exponential distributions . the concept of maximization of information measure",
    "can be attributed to kullback s minimum discrimination theorem  @xcite , which establishes important connections between statistics and information theory .",
    "this theorem shows that exponential distributions can be obtained by minimizing kullback - leibler divergence under given moment constraints .",
    "one can consider maximum entropy  @xcite as a special case , where maximization of shannon entropy under moment constraints leads to exponential distributions .",
    "for example , given mean and variance of a random variable , maximum entropy gives rise to gaussian distribution .",
    "while exponential distributions have been extensively studied and used in statistical modeling , the power - law behavior has been observed in most of the real - world data , e.g. ,  @xcite .",
    "the importance of tsallis entropy can be attributed to its connections with power - law distributions , which has been studied in different contexts like finance , earthquakes and network traffic  @xcite .",
    "compared to the exponential family , the tsallis distributions , _",
    "i.e. _ , the family of distributions resulting from maximization of tsallis entropy , have an additional shape parameter @xmath0 , similar to that in   that controls the nature of the power - law tails .",
    "one of the most studied tsallis entropy maximizer is the @xmath0-gaussian distribution  @xcite , which is a power - law generalization of gaussian distribution . in this paper",
    ", we study the @xmath0-gaussian distribution in the context of smoothed functional algorithms for stochastic optimization .",
    "stochastic techniques play a key role in optimization problems , where the objective function does not have an analytic expression .",
    "such problems are often encountered in discrete event systems , which are quite common in engineering and financial world .",
    "most often , the data , obtained via statistical survey or simulation , contains only noisy estimates of the objective function to be optimized .",
    "one of the most commonly used solution methodologies involves stochastic approximation algorithms , originally due to  @xcite , which is used to find the zeros of a given function .",
    "based on this approach , gradient descent algorithms have been developed , in which the parameters controlling the system track the zeros of the gradient of the objective . however , these algorithms require an estimate of the cost gradient .",
    "@xcite provide such a gradient estimate using several parallel simulations of the system .",
    "more efficient techniques for gradient estimation , have been developed based on the smoothed functional approach  @xcite , simultaneous perturbation stochastic approximation  @xcite etc .",
    "a stochastic variation of newton - based optimization methods , also known as adaptive newton - based schemes has also been studied in literature  @xcite .    when the above schemes for gradient estimation are employed in optimization methods involving long - run average cost objective , the time complexity of the algorithms increase as the long - run average cost needs to be estimated after each parameter update .",
    "a more efficient approach is to simultaneously perform the long - run averaging and parameter updates using different step - size schedules .",
    "these classes of algorithms constitute the multi - timescale stochastic approximation algorithms  @xcite .",
    "two - timescale optimization algorithms have been developed using simultaneous perturbations  @xcite and smoothed functional  @xcite schemes .",
    "the main issue with such algorithms is that , although convergence of the algorithm to a local optimum is guaranteed , the global optimum is often not achieved in practice .",
    "@xcite proves that the gradient sf schemes based on gaussian perturbations converge to a local minimum , and provides comparison of the performance of various multi - timescale algorithms for stochastic optimization on a queuing system .",
    "the results presented there indicate that the performance of the sf algorithms depends considerably on several tuning parameters , such as the variance of the gaussian distribution , and also the step - sizes .",
    "the smoothed functional schemes for simulation based optimization have become popular due to their smoothing effects on local fluctuations .",
    "we derive for the first time , smoothed functional algorithms with power - law ( @xmath0-gaussian ) perturbations .",
    "prior work  @xcite indicated that the class of distributions that can be used for the perturbation random variables in sf algorithms includes gaussian , cauchy and uniform distributions .",
    "our main contribution is to show that the @xmath0-gaussian family of distributions belong to this class , and encompasses the above three distributions as special cases .",
    "this allows us to work with a larger class of distributions in sf algorithms , in an unified way , where the `` shape parameter '' of the @xmath0-gaussian controls its power - law behavior .",
    "this parameter also controls the smoothness of the convolution , thereby providing additional tuning .",
    "we show that the multivariate @xmath0-gaussian distribution satisfies all the conditions for smoothing kernels discussed in  @xcite",
    ". we then present estimators for gradient of a function using the @xmath0-gaussian smoothing kernel .",
    "we also present multi - timescale algorithms for stochastic optimization using @xmath0-gaussian based sf that incorporate gradient based search procedures , and prove the convergence of the proposed algorithms to the neighborhood of a local optimum .",
    "the convergence analysis presented in this paper differs from the approaches that have been studied earlier  @xcite . here",
    ", we provide a more straightforward technique using standard results from  @xcite .",
    "further , we perform simulations on a queuing network to illustrate the benefits of the @xmath0-gaussian based sf algorithms compared to their gaussian counterparts .",
    "a shorter version of this paper containing only the one - simulation @xmath0-gaussian sf algorithm , and without the convergence proof , has been presented in ieee 2012 international symposium on information theory  @xcite .",
    "the rest of the paper is organized as follows .",
    "the framework for the optimization problem and some preliminaries on sf and @xmath0-gaussians are presented in section  [ preliminaries ] .",
    "section  [ qgrad ] validates the use of @xmath0-gaussian as smoothing kernel , and presents gradient descent algorithms using @xmath0-gaussian sf .",
    "the convergence analysis of the proposed algorithms is discussed in section  [ gqsf_convergence ] .",
    "section  [ sim_results ] presents simulations based on a numerical setting .",
    "finally , section  [ conclusion ] provides the concluding remarks . in the appendix",
    ", we discuss a sampling technique for multivariate @xmath0-gaussians that is used in the proposed algorithms .",
    "let @xmath8 be a parameterized markov process , depending on a tunable parameter @xmath9 , where @xmath10 is a compact and convex subset of @xmath11 .",
    "let @xmath12 denote the transition kernel of @xmath13 when the operative parameter is @xmath9 .",
    "let @xmath14 be a lipschitz continuous cost function associated with the process .",
    "[ ergodic ] the process @xmath13 is ergodic for any given @xmath15 as the operative parameter , _",
    "i.e. _ , as @xmath16 , @xmath17,\\ ] ] where @xmath18 is the stationary distribution of @xmath13 .",
    "our objective is to minimize the long - run average cost @xmath19 by choosing an appropriate @xmath9 .",
    "the existence of the above limit is assured by assumption  [ ergodic ] and the fact that @xmath20 is continuous , hence measurable .",
    "in addition , we assume that the average cost @xmath21 satisfies the following requirement .",
    "[ differentiable ] the function @xmath22 is continuously differentiable for all @xmath9 .    a random sequence of parameter vectors , @xmath23 ,",
    "controlling a process @xmath24 , is said to be non - anticipative if the conditional probability @xmath25 almost surely for @xmath26 and all borel sets @xmath27 , where @xmath28 , @xmath26 are associated @xmath29-fields .",
    "one can verify that under a non - anticipative parameter sequence @xmath30 , the sequence @xmath31 is markov .",
    "we assume the existence of a stochastic lyapunov function .",
    "[ lyapunov ] let @xmath30 be a non - anticipative sequence of random parameters controlling the process @xmath13 , and @xmath32 @xmath33 , @xmath26 be a sequence of associated @xmath29-fields .",
    "there exists @xmath34 , a compact set @xmath35 , and a continuous function @xmath36 , with @xmath37 , such that    a.   @xmath38 < \\infty$ ] , and b.   @xmath39 \\leqslant v(y_n ) - \\epsilon_0 $ ] , whenever @xmath40 , @xmath26 .",
    "while assumption  [ differentiable ] is a technical requirement , assumption  [ lyapunov ] ensures that the process under a tunable parameter remains stable .",
    "assumption  [ lyapunov ] will not be required , for instance , if , in addition , the single - stage cost function @xmath20 is bounded .",
    "it can be seen that the sequence of parameters obtained using any of our algorithms below form a non - anticipative sequence .      here , we present the idea behind the smoothed functional approach proposed by @xcite .",
    "we consider a real - valued function @xmath41 , defined over a compact set @xmath10 . its smoothed functional",
    "is defined as @xmath42 = \\int\\limits_{-\\infty}^{\\infty}g_{\\beta}(\\eta)f(\\theta-\\eta)\\:\\mathrm{d}\\eta   = \\int\\limits_{-\\infty}^{\\infty}g_{\\beta}(\\theta-\\eta)f(\\eta)\\:\\mathrm{d}\\eta,\\ ] ] where @xmath43 is a kernel function , with a parameter @xmath44 taking values from @xmath45 .",
    "the idea behind using smoothed functionals is that if @xmath46 is not well - behaved , _",
    "i.e. _ , it has a fluctuating character , then @xmath47 $ ] is `` better - behaved '' .",
    "this can ensure that any optimization algorithm with objective function @xmath46 does not get stuck at a local minimum , but converges to a global minimum .",
    "the parameter @xmath44 controls the degree of smoothness .",
    "@xcite established that the sf algorithm achieves these properties if the kernel function satisfies the following sufficient conditions :    1 .",
    "@xmath48 , where @xmath49 corresponds to @xmath50 with @xmath51 , _",
    "i.e. _ , @xmath52 2 .",
    "@xmath53 is piecewise differentiable in @xmath54 , 3 .",
    "@xmath53 is a probability distribution function , _",
    "i.e. _ , @xmath47 = \\mathsf{e}_{g_{\\beta}(\\eta)}[f(\\theta-\\eta)]$ ] , 4 .",
    "@xmath55 , where @xmath56 is the dirac delta function , and 5 .",
    "@xmath57 = f(\\theta)$ ] .",
    "a two - sided form of sf is defined as @xmath58   & = \\frac{1}{2}\\int\\limits_{-\\infty}^{\\infty}g_{\\beta}(\\eta)\\big(f(\\theta-\\eta)+f(\\theta+\\eta)\\big)\\:\\mathrm{d}\\eta \\nonumber \\\\&= \\frac{1}{2}\\int\\limits_{-\\infty}^{\\infty}g_{\\beta}(\\theta-\\eta)f(\\eta)\\:\\mathrm{d}\\eta + \\frac{1}{2}\\int\\limits_{-\\infty}^{\\infty}g_{\\beta}(\\eta-\\theta)f(\\eta)\\:\\mathrm{d}\\eta\\;. \\label{sf2}\\end{aligned}\\ ] ] the gaussian distribution satisfies the above conditions , and has been used as a smoothing kernel @xcite .",
    "the sf approach provides a method for estimating the gradient of any function , which satisfies assumptions  [ ergodic][lyapunov ]  @xcite .",
    "@xcite uses the gaussian smoothing , and derives a gradient estimator from   as @xmath59 for large @xmath60 , @xmath61 and small @xmath44 .",
    "the stochastic process @xmath62 is governed by parameter @xmath63 , where @xmath64 is obtained through an iterative scheme , and @xmath65 is a @xmath66-dimensional vector of i.i.d .",
    "standard gaussian random variable .",
    "similarly , a two - simulation gradient estimator has been suggested using , which is of the following form @xmath67 for large @xmath60 , @xmath61 and small @xmath44 , where @xmath62 and @xmath68 are two processes governed by parameters @xmath63 and @xmath69 , respectively , @xmath70 and @xmath71 being as before .",
    "a continuous form of the shannon entropy , also known as differential entropy , has been extensively studied in statistical mechanics , probability and statistics .",
    "it is defined as @xmath72 where @xmath73 is a p.d.f .",
    "defined on the sample space @xmath4 . following the lines of discrete form generalization given in  @xcite",
    ", @xcite provides a measure theoretic formulation of continuous form tsallis entropy functional , defined as @xmath74 this function results when the natural logarithm in   is replaced by the @xmath0-logarithm defined earlier .",
    "the differential shannon entropy can be retrieved from   as @xmath75 .",
    "the @xmath0-gaussian distribution was developed to describe the process of lvy super - diffusion  @xcite , but has been later studied in other fields , such as finance  @xcite and statistics  @xcite .",
    "its importance lies in its power - law nature , due to which the tails of the @xmath0-gaussian decay at a slower rate than the gaussian distribution , depending on @xmath0 .",
    "it results from maximizing tsallis entropy under certain ` deformed ' moment constraints , known as normalized @xmath0-expectation defined by @xmath76 this form of an expectation considers an escort distribution @xmath77 , and has been shown to be compatible with the foundations of nonextensive statistics  @xcite . @xcite",
    "maximized tsallis entropy under the constraints , @xmath78 and @xmath79 , which are known as @xmath0-mean and @xmath0-variance , respectively .",
    "these are generalizations of standard first and second moments , and tend to the usual mean and variance , respectively , as @xmath80 .",
    "this results in the @xmath0-gaussian distribution is used to maintain consistency with smoothed functional notations , which is used in section  [ qgrad ] . ] that has the form @xmath81 where , @xmath82 is called the tsallis cut - off condition  @xcite , which ensures that the above expression is defined , and @xmath83 is the normalizing constant , which is given by @xmath84 with @xmath85 being the gamma function , which exists over the specified intervals . the function",
    "defined in   is not integrable for @xmath86 , and hence , @xmath0-gaussian is a probability density function only when @xmath87 .",
    "a multivariate form of the @xmath0-gaussian distribution has been discussed in  @xcite .",
    "considering the @xmath0-mean and @xmath0-covariance matrix to be @xmath88 and @xmath89 respectively , the @xmath66-variate @xmath0-gaussian distribution can be expressed as @xmath90 for all @xmath91 , where the normalizing constant @xmath92    as in the one - dimensional case , the distribution is only defined for @xmath93 .",
    "can be mapped to the multivariate students-@xmath94 distribution .",
    "the support set of the above distribution is given by @xmath95 the multivariate normal distribution can be obtained as a special case when @xmath80 , while for @xmath96 , we obtain the uniform distribution with an infinitesimally small support around its mean .",
    "it is interesting to note that for @xmath97 , these distributions have a one - to - one correspondence with students-@xmath94 distribution , and in particular , for @xmath98 , we retrieve the cauchy distribution .",
    "a similar distribution can also be obtained by maximizing rnyi entropy  @xcite . in this paper",
    ", we study the multivariate @xmath0-gaussian distribution as a smoothing kernel , and develop smoothed functional algorithms based on it .",
    "the first step in applying @xmath0-gaussians for sf algorithms is to ensure that the distribution satisfies the rubinstein conditions ( properties ( p1)(p5 ) in section  [ preliminaries_sf ] ) .",
    "the rest of the paper uses the multivariate form of @xmath0-gaussian  , with the @xmath0-mean @xmath99 , and @xmath0-covariance matrix @xmath100 , _ i.e. _ , the components are uncorrelated .",
    "@xmath66-dimensional @xmath0-gaussian distribution , with @xmath0-covariance @xmath101 satisfies the kernel properties ( p1)(p5 ) for all @xmath102 , @xmath7 .    1 .   from  ,",
    "it is evident that @xmath103 .",
    "2 .   for @xmath104 , @xmath105 for all @xmath106 . thus , @xmath107 for @xmath108 ,   holds when @xmath109 . on the other hand , when @xmath110 , we have @xmath111 and hence , @xmath112 .",
    "thus , @xmath113 is differentiable for @xmath97 , and piecewise differentiable for @xmath108 .",
    "3 .   @xmath113 is a distribution for @xmath102 and hence , the corresponding sf @xmath114 , parameterized by both @xmath0 and @xmath44 , can be written as @xmath115 = \\mathsf{e}_{g_{q,\\beta}(x)}[f(\\theta - x)]$ ] .",
    "4 .   @xmath116 is a distribution satisfying @xmath117 .",
    "so , @xmath118 . 5 .",
    "this property trivially holds due to convergence in mean as @xmath119 = \\int\\limits_{-\\infty}^{\\infty}{\\lim_{\\beta\\to0}g_{q,\\beta}(x)f(\\theta - x)dx } = \\int\\limits_{-\\infty}^{\\infty}{\\delta(x)f(\\theta - x)\\:\\mathrm{d}x } = f(\\theta).\\ ] ]    hence the claim .    from the above result",
    ", it follows that @xmath0-gaussian can be used as a kernel function , and hence , given a particular value @xmath120 and some @xmath121 , the one - sided and two - sided sfs of any function @xmath122 are respectively given by @xmath123 & = \\int\\limits_{\\omega_q } g_{q,\\beta}(\\theta - x ) f(x ) \\:\\mathrm{d}x , \\\\s'_{q,\\beta}[f(\\theta ) ]   & = \\frac{1}{2}\\int\\limits_{\\omega_q}g_{q,\\beta}(\\theta - x)f(x)\\:\\mathrm{d}x + \\frac{1}{2}\\int\\limits_{\\omega_q}g_{q,\\beta}(x-\\theta)f(x)\\:\\mathrm{d}x , \\label{qg_sf2_defn}\\end{aligned}\\ ] ] where the nature of the sfs are controlled by both @xmath0 and @xmath44 .",
    "the objective is to estimate the gradient of the average cost @xmath124 using the sf approach , where existence of @xmath124 follows from assumption  [ differentiable ] .",
    "the gradient of smoothed functional ( smoothed gradient ) is defined as  @xcite @xmath125 =   \\int\\limits_{\\omega_q } \\nabla_{\\theta}g_{q,\\beta}(\\theta-\\eta ) j(\\eta ) \\:\\mathrm{d}\\eta\\;,\\ ] ] where @xmath126 is the support set defined as in  . as there is no functional relationship between @xmath15 and @xmath54 over @xmath126 , _",
    "i.e. _ , @xmath127 for all @xmath128 , @xmath129 where @xmath130 . hence ,",
    "substituting @xmath131 , and using the symmetry of @xmath132 and @xmath133 , we can write @xmath134   & = \\left(\\frac{2}{\\beta(n+2-nq)}\\right)\\int\\limits_{\\omega_q }   \\frac{\\eta'}{\\rho{(\\eta ' ) } } g_{q}(\\eta ' ) j(\\theta+\\beta\\eta ' ) \\:\\mathrm{d}\\eta ' \\nonumber \\\\&= \\left(\\frac{2}{\\beta(n+2-nq)}\\right)\\mathsf{e}_{g_{q}(\\eta ' ) } \\left[\\left .",
    "\\frac{\\eta'}{\\rho{(\\eta ' ) } } j(\\theta+\\beta\\eta ' ) \\right|",
    "\\theta \\right]\\;. \\label{grad_sf1_formula}\\end{aligned}\\ ] ]    in the sequel ( proposition  [ grad_sf1_convergence ] ) , we show that @xmath135-\\nabla_{\\theta}j(\\theta)\\right\\vert \\to 0 $ ] as @xmath136 .",
    "hence , for large @xmath60 and small @xmath44 , the form of gradient estimate suggested by   is @xmath137 where @xmath138 are uncorrelated identically distributed standard @xmath0-gaussian distributed random vectors . considering that in two - timescale algorithms ( discussed later ) , the value of @xmath15 is updated concurrently with the gradient estimation procedure",
    ", we estimate @xmath139 at each stage . by ergodicity assumption ( assumption  [ ergodic ] ) , we can write   as @xmath140 for large @xmath61 , where the process @xmath62 has the same transition kernel as defined in assumption  [ ergodic ] , except that it is governed by parameter @xmath141 .      in a similar manner , based on  , the gradient of the two - sided sf can be written as @xmath142   = \\frac{1}{2}\\int\\limits_{\\omega_q } \\nabla_{\\theta}g_{\\beta}(\\theta-\\eta)j(\\eta)\\:\\mathrm{d}\\eta + \\frac{1}{2}\\int\\limits_{\\omega_q } \\nabla_{\\theta}g_{\\beta}(\\eta-\\theta)j(\\eta)\\:\\mathrm{d}\\eta.\\ ] ] the first integral can be obtained as in  .",
    "the second integral is evaluated as @xmath143 where @xmath144 .",
    "thus , we obtain the gradient as a conditional expectation @xmath142 = \\left(\\frac{1}{\\beta(n+2-nq)}\\right)\\mathsf{e}_{g_{q}(\\eta ) } \\left[\\left.\\frac{\\eta}{\\rho{(\\eta ) } } \\big(j(\\theta+\\beta\\eta)-j(\\theta-\\beta\\eta)\\big)\\right|\\theta\\right ] . \\label{grad_sf2_formula}\\ ] ] in sequel ( proposition  [ grad_sf2_convergence ] ) we show that @xmath145-\\nabla_{\\theta}j(\\theta)\\right\\vert \\to 0 $ ] as @xmath136 , which can be used to approximate  , for large @xmath60 , @xmath61 and small @xmath44 , as @xmath146 where @xmath62 and @xmath68 are governed by @xmath141 and @xmath147 respectively .",
    "we propose two - timescale algorithms based on the estimates obtained in   and .",
    "let @xmath148 and @xmath149 be two step - size sequences satisfying the following .",
    "[ stepsize ] @xmath148 and @xmath149 are two positive step - size sequences satisfying @xmath150 , @xmath151 , @xmath152 and @xmath153 , _",
    "i.e. _ , @xmath154 as @xmath155 .",
    "it must be noted that in the algorithms , although @xmath60 is chosen to be a large quantity ( to ensure convergence ) , the quantity @xmath61 is arbitrarily picked and can be any finite positive number .",
    "the averaging of the inner summation in   and   is obtained in our algorithms using two - timescale stochastic approximation . in principle",
    ", one may select @xmath156 .",
    "however , it is generally observed that a value of @xmath61 typically between @xmath157 and @xmath158 results in better performance  @xcite .",
    "further , the algorithms require generation of @xmath66-dimensional random vectors , consisting of uncorrelated @xmath0-gaussian distributed random variates .",
    "this method is described in appendix .    for @xmath159 ,",
    "let @xmath160 @xmath161 represent the projection of @xmath15 onto the set @xmath10 . for simulation",
    ", we need to project the perturbed random vectors @xmath63 onto @xmath10 using the above projection .",
    "the quantities @xmath162 are used to estimate @xmath124 in the recursions .",
    "fix @xmath60 , @xmath61 , @xmath0 and @xmath44 set @xmath163 fix the parameter vector @xmath164 output @xmath165 as the final parameter vector    the g@xmath0-sf2 algorithm is similar to the g@xmath0-sf1 algorithm , except that we use two parallel simulations @xmath166 and @xmath167 governed with parameters @xmath63 and @xmath69 respectively , and update the gradient estimate , in step 9 , using the single - stage cost function of both simulations as in  .",
    "fix @xmath60 , @xmath61 , @xmath0 and @xmath44 set @xmath163 fix the parameter vector @xmath164 output @xmath165 as the final parameter vector",
    "we provide here a more straightforward technique to prove that the algorithms converge to a local optimum as compared to  @xcite . before presenting the details of convergence analysis , we present the following result on @xmath0-gaussians .",
    "it provides an expression for the moments of @xmath66-variate @xmath0-gaussian distributed random vector .",
    "this is a consequence of the results presented in  @xcite .",
    "this result plays a key role in the proofs discussed below .",
    "[ moments ] suppose @xmath168 is a random vector , where the components are uncorrelated and identically distributed , each being distributed according to a @xmath0-gaussian distribution with zero @xmath0-mean and unit @xmath0-variance , with parameter @xmath169 .",
    "also , let @xmath170 .",
    "then , for any @xmath171 , we have @xmath172 = \\left\\{\\begin{array}{l }   \\bar{k}\\displaystyle\\left(\\frac{n+2-nq}{|1-q|}\\right)^{\\sum\\limits_{i=1}^{n}\\frac{b_i}{2 } }   \\left(\\prod\\limits_{i=1}^{n}\\frac{b_i!}{2^{b_i}\\left(\\frac{b_i}{2}\\right)!}\\right ) \\\\",
    "\\hspace{10mm}\\text{if } b_i \\text { is even for all } i = 1,2,\\ldots , n , \\\\\\\\",
    "0    \\hspace{8mm}\\text{otherwise , } \\end{array}\\right . \\label{moments_eqn}\\ ] ] where @xmath173 exists only if the arguments in the above gamma functions are positive , which holds for @xmath174 if @xmath108 , and @xmath175 if @xmath104 .",
    "since @xmath176 , and @xmath177 is non - negative over @xmath126 , we have @xmath178 \\\\&= \\frac{1}{k_{q , n } } \\int\\limits_{\\omega_q } \\left(x^{(1)}\\right)^{b_1}\\left(x^{(2)}\\right)^{b_2}\\ldots\\left(x^{(n)}\\right)^{b_n }   \\left(1 - \\frac{(1-q)}{\\big(n+2-nq\\big)}\\vert{x}\\vert^2\\right)^{\\frac{1}{1-q}-b}\\:\\mathrm{d}x . \\end{aligned}\\ ] ]    the second equality in   can be easily proved .",
    "if for some @xmath179 , @xmath180 is odd , then the above function is odd , and its integration is zero over @xmath126 , which is symmetric with respect to any axis by definition . for the other cases , since the function is even , the integral is same over every orthant .",
    "hence , we may consider the integration over the first orthant , _",
    "i.e. _ , where each component is positive . for @xmath108 , we can reduce the above integral , using  ( * ? ? ? * equation ( 4.635 ) ) , to obtain @xmath181 & =   \\frac{\\prod\\limits_{i=1}^{n } \\gamma\\left(\\frac{b_i+1}{2}\\right)}{k_{q , n}\\gamma ( \\bar{b } ) } \\left(\\frac{n+2-nq}{1-q}\\right)^{\\bar{b } }   \\int\\limits_{0}^{1 } ( 1-y)^{\\left(\\frac{1}{1-q}-b\\right)}y^{\\left(\\bar{b}-1\\right)}\\mathrm{d}x   \\label{integral } \\ ] ] where we set @xmath182 .",
    "one can observe that the integral in   is in the form of a beta function .",
    "since @xmath180 s are even , we can expand @xmath183 using the expansion of gamma function of half - integers to get @xmath184 .",
    "the claim can be obtained by substituting @xmath185 from   and using the relation @xmath186 .",
    "it is easy to verify that all the gamma functions in the equality are positive provided @xmath174 .",
    "the result for the interval @xmath104 can be proved in a similar way ( see equations ( 4.635 ) and ( 4.636 ) of  @xcite ) .",
    "however , in this case the gamma functions are positive if @xmath187 satisfy the mentioned condition .",
    "it may be noted here that this is always true for any dimension if @xmath188 since @xmath0-gaussians are defined only when @xmath189 .",
    "it is easy to verify the following result in the limiting case of @xmath80 .",
    "this would help to ensure that the convergence analysis done in this paper also holds in the case of gaussian sf .",
    "[ moments_coro ] in the limiting case , as @xmath80 , @xmath190 = \\prod_{i=1}^{n } \\mathsf{e}_{g(x ) } \\left[\\left(x^{(i)}\\right)^{b_i}\\right]\\;.\\ ] ]      first , let us consider the update along the faster timescale , _",
    "i.e. _ , step  9 of the g@xmath0-sf1 algorithm .",
    "we define @xmath191 , @xmath192 and @xmath193 for @xmath194 , @xmath26 .",
    "it follows from assumption  [ stepsize ] that @xmath195 , @xmath196 and @xmath197 .",
    "we can rewrite step  9 of algorithm 1 as the following iteration for all @xmath198 @xmath199,\\ ] ] where @xmath200 here , @xmath201 is defined as in  , and @xmath202 is a markov process parameterized by @xmath203 .",
    "let @xmath204 denote the @xmath29-field generated by the mentioned quantities .",
    "we can observe that @xmath205 is a filtration , where @xmath206 is @xmath207-measurable for each @xmath198 .",
    "we summarize the results presented in  ( * ? ? ?",
    "* chapter 6 , lemma 3  theorem 9 ) in the following theorem .",
    "this result leads to the stability and convergence of iteration  , which runs on the faster timescale .",
    "[ borkar_cor_8 ] consider the iteration , @xmath208 $ ] .",
    "let the following conditions hold :    1 .",
    "@xmath202 is a markov process satisfying assumptions  [ ergodic ] and  [ lyapunov ] , 2 .   for each @xmath106 and @xmath209 for all @xmath210",
    ", @xmath211 has a unique invariant probability measure @xmath212 , 3 .",
    "@xmath213 are step - sizes satisfying @xmath214 and @xmath215 , 4 .",
    "@xmath216 is lipschitz continuous in its first argument uniformly w.r.t the second , 5 .",
    "@xmath217 is a martingale difference noise term with bounded variance , 6 .   if @xmath218 $ ] , then the limit @xmath219 exists uniformly on compacts , and 7 .",
    "the ode @xmath220 is well - posed and has the origin as the unique globally asymptotically stable equilibrium",
    ".    then the update @xmath221 satisfies @xmath222 , almost surely , and converges to the stable fixed points of the ordinary differential equation ( ode ) @xmath223    rewriting the update   as @xmath224 - z(p ) + a_p\\big ] , \\label{faststep1}\\ ] ] where @xmath225 $ ] is @xmath207-measurable .",
    "the following result shows that @xmath226 satisfies condition  5 in theorem  [ borkar_cor_8 ] .",
    "[ a_n_convergence ] for all values of @xmath120 , @xmath227 is a martingale difference sequence with a bounded variance .",
    "it is easy to see that for all @xmath198 , @xmath228 = 0 $ ] .",
    "so @xmath227 is a martingale difference sequence .",
    "expanding the terms , we have @xmath229 \\\\&\\leqslant \\frac{8}{\\beta^2(n+2-nq)^2 } \\mathsf{e}\\left[\\left .",
    "\\left(\\frac{\\vert\\tilde{\\eta}(p)\\vert h(y_p)}{\\rho{(\\tilde{\\eta}(p))}}\\right)^2 + \\left(\\left.\\mathsf{e}\\left[\\frac{\\vert\\tilde{\\eta}(p)\\vert h(y_p)}{\\rho{(\\tilde{\\eta}(p))}}\\right|\\mathcal{g}_{p-1}\\right]\\right)^2 \\right|\\mathcal{g}_{p-1}\\right].\\end{aligned}\\ ] ] applying conditional jensen s inequality on the second term , we obtain @xmath230 & \\leqslant \\frac{16}{\\beta^2(n+2-nq)^2 }   \\mathsf{e}\\left[\\left.\\frac{\\vert\\tilde{\\eta}(p)\\vert^2}{{\\rho{(\\tilde{\\eta}(p))}^2}}{h^2(y_p)}\\right| \\mathcal{g}_{p-1}\\right ] . \\label{jensen_g1}\\end{aligned}\\ ] ] for @xmath231 , we use holder s inequality to write   as @xmath230 & \\leqslant \\frac{16}{\\beta^2(n+2-nq)^2 }   \\sup\\limits_{\\eta } \\left(\\frac{\\vert\\tilde{\\eta}(p)\\vert^2}{{\\rho{(\\tilde{\\eta}(p))}^2}}\\right ) \\mathsf{e}\\left[\\left.{h^2(y_p)}\\right| \\mathcal{g}_{p-1}\\right ] \\\\&\\leqslant \\frac{16}{\\beta^2 ( 1-q)(n+2-nq ) } \\mathsf{e}\\left[\\left.{h^2(y_p)}\\right| \\mathcal{g}_{p-1}\\right].\\end{aligned}\\ ] ] since , @xmath232 and @xmath233 for all @xmath234 . by lipschitz continuity of @xmath20",
    ", there exists @xmath235 such that @xmath236 for all @xmath3 , and hence , by assumption  [ lyapunov ] , we can claim @xmath237 \\leqslant 2\\alpha_1 ^ 2\\left ( 1 + \\mathsf{e } \\left[\\vert{y_p}\\vert^2|\\mathcal{g}_{p-1}\\right]\\right ) < \\infty \\text { a.s .",
    "} \\label{eq_jensen_g1}\\end{aligned}\\ ] ] on the other hand , for @xmath238 , we apply cauchy - schwartz inequality for each of the components in   to obtain @xmath230 & \\leqslant \\frac{16}{\\beta^2(n+2-nq)^2 } \\sum_{j=1}^{n } \\mathsf{e}\\left[\\left.\\frac{\\left(\\tilde{\\eta}^{(j)}(p)\\right)^2}{{\\rho{(\\tilde{\\eta}(p))}^2}}{h^2(y_p)}\\right| \\mathcal{g}_{p-1}\\right ] \\\\&\\leqslant \\frac{16}{\\beta^2(n+2-nq)^2 } \\sum_{j=1}^{n } \\mathsf{e}\\left[\\frac{\\left(\\tilde{\\eta}^{(j)}(p)\\right)^4}{{\\rho{(\\tilde{\\eta}(p))}^4}}\\right]^{1/2 } \\mathsf{e}\\left[\\left.{h^4(y_p)}\\right| \\mathcal{g}_{p-1}\\right]^{1/2}\\;.\\end{aligned}\\ ] ]    the second expectation can be shown to be finite as in  , while we apply proposition  [ moments ] to study the existence of @xmath239 $ ] .",
    "we can observe that in this case , @xmath240 and @xmath241 if @xmath242 , otherwise @xmath243 . and so @xmath188",
    ". proposition  [ moments ] ensures that the term is finite , and hence , the claim .",
    "we can write the parameter update ( step  13 of g@xmath0-sf1 ) as @xmath244 where @xmath245 since @xmath246 .",
    "thus , the parameter update recursion can be seen to track the ode @xmath247 hence , the recursion @xmath70 , @xmath26 appears quasi - static when viewed from the timescale of @xmath248 , and hence , in the update  , one may let @xmath249 and @xmath250 for all @xmath210 . consider the following ode @xmath251    [ z_bounded ] the sequence @xmath252 is uniformly bounded with probability 1 .",
    "further , @xmath253 almost surely as @xmath254 .    it can be easily verified that iteration   satisfies all the conditions of theorem  [ borkar_cor_8 ] .",
    "thus , by theorem  [ borkar_cor_8 ] , @xmath252 converges to ode   as @xmath255 = \\frac{2\\eta j(\\theta+\\beta\\eta)}{\\beta(n+2-nq)\\rho{(\\eta)}}\\;.\\ ] ] we can also see that @xmath256 all the conditions in theorem  [ borkar_cor_8 ] are seen to be verified and the claim follows .    from lemma  [ z_bounded ] , steps 13 and 15 of g@xmath0-sf1 can be written as @xmath257\\right )     \\nonumber \\\\&= \\mathcal{p}_c \\bigg(\\theta(n ) + a(n)\\left[- \\nabla_{\\theta(n ) } j\\big(\\theta(n)\\big ) + \\delta \\big(\\theta(n)\\big ) + \\xi_n\\right]\\bigg ) , \\label{slowstep}\\end{aligned}\\ ] ] where the error in the gradient estimate is given by @xmath258\\ ] ] and the noise term is @xmath259 - \\frac{2\\eta(n)j\\big(\\theta(n)+\\beta\\eta(n)\\big)}{\\beta(n+2-nq)\\rho{(\\eta(n ) ) } }             \\nonumber \\\\&= \\frac{2}{\\beta(n+2-nq)}\\bigg ( \\mathsf{e}_{g_q(\\eta)}\\left[\\left.\\frac{\\eta(n)}{\\rho{(\\eta(n))}}j\\big(\\theta(n)+\\beta\\eta(n)\\big)\\right| \\theta(n)\\right ] - \\frac{\\eta(n)}{\\rho{(\\eta(n))}}j\\big(\\theta(n)+\\beta\\eta(n)\\big)\\bigg ) , \\label{noise_term}\\end{aligned}\\ ] ] which is a martingale difference term .",
    "let @xmath260 denote the @xmath29-field generated by the mentioned quantities .",
    "we can observe that @xmath261 is a filtration , where @xmath262 are @xmath263-measurable for each @xmath26 .",
    "we state the following result due  ( * ? ? ?",
    "* theorem  5.3.1 , pp 189196 ) , adapted to our scenario , which leads to the convergence of the updates in  .",
    "[ kushner_thm_5.3.1 ] given the iteration , @xmath264 , where    1 .",
    "@xmath265 represents a projection operator onto a closed and bounded constraint set @xmath10 , 2 .",
    "@xmath266 is a continuous function , 3 .",
    "@xmath267 is a positive sequence satisfying @xmath268 , @xmath269 , and 4 .",
    "@xmath270 converges a.s .    under the above conditions , the update @xmath271 converges to the asymptotically stable fixed points of the ode @xmath272 where @xmath273 .",
    "the next result shows that the noise term @xmath274 satisfies the last condition in lemma  [ kushner_thm_5.3.1 ] , while the subsequent result proves the error term @xmath275 is considerably small .",
    "[ xi_n_convergence ] let @xmath276 .",
    "then , for all values of @xmath120 , @xmath277 is an almost surely convergent martingale sequence .",
    "we can easily observe that for all @xmath278 , @xmath279 $ ] @xmath280 - \\mathsf{e}\\left[\\left.\\frac{\\eta(k)j\\big(\\theta(k)+\\beta\\eta(k)\\big)}{\\beta\\rho{(\\eta(k))}}\\right|\\mathcal{f}_k\\right ] \\bigg ) .",
    "\\end{aligned}\\ ] ] so @xmath279 = 0 $ ] , since @xmath281 is @xmath282-measurable , whereas @xmath283 is independent of @xmath282 .",
    "it follows that @xmath284 is a martingale difference sequence , and hence @xmath277 is a martingale sequence .",
    "now , use of conditional jensen s inequality leads to @xmath285 & = \\sum_{j=1}^n \\mathsf{e}\\left[\\left.\\left(\\xi_{k}^{(j)}\\right)^2\\right|\\mathcal{f}_{k}\\right ] \\\\&\\leqslant \\frac{16}{\\beta^2(n+2-nq)^2 } \\sum_{j=1}^n \\mathsf{e}\\left[\\left.\\frac{\\left(\\eta(k)^{(j)}\\right)^2}{{\\rho{(\\eta(k))}^2}}{j\\big(\\theta(k)+\\beta\\eta(k)\\big)^2}\\right|\\theta(k)\\right ] . \\ ] ]    for any @xmath286 , by definition @xmath287 $ ] , where the expectation is with respect to the stationary measure . by jensen s inequality",
    ", we can claim @xmath288 $ ] and @xmath289 $ ] for all @xmath286 . using these facts along with arguments similar to lemma  [ a_n_convergence ]",
    ", it can be seen that @xmath290   < \\infty$ ] for all @xmath291 , and hence , if @xmath292 , @xmath293 & = \\sum_{n=0}^{\\infty } a(n)^2\\mathsf{e}\\left[\\vert\\xi_{n}\\vert^2\\right ] \\leqslant \\sum_{n=0}^{\\infty } a(n)^2 \\sup_n \\mathsf{e}\\left[\\vert\\xi_{n}\\vert^2\\right ] < \\infty \\text { a.s.}\\end{aligned}\\ ] ] the claim follows from martingale convergence theorem  @xcite .",
    "[ grad_sf1_convergence ] for a given @xmath294 , @xmath7 , and for all @xmath9 , the error term @xmath295-\\nabla_{\\theta}j(\\theta)\\big\\vert = o(\\beta).\\ ] ]    for small @xmath121 , using taylor series expansion of @xmath296 around @xmath9 , @xmath297 so we can write   as @xmath298 \\nonumber = \\frac{2}{(n+2-nq)}\\bigg ( \\frac{j(\\theta)}{\\beta}\\mathsf{e}_{g_q(\\eta)}\\left[\\frac{\\eta}{\\rho{(\\eta)}}\\right ]   + \\mathsf{e}_{g_q(\\eta)}\\left[\\frac{\\eta\\eta^t}{\\rho{(\\eta)}}\\right]\\nabla_{\\theta}j(\\theta )   \\\\&\\hspace{60 mm } + \\frac{\\beta}{2}\\mathsf{e}_{g_q(\\eta)}\\left[\\left.\\frac{\\eta\\eta^{t}\\nabla_{\\theta}^2j(\\theta)\\eta}{\\rho{(\\eta)}}\\right|\\theta\\right ] + o(\\beta)\\bigg)\\;. \\label{gqsf1_expand}\\end{aligned}\\ ] ] we consider each term in  .",
    "the @xmath299 component in the first term is @xmath300 = 0 $ ] by proposition  [ moments ] for all @xmath301 .",
    "similarly , the @xmath299 component in the third term can be written as @xmath302^{(i ) } =   \\frac{\\beta}{2}\\sum\\limits_{j=1}^{n}\\sum\\limits_{k=1}^{n}\\left[\\nabla_{\\theta}^2j(\\theta)\\right]_{j , k } \\mathsf{e}_{g_q(\\eta)}\\left[\\frac{\\eta^{(i)}\\eta^{(j)}\\eta^{(k)}}{\\rho{(\\eta)}}\\right].\\ ] ] it can be observed that in all cases , each term in the summation is an odd function , and so from proposition  [ moments ] , we can show that the third term in   is zero . using a similar argument",
    ", we claim that the off - diagonal terms in @xmath303 $ ] are zero , while the diagonal terms are of the form @xmath304 $ ] , which exists for all @xmath305 as the conditions in proposition  [ moments ] are always satisfied on this interval .",
    "further , @xmath306 = \\frac{(n+2-nq)}{2}\\;. \\label{grad_expand_term}\\ ] ] the claim follows by substituting the above expression in  .",
    "now , we consider the following ode for the slowest timescale recursion @xmath307 where @xmath308 . in accordance with lemma  [ kushner_thm_5.3.1 ]",
    ", it can be observed that the stable points of lie in the set @xmath309    we have the following key result which shows that iteration   tracks ode  , and hence , the convergence of our algorithm is proved .",
    "[ thm_gqsf1 ] under assumptions  [ ergodic ]  [ stepsize ] , given @xmath310 and @xmath305 , there exists @xmath311 such that for all @xmath312 $ ] , the sequence @xmath30 obtained from g@xmath0-sf1 converges to a point in the @xmath313-neighborhood of the stable attractor of  , defined as @xmath314 with probability 1 as @xmath155 .",
    "it immediately follows from lemmas  [ kushner_thm_5.3.1 ] and  [ xi_n_convergence ] that the update in   converges to the stable fixed points of the ode @xmath315 now starting from the same initial condition , the trajectory of   converges to that of   uniformly over compacts , as @xmath316 .",
    "since from proposition  [ grad_sf1_convergence ] , we have @xmath317 for all @xmath318 , the claim follows . it may be noted that we can arrive at the same claim more technically using hirsch s lemma  @xcite .",
    "since the proof of convergence here is along the lines of g@xmath0-sf1 , we do not describe it explicitly .",
    "we just briefly describe the modifications that are required in this case . in the faster timescale , as @xmath155 , the updates given by @xmath319 track the function @xmath320 so we can rewrite the slower timescale update for g@xmath0-sf2 algorithm , in a similar manner as  , where the noise term @xmath274 has two components , due to the two parallel simulations , each being bounded ( as in lemma  [ xi_n_convergence ] ) .",
    "we have the following proposition for the error term @xmath321-\\nabla_{\\theta}j(\\theta)\\;.\\ ] ]    [ grad_sf2_convergence ] for a given @xmath294 , @xmath7 , and for all @xmath9 ,    @xmath322-\\nabla_{\\theta}j(\\theta)\\big\\vert = o(\\beta).\\ ] ]    using taylor s expansion , we have for small @xmath44 , @xmath323 one can use similar arguments as in proposition  [ grad_sf1_convergence ] to rewrite   as @xmath324   & = \\frac{1}{(n+2-nq ) } \\mathsf{e}_{g_q(\\eta)}\\left[\\frac{2}{\\rho{(\\eta)}}\\eta\\eta^t\\right]\\nabla_{\\theta}j(\\theta ) + o(\\beta),\\end{aligned}\\ ] ] which leads to the claim .",
    "finally , we have a similar result to prove the convergence of the g@xmath0-sf2 algorithm .    [ thm_gqsf2 ] under assumptions  [ ergodic ]  [ stepsize ] ,",
    "for @xmath310 and @xmath305 , there exists @xmath311 such that for all @xmath312 $ ] , the sequence @xmath30 obtained from g@xmath0-sf2 converges to a point in the @xmath313-neighborhood of the stable attractor of  , with probability 1 as @xmath155 .",
    "theorems  [ thm_gqsf1 ] and  [ thm_gqsf2 ] give the existence of some @xmath325 for a given @xmath310 such that the gradient - descent algorithms converge to @xmath313-neighborhood of a local minimum .",
    "however , these results do not give the precise value of @xmath326 .",
    "further , they do not guarantee that this neighborhood lies within a close proximity of a global minimum .",
    "we make a note on the analysis for gaussian sf algorithms .",
    "though the above results exclude the case @xmath327 , it is easy to verify that all the claims hold as @xmath328 due to corollary  [ moments_coro ] . hence , the above convergence analysis provides an alternative to the analysis presented in  @xcite for gaussian sf algorithms .",
    "we consider a multi - node network of @xmath329 queues with feedback as shown in the figure below .",
    "there are @xmath330 nodes , which are fed with independent poisson external arrival processes with rates @xmath331 , respectively . after departing from the @xmath299 node , a customer either leaves the system with probability @xmath332 or enters the @xmath333 node with probability @xmath334 .",
    "once the service at the @xmath335 node is completed , the customer may rejoin the @xmath336 node with probability @xmath337 .",
    "the service time processes of each node , @xmath338 are defined as @xmath339 where for all @xmath340 , @xmath341 are constants and @xmath342 are independent samples drawn from the uniform distribution on @xmath343 .",
    "the service time of each node depends on the @xmath344-dimensional tunable parameter vector @xmath345 , whose individual components lie in a certain interval @xmath346 $ ] , @xmath347 , @xmath340 .",
    "@xmath348 represents the @xmath349 update of the parameter vector at the @xmath299 node , and @xmath350 represents the target parameter vector corresponding to the @xmath299 node .",
    "the cost function is chosen to be the sum of the total waiting times of all the customers in the system .",
    "for the cost to be minimum , @xmath351 should be minimum , and hence , we should have @xmath352 , @xmath353 .",
    "let us denote @xmath354 and @xmath355 it is evident that @xmath356 , where @xmath357 . in order to compare the performance of the various algorithms , we consider the performance measure to be the euclidean distance between @xmath70 and @xmath358 , @xmath359^{1/2}.\\ ] ] the choice for such a performance measure is due to the fact that when the above distance is low , the queuing network provides globally optimal performance .",
    "hence , in the results presented below , a low value of the distance ( performance measure ) implies that the algorithm converges to a closer proximity of the global minimum .      for the simulations ,",
    "we first consider a two queue network with the arrival rates at the nodes being @xmath360 and @xmath361 respectively .",
    "we consider that all customers leaving node-1 enter node-2 , _ i.e. _ , @xmath362 , while customers serviced at node-2 may leave the system with probability @xmath363 .",
    "we also fix the constants in the service times at @xmath364 and @xmath365 , respectively , for the two nodes .",
    "the service time parameters for either node are two - dimensional vectors , @xmath366 , with components lying in the interval @xmath346 = [ 0.1,0.6]$ ] for @xmath367 , @xmath368 .",
    "thus the constrained space @xmath10 is given by @xmath369 ^ 4\\subset\\mathbb{r}^4 $ ] .",
    "we fix the target parameter at @xmath370 .",
    "the simulations were performed on an intel core @xmath371 machine with @xmath372 memory space and linux operating system .",
    "we run the algorithms by varying the values of @xmath0 and @xmath44 , while all the other parameters are held fixed at @xmath373 and @xmath374 .",
    "for all the cases , the initial parameter is assumed to be @xmath375 .",
    "for each set of parameters , 20 independent runs were performed with each run of @xmath376 iterations taking about 0.5 seconds .",
    "we compare the performance of the proposed algorithms with gradient based sf algorithms proposed in  @xcite , which use gaussian smoothing .",
    "box - mller method has been used to sample standard gaussian vectors , while samples from multivariate @xmath0-gaussians are drawn using the method discussed in appendix .",
    "this method uses generation of a @xmath377-distributed random variable , which is implemented using standard methods described in  ( * ? ? ?",
    "* algorithms  4.33 and  4.37 ) .",
    "figure  [ plot_4dim ] shows the convergence behavior of the gaussian and proposed @xmath0-gaussian based algorithms with @xmath378 , where the smoothness parameter is @xmath379 .",
    ".performance of all algorithm for different values of @xmath0 with @xmath379 , and step - sizes @xmath380 and @xmath381 . [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     we perform similar experiments in a higher dimensional case . for this",
    ", we consider a four node network with @xmath382 for all @xmath383 .",
    "the probability of leaving the system after service at each node is @xmath384 for all nodes .",
    "the service process of each node is controlled by a @xmath157-dimensional parameter vector , and a constant set at @xmath385 .",
    "thus , we have a @xmath386-dimensional constrained optimization problem , where each component can vary over the interval @xmath387 $ ] and the target is @xmath388 .",
    "the parameters of the algorithms are held fixed at @xmath373 , @xmath374 and @xmath389 .",
    "each component in the initial parameter vector is assumed to be @xmath390 for all @xmath391 .",
    "the step - sizes were taken to be @xmath392 and @xmath393 .",
    "for each @xmath394 tuple , 20 independent runs were performed .",
    "each run of @xmath376 iterations took about 1 second on an average .",
    "figure  [ plot_50dim ] shows the convergence behavior of the algorithms in this case .",
    "the performance of one - simulation algorithm is relatively poor .",
    "table  [ gq2_20 ] shows the performance of g@xmath0-sf2 algorithms for different @xmath394-pairs .",
    "we observe that the trend is similar in this case showing that the algorithms scale in a similar manner .",
    "the @xmath0-gaussian distribution is an important power - law distribution that has connections with generalized information measures .",
    "the power - law behavior of @xmath0-gaussians provide a better control over smoothing of functions as compared to the gaussian distribution .",
    "we have extended the gaussian smoothed functional approach for gradient estimation to the @xmath0-gaussian case by showing that the @xmath0-gaussian distribution satisfies the rubinstein conditions  @xcite .",
    "further , we developed optimization algorithms that incorporate @xmath0-gaussian smoothing .",
    "this extension turns out to be more significant when we note that the @xmath0-gaussians encompass all the existing smoothing kernels - gaussian ( @xmath80 ) , cauchy ( @xmath395 ) and uniform ( @xmath96 ) .",
    "we proposed two @xmath0-gaussian sf algorithms for simulation optimization .",
    "we use a queuing network example to show that for certain values of @xmath0 , the results provided by the proposed algorithms are significantly better than gaussian sf algorithms .",
    "our simulation results even indicate that for some @xmath0-values , the performance is better as compared to all the above mentioned special cases .",
    "we also presented proof of convergence of the proposed algorithms to a local minimum of the objective function .",
    "it would be interesting to develop hessian estimators incorporating the @xmath0-gaussian smoothed functionals , and developing newton based algorithms along these lines .",
    "the algorithms discussed in the paper require generation of a multivariate @xmath0-gaussian distributed random vector , whose individual components are uncorrelated and identically distributed .",
    "this implies that the random variables are @xmath0-independent  @xcite . for the limiting case of @xmath80",
    ", @xmath0-independence is equivalent to independence of the random variables .",
    "hence , we can use standard algorithms to generate i.i.d .",
    "this is typically not possible for @xmath0-gaussians with @xmath7 .",
    "@xcite proposed an algorithm for generating one - dimensional @xmath0-gaussian distributed random variables using generalized box - mller transformation .",
    "but , there exists no standard algorithm for generating @xmath66-variate @xmath0-gaussian random vectors .",
    "a method can be obtained by making use of the one - to - one correspondence between @xmath0-gaussian and students-@xmath94 distributions for @xmath97 .",
    "further , a duality property of @xmath0-gaussians can be used to relate the distributions for @xmath396 and @xmath231 .",
    "this observation , first made by  @xcite , is shown below .",
    "we denote the @xmath0-gaussian distribution with @xmath0-mean @xmath88 and @xmath0-covariance @xmath89 as @xmath397 . based on this",
    ", we formally present an algorithm , which can be used to generate multivariate @xmath0-gaussian distribution .",
    "theoretical justification behind the algorithm is provided in the following results  @xcite .",
    "a.  rnyi . on measures of entropy and information . in _",
    "fourth berkeley symposium on mathematical statistics and probability , 1960 _ , volume  1 , pages 547561 , berkeley , california , 1961 .",
    "university of california press .",
    "a.  perez .",
    "risk estimates in terms of generalized @xmath411-entropies . in _ proceedings of the colloquium on information theory , debrecen 1967 _ , pages 299315 , budapest , 1968",
    ". journal bolyai mathematical society .",
    "s.  bhatnagar and v.  s. borkar .",
    "two timescale stochastic approximation scheme for simulation - based parametric optimization .",
    "_ probability in the engineering and informational sciences _ , 12:0 519531 , 1998 .",
    "s.  bhatnagar , m.  c. fu , s.  i. marcus , and i.  j. wang .",
    "two - timescale simultaneous perturbation stochastic approximation using deterministic perturbation sequences .",
    "_ acm transactions on modeling and computer simulation _ , 130 (",
    "2):0 180209 , 2003 .",
    "a.  dukkipati , s.  bhatnagar , and m.  n. murty . on measure - theoretic aspects of nonextensive entropy functionals and corresponding maximum entropy prescriptions .",
    "_ physica a : statistical mechanics and its applications _ , 3840 ( 2):0 758774 , 2007 .",
    "j.  costa , a.  hero , and c.  vignat . on solutions to multivariate maximum @xmath1-entropy problems .",
    "_ energy minimization methods in computer vision and pattern recognition , lecture notes in computer science _ ,",
    "2683:0 211226 , 2003 .",
    "w.  j. thistleton , j.  a. marsh , k.  nelson , and c.  tsallis .",
    "generalized box - muller method for generating @xmath0-gaussian random deviates .",
    "_ ieee transactions on information theory _ , 530 (",
    "12):0 48054810 , 2007 ."
  ],
  "abstract_text": [
    "<S> the importance of the @xmath0-gaussian family of distributions lies in its power - law nature , and its close association with gaussian , cauchy and uniform distributions . </S>",
    "<S> this class of distributions arises from maximization of a generalized information measure . </S>",
    "<S> we use the power - law nature of the @xmath0-gaussian distribution to improve upon the smoothing properties of gaussian and cauchy distributions . based on this , </S>",
    "<S> we propose a smoothed functional ( sf ) scheme for gradient estimation using @xmath0-gaussian distribution . </S>",
    "<S> our work extends the class of distributions that can be used in sf algorithms by including the @xmath0-gaussian distributions , which encompass the above three distributions as special cases . using the derived gradient estimates , we propose two - timescale algorithms for optimization of a stochastic objective function with gradient descent method . </S>",
    "<S> we prove that the proposed algorithms converge to a local optimum . </S>",
    "<S> performance of the algorithms is shown by simulation results on a queuing model . </S>"
  ]
}