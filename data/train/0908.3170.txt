{
  "article_text": [
    "the first question that arises is how do temporal entropies relate to the traditional reading of the rts being short or long . for individual words",
    ", i investigated the relationship between both measures .",
    "for this , i used mem regressions both on the plain ( and logged ) rts , and on the rt self - informations .",
    "the relation between the random effect adjustments for individual words between models ( corrected by the corresponding general intercept ) summarized the relationship between the temporal entropy and rt .",
    "[ fig : rt.selfinf ] plots the relation between the individual response self - informations and the corresponding rts .",
    "note that this relation is non - trivial ; for the bulk of the responses , it follows a non - linear u - shaped pattern .",
    "therefore one can not directly assume a simple relation between the results of both measures .",
    "in contrast , fig .  [",
    "fig : nonlinear ] compares the estimated mean rt with the temporal entropies for each word in each of the tasks .",
    "the dashed lines plot the best fitting linear regression between both measures .",
    "the figure suggests that the temporal entropies are linearly related to the average rts .",
    "this ( apparently ) linear relation is very strong , with explained variance values of 64% in wn and up to 81% in vld . at first sight",
    ", this finding could be understood as a stronger restatement of hick s law @xcite : these studies provided evidence that the average rt to a stimulus is directly proportional to the informational content of the stimulus .",
    "the current results seem to suggest that the amount of information processing caused by the stimuli is also directly proportional to the average rt .",
    "this would indicate that there is a constant  possibly task - dependent  information processing speed of the cognitive system .",
    "further support for this interpretation could come from the very similar slopes of the linear regression lines in the figure across visual lexical decision ( @xmath15 bits / s ) and word naming ( @xmath16 bits / s ) indicating that this processing speed might be constant even across tasks .",
    "this would go one step further than hick s law by saying that the average rt would be directly proportional to the amount of information that has been _ processed _ ( rather than just contained by the stimulus ) .",
    "although this would be a suggestive interpretation , some caution needs to be taken before accepting this conclussion .",
    "analysis  iv will provide a more in - depth study of the issue of information processing speed and its implications for rt distributions .",
    "i now turn to an investigation of the effect of informational contents of stimulus and task complexity on the amount of information processing .      for this analysis",
    ", i selected subsets of the elp database that contained responses by multiple subjects to the same word stimuli across two tasks .",
    "this enabled direct pairwise comparison between rts across the tasks .",
    "responses in vld have generally slightly longer latencies than in wn .",
    "this is corroborated in this dataset . comparing the estimated average rts to individual words in vld and wn ( as predicted by the mem models above ) , revealed that vld rts are longer than in wn ( average difference @xmath17 ms . ;",
    "95% ci @xmath18 $ ] ms . ; paired @xmath19 = 15.09 $ ] , @xmath20 two - tailed ) and so are the corresponding log rts ( average difference @xmath21 ; 95% ci @xmath22 $ ] ; paired @xmath19 = 6.46 $ ] , @xmath20 two - tailed ) .",
    "this is also the case for the temporal entropies , that is , the temporal entropy of a word in vld is on average larger than the temporal entropy of the same word in wn ( average difference @xmath23 bits . ;",
    "95% ci @xmath24 $ ] bits ; paired @xmath19 = 113.42 $ ] , @xmath20 two - tailed ) .",
    "the magnitude of the @xmath25 statistic was much larger for the difference in temporal entropies than for the rts , whether logged or untransformed .",
    "furthermore , the average difference between conditions was also much larger for the temporal entropies than for the rt measures : whereas the rts to words in vld were 3.1% longer with respect to their wn latencies ( and down to .2% in logarithmic scale ) , they were 8.5% more complex .",
    "this suggested that temporal entropy provides a clearer differentiation between the tasks than did the magnitude of the rts .",
    "this is depicted in fig .",
    "[ fig : cross - task ] .",
    "the panels compare the estimated mean rts ( left panel ) , mean log rts ( middle panel ) , and temporal entropies ( right panel ) for wn ( horizontal axes ) and vld ( vertical axes ) .",
    "the grey dots correspond to individual words , and the dashed line is the identity condition .",
    "whereas in the rt measures the difference between tasks ( i.e. , asymmetry with respect to the identity line ) is barely noticeable , the temporal entropy measure sharply separates the two tasks . only in 18 out of the 1,986 ( less than 1% ) studied words",
    "did the temporal entropy have a higher value in wn than it did in vld .",
    "table  [ tab : effects ] shows that the mem models on the temporal self - informations , in both datasets , revealed main effects of the lexical and letter information variables on the temporal entropies that were fully consistent with the theoretical predictions .",
    "as predicted above , the fixed - effect regression coefficients ( @xmath26 ) were in all cases positive and smaller than one .",
    "also , as indicated by the @xmath27 log - likelihood tests , the suggested random slopes ( see materials and methods ) constituted a significant improvement on the basic models .",
    "for comparison , the table also includes the results of running mems on the log rts using the same predictors that were used with the temporal self - informations , revealing a nearly identical pattern .",
    "the values of the main effect coefficients provided a lower - bound estimate for the value of the dimensionless coefficients @xmath8 ; recall that this task - specific constant measures the scaling between the entropy of the rts and the entropy of the system . in the case of vld ,",
    "this lower - bound was @xmath28 , and for wn it was @xmath29 .",
    "notice that the @xmath30 estimates for both tasks were rather similar ; in fact , if the standard errors of the estimates were also taken into account , there was no reason to believe that the estimates were at all different .",
    "as discussed above , the @xmath30 lower - bounds , combined with the fixed effect estimates of the regression ( @xmath31 ) can be used to estimate the upper - bound for the possible contribution of one bit of information contained by the properties of the stimulus , into the amount of information about it that is actually _ processed _ by the system ( @xmath7 ) . in vld , one bit of lexical information corresponded to a mean of at most one bit of cognitive processing , while one bit of letter information resulted in a maximum of .18 bits of cognitive processing . similarly , in word naming , one bit of lexical information elicited at most .32 bits of cognitive processing , while each bit of letter information could maximally correspond to a full bit of processing .",
    "these estimations put numbers into the intuition that lexical information is more relevant in vld than it is in wn , and that letter identity information is more important for wn than it is for vld . in both cases ,",
    "much of the information that the stimulus contains is not at all processed , presumably because it is not useful for the task at hand .",
    "i now return to the linear relationship between mean rts and temporal entropy that was suggested to be taken with caution in analysis  i. the question arises as to the implications that the arguably linear relationship in fig .  [ fig : nonlinear ] would have for the shape rt distribution .",
    "this question was addressed using the principle of maximum entropy @xcite .",
    "a maximum entropy analysis revealed that , under the assumption of an existing mean @xmath32 that is linearly related to the entropy , the most likely relationship between the entropy and the mean is described by : @xmath33 -\\log \\kappa_1 = a + b\\mu , \\label{eq : impossible}\\ ] ] where @xmath34 , @xmath35 are the intercept and slope of the assumed linear relation , and @xmath36 , @xmath37 are constants for a given dataset . notice that a new term @xmath38 $ ] corresponding to the mean of the log rts appeared in the relationship , without it having been assumed _ a priori_. this indicates that , if one assumed that there is a linear relationship between the mean rt and the temporal entropy , one should also assume that there is a linear relationship between the mean log rt and the temporal entropy .",
    "this entails a sort of probabilistic _ reductio ad absurdum _ : it says that the temporal entropy is linearly related to _ both _ the mean rt and the mean log rts .",
    "that is , unless the mean rt and the mean log rt were independent of each other  and they were not ",
    "the relationship between mean rt and temporal entropy was most likely to be nonlinear after all , despite the seemingly linear appearance of the plots of fig .",
    "[ fig : nonlinear ] .    to investigate this prediction ,",
    "i performed a linear regression on the temporal entropies of individual words in each task , with both mean rt and mean log rt as co - variates ( the effect of mean log rt was considered only after partialling out the effect of mean rt ) . as was predicted by the maximum entropy analysis , both the visual lexical decision and",
    "the word naming datasets revealed significant linear contributions of mean rt ( vld : @xmath39=11,444.43 ; p<.0001 $ ] ; wn : @xmath40=4,221.6 ; p<.0001 $ ] ) together with a significant correction introduced by the mean log rt ( vld : @xmath41=658.61 ; p<.0001 $ ] ; wn : @xmath42=379.7 ; p<.0001 $ ] ) . in both cases ,",
    "the corrected estimate of explained variance increased by about 5% when taking the mean log rts also into account ( from 81% to 86% in vld , and from 64% to 70% in wn ) .. additional regressions also confirmed this relation with the second moment , which added approximately a further 5% explained variance to each dataset .",
    "i focus the discussion only on the additional contribution of the mean log rts , as this is sufficient to introduce the necessary non - linearity while being considerably simpler ( the estimation of the second moment required additional methods not discussed here ) . in any case the consideration of the second moments did not produce any significant change on the results to follow . ]",
    "the non - linearity is summarized by the additional solid lines in fig .",
    "[ fig : nonlinear ]",
    ". these estimate how the correction introduced by considering the mean log rt looked like .",
    "both datasets showed a pattern reminiscent of a hockey stick , in which there were small bendings at the bottom of the ranges that became increasingly straight with increasing mean time .",
    "this was due to the decreasing importance of the log rt relative to the untransformed rt that grows much faster .    the adaptive average processing speed is further illustrated in fig .",
    "[ fig : gradients ] . the figure plots , for each of the tasks , the evolution of the average instantaneous information processing rate , as a function of the total amount of processing .",
    "the average instantaneous rates were obtained by taking the numerically estimated gradients of the solid lines in fig .",
    "[ fig : nonlinear ] : @xmath43}.\\ ] ] the figure shows that , in both tasks , the information processing rates increased monotonically with amount of processing , this is , the more the amount of processing that was needed , the faster it happened .",
    "in addition , there seemed to be three linear regimes for this increase that were rather similar across tasks .",
    "it is possible that the third regime , corresponding to the slower slope lines in the high values of temporal entropy were at least partly a consequence of the truncation point at 4,000  ms .",
    "slow words will be more likely to be affected by this truncation , as a proportionally larger part of their density mass was chopped off , leading to possible underestimations of both the mean rt and the temporal entropy .",
    "finally , the lower - bound estimates @xmath30 in each task obtained in analysis  iii , open the possibility of guessing the upper - bounds for the range of variation of the overall cognitive information processing speed in each of the tasks , as a function of the rate in terms of temporal entropy : @xmath44 } = \\bar{r}^\\star.\\ ] ] in vld this gave a range of upper - bound values ( @xmath45 ) going from around 33 bits / s to about 61 bits / s , and in wn the range went between 22 bits / s and 60 bits / s .",
    "this study has introduced a novel interpretation for rt experiments .",
    "the conventional approach is to consider how long responses take to occur .",
    "instead , i proposed to investigate whether the temporal distribution of responses is more or less _",
    "complex_. i argued that this complexity of the reaction time distribution reflects the underlying state of complexity in the cognitive system , and the empirical evidence has supported this view .",
    "this enables a shift from studying how much information is contained in stimuli or tasks , to directly investigating the amount of this information that is actually processed .",
    "as evidenced by the comparison between vld and wn , the temporal entropy measure is remarkably more sensitive than the traditional rt magnitudes  whether in untransformed or logarithmic scale  in distinguishing tasks with different properties .",
    "there has been a growing interest in techniques that enable going beyond the mean in the description of rt data @xcite .",
    "these proposals consist in studying either the quantiles or the higher moments of the distribution , or the parameters of some distribution family that is assumed _ a priori _ , which are estimated separately for different participants and/or experimental conditions .",
    "i have proposed a considerably more simple , and model - free , measure : the entropy of the rt distribution summarizes the cognitively relevant aspects of its shape .",
    "the working assumption , from an information processing perspective , is that any variation in the amount of processing must be reflected in the entropy of the distribution . by implication",
    ", the temporal entropy should be a sufficient statistic to reflect the effects of different cognitive manipulations .",
    "furthermore , the new measure uses the random effect structure of the experiment , and is thus less sensitive to the sometimes very reduced number of points of each individual condition ( see , e.g , @xcite ) .",
    "entropy is , by definition , an additive measure .",
    "different contributions of independent factors can then be considered in an plainly additive manner .",
    "current practice in the analysis of rt data recommends transforming the rts prior to statistical analyses , either using a logarithmic or a reciprocal transform .",
    "this has the undesirable effect of breaking the additive interpretability of effects , forcing researchers to delve into complex multiplicative processes @xcite .",
    "in contrast , the approach proposed here remains in the additive domain , while at the same time being able to capture complex aspects of the distributional shape .",
    "this is achieved while keeping with a model - free approach , that is , no particular distributional shape needs to be assumed .",
    "an important consequence of this analysis is the conclusion that hick s law needs to be extended and reformulated .",
    "strictly speaking , the law proposed by hick , hyman , and others @xcite concerns only the relation between information contained in the stimuli and mean rt . crucially , in this study , i extend this argument to the information about the stimuli that is actually processed , this is to say , the relevant information . in this case , the relationship between processed information and mean rt is not linear , even though it might seem linear to the naked eye .",
    "the non - linearity was suggested by maximum entropy theoretical analysis and confirmed by the empirical data .",
    "note that collectively these results strengthen the argument considerably : the non - linearity is not just obtained from a fit to the data , but was predicted _ a priori _ in detail . although the adjustment might seem small on the average entropies , inspection of fig .",
    "[ fig : gradients ] makes it clear that that the non - linear relation allows the information processing rate to double in some contexts .",
    "this finding suggests an adaptive system where the processing load is dynamically adjusted to the task demands . in a way , it is a ` lazy ' system in the sense of zipf @xcite .    not surprisingly , the increase of information processing rate with stimulus complexity is consistent with the findings of kostic @xcite for vld , even in the estimates of the information processing rate . in a vld task using serbian words ,",
    "kostic estimated that , depending on the average informational content of the particular stimuli in an experiment , the information processing rate ranged from about 30 to about 100 bits / s .",
    "this is consistent with the maximum rate ( i.e. ,  @xmath45 ) ranging from 30 to 60 bits / s that i have derived .",
    "the difference between both estimates are possibly due to the difference between information contained in the stimuli that kostic studied , and information that is processed about them that has been studied here .",
    "notice also that kostic s estimates refer to the aggregated averages over full experiments .",
    "in contrast , the technique proposed here enables obtaining estimates for individual stimuli .",
    "the principal contribution of the present study is that the entropy of an rt distribution provides an index of the amount of information processing that has taken place .",
    "stated alternatively , changes in the entropy of rt distributions reflect changes in the underlying state of the cognitive system .",
    "i have estimated that , in the tasks under study , a minimum of around 10% of the increase in the entropy of the cognitive system is reflected in an increase of temporal entropy ( i.e. , the @xmath30 estimates ) .",
    "this finding provides a handle by which rt data can be used to establish the link between higher cognitive function and its metabolic counterparts , that was proposed in the seminal study of kirkaldy @xcite .",
    "furthermore , the characterization of behavioral responses in terms of entropy enables the consistent treatment of behavioral and neurophysiological signals using the same theoretical tools ( see @xcite for a review on information theory in the study of neurophysiological signals ) .",
    "i retrieved from the elp database @xcite the individual lexical decision rts and word naming latencies to 1,986 nouns and verbs .",
    "the set of selected words corresponds to those words used in a previous study that compared the lexical decision and naming tasks @xcite .",
    "the selection of only a subset of the words enables us to keep the models below tractable .",
    "all responses that were not marked as correct in the database were excluded from further analysis .",
    "in addition , i also excluded all responses equal or longer than 4,000 ms .",
    ", as beyond this limit responses appear truncated in the elp database .",
    "this left a total of 64,087 responses ( from 816 different participants ) in the lexical decision dataset and 53,403 ( from 445 participants ) in the naming one .",
    "along with the individual rts , the surface frequency of the words ( extracted from the celex database @xcite ) and their length in characters was also recorded .",
    "the word frequency and word length measures were transformed into information theoretical measures , the self - information of the word , and its average informational content due to its letters .",
    "the individual self - information values for each rt in the visual lexical decision and naming datasets were estimated by kernel density estimation ( kde ; @xcite ) with gaussian kernels , and imposing bounds on the distribution at the truncation points of 0  ms .",
    "and 4,000  ms . ,",
    "beyond which the density was estimated to have a value of zero ( with the adequate normalization to integrate to one in that interval ) .",
    "the estimates combine a direct kde for the left tail of the distribution , and a retransformed estimate on logarithmic scale for the right tails .",
    "the reason for this dual estimate was to avoid the high - frequency noise that kde introduces on the right tails of heavy - tailed distributions . estimating the distribution in logarithmic scale greatly attenuates the noise on the right tail @xcite , but it introduces additional noise in the vicinity of zero , thus the dual estimation . to guarantee a certain smoothness in the transition between the untransformed and the logarithmic kde , in the area around the distributional mode i used a weighted average between both estimates . from the total support grid of 512 points where the densities were estimated , 40 points to the left of the mode received an weighting linearly decreasing in favor of the transformed estimate .",
    "this pattern was reversed in the 40 points to the right of the mode , where the weighting was linearly increased to favor the logarithmic estimates .",
    "the mode itself received a plain .5/.5 average between both estimates .",
    "the individual self - information values correspond to the the minus logarithm of the estimated density , using two as the base of the logarithm to obtain an estimate in bits .",
    "the individual values for each response were interpolated from the 512 point grid on which the densities were estimated .",
    "[ fig : rt.selfinf ] shows the result of this process for both datasets under study .      to investigate the relationship between temporal entropy and mean rts ( analyses  i , ii , & iv ) , i fitted two mem models to the data . in both cases ,",
    "the models included an intercept , and two random effects , one of word and one of subject , as predictors .",
    "the first model had the rt as dependent variable ( so that the average predictions of the model correspond to the mean rts ) , and the second model had the self - information of the rts as independent variables ( thus as argued above , the average predictions of the model correspond to temporal entropies ) .",
    "the individual estimates of either mean rt or temporal entropy for each word were computed as the sum of the corresponding model s intercept with the particular random effect adjustment for that word .    to investigate the predictions on the informational content of stimuli ( analysis  iii )",
    ", i performed mem regressions to the temporal self - information and to the log rts in each task .",
    "as before , these regressions included random effects of both subject and word .",
    "in addition , i also included fixed effect covariates measuring the lexical self - information and the average information content of its letters .",
    "an additional factor needed to be considered in these mems . the measure of",
    "the informational content of the letters in a word refers to the average case .",
    "however , different words will contain combinations of letters that contain more or less information than the average ( i.e. , they are either more frequent than usually or rarer than usual ) .",
    "therefore , the effect of the information of the letters can vary with respect to word identity . to address this problem ,",
    "the mem models included the possibility of a random slope making the effect of letter - based information variable for different words .",
    "similarly , the experience that different people have had with different words varies in both quantitative and qualitative terms .",
    "this was accounted for by introducing a random slope that enables the variation of the lexical self - information effect across individual participants .",
    "all mem regressions were fitted using a restricted maximum likelihood algorithm , as implemented in the _ r _ package `` lme4 '' @xcite .",
    "the author is indebted to l.  b. feldman , d. filipovi - urevi , i.  j. grainger , and m. mondragn for helpful suggestions .",
    "39    donders , f.  c. ( 1869 ) on the speed of mental processes . , 412431 .",
    "brillouin , l. ( 1956 ) _ science and information theory_. ( academic press , new york ) .",
    "kirkaldy , j.  s. ( 1965 ) the thermodynamics of the human brain .",
    ", 981986 .",
    "shannon , c.  e. ( 1948 ) a mathematical theory of communication . , 379423 , 623656 .",
    "bates , d.  m. ( 2005 ) fitting linear mixed models in r. , 2730 .",
    "baayen , r.  h. ( 2007 ) _ analyzing linguistic data : a practical introduction to statistics using r_. ( cambridge university press , cambridge , uk ) .",
    "baayen , r.  h , davidson , d.  j , & bates , d.  m. ( 2008 ) mixed - effects modeling with crossed random effects for subjects and items .",
    ", 390412 .",
    "hick , w.  e. ( 1952 ) on the rate of gain of information .",
    ", 1126 .",
    "hyman , r. ( 1953 ) stimulus information as a determinant of reaction time .",
    ", 188196 .",
    "hellyer , s. ( 1963 ) stimulus - response coding and the amount of information as determinants of reaction time .",
    ", 521522 .",
    "longstreth , l.  e , el - zahhar , n , & alcorn , m.  b. ( 1985 ) exceptions to hick s law : explorations with a response duration measure .",
    ", 417434 .",
    "welford , a.  t. ( 1987 ) comment on `` exceptions to hick s law : explorations with a response duration measure '' ( longstreth , el - zahhar , & alcorn , 1985 ) . , 312314 .",
    "longstreth , l.  e & alcorn , m.  b. ( 1987 ) hick s law versus a power law : reply to welford .",
    ", 315316 .",
    "norwich , k.  e , seburn , c , & axelrad , e. ( 1989 ) an informational approach to reaction times .",
    ", 347358 .",
    "norwich , k.  e. ( 1993/2003 ) _ information , sensation , and perception_. ( academic press , san diego ) .",
    "online edition by e. barull , biopsychology.org .",
    "kosti , a. ( 2005 ) the effects of the amount of information on processing of inflected morphology , ( laboratory for experimental psychology , university of belgrade , belgrade , serbia ) , technical report .",
    "kosti , a. ( 1991 ) informational approach to processing inflected morphology : standard data reconsidered .",
    ", 6270 .",
    "kosti , a , markovi , t , & baucal , a. ( 2003 ) in _ morphological structure in language processing _ ,",
    "baayen , r.  h & schreuder , r. ( mouton de gruyter , berlin ) , pp .",
    "mcdonald , s.  a & shillcock , r.  c. ( 2001 ) rethinking the word frequency effect : the neglected role of distributional information in lexical processing . , 295322 .    ,",
    "f , kosti , a , & baayen , r.  h. ( 2004 ) putting the bits together : an information theoretical perspective on morphological processing . , 413421 .    ,",
    "f. ( 2007 ) co - occurrence and the effect of inflectional paradigms . , 247263 .",
    "filipovi - urevi , d. ( 2007 ) _ the polysemy effect in serbian language . _ in experimental psychology ( faculty of philosophy , university of belgrade , serbia ) .",
    "milin , p , filipovi - urevi , d , & moscoso  del  prado  martn , f. ( 2009 ) the simultaneous effects of inflectional paradigms and classes on lexical recognition : evidence from serbian . , 5064 .",
    "balota , d.  a , yap , m.  j , cortese , m.  j , hutchison , k.  a , kessler , b , loftis , b , neely , j.  h , nelson , d.  l , simpson , g.  b , & treiman , r. ( 2007 ) the english lexicon project . , 44559 .",
    "jaynes , e.  t. ( 1957 ) information theory and statistical mechanics .",
    ", 620630 .",
    "jaynes , e.  t. ( 1957 ) information theory and statistical mechanics ii . , 171190 .",
    "ratcliff , r. ( 1978 ) a theory of memory retrieval .",
    ", 59108 .",
    "luce , r.  d. ( 1986 ) _ response times : their role in inferring elementary mental organization_. ( oxford university press , new york ) .",
    "heathcote , a , popiel , s.  j , & mewhort , d. j.  k. ( 1991 ) analysis of response time distributions : an example using the stroop task .",
    ", 340347 .    , t. ( 2002 ) analysis of response time distributions . in _ stevens handbook of experimental psychology ( 3rd edition ) , volume iv : methodology in experimental psychology _ , eds .",
    "wixted , j.  t & pashler , h. ( wiley press , new york ) , pp .",
    "461516 .",
    "rouder , j.  n , lu , j , speckman , p , sun , d , & jiang , y. ( 2005 ) a hierarchical model for estimating response time distributions . ,",
    "195223 .",
    "balota , d.  a , yap , m.  j , cortese , m.  j , & watson , j.  m. ( 2008 ) beyond mean response latency : response time distributional analyses of semantic priming . , 495523 .",
    "holden , j.  g , van  orden , g.  c , & turvey , m.  t. ( 2009 ) dispersion of response times reveals cognitive dynamics .",
    ", 318342 .",
    "zipf , g. ( 1949 ) _ human behavior and the principle of least effort_. ( addison - wesley , reading , ma ) .",
    "borst , a & theunissen , f.  e. ( 1999 ) information theory and neural coding .",
    ", 947957 .",
    "baayen , r.  h , feldman , l.  b , & schreuder , r. ( 2006 ) morphological influences on the recognition of monosyllabic monomorphemic words .",
    ", 290313 .",
    "newman , m.  e.  j. ( 2005 ) power laws , pareto distributions and zipf s law .",
    ", 323351 .",
    "baayen , r.  h , piepenbrock , r , & gulikers , l. ( 1995 ) _ the celex lexical database ( cd - rom)_. ( linguistic data consortium , university of pennsylvania , philadelphia ) .",
    "parzen , e. ( 1962 ) on estimation of a probability density function and mode .",
    ", 10651076 .                ) in the vertical axis .",
    "the estimates were obtained as the numerical gradients of the solid black lines in fig .",
    "[ fig : nonlinear ] .",
    "the rugs at the bottom of the panels illustrate the approximate number of points on which each portion of the curve is estimated . ]",
    "[ tab : effects ]",
    "if @xmath25 are the times of responses elicited in an condition @xmath46 that requires an amount of information processing @xmath47 , it can be predicted that : @xmath48 where @xmath49 is the differential entropy [ 1 ] of the rt distribution in the resting state , @xmath50 $ ] is a constant indicating the proportional reflection of the increase of the system s entropy into the rt distribution , and @xmath51 is the differential entropy of the distribution of rts : @xmath52 the inequality in the right side of eqn .",
    "[ eq : prediction1 ] results from the fact that the in order to process the information , the system forcefully needs to have increased its entropy ( @xmath53 ) .    information processing is the result of an experimental situation . in a particular task , the amount of processing required will be different for different stimuli .",
    "therefore , one can also consider a task - specific informational content of the stimuli themselves , which is external to the cognitive system .",
    "the amount of information processing involved in processing a particular experimental condition is bound to be lower or equal to the total information content of the stimulus , this is to say , the amount of information that is available limits the amount of information that can be processed . denoting this external informational content of a particular stimulus in a given task by @xmath54",
    "we see that : @xmath55 furthermore , if the estimate of the stimulus information content is relatively accurate , the amount of information that is processed should be proportional to what is available : @xmath56 where @xmath57 $ ] represents the proportion of available information that is processed . combining eqns .",
    "[ eq : prediction1 ] , [ eq : entropy - integral ] , and [ eq : proportion ] one obtains : @xmath58    taking the @xmath8 proportion to be relatively constant across participants in a given experimental context , in principle , the relationship in eqn .",
    "[ eq : prediction2 ] could be tested experimentally , providing a direct measurement of the information processing involved in a task across conditions . however , direct application of these expressions to actual experimental data is problematic .",
    "the value of the @xmath3 term is unknown , and it does not seem easy to estimate it ( but see also [ 2 ] ) .",
    "in addition , the actual distribution of rts ( @xmath13 ) is itself unknown , only a particular sample of rts obtained in an experiment is available , and it is in most cases rather sparse .",
    "the first problem is circunvented by studying the _ relative _ increase in rt entropy elicited by several experimental conditions @xmath59 . between any two given conditions @xmath60 and @xmath61 . in this case , from eqn .",
    "[ eq : prediction2 ] one finds that : @xmath62 \\simeq \\frac{1}{k } \\left [ h\\left ( t_i\\right ) - h\\left ( t_j\\right ) \\right ] , \\label{eq : difference}\\ ] ] where @xmath63 and @xmath64 refer to the rts obtained in conditions @xmath60 and @xmath61 .",
    "this can be readily extended to a regression situation in which the informational content of the stimuli is varied continuously .",
    "in such a case , if @xmath65 represents the rts for a particular stimulus @xmath46 : @xmath66 note that the @xmath67 and @xmath68 coefficients above are themselves meaningful .",
    "on the one hand , the intercept coefficient @xmath67 reflects the baseline level of temporal entropy scaled up by the proportionality constant ( @xmath69).this implies that one can force @xmath70 . on the other hand , the slope coefficient @xmath68 corresponds to the increase in information processing per processed unit of information .",
    "this is so because the @xmath71 and @xmath8 coefficients already index what proportion of the stimulus information is processed , and how this processing relates to the rt distribution entropy . therefore , trivially , @xmath72 . including an explicit error term @xmath12 results in : @xmath73 with @xmath49 and",
    "@xmath74 $ ] .",
    "this provides a direct route to testing the relationship . a linear regression with the informational content of the stimuli as predictor and the entropy of the corresponding rt distribution as dependent variable provides a direct estimate of the parameters ( scaled by @xmath8 ) .",
    "it is worth noting here that eqn .",
    "[ eq : regression - final ] would also enable reasoning in the opposite direction .",
    "given an estimate of the cognitive cost of processing different stimuli , one could also obtain an estimate of their relevant informational load .",
    "the second problem concerns the estimation of the entropy of the rt distribution in a particular condition ( @xmath75 ) . in a typical repeated measures",
    "experimental design , several participants respond to to different stimuli .",
    "in these cases the entropy of the rt distribution is determined not only by the known informational content of the stimuli , but also by differences on the entropy of the rt distributions of particular participants , and by additional informative issues of the stimuli that are unknown to the experimenter or difficult to control for .",
    "thus the entropy on the overall rt distribution is the sum of multiple sources of uncertainty : @xmath76 where @xmath77 is the rt entropy that is intrisical to the particular stimulus @xmath10 , and @xmath78 is the rt entropy of a particular participant @xmath11 . taking this into account , for a response of an individual participant @xmath11 to a stimulus @xmath10 , we need to extend ( [ eq : regression - final ] ) to : @xmath79 by definition , the entropies can be reformulated in as the expected values of the negated log - probabilities ( i.e. , the self - informations ; [ 1 ] ) of the rts .",
    "considering simultaneously the effects of @xmath80 independent known sources of information : @xmath81 +   \\mathrm{e}_s\\left[-\\log p\\left(t\\right)\\right ] + \\mathrm{e}_p\\left[-\\log p\\left(t\\right)\\right ] + \\varepsilon =   \\mathrm{e}\\left[-\\log p\\left(t\\right)\\right ] .",
    "\\label{eq : regression - mixed2}\\ ] ] this corresponds to the expression of a regression model with an intercept , a covariate @xmath54 , and two random effects @xmath10 and @xmath11 , with the self - information of the individual rts as dependent variable . the parameters of the regression have a direct interpretation .",
    "the intercept corresponds to the baseline entropy of the system for the task at hand , it is thus a measure of overall task complexity .",
    "the fixed effect coefficients correspond to the @xmath4 products , that is , the influence of the known informational content of the stimulus on the amount of processing , weighted by the proportion of processing that is reflected in the increase in rt complexity . in general , to ensure that @xmath82 $ ] it must hold that @xmath83 , where @xmath84 are the estimated fixed effect coefficients of the regression .",
    "this provides a useful lower - bound for this parameter .",
    "furthermore , the limitations above also force that all fixed effect coefficients must fall in the range @xmath9 $ ] .",
    "the word length counts were transformed into an informational measure using a corpus based estimate of the average entropy rate of english of 1.23 bits per letter [ 3 ] discounting an estimate of .04 bits per characters estimated to reflect the information carried by spaces or case information [ 4 ] . actually estimated .06 bits per character for spaces and case , but this was scaled down to account for the difference in the overall estimate with the [ 3 ] estimates , which are considered the best available approximations . ] thus , the information content of a word due to its letters was estimated as .",
    "@xmath85 where @xmath86 is the word length in letters of the word @xmath87 .",
    "different words in a language vary with respect to the amount of information that they convey . generally speaking , to quantify the precise amount of information that is conveyed by a word seems at best very difficult .",
    "however , coming up with a theoretical , _ a - priori _ estimate of the information contained by a word would require the joint consideration of multiple linguistic and contextual factors , many of which are yet poorly understood .",
    "here , i only consider one simple measure of a word s informativity , its self - information derived from its frequency : @xmath88 where @xmath89 is the relative frequency of occurrence of the word @xmath87 in the celex database [ 5 ] .",
    "what does the knowledge that there is a linear relationship between the mean rt and the temporal entropy tell us about the distributional shape ?",
    "the less biased or more reasonable distributional shape to believe in is the one that satisfies the constraint while introducing as little additional knowledge as possible [ 6,7 ] .",
    "if @xmath90 is the distribution that reflects our full ignorance about the possible values of the rt , one should choose a new distribution @xmath13 that satisfies the constraints while being as similar as possible to the ` know - nothing ' distribution @xmath90 .",
    "mathematically , this is given by the shannon - jaynes entropy , that is , the kullback - leibler divergence [ 8 ] between @xmath13 and @xmath90 : @xmath91    in his introduction of the transformation groups argument , jaynes derived the shape of the full ignorance prior for the rate parameter @xmath92 of a poisson distribution : @xmath93 the justification for the necessity of this choice comes from a general consistency argument .",
    "consider two separate observers who were to assign probabilities to the rate of occurrence of an event .",
    "the two observers use different mechanisms to measure time , using perhaps different units ( e.g. , one uses milliseconds and the other uses minutes ) .",
    "if both observers are fully ignorant of the nature of the process , the most reasonable thing would be that they would asign an _ a priori _ probability distribution that reflects their complete ignorance . by ignorance",
    "it is meant that the observers know strictly _ nothing _ about the process that generates these events further than that they might happen with a rate of occurrence equal or greater than zero .",
    "obviously , as the level of ignorance of the observers is equivalent , any consistent prior distribution would be one by which both observers assign exactly the same probability distribution to the rate , irrespective of the measuring units they each use .",
    "this requires a prior probability that is in accord with eqn .",
    "[ eq : jaynes - poisson ] .",
    "note that the prior distribution in eqn .",
    "[ eq : jaynes - poisson ] is an improper one : it can not integrate to one in the domain @xmath94 .",
    "however , in bayesian and maximum - entropy analyses this does not constitute a problem , as only the posterior needs to be normalized ( cf .",
    ", [ 9 ] ) .",
    "furthermore , the recorded rts in any experiment have a practical upper - bound at some time @xmath95 , and in such cases the proposed prior is proper .",
    "the argument of [ 10 ] can be readily extended to obtain a full ignorance prior for the times at which events occur ; one that can then be used for the analysis of rt distributions .",
    "the rate at which events occur is the reciprocal of the times at which they happen ( @xmath96 ) . therefore , knowing the prior distribution for the rate , one can directly infer the prior for the times themselves , such that both priors are consistent with each other ( e.g. , in the example above , a third ignorant observer might have decided to infer the rates from the times , and his state of ignorance must be equivalent to that of the other two observers ) : @xmath97 where @xmath98 is the constant part of the prior distribution of the rates . in sum",
    ", the ignorance prior for the times must be the same as the ignorance prior for the rates .",
    "the problem is then to maximize eqn .",
    "[ eq : shannon - jaynes ] subject to the constraints : @xmath99   \\int_{0}^{t_{\\max } } p(t)\\ , t \\,\\mathrm{d}t & =   \\mu \\label{eq : constc1}\\\\[6pt ] -\\int_{0}^{t_{\\max } } p(t)\\log p(t ) \\,\\mathrm{d}t & =   a + b\\mu \\label{eq : constc2}\\end{aligned}\\ ] ] constraint  [ eq : constc0 ] is the usual normalization requirement for proper distributions , [ eq : constc1 ] represents the assumption of an existing finite mean rt @xmath32 , and [ eq : constc2 ] expresses the proposed linear relation between the mean rt and the temporal entropy .",
    "this is an optimization problem that can be solved using the method of lagrange multipliers from variational calculus .",
    "this results in a distribution of the form : @xmath100 which is a power law ( with exponent -1 ) with an exponential cutoff . plugging eqn .",
    "[ eq : maxentd1 ] into the linear relation constraint of eqn .",
    "[ eq : constc2 ] , and simplifying using eqn .",
    "[ eq : constc0 ] and eqn .",
    "[ eq : constc1 ] , one finds that : @xmath33 -\\log \\kappa_1 = a + b\\mu , \\label{eq : impossible}\\ ] ]    it is important to notice here that the distribution in eqn .",
    "[ eq : maxentd1 ] is in fact a rather implausible one for rt distributions ; it is monotonically decreasing .",
    "the argument is not that this is the best , or even a good , distribution to account for rts , but rather that it is the most reasonable one to believe in if one assumed _ only _ the information that was given .",
    "including further knowledge about the distribution in the form of additional constraints will produce a distribution that is more and more similar to the actual rt one .",
    "as an example , consider that one also assumed that the distribution has a known variance ( which would also be safe assumption provided the rts are truncated at some @xmath95 ) .",
    "including also information on the second moment of the distribution amounts to adding one further constraint to eqns .",
    "[ eq : constc0 ] , [ eq : constc1 ] , and [ eq : constc2 ] : @xmath101 where @xmath102 is the finite value of the second moment . in this case",
    ", the resulting distribution would be : @xmath103 and the relation between mean and entropy would now be : @xmath104 -\\log \\kappa_1 = a + b\\mu .",
    "\\label{eq : impossible2}\\ ] ] notice that the mean log rt term is still present .",
    "the origin of this term lies in the ignorance prior itself .",
    "threfore , even if one included many additional constraints , such as futher higher moments , quantile values , or actual observed values , the term will remain there .      as the vld dataset contained more responses per word that the wn one , a possibility is that the increase in entropy was due to a bias in the entropy estimates introduced by sample size .",
    "this possibility was discarded by a re - sampling analysis : random downsampling of the vld dataset to the same size as the wn one did not affect the results above in any significant way .",
    "another plausible confound is that the larger number of participants in the vld dataset ( 816 _ vs. _ 445 ) might have led to higher heterogeneity and thus larger estimates of the temporal entropy , despite the participants having been controlled for by treating them as an explicit random effect . to investigate this possibility",
    ", i reduced the vld dataset to include only the 445 participants that provided the highest number of responses .",
    "this resulted in a drastic reduction in the size of the vld dataset , which went down to 38,639 responses from the original 64,087 ( 60.3% ) , and thus became much smaller than the wn dataset ( 53,403 responses ) .",
    "still , the results remained virtually unchanged , save for the difference in log rts that failed to reach significance in the downsampled analysis ( @xmath105 ) .",
    "the corresponding plots were plainly indistinguishable from those of fig .  3 from the main text , as were the estimates of the difference between tasks .",
    "the individual word entropy estimates from the reduced dataset accounted for 76% of the variance of the entropy estimates from the full dataset .",
    "this is specially remarkable considering that , for some of these words , the reduced dataset was left with as few as a single response .      1",
    ".   shannon , c.  e. ( 1948 ) a mathematical theory of communication . _",
    "bell syst tech j _ * 27 * , 379423 , 623656 .",
    "moscoso del prado martn , f. ( 2009 ) _ the baseline for response latency distributions_. ( submitted manuscript ) .",
    "available from nature precedings http://hdl.handle.net/10101/npre.2009.3622.1 3 .",
    "rosenfeld , r. ( 1996 ) a maximum entropy approach to adaptive statistical language modelling .",
    "_ comp speech & lang _ * 10 * , 187228 .",
    "4 .   brown , p.  f , della  pietra , v.  j , mercer , r.  l , della  pietra , s.  a , & lai , j.  c. ( 1992 ) an estimate of an upper bound for the entropy of english .",
    "_ comp ling _ * 18 * , 3140 . 5 .",
    "baayen , r.  h , piepenbrock , r , & gulikers , l. ( 1995 ) _ the celex lexical database ( cd - rom)_. ( linguistic data consortium , university of pennsylvania , philadelphia , pa ) . 6 .",
    "jaynes , e.  t. ( 1957 ) information theory and statistical mechanics .",
    "_ phys rev _ * 106 * , 620630 .",
    "jaynes , e.  t. ( 1957 ) information theory and statistical mechanics ii .",
    "_ phys rev _ * 108 * , 171190 .",
    "kullback , s & leibler , r.  a. ( 1951 ) on information and sufficiency .",
    "_ ann math stat _ * 22 * , 7986 .",
    "sivia , d.  s & skilling , j. ( 2006 ) _ data analysis : a bayesian tutorial ( 2nd edition)_. ( oxford university press , oxford , uk ) . 10 .",
    "jaynes , e.  t. ( 1968 ) prior probabilities .",
    "_ ieee trans sys sci & cyb _ * ssc4 * , 227241 ."
  ],
  "abstract_text": [
    "<S> i present a new approach for the interpretation of reaction time ( rt ) data from behavioral experiments . from a physical perspective </S>",
    "<S> , the entropy of the rt distribution provides a model - free estimate of the amount of processing performed by the cognitive system . in this way </S>",
    "<S> , the focus is shifted from the conventional interpretation of individual rts being either long or short , into their distribution being more or less complex in terms of entropy . </S>",
    "<S> the new approach enables the estimation of the cognitive processing load without reference to the informational content of the stimuli themselves , thus providing a more appropriate estimate of the cognitive impact of different sources of information that are carried by experimental stimuli or tasks . </S>",
    "<S> the paper introduces the formulation of the theory , followed by an empirical validation using a database of human rts in lexical tasks ( visual lexical decision and word naming ) . </S>",
    "<S> the results show that this new interpretation of rts is more powerful than the traditional one . </S>",
    "<S> the method provides theoretical estimates of the processing loads elicited by individual stimuli . </S>",
    "<S> these loads sharply distinguish the responses from different tasks . </S>",
    "<S> in addition , it provides upper - bound estimates for the speed at which the system processes information . </S>",
    "<S> finally , i argue that the theoretical proposal , and the associated empirical evidence , provide strong arguments for an adaptive system that systematically adjusts its operational processing speed to the particular demands of each stimulus . </S>",
    "<S> this finding is in contradiction with hick s law , which posits a relatively constant processing speed within an experimental context . </S>",
    "<S> + * keywords : * cognition @xmath0 entropy @xmath0 lexical decision @xmath0 reaction time @xmath0 word naming    ever since its introduction by donders @xcite in the very early days of experimental psychology , reaction time ( rt ) has been among the most widely used measures of cognitive processing in human and animal behavioral experiments . </S>",
    "<S> very generally speaking , following donders seminal work , the logic underlying the analysis of data in rt experiments is that , information processing takes time , thus the average time taken to initiate or complete a task reflects the duration of the process(es ) that are involved in the task . </S>",
    "<S> therefore , if certain types of stimuli , tasks , or groups of subjects elicit longer rts than others , it is generally inferred that the former involve more cognitive processing than the latter . in this study , </S>",
    "<S> i propose a qualitatively different perspective on the understanding of rt data : rather than focusing on whether some experimental conditions elicit shorter or longer rts than others , i investigate whether different conditions elicit rt distributions with different degrees of _ complexity_. as i will argue , an increase in the complexity of the rt distribution constitutes an indirect measure of the amount of information processing that has been performed by the system . </S>",
    "<S> for this , i take a psychologically naive , model - free , approach : instead of guiding the rt analysis using knowledge about the relevant neural and/or psychological processes that give rise to rts , i intend to draw inferences on the former by studying only the properties of the latter .    the cognitive system can be considered a system in the thermodynamical sense of the word . in particular </S>",
    "<S> , it is an open system that exchanges energy ( and information ) with its environment . performing an experimental task </S>",
    "<S> involves an exchange of information with the environment . </S>",
    "<S> the experimental instructions and the presentation of stimuli are a source of information . </S>",
    "<S> performing the experimental task requires the processing of this external information , and information processing is costly in energy terms . as discussed by brillouin @xcite , </S>",
    "<S> the acquisition of information by any part of a system must be offset with a decrease of information somewhere else . in brillouin s terms </S>",
    "<S> there is a balance between gained and lost ` negentropy ' , that is , information . </S>",
    "<S> having received energy and information , the stimulus is processed and a response is initiated . </S>",
    "<S> once more , this process involves a further exchange of negentropy and energy with the environment . </S>",
    "<S> an ideal system with perfect efficiency could perhaps achieve a perfect balance between the received , and the spent negentropy . </S>",
    "<S> however , as the efficiency is never perfect , some negentropy will be lost in the process . </S>",
    "<S> eventually , in the case of the cognitive system , this loss of negentropy can be compensated for by a supply of energy , normally by metabolic means , that would enable the system to return to its ` resting ' state . in short , the processing of experimental stimuli should temporarily increase the entropy of the cognitive system by an amount directly proportional to the amount of information that has been processed , corresponding to the negentropy that was wasted in the process . </S>",
    "<S> in essence , a measure of the increases in the entropy of the cognitive system elicited by different experimental conditions or stimuli would provide an estimate of the amount of information that has been processed ( see @xcite for a detailed physical description of this type of processes ) .    </S>",
    "<S> measuring the overall state of entropy of the cognitive system might not be an easy task , as it would involve a quantification of the uncertainty in the state of all the microscopic units in the system . </S>",
    "<S> however , collateral measures of the ` noise ' emitted by the system should reflect increases in its state of complexity . </S>",
    "<S> this is to say , if the system is in a higher state of complexity , the noises it emits will also increase in their complexity . </S>",
    "<S> the random variability of times at which responses happen in a particular experimental condition can be considered as part of this emitted ` noise ' . </S>",
    "<S> therefore the uncertainty ( _ i.e. , _ entropy , in its statistical sense @xcite ) of this distribution can be taken to reflect the state of the system that generated them . </S>",
    "<S> my working assumption is that one can measure the entropy of the distribution of rts in a particular condition ( i.e. , the _ temporal entropy _ ) , and make inferences about variations in the entropy ( in its physical sense ) of the underlying system . in short , </S>",
    "<S> an increase in the informational entropy of an rt distribution is directly proportional to the amount of information that has been processed .    in a typical repeated measures </S>",
    "<S> rt experiment , the differential entropy @xcite of the rt distribution can be expressed as a mixed effect model ( mem ; see @xcite for recent introductions to this technique ) with meaningful ( and thus very constrained ) parameter values : @xmath1 & = h_0 + k \\sum_{i=1}^{n}\\big [ \\theta_i i_i(s , p ) \\big ] + \\varepsilon \\nonumber \\\\    & \\qquad +   \\mathrm{e}_s\\left[-\\log p\\left(t\\right)\\right ] + \\mathrm{e}_p\\left[-\\log p\\left(t\\right)\\right ] . </S>",
    "<S> \\label{eq : regression - main}\\end{aligned}\\ ] ] in this model , the independent variable is the self - information of the rts ( i.e. , @xmath2 ) , whose expected value is  by definition  the entropy . </S>",
    "<S> the intercept of the model ( @xmath3 ) corresponds to the baseline entropy of the rt distribution , which must always be positive and provides an indication of task complexity . </S>",
    "<S> the fixed effect coefficients ( @xmath4 ) indicate the relative contribution of the @xmath5-th known source of information in the stimuli ( @xmath6 ) , and must all be positive and smaller than or equal to one . in this product , </S>",
    "<S> the @xmath7 represent the proportion of the @xmath5-th source of information that is processed . on the other hand </S>",
    "<S> , @xmath8 is constant for all sources of information representing the proportion of the wasted negentropy that is reflected in the rt variability . </S>",
    "<S> therefore both @xmath8 and the @xmath7 must also lie within the @xmath9 $ ] interval . </S>",
    "<S> this has the additional implication that @xmath8 is bound to be larger than or equal to the largest observed fixed effect value , least the estimated value for some of the @xmath7 would be greater than one . </S>",
    "<S> the last two terms on the right - hand side of eqn .  </S>",
    "<S> [ eq : regression - main ] correspond to random effects of the individual stimulus @xmath10 and participant @xmath11 . </S>",
    "<S> these correspond to other unknown sources of information linked to the identity of the stimulus or participant that are not accounted for by the @xmath6 . finally , @xmath12 accounts for the error in the estimations . </S>",
    "<S> if estimates of @xmath13 and of @xmath14 can somehow be obtained , this relationship can be tested directly .    </S>",
    "<S> information theory has a long history in the study of behavior , particularly so in the study of rts . </S>",
    "<S> very soon after shannon s development of information theory in telecommunications @xcite , psychologists were applying it to the study of human rts . </S>",
    "<S> this produced one of the few standing laws of experimental psychology : the time it takes to make a choice is linearly related to the entropy of the possible alternatives ; this is now referred to as hick s law @xcite . despite some possible corrections ( see , e.g. , @xcite ) , the main claim of this law is still accepted today . </S>",
    "<S> more recently , a direct relation between the perceptual information content of stimuli and rts has been found in psychophysical studies @xcite . </S>",
    "<S> interestingly , it is suggested that the rate at which information is collected could vary with the intensity of the stimulus @xcite . </S>",
    "<S> indeed , for higher cognitive functions  </S>",
    "<S> language in particular  evidence has been presented recently that the rate of information processing is not constant , rather it is linearly related to the average informational load of the stimuli ( i.e. , words ) in a particular experimental context @xcite . </S>",
    "<S> this seems to go against the spirit of hick s law in that information processing speed might not be constant within an experiment after all . in this </S>",
    "<S> study the focus is shifted from the amount of information that is _ carried _ by the stimuli , to the amount of information about them that is actually _ </S>",
    "<S> processed_. as i will show , this change of perspective has important implications for the theoretical and empirical validity of hick s law .    here </S>",
    "<S> , i investigate the usefulness of the thermodynamical argument to understand behavioral data . </S>",
    "<S> i focus on lexical stimuli , as these are less amenable to informational content measurements than plainly perceptual ones ( but see also @xcite for approaches to quantifying different aspects of lexical complexity using information - theoretical measures ) . </S>",
    "<S> the empirical confirmation of the theory makes use of two large sets from the english lexicon project database ( elp ; @xcite ) of english visual lexical decision ( vld ) and word naming ( wn ) data . </S>",
    "<S> the empirical evidence consists of four analyses . </S>",
    "<S> the first one investigates the relationship between the temporal entropy variable ( i.e. , complexity of rts described above ) with the traditional average rt ( magnitude of rts ) interpretation of response latency data . </S>",
    "<S> this serves as a first validation of the plausibility of the approach , and it reveals its relation to hick s law . the second part of the analysis tests the power of the method to distinguish between the vld and wn tasks , despite the great similarity of their average rts . </S>",
    "<S> the third part provides a direct test of the theoretical development expressed in eqn .  </S>",
    "<S> [ eq : regression - main ] for particular properties of the stimuli . </S>",
    "<S> finally , the fourth part goes to further depth about the implications of the relationship between mean rt and temporal entropy , and how these implications provide strong evidence ( both theoretical and empirical ) against hick s law . </S>"
  ]
}