{
  "article_text": [
    "the original method of maximum entropy , maxent @xcite , was designed to assign probabilities on the basis of information in the form of constraints .",
    "it gradually evolved into a more general method , the method of maximum relative entropy ( abbreviated me ) @xcite - caticha07 , which allows one to update probabilities from arbitrary priors unlike the original maxent which is restricted to updates from a uniform background measure .",
    "the realization @xcite that me includes not just maxent but also bayes rule as special cases is highly significant .",
    "first , it implies that me is _ capable of reproducing every aspect of orthodox bayesian inference _ and proves the complete compatibility of bayesian and entropy methods .",
    "second , it opens the door to tackling problems that could not be addressed by either the maxent or orthodox bayesian methods individually .",
    "the main goal of this paper is to explore this latter possibility : the problem of processing data plus additional information in the form of expected values .    when using bayes rule it is quite common to impose constraints on the prior distribution . in some cases these constraints",
    "are also satisfied by the posterior distribution , but these are special cases . in general , constraints imposed on priors do not propagate  to the posteriors .",
    "although bayes rule can handle _ some _ constraints , we seek a procedure capable of enforcing _ any _ constraint on the posterior distributions .",
    "after a brief review of how me processes data and reproduces bayes rule , we derive our main result , the general canonical  form of the posterior distribution for the problem of simultaneous updating with data and moment constraints .",
    "the final result is deceivingly simple : bayes rule is modified by a canonical  exponential factor .",
    "although this result is very simple , it should be handled with caution : once we consider several sources of information such as multiple constraints we must confront the problem of non - commuting constraints .",
    "we discuss the question of whether they should be processed simultaneously , or sequentially , and in what order .",
    "our general conclusion is that these different alternatives correspond to different states of information and accordingly we expect that they will lead to different inferences .    as an illustration , the multinomial example of die tosses",
    "is solved in some detail for two problems .",
    "they appear superficially similar but are in fact very different .",
    "the first die problem requires that the constraints be processed sequentially .",
    "this corresponds to the familiar situation of using maxent to derive a prior and then using bayes to process data .",
    "the second die problem , which requires that the constraints be processed simultaneously , provides a clear example that lies beyond the reach of bayes rule .",
    "our first concern when using the me method to update from a prior to a posterior distribution is to define the space in which the search for the posterior will be conducted .",
    "we wish to infer something about the value of a quantity @xmath0 on the basis of three pieces of information : prior information about @xmath1 ( the prior ) , the known relationship between @xmath2 _ and _ @xmath1 ( the model ) , and the observed values of the data @xmath3 . and @xmath2 to represent one or many unknown variables , @xmath4 , and one or multiple experiments , @xmath5 .",
    "] since we are concerned with both @xmath2 _ and _ @xmath1 , the relevant space is neither @xmath6 nor @xmath7 but the product @xmath8 and our attention must be focused on the joint distribution @xmath9 .",
    "the selected joint posterior @xmath10 is that which maximizes the entropy,@xmath11=-\\tint dxd\\theta ~p(x,\\theta ) \\log \\frac{p(x,\\theta ) } { % p_{\\text{old}}(x,\\theta ) } ~,~   \\label{entropy}\\]]subject to the appropriate constraints .",
    "all prior information is codified into the _ joint prior _ @xmath12 .",
    "both @xmath13 ( the familiar bayesian prior distribution ) and @xmath14 ( the likelihood ) contain prior information .",
    "the new information is the observed data @xmath15 , which in the me framework must be expressed in the form of a constraint on the allowed posteriors .",
    "the family of posteriors @xmath9 that reflects the fact that @xmath2 is now known to be @xmath16 is such that @xmath17this amounts to an _ infinite _ number of constraints on @xmath9 : for each value of @xmath2 there is one constraint and one lagrange multiplier @xmath18 .    maximizing @xmath19 , ( [ entropy ] ) , subject to the constraints ( [ data constraint ] ) plus normalization , @xmath20 + \\tint dx\\,\\lambda ( x)\\left [ \\tint d\\theta ~p(x,\\theta ) -\\delta ( x - x^{\\prime } ) % \\right ] \\right\\ } = 0~,\\]]yields the joint posterior , @xmath21where @xmath22 is a normalization constant , and @xmath23 is determined from ( [ data constraint ] ) , @xmath24the final expression for the joint posterior is@xmath25and the marginal posterior distribution for @xmath1 is@xmath26which is the familiar bayes conditionalization rule .    to summarize : @xmath27 is updated to @xmath28 with @xmath29 fixed by the observed data while @xmath30 remains unchanged .",
    "we see that in accordance with the minimal updating philosophy that drives the me method _ one only updates those aspects of one s beliefs for which corrective new evidence ( in this case , the data ) has been supplied_.",
    "here we generalize the previous section to include additional information about @xmath1 in the form of a constraint on the expected value of some function @xmath31 , @xmath32we emphasize that constraints imposed at the level of the prior need not be satisfied by the posterior . what we do here differs from the standard bayesian practice in that we _ require _ the constraint to be satisfied by the posterior distribution .    maximizing the entropy ( [ entropy ] ) subject to normalization , the data constraint ( [ data constraint ] ) , and the moment constraint ( [ < f > ] ) yields the joint posterior,@xmath33where @xmath22 is a normalization constant,@xmath34the lagrange multipliers @xmath23 are determined from the data constraint , ( [ data constraint]),@xmath35so that the joint posterior becomes@xmath36 is determined by imposing that the posterior @xmath10 satisfy ( [ < f > ] ) .",
    "this yields an implicit equation for @xmath37 , @xmath38note that since @xmath39 the resultant @xmath37 will depend on the observed data @xmath16 . finally , the new marginal distribution for @xmath1 is@xmath40for @xmath41 ( no moment constraint ) we recover bayes rule . for @xmath42 bayes rule",
    "is modified by a canonical  exponential factor .",
    "the me method allows one to process information in the form of constraints .",
    "when we are confronted with several constraints we must be particularly cautious . in what order",
    "should they be processed ? or should they be processed at the same time ?",
    "the answer depends on the nature of the constraints and the question being asked .",
    "we refer to constraints as _ commuting _ when it makes no difference whether they are handled simultaneously or sequentially .",
    "the most common example is that of bayesian updating on the basis of data collected in multiple experiments : for the purpose of inferring @xmath1 it is well - known that the order in which the observed data @xmath43 is processed does not matter .",
    "the proof that me is completely compatible with bayes rule implies that data constraints implemented through @xmath44 functions , as in ( [ data constraint ] ) , commute .",
    "it is useful to see how this comes about .",
    "when an experiment is repeated it is common to refer to the value of @xmath2 in the first experiment and the value of @xmath2 in the second experiment .",
    "this is a dangerous practice because it obscures the fact that we are actually talking about _ two _ separate variables .",
    "we do not deal with a single @xmath2 but with a composite @xmath45 and the relevant space is @xmath46 .",
    "after the first experiment yields the value @xmath47 , represented by the constraint @xmath48 , we can perform a second experiment that yields @xmath49 and is represented by a second constraint @xmath50 .",
    "these constraints @xmath51 and @xmath52 commute because they refer to _ different _ variables @xmath53 and @xmath54 .",
    "an experiment , once performed and its outcome observed , can not be _ un - performed _ and its result can not be _ un - observed _ by a second experiment .",
    "thus , imposing one constraint does not imply a revision of the other .    in general constraints need not commute and when this is the case the order in which they are processed is critical .",
    "for example , suppose the prior is @xmath55 and we receive information in the form of a constraint , @xmath56 .",
    "to update we maximize the entropy @xmath57 $ ] subject to @xmath56 leading to the posterior @xmath58 as shown in figure 1 .",
    "next we receive a second piece of information described by the constraint @xmath59 . at this point",
    "we can proceed in essentially two different ways :    * ( a ) sequential updating . *",
    "having processed @xmath60 , we use @xmath58 as the current prior and maximize @xmath61 $ ] subject to the new constraint @xmath59 .",
    "this leads us to the posterior @xmath62 .    *",
    "( b )  simultaneous updating .",
    "* use the original prior @xmath63 and maximize @xmath57 $ ] subject to both constraints @xmath56 and @xmath59 simultaneously .",
    "this leads to the posterior @xmath64 .",
    "as the current prior and maximize @xmath65 $ ] subject to both constraints @xmath60 and @xmath59 simultaneously .",
    "fortunately , and this is a valuable check for the consistency of the me method , it is easy to show that case ( c ) is equivalent to case ( b ) .",
    "whether we update from @xmath66 or from @xmath58 the selected posterior is @xmath67 . ]    to decide which path ( a ) or ( b ) is appropriate , we must be clear about how the me method treats constraints . the me machinery interprets a constraint such as @xmath60 in a very mechanical way : all distributions satisfying @xmath68 are in principle allowed and all distributions violating @xmath60 are ruled out .",
    "updating to a posterior @xmath58 consists precisely in revising those aspects of the prior @xmath66 that disagree with the new constraint @xmath60 .",
    "however , there is nothing final about the distribution @xmath58 .",
    "it is just the best we can do in our current state of knowledge and we fully expect that future information may require us to revise it further . indeed , when new information @xmath59 is received we must reconsider whether the original @xmath56 remains valid or not .",
    "are _ all _ distributions satisfying the new @xmath59 really allowed , even those that violate @xmath60 ? if this is the case then the new @xmath59 takes over and we update from @xmath58 to @xmath69 . the constraint",
    "@xmath60 may still retain some lingering effect on the posterior @xmath62 through @xmath70 but in general @xmath60 has now become obsolete .",
    "alternatively , we may decide that the old constraint @xmath60 retains its validity .",
    "the new @xmath59 is not meant to revise @xmath60 but to provide an additional refinement of the family of allowed posteriors . in this case",
    "the constraint that correctly reflects the new information is not @xmath59 but the more restrictive @xmath71 .",
    "the two constraints should be processed simultaneously to arrive at the correct posterior @xmath64 .    to summarize : sequential updating is appropriate when old constraints become obsolete and are superseded by new information ; simultaneous updating is appropriate when old constraints remain valid .",
    "the two cases refer to different states of information and therefore _ we expect _ that they will result in different inferences .",
    "these comments are meant to underscore the importance of understanding what information is being processed ; failure to do so will lead to errors that do not reflect a shortcoming of the me method but rather a misapplication of it .",
    "this is a loaded die example illustrating the appropriateness of sequential updating .",
    "the background information is the following : a certain factory makes loaded dice . unfortunately because of poor quality control , the dice are not identical and it is not known how each die is loaded .",
    "it is known , however , that the dice produced by this factory are such that face @xmath72 is on the average twice as likely to come up as face number @xmath73 .",
    "the mathematical representation of this  situation is as follows .",
    "the fact that we deal with dice is modelled in terms of multinomial distributions .",
    "the probability that casting a @xmath74-sided die @xmath75 times yields @xmath76 instances for the @xmath77 face is@xmath78where @xmath79 with @xmath80 , and @xmath81 with @xmath82 .",
    "the generic problem is to infer the parameters @xmath1 on the basis of information about moments of @xmath1 and data @xmath83 .",
    "the additional information about how the dice are loaded is represented by the constraint @xmath84 .",
    "note that this piece of information refers to the factory as a whole and not to any individual die .",
    "the constraint is of the general form of ( [ < f>])@xmath85for this particular factory @xmath86 , and all @xmath87 except for @xmath88 and @xmath89 .",
    "now that the background information has been given , here is our first example .",
    "we purchase a die . on the basis of our general knowledge of dice",
    "we are led to write down a joint prior @xmath90(the particular form of @xmath13 is not important for our current purpose so for the sake of definiteness we can choose it flat . ) at this point the only information we have is that we have a die and it came from a factory described by @xmath60 .",
    "accordingly , we use me to update to a new joint distribution .",
    "this is shown as @xmath58 in figure 1 .",
    "the relevant entropy is@xmath11=-\\tsum\\limits_{m}\\tint d\\theta ~p(x,\\theta ) \\log \\frac{% p(x,\\theta ) } { p_{\\text{old}}(x,\\theta ) } ~,\\]]where @xmath91maximizing @xmath19 subject to normalization and @xmath60 gives the @xmath58 posterior@xmath92where the normalization constant @xmath93 and the lagrange multiplier @xmath94 are determined from@xmath95the joint distribution @xmath96 can be rewritten as@xmath97    to find out more about this particular die we toss it @xmath75 times and obtain data @xmath98 which we represent as a new constraint@xmath99our goal is to infer the @xmath1 that apply to our particular die .",
    "the original constraint @xmath60 applies to the whole factory while the new constraint @xmath59 refers to the actual die of interest and thus takes precedence over @xmath100 as @xmath101 we expect @xmath60 to become less and less relevant .",
    "therefore the two constraints should be processed sequentially .    using me ,",
    "that is ( [ solution b ] ) , we impose @xmath59 and update from @xmath102 to a new joint distribution ( shown as @xmath103 in figure 1)@xmath104marginalizing over @xmath105 and using ( [ p1 ] ) the final posterior for @xmath1 is@xmath106where@xmath107    the readers will undoubtedly recognize that ( [ posterior a ] ) is precisely the result obtained by using maxent to obtain a prior , in this case @xmath108 given in ( [ p1 ] ) , and then using bayes theorem to take the data into account .",
    "this familiar result has been derived in some detail for two reasons : first , to reassure the readers that me does reproduce the standard solutions to standard problems and second , to establish a contrast with the example discussed next .",
    "here is a different problem illustrating the appropriateness of simultaneous updating .",
    "the background information is the same as in the previous example .",
    "the difference is that the factory now hires a quality control engineer who wants to learn as much as he can about the factory .",
    "his initial knowledge is described by the same prior @xmath109 , ( [ p0 ] ) . after some inquiries he is told that the only available information is @xmath110 .",
    "not satisfied with this limited information he decides to collect data that reflect the production of the whole factory .",
    "randomly chosen dice are tossed @xmath75 times yielding data @xmath111 which is represented as a constraint,@xmath112the apparent resemblance with ( [ c2a ] ) may be misleading : ( [ c2a ] ) refers to a single die , while ( [ c2b ] ) now refers to the whole factory .",
    "the goal here is to infer the distribution of @xmath1 that describes the overall population of dice produced by the factory .",
    "the new constraint @xmath113 is information in addition to , rather than instead of , the old @xmath60 : the two constraints should be processed simultaneously . from ( [",
    "joint posterior ] ) the joint posterior is or from @xmath58 we obtain the same posterior @xmath114 . ]",
    "latexmath:[\\[p_{\\text{new}}^{(b)}(m,\\theta ) = \\delta ( m - m^{\\prime } ) p_{\\text{old}}(\\theta    over @xmath105 the posterior for @xmath1 is @xmath116where the new normalization constant is @xmath117this looks like the sequential case , ( [ posterior a ] ) , but there is a crucial difference : @xmath118 and @xmath119 . in the sequential updating case ,",
    "the multiplier @xmath120 is chosen so that the intermediate @xmath58 satisfies @xmath60 while the posterior @xmath69 only satisfies @xmath59 . in the simultaneous updating case",
    "the multiplier @xmath37 is chosen so that the posterior @xmath114 satisfies both @xmath60 and @xmath59 or @xmath71 .",
    "ultimately , the two distributions @xmath121 are different because they refer to different problems : @xmath122 refers to a single die , while @xmath123 applies to all the dice produced by the factory . functions , had the constraints been processed sequentially but in the opposite order , first the data @xmath59 , and then the moment @xmath60 , the resulting posterior would be the same as for simultaneous update to @xmath124 . ]",
    "the realization that the me method incorporates bayes rule as a special case has allowed us to go beyond bayes rule to process both data and expected value constraints simultaneously . to put it bluntly , anything one can do with bayes can also be done with me with the additional ability to include information that was inaccessible to bayes alone .",
    "this raises several questions and we have offered a few answers .",
    "first , it is not uncommon to claim that the non - commutability of constraints represents a _ problem _ for the me method .",
    "processing constraints in different orders might lead to different inferences and this is said to be unacceptable .",
    "we have argued that , on the contrary , the information conveyed by a particular sequence of constraints is not the same information conveyed by the same constraints in different order . since different informational states should in general lead to different inferences , the way me handles non - commuting constraints should not be regarded as a _ shortcoming _ but rather as a _ feature _ of the method .",
    "second , we are capable of processing both data and moments .",
    "is this kind of information of purely academic interest or is it something we might encounter in real life ? at this early stage",
    "our answer must be tentative : we have given just one example  the die factory  which we think is fairly realistic .",
    "however , we feel that other applications ( e.g. in econometrics and ecology ) can be handled in this way as well.giffinecon07,giffineco07    finally , is it really true that this type of problem lies beyond the reach of bayesian methods ?",
    "after all , we can always interpret an expected value as a sample average in a sufficiently large number of trials .",
    "we can always construct a large imaginary ensemble of experiments .",
    "entropy methods then become in principle _ superfluous _ ; all we need is probability . the problem with inventing _ imaginary _",
    "ensembles to do away with entropy in favor of mere probabilities , or to do away with probabilities in favor of more intuitive frequencies , is that the ensembles are just what they are claimed to be , imaginary .",
    "they are purely artificial constructions invented for the purpose of handling incomplete information .",
    "it seems to us that a safer way to proceed is to handle the available information directly as given ( i.e. , as expected values ) without making additional assumptions about an imagined reality .",
    "* acknowledgements : * we would like to acknowledge valuable discussions with c. cafaro , k. knuth , and c. rodrguez",
    ".    9 e. t. jaynes , phys .",
    "rev . * 106 * , 620 and * 108 * , 171 ( 1957 ) ; r. d. rosenkrantz ( ed . ) , _",
    "e. t. jaynes : papers on probability , statistics and statistical physics _",
    "( reidel , dordrecht , 1983 ) ; e. t. jaynes , _ probability theory : the logic of science _",
    "( cambridge university press , cambridge , 2003 ) .",
    "j. e. shore and r. w. johnson , ieee trans .",
    "theory * it-26 * , 26 ( 1980 ) ; ieee trans .",
    "theory * it-27 * , 26 ( 1981 ) .",
    "j. skilling , the axioms of maximum entropy , _ maximum - entropy and bayesian methods in science and engineering _ , g. j. erickson and c. r. smith ( eds . ) ( kluwer , dordrecht , 1988 ) .",
    "a. caticha , relative entropy and inductive inference , _ bayesian inference and maximum entropy methods in science and engineering _ , g. j. erickson and y. zhai ( eds . ) , aip conf",
    "* 707 * , 75 ( 2004 ) ( arxiv.org/abs/physics/0311093 ) .",
    "a. caticha and a. giffin , updating probabilities , _ bayesian inference and maximum entropy methods in science and engineering _ , ed . by ali mohammad - djafari ( ed .",
    ") , aip conf . proc . * 872 * , 31 ( 2006 ) ( http://arxiv.org/abs/physics/0608185 ) .",
    "a. caticha , information and entropy , presented at the _",
    "27th international workshop on bayesian inference and maximum entropy methods in science and engineering _ , saratoga springs , ny , 2007 .",
    "a. giffin , updating probabilities with data and moments : an econometric example , to be presented at the _ 3rd econophysics colloquium _ , ancona , italy , 2007 .",
    "a. giffin , updating probabilities with data and moments : an ecological example , to be presented at the _ 7th international conference on complex systems _ , boston , 2007 .",
    "here we pursue the calculation of the posterior ( [ posterior b ] ) in more detail . to be specific we choose a flat prior , @xmath125 . then , dropping the superscript ( b ) ,      where @xmath127 differs from @xmath128 in ( [ zeta ] ) only by a combinatorial coefficient , @xmath129and @xmath37 is determined from ( [ f ] ) which in terms of @xmath127 now reads @xmath130 .",
    "a brute force calculation gives @xmath127 as a nested hypergeometric series , @xmath131where each @xmath132 is written as a sum of @xmath133 functions ,      the index @xmath135 takes all values from @xmath136 to @xmath137 and the other symbols are defined as follows : @xmath138 , @xmath139 , and @xmath140with @xmath141 .",
    "the terms that have indices @xmath142 are equal to zero ( i.e. @xmath143 etc . ) .",
    "a few technical details are worth mentioning : first , one can have singular points when @xmath144 . in these cases",
    "the sum must be evaluated as the limit as @xmath145 second , since @xmath146 and @xmath147 are positive integers the gamma functions involve no singularities .",
    "lastly , the sums converge because @xmath148 .",
    "the normalization for the first die example , ( [ z2 ] ) , can be calculated in a similar way .",
    "currently , for small values of @xmath74 ( less than 10 ) it is feasible to evaluate the nested sums numerically ; for larger values of @xmath74 it is best to evaluate the integral for @xmath127 using sampling methods . a more detailed version of the multinomial example",
    "is worked out in giffinecon07 ."
  ],
  "abstract_text": [
    "<S> we use the method of maximum ( relative ) entropy to process information in the form of observed data and moment constraints . </S>",
    "<S> the generic canonical  form of the posterior distribution for the problem of simultaneous updating with data and moments is obtained . </S>",
    "<S> we discuss the general problem of non - commuting constraints , when they should be processed sequentially and when simultaneously . as an illustration , the multinomial example of die tosses </S>",
    "<S> is solved in detail for two superficially similar but actually very different problems .     </S>",
    "<S> address = department of physics , university at albany  </S>",
    "<S> suny , albany , ny 12222,usa </S>"
  ]
}