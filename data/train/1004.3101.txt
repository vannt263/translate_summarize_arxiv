{
  "article_text": [
    "clustering algorithms group data according to the given criteria .",
    "for example , it may be a model based on _ spectral clustering _",
    "@xcite or _ prototype based _ model @xcite .    in this paper",
    "we consider a _ prototype based _ approach which may be described as follows .",
    "initially , we have to choose @xmath0 _ prototypes_. corresponding empirical clusters will be defined in accordance to the criteria of the nearest prototype measured by the distance @xmath1 . respectively",
    ", we will generate initial @xmath0 clusters . as a second minimisation step we will recompute _ cluster centers _ or _ @xmath1-means _ @xcite using data strictly from the corresponding clusters .",
    "then , we can repeat clustering step using new prototypes obtained from the previous step as a cluster centers . above algorithm",
    "has descending property . respectively",
    ", it will reach local minimum in a finite number of steps .",
    "pollard @xcite demonstrated that the classical @xmath2-means algorithm in @xmath3 with squared loss function satisfies the key theorem of learning theory @xcite , p.36 , _ `` the minimal empirical risk must converge to the minimal actual risk''_.    a new clustering algorithm in probabilistic space @xmath4 was proposed in @xcite .",
    "it provides an attractive approach based on the kullback leibler divergence .",
    "the above methodology requires a general formulation and framework which we will present in the following section  [ sec : clustprobspace ] .",
    "section  [ sec : defin ] extends the methodology of @xcite in order to cover the case of @xmath4 with kullback leibler divergence . using the results and definitions of the section  [ sec : defin ] , we investigate relevant properties of @xmath4 in the section  [ sec : probframe ] and prove a strong consistence of the empirical risk minimisation inductive principle .",
    "determination of the number of clusters @xmath0 represents an important problem . for example",
    ", @xcite proposed the @xmath5-means algorithm which is based on the @xmath6 fit of the data within particular cluster . usually attempts to estimate the number of gaussian clusters will lead to a very high value of @xmath0 @xcite .",
    "most simple criteria such as @xmath7 ( _ akaike information criterion _ @xcite ) and @xmath8 ( _ bayesian information criterion _",
    "@xcite , @xcite ) either overestimate or underestimate the number of clusters , which severely limits their practical usability .",
    "we introduce in section  [ ssec : detnumclust ] special clustering regularization .",
    "this regularization will restrict creation of a new cluster which is not big enough and which is not sufficiently different comparing with existing clusters .",
    "in this paper we will consider a sample of i.i.d .",
    "observations @xmath9 drawn from probability space @xmath10 where probability measure @xmath11 is assumed to be unknown .",
    "key in this scenario is an encoding problem . assuming that we have a codebook @xmath12 with _",
    "@xmath13 indexed by the code @xmath14 , the aim is to encode any @xmath15 by some @xmath16 such that the distortion between @xmath17 and @xmath16 is minimized : @xmath18 where @xmath19 is a loss function .    using criterion ( [ eq : code ] ) we split empirical data into @xmath0 clusters . as a next step",
    "we compute the _ cluster center _ specifically for any particular cluster in order to minimise overall distortion error .",
    "we estimate actual distortion error @xmath20 : = { { \\mathbf{e}}}\\hspace{0.1 in } { \\mathcal{l}}(x , { \\mathcal{q}})\\end{aligned}\\ ] ] by the empirical error @xmath21 : =     \\frac{1}{n } \\sum_{t=1}^n { \\mathcal{l}}(x_t , { \\mathcal{q}})\\end{aligned}\\ ] ] where @xmath22 .    the following theorem , which may be proved similarly to the theorems 4 and 5 of @xcite , formulates the most important descending and convergence properties within the _ clustering minimisation _ ( cm ) framework :    [ th : rpm ] the @xmath23-algorithm includes 2 steps : * clustering step * : recompute @xmath24 according to ( [ eq : code ] ) for a fixed prototypes from the given codebook @xmath25 , which will be updated as a cluster centers from the next step ,    * minimisation step * : recompute cluster centers for a fixed mapping @xmath24 or minimize the objective function ( [ eq : cda - emp ] ) over @xmath25 , and    \\1 ) monotonically decreases the value of the objective function ( [ eq : cda - emp ] ) ;    \\2 ) converges to a local minimum in a finite number of steps if minimisation step has exact solution .",
    "we define an optimal actual codebook @xmath26 by the following condition : @xmath27    the following relations are valid @xmath28        \\le \\re^{(k)}_\\mathrm{emp}[\\overline{{\\mathcal{q } } } ] ;   \\hspace{0.1 in }         \\re^{(k)}_\\mathrm{emp}[\\overline{{\\mathcal{q } } } ] \\rightarrow        \\re^{(k)}[\\overline{{\\mathcal{q } } } ] \\hspace{0.1 in } a.s.\\ ] ] where @xmath29 is an optimal empirical codebook : @xmath30 the main target is to demonstrate asymptotical ( _ almost sure _ ) convergence @xmath31 \\hspace{0.1 in } a.s .",
    "\\hspace{0.1 in } \\left ( n \\rightarrow \\infty \\right).\\ ] ] in order to prove ( [ eq : defn3 ] ) we define in section  [ sec : defin ] general model which has direct relation to the model in probabilistic space @xmath4 with with @xmath32 divergence @xcite .",
    "the proof of the main result which is formulated in the theorem  [ th : uconv ] includes two steps :    1 .   by lemma",
    "[ th : mlemma ] we prove existence of @xmath33 such that @xmath34 for all @xmath35 where subset @xmath36 satisfies condition : @xmath37 for all @xmath38 ; and 2 .   by lemma",
    "[ th : slemma ] we prove ( under some additional constraints of general nature ) @xmath39 -    \\re^{(k)}[{\\mathcal{q } } ] | \\rightarrow 0 \\hspace{0.1 in } a.s.\\ ] ]",
    "in this section we employ some ideas and methods proposed in @xcite , and which cover the case of @xmath3 with loss function @xmath40 where @xmath41 is a strictly increasing function .",
    "let us assume that the following structural representation with @xmath11-integrable vector - functions @xmath42 and @xmath43 is valid @xmath44 let us define subsets of @xmath45 as extensions of the empirical clusters :    @xmath46    @xmath47 .",
    "then , we can re - write ( [ eq : cda ] ) as follows @xmath48 : =   \\sum_c \\langle { { \\mathbf{\\xi}}}({\\mathcal{x}}_c ) , \\eta(q(c ) ) \\rangle\\ ] ] where @xmath49 .",
    "we define a ball with radius @xmath50 and a corresponding reminder in @xmath45    @xmath51    the following properties are valid @xmath52 for all @xmath53 and any @xmath54 ; @xmath55    suppose , that @xmath56    the following distances will be used below : @xmath57 @xmath58    suppose , that @xmath59 for any fixed @xmath60 .",
    "* remark 1 * we assume that @xmath61 for any fixed @xmath62 , alternatively , the following below lemma  [ th : mlemma ] become trivial .",
    "[ th : mlemma ] suppose , that the structure of the loss function @xmath63 is defined in ( [ eq : loss ] ) under condition ( [ eq : gcnd4 ] ) .",
    "probability distribution @xmath11 satisfies condition ( [ eq : gcnd6 ] ) and the number of clusters @xmath64 is fixed .",
    "then , we can select large enough radius @xmath65 and @xmath66 such that all components of the optimal empirical codebook @xmath29 defined in ( [ eq : defn2 ] ) will be within the ball @xmath67 : @xmath68 if sample size is large enough : @xmath69 .    _",
    "proof : _ existence of the element @xmath70 such that @xmath71 follows from ( [ eq : prpt1 ] ) and ( [ eq : gcnd6 ] ) .",
    "suppose that @xmath72 we can construct @xmath73 in accordance with condition ( [ eq : gcnd4 ] ) and ( [ eq : gcnd5 ] ) : @xmath74 suppose , there are no empirical prototypes within @xmath73 .",
    "then , in accordance with definition ( [ eq : cond1 ] ) @xmath75 \\geq d_{{{\\mathbf{a } } } } + \\epsilon > d_{{{\\mathbf{a } } } }      \\hspace{0.07 in } \\forall n > 0.\\ ] ] above _ contradicts _ to ( [ eq : defn ] ) and ( [ eq : conv1 ] ) .",
    "therefore , at least one prototype from @xmath29 must be within @xmath73 if @xmath76 is large enough ( this fact is valid for @xmath26 as well ) . without loss of generality",
    "we assume that @xmath77 the proof of the lemma has been completed in the case if @xmath78 .",
    "following the method of mathematical induction , suppose , that @xmath79 and @xmath80 then , we define a ball @xmath81 by the following conditions @xmath82 existence of the @xmath83 in ( [ eq : cond2 ] ) follows from ( [ eq : prpt1 ] ) and ( [ eq : gcnd6 ] ) .    by definition of the distance @xmath84 and ball @xmath73 @xmath85",
    "now , we can define reminder @xmath86 in accordance with condition ( [ eq : gcnd4 ] ) : @xmath87 suppose , that there is at least one prototype within @xmath88 , for example , @xmath89 . on the other hand ,",
    "we know about ( [ eq : inside ] ) .",
    "let us consider what will happen if we will remove @xmath90 from the optimal empirical codebook @xmath29 ( the case of optimal actual risk @xmath26 may be considered similarly ) and will replace it by @xmath91 :    1 .   as a consequence of ( [ eq : cond3 ] ) and ( [ eq : cond4 ] ) all empirical data within @xmath81",
    "are closer to @xmath91 anyway , means the data from @xmath81 will not increase empirical ( or actual ) risk ( [ eq : cda - emp ] ) ; 2 .   by definition , @xmath92 and in accordance with the condition ( [ eq : cond2 ] ) an empirical risk increases because of the data within @xmath93 must be strictly less compared with @xmath94 for all large enough @xmath35 ( actual risk increase will be strictly less compared with @xmath94 for all @xmath95 ) .",
    "above _ contradicts _ to the condition ( [ eq : optim ] ) and ( [ eq : conv1 ] ) .",
    "therefore , all prototypes from @xmath26 must be within @xmath96 for all @xmath95 , and @xmath34 if @xmath76 is large enough .",
    "@xmath97      let @xmath98 denote the family of @xmath11-integrable functions on @xmath45 .    a sufficient condition for _ uniform slln _ ( [ eq : uslln ] ) is : for each @xmath99 there exists a _ finite _ class @xmath100 such that to each @xmath101 there are functions @xmath102 and @xmath103 with the following 2 properties :    @xmath104 for all @xmath105 ; @xmath106 .",
    "we shall assume here existence of the function @xmath41 such that @xmath107 for all @xmath108 where @xmath109 .",
    "[ th : slemma ] suppose that the number of clusters @xmath0 is fixed and the loss function @xmath63 is defined by ( [ eq : loss ] ) under conditions ( [ eq : cmpct ] ) and @xmath110 then , the asymptotical relation ( [ eq : uslln ] ) is valid for any @xmath111 .    _",
    "proof : _ let us consider the definition of hausdorff metric @xmath112 in @xmath113 : @xmath114 and denote by @xmath115 a subset in @xmath113 which was obtained from @xmath116 as a result of @xmath43-transformation . according to the condition ( [ eq : cmpct ] ) , @xmath115 represents a compact set .",
    "it means , existence of a finite subset @xmath117 for any @xmath118 such that @xmath119 where @xmath120 is defined in ( [ eq : frst ] ) .",
    "we denote by @xmath121 subset which corresponds to @xmath122 according to the @xmath43-transformation . respectively",
    ", we can define transformation ( according to the principle of the nearest point ) @xmath123 from @xmath116 to @xmath124 , and @xmath125 where closeness may be tested independently for any particular component of @xmath25 , that means absolute closeness .",
    "in accordance with the cauchy - schwartz inequality , the following relations take place @xmath126 finally , @xmath127 where @xmath128 is the absolutely closest codebook for the arbitrary @xmath129 . @xmath97",
    "following @xcite , we assume that the probabilities @xmath130 , represent relations between observations @xmath131 and attributes or classes @xmath132 .",
    "accordingly , we will define probabilistic space @xmath4 of all @xmath133-dimensional probability vectors with _ kullback - leibler _ ( @xmath32 )",
    "divergence : @xmath134 * graphical example*. figure 1(a ) illustrates first two coordinates of the synthetic data in @xmath135 .",
    "third coordinate is not necessary because it is a function of the first two coordinates .    ;",
    "( c ) convergence of the @xmath23 algorithm based on @xmath32 divergence in the case of 6 clusters ; ( d ) behavior of the empirical error ( [ eq : cda - emp ] ) ( blue , dashed ) and empirical error with cost term ( [ eq : rpm1 ] ) where @xmath136 . ]    * remark 2 * as it was demonstrated in @xcite , cluster centers @xmath137 in the space @xmath4 with @xmath32-divergence must be computed using @xmath2-means : @xmath138 where @xmath139 if @xmath140 and @xmath141 is the number of observations in the cluster @xmath142 , @xmath143 .    in difference to the model of @xcite in @xmath144 , the structure ( [ eq : loss ] )",
    "covers an important case of @xmath4 with @xmath32-divergence : @xmath145    @xmath146    * definition*. we will call element @xmath147 as 1 ) _ uniform center _ if @xmath148 ; as 2 ) _ absolute margin _ if @xmath149 .",
    "[ th : lem5 ] the ball @xmath150 contains only one element named as uniform center in the case if @xmath151 , and @xmath152 if @xmath153 .    _",
    "proof : _ suppose , that @xmath154 is a uniform center .",
    "then , @xmath155 for all @xmath147 . in any other case",
    ", one of the components of @xmath154 must be less than @xmath156 . respectively",
    ", we can select corresponding component of the probability vector @xmath157 as @xmath158 .",
    "therefore , @xmath159 and @xmath160 . @xmath97",
    "[ th : lemma3 ] the @xmath32 divergence in probabilistic space @xmath4 always satisfies condition ( [ eq : frst ] ) where vector - function @xmath42 is expressed by ( [ eq : entrp ] ) with the following upper bounds : @xmath161    [ th : lemma4 ] the following relations are valid in @xmath4    1 .",
    "@xmath162 for all @xmath163 ; 2 .",
    "@xmath164 for all @xmath165 , and any @xmath166 .",
    "_ proof : _ as far as @xmath167 , the first statement may be regarded as consequence of the second .",
    "suppose , that @xmath168 and @xmath169 .",
    "then , we can select @xmath170 , and @xmath171 - _",
    "contradiction_. @xmath97    [ axm : kldivps ] the @xmath32 divergence in @xmath4 always satisfies conditions ( [ eq : gcnd4 ] ) and @xmath172 for all @xmath173 where the distance @xmath174 is defined in ( [ eq : metr1 ] ) .",
    "_ proof : _ suppose , that @xmath175 and @xmath176",
    ". then , @xmath177 for all @xmath178 . on the other hand",
    ", the entropy @xmath179 may not be smaller comparing with @xmath180 .",
    "the low bound is _",
    "proved_. in order to prove the upper bound we shall suppose without loss of generality that @xmath181 , and all other components are proportional .",
    "[ th : uconv ] suppose that probability measure @xmath11 satisfies condition ( [ eq : gcnd6 ] ) in probabilistic space @xmath4 with @xmath32 divergence and number of clusters @xmath0 is fixed .",
    "then , the minimal empirical error ( [ eq : defn2 ] ) will converge to the minimal actual error ( [ eq : expect ] ) with probability 1 or a.s .",
    "_ proof : _ follows directly from the lemmas  [ th : mlemma ] , [ th : slemma ] , [ th : lemma3 ] and [ th : lemma4 ] .",
    "* remark 3 * condition ( [ eq : gcnd6 ] ) will not be valid if and only if a probability of the subset of all absolute margins is strictly positive .",
    "note that in order to avoid any problems with consistency we can generalise definition of @xmath32-divergence using special smoothing parameter @xmath182 : @xmath183 where @xmath184 and @xmath185 , @xmath186 is uniform center .",
    "let us introduce the following definitions : @xmath187 @xmath188 where @xmath189 and @xmath190 .",
    "we define in this section a regularisation to restrict usage of unnecessary clusters .",
    "this regularisation is based on the following two conditions :    1 .",
    "@xmath191 ( _ significance of any particular cluster _ ) ; 2 .",
    "@xmath192 ( _ difference between any 2 clusters @xmath193 and @xmath194 _ ) .",
    "according to @xcite , if more prototypes are used for the @xmath0-means clustering , the algorithm splits clusters , which means that it represents a single cluster by more than one prototype .",
    "the following proposition  [ axm : ncx2 ] considers clustering procedure in an inverse direction .",
    "[ axm : ncx2 ] the following representations are valid @xmath195    _ proof : _ in accordance with above definitions @xmath196 and @xmath197 where the second equation follows directly from the first one .",
    "@xmath97    [ axm : ncx4 ] assuming that we merge first @xmath198 clusters , @xmath199 ,",
    "the following relation is valid @xmath200    * remark 4 * first @xmath198 clusters were chosen in order to simplify notifications and without loss of generality .    as a result of standard application of jensen s inequality to ( [ eq : dncln1 ] )",
    "we can formulate similar results in terms of particular differences between clusters .",
    "[ axm : ncx5 ] the following relation is valid @xmath201 for any @xmath202 .    as a direct consequence of ( [ eq : dncln2 ] ) , we derive formula for the case of two clusters indexed by @xmath193 and @xmath203 : @xmath204    the coefficient @xmath205 in ( [ eq : mgij ] ) represents an increasing function of probabilities @xmath206 and @xmath207 . respectively",
    ", we form regularized empirical risk by including additional @xmath208 term in ( [ eq : defn2 ] ) : @xmath209 + c(k)\\ ] ] where @xmath210 minimizing above regularized empirical risk as a function of number of clusters @xmath0 we will make required selection of the clustering size ( see figures  [ fig : figure1](d ) ) .",
    "note a structural similarity between ( [ eq : rpm1a ] ) and akaike information criterion @xcite and @xcite , which has different grounds .",
    "in accordance with aic , the empirical log - likelihood is greater compared with the actual log - likelihood because we use the same data in order to estimate the required parameters .",
    "asymptotically , the bias represents a linear function of the number of the used parameters .",
    "cluster analysis , an unsupervised learning method @xcite , is widely used to study the structure of the data when no specific response variable is specified . recently",
    ", several new clustering algorithms ( e.g. , graph - theoretical clustering , model - based clustering ) have been developed with the intention to combine and improve the features of traditional clustering algorithms .",
    "however , clustering algorithms are based on different assumptions , and the performance of each clustering algorithm depends on properties of the input dataset . therefore , the winning clustering algorithm does not exist for all datasets , and the optimization of existing clustering algorithms is still a vibrant research area @xcite .",
    "probabilistic space with @xmath32-divergence represents an essentially different case compared with euclidean space with standard squared metric . in this paper",
    "we considered an illustration with a simple synthetic example .",
    "however , many real - life datasets may be transferred into probabilistic space as a result of the proper normalisation .",
    "for example , we know that all elements of the colon dataset are strictly positive .",
    "we can normalise any row of the colon matrix ( which has interpretation as a gene ) by division by the sum of the corresponding elements . as a next step",
    ", we can apply the model of section  [ sec : probframe ] in order to reduce dimensionality of the gene expression data .",
    "this analysis has an important role to play in the discovery , validation and understanding of various classes and subclasses of cancer @xcite .",
    "h. akaike .",
    "information theory and an extension of the maximum likelihood principle . \" _",
    "petrov and f. csaki , proc .",
    "2nd international symposium on information theory , akademaii - kiado , budapest _ , pp .",
    "267 - 281 , 1973 .",
    "y. liu , d. hayes , a. nobel and j. marron . statistical significance of clustering for high - dimensional , low - sample size data . \" _ journal of the american statistical association _",
    "103(483 ) , pp . 1281 - 1293 , 2008 ."
  ],
  "abstract_text": [
    "<S> in this paper we formulate in general terms an approach to prove strong consistency of the _ empirical risk minimisation _ inductive principle applied to the prototype or distance based clustering . </S>",
    "<S> this approach was motivated by the divisive information - theoretic feature clustering model in probabilistic space with kullback - leibler divergence which may be regarded as a special case within the _ clustering minimisation _ framework . </S>",
    "<S> also , we propose clustering regularization restricting creation of additional clusters which are not significant or are not essentially different comparing with existing clusters . </S>"
  ]
}