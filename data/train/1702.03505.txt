{
  "article_text": [
    "convolutional neural networks ( cnns )  @xcite have made great achievements in the tasks of image classification and image processing .",
    "they have already been employed for practical use in various situations .",
    "the cnns are known to be robust to small parallel shift thanks to their architecture : units in a cnn have their own local receptive fields , share weight parameters with other units , and are sandwiched by the pooling layers .",
    "the performance of the cnns has been improved by development of new network architectures year after year .",
    "a brief history is found in the results of imagenet large scale visual recognition competition ( ilsvrc ) , which is a competition of image processing by machine learning .",
    "after an 8-layer cnn alexnet  @xcite recorded the highest accuracy in ilsvrc in 2012 , a 19-layer cnn vgg  @xcite and a 22-layer cnn googlenet  @xcite updated the state - of - the - art results in 2014 by deepening the number of layers , increasing the number of internal weight parameters , and thereby expanding the expression ability of the network . a further deep neural network model , resnet  @xcite , recorded the highest accuracy in ilsvrc in 2015 .",
    "this new model has more than 100 convolution layers and has new shortcut connections that do not perform convolution .",
    "this architecture propagates the gradient through the shortcut connections and overcomes the gradient vanishing problem  @xcite , which is considered as an obstacle to accuracy improvement of the deep neural networks . in 2016 ,",
    "the model based on resnet also broke the record .",
    "many studies have focused on devising the structure of the cnns to prevent gradient vanishing problem ( e.g. , densenet  @xcite ) .    on the other hand ,",
    "the fact that the cnn still has a limited robustness to geometric transformations other than parallel shift has not attracted enormous attention .",
    "the notable geometrical transformations of objects in image are scaling and rotation in addition to parallel shift  @xcite . scaled and rotated objects in images",
    "are not recognized correctly or at least require parameters in addition to parameters for recognizing the original objects , resulting in a limitation to the expression ability of the cnns .",
    "several previous studies tried to acquire the invariance by combining with geometric transformation layers  @xcite .",
    "these layers require localization of a object , a repeated computation for a single image , or computation between all - to - all pairs of pixels , and therefore , they require much computation time and are not suited for combining with a deep cnn .    in this paper , we propose a novel network architecture called _ weight - shared multi - stage network _",
    "( wsms - net ) .",
    "in contrast to ordinary cnns built by stacking a number of convolution layers , the wsms - net has a multistage architecture consisting of multiple ordinary cnns arranged in parallel .",
    "a conceptual diagram of the wsms - net is depicted in fig .",
    "[ fig : scale ] .",
    "each stage of the wsms - net consists of all or part of the same cnn : the weight of each convolution layer in each stage is shared with that of the corresponding layer of the other stages . each stage is given a scaled image of an original input image .",
    "the features extracted at the all the stages are integrated at the _ integration layer _ and used for image classification .",
    "thanks to this architecture , similar features are extracted even from objects of different scales at different stages , and thereby , the same objects of different scales are considered to be in the same class .",
    "we apply the wsms - net to existing deep cnns and the experimental results demonstrate that the combination with the wsms - net improves the classification accuracy of the original deep cnns by acquiring the scale invariance or at least the robustness to scaling of objects .",
    "the cnns have been introduced with the shared weight parameters , the limited receptive fields , and the pooling layer  @xcite , inspired by biological architecture of mammalian visual cortex  @xcite .",
    "thanks to the architecture , the cnns suppress increase in the number of weight parameters and acquire a robustness to parallel shift of objects in image . in recent years , cnns have made remarkable improvements in image classification with the deeper and deeper architecture .",
    "the cnns with many convolution layers such as alexnet  @xcite , vgg  @xcite , and googlenet  @xcite have achieved high expression ability by increasing the number of internal weight parameters and classified images with high precision . however , by stacking more and more convolution layers in cnns , gradient vanishing problem arises  @xcite . in general ,",
    "cnns use the back - propagation algorithm , which calculates the gradient obtained at the output layer , back - propagates the gradient in the direction from the output layer to the input layer , and updates the weight parameters .",
    "when the layers of the network become very deep , the gradient information from the output layer does not pass well to the input layer and layers near the input , and learning procedure does not progress well . against this problem , resnet  @xcite and densenet  @xcite were proposed as the new network architectures that enable the learning with further deep architecture .",
    "resnet is the network that recorded the highest accuracy in ilsvrc image classification task in 2015 . from 2015 ,",
    "following resnet , many ideas and new architectures have been proposed  @xcite , and in ilsvrc 2016 , an extended version of resnet broke the record  @xcite .",
    "the main idea of these network is addition of extra connections , which propagate the gradient information successfully and enable robust learning even with 100 or more convolution layers .",
    "[ [ resnet ] ] * resnet * + + + + + + + +    resnet ( residual network )  @xcite is a new form of cnn that enables the cnn to overcome the gradient vanishing problem . a simple structure of resnet is depicted in fig .  [",
    "fig : resnet ] .",
    "the basic structure of resnet is called _ residual block _ , which is composed of two convolution layers and a shortcut connection which does not pass through the convolution layers : the shortcut connection simply performs identity mapping .",
    "the result obtained from the usual convolution layers is denoted as @xmath0 and that from the shortcut connection is denoted as @xmath1 .",
    "the output obtained from the whole block is @xmath2 . in deep cnns",
    ", the gradient information has a risk of vanishing at the feature extraction in the convolution layer @xmath0 .",
    "however , by adding the identity mapping here , the gradient information can be transmitted without any risk of vanishing .",
    "the whole network of resnet is built by stacking this residual block in many layers .",
    "resnet needs no extra parameters and the gradient can be calculated in the same manner as the conventional cnns .    [",
    "[ densenet ] ] * densenet * + + + + + + + + + +    densenet ( densely connected convolutional network )  @xcite is an improved network based on the concept of resnet and its accuracy of image classification is better than that of resnet . a basic architecture of densenet",
    "is depicted in fig .",
    "[ fig : denseblock ] .",
    "as its name implies , the structure of densenet connects layers densely . differently from resnet , in which a shortcut is connected across each two convolution layers ,",
    "a shortcut is connected from one layer to all subsequent layers in densenet . in addition , the output of the shortcut is not added to but is concatenated with the output of the subsequent convolution layer in the direction of the channels of the feature map .",
    "as the depth of network is deepened , the number of the channels of the feature map increases linearly .",
    "this feature is considered to be better than the conventional architectures of ordinary cnns and resnet , with which the number of the channels is doubled at fixed timings like 16 , 32 , 64 , and 128  @xcite .",
    "the number @xmath3 of the increased channels per layer is called _",
    "growth rate_. this basic architecture depicted in fig .",
    "[ fig : denseblock ] is called _ dense block _ and the whole network of densenet is built by stacking the dense block . the original densenet is constructed with three dense blocks : the size of the feature map in a single dense block is fixed and the shortcuts are not connected across each dense block . instead , a @xmath4 convolution layer that does not change the size or the number of channels is placed after a dense block for cushioning , followed by a pooling layer thinning out the features .",
    "a convolution layer is placed before the first dense block for increasing the number of channels of the input image .",
    "although the development of new network models such as resnet and densenet has overcome the gradient vanishing problem , the cnns still do not have established solution to invariance to geometrical transformations such as scaling and rotation .",
    "lack of such invariance is an obstacle of improvement of deep cnns .",
    "several studies have tried to absorb the geometrical transformations by neural networks  @xcite .",
    "spatial transformer network ( stn ) and deep recurrent attentive writer ( draw ) infer the parameters such as position and angle of the geometrical transformation of an object in image and correct the shape of the object by using neural networks .",
    "they are general and powerful approaches against geometrical transformations .",
    "however , the stn requires additional cnns to localize and correct the object in image .",
    "draw requires repeated computation between all - to - all pairs of the input image and the output image to adjust parameters gradually for each image .",
    "they require many additional parameters and computation time , and thereby , they are not suited for combining with a deep cnn .",
    "in this study , we propose a novel network architecture called _ weight - shared multi - stage network _ ( wsms - net ) to acquire the robustness to scaling of objects .",
    "the basic architecture of wsms - net is shown in fig .",
    "[ fig : msnet_related_work ] .",
    "the wsms - net is constructed with @xmath5 stages and each stage is just all or part of an ordinary cnn that can be divided into @xmath3 convolution blocks for @xmath6 . each convolution block is constructed with some convolution layers . between the blocks , pooling layer such as max pooling  @xcite is placed to downsize the feature map by half .",
    "the first stage is constructed with @xmath3 blocks while the second stage has @xmath7 blocks , the third stage has @xmath8 blocks , and so on .",
    "the blocks at the same depth of all the stages completely share their weight parameters of convolution layers .",
    "note that when batch normalization  @xcite is employed , its parameters are not shared . the first stage is given the original image , while the second stage is given the image resized to half by average pooling  @xcite . in a similar way",
    ", the @xmath9-th stage is given the image resized to half of the size of the image given to the ( @xmath10)-th stage .",
    "therefore , the sizes of the output feature maps of all the stages are the same",
    ". then , these feature maps are concatenated in the channel direction at the ends of all the stages .",
    "after concatenation , all the feature maps are integrated at the integration layer and are used as input to the last fully connected layer for classification process .",
    "thanks to these processes , various features are extracted from the input image of multiple sizes in the multiple stages and contribute to the classification .",
    "we explain the detailed concept of the wsms - net .",
    "let us consider the image a and the image b depicted in fig .",
    "[ fig : scale ] .",
    "there are two stages : the first stage is given the original @xmath11 image as input and the second stage is given @xmath12 image , being resized to half , as input .",
    "the image a is an image depicting the whole of an automobile and the image b is an image containing only the part around the left front tire of a sports car . in the training phase ,",
    "the wsms - net is given the image a and learns the features related to the automobile .",
    "in the first stage , since the size of the image remains at @xmath13 , the wsms - net learns the local features of the car such as the shapes of tires , windows , headlights , and license plate .. in the second stage , since the image is resized to @xmath14 , the wsms - net learns the features such as the whole shape of the car but ignores the local features learned in the first stage .",
    "the image b is an image depicting only a part of the sports car ; a tire and a headlight .",
    "when the wsms - net is given the image b , the feature maps corresponding to the local features learned with the image a in the first stage naturally respond to the image b in the second stage .",
    "an image of the whole of an object in a close - up and another image of the object in a long shot also share their features in the second stage and the first stage in a similar way .",
    "we emphasize that the wsms - net is not a data augmentation  @xcite . with a data augmentation , the cnns",
    "are always given a single image resized with various scaling factors and try to classify it without other clues .",
    "therefore , the cnns should learn the features of a great variety by using numerous weight parameters . on the other hand ,",
    "the wsms - net is required to find a feature contributing to classification in at least one of its multiple stages , which are given images of multiple scales . in fig .",
    "[ fig : scale ] , the wsms - net recognizes the object in the image b as a car according to the local features in the second stage . then , the first stage is not required to learn more detailed features such as tire tread pattern . as a result , the variety of the features that the wsms - net should learn is reduced and the increase in the number of weight parameters is suppressed . the wsms - net can be considered to share weight parameters in front - back direction for images , while the ordinary cnns share weight parameters only in vertical and horizontal directions .      by combining with the wsms - net , resnet and densenet are expected to classify the images that could not be dealt with by the original cnns . in this paper , wsms - net combined with resnet",
    "is called _ wsms - resnet _ and wsms - net combined with densenet is called _ wsms - densenet_. for building wsms - resnet and wsms - densenet , the number of stages and the shape of the integration layer should be optimized .",
    "the integration layer is an extra layer placed just after the concatenation layer that concatenates all the feature maps from all the stages , and integrates all the feature maps before the classification . for comparison",
    ", we also introduce a more trivial network called _ multi - stage network _",
    "( ms - net ) .",
    "ms - net has multiple stages like wsms - net but each stage has its own parameters like an ensemble of multiple cnns ( e.g. , @xcite ) .",
    "ms - net has much more parameters than wsms - net but does not have the feature of wsms - net described above .",
    "the number of stages is a fundamental factor of wsms - net .",
    "both resnet and densenet can be divided into several compartments by pooling layers ( average pooling  @xcite or max pooling  @xcite ) , which downsize the feature maps by half .",
    "a compartment of resnet consists of several residual blocks and a compartment of densenet consists of a single dense block .",
    "thus , one compartment can be considered as a single convolution block of wsms - net , and wsms - resnet and wsms - densenet have up to the number of the compartments for obtaining the feature maps of the same size at the integration layer .",
    "we have not yet described the integration layer in detail . in wsms - net",
    ", the feature maps obtained from all the stages are concatenated .",
    "the final number of the channels of the feature map is larger than that in the original network .",
    "the final feature map is often given to a global pooling layer and becomes a feature vector before the last fully connected layer , whose outputs denote the inferred class labels . before the global pooling layer , the integration layer is given the concatenated feature map . in the simplest way , the integration layer does nothing , and the last fully connected layer is given the feature vector longer than that of the original network .",
    "this integration layer is called _ no conv _ , hereafter .",
    "otherwise , the integration layer can be a convolution layer with the @xmath15 kernel and the @xmath16 output channels .",
    "we evaluated a @xmath17 convolution layer as the integration layer because both resnet and densenet mainly employ @xmath17 convolution layers .",
    "this integration layer is also called @xmath17 _ conv_. in addition , we evaluated a @xmath18 convolution layer as the integration layer called @xmath18 _",
    "conv_. we set the number @xmath16 of the output feature maps to 128 in this study .",
    "[ tab : wsms - resnet_cifar-10 ]    [ cols=\"<,^,^,^,^,^ \" , ]",
    "[ [ wsms - resnet - for - cifar-10-and - cifar-100 ] ] * wsms - resnet for cifar-10 and cifar-100 * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    this section evaluates resnet  @xcite with wsms - net for cifar-10 and cifar-100  @xcite .",
    "cifar-10 and cifar-100 are datasets of @xmath19 rgb images of natural scene objects : each consists of 50,000 training images and 10,000 test images .",
    "each image is given one of 10 class labels by hand in cifar-10 dataset , while each image is given one of 100 class labels in cifar-100 dataset : the number of per class images is reduced in cifar-100 .",
    "the 110-layer resnet was reported as the model achieving the highest accuracy for cifar-10 dataset when the reference  @xcite was published .",
    "the 110-layer resnet has three compartments and each compartment consists of 18 residual blocks .",
    "the sizes of the feature maps are @xmath19 , @xmath20 , and @xmath21 in the first , second , and third compartments , respectively .",
    "the number of the channels of the feature map is three at the input and is increased to 16 , 32 , and 64 by the convolution layers placed before the first , second , and third compartments , respectively . after the third compartment , a global average pooling",
    "is performed on the @xmath21 feature map of @xmath22 channels , resulting in a @xmath4 feature map of @xmath22 channels , i.e. , a 64-dimensional feature vector .",
    "this feature vector is given to the final fully connected layer for classification .",
    "we constructed a wsms - resnet by combining the 110-layer resnet  @xcite with wsms - net .",
    "the number of stages of the wsms - resnet was set to three so that each compartment of the 110-layer resnet corresponds to a convolution block of the wsms - net .",
    "an input image having the size of @xmath19 was downsized to @xmath20 for the second stage , and was downsized to @xmath21 for the third stage .",
    "the first stage was just the same as the original 110-layer resnet before the global pooling layer .",
    "the second stage was composed of the first two compartments , in which the size of the feature map was @xmath20 in the first convolution block and @xmath21 in the second convolution block .",
    "also , the third stage is composed of the first compartment of the original 110-layer resnet using the @xmath21 feature map .",
    "the integration layer was given the feature map of @xmath23 channels .",
    "with the integration layer of no conv , the feature maps became a 112-dimensional feature vector through the global average pooling .",
    "otherwise , the feature map was projected to @xmath21 feature maps of 128 channels through the integration layer of @xmath24 conv or @xmath4 conv before the global average pooling .",
    "the hyperparameters and other conditions of the wsms - resnt followed those used for the 110-layer resnet in the original study  @xcite .",
    "all the images were normalized with mean and variance of each channel of the feature map , and @xmath25 cropping and random flipping in the horizontal direction were employed as further data normalization and data augmentation .",
    "batch normalization  @xcite and relu activation function  @xcite were used .",
    "the weight parameters were initialized following the algorithm proposed in the reference  @xcite .",
    "the wsms - resnet was trained using the momentum sgd algorithm with the momentum parameter of 0.9 , the mini - batch size of 128 , and the weight decay of @xmath26 over 164 epochs .",
    "the learning rate was set to 0.01 , and then , it was changed to 0.1 , 0.01 , and 0.001 at the 2nd , the 82nd , and the 123rd epochs , respectively .",
    "[ [ classification - results ] ] * classification results * + + + + + + + + + + + + + + + + + + + + + + + +    table  [ tab : wsms - resnet_cifar-10 ] summarizes the results of the wsms - resnets and the original resnets . since the original study  @xcite did not report the results for cifar-100 , we evaluated the 110-layer resnet for cifar-100 in addition to the wsms - resnet for comparison .",
    "following the original study  @xcite , we obtained the mean test error rate of five trials .",
    "for cifar-10 , the error rate of our proposed wsms - resnet with the @xmath4 conv integration layer is 6.36 % : this accuracy is obviously superior to the error rate of 6.61 % obtained from the original resnet .",
    "however , the number of parameters of wsms - resnet was increased from 1.73 m to 1.75 m . for fair comparison",
    ", we also evaluated the deeper resnets of 116-layer and 122-layer . in spite of a larger increase in the number of parameters",
    ", the deeper resnets achieved the accuracies at a similar level to the 110-layer resnet .",
    "these results demonstrate that the increase in the number of parameters along with the increase in the depth has a limitation in improvement of classification accuracy and that our proposed wsms - net enables the original resnet to achieve a better classification accuracy .",
    "ms - resnet achieved a better error rate than the original resnet but the worse error rate than the wsms - resnet in spite of a massive increase in the number of parameters , indicating that the improvement of the wsms - resnet is thanks to the shared parameters rather than the network architecture . even though the wsms - resnet has the limited increase in the number of parameters thanks to shared weight parameters compared with the ms - resnet , the wsms - resnet and the ms - resnet have the equal increase in the number of calculations owing to the second and third stages .",
    "therefore , we evaluated the number of multiplications in all convolution layers in the original resnet and the wsms - resnet .",
    "the number of multiplications is about 252 m in the original 110-layer resnet and is about 301 m in the 111-layer wsms - resnet with the @xmath4 conv integration layer : our proposed wsms - resnet gets only a 20 % increase in the number of multiplications .",
    "in general , the second stage requires less than 25 % computations of the first stage since the second stage is given an image resized to half .    in the case of cifar-100 ,",
    "the wsms - resnet with @xmath4 conv integration layer also surpassed the original resnet .",
    "unlike the case of cifar-10 , the deeper resnets and the ms - resnet achieved better results than the original resnet , and the wsms - resnet with @xmath24 conv integration layer achieved the best result .",
    "we consider that the difference between cifar-10 and cifar-100s is caused by complexity of classification task .",
    "classification of cifar-100 is more difficult because of the larger number of classes and the limited numbers of samples , and thus , requires much more weight parameters than classification of cifar-10 .",
    "therefore , the wsms - resnet with @xmath24 conv integration layer , having more weight parameters , is simply more advantageous than the wsms - resnet with @xmath4 conv integration layer .",
    "the deeper resnet and the ms - resnet are also better than the original resnet thanks to their large numbers of weight parameters .",
    "[ [ wsms - densenet - for - cifar-10-and - cifar-100 ] ] * wsms - densenet for cifar-10 and cifar-100 * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this section , we combined densenet of growth rate @xmath27 with wsms - net , and constructed wsms - densenet .",
    "the densenet ( @xmath27 ) was reported as the model achieving the second highest classification accuracy among the results of cifar-10 and cifar-100 datasets in the original study  @xcite .",
    "note that we evaluated the model achieving the highest classification accuracy , densenet - bc , implemented on the source code published by the authors of the original study at  https://github.com/liuzhuang13/densenet , but failed reproducing the results : this is why we used the densenet ( @xmath27 ) .",
    "the densenet ( @xmath27 ) has three dense blocks , and the sizes of the feature map are @xmath19 , @xmath20 , and @xmath21 in the first , second , and third dense blocks , respectively .",
    "the number of the channels is three at the input , is increased to 16 by the convolution layer placed before the first dense block , and is increased by the growth rate @xmath27 at every convolution layer .",
    "each dense block consists of 32 convolution layers .",
    "therefore , the number of the channels reaches to @xmath28 . after the third dense block , a global average pooling",
    "is performed , resulting in a 2320-dimensional feature vector given to the last fully connected layer for classification .",
    "we constructed wsms - densenet of growth rate @xmath27 by combining densenet with wsms - net .",
    "each dense block of densenet was treated as a convolution block of wsms - net .",
    "the final numbers of the channels of the first , second , and third stages were @xmath28 , @xmath29 , and @xmath30 , respectively .",
    "hence , the integration layer was given the feature map of @xmath31 channels .",
    "the hyperparameters and the other conditions of wsms - densenet followed those of the original densenet ( @xmath27 ) .",
    "all the images were normalized with mean and variance of each filter , and @xmath25 cropping and random flipping in the horizontal direction were employed as further data normalization and data augmentation .",
    "batch normalization  @xcite and relu activation function  @xcite were used .",
    "the weight weight parameters were initialized following the algorithm proposed in the reference  @xcite .",
    "wsms - densenet was trained using the momentum sgd algorithm with the momentum parameter of 0.9 and the weight decay of @xmath26 over 300 epochs .",
    "the learning rate was set to 0.1 , and then , it was reduced to 0.01 and 0.001 at the 150th and the 225th epochs , respectively . note that only the mini - batch size was changed from 64 to 32 owing to the capacity of the computer we used .",
    "[ [ classification - results-1 ] ] * classification results * + + + + + + + + + + + + + + + + + + + + + + + +    table  [ tab : wsms - densenet_cifar-10 ] summarizes the results of wsms - densenet ( @xmath27 ) and the original densenets . for cifar-10 dataset ,",
    "the error rate of our proposed wsms - densenet with the @xmath4 conv integration layer is 3.51 % , which is better than the error rate of 3.74 % obtained from the original densenet ( @xmath27 ) .",
    "we also evaluated densenet ( @xmath32 ) for fair comparison as is the case with wsms - resnet for cifar-10 dataset and confirmed that densenet ( @xmath32 ) achieved the worse error rate of 3.82 % in spite of a large increase in the number of weight parameters .",
    "the ms - densenet was also evaluated and achieved a worse error rate .",
    "the number of multiplications is about 6,889 m in the original 100-layer densenet and is about 8,454 m in the 100-layer wsms - densenet with the @xmath4 conv integration layer : our proposed wsms - densenet gets only a 20 % increase in the number of multiplications .    also for cifar-100 , our proposed wsms - densenet with",
    "the @xmath4 conv integration layer achieved better accuracy of 18.45 % than the original densenet .",
    "the densenet ( @xmath32 ) and the ms - densenet also achieved accuracies better than the original densenet ( @xmath27 ) but worse than the wsms - densenet . in conclusion ,",
    "regardless of the difference in the structure between resnet and densenet , the experimental results demonstrate that the increase in the depth has a limitation in improvement of classification accuracy and the wsms - net with @xmath4 conv integration layer achieved a better accuracy than the original networks .",
    "[ [ misclassified - images ] ] * misclassified images * + + + + + + + + + + + + + + + + + + + + + +    we collected test images from cifar-10 that the densenet ( @xmath27 ) and the densenet ( @xmath32 ) misclassified but the wsms - densenet ( @xmath27 ) classified correctly for evaluating our proposed wsms - net .",
    "random examples of such test images are shown in fig .",
    "[ fig : scaleresult24 ] . many images in fig .",
    "[ fig : scaleresult24 ] show objects that were taken in close - up and whose large portions were cropped , or show objects taken against backgrounds in long shot . for comparison ,",
    "example images were randomly chosen from all the test images as shown in fig .",
    "[ fig : cifar10test ] . comparison between them demonstrates that the wsms - net additionally classifies images showing scaled objects .",
    "we conclude that the wsms - net acquires the scale invariance or at least is more robust to scaling of objects , and therefore , it achieves better classification accuracy than the original cnns in spite of the limited increase in the number of weight parameters .",
    "a more detailed comparison is a future work .",
    "in this study , we propose a novel network architecture of convolutional neural networks ( cnns ) called _ weight - shared multi - stage network _ ( wsms - net ) to improve classification accuracy by acquiring the robustness to scaling of objects .",
    "the wsms - net consists of multiple stages of cnns given input images of different sizes .",
    "all the feature maps obtained from all the stages are concatenated and integrated at the ends of the stages .",
    "the experimental results demonstrated that the wsms - net achieved better classification accuracy and had higher robustness to scaling of objects than existing cnn model .",
    "future work include a more detailed evaluation of the scale invariance , evaluation with another dataset , and evaluation with another architecture of cnn .",
    "k.  fukushima , `` neocognitron : a self - organizing neural network model for a mechanism of pattern recognition unaffected by shift in position , '' _ biological cybernetics _ ,",
    "36 , no .  4 , pp . 193202 , 1980 .          k.  he , x.  zhang , s.  ren , and j.  sun , `` delving deep into rectifiers : surpassing human - level performance on imagenet classification , '' in _ proc . of the ieee international conference on computer vision ( iccv2016 )",
    "11 - 18-dece , 2016 , pp .",
    "10261034 .",
    "k.  q. huang , gao and sun , yu and liu , zhuang and sedra , daniel and weinberger , `` deep networks with stochastic depth , '' in _ proc . of european conference on computer vision ( eccv2016 )",
    "9905 , 2016 , pp . 646661 .",
    "a.  krizhevsky , i.  sutskever , and g.  e. hinton , `` imagenet classification with deep convolutional neural networks , '' in _ proc . of advances in neural information processing systems ( nips2012 )",
    "_ , f.  pereira , c.  j.  c. burges , l.  bottou , and k.  q. weinberger , eds.1em plus 0.5em minus 0.4em curran associates , inc . , 2012 , pp",
    ". 10971105 .",
    "q.  le , j.  ngiam , z.  chen , d.  h. chia , and p.  koh , `` tiled convolutional neural networks . '' in _ proc . of advances in neural information processing systems ( nips2010 )",
    "_ , j.  d. lafferty , c.  k.  i. williams , j.  shawe - taylor , r.  s. zemel , and a.  culotta , eds.1em plus 0.5em minus 0.4em curran associates , inc . , 2010 , pp . 12791287 .    y.  lecun , b.  boser , j.  s. denker , d.  henderson , r.  e. howard , w.  hubbard , and l.  d. jackel , `` backpropagation applied to handwritten zip code recognition , '' _ neural computation _ , vol .  1 , no .  4 , pp .",
    "541551 , 1989 .",
    "c.  y. lee , s.  xie , p.  gallagher , z.  zhang , and z.  tu , `` deeply - supervised nets , '' in _ proc . of the 18th international conference on artificial intelligence and statistics ( aistats2015 ) _ , vol .  2 , 2015 , p.  5 .",
    "j.  max , k.  simonyan , a.  zisserman , and k.  kavukcuoglu , `` spatial transformer networks , '' in _ proc . of advances in neural information processing systems ( nips2015 ) _ , c.  cortes , n.  d. lawrence , d.  d. lee , m.  sugiyama , and r.  garnett , eds.1em plus 0.5em minus 0.4emcurran associates , inc .",
    ", 2015 , pp . 20172025 .            c.  szegedy , w.  liu , y.  jia , p.  sermanet , s.  reed , d.  anguelov , d.  erhan , v.  vanhoucke , and a.  rabinovich , `` going deeper with convolutions , '' in _ proc . of the ieee computer society conference on computer vision and pattern recognition ( cvpr2015 )",
    "07 - 12-june , 2015 , pp . 19 .",
    "y.  zhang , d.  zhou , s.  chen , s.  gao , and y.  ma , `` single - image crowd counting via multi - column convolutional neural network , '' in _ proc . of the ieee conference on computer vision and pattern recognition ( cvpr2016 )",
    "_ , 2016 , pp . 589597 ."
  ],
  "abstract_text": [
    "<S> convolutional neural networks ( cnns ) have demonstrated remarkable results in image classification tasks for benchmark and practical uses . </S>",
    "<S> the cnns with deeper architectures have achieved higher performances recently thanks to their robustness to parallel shift of objects in images aw well as their numerous parameters and resulting high expression ability . </S>",
    "<S> however , the cnns have a limited robustness to other geometric transformations such as scaling and rotation . </S>",
    "<S> this problem is considered to limit performance improvement of the deep cnns but there is no established solution . </S>",
    "<S> this study focuses on scale transformation and proposes a novel network architecture called _ weight - shared multi - stage network _ ( wsms - net ) , consisting of multiple stages of cnns . </S>",
    "<S> the wsms - net is easily combined with existing deep cnns , such as resnet and densenet , and enables them to acquire a robustness to scaling of objects . </S>",
    "<S> the experimental results demonstrate that existing deep cnns combined with the proposed wsms - net achieve higher accuracy for image classification tasks only with a little increase in the number of parameters .    </S>",
    "<S> shell : bare demo of ieeetran.cls for ieee journals    image classification , scale invariance , convolutional neural network , shared weights </S>"
  ]
}