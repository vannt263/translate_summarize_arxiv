{
  "article_text": [
    "in cuda @xcite , an application is composed of host and device codes .",
    "the host code executes on cpu and the device code executes on system s accelerator .",
    "the host controls the operations of the device through procedure calls to cuda api .",
    "cuda allows programmers to explicitly allocate memory on device and transfer data between the host and the device .",
    "the device obtains the device code from kernel and executes it by thousands of light - weight threads , in simt style @xcite .",
    "all threads share common off - chip dram memory or global memory . in software",
    ", threads are grouped into coarser scheduling elements , referred to as the thread block .",
    "threads within the same block execute concurrently and communicate through a fast , per - block , on - chip software - managed cache , referred to as shared memory .",
    "shared memory is much faster than global memory ; e.g. under gtx 280 , the latency of global memory and shared memory are 440 and 38 core cycles , respectively @xcite .",
    "openacc api introduces a set of compiler directives , library routines , and environment variables to offload a region of code from the cpu to the system s accelerator @xcite .",
    "we refer to this region as the accelerator region .",
    "openacc has two classes of directives : i ) data management and ii ) parallelism control .",
    "each directive has clauses providing finer - grain control .",
    "data management directives perform data allocation on the accelerator , data transfer between the host and the accelerator , and passing pointers to the accelerator .",
    "parallelism control directives allow the programmer to mark regions of code , usually work - sharing loops , intended to run in parallel on the accelerator .",
    "they also control parallelism granularity , variable sharing / privatization , and variable reduction .",
    "openacc introduces four levels of parallelism : gang , worker , vector , and thread . in cuda terminology , these terms best map to kernel , thread block , warp , and thread , respectively .    .... # pragma acc data copy(a[0:len*len],b[0:len*len],c[0:len*len ] )   # pragma acc kernels   # pragma acc loop independent     for(i=0 ; i < len ; + + i ) { # pragma acc loop independent   for(j=0 ; j < len ; + + j ) {     float sum=0 ;     for(l=0 ; l < len ; + + l ) sum + = a[i*len+l]*b[l*len+j ] ;    c[i*len+j]=sum ;    } }                             ( a ) openacc .",
    "_ _ global _ _ void matrixmul(int * a , int * b , int * c , int len ) {   int i = threadidx.x+blockidx.x*blockdim.x ;    int j = threadidx.y+blockidx.y*blockdim.y ;   for(int l=0 ; l < len ; + + l ) sum = a[i*len+l]*b[l*len+j ] ;   c[i*len+j]=sum ; } int main ( ) {   ...   bytes = len*len*sizeof(int ) ;   cudamalloc(&a_d , bytes ) ;   cudamalloc(&b_d , bytes ) ;   cudamalloc(&c_d , bytes ) ;   cudamemcpy(a_d , a , bytes , cudamemcpyhosttodevice ) ;   cudamemcpy(b_d , b , bytes , cudamemcpyhosttodevice ) ;   dim3 gridsize(len/16,len/16 ) , blocksize(16,16 ) ;   matrixmul<<<gridsize , blocksize>>>(a_d , b_d , c_d , len ) ;   cudamemcpy(c , c_d , bytes , cudamemcpydevicetohost ) ;   ... }                              ( b ) cuda . ....      listing 1a and 1b illustrate a simple matrix - matrix multiplication in openacc and cuda , respectively . ignoring the directive lines ,",
    "listing 1a shows the baseline serial multiplication of a and b , storing the result in c. each matrix is len*len in size .",
    "the outer loops iterated by i and j variables can be executed in parallel .",
    "listing 1a shows how these loops can be parallelized using openacc . in this code , _ kernels _ directive marks a region intended to be executed on the accelerator .",
    "_ loop _ directive guides the compiler to consider the loop as a parallel work - sharing loop .",
    "programmers can control the parallelism using kernels and loop directives . as an example of parallelism control ,",
    "the independent clause is used to force the compiler to parallelize the loop .",
    "this clause overwrites the compiler s auto - vectorization and loop dependency checking . in listing 1a , data clauses hint the compiler to copy a , b , and c arrays from the host to the accelerator , and copy them out from the accelerator to the host . for each array ,",
    "the [ start : n ] pair indicates that n elements should be copied from the start element of array . ) .",
    "listing 1b shows how the parallelization can be exploited in cuda .",
    "global indicates the declaration of kernel code .",
    "parallel threads execute the kernel and operate on different matrix elements , based on their unique indexes ( i and j ) . inside the host code ,",
    "device memory is allocated for a , b , and c , keeping the pointer in a_d , b_d , and c_d , respectively .",
    "then , input matrices are copied into device memory .",
    "then , a total of len*len light - weight accelerator threads are launched on the device to execute matrixmul kernel .",
    "after kernel completion , the resulting matrix c d is copied back to the host memory . as presented in listing 1",
    ", openacc significantly reduces the accelerator programming effort in comparison to cuda .",
    "openacc hides low - level accelerator - related code from the programmer and provides a unified view over both host and accelerator code .",
    "ipmacc compilation process starts with an input c / c++ code which is enhanced by openacc api to take advantage of accelerators .",
    "the output of the process can be an object code , binary , or c / c++ source code targeted for either cuda- or opencl - capable accelerator .",
    "figure [ pipeline ] shows the diagram of compilation process which consists of four major stages . in this section",
    ", we describe the compilation process in more details .",
    "descriptions in this section can be used to modify ipmacc to generate customized code .",
    "this stage performs pre - processing to normalize / verify the syntax of input c / c++ and openacc api .",
    "we use uncrustify @xcite to validate c / c++ syntax and normalize its notation .",
    "for example , the region of control ( if ) and loop ( while , for ) statements will be fully - bracketed .",
    "such polishing passes mark the scope associated with each openacc region ; simplifying subsequent stages .",
    "we have also developed a set of scanners to validate openacc api syntax .",
    "the scanners assert the validity of directive in respect to openacc api and assert the validity of clauses in respect to directive .",
    "it also asserts openacc restrictions of using nested directives .",
    "this stage transforms the amended input code to intermediate xml form .",
    "the xml form has only three types of tags : c code , openacc pragma , and for loop . during compilations ,",
    "the codes encompassed in the c code tags remain unmodified .",
    "openacc pragma tags will be replaced by proper calls to implement the accelerator - oriented operations . for loop tags",
    "will be either parallelized on accelerator or stay as they are ( e.g. serial ) .",
    "this decision is made based on the preceding openacc directive ( e.g. loop directive ) or auto - vectorization optimizations .",
    "this intermediate representation separates the host / accelerator and remarkably facilitates openacc translation in next stage .",
    "there are nine sequential steps to translate the intermediate xml form to the final cuda / opencl source code .",
    "this step returns the xml form to c / c++ form while replacing accelerator - related codes ( openacc pragma tags ) with a dummy function call .",
    "meanwhile , the process maintains the openacc information related to dummy functions .",
    "these are essences used to generate corresponding cuda / opencl codes .",
    "the process conceptually splits the code into two codeblocks ( while they are already linked through dummy functions ) : i ) the code bounded by openacc api ( referred to as regions code ) , and ii ) the code placed outside of openacc boundaries ( referred to as original code ) .",
    "original code is executed by host .",
    "regions code , which includes openacc data clauses and kernels regions , is executed either by host or accelerator .    in this terminology , at this step , each regions code is replaced by a dummy function call in original code generating flat host code plus a number of dummy function calls . at the end of compilation ,",
    "dummy functions are replaced by the target - accelerator codes launching computations on the target accelerator , controlling memory transfers between accelerator and host , and synchronizing host - accelerator operations .",
    "this step calculates the abstract syntax tree ( ast ) of original code .",
    "we use ast representation to find further information about variables , types , and functions which are used in the regions code but are not declared in that scope .",
    "particularly , the operation searches for the type of variables , size of arrays , and declaration of user - defined non - standard functions / types .",
    "the scope of regions code contains the declarations / prototypes ( function , variable , or type ) which are used in the region .",
    "it consists of global scope in addition to the scope where the regions code is called .",
    "since a dummy function call is the representative of a region code , the scope of dummy function s parent is the scope of declarations / prototypes used in the regions code .",
    "accordingly , this step finds the parent function calling each dummy call ( notice that dummy functions are unique and have only one parent ) . the global scope and the scope of parent function",
    "are searched for the declarations / prototypes that are referred to in the regions code .",
    "this stage constructs the body of kernel  the targeted code to be executed on the accelerator .",
    "this construction includes : i ) specifying the available parallelism , ii ) sharing loop iterations between concurrent accelerator threads , iii ) performing variable reductions , iv ) regenerating out - defined declarations / prototypes , and v ) specifying kernel arguments .",
    "the dummy function calls which correspond to openacc memory management clauses are replaced by openacc function calls that implement the data management operations ( table 1 ) .",
    "data managements include host - accelerator pointer exchange , data copy in / out to / from accelerator , and memory allocation .",
    "memory allocation essentially needs the size of memory .",
    "the size is either provided by the programmer manually ( through the clauses parameters ) or detected by the compiler automatically ( only the fixed - size arrays are identifiable ) .",
    "this step finds the non - standard types and non - built - in functions which are called in the region code .",
    "subsequently , it searches the ast of original code , which is generated on the 2nd step , to find the declarations of these user - defined non - standard functions / types .",
    "later , these declarations will be appended to the final kernel code .",
    "this step replaces each dummy function associated with kernel calls with a codeblock performing kernel body setup , kernel argument arrangement , and kernel invocation .",
    "there is an extra code in case of variable reduction ( e.g. reduction clause in loop directive ) that merges results across different thread blocks .",
    "we implement variable reduction according to the algorithm proposed in @xcite .      at this step , the original code has been enhanced to launch computation on the target accelerator ( cuda / opencl -capable accelerator ) .",
    "this step stores the enhanced original code on disk , in the same path as the input c / c++ file with _",
    "ipmacc.[cu / c / cpp ] suffix .",
    "this stage invokes system compiler ( nvcc for cuda backend and g++ in other cases ) to generate the target binary .",
    "input to this stage is the source code which is generated in stage 3 and the output can be an object code or executable binary .",
    "operations of this stage are controlled by the ipmacc compilation flags .",
    "* benchmarks . * we use benchmarks from nvidida cuda sdk @xcite and rodinia benchmark suit @xcite .",
    "nvidia cuda sdk includes a large set of cuda test - cases , each implementing a massively - parallel body of an application in cuda very efficiently .",
    "each test - case also includes a serial c / c++ implementation .",
    "we developed an openacc version of these benchmarks over the serial c / c++ code .",
    "rodinia is a gpgpu benchmark suite composed of a wide set of workloads implemented in c / c++ .",
    "originally , each of these benchmarks was implemented in cuda and opencl parallel models .",
    "recently , openacc implementation of the benchmarks has been added by a third - party @xcite .",
    "we include dyadic convolution and n - body simulation from cuda sdk and the remaining benchmarks from rodinia .    * openacc compiler . *",
    "we use our in - house framework , ipmacc , for compiling openacc applications .",
    "ipmacc translates openacc to either cuda or opencl and executes openacc application over cuda or opencl runtime ( e.g. nvidia gpus or amd gpus ) .",
    "we validated the correctness of our framework by comparing the results of openacc benchmarks against the serial and cuda version .",
    "* performance evaluations .",
    "* we compile the openacc version of benchmarks by our framework and run it over cuda runtime .",
    "we compare these to cuda implementations available in cuda sdk and rodinia . in order to evaluate performance , we report the total time of kernel execution , kernel launch , and memory transfer between host and accelerator .",
    "we use _ nvprof _ for measuring these times @xcite . for kernel execution and memory transfers time",
    ", we report the time that _ nvprof _ reports after kernels / transfers completion . for kernel launch time",
    ", we report the time measured by _ nvprof _ in calling _ cudalaunch _ , _ cudasetupargument _ , and _ cudaconfigurecall _ api procedures .",
    "every reported number is the harmonic mean of 30 independent runs .",
    "* platforms . *",
    "we perform the evaluations under a cuda - capable accelerator .",
    "we use nvidia tesla k20c as the accelerator .",
    "this system uses nvidia cuda 6.0 @xcite as the cuda implementation backend .",
    "the other specifications of this system are as follows : cpu : intel xeon cpu e5 - 2620 , ram : 16 gb , and operating system : scientific linux release 6.5 ( carbon ) x86_64 . we use gnu gcc 4.4.7 for compiling c / c++ files .",
    "in this section , we compare a set of openacc applications to their highly optimized cuda version . our goal is to identify openacc s programming limitations resulting in the performance gap between openacc and cuda performance .",
    "see methodology section for applications , compilers , and hardware setup .",
    "figure [ motiv ] reports the execution time for openacc applications , compared to their cuda version .",
    "the figure reports the breakdown of time spent on the accelerator ; kernel launch ( launch ) , kernel execution ( kernel ) , or memory transfer between host and accelerator ( memory ) .",
    "kernel launch time includes the time spent on setting kernel arguments and launching the kernel on the accelerator . in most cases ,",
    "cuda s kernel launch / execution portion is shorter than openacc . also , memory transfer times are comparable on both cuda and openacc . there are exceptions where openacc memory transfers are faster ( e.g. backpro . ) or kernel time of cuda and openacc are equal ( e.g. nearest . ) .",
    "we investigate the differences between cuda and openacc in the following sections .",
    "table[x = num , y = memory ] motivation2.dat ; table[x = num , y = kernel ] motivation2.dat ; table[x = num , y = launch ] motivation2.dat ;    ( title ) at ( 0.08,-1.6 cm ) backprop ; ( title ) at ( 0.16,-2 cm ) bfs ; ( title ) at ( 0.24,-1.6 cm ) dyadic . ; ( title ) at ( 0.30,-2 cm ) hotspot ; ( title ) at ( 0.38,-1.6 cm ) matrix mul . ; ( title ) at ( 0.44,-2 cm ) n - body ; ( title ) at ( 0.52,-1.6 cm ) nearest .",
    "; ( title ) at ( 0.58,-2 cm ) needle - wunsch ; ( title ) at ( 0.66,-1.6 cm ) pathfinder ; ( title ) at ( 0.72,-2 cm ) srad ;      in this section , we discuss applications separately providing insight into why cuda and openacc implementations presented in figure [ motiv ] have different kernel launch , kernel execution , and memory transfer times .    *",
    "back propagation . *",
    "back propagation ( backpro . ) is a machine - learning algorithm used to train the weights in a three - layer neural network . in both openacc and cuda versions ,",
    "there are six back - to - back serial operations where the output of each stage is fed to the immediate next stage as input .",
    "each stage can be performed in parallel on the accelerator .",
    "openacc implementation performs faster memory transfers and slower kernel launch / execution , compared to cuda .",
    "this is explained by the fact that the openacc version executes all six stages on gpu , while the cuda version alternates between cpu and gpu for execution .",
    "alternating between cpu and gpu imposes extra memory transfer overhead .    *",
    "bfs visits all the nodes in the graph and computes the visiting cost of each node . each node is visited only once .",
    "parallel threads of a kernel visit the nodes belonging to the same graph depth concurrently and the algorithm traverses through the depth iteratively .",
    "the operation stops once there is no child to visit .",
    "compared to the cuda version , the openacc version of bfs spends less time on memory transfers .",
    "this can be explained by the fact that the openacc version performs data initializations on the gpu .",
    "however , the cuda version initializes the inputs on the host and transfers the inputs to gpu .",
    "compared to the cuda version , openacc spends more time on kernel execution , since it forces a debilitating reduction on a global variable .",
    "the global variable is a boolean indicating whether there remains more nodes to visit or not .",
    "cuda avoids global reduction by initializing the variable to false on the host and imposing a control - flow divergent in the kernel to guard the global variable from false writes ( allowing true writes ) .",
    "* dyadic convolution . * dyadic convolution ( dyadic . ) is an algebra operation calculating the xor - convolution of two sequences .",
    "the openacc implementation parallelizes output calculations , where each thread calculates one output element .",
    "although this implementation is fast to develop , it exhibits a high number of irregular memory accesses . to mitigate irregular memory accesses , the cuda version uses fast walsch - hadamard transformation ( fwht ) for implementing dyadic convolution ( as described in @xcite ) .",
    "as reported in figure [ motiv ] , both openacc and cuda versions spend almost the same amount of time on memory transfers . while the cuda version launches several kernels , openacc launches only one kernel .",
    "this explains why the cuda version imposes higher kernel launch overhead . in cuda",
    "the kernels execution time is 82% faster than openacc .",
    "this is due to the fact that the cuda version uses fwht to mitigate irregular memory accesses .",
    "although openacc can implement dyadic convolution using fwht , the same fwht algorithm used in cuda can not be implemented in openacc .",
    "cuda fwht uses shared memory to share intermediate writes locally between neighbor threads , which is not possible under openacc standard .",
    "* hotspot . *",
    "hotspot simulates chip characteristics to model the temperature of individual units . at every iteration",
    ", the algorithm reads the temperature and power consumption of each unit and calculates new temperatures .",
    "although both openacc and cuda spend the same amount of time on memory transfers , cuda kernel is much faster .    in hotspot ,",
    "the temperature of each unit depends on its power consumption and neighbors temperatures .",
    "cuda kernel exploits this behavior to localize the communication and reduce global memory accesses as follows . in cuda , threads of the same thread block calculate the temperature of neighbor units .",
    "the cuda version locally updates the new temperature of neighbor units using the threads of the same thread block .",
    "this local communication reduces the number of kernel launches used to synchronize the temperature across all thread blocks , explaining why the cuda version performs faster kernel launches and comes with shorter execution time . in openacc , unlike cuda , the software - managed cache can not be exploited for local communication .",
    "hence , in openacc there are higher number of global synchronizations and kernel launches , which in turn harms performance .",
    "* matrix multiplication .",
    "* matrix multiplication ( matrix mul . ) performs multiplication of two 1024x1024 matrices .",
    "both cuda and openacc implementations use output parallelization , calculating each element of the output matrix in parallel .",
    "cuda version is different from openacc as it processes input matrices tile - by - tile . by processing in tiles ,",
    "cuda version fetches the input tiles in few well - coalesced accesses into software - managed cache and shares the tiles among the threads of the same thread block .",
    "while kernel launch and memory transfer times are nearly the same across cuda and openacc , cuda kernel time is much lower than openacc .",
    "cuda version takes advantage of software - managed cache in two ways .",
    "first , cuda version merges the required data of the thread block and fetches them once , minimizing redundant memory accesses across thread of the same thread block .",
    "second , software - managed cache removes cache conflict misses , since the replacement policy is controlled by the programmer . under openacc , although the threads have very high spatial locality , parsing the matrix row - by - row at a time highly pollutes the cache , returning high number of conflict misses .",
    "also having multiple thread blocks per sm exacerbates this effect .    * n - body simulation .",
    "* n - body models a system of particles under the influence of gravity force . in each timestep ,",
    "operations of o(@xmath0 ) complexity are performed ( for a system of n particles ) to calculate forces between all pairs of particles .",
    "inherently , there are many redundant memory reads , since the mass and position information of each particle is fetched by other particles n-1 times to calculate its interaction with other particles .",
    "while both cuda and openacc memory transfers take about the same time , cuda kernels are much faster .",
    "the cuda version tiles the computations to reduce redundant memory reads @xcite . cuda exploits shared memory to share the particles among all threads of a thread block .",
    "in openacc , however , the redundant memory accesses are not filtered out by the software - managed cache . as reported",
    ", redundant memory accesses can degrade performance significantly .",
    "* nearest neighbor . * nearest neighbor ( nearest . ) finds the five closest points to a target position .",
    "the euclidean distance between the target position and each of the points is calculated and the top five points with the lowest distance are returned .",
    "openacc and cuda versions both calculate euclidean distances for each point in parallel .",
    "openacc and cuda versions spend about the same time on kernel launch , kernel execution , and memory transfer .",
    "this is explained by the similarity of parallelization methods applied in both openacc and cuda .",
    "* needleman - wunsch .",
    "* needleman - wunsch ( needle . ) is a sequence alignment algorithm used in bioinformatics . in either cuda or openacc , traverses a 2d matrix and updates the costs . upon updating a new cost ,",
    "four memory locations are read and one location is written .",
    "although both cuda and openacc versions spend the same amount of time on memory transfers , cuda kernel launch / executions are much faster than openacc kernels .",
    "the cuda version fetches a data chunk of _ costs _ matrix into shared memory and traverses the matrix at the shared memory bandwidth .",
    "this mechanism comes with three advantages : i ) filtering redundant global memory accesses by shared memory , ii ) minimizing global communication by sharing intermediate results stored in the shared memory , iii ) reducing the number of kernel launches and global communications .",
    "the fewer number of kernel launches explains why the launch time of cuda is much less than openacc",
    ".    * pathfinder . * in pathfinder ( pathfin . )",
    "kernel , every working element iteratively finds the minimum of three consequent elements in an array .",
    "the cuda version of pathfinder performs two optimizations : i ) finding the minimum by accessing the data from shared memory , and ii ) sharing the updated minimum locally among neighbor threads for certain iterations and then reflecting the changes globally to other threads .",
    "such local communications reduce the number of global synchronizations and kernel launches .    however , openacc s api is not flexible enough to allow the programmer exploit the shared memory in a similar way",
    ". therefore neighbor threads in the openacc version do not communicate via shared memory .",
    "therefore , each thread fetches the same data multiple times and threads communicate only through global memory .",
    "communication through global memory is implemented through consequent kernel launches .",
    "this explains why openacc imposes higher kernel launch overhead .",
    "* speckle reducing anisotropic diffusion . *",
    "speckle reducing anisotropic diffusion ( srad ) is an image processing benchmark performing noise reduction through partial differential equations iteratively .",
    "compared to cuda , the kernel time of openacc version is less .",
    "three code blocks construct the computation iterative body of this benchmark : one reduction region and two data parallel computations .",
    "our evaluation shows openacc version performs  5% slower than cuda , upon executing two data parallel computations .",
    "however , openacc outperforms cuda in executing the reduction portion .",
    "this is explained by the difference in reduction implementations .",
    "our openacc framework performs the reduction in two levels : reducing along threads of thread block on gpu and reducing along thread block on cpu . in the cuda version ,",
    "however , reduction is performed by multiple serial kernel launches , all on the gpu .",
    "the openacc version spends less time on executing the kernel as part of the computation is carried on host .",
    "meanwhile , performing two levels of reduction imposes the overhead of copying intermediate data from gpu to cpu .",
    "this explains why the openacc version spends slightly more time on memory transfers and less time on kernel launch / execution .",
    "reyes et al . @xcite introduce an open - source tool , named accull , to execute openacc applications on accelerators .",
    "the tool consists of a source to source compiler and a runtime library .",
    "the compiler translates openacc notations to the runtime library routines .",
    "the runtime library routines are implemented in both cuda and opencl .",
    "tian et al . @xcite introduce an openacc implementation integrated in openuh @xcite .",
    "they evaluate the impact of mapping loop iterations over gpu parallel work - items .",
    "lee and vetter @xcite introduce a framework for compiling , debugging , and profiling openacc applications .",
    "they also openarc directives allowing openacc programmer to map openacc arrays to cuda memory spaces , including shared and texture memory spaces .",
    "they do not investigate the effectiveness of their proposal for these mappings .",
    "based on the short introduction that they present , we believe their proposal for utilizing shared memory is different from ours in two ways .",
    "firstly , while openarc directive needs programmer to separate shared memory array and corresponding global memory array in the code , fcw separates the arrays automatically , based on the information presented by the programmer . secondly ,",
    "while openarc directive allows fine - grained control to openacc programmer to perform fetch , synchronization , and writeback , fcw implicitly handles fetch , synchronization , and writeback .",
    "based on these differences , we consider fcw as a high - level proposal for utilizing smc and openarc as a low - level fine - grained control over smc .",
    "nakao et al .",
    "@xcite introduce xacc as an alternative to mpi+openacc programming model to harness the processing power of cluster of accelerators .",
    "xacc offers higher productivity since xacc abstractions reduce the programming efforts . under small and medium problem sizes",
    ", xacc performs up to 2.7 times faster than mpi+openacc .",
    "this higher performance comes from the peach2 interface that xacc communicates through .",
    "peach2 performs faster than gpudirect rdma over infiniband under data transfer size of below 256 kb .",
    "increasing the problem size , xacc and mpi+openacc perform comparable , since the latency of peach2 and gpudirect rdma over infiniband would be equal .",
    "jorg arndt .",
    "matters computational .",
    "springer , 2011 .",
    "chapter 23 , page 481 .",
    "s. che et al .",
    ", rodinia : a benchmark suite for heterogeneous computing , iiswc 2009 .",
    "mark hariss . optimizing parallel reduction in cuda .",
    "available : http://developer.download.nvidia.com/assets/cuda/files/reduction.pdf s. lee and j. s. vetter , openarc : extensible openacc compiler framework for directive - based accelerator programming study .",
    "proceedings of the first workshop on accelerator programming using directives . 2014 .",
    "piscataway , nj , usa . c. liao et al .",
    ", openuh : an optimizing , portable openmp compiler , concurrency and computation : practice and experience , vol .",
    "18 , pp . 2317.2332 , 2007 .",
    "e. lindholm et al . , nvidia tesla : a unified graphics and computing architecture , ieee micro 2008 .",
    "m. nikao et al .",
    "xcalableacc : extension of xcalablemp pgas language using openacc for accelerator clusters .",
    "proceedings of the first workshop on accelerator programming using directives . 2014 .",
    "piscataway , nj , usa . j. nickolls et al . , scalable parallel programming with cuda , queue , vol .",
    "40.53 , mar .",
    "nvidia corporation , .cuda toolkit 6.0 ,",
    "available : https://developer.nvidia.com/cuda-downloads nvidia corporation , profiler s user guide , 2014 .",
    "available : http://docs.nvidia.com/cuda/profiler-users-guide/ j. p. l. nyland and mark harris , gpu gems 3 , 2007 .",
    "modified rodinia benchmark suite , 2013 .",
    "[ online ] .",
    "available : https://github.com/pathscale/rodinia r. reyes et al .",
    ", accull : an openacc implementation with cuda and opencl support , euro - par 2012 .",
    "x. tian et al . compiling a high - level directive - based programming model for gpgpus .",
    "the 26th international workshop on languages and compilers for parallel computing .",
    "2013 . the openacc application programming interface , 2013 . [ online ] .",
    "available : http://www.openacc-standard.org uncrustify : source code beautifier for c , c++ , c # , objectivec , d , java , pawn and vala .",
    "available : http://uncrustify.sourceforge.net/ h. wong et al . ,",
    "demystifying gpu microarchitecture through microbenchmarking , ispass 2010 ."
  ],
  "abstract_text": [
    "<S> in this paper we introduce ipmacc , a framework for translating openacc applications to cuda or opencl . </S>",
    "<S> ipmacc is composed of set of translators translating openacc for c applications to cuda or opencl . </S>",
    "<S> the framework uses the system compiler ( e.g. nvcc ) for generating final accelerator s binary . </S>",
    "<S> the framework can be used for extending the openacc api , executing openacc applications , or obtaining cuda or opencl code which is equivalent to openacc code . </S>",
    "<S> we verify correctness of our framework under several benchmarks included from rodinia benchmark suit and cuda sdk . </S>",
    "<S> we also compare the performance of cuda version of the benchmarks to openacc version which is compiled by our framework . by comparing cuda and openacc versions , </S>",
    "<S> we discuss the limitations of openacc in achieving a performance near to highly - optimized cuda version . </S>"
  ]
}