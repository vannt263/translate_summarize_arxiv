{
  "article_text": [
    "time series forecasting is a fundamental subject in the mathematical statistics literature .",
    "the parametric approach contains a wide range of models associated with efficient estimation and prediction methods , see e.g. @xcite .",
    "classical parametric models include linear processes such as arma models @xcite .",
    "more recently , non - linear processes such as stochastic volatility and arch models received a lot of attention in financial applications - see , e.g. , the seminal paper by nobel prize winner @xcite , and @xcite for a more recent introduction .",
    "however , parametric assumptions rarely hold on data . assuming that the data satisfy a model can biased the prediction and underevaluate the risks , see among others the the polemical but highly informative discussion in @xcite .    in the last few years",
    ", several universal approaches emerged from various fields such as non - parametric statistics , machine learning , computer science and game theory .",
    "these approaches share some common features : the aim is to build a procedure that predicts the time series as well as the best predictor in a given set of initial predictors @xmath5 , without any parametric assumption on the distribution of the observed time series .",
    "however , the set of predictors can be inspired by different parametric or non - parametric statistical models .",
    "we can distinguish two classes in these approaches , with different quantification of the objective , and different terminologies :    * in the `` prediction of individual sequences '' approach , predictors are usually called `` experts '' . the objective is online prediction : at each date @xmath6 , a prediction of the future realization @xmath7 is based on the previous observations @xmath8 , ... ,",
    "@xmath9 , the objective being to minimize the cumulative prediction loss .",
    "see for example @xcite for an introduction .",
    "* in the statistical learning approach , the given predictors are sometimes referred as `` models '' or `` concepts '' .",
    "the batch setting is more classical in this approach .",
    "a prediction procedure is built on a complete sample @xmath10 , ... , @xmath11 .",
    "the performance of the procedure is compared on the expected loss , called the risk , with the best predictor , called the `` oracle '' .",
    "the environment is not deterministic and some hypotheses like mixing or weak dependence are required : see @xcite .    in both settings ,",
    "one is usually able to predict a time series as well as the best model or expert , up to an error term that decreases with the number of observations @xmath0 .",
    "this type of results is referred in statistical theory as oracle inequalities . in other words ,",
    "one builds on the basis of the observations a predictor @xmath12 such that @xmath13 where @xmath14 is a measure of the prediction risk of the predictor @xmath15 . in general , the remainder term is of the order @xmath16 in both approaches , where @xmath3 measures the complexity of @xmath5 .",
    "see , e.g. , @xcite for the `` individual sequences '' approach ; for the `` statistical learning approach '' the rate @xmath17 is reached in @xcite with the absolute loss function and under a weak dependence assumption .",
    "different procedures are used to reach these rates .",
    "let us mention the empirical risk minimization @xcite and aggregation procedures with exponential weights , usually referred as ewa @xcite or gibbs estimator @xcite in the batch approach , linked to the weighted majority algorithm of the online approach @xcite , see also @xcite .",
    "note that results from the `` individual sequences '' approach can sometimes be extended to the batch setting , see e.g. @xcite for the iid case , and @xcite for mixing time series .    in this paper",
    ", we extend the results of @xcite to the case of a general loss function .",
    "another improvement with respect to @xcite is to study both the erm and the gibbs estimator under various hypotheses .",
    "we achieve here inequalities of the form of   that hold with large probability ( @xmath18 for any arbitratily small confidence level @xmath19 ) with @xmath16 . we assume to do so that the observations are taken from a bounded stationary process @xmath20 ( see @xcite however for some possible extensions to unbounded observations ) .",
    "we also assume weak dependence conditions on the process process @xmath20 .",
    "then we prove that the fast rate @xmath21 can be reached for some loss functions including the quadratic loss .",
    "note that @xcite deal with the quadratic loss , their rate can be better than @xmath17 but can not reach @xmath22 .",
    "our main results are based on pac - bayesian oracle inequalities .",
    "the pac - bayesian point of view emerged in statistical learning in supervised classification using the @xmath23-loss , see the seminal papers @xcite .",
    "these results were then extended to general loss functions and more accurate bounds were given , see for example @xcite . in pac - bayesian inequalities",
    "the complexity term @xmath3 is defined thanks to a prior distribution on the set @xmath5 .",
    "the paper is organized as follows : section  [ section_context ] provides notations used in the whole paper .",
    "we give a definition of the gibbs estimator and of the erm in section  [ section_description ] .",
    "the main hypotheses necessary to prove theoretical results on these estimators are provided in section  [ section_hypothesis ] .",
    "we give examples of inequalities of the form   for classical set of predictors @xmath5 in section  [ section_examples ] .",
    "when possible , we also prove some results on the erm in these settings . these results only require a general weak - dependence type assumption on the time series to forecast .",
    "we then study fast rates under a stronger @xmath24mixing assumptions of @xcite in section [ section_fastrates ] .",
    "note that the @xmath25-mixing setting coincides with the one of @xcite when @xmath20 is stationary .",
    "in particular , we are able to generalize the results of @xcite on sparse regression estimation to the case of autoregression . in section  [ section_application ]",
    "we provide an application to french gdp forecasting .",
    "a short simulation study is provided in section  [ section_simulation ] .",
    "finally , the proofs of all the theorems are given in appendices  [ sectiongeneralpacbayes ] and  [ sectionproofs ] .",
    "let @xmath26 denote the observations at time @xmath27 of a time series @xmath28 defined on @xmath29 .",
    "we assume that this series is stationary and take values in @xmath30 equipped with the euclidean norm @xmath31 .",
    "we fix an integer @xmath32 , that might depend on @xmath0 , @xmath33 , and assume that family of predictors is available : @xmath34 . for any parameter @xmath35 and any time @xmath6",
    ", @xmath36 is the prediction of @xmath37 returned by the predictor @xmath35 when given @xmath38 . for the sake of shortness",
    ", we use the notation : @xmath39 we assume that @xmath40 is a linear function . let us fix a loss function @xmath41 that measures a distance between the forecast and the actual realization of the series",
    ". assumptions on @xmath41 will be given in section  [ section_hypothesis ] .",
    "for any @xmath15 we define the prediction risk as @xmath42\\ ] ] ( @xmath14 does not depend on @xmath6 thanks to the stationarity assumption ) .    using the statistics terminology , note that we may want to include parametric set of predictors as well as non - parametric ones ( i.e. respectively finite dimensional and infinite dimensional @xmath5 ) .",
    "let us mention classical parametric and non - parametric families of predictors :    [ exm - arpred ] define the set of linear autoregressive predictors as @xmath43 for @xmath44 .",
    "in order to deal with non - parametric settings , we will also use a model - selection type notation : @xmath45 .",
    "[ ex - nlin ] consider non - parametric auto - regressive predictors @xmath46 where @xmath47 and @xmath48 is a dictionnary of functions @xmath49 ( e.g. fourier basis , wavelets , splines ... ) .",
    "as the objective is to minimize the risk @xmath50 , we use the empirical risk @xmath51 as an estimator of @xmath50 .    for any @xmath15 ,",
    "@xmath52    we define the empirical risk minimizer estimator ( erm ) by @xmath53    let @xmath54 be a @xmath55-algebra on @xmath5 and @xmath56 denote the set of all probability measures on @xmath57 .",
    "the gibbs estimator depends on a fixed probability measure @xmath58 called the _",
    "prior _ that will be involved when measuring the complexity of @xmath5 .",
    "[ def_est ] define the gibbs estimator with inverse temperature @xmath59 as @xmath60    the choice of @xmath61 and @xmath62 in practice is discussed in section  [ section_examples ] .",
    "our results assert that the risk of the erm or gibbs estimator is close to @xmath63 up to a remainder term @xmath64 called the rate of convergence . for the sake of simplicity , let @xmath65 be such that @xmath66 if @xmath67 does not exist , it is replaced by an approximative minimizer @xmath68 satisfying @xmath69 where @xmath70 is negligible w.r.t .",
    "@xmath64 ( e.g. @xmath71 ) .",
    "we want to prove that the erm satisfies , for any @xmath19 , @xmath72 where @xmath73 as @xmath74 .",
    "we also want to prove that and that the gibbs estimator satisfies , for any @xmath19 , @xmath75 where @xmath76 as @xmath74 for some @xmath77 . to obtain such results called _ oracle inequalities _",
    ", we require some assumptions discussed in the next section .",
    "we prove oracle inequalities under assumptions of two different types . on the one hand ,",
    "assumptions * liploss@xmath78 * and * lip@xmath79 * hold respectively on the loss function @xmath41 and the set of predictors @xmath5 . in some extent , we choose the loss function and the predictors , so these assumptions can always be satisfied .",
    "assumption * margin@xmath80 * also holds on @xmath41 .    on the other hand , assumptions * bound@xmath81 * , * weakdep@xmath82 * , * phimix@xmath83",
    "* hold on the dependence and boundedness of the time series . in practice , we can not know whether these assumptions are satisfied on data .",
    "however , remark that these assumptions are not parametric and are satisfied for many classical models , see @xcite . + * assumption liploss@xmath78 , @xmath84 * : the loss function @xmath41 is given by @xmath85 for some convex @xmath86-lipschitz function @xmath87 such that @xmath88 and @xmath89 .",
    "a classical example in statistics is given by @xmath90 , see @xcite .",
    "it satisfies * liploss@xmath78 * with @xmath91 . in @xcite ,",
    "the loss function used is the quadratic loss @xmath92 .",
    "it satisfies * liploss@xmath93 * for time series bounded by a constant @xmath94 .",
    "[ exm_quantile ] the class of quantile loss functions introduced in @xcite is given by @xmath95 where @xmath96 and @xmath97 , @xmath98 .",
    "the risk minimizer of @xmath99 is the quantile of order @xmath100 of the random variable @xmath101 . choosing this loss function one can deal with rare events and build confidence intervals , see @xcite . in this case , * liploss@xmath78 * is satisfied with @xmath102 .",
    "* assumption lip@xmath79 , @xmath103 * : for any @xmath15 there are coefficients @xmath104 for @xmath105 such that , for any @xmath8 , ... , @xmath106 and @xmath107 , ... , @xmath108 , @xmath109 with @xmath110 .",
    "* assumption bound@xmath81 , @xmath94 * : we assume that @xmath111 almost surely .",
    "+ remark that under assumptions * liploss@xmath78 * , * lip@xmath79 * and * bound@xmath112 * , the empirical risk is a bounded random variable .",
    "such a condition is required in the approach of individual sequences .",
    "we assume it here for simplicity but it is possible to extend the slow rates oracles inequalities to unbounded cases see @xcite .",
    "assumption * weakdep@xmath113 * is about the @xmath114-weak dependence coefficients of @xcite .    for any @xmath115 ,",
    "define the @xmath116-weak dependence coefficients of a bounded stationary sequence @xmath20 by the relation @xmath117 - \\mathbb{e}\\left[f(x_{j_1},\\dots , x_{j_\\ell})\\right ] \\bigr\\|_{\\infty}\\end{gathered}\\ ] ] where @xmath118 is the set of @xmath119-lipshitz functions of @xmath32 variables @xmath120    the sequence @xmath121 is non decreasing with @xmath32 .",
    "the idea is that as soon as @xmath122 behaves``almost independently '' from @xmath123 , @xmath124 , @xmath125 then @xmath126 becomes negligible .",
    "actually , it is known that for many classical models of stationary time series , the sequence is upper bounded , see @xcite for details .    * assumption weakdep@xmath83 , @xmath127 * : @xmath128 for any @xmath115 .",
    "examples of processes satisfying * weakdep@xmath113 * are provided in @xcite .",
    "it includes bernoulli shifts @xmath129 where the @xmath130 are iid , @xmath131 and @xmath132 satisfies a lipschitz condition : @xmath133 then @xmath20 is bounded by @xmath134 and satisfies * weakdep@xmath113 * with @xmath135 . in particular , solutions of linear @xmath136 models with bounded innovations",
    "satisfy * weakdep@xmath113*.    in order to prove the fast rates oracle inequalities , a more restrictive dependence condition is assumed .",
    "it holds on the uniform mixing coefficients introduced by @xcite .",
    "the @xmath25-mixing coefficients of the stationary sequence @xmath137 with distribution @xmath138 are defined as @xmath139    * assumption phimix@xmath140 , @xmath141 * : @xmath142 + this assumption appears to be more restrictive than * weakdep@xmath113 * for bounded time series :    @xmath143    ( this result is not stated in @xcite but it is a direct consequence of the last inequality in the proof of corollaire 1 , p. 907 in @xcite ) .    finally , for fast rates oracle inequalities , an additional assumption on the loss function @xmath41 is required . in the iid case , such a condition is also required .",
    "it is called margin assumption , e.g. in @xcite , or bernstein hypothesis , @xcite . + * assumption margin@xmath80 , @xmath144 * : @xmath145 ^ 2 \\right\\ }   \\\\ \\leq \\mathcal{k } \\left[r(\\theta)-r(\\overline{\\theta})\\right ] .",
    "\\end{gathered}\\ ] ]    as assumptions * margin@xmath80 * and * phimix@xmath113 * are used only to obtain fast rates , we give postpone examples to section [ section_fastrates ] .",
    "in this section , we give oracle inequalities and/or   with slow rates of convergence @xmath146 .",
    "the proof of these results are given in section  [ sectionproofs ] .",
    "note that the results concerning the gibbs estimator are actually corollaries of a general result , theorem  [ main_result ] , stated in section  [ sectiongeneralpacbayes ] .",
    "we introduce the following notation for the sake of shortness .",
    "when assumptions , * liploss@xmath78 * , * lip*(@xmath147 ) and * weakdep@xmath83 * are satisfied , we say that we are under the set of assumption * slowrates(@xmath148 ) * where @xmath149 .",
    "consider first the toy example where @xmath5 is finite with @xmath150 , @xmath151 . in this case , the optimal rate in the iid case is known to be @xmath152 , see e.g. @xcite .",
    "[ corofinite ] assume that @xmath150 and that * slowrates(@xmath148 ) * is satisfied for @xmath153 .",
    "let @xmath61 be the uniform probability distribution on @xmath5 .",
    "then the oracle inequality is satisfied for any @xmath59 , @xmath19 with @xmath154    the choice of @xmath62 in practice in this toy example is already not trivial .",
    "the choice @xmath155 yields the oracle inequality : @xmath156 however , this choice is not optimal and one would like to choose @xmath62 as the minimizer of the upper bound @xmath157 however @xmath158 and the constants @xmath159 and @xmath160 are , usually , unknown . in this context",
    "we will prefer the erm predictor that performs as well as the gibbs estimator with optimal @xmath62 :    [ thfinite ] assume that @xmath150 and that * slowrates(@xmath148 ) * is satisfied for @xmath153 .",
    "then the oracle inequality is satisfied for any @xmath19 with @xmath161 = \\frac{4\\kappa}{1-{k}/{n } } \\sqrt{\\frac{\\log\\left ( { 2m}/{\\varepsilon}\\right)}{n } }    .\\ ] ]      we focus on the linear predictors given in example  [ exm - arpred ] .",
    "[ ar - thm1 ] consider the linear autoregressive model of @xmath162 predictors @xmath163 with @xmath164 such that * lip@xmath79 * is satisfied .",
    "assume that assumptions * bound@xmath81 * , * liploss@xmath78 * and * weakdep@xmath113 * are satisfied .",
    "let @xmath61 be the uniform probability distribution on the extended parameter set @xmath165 .",
    "then the oracle inequality is satisfied for any @xmath59 , @xmath19 with @xmath166    in theory , @xmath62 can be chosen of the order @xmath167 to achieve the optimal rates @xmath168 up to a logarithmic factor .",
    "but the choice of the optimal @xmath62 in practice is still a problem .",
    "the erm predictor still performs as well as the gibbs predictor with optimal @xmath62 .",
    "[ ar - thm2 ] under the assumptions of theorem [ ar - thm1 ] , the oracle inequality is satisfied for any @xmath19 with @xmath169.\\end{gathered}\\ ] ]    the additional constraint on @xmath62 does not depend on @xmath0 .",
    "it is restrictive only when @xmath170 , the complexity of the autoregressive model , has the same order than @xmath0 .",
    "for @xmath0 sufficiently large and @xmath171 satisfying the constraint @xmath172 we obtain the oracle inequality @xmath173 theorems  [ ar - thm1 ] and [ ar - thm2 ] are both direct consequences of the following results about general classes of predictors .",
    "we state a general result about finite - dimensional families of predictors .",
    "the complexity @xmath170 of the autoregressive model is replaced by a more general measure of the dimension @xmath174 .",
    "we also introduce some general measure @xmath175 of the diameter that will , for most compact models , be linked to the diameter of the model .",
    "[ thmgibbs1 ] assume that * slowrates(@xmath148 ) * is satisfied and the existence of @xmath176 and @xmath177 satisfying the relation @xmath178 then the oracle inequality is satisfied for any @xmath59 , @xmath19 with @xmath179    a similar result holds for the erm predictor under a more restrictive assumption on the structure of @xmath5 , see remark  [ rmkassumptions ] below .",
    "[ thmerm ] assume that    1 .",
    "@xmath180 , 2 .",
    "@xmath181 a.s .",
    "for some @xmath182 and all @xmath183 .    assume also that * bound@xmath81 * , * liploss@xmath78 * and * weakdep@xmath113 * are satisfied and that * lip*(@xmath147 ) holds on the extended model @xmath184",
    ". then the oracle inequality is satisfied for any @xmath19 with @xmath185.\\ ] ]    this result yields to nearly optimal rates of convergence for the erm predictors .",
    "indeed , for @xmath0 sufficiently large and @xmath186 we obtain the oracle inequality @xmath187 thus , the erm procedure yields prediction that are close to the oracle with an optimal rate of convergence up to a logarithmic factor .",
    "consider the linear autoregressive model of @xmath162 predictors studied in theorems  [ ar - thm1 ] and  [ ar - thm2 ] .",
    "then * lip*(@xmath147 ) is automatically satisfied with @xmath188 .",
    "the assumptions of theorem  [ thmerm ] are satisfied with @xmath189 and @xmath190 .",
    "moreover , thanks to remark  [ rmkassumptions ] , the assumptions of theorem  [ thmgibbs1 ] are satisfied with @xmath191",
    ". then theorems  [ ar - thm1 ] and  [ ar - thm2 ] are actually direct consequences of theorems  [ thmgibbs1 ] and  [ thmerm ] .",
    "note that the context of theorem  [ thmerm ] are less general than the one of theorem  [ thmgibbs1 ] :    [ rmkassumptions ] under the assumptions of theorem  [ thmerm ] we have for any @xmath15 @xmath192 define @xmath61 as the uniform distribution on @xmath193 .",
    "we derive from simple computation the inequality @xmath194 thus , in any case , @xmath195 and the assumptions of theorem  [ thmgibbs1 ] are satisfied for @xmath196 and @xmath197 .    as a conclusion , for some predictors set with a non classical structure , the gibbs estimator might be preferred to the erm .",
    "consider now several models of predictors @xmath198 , ... , @xmath199 and consider @xmath200 ( disjoint union ) .",
    "our aim is to predict as well as the best predictors among all @xmath201 s , but paying only the price for learning in the @xmath201 that contains the oracle . in order to get such a result ,",
    "let us choose @xmath202 priors @xmath203 on each models such that @xmath204 for all @xmath205 .",
    "let @xmath206 be a mixture of these priors with prior weights @xmath207 satisfying @xmath208 .",
    "denote @xmath209 the oracle of the model @xmath201 for any @xmath210 . for any @xmath59 ,",
    "denote @xmath211 the gibbs distribution on @xmath201 and @xmath212 the corresponding gibbs estimator .",
    "a gibbs predictor based on a model selection procedure satisfies an oracle inequality with slow rate of convergence :    [ thmgibbs2 ] assume that :    1 .",
    "* bound*@xmath81 is satisfied for some @xmath94 ; 2 .   *",
    "liploss@xmath78 * is satisfied for some @xmath84 ; 3 .   *",
    "weakdep@xmath83 * is satisfied for some @xmath127 ; 4 .   for any @xmath205 we have 1 .   *",
    "lip*(@xmath213 ) is satisfied by the model @xmath201 for some @xmath214 , 2 .",
    "there are constants @xmath215 and @xmath216 are such that @xmath217    denote @xmath218 and define @xmath219 where @xmath220 minimizes the function of @xmath221 @xmath222 with @xmath223 .\\ ] ] then , with probability at least @xmath18 , the following oracle inequality holds @xmath224.\\ ] ]    the proof is given in appendix  [ sectionproofs ] .",
    "a similar result can be obtained if we replace the gibbs predictor in each model by the erm predictor in each model .",
    "the resulting procedure is known in the iid case under the name srm ( structural risk minimization ) , see @xcite , or penalized risk minimization , @xcite . however , as it was already the case for a fixed model , additional assumptions are required to deal with erm predictors . in the model - selection context , the procedure to choose among all the erm predictors also depends on the unknown @xmath225 s .",
    "thus the model - selection procedure based on gibbs predictors outperforms the one based on the erm predictors .",
    "in this section , we study conditions under which the rate @xmath226 can be achieved .",
    "these conditions are restrictive :    * now @xmath227 , i.e. the process @xmath228 is real - valued ; * the dependence condition * weakdep@xmath83 * is replaced by * phimix@xmath83 * ; * we assume additionally * margin@xmath80 * for some @xmath229 .",
    "let us provide some examples of processes satisfying the uniform mixing assumption * phimix@xmath82*. in the three following examples @xmath230 denotes an iid sequence ( called the innovations ) .",
    "consider the stationary solution @xmath20 of an ar(@xmath231 ) model : @xmath232 , @xmath233 .",
    "assume that @xmath230 is bounded with a distribution possessing an absolutely continuous component .",
    "if @xmath234 has no root inside the unit disk in @xmath235 then @xmath20 is a geometrically @xmath25-mixing processe , see @xcite and * phimix@xmath113 * is satisfied for some @xmath236 .",
    "consider the stationary process @xmath20 such that @xmath237 for all @xmath238 . by definition ,",
    "the process @xmath20 is stationary and @xmath25-dependent - it is even @xmath231-dependent , in the sense that @xmath239 for @xmath240 .",
    "thus * phimix@xmath83 * is satisfied for some @xmath241 .    for extensions of the ar(@xmath231 ) model of the form @xmath242 ,",
    "@xmath243-mixing coefficients can also be computed and satisfy * phimix@xmath113*. see e.g. @xcite .",
    "we now provide an example of predictive model satisfying all the assumptions required to obtain fast rates oracle inequalities , in particular * margin@xmath80 * , when the loss function @xmath41 is quadratic , i.e. @xmath244 :    [ exm_margin ] consider example  [ ex - nlin ] where @xmath245 for functions @xmath48 of @xmath246 to @xmath247 , and @xmath248 .",
    "assume the @xmath249 upper bounded by @xmath119 and @xmath250 such that * lip@xmath79*. moreover * liploss@xmath78 * is satisfied with @xmath251 .",
    "assume that @xmath252 in order to have : @xmath253 ^ 2 \\right\\ } \\\\   & = \\e \\bigl\\ {    \\left[f_{\\theta}(x_{q}, ... ,x_{1 } ) - f_{\\overline{\\theta}}(x_{q}, ... ,x_{1 } )                \\right]^2        \\\\ & \\quad \\quad \\quad \\quad \\quad \\left[2x_{q+1 } - f_{\\theta}(x_{q}, ... ,x_{1 } )              - f_{\\overline{\\theta}}(x_{q}, ...",
    ",x_{1})\\right]^2 \\bigr\\ }   \\\\   & \\leq   \\e   \\left\\ {    \\left[f_{\\theta}(x_{q}, ... ,x_{1 } ) - f_{\\overline{\\theta}}(x_{q}, ... ,x_{1 } )                \\right]^2   4 \\mathcal{b}^{2 } ( 1+r)^2 \\right\\ } \\\\ & \\leq 4 \\mathcal{b}^{2 } ( 1+r)^2 \\left[r(\\theta)-r(\\overline{\\theta})\\right ] \\text { by pythagorean theorem.}\\end{aligned}\\ ] ] assumption * margin@xmath80 * is satisfied with @xmath254 . according to theorem  [ thmfastrates ] below , the oracle inequality with fast rates holds as soon as assumption * phimix@xmath113 * is satisfied .",
    "we only give oracle inequalities for the gibbs predictor in the model - selection setting . in the case of one single model ,",
    "this result can be extended to the erm predictor . for several models ,",
    "the approach based on the erm predictors requires a penalized risk minimization procedure as in the slow rates case . in the fast rates case ,",
    "the gibbs predictor itself directly have nice properties .",
    "let @xmath200 ( disjoint union ) , choose @xmath206 and denote @xmath255 as previously .",
    "[ thmfastrates ] assume that :    1 .",
    "* margin@xmath80 * and * liploss@xmath78 * are satisfied for some @xmath86 , @xmath256 ; 2 .",
    "* bound*@xmath81 is satisfied for some @xmath94 ; 3 .",
    "* phimix@xmath81 * is satisfied for some @xmath257 ; 4 .   *",
    "lip*(@xmath147 ) is satisfied for some @xmath103 ; 5 .   for any @xmath205",
    ", there exist @xmath215 and @xmath258 satisfying the relation @xmath259    then for @xmath260 the oracle inequality for any @xmath19 with @xmath261    compare with the slow rates case , we do nt have to optimize with respect to @xmath62 as the optimal order for @xmath62 is independent of @xmath221 . in practice , the value of @xmath62 provided by theorem [ thmfastrates ] is too conservative . in the iid case ,",
    "it is shown in @xcite that the value @xmath262 , where @xmath263 is the variance of the noise of the regression yields good results . in our simulations results",
    ", we will use @xmath264 , where @xmath265 is the empirical variance of the observed time series .",
    "notice that for the index @xmath266 such that @xmath267 we obtain : @xmath268 so , the oracle inequality achieves the fast rate @xmath269 where @xmath266 is the model of the oracle .",
    "however , note that the choice @xmath270 does not necessarily reach the infimum in theorem  [ thmfastrates ] .",
    "let us compare the rates in theorem  [ thmfastrates ] to the ones in @xcite . in @xcite ,",
    "the optimal rate @xmath226 is never obtained .",
    "the paper @xcite proves fast rates for online algorithms that are also computationally efficient , see also @xcite .",
    "the fast rate @xmath226 is reached when the coefficients @xmath271 are geometrically decreasing .",
    "in other cases , the rate is slower .",
    "note that we do not suffer such a restriction .",
    "the gibbs estimator of theorem  [ thmfastrates ] can also be computed efficiently thanks to mcmc procedures , see @xcite .",
    "let the predictors be the linear autoregressive predictors @xmath272 for any @xmath273 , define the model : @xmath274 let us remark that we have the disjoint union @xmath275 we choose @xmath276 as the uniform probability measure on @xmath277 and @xmath278 .",
    "[ corsparse ] assume that @xmath252 and * phimix@xmath83 * is satisfied for some @xmath241 as well as * bound*@xmath81 .",
    "then the oracle inequality is satisfied for any @xmath279 with @xmath280 for some constant @xmath281 .",
    "this extends the results of @xcite to the case of autoregression .",
    "the proof follows the computations of example  [ exm_margin ] that we do not reproduce here : we check the conditions * liploss@xmath78 * with @xmath251 , * lip@xmath79 * and * margin@xmath80 * with @xmath282",
    ". we can apply theorem  [ thmfastrates ] with @xmath283 and @xmath284 .",
    "every quarter @xmath285 , the french national bureau of statistics , insee , publishes the growth rate of the french gdp ( gross domestic product ) .",
    "since it involves a huge amount of data that take months to be collected and processed , the computation of the gdp growth rate @xmath286 takes a long time ( two years ) .",
    "this means that at time @xmath6 , the value @xmath287 is actually not known .",
    "however , a preliminary value of the growth rate is published 45 days only after the end of the current quarter @xmath6 .",
    "this value is called a _ flash estimate _ and is the quantity that insee forecasters actually try to predict , at least in a first time . as we want to work under the same constraint as the insee",
    ", we will now focus on the prediction on the flash estimate and let @xmath288 denote this quantity . to forecast at time @xmath6",
    ", we will use :    1 .",
    "the past forecastings @xmath289 , @xmath290 ; 2 .",
    "past _ climate indicators _ @xmath291 , @xmath290 , based on _",
    "business surveys_.    business surveys are questionnaires of about ten questions sent monthly to a representative panel of french companies ( see @xcite for more details ) . as a consequence ,",
    "these surveys provide informations from the economic decision makers .",
    "moreover , they are available each end of months and thus can be used to forecast the french gdp .",
    "insee publishes a composite indicator , the _ french business climate indicator _ that summarizes information of the whole business survey , see @xcite .",
    "following @xcite , let @xmath292 be the mean of the last three ( monthly based ) climate indicators available for each quarter @xmath293 at the date of publication of @xmath288 .",
    "all these values ( gdp , climate indicator ) are available from the insee website .",
    "note that a similar approach is used in other countries , see e.g. @xcite on forecasting the european union gdp growth thanks to eurostats data .    in order to provide a quantification of the uncertainty of the forecasting ,",
    "associated interval confidences are usually provided .",
    "the asa and the nber started using density forecasts in 1968 , while the central bank of england and insee provide their prediction with a _ fan chart _ , see ee @xcite for surveys on density forecasting and @xcite for fan charts .",
    "however , the statistical methodology used is often crude and , until 2012 , the fan charts provided by the insee was based on the homoscedasticity of the gaussian forecasting errors , see @xcite",
    ". however , empirical evidences are    1 .",
    "the gdp forecasting is more uncertain in a period of crisis or recession ; 2 .",
    "the forecasting errors are not symmetrically distributed .",
    "define @xmath294 as the data observed at time @xmath6 : @xmath295 .",
    "we use the quantile loss function ( see example [ exm_quantile ] page ) for some @xmath296 of the quantity of interested @xmath288 : @xmath297 we use the family of forecasters proposed by @xcite given by the relation @xmath298 where @xmath299 . fix @xmath300 and @xmath301 let us denote @xmath302 $ ] the risk of the forecaster @xmath303 and let @xmath304 denote the associated empirical risk . we let @xmath305 denote the erm with quantile loss @xmath306 : @xmath307    we apply theorem  [ thmerm ] as * lip*@xmath79 is satisfied @xmath308 with @xmath309 and * liploss@xmath78 * with @xmath91 . if the observations are bounded , stationary such that * weakdep@xmath113 * holds for some @xmath127 , the assumptions of theorem  [ thmerm ] are satisfied with @xmath190 and @xmath310 :    [ thm_appli ] let us fix @xmath311 . if the observations are bounded , stationary such that * weakdep@xmath113 * holds for some @xmath127 then for any @xmath19 and @xmath0 large enough , we have @xmath312    in practice the choice of @xmath313 has little importance as soon as @xmath313 is large enough ( only the theoretical bound is influenced ) . as a consequence we take @xmath314 in our experiments .",
    "the results are shown in figure [ fig05 ] for forecasting corresponding to @xmath315 .",
    "figure [ fig025 ] represents the confidence intervals of order @xmath316 , i.e. @xmath317 and @xmath318 ( left ) and for confidence interval of order @xmath319 , i.e. @xmath320 and @xmath321 ( right ) .",
    "we report only the results for the period 2000-q1 to 2011-q3 ( using the period 1988-q1 to 1999-q4 for learning ) .",
    ", width=264,height=188 ]    [ cols=\"^,^ \" , ]     note that the gibbs predictor performs better on models   and   while the aic predictor performs slightly better on model  .",
    "the difference tends to be negligible when @xmath0 grows - this is coherent with the fact that we develop here a non - asymptotic theory .",
    "note that the gibbs predictor performs also well in the case of a gaussian noise where the boundedness assumption is not satisfied .",
    "9    a.  agarwal and j.  c. duchi , _ the generalization ability of online algorithms for dependent data _",
    ", ieee transactions on information theory ( to appear ) , 2011 .",
    "h.  akaike , _ information theory and an extension of the maximum likelihood principle _ , 2nd international symposium on information theory ( b.  n. petrov and f.  csaki , eds . ) , budapest : akademia kiado , 1973 , pp .",
    "267281 .",
    "p.  alquier and p.  lounici , _ pac - bayesian bounds for sparse regression estimation with exponential weights _ , electronic journal of statistics * 5 * ( 2011 ) , 127145 .",
    "p.  alquier and x.",
    "li , _ prediction of quantiles bu statistical learning and application to gdp forecasting _",
    ", accepted for ds12 , 2012 .",
    "p.  alquier , _ pac - bayesian bounds for randomized empirical risk minimizers _ , mathematical methods of statistics * 17 * ( 2008 ) , no",
    ".  4 , 279304 .    k.  b. athreya and s.  g. pantula , _ mixing properties of harris chains and autoregressive processes _ , j. appl . probab . * 23 * ( 1986 ) , no .  4 , 880892 .",
    "audibert , _ pac - bayesian aggregation and multi - armed bandits _ , hdr universit paris est , 2010 .",
    "p.  alquier and o.  wintenberger , _ model selection for weakly dependent time series forecasting _ , bernoulli * 18 * ( 2012 ) , no .  3 , 883193 .",
    "g.  biau , o.  biau , and l.  rouvire , _ nonparametric forecasting of the manufacturing output growth with firm - level survey data _ , journal of business cycle measurement and analysis * 3 * ( 2008 ) , 317332 .",
    "a.  belloni and v.  chernozhukov , _",
    "l1-penalized quantile regression in high - dimensional sparse models _ , the annals of statistics * 39 * ( 2011 ) , no .  1 , 82130 .    yannick baraud , f.  comte , and g.  viennet , _ model selection for ( auto-)regression with dependent data _ , esaim probab . statist .",
    "* 5 * ( 2001 ) , 3349 .",
    "p.  brockwell and r.  davis , _ time series : theory and methods ( 2nd edition ) _ , springer , 2009 .",
    "e.  britton , p.  fisher , and j.  whitley , _ the inflation report projections : understanding the fan chart _",
    ", bank of england quarterly bulletin * 38 * ( 1998 ) , no .  1 , 3037 .    l.  birg and p.  massart , _ gaussian model selection _ ,",
    "journal of the european mathematical society * 3 * ( 2001 ) , no .  3 , 203268 .",
    "g.  biau and b.  patra , _ sequential quantile prediction of time series _ , ieee transactions on information theory * 57 * ( 2011 ) , 16641674 .",
    "o.  catoni , _ a pac - bayesian approach to adaptative classification _ , 2003 .",
    "o.  catoni , _ statistical learning theory and stochastic optimization _ ,",
    "springer lecture notes in mathematics , 2004 .",
    "o.  catoni , _ pac - bayesian supervised classification ( the thermodynamics of statistical learning ) _ , lecture notes - monograph series , vol .",
    "56 , ims , 2007 .",
    "n.  cesa - bianchi and g.  lugosi , _ prediction , learning , and games _ , cambridge university press , new york , 2006 .",
    "l.  clavel and c.  minodier , _ a monthly indicator of the french business climate _",
    ", documents de travail de la dese , 2009 .",
    "m.  cornec , _ constructing a conditional gdp fan chart with an application to french business survey data _ , 30th ciret conference , new york , 2010 .",
    "j.  c. duchi , a.  agarwal , m.  johansson , and m.  i. jordan , _ ergodic mirror descent _ , preprint arxiv:1105.4681 , 2012 .",
    "j.  dedecker , p.  doukhan , g.  lang , j.  r. len , s.  louhichi , and c.  prieur , _ weak dependence , examples and applications _ , lecture notes in statistics , vol .",
    "190 , springer - verlag , berlin , 2007 .",
    "m.  devilliers , _ les enqutes de conjoncture _ , archives et documents , no .",
    "101 , insee , 1984 .",
    "e.  dubois and e.  michaux , _ talonnages  laide denqutes de conjoncture : de nouvaux rsultats _ , conomie et prvision , no . 172 , insee , 2006 .",
    "p.  doukhan , _ mixing _ , lecture notes in statistics , springer , new york , 1994 .",
    "k.  dowd , _ the inflation fan charts : an evaluation _ , greek economic review * 23 * ( 2004 ) , 99111 .",
    "a.  dalalyan and j.  salmon , _ sharp oracle inequalities for aggregation of affine estimators _ , the annals of statistics ( to appear ) , 2012 .",
    "a.  dalalyan and a.  tsybakov , _",
    "aggregation by exponential weighting , sharp pac - bayesian bounds and sparsity _ , machine learning * 72 * ( 2008 ) , 3961 .    f.  x. diebold , a.  s. tay , and k.  f. wallis , _ evaluating density forecasts of inflation : the survey of professional forecasters _ , discussion paper no.48 , esrc macroeconomic modelling bureau , university of warwick and working paper no.6228 , national bureau of economic research , cambridge , mass . , 1997 .    m.  d. donsker and s.  s. varadhan , _ asymptotic evaluation of certain markov process expectations for large time .",
    "_ , communications on pure and applied mathematics * 28 * ( 1976 ) , 389461 .    r.  f. engle , _ autoregressive conditional heteroscedasticity with estimates of variance of united kingdom inflation _ ,",
    "econometrica * 50 * ( 1982 ) , 9871008 .    c.  francq and j .- m .",
    "zakoian , _ garch models : structure , statistical inference and financial applications _ , wiley - blackwell , 2010 .",
    "s.  gerchinovitz , _ sparsity regret bounds for individual sequences in online linear regression _ , proceedings of colt11 , 2011 .",
    "j.  hamilton , _ time series analysis _ , princeton university press , 1994 .",
    "i.  a. ibragimov , _ some limit theorems for stationary processes _ , theory of probability and its application * 7 * ( 1962 ) , no .  4 , 349382 .    r.  koenker and g.  jr .",
    "bassett , _ regression quantiles _ , econometrica * 46 * ( 1978 ) , 3350 .",
    "r.  koenker , _ quantile regression _ , cambridge university press , cambridge , 2005 .",
    "s.  kullback , _ information theory and statistics _ , wiley , new york , 1959 .",
    "g.  lecu , _ interplay between concentration , complexity and geometry in learning theory with applications to high dimensional data analysis _ ,",
    "hdr thesis , universit paris - est marne - la - valle , 2011 .",
    "x.  li , _ agrgation de prdicteurs applique  la conjoncture _ , rapport de stage de m2 - universit paris 6 - insee sous la direction de matthieu cornec , 2010 .",
    "n.  littlestone and m.k .",
    "warmuth , _ the weighted majority algorithm _ , information and computation * 108 * ( 1994 ) , 212261 .",
    "d.  a. mcallester , _ pac - bayesian model averaging _ , procs .",
    "of of the 12th annual conf .",
    "on computational learning theory , santa cruz , california ( electronic ) , acm , new - york , 1999 , pp .",
    "164170 .",
    "r.  meir , _",
    "nonparametric time series prediction through adaptive model selection _",
    ", machine learning * 39 * ( 2000 ) , 534 .",
    "c.  minodier , _ avantages compars des sries premires valeurs publies et des sries des valeurs rvises _ , documents de travail de la dese , 2010 .    d.  s. modha and e.  masry , _ memory - universal prediction of stationary random processes _ , ieee transactions on information theory * 44 * ( 1998 ) , no .  1 , 117133 .    s.  p. meyn and r.  l. tweedie , _ markov chains and stochastic stability _ , communications and control engineering series , springer - verlag london ltd . ,",
    "london , 1993 .",
    "e.  mammen and a.  b. tsybakov , _ smooth discrimination analysis _ , the annals of statistics * 34 * ( 1999 ) , no .  5 , 18081829 .    ,",
    "_ r : a language and environment for statistical computing _ ,",
    "r foundation for statistical computing , vienna , 2008 .",
    "e.  rio , _ ingalits de hoeffding pour les fonctions lipschitziennes de suites dpendantes _ , comptes rendus de lacadmie des sciences de paris , srie i * 330 * ( 2000 ) , 905908 .",
    "samson , _ concentration of measure inequalities for markov chains and @xmath25-mixing processes _ , the annals of probability * 28 * ( 2000 ) , no .  1 , 416461 .    y.  seldin , f.  laviolette , n.  cesa - bianchi , j.  shawe - taylor , j.  peters , and p.  auer , _ pac - bayesian inequalities for martingales _ , ieee transactions on information theory ( to appear ) , 2012 .",
    "g.  stoltz , _ agrgation squentielle de prdicteurs : mthodologie gnrale et applications  la prvision de la qualit de lair et  celle de la consommation lectrique _",
    ", journal de la sfds * 151 * ( 2010 ) , no .  2 , 66106 .",
    "j.  shawe - taylor and r.  williamson , _ a pac analysis of a bayes estimator _ , proceedings of the tenth annual conference on computational learning theory , colt97 , acm , 1997 , pp .",
    "n.  n. taleb , _ black swans and the domains of statistics _ , the american statistician * 61 * ( 2007 ) , no .  3 , 198200 .",
    "a.  tsybakov , _ optimal rates of aggregation _ , learning theory and kernel machines ( b.  schlkopf and m.  k. warmuth , eds . ) , springer lncs , 2003 , pp .",
    "303313 .",
    "a.  s. tay and k.  f. wallis , _ density forecasting : a survey _ , journal of forecasting * 19 * ( 2000 ) , 235254 .    v.  vapnik , _ the nature of statistical learning theory _ , springer , 1999 .",
    "vovk , _ aggregating strategies _ , proceedings of the 3rd annual workshop on computational learning theory ( colt ) , 1990 , pp .",
    "372283 .",
    "o.  wintenberger , _ deviation inequalities for sums of weakly dependent time series _ , electronic communications in probability * 15 * ( 2010 ) , 489503 .",
    "theorems  [ corofinite ] and  [ thmgibbs1 ] are actually both corollaries of a more general result that we would like to state for the sake of completeness .",
    "this result is the analogous of the pac - bayesian bounds proved by catoni in the case of iid data  @xcite .",
    "[ main_result ] let us assume that * lowrates(@xmath148 ) * is satisfied for some @xmath153 .",
    "then , for any @xmath62 , @xmath19 we have @xmath322 \\right\\ } \\\\",
    "\\geq 1-\\varepsilon.\\end{gathered}\\ ] ]    this result is proved in appendix  [ sectionproofs ] , but we can now provide the proofs of theorems  [ corofinite ] and  [ thmgibbs1 ] .",
    "_ proof of theorem  [ corofinite ] .",
    "_ we apply theorem  [ main_result ] for @xmath323 and restrict the @xmath324 in the upper bound to dirac masses @xmath325 .",
    "we obtain @xmath326 , and the upper bound for @xmath327 becomes : @xmath328   \\\\   & = \\inf_{\\theta\\in\\theta } \\left [ r(\\theta ) + \\frac{2\\lambda \\kappa^{2}}{n\\left(1-{k}/{n}\\right)^2 } + \\frac { 2 \\log\\left({2m}/{\\varepsilon}\\right ) } { \\lambda }   \\right].\\end{aligned}\\ ] ] @xmath329    _ proof of theorem  [ thmgibbs1 ] . _",
    "an application of theorem  [ main_result ] yields that with probability at least @xmath18 @xmath330.\\ ] ] let us estimate the upper bound at the probability distribution @xmath331 defined as @xmath332 then we have : @xmath333.\\end{gathered}\\ ] ] under the assumptions of theorem  [ thmgibbs1 ] we have : @xmath334.\\ ] ] the infimum is reached for @xmath335 and we have : @xmath336 @xmath329",
    "we will use rio s inequality @xcite that is an extension of hoeffding s inequality in a dependent context . for the sake of completeness , we provide here this result when the observations @xmath337 come from a stationary process @xmath20    [ rio ] let @xmath338 be a function @xmath339 such that for all @xmath8 , ... ,",
    "@xmath340 , @xmath107 , ... ,",
    "@xmath341 , @xmath342 then , for any @xmath293 , we have @xmath343   - h(x_{1},\\ldots , x_{n } ) \\right\\}})\\right ) \\leq \\exp\\big({\\frac{t^{2 } n \\left(\\mathcal{b } + \\theta_{\\infty , n}(1)\\right)^{2}}{2}}\\big ) .\\ ] ]    others exponential inequalities can be used to obtain pac - bounds in the context of time series : the inequalities in @xcite for mixing time series , and @xcite under weakest `` weak dependence '' assumptions , @xcite for martingales .",
    "lemma  [ rio ] is very general and yields optimal low rates of convergence .",
    "for fast rates of convergence , we will use samson s inequality that is an extension of bernstein s inequality in a dependent context .",
    "[ lapmx ] let @xmath344 , @xmath345 be a stationary process on @xmath346 and @xmath347 denote its @xmath25-mixing coefficients .",
    "for any measurable function @xmath348 $ ] , any @xmath349 , we have @xmath350 where @xmath351 , @xmath352 and @xmath353 .",
    "_ proof of lemma  [ lapmx ] .",
    "_ this result can be deduced easily from the proof of theorem 3 of @xcite which states a more general result on empirical processes . in page 457 of @xcite , replace the definition of @xmath354 by @xmath355 ( following the notations of @xcite ) . then check that all the arguments of the proof remain valid , the claim of lemma  [ lapmx ] is obtained page 460 , line 7 .",
    "@xmath329 + we also remind the variational formula of the kullback divergence .",
    "[ legendre ] for any @xmath356 , for any measurable upper - bounded function @xmath357 we have : @xmath358 moreover , the supremum with respect to @xmath359 in the right - hand side is reached for the gibbs measure @xmath360 defined by @xmath361 $ ] .",
    "actually , it seems that in the case of discrete probabilities , this result was already known by kullback ( problem 8.28 of chapter 2 in @xcite ) . for a complete proof of this variational formula , even in the non integrable cases ,",
    "we refer the reader to @xcite .",
    "[ [ technical - lemmas - for - the - proofs - of - theorems - thfinite - thmerm - thmgibbs2-and - main_result ] ] technical lemmas for the proofs of theorems [ thfinite ] , [ thmerm ] , [ thmgibbs2 ] and [ main_result ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~      _ proof of lemma  [ xiaoyin ] .",
    "_ let us fix @xmath59 and @xmath15 .",
    "let us define the function @xmath338 by : @xmath363 we now check that @xmath338 satisfies  , remember that @xmath364 so @xmath365 where we used assumption * liploss@xmath78 * for the last inequality .",
    "so we have @xmath366 where we used assumption * lip*@xmath79 .",
    "so we can apply lemma  [ rio ] with @xmath367 , @xmath368 , and @xmath369 : @xmath370}\\right ) \\leq \\exp\\big({\\frac{\\lambda^{2 } k^{2 } ( 1+l)^{2 }      \\left(\\mathcal{b } + \\theta_{\\infty , n}(1)\\right)^{2}}{2 n      \\left(1- { k}/{n}\\right)^{2 } } } \\big)\\\\ \\leq \\exp\\big({\\frac{\\lambda^{2}k^{2 } ( 1+l)^{2 }      \\left(\\mathcal{b } + \\mathcal{c}\\right)^{2}}{2 n      \\left(1-\\frac{k}{n}\\right)^{2 } } } \\big)\\end{gathered}\\ ] ] by assumption * weakdep@xmath83*. this ends the proof of the first inequality .",
    "the reverse inequality is obtained by replacing the function @xmath338 by @xmath371 .",
    "@xmath329 + we are now ready to state the following key lemma .",
    "_ proof of lemma  [ pacbayes ] .",
    "_ let us fix @xmath374 and @xmath59 , and apply the first inequality of lemma  [ xiaoyin ] .",
    "we have : @xmath375 and we multiply this result by @xmath376 and integrate it with respect to @xmath377 .",
    "an application of fubini s theorem yields @xmath378 we apply lemma  [ legendre ] and we get : @xmath379 as @xmath380 , we have : @xmath381 using the same arguments than above but starting with the second inequality of lemma  [ xiaoyin ] : @xmath382 we obtain : @xmath383 \\rho({\\rm d}\\theta )   - \\frac{\\lambda^2 \\kappa^{2}}{n\\left(1-\\frac{k}{n}\\right)^2 } - \\log \\left(\\frac{2}{\\varepsilon}\\right ) - \\mathcal{k}(\\rho,\\pi )   \\right\\ } \\geq 0   \\right\\ } \\leq \\frac{\\varepsilon}{2}.\\ ] ] a union bound ends the proof . @xmath329 + the following variant of lemma  [ pacbayes ] will also be useful .",
    "_ proof of lemma  [ pacbayes2 ] .",
    "following the proof of lemma  [ pacbayes ] we have : @xmath385 now , we use the second inequality of lemma  [ xiaoyin ] , with @xmath386 : @xmath387 but then , we directly apply markov s inequality to get : @xmath388 here again , a union bound ends the proof .",
    "_ proof of theorem  [ main_result ] .",
    "_ we apply lemma  [ pacbayes ] .",
    "so , with probability at least @xmath18 we are on the event given by  .",
    "from now , we work on that event .",
    "the first inequality of  , when applied to @xmath389 , gives @xmath390 according to lemma [ legendre ] we have : @xmath391 so we obtain @xmath392 we now estimate from above @xmath393 by @xmath14 .",
    "applying the second inequality of   and plugging it into inequality  [ eq1 ] gives @xmath394 we end the proof by the remark that @xmath395 is convex and so by jensen s inequality @xmath396 @xmath329    _ proof of theorem  [ thmgibbs2 ] .",
    "_ let us apply lemma  [ pacbayes ] in each model @xmath201 , with a fixed @xmath397 and confidence level @xmath398 .",
    "we obtain , for all @xmath221 , @xmath399 we put @xmath400 , a union bound gives leads to : @xmath401 from now , we only work on that event of probability at least @xmath18 .",
    "remark that @xmath402 @xmath329        _ proof of theorem  [ thfinite ] .",
    "_ we choose @xmath61 as the uniform probability distribution on @xmath5 and @xmath59 .",
    "we apply lemma  [ pacbayes2 ] .",
    "so we have , with probability at least @xmath18 , @xmath403 we restrict the @xmath324 in the first inequality to dirac masses @xmath325 and we obtain : @xmath404 in particular , we apply the first inequality to @xmath405 . we remind that @xmath406 minimizes @xmath407 on @xmath5 and that @xmath405 minimizes @xmath408 on @xmath5 , and so we have @xmath409 the result still holds if we choose @xmath62 as a minimizer of @xmath410 @xmath329    _ proof of theorem  [ thmerm ] . _",
    "we put @xmath411 .",
    "we choose @xmath61 as the uniform probability distribution on @xmath308 .",
    "we apply lemma  [ pacbayes2 ] .",
    "so we have , with probability at least @xmath18 , @xmath403 so for any @xmath359 , @xmath412\\rho({\\rm d}\\theta )           + \\int r { \\rm d } \\rho \\\\   \\leq \\int[r(\\hat{\\theta}^{erm } ) - r(\\theta)]\\rho({\\rm d}\\theta )   + \\int r_n { \\rm d}\\rho         + \\frac{\\lambda \\kappa^2 } { n \\left(1-{k}/{n}\\right)^{2 } }       + \\frac{\\mathcal{k}(\\rho,\\pi ) + \\log\\left({2}/{\\varepsilon}\\right)}{\\lambda } \\\\   \\leq \\int[r(\\hat{\\theta}^{erm } ) - r(\\theta)]\\rho({\\rm d}\\theta )   + \\int [ r_n(\\theta)-r_n(\\hat{\\theta}^{erm } ) ] \\rho({\\rm d}\\theta )          + r_n(\\hat{\\theta}^{erm } )",
    "\\\\        \\shoveright{+ \\frac{\\lambda \\kappa^2 } { n \\left(1-{k}/{n}\\right)^{2 } }       + \\frac{\\mathcal{k}(\\rho,\\pi ) + \\log\\left({2}/{\\varepsilon}\\right)}{\\lambda } } \\\\ \\leq 2 k \\psi \\int \\|\\theta-\\hat{\\theta}^{erm}\\|_{1 } \\rho({\\rm d}\\theta )          + r_n(\\overline{\\theta } )        + \\frac{\\lambda \\kappa^2 } { n \\left(1-{k}/{n}\\right)^{2 } }       + \\frac{\\mathcal{k}(\\rho,\\pi ) + \\log\\left({2}/{\\varepsilon}\\right)}{\\lambda } \\\\",
    "\\leq 2 k \\psi \\int \\|\\theta-\\hat{\\theta}^{erm}\\|_{1 } \\rho({\\rm d}\\theta )          + r(\\overline{\\theta } )        + \\frac{2 \\lambda \\kappa^2 } { n \\left(1-{k}/{n}\\right)^{2 } }       + \\frac{\\mathcal{k}(\\rho,\\pi ) + 2 \\log\\left({2}/{\\varepsilon}\\right)}{\\lambda}.\\end{gathered}\\ ] ] now we define , for any @xmath413 , @xmath414 by @xmath415 so in particular , we have , for any @xmath413 , @xmath416 but for any @xmath417 ,",
    "@xmath418 so we have @xmath419 we optimize this result by taking @xmath420 , which is smaller than @xmath119 as soon as @xmath421 , we get : @xmath422 we just choose @xmath62 as the minimizer of the r.h.s .",
    ", subject to @xmath421 , to end the proof .",
    "we apply lemma [ lapmx ] to @xmath426 , @xmath427 , @xmath428,\\end{gathered}\\ ] ] and so @xmath429,\\ ] ] and the @xmath430 are uniformly mixing with coefficients @xmath431 . note that @xmath432 by * phimix@xmath113*. for any @xmath35 and @xmath433 in @xmath5 let us put @xmath434^{2}\\right\\}.\\ ] ] we are going to apply lemma  [ lapmx ] .",
    "remark that @xmath435 .",
    "also , @xmath436 where we used liploss@xmath78 for the first inequality and lip@xmath79 and phimix@xmath437 for the second inequality .",
    "this implies that @xmath438 , so we can apply lemma  [ lapmx ] for any @xmath439 $ ] , we have @xmath440 \\leq   \\frac{8k\\mathcal{c } v(\\theta,\\overline{\\theta } ) \\lambda^2}{n - k } .\\ ] ] notice finally that margin@xmath80 leads to @xmath441\\ ] ] this proves the first inequality of lemma  [ exprisk ] .",
    "the second inequality is proved exacly in the same way , but replacing @xmath442 by @xmath443 .",
    "_ proof of lemma  [ pacbayesolivier ] .",
    "_ let us fix @xmath447 , @xmath62 and @xmath15 , and apply the first inequality of lemma  [ exprisk ] .",
    "we have : @xmath424\\right\\ }   \\leq 1,\\ ] ] and we multiply this result by @xmath376 and integrate it with respect to @xmath377 .",
    "fubini s theorem gives : @xmath448\\biggr\\ } \\pi({\\rm d}\\theta ) \\\\",
    "\\leq \\frac{\\varepsilon}{2}.\\end{gathered}\\ ] ] we apply lemma  [ legendre ] and we get : @xmath449\\biggr\\ }    \\leq \\frac{\\varepsilon}{2}.\\end{gathered}\\ ] ] as @xmath380 , we have : @xmath450 - \\mathcal{k}(\\rho,\\pi)\\geq 0   \\biggr\\ } \\leq \\frac{\\varepsilon}{2}.\\end{gathered}\\ ] ] let us apply the same arguments starting with the second inequality of lemma  [ exprisk ] .",
    "we obtain : @xmath451 \\geq 0   \\biggr\\ } \\leq \\frac{\\varepsilon}{2}.\\end{gathered}\\ ] ] a union bound ends the proof .",
    "@xmath329      _ proof of theorem  [ thmfastrates ] .",
    "_ fix @xmath452 .",
    "applying lemma  [ pacbayesolivier ] , we assume from now that the event of probability at least @xmath18 given by this lemma is satisfied .",
    "in particular we have @xmath453 , @xmath454 in particular , thanks to lemma  [ legendre ] , we have : @xmath455 now , we apply the second inequality of lemma  [ pacbayesolivier ] : @xmath456       + 2 \\frac{\\mathcal{k}(\\rho,\\pi ) + \\log\\left({2}/{\\varepsilon}\\right)}{\\lambda } } {    \\left(1-\\frac{8k\\mathcal{c } \\lambda } { n - k}\\right ) } \\\\",
    "\\leq \\inf _ { j } \\inf_{\\rho\\in\\mathcal{m}_{+}^{1}(\\theta_j ) } \\frac { \\left(1+\\frac{8k\\mathcal{c } \\lambda } { n - k}\\right ) \\left[\\int r { \\rm d}\\rho        - r(\\overline{\\theta } ) \\right ]       + 2 \\frac{\\mathcal{k}(\\rho_j,\\pi ) + \\log\\left(\\frac{2}{\\varepsilon p_j}\\right)}{\\lambda } } {   \\left(1-\\frac{8k\\mathcal{c } \\lambda } { n - k}\\right ) } \\\\",
    "\\leq \\inf _ { j } \\inf_{\\delta>0 } \\frac {   \\left(1+\\frac{8k\\mathcal{c } \\lambda } { n - k}\\right ) \\left [ r(\\overline{\\theta}_j ) + \\delta        - r(\\overline{\\theta } ) \\right ]       + 2 \\frac{d_j \\log\\left(\\frac{d_j}{\\delta}\\right ) + \\log\\left(\\frac{2}{\\varepsilon p_j}\\right)}{\\lambda } } {    \\left(1-\\frac{8k\\mathcal{c } \\lambda } { n - k}\\right ) } \\end{gathered}\\ ] ] by restricting @xmath359 as in the proof of theorem  [ thmgibbs1 ] .",
    "first , notice that our choice @xmath457 leads to @xmath458       + 2 \\frac{d_j \\log\\left(\\frac{d_j}{\\delta}\\right)+ \\log\\left(\\frac{2}{\\varepsilon p_j}\\right)}{\\lambda } \\right\\ } \\\\ & \\leq 4 \\inf _ { j } \\inf_{\\delta>0 } \\left\\ {   r(\\overline{\\theta}_j ) + \\delta        - r(\\overline{\\theta } )       +   \\frac{d_j \\log\\left(\\frac{d_j}{\\delta}\\right)+ \\log\\left(\\frac{2}{\\varepsilon p_j}\\right)}{\\lambda } \\right\\}.\\end{aligned}\\ ] ] taking @xmath459 leads to @xmath460 finally , we replace the last occurences of @xmath62 by its value : @xmath461 jensen s inequality leads to : @xmath462 @xmath329"
  ],
  "abstract_text": [
    "<S> we establish rates of convergences in time series forecasting using the statistical learning approach based on oracle inequalities . </S>",
    "<S> a series of papers ( e.g. @xcite ) extends the oracle inequalities obtained for iid observations to time series under weak dependence conditions . </S>",
    "<S> given a family of predictors and @xmath0 observations , oracle inequalities state that a predictor forecasts the series as well as the best predictor in the family up to a remainder term @xmath1 . </S>",
    "<S> using the pac - bayesian approach , we establish under weak dependence conditions oracle inequalities with optimal rates of convergence @xmath1 . </S>",
    "<S> we extend results given in @xcite for the absolute loss function to any lipschitz loss function with rates @xmath2 where @xmath3 measures the complexity of the model . </S>",
    "<S> we apply the method for quantile loss functions to forecast the french gdp . under additional conditions on the loss functions ( satisfied by the quadratic loss function ) and on the time series , </S>",
    "<S> we refine the rates of convergence to @xmath4 . </S>",
    "<S> we achieve for the first time these fast rates for uniformly mixing processes . </S>",
    "<S> these rates are known to be optimal in the iid case , see @xcite , and for individual sequences , see @xcite . </S>",
    "<S> in particular , we generalize the results of @xcite on sparse regression estimation to the case of autoregression .    , </S>"
  ]
}