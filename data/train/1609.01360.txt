{
  "article_text": [
    "there has been a strong interest toward obtaining highly efficient deep neural network architectures that maintain strong modeling power for different applications such as self - driving cars and smartphone applications where the available computing resources are practically limited to a combination of low - power , embedded gpus and cpus with limited memory and computing power .",
    "the optimal brain damage method  @xcite was one of the first approaches in this area , where synapses were pruned based on their strengths .",
    "@xcite proposed a network compression framework where vector quantization was leveraged to shrink the storage requirements of deep neural networks .",
    "et al . _",
    "@xcite utilized pruning , quantization and huffman coding to further reduce the storage requirements of deep neural networks .",
    "hashing is another trick utilized by chen _",
    "et al . _",
    "@xcite to compress the network into a smaller amount of storage space .",
    "low rank approximation  @xcite and sparsity learning  @xcite are other strategies used to sparsify deep neural networks . recently ,",
    "shafiee _ et al . _",
    "@xcite tackled this problem in a very different manner by proposing a novel framework for synthesizing highly efficient deep neural networks via the idea of evolutionary synthesis .",
    "differing significantly from past attempts at leveraging evolutionary computing methods such as genetic algorithms for creating neural networks  @xcite , which attempted to create neural networks with high modeling capabilities in a direct but highly computationally expensive manner , the proposed novel _ evolutionary deep intelligence _",
    "approach mimics biological evolution mechanisms such as random mutation , natural selection , and heredity to synthesize successive generations of deep neural networks with progressively more efficient network architectures .",
    "the architectural traits of ancestor deep neural networks are encoded via probabilistic ` dna ' sequences , with new offspring networks possessing diverse network architectures synthesized stochastically based on the ` dna ' from the ancestor networks and computational environmental factor models , thus mimicking random mutation , heredity , and natural selection .",
    "these offspring networks are then trained , much like one would train a newborn , and have more efficient , more diverse network architectures while achieving powerful modeling capabilities .",
    "an important aspect of evolutionary deep intelligence that is particular interesting and worth deeper investigation is the genetic encoding scheme used to mimic heredity , which can have a significant impact on the way architectural traits are passed down from generation to generation and thus impact the quality of descendant deep neural networks .",
    "a more effective genetic encoding scheme can facilitate for better transfer of important genetic information from ancestor networks to allow for the synthesis of even more efficient and powerful deep neural networks in the next generation .",
    "as such , a deeper investigation and exploration into the incorporation of synaptic clustering into the genetic encoding scheme can be potentially fruitful for synthesizing highly efficient deep neural networks that are more geared for improving not only memory and storage requirements , but also be tailored for devices designed for highly parallel computations such as embedded gpus .    in this study",
    ", we introduce a new synaptic cluster - driven genetic encoding scheme for synthesizing highly efficient deep neural networks over successive generations .",
    "this is achieved through the introduction of a multi - factor synapse probability model where the synaptic probability is a product of both the probability of synthesis of a particular cluster of synapses and the probability of synthesis of a particular synapse within the synapse cluster .",
    "this genetic encoding scheme effectively promotes the formation of synaptic clusters over successive generations while also promoting the formation of highly efficient deep neural networks .",
    "the proposed genetic encoding scheme decomposes synaptic probability into a multi - factor probability model , where the architectural traits of a deep neural network are encoded probabilistically as a product of the probability of synthesis of a particular cluster of synapses and the probability of synthesis of a particular synapse within the synapse cluster . *",
    "cluster - driven genetic encoding .",
    "* let the network architecture of a deep neural network be expressed by @xmath1 , with @xmath2 denoting the set of possible neurons and @xmath3 denoting the set of possible synapses in the network .",
    "each neuron @xmath4 is connected via a set of synapses @xmath5 to neuron @xmath6 such that the synaptic connectivity @xmath7 is associated with a @xmath8 denoting its strength .",
    "the architectural traits of a deep neural network in generation @xmath9 can be encoded by a conditional probability given its architecture at the previous generation @xmath10 , denoted by @xmath11 , which can be treated as the probabilistic ` dna ' sequence of a deep neural network .    without loss of generality , based on the assumption that synaptic connectivity characteristics in an ancestor network are desirable traits to be inherited by descendant networks",
    ", one can instead encode the genetic information of a deep neural network by synaptic probability @xmath12 , where @xmath13 encodes the synaptic strength of each synapse @xmath14 . in the proposed genetic encoding scheme ,",
    "we wish to take into consideration and incorporate the neurobiological phenomenon of synaptic clustering  @xcite , where the probability of synaptic co - activation increases for correlated synapses encoding similar information that are close together on the same dendrite .",
    "to explore the idea of promoting the formation of synaptic clusters over successive generations while also promoting the formation of highly efficient deep neural networks , the following multi - factor synaptic probability model is introduced : @xmath15\\end{aligned}\\ ] ] where the first factor ( first conditional probability ) models the probability of the synthesis of a particular cluster of synapses , @xmath16 , while the second factor models the probability of a particular synapse , @xmath17 , within synaptic cluster @xmath18 .",
    "more specifically , the probability @xmath19 represents the likelihood that a particular synaptic cluster , @xmath16 , be synthesized as a part of the network architecture in generation @xmath9 given the synaptic strength in generation @xmath10 . for example , in a deep convolutional neural network , the synaptic cluster @xmath18 can be any subset of synapses such as a kernel or a set of kernels within the deep neural network .",
    "the probability @xmath20 represents the likelihood of existence of synapse @xmath21 within the cluster @xmath18 in generation @xmath9 given its synaptic strength in generation @xmath10 . as such , the proposed synaptic probability model not only promotes the persistence of strong synaptic connectivity in offspring deep neural networks over successive generations , but also promotes the persistence of strong synaptic clusters in offspring deep neural networks over successive generations . *",
    "cluster - driven evolutionary synthesis .",
    "* in the seminal paper on evolutionary deep intelligence by shafiee  _ et al . _",
    "@xcite , the synthesis probability @xmath22 is composed of the synaptic probability @xmath12 , which mimic heredity , and environmental factor model @xmath23 which mimic natural selection by introducing quantitative environmental conditions that offspring networks must adapt to : @xmath24 in this study , is reformulated in a more general way to enable the incorporation of different quantitative environmental factors over both the synthesis of synaptic clusters as well as each synapse : @xmath25 \\vspace{-0.45 cm}\\end{aligned}\\ ] ] where @xmath26 and @xmath27 represents environmental factors enforced at the cluster and synapse levels , respectively .",
    "* realization of cluster - driven genetic encoding . * in this study , a simple realization of the proposed cluster - driven genetic encoding scheme is presented to demonstrate the benefits of the proposed scheme . here , since we wish to promote the persistence of strong synaptic clusters in offspring deep neural networks over successive generations , the probability of the synthesis of a particular cluster of synapses , @xmath16 is modeled as @xmath28 where @xmath29 encodes the truncation of a synaptic weight and @xmath30 is a normalization factor to make   a probability distribution , @xmath31 $ ] .",
    "the truncation of synaptic weights in the model reduces the influence of very weak synapses within a synaptic cluster on the genetic encoding process .",
    "the probability of a particular synapse , @xmath17 , within synaptic cluster @xmath18 , denoted by @xmath32 can be expressed as : @xmath33 where @xmath34 is a layer - wise normalization constant . by incorporating both of the aforementioned probabilities in the proposed scheme ,",
    "the relationships amongst synapses as well as their individual synaptic strengths are taken into consideration in the genetic encoding process .",
    "evolutionary synthesis of deep neural networks across several generations is performed using the proposed genetic encoding scheme , and their network architectures and accuracies are investigated using three benchmark datasets : mnist  @xcite , stl-10  @xcite and cifar10  @xcite .",
    "the lenet-5 architecture  @xcite is selected as the network architecture of the original , first generation ancestor network for mnist and stl-10 , while the alexnet architecture  @xcite is utilized for the ancestor network for cifar10 , with the first layer modified to utilize @xmath35 kernels instead of @xmath36 kernels given the smaller image size in cifar10 .",
    "the environmental factor model being imposed at different generations in this study is designed to form deep neural networks with progressively more efficient network architectures than its ancestor networks while maintaining modeling accuracy .",
    "more specifically , @xmath37 and @xmath38 is formulated in this study such that an offspring deep neural network should not have more than 80% of the total number of synapses in its direct ancestor network .",
    "furthermore , in this study , each kernel in the deep neural network is considered as a synaptic cluster in the synapse probability model . in other words ,",
    "the probability of the synthesis of a particular synaptic cluster ( i.e , @xmath19 ) is modeled as the truncated summation of the weights within a kernel .",
    "* results & discussion*. in this study , offspring deep neural networks were synthesized in successive generations until the accuracy of the offspring network exceeded 3% , so that we can better study the changes in architectural efficiency in the descendant networks over multiple generations .",
    "table  [ tab : mnist - stl10res ] shows the architectural efficiency ( defined in this study as the total number of synapses of the original , first - generation ancestor network divided by that of the current synthesized network ) versus the modeling accuracy at several generations for three datasets .",
    "as observed in table  [ tab : mnist - stl10res ] , the descendant network at the 13th generation for mnist was a staggering @xmath0125-fold more efficient than the original , first - generation ancestor network without exhibiting a significant drop in the test accuracy ( @xmath01.7% drop ) .",
    "this trend was consistent with that observed with the stl-10 results , where the descendant network at the 10th generation was @xmath056-fold more efficient than the original , first - generation ancestor network without a significant drop in test accuracy ( @xmath01.2% drop ) .",
    "it also worth noting that since the training dataset of the stl-10 dataset is relatively small , the descendant networks at generations 2 to 8 actually achieved higher test accuracies when compared to the original , first - generation ancestor network , which illustrates the generalizability of the descendant networks compared to the original ancestor network as the descendant networks had fewer parameters to train .",
    "finally , for the case of cifar10 where a different network architecture was used ( alexnet ) , the descendant network at the 6th generation network was @xmath014.4-fold more efficient than the original ancestor network with @xmath02% drop in test accuracy , thus demonstrating the applicability of the proposed scheme for different network architectures .",
    ".cluster efficiency of the convolutional layers ( layers 1 - 3 ) and fully connected layer ( layer 4 ) at first and the last reported generations of deep neural networks for mnist and stl-10 .",
    "columns ` e ' show overall cluster efficiency for synthesized deep neural networks . [",
    "cols=\"<,^,^,^,^,^,^,<,^,^,^,^,^,^ \" , ]     * embedded gpu ramifications*. table  [ tab : numkernelres ] and  [ tab : cifar ] shows the cluster efficiency per layer of the synthesized deep neural networks in the last generations , where cluster efficiency is defined in this study as the total number of kernels in a layer of the original , first - generation ancestor network divided by that of the current synthesized network .",
    "it can be observed that for mnist , the cluster efficiency of last - generation descendant network is @xmath09.7x , which may result in a near 9.7-fold potential speed - up in running time on embedded gpus by reducing the number of arithmetic operations by @xmath09.7-fold compared to the first - generation ancestor network , though computational overhead in other layers such as relu may lead to a reduction in actual speed - up . the potential speed - up from the last - generation descendant network for stl-10 is lower compared to mnist dataset , with the reported cluster efficiency in last - generation descendant network @xmath06x .",
    "finally , the cluster efficiency for the last generation descendant network for cifar10 is @xmath02.8x , as shown in table  [ tab : cifar ] .",
    "these results demonstrate that not only can the proposed genetic encoding scheme promotes the synthesis of deep neural networks that are highly efficient yet maintains modeling accuracy , but also promotes the formation of highly sparse synaptic clusters that make them highly tailored for devices designed for highly parallel computations such as embedded gpus .      this research has been supported by canada research chairs programs , natural sciences and engineering research council of canada ( nserc ) , and the ministry of research and innovation of ontario .",
    "the authors also thank nvidia for the gpu hardware used in this study through the nvidia hardware grant program .",
    "d.  white and p.  ligomenides , `` gannet : a genetic algorithm for optimizing topology and weights in neural network design , '' in _ international workshop on artificial neural networks_.1em plus 0.5em minus 0.4emspringer , 1993 , pp . 322327 .",
    "o.  welzel , c.  h. tischbirek , j.  jung , e.  m. kohler , a.  svetlitchny , a.  w. henkel , j.  kornhuber , and t.  w. groemer , `` synapse clusters are preferentially formed by synapses with large recycling pool sizes , '' _ plos one _ , vol .  5 , no .  10 , p. e13514 , 2010 .",
    "g.  kastellakis , d.  j. cai , s.  c. mednick , a.  j. silva , and p.  poirazi , `` synaptic clustering within dendrites : an emerging theory of memory formation , '' _ progress in neurobiology _ , vol .",
    "1935 , 2015 ."
  ],
  "abstract_text": [
    "<S> there has been significant recent interest towards achieving highly efficient deep neural network architectures . a promising paradigm for achieving this is the concept of _ evolutionary deep intelligence _ , which attempts to mimic biological evolution processes to synthesize highly - efficient deep neural networks over successive generations . </S>",
    "<S> an important aspect of evolutionary deep intelligence is the genetic encoding scheme used to mimic heredity , which can have a significant impact on the quality of offspring deep neural networks . </S>",
    "<S> motivated by the neurobiological phenomenon of synaptic clustering , we introduce a new genetic encoding scheme where synaptic probability is driven towards the formation of a highly sparse set of synaptic clusters . </S>",
    "<S> experimental results for the task of image classification demonstrated that the synthesized offspring networks using this synaptic cluster - driven genetic encoding scheme can achieve state - of - the - art performance while having network architectures that are not only significantly more efficient ( with a @xmath0125-fold decrease in synapses for mnist ) compared to the original ancestor network , but also tailored for gpu - accelerated machine learning applications . </S>"
  ]
}