{
  "article_text": [
    "recursive bayesian estimation ( or filtering ) is a technique for recursively estimating the state of a random process observed via noisy measurements .",
    "if the underlying dynamical model is linear and gaussian we have the celebrated kalman filter @xcite which is an exact solution to the bayesian filtering problem .",
    "unfortunately , in many practical scenarios of interest , the bayes filter is not exactly computable .",
    "therefore , we seek techniques to approximate this ideal filter . the kalman filter can be applied in more general settings @xcite as an approximation .",
    "particle filtering is a more general approximation method that is easily applied to nonlinear and non - gaussian state - space models .",
    "the particle filter approximates the bayesian filter via monte carlo simulation / sampling . the samples ( or particles )",
    "are propagated through a sequential importance sampling mechanism that attempts to capture the dynamics of the unobservable process and the likelihood of the observations available .",
    "other approximations exist such as gaussian mixture filters etc . @xcite .",
    "the particle filter has been widely studied in theory and in countless practical applications @xcite . in @xcite",
    "the authors prove that the error can be controlled uniformly in time , thus providing a solid mathematical support for application of the filter in numerous fields .",
    "unfortunately , the particle filter computation is strongly dependent on the dimension of the underlying estimation problem .",
    "specifically , the error bound grows exponentially with the system s dimension , making the filter infeasible in most high - dimensional applications .",
    "this problem is known as the _ curse of dimensionality _",
    "a heuristic explanation of this phenomenon for a particular case can be found in @xcite . in @xcite",
    "the authors give a precise relation between the dimension of the system and the number of particle required to avoid weight degeneracy @xcite .",
    "the fact that the approximation error is exponential in the dimension and only inversely controlled by the sample size implies that an incredibly large number of particles are required when dealing with a high dimensional system if we want to control the error at a reasonable level .",
    "obviously , a large number of particles means a heavy computational burden , that is often simply prohibitive .",
    "recent studies @xcite however suggest that high - dimensional particle filtering may be feasible in particular applications and/or if one is willing to accept a degree of systematic bias . in @xcite",
    ", the particle filter is applied in a static setting where the objective is to sample from some high - dimensional target distribution . in this case , through a sequence of intermediate and simpler distributions , it is shown that the particle filter will converge to a sampled representation of the target distribution with a typical monte carlo error ( inverse in the number of particles ) given a complexity on the order of the dimension squared .",
    "although @xcite deals only , in essence , with a static problem of sampling from a fixed target distribution , the analysis introduces a novel way of thinking about high - dimensional particle filtering which may carry over to dynamic filtering problems .",
    "related work appears in @xcite .      in @xcite",
    "the authors consider particle filtering in large - scale dynamic random fields .",
    "they assume the dynamics of the underlying process are localised to a neighbourhood of the field and the observations are local to each site .",
    "they exploit this idea by localising the algorithm during the update phase .",
    "they argue that the difficulty in high dimensional particle filtering is due largely to the dimension of the observation and the nonlinearity of the update operation .",
    "therefore , they partition the field into independent blocks and correct every marginalised block separately .",
    "the posterior is simply the product of the blocked marginals .",
    "the real contribution of @xcite is a descriptive and technical analysis that shows the error introduced due to the localisation procedure can be readily controlled if the dynamics of the random field at each site are only locally dependent on those sites within close proximity .",
    "the standard sampling approximation error is shown to be exponential in only the size of the individual blocks .",
    "the number of samples / particles controls the sampling approximation error at the typical rate while the error due to the localisation process is a systematic bias that can only be controlled through an increase in the block size .",
    "since each block is updated independently , parallel implementation is readily applicable and the computational burden may be alleviated , albeit this remains to be seen in practice . while the results of @xcite are at the proof - of - concept stage , the idea is incredibly powerful .",
    "the authors in @xcite show that although the total approximation error can be controlled uniformly in time , it suffers from a spatial inhomogeneity .",
    "specifically , the nodes close to the block boundaries display a larger error than those far removed from the boundaries ( as one might expect )",
    ". a simple approach to average this spatial inhomogeneity is given in @xcite where adaptive partitioning of the field is employed .      in this paper",
    "we consider again the idea proposed in @xcite and propose a modified particle filtering algorithm that displays an additional degree of freedom .",
    "the idea proposed herein is to enlarge the blocks during the update phase , allowing for more observations to be employed during the correction at each block .",
    "the main contribution is the addition of a new parameter that captures how much we enlarge each block prior to the update .",
    "obviously , by enlarging each block prior to updating we reduce the bias error but we increase the complexity involved in updating each ( enlarged ) block . by designing an appropriate tradeoff between the various tuning parameters it is possible to reduce the total error bound via allowing a temporary enlargement of the update operator without increasing the overall computational burden .",
    "we borrow the problem setup and notation directly from @xcite .    consider a markov chain @xmath0 defined on a polish state space @xmath1 with transition density @xmath2 with respect to a reference measure @xmath3",
    ". moreover consider a process @xmath4 , defined on a polish space @xmath5 , conditionally independent given @xmath0 , with a transition density @xmath6 with respect to a measure @xmath7 .",
    "the process @xmath0 is observed via the process @xmath4 .",
    "our aim is to estimate the probability of the state @xmath8 given the measurements up to that time and the initial condition @xmath9 .",
    "therefore we introduce the filter @xmath10\\ ] ] it can be easily seen , using bayes rule , that the filter can be written in a recursive way @xmath11 where the operator @xmath12 is defined as follows @xmath13 moreover , the above operator is typically split into two sub - steps @xmath14 where @xmath15 is a prediction step , and @xmath16 is a correction ( or update ) step . in the prediction step ,",
    "the measure is transformed according to the density @xmath17 , while in the update step we use the new information @xmath18 to correct the predicted measure .",
    "we then write the recursion as follows @xmath19 the classic bootstrap particle filter uses @xmath20 particles ( or samples ) to approximate the measure @xmath21 . given a sampled approximation of @xmath22 ,",
    "the particles are first moved according to the transition @xmath17 in order to approximate a sampled representation of the prediction .",
    "the update then computes a weighted posterior empirical measure via @xmath23 .",
    "eventually , a resample step is added in order to avoid weight degeneracy @xcite .",
    "more formally , denoting the bootstrap filter by @xmath24 , we have @xmath25 where @xmath26 and @xmath27 represents the sampling operator here defined @xmath28 it is possible to prove that @xmath29\\leq { a_0}/{\\sqrt{n}}\\ ] ] with @xmath30 independent of time .",
    "unfortunately , the constant @xmath31 typically depends ( exponentially ) on the dimension of the underlying problem .",
    "intuition for this exponential dependence is given in @xcite .",
    "we now consider the pair @xmath32 as a random field @xmath33 indexed on a finite undirected graph @xmath34 .",
    "the vertex set @xmath35 will represents the collection of sites and the edge set @xmath36 the spatial relationships between them .",
    "the cardinality of @xmath35 captures , in some sense , the dimension of interest .",
    "more formally , the spaces @xmath1 and @xmath5 are defined as products @xmath37 , @xmath38 . the reference measures are products @xmath39 , where @xmath40 and @xmath41 are reference measures on @xmath42 and @xmath43 respectively .",
    "the transition densities are defined as @xmath44 where @xmath45 and @xmath46 are densities with respect to the reference measures @xmath40 and @xmath41 . from the definition",
    "we can see that the observations @xmath18 are assumed to be completely local , in the sense that @xmath47 depends uniquely on the value assumed by @xmath48 .",
    "the process @xmath0 is local in the sense that the state at a site @xmath49 depends only on the state at nearby sites .",
    "we state this formally .",
    "consider the graph @xmath50 equipped with the distance @xmath51 defined by the number of hops along the shortest path connecting @xmath49 and @xmath52 .",
    "we can define the neighbourhood of a site @xmath49 as @xmath53 where @xmath54 represents the range of interaction .",
    "then we assume @xmath55 where we write for @xmath56 , @xmath57 .",
    "in other words , the random field @xmath0 is local in the sense that given @xmath58 the present state @xmath59 depends only on @xmath60 .",
    "in @xcite the authors propose an application of the blocked filter algorithm to the field model just explained , exploiting the local dynamic dependencies .",
    "we briefly illustrate this algorithm .",
    "consider a partition @xmath61 of @xmath35 into non - overlapping blocks with a union equal to @xmath35 .",
    "the idea is to create independence across blocks on @xmath35 by marginalising after the prediction step .",
    "we then update each block separately and finally we form @xmath62 via the product of the independent ( updated ) blocked marginals .",
    "more formally , consider the block operator @xmath63 on the space @xmath64 of measures on @xmath1 , defined by @xmath65 where @xmath66 is the marginal of the measure @xmath67 on the subset @xmath68 .",
    "then the proposed block filter can be written as a recursion @xmath69 , @xmath70 where the operator @xmath71 consists of four steps @xmath72    we make the following definition .    given @xmath73 and a subset @xmath56",
    "we define a distance of the marginals on @xmath74 as follows @xmath75^{\\frac{1}{2}}\\ ] ] where the expectation is taken with respect to the random sampling and @xmath76 is the class of measurable function on @xmath1 that depends only on the values on @xmath74 , that is @xmath77 when @xmath78 .",
    "if @xmath79 we omit the subscript and write @xmath80 .",
    "with no expectation it follows that @xmath81 is equivalent to the total variation which we write as @xmath82 .",
    "the two norms are interchangeable when no sampling occurs .",
    "now , given a set @xmath56 we define the boundary and the interior @xmath83 and given a partition @xmath84 , we define the following quantities @xmath85 where the first quantity is independent of the partition .",
    "the result proven in @xcite is the following .",
    "[ rebeschini main ] there exists a constant @xmath86 , depending only on the quantities @xmath87 such that if there exists @xmath88 and @xmath89 such that @xmath90 then for every @xmath91 , @xmath92 , @xmath93 and @xmath94 we have @xmath95\\ ] ] where the constants @xmath96 are positive , finite and dependent only on @xmath97 and @xmath54 .",
    "the intuition is that the algorithm approximation error is exponential in @xmath98 rather then in @xmath99 but that the error at some individual locations increases with the proximity of those locations to the border of the blocks .",
    "this leads to a spatial inhomogeneity as seen in the first term of the bound .",
    "a first attempt to achieve a spatially homogeneous error bound can be found in @xcite .",
    "the idea is to consider a finite number @xmath100 of partitions @xmath101 and to apply them cyclically",
    ". clearly we have to choose the partitions is such a way there is no node that is consistently close to a border .",
    "this condition is expressed by a bound on the average , or exponential average , of the border distance .",
    "write @xmath103    clearly @xmath104 and @xmath105 represent how well balanced the collection of partitions are .",
    "define @xmath106 and @xmath107 .",
    "[ bertoli main ] there exists a constant @xmath86 , depending only on the quantities @xmath108 , @xmath109 such that if there exists @xmath88 and @xmath89 such that @xmath90 then for every @xmath91 , @xmath92 and @xmath110 we have @xmath111 where @xmath112 depend only on @xmath113 , @xmath114 , @xmath54 , @xmath108 and @xmath115 in this case .    if @xmath116 where @xmath117 for all @xmath118 , then the bound is completely spatially invariant .",
    "see @xcite for further discussion on this method .",
    "suppose now we are given a partition @xmath84 over @xmath35 but it turns out we are interested only in estimating the marginal of @xmath62 on a particular block @xmath119 .",
    "we could first redefine the partition with a larger block encompassing @xmath120 and a bunch of single site blocks ( to speed up the overall computation ) .",
    "it is of course not possible to define a partition in this manner for multiple blocks of interest .",
    "however , the idea proposed here is based on extending the state space by creating multiple independent copies of the measurements ( and states ) that are then used in different ( and independent ) enlarged blocks .",
    "we introduce some new notation .",
    "consider a parameter @xmath121 , that we will consider fixed throughout the rest of the paper . then define , for any @xmath119 ,",
    "an enlarged block @xmath122 now define the enlarged spaces @xmath123 consider the collection @xmath124 .",
    "this is no longer a partition of @xmath35 .",
    "however , @xmath125 is a partition on @xmath126 , and here we can apply the blocking and updating operators associated with @xmath125 .",
    "we use the superscript @xmath36 to note enlarged objects .",
    "the measures @xmath127 and @xmath128 are defined straightforwardly .",
    "the block operator becomes @xmath129 to update , we need the same operator @xmath130 redefined on the new space @xmath131 , @xmath132 we also define @xmath133 now we can write the enlarged blocked filter algorithm as a recursion @xmath134 where @xmath135 .",
    "now we have five steps . skipping the prediction / sample steps",
    ", graphically we have @xmath136    to write out the explicit expression of the filter we note that @xmath137 where @xmath138 .",
    "therefore , splitting a variable @xmath139 in @xmath140 with @xmath91 and @xmath141 ( where now we put @xmath36 as subscript just for notational simplicity ) and an enlarged block @xmath142 where @xmath143 , we can write @xmath144 }      { \\int\\prod_{k'\\in\\mathcal{k}}\\left[\\prod_{w\\in \\overline{k'}}~p^w(x_0,x^w)~g^w(x^w , y_s^w)~\\nu(dx_0)\\psi^{\\overline{k'}}(dz^{\\overline{k'}}\\right]}\\ ] ] @xmath145 }      { \\int\\prod_{k'\\in\\mathcal{k } } \\left[\\prod_{w\\in k'}p^w(x_0,x^w)~g^w(x^w , y_s^w)\\prod_{w\\in k'^e}p^w(x_0,z_e^w)~g^w(z_e^w , y_s^w)~\\nu(dx_0)\\psi^{k'}(dx^{k'})\\psi^{k'^e}(dz^{k'^e}_e)\\right]}\\ ] ] where for @xmath146 we write @xmath147 .",
    "define an ideal enlarged blocked filter @xmath148 where @xmath149 . fix @xmath56 .",
    "we then use the triangle inequality to decompose the error according to @xmath150 where we refer to the first and second decomposed terms as the bias and variance respectively .",
    "the bias represents the error introduced solely as a result of the blocking operation . in the standard bootstrap filter",
    ", this bias term vanishes and the typical analysis considers only the variance term .",
    "going forward , we consider bounding both the bias and the variance .",
    "we stress however , that the bias is fundamentally more interesting as it pertains directly to the localisation idea considered herein .",
    "indeed , the sampling operation that leads to the variance term could be replaced with other approximation techniques with no loss of generality ( albeit a different approximation error than detailed subsequently ) .    for sake of completeness / clarity we firstly state a result that includes both a bias and a variance bound .",
    "[ main result ] suppose there exists a constant @xmath86 , depending only on @xmath108 and @xmath109 and assume @xmath151 then for every time @xmath92 , @xmath91 , @xmath93 and @xmath94 we have @xmath152\\ ] ] where the constants @xmath153 depend only on @xmath113 , @xmath114 , @xmath54 , @xmath108 , @xmath109 , @xmath154 .",
    "this single ( total error ) bound is derived in practice as two separate bounds which we now explicitly state .",
    "[ bias theorem ] assume there exists @xmath155 such that @xmath156 and such that @xmath157 let @xmath158 .",
    "then for every @xmath92 we have @xmath159 for every @xmath160 , @xmath119 and @xmath94 .    the only difference between this bias bound and the bias bound in @xcite is the presence of @xmath161 in place of @xmath162 . for a given partition @xmath84 any enlargement of the blocks in @xmath84 yielding @xmath125 results in a tighter bias bound as expected .",
    "[ variance theorem ] assume there exists @xmath163 such that @xmath164 and such that @xmath165 let @xmath166 where @xmath167 .",
    "then for every @xmath92 we have @xmath168 for every @xmath160 , @xmath119 and @xmath94 .",
    "again , the only significant difference between this variance bound and the variance bound in @xcite is the presence of @xmath161 in place of @xmath162 .",
    "the variance depends inversely on the number of samples and exponentially in the size of the enlarged blocks .",
    "roughly , we now explain how one may implement the enlarged blocked filter to reduce the bias as compared with the algorithm proposed in @xcite while maintaining a comparable variance and computational complexity .",
    "suppose firstly that one has a random field over @xmath99 sites and the computational power available ( defining a bound on @xmath20 ) ensures that blocks of size @xmath169 can be readily handled for some @xmath170 .",
    "then the complexity of the blocked particle filter proposed in @xcite can , in a sense , be regarded as being of order @xmath171 .",
    "really , one can imagine @xmath172 particle filters running in parallel over each block and each with complexity on the order of @xmath173 .    to exploit the enlarged blocked particle filter",
    ", one should start with a larger number @xmath174 of smaller blocks which when enlarged are mostly of the size @xmath169 . then , the complexity of the enlarged blocked particle filter proposed herein is on the order @xmath175 .",
    "one immediately sees that the variance of the enlarged blocked particle filter is mostly on the same order as that of the algorithm proposed in @xcite and the computational complexity has only increased linearly .",
    "however , in almost all cases ( and certainly with well - designed partitions ) one will achieve a reduction in the bias at any given site in the random field .",
    "we consider a special but interesting case in which a spatial homogeneous total error bound is obtained , the bias bound is better ( tighter ) than in @xcite , and the computational requirements largely unchanged when compared with the algorithm in @xcite .",
    "assume the same hypothesis of theorem [ bias theorem ] .",
    "consider the partition @xmath176 and suppose @xmath177 .",
    "then for every @xmath178 , @xmath160 , and @xmath110 we have @xmath179    this bound is spatially homogeneous and with @xmath177 it is strictly less than the bias bound introduced in @xcite .",
    "note that while the bias bound here is spatially homogeneous , the actual bias may still be inhomogeneous since this result is potentially based on over bounding . on the other hand ,",
    "it is possible to apply the adaptive scheme proposed in @xcite with the enlarged blocked filter and potentially achieve true spatial homogeneity .",
    "the idea of the enlarged blocked particle filter is essentially based on the principle that larger blocks lead to a reduction in the bias introduced due to blocking .",
    "so , why not just start with larger blocks ?    * well , irrespective of the size of the blocks , if one applies the standard blocked particle filter of @xcite then there will always exist sites on the border of a block . *",
    "if we extend ( or enlarge ) the blocks as proposed herein , we ( typically ) reduce the bias at each site ( and particularly those sites that were on the border of a block in the original partition ) . *",
    "if we increase the number of samples @xmath20 with a fixed number of larger blocks ( in the original partition ) then while we can reduce the variance we have no effect on the bias for those sites on the border .",
    "* if we start with small blocks in the original partition and then simultaneously enlarge the blocks along with the number of samples @xmath20 then it may be possible maintain a given variance ( or even reduce the variance ) as compared to a partition with larger original block sizes but with a guaranteed smaller bias at each site .",
    "the high - level point is that it is computationally more desirable to run a few extra parallel implementations of the particle filter ( corresponding to more ( enlarged ) blocks ) and obtain a tighter bias bound than it is to run a few less parallel implementations of the particle filter for the same variance bound but a larger bias bound .",
    "this is only possible through enlargement of the blocks as described herein .",
    "finally , we comment on the matter of consistency ( as defined in say @xcite ) and observational double counting . consider the partition @xmath176 and suppose @xmath180 for each @xmath181 .",
    "practically , following the standard prediction step , the enlarged blocked filter is of the form @xmath182 which is mathematically equivalent to @xmath183 .",
    "the point of this illustration is to highlight that even in this case , involving the most extreme enlargement possible , we are not double counting information or effectively applying measurements twice , and the enlarged blocked particle filter is consistent as per @xcite .      in this section we provide a summary of the proof strategy . clearly the main result in theorem [ main result ] is immediately implied by theorems [ bias theorem ] and [ variance theorem ] .",
    "much of the technical analysis required in the proof of theorem [ main result ] is similar to that originally detailed in @xcite .    in the case of the bias @xmath184 ,",
    "one first derives a local stability property for the filter @xmath185 which implies that the marginal over a local set @xmath146 of the initial state @xmath9 is forgotten exponentially fast .",
    "such a property also implies that any approximation errors in , say , the initial state are also forgotten .",
    "it then follows that if one can bound the one - step approximation error @xmath186 at any time , then in conjunction with the local stability property one will obtain a time - uniform bound on the bias over a local region of the field .    in the case of the variance @xmath187 , a similar idea",
    "is used except one first establishes stability for the ideal enlarged blocked filter @xmath188 .",
    "then , one must bound the one - step approximation error @xmath189 at any time . putting the stability property and",
    "the bound on the one - step approximation together , one achieves the desired time - uniform bound on the variance of a block in the adaptively blocked filter .",
    "we have obviously glossed over much of the intricacies involved in the proof in this summary .",
    "for example , in the case of the bias , the property introduced in @xcite and referred to as the decay of correlations must be established to hold uniformly in time for the ideal block filter @xmath188 .",
    "this property captures a notion of spatial stability where the state at some site in the random field is forgotten as one moves away from that site .",
    "rebeschini et al .",
    "provide a novel measure of this decay that allows them to establish local stability of the filter @xmath185 and to establish a bound on the one - step approximation error @xmath186 .",
    "conceptually , a property like the decay of correlations is necessary to establish such results .",
    "summarising , the steps needed to prove the bias bound are    1 .",
    "proving a ( local ) stability result for the ideal bayesian filter ; 2 .   proving that a desired decay of correlation property holds uniformly in time for the measure @xmath190",
    "controlling the one time - step error introduced by the new enlarged blocked filter ; 4 .   putting all these results together and finalising theorem [ bias theorem ] .",
    "the variance analysis follows much the same path with the prime difficulty being establishment of local stability for the ideal enlarged blocked filter . summarising the steps involved in proving the variance bound ,    1 .",
    "proving a local stability result for the ideal enlarged blocked filter ; 2 .   controlling the one time - step error due to the sampling in the enlarged blocked particle filter",
    "putting these results together and finalising theorem [ variance theorem ]    the proof details are omitted in this version of the work due to their similarity with those details presented in @xcite , but are available upon request .",
    "we have presented a modified version of the blocked particle filter originally proposed in @xcite .",
    "the main feature of our algorithm is that we add a new parameter that can be tuned to decrease the bias as compared to @xcite .",
    "the high - level argument for this approach is that it is computationally more desirable to run a few extra parallel implementations of the particle filter ( corresponding to more ( enlarged ) blocks ) and obtain a tighter bias bound than it is to run a few less parallel implementations of the particle filter for the same variance bound but a larger bias bound .",
    "this gain in bias reduction , with the same variance , and only a linear increase in the computational complexity , is only possible through enlargement of the blocks as described herein .",
    "finally , we also point out that the same adaptive approach to changing partitions proposed in @xcite could be applied in the case of the enlarged blocked filter and this is an additional method for spatial smoothing and may be of interest in those cases in which the underlying model is time - varying ."
  ],
  "abstract_text": [
    "<S> particle filtering is a powerful approximation method that applies to state estimation in nonlinear and non - gaussian dynamical state - space models . </S>",
    "<S> unfortunately , the approximation error depends exponentially on the system dimension . </S>",
    "<S> this means that an incredibly large number of particles may be needed to appropriately control the error in very large scale filtering problems . </S>",
    "<S> the computational burden required is often prohibitive in practice . </S>",
    "<S> rebeschini and van handel ( 2013 ) analyse a new approach for particle filtering in large - scale dynamic random fields . through a suitable localisation operation </S>",
    "<S> they reduce the dependence of the error to the size of local sets , each of which may be considerably smaller than the dimension of the original system . </S>",
    "<S> the drawback is that this localisation operation introduces a bias . in this work , </S>",
    "<S> we propose a modified version of rebeschini and van handel s blocked particle filter . </S>",
    "<S> we introduce a new degree of freedom allowing us to reduce the bias . </S>",
    "<S> we do this by enlarging the space during the update phase and thus reducing the amount of dependent information thrown away due to localisation . by designing an appropriate tradeoff between the various tuning parameters it is possible to reduce the total error bound via allowing a temporary enlargement of the update operator without really increasing the overall computational burden . </S>"
  ]
}