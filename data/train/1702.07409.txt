{
  "article_text": [
    "erasure coding is used for fault tolerance and providing required reliability and availability of data in distributed data storage systems @xcite . as the modern data storage systems evolved to have different requirements , the set of constraints on",
    "the design of erasure codes has changed .",
    "for instance , previous research on erasure codes mostly focused on optimizing the coding overhead i.e. , minimization of storage space consumption for the given target data reliability @xcite , @xcite .",
    "moreover , some of these designs considered pure xor operation to both provide required reliability and efficient computation @xcite .",
    "more recently , locally repairable codes have attracted attention due to their efficient utilization of network resources and eventually achieve better data reliability @xcite . besides the proprietary implementations of erasure codes , many open source implementations based on different mathematical operations are available online @xcite .",
    "main objective of these studies is to provide overhead optimal and fast erasure coding libraries without any consideration of the available network and computational resources and repair efficiency in a distributed setting .",
    "founsure 1.0 utilizes three - dimensional bipartite graph to construct multi - functional erasure code .",
    "the design space includes computation complexity , coding overhead and repair bandwidth as different dimensions of optimization .",
    "if one prefers to have an overhead optimal design with good performance , then it might be sufficient to use the most recent jerasure 2.0 @xcite erasure coding library by dr .",
    "plank and dr .",
    "greenan , now fully supported by _ redhat _ _ ceph _ community @xcite . on the other hand ,",
    "the main objective of founsure 1.0 is to provide different points in three dimensional design space based on the requirements of the storage applications through a set of parameter configurations .",
    "founsure 1.0 library is successfully adapted and used together with a deduplication engine more recently and resulted in a conference paper publication @xcite .",
    "the encoding process of founsure 1.0 begins with a two - dimensional simple bipartite graph which leads to non - systematic fountain - like code @xcite .",
    "the degree distribution of this code is specially selected to meet a good trade - off point between computation complexity , coding overhead and repair bandwidth .",
    "version 1.0 supports luby s robust soliton distribution ( rsd ) @xcite as well as shokrollahi s finite max - degree distribution @xcite by default .",
    "however , we note that these distributions are optimized for minimum coding overhead only . on top of this bipartite graph , encoding engine generates check nodes for `` data - only '' ( refered as check # 1 ) , `` data & coding '' ( refered as check # 2 ) and `` coding - only '' ( refered as check # 3 ) chunks / symbols .",
    "these check nodes can be represented in three dimensional space and shall be used to provide advanced decoding , repair and update features of founsure . throughout the document , we use nodes , chunks and symbols interchangeably .",
    "founsure 1.0 uses belief propagation ( bp ) @xcite ( a.k.a .",
    "message passing ) algorithm to resolve or decode the user data , to repair the encoded data or update the encoded data . in version 1.0",
    ", the user data does not appear at the output in pure / plain format .",
    "in otherwords , one can not read off data from the encoder output without any further processing . therefore , using founsure 1.0 encoding , user data can be considered encrypted automatically .",
    "we use pseudo - random number generators and seeds to generate graph connections of the underlying bipartite graph .",
    "so without the key / s ( referred as seed / s in our implementation ) , there is no way to recover original user data .",
    "therefore , founsure 1.0 software package also provides a user configurable light weight built - in encryption feature .",
    "this paper will briefly describe the set of functionalities provided with founsure 1.0 and the details of the software architecture .",
    "the source code , as well as a comprehensive user guide , few test results and related documentation is available from _ http://www.suaybarslan / founsure.html_.",
    "founsure 1.0 has the following three executable main functions that achieve four important functionalities .    * * founsureenc * : encoder engine that generates @xmath0 number of data chunks under a local _ coding _ directory and a metadata file that include information about the file as well as the coding parameters . *",
    "* founsuredec * : decoder engine that requires a local _ coding _ directory , a valid file name and an associated metadata to run multiple bp passes . *",
    "* founsurerep * : repair engine that * * fixes / repairs one or more coding chunks if they are erased , corrupted or flagged as unavailable . *",
    "* generates extra coding chunks if update is requested .",
    "update is a useful functionality particularly if data reliability is decreased / degraded over time .",
    "there are also utility functions of founsure 1.0 which are provided to help users to make correct design choices on reliability , complexity and storage space efficiency .",
    "we also use utility functions to trigger update functionality as will be demonstrated later .",
    "utility functions do not directly process user data .",
    "version 1.0 currently supports two utility functions as listed below .    * * simdisk * : this function can be used to exhaust all the possible combinations of disk failures for a given set of coding parameters . in other words , this function checks whether the provided coding parameters are sufficient to achieve a user defined reliability goal .",
    "therefore , running this function can help us design target - policy erasure codes .",
    "* * genchecks * : this utility function is crucial for two different functionalities : ( 1 ) fast / efficient repair and ( 2 ) smooth update . in the repair process , it generates two types of checks : check # 2 and check # 3 and writes them to a < testfile>_check.data file in a format described within this document . in case of update , it modifies the metadata file as well as < testfile>_check.data file so that the coding chunks can be updated by running * founsurerep * function .",
    "next , we provide the details of founsure 1.0 encoding and decoding operations , particularly the implementation details of * founsureenc * and * founsuredec * functions . for theoretical details , we refer the reader to the reference document @xcite .      in graph terminology , nodes ( sometimes referred to as equations ) are represented as graph vertices and node relationships are represented by edges of the graph",
    ". there are three types of nodes in a 3-d bipartite graph ; data nodes , coding nodes and check nodes .",
    "the coding nodes represent a set of linear combinations of data nodes generated through a predetermined mathematical function such as exclusive or ( xor ) logic operation .",
    "the final node type is known as check nodes .",
    "these nodes represent all the local sets of data and coding nodes for which a certain mathematical relationship is satisfied such as even / odd _ parity_. a simple mathematical function that is used by founsure 1.0 is xor operation that utilizes multiple data blocks and generate a single block of information .",
    "we use @xmath1 flag to indicate the file name , @xmath2 to indicate the total number data nodes where @xmath3 of these are the original user data nodes , @xmath4 to indicate the total number of coding nodes and @xmath5 to indicate the number of bytes to store per node .    in * founsureenc *",
    "function , data file with _ filesize _ bytes is partitioned into multiple @xmath6 bytes and each partition is encoded independently as shown in figure [ encoder_struc ] . as of version 1.0 , _ partition coupling _ is not supported between distinct partitions .",
    "this technique is currently under investigation and might have interesting performance improvements to our design / implementation in analogy to spatially - coupled ldpc codes @xcite .",
    "if _ filesize _ is not a multiple of @xmath6 bytes , then we use zero padding to make _ filesize _ a multiple .",
    "* founsureenc * also checks whether @xmath7 .",
    "if not , the least biggest @xmath4 is selected automatically to satisfy @xmath7 .",
    "bytes of buffer and generates coding chunks and write it to shaded area on memory before the they are written to distinct drives .",
    "the file on disk is striped and processed in a looped subprocess as shown .",
    "the number of stripes is stored in @xmath8 variable and written to file metadata file.,scaledwidth=80.0% ]    encoding proceeds as follows .",
    "first , a memory space of worth @xmath9 bytes is allocated .",
    "next , check # 1 equations are generated by the efficient array ldpc encoding @xcite , @xcite .",
    "an extra @xmath10 chunks are created to make up a total of @xmath2 chunks of data .",
    "this process is shown as in figure [ encoder_struc ] .",
    "later , a total of @xmath4 coding chunks are generated from the whole set of @xmath2 data chunks based on a fountain - like code with `` finitedist '' degree and pseudo random selection distributions .",
    "this process is shown as in figure [ encoder_struc ] .",
    "finally , @xmath4 coding chunks are distributed ( striped ) equally across distinct output files for allocation on _ s _ number of drives .",
    "we repeat this process for each data partition in a looped subprocess and append coding chunks at the end of the corresponding output files . for",
    "< filename>.ext _ file , we use _",
    "< filename>_disk0 .. 0i.ext _ to refer to the @xmath11th output file .",
    "the number of zeros that appear in the name of output files is controlled by the `` parameter.h '' variable disk_indx_strng_len .    in our implementation , we have distinct object definitions for encoding , decoding , and repair operations .",
    "these objects have extension `` * obj '' and include all the common parameters in their object fields .",
    "for instance , both encoding and/or decoding functions accept _ encoderobj _ and/or _ decoderobj _ constructs as inputs .",
    "similarly , @xmath3 and @xmath2 variables can be accessed using standard way _ encoderobj.sizesb _ and _",
    "encoderobj.sizek_.    each encoding / decoding object is associated with an initial seed value ( _ encoderobj.seed _ , default value is 1389488782 ) from which other seed values as well as the local sets of data chunks are pseudo - randomly created . each coding chunk within _ encoderobj _ and _ decoderobj _ has their own unique i d . these ids are used to identify the coding chunks which might be erased .",
    "the seed value is used by the pseudorandom generator to produce a sequence of integers which form the basis of coding chunk degree number assignment as well as the selected data chunks for coding chunk computations .",
    "these numbers are stored as part of the object and can be regenerated using the same initial seed .",
    "let us assume we have _ s _ number of output files , then we use _ encoderobj.seed _ + @xmath11 as the seed of the @xmath12 output file with @xmath13 .",
    "bytes each and a precode is applied to find check # 1 equations .",
    "we add @xmath10 extra symbols to satisfy check equations as shown .",
    "founsure encoding engine generates @xmath4 coding chunks from @xmath2 chunks as shown.,scaledwidth=80.0% ]    coding symbols choose degrees first ( from an appropriate degree distribution @xmath14 ) and that degree is used to select the number of data symbol neighbors to be involved in computation . the degree distribution @xmath14 is typically selected to minimize the coding overhead .",
    "for instance the following degree distribution is proposed for raptor codes @xcite @xmath15    however , founsure does not necessarily minimize overhead .",
    "it may optimize overhead , repair bandwidth and complexity at the same time .",
    "we recommend to choose degree distributions that will give us a good trade - off point between these three objectives .",
    "although there is no optimal point for all applications , founsure is designed to be highly configurable to fit in different requirements of modern storage systems .",
    "we run * founsuredec * , when we want to collect a subset of output data files and recover the original data file .",
    "decoder is based on bp algorithm a summary of which is provided in * algorithm 1*. bp function admits * decoderobj * and the generator matrix of the code @xmath16 .",
    "the decoder utilizes the information contained in metadata file to generate ( prepare ) the contents of _ decoderobj_. it works in a similar fashion to * founsureenc * i.e. , it reads the striped coding chunks , runs bp algorithm at most twice ( once for the outer graph code and if need be , additional one for the inner array ldpc precode ) and recovers the @xmath17 bytes at each turn .",
    "finally , these bytes are written to decoded / recovered data file by calling standard kernel i / o commands .    having all computation based on pseudorandomly selected chunks and carrying out these computations solely in terms of simple xor logic has the cost of making the code non - optimal in terms of overhead ( tough it might be near - optimal ) .",
    "if @xmath4 coding symbols are distributed over @xmath0 drives and when one of the drives fail , a subset of coding symbols are lost . in order to find what fraction of @xmath1-failure combinations can be tolerated ,",
    "we provide a utility function *",
    "we have three types of check nodes as will be discussed next .",
    "* checks # 1 * : these check equations are defined by the precode of the founsure ( for 1.0 , we selected an array lpdc code family @xcite ) .",
    "based on the selection of good precodes , their mathematical and coding parameter selections etc . , the graph connections are automatically determined .",
    "founsure 1.0 includes a precode support based on a binary array ldpc code and future releases will include external precode support which can be provided by the user using a preformatted input file",
    ". please see precoding section to find more information about the construction of these check equations .",
    "* checks # 2 * : this type of check equations are generated by * algorithm 3*. one of the special features of these checks is that only one neighbor is selected from the data nodes and the rest of the check terms are from the coding nodes .",
    "this special feature can be used to partially decode the data without running the complete decoder and unnecessarily reconstruct the unrequested parts of the data .",
    "an application of this could be multimedia in which the region of interest ( roi ) can be directly reconstructed using this type of check equations .",
    "* checks # 3 * : these check equations are generated by * algorithm 3*. these checks generate the local groups based on the coding nodes .",
    "these checks are primarily used to repair the erased , unavailable or unresponsive coding nodes in case of failures .",
    "a @xmath18 founsure code takes @xmath3 data chunks ( @xmath17 bytes ) and initially generates @xmath10 check # 1 parity chunks based on the binary array ldpc codes @xcite .",
    "this special choice of array ldpc codes enable efficient encoding operation ( linear with blocklength ) and improves the complexity performance of the overall founsure code .",
    "@xmath19 @xmath20 largest_prime_factor(@xmath2 ) @xmath21",
    "@xmath22 @xmath23 * return *",
    "@xmath24    the procedure outlined above uses a generic function largest_prime_factor ( . ) which chooses the largest prime factor of the argument .",
    "the rate of the array ldpc is defined as @xmath25 .",
    "the user can choose any @xmath2 , @xmath4 and @xmath26 and hence we can calculate @xmath27 .",
    "let @xmath28 largest_prime_factor(@xmath2 ) , we may not be able to find @xmath29 to be an integer .",
    "we can definitely use floor function to get an estimate of @xmath30 .",
    "however , the array ldpc code performance is heavily dependent on @xmath31 and @xmath30 values and there is no array ldpc code for all @xmath32 pairs .",
    "if ( @xmath33 ) pair are small , the code performance is observed to be pretty bad . for this reason , we provide an algorithm that reasonably chooses a good performing ldpc code and satisfies ( within some error margin ) the user provided parameters @xmath34 and @xmath26 at the same time .",
    "let us define the following system parameters before we formally give the algorithm that determines the closest good performing ldpc code for the user provided parameters @xmath34 and @xmath26 .",
    "these system parameters are defined in `` parameter.h '' file .    * diff_th : allowed error threshold between the estimated and user provided @xmath3 values . *",
    "rrate_th : allowed error threshold between the estimated and user provided precode rate .",
    "* red_byte_th : allowed redundant zero bytes to be appended at the end of the file for parameter consistency .",
    "* rand_win_max : random number search window maximum value . *",
    "rand_win_min : random number search window minimum value .",
    "* array_min_jj : minimum array ldpc `` _ _ jj _ _ '' parameter value .",
    "* array_min_kk : minimum array ldpc `` _ _ kk _ _ '' parameter value . * tries_th : threshold on the number of tries before incrementing diff_th and rrate_th . * delta_diff_th : step size increment for diff_th * delta_rrate_th : step size increment for rrate_th .",
    "next , we provide the algorithm that returns the estimated values of @xmath3 , @xmath2 and redundant number of zeros ( @xmath35 ) .",
    "the algorithm admits four inputs , namely @xmath26 , @xmath36 , @xmath3 and @xmath5 .",
    "initial values of system parameters shall be set by `` parameter.h '' file and are changed locally within the function implementing * algorithm 2*.    @xmath37 @xmath38(rand_win_max + rand_win_min ) - rand_win_min @xmath39 largest_prime_factor(@xmath2 ) @xmath40",
    "@xmath41 @xmath42 @xmath43 _ filesize _ / @xmath44 + 1 @xmath45 diff_th @xmath46 diff_th + delta_diff_th rrate_th @xmath46 rrate_th + delta_rrate_th @xmath47 @xmath48 print error and exit .",
    "@xmath43 _ filesize",
    "_ / @xmath49 + 1 @xmath50 - _ filesize _ * return * @xmath51      check # 3 nodes are the most suitable node type for the repair process since it directly defines the relationship between the coded chunks .",
    "it is not hard to see having more of these type of nodes ( created independently or dependently ) gives alternative ways of repairing a given node in case of different combinations of node failures occur in the network . in other words ,",
    "_ the more of these type of checks , the more potential we have for the regeneration of the lost coded chunks_. with regard to this observation , we can use two techniques to increase the number of check # 3 type information based on check # 1 and check # 2 equations :    * identify the coded nodes with degree one ( say we have @xmath52 of those ) . identify their data node neighbors .",
    "there are @xmath52 check # 2 type check equations that connect these data nodes with the coded nodes .",
    "since the corresponding @xmath52 coded nodes carry the same information , we can use these check # 2 type check equations as check # 3 check equation . note that since check # 2 and check # 3 equations are derived from the same founsure base graph",
    ", this technique is likely to generate already existent local groups or dependent local groups for coded nodes . *",
    "note that check # 1 is user defined although subject to a predefined structure .",
    "this defines local groups over the data nodes .",
    "since each data node is linked to local groups of coded nodes through check # 2 , we can use this relationship to derive check # 3 type check equations .",
    "for example , suppose we have the following check # 1 local group defined for data nodes @xmath53 , @xmath54 and @xmath55 : @xmath56 .",
    "also , suppose that we have the following check # 2 equations : @xmath57 @xmath58 @xmath57 @xmath59 @xmath57 @xmath60 + thus , we can find a check # 3 type equation given by @xmath61 by observing the following equivalence , @xmath62 + note that since this technique uses check # 1 , it is likely to generate distinct check # 3 local groups and help improve repair performance .",
    "we use a heuristic algorithm to generate group # 2 and group # 3 check equations at the same time .",
    "we let @xmath16 be the input generator matrix of the founsure base graph code .",
    "we use @xmath63 to refer to the @xmath11th row of @xmath64 and @xmath65 to refer to the @xmath11th column of @xmath64 .",
    "the algorithm uses xor operation ( @xmath66 ) to sparsify the generator matrix @xmath64 .",
    "if @xmath64 is full rank , i.e. , @xmath67**b**@xmath68 and algorithm converges succesfully , at the end we should be able to find a permutation matrix @xmath69 such that @xmath70 $ ] .",
    "also , @xmath71 shall hold all the local sets i.e. , check # 2 equations in the first @xmath2 columns and check # 3 equations in the last @xmath72 columns .",
    "we can express different types of checks as the union of all check # 2 and check # 3 check equations as given by @xmath73    here we provide the details of the algorithm below using a pseudocode formulation .",
    "we use a simple function zero_columns ( . ) that finds the number of nonzero columns of the matrix in the argument .",
    "since @xmath64 is typically sparse , we use sparse representation of matrices in our founsure 1.0 implementation and xor operation is replaced by set - union operation .",
    "@xmath74 @xmath75 @xmath76 @xmath77 @xmath78 * break * ; [ euclidendwhile ] * return * @xmath79      check # 1 is determined by a binary ldpc array codes as explained before .",
    "this choice is user specific and its rate usually depends on the precode rate required by the code itself as well as the reliability imposed by the application .",
    "the graph connections are given by the constraints of array codes .",
    "unlike check equations # 1 , check equations # 2 and # 3 are determined by the founsure base code i.e. , lt - like ( fountain ) code . based on the generator matrix of the code , an iterative heuristic low - complexity algorithm is run to determine @xmath4 equations .",
    "if the algorithm converges , then we should have @xmath2 equations for # 2 and @xmath72 equations for # 3 .",
    "the algorithm produces a correct set of local equations ( sets ) but does not guarantee those equations to be independent .",
    "however , in generating those equations , we do not employ any matrix inversion ( which is quite costly for large size matrices ) to find check equations and hence we trade off the efficiency by performance .",
    "the function that generates check # 2 and # 3 local groups is the utility function * genchecks*.    the function * genchecks * assumes that a metadata file is already generated by a previous run of the encoder * founsureenc*. * genchecks * generates check groups and modifies the _ meta_data _ file ( appends the size of check data in terms of _",
    "sizeof(int ) _ bytes at the end of the metadata file if `` -m '' flag parameter is true ) .",
    "the check information is stored in another binary file called _",
    "< filename>_check.data_. this file stores an integer array with a specific format as a binary file .",
    "the reason for introducing a format is to use bulk read / write capabilities of _ fread _ and _ fwrite _ c library functions which will make kernel s i / o performance acceptable .",
    "the format is pretty straightforward and can be improved .",
    "note that the integer value of the first _",
    "sizeof(int ) _ bytes is either 0 or 1 .    *",
    "if it is 0 ( group # 3 ) , then the next integer value ( next _ sizeof(int ) _ bytes ) gives the degree number i.e. , the number of integers to be read as part of one local group for coded symbols . *",
    "if it is 1 ( group # 2 ) , then the next integer value ( next _ sizeof(int ) _ bytes ) gives the data symbol index which is involved with a local set whose degree is given by the following integer ( next _ sizeof(int ) _ bytes ) .",
    "this degree also indicates the next `` degree '' number , i.e. , the number of integers to be read as part of one local group for coded symbols .",
    "the beauty of the proposed * algorithm 3 * is that if it converges , then all of the data symbols are covered exactly by one particular group # 2 local set .",
    "let us give an example and suppose that we have the following integer array stored in _",
    "filename>_check.data _ : @xmath80    if we decode this integer array , we will be able to say that the first local set belongs to group # 3 and this set has four elements .",
    "in other words , 13th , 56th,17th and 66th coded symbols form a local group .",
    "the next local set belongs to group # 2 and the associated data symbol index is 19 .",
    "this data symbol along with 11th and 13th coded symbols ( two coded symbols ) form a local group .",
    "this way we can decode the whole integer array stored in < filename>_check.data .",
    "if the algorithm converges , there should be @xmath2 leading 1 s and @xmath72 leading 0 s in the integer array .",
    "note that the total number of integers contained in the array is given by @xmath81 where @xmath82 is the total number of elements in local group # 2 indexed by @xmath83 ( without the data symbol ) and group # 3 indexed by @xmath83 .",
    "we also note that even if the algorithm does not converge , the maximum memory occupancy possible is @xmath84 _",
    "sizeof(int ) _ bytes , so it is enough to allocate that much memory for the file without encountering a segmentation fault .",
    "[ founsurefunctions ] summarizes how different functions of founsure interact with each other , what other metadata is generated and what are the read / write permissions granted to each of these functions .",
    "when the repair process is initiated , memory allocation as well as repair object preparation starts .",
    "the main engine shall look for _",
    "< filename>_check.data _ under @xmath85 directory . if it finds one and if the metadata is appropriately formatted , it will read - in and format check # 2 and check # 3 equations for the repair object . a bulk read",
    "is called and all the content is transferred to memory ( inside the buffer pointed by @xmath86 ) . since founsure s encoding , decoding , repair and update operations are based on belief propagation algorithm , it sequentially searches only one unknown over the available local sets in a loop . in order to reduce the computation and bandwidth , it is essential that repair / decode process use low - degree check # 3 equations first so that we do not have to run though the end of the loop to complete the process .",
    "founsure implementation extracts check # 3 equations from the buffer ( @xmath86 ) using the standard @xmath87 and then fills in the appropriate fields of repair object .",
    "the ordering can be enabled or disabled for check # 2 and # 3 equations using parameters order_check_2 and order_check_3 in `` parameter.h '' file .",
    "an update process is about making the code stronger or weaker by either generating more redundancy ( in case of increased failures ) or taking away unwanted redundancy ( in case of using more reliable devices for storing information ) .",
    "if we would like to make the existent code weaker , it is easy .",
    "we just modify the metadata accordingly and erase the redundancy .",
    "so for the rest of this section , we particularly mean making the code stronger when we update the code .    founsure update process is tightly related to repair process .",
    "this is mainly because updating a code is nothing but repairing the needed blocks of information to help increase the reliability of data .",
    "we call * genchecks * to update the current code using the flag ` -e '",
    ". there must be a valid metadata associated with the code at the time * genchecks * is called .",
    "code update process will rewrite @xmath4 , the number of bytes used for the integer array due to check # 2 and check # 3 equations and update _ <",
    "filename>_check.data_. this series of modifications do not make any changes to the data / coding chunks . in order to trigger / sync changes with data ,",
    "we finally need to call * founsurerep * function with the appropriate file name .",
    "here are the set of commands to use encoding , decoding , repair and update features of founsure 1.0 .",
    "note that founsure 1.0 comes with man pages or you can always use  -h \" flag command for immediate help when you call founsure functions .",
    "the following command will encode a test file _ testfile.txt _ with @xmath88 data chunks with each chunk occupying @xmath89 bytes .",
    "the encoder generates @xmath90 coding chunks using @xmath91`finitedist ' degree distribution and @xmath92`arrayldpc ' precoding .",
    "finally , generated chunks are striped / written to @xmath93 distinct files for default disk / drive allocation under _ /coding _ directory . the flag  -v \" is used to output parameter information used during the encoding operation .",
    "founsure encoder also generates a metadata file with critical coding parameters which will later be useful for decoding , repair and update operations . without an appropriate metadata ,",
    "founsure can not operate on files .",
    "+   +   + now , let us erase one of the coding chunks and run founsure decoder .",
    "the decoder shall generate a decoded file _",
    "test_file_decoded.txt _ under _ /coding _ directory . you can use ",
    "diff \" command to compare this file with the original .",
    "+   +   + note that * founsuredec * function does not recover the lost drive data _ coding / testfile_disk0007.txt _ , because this function is responsible only for the original data recovery process . in storage systems",
    "however , we need to recover lost data to keep the system data reliability at an acceptable level . in founsure 1.0 , it is extremely easy to initiate repair ( current version only supports exact repair at the moment ) process by running the following command .",
    "+   +   + this would trigger the conventional repair operation and first shall decode the entire data and then re - run partial encoding to generate the lost chunks .",
    "* founsurerep * outputs the pure computation speed as well as the bandwidth consumed due to repair . if you observe carefully , conventional repair is heavily time and bandwidth consuming operation .",
    "in fact , due to non - optimal overhead , the number of bytes that need to be transferred for the conventional repair is little larger than the size of the original user file .",
    "founsure 1.0 supports fast and efficient repair as well . in order to use this feature",
    ", one needs to modify the metadata file and create an extra helping data / file called _ testfile_check.data _ which shall contain information for fast repair .",
    "details can be found later in the document . in order to make these changes , we primarily run * genchecks * function .",
    "finally , we can re - run the repair function as before and you will realize from the comments printed out that the function will be able to recognize that there is available information for fast / efficient repair and will run that process instead of switching to conventional repair .",
    "you should be able to observe the reduced bandwidth consumed by the repair operation .",
    "+   +   + we can also use * genchecks * to trigger ` update ' functionality .",
    "for example , let us assume that the system reliability is degraded due to drive wear and we want to generate extra two drive - worth information in addition to already generated 10 drive information .",
    "we use ` -e ' flag to modify metadata file as well as _",
    "_ for the update operation .",
    "this shall change the code and all its related parameters . however in order to apply it to encoded data , we shall use * founsurerep * to generate new coding chunks and output files . alternatively , you can erase drive info as well by supplying negative values for ` -e ' flag . in this case , you do not need to call * founsurerep * because there is nothing to generate .",
    "you can simply erase corresponding drive chunks after you scale the system down .",
    "to best of my knowledge , founsure is the most flexible erasure coding library that is open source and can be configured based on the requirements of the application . with the current software architecture , many more functionalities can be integrated such as partial user data construction and advanced error detection for failure localization .",
    "although the current version does not fully utilize , founsure has highly parallel architecture and lends itself to parallel programming . unlike founsure , existing research is mostly focused on overhead optimal designs using the parallel hardware .",
    "originally , founsure is developped for data storage systems , it can simply be applied to packet switched networks in which the underlying channel is erasure channel or sporadic erasure channels . with advanced features such as error detection , multi - threaded implementations , advanced decoding , founsure can further be used for error correction which would open up more fields of its application such as image reconstruction and data protection over noisy communication channels .",
    "we developed an erasure coding library that can be used to solve the tradeoff between computation complexity , coding overhead and repair bandwidth .",
    "founsure can be thought as the base software on which we can put lots of different features that will make it more application - centric and configurable for future generation reliable system requirements .",
    "i would like to thank the scientific and technological research council of turkey ( tubitak ) and quantum corporation who provided the support for maturing ideas , as well as hardware platforms for extensive testing .",
    "00    b. nisbet , `` fas storage systems : laying the foundation for application availability , '' network appliance white paper : http://www.netapp.com/us/library/analyst-reports/ar1056.html , february , 2008 .",
    "i. s. reed and g. solomon , `` polynomial codes over certain finite fields , '' journal of the society for industrial and applied mathematics , 8 , 1960 , pp .",
    "300 - 304 .",
    "j. blomer , m. kalfane , m. karpinski , r. karp , m. luby and d. zuckerman , `` an xor - based erasure - resilient coding scheme , '' technical report tr-95 - 048 , international computer science institute , august , 1995 .",
    "m. blaum , j. brady , j. bruck and j. menon , `` evenodd : an efficient scheme for tolerating double disk failures in raid architectures , '' ieee transactions on computing , 44(2 ) , february , 1995 , pp .",
    "192 - 202 .",
    "m. asteris , d. papailiopoulos , a. g. dimakis , r. vadali , s. chen , and d. borthakur . xoring elephants : novel erasure codes for big data .",
    "proceedings of the vldb endowment ( pvldb ) , 6(5 ) , 2013 .",
    "a. partow , `` schifra reed - solomon ecc library , '' open source code distribution : http://www.schifra.com/downloads.html , 2000 - 2007 .",
    "j. s. plank , s. simmerman and c. d. schuman , `` jerasure : a library in c / c++ facilitating erasure coding for storage applications - version 1.2 , '' university of tennessee , cs-08 - 627 , august , 2008 , http://www.cs.utk.edu/  plank / plank / papers / cs-08 - 627.html .",
    "j. s. plank , s. simmerman , and c. d. schuman , `` jerasure : a library in c / c++ facilitating erasure coding for storage applications - version 1.2 , '' university of tennessee , tech .",
    "cs-08 - 627 , august 2008 .",
    "available online : http://www.ceph.com .",
    "s. s. arslan , t. goker and r. wideman ,  a joint dedupe - fountain coded archival storage , \" _",
    "ieee icc17 _ , paris , france , may 2017 .",
    "`` reverend bayes on inference engines : a distributed hierarchical approach . '' _",
    "aaai-82 : pittsburgh , pa . second national conference on artificial intelligence _ , menlo park , california : aaai press .",
    ". 133136 .",
    "d. j. c. mackay , ",
    "fountain codes ,  in iee proc .",
    "10621068 , dec . 9 , 2005 .",
    "m. luby ,  lt - codes ,  in proc .",
    "43rd annu .",
    "ieee symp .",
    "foundations of computer science ( focs ) , vancouver , bc , canada , nov .",
    "2002 , pp . 271280 .",
    "a. shokrollahi ,  raptor codes ,  ieee trans . on information theory , june 2006 .",
    "suayb s. arslan `` incremental redundancy , fountain codes and advanced topics , '' 2014 .",
    "avaialable online : arxiv:1402.6016 .",
    "a. j. felstrom and k. s. zigangirov , `` time - varying periodic convolutional codes with low - density parity - check matrix , '' ieee trans .",
    "theory , vol .",
    "45 , no . 6 , pp",
    ". 21812190 , sep .",
    "1999 .    j. l. fan , `` array codes as low - density parity - check codes , '' in proc .",
    "2nd intl symposium on turbo codes and related topics , brest , france , sept .",
    "2000 , pp .",
    "543 - 546 , .",
    "e. eleftheriou and s. olcer , `` low - density parity - check codes for digital subscriber lines , '' in proc .",
    "communications ( icc 2002 ) , vol . 3 , 2002 , pp .",
    ".code metadata ( mandatory ) [ cols=\"<,<,<\",options=\"header \" , ]"
  ],
  "abstract_text": [
    "<S> founsure is an open - source software library , distributed under lgplv3 license and implements a multi - dimensional graph - based erasure coding entirely based on fast exclusive or ( xor ) logic . </S>",
    "<S> its implementation uses compiler optimizations to generate the right assembly for the given simd - enabled architectures . </S>",
    "<S> founsure 1.0 supports a variety of features that shall find interesting applications in modern data storage as well as communication and computer network systems which are becoming hungry in terms of network bandwidth , computational resources and average consumed power . in particular , founsure erasure code provides a three dimensional design space i.e. , computation complexity , coding overhead and repair bandwidth to meet the requirements of modern distributed data storage and processing systems in which the data needs to be protected against device / hardware failures </S>",
    "<S> .    distributed storage , erasure coding , , fountain coding , single instruction multiple data ( simd ) , erasure coding , openmp , reliability . </S>"
  ]
}