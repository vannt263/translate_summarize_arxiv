{
  "article_text": [
    "the _ sp theory of intelligence _ , which has been under development since about 1987 , aims to simplify and integrate concepts across artificial intelligence , mainstream computing and human perception and cognition , with information compression as a unifying theme .",
    "the name ` sp ' is short for _ simplicity _ and _ power _ , because compression of any given body of information , @xmath0 , may be seen as a process of reducing informational ` redundancy ' in @xmath0 and thus increasing its ` simplicity ' , whilst retaining as much as possible of its non - redundant expressive ` power ' . likewise with occam s razor ( section [ occams_razor_section ] , below ) .",
    "aspects of the theory , as it has developed , have been described in several peer - reviewed articles .. ] the most comprehensive description of the theory as it stands now , with many examples , is in @xcite .",
    "but this book , with more than 450 pages , is too long to serve as an introduction to the theory .",
    "this article aims to meet that need , with a fairly full description of the theory and a selection of examples . for the sake of brevity",
    ", the book will be referred to as ` _ bk _ ' .",
    "the next section describes the origins and motivation for the sp theory , section [ introduction_to_sp_theory_section ] introduces the theory , sections [ multiple_alignment_section ] and [ unsupervised_learning_section ] fill in a lot of the details , while the following sections describe aspects of the theory and what it can do .",
    "the following subsections outline the origins of the sp theory , how it relates to some other research , and how it has developed .",
    "much of the inspiration for the sp theory is a body of research , pioneered by fred attneave @xcite , horace barlow @xcite , and others , showing that several aspects of the workings of brains and nervous systems may be understood in terms of information compression .",
    "idea was right in drawing attention to the importance of redundancy in sensory messages ... but it was wrong in emphasizing the main technical use for redundancy , which is compressive coding . ''",
    "as we shall see , the sp theory is closer to barlow s original thinking than what he said later . ] for example , when we view a scene with two eyes , the image on the retina of the left eye is almost exactly the same as the image on the retina of right eye , but our brains merge the two images into a single percept , and thus compress the information @xcite .",
    "more immediately , the theory has grown out of my own research , developing models of the unsupervised learning of a first language , where the importance of information compression became increasingly clear ( eg , * ? ? ? * ) .. ]    the theory also draws on principles of ` minimum length encoding ' pioneered by @xcite , and others .",
    "and it has become apparent that several aspects of computing , mathematics , and logic may be understood in terms of information compression ( _ bk _ , chapters 2 and 10 ) .    at an abstract level",
    ", information compression can bring two main benefits :    * for any given body of information , @xmath0 , information compression may reduce its size and thus facilitate the storage , processing and transmission of @xmath0 . *",
    "perhaps more important is the close connection between information compression and concepts of prediction and probability ( see , for example , * ? ? ?",
    "* ) . in the sp system",
    ", it is the basis for all kinds of inference , and for calculations of probabilities .    in animals",
    ", we would expect these things to have been favoured by natural selection because of the competitive advantage they can bring .",
    "notwithstanding the ` qwerty ' phenomenon , there is reason to believe that information compression , properly applied , may yield comparable advantages in artificial systems .      in the sp theory , the matching and unification of patterns is seen as being closer to the bedrock of information compression than more mathematical techniques such as wavelets or arithmetic coding , and closer to the bedrock of information processing and intelligence than , say , concepts of probability .",
    "a working hypothesis in this programme of research is that , by staying close to relatively simple , ` primitive ' , concepts of matching patterns and unifying them , there is a better chance of cutting through unnecessary complexity , and in gaining new insights and better solutions to problems . the mathematical basis of wavelets , arithmetic coding , and probabilities ,",
    "may itself be founded on the matching and unification of patterns ( _ bk _ , chapter 10 ) .      in accordance with occam s razor , the sp system aims to combine conceptual simplicity with descriptive and explanatory power .",
    "apart from this widely - accepted principle , the drive for simplification and integration of concepts in this research programme has been motivated in part by allen newell s critique of some kinds of research in cognitive science @xcite , and in part by the apparent fragmentation of research in artificial intelligence and mainstream computing , with their myriad of concepts and many specialisms .    in attempting to simplify and integrate ideas",
    ", the sp theory belongs in the same tradition as unified theories of cognition such as soar @xcite and act - r @xcite . and",
    "it chimes with the resurgence of interest in understanding artificial intelligence as a whole ( see , for example , * ? ? ?",
    "* ) and with research on ` natural computation ' @xcite .",
    "although the sp programme shares some objectives with projects such as the gdel machine @xcite , and ` universal artificial intelligence ' @xcite , the approach is very different .      in this research , it is assumed that knowledge in the sp system should normally be transparent or comprehensible , much as in the ` symbolic ' tradition in artificial intelligence ( see also section [ donsvic_section ] ) , and distinct from the kind of ` sub - symbolic ' representation of knowledge that is the rule in , for example , ` neural networks ' as they are generally conceived in computer science .    as we shall see in section [ representation_of_knowledge_section ] and elsewhere in this article",
    ", sp patterns in the multiple alignment framework may serve to represent a variety of kinds of knowledge , in symbolic forms .      in developing the theory",
    ", it was apparent at an early stage that existing systems  such my models of language learning . ] and systems like prolog  would need radical re - thinking to meet the goal of simplifying and integrating ideas across a wide area .    the first published version of the sp theory @xcite described ` some unifying ideas in computing '",
    ". early work on the sp computer model concentrated on developing an improved version of the ` dynamic programming ' technique for the alignment of two sequences ( see _ bk _ , appendix a ) as a possible route to modelling human - like flexibility in pattern recognition , analysis of language , and the like .",
    "about 1992 , it became apparent that the explanatory range of the theory could be greatly expanded by forming alignments of 2 , 3 , or more sequences , much as in the ` multiple alignment ' concept of bioinformatics .",
    "that idea was developed and adapted in new versions of the sp model , and incorporated in new procedures for unsupervised learning .",
    "aspects of the theory , with many examples , have been developed in @xcite .",
    "the main elements of the sp theory are :    * the sp theory is conceived as an abstract brain - like system that , in an ` input ' perspective , may receive ` new ' information via its senses , and store some or all of it in its memory as ` old ' information , as illustrated schematically in figure [ sp_input_perspective_figure ] .",
    "there is also an ` output ' perspective , described in section [ decompression_by_compression_section ] .",
    "* the theory is realised in the form of a computer model , introduced in section [ computer_model_section ] , below , and described more fully later . *",
    "all new and old information is expressed as arrays ( _ patterns _ ) of atomic symbols in one or two dimensions .",
    "an example of an sp pattern may be seen in each row in figure [ parsing_1_figure ] .",
    "each symbol can be matched in an all - or - nothing manner with any other symbol .",
    "any meaning that is associated with an atomic symbol or group of symbols must be expressed in the form of other atomic symbols . *",
    "each pattern has an associated frequency of occurrence which may be assigned by the user or derived via the processes for unsupervised learning .",
    "the default value for the frequency of any pattern is 1 . *",
    "the system is designed for the unsupervised learning of old patterns by compression of new patterns .. ] * an important part of this process is , where possible , the economical ( compressed ) encoding of new patterns in terms of old patterns .",
    "this may be seen to achieve such things as pattern recognition , parsing or understanding of natural language , or other kinds of interpretation of incoming information in terms of stored knowledge , including several kinds of reasoning . * in keeping with the remarks in section [ matching_and_unification_section ] , compression of information is achieved via the matching and unification ( merging ) of patterns . in this , there are key roles for the frequency of occurrence of patterns , and their sizes . * the concept of _ multiple alignment _",
    ", described in section [ multiple_alignment_section ] , is a powerful central idea , similar to the concept of multiple alignment in bioinformatics but with important differences .",
    "* owing to the intimate connection , previously mentioned , between information compression and concepts of prediction and probability , it is relatively straightforward for the sp system to calculate probabilities for inferences made by the system , and probabilities for parsings , recognition of patterns , and so on ( section [ ma_probabilities_section ] ) . * in developing the theory , i have tried to take advantage of what is known about the psychological and neurophysiological aspects of human perception and cognition , and to ensure that the theory is compatible with such knowledge ( see section [ perception_cognition_neuroscience_section ] ) .",
    "the sp theory is realised most fully in the sp70 computer model , with capabilities in the building of multiple alignments and in unsupervised learning .",
    "this will be referred to as the sp model , although in some cases examples are from a subset of the model or slightly earlier precursors of it .",
    "the sp model and its precursors have played a key part in the development of the theory :    * as an antidote to vagueness .",
    "as with all computer programs , processes must be defined with sufficient detail to ensure that the program actually works .",
    "* by providing a convenient means of encoding the simple but important mathematics that underpins the sp theory , and performing relevant calculations , including calculations of probability .",
    "* by providing a means of seeing quickly the strengths and weaknesses of proposed mechanisms or processes .",
    "many ideas that looked promising have been dropped as a result of this kind of testing .",
    "* by providing a means of demonstrating what can be achieved with the theory .",
    "the workings of the sp model is described in some detail in _ bk _ ( sections 3.9 , 3.10 , and 9.2 ) and more briefly in sections [ multiple_alignment_section ] and [ unsupervised_learning_section ] , below .",
    "the source code for the models , with associated documents and files , may be downloaded via links under the heading ` source code ' at the bottom of the page on http://bit.ly/wtxa3g[bit.ly/wtxa3g ] .",
    "the two main elements of the models , described in the following two sections , are the building of multiple alignments and the unsupervised learning of old patterns .",
    "the sp model may be regarded as a first version of the _ sp machine _",
    ", an expression of the sp theory and a means for it to be applied .",
    "a useful step forward in the development of the sp theory would be the creation of a high - parallel , open - source version of the sp machine , accessible via the web , and with a good user interface .",
    "this would provide a means for researchers to explore what can be done with the system and to improve it . how things may develop is shown schematically in figure [ sp_machine_figure ] .",
    "the high - parallel search mechanisms in any of the existing internet search engines would probably provide a good foundation for the proposed development .",
    "further ahead , there may be a case for the creation of new kinds of hardware , dedicated to the building of multiple alignments and other processes in the sp framework ( * ? ? ?",
    "* section 6.13 ) .      like most theories , the sp theory has shortcomings , but it appears that they may be overcome . at present ,",
    "the most immediate problems are :    * _ processing of information in two or more dimensions_. no attempt has yet been made to generalise the sp model to work with patterns in two dimensions , although that appears to be feasible to do , as outlined in _",
    "( section 13.2.1 ) . as noted in _",
    "( section 13.2.2 ) , it is possible that information with dimensions higher than two may be encoded in terms of patterns in one or two dimensions , somewhat in the manner of architects drawings .",
    "a 3d structure may be stitched together from several partially - overlapping 2d views , in much the same way that , in digital photography , a panoramic view may be created from partially - overlapping pictures ( * ? ? ?",
    "* sections 6.1 and 6.2 ) .",
    "* _ recognition of perceptual features in speech and visual images_. for the sp system to be effective in the processing of speech or visual images , it seems likely that some kind of preliminary processing will be required to identify low level perceptual features such as , in the case of speech , phonemes , formant ratios , or formant transitions , or , in the case of visual images , edges , angles , colours , luminances , or textures . in vision",
    ", at least , it seems likely that the sp framework itself will prove relevant since edges may be seen as zones of non - redundant information between uniform areas containing more redundancy and , likewise , angles may be seen to provide significant information where straight edges , with more redundancy , come together ( * ? ? ?",
    "* section 3 ) . as a stop - gap solution",
    ", the preliminary processing may be done using existing techniques for the identification of low - level perceptual features ( * ? ? ?",
    "* chapter 13 ) .",
    "* _ unsupervised learning_. a limitation of the sp computer model as it is now is that it can not learn intermediate levels of abstraction in grammars ( eg , phrases and clauses ) , and it can not learn the kinds of discontinuous dependencies in natural language syntax that are described in sections [ discontinuous_dependencies_section ] to [ aux_verb_2_section ] .",
    "i believe these problems are soluble and that solving them will greatly enhance the capabilities of the system for the unsupervised learning of structure in data ( section [ unsupervised_learning_section ] ) . * _ processing of numbers_. the sp model works with atomic symbols such as ascii characters or strings of characters with no intrinsic meaning . in itself ,",
    "the sp system does not recognise the arithmetic meaning of numbers such as ` 37 ' or ` 652 ' and will not process them correctly .",
    "however , the system has the potential to handle mathematical concepts if it is supplied with patterns representing peano s axioms or similar information ( _ bk _ , chapter 10 ) . as a stop - gap solution in the sp machine",
    ", existing technologies may provide whatever arithmetic processing may be required .",
    "the concept of _ multiple alignment _ in the sp theory has been adapted from a similar concept in bioinformatics , where it means a process of arranging , in rows or columns , two or more dna sequences or amino - acid sequences so that matching symbols  as many as possible  are aligned orthogonally in columns or rows .",
    "multiple alignments like these are normally used in the computational analysis of ( symbolic representations of ) sequences of dna bases or sequences of amino acid residues as part of the process of elucidating the structure , functions or evolution of the corresponding molecules .",
    "an example of this kind of multiple alignment is shown in figure [ dna_figure ] .",
    "as in bioinformatics , a multiple alignment in the sp system is an arrangement of two or more patterns in rows ( or columns ) , with one pattern in each row ( or column ) .",
    "the main difference between the two concepts is that , in bioinformatics , all sequences have the same status , whereas in the sp theory , the system attempts to create a multiple alignment which enables one new pattern ( sometimes more ) to be encoded economically in terms of one or more old patterns .",
    "other differences are described in _",
    "( section 3.4.1 ) .    in figure",
    "[ parsing_1_figure ] , row 0 contains a new pattern representing a sentence : `` t h i s b o y l o v e s t h a t g i r l ` ' , while each of rows 1 to 8 contains an old pattern representing a grammatical rule or a word with grammatical markers .",
    "this multiple alignment , which achieves the effect of parsing the sentence in terms of grammatical structures , is the best of several built by the model when it is supplied with the new pattern and a set of old patterns that includes those shown in the figure and several others as well .    in this example , and others in this article , ` best ' means that the multiple alignment in the figure is the one that enables the new pattern to be encoded most economically in terms of the old patterns , as described in section [ ma_evaluation_section ] , below .",
    "this section describes in outline how multiple alignments are evaluated in the sp model .",
    "more detail may be found in _",
    "( section 3.5 ) .",
    "each old pattern in the sp system contains one or more ` identification ' symbols or _ id - symbols _ which , as their name suggests , serve to identify the pattern .",
    "examples of id - symbols in figure [ parsing_1_figure ] are `` d ` ' and `` 0 ` ' at the beginning of `` d 0 t h i s # d ` ' ( row 6 ) , and `` n ` ' and `` 1 ` ' at the beginning of `` n 1 b o y # n ` ' ( row 8) .    associated with each type of symbol ( where a ` type ' of symbol is any one of a set of symbols that match each other exactly ) is a notional _ code _ or bit pattern that serves to distinguish the given type from all the others .",
    "this is only notional because the bit patterns are not actually constructed .",
    "all that is needed for the purpose of evaluating multiple alignments is the size of the notional bit pattern associated with each type .",
    "this is calculated via the shannon - fano - elias coding scheme ( described in @xcite ) , using information about the frequency of occurrence of each old pattern , so that the shortest codes represent the most frequent symbol types and _ vice versa_. notice that these bit patterns and their sizes are totally independent of the names for symbols that are used in written accounts like this one : names that are chosen purely for their mnemonic value .",
    "given a multiple alignment like the one shown in figure [ parsing_1_figure ] , one can derive a _",
    "code pattern _ from the multiple alignment in the following way :    1 .",
    "scan the multiple alignment from left to right looking for columns that contain an id - symbol by itself , not aligned with any other symbol .",
    "2 .   copy these symbols into a code pattern in the same order that they appear in the multiple alignment .",
    "the code pattern derived in this way from the multiple alignment shown in figure [ parsing_1_figure ] is `` s 0 1 0 1 0 # s ` ' .",
    "this is , in effect , a compressed representation of those symbols in the new pattern that are aligned with old symbols in the multiple alignment . in this case",
    ", the code pattern is a compressed representation of _ all _ the symbols in the new pattern but it often happens that some of the symbols in the new pattern are not matched with any old symbols and then the code pattern will represent only those new symbols that are aligned with old symbols .    in the context of natural language processing , it perhaps more plausible to suppose that the encoding of a sentence is some kind of representation of the meaning of the sentence , instead of a pattern like `` s 0 1 0 1 0 # s ` ' .",
    "how a meaning may be derived from a sentence via multiple alignment is described in _",
    "( section 5.7 ) .      given a code pattern like `` s 0 1 0 1 0 # s ` ' , we may calculate a ` compression difference ' as :    @xmath1    or a ` compression ratio ' as :    @xmath2    where @xmath3 is the total number of bits in those symbols in the new pattern that are aligned with old symbols in the alignment and @xmath4 is the total number of bits in the symbols in the code pattern , and the number of bits for each symbol is calculated via the shannon - fano - elias scheme as mentioned above .",
    "@xmath5 and @xmath6 are each an indication of how effectively the new pattern ( or those parts of the new pattern that are aligned with symbols within old patterns in the alignment ) may be compressed in terms of the old patterns that appear in the given multiple alignment .",
    "the @xmath5 of a multiple alignment  which has been found to be more useful than @xmath6may be referred to as the _ compression score _ of the multiple alignment .    in each of these equations",
    ", @xmath3 is calculated as :    @xmath7    where @xmath8 is the size of the code for @xmath9th symbol in a sequence , @xmath10 , comprising those symbols within the new pattern that are aligned with old symbols within the multiple alignment .",
    "@xmath4 is calculated as :    @xmath11    where @xmath8 is the size of the code for @xmath9th symbol in the sequence of @xmath12 symbols in the code pattern derived from the multiple alignment .",
    "this section describes in outline how the sp model builds multiple alignments .",
    "more detail may be found in _",
    "( section 3.10 ) .",
    "multiple alignments are built in stages , with pairwise matching and alignment of patterns . at each stage",
    ", any partially - constructed multiple alignment may be processed as if it was a basic pattern and carried forward to later stages .",
    "this is broadly similar to some programs for the creation of multiple alignments in bioinformatics . , retrieved 2013 - 05 - 08 .",
    "] at all stages , the aim is to encode new information economically in terms of old information and to weed out multiple alignments that score poorly in that regard .",
    "the model may create old patterns for itself , as described in section [ unsupervised_learning_section ] , but when the formation of multiple alignments is the focus of interest , old patterns may be supplied by the user . in all cases ,",
    "new patterns must be supplied by the user .    at each stage of building multiple alignments ,",
    "the operations are as follows :    1 .",
    "identify a set of ` driving ' patterns and a set of ` target ' patterns . at the beginning ,",
    "the new pattern is the sole driving pattern and the old patterns are the target patterns . in all subsequent stages ,",
    "the best of the multiple alignments formed so far ( in terms of their @xmath5 scores ) are chosen to be driving patterns and the target patterns are the old patterns together with a selection of the best multiple alignments formed so far , including all of those that are driving patterns .",
    "2 .   compare each driving pattern with each of the target patterns to find full matches and good partial matches between patterns .",
    "this is done with a process that is essentially a form of ` dynamic programming ' @xcite , somewhat like the winmerge utility for finding similarities and differences between files .. ] the process is described quite fully in _",
    "( appendix a ) and outlined in section [ matching_section ] , below .",
    "the main difference between the sp process and others , is that the former can deliver several alternative matches between a pair of patterns , while winmerge and standard methods for finding alignments deliver one ` best ' result .",
    "3 .   from the best of the matches found in the current stage , create corresponding multiple alignments and add them to the repository of multiple alignments created by the program .",
    "this process of matching driving patterns against target patterns and building multiple alignments is repeated until no more multiple alignments can be found .",
    "for the best of the multiple alignments created since the start of processing , probabilities are calculated , as described in section [ ma_probabilities_section ] .",
    "figure [ pattern_matching_figure ] shows with a simple example how the sp model finds good full and partial matches between a ` query ' string of atomic symbols ( alphabetic characters in this example ) and a ` database ' string :    1 .",
    "the query is processed left to right , one symbol at a time .",
    "each symbol in the query is , in effect , broadcast to every symbol in the database to make a yes / no match in each case .",
    "every positive match ( hit ) between a symbol from the query and a symbol in the database is recorded in a _ hit structure _ , illustrated in the figure .",
    "if the memory space allocated to the hit structure is exhausted at any time then the hit structure is purged : the leaf nodes of the tree are sorted in reverse order of their probability values and each leaf node in the bottom half of the set is extracted from the hit structure , together with all nodes on its path which are not shared with any other path . after the hit structure has been purged , the recording of hits may continue using the space which has been released .",
    "( described in _",
    "bk _ , section a.2 ) .",
    "each node in the hit structure shows the ordinal position of a query symbol and the ordinal position of a matching database symbol .",
    "each path from the root node to a leaf node represents a sequence of hits.,scaledwidth=70.0% ]      because of the way each model searches for a global optimum in the building of multiple alignments , it does not depend on the presence or absence of any particular feature or combination of features . up to a point",
    ", plausible results may be obtained in the face of errors of omission , commission and substitution in the data .",
    "this is illustrated in the two multiple alignments in figure [ noisy_data_recognition_figure ] where the new pattern in row 0 of ( b ) is the same sentence as in ( a ) ( `` t w o k i t t e n s p l a y ` ' ) but with the omission of the `` w ` ' in `` t w o ` ' , the substitution of `` m ` ' for `` n ` ' in `` k i t t e",
    "n s ` ' , and the addition of `` x ` ' within the word `` p l a y ` ' . despite these errors ,",
    "the best multiple alignment created by the sp model is , as shown in ( b ) , the one that we judge intuitively to be ` correct ' .",
    "this kind of ability to cope gracefully with noisy data is very much in keeping with our ability to understand speech in noisy surroundings , to understand written language despite errors , and to recognise people , trees , houses , and the like , despite fog , snow , falling leaves , or other things that may obstruct our view . in a similar way , it is likely to prove useful in artificial systems for such applications as the processing of natural language and the recognition of patterns .      in considering the matching and unification of patterns ,",
    "it not hard to see that , for any body of information @xmath0 , except very small examples , there is a huge number of alternative ways in which patterns may be matched against each other , there will normally be many alternative ways in which patterns may be unified , and exhaustive search is not tractable ( _ bk _ , section 2.2.8.4 ) .",
    "however , with the kinds of heuristic techniques that are familiar in other ai applications  reducing the size of the search space by pruning the search tree at appropriate points , and being content with approximate solutions which are not necessarily perfect  this kind of matching becomes quite practical .",
    "much the same can be said about the heuristic techniques used for the building of multiple alignments ( section [ building_multiple_alignments_section ] ) and for unsupervised learning ( section [ ul_sp_model_section ] ) .",
    "for the process of building multiple alignments in the sp model , the time complexity in a serial processing environment , with conservative assumptions , has been estimated to be o@xmath13 , where @xmath14 is the size of the pattern from new ( in bits ) and @xmath15 is the sum of the lengths of the patterns in old ( in bits ) . in a parallel processing environment , the time complexity may approach o@xmath16 , depending on how well the parallel processing is applied . in serial and parallel environments ,",
    "the space complexity has been estimated to be o@xmath17 .",
    "although the data sets used with the current sp model have generally been small , there is reason to be confident that the models can be scaled up to deal with large data sets because the kind of flexible matching of patterns which is at the heart of the sp model is done very fast and with huge volumes of data by all the leading internet search engines . as was suggested in section",
    "[ sp_machine_section ] , the relevant processes in any one of those search engines would probably provide a good basis for the creation of a high - parallel version of the sp machine .      as described in _",
    "( chapter 7 ) , the formation of multiple alignments in the sp framework supports several kinds of probabilistic reasoning .",
    "the core idea is that any old symbol in a multiple alignment that is _ not _ aligned with a new symbol represents an inference that may be drawn from the multiple alignment .",
    "this section outlines how probabilities for such inferences may be calculated .",
    "there is more detail in _ bk _ ( section 3.7 ) .",
    "any sequence of @xmath18 symbols , drawn from an alphabet of @xmath19 alphabetic types , represents one point in a set of @xmath20 points where @xmath20 is calculated as :    @xmath21    _ if we assume that the sequence is random or nearly so _ , which means that the @xmath20 points are equi - probable or nearly so , the probability of any one point ( which represents a sequence of length @xmath18 ) is close to :    @xmath22    this equation may be used to calculate the absolute probability of the code pattern that may be derived from any given multiple alignment ( as described in section [ ma_evaluation_section ] ) .",
    "that number may also be regarded as the absolute probability of any inferences that may be drawn from the multiple alignment . in this calculation",
    ", @xmath18 is the sum of all the bits in the symbols of the code pattern and @xmath19 is 2 .",
    "as we shall see ( section [ generalisation_probabilities_section ] ) , equation [ pabs_equation ] may , with advantage , be generalised by replacing @xmath18 with a value , @xmath23 , calculated in a slightly different way .",
    "the absolute probabilities of multiple alignments , calculated as described in the last subsection , are normally very small and not very interesting in themselves . from the standpoint of practical applications",
    ", we are normally interested in the _ relative _ values of probabilities , calculated as follows .    1 .   for the multiple alignment which has the highest @xmath5 ( which we shall call the _ reference multiple alignment _ ) , identify the _ reference set of symbols in new _ , meaning the symbols from new which are encoded by the multiple alignment .",
    "2 .   compile a _ reference set of multiple alignments _ which includes the reference multiple alignment and all other multiple alignments ( if any ) which encode exactly the reference set of symbols from new , neither more nor less .",
    "3 .   calculate the sum of the values for @xmath24 in the reference set of multiple alignments : + @xmath25 + where @xmath26 is the size of the reference set of multiple alignments and @xmath27 is the value of @xmath24 for the @xmath9th multiple alignment in the reference set .",
    "4 .   for each multiple alignment in the reference set , calculate its relative probability as : + @xmath28    the values of @xmath29 , calculated",
    "as just described , provide an effective means of comparing the multiple alignments in the reference set .",
    "the value of @xmath18 , calculated as described in section [ absolute_probabilities_section ] , may be regarded as the informational ` cost ' of encoding the new symbol or symbols that appear in the multiple alignment , excluding those new symbols that have _ not _ appeared in the multiple alignment .",
    "this is ok but it is somewhat restrictive because it means that if we want to calculate relative probabilities for two or more multiple alignments they must all encode the same symbol or symbols from new .",
    "we can not easily compare multiple alignments that encode different new symbols .",
    "the generalisation proposed here is that , in the calculation of absolute probabilities , a new value , @xmath23 , would be used instead of @xmath18 .",
    "this would be calculated as :    @xmath30    where @xmath18 is the total number of bits in the symbols in the code patterns ( as in section [ absolute_probabilities_section ] ) and @xmath31 is the total number of bits in the new symbols that have _ not _ appeared in the multiple alignment .",
    "the rationale is that , to encode _ all _ the symbols in new , we can use the code pattern to encode those new symbols that do appear in the multiple alignment and , for each of the remaining new symbols , we can simply use its code .",
    "the advantage of this scheme is that we can compare any two or more multiple alignments , regardless of the number of new symbols that appear in the multiple alignment .",
    "it often happens that a given pattern from old , or a given symbol type within patterns from old , appears in more than one of the multiple alignments in the reference set . in cases like these",
    ", one would expect the relative probability of the pattern or symbol type to be higher than if it appeared in only one multiple alignment . to take account of this kind of situation ,",
    "the sp model calculates relative probabilities for individual patterns and symbol types in the following way :    1 .",
    "compile a set of patterns from old , each of which appears at least once in the reference set of multiple alignments .",
    "no single pattern from old should appear more than once in the set .",
    "2 .   for each pattern , calculate a value for its relative probability as the sum of the @xmath29 values for the multiple alignments in which it appears .",
    "if a pattern appears more than once in a multiple alignment , it is only counted once for that multiple alignment .",
    "3 .   compile a set of symbol types which appear anywhere in the patterns identified in step 2 .",
    "4 .   for each alphabetic symbol type identified in step 3 , calculate its relative probability as the sum of the relative probabilities of the patterns in which it appears .",
    "if it appears more than once in a given pattern , it is only counted once .",
    "the foregoing applies only to symbol types which do not appear in new .",
    "any symbol type that appears in new necessarily has a probability of @xmath32because it has been observed , not inferred .",
    "a potentially useful feature of the sp system is that the processes which serve to analyse or parse a new pattern in terms of old patterns , and to create an economical encoding of the new pattern , may also work in reverse , to recreate the new pattern from its encoding .",
    "this is the ` output ' perspective , mentioned in section [ introduction_to_sp_theory_section ] .",
    "if the new pattern is the code sequence `` s 0 1 0 1 0 # s ` ' ( as described in section [ multiple_alignment_section ] ) , and if the old patterns are the same as were used to create the multiple alignment shown in figure [ parsing_1_figure ] , then the best multiple alignment found by the system is the one shown in figure [ parsing_2_figure ] .",
    "this multiple alignment contains the same words as the original sentence ( `` t h i s b o y l o v e s t h a t g i r l ` ' ) , in the same order as the original .",
    "readers who are familiar with prolog , will recognise that this process of recreating the original sentence from its encoding is similar in some respects to the way in which an appropriately - constructed prolog program may be run ` backwards ' , deriving ` data ' from ` results ' .",
    "how is it possible to decompress the compressed code for the original sentence by using information compression ?",
    "this apparent paradox  decompression by compression  may be resolved by ensuring that , when a code pattern like `` s 0 1 0 1 0 # s ` ' is used to recreate the original data , each symbol is treated , at least notionally , as if contained a few more bits of information than is strictly necessary . that residual redundancy allows the system to recreate the original sentence by the same process of compression as was used to create the original parsing and encoding . ) , or in backup copies for data . ]",
    "this process of creating a relatively large pattern from a relatively small encoding provides a model for the creation of sentences by a person or an artificial system .",
    "but instead of the new pattern being a rather dry code , like `` s 0 1 0 1 0 # s ` ' , it would be more plausible if it were some kind of representation of the meaning of the sentence , like that mentioned in section [ ma_evaluation_section ] .",
    "how a sentence may be generated from a representation of meaning is outlined in _ bk _",
    "( section 5.7.1 ) .",
    "similar principles may apply to other kinds of ` output ' , such as planning an outing , cooking a meal , and so on .",
    "as was mentioned in section [ information_compression_section ] , part of the inspiration for the sp theory has been a programme of research developing models of the unsupervised learning of language .",
    "but although the snpr model @xcite is quite successful in deriving plausible grammars from samples of english - like artificial language , it has proved to be quite unsuitable as a basis for the sp theory . in order to accommodate other aspects of intelligence , such as pattern recognition , reasoning , and problem solving ,",
    "it has been necessary to develop an entirely new conceptual framework , with multiple alignment at centre stage .",
    "so there is now the curious paradox that , while the sp theory is rooted in work on unsupervised learning , and that kind of learning has a central role in the theory , the sp model does much the same things as the earlier model , and with similar limitations ( sections [ unfinished_business_section ] and [ sp_model_limitations_section ] ) .",
    "but i believe that the new conceptual framework has many advantages , that it provides a much sounder footing for further developments , and that with some reorganisation of the learning processes in the sp computer model , its current weaknesses may be overcome ( section [ sp_model_limitations_section ] ) .",
    "the outline of the sp model in this section aims to provide sufficient detail for a good intuitive grasp of how it works .",
    "a lot more detail may be found in _",
    "( chapter 9 ) .",
    "in addition to the processes for building multiple alignments , the sp model has processes for deriving old patterns from multiple alignments , evaluating sets of newly - created old patterns in terms of their effectiveness for the economical encoding of the new information , and the weeding out low - scoring sets .",
    "the system does not merely record statistical information , it uses that information to learn new structures .",
    "the process of deriving old patterns from multiple alignments is illustrated schematically in figure [ unsupervised_learning_figure ] . as was mentioned in section [ introduction_to_sp_theory_section ] ,",
    "the sp system is conceived as an abstract brain - like system that , in ` input ' mode , may receive ` new ' information via its senses and store some or all of it as ` old ' information . here",
    ", we may think of it as the brain of a baby who is listening to what people are saying .",
    "let s imagine that he or she hears someone say `` t h a t b o y r u n s ` . ' if the baby has never heard anything similar , then , if it is stored at all , that new information may be stored as a relatively straightforward copy , something like the old pattern shown in row 1 of the multiple alignment in part ( a ) of the figure .",
    "now let us imagine that the information has been stored and that , at some later stage , the baby hears someone say `` t h a t g i r l r u n s ` ' .",
    "then , from that new information and the previously - stored old pattern , a multiple alignment may be created like the one shown in part ( a ) of figure [ unsupervised_learning_figure ] . and , by picking out coherent sequences that are either fully matched or not matched at all , four putative words may be extracted : `` t h a t ` ' , `` g i r l ` ' , `` b o y ` ' , and `` r u n s ` ' , as shown in the first four patterns in part ( b ) of the figure . in each newly - created old pattern",
    "there are additional symbols such as `` b ` ' , `` 2 ` ' , and `` # b ` ' that are added by the system , and which serve to identify the pattern , to mark its boundaries , and to mark its grammatical category or categories .",
    "in addition to these four patterns , a fifth pattern is created , `` e 6 b # b c # c d # d # e ` ' , as shown in the figure , that records the sequence `` t h a t  ...  r u n s ` ' , with the category `` c # c ` ' in the middle representing a choice between `` b o y ` ' and `` g i r l ` ' . part ( b ) in the figure is the beginnings of a grammar to describe that kind of phrase .",
    "the example just described shows how old patterns may be derived from a multiple alignment but it gives a highly misleading impression of how the sp model actually works . in practice , the program forms many multiple alignments that are much less tidy than the one shown and it creates many old patterns that are clearly ` wrong '",
    "however , the program contains procedures for evaluating candidate sets of patterns ( ` grammars ' ) and weeding out those that score badly in terms of their effectiveness for encoding the new information economically . out of all the muddle , it can normally abstract one or two ` best ' grammars and these are normally ones that appear intuitively to be ` correct ' , or nearly so",
    ". in general , the program can abstract one or more plausible grammars from a sample of english - like artificial language , including words , grammatical categories of words , and sentence structure .    in accordance with the principles of minimum length encoding @xcite ,",
    "the aim of these processes of sifting and sorting is to minimise @xmath33 , where @xmath34 is the size ( in bits ) of the grammar that is under development and @xmath35 is the size ( in bits ) of the new patterns when they have been encoded in terms of the grammar .    for a given grammar comprising patterns @xmath36 ,",
    "the value of @xmath34 is calculated as :    @xmath37    where @xmath38 is the number of symbols in the @xmath9th pattern and @xmath39 is the encoding cost of the @xmath40th symbol in that pattern .",
    "given that each grammar is derived from a set @xmath41 of multiple alignments ( one multiple alignment for each pattern from new ) , the value of @xmath35 for the grammar is calculated as :    @xmath42    where @xmath43 is the size , in bits , of the code string derived from the @xmath9th multiple alignment ( section [ ma_evaluation_section ] ) .",
    "for a given set of patterns from new , a tree of alternative grammars is created with branching occurring wherever there are two or more alternative multiple alignments for a given pattern from new .",
    "the tree is grown in stages and pruned periodically to keep it within reasonable bounds . at each stage , grammars with high values for @xmath33 ( which will be referred to as @xmath44 ) are eliminated .",
    "figure [ plotting_figure ] shows cumulative values for @xmath34 , @xmath35 and @xmath44 as the sp model searches for good grammars for a succession of 8 new patterns , each of which represents a sentence .",
    "each point on each of the lower three graphs represents the relevant value ( on the scale at the left ) from the best grammar found after a given pattern from new has been processed .",
    "the graph labelled ` @xmath45 ' shows cumulative values on the scale at the left for the succession of new patterns .",
    "the graph labelled ` @xmath46 ' shows the amount of compression achieved ( on the scale to the right ) .    , @xmath35 and @xmath44 and related variables as learning proceeds , as described in the text.,scaledwidth=90.0% ]      as mentioned before ( section [ unfinished_business_section ] ) , there are two main weaknesses in the processes for unsupervised learning in the sp model as it is now : the model does not learn intermediate levels in a grammar ( phrases or clauses ) or discontinuous dependencies of the kind described in sections [ discontinuous_dependencies_section ] to [ aux_verb_2_section ] .",
    "it appears that some reorganisation of the learning processes in the model would solve both problems .",
    "what seems to be needed is a tighter focus on the principle that , with appropriately - constructed old patterns , multiple alignments may be created without the kind of mis - match between patterns that may be seen in figure [ unsupervised_learning_figure ] ( a ) ( `` g i r l ` ' and `` b o y ` ' do not match each other ) , and that any such multiple alignment may be treated as if it was a simple pattern .",
    "that reform should facilitate the discovery of structures at multiple levels and the discovery of structures that are discontinuous in the sense that they can bridge intervening structures .      as with the building of multiple alignments ( section [ ma_computational_complexity_section ] ) , the computational complexity of learning in the sp model",
    "is kept under control by pruning the search tree at appropriate points , aiming to discover grammars that are reasonably good and not necessarily perfect .    in a serial processing environment ,",
    "the time complexity of learning in the sp model has been estimated to be o@xmath47 where @xmath20 is the number of patterns in new . in a parallel processing environment",
    ", the time complexity may approach o@xmath48 , depending on how well the parallel processing is applied . in serial or parallel environments ,",
    "the space complexity has been estimated to be o@xmath48 .      in our dealings with the world ,",
    "certain kinds of structures appear to be more prominent and useful than others : in natural languages , there are words , phrase and sentences ; we understand the visual and tactile worlds to be composed of discrete ` objects ' ; and conceptually , we recognise classes of things like ` person ' , ` house ' , ` tree ' , and so on .",
    "it appears that these ` natural ' kinds of structure are significant in our thinking because they provide a means of compressing sensory information , and that compression of information provides the key to their learning or discovery . at first sight",
    ", this looks like nonsense because popular programs for compression of information , such as those based on the lzw algorithm , or programs for jpeg compression of images , seem not to recognise anything resembling words , objects , or classes .",
    "but those programs are designed to work fast on low - powered computers . with other programs that are designed to be relatively thorough in their compression of information",
    ", natural structures can be revealed :    * figure [ discovery_of_words_figure ] shows part of a parsing of an unsegmented sample of natural language text created by the mk10 program @xcite using only the information in the sample itself and without any prior dictionary or other knowledge about the structure of language .",
    "although all spaces and punctuation had been removed from the sample , the program does reasonably well in revealing the word structure of the text .",
    "statistical tests confirm that it performs much better than chance .",
    "* the same program does quite well  significantly better than chance  in revealing phrase structures in natural language texts that have been prepared , as before , without spaces or punctuation  but with each word replaced by a symbol for its grammatical category @xcite .",
    "although that replacement was done by a person trained in linguistic analysis , the discovery of phrase structure in the sample is done by the program , without assistance . * the snpr program for grammar discovery @xcite can ,",
    "without supervision , derive a plausible grammar from an unsegmented sample of english - like artificial language , including the discovery of words , of grammatical categories of words , and the structure of sentences .",
    "* in a similar way , with samples of english - like artificial languages , the sp model has demonstrated an ability to learn plausible structures including words , grammatical categories of words , and the structure of sentences .",
    "it seems likely that the principles that have been outlined in this subsection may be applied not only to the discovery of words , phrases and grammars in language - like data but also to such things as the discovery of objects in images @xcite , and classes of entity in all kinds of data .",
    "these principles may be characterised as _ the discovery of natural structures via information compression _ , or ` donsvic ' for short .",
    "issues that arise in the learning of a first language and , probably , in other kinds of learning , are illustrated in figure [ generalisation_figure ] :    * given that we learn from a finite sample , represented by the smallest envelope in the figure , how do we generalise from that finite sample to a knowledge of the language corresponding to the middle - sized envelope , without overgeneralising into the region between the middle envelope and the outer one ?",
    "* how do we learn a ` correct ' version of our native language despite what is marked in the figure as ` dirty data ' ( sentences that are not complete , false starts , words that are mis - pronounced , and more ) ?    . in ascending order size , they are : the finite sample of utterances from which a child learns ; the ( infinite ) set of utterances in @xmath18 ; and the ( infinite ) set of all possible utterances . adapted from figure 7.1 in @xcite , with permission.,scaledwidth=50.0% ]",
    "one possible answer is that mistakes are corrected by parents , teachers , and others .",
    "but the weight of evidence is that children can learn their first language without that kind of assistance .",
    "a better answer is the principle of minimum length encoding ( described in its essentials in section [ evaluating_and_selecting_section ] ) :    * as a general rule , the greatest reductions in @xmath33 are achieved with grammars that represent moderate levels of generalisation , neither too little nor too much . in practice , the snpr program , which is designed to minimise @xmath33 , has been shown to produce plausible generalisations , without over - generalising @xcite . * any particular error is , by its nature , rare and so in the search for useful patterns ( which , other things being equal , are the more frequently - occurring ones ) , it is discarded from the grammar along with other ` bad ' structures . in the case of lossless compression , errors in any given body of data , @xmath0 , would be retained in the encoding of @xmath0 . but with learning , it is normally the grammar and not the encoding that is the focus of interest . in practice ,",
    "the mk10 and snpr programs have been found to be quite insensitive to errors ( of omission , addition , or substitution ) in their data , much as in the building of multiple alignments ( section [ ma_noisy_data_section ] ) .      in many theories of learning ,",
    "the process is seen as gradual : behaviour is progressively shaped by rewards or punishments or other kinds of experience .",
    "but any theory of learning in which the process is necessarily gradual is out of step with our ordinary experience that we can and do learn things from a single experience , especially if that single experience is very significant for us ( _ bk _ , section 11.4.4.1 ) .    in the sp theory ,",
    "one - trial learning is accommodated in the way the system can store new information directly . and the gradual nature of , for example , language learning , may be explained by the complexity of the process of sifting and sorting the many alternative sets of candidate patterns to find one or more sets that are good in terms of information compression ( _ bk _ , section 11.4.4.2 ) .",
    "drawing mainly on _ bk _ ( chapters 4 to 11 ) , this and the following sections describe , with a selection of examples , how the sp theory relates to several areas in artificial intelligence , mainstream computing , and human perception and cognition .    in _ bk",
    "_ ( chapter 4 ) , i have argued that the sp system is equivalent to a universal turing machine @xcite , in the sense that anything that may be computed with a turing machine may , in principle , also be computed with an sp machine .",
    "the ` in principle ' qualification is necessary because the sp theory is still not fully mature and there are still some weaknesses in the sp computer models .",
    "the gist of the argument is that the operation of a post canonical system @xcite may be understood in terms of the sp theory and , since it is accepted that the post canonical system is equivalent to the turing machine ( as a computational system ) , the turing machine may also be understood in terms of the sp theory .",
    "the key differences between the sp theory and earlier theories of computing are that the sp theory has a lot more to say about the nature of intelligence than earlier theories , that the theory is founded on principles of information compression via the matching and unification of patterns ( ` computing as compression ' ) , and that it includes mechanisms for building multiple alignments and for heuristic search that are not present in earlier models .",
    "in conventional computing systems , compression of information may be seen in the matching of patterns with at least implicit unification of patterns that match each other  processes that appear in a variety of guises ( _ bk _ , chapter 2 ) . and three basic techniques for the compression of information_chunking - with - codes _ , _ schema - plus - correction _ , and _ run - length coding_may be seen in various forms in the organisation of computer programs ( _ ibid_. ) .      in a similar way , several structures and processes in mathematics and logic may be interpreted in terms of information compression via the matching and unification of patterns , and the compression techniques just mentioned ( _ bk _ , chapter 10 ) .",
    "for example , multiplication ( as repeated addition ) and exponentiation ( as repeated multiplication ) may be seen as examples of run - length coding ; a function with parameters may be seen as an example of schema - plus - correction ; the chunking - with - codes technique may be seen in the organisation of number systems ; and so on .",
    "as we have seen , the sp system is fundamentally probabilistic .",
    "if it is indeed turing - equivalent , as suggested above , and if the turing machine is regarded as a definition of ` computing ' , then we may conclude that computing is fundamentally probabilistic .",
    "that may seem like a strange conclusion in view of the clockwork certainties that we associate with the operation of ordinary computers and the workings of mathematics and logic .",
    "there are at least three answers to that apparent contradiction :    * it appears that computing , mathematics and logic are more probabilistic than our ordinary experience of them might suggest .",
    "gregory chaitin has written : `` i have recently been able to take a further step along the path laid out by gdel and turing . by translating a particular computer program into an algebraic equation of a type that was familiar even to the ancient greeks , i have shown that there is randomness in the branch of pure mathematics known as number theory .",
    "my work indicates that  to borrow einstein s metaphor ",
    "god sometimes plays dice with whole numbers . ''",
    "* the sp system may imitate the clockwork nature of ordinary computers by delivering probabilities of 0 and 1",
    ". this can happen with certain kinds of data , or tight constraints on the process of searching the abstract space of alternative matches , or both those things .",
    "* it seems likely that the all - or - nothing character of conventional computers has its origins in the low computational power of early computers . in those days , it was necessary to apply tight constraints on the process of searching for matches between patterns .",
    "otherwise , the computational demands would have been overwhelming .",
    "similar things may be said about the origins of mathematics and logic , which have been developed for centuries without the benefit of any computational machine , except very simple and low - powered devices .",
    "now that it is technically feasible to apply large amounts of computational power , constraints on searching may be relaxed .",
    "within the multiple alignment framework ( section [ multiple_alignment_section ] ) , sp patterns may serve to represent several kinds of knowledge , including grammars for natural languages , ; _ bk _ ( chapter 5 ) . ]",
    "ontologies , class hierarchies with inheritance of attributes , including cross - classification or multiple inheritance , part - whole hierarchies and their integration with class - inclusion hierarchies , ; _ bk _ ( section 6.4 ) . ] decision networks and trees , relational tuples , if - then rules , associations of medical signs and symptoms , causal relations , and concepts in mathematics and logic such as ` function ' , ` variable ' , ` value ' , ` set ' , and ` type definition . '    the use of one simple format for the representation of knowledge facilitates the seamless integration of different kinds of knowledge .",
    "one of the main strengths of the sp system is in natural language processing ( _ bk _ , chapter 5 ) :    * as illustrated in figures [ parsing_1_figure ] , [ noisy_data_recognition_figure ] and [ parsing_2_figure ] , grammatical rules , including words and their grammatical markers , may be represented with sp patterns . *",
    "both the parsing and production of natural language may be modelled via the building of multiple alignments ( section [ decompression_by_compression_section ] ; _ bk _ , section 5.7 ) .",
    "* the system can accommodate syntactic ambiguities in language ( _ bk _ , section 5.2 ) and also recursive structures ( _ bk _ , section 5.3 ) . *",
    "the framework provides a simple but effective means of representing discontinuous dependencies in syntax ( sections [ discontinuous_dependencies_section ] to [ aux_verb_2_section ] , below ; _ bk _ , sections 5.4 to 5.6 ) .",
    "* the system may also model non - syntactic ` semantic ' structures such as class - inclusion hierarchies and part - whole hierarchies ( section [ part - whole_class - inclusion_section ] ) . * because there is one simple format for different kinds of knowledge , the system facilitates the seamless integration of syntax with semantics ( _ bk _ , section 5.7 ) . *",
    "the system is robust in the face of errors of omission , commission or substitution in data ( sections [ ma_noisy_data_section ] and [ gen_overgen_noisy_section ] ) . *",
    "the importance of context in the processing of language @xcite is accommodated in the way the system searches for a global best match for patterns : any pattern or partial pattern may be a context for any other .",
    "the way in which the sp system can record discontinuous dependencies in syntax may be seen in both of the two parsings in figure [ noisy_data_recognition_figure ] . the pattern in row 8 of each multiple",
    "alignment records the syntactic dependency between the plural noun phrase ( `` t w o k i t t e n s ` ' ) which is the subject of the sentence  marked with `` np`'and the plural verb phrase ( `` p l a y`')marked with `` vp`'which belongs with it .",
    "this kind of dependency is discontinuous because it can bridge arbitrarily large amounts of intervening structure such as , for example , ` from the west ' in a sentence like ` winds from the west are strong ' .",
    "this method of marking discontinuous dependencies can accommodate overlapping dependencies such as number dependencies and gender dependencies in languages like french ( _ bk _ , section 5.4 ) .",
    "it also provides a means of encoding the interesting system of overlapping and interlocking dependencies in english auxiliary verbs , described by noam chomsky in _",
    "syntactic structures _ @xcite .    in that book ,",
    "the structure of english auxiliary verbs is part of chomsky s evidence in support of transformational grammar .",
    "despite the elegance and persuasiveness of his arguments , it turns out that the structure of english auxiliary verbs may be described with non - transformational rules in , for example , definite clause grammars @xcite , and also in the sp system , as outlined in the subsections that follow .      in english ,",
    "the syntax for main verbs and the auxiliary verbs which may accompany them follows two quasi - independent patterns of constraint which interact in an interesting way .",
    "the _ primary constraints _ may be expressed with this sequence of symbols ,    * *    .... m h b b v , ....    which should be interpreted in the following way :    * each letter represents a category for a single word : * * `` m ` ' stands for ` modal ' verbs like ` will ' , ` can ' , ` would ' etc .",
    "* * `` h ` ' stands for one of the various forms of the verb ` to have ' . * * each of the two instances of `` b ` ' stands for one of the various forms of the verb ` to be ' . * * `` v ` ' stands for the main verb which can be any verb except a modal verb ( unless the modal verb is used by itself ) . *",
    "the words occur in the order shown but any of the words may be omitted .",
    "* questions of ` standard ' form follow exactly the same pattern as statements except that the first verb , whatever it happens to be ( `` m ` ' , `` h ` ' , the first `` b ` ' , the second `` b ` ' or `` v ` ' ) , precedes the subject noun phrase instead of following it .    here",
    "are two examples of the primary pattern with all of the words included :    * *    .... it will have been being washed      m     h     b      b      v    will it have been being washed ?",
    "m        h     b      b      v ....    the _ secondary constraints _ are these :    * apart from the modals , which always have the same form , the first verb in the sequence , whatever it happens to be ( `` h ` ' , the first `` b ` ' , the second `` b ` ' or `` v ` ' ) , always has a ` finite ' form ( the form it would take if it were used by itself with the subject ) . *",
    "if an `` m ` ' auxiliary verb is chosen , then whatever follows it ( `` h ` ' , first `` b ` ' , second `` b ` ' , or `` v ` ' ) must have an ` infinitive ' form ( i.e. , the ` standard ' form of the verb as it occurs in the context ` to  ... ' , but without the word ` to ' ) . *",
    "if an `` h ` ' auxiliary verb is chosen , then whatever follows it ( the first `` b ` ' , the second `` b ` ' or `` v ` ' ) must have a past tense form such as ` been ' , ` seen ' , ` gone ' , ` slept ' , ` wanted ' etc .",
    "in chomsky s _ syntactic structures _",
    "@xcite , these forms were characterised as _ en _ forms and the same convention has been adopted here . * if the first of the two `` b ` ' auxiliary verbs is chosen , then whatever follows it ( the second `` b ` ' or `` v ` ' ) must have an _ ing _ form , e.g. , ` singing ' , ` eating ' , ` having ' , ` being ' etc . *",
    "if the second of the two `` b ` ' auxiliary verbs is chosen , then whatever follows it ( only the main verb is possible now ) must have a past tense form ( marked with _ en",
    "_ , as above ) .",
    "* the constraints apply to questions in exactly the same way as they do to statements .",
    "figure [ english_sentences_figure ] shows a selection of examples with the dependencies marked .      without reproducing all the detail in _ bk _ ( section 5.5 )",
    ", we can see from figures [ aux_verb_parsing_1_figure ] and [ aux_verb_parsing_2_figure ] how the primary and secondary constraints may be applied in the multiple alignment framework .    in each figure ,",
    "the sentence to be analysed is shown as a new pattern in column 0 .",
    "the primary constraints are applied via the matching of symbols in old patterns in the remaining columns , with a consequent interlocking of the patterns so that they recognise sentences of the form `` m h b b v ` ' , with options as described above .    in figure",
    "[ aux_verb_parsing_1_figure ] , . ]",
    "the secondary constraints apply as follows :    * the first verb , `` is ` ' , is marked as having the finite form ( with the symbol `` fin ` ' in columns 5 and 7 ) .",
    "the same word is also marked as being a form of the verb ` to be ' ( with the symbol `` b ` ' in columns 4 , 5 and 6 ) .",
    "because of its position in the parsing , we know that it is an instance of the second `` b ` ' in the sequence `` m h b b v ` ' . * the second verb , `` washed ` ' ,",
    "is marked as being in the _ en _ category ( with the symbol `` en ` ' in columns 1 and 4 ) .",
    "* that a verb corresponding to the second instance of `` b ` ' must be followed by an _",
    "en _ kind of verb is expressed by the pattern `` b xv en ` ' in column 4 .    in figure",
    "[ aux_verb_parsing_2_figure ] , the secondary constraints apply like this :    * the first verb `` will ` ' is marked as modal ( with `` m ` ' in columns 7 , 8 and 14 ) . * the second verb , `` have ` ' ,",
    "is marked as having the infinitive form ( with `` inf ` ' in columns 11 and 14 ) and it is also marked as a form of the verb ` to have ' ( with `` h ` ' in columns 11 , 12 , and 15 ) . * that a modal verb must be followed by a verb of infinitive form is marked with the pattern `` m inf ` ' in column 14 . * the third verb , `` been ` ' , is marked as being a form of the verb ` to be ' ( with `` b ` ' in columns 2 , 3 and 16 ) . because of its position in the parsing , we know that it is an instance of the second `` b ` ' in the sequence `` m h b b v ` ' .",
    "this verb is also marked as belonging to the _ en _ category ( with `` en ` ' in columns 2 and 15 ) .",
    "* that an `` h ` ' verb must be followed by an `` en ` ' verb is marked with the pattern `` h en ` ' in column 15 .",
    "* the fourth verb , `` broken ` ' , is marked as being in the _ en _ category ( with `` en ` ' in columns 4 and 16 ) .",
    "* that a `` b ` ' verb ( second instance ) must be followed by an `` en ` ' verb is marked with the pattern `` b xv en ` ' in column 16 .",
    "the system also has some useful features as a framework for pattern recognition ( _ bk _ , ( chapter 6 ) :    * it can model pattern recognition at multiple levels of abstraction , as described in _",
    "( section 6.4.1 ) , and with the integration of class - inclusion relations with part - whole hierarchies ( section [ part - whole_class - inclusion_section ] ; _ bk _ , section 6.4.1 ) .",
    "* the sp system can accommodate ` family resemblance ' or _",
    "categories , meaning that recognition does not depend on the presence absence of any particular feature or combination of features .",
    "this is because there can be alternatives at any or all locations in a pattern , and also because of the way the system can tolerate errors in data ( next point ) . *",
    "the system is robust in the face of errors of omission , commission or substitution in data ( section [ ma_noisy_data_section ] ) . *",
    "the system facilitates the seamless integration of pattern recognition with other aspects of intelligence : reasoning , learning , problem solving , and so on . *",
    "a probability may be calculated for any given identification , classification , or associated inference ( section [ ma_probabilities_section ] ) . * as in the processing of natural language ( section [ nl_processing_section ] ) , the importance of context in recognition @xcite is accommodated in the way the system searches for a global best match for patterns . as before , any pattern or partial pattern may be a context for any other .",
    "one area of application is medical diagnosis @xcite , viewed as pattern recognition",
    ". there is also potential to assist in the understanding of natural vision and in the development of computer vision , as discussed in @xcite .",
    "a strength of the multiple alignment concept is that it provides a simple but effective vehicle for the representation and processing of class - inclusion hierarchies , part - whole hierarchies , and their integration .",
    "figure [ class_hierarchy_figure ] shows the best multiple alignment found by the sp model with the new pattern `` white - bib eats furry purrs ` ' ( column 0 ) representing some features of an unknown creature , and with a set of old patterns representing different classes of animal , at varying levels of abstraction . from this multiple alignment",
    ", we may conclude that the unknown entity is an animal ( column 1 ) , a mammal ( column 2 ) , a cat ( column 3 ) and the specific individual ` tibs ' ( column 4 ) .",
    "the framework also provides for the representation of heterarchies or cross classification : a given entity , such as ` jane ' ( or a class of entities ) , may belong in two or more higher - level classes that are not themselves hierarchically related , such as ` woman ' and ` doctor . '    the way that class - inclusion relations and part - whole relations may be combined in one multiple alignment is illustrated in figure [ class_part_plant_figure ] . here",
    ", some features of an unknown plant are expressed as a set of new patterns , shown in column 0 : the plant has chlorophyll , the stem is hairy , it has yellow petals , and so on .    from this multiple alignment , we can see that the unknown plant is most likely to be the meadow buttercup , _ ranunculus acris _ , as shown in column 1 . as such",
    ", it belongs in the genus _ ranunculus _ ( column 6 ) , the family _ ranunculaceae _ ( column 5 ) , the order _ ranunculales _ ( column 4 ) , the class _ angiospermae _ ( column 3 ) , and the phylum _ plants _ ( column 2 ) .",
    "each of these higher - level classifications contributes information about attributes of the plant and its division into parts and sub - parts .",
    "for example , as a member of the class _ angiospermae _ ( column 3 ) , the plant has a shoot and roots , with the shoot divided into stem , leaves , and flowers ; as a member of the family _ ranunculaceae _ ( column 5 ) , the plant has flowers that are ` regular ' , with all parts ` free ' ; as a member of the phylum _ plants _ ( column 2 ) , the buttercup has chlorophyll and creates its own food by photosynthesis ; and so on .      in the example just described , we can infer from the multiple alignment , very directly , that the plant which has been provisionally identified as the meadow buttercup performs photosynthesis ( column 2 ) , has five petals ( column 6 ) , is poisonous ( column 5 ) , and so on . as in other object - oriented systems , the first of these attributes has been ` inherited ' from the class ` plants ' , the second from the class _ ranunculus _ , and the third from the class _ ranunculaceae_. these kinds of inference illustrate the close connection , often remarked , between pattern recognition and inferential reasoning ( see also * ? ? ?",
    "the sp system can model several kinds of reasoning including inheritance of attributes ( as just described ) , one - step ` deductive ' reasoning , abductive reasoning , reasoning with probabilistic decision networks and decision trees , reasoning with ` rules ' , nonmonotonic reasoning and reasoning with default values , reasoning in bayesian networks ( including ` explaining away ' ) , causal diagnosis , and reasoning which is not supported by evidence ( _ bk _ , chapter 7 ) .    since these several kinds of reasoning all flow from one computational framework ( multiple alignment )",
    ", they may be seen as aspects of one process , working individually or together without awkward boundaries .",
    "plausible lines of reasoning may be achieved , even when relevant information is incomplete .",
    "probabilities of inferences may be calculated , including extreme values ( 0 or 1 ) in the case of logic - like ` deductions ' .",
    "a selection of examples is described in the following subsections .",
    "conventional deductive reasoning is _ monotonic _ because deductions made on the strength of current knowledge can not be invalidated by new knowledge : the conclusion that `` socrates is mortal '' , deduced from `` all humans are mortal '' and `` socrates is human '' , remains true for all time , regardless of anything we learn later .",
    "by contrast , the inference that `` tweety can probably fly '' from the propositions that `` most birds fly '' and `` tweety is a bird '' is _ nonmonotonic _ because it may be changed if , for example , we learn that tweety is a penguin .",
    "this section presents some examples which show how the sp system can accommodate nonmonotonic reasoning .",
    "the idea that ( all ) birds can fly may be expressed with an sp pattern like `` bd bird name # name canfly warm - blooded wings feathers  ...  # bd ` ' .",
    "this , of course , is an oversimplification of the real - world facts because , while it true that the majority of birds fly , we know that there are also flightless birds like ostriches , penguins and kiwis .    in order to model these facts more closely , we need to modify the pattern that describes birds to be something like this :  `",
    "bd bird name # name f # f warm - blooded wings feathers  ...  # bd ` .",
    "and , to our database of old patterns , we need to add patterns like this :    * *    .... default bd f canfly # f # bd # default p penguin bd f cannotfly # f # bd ... # p o ostrich bd f cannotfly # f # bd ... # o. ....    now , the pair of symbols `` f # f ` ' in `` bd bird name # name f # f warm - blooded wings feathers  ...  # bd ` ' functions like a ` variable ' that may take the value `` canfly ` ' if a given class of birds can fly and `` cannotfly ` ' when a type of bird can not fly .",
    "the pattern `` p penguin bd f cannotfly # f # bd  ...  # p ` ' shows that penguins can not fly and , likewise , the pattern `` o ostrich bd f cannotfly # f # bd  ...  # o ` ' shows that ostriches can not fly . the pattern `` default bd f canfly # f # bd # default ` ' , which has a substantially higher frequency than the other two patterns , represents the default value for the variable which is `` canfly ` ' .",
    "notice that all three of these patterns contains the pair of symbols `` bd  ...  # bd ` ' showing that the corresponding classes are all subclasses of birds .",
    "when the sp model is run with `` bird tweety ` ' in new and the same patterns in old as before , modified as just described , the three best multiple alignments found are those shown in figures [ nonmon_figure_1a_figure ] , [ nonmon_figure_1b_figure ] and [ nonmon_figure_1c_figure ] .",
    "the first multiple alignment tells us that , with a relative probability of 0.66 , tweety may be the typical kind of bird that can fly .",
    "the second multiple alignment tells us that , with a relative probability of 0.22 , tweety might be an ostrich and , as such , he or she would not be able to fly .",
    "likewise , the third multiple alignment tells us that , with a relative probability of 0.12 , tweety might be a penguin and would not be able to fly .",
    "the values for probabilities in this simple example are derived from guestimated frequencies that are , almost certainly , not ornithologically correct .",
    "figure [ nonmon_figure_2_figure ] shows the best multiple alignment found by the sp model when it is run again , with `` penguin tweety ` ' in new instead of `` bird tweety ` ' .",
    "this time , there is only one multiple alignment in the reference set and its relative probability is 1.0 .",
    "correspondingly , all inferences that we can draw from this multiple alignment have a probability of 1.0 . in particular , we can be confident , within the limits of the available knowledge , that tweety can not fly .    in a similar way ,",
    "if tweety were an ostrich , we would be able to say with confidence ( p = 1.0 ) that he or she would not be able to fly .",
    "a bayesian network is a directed , acyclic graph like the one shown in figure [ alarm_bayesian_network_figure ] , below , where each node has zero or more ` inputs ' ( connections with nodes that can influence the given node ) and one or more ` outputs ' ( connections to other nodes that the given node can influence ) .",
    "each node contains a set of conditional probability values , each one the probability of a given output value for a given input value or combination of input values . with this information ,",
    "conditional probabilities of alternative outputs for any node may be computed for any given _ combination _ of inputs . by combining these calculations for sequences of nodes",
    ", probabilities may be propagated through the network from one or more ` start ' nodes to one or more ` finishing ' nodes .",
    "this section describes how the sp system may perform that kind of probabilistic reasoning , and some advantages compared with bayesian networks .",
    "judea pearl @xcite describes the phenomenon of ` explaining away ' like this : `` if a implies b , c implies b , and b is true , then finding that c is true makes a _ less _ credible . in other words ,",
    "finding a second explanation for an item of data makes the first explanation less credible . ''",
    "( his italics ) .",
    "here is an example :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ normally an alarm sound alerts us to the possibility of a burglary .",
    "if somebody calls you at the office and tells you that your alarm went off , you will surely rush home in a hurry , even though there could be other causes for the alarm sound .",
    "if you hear a radio announcement that there was an earthquake nearby , and if the last false alarm you recall was triggered by an earthquake , then your certainty of a burglary will diminish .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the causal relationships in the example just described may be captured in a bayesian network like the one shown in figure [ alarm_bayesian_network_figure ] .",
    "pearl argues that , with appropriate values for conditional probabilities , the phenomenon of `` explaining away '' can be explained in terms of this network ( representing the case where there is a radio announcement of an earthquake ) compared with the same network without the node for `` radio announcement '' ( representing the situation where there is no radio announcement of an earthquake ) .      to see how this phenomenon may be understood in terms of the sp theory , consider , first , the set of patterns shown in figure [ alarm_patterns_figure ] , which are to be stored in old . the patterns in the figure show events which occur together in some notional sample of the ` world ' together with their frequencies of occurrence in the sample .    as with other knowledge - based systems",
    ", we shall assume that the ` closed - world ' assumption applies so that the absence of any pattern may be taken to mean that the corresponding combination of events did not occur in the period when observations were made .    the first pattern ( `` burglary alarm ` ' ) shows that there were 1000 occasions when there was a burglary and the alarm went off and the second pattern ( `` earthquake alarm ` ' ) shows just 20 occasions when there was an earthquake and the alarm went off ( presumably triggered by the earthquake ) .",
    "thus we have assumed that , as triggers for the alarm , burglaries are much more common than earthquakes .",
    "since there is no pattern showing that the alarm sounded when there was a burglary and an earthquake at the same time , we may assume , via the closed - world assumption , that nothing like that happened during the sampling period .",
    "the third pattern ( `` alarm phone_alarm_call ` ' ) shows that , out of the 1020 cases when the alarm went off , there were 980 cases where a phone call about the alarm was made . since there is no pattern showing phone calls about the alarm in any other context , the closed - world assumption allows us to assume that there were no false positives ( eg .",
    ", hoaxes)phone calls about the alarm when no alarm had sounded .    the fourth pattern ( `` earthquake radio_earthquake_announcement ` ' ) shows that , in the sampling period , there were 40 occasions when there was an earthquake with an announcement about it on the radio . and the fifth pattern ( `` e1 earthquake e2 ` ' ) shows that an earthquake has occurred on 40 occasions in contexts where the alarm did not ring and there was no radio announcement .",
    "are intended to reflect the two probabilities suggested for this example in @xcite : `` ... the [ alarm ] is sensitive to earthquakes and can be accidentally ( p = 0.20 ) triggered by one . ...",
    "if an earthquake had occurred , it surely ( p = 0.40 ) would be on the [ radio ] news . '' ]    as before , the absence of patterns like `` earthquake alarm radio_earthquake_announcement ` ' representing cases where an earthquake triggers the alarm and also leads to a radio announcement , allows us to assume via the closed - world assumption that cases of that kind have not occurred in the sampling period .      in these patterns and in the multiple alignments shown below",
    ", the left - to - right order of symbols may be regarded as an approximation to the order of events in time .",
    "thus in the first two patterns , events that can trigger an alarm precede the sounding of the alarm .",
    "likewise , in the third pattern , `` alarm ` ' ( meaning that the alarm has sounded ) precedes `` phone_alarm_call ` ' ( a phone call to say the alarm has sounded ) . a single dimension can only approximate the order of events in time because it can not represent events which overlap in time or which occur simultaneously .",
    "however , this kind of approximation has little or no bearing on the points to be illustrated here .",
    "other points relating to the patterns shown in figure [ alarm_patterns_figure ] include :    * no attempt has been made to represent the idea that `` the last false alarm you recall was triggered by an earthquake '' @xcite . at some stage in the development of the sp system , there will be a need to take account of recency ( _ bk _ , section 13.2.6 ) . * with these imaginary frequency values",
    ", it has been assumed that burglaries ( with a total frequency of occurrence of 1160 ) are much more common than earthquakes ( with a total frequency of 100 ) . as we shall see , this difference reinforces the belief that there has been a burglary when it is known that the alarm has gone off ( but without additional knowledge of an earthquake ) . * in accordance with pearl s example ( p. 49 ) ( but contrary to the phenomenon of looting during earthquakes ) , it has been assumed that earthquakes and burglaries are independent .",
    "if there was some association between them , then , in accordance with the closed - world assumption , there should be a pattern in figure [ alarm_patterns_figure ] representing the association .      receiving a phone call to say that the burglar alarm at one s house has gone off may be represented by placing the symbol `` phone_alarm_call ` ' in new .",
    "figure [ alarm_alignments_1_figure ] shows , at the top , the best multiple alignment formed by the sp model in this case , with the patterns from figure [ alarm_patterns_figure ] in old . the other two multiple alignments in the reference set are shown below the best multiple alignment , in order of cd value and relative probability .",
    "the actual values for @xmath5 and relative probability are given in the caption to figure [ alarm_patterns_figure ] .",
    "the unmatched old symbols in these multiple alignments represent inferences made by the system .",
    "the probabilities for these inferences which are calculated by the sp model ( as outlined in section [ ma_probabilities_section ] ) are shown in table [ symbol_probabilities_table ] .",
    "these probabilities do not add up to 1 and we should not expect them to because any given multiple alignment may contain two or more of these symbols .",
    "the most probable inference is the rather trivial inference that the alarm has indeed sounded .",
    "this reflects the fact that there is no pattern in figure [ alarm_patterns_figure ] representing false positives for telephone calls about the alarm .",
    "apart from the inference that the alarm has sounded , the most probable inference ( p = 0.328 ) is that there has been a burglary .",
    "however , there is a distinct possibility that there has been an earthquake  but the probability in this case ( p = 0.016 ) is much lower than the probability of a burglary .",
    ".the probabilities of unmatched old symbols , calculated by the sp model for the three multiple alignments shown in figure [ alarm_alignments_1_figure ] . [ cols= \" <",
    ", < \" , ]     it is interesting to see that the best diagnosis derived by the sp model ( @xmath49 is bad ) and the second best diagnosis ( @xmath50 is bad ) are in accordance with first two diagnoses obtained by pearl s method .",
    "the remaining five diagnoses derived by the sp model are different from the one obtained by pearl s method ( @xmath51 and @xmath52 are bad ) but this is not altogether surprising because detailed frequencies or probabilities are different from pearl s example and there are differences in assumptions that have been made .",
    "the sp theory provides a versatile model for database systems , with the ability to accommodate object - oriented structures , as well as relational ` tuples ' , and network and tree models of data @xcite .",
    "it lends itself most directly to information retrieval in the manner of query - by - example but it appears to have potential to support the use of natural language or query languages such as sql .    unlike some ordinary database systems :    *",
    "the storage and retrieval of information is integrated with other aspects of intelligence such as pattern recognition , reasoning , planning , problem solving , and learning  as outlined elsewhere in this article .",
    "* the sp system provides a simple but effective means of combining class hierarchies with part - whole hierarchies , with inheritance of attributes ( section [ part - whole_class - inclusion_section ] ) . *",
    "it provides for cross - classification with multiple inheritance .",
    "* there is flexibility and versatility in the representation of knowledge arising from the fact that the system does not distinguish ` parts ' and ` attributes ' ( * ? ? ?",
    "* section 4.2.1 ) . * likewise , the absence of a distinction between ` class ' and ` object ' facilitates the representation of knowledge and eliminates the need for a ` metaclass ' ( * ? ? ?",
    "* section 4.2.2 ) . *",
    "sp patterns provide a simpler and more direct means of representing entity - relationship models than do relational tuples ( * ? ? ?",
    "* section 4.2.3 ) .",
    "the sp framework provides a means of planning a route between two places , and , with the translation of geometric patterns into textual form , it can solve the kind of geometric analogy problem that may be seen in some puzzle books and iq tests ( _ bk _ , chapter 8) .",
    "figure [ geometric_analogy_figure ] shows an example of the latter kind of problem . the task is to complete the relationship ``",
    "a is to b as c is to ? '' using one of the figures ` d ' , ` e ' , ` f ' or ` g ' in the position marked with ` ? ' . for this example , the ` correct ' answer is clearly ` e ' .",
    "quote marks have been used for the word ` correct ' because in many problems of this type , there may be two or even more alternative answers for which cases can be made and there is a corresponding uncertainty about which answer is the right one .",
    "computer - based methods for solving this kind of problem have existed for some time ( e.g. , evans @xcite well - known heuristic algorithm ) . in more recent work @xcite ,",
    "minimum length encoding principles have been applied to good effect .",
    "this kind of problem may also be understood in terms of the sp concepts .        as in most previous work",
    ", the proposed solution assumes that some mechanism is available which can translate the geometric forms in each problem into patterns of text symbols like other patterns in this article .",
    "for example , item ` a ' in figure [ geometric_analogy_figure ] may be described as `` small circle inside large triangle ` ' .    how this kind of translation may be done is not part of the present proposals ( one such translation mechanism is described in @xcite ) .",
    "as noted elsewhere @xcite , successful solutions for this kind of problem require consistency in the way the translation is done .",
    "for this example , it would be unhelpful if item ` a ' in figure [ geometric_analogy_figure ] were described as `` large triangle outside small circle ` ' while item ` c ' were described as `` small square inside large ellipse ` ' .",
    "for any one puzzle , the description needs to stick to one or other of ` x outside y ' or ` y inside x'and likewise for ` above / below ' and ` left - of / right - of ' .",
    "given that the diagrammatic form of the problem has been translated into patterns as just described , this kind of problem can be cast as a problem of partial matching , well within the scope of the sp model . to do this ,",
    "symbolic representations of item a and item b in figure [ geometric_analogy_figure ] are treated as a single pattern , thus :    * *    .... small circle inside large triangle ;       large circle above small triangle ....    and this pattern is placed in new .",
    "four other patterns are constructed by pairing a symbolic representation of item c ( on the left ) with symbolic representations of each of d , e , f and g ( on the right ) , thus :    * *    .... c1 small square inside large ellipse ;       d small square inside large circle # c1 c2 small square inside large ellipse ;       e large square above small ellipse # c2 c3 small square inside large ellipse ;       f small circle left - of large square # c3 c4 small square inside large ellipse ;       g small ellipse above large rectangle # c4 .",
    "....    these four patterns are placed in old , each with an arbitrary frequency value of 1 .",
    "figure [ geometric_analogy_alignment_figure ] shows the best multiple alignment found by the sp model with new and old as just described . the multiple alignment is a partial match between the new pattern ( in column 0 ) and the second of the four patterns in old ( in column 1 )",
    "this corresponds with the ` correct ' result ( item e ) as noted above .",
    "since information compression is central to the workings of the sp system , it is natural to consider whether the system might provide useful insights in that area . in that connection",
    ", the most promising aspects of the sp system appear to be :    * the discovery of recurrent patterns in data via the building of multiple alignments , with heuristic search to sift out the patterns that are most useful in terms of compression . * the potential of the system to detect and encode discontinuous dependencies in data .",
    "it appears that there is potential here to extract kinds of redundancy in information that are not accessible via standard methods for the compression of information .    in terms of the trade - off that exists between computational resources that are required and the level of compression that can be achieved",
    ", it is intended that the system will operate towards the ` up market ' end of the spectrum  by contrast with lzw algorithms and the like , which have been designed to be ` quick - and - dirty ' , sacrificing performance for speed on low - powered computers .",
    "since much of the inspiration for the sp theory has come from evidence , mentioned in section [ information_compression_section ] , that , to a large extent , the workings of brains and nervous systems may be understood in terms of information compression , the theory is about perception and cognition as well as artificial intelligence and mainstream computing .",
    "that said , the main elements of the theory  the multiple alignment concept in particular  are theoretical constructs derived from what appears to be necessary to model , in an economical way , such things as pattern recognition , reasoning , and so on . in _",
    "( chapter 12 ) , there is some discussion of how the sp concepts relate to a selection of issues in human perception and cognition .",
    "a particular interest at the time of writing ( after that chapter was written ) is the way that the sp theory may provide an alternative to quantum probability as an explanation of phenomena such as the ` conjunction fallacy ' ( see , for example , * ? ? ?",
    "* ) .    in _ bk",
    "_ ( chapter 11 ) , i have described in outline , and tentatively , how such things as sp patterns and multiple alignments may be realised with neurons and connections between them . the cortex of the brains of mammals  which is , topologically , a two - dimensional sheet  may be , in some respects , like a sheet of paper on which _ pattern assemblies _ may be written .",
    "these are neural analogues of sp patterns , shown schematically in figure [ neural_analogue_figure ] . unlike information written on a sheet of paper , there are neural connections between patterns  as shown in the figure  and communications amongst them .",
    "these proposals , which are adapted with modifications from hebb s @xcite concept of a ` cell assembly ' , are very different from how artificial ` neural networks ' are generally conceived in computer science . , retrieved 2013 - 05 - 10 . ] as noted in section [ one - trial_learning_section ] , learning in the sp system is very different from learning in that kind of network  or hebbian learning .",
    "the sp theory aims to simplify and integrate concepts across artificial intelligence , mainstream computing and human perception and cognition , with information compression as a unifying theme .",
    "the matching and unification of patterns and the concept of multiple alignment are central ideas .    in accordance with occam s razor ,",
    "the sp system combines conceptual simplicity with descriptive and explanatory power .",
    "a relatively simple mechanism provides an interpretation for a range of concepts and phenomena in several areas including conepts of ` computing ' , aspects of mathematics and logic , representation of knowledge , natural language processing , pattern recognition , several kinds of probabilistic reasoning , information storage and retrieval , planning and problem solving , information compression , neuroscience , and human perception and cognition .",
    "h.  b. barlow .",
    "sensory mechanisms , the reduction of redundancy , and intelligence . in hmso ,",
    "editor , _ the mechanisation of thought processes _ , pages 535559 . her majesty s stationery office , london , 1959 .",
    "t.  belloti and a.  gammerman .",
    "experiments in solving analogy problems using minimal length encoding . in _ proceedings of stream 1 : `` computational learning and probabilistic reasoning '' , applied decision technologies , brunel university , april 1995 _ , 1996 .                t.  g. evans . a program for the solution of a class of geometric - analogy intelligence - test questions . in m.",
    "l. minsky , editor , _ semantic information processing _ , pages 271353 . mit press , cambridge mass . ,",
    "1968 .",
    "m.  hutter .",
    "_ universal artificial intelligence : sequential decisions based on algorithmic probability_. springer , berlin , 2005 .",
    "isbn 3 - 540 - 22139 - 5 , http://www.hutter1.net/ai/uaibook.htm[www.hutter1.net/ai/uaibook.htm ] .",
    "a.  newell .",
    "you ca nt play 20 questions with nature and win : projective comments on the papers in this symposium . in w.",
    "g. chase , editor , _ visual information processing _ ,",
    "pages 283308 . academic press , new york , 1973 .",
    "f.  c.  n. pereira and d.  h.  d. warren .",
    "efinite clause grammars for language analysis - a survey of the formalism and a comparison with augmented transition networks .",
    "_ artificial intelligence _ , 13:0 231278 , 1980 .",
    "j.  schmidhuber , k.  r. thrisson , and m.  looks , editors .",
    "_ proceedings of the fouth international conference on artificial general intelligence ( agi 2011 ) _ , volume 6830 of _ lecture notes in artificial intelligence _ , 2011 .",
    "isbn 978 - 3 - 642 - 22886 - 5 .",
    "j.  g. wolff . learning syntax and meanings through optimization and distributional analysis . in y.  levy , i.  m. schlesinger , and m.  d.  s. braine , editors , _ categories and processes in language acquisition _ ,",
    "pages 179215 .",
    "lawrence erlbaum , hillsdale , nj , 1988 .",
    "see http://bit.ly/zigjyc[bit.ly/zigjyc ] .",
    "j.  g. wolff .",
    "_ unifying computing and cognition : the sp theory and its applications_. cognitionresearch.org , menai bridge , 2006 .",
    "isbns : 0 - 9550726 - 0 - 3 ( ebook edition ) , 0 - 9550726 - 1 - 1 ( print edition ) .",
    "distributors , including amazon.com , are detailed on http://bit.ly/wmb1rs[bit.ly/wmb1rs ] .",
    "j.  g. wolff .",
    "medical diagnosis as pattern recognition in a framework of information compression by multiple alignment , unification and search . _ decision support systems _ , 42:0 608625 , 2006 . see http://bit.ly/xe7prg[bit.ly/xe7prg ] .",
    "j.  g. wolff .",
    "the sp theory and the representation and processing of knowledge . in z.",
    "ma , editor , _ soft computing in ontologies and semantic web _ , pages 79101 .",
    "springer - verlag , heidelberg , 2006 .",
    "see http://bit.ly/zfrg4v[bit.ly/zfrg4v ] .",
    "j.  g. wolff .",
    "application of the sp theory of intelligence to the understanding of natural vision and the development of computer vision .",
    "2013 . in preparation .",
    "see http://bit.ly/xj3ndy[bit.ly/xj3ndy ] ( pdf ) ."
  ],
  "abstract_text": [
    "<S> this article is an overview of the _ sp theory of intelligence _ , which aims to simplify and integrate concepts across artificial intelligence , mainstream computing and human perception and cognition , with information compression as a unifying theme . </S>",
    "<S> it is conceived as a brain - like system that receives ` new ' information and stores some or all of it in compressed form as ` old ' information ; and it is realised in the form of a computer model , a first version of the _ </S>",
    "<S> sp machine_. the matching and unification of patterns and the concept of _ multiple alignment _ are central ideas . using heuristic techniques , the system builds multiple alignments that are ` good ' in terms of information compression . for each multiple alignment </S>",
    "<S> , probabilities may be calculated for associated inferences . </S>",
    "<S> unsupervised learning is done by deriving new structures from partial matches between patterns and via heuristic search for sets of structures that are ` good ' in terms of information compression . </S>",
    "<S> these are normally ones that people judge to be ` natural ' , in accordance with the ` donsvic ' principle  the discovery of natural structures via information compression . </S>",
    "<S> the sp theory provides an interpretation for concepts and phenomena in several other areas including ` computing ' , aspects of mathematics and logic , the representation of knowledge , natural language processing , pattern recognition , several kinds of reasoning , information storage and retrieval , planning and problem solving , information compression , neuroscience , and human perception and cognition . </S>",
    "<S> examples include the parsing and production of language with discontinuous dependencies in syntax , pattern recognition at multiple levels of abstraction and its integration with part - whole relations , nonmonotonic reasoning and reasoning with default values , reasoning in bayesian networks including ` explaining away ' , causal diagnosis , and the solving of a geometric analogy problem .    </S>",
    "<S> _ keywords _ : information compression , artificial intelligence , multiple alignment , computing , representation of knowledge , natural language processing , pattern recognition , information retrieval , probabilistic reasoning , planning , problem solving , unsupervised learning . </S>"
  ]
}