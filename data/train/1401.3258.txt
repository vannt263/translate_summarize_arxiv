{
  "article_text": [
    "in the study of networks , the data used to define nodes and connections often come from multiple sources .",
    "these sources generally have nontrivial levels of noise and ambiguous utility , and the process of combining them into a single graph representation is critically important .",
    "for example , suppose we are studying a social network and wish to detect communities .",
    "the data that indicate membership in the same community are plentiful : communication data , co - authorship , reported friendship , and many others .",
    "each of these associations carries different levels of information about the underlying social structure , and each may accurately represent only some of the individuals .",
    "some groups of friends communicate primarily through facebook and others via instagram , etc . the best way to amalgamate",
    "this information is far from clear , and recent research has demonstrated the impact of graph representation on the performance of machine learning algorithms @xcite .    further complicating matters ,",
    "the quality of the aggregated graph depends heavily on the application domain . a graph representation that retains only edges within communities is conducive for community detection , but some cross - community edges are critical to predict the spread of a virus .",
    "the best graph representations for these two tasks may come from the same data sources but are qualitatively different .",
    "even though the impact of the graph representation on subsequent analysis has been widely studied , techniques for learning the right graph representations are lacking .",
    "current practices often aggregate different graph sources ad - hoc , making it difficult to compare algorithms across application domains or even within the same domain using different data sources .",
    "the immediacy for rigorous approaches on representation learning of graphs is even more apparent in the big data regime , where challenges connected to variety and veracity exacerbate the challenges of volume and velocity .    in this paper",
    ", we present a graph aggregation framework designed to make the process of learning the underlying graph representation rigorous with respect to application specific requirements .",
    "our framework is called _ locally boosted graph aggregation ( lbga)_. lbga extracts the application - specific aspects of the learning objective as an event @xmath0 representing an operation on the graph ( e.g. a clustering algorithm , a random walk , etc . ) and a local quality measure @xmath1 .",
    "the framework then incorporates this information into a reward system that promotes the presence of good edges and the absence of bad edges , in a fashion inspired by boosting literature .",
    "we demonstrate lbga with the application of community detection . in this context",
    "the goal of graph representation learning is to aggregate the different data sources into a single graph which makes the true community structure easy to detect .",
    "lbga evaluates the graph data locally , so that it can choose the data sources which most accurately represent the local structure of communities observed in real networks  @xcite . in the absence of ground truth knowledge or one efficiently computable measure that can capture true community quality",
    ", lbga relies on the pair of a graph clustering algorithm @xmath0 and a local clustering metric @xmath1 as an evaluation proxy .",
    "we show through empirical analysis that our algorithm can learn a high - quality global representation guided by the local quality measures considered .",
    "we make the following contributions :    1 .",
    "we present an aggregation framework the learns a useful graph representation with respect to an application requiring only a local heuristic measure of quality to operate .",
    "2 .   our framework incorporates both edge and non - edge information , making it robust and suitable for sparse , noisy real - world networks .",
    "we demonstrate the success of our algorithm with respect to community detection , testing it against both synthetic and real data .",
    "we describe how the result of our algorithm can be used to compare the utility and quality of the data sources used .",
    "the rest of the paper is organized as follows . in section",
    "[ sec : related ] we give a brief overview of related literature . in section",
    "[ sec : lbga ] we discuss in detail the lbga framework . in section  [ sec : experiments ] we present the experimental analysis and results .",
    "representation learning has garnered a lot of interest and research in recent years .",
    "its goal is to introduce more rigor and formalism to the often ad - hoc practices of transforming raw , noisy , multi - source data into inputs for data mining and machine learning algorithms . within this area ,",
    "representation learning of graph - based data includes modeling decisions about the nodes of the graph , the edges , as well as the critical features that characterize them both .    in this context ,",
    "rossi et al .",
    "@xcite discuss transformations to heterogeneous graphs ( graphs with multiple node types and/or multiple edge types ) in order to improve the quality of a learning algorithm such as community detection or link prediction . within their taxonomy",
    ", our work falls under the link interpretation and link re - weighting algorithms @xcite .",
    "our setting is different because we explicitly allow different edge types between the same pair of vertices . also ,",
    "our approach is stochastic , which we find necessary for learning a robust representation and weeding out noise .",
    "clustering in multi - edge graphs @xcite is another area with close connections to our work .",
    "a common thread among these existing approaches is clustering by leveraging shared information across different graph representations of the same data .",
    "these approaches do not address scenarios where the information provided by the different sources is complementary or the overlap is scarce .",
    "in contrast , our approach iteratively selects those edge sources that lead to better clustering quality , independently of disagreement across the different features .",
    "@xcite present approaches for identifying the right graph aggregation , given a complete ground truth clustering , or a portion of it ( i.e. : the cluster assignment is known only for a subset of the vertices in the graph ) .",
    "our framework requires no such knowledge , but we do use ground truth to validate our experiments on synthetic data ( section [ sec : validation ] ) .",
    "balcan and blum present in @xcite a list of intuitive properties a similarity function needs to have in order to be able to cluster well .",
    "however , testing whether a similarity function has the discussed properties is np - hard , and often dependent on having ground truth available .",
    "our model instead uses an efficiently computable heuristic as a rough guide .",
    "our framework is related to both boosting @xcite and bandit learning techniques ( see @xcite for an overview ) . in boosting , we assume we have a collection of _ weak learners _ for classification , whose performance is only slightly better than random . in his seminal paper @xcite",
    ", schapire showed that such learners can be combined to form an arbitrarily strong learner .",
    "we think of different data sources as weak learners in that they offer knowledge on when an edge should be present .",
    "then the question becomes whether one can `` boost '' the knowledge in the different graphs to make one graph representation that is arbitrarily good .",
    "unfortunately , our problem setting does not allow pure boosting .",
    "first , boosting assumes the learners are equally good ( in the sense that they are all slightly better than random ) ; but graph representations can be pure noise or can even provide _ bad _ advice . and second , boosting has access to ground truth .",
    "even if we had graph representations that were all `` good , '' the quality changes based on the application and many applications have no standard measure of quality .",
    "our second inspiration , bandit learning , compensates for these issues . in",
    "bandit learning an algorithm receives rewards as it explores a set of actions , and the goal is to compete against the best action in hindsight ( minimizing some notion of regret ) .",
    "the model has many variants , but two ubiquitous features are expert advice and adversaries .",
    "expert advice consists of functions suggesting to the algorithm what action to take in each round .",
    "the adversarial setting involves an adversary who knows everything but the random choices made by the algorithm in advance , and sets the experts or rewards so as to incur the largest regret .",
    "the similarity to graph representation learning is clear : we have a set of graphs giving potentially bad advice about their edges and we can set up an artificial reward system based on our application . in our setting",
    "we only care if the graph representation is good at the end , while bandit learning often seeks to maximize cumulative rewards during learning .",
    "there are bandit settings that only care about the final result ( e.g. , the pure exploration model of bubeck et al .",
    "@xcite ) , but to the best of our knowledge no theoretical results in the bandit literature immediately apply to our framework .",
    "this is largely because we rely on heuristic proxies to measure the quality of a graph , so even if the bandit learning objective is optimized we can not guarantee the result is useful .",
    "nevertheless we can adapt the successful techniques and algorithms for boosting and bandit learning , and hope they produce useful graphs in practice .",
    "as the rest of this paper demonstrates , they do indeed .",
    "the primary technique we adapt from bandits and boosting is the multiplicative weights update algorithm ( mwua ) @xcite .",
    "the algorithm works as follows .",
    "a list of weights is maintained on each element @xmath2 of a finite set @xmath3 . at each step of some process an element @xmath4",
    "is chosen ( in our case , by normalizing the weights to a probability distribution and sampling ) , a reward @xmath5 is received , and the weight for @xmath4 is multiplied or divided by @xmath6 , where @xmath7 is a fixed parameter controlling the rate of update .",
    "after many rounds , the elements with the highest weight are deemed the best and used for whatever purpose needed .",
    "our learning framework can succinctly be described as running mwua for each possible edge , forming a candidate graph representation @xmath8 in each round by sampling from all edge distributions , and computing local rewards on @xmath8 to update the weights for the next round . over time",
    "@xmath8 stabilizes and we produce it as output .",
    "the remainder of this section fleshes out the details of this sketch and our specific algorithm implementing it .",
    "let @xmath9 be a set of unweighted , undirected graphs defined on the same vertex set @xmath10 .",
    "we think of each @xmath11 as `` expert advice '' suggesting for any pair of vertices @xmath12 whether to include edge @xmath13 or not . our primary goal is to combine the information present in the @xmath11 to produce a global graph representation @xmath14 suitable for a given application .",
    "the framework we present is described in the context of community detection , but we will note what aspects can be generalized .",
    "each round has four parts : producing the aggregate candidate graph @xmath8 , computing a clustering @xmath0 for use in measuring the quality of @xmath8 , computing the local quality of each edge , and using the quality values to update the weights for the edges .",
    "after some number of rounds @xmath15 , the process ends and we produce @xmath16 .    * aggregated candidate graph @xmath8 * : in each round produce a graph @xmath8 as follows .",
    "maintain a non - negative weight @xmath17 for each graph @xmath11 and each edge @xmath18 in @xmath19 .",
    "normalize the set of all weights for an edge @xmath20 to a probability distribution over the @xmath11 ; thus one can sample an @xmath11 proportionally to its weight . for each edge , sample in this way and include the edge in @xmath8 if it is present in the drawn @xmath11 .    *",
    "event @xmath21 * : after the graph @xmath8 is produced , run a clustering algorithm @xmath0 on it to produce a clustering @xmath21 . in this paper",
    "we fix @xmath0 to be the walktrap algorithm @xcite , though we have observed the effectiveness of other clustering algorithms as well . in general @xmath0",
    "can be any event , and in this case we tie it to the application by making it a simple clustering algorithm .",
    "* local quality measure * : define a _ local quality measure _",
    "@xmath22 to be a @xmath23$]-valued function of a graph @xmath24 , an edge @xmath25 of @xmath24 , and a clustering @xmath26 of the vertices of g. the quality of @xmath18 in @xmath8 is the `` reward '' for that edge , and it is used to update the weights of each input graph @xmath11 .",
    "more precisely , the reward for @xmath18 in round @xmath27 is @xmath28 .    *",
    "update rule * : update the weights using mwua as follows .",
    "define two learning rate parameters @xmath29 , with the former being used to update edges from @xmath8 that are present in @xmath11 and the latter for edges not in @xmath11 . in particular , suppose @xmath30 is the quality of the edge @xmath18 in @xmath8 . then , the update rule is defined as follows : @xmath31      we presently describe the two local quality measures we use for community detection . the first , which we call _ edge consistency _ ( @xmath32 ) captures the intuitive clustering quality notion that edges with endpoints in the same cluster are superior to edges across clusters :    @xmath33 @xmath32 offers a quality metric that is inextricably tied to the performance of the chosen clustering algorithm . the idea behind edge consistency can also be combined with any quality function @xmath1 to produce a `` consistent '' version of @xmath1 . simply evaluate @xmath1 when the edge is within a cluster , and @xmath34 when the edge is across clusters .",
    "note that @xmath1 need not depend on a clustering of the graph or the clustering algorithm , and it can represent algorithmic - agnostic measures of clustering quality .    as an example of such a measure @xmath1 , we consider the metric of _ neighborhood overlap _",
    "( @xmath35 ) , which uses the idea that vertices that share many neighbors are likely to be in the same community .",
    "no declares that the quality of @xmath18 is equal to the ( normalized ) cardinality of the intersection of the neighborhoods of @xmath36 and @xmath37 :    @xmath38 where @xmath39 represents the neighborhood of vertex @xmath40 .",
    "we have also run experiments using more conventional normalizing mechanisms , such as the dice and jaccard indices  @xcite ) , but our neighborhood overlap metric outperforms them by at least 10% in our experiments .",
    "we argue this is due to the use of a global normalization factor , as opposed to a local one , which is what dice and jaccard indices use .",
    "this , for example , gives stronger feedback to edges adjacent to high degree nodes .",
    "for brevity and simplicity , we omit our results for jaccard and dice indices and focus on neighborhood overlap . in our experimental analysis ( section  [ sec : results ] ) we use the consistent version of @xmath35 , which we denote _",
    "consistentno_.    while we demonstrate the utility of the lbga framework by using @xmath32 and @xmath41 , the design of the framework is modular , in that the mechanism for rewarding the `` right '' edges is independent from the definition of reward .",
    "this allows us to plug in other quality metrics to guide the graph representation learning process for other applications , a key goal in lbga s design .",
    "processing every edge in every round of the lbga framework is inefficient .",
    "our implementation of lgba , given by algorithm  [ alg : nef ] , improves efficiency by fixing edges whose weights have grown so extreme so as to be picked with overwhelming or negligible probability ( with probability @xmath42 or @xmath43 for a new parameter @xmath44 ) . in practice",
    "this produces a dramatic speedup on the total runtime of the algorithm .",
    "the worst - case time complexity is the same , but balancing parallelization and the learning parameters suffices for practical applications .",
    "in addition , our decision to penalize non - edges ( @xmath45 ) also improves runtime from the alternative ( @xmath46 . in our experiments non - edge feedback causes @xmath8 to convergence in roughly half as many rounds as when only presence of edge is considered as indication of relational structure .",
    "we also note that algorithm [ alg : nef ] stays inside the `` boundaries '' determined by the input graphs @xmath11 .",
    "it never considers edges that are not suggested by _ some _",
    "@xmath11 , nor does it reject an edge suggest by all @xmath11 .",
    "thus , when we discuss sparsity of our algorithm s output in our experiments , we mean with respect to the number of edges in the union of the input graphs .",
    "we presently describe the datasets used for analysis and provide quantitative results for the performance of algorithm [ alg : nef ] .",
    "our primary synthetic data model is the stochastic block model @xcite , commonly used to model explicit community structure .",
    "we construct a probability distribution @xmath47 over graphs as follows . given a number @xmath48 of vertices and a list of cluster ( block ) sizes @xmath49 such that @xmath50 , we partition the @xmath48 vertices into @xmath51 blocks @xmath52 , @xmath53 .",
    "we declare that the probability of an edge occurring between a vertex in block @xmath54 and block @xmath55 is given by the @xmath56 entry of a @xmath51-by-@xmath51 matrix @xmath57 . in order to simulate different scenarios , we consider the following three cases .    _ global stochastic block model ( gsbm ) : _ in this model we have @xmath58 input graphs @xmath59 , each drawn from the stochastic block model @xmath60 , represents a simpler case of the stochastic block model , where the within - cluster probabilities are uniform across blocks and blocks have the same size . ] with @xmath61 and @xmath62 defined as :    @xmath63 where @xmath64 represents the within - cluster edge probability and @xmath65 represents the across - cluster edge probability in graph @xmath11 .",
    "the ratio @xmath66 is commonly referred to as the _ signal to noise _ ratio and captures the strength of community structure within @xmath11 .",
    "we use the gsbm case to model a scenario where each graph source has a global ( or uniform ) contribution toward the quality of the targeted graph representation @xmath14 .    _ local stochastic block model ( lsbm ) : _ this scenario captures the notion that one graph source accurately describes one community , while another source fares better for a different community .",
    "for example , if we have two underlying communities , and two graph sources @xmath67 , then we use the following two block matrices to represent them :    @xmath68    this naturally extends to a general formulation of the lsbm model for @xmath58 communities .",
    "_ erds - rnyi ( er ) model : _ finally , we consider the case of the erds - rnyi random graph  @xcite , where any two vertices have equal probability of being connected .",
    "this model provides an example of a graph with no community structure .",
    "note that the er model is a special case of both gsbm and lsbm with @xmath69 . in our experimental analysis",
    "we consider cases where an er model is injected into instances of gsbm and lsbm in order to capture a range of structure and noise combinations .",
    "[ datasets ]      our primary real - world dataset is dblp @xcite , a comprehensive online database documenting research in computer science .",
    "we extracted the subset of the dblp database corresponding to researchers who have published at two conferences : the symposium on the theory of computing ( stoc ) , and the symposium on foundations of computer science ( focs ) .",
    "the breadth of topics presented at these conferences implies a natural community structure organized by sub - field .",
    "each node in the dblp graph represents an author , and we use two graphs on this vertex set : the _ co - authorship _ graph and the _ title similarity _ graph . for the latter , we consider two titles to be similar if they contain at least three words in common ( excluding stop words ) . we considered a total of 5234 papers .",
    "table  [ datasets ] contains a summary of all the datasets used for the experimental analysis and their parameters .      in our work , the optimality of the graph representation",
    "is closely coupled with the quality of community structure captured by the representation .",
    "this gives us several ways of evaluating the quality of the results produced by our algorithm .",
    "we consider notions of quality reflected at different levels : the quality of cluster assignment , the quality of graph representation , and the quality of graph source weighting .",
    "_ quality of cluster assignment : _ we use the normalized mutual information ( nmi ) measure @xcite to capture how well the ground truth clustering overlaps with the clustering on the graph representation output from our algorithm .",
    "_ quality of graph representation : _ an ideal graph representation that contains community structure would consist of disjoint cliques or near - cliques corresponding to the communities .",
    "we use the measure of modularity @xcite to capture this notion of representation quality .",
    "modularity is a popular measure that compares a given graph and clustering to a null model . as we illustrate in section  [ sec : results ] , an optimal graph representation can do better than just produce a perfect clustering .",
    "it can also remove cross - community edges and produce a sparser representation , which is what our algorithm does .",
    "we note two extreme graph representation cases , the empty graph which is perfectly modular in a degenerate sense , and the union graph which is a trivial aggregation . to signal these cases in our results , we display the _ sparsity _ of the produced graph @xmath14 , defined as the fraction of edges in @xmath14 out of the total set of edges in all input graphs .    _ quality of graph source weighting : _ the quality of the aggregation process is captured by the right weighting of individual edge sources .",
    "edge sources ( input graphs ) that are more influential in uncovering the underlying community structure have higher weights on average .",
    "similarly , edge types that contribute equally should have equal weights , and edge types with no underlying structure should have low weights .      for illustration",
    ", we show in figure [ fig : local - sbm ] the performance of algorithm [ alg : nef ] when @xmath41 is used as a local quality metric and lsbm-3 ( see table [ datasets ] for details ) is used to generate the input graphs .",
    "note that the algorithm converges quickly to a graph which results in a perfect clustering as measured by nmi .",
    "we also plot the modularity of the resulting graph produced in each round , seeing that it far exceeds the `` baseline '' modularity of the union of the input graphs .",
    "this tells us the learning algorithm is able to discard the noisy edges in the model .",
    "finally , we plot the number of edges in the graph produced in each round , and the average edge weight for each input graph .",
    "this verifies that our algorithm complies with our edge - type weighting and sparsity requirements .",
    "indeed , the algorithm produces a relatively sparse graph , using about 40% of the total edges available and weights edges from the erds - rnyi source appropriately .",
    "our algorithm hence achieves a superior graph than the union , while preserving the underlying community structure so as to be amenable to clustering .    in figure",
    "[ fig : dblp ] , we show results for the dblp dataset .",
    "our algorithm selects title similarity as having more influence in recovering communities for the stoc / focs conferences .",
    "researchers attending these conferences represent a small community as a whole with many of them sharing co - authorship on papers with diverse topics . in this sense",
    ", it is not surprising that title similarity serves as a better proxy for capturing the more pronounced division along topics .",
    "a summary of our algorithm s results for the @xmath32 and @xmath41 quality measures are shown in table  [ ec_no ] .",
    "overall , we find that algorithm [ alg : nef ] converges to graphs of high modularity and that induce correct clusterings in almost all the cases , the challenging case being when snr is low .",
    "moreover the algorithm weights the different input graphs appropriately to their usefulness .",
    "we find that the edge consistency measure outperforms neighborhood overlap in terms of overlap with ground truth clustering ( nmi value ) , but that in the cases where they both produce perfect clusterings , @xmath41 produces sparser , more modular graphs .",
    "this is especially true for the dblp data set .    .",
    "plots in order top to bottom : 1 .",
    "nmi of @xmath21 with the ground truth clustering , 2 .",
    "modularity of @xmath8 w.r.t @xmath21 , with the horizontal line showing the modularity of the union of the input graphs w.r.t .",
    "ground truth , 3 .",
    "the number of edges in @xmath8 , 4 .",
    "the average probability weight ( quality ) of edges of @xmath11 .",
    "the erds - rnyi graph converges to low weight by round 300 . ]",
    "we present the locally boosted graph aggregation framework , a general framework for learning graph representations with respect to an application . in this paper , we demonstrate the strength of the framework with the application of community detection , but the framework can be extended to other inference goals in graphs such as link prediction or diffusion estimation .",
    "our framework offers a flexible , local weighting and aggregation of different edge sources in order to better represent the variability of relational structure observed in real networks .",
    "inspired by concepts in boosting and bandit learning approaches , lbga is designed to handle aggregations of noisy and disparate data sources , therefore marking a departure from methods that assume overlap and usefulness among all data sources considered .",
    "we demonstrated the utility of our framework for a range of aggregation scenarios with different levels of signal to noise .    for future work",
    ", we plan to analyze the utility of our framework with respect to other graph learning applications , as well as present more thorough comparisons of our framework with existing multigraph clustering algorithms .",
    "finally , we will explore the potential for theoretical performance guarantees , akin to those of boosting and bandit learning ."
  ],
  "abstract_text": [
    "<S> learning the right graph representation from noisy , multi - source data has garnered significant interest in recent years . </S>",
    "<S> a central tenet of this problem is relational learning . here </S>",
    "<S> the objective is to incorporate the partial information each data source gives us in a way that captures the true underlying relationships . to address this challenge </S>",
    "<S> , we present a general , boosting - inspired framework for combining weak evidence of entity associations into a robust similarity metric . </S>",
    "<S> we explore the extent to which different quality measurements yield graph representations that are suitable for community detection . we then present empirical results on both synthetic and real datasets demonstrating the utility of this framework . </S>",
    "<S> our framework leads to suitable global graph representations from quality measurements local to each edge . </S>",
    "<S> finally , we discuss future extensions and theoretical considerations of learning useful graph representations from weak feedback in general application settings . </S>"
  ]
}