{
  "article_text": [
    "our main concern in this paper is the construction of iterative algorithms to solve inverse problems with an @xmath0-penalization or an @xmath0-constraint , and that converge faster than the iterative algorithm proposed in @xcite ( see also formulas and below ) .",
    "before we get into technical details , we introduce here the background , framework , and notations for our work .    in many practical problems",
    ", one can not observe directly the quantities of most interest ; instead their values have to be inferred from their effect on observable quantities .",
    "when this relationship between observable @xmath1 and interesting quantity @xmath2 is ( approximately ) linear , as it is in surprisingly many cases , the situation can be modeled mathematically by the equation @xmath3 where @xmath4 is a linear operator mapping a vector space @xmath5 ( which we assume to contain all possible `` objects '' @xmath2 ) to a vector space @xmath6 ( which contains all possible data @xmath1 ) .",
    "the vector spaces @xmath5 and @xmath6 can be finite or infinite  dimensional ; in the latter case , we assume that @xmath7 and @xmath6 are ( separable ) hilbert spaces , and that @xmath8 is a bounded linear operator .",
    "our main goal consists in reconstructing the ( unknown ) element @xmath9 , when we are given @xmath1 .",
    "if @xmath4 is a `` nice '' , easily invertible operator , and if the data @xmath1 are free of noise , then this is a trivial task .",
    "often , however , the mapping @xmath4 is ill - conditioned or not invertible .",
    "moreover , typically ( [ eq:1 ] ) is only an idealized version in which noise has been neglected ; a more accurate model is @xmath10 in which the data are corrupted by an ( unknown ) noise . in order to deal with this type of reconstruction problem a _ regularization _",
    "mechanism is required @xcite .",
    "regularization techniques try , as much as possible , to take advantage of ( often vague ) prior knowledge one may have about the nature of @xmath2 . the approach in this paper",
    "is tailored to the case when @xmath2 can be represented by a _ sparse _ expansion ,",
    "i.e. , when @xmath2 can be represented by a series expansion with respect to an orthonormal basis or a frame @xcite that has only a small number of large coefficients . in this paper , as in @xcite , we model the sparsity constraint by adding an @xmath11term to a functional to be minimized ; it was shown in @xcite that this assumption does indeed correspond to a regularization scheme .",
    "several types of signals appearing in nature admit sparse frame expansions and thus , sparsity is a realistic assumption for a very large class of problems .",
    "for instance , natural images are well approximated by sparse expansions with respect to wavelets or curvelets @xcite .",
    "sparsity has had already a long history of successes .",
    "the design of frames for sparse representations of digital signals has led to extremely efficient compression methods , such as jpeg2000 and mp3 @xcite .",
    "a new generation of optimal numerical schemes has been developed for the computation of sparse solutions of differential and integral equations , exploiting adaptive and greedy strategies @xcite .",
    "the use of sparsity in inverse problems for data recovery is the most recent step of this concept s long career of `` simplifying and understanding complexity '' , with an enormous potential in applications @xcite .",
    "in particular , the observation that it is possible to reconstruct sparse signals from vastly incomplete information just seeking for the @xmath0-minimal solutions @xcite has led to a new line of research called _ sparse recovery _ or _ compressed sensing _ , with very fruitful mathematical and applied results .",
    "before starting our discussion let us briefly introduce some of the notations we will need . for some countable index set @xmath12",
    "we denote by @xmath13 , @xmath14 , the space of real sequences @xmath15 with norm @xmath16 as usual . for",
    "simplicity of notation , in the following @xmath17 will denote the @xmath18-norm @xmath19 .",
    "+ as is customary for an index set , we assume we have a natural enumeration order for the elements of @xmath12 , using ( implicitly ) a one - to - one map @xmath20 from @xmath12 to @xmath21 . in some convergence proofs , we shall use the shorthand notations @xmath22 for @xmath23 , and ( in the case where @xmath12 is infinite ) @xmath24 for @xmath25 .",
    "+ we also assume that we have a suitable frame @xmath26 indexed by the countable set @xmath12 .",
    "this means that there exist constants @xmath27 such that @xmath28 orthonormal bases are particular examples of frames , but there also exist many interesting frames in which the @xmath29 are not linearly independent .",
    "frames allow for a ( stable ) series expansion of any @xmath30 of the form @xmath31 where @xmath32 .",
    "the linear operator @xmath33 ( called the _ synthesis map _ in frame theory ) is bounded because of ( [ frame_ineq ] ) .",
    "when @xmath34 is a frame but not a basis , the coefficients @xmath35 need not be unique . for more details on frames and their differences from bases",
    "we refer to @xcite .",
    "we shall assume that @xmath2 is sparse , i.e. , that @xmath2 can be written by a series of the form ( [ expand_f ] ) with only a small number of non - vanishing coefficients @xmath35 with respect to the frame @xmath36 , or that @xmath2 is _ compressible _ , i.e. , that @xmath2 can be well - approximated by such a sparse expansion .",
    "this can be modeled by assuming that the sequence @xmath37 is contained in a ( weighted ) @xmath38-space .",
    "indeed , the minimization of the @xmath38 norm promotes such sparsity .",
    "( this has been known for many years , and put to use in a wide range of applications , most notably in statistics .",
    "david donoho calls one form of it the logan phenomenon in @xcite  see also @xcite  , after its first observation by ben logan @xcite . )",
    "these considerations lead us to model the reconstruction of a sparse @xmath2 as the minimization of the following functional : @xmath39 where we will assume that the data @xmath1 and the linear operator @xmath40 are given",
    ". the second term in ( [ functional ] ) is often called the _ penalization _ or _ regularizing _ term ; the first term goes by the name of _ discrepancy _ , @xmath41 in what follows we shall drop the subscript @xmath42 , because the space in which we work will always be clear from the context .",
    "we discuss the problem of finding ( approximations to ) @xmath43 in @xmath44 that minimize the functional ( [ functional ] ) .",
    "( we adopt the usual convention that for @xmath45 , the penalty term `` equals '' @xmath46 , and that , for such @xmath47 , @xmath48 for all @xmath49 .",
    "since we want to minimize @xmath50 , we shall consider , implicitly , only @xmath51 . ) the solutions @xmath52 to the original problem are then given by @xmath53 .",
    "several authors have proposed independently an iterative soft - thresholding algorithm to approximate the solution @xmath54 @xcite .",
    "more precisely , @xmath43 is the limit of sequences @xmath55 defined recursively by @xmath56~~ , \\label{ddd04iteration}\\ ] ] starting from an arbitrary @xmath57 , where @xmath58 is the soft - thresholding operation defined by @xmath59 with @xmath60 convergence of this algorithm was proved in @xcite .",
    "soft - thresholding plays a role in this problem because it leads to the unique minimizer of a functional combining @xmath18 and @xmath11norms , i.e. , ( see @xcite ) @xmath61 we will call the iteration the _ iterative soft - thresholding algorithm _ or the _ thresholded landweber",
    "iteration_.    \\(a ) ( b )",
    "the problem of finding the sparsest solution to the under - determined linear equation @xmath62 is a hard combinatorial problem , not tractable numerically except in relatively low dimensions .",
    "for some classes of @xmath63 , however , one can prove that the problem reduces to the convex optimization problem of finding the solution with the smallest @xmath0 norm @xcite . even for @xmath63 outside this class",
    ", @xmath11 minimization seems to lead to very good approximations to the sparsest solutions .",
    "it is in this sense that an algorithm of type ( [ ddd04iteration ] ) could conceivably be called ` fast ' : it is fast compared to a brute - force exhaustive search for the sparsest @xmath37 .    a more honest evaluation of the speed of convergence of algorithm ( [ ddd04iteration ] ) is a comparison with _ linear _ solvers that minimize the corresponding @xmath18 penalized functional , such as , e.g. , the conjugate gradient method .",
    "one finds , in practice , that the thresholded landweber iteration ( [ ddd04iteration ] ) is not competitive at all in this comparison .",
    "it is , after all , the composition of thresholding with the ( linear ) landweber iteration @xmath64 , which is a gradient descent algorithm with a fixed step size , known to converge usually quite slowly ; interleaving it with the nonlinear thresholding operation does unfortunately not change this slow convergence . on the other hand ,",
    "this nonlinearity did foil our attempts to `` borrow a leaf '' from standard linear steepest descent methods by using an adaptive step length  once we start taking larger steps , the algorithm seems to no longer converge in at least some numerical experiments .",
    "we take a closer look at the characteristic dynamics of the thresholded landweber iteration in figure [ pathsfig1 ] . as this plot of the discrepancy @xmath65 versus @xmath66 shows",
    ", the algorithm converges initially relatively fast , then it overshoots the value @xmath67 ( where @xmath68 ) , and it takes very long to re - correct back . in other words , starting from @xmath69 , the algorithm generates a path @xmath70 that is initially fully contained in the @xmath0-ball @xmath71 , with @xmath72 .",
    "then it gets out of the ball to slowly inch back to it in the limit .",
    "a first intuitive way to avoid this long `` external '' detour is to force the successive iterates to remain within the ball @xmath73 .",
    "one method to achieve this is to substitute for the thresholding operations the projection @xmath74 , where , for any closed convex set @xmath75 , and any @xmath37 , we define @xmath76 to be the unique point in @xmath75 for which the @xmath77distance to @xmath37 is minimal . with a slight abuse of notation",
    ", we shall denote @xmath74 by @xmath78 ; this will not cause confusion , because it will be clear from the context whether the subscript of @xmath79 is a set or a positive number .",
    "we thus obtain the following algorithm : pick an arbitrary @xmath80 , for example @xmath69 , and iterate @xmath81 .",
    "\\label{pliteration}\\ ] ] we will call this the _ projected landweber iteration_.    the typical dynamics of this projected landweber algorithm are illustrated in fig .",
    "[ pathsfig1](a ) .",
    "the norm @xmath66 no longer overshoots @xmath82 , but quickly takes on the limit value ( i.e. , @xmath83 ) ; the speed of convergence remains very slow , however .",
    "in this projected landweber iteration case , modifying the iterations by introducing an adaptive `` descent parameter '' @xmath84 in each iteration , defining @xmath85 by @xmath86 , \\label{pgiteration}\\ ] ] does lead , in numerical simulations , to promising , converging results ( in which it differs from the soft - thresholded landweber iteration , where introducing such a descent parameter did not lead to numerical convergence , as noted above ) .",
    "the typical dynamics of this modified algorithm are illustrated in fig .",
    "[ pathsfig1](b ) , which clearly shows the larger steps and faster convergence ( when compared with the projected landweber iteration in fig .",
    "[ pathsfig1](a ) )",
    ". we shall refer to this modified algorithm as the _ projected gradient iteration _ or the _ projected steepest descent _ ; it will be the main topic of this paper .",
    "the main issue is to determine how large we can choose the successive @xmath87 , and still prove norm convergence of the algorithm in @xmath44 .",
    "there exist results in the literature on convergence of projected gradient iterations , where the projections are ( as they are here ) onto convex sets , see , e.g. , @xcite and references therein .",
    "these results treat iterative projected gradient methods in much greater generality than we need : they allow more general functionals than @xmath88 , and the convex set on which the iterative procedure projects need not be bounded . on the other hand , these general results typically have the following restrictions :    * the convergence in infinite - dimensional hilbert spaces ( i.e. , @xmath12 is countable but infinite ) is proved only in the weak sense and often only for subsequences ; * in @xcite the descent parameters are typically restricted to cases for which @xmath89 . in @xcite , it is shown that the algorithm converges weakly for any choice of @xmath90 $ ] , for @xmath91 arbitrarily small . of most interest",
    "to us is the case where the @xmath87 are picked adaptively , can grow with @xmath92 , and are not limited to values below @xmath93 ; this case is not covered by the methods of either @xcite or @xcite .    to our knowledge",
    "there are no results in the literature for which the whole sequence @xmath94 converges in the hilbert space norm to a unique accumulation point , for `` descent parameters '' @xmath95 .",
    "it is worthwhile emphasizing that strong convergence is not automatic : in ( * ? ?",
    "* remark 5.12 ) , the authors provide a counterexample in which strong convergence fails .",
    "( this question had been open for some time . )",
    "one of the main results of this paper is to prove a theorem that establishes exactly this type of convergence ; see theorem [ finaltheorem ] below .",
    "moreover , the result is achieved by imposing a choice of @xmath96 which ensures a monotone decay of a suitable energy .",
    "this establishes a principle of _ best descent _ similar to the well - known steepest - descent in unconstrained minimization .",
    "before we get to this theorem , we need to build some more machinery first .",
    "in this section we discuss some properties of @xmath18-projections onto @xmath0-balls .",
    "in particular , we investigate their relations with thresholding operators and their explicit computation .",
    "we also estimate the time complexity of such projections in finite dimensions .",
    "we first observe a useful property of the soft - thresholding operator .    for any fixed @xmath97 and for @xmath98 , @xmath99 is a piecewise linear , continuous , decreasing function of @xmath100 ; moreover , if @xmath101 then @xmath102 and @xmath103 for @xmath104 .",
    "[ piecewiselemma ]    _ proof : _",
    "@xmath105 ; the sum in the right hand side is finite for @xmath98 . @xmath106    a schematic illustration is given in figure [ normfig ] .    , @xmath99 is a piecewise linear continuous and decreasing function of @xmath100 ( strictly decreasing for @xmath107 ) .",
    "the knots are located at @xmath108 and @xmath109 .",
    "finding @xmath100 such that @xmath110 ultimately comes down to a linear interpolation .",
    "the figure is made for the finite dimensional case . ]",
    "the following lemma shows that the @xmath18 projection @xmath111 can be obtained by a suitable thresholding of @xmath112 .    if @xmath113 , then the @xmath18 projection of @xmath112 on the @xmath0 ball with radius @xmath82 is given by @xmath114 where @xmath115 ( depending on @xmath112 and @xmath82 ) is chosen such that @xmath116 . if @xmath117 then @xmath118.[l1projectionlemma ]    _ proof : _ suppose @xmath113 . because , by lemma [ piecewiselemma ] , @xmath119 is continuous in @xmath115 and @xmath120 for sufficiently large @xmath115 , we can choose @xmath115 such that @xmath116 .",
    "( see figure [ normfig ] . ) on the other hand ( see above , or @xcite ) , @xmath121 is the unique minimizer of @xmath122 , i.e. , @xmath123 for all @xmath124 . since @xmath125 , it follows that @xmath126 hence @xmath127 is closer to @xmath112 than any other @xmath37 in @xmath73 . in other words , @xmath128 .",
    "@xmath106    these two lemmas prescribe the following simple recipe for computing the projection @xmath111 . in a first step ,",
    "sort the absolute values of the components of @xmath112 ( an @xmath129 operation if @xmath130 is finite ) , resulting in the rearranged sequence @xmath131 , with @xmath132 for all @xmath133 .",
    "next , perform a search to find @xmath134 such that @xmath135 or equivalently , @xmath136 the complexity of this step is again @xmath129 .",
    "finally , set + @xmath137 , and @xmath138 .",
    "then @xmath139    these formulas were also derived in ( * ? ? ? * lemma 4.1 and lemma 4.2 ) , by observing that @xmath140 , where @xmath141 the latter is again a thresholding operator , but it is related to an @xmath142 penalty term .",
    "similar descriptions of the @xmath18 projection onto @xmath0 balls appear also in @xcite .",
    "finally , @xmath78 has the following additional properties :    [ convex_basic_lm ] for any @xmath143 , @xmath144 is characterized as the unique vector in @xmath73 such that @xmath145 moreover the projection @xmath78 is non - expansive : @xmath146 for all @xmath147.[contractionlemma ]    the proof is standard for projection operators onto convex sets ; we include it because its technique will be used often in this paper .",
    "_ proof : _ because @xmath73 is convex , @xmath148 for all @xmath149 and @xmath150 $ ] .",
    "it follows that @xmath151 \\,\\|^2 $ ] for all @xmath152 $ ] .",
    "this implies @xmath153 for all @xmath154 $ ] .",
    "it follows that @xmath155 which proves ( [ convex_basic ] ) .",
    "setting @xmath156 in ( [ convex_basic ] ) , we get , for all @xmath157 , @xmath158 switching the role of @xmath37 and @xmath159 one finds : @xmath160 by combining these last two inequalities , one finds : @xmath161 or @xmath162 by cauchy - schwarz this gives @xmath163 from which inequality ( [ contraction ] ) follows.@xmath106",
    "we have now collected all the terminology needed to identify some conditions on the @xmath87 that will ensure convergence of the @xmath55 , defined by , to @xmath164 , the minimizer in @xmath73 of @xmath165 . for notational simplicity",
    "we set @xmath166 . with this notation ,",
    "the thresholded landweber iteration ( [ ddd04iteration ] ) can be written as @xmath167 as explained above , we consider , instead of straightforward soft - thresholding with fixed @xmath100 , adapted soft - thresholding operations @xmath168 that correspond to the projection operator @xmath78 : @xmath169 the dependence of @xmath170 on @xmath82 is described above ; @xmath82 is kept fixed throughout the iterations .",
    "if , for a given value of @xmath100 , @xmath82 were picked such that @xmath171 ( where @xmath172 is the minimizer of @xmath173 ) , then lemma [ l1projectionlemma ] would ensure that @xmath174 .",
    "of course , we do nt know , in general , the exact value of @xmath175 , so that we ca nt use it as a guideline to pick @xmath82 . in practice , however , it is customary to determine @xmath172 for a range of @xmath100-values ; this then amounts to the same as determining @xmath164 for a range of @xmath82 .",
    "we now propose to change the step @xmath176 into a step @xmath177 ( in the spirit of the `` classical '' steepest descent method ) , and to define the algorithm : pick an arbitrary @xmath80 , for example @xmath69 , and iterate @xmath178 in this section we prove the norm convergence of this algorithm to a minimizer @xmath164 of @xmath179 in @xmath73 , under some assumptions on the descent parameters @xmath180 .",
    "we begin with the following characterization of the minimizers of @xmath88 on @xmath73 .",
    "[ fixpt ] the vector @xmath181 is a minimizer of @xmath182 on @xmath73 if and only if @xmath183 for any @xmath184 , which in turn is equivalent to the requirement that @xmath185    to lighten notation , we shall drop the subscript @xmath82 on @xmath164 whenever no confusion is possible .",
    "+ _ proof : _ if @xmath186 minimizes @xmath88 on @xmath73 , then for all @xmath149 , and for all @xmath187 $ ] , @xmath188 this implies @xmath189 it follows from this that , for all @xmath149 and for all @xmath190 , @xmath191 by lemma [ contractionlemma ] this implies .",
    "conversely , if @xmath192 , then for all @xmath149 and for all @xmath187 $ ] : @xmath193 this implies @xmath194 in other words : @xmath195 or @xmath196 this implies that @xmath186 minimizes @xmath88 on @xmath73 .",
    "@xmath106 + the minimizer of @xmath88 on @xmath73 need not be unique .",
    "we have , however    [ kern ] if @xmath197 are two distinct minimizers of @xmath165 on @xmath73 , then @xmath198 , i.e. , @xmath199 .",
    "+ conversely , if @xmath200 , if @xmath201 minimizes @xmath179 and if @xmath202 then @xmath203 minimizes @xmath179 as well .",
    "_ proof : _ the converse is obvious ; we prove only the direct statement . from the last inequality in the proof of lemma [ fixpt ] we obtain @xmath204 , which implies @xmath205 .",
    "@xmath106 + in what follows we shall assume that the minimizers of @xmath88 in @xmath73 are not global minimizers for @xmath88 , i.e. , that @xmath206 .",
    "we know from lemma [ l1projectionlemma ] that @xmath207 can be computed for @xmath208 simply by finding the value @xmath209 such that @xmath210 ; one has then @xmath211 . using this we prove    [ thrval ] let @xmath47 be the common image under @xmath63 of all minimizers of @xmath88 on @xmath73 , i.e. , for all @xmath186 minimizing @xmath88 in @xmath73 , @xmath212",
    ". then there exists a unique value @xmath98 such that , for all @xmath184 and for all minimizing @xmath186 @xmath213 moreover , for all @xmath214 we have that if there exists a minimizer @xmath186 such that @xmath215 , then @xmath216    _ proof : _ from lemma [ l1projectionlemma ] and lemma [ fixpt ] , we know that for each minimizing @xmath186 , and each @xmath184 , there exists a unique @xmath217 such that @xmath218 for @xmath219 we have @xmath220 ; this implies @xmath221 and also that @xmath222 . if @xmath223 then @xmath224 .",
    "it follows that @xmath225 does not depend on the choice of @xmath186 .",
    "moreover , if there is a minimizer @xmath186 for which @xmath219 , then @xmath226 .",
    "@xmath106 +    if , for some @xmath214 , two minimizers @xmath227 satisfy @xmath219 and @xmath228 , then @xmath229 .",
    "_ proof : _ this follows from the arguments in the previous proof ; @xmath219 implies @xmath230 .",
    "similarly , @xmath231 implies @xmath232 , so that @xmath233 .",
    "@xmath106 + this immediately leads to    [ gammaset ] for all @xmath234 that minimize @xmath88 , there are only finitely many @xmath219 .",
    "more precisely , @xmath235 moreover , if the vector @xmath236 is defined by @xmath237 then @xmath238 for each minimizer @xmath186 of @xmath88 in @xmath73 .",
    "_ proof : _ we have already proved the set inclusion . note that , since @xmath239 , the set @xmath240 is necessarily a finite set .",
    "we also have , for each minimizer @xmath201 , @xmath241 @xmath106    [ conven ] by changing , if necessary , signs of the canonical basis vectors , we can assume , without loss of generality , that @xmath242 for all @xmath243 .",
    "we shall do so from now on .",
    "we shall now impose some conditions on the @xmath244 .",
    "we shall see examples in section [ numericsection ] where these conditions are verified .",
    "[ conditionb ] we say that the sequence @xmath245 _ satisfies condition ( b ) with respect to the sequence _",
    "@xmath246 if there exists @xmath247 so that : @xmath248    we shall often abbreviate this by saying that ` the @xmath87 satisfy condition ( b ) ' .",
    "the constant @xmath249 used in this definition is @xmath250 .",
    "( we can always assume , without loss of generality , that @xmath251 ; if necessary , this can be achieved by a suitable rescaling of @xmath63 and @xmath1 . )",
    "note that the choice @xmath252 for all @xmath92 , which corresponds to the projected landweber iteration , automatically satisfies condition ( b ) ; since we shall show below that we obtain convergence when the @xmath87 satisfy condition ( b ) , this will then establish , as a corollary , convergence of the projected landweber iteration algorithm as well .",
    "we shall be interested in choosing , adaptively , larger values of @xmath87 ; in particular , we like to choose @xmath87 as large as possible .",
    "*   condition ( b ) is inspired by the standard length - step in the steepest descent algorithm for the ( unconstrained , unpenalized ) functional @xmath179 . in this case",
    ", one can speed up the standard landweber iteration @xmath253 by defining instead @xmath254 , where @xmath255 is picked so that it gives the largest decrease of @xmath179 in this direction .",
    "this gives @xmath256 \\left[\\|k",
    "k^*(y -k x\\n)\\|^2\\right]^{-1}\\,.\\ ] ] in this linear case , one easily checks that @xmath257 also equals @xmath258 \\left[\\|k(x\\np1-x\\n)\\|^2\\right]^{-1}\\,;\\ ] ] in fact , it is this latter expression for @xmath255 ( which inspired the formulation of condition ( b ) ) that is most useful in proving convergence of the steepest descent algorithm . * because the definition of @xmath259 involves @xmath87 , the inequality ( b2 ) , which uses @xmath259 to impose a limitation on @xmath87 , has an `` implicit '' quality . in practice",
    ", it may not be straightforward to pick @xmath87 appropriately ; one could conceive of trying first a `` greedy '' choice , such as e.g. @xmath260 ; if this value works , it is retained ; if it does nt , it can be gradually decreased ( by multiplying it with a factor slightly smaller than 1 ) until ( b2 ) is satisfied .",
    "( a similar way of testing appropriate step lengths is adopted in @xcite . )    in this section we prove that if the sequence @xmath261 is defined iteratively by , and if the @xmath87 used in the iteration satisfy condition ( b ) ( with respect to the @xmath262 ) , then the ( weak ) limit of any weakly convergent subsequence of @xmath263 is necessarily a minimizer of @xmath88 in @xmath73 .",
    "[ minimfbeta ] assume @xmath264 and @xmath265 .",
    "for arbitrary fixed @xmath37 in @xmath73 , define the functional @xmath266 by @xmath267 then there is a unique choice for @xmath268 in @xmath73 that minimizes the restriction to @xmath73 of @xmath269 .",
    "we denote this minimizer by @xmath270 ; it is given by @xmath271 .",
    "_ proof : _ first of all , observe that the functional @xmath272 is strictly convex , so that it has a unique minimizer on @xmath73 ; let @xmath273 be this minimizer .",
    "then for all @xmath149 and for all @xmath187 $ ] @xmath274\\\\ & & \\phantom{xx } + \\frac{t^2}{\\beta } \\| w - \\hat x\\|^2 \\geq 0\\\\    & \\rightarrow & \\quad   \\left [ \\beta \\langle k x - y , k(w -\\hat x ) \\rangle   +    \\langle \\hat x - x , w - \\hat x \\rangle \\right ] + \\frac{t } { 2 } \\| w - \\hat x\\|^2 \\geq 0\\\\    & \\rightarrow & \\quad \\langle   \\hat x   -x   + \\beta k^*(k x -y),w- \\hat x   \\rangle \\geq 0 \\\\   & \\rightarrow & \\quad \\langle x + \\beta k^*(y - k x)- \\hat x , w- \\hat x   \\rangle \\leq 0.\\end{aligned}\\ ] ] the latter implication is equivalent to @xmath275 by lemma [ convex_basic_lm ] .",
    "@xmath106 + an immediate consequence is    [ asymptreg ] if the @xmath262 are defined by , and the @xmath244 satisfy _ condition ( b ) _ with respect to the @xmath262 , then the sequence @xmath276 is decreasing , and @xmath277    _ proof : _ comparing the definition of @xmath259 in with the statement of lemma [ minimfbeta ] , we see that @xmath278 , so that @xmath259 is the minimizer , for @xmath279 , of @xmath280 . setting @xmath281 ,",
    "we have @xmath282 we also have @xmath283 this implies @xmath284 therefore , the series @xmath285 converges and @xmath286 .",
    "@xmath106 + because the set @xmath287 is bounded in @xmath38 ( @xmath288 are all in @xmath73 ) , it is bounded in @xmath44 as well ( since @xmath289 ) . because bounded closed sets in @xmath44 are weakly compact ,",
    "the sequence @xmath263 must have weak accumulation points .",
    "we now have    [ weakconv ] if @xmath290 is a weak accumulation point of @xmath263 then @xmath290 minimizes @xmath88 in @xmath73 .",
    "_ proof : _ let @xmath291 be a subsequence converging weakly to @xmath290 . then for all @xmath292 @xmath293 therefore @xmath294 . from lemma [ asymptreg ] we have @xmath295 , so that we also have @xmath296 . by the definition of @xmath259 ( @xmath297 ) , and by lemma [ contractionlemma ]",
    ", we have , for all @xmath149 , @xmath298 in particular , specializing to our subsequence and taking the @xmath299 , we have @xmath300 because @xmath301 , for @xmath302 , and @xmath303 is uniformly bounded , we have @xmath304 so that our inequality reduces to @xmath305 by adding @xmath306 , which also tends to zero as @xmath302 , we transform this into @xmath307 since the @xmath308 are all in @xmath309 $ ] , it follows that @xmath310 or @xmath311 \\leq 0,\\ ] ] where we have used the weak convergence of @xmath312 .",
    "this can be rewritten as @xmath313 \\leq 0.\\ ] ] since @xmath314 , we have @xmath315 \\geq 0.\\ ] ] we conclude thus that @xmath316 so that @xmath290 is a minimizer of @xmath88 on @xmath73 , by lemma [ fixpt ] .",
    "@xmath106      in this subsection we show how the weak convergence established in the preceding subsection can be strengthened into norm convergence , again by a series of lemmas . since the distinction between weak and strong convergence makes sense only when the index set @xmath12 is infinite , we shall implicitly assume this is the case throughout this section .",
    "[ strongconv ] for the subsequence @xmath317 defined in the proof of _ proposition [ weakconv ] _ , + @xmath318 .",
    "_ proof : _ specializing the inequality to @xmath319 , we obtain @xmath320 \\leq 0;\\ ] ] together with @xmath321 ( a consequence of the weak convergence of @xmath322 to @xmath323 ) , this implies @xmath324 , and thus @xmath325 .",
    "@xmath106 +    under the same assumptions as in _ proposition [ weakconv ] _ , there exists a subsequence @xmath326 of @xmath327 such that @xmath328    _ proof : _ let @xmath329 be the subsequence defined in the proof of proposition [ weakconv ] .",
    "define now @xmath330 and @xmath331 .",
    "since , by lemma [ asymptreg ] , @xmath332 , we have @xmath333 .",
    "on the other hand , @xmath334 where we have used proposition [ weakconv ] ( @xmath290 is a minimizer ) and lemma [ fixpt ] ( so that + @xmath335 ) . by lemma [ strongconv ] , @xmath336 . since the @xmath308 are uniformly bounded , we have , by formula , @xmath337 combining this with @xmath338 , we obtain @xmath339 @xmath340 since the @xmath341 are uniformly bounded , they must have at least one accumulation point .",
    "let @xmath342 be such an accumulation point , and choose a subsequence @xmath343 such that @xmath344 . to simplify notation , we write @xmath345 , @xmath346 , @xmath347 .",
    "we have thus @xmath348 denote @xmath349 and @xmath350 .",
    "we have now @xmath351 since both terms on the right hand side converge to zero for @xmath352 ( see ) , we have @xmath353 without loss of generality we can assume @xmath354 .",
    "by lemma [ l1projectionlemma ] there exists @xmath209 such that @xmath355 . because @xmath356 as @xmath357 , this implies that , for some finite @xmath358 , @xmath359 for @xmath360 .",
    "pick now any @xmath361 that satisfies @xmath362 .",
    "there exists a finite @xmath363 so that @xmath364 .",
    "set @xmath365 , and define the vector @xmath366 by @xmath367 if @xmath368 , @xmath369 if @xmath370 .    by the weak convergence of the @xmath371 ,",
    "we can , for this same @xmath372 , determine @xmath373 such that , for all @xmath374 , @xmath375 .",
    "define new vectors @xmath376 by @xmath377 if @xmath378 , @xmath379 if @xmath380 .",
    "because of , there exists @xmath381 such that @xmath382 for @xmath383 .",
    "consider now @xmath384 .",
    "we have @xmath385    on the other hand , lemma [ l1projectionlemma ] tells us that there exists @xmath386 such that @xmath387 , where we used in the last equality that @xmath369 for @xmath388 and @xmath377 for @xmath389 . from @xmath390 we conclude that @xmath391 for all @xmath392 .",
    "we then deduce @xmath393 ^ 2 \\\\ & = & \\sum_{|\\lambda|>k_0 } \\min\\left (    \\min\\left ( |\\tuell_\\lambda|,\\mu\\right)^2~.\\end{aligned}\\ ] ]      we have thus obtained what we set out to prove : the subsequence @xmath399 of @xmath400 satisfies that , given arbitrary @xmath401 , there exists @xmath402 so that , for @xmath403 , @xmath404 . @xmath106    in this proof we have implicitly assumed that @xmath405 .",
    "given that @xmath406 , this assumption can be made without loss of generality , because it is not possible to have @xmath406 and @xmath407 infinitely often , as the following argument shows .",
    "find @xmath408 such that @xmath409 and , @xmath410 and @xmath411 : @xmath412 .",
    "then @xmath413 .",
    "hence , @xmath414 , @xmath415 .",
    "at the cost of more technicalities it is possible to show that the whole subsequence @xmath329 defined in the proof of proposition [ weakconv ] converges in norm to @xmath290 , i.e. , that @xmath416 , without going to a subsequence @xmath399 .    the following proposition summarizes in one statement all the findings of the last two subsections .",
    "[ normconv ] every weak accumulation point @xmath290 of the sequence @xmath263 defined by is a minimizer of @xmath88 in @xmath73 .",
    "moreover , there exists a subsequence @xmath417 of @xmath261 that converges to @xmath290 in norm .      in this subsection",
    "we prove that the accumulation point @xmath290 of @xmath263 is unique , so that the entire sequence @xmath263 converges to @xmath290 in norm .",
    "( note that two sequences @xmath263 and @xmath418 , both defined by the same recursion , but starting from different initial points @xmath419 , can still converge to different limits @xmath290 and @xmath420 . )",
    "we start again from the inequality @xmath421 for all @xmath149 and for all @xmath422 , and its many consequences .",
    "define @xmath423 to be the set of minimizers of @xmath88 on @xmath73 . by lemma [ kern ] , @xmath424 , where @xmath186 is an arbitrary minimizer of @xmath88 in @xmath73 . by the convention adopted in remark [ conven ] , @xmath425 moreover , for each element @xmath426 , @xmath427 if @xmath428 ( see lemma [ gammaset ] ) .",
    "the set @xmath423 is both closed and convex .",
    "we define the corresponding ( nonlinear ) projection operator @xmath429 as usual , @xmath430 because @xmath423 is convex , this projection operator has the following property : @xmath431 ( the proof is standard , and is essentially given in the proof lemma [ contractionlemma ] , where in fact only the convexity of @xmath73 was used . ) for each @xmath422 , we introduce now @xmath432 and @xmath433 defined by @xmath434 specializing equation to @xmath262 , we obtain , for all @xmath435 and for all @xmath436 : @xmath437 or @xmath438 because @xmath432 is a minimizer , we can also apply lemma [ fixpt ] to @xmath432 and conclude @xmath439    with these inequalities , we can prove the following crucial result .",
    "[ monolem ] for any @xmath435 , and for any @xmath422 , @xmath440    _ proof : _ we set @xmath441 in , leading to @xmath442 where we have used that @xmath443 .",
    "we also have , setting @xmath444 in the @xmath445-version of , @xmath446 or @xmath447 where we have used @xmath448 .",
    "it follows that @xmath449 or @xmath450 which is also equivalent to @xmath451\\ ] ] @xmath452 adding @xmath453 to , we have @xmath454\\\\   & \\leq & \\frac{r}{2 } \\| x\\n - x\\np1\\|^2\\\\ & = & \\frac{r}{2 } \\left [ \\| x\\n- \\tilde x \\|^2 + \\|",
    "x\\np1 - \\tilde x \\|^2 - 2 \\langle   x\\n -\\tilde x , x\\np1 - \\tilde x \\rangle \\right ] .\\end{aligned}\\ ] ] it follows that @xmath455 @xmath456 \\leq 0,\\ ] ] which , in turn , implies that @xmath457 this can be rewritten as @xmath458   \\left [   \\left ( 1 - \\frac{r}{2 } \\right ) \\| x\\np1- \\tilde x \\| + \\frac{r}{2 }   \\|\\tilde x - x\\n\\|   \\right ] \\leq 0,\\ ] ] which implies @xmath459 .",
    "@xmath106    we are now ready to state the main result of our work .",
    "the sequence @xmath460 as defined in , where the _ step - length sequence _ @xmath461 satisfies condition ( b ) with respect to the @xmath262 , converges in norm to a minimizer of @xmath88 on @xmath73 . [ finaltheorem ]    _ proof : _ the sequence @xmath263 has a least one accumulation point @xmath290 . by proposition",
    "[ weakconv ] @xmath290 minimizes @xmath88 in @xmath73 . by proposition [ normconv",
    "] @xmath263 has a subsequence @xmath462 that converges to @xmath290 . by lemma",
    "[ monolem ] @xmath463 decreases monotonically , hence it has a limit for @xmath464 , and @xmath465 @xmath106",
    "we conduct a number of numerical experiments to gauge the effectiveness of the different algorithms we discussed .",
    "all computations were done in mathematica 5.2 @xcite on a 2ghz workstation with 2 gb memory .",
    "we are primarily interested in the behavior , as a function of time ( not number of iterations ) , of the relative error @xmath466 . to this end , and for a given operator @xmath63 and data @xmath1 , we need to know in advance the actual minimizer @xmath54 of the functional ( [ functional ] )",
    ".    one can calculate the minimizer exactly ( in practice up to computer round - off ) with a finite number of steps using the lars algorithm described in @xcite ( the variant called ` lasso ' , implemented independently by us ) .",
    "this algorithm scales badly , and is useful in practice only when the number of non - zero entries in the minimizer @xmath43 is sufficiently small .",
    "we made our own implementation of this algorithm to make it more directly applicable to our problem ( i.e. , we do not renormalize the columns of the matrix to have zero mean and unit variance , as it is done in the statistics context @xcite ) .",
    "we also double - check the minimizer obtained in this manner by verifying that it is indeed a fixed point of the iterative thresholding algorithm ( [ ddd04iteration ] ) ( up to machine epsilon ) .",
    "we then have an ` exact ' minimizer @xmath467 together with its radius @xmath468 ( used in the projected algorithms ) and , according to lemma [ thrval ] , the corresponding threshold @xmath469 with @xmath470 ( used in the iterative thresholding algorithm ) .",
    "+ the numerical examples below are listed in order of increasing complexity ; they illustrate that the algorithms can behave differently for different examples . in these experiments",
    "we choose @xmath471 , ( where , as before , @xmath472 ) ; @xmath473 is the standard descent parameter from the classical linear steepest descent algorithm .    1 .",
    "when @xmath63 is a partial fourier matrix ( i.e. , a fourier matrix with a prescribed number of deleted rows ) , there is no advantage in using a dynamical step size @xmath474 as this ratio is always equal to 1 .",
    "this trivially fulfills condition ( b ) in section [ genpropsec ] .",
    "the performance of the projected steepest descent iteration simply equals that of the projected landweber iterations .",
    "2 .   by combining a scaled partial fourier",
    "transform with a rank 1 projection operator , we constructed our second example , in which @xmath63 is a @xmath475 matrix , of rank @xmath476 , with largest singular value equal to 0.99 and all the other singular values between 0.01 and 0.11 . because of the construction of the matrix , the fft algorithm provides a fast way of computing the action of this matrix on a vector . for the @xmath1 and @xmath100 that were chosen ,",
    "the limit vector @xmath172 has @xmath477 nonzero entries .",
    "for this example , the lars procedure is slower than thresholded landweber , which in turn is significantly slower than projected steepest descent . to get within a distance of the true minimizer corresponding to a @xmath478 relative error",
    ", the projected steepest descent algorithm takes @xmath479 , the thresholded landweber algorithm @xmath480 , and lars @xmath481 .",
    "( the relatively poor performance of lars in this case is due to the large number of nonzero entries in the limit vector @xmath172 ; the complexity of lars is cubic in this number of nonzero entries . ) in this case , the @xmath482 are much larger than 1 ; moreover , they satisfy condition ( b ) of section [ genpropsec ] at every step .",
    "we illustrate the results in figure [ psdfig ] .",
    "the last example is inspired by a real - life application in geoscience @xcite , in particular an application in seismic tomography based on earthquake data .",
    "the object space consists of the wavelet coefficients of a 2d seismic velocity perturbation .",
    "there are @xmath483 degrees of freedom . in this particular case",
    "the number of data is @xmath484 .",
    "hence the matrix @xmath63 has @xmath484 rows and @xmath483 columns .",
    "we apply the different methods to the same noisy data that are used in @xcite and measure the time to convergence up to a specified relative error ( see table [ geodistancetable ] and figure [ geodistancefig ] ) .",
    "this example illustrates the slow convergence of the thresholded landweber algorithm ( [ ddd04iteration ] ) , and the improvements made by a projected steepest descent iteration ( [ pgiteration ] ) with the special choice @xmath485 above . in this case",
    ", this choice turns out _ not _ to satisfy condition ( b ) in general .",
    "one could conceivably use successive corrections , e.g. by a line - search , to determine , starting from @xmath473 , values of @xmath486 that would satisfy condition ( b ) , and thus guarantee convergence as established by theorem 5.18 .",
    "this would slow down the method considerably .",
    "the @xmath473 seem to be in the right ballpark , and provide us with a numerically converging sequence .",
    "we also implemented the projected landweber algorithm ( [ pliteration ] ) ; it is listed in table [ geodistancetable ] and illustrated in figure [ geodistancefig ] .",
    "+ the matrix @xmath63 in this example is extremely ill - conditioned : its largest singular value was normalized to 1 , but the remaining singular values quickly tend to zero .",
    "the threshold was chosen , according to the ( known or estimated ) noise level in the data , so that @xmath487 ( = the number of data points ) , where @xmath488 is the data noise level ; this is a standard choice that avoids overfitting .",
    "+ in figure [ geodistancefig ] , we see that the thresholded landweber algorithm takes more than @xmath489 hours ( corresponding to @xmath490 iterations ) to converge to the true minimizer within a @xmath491 relative error , as measured by the usual @xmath18 distance .",
    "the projected steepest descent algorithm is about four times faster and reaches the same reconstruction error in about @xmath492 hours ( @xmath493 iterations ) .",
    "due to one additional matrix - vector multiplication and , to a minor extent , the computation of the projection onto an @xmath0-ball , one step in the projected steepest descent algorithm takes approximately twice as long as one step in the thresholded landweber algorithm . for the projected landweber algorithm",
    "there is an advantage in the first few iterations , but after a short while , the additional time needed to compute the projection @xmath78 ( i.e. , to compute the corresponding variable thresholds ) makes this algorithm slower than the iterative soft - thresholding .",
    "we illustrate the corresponding cpu time in table [ geodistancetable ] . + it is worthwhile noticing that for the three algorithms the value of the functional ( [ functional ] ) converges much faster to its limit value than the minimizer itself : when the reconstruction error is 10% , the corresponding value of the functional is already accurate up to three digits with respect to the value of the functional at @xmath467 .",
    "we can imagine that in this case the functional has a long narrow `` valley '' with a very gentle slope in the direction of the eigenvectors with small ( or zero ) singular values .",
    "+ .table illustrating the relative performance of three algorithms : thresholded landweber , projected landweber and projected steepest descent , for the third example .",
    "[ cols= \" > , > , > , > , > , > , > \" , ]    + the path in the @xmath494 vs. @xmath179 plane followed by the iterates is shown in figure [ pathsfig1 ] . the projected steepest descent algorithm , by construction , stays within a fixed @xmath0-ball , and , as already mentioned , converges faster than the thresholded landweber algorithm .",
    "the path followed by the lars algorithm is also pictured .",
    "it corresponds with the so - called _ trade - off _ curve which can be interpreted as the border of the area that is reachable by any element of the model space , i.e. , it is generated by @xmath43 for decreasing values of @xmath98 .",
    "+ in this particular example , the number of nonzero components of @xmath467 equals @xmath495 .",
    "the lars ( exact ) algorithm only takes @xmath496 seconds , which is _ much _",
    "faster than any of the iterative methods demonstrated here .",
    "however , as illustrated above , by the second example , lars looses its advantage when dealing with larger problems where the minimizer is not sparse in absolute number of entries , as is the case in , e.g. , realistic problems of global seismic tomography .",
    "indeed , the example presented here is a `` toy model '' for proof - of - concept for geoscience applications .",
    "the 3d model will involve millions of unknowns and solutions that may be sparse compared with the total number of unknowns , but not sparse in absolute numbers . because the complexity of lars is cubic in the number of nonzero components of the solution ,",
    "such 3d problems are expected to lie beyond its useful range .",
    "the projected iterations ( [ projlwiteration ] ) and ( [ projsditeration ] ) are related to the pocs ( projection on convex sets ) technique @xcite .",
    "the projection of a vector @xmath112 on the solution space @xmath497 ( a convex set , assumed here to be non - empty ; no such assumption was made before because the functional ( [ functional ] ) always has a minimum ) is given by : @xmath498 hence , alternating projections on the convex sets @xmath497 and @xmath73 give rise to the algorithm @xcite : : pick an arbitrary @xmath80 , for example @xmath69 , and iterate @xmath499 this may be practical in case of a small number of data or when there is structure in @xmath63 , i.e. , when @xmath500 is efficiently inverted . approximating @xmath501 by the unit matrix , yields the projected landweber algorithm ( [ projlwiteration ] ) ; approximating @xmath502 by a constant multiple of the unit matrix yields the projected gradient iteration ( [ projsditeration ] ) if one chooses the constant equal to @xmath244 .    [ adaptivefigure ]",
    "the projected methods discussed in this paper produce iterates that ( except for the first few ) live on the ` skin ' of the @xmath0-ball of radius @xmath82 , as shown in fig .",
    "[ pathsfig1 ] .",
    "we have found even more promising results for an ` interior ' algorithm in which we still project on @xmath0-balls , but now with a slowly increasing radius , i.e. , @xmath503 where @xmath504 is the prescribed maximum number of iterations ( the origin is chosen as the starting point of this iteration ) .",
    "we do not have a proof of convergence of this ` interior point type ' algorithm .",
    "we observed ( also without proof ) that the path traced by the iterates @xmath262 ( in the @xmath494 vs. @xmath179 plane ) is very close to the trade - off curve ( see fig .",
    "5 ) ; this is a useful property in practice since at least part of the trade - off curve should be constructed anyway .",
    "+ note that the strategy followed by these algorithms is similar to that of lars @xcite , in that they both start with @xmath69 and slowly increase the @xmath0 norm of the successive approximations .",
    "it is also related to @xcite .",
    "+ while we were finishing this paper , michael friedlander informed us of their numerical results in @xcite which are closely related to our approach , although their analysis is limited to finite dimensions .",
    "+ different , but closely related is also the recent approach by figueiredo , nowak , and wright @xcite .",
    "the authors first reformulate the minimization of as a bound - constrained quadratic program in standard form , and then they apply iterative projected gradient iterations , where the projection act componentwise by clipping to zero negative components .",
    "we have presented convergence results for accelerated projected gradient methods to find a minimizer of an @xmath0 penalized functional .",
    "the innovation due to the introduction of ` condition ( b ) ' is to guarantee strong convergence for the full sequence .",
    "numerical examples confirm that this algorithm can outperform ( in terms of cpu time ) existing methods such as the thresholded landweber iteration or even lars .",
    "it is important to remark that the speed of convergence may depend strongly on how the operator is available .",
    "because most of the time in the iterations is consumed by matrix - vector multiplications ( as is often the case for iterative algorithms ) , it makes a big difference whether @xmath63 is given by a full matrix or a sparse matrix ( perhaps sparse in the sense that its action on a vector can be computed via a fast algorithm , such as the fft or a wavelet transform ) .",
    "the applicability of the projected algorithms hinges on the observation that the @xmath18 projection on an @xmath0 ball can be computed with a @xmath505-algorithm , where @xmath506 is the dimension of the underlying space .",
    "there is no universal method that performs best for any choice of the operator , data , and penalization parameter . as a general rule of thumb",
    "we expect that , among the algorithms discussed in this paper for which we have convergence proofs ,    * the thresholded landweber algorithm ( [ ddd04iteration ] ) works best for an operator @xmath63 close to the identity ( independently of the sparsity of the limit ) , * the projected steepest descent algorithm works best for an operator with a relatively nice spectrum , i.e. , with not too many zeroes ( also independently of the sparsity of the minimizer ) , and * the exact ( lars ) method works best when the minimizer is sparse in absolute terms .",
    "obviously , the three cases overlap partially , and they do not cover the whole range of possible operators and data . in future work we intend to investigate algorithms that would further improve the performance for the case of a large ill - conditioned matrix and a minimizer that is relatively sparse with respect to the dimension of the underlying space .",
    "we intend , in particular , to focus on proving convergence and other mathematical properties of .",
    "m.  f. acknowledges the financial support provided by the european union s human potential programme under the contract moif - ct-2006 - 039438 .",
    "i.  l. is a post - doctoral fellow with the f.w.o .- vlaanderen ( belgium ) .",
    "m.f . and i.l .",
    "thank the program in applied and computational mathematics , princeton university , for the hospitality during the preparation of this work .",
    "i.  d. gratefully acknowledges partial support from nsf grants dms-0245566 and 0530865 .",
    "a.  chambolle , r.  a. devore , n .- y .",
    "lee , and b.  j. lucier , _ nonlinear wavelet image processing : variational problems , compression , and noise removal through wavelet shrinkage _ , ieee trans .",
    "image process .",
    "* 7 * ( 1998 ) , no .  3 , 319335 .                  .",
    "dahlke , m. fornasier , t. raasch , r. stevenson , and m. werner , _ adaptive frame methods for elliptic operator equations : the steepest descent approach _ , i m a j. numer . anal .",
    "( 2007 ) , ` doi 10.1093/imanum / drl035 ` .    .",
    "dahlke and p. maass , _ an outline of adaptive wavelet galerkin methods for tikhonov regularization of inverse parabolic problems .",
    "_ , hon , yiu - chung ( ed . ) et al .",
    ", recent development in theories and numerics .",
    "proceedings of the international conference on inverse problems , hong kong , china , january 9 - 12 , 2002 . river edge , nj :",
    "world scientific .",
    "56 - 66 , 2003 .                            m.  a.  t. figueiredo , r.  d. nowak , and s.  j. wright , _ gradient projection for sparse reconstruction : application to compressed sensing and other inverse problems _ , to appear in ieee journal of selected topics in signal processing , ( 2007 ) .",
    "i.  loris , g.  nolet , i.  daubechies , and f.  a. dahlen , _ tomographic inversion using @xmath0-norm regularization of wavelet coefficients _ , geophysical journal international * 170 * ( 2007 ) , no .  1 , 359370 .",
    "mallat , _ a wavelet tour of signal processing .",
    "_ , san diego , ca : academic press . , 1999 ."
  ],
  "abstract_text": [
    "<S> regularization of ill - posed linear inverse problems via @xmath0 penalization has been proposed for cases where the solution is known to be ( almost ) sparse . </S>",
    "<S> one way to obtain the minimizer of such an @xmath0 penalized functional is via an iterative soft - thresholding algorithm . </S>",
    "<S> we propose an alternative implementation to @xmath0-constraints , using a gradient method , with projection on @xmath0-balls . </S>",
    "<S> the corresponding algorithm uses again iterative soft - thresholding , now with a variable thresholding parameter . </S>",
    "<S> we also propose accelerated versions of this iterative method , using ingredients of the ( linear ) steepest descent method . </S>",
    "<S> we prove convergence in norm for one of these projected gradient methods , without and with acceleration . </S>"
  ]
}