{
  "article_text": [
    "although the concept of entropy plays important roles in diverse fields ranging from classical thermodynamics of clausius , @xcite quantum mechanics and uncertainty , @xcite black holes , @xcite coding and computation,@xcite to information technology , @xcite it does not seem to have a standard definition . here , we are interested in its application to nonequilibrium statistical thermodynamics . in classical thermodynamics , it is defined as a _ thermodynamic _ quantity with no association of any notion of a microstate and its probability , while modern approach to  statistical thermodynamics , primarily due to boltzmann and gibbs , requires a probabilistic approach in terms of microstates . in this work ,",
    "we are primarily interested in an isolated system @xmath10 .",
    "quantities for the isolated system will carry a suffix @xmath11 ; quantities without the suffix will refer to any body , which need not be isolated , such as a part @xmath12 of @xmath10 ; see fig .",
    "[ fig.sys ] .",
    "the microstates for @xmath10 are determined by the set @xmath13 of extensive observables ( the energy @xmath14 , the volume @xmath15 , the number of particles @xmath16 , etc . ) specifying the isolated system .",
    "while temporal evolution is not our primary interest in this work , we still need to remember the importance of temporal evolution in thermodynamics .",
    "we will say that two microstates belonging to the microstate subspace @xmath17 are `` connected '' if one evolves from the other after some time @xmath18 . before this time",
    ", they will be treated as `` disconnected . ''",
    "let @xmath19 denote the maximum @xmath20 over all pairs of microstates .",
    "the space @xmath17 is _ simply connected _ for all times longer than @xmath19 in that each microstate can evolve into another microstate @xmath21 in due time . for @xmath22 ,",
    "the space @xmath17 will consist of _ disjoint components , _ an issue that neither boltzmann nor gibbs has considered to the best of our knowledge .",
    "but the issue , which we consider later in sec .",
    "[ marker_disjoint space ] , becomes important in considering nonequilibrium states .",
    "[ ptb ]    system_modified_1.eps    boltzmann assumes _ equal probability _ of various microstates in the simply connected set @xmath17 .",
    "thus , @xmath19 can be identified with the equilibration time @xmath23 for @xmath10 .  under the equiprobable assumption",
    ", boltzmann identifies the entropy in terms of the number of microstates@xcite @xmath24 in @xmath25 : @xmath26 we will set the boltzmann constant to be unity throughout the work so that the entropy will always be a _",
    "pure number_. the idea behind the above formula implicitly appears for the first time in a paper @xcite by boltzmann , and then appears more or less in the above form later in his lectures@xcite where he introduces the combinatorial approach for the first time to statistical mechanics .",
    "the formula itself does not appear but is implied when he takes the logarithm of the number of combinations.@xcite ( there is another formulation for entropy given by boltzmann,@xcite which is also known as the boltzmann entropy,@xcite that we will discuss later and that has a restricted validity ; see eq .",
    "( [ boltzmann_s_1 ] ) . )",
    "gibbs , also using the probabilistic approach , gives the following formula for the entropy in a _",
    "canonical ensemble_:@xcite@xmath27 where @xmath28 is the canonical ensemble probability of the @xmath29th microstate of the system , and the sum is over all microstates corresponding to all possible energies @xmath30 ( with other elements in @xmath31 held fixed ) ; their set is denoted by @xmath32 in the above sum .",
    "the gibbsian approach assumes an ensemble at a given instant , while the boltzmann approach considers the evolution of a particular system in time ; see for example a recent review.@xcite in equilibrium , both entropy expressions yield the same result . in quantum mechanics ,",
    "this entropy is given by the von neumann entropy formulation@xcite in terms of the density matrix @xmath33:@xmath34 the entropy formulation in the information theory@xcite has a form that appears to be similar in form to the above gibbs entropy even though the temperature has no significance in the information theory .",
    "there is also another statistical formulation of entropy , heavily used in the literature , in terms of the phase space distribution function @xmath35 , which follows from boltzmann s celebrated h - theorem:@xmath36 here , @xmath37 denotes a point in the phase space .",
    "this quantity is not only not dimensionless but , as we will show later , is not the correct formulation in general ; see eq .",
    "( [ discrete_s ] ) .",
    "the classical thermodynamics entropy @xmath38 is oblivious to the microstates and their probabilities and deals with @xmath39 , etc . as the observables of the system . in equilibrium",
    ", the entropy @xmath40is a state function , and can be expressed as a function of the observables .",
    "this functional dependence results in the gibbs fundamental relation @xmath41 in terms of the observables @xmath42 . for a lattice model",
    ", @xmath38 is non - negative in accordance with the boltzmann definition of @xmath43 , but is known to become negative for a continuum model such as for an ideal gas .",
    "the latter observation implies that such continuum models are not realistic as they violate nernst s postulate(the third law ) and require _ quantum mechanics _ to ensure non - negativity of the entropy.@xcite even the change @xmath8 , the heat capacity , etc .",
    "do not satisfy thermodynamic consequences of nernst s postulate .    by invoking nernst s postulate ( the equilibrium entropy vanishes at absolute zero )",
    ", one can determine the equilibrium entropy everywhere _",
    "uniquely_. the consensus is that in equilibrium , the thermodynamic entropy is not different from the above statistical entropies due to boltzmann and gibbs .",
    "however , there is at present no consensus when the system is out of equilibrium .",
    "there is also some doubt whether the nonequilibrium thermodynamic entropy has any meaning in classical thermodynamics .",
    "we will follow clausius  and take the view here that the thermodynamic entropy is a well - defined notion even for an irreversible process going on in a body for which clausius@xcite writes @xmath44 in terms of the exchange heat @xmath45 with the medium .",
    "the question that arises is whether the two statistical definitions can be applied to a body out of equilibrium .",
    "we find the answer to be affirmative .",
    "the next question that arises is the following : do they always give the same results ?",
    "we will show that under certain conditions , they give the same results .",
    "this is important as the literature is not very clear on this issue.@xcite     for an isolated system , which we mostly consider here , we are not concerned with any thermostat or external working source . as a consequence ,",
    "observables @xmath14,@xmath15,@xmath16 , etc . in @xmath13",
    "must remain constant even if the system is out of equilibrium . while this will simplify our discussion to some extent",
    ", it will also create a problem as we will discuss later .",
    "we discuss the concept of a nonequilibrium state , state variables and state functions in the next section .",
    "we introduce the statistical entropy formulation in sec .",
    "iii and show its equivalence with thermodynamic nonequilibrium entropy when the latter is a state function . in sec .",
    "iv , we carry out an explicit quantum calculation of the nonequilibrium statistical entropy .  in sec .",
    "v , we consider a @xmath2-d lattice model  appropriate for tonks gas in continuum so that the statistical lattice entropy can be calculated rigorously .",
    "we take the continuum limit and compare the resulting entropy with the continuum entropy of the tonks gas and obtain an interesting result .",
    "we discuss the semi - classical approximation of the statistical entropy in sec .",
    "vi and show that the formulation in eq .",
    "( [ sf_s ] ) does not determine the entropy .",
    "a brief summary and discussion is presented in the final section .",
    "the second law states that the irreversible ( denoted by a suffix i ) entropy generated in any infinitesimal physical process going on in a body satisfies the inequality@xmath46 the equality occurs for a reversible process . for an isolated system , there is no exchange ( denoted by a suffix e ) entropy change @xmath47 with the medium so that @xmath48 in any arbitrary process . in an isolated system",
    "satisfies@xmath49 the law refers to the _ thermodynamic entropy_. it is not a state function in the conventional sense ( @xmath50 ) if @xmath10 is not in equilibrium simply because as a state function @xmath50 must remain constant for constant @xmath13 ; see below also .",
    "as the thermodynamic entropy is not measurable except when the process is reversible , the second law remains useless as a computational tool .",
    "in particular , it says nothing about the rate at which the irreversible entropy increases .",
    "therefore , it is useful to obtain a computational formulation of the entropy , the _ statistical entropy_. this will be done in the next section .",
    "the onus is on us to demonstrate that the statistical entropy also satisfies this law if it is to represent the thermodynamic entropy .",
    "this by itself does not prove that the two are the same .",
    "it is not been possible to show that the statistical entropy is identical to the thermodynamic entropy in general . here , we show their equivalence only when the nonequilibrium thermodynamic entropy is a _ state function _ of nonequilibrium state variables to be introduced below .      for an isolated body in equilibrium",
    ", the entropy can be expressed as a function of its observables ( variables that can be controlled by an observer ) , as is easily seen form the gibbs fundamental relation in eq .",
    "( [ gibbs_fundamental ] ) .",
    "the thermodynamic state , also known as the macrostate @xmath51 of a body in equilibrium remains the same unless it is disturbed .",
    "therefore , we can identify the equilibrium state @xmath52 of the body by its observables .",
    "accordingly , its equilibrium entropy @xmath50 can be expressed as a function of its observables @xmath13 .",
    "this is expressed by saying that the equilibrium entropy is a _ state function _ and @xmath13  is the set of _ state variables_.    the above conclusion is most certainly not valid for a body out of equilibrium , which  we take to be isolated .",
    "if the body is not in equilibrium , its ( macro)state @xmath53 will continuously change , which is reflected in its entropy change ( increase ) in time ; this requires expressing its entropy as @xmath54 with an _ explicit time - dependence _ , since @xmath55 for an isolated body .",
    "the change in the entropy and the macrostate must come from the variations of additional variables , distinct from the observables , that keep changing with time until the body comes to equilibrium as explained elsewhere.@xcite these variables can not be controlled by the observer .",
    "once the body has come to equilibrium , the entropy has no explicit time - dependence and becomes a state function . in this state",
    ", the entropy has its maximum possible value for given @xmath13 .",
    "in other words , when the entropy becomes a state function , it achieves the maximum possible value for the given set of state variables , here given by @xmath13 .",
    "this conclusion about the entropy will play an important role below .",
    "we assume that there is a set @xmath56 of  additional variables , known as the _ internal variables _ ( sometimes also called hidden variables )",
    ". we will refer to the variables in @xmath13 and @xmath56  as ( _ nonequilibrium _ )",
    "_ state variables _ ( see below for justification ) and denote them collectively as @xmath57 in the following . from theorem 4 presented",
    "elsewhere,@xcite it follows that with a proper choice of the number of internal variables , the entropy can be written as @xmath58 with no explicit @xmath59-dependence .",
    "the situation is now almost identical to that of an isolated body in equilibrium :  the entropy is a function of @xmath60 with no explicit time - dependence .",
    "this allows us to identify @xmath60  as the set of nonequilibrium state variables .",
    "thus , @xmath53 can be specified by @xmath60 so that the entropy becomes a _",
    "state function_. this allows us to extend eq .",
    "( [ gibbs_fundamental ] ) to@xmath61 in which the partial derivatives are related to the fields of the system:@xmath62 these fields will change in time unless the system has reached equilibrium .    as @xmath60  changes in time , @xmath53 changes , but at each instant the ( nonequilibrium ) entropy as a state function , has a maximum possible value for given @xmath60 even though @xmath63 . in our previous work,@xcite",
    "we have identified this particular state as an _ internal equilibrium state _ , but its physical significance as presented above was not discussed . for a state that is not in internal equilibrium",
    ", the entropy must retain an explicit time - dependence . in this case , the derivatives in eq .",
    "( [ fields_isolated ] ) can not be identified as state variables like , temperature , pressure , etc .",
    "it may appear to a reader that the concept of entropy being a state function is very restrictive .",
    "this is not the case as this concept , although not recognized by several workers , is implicit in the literature where the relationship of the thermodynamic entropy with state variables is investigated . to appreciate this",
    ", we observe that the entropy of a body in internal equilibrium@xcite is given by the boltzmann formula@xmath64 in terms of the number of microstates corresponding to @xmath65 . in classical nonequilibrium thermodynamics,@xcite the entropy is always taken to be a state function . in the edwards approach@xcite for granular materials , all microstates are equally probable as is required for the above boltzmann formula .",
    "bouchbinder and langer@xcite assume that the nonequilibrium entropy is given by eq .",
    "( [ boltzmann_s_extended ] ) .",
    "lebowitz@xcite also takes the above formulation for his definition of the nonequilibrium entropy . as a matter of fact",
    ", we are not aware of any work dealing with entropy computation that does not assume the nonequilibrium entropy to be  a state function . this does not , of course , mean that all states of a system are internal equilibrium states . for states that are not in internal equilibrium ,",
    "the entropy is not a state function so that it will have an explicit time dependence .",
    "but , as shown elsewhere,@xcite this can be avoided by enlarging the space of internal variables .",
    "the choice of how many internal variables are needed will depend on experimental time scales and can not be answered in generality at present .",
    "we hope to come back to this issue in a future publication .    for a general body that is not isolated",
    ", the concept of its internal equilibrium state plays a very important role in that the body can come back to this state several times in a nonequilibrium process . in a cyclic nonequilibrium process",
    ", such a state can repeat itself in time after some cycle time @xmath66 so that all state variables and functions including the entropy repeat themselves:@xmath67 this ensures @xmath68 in a cyclic process .",
    "all that is required for the cyclic process to occur is that the body must start and end in the same internal equilibrium state ; however , during the remainder of the cycle , the body need not be in internal equilibrium .",
    "we provide a very general formulation of the statistical entropy , which will also demonstrate that the entropy is a _ statistical average_. we consider a macrostate @xmath69 of @xmath10  at a given instant @xmath59 . in the following ,",
    "we suppress @xmath59 unless necessary .",
    "the macrostate @xmath70 refers to the set of microstates @xmath71  and their probabilities @xmath72 . for the computation of combinatorics ,",
    "the probabilities are handled in the following abstract way .",
    "we consider a large number @xmath73 of independent _ replicas _ or _",
    "samples _ of @xmath10 , with @xmath74  some large constant integer and @xmath75  the number of distinct microstates @xmath76 .",
    "the samples should be thought of as identically prepared experimental samples.@xcite let @xmath77 denote the sample space spanned by @xmath76 .        as @xmath78 @xmath17 , @xmath77",
    "is simply connected if @xmath17 is , which we assume in this section .",
    "let @xmath79 denote the number of @xmath76-samples ( samples in the @xmath76-microstate ) so that@xmath80 the above sample space is a generalization of the _ ensemble _ introduced by gibbs , except that the latter is restricted to an equilibrium system , whereas our sample space refers to the system in any arbitrary state so that @xmath81 may be time - dependent",
    ". in the semi - classical approximation , see sec .",
    "[ marker_semiclassical ] , one can similarly take the sample space to represent the classical phase space of boltzmann",
    ". the ( _ sample _ or _ ensemble _ ) _ average _ of some quantity @xmath82  over these samples is given by@xmath83 where @xmath84 is the value of @xmath82 in @xmath76 .",
    "the samples are , by definition , _ independent _ of each other so that there are no correlations among them .",
    "because of this , we can treat the samples to be the outcomes of some random variable , the macrostate @xmath53 .",
    "this independence property of the outcomes is crucial in the following , and does not imply that they are equiprobable .",
    "the number of ways @xmath85 to arrange the @xmath86 samples into @xmath75 distinct microstates is@xmath87 taking its natural log to obtain an _ additive _ quantity per sample@xmath88 and using stirling s approximation , we see easily that @xmath89 , which we hope to identify later with the entropy @xmath58 of the isolated system , can be written as the average of the negative of@xmath90 what gibbs @xcite calls the _ index of probability : _",
    "@xmath91 where we have also shown an explicit time - dependence for the reason that will become clear below .",
    "the above derivation is based on fundamental principles and does not require the system to be in equilibrium ; therefore , it is always applicable . to the best of our knowledge",
    ", even though such an expression has been extensively used in the literature , it has been used _ without _ any derivation ; one simply appeals to this form by invoking it as the information entropy ; however , see sec .",
    "[ marker_summary ] .    because of its similarity in form with @xmath92 in eq .",
    "( [ gibbs_s ] ) , we will refer to @xmath93 as the gibbs statistical entropy from now on .",
    "as the nonequilibrium thermodynamic entropy for a process in which the system is always in internal equilibrium can be determined by integrating the gibbs fundamental relation in eq .",
    "( [ gibbs_fundamental_extended ] ) , we can compare it with the statistical entropy introduced above .",
    "however , such an integration is not possible for a process involving states that are arbitrary ( not in internal equilibrium ) .",
    "therefore , there is no meaning to compare @xmath93 with the corresponding thermodynamic entropy whose value can not be determined . to identify @xmath94 with the nonequilibrium",
    "thermodynamic entropy requires the following additional steps :    1 .",
    "it is necessary to establish that @xmath0 satisfies eq .",
    "( [ second_law ] ) .",
    "2 .   for an equilibrium canonical system",
    ", it is necessary to establish that @xmath95 is identical to the equilibrium thermodynamic entropy given by @xmath92.@xcite 3 .",
    "it is necessary to show that @xmath0 is identical to the nonequilibrium thermodynamic entropy of the system that is out of equilibrium but whose entropy is a state function .",
    "there are several proofs available in the literature@xcite for ( 1 ) .",
    "therefore , we will not be concerned with ( 1 ) anymore .",
    "we will prove ( 2 ) and ( 3 ) in sect .",
    "[ marker_open system ] .",
    "the maximum possible value of @xmath0 for given @xmath60 occurs when @xmath76 are _ equally probable_:@xmath96  in this case , the explicit time dependence in @xmath0 will _ disappear _ and we have @xmath97 which is identical in form to the boltzmann ( thermodynamic ) entropy in eq .",
    "( [ boltzmann_s ] ) for an isolated body in equilibrium , except that the current formulation has been extended to an isolated body out of equilibrium ; see also eq . (",
    "[ boltzmann_s_extended ] ) .",
    "the only requirement is that all microstates in @xmath98 are equally probable .",
    "the statistical entropy in this case becomes a state function .    applying the above formulation to a macrostate characterized by a given @xmath13 and consisting of microstates @xmath99 forming the set @xmath100 with probabilities @xmath101 , we find that @xmath102 is the entropy of this macrostate , where @xmath103 is the number of distinct microstates @xmath104",
    ". it should be obvious that@xmath105 again , under the equiprobable assumption @xmath106 @xmath17  denoting the space spanned by microstates @xmath107 , the above entropy takes its maximum possible value@xmath108 which is identical in value to the boltzmann ( thermodynamic ) entropy in eq .",
    "( [ boltzmann_s ] ) for an isolated body in equilibrium .",
    "the maximum value occurs at @xmath109 .",
    "it is evident that@xmath110\\leq\\mathcal{s}_{0}[\\mathbf{z}_{0}(t)]\\leq\\mathcal{s}_{0}(\\mathbf{x}_{0 } ) .",
    "\\label{entropy_inequalities0}\\ ] ]    the anticipated identification of nonequilibrium thermodynamic entropy with @xmath89 under some restrictions allows us to identify @xmath89 as the _ statistical entropy _ formulation of the thermodynamic entropy . from now on",
    ", we will refer to the general entropy in eq .",
    "( [ gibbs_formulation ] ) in terms of microstate probabilities as the _ time - dependent gibbs formulation _ of the entropy or simply the _ gibbs entropy _ , and will not make any distinction between the statistical and thermodynamic entropies .",
    "accordingly , we will now use the regular symbol @xmath38 for @xmath89 throughout the work , unless clarity is needed .",
    "we will refer to @xmath58 in terms of microstate number @xmath111 in eq .",
    "( [ s_boltzmann0])as the _ time - dependent boltzmann formulation _ of the entropy or simply the boltzmann entropy,@xcite whereas @xmath50 in eq .",
    "( [ s_boltzmann ] ) represents the equilibrium ( boltzmann ) entropy .",
    "it is evident that the gibbs formulation in eqs .",
    "( [ gibbs_formulation ] ) and ( [ conventional_entropy0 ] ) supersedes the boltzmann formulation in eqs.([s_boltzmann0 ] ) and ( [ s_boltzmann ] ) , respectively , as the former contains the latter as a special limit .",
    "however , it should be also noted that there are competing views on which entropy is more general.@xcite we believe that the above derivation , being general , makes the gibbs formulation more fundamental .",
    "the continuity of @xmath112 follows directly from the continuity of @xmath113 .",
    "the existence of the statistical entropy @xmath112 follows from the observation that it is bounded above by @xmath114 and bounded below by @xmath11 , see eq .",
    "( [ s_boltzmann0 ] ) .",
    "it should be stressed that @xmath115 is not the number of microstates of the @xmath116 replicas ; the latter is given by @xmath117^{\\mathcal{n}}$ ] .",
    "thus , the entropy in eq .",
    "( [ ensemble_entropy_formulation ] ) should not be confused with the boltzmann entropy , which would be given by @xmath118 .",
    "it should be mentioned at this point that boltzmann uses the combinatorial argument to obtain the entropy of a gas , see eq .",
    "( [ boltzmann_s_1 ] ) , resulting in an expression similar to that of the gibbs entropy in eq .",
    "( [ gibbs_s ] ) except that the probabilities appearing in his formulation represents the probability of various discrete states of a particle , and should not be confused with the microstate probabilities used here ; see sec .",
    "[ marker_jaynes ] .",
    "the approach of boltzmann is _ limited _ to that of an ideal gas only and is not general as it neglects the correlations present due to the interactions between particles.@xcite on the other hand , our approach is valid for any system with any arbitrary interactions between particles as all microstates in the collection are _",
    "independent_.      using the above formulation of @xmath119 , we have determined@xcite the statistical formulation of the entropy for a system @xmath12 , which is a small but macroscopically large part of @xmath10 ; see fig .",
    "[ fig.sys ] .",
    "it is assumed that the system and the medium are _ quasi - independent _ so that @xmath1 can be expressed as a sum of entropies @xmath120 and @xmath121 of the system and the medium , respectively : @xmath122 the two statistical entropies are given by an identical formulation @xmath123 respectively . here",
    ", @xmath124 with probability @xmath125 denotes a microstate of @xmath12 and @xmath126 with probability @xmath127 that of the medium . in the derivation,@xcite",
    "we have neither assumed the medium nor the system to be in internal equilibrium ; only quasi - independence is assumed .",
    "the above formulation of statistical entropies will not remain valid if the two are not quasi - independent .",
    "the same will also be true of the thermodynamic entropies .",
    "for the system to be in internal equilibrium , its statistical entropy @xmath120 must be maximized under the constraints imposed by the medium .",
    "the constraints are on the average values of the state variables : @xmath128 where @xmath129  is the value of @xmath130 in @xmath124 .",
    "the condition for internal equilibrium is obtained by varying @xmath125 without changing the microstates , i.e. @xmath129 .  using the lagrange multiplier technique , it is easy to see that the condition for this in terms of the lagrange multipliers whose definitions are obvious is @xmath131 the lagrange multipliers are the same for all microstates and the scalar product is over the elements in the set @xmath129",
    ". it now follows that@xmath132 using the same scalar product as above .",
    "it is now easy to identify the lagrange multipliers by observing that @xmath133 comparing this relation with the gibbs fundamental relation for the system , which follows from eq .",
    "( [ gibbs_fundamental_extended ] ) when applied to the system , we find@xmath134 accepting this identification now allows us to conclude that the statistical entropy @xmath120 in eq .",
    "( [ entropies ] ) is no different than the nonequilibrium thermodynamic entropy of the same system in internal equilibrium but in a medium . a special case of such a system is the ( equilibrium ) canonical ensemble of gibbs .",
    "this proves ( 2 ) mentioned in sect .",
    "[ marker_probabilities ] .  in equilibrium ,",
    "the lagrange multipliers associated with the internal variables vanish and eq . ( [ general entropy differential ] )",
    "reduce to@xmath135 the significance of @xmath136  is quite obvious . in internal equilibrium , it is given by @xmath137 moreover , as the nonequilibrium entropy in internal equilibrium is a state function , it can in principle be measured or calculated by integrating eq .",
    "( [ general entropy differential ] ) . therefore",
    ", its value can be compared with the statistical entropy .",
    "the above identification in eq .",
    "( [ general entropy differential ] ) then proves ( 3 ) .",
    "if the thermodynamic entropy is not a state function , it can not be measured or computed .",
    "thus , while the statistical entropy can be computed in principle in all cases , as shown below explicitly , there is no way to compare its value with the thermodynamic entropy in all cases .",
    "thus , no comment can be made about their relationship in general .",
    "we merely conjecture that as the two entropies are the same when the thermodynamic entropy is a state function , it is no different from its statistical analog even when it is not a state function .      the consideration of dynamics  resulting in the simple connectivity of the sample ( or phase ) space has played a pivotal role in developing the kinetic theory of gases,@xcite where the interest is at high temperatures.@xcite as dynamics is very fast here , it is well known that the ensemble entropy agrees with its temporal formulation . however , at low temperatures , where dynamics becomes sluggish as in a glass,@xcite the system can be _ confined _ into disjoint components .    sample ( or phase ) space confinement at a phase transition such as a liquid - gas transition is well known in equilibrium statistical mechanics.@xcite it also occurs when the system undergoes symmetry breaking such as during magnetic transitions , crystallizations , etc .",
    "but confinement can also occur under nonequilibrium conditions , when the observational time scale @xmath138  becomes shorter than the equilibration time @xmath139,@xcite such as for glasses , whose behavior and properties have been extensively studied .",
    "the issue has been recently considered by us,@xcite where only energy as an observable was considered .",
    "the discussion is easily extended to the present case when confinement occurs for whatever reasons into one of the thermodynamically significant number of disjoint components @xmath140 , each component corresponding to the same set @xmath57 or @xmath130 ( we suppress the dependence for simplicity ) , depending on whether the body is isolated or not . such a situation arises , for example , in ising magnets at the ferromagnetic transition . ,",
    "where the system is either confined to @xmath141 with positive magnetization or @xmath142 with negative magnetization .",
    "even a weak external magnetic field @xmath143 , that we can _ control _ as an observer , will allow the system to make a choice between the two parts of @xmath144 .",
    "it just happens that in this case @xmath145 and is thermodynamically insignificant .",
    "the situation with glasses or other amorphous materials is very different.@xcite in the first place , @xmath144  is a union of _ thermodynamically significant _",
    "number @xmath146  disjoint components .",
    "in the second place , there is no analog of a symmetry breaking field .",
    "therefore , there is no way to prepare a sample in a given component @xmath147 .",
    "thus , the samples will be found in all different components .",
    "taking into consideration disjointness of the components generalizes the number of configurations in eq .",
    "( [ combinations ] ) to@xmath148 where @xmath149 denotes the number of sample in the microstate @xmath150 in the @xmath151-th component . in terms of @xmath152 , this combination immediately leads to@xmath153 for the statistical entropy of the system and has already been used earlier@xcite by us ; see sec .",
    "4.3.3 there .  from what has been said above ,",
    "this statistical entropy is also the thermodynamic entropy of a nonequilibrium state under component confinement for which the entropy is a state function of @xmath57 .",
    "therefore , as before , we take @xmath89 to be the general expression of the nonequilibrium thermodynamic entropy and use @xmath38 in place of @xmath89 .",
    "introducing@xmath154 it is easy to see@xcite that @xmath155 here , the entropy of the component @xmath147 in terms of the reduced microstate probability @xmath156 is@xmath157 so that the first contribution is its average over all components .",
    "the second term is given by@xmath158 and represents the component entropy .",
    "it is this entropy that is related to the residual entropy@xcite in disordered systems .",
    "the same calculation for a system in a medium will result in an identical formulation for the entropy as in eq . ( [ s_component ] ) except that the sum is over components and microstates of the system .",
    "we consider a gas of non - interacting identical structureless particles with no spin , each of mass @xmath159 , confined to a @xmath2-dimensional box of initial size @xmath160 with impenetrable walls ( infinite potential well ) .",
    "initially , the gas is in thermodynamic _ equilibrium _ with a medium at fixed temperature @xmath161 and pressure  @xmath162 .",
    "the gas is then isolated by disconnecting it from the medium . in time , the isolated gas expands , may be in a nonequilibrium fashion .",
    "we wish to calculate its entropy as a function of the box size @xmath163 .     as there are no interactions between the particles , the wavefunction @xmath164 for the gas is a product of individual particle wavefunctions @xmath165 .",
    "thus , we can focus on a single particle to study the nonequilibrium behavior of the gas.@xcite the simple model of a particle in a box has been extensively studied in the literature but with a very different emphasis .",
    "@xcite the particle only has non - degenerate eigenstates whose energies are determined by @xmath166 , @xmath167 , and  a quantum number @xmath168 .",
    "we use the energy scale @xmath169 to measure the energy of the eigenstate so that@xmath170 the corresponding eigenfunctions are given by@xmath171 the pressure generated by the eigenstate on the walls is given by @xcite @xmath172 in terms of the eigenstate probability @xmath173 , the average energy and pressure are given by    [ particle energy pressure]@xmath174 the entropy follows from eq .",
    "( [ gibbs_formulation ] ) and is given for the single particle case by    @xmath175 the time dependence in @xmath176  or @xmath177 is due to the time dependence in @xmath125 and @xmath178 .",
    "even for an isolated system , for which @xmath179 remains constant , @xmath125 can not remain constant when the gas is not in equilibrium if @xmath180 is held fixed after expansion .",
    "this follows directly from the second law@xcite and creates a conceptual problem because the eigenstates are mutually orthogonal and there can be no transitions among them to allow for a change in @xmath125 .      a way to change @xmath125 in an isolated system is to require the presence of some stochastic interactions , whose presence allows for transitions among eigenstates.@xcite as these transitions are happening within the system , we can treat them as `` chemical reactions '' between different eigenstates@xcite by treating each eigenstate @xmath168 as a chemical species . during the transition , these species undergoes chemical reactions to allow for the changes in their probabilities .",
    "we follow this analogy further and extend the traditional approach@xcite to the present case . for the sake of simplicity",
    ", our discussion will be limited to the ideal gas in a box ; the extension to any general system is trivial .",
    "therefore , we will use microstates instead of eigenstates in the following to keep the discussion general .",
    "let there be @xmath181 particles in the @xmath168th microstate at some instant @xmath59 so that @xmath182 at all times , and @xmath183 .",
    "we will consider the general case that also includes the case in which final microstates refer to a box size @xmath184 different from its initial value @xmath180 .",
    "let us use @xmath185 to denote the reactants ( initial microstates ) and @xmath186 to denote the products ( final microstates ) .",
    "for the sake of simplicity of argument , we will assume that transitions between microstates is described by a single chemical reaction , which is expressed in stoichiometry form as @xmath187 let @xmath188 and @xmath189 denote the population of @xmath185 and @xmath186 , respectively , so that @xmath190 .",
    "accordingly , @xmath183 for the reactant and @xmath191 for the product . the single reaction is described by a single extent of reaction @xmath192 and we have @xmath193 it is easy to see that the coefficients satisfy an important relation@xmath194 which reflects the fact that the change @xmath195 in the reactant microstates is the same as in the product microstates .",
    "the _ affinity _ in terms of the chemical potentials is given by@xmath196 and will vanish only in `` equilibrium , '' i.e. only when @xmath125 s attain their equilibrium values .",
    "otherwise , @xmath197 will remain non - zero .",
    "it acts as the thermodynamic force in driving the chemical reaction.@xcite but we must wait long enough for the reaction to come to completion , which happens when @xmath197 and @xmath198 both vanish .",
    "the extent of reaction @xmath192 is an example of an internal variable .",
    "for the ideal gas under consideration , there does not seem to be any other internal variable as particles have no internal structures . in the following",
    ", we will assume only one internal variable @xmath199 .",
    "[ ptb ]    entropy_comparison.eps      the box expands as a function of time , which need not be quasi - static ( extremely slow ) so there is no reason to assume that the gas remains in equilibrium after expansion .",
    "the entropy of the gas per particle can be obtained by calculating @xmath200 for the particle under consideration .",
    "henceforth , we will call @xmath201 the entropy of the particle , which shares the property that the irreversible entropy change @xmath202 will never be negative .",
    "all the above discussion about the chemical reaction is easily translated to the study of a particle in box without any change .",
    "the change @xmath203 is caused by the transitions between different eigenstates .",
    "we consider the gas in _ equilibrium _ at some initial temperature @xmath161 in a box of length @xmath204 , which we take to be @xmath205 .",
    "this is obtained by keeping the box in a medium of temperature @xmath161 .",
    "the corresponding microstate probabilities follow the boltzmann law ( @xmath206):@xmath207 where @xmath208 denotes the equilibrium partition function .",
    "the energy per particle in this gas is denoted by @xmath209 obtained by replacing @xmath210 by @xmath211 in eq .",
    "( [ particle energy pressure ] ) ; the corresponding pressure is @xmath212 .",
    "the equilibrium entropy can be obtained by using @xmath211 in @xmath213 given in eq .",
    "( [ entropies ] ) .",
    "the initial temperature @xmath161 for @xmath205 is taken to be @xmath214 so that the initial energy @xmath215 .",
    "we now consider equilibrium states having the same initial thermodynamic energy @xmath209 for different values of @xmath180 , even though the eigenstate energies and their boltzmann probabilities vary with @xmath180 .",
    "the corresponding equilibrium entropy @xmath216  as a function of @xmath180 is shown in fig .",
    "[ fig.entropy ] by the continuous curve .",
    "since the energy is constant , the product @xmath217 is also constant ; see eq .",
    "( [ particle energy pressure ] ) .",
    "thus , @xmath218 is a decreasing function of @xmath180 . from the slope of the upper curve in fig .",
    "[ fig.entropy ] which decreases with @xmath180 , we also conclude that @xmath219 is a decreasing function of @xmath180 .",
    "thus , @xmath220 is an increasing function of @xmath180 .",
    "however , our calculation to be presented elsewhere @xcite shows that @xmath221 is also an increasing function of @xmath180 .",
    "the eigenstates for a box of size @xmath180 are given in eq .",
    "( [ eigenfunctions ] ) .",
    "we now consider nonequilibrium states .",
    "for this , we isolate the box from its medium and consider its free expansion as it expands suddenly from @xmath204 to a new size @xmath222 .",
    "because of its isolation , its energy remains @xmath209 during this expansion .",
    "as the expansion is sudden , the initial eigenfunctions @xmath223 for @xmath204 have no time to change , but are no longer the eigenfunctions of the new size @xmath180 ; the latter are given by @xmath224 in eq .",
    "( [ eigenfunctions ] ) for @xmath180 .",
    "however , @xmath223 can be expanded in terms of @xmath224 as a sum over @xmath168 .",
    "we call this the quantum superposition principle .",
    "the corresponding expansion coefficients @xmath225 are easily seen to be@xcite@xmath226 using @xmath225  and @xmath227 , we can determine the probability @xmath228 for the @xmath168th microstate in the new box , which allows us to determine all thermodynamic averages for the new box .",
    "we have checked that the new probabilities add to @xmath2 and that the ( average ) energy after the free expansion is equal to @xmath209 to within our computational accuracy .",
    "thus , @xmath229 in the sudden expansion .",
    "this is consistent with the fact that the gas does no external work and that no external heat is exchanged .",
    "despite this , the free expansion is spontaneous once the confining walls have moved .",
    "therefore , the ( thermodynamic ) entropy of the gas must increase in this process in accordance with the second law .",
    "we use @xmath228  to evaluate the nonequilibrium statistical entropy , which is shown by the dashed curve in fig .",
    "[ fig.entropy ] .",
    "the significance of this curve is as follows : choose a particular value @xmath180 in this graph .",
    "then , the nonequilibrium entropy for this @xmath180 is given by numerically evaluating the sum @xmath230 this is the entropy after the sudden expansion from the initial state at @xmath205 and follows from the quantum superposition principle .",
    "evidently , this entropy is higher that the initial equilibrium entropy @xmath231 .",
    "it is also obvious that this entropy has a memory of the initial state at @xmath205 and @xmath214 .",
    "therefore , it does not represent the equilibrium entropy .",
    "if we now wait at the new value of @xmath180 , the isolated gas in the new box will relax to approach its equilibrium state in which its nonequilibrium entropy will gradually increase until it becomes equal to its value on the upper curve .",
    "a careful reader would have realized by this time that the proposed entropy form in eq .",
    "( [ gibbs_formulation ] ) is not at all the same as the standard classical formulation of entropy , such as for the ideal gas , which can be negative at low temperatures or at high pressures .",
    "the issue has been discussed elsewhere@xcite but with a very different perspective . here , we visit the same issue from a very different perspective that allows us to investigate if and how the entropy in continuum models is related to the proposed entropy in this work . for this , we turn to a very simple continuum model in classical statistical mechanics : the tonks gas,@xcite which is an athermal model and contains the ideal gas as a limiting case when the rod length @xmath232 vanishes .",
    "we will simplify the discussion by considering the tonks gas in one dimension .",
    "the gas consists of @xmath233  impenetrable rods , each of length @xmath232 lying along a line of length @xmath180 .",
    "we will assume @xmath233 to be fixed , but allow @xmath232 and @xmath180 to change with the state of the system , such as its pressure .. the configurational entropy per rod determined by the configurational partition function is found to be@xcite@xmath234 , \\label{tonks_s}\\ ] ] where @xmath235 is the `` volume '' available per rod @xmath236 .",
    "even though the above result is derived for an equilibrium tonks gas , it is easy to see that the same result also applies for the gas in internal equilibrium .",
    "the only difference is that the parameters in the model are also functions of internal variables now .",
    "the entropy vanishes when @xmath237 and becomes negative for all @xmath238 .",
    "indeed , it diverges to @xmath239 in the incompressible limit @xmath240 .",
    "this is contrary to the boltzmann approach in which the entropy is determined by the number of microstates ( cf .",
    "( [ boltzmann_s ] ) ) or the gibbs approach ( cf .",
    "( [ gibbs_formulation ] ) ) and can never be negative .",
    "can we reconcile the contradiction between the continuum entropy and the current statistical formulation ?",
    "we now demonstrate that the above entropy for the tonks gas is derivable from the current statistical approach under some approximation , to be noted below , by first considering a lattice model for the tonks gas and then taking its continuum limit .",
    "it is in the lattice model can we determine the number of microstates . in a continuum , this number is always _ unbounded _ ( see below also ) . for this",
    "we consider a @xmath2-d lattice @xmath241 with @xmath242 sites ; the lattice spacing , the distance between two consecutive sites , is given by @xmath6 .",
    "we take @xmath243 so that @xmath244 is the length of the the lattice @xmath241 .",
    "we randomly select @xmath233 sites out of @xmath242 .",
    "the number of ways , which then represents the number of configurational microstates , is given by@xmath245 after the choice is made , we replace each selected site by @xmath246 consecutive sites , each site representing an atoms in a rod , to give rise to a rod of length @xmath247 .",
    "it is clear that @xmath6  also changes with the state of the system .",
    "the number of sites in the resulting lattice @xmath248 is @xmath249 so that the length of @xmath250 is given by @xmath251 since @xmath252 .",
    "we introduce the number densities @xmath253 , @xmath254 and @xmath255 .",
    "a simple calculation shows that @xmath256 is given by@xmath257.\\ ] ] this result can also be obtained by taking the athermal entropy for a polydisperse polymer solution a bethe lattice@xcite by setting the coordination number @xmath258 to be @xmath259 .",
    "we now take the continuum limit @xmath260  for _ fixed _ @xmath261 and @xmath33 , that is _ fixed _",
    "@xmath262 and @xmath263 , respectively . in this limit , @xmath264 , and @xmath265 .",
    "use of these limits in @xmath213 yields@xmath266 the continuum limit of the entropy from the boltzmann approach has resulted in a diverging entropy regardless of the value of @xmath261,@xcite a well known result . by introducing an arbitrary _ constant _ @xmath267 with the dimension of length ,",
    "we can rewrite @xmath213 as@xmath268 in which the first term remains finite in the continuum limit , and the second term contains the divergence .",
    "the diverging part , although explicitly independent of @xmath261 , still depends on the state of the gas through @xmath6 , and can not be treated as a constant unless we assume @xmath6  to be independent of the state of the gas .",
    "it is a common practice to approximate the lattice spacing @xmath6 as a constant . in that case , the diverging term represents a constant that can be subtracted from @xmath269 . recognizing that @xmath270 , we see that the first term in eq .",
    "( [ cont_manipulation ] ) is nothing but the entropy of  the tonks gas in eq .",
    "( [ tonks_s ] ) for the arbitrary constant @xmath271 .",
    "however , this equivalence only occurs in the state independent constant-@xmath6 approximation .",
    "as the second term above has been discarded , the continuum entropy @xmath272 also has _ no _ simple relationship with the number ( @xmath273 ) of microstates in the continuum limit , which means that the continuum entropy can not be identified as the boltzmann entropy in eq .",
    "( [ s_boltzmann ] ) . to see this more clearly ,",
    "let us focus on the centers of mass of each rod , which represent one of the @xmath233 sites that were selected in @xmath241 .",
    "each of the @xmath168 sites @xmath274 , @xmath275 , is free to move over @xmath262 .",
    "volume @xmath276 , also called the probability and denoted by @xmath277 by boltzmann,@xcite of the corresponding phase space @xmath278 is @xmath279 . however , contrary to the conventional wisdom,@xcite @xmath280 does not yield @xmath272 .",
    "the correct expression is given by the gibbs - modified adimensional volume @xmath281 , i.e. @xmath282 the presence of @xmath283 is required to restrict the volume due to indistinguishability of the rods  la gibbs . for large @xmath233 , this quantity correctly gives the entropy @xmath272 .",
    "however , this quantity is not only not an integer , it also can not be _ always _ larger than or equal to unity , as noted above .",
    "the analog of a quantum microstate in classical statistical mechanics is normally obtained by recognizing that in the _ adiabatic approximation _ , each small phase space cell of volume element @xmath284 in terms of generalized coordinates @xmath258 and momenta @xmath285 of size",
    "@xmath286 corresponds to a microstate,@xcite where @xmath287 is the degrees of freedom of the system .",
    "the latter follows from the bohr - sommerfeld quantization rule for a periodic motion .",
    "the adiabatic approximation requires the parameters characterizing the system to vary extremely slowly .",
    "we will assume one such parameter @xmath151 ( such as the volume @xmath288 ) so that @xcite @xmath289 where @xmath290 is the period of oscillation for constant @xmath151 ; the system would be isolated in the latter case . in the above approximation , the energy of the system will vary very slowly and can be taken to be constant over a period of oscillation .",
    "the action taken over the closed path for the constant value of @xmath151 and the energy is quantized @xcite:@xmath291 this observation is the justification of the above cell size of a classical microstate .",
    "thus , the number of `` classical '' microstates is given by@xmath292 where @xmath293 is the phase space volume corresponding to the system .",
    "this allows us to divide the phase space into @xmath294 cells , index by @xmath168 , of volume @xmath295 and `` centered '' at @xmath274 at some time which we call the initial time @xmath296 .",
    "we will denote the evolution of the cell at time @xmath59 by the location of its `` center '' @xmath297 and its volume element @xmath298 . in terms of the distribution function @xmath35 in the phase space , the @xmath168th microstate probability therefore is given by@xmath299 evidently , @xmath300 at all times .",
    "the entropy and the average of any observable @xmath301 of the system are given by    @xmath302,\\label{discrete_s}\\\\ \\overline{o}(t )   &   \\equiv{\\textstyle\\sum\\nolimits_{k } } o(x_{k},t)f(x_{k},t)dx_{k}(t ) , \\label{discrete_o}\\ ] ]    the sum being over all  @xmath294 microstates . while it is easy to see that continuum analogs for eqs .",
    "( [ f - normalization ] ) and ( [ discrete_o ] ) are easily identified , this is not so for the entropy in eq .",
    "( [ discrete_s]).@xcite however , it should be obvious that @xmath303 in eq .",
    "( [ sf_s ] ) can not be a candidate for the statistical entropy @xmath304    it is well known @xcite that the system in the adiabatic limit remains in the same quantum state . for hamiltonian dynamics , the conservation of the phase space cell volume under evolution ensures @xmath295 for each cell so that @xmath305 .",
    "this results in the incompressibility of the phase space . in equilibrium , @xmath306 is not only uniform but also constant , and we conclude from eq .",
    "( [ f - normalization ] ) that this value is @xmath307 .",
    "accordingly , @xmath308 in equilibrium as expected and we obtain the equilibrium entropy @xmath309 .",
    "the situation is far from clear for nonequilibrium states . as the example of expansion of the box shows ,",
    "the system is no longer restricted to be in the same microstate , which means that the microstate energy is no longer a constant and the phase space trajectory is no longer closed .",
    "thus , the suitability of the bohr - sommerfeld quantization is questionable , and care must be exercised to identify the microstates .",
    "we will adopt the following prescription .",
    "we consider some equilibrium state ( uniform @xmath310 ) of the isolated system to identify the cell volume @xmath311 @xmath286 .",
    "once the identification has been done , we will no longer worry about its relationship with @xmath286 , and only deal with the cells .",
    "we then follow the evolution of each cell in time in a nonequilibrium process during which @xmath305 may not hold .",
    "thus the volume of each cell may no longer be constant .",
    "the process may also result in changes in @xmath306.@xcite indeed , it is quite possible that @xmath312 diverges at the same time that @xmath313 vanishes.@xcite however , their product , which determines the microstate probability , must remain strictly bounded and @xmath314 .",
    "in particular , as the cell volume shrinks to zero , @xmath312 must diverge to keep the product bounded .",
    "thus , the divergence@xcite of @xmath312 alone does not imply that the entropy diverges to negative infinity .",
    "this is easily seen by the following 1-d damped oscillator , the standard prototype of a dissipative system:@xcite @xmath315 with a ( positive ) damping coefficient , which is chosen such that @xmath316 just for simplicity .",
    "we have the case of aperiodic damping .",
    "we will only consider the long time behavior .",
    "it is easy to see that in this limit ( @xmath317 )    @xmath318 and their product remains bounded , as expected .",
    "boltzmann@xcite provides the following alternative expression of the entropy@xcite in terms of a single particle probability @xmath319 for the particle to be in the @xmath320th state:@xmath321 not to be confused with that in eq .",
    "( [ boltzmann_s ] ) .",
    "boltzmann is only interested in the maximum entropy , which occurs when all states are equally probable . in this case , @xmath322 where @xmath323 is the number of possible states of a single particle in the gas . in general ,",
    "particles are not independent due to interactions and number of possible states @xmath324 .",
    "accordingly , maximum gibbs entropy @xmath325 per particle is _ less _ than the corresponding equiprobable boltzmann entropy @xmath326 .",
    "however , jaynes@xcite gives a much stronger results , see his eq .",
    "( 5):@xmath327 the equality occurs only if there are no interactions between the particles , as we have asserted above .",
    "recognizing that there does not exists a first principles statistical formulation of nonequilibrium thermodynamic entropy for an isolated system in terms of microstate probabilities , we have attempted to fill in the gap .",
    "we use a formal approach ( frequentist interpretation of probability ) by extending the equilibrium ensemble of gibbs to a nonequilibrium ensemble , which is nothing but a large number @xmath86 of samples of the thermodynamic system under consideration .",
    "accordingly , we refer to the ensemble as a sample space .",
    "the formal approach enables us to evaluate the combinatorics for a given set of microstate probabilities .",
    "the resulting statistical entropy is independent of the number of samples and depends only on the probabilities as is seen from eqs .",
    "( [ gibbs_formulation ] ) and ( [ s_component ] ) .",
    "thus , the use of a large number of samples is merely a formality and is not required in practice .",
    "we have shown that in equilibrium , the statistical entropy is the same as the equilibrium thermodynamic entropy : @xmath328 .",
    "but we have also shown that the statistical entropy is equal to the nonequilibrium thermodynamic entropy , provided the latter is a state function of the nonequilibrium state variables @xmath60 : @xmath329 .",
    "we can not make any comment about the relationship between @xmath93 and @xmath119 for the simple reason that there is no way to measure or calculate a non - state function @xmath119 .",
    "we should remark here that the standard approach to calculate nonequilibrium entropy is to use the classical nonequilibrium thermodynamics@xcite or its variant , which treats the entropy at the local level as a state function .",
    "some readers may think that our statistical formulation is no different than that used in the information theory . we disagree .",
    "for one , there is no concept of internal variables @xmath56 in the latter theory .",
    "because of this , our approach allows us to consider three levels of description so that we can consider three different entropies @xmath330 and @xmath50 satisfying the inequalities in eq .",
    "( [ entropy_inequalities0 ] ) .",
    "the information theory can only deal with two levels of entropies .",
    "there is also no possibility of a residual entropy in the latter .    for an isolated system in internal equilibrium ( @xmath331 for @xmath332 )",
    ", just a single sample will suffice to determine the entropy as samples are _ unbiased_. the entropy in this case is no different than the `` entropy '' @xmath333 of a single sample:@xcite@xmath334 where @xmath335 represents @xmath111 or @xmath103 .",
    "however , this simplicity is lost as soon as the system is not in internal equilibrium . here",
    ", one must consider averaging over all microstates .",
    "changes in microstate probabilities result in changes in the entropy .",
    "there are two ways probabilities can change within an isolated system , both of them being irreversible in nature .",
    "one cause of changes is due to the quantum nature as seen in the sudden expansion of the box . here",
    ", the parameter @xmath151 ( @xmath336 ) changes non - adiabatically and creates irreversibility .",
    "the resulting irreversible change in the entropy for the @xmath2-@xmath337 gas has been calculated and shown by the lower curve in fig .",
    "[ fig.entropy ] .",
    "the other cause of probability changes is due to the `` chemical reaction '' going on among the microstates that brings about equilibration in the system .",
    "the corresponding irreversible rise in the entropy for the gas is shown by the difference between the two curves in fig .",
    "[ fig.entropy ] .",
    "the interaction of a body with its medium can also result in the changes in microstate probabilities , and has been considered elsewhere.@xcite    we consider the continuum analog of the statistical formulation of entropy and show that the standard formulation , @xmath303 in eq . ( [ sf_s ] ) , is not a good candidate of the nonequilibrium entropy .",
    "it is then argued that the divergence of @xmath312 in some cases , see the discussion above , makes @xmath303 diverge to @xmath239 , even though the statistical entropy remains finite and positive .",
    "thus , @xmath303 can not be equated with our statistical formulation , a generalization of the gibbs formulation .",
    "we suggest that our statistical gibbs formulation can be applied to any nonequilibrium state .                                      the number of combinations in eq .",
    "( 35 ) on p. 56 in boltzmann@xcite is denoted by @xmath277 , but it is not the number of microstates .",
    "the two become the same only when @xmath277 is maximized as discussed on p. 58 ."
  ],
  "abstract_text": [
    "<S> we consider an isolated system in an arbitrary state and provide a general formulation using first principles for an _ additive _ and _ non - negative _ statistical quantity @xmath0 that is shown to reproduce the equilibrium thermodynamic entropy of the isolated system . </S>",
    "<S> we further show that @xmath0 represents the nonequilibrium thermodynamic entropy @xmath1 when the latter is a state function of nonequilibrium state variables ; see text . </S>",
    "<S> we consider an isolated @xmath2-d ideal gas and determine its non - equilibrium statistical entropy @xmath0 as a function of the box size as the gas expands freely isoenergetically , and compare it with the equilibrium thermodynamic entropy @xmath3 . </S>",
    "<S> we find that @xmath4 @xmath3 in accordance with the second law , as expected . to understand how @xmath0 is different from thermodynamic entropy of classical continuum models that is known to become _ negative _ under certain conditions , we calculate @xmath5for a @xmath2-d lattice model and discover that it can be related to the thermodynamic entropy of the continuum @xmath2-d tonks gas by taking the lattice spacing @xmath6 go to zero . </S>",
    "<S> however , @xmath7 @xmath8 since @xmath6 is state - dependent . </S>",
    "<S> we discuss the semi - classical approximation of our entropy and show that the standard quantity @xmath9 in the boltzmann s h - theorem , see eq . </S>",
    "<S> ( [ sf_s ] ) , does not directly correspond to the statistical entropy . </S>"
  ]
}