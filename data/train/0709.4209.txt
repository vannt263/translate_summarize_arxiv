{
  "article_text": [
    "networks of interacting genes coordinate complex cellular processes , such as responding to stress , adapting the metabolism to a varying diet , maintaining the circadian cycle or producing an intricate spatial arrangement of differentiated cells during development @xcite .",
    "the success of such regulatory modules is at least partially characterized by their ability to produce reliable responses to repeated stimuli or changes in the environment over a wide dynamic range , and to perform the genetic computations reproducibly , either on a day - by - day or generation timescale . in doing",
    "so the regulatory elements are confronted by noise arising from physical processes that implement such genetic computations , and this noise ultimately traces its origins back to the fact that the state variables of the system are concentrations of chemicals and `` computations '' are really reactions between individual molecules , usually present at low copy numbers @xcite .",
    "it is useful to picture the regulatory module as a device that , given some input , computes an output , which in our case will be a set of expression levels of regulated genes .",
    "sometimes the inputs to the module are easily identified , such as when they are the actual chemicals that a system detects and responds to , for example chemoattractant molecules , hormones or transcription factors ( tfs ) .",
    "there are cases , however , when it is beneficial to think about the inputs on a more abstract level : in embryonic development we talk of `` positional information '' and think of the regulatory module as trying to produce a different gene expression footprint at each spatial location @xcite ; alternatively , circadian clocks generate distinguishable gene expression profiles corresponding to various phases of the day @xcite . regardless of whether we view the input as a physical concentration of some transcription factor or perhaps a position within the embryo , and whether the computation is complicated or as simple as an inversion produced by a repressor",
    ", we want to quantify its reliability in the presence of noise , and ask what the biological system can do to maximize this reliability .",
    "if we make many observations of a genetic regulatory element in its natural conditions we are collecting samples drawn from a distribution @xmath0 , where @xmath1 describes the state of the input and @xmath2 the state of the output . saying that the system is able to produce a reliable response @xmath2 across the spectrum of naturally occurring input conditions @xmath3 amounts to saying that the dependency  either linear or strongly non - linear  between the input and output is high , i.e. far from random",
    "shannon has shown how to associate a unique measure , the mutual information @xmath4 , with the notion of dependency between two quantities drawn from a joint distribution @xcite :    @xmath5    the resulting quantity is a measure in bits and is essentially the logarithm of the number of states in the input that produce distinguishable outputs given the noise . a device that has one bit of capacity",
    "can be thought of as an `` on - off '' switch , two bits correspond to four distinguishable regulatory settings , and so on .",
    "although the input is usually a continuous quantity , such as nutrient concentration or phase of the day , the noise present in the regulatory element corrupts the computation and does not allow the arbitrary resolution of a real - valued input to propagate to the output ; instead , the mutual information tells us how precisely different inputs are distinguishable to the organism .",
    "experimental or theoretical characterization of the joint distribution , @xmath0 , for a regulatory module can be very difficult if the inputs and outputs live in a high - dimensional space .",
    "we can proceed , nevertheless , by remembering that the building blocks of complex modules are much simpler , and finally must reduce to the point where a single gene is controlled by transcription factors that bind to its promoter region and tune the level of its expression . while taking a simple element out of its network will not be illuminating about how the network as a whole behaves in general  especially if there are feedback loops  there may be cases where the information flow is `` bottlenecked '' through a single gene , and its reliability will therefore limit that of the network .",
    "in addition , the analysis of a simple regulatory element will provide directions for taking on more complicated systems ; see ref @xcite for a recent related analysis .",
    "our aim is therefore to understand the reliability of a simple genetic regulatory element , that is , of a single activator or repressor transcription factor controlling the expression level of its downstream gene .",
    "we will identify the concentration @xmath6 of the transcription factor as the only input , @xmath7 , and the expression level of the downstream gene @xmath8 as the only relevant output , @xmath9 . the regulatory element itself",
    "will be parametrized by input / output kernel , @xmath10 , i.e. the distribution ( as opposed to a `` deterministic '' function @xmath11 in case of a noiseless system ) of possible outputs given that the input is fixed to some particular level @xmath6 . for each such kernel",
    ", we will then compute the maximum amount of information , @xmath12 , that can be transmitted through it , and examine how this capacity depends on the properties of the kernel .",
    "( red dotted lines ) ; in this simple example we assume that the distribution is gaussian and therefore fully characterized by its mean and variance.,width=288 ]",
    "the input / output kernel of a simple regulatory element , @xmath10 , is determined by the biophysics of transcription factor - dna interaction , transcription and translation . in contrast , the distribution of inputs , @xmath13 , that the cell uses during its `` typical '' lifetime , is free for the cell to change .",
    "the cell s transcription factor expression footprint is its representation of the environment and internal state , and the form of this representation can be the target of adaptation or evolutionary processes .",
    "together , the input / output kernel and the distribution of inputs define the joint distribution , @xmath14 , and consequently the mutual information of eq ( [ genmutinf ] ) between the input and the output , @xmath12 .    maximizing the information between the inputs and outputs , which corresponds to our notions of reliability in representation and computation ,",
    "will therefore imply a specific matching between the given input / output kernel and the distribution of inputs , @xmath13 , that is being optimized . if one believes that a specific regulatory element has been tuned for maximal information transmission , then the optimal solution for the inputs , @xmath15 , and the resulting optimal distribution of outputs , @xmath16 , become experimentally verifiable predictions .",
    "if , on the other hand , the system is not really maximizing information transmission , then the capacity achievable with a given kernel and its optimal input distribution , @xmath17 $ ] , can still be regarded as a ( hopefully revealing ) upper bound on the true information transmission of the system . during the past",
    "decades the measurements of regulatory elements have focused on recovering the mean response of a gene under the control of a transcription factor that had its activity modulated by experimentally adjustable levels of inducer or inhibitor molecules @xcite .",
    "typically , a sigmoidal response is observed with a single regulator , as in fig [ f - schemeio2 ] , and more complicated regulatory `` surfaces '' are possible when there are two or more simultaneous inputs to the system @xcite . in our notation , these experiments measure the conditional average over the distribution of outputs , @xmath18 .",
    "developments in flow cytometry and single - cell microscopy enabled the experimenters to start tracking in time and across the population of cells the expression levels of fluorescent reporter genes and thus open a window into the behavior of fluctuations .",
    "consequently , work exploring the noise in gene expression , or @xmath19 , has begun to accumulate , on both the experimental and biophysical modeling side @xcite . the efforts to further characterize and understand this noise were renewed by theoretical work by swain and coworkers @xcite",
    "that has shown how to separate intrinsic and extrinsic components of the noise , i.e. the noise due to the stochasticity of the observed regulatory process in a single cell , and the noise contribution that arises because typical experiments make many single - cell measurements and the internal chemical environments of these cells differ across the population .",
    "we start by showing how the optimal distributions can be computed analytically if the input / output kernel is gaussian and the noise is small , and proceed by presenting the exact numerical solution later . let us assume then that the first and second moments of the conditional distribution are given , and write the input / output kernel as a set of gaussian distributions @xmath20 , or explicitly : @xmath21 ^ 2}\\over{2 \\sigma_g^2 ( c ) } }   \\bigg{\\ } } , \\label{gaussianioo}\\ ] ] where both the mean response , @xmath22 , and the noise , @xmath23 , depend on the input , as illustrated in fig [ f - schemeio2 ] .",
    "we rewrite the mutual information between the input and the output of eq ( [ genmutinf ] ) in the following way : @xmath24 the first term can be evaluated exactly for gaussian distributions , @xmath25 .",
    "the integral over @xmath8 is just the calculation of the ( negative of the ) entropy of the gaussian , and the first term therefore evaluates to @xmath26   \\rangle_{p(c)}=-\\frac{1}{2}\\langle \\log_2{2\\pi e \\sigma^2_g(c)}\\rangle_{p(c)}$ ] .    in the second term of eq",
    "( [ mutinf ] ) , the integral over @xmath8 can be viewed as calculating @xmath27 under the distribution @xmath10 . for an arbitrary continuous function",
    "@xmath28 we can expand the integrals with the gaussian measure around the mean : @xmath29 the first term of the expansion simply evaluates to @xmath30 .",
    "the series expansion would end at the first term if we were to take the small noise limit , @xmath31 .",
    "the second term of the expansion is zero because of symmetry , and the third term evaluates to @xmath32 .",
    "we apply the expansion of eq ( [ mutexp ] ) and compute the second term in the expression for the mutual information , eq ( [ mutinf ] ) , with @xmath33 . taking only the zeroth order of the expansion , we get @xmath34;\\ ] ]",
    "we can rewrite the probability distributions in terms of @xmath35 , using @xmath36 . to maximize the information transmission we form the following lagrangian and introduce the multiplier @xmath37 that keeps the resulting distribution normalized : @xmath38=-\\int d\\bar{g}\\,p(\\bar{g})\\log_2\\left(\\sqrt{2\\pi e}\\sigma_g(\\bar{g } ) p(\\bar{g})\\right)- \\lambda\\int d\\bar{g}\\,p(\\bar{g}).\\ ] ] the optimal solution",
    "is obtained by taking a variational derivative with respect to @xmath39 , @xmath40}{\\delta p(\\bar{g})}=0 $ ] .",
    "the solution is @xmath41 by inserting the optimal solution , eq ( [ optg ] ) , into the expression for mutual information , eq ( [ mutinf ] ) , we get an explicit result for the capacity : @xmath42 , \\label{iopt}\\ ] ] where @xmath43 is the normalization of the optimal solution in eq ( [ optg ] ) : @xmath44    the optimization with respect to the distribution of inputs , @xmath13 , has led us to the result for the optimal distribution of _ mean outputs _",
    ", eq ( [ optg ] ) .",
    "we had to assume that the input / output kernel is gaussian and that the noise is small , and we refer to this result as the small - noise approximation ( sna ) for channel capacity .",
    "note that in this approximation only the knowledge of the noise in the output as a function of mean output , @xmath45 , matters for capacity computation and the direct dependence on the input @xmath6 is irrelevant .",
    "this is important because the behavior of intrinsic noise as a function of the mean output is an experimentally accessible quantity @xcite .",
    "note also that for big enough noise the normalization constant @xmath43 will be small compared to @xmath46 , and the small - noise capacity approximation of eq ( [ iopt ] ) will break down by predicting negative information values .",
    "simple regulatory elements usually have a monotonic , saturating input / output relation , as shown in fig [ f - schemeio2 ] , and ( at least ) a shot noise component whose variance scales with the mean .",
    "if the noise strength is increased , the information transmission must drop and , even with the optimally tuned input distribution , eventually yield only a bit or less of capacity .",
    "intuitively , the best such a noisy system can do is to utilize only the lowest and highest achievable input concentrations , and ignore the continuous range in between .",
    "thus , the mean responses will be as different as possible , and the noise at low expression will also be low because it scales with the mean .",
    "more formally , if only @xmath47 are used as inputs , then the result is either @xmath48 or @xmath49 ; the optimization of channel capacity reduces to finding @xmath50 , with @xmath51 .",
    "this problem can be solved by realizing that each of the two possible input concentrations produces their respective gaussian output distributions , and by maximizing information by varying @xmath50 . simplifying even further",
    ", we can threshold the outputs and allow @xmath8 to take on only two values instead of a continuous range ; then , each of the two possible inputs , `` min '' and `` max '' , maps into two possible outputs , `` on '' and `` off '' , and confusion in the channel arises because `` min '' input might be misunderstood as `` on '' output and vice versa with probabilities given by the output distribution overlaps , as shown schematically in fig [ f - bna ] .     and full @xmath52 induction as trying to convey a single binary decision , and construct the corresponding encoding table ( inset ) by discretizing the output using the threshold @xmath53 .",
    "the capacity of such an asymmetric binary channel is degraded from the theoretical maximum of 1 bit , because the distributions overlap ( blue and red ) . for unclipped gaussians",
    "the optimal threshold @xmath53 is at the intersection of two alternative pdfs , but in general one searches for the optimal @xmath53 that maximizes information in eq ( [ bnas]).,width=288 ]    in the latter case we can use the analytic formula for the capacity of the binary asymmetric channel . if @xmath54 is the probability of detecting an `` off '' output if `` max '' input was sent , and @xmath55 is a probability of receiving an `` off '' output if `` min '' input was sent , and @xmath56 is a binary entropy function : @xmath57 then the capacity of such asymmetric channel is @xcite : @xmath58 because this approximation reduces the continuous distribution of outputs to only two choices , `` on '' or `` off '' , it can underestimate the true channel capacity and is therefore a lower bound .",
    "the information between the input and output in eq ( [ mutinf ] ) can be maximized numerically for any input / output kernel , @xmath10 , if the variables @xmath6 and @xmath8 are discretized , making the solution space that needs to be searched , @xmath59 , finite .",
    "one possibility is to use a gradient descent - based method and make sure that the solution procedure always stays within the domain boundaries @xmath60 for every @xmath61 .",
    "alternatively , a procedure known as blahut - arimoto algorithm has been derived specifically for the purpose of finding optimal channel capacities @xcite .",
    "both methods yield consistent solutions , but we prefer to use the second one because of faster convergence and convenient inclusion of constraints on the cost of coding ( see appendix [ ap : opt ] for details )",
    ".    one should be careful in interpreting the results of such naive optimization and worry about the artifacts introduced by discretization of input and output domains .",
    "after discretization , the formal optimal solution is no longer required to be smooth and could , in fact , be composed of a collection of dirac - delta function spikes . on the other hand , the real ,",
    "physical concentration @xmath6 can not be tuned with arbitrary precision in the cell ; it is a result of noisy gene expression , and even if this noise source were removed , the _ local _ concentration at the binding site would still be subject to fluctuations caused by randomness in diffusive flux @xcite .",
    "the blahut - arimoto algorithm is completely agnostic as to which ( physical ) concentrations belong to which bins after concentration has been discretized , and so it could assign wildly different probabilities to concentration bins that differ in concentration by less than @xmath62 ( i.e. the scale of local concentration fluctuations ) , making such a naive solution physically unrealizable . in appendix",
    "[ ap : opt ] we describe how to properly use blahut - arimoto algorithm despite the difficulties induced by discretization .",
    "if enough data were available , one could directly sample @xmath10 and proceed by calculating the optimal solutions as described previously . here",
    "we start , in contrast , by assuming a gaussian model of eq ( [ gaussianioo ] ) , in which the mean , @xmath22 , and the output variance , @xmath63 , are functions of the transcription factor concentration , @xmath6 .",
    "our goal for this section is to build an effective microscopic model of transcriptional regulation and gene expression , and therefore define both functions with a small number of biologically interpretable parameters . in the subsequent discussion we plan to vary those and thus systematically observe the changes in information capacity .    in the simplest picture ,",
    "the interaction of the tf with the promoter site consists of binding with a ( second order ) rate constant @xmath64 and unbinding at a rate @xmath65 . in a somewhat more complicated case where @xmath66 tf molecules cooperatively activate the promoter ,",
    "the analysis still remains simple as long as the favorable interaction energy between the tfs is sufficient to make only the fully occupied ( and thus activated ) and the empty ( and thus inactivated ) states of the promoter likely ; this effective two - state system is once more describable with a single rate for switching off the promoter , @xmath65 , and the corresponding activation rate has to be @xmath67 ( see ref @xcite , in particular appendix b ) .",
    "generally , therefore , the equilibrium occupancy of the site will be : @xmath68 where the hill coefficient , @xmath66 , captures the effects of cooperative binding , and @xmath69 is the equilibrium constant of binding . the mean expression level @xmath8 is then : @xmath70 where @xmath35 has been normalized to vary between 0 and 1 , and @xmath71 is the maximum expression level . in what follows",
    "we will assume the activator case , where @xmath72 , and present the result for the repressor at the end .",
    "the fluctuations in occupancy have a ( binomial ) variance @xmath73 and a correlation time @xmath74 @xcite .",
    "if the expression level of the target gene is effectively determined by the average of the promoter site occupancy over some window of time @xmath75 , then the contribution to variance in the expression level due to the `` on - off '' promoter switching will be : @xmath76 where in the last step we use the fact that @xmath77 .    at low tf concentrations the arrival times of single transcription factor molecules to the binding site",
    "are random events .",
    "recent measurements @xcite seem to be consistent with the hypothesis that this variability in diffusive flux contributes an additional noise term @xcite , similar to the berg - purcell limit to chemoattractant detection in chemotaxis .",
    "the noise in expression level due to fluctuations in the binding site occupancy , or the total _ input _",
    "noise , is therefore a sum of this diffusive component ( see eq ( 11 ) of ref @xcite ) and the switching component of eq ( [ varf1 ] ) : @xmath78 where @xmath79 is the diffusion constant for the tf and @xmath80 is the receptor site size , @xmath81 for a typical binding site on the dna .    to compute the information capacity in the small noise limit using the simple model developed",
    "so far we need the constant @xmath43 from eq ( [ zz ] ) , which is defined as an integral over expression levels .",
    "as both input noise terms are proportional to @xmath82 , the integral must take the form : @xmath83 where @xmath84 is a function that approaches a constant as @xmath85 .",
    "strangely , we see that this integral diverges near full induction ( @xmath86 ) , which means that the information capacity also diverges .",
    "naively we expect that modulations in transcription factor concentration are _ not _ especially effective at transmitting regulatory information once the relevant binding sites are close to complete occupancy .",
    "more quantitatively , the sensitivity of the site occupancy to changes in tf concentration , @xmath87 , vanishes as @xmath88 , and hence small changes in tf concentration will have vanishingly small effects .",
    "our intuition breaks down , however , because in thinking only about the mean occupancy we forget that even very small changes in occupancy could be effective if the noise level is sufficiently small .",
    "as we approach complete saturation , the variance in occupancy decreases , and the correlation time of fluctuations becomes shorter and shorter ; together these effects cause the standard deviation as seen through an averaging time @xmath75 to decrease faster than @xmath87 , and this mismatch is the origin of the divergence in information capacity .",
    "of course the information capacity of a physical system ca nt really be infinite ; there must be an extra source of noise ( or reduced sensitivity ) that becomes limiting as @xmath89 .",
    "the noise in eq ( [ inputnoise ] ) captures only the _ input _ noise , i.e. the noise in the protein level caused by the fluctuations in the occupancy of the binding site .",
    "in contrast , the _ output _ noise arises even when the occupancy of the binding site is fixed ( for example , at full induction ) , and originates in the stochasticity in transcription and translation .",
    "the simplest model postulates that when the activator binding site is occupied with fractional occupancy @xmath90 , mrna molecules are synthesized in a poisson process at a rate @xmath91 that generates @xmath92 mrna molecules on average during the lifetime of a single mrna molecule , @xmath93 .",
    "every message is a template for the production of proteins , which is another poisson process with rate @xmath94 .",
    "if the integration time is larger than the lifetime of single mrna molecules , @xmath95 , the mean number of proteins produced is @xmath96 , and the variance associated with both poisson processes is @xcite : @xmath97 where @xmath98 is the burst size , or the number of proteins synthesized per mrna .",
    "we can finally put the results together by adding the input noise eq ( [ inputnoise ] ) and the output noise eq ( [ noise - output ] ) , and expressing both in terms of the normalized expression level @xmath22 : @xmath99 with the relevant parameters @xmath100 explained in table [ t - params ] .",
    "note that both repressor and activator cases differ only in the shape of the input noise contributions ( especially for low cooperativity @xmath66 ) .",
    "note further that the output noise increases monotonically with mean expression @xmath35 , while the input noise peaks at the intermediate levels of expression . to make the examination of the parameter space in the next section feasible , we set @xmath101 ; models with switching noise instead of diffusive noise produce qualitatively similar results .",
    ".gaussian noise model parameters .",
    "note that if burst size @xmath102 , then the output noise is determined by the average number of mrna molecules , @xmath103 .",
    "note further that if the on - rate is diffusion limited , i.e. @xmath104 , then both input noise magnitudes , @xmath105 and @xmath106 , are proportional and decrease with increasing @xmath65 , or alternatively , with increasing @xmath107 . [",
    "cols=\"<,^,<\",options=\"header \" , ]",
    "having at our disposal both a simple model of signals and noise and a numerical way of finding the optimal solutions given an arbitrary input / output kernel , we are now ready to examine the channel capacity as a function of the noise parameters from table [ t - params ] .",
    "our first result , shown in fig [ f - cpanel1 ] , concerns the simplest case of an activator with no cooperativity , @xmath108 ; for this case , the noise in eq ( [ actnoise ] ) simplifies to : @xmath109 here we have assumed that there are two relevant sources of noise , i.e. the output noise ( which we parametrize by @xmath110 and plot on the horizontal axis ) and the input diffusion noise ( parametrized by @xmath105 , vertical axis ) .",
    "each point of the noise plane in fig [ f - cpanel1]a therefore represents a system characterized by a gaussian noise model , eq ( [ gaussianioo ] ) , with variance given by eq ( [ exnoise ] ) above .",
    "as expected , the capacity increases most rapidly when the origin of the noise plane is approached approximately along its diagonal , whereas along each of the edges one of the two noise sources effectively disappears , leaving the system dominated by either output or input noise alone .",
    "we pick two illustrative examples , the blue and the red systems of figs [ f - cpanel1]b and [ f - cpanel1]c , that have realistic noise parameters .",
    "the blue system has , apart for the decreased cooperativity ( @xmath108 instead of @xmath111 ) , the characteristics of the bicoid - hunchback regulatory element in _ drosophila melanogaster _ @xcite ; the red system is dominated by output noise with characteristics measured recently for about 40 yeast genes @xcite .",
    "we would like to emphasize that both the small - noise approximation and the exact solution predict that these realistic systems are capable of transmitting more than 1 bit of regulatory information and that they , indeed , could transmit up to about 2 bits .",
    "in addition , we are also reminded that while the distributions ( for example the optimal output distribution in fig [ f - cpanel1]b ) can look bimodal and this has often been taken as an indication that there are two relevant states of the output , such distributions really can have capacities above 1 bit ; similarly , distributions without prominent features , such as monotonically decreasing optimal output distribution of fig [ f - cpanel1]c , should also not be expected necessarily to have low capacities .    ) and no cooperativity ( @xmath108 ) .",
    "panel a shows the exact capacity calculation ( thick line ) and the small noise approximation ( dashed line ) .",
    "panel b displays the details of the blue point in a : the noise in the output is shown as a function of the input , with a peak being characteristic of a dominant input noise contribution ; also shown is the exact solution ( thick black line ) and the small - noise approximation ( dashed black line ) to the optimal distribution of output expression levels .",
    "panel c similarly displays details of the system denoted by a red dot in a ; here the output noise is dominant and both approximate and exact solutions for the optimal distribution of outputs show a trend monotonically decreasing with the mean output . ,",
    "width=316 ]    a closer look at the overall agreement between the small - noise approximation ( dashed lines in fig [ f - cpanel1]a ) and the exact solution ( thick lines ) shows that the small - noise approximation underestimates the true capacity , consistent with our remark that for large noise the approximation will incorrectly produce negative results ; at the 2-bit information contour the approximation is about @xmath112 off but improves as the capacity is increased .    in the high noise regime",
    "we are making yet another approximation , the validity of which we now need to examine . in our discussion about the models of signals and noise we assumed that we can talk about the fractional occupancy of the binding site and the _ continuous _ concentrations of mrna , transcription factors and protein , instead of counting these species in discrete units , and that noise can effectively be treated as gaussian .",
    "both of these assumptions are the cornerstones of the langevin approximation for calculating the noise variance @xcite .",
    "if parameters @xmath110 and @xmath105 actually arise due to the underlying microscopic mechanisms described in the section [ sandn ] on signals and noise , we expect that at least for some large - noise regions of the noise plane the discreteness in the number of mrna molecules will become important and the langevin approximation will fail . in such cases (",
    "a much more time - consuming ) exact calculation of the input / output relations using the master equation is possible for some noise models ( see appendix [ ax : lan ] ) ; we show that in the region where @xmath113 the channel capacities calculated with gaussian kernels can be overestimated by @xmath114 or more ; there the langevin calculation gives the correct second moment , but misses the true shape of the distribution .",
    "although both examples with realistic noise parameters , b and c , of fig [ f - cpanel1 ] lie safely in the region where langevin approximation is valid , care should be used whenever both output noise @xmath110 and burst size are large , and @xmath110 is consequently dominated by small number of transcripts .    , with the noise model that includes output ( @xmath110 ) and input diffusion noise ( @xmath105 ) contributions ( see fig [ f - cpanel1 ] for absolute values of @xmath115 ) .",
    "panel b shows @xmath116 for the noise model that includes output noise ( @xmath110 ) and input switching noise ( @xmath106 ) contributions ; this plot is independent of cooperativity , @xmath66.,width=336 ]    is there any difference between activators and repressors in their capacity to convey information about the input ?",
    "we concluded section [ sandn ] on the noise models with separate expressions for activator noise , eq ( [ actnoise ] ) , and repressor noise , eq ( [ repnoise ] ) ; focusing now on the repressor case , we recompute the information in the same manner as we did for the activator in fig [ f - cpanel1]a , and display the difference between the capacities of the repressor and activator with the same noise parameters in fig [ f - actrep ] .",
    "as expected , the biggest difference occurs above the main diagonal , where the input noise dominates over the output noise . in this region",
    "the capacity of the repressor can be bigger by as much as third than that of the corresponding activator .",
    "note that as @xmath117 , the activator and repressor noise expressions become indistinguishable and the difference in capacity vanishes for the noise models with output and input diffusion noise contributions , eqs ( [ actnoise ] , [ repnoise ] ) .",
    "the behavior of the regulatory element is conveniently visualized in fig [ f - axs ] by plotting a cut through the noise plane along its main diagonal . moving along this cut scales the total noise variance of the system up or down by a multiplicative factor , and allows us to observe the overall agreement between the exact solution and small- and large - noise approximations .",
    "in addition we point out the following interesting features of fig [ f - axs ] that will be examined more closely in subsequent sections .",
    "first , the parameter region in non - cooperative case , in which the capacity falls below one bit and the large noise approximation is applicable , is small and shrinks further at higher cooperativity .",
    "this suggests that a biological implementation of a reliable binary channel could be relatively straightforward , assuming our noise models are appropriate .",
    "moreover , there exist distributions not specifically optimized for the input / output kernel , such as the input distribution uniform in @xmath118 that we pick as illustrative example in fig [ f - axs ] ( thick black line ) ; we find that this simple choice can achieve considerable information transmission , and are therefore motivated to raise a more general question about the sensitivity of channel capacity with respect to perturbations in the optimal solution , @xmath15 .",
    "we revisit this idea more systematically in the next section .    ; panel b , strong cooperativity , @xmath119 ) we take a cross - section through the noise plane in fig [ f - cpanel1 ] along the main diagonal , where the values for noise strength parameters @xmath110 and @xmath105 are equal .",
    "the exact optimal solution is shown in red . by moving along the diagonal of the noise plane ( and along the horizontal axis in the plots above )",
    "one changes both input and output noise by the same multiplicative factor @xmath120 , and since , in small - noise approximation , @xmath121 , @xmath122 , that factor results in an additive change in capacity by @xmath123 .",
    "we can use the large noise approximation lower bound on capacity for the case @xmath108 , in the parameter region where capacities fall below 1 bit.,width=336 ]    second , it can be seen from fig [ f - axs ] that at small noise the cooperativity has a minor effect on the channel capacity .",
    "this is perhaps unexpected as the shape of the mean response @xmath22 strongly depends on @xmath66 .",
    "we recall , however , that mutual information @xmath12 is invariant to any invertible reparametrization of either @xmath8 or @xmath6 .",
    "in particular , changing the cooperativity or the value of the equilibrium binding constant , @xmath69 , in theory only results in an invertible change in the input variable @xmath6 , and therefore the change in the steepness or midpoint of the mean response must not have any effect on @xmath12 .",
    "this argument does break down in the high noise regime , where the cooperative system achieves capacities above one bit while the non - cooperative system fails to do so .",
    "reparametrization invariance would work only if the input concentration could extend over the whole positive interval , from zero to infinity .",
    "the substantial difference between capacities of cooperative and non - cooperative systems in fig [ f - axs ] at _ low capacity _ stems from the fact that in reality the cell ( and our computation ) is limited to a finite range of concentrations , @xmath124 $ ] , instead of the whole positive half - axis , @xmath125 .",
    "we explore the issue of limited input dynamic range further in the next section .",
    "finally , we draw attention to the simple linear scaling of the channel capacity with the logarithm of the total noise strength in small noise approximation , as explained in the caption of fig [ f - axs ] . in general , increasing the number of input and output molecules by a factor of four will decrease the relative input and output noise by a factor of @xmath126 , and therefore , in the small noise approximation , increase the capacity by @xmath127 .",
    "if one assumes that the cell can make transcription factor and output protein molecules at no cost , then scaling of the noise variance along the horizontal axis of fig [ f - axs ] is inversely proportional to the total number of signaling molecules used by the regulatory element , and its capacity can grow without bound as more and more signaling molecules are used .",
    "if , however , there are metabolic or time costs to making more molecules , our optimization needs to be modified appropriately , and we present the relevant computation in section [ scosts ] on the costs of coding .      in the analysis presented so far we have not paid any particular attention to the question of whether the optimal input distributions are biologically realizable or not .",
    "we will proceed to relax some of the idealizations made until now and analyze the corresponding changes in the information capacity .",
    "we start by considering the impact on channel capacity of changing the allowed dynamic range to which the input concentration is restricted .",
    "figure [ f - costs_x]a displays the capacity as a function of the dynamic range , output noise and cooperativity .",
    "the main feature of the plot is the difference between the low and high cooperativity cases at each noise level ; regardless of cooperativity the total information at infinite dynamic range would saturate at approximately the same value ( which depends on the output noise magnitude ) .",
    "however , highly cooperative systems manage to reach a high fraction of @xmath128 or more of their saturated information capacity even at reasonable dynamic ranges of 25 to 100-fold ( meaning that the input concentration varies between @xmath129 $ ] or @xmath130 $ ] , respectively ) , whereas low cooperativity systems require a much bigger dynamic range for the same effect .",
    "the decrease in capacity with decreasing dynamic range is a direct consequence of the nonlinear relationship between the concentration and occupancy , eq ( [ meanio ] ) , and for low cooperativity systems means being unable to fully shut down or fully induce the promoter . in theory ,",
    "eq ( [ actnoise ] ) predicts that @xmath131 , making the state in which the gene is `` off '' very informative about the input .",
    "if , however , the gene can not be fully repressed either because there is always some residual input , @xmath132 , or because there is leaky expression even when the input is exactly zero , then at any biologically reasonable input dynamic range some capacity will be lost .",
    "is changed ( `` 25-fold range '' means @xmath133 $ ] ) .",
    "the regulatory element is a repressor with either no cooperativity ( dashed line ) or high cooperativity , @xmath119 ( thick line ) .",
    "we plot three high - low cooperativity pairs for different choices of the output noise magnitude ( high noise in light gray , @xmath134 ; medium noise in dark gray , @xmath135 ; low noise in black , @xmath136 ) .",
    "panel b shows the sensitivity of channel capacity to perturbations in the optimal input distribution . for various systems from fig [ f - cpanel1 ]",
    "we construct suboptimal input distributions , as described in the text , compute the fraction of capacity lost relative to the unperturbed optimal solution and plot this fraction against the optimal capacity of that system ( black dots ) ; extrapolated absolute capacity left when the input tends to be very different from optimal , i.e. @xmath137 , is plotted in red .",
    ", width=336 ]    next , we briefly discuss how precisely tuned the resulting optimal distributions have to be to take full advantage of the regulatory element s capacity .",
    "for each point in the noise plane of fig [ f - cpanel1]a the optimal input distribution @xmath15 is perturbed many times to create an ensemble of suboptimal inputs @xmath138 ( see appendix [ ax : pert ] ) . for each @xmath138",
    ", we compute , first , its distance away from the optimal solution by means of jensen - shannon divergence , @xmath139 @xcite ; next , we use the @xmath138 to compute the suboptimal information transmission @xmath140 . the divergence @xmath141 is a measure of similarity between two distributions and ranges between 0 ( distributions are the same ) and 1 ( distributions are very different ) ; @xmath142 approximately corresponds to the number of samples one would have to draw to say with confidence that they were selected either from @xmath143 or @xmath144 .",
    "a scatter plot of many such pairs @xmath145 obtained with various perturbations @xmath138 for each system of the noise plane characterizes the sensitivity of the optimal solution for that system ; the main feature of such a plot , fig [ f - optperturb ] , is the linear ( negative ) slope that describes the fraction of channel capacity lost for a unit of jensen - shannon distance away from the optimal solution .",
    "figure [ f - costs_x]b displays these fractions as a function of the optimal capacity , and each system from the noise plane shown in fig [ f - cpanel1 ] is represented by a black dot .",
    "we note that systems with higher capacities require more finely tuned solutions and suffer a larger fractional ( and thus absolute ) loss if the optimal input distribution is perturbed .",
    "importantly , if the linear slopes are taken seriously and are used to extrapolate towards distributions that are very different from optimal , @xmath137 , we observe that for most of the noise plane the leftover capacity still remains about a bit , indicating that biological regulatory elements capable of transmitting an `` on - off '' decision perhaps are not difficult to construct . on the other hand ,",
    "transmitting significantly more than one bit requires some degree of tuning that matches the distribution of inputs to the characteristics of the regulatory element .",
    "real regulatory elements must balance the pressure to convey information reliably with the cost of maintaining the cell s internal state , represented by the expression levels of transcription factors .",
    "the fidelity of the representation is increased ( and the fractional fluctuation in their number is decreased ) by having more molecules `` encode '' a given state . on the other hand ,",
    "making or degrading more transcription factors puts a metabolic burden on the cell , and frequent transitions between various regulatory states could involve large time lags as , for example , the regulation machinery attempts to keep up with a changed environmental condition , by accumulating or degrading the corresponding tf molecules .",
    "in addition , the output genes themselves that get switched on or off by transcription factors and therefore `` read out '' the internal state must not be too noisy , otherwise the advantage of maintaining precise transcription factor levels is lost .",
    "suppose that there is a cost to the cell for each molecule of output gene that it needs to produce , and that this incremental cost per molecule is independent of the number of molecules already present .",
    "then , on the output side , the cost must be proportional to @xmath146 .",
    "we remember that in optimal distribution calculations @xmath8 is expressed as relative to the maximal expression , such that its mean is between zero and one . to get an absolute cost in terms of the number of molecules",
    ", this normalized @xmath35 therefore needs to be multiplied by the inverse of the output noise strength , @xmath147 , as the latter scales with @xmath71 ( see table [ t - params ] ) .",
    "the contribution of the output cost is thus @xmath148 .    on the input side ,",
    "the situation is similar : the cost must be proportional to @xmath149 , where our optimal solutions are expressed , as usual , in dimensionless concentration units , @xmath150 . in either of the two input noise models ( i.e. diffusion or switching input noise ) , with diffusion constant held fixed , @xmath151 or @xmath152 .",
    "see appendix [ ax : nb ] for the notes on the effects of non - specific binding of transcription factors to the dna",
    ".    collecting all our thoughts on the costs of coding , we can write down the `` cost functional '' as the sum of input and output cost contributions : @xmath153\\rangle=\\frac{v_1}{\\beta } \\int dc\\ , p(c)\\ , c + \\frac{v_2}{\\alpha } \\int dc\\ , p(c)\\ , \\int dg\\ , p(g|c)\\ , g , \\label{cconstraint}\\ ] ] where @xmath154 and @xmath155 are proportional to the unknown costs per molecule of input or output , respectively , and @xmath110 and @xmath105 are noise parameters of table [ t - params ] .",
    "this ansatz captures the intuition that while decreasing noise strengths will increase information transmission , it will also increase the cost . instead of maximizing the information without regard to the cost ,",
    "the new problem to extremize is : @xmath156&= & i[p(c ) ] - \\phi \\langle\\mathcal{c}[p(c)]\\rangle - \\lambda \\int dc\\,p(c ) , \\end{aligned}\\ ] ] and the lagrange multiplier @xmath157 has to be chosen so that the cost of the resulting optimal solution @xmath158\\rangle$ ] equals some predefined cost @xmath159 that the cell is prepared to pay .",
    ", with the imposed constraint that the average total ( input + output ) cost is fixed to some @xmath159 ; as the cost is increased , the optimal solution ( green dot ) moves along the arrows on a dark green line ( the contours change correspondingly , not shown ) .",
    "light green line shows activator with cooperativity @xmath119 , dark and light red lines show repressors without and with cooperativity ( @xmath119 ) .",
    "panel b shows the achievable capacity as a function of cost for each line in panel a.,width=336 ]    we now wish to recreate the noise plane of fig [ f - cpanel1 ] , while constraining the total cost of each solution to @xmath159 .",
    "to be concrete and pick the value for the cost and proportionality constants in eq ( [ cconstraint ] ) , we use the estimates from _ drosophila _ noise measurements and analysis in @xcite , which assign to the system denoted by a blue dot in fig [ f - cpanel1]a , the values of @xmath160 bicoid molecules of input at @xmath69 , and a maximal induction of @xmath161 hunchback molecules if the burst size @xmath162 were 10 .",
    "figure [ f - costs]a is the noise plane for an activator with no cooperativity , as in fig [ f - cpanel1 ] , but with the cost limited to an average total of @xmath163 molecules of input and output per nucleus . there is now one optimal solution denoted by a green dot ( with a dominant input noise contribution ) ; if one tries to choose a system with lower input or output noise , the cost constraint forces the input distribution , @xmath13 , and the output distribution , @xmath164 , to have very low probabilities at high induction , consequently limiting the capacity .",
    "clearly , a different system will be optimal if another total allowed cost @xmath159 is selected .",
    "the dark green line on the noise plane in fig [ f - costs]a corresponds to the flow of the optimal solution for an activator with no cooperativity if the allowed cost is increased , and the corresponding cost - capacity curve is shown in fig [ f - costs]b .",
    "the light green line is the trajectory of the optimal solution in the noise plane of the activator system with cooperativity @xmath119 , and the dark and light red trajectories are shown for the repressor with @xmath108 and @xmath119 , respectively .",
    "we note first that the behavior of the cost function is quite different for the activator ( where low input implies low output and therefore low cost ; and conversely high input means high output and also high cost ) and the repressor ( where input and output are mutually exclusively high or low and the cost is intermediate in both cases ) . secondly , in fig",
    "[ f - costs]b we observe that the optimal capacity as a function of cost is similar for the activators and repressors , in contrast to the comparison of fig [ f - actrep ] , where repressors provided higher capacities .",
    "thirdly , we note in the same figure that increasing the cooperativity at fixed noise strength @xmath105 brings a substantial increase , of almost a bit over the whole cost range , in the channel capacity , in agreement with our previous observations about the interaction between capacity and the dynamic range .",
    "the last and perhaps the most significant conclusion is that even with input distributions matched to maximize the transmission at a fixed cost , the capacity still only scales roughly linearly with the logarithm of the number of available signaling molecules , and this fact must ultimately be limiting in a single regulatory element .",
    "we have tried to analyze a simple regulatory element as an information processing device .",
    "one of our major results is that one can not discuss an element in isolation from the statistics of the input that it is exposed to . yet in cells the inputs are often transcription factor concentrations that `` encode '' the state of various genetic switches , from those responsible for cellular identity to those that control the rates of metabolism and cell division , and the cell exerts control over these concentrations .",
    "while it _ could _ use different distributions to represent various regulatory settings , we argue that the cell _ should _ use the one distribution that allows it to make the most of its genetic circuitry  the distribution that maximizes the dependency , or mutual information , between inputs and outputs .",
    "mutual information can then be seen both as a measure of how well the cell is doing by using _ its _ encoding scheme , and",
    "the best it could have done using the _ optimal _ scheme , which we can compute ; comparison between the optimal and measured distributions gives us a sense of how close the organism is to the achievable bound @xcite .",
    "moreover , mutual information has absolute units , i.e. bits , that have a clear interpretation in terms of the number of discrete distinguishable states that the regulatory element can resolve .",
    "this last fact helps clarify the ongoing debates about what is the proper noise measure for genetic circuits , and in what context a certain noise is either `` big '' or `` small '' ( as it is really a function of the inputs ) .",
    "information does not replace the standard _ noise - over - the - mean _ measure  noise calculations or measurements are still necessary to compute the element s capacity  but does give it a functional interpretation .",
    "we have considered a class of simple parametrizations of signals and noise that can be used to fit measurements for several model systems , such as bicoid - hunchback in the fruit fly , a number of yeast genes , and the _ lac _ repressor in _ escherichia coli _ ( see ref @xcite for the latter ) .",
    "we find that the capacities of these realistic elements are generally larger than 1 bit , and can be as high as 2 bits . by simple inspection of optimal output distributions in figs [ f - cpanel1]b or [ f",
    "- cpanel1]c it is difficult to say anything about the capacity : the distribution might look bimodal yet carry more than one bit , or might even be a monotonic function without any obvious structure , indicating that the information is encoded in the graded response of the element .",
    "when the noise is sufficiently high , on the other hand , the optimal strategy is that of achieving one bit of capacity and only utilizing maximum and minimum available levels of transcription factors for signaling .",
    "the set of distributions that achieve capacities close to the optimal one is large , suggesting that perhaps one - bit switches are not difficult to implement biologically , while in contrast we find that transmission of much more than one bit requires some tuning of the system .",
    "finally , we discussed how additional biophysical constraints can modify the channel capacity . by assuming a linear cost model for signaling molecules and a limited input dynamic range , the capacity and cost couple in an interesting way and the maximization principle allows new questions to be asked .",
    "for example , increasing the cooperativity reduces the cost , as we have shown ; on the other hand , it increases the sensitivity to fluctuations in the input , because the input noise strength @xmath105 is proportional to the square of hill s coefficient , @xmath165 . in a given system",
    "we could therefore predict the optimal effective cooperativity , if we knew the real cost per molecule .",
    "further work is needed to tease out the consequences of cost ( if any ) from experimental data .",
    "the principle of information maximization clearly is not the only possible lens through which regulatory networks are to be viewed .",
    "one can think of examples where there are constraints on the _ dynamics _ , something that our analysis has ignored by only looking at steady state behavior ; for instance , the chemotactic network of _ escherichia coli _ has to perfectly adapt in order for the bacterium to be able to climb the attractant gradients . alternatively , suppose that a system needs to convey only a single bit , but it has to be done reliably in a fluctuating environment , perhaps by being _",
    "robust _ to the changes in outside temperature . in this case",
    "it seems that both concepts , that of maximal information transmission and the robustness to fluctuations in certain auxiliary variables which also influence the noise , could be included into the same framework , but the issue needs further work . more generally , however , these and similar examples assume that one has identified in advance the biologically relevant features of the system , e.g. _ perfect adaptation _ or _ robustness _ , and that there exists a problem - specific error measure which the regulatory network is trying to minimize .",
    "such a measure could then either replace or complement the assumption - free information theoretic approach presented here .",
    "we emphasize that the kind of analysis carried out here is not restricted to a single regulatory element . as was pointed out in the introduction , the inputs @xmath1 and the outputs @xmath2 of the regulatory module can be multi - dimensional , and the module could implement complex internal logic with multiple feedback loops .",
    "it seems that especially in such cases , when our intuition about the noise  now a function of multiple variables  starts breaking down , the information formalism could prove to be helpful .",
    "although the solution space that needs to be searched in the optimization problem grows exponentially in the inputs , there are biologically relevant situations that nevertheless appear tractable : for example , when there are multiple readouts of the same input , or combinatorial regulation of a single output by a pair of inputs ; in addition , knowing that the capacities of a single input / output chain are on the order of a few bits also means that only a small number of distinct input levels for each input need to be considered . some cases of interest therefore appear immediately amenable to biophysical modeling approaches and the computation of channel capacities , as presented in this paper .",
    "we have focused here on the theoretical exploration of information capacity in simple models .",
    "it is natural to ask how our results relate to experiment .",
    "perhaps the strongest connection would be if biological systems really were selected by evolution to optimize information flow in the sense we have discussed .",
    "if this optimization principle applies to real regulatory elements , then , for example , given measurements on the input / output relation and noise in the system we can make parameter free predictions for the distribution of expression levels that cells will use .",
    "initial efforts in this direction , using the bicoid - hunchback element in the _ drosophila _ embryo as an example , are described in ref @xcite .",
    "it is worth noting that a parallel discussion of optimization principles for information transmission has a long history in the context of neural coding , where we can think of the distribution on inputs as given by the sensory environment and optimization is used to predict the form of the input / output relation @xcite .",
    "although there are many open questions , it would be attractive if a single principle could unify our understanding of information flow across such a wide range of biological systems .",
    "we thank t gregor , jb kinney , dw tank and ef wieschaus for helpful discussions .",
    "this work was supported in part by nih grants p50 gm071508 and r01 gm077599 , nsf grant phy-0650617 , by the swartz foundation , the burroughs wellcome fund program in biological dynamics ( gt ) and by us department of energy grant de - fg02 - 91er40671 ( cc ) .",
    "if we treat the kernel on a discrete @xmath166 grid we can easily choose such @xmath13 as to maximize the mutual information @xmath167 between the expression level and the concentration . the problem can be stated in terms of the following variational principle : @xmath168=\\sum_{c , g}p(g|c)p(c)\\log_2\\frac{p(g|c)}{p(g ) } - \\lambda\\sum_c p(c ) , \\label{func}\\ ] ] where the multiplier @xmath37 enforces the normalization of @xmath13 , and @xmath164 itself is a function of the unknown distribution ( since @xmath169 ) .",
    "the solution @xmath15 of this problem achieves the capacity , @xmath167 , of the channel . the original idea behind the blahut - arimoto approach @xcite was to understand that the maximization of eq ( [ func ] ) using variational objects @xmath59 is equivalent to the following maximization : @xmath170 \\sim \\max_{p(c)}\\max_{p(c|g ) } \\mathcal{l}'[p(c),p(c|g)],\\ ] ] where @xmath171=\\sum_{g , c}p(c)p(g|c)\\log\\frac{p(c|g)}{p(c)}-\\lambda\\sum_c p(c ) .",
    "\\label{baa1}\\ ] ] in words , finding the extremum in variational object @xmath13 is equivalent to a double maximization of a modified lagrangian , where both @xmath13 and @xmath172 are treated as independent variational objects .",
    "the extremum of the modified lagrangian is achieved exactly when the consistency condition @xmath173 holds .",
    "this allows us to make an iterative algorithm that we detail below , where eq ( [ baa1 ] ) is solved for the optimal @xmath13 and evaluated at some `` known '' @xmath172 , which is in turn updated with the newly obtained estimate of @xmath13 .    before describing the algorithm",
    "let us also suppose that each input signal @xmath6 carries some metabolic or time cost to the cell .",
    "then we can introduce a cost vector @xmath174 that assigns a cost to each codeword @xmath6 , and require of the solution the following : @xmath175 where @xmath159 is the maximum allowed expense .",
    "the constraint can be introduced into the functional , eq ( [ func ] ) or eq ( [ baa1 ] ) , through an appropriate lagrange multiplier ; the same approach can be taken to introduce the cost of coding for the output words , @xmath176 , because it reduces to an additional `` effective '' cost for the input , @xmath177 .",
    "as was pointed out in the main text , after discretization we have no guarantees that the optimal distribution @xmath59 is going to be smooth .",
    "one way to address this problem is to enforce the smoothness on the scale set by the precision at which the input concentration can be controlled by the cell , @xmath178 , by penalizing big derivatives in the lagrangian of eq ( [ baa1 ] ) .",
    "an alternative way is to find the spiky solution ( without imposing any direct penalty term ) , but interpret it not as a real , `` physical '' concentration distribution , but rather as the distribution of concentrations that the cell attempts to generate , @xmath179 . in this case , however , the limited resolution of the input , @xmath178 , must be referred to the output as an additional effective noise in gene expression , @xmath180 .",
    "the optimal solution @xmath181 is therefore the distribution of the levels that the cell would use if it had infinitely precise control over choosing various @xmath179 ( i.e. if the input noise were absent ) , but the physical concentrations are obtained by convolving this optimal result @xmath181 with a gaussian of width @xmath182 .",
    "although we chose to use the second approach to compute the results of this paper , we will , for completeness , describe next how to include the smoothness constraint into the functional explicitly .",
    "if the smoothness of the input distribution @xmath13 is explicitly constrained in the optimization problem , then it will be controllable through an additional lagrange multiplier , and both ways of computing the capacity  that of referring the limited input resolution @xmath178 to the noise in the output , and that of including it as a smoothness constraint on the input distribution  will be possible within a single framework .",
    "we proceed by analogy to field theories in which the kinetic energy terms of the form @xmath183 constrain the gradient magnitude , and form the following functional : @xmath156&=&\\mathrm{i}(c;g ) - \\lambda_0 \\sum_c p(c ) - \\label{sba1}\\\\ & -&\\phi_1 \\sum_c p(c ) v_1(c)-\\phi_2 \\sum_g p(g)v_2(g ) - \\label{sba2}\\\\ & - & \\theta \\sum_c \\left(\\frac{\\delta",
    "p}{\\delta c}\\sigma(c)\\right)^2 \\label{sba3}.\\end{aligned}\\ ] ] eq ( [ sba1 ] ) maximizes the capacity with respect to variational objects @xmath13 while keeping the distribution normalized ; eq ( [ sba2 ] ) imposes cost @xmath184 on input symbols and cost @xmath185 on output symbols ; finally , eq ( [ sba3 ] ) limits the derivative of the resulting solution . the difference operator @xmath186 is defined for an arbitrary function @xmath187 : @xmath188 @xmath189 assigns a different weight to various intervals on the input axis , @xmath6 . if the input can not be precisely controlled , but has an uncertainty of @xmath189 at mean input level @xmath6 , we require that the optimal probability distribution must not change much as the input fluctuates on the scale @xmath189 .",
    "in other words , we require for each input concentration that : @xmath190 the term in eq.[sba3 ] constrained by lagrange multiplier @xmath53 can be seen as the sum of squares of such variations over all possible values of the input .    by differentiating the functional , eq ( [ baa1 ] ) , that includes the relevant constraints , with respect to @xmath59 we get the following equation : @xmath191\\frac{\\sigma^2(c_i)}{(c_{i+1}-c_i)^2}-\\left[p(c_{i})-p(c_{i-1})\\right]\\frac{\\sigma^2(c_{i-1})}{(c_{i}-c_{i-1})^2}\\right\\}.\\end{aligned}\\ ] ]",
    "let us denote by @xmath192 the term in braces .",
    "the solution for @xmath13 is therefore given by : @xmath193 we can now continue to use the blahut - arimoto trick of pretending that @xmath172 is an independent variational object , and that @xmath13 has to be solved with @xmath172 held fixed ; however , even in that case , eq ( [ sba4 ] ) is an implicit equation for @xmath13 which needs to be solved by numerical means .",
    "the complete iterative prescription is therefore as follows : @xmath194    again , eq ( [ iter1 ] ) has to be solved on its own by numerical means as the variational objects for iteration @xmath195 appear both on the left- and right - hand sides .",
    "the input and output costs of coding are neglected if one sets @xmath196 ; likewise , smoothness constraint is ignored for @xmath197 , in which case eq ( [ iter1 ] ) is the same as in the original blahut - arimoto derivation and it gives the value of @xmath198 explicitly .",
    "for the capacities computed in this paper we have calculated the effective output noise that includes the intrinsic output noise as well as the input noise that has been referred to the output ( see section [ sandn ] ) ; we can therefore set @xmath197 .",
    "this approach treats all sources of noise on the same footing and allows us to directly compare the magnitudes of noise sources at the input and the output",
    ". we also note that it makes sense to compute and compare the optimal distribution of outputs rather than inputs : the input / output kernels are degenerate and there are various input distributions ( differing either in the regions that give saturated or zero response , or by having variations on a scale below @xmath62 ) that will yield essentially the same distribution of outputs .",
    "langevin approximation assumes that the fluctuations of a quantity around its mean are gaussian and proceeds to calculate their variance @xcite . for the calculation of exact channel capacity we must calculate the full input / output relation , @xmath10 .",
    "even if langevin approach ends up giving the correct variance as the function of the input , @xmath23 , the shape of the distribution might be far from gaussian .",
    "we expect such a failure when the number of mrna is very small : the distribution of expression levels might be then multi - peaked , with peaks corresponding to @xmath199 proteins , where @xmath162 is the burst size ( number of proteins produced per transcript lifetime ) .    in the model used in eq ( [ actnoise ] ) , parameter @xmath200 determines the output noise ; @xmath201 , where @xmath202 is the average number of transcripts produced during the integrating time ( i.e. the longest averaging timescale in the problem , for example the protein lifetime or cell doubling time ) .",
    "if @xmath102 , then the output noise is effectively determined only by the number of transcripts , @xmath203 .",
    "we should therefore be particularly concerned what happens as @xmath202 gets small .",
    "our plan is therefore to solve for @xmath10 exactly by finding the stationary solution of the master equation in the case where the noise consists of the output and switching input contributions . in this approach ,",
    "we explicitly treat the fact that the number of transcribed messages , designated by @xmath204 , is discrete .",
    "we start by calculating @xmath205 .",
    "the state of the promoter is described index @xmath206 , which can be 0 or 1 , depending on whether the promoter is bound by the transcription factor or not , respectively .",
    "normalization requires that for each value of @xmath6 : @xmath207 the time evolution of the system is described by the following set of equations for an activator : @xmath208 where @xmath209 is the integrating time , @xmath65 is the rate for switching into the inactive state ( off - rate of the activator ) , @xmath64 is the second - order on - rate , and @xmath91 is the rate of mrna synthesis . these constants combine to give @xmath210 and the input switching noise strength @xmath211 , see table [ t - params ] .",
    "this set of equations is supplemented by appropriate boundary conditions for @xmath212 . to find a steady state distribution @xmath213 , we set the left - hand side to zero and rewrite the set of equations ( with high enough cutoff value of @xmath214 ) in matrix form : @xmath215 where @xmath216 and @xmath217 .",
    "matrix @xmath218 ( of dimension @xmath219 rows and @xmath220 columns ) contains , in its last row , only ones , which enforces normalization .",
    "the resulting system is a non - singular band - diagonal system that can be easily inverted .",
    "the input / output relation for the number of messages is given by taking @xmath221 .",
    "having found the distribution for the number of transcripts we then convolve it another poisson process , @xmath222 , i.e. @xmath223 .",
    "finally , the result is rediscretized such that mean expression @xmath35 runs from 0 to 1 .",
    "note that the langevin approximation only depends on the combination of the burst size @xmath162 and the mean number of transcripts @xmath202 through @xmath110 ; in contrast , the master equation solution depends on both @xmath162 and @xmath202 independently .",
    "the generalization of this calculation to repressors or hill - coefficient - type cooperativity is straightforward .",
    "fig [ f - master]c shows that the langevin approximation yields correct second moments of the output distribution ; however , gaussian distributions themselves are , for large burst sizes and small number of messages , inconsistent with the exact solutions , as can be seen in fig [ f - master]a . in the opposite limit where the number of messages is increased and burst size kept small , see fig [ f - master]b , normal distributions are an excellent approximation . despite these difficulties the information capacity calculated with either gaussian or master input / output relations differs by at most @xmath224 over a large range of burst sizes @xmath162 and values for @xmath110 , illustrated by fig [ f - master]d .",
    ", compared to their gaussian approximations ( gray ) .",
    "panel a shows the distribution of outputs at maximal induction , @xmath49 for a system with a large burst size , @xmath225 and a large output noise @xmath226 ( i.e. the average number of messages is  6 , as is evident from the number of peaks , each of which corresponds to a burst of translation at different number of messages ) .",
    "panel b shows the same distribution for smaller output noise , @xmath227 and @xmath228 ; here gaussian approximation performs well .",
    "both cases are computed with switching noise parameter @xmath229 , and cooperativity of @xmath230 .",
    "panel c shows in color - code the error made in computing the standard deviation of the output given @xmath6 ; the error measure we use is the maximum difference between the exact and gaussian results over the full range of concentrations : @xmath231_{\\rm master}-[\\sigma_g(c)/g_0]_{\\rm gaussian}\\right\\}$ ] .",
    "as expected the error decreases with decreasing output noise .",
    "panel d shows that the capacity is overestimated by using an approximate kernel , but the error again decreases with decreasing noise as langevin becomes an increasingly good approximation to the true distribution . in the worst case",
    "the approximation is about @xmath224 off .",
    "gaussian computation only depends on @xmath110 and not separately on burst size , so we plot only one curve for @xmath232.,width=316 ]",
    "to examine the sensitivity to the perturbations in the optimal input distributions for fig [ f - costs_x ] we need to generate an ensemble of perturbations .",
    "we pick an _ ad hoc _ prescription , whereby the optimal solution is taken , and we add to it 5 lowest harmonic modes on the input domain , each with a weight that is uniformly distributed on some range .",
    "the range determines whether the perturbation is small or not .",
    "the resulting distribution is clipped to be positive and renormalized .",
    "this choice was made to induce low - frequency perturbations ( high frequency perturbations get averaged out because the kernel is smooth ) .",
    "then , for an ensemble of 100 such perturbations , @xmath233 , and for every system of the noise plane in fig [ f - cpanel1]a , the divergence of the perturbed input distribution to the true solution , @xmath234 , is computed , as well as the information transmission , @xmath235 $ ] . figure [ f - optperturb ] plots the @xmath145 scatter plots for @xmath236 representative systems with varying amounts of output ( @xmath110 ) and input ( @xmath105 ) noise , taken from fig [ f - cpanel1]a uniformly along the horizontal and vertical axes .",
    "grid of points in the noise plane of fig [ f - cpanel1]a , such that the output noise increases along the horizontal edge of the figure and the input noise along the vertical edge .",
    "each subplot shows a scatter plot of 100 perturbations from the ideal solution ; the jensen - shannon distance from the optimal solution , @xmath141 , is plotted on the horizontal axis and the channel capacity ( normalized to maximum when there is no perturbation ) , @xmath237 , on the vertical axis .",
    "red lines are best linear fits.,width=336 ]    figure [ f - optperturb ] shows that as we move towards systems with higher capacity ( lower left corner ) , perturbations to the optimal solution that are at the same distance from the optimum as in the low capacity systems ( upper right corner ) , will cause greater relative loss ( and therefore an even greater absolute loss ) in capacity . as expected , higher capacity systems must be better tuned , but even for the highest capacity system considered , a perturbation of around @xmath238 will only cause an average @xmath239 loss in capacity .",
    "we also note that for systems with high capacity the linear relationship between the the divergence @xmath141 and capacity @xmath140 provides a better fit than for systems with small capacity .",
    "one needs to make a careful distinction between the total concentration of the input transcription factors , @xmath240 , and the free concentration @xmath241 , diffusing in solution in the nucleus .",
    "we imagine the true binding site embedded in a pool of non - specific binding sites  perhaps all other short fragments of dna  and there being an ongoing competition between one functional site ( with strong affinity ) and large number of weaker non - specific sites .",
    "if these non - specific sites are present at concentration @xmath242 in the cell , and have affinities drawn from some distribution @xmath243 , the relationship between the free and the total concentration of the input is : @xmath244 importantly , the concentration that enters all information capacity calculations is the _ free _",
    "concentration @xmath241 , because it directly determines both the promoter occupancy in eq ( [ meanio ] ) as well as diffusive noise ; on the other hand , the cell can influence the free concentration only by producing more or less of the transcrption factor , i.e. by varying ( and paying for ) the _ total _ concentration . if the free concentration is well below the strength of the non - specific binding , @xmath245 , eq ( [ nscost ] ) can be approximated by @xmath246 , with the total and free concentrations being proportional to each other .",
    "because the cost functional , eq ( [ cconstraint ] ) , is only determined to within a factor anyway , the presence of non - specific sites will effectively just rescale the cost per free molecule of transcription factor .",
    "a separate calculation is needed to show that the presence of non - specific binding does not appreciably increase the noise in gene expression ( to be presented elsewhere ) .",
    "pa lawrence , _ the making of a fly : the genetics of animal design _ ( blackwell , oxford , 1992 ) .",
    "m levine & eh davidson , gene regulatory networks for development . _ proc natl acad sci usa _ * 102 , * 49364942 ( 2005 ) .    ap gasch , pt spellman , cm kao , o carmel - harel , mb eisen , g storz , d botstein & po brown , genomic expression programs in the response of yeast cells to environmental changes . _ mol biol cell _ * 11 , * 42414257 ( 2000 ) .          ce shannon , a mathematical theory of communication .",
    "_ bell sys .",
    "j. _ * 27 , * 379423 & 623656 ( 1948 ) . reprinted in ce shannon & w weaver , _ the mathematical theory of communication _ ( university of illinois press , urbana , 1949 )"
  ],
  "abstract_text": [
    "<S> changes in a cell s external or internal conditions are usually reflected in the concentrations of the relevant transcription factors . these proteins in turn modulate the expression levels of the genes under their control and sometimes need to perform non - trivial computations that integrate several inputs and affect multiple genes . at the same time , the activities of the regulated genes would fluctuate even if the inputs were held fixed , as a consequence of the intrinsic noise in the system , and such noise must fundamentally limit the reliability of any genetic computation . here </S>",
    "<S> we use information theory to formalize the notion of information transmission in simple genetic regulatory elements in the presence of physically realistic noise sources . </S>",
    "<S> the dependence of this `` channel capacity '' on noise parameters , cooperativity and cost of making signaling molecules is explored systematically . </S>",
    "<S> we find that , at least in principle , capacities higher than one bit should be achievable and that consequently genetic regulation is not limited the use of binary , or `` on - off '' , components . </S>"
  ]
}