{
  "article_text": [
    "this work stems from the attempt to address the optimal infinite - horizon constrained control of discrete - time stochastic processes by a model predictive control strategy  @xcite .",
    "we focus on linear dynamical systems driven by stochastic noise and a control input , and consider the problem of finding a control policy that minimizes an expected cost function while simultaneously fulfilling constraints on the control input and on the state evolution .",
    "in general , no control policy exists that guarantees satisfaction of deterministic ( hard ) constraints over the whole infinite horizon .",
    "one way to cope with this issue is to relax the constraints in terms of probabilistic ( soft ) constraints  @xcite .",
    "this amounts to requiring that constraints will not be violated with sufficiently large probability or , alternatively , that an expected reward for the fulfillment of the constraints is kept sufficiently large .",
    "two considerations lead to the reformulation of an infinite horizon problem in terms of subproblems of finite horizon length .",
    "first , given any bounded set ( e.g. a safe set ) , the state of a linear stochastic dynamical system is guaranteed to exit the set at some time in the future with probability one whatever the control policy .",
    "therefore , soft constraints may turn the original ( infeasible ) hard - constrained optimization problem into a feasible problem only if the horizon length is finite .",
    "second , even if the constraints are reformulated so that an admissible infinite - horizon policy exists , the computation of such a policy is generally intractable .",
    "the aim of this note is to show that , for certain parameterizations of the policy space  @xcite and the constraints , the resulting finite horizon optimization problem is tractable .",
    "an approach to infinite horizon constrained control problems that has proved successful in many applications is model predictive control  @xcite . in model predictive control , at every time @xmath0 , a finite - horizon approximation of the infinite - horizon problem is solved but only the first control of the resulting policy is implemented . at the next time",
    "@xmath1 , a measurement of the state is taken , a new finite - horizon problem is formulated , the control policy is updated , and the process is repeated in a receding horizon fashion . under time - invariance assumptions ,",
    "the finite - horizon optimal control problem is the same at all times , giving rise to a stationary optimal control policy that can be computed offline .    motivated by the previous considerations , here we study the convexity of certain stochastic finite - horizon control problems with soft constraints .",
    "convexity is central for the fast computation of the solution by way of numerical procedures , hence convex formulations  @xcite or convex approximations  @xcite of the stochastic control problems are commonly sought .",
    "however , for many of the classes of problems considered here , tight convex approximations are usually difficult to derive . one may argue that non - convex problems can be tackled by randomized algorithms  @xcite .",
    "however , randomized solutions are typically time - consuming and can only provide probabilistic guarantees .",
    "in particular , this is critical in the case where the system dynamics or the problem constraints are time - varying , since in that case optimization must be performed in real - time .",
    "here we provide conditions for the convexity of chance constrained stochastic optimal control problems .",
    "we derive and compare several explicit convex approximations of chance constraints for gaussian noise processes and for polytopic and ellipsoidal constraint functions . finally , we establish conditions for the convexity of a class of expectation - type constrains that includes standard integrated chance constraints  @xcite as a special case . for integrated chance",
    "constrains on gaussian processes with polytopic constraint functions , an explicit formulation of the optimization problem is also derived .",
    "the optimal constrained control problem we concentrate on is formulated in section  [ sec : ps ] .",
    "a convenient parametrization of the control policies and the convexity of the objective function are discussed at this stage .",
    "next , two probabilistic formulations of the constraints and conditions for the convexity of the space of admissible control policies are discussed : section  [ sec : cc ] is dedicated to chance constraints , while section  [ sec : icc ] is dedicated to integrated chance constraints . in  section  [ sec : num ] , numerical simulations are reported to illustrate and discuss the results of the paper .",
    "let @xmath2 and @xmath3 . consider the following dynamical model : for @xmath4 , @xmath5 where @xmath6 is the state , @xmath7 is the control input , @xmath8 , @xmath9 , and @xmath10 is a stochastic noise input defined on an underlying probability space @xmath11 .",
    "no assumption on the probability distribution of the process @xmath12 is made at this stage .",
    "we assume that at any time @xmath4 , @xmath13 is observed exactly and that , for given @xmath14 , @xmath15 .    fix a horizon length @xmath16 .",
    "the evolution of the system from @xmath17 through @xmath18 can be described in compact form as follows : @xmath19 where @xmath20 @xmath21 let @xmath22 and @xmath23 , with @xmath24 , be measurable functions .",
    "we are interested in constrained optimization problems of the following kind : @xmath25 \\\\",
    "\\textrm{subject to}\\quad&\\textrm{(\\ref{eq : compactdyn})}\\quad\\textrm{and}\\quad \\eta(\\bar{x},\\bar{u})\\leq 0 \\end{aligned}\\ ] ] where the expectation @xmath26 $ ] is defined in terms of the underlying probability space @xmath11 , @xmath27 is a class of causal deterministic state - feedback control policies and the inequality in  ( [ eq : problem ] ) is interpreted componentwise .",
    "[ thm : examples ] in the ( unconstrained ) linear stochastic control problem  @xcite , @xmath12 is gaussian white noise and the aim is to minimize @xmath28,\\ ] ] where the matrices @xmath29 and @xmath30 are positive definite for all @xmath0 , with respect to causal feedback policies subject to the system dynamics  ( [ eq : system ] ) .",
    "this problem fits easily in our framework ; it suffices to define @xmath31 with @xmath32 ( the notation @xmath33 indicates that @xmath34 is a positive definite matrix ) . in our framework , though , the input noise sequence may have an arbitrary correlation structure , the cost function may be non - quadratic and , most importantly , constraints may be present .",
    "standard constraints on the state and the input are also formulated easily .",
    "for instance , sequential ellipsoidal constraints of the type @xmath35 with @xmath36 for @xmath37 and @xmath38 , are captured by the definition @xmath39 where , for @xmath40 , @xmath41 and each matrix @xmath42 is immediately constructed in terms of the @xmath43 .",
    "our framework additionally allows for cross - constraints between states and inputs at different times .      by the hypothesis that the state is observed without error",
    ", one may reconstruct the noise sequence from the sequence of observed states and inputs by the formula @xmath44 in light of this , and following  @xcite , we shall consider policies of the form : @xmath45 where the feedback gains @xmath46 and the affine terms @xmath47 must be chosen based on the control objective . with this definition ,",
    "the value of @xmath48 at time @xmath0 depends on the values of @xmath12 up to time @xmath49 . using  ( [ eq : noiserec ] )",
    "we see that @xmath50 is a function of the observed states up to time @xmath0 .",
    "it was shown in  @xcite that there exists a ( nonlinear ) bijection between control policies in the form  ( [ eq : controlpolicy ] ) and the class of affine state feedback policies .",
    "that is , provided one is interested in affine state feedback policies , parametrization  ( [ eq : noiserec ] ) constitutes no loss of generality .",
    "of course , this choice is generally suboptimal , since there is no reason to expect that the optimal policy is affine , but it will ensure the tractability of a large class of optimal control problems . in compact notation , the control sequence up to time @xmath51 is given by @xmath52 where @xmath53 note the lower triangular structure of @xmath54 that enforces causality .",
    "the resulting closed - loop system dynamics can be written compactly as the equality constraint @xmath55    let us denote the parameters of the control policy by @xmath56 and write @xmath57 to emphasize the dependence of @xmath58 and @xmath59 on @xmath60 . from now on we will consider the optimization problem @xmath61 \\label{eq : objective } \\\\",
    "\\textrm{subject to}\\quad&\\eqref{e : ubardef},~\\eqref{eq : cloopsys}\\textrm { and } \\\\",
    "& \\eta(\\bar{x}_\\theta,\\bar{u}_\\theta)\\leq 0 , \\label{eq : constraint}\\end{aligned}\\ ] ] where @xmath62 is the linear space of optimization parameters in the form  .    with the above parametrization of the control policy ,",
    "both @xmath63 and @xmath64 are affine functions of the parameters @xmath60 ( for fixed @xmath65 ) and of the process noise @xmath66 ( for fixed @xmath60 ) .",
    "most of the results developed below rely essentially on this property .",
    "it was noticed in  @xcite that a parametric causal feedback control policy with the same property can be easily defined based on indirect observations of the state , provided the measurement model is linear .",
    "the method enables one to extend the results of this paper to the case of linear output feedback . for the sake of conciseness",
    ", this extension will not be pursued here .      in general",
    ", no control policy can ensure that the constraint  ( [ eq : constraint ] ) is satisfied for all outcomes of the stochastic input  @xmath66 . in the standard lqg setting , for instance , any nontrivial constraint on the system state",
    "would be violated with nonzero probability .",
    "we therefore consider relaxed formulations of the constrained optimization problem   of the form @xmath61 \\label{eq : objective-2 } \\\\",
    "\\textrm{subject to}\\quad&\\eqref{e : ubardef},~\\eqref{eq : cloopsys}\\textrm { and }   \\\\ & \\ee [ \\phi\\circ \\eta(\\bar{x}_\\theta,\\bar{u}_\\theta ) ] \\leq 0 , \\label{eq : constraint-2}\\end{aligned}\\ ] ] where @xmath67 , with @xmath68 , is a convenient measurable function and the inequality is again interpreted componentwise . for appropriate choices of @xmath69 ,",
    "this formulation embraces most common probabilistic constraint relaxations , including chance constraints ( see e.g.  @xcite ) , integrated chance constraints  @xcite , and expectation constraints  ( see e.g.  @xcite ) .",
    "we are interested in the convexity of the optimization problem  . first we establish a general convexity result .",
    "[ thm : phietaconvex ] let @xmath70 be a probability space , @xmath62 be a convex subset of a vector space and @xmath71 be convex .",
    "let @xmath72 and @xmath73 be measurable functions and define @xmath74.\\ ] ] assume that :    1 .",
    "[ item : i ] the mapping @xmath75 is convex for almost all @xmath76 ; 2 .",
    "@xmath77 is monotone nondecreasing and convex ; 3 .",
    "@xmath78 is finite for all @xmath79 .",
    "then the mapping @xmath80 is convex .",
    "fix a generic @xmath76 . since @xmath81 is convex in @xmath60 and @xmath77 is monotone nondecreasing , for any @xmath82 and any @xmath83 $ ] , @xmath84 moreover , since @xmath77 is convex ,",
    "@xmath85 since these inequalities hold for almost all @xmath76 , it follows that @xmath86 \\leq & \\,\\ , \\ee[\\alpha\\varphi\\big ( \\gamma(\\omega , \\theta)\\big)+(1-\\alpha)\\varphi\\big(\\gamma(\\omega,\\theta')\\big ) ] \\\\ = & \\,\\ , \\alpha\\ee[\\varphi\\big ( \\gamma(\\omega , \\theta)\\big)]+(1-\\alpha)\\ee[\\varphi\\big(\\gamma(\\omega,\\theta')\\big)],\\end{aligned}\\ ] ] which proves the assertion .    assumption  ( iii ) can be replaced by either of the following :    * @xmath87 , @xmath88 .",
    "* @xmath89 , @xmath88 .",
    "let us now make the following standing assumption .",
    "[ a : convexobj ] @xmath90 is a convex function of @xmath91 and @xmath92 $ ] is finite for all @xmath79 .",
    "[ proposition : costconvexity ] under assumption  [ a : convexobj ] , @xmath92 $ ] is a convex function of @xmath60 .",
    "first , note that the set @xmath62 of admissible parameters @xmath60 is a linear space .",
    "let us write @xmath93 @xmath94 and @xmath95 to express the dependence of @xmath66 , @xmath96 and @xmath97 on the random event @xmath76 .",
    "fix @xmath98 arbitrarily .",
    "since the mapping @xmath99 is affine and the mapping @xmath100 is assumed convex , their combination @xmath101 is a convex function of @xmath60 .",
    "then , the result follows from proposition  [ thm : phietaconvex ] with @xmath102 and @xmath77 equal to the identity map .    by virtue of the alternative assumptions ( iii@xmath103 ) and ( iii@xmath104 ) of proposition  [ thm : phietaconvex ] , the requirement that @xmath92 $ ]",
    "be finite for all @xmath60 may be relaxed .",
    "a sufficient requirement is that there exist no two values @xmath60 and @xmath105 such that @xmath106 and @xmath107 . in particular",
    ", the result applies to quadratic cost functions of the type  ( [ eq : quadratic ] ) , with @xmath108 .    in general , the relaxed constraint   is nonconvex even if the components @xmath109 , with @xmath110 , of the vector function @xmath111 are convex . in the next sections we will study the convexity and provide convex approximations of   for different approaches to probabilistic relaxation of hard constraints , i.e. for different choices of the function @xmath69 .",
    "for a given @xmath1120,1[$ ] , we relax the hard constraint @xmath113 by requiring that it be satisfied with probability @xmath114 . hence we address the optimization problem @xmath61 \\label{e : objectivecc } \\\\",
    "\\textrm{subject to}\\quad&\\eqref{e : ubardef},~\\eqref{eq : cloopsys}\\textrm { and } \\label{e : dynconstr } \\\\",
    "& \\pp(\\eta(\\bar x_\\theta , \\bar u_\\theta ) \\le 0)\\geq 1-\\alpha \\label{e : constr}.\\end{aligned}\\ ] ] the smaller @xmath115 , the better the approximation of the hard constraint  ( [ eq : constraint ] ) at the expense of a more constrained optimization problem .",
    "this problem is obtained as a special case of problem   by setting @xmath116 and defining @xmath69 as @xmath117-\\infty , 0]}}(\\eta_i)-\\alpha,\\ ] ] where @xmath118-\\infty , 0]}}(\\cdot)$ ] is the standard indicator function .",
    "we now study the convexity of with respect to @xmath60 .",
    "first assume that the feedback term @xmath54 in   is fixed and consider the convexity of the optimization problem   with respect to the open loop control action @xmath119 .",
    "that is , for a given @xmath120 in the form  , the parameter space @xmath62 becomes the set @xmath121 . for the given",
    "@xmath122 and @xmath110 define @xmath123 where @xmath124 is the @xmath125-th element of the constraint function @xmath111 .",
    "define @xmath126\\ ] ] and @xmath127 .",
    "observe that @xmath128 corresponds to the constraint set dictated by   when @xmath54 is fixed .",
    "[ thm : convexopenloop ] assume that @xmath65 has a continuous distribution with log - concave probability density and that , for @xmath110 , @xmath129 is quasi - convex . then , for any value of @xmath130\\,0,1[$ ] , @xmath128 is convex . as a consequence , under assumption  [ a : convexobj ] and for any @xmath130\\,0,1[$ ] , the optimization problem @xmath61\\\\ \\mathrm{subject~to}\\quad&\\eqref{e : ubardef},~\\eqref{eq : cloopsys}~\\mathrm{and}~p(\\bar{d})\\geq 1-\\alpha\\end{aligned}\\ ] ] is convex .",
    "it follows from  ( * ? ? ?",
    "* theorem 10.2.1 ) that   is a log - concave function of @xmath131 , i.e. the mapping @xmath132 is concave . since @xmath133 is a monotone increasing function , we may write that @xmath134 . hence , @xmath128 is a convex set .",
    "the convexity of the optimization problem follows readily from assumption  [ a : convexobj ] .    among others ,",
    "gaussian , exponential and uniform distributions are continuous with log - concave probability density . as for the functions @xmath135 ,",
    "one case of interest where the assumptions of proposition  [ thm : convexopenloop ] are fulfilled is when @xmath136 is affine in @xmath58 and @xmath59 .",
    "this is the case of polytopic constraints , which will be treated extensively in the next section .",
    "apparently , this convexity result can not be applied to the ellipsoidal constraints treated subsequently in section  [ sec : ellipconstr ] , nor can it be extended to the general constraint   with both @xmath119 and @xmath54 varying . loosely speaking ,",
    "the latter is because the functions @xmath135 are not simultaneously quasi - concave in @xmath66 and @xmath122 . in the next sections we will develop convex conservative approximations of   for various definitions of @xmath111 .      throughout the rest of section  [ sec : cc ]",
    "we shall rely on the following assumption .",
    "[ a : gaussian ] @xmath65 is a gaussian random vector with mean zero and covariance matrix @xmath137 , denoted by  @xmath138 .",
    "polytopic constraint functions @xmath139 where @xmath140 , and @xmath141 , describe one of the most common types of constraints . in light of   and  , @xmath142 where @xmath143 and @xmath144 .",
    "it is thus apparent that @xmath145 is affine in the parameters @xmath60 . yet , in general , constraint   is nonconvex .",
    "we now describe three approaches to approximate constraints   that lead to convex conservative constraints .",
    "constraint   requires us to satisfy @xmath146 with probability of at least @xmath114 . here",
    "@xmath147 and the inequality @xmath148 is interpreted componentwise .",
    "one idea is to select coefficients @xmath149 such that @xmath150 and to satisfy the inequalities @xmath151 , with @xmath110 .",
    "note that this choice is obtained in   by setting @xmath152 and @xmath153-\\infty,0]}}(\\eta_i)-\\alpha_i$ ] where , for @xmath110 , @xmath154 denotes the @xmath125-th component of function @xmath69 .",
    "let @xmath155 and @xmath156 be the @xmath125-th entry of @xmath157 and the @xmath125-th row of @xmath158 , respectively , and let @xmath159 be a symmetric real matrix square root of @xmath160 .",
    "[ prop : constrsep ] let @xmath1610,1[$ ] . under assumption",
    "[ a : gaussian ] , the constraint @xmath162 with @xmath111 defined as in  , is equivalent to the second - order cone constraint in the parameters @xmath163 @xmath164 where @xmath165 and @xmath166 is the inverse of the standard error function @xmath167 . as a consequence , under assumption  [ a : convexobj ] and if @xmath168 , the problem @xmath61 \\\\ \\mathrm{subject~to}\\quad&\\eqref{e : ubardef},~\\eqref{eq : cloopsys}~\\mathrm{and } ~h_{i , \\theta } + \\beta_i{\\left\\lvert{\\bar \\sigma^{\\frac{1}{2}}p_{i , \\theta}}\\right\\rvert}\\leq 0,~ i=1,\\ldots , r\\end{aligned}\\ ] ] is a convex conservative approximation of problem  .    from eq",
    ".   we can write @xmath169 . since @xmath170",
    ", @xmath171 is also gaussian with distribution @xmath172 .",
    "it is easily seen that , for any scalar gaussian random variable @xmath173 with distribution @xmath174 , @xmath175 where @xmath176 .",
    "hence the constraint @xmath177 is equivalent to @xmath178 , where @xmath179 .",
    "since @xmath180 and @xmath181 are both affine in the parameters @xmath182 , the above constraint is a second - order cone constraint in @xmath60 .",
    "it is easy to see that this choice guarantees that @xmath183",
    ".      the approach of section  [ s : sep ] may be too conservative since the probability of a union of events is approximated by the sum of the probabilities of the individual events .",
    "one can also calculate a conservative approximation of the union at once .",
    "constraint   with @xmath184 as in   restricts the choice of @xmath158 and @xmath157 to be such that , with a probability of @xmath114 or more , the realization of random vector @xmath185 lies in the negative orthant @xmath186 . in general , it is difficult to describe this constraint explicitly since it involves the integration of the probability density of @xmath111 over the negative orthant .",
    "however , an explicit approximation of the constraint can be computed by ensuring that the @xmath187 confidence ellipsoid of @xmath111 is contained in the negative orthant . fulfilling",
    "this requirement automatically implies that the probability of @xmath146 is strictly greater than @xmath114 .",
    "since @xmath188 , it follows that @xmath189 , with @xmath190 . consider the case where @xmath191 is invertible .",
    "define the @xmath192-dimensional ellipsoid @xmath193 where @xmath194 is a parameter specifying the size of the ellipsoid .",
    "notice that , in general , @xmath191 is invertible when @xmath195 ( i.e. the number of constraints is less than the total dimension of the process noise ) .",
    "if @xmath196 , as there are @xmath197 independent random variables in the optimization problem , the following result still holds with @xmath197 in place of @xmath192 .",
    "[ prop : confellip ] let @xmath1980 , 1[$ ] . under assumption",
    "[ a : gaussian ] , the constraint @xmath199 \\ge 1-\\alpha$ ] with @xmath111 defined as in   is conservatively approximated by the constraint @xmath200\\,^r,\\ ] ] where @xmath201 and @xmath202 is the probability distribution function of a @xmath203 random variable with @xmath192 degrees of freedom .",
    "moreover ,   can be reformulated as the set of second - order cone constraints @xmath204 as a consequence , under assumption  [ a : convexobj ] , the problem @xmath61 \\\\ \\mathrm{subject~to}\\quad&\\eqref{e : ubardef},~\\eqref{eq : cloopsys}~\\mathrm{and}~\\eqref{eq : equivconfellip}\\end{aligned}\\ ] ] is a convex conservative approximation of problem  .    since @xmath205 , the random variable @xmath206 is @xmath203 with @xmath192 degrees of freedom . then , choosing @xmath207 such that @xmath208 guarantees that @xmath209 is the @xmath187 confidence ellipsoid for @xmath145 . finally , under  , @xmath199 \\ge \\pp[\\eta(\\bar x_\\theta , \\bar u_\\theta)\\in{\\mathcal}e\\big(h_\\theta,\\bar \\sigma_\\theta , \\beta(\\alpha ) \\big)]=1-\\alpha$ ] , which proves the first claim .",
    "to prove the second claim , note that   can alternatively be represented as @xmath210 where @xmath211 .",
    "since @xmath212 if and only if @xmath213 , where the @xmath214 denote the standard basis vectors in @xmath215 , we may rewrite   as @xmath216 or equivalently @xmath217 for @xmath218 . for each @xmath125 , the supremum is attained with @xmath219 ; therefore , the above is equivalent to @xmath220 .",
    "clearly , @xmath221 .",
    "moreover , since @xmath222 , we have @xmath223 therefore , constraint   reduces to  .",
    "since the variables @xmath155 and @xmath224 are affine in the original parameters @xmath225 , this is an intersection of second order cone constraints . as a result , under the additional assumption  [ a : convexobj ] ,",
    "the optimization of @xmath92 $ ] with   in place of   is a convex conservative approximation of  .      in light of propositions  [ prop : constrsep ] and  [ prop : confellip ] , both approaches lead to formulating constraints of the form @xmath226 with @xmath110 , where @xmath227 and @xmath228 are the mean and the standard deviation of the scalar random variables @xmath229 , and smaller values of the constant @xmath207 correspond to less conservative approximations of the original chance constraint . for a given value of @xmath115 , the value of @xmath207 depends on the number of constraints @xmath192 in a way that differs in the two cases . in the confidence ellipsoid method , in particular , the value of @xmath207 is determined by @xmath230 ( the total dimension of the process noise ) when @xmath231 . in figure",
    "[ figure : betagrowth ] , we compare the growth of @xmath207 in the two approaches under the assumption that @xmath192 grows linearly with the horizon length @xmath232 .",
    "( for the constraint separation method we choose @xmath233 , so that the value of @xmath207 is the same for all constraints . )     as a function of the horizon length @xmath232 for the ellipsoidal method ( dashed line ) and the constraint separation method ( solid lines ) .",
    "it is assumed that @xmath234 , where @xmath235 is the dimension of the system state and of the process noise , @xmath236 is the dimension of the input @xmath48 .",
    "@xmath237 reflects the number of constraints per stage ; we take @xmath238 .",
    "the value of @xmath207 is invariant with respect to @xmath239 ( single dashed line ) for the ellipsoidal method , while it increases with @xmath239 ( multiple solid lines ) for the constraint separation method . ,",
    "width=415 ]    the increase of @xmath207 is quite rapid in the confidence ellipsoid method , which is only effective for a small number of constraints .",
    "an explanation of this phenomenon is provided by the following fact , that is better known by the name of ( classical ) `` concentration of measure '' inequalities ; proofs may be found in , e.g. ,  @xcite .    [",
    "p : conc ] let @xmath240 be the @xmath192-dimensional gaussian measure with mean @xmath241 and ( nonsingular ) covariance @xmath242 , i.e. , @xmath243 then for @xmath2440 , 1[$ ] ,    1 .",
    "@xmath245 ; 2 .",
    "@xmath246 .    the above proposition states that as the dimension @xmath192 of the gaussian measure increases",
    ", its mass concentrates in an ellipsoidal shell of ` mean - size ' @xmath247 .",
    "it readily follows that since @xmath248 is a @xmath192-dimensional gaussian random vector , its mass concentrates around a shell of size @xmath249 .",
    "note that the bounds corresponding to ( i ) and ( ii ) of proposition  [ p : conc ] in the case of @xmath248 are independent of the optimization parameters @xmath60 ; of course the relative sizes of the confidence ellipsoids change with @xmath60 ( because the mean and the covariance of @xmath248 depend on @xmath60 ) , but proposition  [ p : conc ] shows that the size of the confidence ellipsoids grow quite rapidly with the dimension of the noise and the length of the optimization horizon .",
    "intuitively one would expect the ellipsoidal constraint approximation method to be more effective than the cruder approximation by constraint separation .",
    "figure  [ figure : betagrowth ] and proposition  [ p : conc ] however suggest that this is not the case in general ; for large numbers of constraints ( e.g. longer mpc prediction horizon ) the constraint separation method is the less conservative .      for any @xmath192-dimensional random vector @xmath111",
    ", we have @xmath250-\\infty , 0]}}(\\eta_i)\\right]$ ] .",
    "using this fact one can arrive at conservative convex approximations of the chance - constraint  ( [ e : constr ] ) by replacing the function in the expectation with appropriate approximating functions . for @xmath251 , @xmath110 , consider @xmath252    [ p : expcons ] for any @xmath192-dimensional random vector @xmath111 , @xmath253\\geq 1- \\pp[\\eta\\leq 0]$ ] .",
    "for every fixed value of @xmath111 , it holds that @xmath254-\\infty , 0]}}(\\eta_i)$ ] . hence @xmath253\\geq",
    "\\ee[1-\\prod_{i=1}^{r}{\\mathbf 1_{]-\\infty , 0]}}(\\eta_i ) ] = 1- \\pp[\\eta\\leq 0]$ ] .",
    "[ proposition : gaussexp ] under assumption  [ a : gaussian ] , for @xmath111 defined as in  , it holds that @xmath255= \\sum_{i=1}^r \\exp \\big(t_ih_{i,\\theta } + \\frac{t_i^2}{2 } ||\\bar\\sigma^{\\frac{1}{2}}p_{i,\\theta}||^2\\big).\\ ] ] as a consequence , under assumption  [ a : convexobj ] and for any choice of @xmath251 , @xmath110 , the problem @xmath61 \\\\ \\mathrm{subject~to}\\quad&\\eqref{e : ubardef},~\\eqref{eq : cloopsys}~\\mathrm{and}~\\sum_{i=1}^r \\exp \\big(t_ih_{i,\\theta } + \\frac{t_i^2}{2 } ||\\bar\\sigma^{\\frac{1}{2}}p_{i,\\theta}||^2\\big)\\leq \\alpha\\end{aligned}\\ ] ] is a convex conservative approximation of problem  .    it is easily seen that , for any @xmath192-dimensional gaussian random vector @xmath111 with mean @xmath256 and covariance matrix @xmath242 , and any vector @xmath257 , @xmath258 = \\exp\\bigl(c^t\\mu+\\frac{1}{2}c^t\\sigma ' c\\bigr)$ ] .",
    "let us now write @xmath259 in place of @xmath248 for shortness . by the hypotheses on @xmath66 , in the light of  , @xmath259 is gaussian with mean @xmath157 and covariance @xmath260 . then , for a vector @xmath261 with zero entries except for a coefficient @xmath262 in the @xmath125-th position , @xmath263=\\exp\\big(c_i^t h_\\theta+\\frac{1}{2}c_i^t p_\\theta\\bar\\sigma p_\\theta^t c_i\\big)= \\exp\\big(t_ih_{i,\\theta}+\\frac{t_i^2}{2}p_{i,\\theta}^t\\bar\\sigma p_{i,\\theta } \\big).\\ ] ] summing up for @xmath110 yields the first result . in order to prove the second statement , note that @xmath264 \\leq \\alpha\\iff & \\log\\ee\\bigl[\\varphi(\\eta_\\theta)\\bigr ] \\leq \\log \\alpha \\\\ \\iff & \\log\\biggl(\\sum_{i=1}^{r } { \\exp\\bigl(t_ih_{i,\\theta}+\\frac{t_i^2}{2}{\\left\\lvert{\\bar\\sigma^{\\frac{1}{2}}p_{i,\\theta}}\\right\\rvert}^2\\bigr)}\\biggr ) \\leq \\log(\\alpha).\\end{aligned}\\ ] ] for each @xmath125 , @xmath265 is a convex function of the optimization parameters @xmath60 . by",
    "* example  3.14 ) , given convex functions @xmath266 , the function @xmath267 is itself convex .",
    "it follows that @xmath268 $ ] is a convex function of @xmath60 and that the constraint set @xmath269\\leq \\log \\alpha\\}=\\{\\theta\\in\\theta:~\\ee[\\varphi(\\eta_\\theta)]\\leq \\alpha\\}\\ ] ] is convex .",
    "finally , from lemma  [ p : expcons ] , if @xmath270\\leq \\alpha$ ] then @xmath271\\geq   1- \\ee[\\varphi(\\eta_\\theta)]\\geq 1-\\alpha$ ] . together with assumption  [ a : convexobj ] , this implies that the optimization problem with constraint latexmath:[$\\sum_{i=1}^r \\exp \\big(t_ih_{i,\\theta } + \\frac{t_i^2}{2 }    of   is a convex conservative approximation of  .",
    "this convex approximation of problem   is obtained in   by setting @xmath116 and @xmath273 .",
    "the result that the approximation is conservative relies essentially on the fact that , with this choice , @xmath274 @xmath275 ( see lemma [ p : expcons ] ) .",
    "this result can be generalized : given any two functions @xmath276 such that @xmath277 @xmath278 , constraint   with @xmath279 is more conservative than the same constraint with @xmath280 .",
    "this type of analysis can be exploited to compare different probabilistic constraints and to minimize the conservatism of the convex approximations with respect with the tunable parameters , but is not fully pursued here .",
    "consider the constraint function @xmath281 where @xmath282 and @xmath283 are given .",
    "then the constraint @xmath146 restricts the vector @xmath284 to an ellipsoid with center @xmath283 and shape determined by @xmath285 .",
    "we now provide an approximation of the chance constraint   that is a semi - definite program in the optimization parameters @xmath286 .",
    "similar to ",
    "[ s : confellipsoids ] , the idea is to ensure that the @xmath187 confidence ellipsoid of @xmath65 is such that   holds .",
    "to this end , let @xmath287 where @xmath288 and @xmath289 .",
    "[ prop : elliplmi ] define @xmath290 , with @xmath291 as in proposition  [ prop : confellip ] , and @xmath292 .",
    "then @xmath293 is a linear matrix inequality ( lmi ) in the unknown parameters @xmath60 and @xmath294 .",
    "if @xmath295 is a solution of  , then @xmath60 satisfies  . as a consequence , under assumption  [ a : convexobj ] , the problem @xmath296 \\\\ \\mathrm{subject~to}\\quad&\\eqref{e",
    ": ubardef},~\\eqref{eq : cloopsys}~\\mathrm{and}~\\eqref{e : bigge}\\end{aligned}\\ ] ] is a convex conservative approximation of problem  .    the inequality   may be equivalently represented as @xmath297 where @xmath298 . since @xmath138 , one can compute @xmath299 such that @xmath300 specifies the required @xmath187 confidence ellipsoid of @xmath65 . hence , we need to ensure that @xmath301 .",
    "this is equivalent to @xmath302 it follows from  @xcite that @xmath303 if and only if there exists @xmath304 such that @xmath305 using schur complements the last relation can be rewritten equivalently as  .",
    "therefore , any solution of   implies  . to verify that   is an lmi , note that @xmath306 and @xmath307 are affine in the optimization variables . together with the assumed convexity of @xmath92 $ ] , the last statement of the proposition follows .",
    "in this section we focus on the problem @xmath61 \\label{eq : objectiveicc } \\\\",
    "\\textrm{subject to}\\quad&\\eqref{e : ubardef},~\\eqref{eq : cloopsys}\\textrm { and } \\\\ & j_i(\\theta)\\leq \\beta_i,~i=1,\\ldots , r \\label{eq : genicc}\\end{aligned}\\ ] ] where , for @xmath110 , @xmath308 $ ] , functions @xmath309 are measurable and the @xmath310 are fixed parameters .",
    "this problem corresponds to problem   when setting @xmath152 and @xmath311 , with @xmath110 . for the choice @xmath312 constraints of the form  ( [ eq : genicc ] )",
    "are known as integrated chance constraints  @xcite .",
    "in fact , one may write ( dropping the dependence of @xmath171 on @xmath96 and @xmath97 to simplify the notation ) @xmath313=\\ee\\left[\\int_{0}^{+\\infty}{\\mathbf 1_{[0,\\eta_i ) } } ( s ) \\,{\\mathrm}ds\\right]=\\int_{0}^{+\\infty}\\pp\\bigl(\\eta_i > s\\bigr)\\,{\\mathrm}ds,\\ ] ] where @xmath314 is the indicator function of set @xmath315 and the second equality follows from tonelli s theorem  ( * ? ? ?",
    "* theorem 4.4.5 ) .",
    "therefore , constraint   is equivalent to @xmath316 whence the name integrated chance constraint .",
    "note that @xmath317 plays the role of a penalty ( or barrier ) function that penalizes violations of the inequality @xmath318 , and @xmath319 is a maximum allowable cost in the sense of  ( [ eq : icc ] ) .",
    "of course , different choices of @xmath317 will not guarantee the equivalence between  ( [ eq : genicc ] ) and  ( [ eq : icc ] ) .",
    "however , they may be useful in deriving other quantitative chance constraint - type approximations .",
    "we now establish sufficient conditions on the @xmath171 and @xmath317 for the convexity of the constraint set @xmath320    the result is again a consequence of proposition  [ thm : phietaconvex ] .",
    "[ thm : etaphiconditions ] let the mappings @xmath321 be measurable and convex , and let the @xmath322 be measurable , monotone nondecreasing and convex .",
    "assume that the @xmath323 are finite for all @xmath60 .",
    "then each @xmath323 is a convex function of @xmath60 and @xmath324 is a convex set . as a consequence , under assumption  [ a : convexobj ] ,  ",
    "is a convex optimization problem .",
    "fix @xmath76 arbitrarily .",
    "since the mapping @xmath325 is affine and @xmath326 is convex by assumption , their composition @xmath327 is a convex function of @xmath60 . using the assumption that @xmath317 is monotone nondecreasing and convex",
    ", we may apply proposition  [ thm : phietaconvex ] with @xmath328 and @xmath317 in place of @xmath77 to conclude that @xmath329 $ ] is convex . hence , for any choice of @xmath319 , the set @xmath330 is convex . since @xmath331 , the convexity of @xmath324 follows . together with assumption  [ a : convexobj ] , this proves that   is a convex optimization problem .",
    "it is worth noting that the function @xmath332 of section  [ section : approxviaexp ] satisfies analogous monotonicity and convexity assumptions with respect to each of the @xmath171 , with @xmath110 . unlike those of section  [ sec : cc ] , this convexity result is independent of the probability distribution of @xmath66 . by virtue of the alternative assumptions ( iii@xmath103 ) and",
    "( iii@xmath104 ) of proposition  [ thm : phietaconvex ] , the requirement that @xmath78 be finite for all @xmath60 may be relaxed .",
    "a sufficient requirement is that there exist no two values @xmath60 and @xmath105 such that @xmath333 and @xmath334 . in particular , provided measurable and convex @xmath171 , definition  ( [ eq : ramp ] ) satisfies all the requirements of proposition  [ thm : etaphiconditions ] .",
    "the ( scalar ) polytopic constraint function : @xmath335 fulfills the hypotheses of proposition  [ thm : etaphiconditions ] .",
    "hence , the corresponding integrated chance constraint is convex .",
    "[ ex : followon1 ] following example  [ thm : examples ] , an interesting case is that of ellipsoidal constraints . for an @xmath336-size positive - semidefinite real matrix @xmath285 and a vector @xmath337 , define @xmath338",
    "this is a convex function of the vector @xmath339 ( it is the composition of the convex mapping @xmath340 , @xmath282 , with the affine mapping @xmath341 ) and hence proposition  [ thm : etaphiconditions ] applies .",
    "a problem setting similar to example  [ ex : followon1 ] with quadratic expected - type cost function and ellipsoidal constraints has been adopted in  @xcite , where hard constraints are relaxed to expected - type constraints of the form @xmath342\\leq \\beta$ ] .",
    "this formulation can be seen as a special case of integrated chance constraints with @xmath343 for all @xmath344 .",
    "the choice of @xmath77 within a large class of functions is an extra degree of freedom provided by our framework that may be exploited to establish tight bounds on the probability of violating the original hard constraints , see lemma  [ p : expcons ] for an example .",
    "even though icc problems are convex in general , deriving efficient algorithms to solve them is still a major challenge  @xcite . for certain iccs ,",
    "it is possible however to derive explicit expressions for the gradients of the constraint function .",
    "provided the cost has a simple ( e.g. quadratic ) form , this allows one to implement standard algorithms ( e.g. interior point methods  @xcite ) for the solution of the optimization problem .",
    "let @xmath66 satisfy assumption  [ a : gaussian ] with , for simplicity , @xmath345 equal to the identity matrix , i.e. @xmath346 . consider the problem with one scalar constraint ( the generalization to multiple ( joint ) constraints is straightforward ) : @xmath347\\leq \\beta ,   \\end{aligned}\\ ] ] where @xmath77 is defined as in  .",
    "let @xmath344 be a gaussian random variable with mean @xmath256 and variance @xmath348",
    ". then @xmath349 = \\sigma g\\bigl(\\frac{\\mu}{\\sigma}\\bigr)\\ ] ] where @xmath350 and @xmath351 is the standard complementary error function .    since @xmath352 it holds that @xmath349 = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{0}^{\\infty}t\\exp\\left(-\\frac{(t-\\mu)^2}{2\\sigma^2}\\right)\\;dt.\\ ] ] by the change of variable @xmath353 one gets @xmath354=&\\,\\ , \\frac{1}{\\sqrt{\\pi}}\\int_{\\frac{-\\mu}{\\sqrt{2}\\sigma}}^{\\infty } ( \\mu+\\sqrt{2}\\sigma y)\\exp(-y^2)dy \\\\ = & \\,\\,\\frac{\\mu}{\\sqrt{\\pi}}\\int_{\\frac{-\\mu}{\\sqrt{2}\\sigma}}^{\\infty } \\exp(-y^2)dy + \\sigma\\sqrt{\\frac{2}{\\pi } } \\int_{\\frac{-\\mu}{\\sqrt{2}\\sigma}}^{\\infty } y\\exp(-y^2)dy\\\\ = & \\,\\,\\frac{\\mu}{2}\\operatorname{erfc}\\left(\\frac{-\\mu}{\\sqrt{2}\\sigma}\\right)+\\frac{\\sigma}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{\\mu^2}{2\\sigma^2}\\right ) , \\ ] ] which is equal to @xmath355 .",
    "simple calculations and the application of this lemma yield the following result .",
    "problem   is equivalent to @xmath356 where @xmath357    note that expectations have been integrated out .",
    "now it is possible to put the problem in a standard form for numerical optimization .",
    "let @xmath358 and @xmath359 be the @xmath125-th column of @xmath122 and @xmath360 , respectively . redefine the optimization variable @xmath60 as the vector @xmath361 .",
    "define @xmath362 , @xmath363 , @xmath364 and @xmath365 by @xmath366    problem   is equivalent to @xmath367 where the matrix @xmath368 in the equality constraint accounts for the causal structure of @xmath122 , while @xmath369 and @xmath370 and @xmath371 are selection matrices such that @xmath372 and @xmath373 .",
    "we conclude the section by documenting the expressions of the gradient and the hessian of the constraint function @xmath374 .",
    "@xmath375 + @xmath376\\ ] ]    where @xmath377 the expressions of gradient and hessian of the quadratic function @xmath378 , used e.g. by interior point method solvers , are quite standard and will not be reported here .",
    "we illustrate some of our results with the help of a simple example .",
    "consider the mechanical system shown in figure  [ f : springs ] .",
    "[ b]@xmath379 [ b]@xmath380 [ b]@xmath381 [ b]@xmath382 [ ] @xmath383 [ ] @xmath384 [ ] @xmath385 [ ] @xmath386 [ t]@xmath387 [ t]@xmath388 [ t]@xmath389 [ t]@xmath390 [ b]@xmath391 [ b]@xmath392 [ t]@xmath393     @xmath394 are displacements from an equilibrium position , @xmath395 are forces acting on the masses .",
    "in particular , @xmath391 is a tension between the first and the second mass , @xmath392 is a tension between the third and the fourth mass , and @xmath393 is a force between the wall ( at left ) and the second mass .",
    "we assume all mass and stiffness constants to be equal to unity , i.e. @xmath396 .",
    "we consider a discrete - time model of this system with noise in the dynamics , @xmath397 where @xmath12 is an i.i.d .",
    "noise process , @xmath398 for all @xmath0 , and @xmath399 .",
    "the discrete - time dynamics are obtained by uniform sampling of a continuous - time model at times @xmath400 , with sampling time @xmath401 and @xmath402 , under the assumption that the control action @xmath50 is piecewise constant over the sampling intervals @xmath403 .",
    "hence , @xmath404 and @xmath405 , where @xmath406 and @xmath407 , defined as @xmath408 are the state and input matrices of the standard ode model of the system .",
    "we are interested in computing the control policy that minimizes the cost function @xmath409,\\ ] ] where the horizon length is fixed to @xmath410 , the weight matrices are defined as @xmath411 ( penalizing displacements but not their derivatives ) and @xmath412 . the initial state is set to @xmath413^t$ ] . in the absence of constraints , this is a finite horizon lqg problem whose optimal solution is the linear time - varying feedback from the state @xmath414 where the matrices @xmath415 are computed by solving , for @xmath416 , the backward dynamic programming recursion @xmath417 with @xmath418 .",
    "simulated runs of the controlled system are shown in figure  [ f : lqg ] .",
    "we shall now introduce constraints on the state and the control input and study the feasibility of the problem with the methods of section  [ sec : cc ] .",
    "the convex approximations to the chance - constrained optimization problems are solved numerically in matlab by the toolbox ` cvx `  @xcite . in all cases",
    "we shall compute a @xmath419-stage affine optimal control policy and apply it to repeated runs of the system .",
    "based on this we will discuss the feasibility of the hard constrained problem and the probability of constraint violation .",
    "let us impose bounds on the control inputs , @xmath420 , @xmath421 and @xmath422 , with @xmath423 , and bounds on the mass displacements , @xmath424 , for @xmath425 and with @xmath426 . in the notation of section  [ s : polyt ] , these constraints are captured by the equation @xmath427 where @xmath428 and @xmath429 , with @xmath430 and @xmath431 this hard constraint is relaxed to the probabilistic constraint @xmath432\\geq 1-\\alpha$ ] .",
    "the resulting optimal control problem is then addressed by constraint separation  ( section  [ s : sep ] ) and ellipsoidal approximation  ( section  [ s : confellipsoids ] ) .    with constraint separation ,",
    "the problem is feasible for @xmath433 . for @xmath434 ,",
    "the application of the suboptimal control policy computed as in proposition  [ prop : constrsep ] yields the results shown in figure  [ f : constrsep ] .         with this policy , the control input saturates within the required bounds whereas the mass displacements stay well below bounds .",
    "in fact , although the required probability of constraint satisfaction is @xmath435 , constraints were never violated in 1000 simulation runs .",
    "this suggests that the approximation incurred by constraint separation is quite conservative , mainly due to the relatively large number of constraints .",
    "it may also be noticed that the variability of the applied control input is rather small .",
    "this hints that the computed control policy is essentially open - loop , i.e. the linear feedback gain is small compared to the affine control term .    with the ellipsoidal approximation method , for the same probability level",
    ", the problem turns out to be infeasible , in accordance with the conclusions of section  [ s : comparison ] . for the sake of investigation",
    ", we loosened the bounds on the mass displacements to @xmath436 for all @xmath125 and @xmath0 .",
    "the problem of proposition  [ prop : confellip ] is then feasible and the results from simulation of the controlled system are reported in figure  [ f : confellip ] .",
    "although the controller has been computed under much looser bounds , the control performance is similar to the one obtained with constraint separation , a clear sign that the ellipsoidal approximation is overly conservative in this case .",
    "another evidence of inaccuracy is the fact that , while the control inputs get closer to the bounds , the magnitude of the displacements is not reduced .",
    "as in the case of constraint separation , the applied control input is insensitive to the specific simulation run , i.e. the control policy is essentially open loop .      consider the constraint function @xmath437 with @xmath438 . unlike the previous section , we do not impose bounds on @xmath439 and @xmath440 at each @xmath441 but instead require that the total `` spending '' on @xmath442 and @xmath48 does not exceed a total `` budget '' .",
    "this constraint can be modelled in the form of section  [ sec : ellipconstr ] , namely @xmath443 with @xmath444 and @xmath445 , where @xmath446 the constrained control policy for @xmath410 and @xmath447 is computed by solving the lmi problem of proposition  [ prop : elliplmi ] .",
    "results from simulations of the closed - loop system are reported in figure  [ f : ellipconstr ] .",
    "once again , constraints were not violated over 1000 simulated runs , showing the conservatism of the approximation .",
    "it is interesting to note that the displacements of the masses are generally smaller than those obtained by the controller computed under affine constraints , at the cost of a slightly more expensive control action .",
    "in contrast with the affine constraints case , the control action obtained here is much more sensitive to the noise in the dynamics , i.e. the feedback action is more pronounced .",
    "we have studied the convexity of optimization problems with probabilistic constraints arising in model predictive control of stochastic dynamical systems .",
    "we have given conditions for the convexity of expectation - type objective functions and constraints .",
    "convex approximations have been derived for nonconvex probabilistic constraints .",
    "results have been exemplified by a numerical simulation study .",
    "open issues that will be addressed in the future are the role of the tunable parameters ( e.g. the @xmath448 in section  [ s : sep ] , the @xmath262 of section  [ section : approxviaexp ] and the @xmath319 in section  [ sec : icc ] ) in the various optimization problems , and the effect of different choices of the icc functions @xmath317 ( section  [ sec : icc ] ) .",
    "directions of future research also include the extension of the results presented here to the case of noisy state measurements , the exact or approximate solution of the stochastic optimization problems in terms of explicit control laws and the control of stochastic systems with probabilistic constraints on the state via bounded control laws .                                                              , _ a full solution to the constrained stochastic closed - loop mpc problem via state and innovations feedback and its receding horizon implementation _ , in proceedings of the 42nd ieee conference on decision and control , vol .  1 , 9 - 12 dec . 2003 , pp .  929934 ."
  ],
  "abstract_text": [
    "<S> we investigate constrained optimal control problems for linear stochastic dynamical systems evolving in discrete time . </S>",
    "<S> we consider minimization of an expected value cost over a finite horizon . </S>",
    "<S> hard constraints are introduced first , and then reformulated in terms of probabilistic constraints . </S>",
    "<S> it is shown that , for a suitable parametrization of the control policy , a wide class of the resulting optimization problems are convex , or admit reasonable convex approximations .    ,    ,    ,    stochastic control ; convex optimization ; probabilistic constraints </S>"
  ]
}