{
  "article_text": [
    "bayesian inference using markov chain monte carlo simulation methods is used extensively in statistical applications . in this approach ,",
    "the parameters are generated from a proposal distribution , or several such proposal distributions , with the generated proposals accepted or rejected using the metropolis - hastings method ; see for example @xcite .    in adaptive sampling",
    "the parameters of the proposal distribution are tuned by using previous draws .",
    "our article deals with diminishing adaptation schemes , which means that the difference between successive proposals converges to zero . in practice",
    ", this usually means that the proposals themselves eventually do not change .",
    "important theoretical and practical contributions to diminishing adaptation sampling were made by @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and @xcite .",
    "the adaptive random walk metropolis method was proposed by @xcite with further contributions by @xcite , @xcite and @xcite . @xcite",
    "propose an adaptive independent metropolis - hastings method with a mixture of normals proposal which is estimated using a clustering algorithm .",
    "although there is now a body of theory justifying the use of adaptive sampling , the construction of interesting adaptive samplers and their empirical performance on real examples has received less attention .",
    "our article aims to fill this gap by introducing a @xmath0-copula based proposal density .",
    "an antithetic version of this proposal is also studied and is shown to increase efficiency when the acceptance rate is above 70% . we also refine the adaptive metropolis - hastings proposal in @xcite by adding a heavy tailed component to allow the sampling scheme to traverse multiple modes more easily . as well as being of interest in its own right , in some of the examples we have also used this refined sampler to initialize the adaptive independent metropolis - hastings schemes . we study the performance of the above adaptive proposals , as well as the adaptive mixture of normals proposal of @xcite , for a number of models and priors using real data .",
    "the models and priors produce challenging but realistic posterior target distributions .",
    "@xcite is a longer version of our article that considers some alternative versions of our algorithms and includes more details and examples .",
    "suppose that @xmath1 is the target density from which we wish to generate a sample of observations , but that it is computationally difficult to do so directly .",
    "one way of generating the sample is to use the metropolis - hastings method , which is now described .",
    "suppose that given some initial @xmath2 we have generated the @xmath3 iterates @xmath4 .",
    "we generate @xmath5 from the proposal density @xmath6 which may also depend on some other value of @xmath7 which we call @xmath8 .",
    "let @xmath9 be the proposed value of @xmath5 generated from @xmath10 .",
    "then we take @xmath11 with probability @xmath12 and take @xmath13 otherwise .",
    "if @xmath14 does not depend on @xmath15 , then under appropriate regularity conditions we can show that the sequence of iterates @xmath16 converges to draws from the target density @xmath1 .",
    "see @xcite for details .    in adaptive sampling",
    "the parameters of @xmath14 are estimated from the iterates @xmath17 . under appropriate regularity conditions the sequence of iterates @xmath18 , converges to draws from the target distribution @xmath1 .",
    "see @xcite , @xcite and @xcite .",
    "we now describe the adaptive sampling schemes studied in the paper .",
    "the adaptive random walk metropolis proposal of @xcite is @xmath19 where @xmath20 is the dimension of @xmath7 and @xmath21 is a multivariate @xmath20 dimensional normal density in @xmath7 with mean @xmath8 and covariance matrix @xmath22 . in , @xmath23 for @xmath24 , with @xmath25 representing the initial iterations , @xmath26 for @xmath27 with @xmath28 ; @xmath29 is a constant covariance matrix , which is taken as the identity matrix by @xcite but can be based on the laplace approximation or some other estimate .",
    "the matrix @xmath30 is the sample covariance matrix of the first @xmath31 iterates .",
    "the scalar @xmath32 is meant to achieve a high acceptance rate by moving the sampler locally , while the scalar @xmath33 is considered to be optimal @xcite for a random walk proposal when the target is multivariate normal .",
    "we note that the acceptance probability for the adaptive random walk metropolis simplifies to @xmath34    we refine the two component random walk metropolis proposal in by adding a third component with @xmath35 and with @xmath36 .",
    "we take @xmath37 if @xmath38 , @xmath39 for @xmath27 and @xmath40 . alternatively",
    ", the third component can be a multivariate @xmath0 distribution with small degrees of freedom .",
    "we refer to this proposal as the three component adaptive random walk .",
    "the purpose of the heavier tailed third component is to allow the sampler to explore the state space more effectively by making it easier to leave local modes .    to illustrate this issue we consider the performance of the two and three component adaptive random walk samplers when the target distribution is a two component and five dimensional multivariate mixture of normals .",
    "each component in the target has equal probability , the first component has mean vector @xmath41 and the second component has mean vector @xmath42 .",
    "both components have identity covariance matrices . for the three component",
    "adaptive random walk we choose @xmath43 .",
    "the starting value is @xmath44 for both adaptive random walk samplers .",
    "figure  [ fig : bimodal : normal ] compares the results and shows that the two component adaptive random walk fails to explore the posterior distribution even after 500 , 000 iterations , whereas the three component adaptive random walk can get out of the local modes .",
    "+      the proposal density of the adaptive independent metropolis - hastings approach of @xcite is a mixture with four terms of the form @xmath45 with @xmath46 the parameter vector for the density @xmath47 .",
    "the sampling scheme is run in two stages , which are described below . throughout each stage ,",
    "the parameters in the first two terms are kept fixed .",
    "the first term @xmath48 is an estimate of the target density and the second term @xmath49 is a heavy tailed version of @xmath48 .",
    "the third term @xmath50 is an estimate of the target that is updated or adapted as the simulation progresses and the fourth term @xmath51 is a heavy tailed version of the third term . in the first stage",
    "@xmath52 is a laplace approximation to the posterior if it is readily available and works well ; otherwise , @xmath52 is a gaussian density constructed from a preliminary run of 1000 iterates or more of the three component adaptive random walk . throughout , @xmath49 has the same component means and probabilities as @xmath48 , but its component covariance matrices are ten times those of @xmath48 .",
    "the term @xmath50 is a mixture of normals and @xmath51 is also a mixture of normals obtained by taking its component probabilities and means equal to those of @xmath50 , and its component covariance matrices equal to 20 times those of @xmath50 .",
    "the first stage begins by using @xmath53 and @xmath49 only with , for example , @xmath54 and @xmath55 , until there is a sufficiently large number of iterates to form @xmath50 .",
    "after that we set @xmath56 and @xmath57 .",
    "we begin with a single normal density for @xmath50 and as the simulation progresses we add more components up to a maximum of four according to a schedule that depends on the ratio of the number of accepted draws to the dimension of @xmath7 .",
    "see  appendix  [ s : sim details ] .    in the second stage ,",
    "@xmath48 is set to the value of @xmath50 at the end of the first stage and @xmath58 and @xmath51 are constructed as described above .",
    "the heavy - tailed densities @xmath49 and @xmath51 are included as a defensive strategy , as suggested by @xcite , to get out of local modes and to explore the sample space of the target distribution more effectively .",
    "it is too computationally expensive to update @xmath50 ( and hence @xmath51 ) at every iteration so we update them according to a schedule that depends on the problem and the size of the parameter vector . see  appendix  [ s : sim details ] .",
    "our article estimates the multivariate mixture of normals density for the third component using the method of @xcite who identify the marginals that are not symmetric and estimate their joint density by a mixture of normals using k - harmonic means clustering .",
    "we also estimated the third component using stochastic approximation but _ without _ first identifying the marginals that are not symmetric .",
    "we studied the performance of both approaches to fitting a mixture of normals and found that the clustering based approach was more robust in the sense that it does not require tuning for particular data sets to perform well , whereas for the more challenging target distributions it was necessary to tune the parameters of the stochastic approximation to obtain optimal results .",
    "the results for the stochastic approximation approach are reported in @xcite .",
    "we note that estimating a mixture of normals using the em algorithm also does not require tuning ; see @xcite , as well as @xcite for the online em",
    ". however , the em algorithm is more sensitive to starting values and is more prone to converge to degenerate solutions , particularly in an mcmc context where small clusters of identical observations arise naturally ; see @xcite for a discussion .",
    "the third proposal distribution is a mixture of a @xmath0 copula and a multivariate @xmath0 distribution .",
    "we use the @xmath0 copula as the major component of the mixture because it provides a flexible and fast method for estimating a multivariate density . in our applications",
    "this means that we assume that after appropriately transforming each of the parameters , the joint posterior density is multivariate @xmath0 . or more accurately , such a proposal density provides a more accurate estimate of the posterior density than using a multivariate normal or multivariate @xmath0 proposal .",
    "the second component in the mixture is a multivariate @xmath0 distribution with low degrees of freedom whose purpose is to help the sampler move out of local modes and explore the parameter space more effectively .",
    "@xcite and @xcite provide an introduction to copulas , with details of the @xmath0 copula given in @xcite .",
    "let @xmath59 be a @xmath20-dimensional @xmath0 density with location @xmath60 , scale matrix @xmath61 and degrees of freedom @xmath62 and let @xmath63 be the corresponding cumulative distribution function .",
    "let @xmath64 and @xmath65 be the probability density function and the cumulative distribution function of the @xmath15th marginal , with @xmath66 its parameter vector . then the density of the @xmath0 copula based distribution for @xmath67 is @xmath68 where @xmath69 is determined by @xmath70 , @xmath71 . for a given sample of observations , or in our case a sequence of draws , we fit the copula by first estimating each of the marginals as a mixture of normals .",
    "for the current degrees of freedom , @xmath72 , of the @xmath0 copula , we now transform each observation @xmath73 to @xmath74 using @xmath75 this produces a sample of @xmath20 dimensional observations @xmath76 which we use to estimate the scale matrix @xmath77 as the sample correlation matrix of the @xmath76 s .",
    "given the estimates of the marginal distributions and the scale matrix @xmath77 , we estimate the degrees of freedom @xmath62 by maximizing the profile likelihood function given by over a grid of values @xmath78 , with @xmath79 representing the gaussian copula .",
    "we could use instead the grid of @xmath62 values in , but this is more expensive computationally and we have found our current procedure works well in practice .    the second component of the mixture is a multivariate @xmath0 distribution with its degrees of freedom @xmath80 fixed and small , and with its location and scale parameters estimated from the @xmath7 iterates using the first and second sample moments .",
    "the @xmath0 copula component of the mixture has a weight of 0.7 and the multivariate @xmath0 component a weight of 0.3 .    to draw an observation from the @xmath0 copula , we first draw @xmath76 from the multivariate @xmath0 distribution with location 0 and scale matrix @xmath77 and degrees of freedom @xmath62 .",
    "we then use a newton - raphson root finding routine to obtain @xmath16 for a given @xmath69 from for @xmath71 .",
    "the details of the computation for the @xmath0 copula and the schedule for updating the proposals is given in appendix  [ s : sim details ] .",
    "using antithetic variables in simulations often increases sampling efficiency ; e.g.  @xcite .",
    "@xcite proposes using antithetic variables in markov chain monte carlo simulation when the target is symmetric .",
    "we apply antithetic variables to the copula based approach which generalizes tierney s suggestion by allowing for nonsymmetric marginals . to the best of our knowledge ,",
    "this has not been done before .",
    "the antithetic approach is implemented as follows .",
    "as above , we determine probabilistically whether to sample from the copula component or the multivariate @xmath0 component . if the copula component is chosen , then @xmath67 is generated as above and @xmath81 is also computed .",
    "the values @xmath67 and @xmath82 are then transformed to @xmath76 and @xmath83 respectively and are accepted or rejected one at a time using the metropolis - hastings method . if we decide to sample from the multivariate @xmath0 component to obtain @xmath76 , we also compute @xmath84 , where @xmath60 is the mean of the multivariate @xmath0 , and accept or reject each of these values one at a time using the metropolis - hastings method .",
    "we note that to satisfy the conditions for convergence in @xcite we would run the sampling scheme in two stages , with the first stage as above . in the second stage we would have a three component proposal , with the first two components the same as above .",
    "the third component would be fixed throughout the second stage and would be the second component density at the end of the first stage .",
    "the third component would have a small probability , e.g. 0.05 .",
    "however , in our examples we have found it unnecessary to include such a third component as we achieve good performance without it .",
    "this section studies the five algorithms discussed in section  [ section : adap : samp ] .",
    "the two component adaptive random walk metropolis ( rwm ) and the three component adaptive random walk metropolis ( rwm3c ) are described in section  [ ss : arwm ] .",
    "the adaptive independent metropolis - hastings with a mixture of normals proposal distribution fitted by clustering ( imh - mn - cl ) described in section  [ ss : aimh ] .",
    "the adaptive independent metropolis - hastings which is a mixture of a @xmath0 copula and a multivariate @xmath0 proposal , with the marginal distributions of the copula estimated by mixtures of normals that are fitted by clustering ( imh - tct - cl ) .",
    "the fifth sampler is the antithetic variable version of imh - tct - cl , which we call imh - tct - cl - a .",
    "these proposals are described in section  [ ss : tct ] .",
    "our study compares the performance of the algorithms in terms of the acceptance rate of the metropolis - hastings method , the inefficiency factors ( if ) of the parameters , and an overall measure of effectiveness which compares the times taken by all samplers to obtain the same level of accuracy .",
    "we define the acceptance rate as the percentage of accepted values of each of the metropolis - hastings proposals .",
    "we define the inefficiency of the sampling scheme for a given parameter as the variance of the parameter estimate divided by its variance when the sampling scheme generates independent iterates .",
    "we estimate the inefficiency factor as @xmath85 where @xmath86 is the estimated autocorrelation at lag @xmath15 and the truncated kernel function @xmath87 if @xmath88 and 0 otherwise @xcite . as a rule of thumb ,",
    "the maximum number of lags @xmath89 is given by the lowest index @xmath15 such that @xmath90 with @xmath91 being the sample size use to compute @xmath86 .",
    "we define the equivalent sample size as @xmath92 , where @xmath93 , which can be interpreted as @xmath94 iterates of the dependent sampling scheme are equivalent to @xmath95 independent iterates .",
    "the acceptance rate and the inefficiency factor do not take into account the time taken by a sampler . to obtain an overall measure of the effectiveness of a sampler ,",
    "we define its equivalent computing time @xmath96 , where @xmath89 is the time per iteration of the sampler and @xmath97 .",
    "we interpret @xmath98 as the time taken by the sampler to attain the same accuracy as that attained by @xmath94 independent draws of the same sampler . for two samplers @xmath99 and @xmath100 , @xmath101 is the ratio of times taken by them to achieve the same accuracy .",
    "we note that the time per iteration for a given sampling algorithm depends to an important extent on how the algorithm is implemented , e.g. language used , whether operations are vectorized , which affects @xmath98 but not the acceptance rates nor the inefficiencies .",
    "this section applies the adaptive sampling schemes to the binary logistic regression model @xmath102 using three different priors for the vector of coefficients @xmath103 .",
    "the first is a non - informative multivariate normal prior , @xmath104 the second is a normal prior for the intercept @xmath105 , and a double exponential , or laplace prior , for all the other coefficients , @xmath106 the regression coefficients are assumed to be independent a priori .",
    "we note that this is the prior implicit in the lasso @xcite .",
    "the prior for @xmath107 is @xmath108 , where @xmath109 means an inverse gamma density with shape @xmath99 and scale @xmath100 .",
    "the double exponential prior has a spike at zero and heavier tails than the normal prior . compared to their posterior distributions under a diffuse normal prior",
    ", this prior shrinks the posterior distribution of the coefficients close to zero to values even closer to zero , while the coefficients far from zero are almost unmodified . in the adaptive sampling schemes we work with @xmath110 rather than @xmath107 as it is unconstrained .",
    "the third prior distribution takes the prior for the intercept as @xmath111 and the prior for the coefficients @xmath112 as the two component mixture of normals , @xmath113 with the regression coefficients assumed a priori independent . @xcite",
    "suggest using this prior for bayesian variable selection , with @xmath114 and @xmath115 small and large variances that are chosen by the user . in our article their values",
    "are given for each of the examples below .",
    "the prior for @xmath116 is uniform . in the adaptive sampling we work with the logit of @xmath116 because it is unconstrained .",
    "this section models the probability of labor force participation by women , @xmath117 , in 1975 as a function of the covariates listed in table  [ t : labor force part ] .",
    "this data set is discussed by @xcite , p.  537 and has a sample size of 753 .",
    ".variables used in labor force participation data regression [ cols= \" < , < \" , ]     [ t : pap truncated ]",
    "our article proposes a new copula based adaptive sampling scheme and a generalization of the two component adaptive random walk designed to explore the target space more efficiently than the proposal of @xcite .",
    "we studied the performance of these sampling schemes as well as the adaptive independent metropolis - hastings sampling scheme proposed by @xcite which is based on a mixture of normals .",
    "all the sampling schemes performed reliably on the examples studied in the article , but we found that the adaptive independent metropolis - hastings schemes had inefficiency factors that were often much lower and acceptance rates that were much higher than the adaptive random walk schemes .",
    "the copula based adaptive scheme often had the smallest inefficiency factors and highest acceptance rates .",
    "for acceptance rates over 70% the antithetic version of the copula based approach was the most efficient .",
    "our results suggest that the copula based proposal provides an attractive approach to adaptive sampling , especially for higher dimensions .",
    "however , the mixture of normals approach of @xcite also performed well and is useful for complicated and possibly multimodal posterior distributions .",
    "the research of robert kohn , ralph silva and xiuyan mun was partially supported by an arc discovery grant dp0667069 .",
    "we thank professor garry barret for the cps data and professor denzil fiebig for the pap smear data .",
    "all the computations were done on intel core 2 quad 2.6 ghz processors , with 4 gb ram ( 800mhz ) on a gnu / linux platform using matlab 2007b . however , in the tct algorithm we computed the univariate cumulative distribution functions and inverse cumulative distribution functions of the @xmath0 , normal and mixture of normals distributions using matlab mex files based on the corresponding matlab code .",
    "in addition , to speed up the computation , we tested each marginal for normality using the jarque - bera test at the 5% level .",
    "if normality was not rejected then we fitted a normal density to the marginal .",
    "otherwise , we estimated the marginal density by a mixture of normals .    in stage 1 of the adaptive sampling schemes imh - mn - cl and imh - mn - sa that use a multivariate mixture of normals , the number of components ( @xmath118 ) used in the third term @xmath119 of the mixture",
    "is determined by the dimension of the parameter vector ( @xmath120 ) and the number of accepted draws ( @xmath121 ) to that stage of the simulation .",
    "in particular , @xmath122 if @xmath123 , @xmath124 if @xmath125 , @xmath126 if @xmath127 and @xmath128 if @xmath129 .",
    "we now give details of the number of iterations , burn - in and updating schedules for all the adaptive independent metropolis - hastings schemes in the paper .",
    "in addition , we update the proposal in stage 1 if in 100 successive iterations the acceptance rate is lower than 0.01 .      *",
    "normal prior : end of first stage = 5 000 ; burn - in = 75 000 ; number of iterations : 100 000 ; updates = [ 50 , 100 , 150 , 200 , 300 , 500 , 700 , 1000 , 2000 , 5000 , 10000 , 20000 , 30000 , 50000 , 75000 ] . * double exponential prior : end of first stage = 5 000 ; burn - in = 100 000 ; number of iterations : 150 000 ; updates = [ 50 , 100 , 150 , 200 , 300 , 500 , 700 , 1000 , 2000 , 5000 , 10000 , 20000 , 30000 , 50000 , 75000 , 100000 ] . * mixture of normals prior : end of first stage = 100 000 ; burn - in = 300 000 ; number of iterations : 400 000 ; updates = [ 100 , 150 , 200 , 300 , 500 , 700 , 1000 , 2000 , 3000 , 5000 , 7500 , 10000 , 15000 , 20000 , 30000 , 50000 , 75000 , 100000 , 125000 , 150000 , 175000 , 200000 , 225000 , 250000 , 300000 ] .",
    "* normal and double exponential priors , quantiles 0.1 , 0.5 and 0.9 : end of first stage = 3 000 ; burn - in = 150 000 ; number of iterations : 200 000 ; updates = [ 100 , 150 , 200 , 300 , 500 , 700 , 1000 , 2000 , 3000 , 5000 , 7500 , 10000 , 15000 , 20000 , 30000 , 50000 , 75000 , 100000 , 150000 ] . * mixture of normals prior , quantiles 0.1 , 0.5 and 0.9 : end of first stage = 200 000 ; burn - in = 400 000 ; number of iterations : 500 000 ; updates = [ 100 , 150 , 200 , 300 , 500 , 700 , 1000 , 2000 , 3000 , 5000 , 7500 , 10000 , 15000 , 20000 , 30000 , 50000 , 75000 , 100000 , 125000 , 150000 , 175000 , 200000 , 225000 , 250000 , 275000 , 300000 , 325000 , 350000 , 375000 , 400000 ] .",
    "probit random effects model , pap smear data .",
    "for all three priors , end of first stage = 5 000 ; burn - in = 10 000 ; number of iterations : 20 000 ; updates = [ 20 , 50 , 100 , 150 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 2000 , 2500 , 3000 , 3500 , 4000 , 4500 , 5000 , 6000 , 7000 , 8000 , 9000 , 10000 , 12000 , 15000 ] . in this example",
    "the importance sampling density is updated every @xmath130 iterations .",
    "we now give the details of the sampling for both adaptive random walk metropolis algorithms .",
    "for the hmda data the number of burn - in iterations was 300 000 and the total number of iterations was 500 000 for all three priors .",
    "the corresponding numbers for the cps data with normal and double exponential priors are 500 000 and 1000 000 , and for the mixture of normals prior 1000 000 and 1500 000 .",
    "the corresponding numbers for the pap smear data are 30 000 and 50 000 .",
    "donald , s.g . ,",
    "green , d.a . and",
    "paarsch , h.j .",
    "differences in wage distributions between canada and the united states : an application of a flexible estimator of distribution functions in the presence of covariates .",
    "_ review of economic studies _ , 67(4 ) , 609 - 633 ."
  ],
  "abstract_text": [
    "<S> our article is concerned with adaptive sampling schemes for bayesian inference that update the proposal densities using previous iterates . </S>",
    "<S> we introduce a copula based proposal density which is made more efficient by combining it with antithetic variable sampling . </S>",
    "<S> we compare the copula based proposal to an adaptive proposal density based on a multivariate mixture of normals and an adaptive random walk metropolis proposal . </S>",
    "<S> we also introduce a refinement of the random walk proposal which performs better for multimodal target distributions . </S>",
    "<S> we compare the sampling schemes using challenging but realistic models and priors applied to real data examples . </S>",
    "<S> the results show that for the examples studied , the adaptive independent metropolis - hastings proposals are much more efficient than the adaptive random walk proposals and that in general the copula based proposal has the best acceptance rates and lowest inefficiencies .    </S>",
    "<S> * keywords * : antithetic variables ; clustering ; metropolis - hastings ; mixture of normals ; random effects . </S>"
  ]
}