{
  "article_text": [
    "ongoing projects and future projects in various disciplines like earth sciences , astronomy , climate variability , cancer research  ( e.g. coral , swot , wise , lsst , ska , jasd , aacr ) [ 1][2][3][4][5][6][7 ] are destined to produce the enormous catalogs which will be geographically distributed .",
    "as the amount of data available at various geographically distributed sources is increasing rapidly , traditional centralized techniques for performing data analytics are proving to be insufficient for handling this data avalanche [ 8 ] .",
    "downloading and processing all the data at a single location results in increased communication as well as infrastructural costs [ 9 ] .    bringing these massive data sets which are distributed geographically to a centralized site",
    "is almost impossible due to the limited bandwidth when compared with the size of the data . and",
    "also solving a problem with large number of dimensions at a central site is not practical as it is computationally expensive . analyzing these massive data",
    "can not be achieved unless the algorithms are capable of handling the decentralized data [ 8 ] .",
    "these data sets might be distributed in two different ways either horizontally or vertically [ 10 ] . in horizontal partition",
    "the number of attributes / dimensions are constant at all n different locations but the number of instances may vary . whereas in vertical partition the number of instances are constant at all n different locations but number of dimensions may vary . in this paper the data is partitioned in vertical manner .",
    "the analysis of these vertically partitioned geographically distributed data sets assume that the data should fit into main memory which is a challenge task in terms of scalability .",
    "estimation of covariance matrix analyses how the data is related among the dimensions .",
    "the task of estimating the covariance matrix of the data sets demand the data to be available at one centralized site [ 15 ] .    in this paper covariance matrix is estimated for vertically partitioned data in a decentralized manner without brining the data to a centralized site .",
    "the proposed distributed approach is compared with the centralized method by bringing the distributed data to one central site .",
    "the estimation of covariance matrix is achieved both in centralized and distributed approach .",
    "the experimental analysis shows how our distributed approach is better than the normal approach in terms of speed - up with exactly same solution .",
    "results are analyzed by considering various partitions of mfeat data set [ 18 ] .",
    "the rest of the organization of the paper is as follows .",
    "section 2 introduces the related work . in section 3 preliminaries and notations",
    "are briefly described . in section 4",
    "we present our distributed approach for distributed covariance matrix ( dcm ) and also discusses the speed - up of our approach when compared with centralized version . in section 5",
    "we present the experimental analysis of our algorithm . at the end in section 6",
    "the conclusions of the paper are mentioned .",
    "estimation of covariance based on divide and conquer approach is discussed by nik et.al in which the computational cost is reduced [ 11 ] .",
    "a regularization and blocking estimator of high dimensional covariance is discussed by et .",
    "al using barndorff nielson hansen estimator [ 12 ] .",
    "modified cholesky decomposition and other decomposition methods are discussed for the estimation of covariance by zheng hao for high dimensional data with limited sample size [ 13 ] .",
    "qi guo et .",
    "al proposed a divide conquer approach based on feature space decomposition for classification [ 14 ] .",
    "the significance of distributed estimation of parameters over centralized method is discussed and belief propagation algorithm is investigated by du jain [ 15 ] .",
    "l1-regularized gaussian maximum likelihood estimator ( mle ) is discussed by cho et.al in recovering a sparse inverse covariance matrix for high - dimensional data which statistically guarantees using a single machine [ 16 ] .",
    "al discussed the distributed approach for multi classification using svm without bringing data to a centralized site.[17 ]",
    "the statistical analysis of the data sets usually investigates the dimensions , to see if there is any relationship between them .",
    "covariance is themeasurement , to find out how much the dimensions vary from the mean with respect to each other .",
    "+ the covariance of two dimensions x , y can be compute as    @xmath0    where @xmath1 and @xmath2 are the mean of the dimensions x and y respectively .",
    "covariance is always computed between the two dimensions .",
    "if the data contains more than two dimensions , there is a requirement to calculate more than one covariance measurement .",
    "the standard way to get the covariance values between the different dimensions of the data set is to compute them all and put them in a matrix .",
    "the covariance matrix for a set of data with k dimensions is :    @xmath3    where @xmath4 is a matrix with @xmath5 rows and @xmath5 columns , and @xmath6 is the @xmath7 dimension . if we have an @xmath5-dimensional data set , then the matrix is a square matrix of @xmath5 dimensions and each value in the matrix is the computed covariance between two distinct dimensions .",
    "consider for an imaginary @xmath5 dimensional data set , using the dimensions @xmath8 , then , the covariance matrix has @xmath5 rows and @xmath5 columns , and the values are :    the _ covariance matrix _",
    "@xmath9 is an @xmath10  matrix which can be written as follows .",
    "@xmath11    along the main diagonal , the covariance value is between one of the dimensions and itself .",
    "these are nothing but the variances for that dimension .",
    "the other point is that since @xmath12 the matrix is symmetrical about the main diagonal .",
    "the data be distributed among t sites with equal number of instances but varied in number of dimensions i.e. vertically partitioned data .    1 .",
    "let the data is distributed among t sites and the sites are labeled as @xmath13 .",
    "+ @xmath14_{l \\times m } = ( x_0 , x_1 , x_2 , ......... x_{t-1})\\ ] ] + where data @xmath15 is a @xmath16 matrix residing at the site @xmath17 and @xmath18 2 .",
    "calculate the local covariances @xmath19 at all t sites parallely .",
    "3 .   if the number of sites are only 2 , either send the corresponding data from @xmath20 to @xmath21 or from @xmath21 to @xmath22 and calculate the cross covariances .",
    "if the number of sites are more than 2 , calculate the cross covariances @xmath23 by sending the corresponding data @xmath15 of @xmath17 to the site @xmath24 as follows . * if the number of sites are even , @xmath25 * * for @xmath26 to @xmath27 * * * @xmath28 = immediate @xmath29 predecessor sites * * for @xmath30 to @xmath31 * * * @xmath28 = immediate @xmath32 predecessor sites * if the number of sites are odd , @xmath33 * * for @xmath34 to @xmath35 * * * @xmath28 = immediate @xmath32 predecessor sites 5 .",
    "merge the local and cross covariances to get the global covariance matrix .",
    "6 .   estimate the eigen components of the global covariance matrix .",
    "* input : * data @xmath15 of all the sites @xmath17 + * output : * eigen vectors    compute @xmath36 mean of all columns of @xmath15 data    compute the covariance matrix @xmath37 where , @xmath38 is the mean of the @xmath39 and @xmath40 column of the the @xmath15 matrix .",
    "send @xmath41 of @xmath20 to @xmath42 and calculate the cross covariances @xmath43",
    "send @xmath15 of @xmath44 to @xmath45 as follows p = k j= * predecessor(p ) * print(j ) p = j p = k j=*predecessor(p ) * print(j ) p = j p = k j=*predecessor(p ) * print(j ) p = j compute the cross covariances @xmath46 merge the local and cross covariances to make the global covariance matrix @xmath47 estimate the eigen vectors @xmath48 where , @xmath49 is the eigen vector of eigen value @xmath50 and @xmath51 is the identity matrix of the same order of @xmath52    * input : * node @xmath5 + * output : * predecessor node    return@xmath53 return@xmath54    the architecture of the proposed approach is shown in figure 1 , where the global covariance matrix is computed by merging the local and cross covariances .",
    "let us consider 3 nodes @xmath55 , @xmath56 , @xmath57 .",
    "the node @xmath55 consists of two columns labeled by x , y .",
    "the node @xmath56 also consists of two columns labeled by z , w .",
    "the node @xmath57 consists of single column labeled by v. the covariance matrix by centralized approach would be ( considering only upper triangular matrix as covariance is symmetric ) :    @xmath58      local covariance of @xmath55 , say @xmath59    @xmath60 local covariance of @xmath56 , say @xmath61    @xmath62 local covariance of @xmath57 , say @xmath63 @xmath64 cross covariance of @xmath55 and @xmath56 , say @xmath65 @xmath66 cross covariance of @xmath56 and @xmath57 , say @xmath67 @xmath68 cross covariance of @xmath55 and @xmath57 , say @xmath69 @xmath70 global covariance matrix by merging the local and cross covariances as given below would be equivalent to the matrix calculated by centralized approach . @xmath71",
    "the data is communicated among the sites in such a manner so that the resources are used in an efficient way .",
    "the computational load is also balanced among the sites to have the good speed - up .",
    "when the number of sites are even i.e. @xmath72 , the first @xmath73 sites will receive the data from their immediate @xmath74 predecessors .",
    "then the remaining @xmath73 sites will receive the data from their immediate @xmath73 predecessors .",
    "sharing of this data by communicating among the sites is illustrated in figure 2 , when the number of sites say t = 4 . here",
    "the value of r = 2 .",
    "so the first 2 sites @xmath20 and @xmath21 will receive the data from its immediate @xmath74 predecessors i.e. @xmath20 will receive data from @xmath75 and @xmath21 will receive data from @xmath20 .",
    "the next 2 sites @xmath76 and @xmath75 will receive the data from its immediate @xmath73 predecessors i.e. @xmath76 will receive the data from @xmath21 , @xmath20 and @xmath21 will receive the data from @xmath20 , @xmath77 .    when the number of sites are odd , all the @xmath78 sites will receive the data from their immediate @xmath73 predecessors .",
    "sharing of this data by communicating among the sites is illustrated in figure 3 , when the number of sites say t = 5 . here",
    "the value of r = 2 .",
    "so all the 5 sites from @xmath20 to @xmath77 will receive the data from its immediate @xmath73 predecessors i.e. @xmath20 will receive the data from @xmath77 and @xmath75 , @xmath21 will receive the data from @xmath20 and @xmath77 , @xmath76 will receive the data from @xmath21 and @xmath20 , @xmath75 will receive data from @xmath76 and @xmath21",
    ". therefore the number of transfers of the sites of the data will be at most r in all the cases .",
    "it is not required that all the sites should have the data of remaining sites .",
    "in centralized version let the data is available in a single matrix @xmath14_{l \\times m } = ( x_0 , x_1 , x_2 , ......... x_{t-1})\\ ] ]    where data @xmath15 is a @xmath16 matrix residing at the site @xmath17 and @xmath18 let the computational time of centralized approach is denoted as @xmath79    @xmath80      as the data is distributed among t sites and the sites are labeled as @xmath13 .",
    "@xmath14_{l \\times m } = ( x_0 , x_1 , x_2 , ......... x_{t-1})\\ ] ] let computational time of global / distributed covariance matrix is denoted as @xmath81 , computational time of ( local covariances ) as @xmath82 , computational time of ( cross covariances ) as @xmath83 and the communication cost as @xmath84    @xmath85    @xmath86    where",
    "i is the predecessors of k as explained in the 4th step of section iv.a      let us denote the speed - up by s @xmath87",
    "@xmath88    consider the t sites with each of @xmath89 columns of data .",
    "@xmath90    @xmath91    @xmath92    @xmath93    @xmath94    * case1 : * @xmath25 ( even )    @xmath95    @xmath96    * case2 : * @xmath97 ( odd )    @xmath98    @xmath99    in both the cases speed - up will be at least @xmath32 times .",
    "site to @xmath100 site .",
    "]     site to @xmath100 site . ]",
    "we implemented the algorithm with the data set mfeat , taken from uci machine learning repository https://archive.ics.uci.edu/ml/datasets.html .",
    "mfeat data consists of 2000 rows and are distributed in six data files as follows [ 18 ] :    1 .",
    "mfeat - fac : 216 profile correlations ; 2 .",
    "mfeat - fou : 76 fourier coefficients of the character ; 3 .",
    "mfeat - kar : 64 karhunen  love coefficients ; 4 .",
    "mfeat - mor : 6 morphological features ; 5 .",
    "mfeat - pix : 240 pixel averages in 2 x 3 windows ; 6 .",
    "mfeat - zer : 47 zernike moments .",
    "the algorithm is implemented using java agent development framework ( jade ) [ 19 ] .",
    "each site data is downloaded to a node which are connected over the network .",
    "so the number of computational nodes is equal to the number of sites .",
    "the communication is established among them using jade to transfer the data .    in our analysis",
    "the vertical partitions are considered from 2 to 6 which is shown in table 1 .",
    "the computational time of local and cross covariances are shown in table ii- table vi for the partitions 6 , 5 , 4 , 3 , 2 respectively .",
    "the cross covariances are chosen as explained in section iv.a , step 4 . in table vii",
    "the communication cost for a given site for sending its predecessors data is shown . in table viii , the computational time of centralized and distributed approaches",
    "are compared .",
    "the computational time of distributed approach is calculated from table ii - table vi and from table vii for various partitions as explained in section iv.c.2 . in our analysis dcm",
    "is compared with centralized approach , the result is exactly same as shown in fig 4 . because we are not losing any data but getting the distributed covariance matrix by merging the local and cross covariances .",
    "the speed - up is shown in fig 5 .",
    "it is observed that the speed - up is increasing with the number of partitions hence scalable .",
    "this is because of increase in parallel computations along with the number of partitions .",
    "there is an elevation in speed up when the number of partitions are @xmath101 5 which promises that it works well even with number of partitions are increasing .    [ cols=\"^,^,^,^,<\",options=\"header \" , ]",
    "we propose an algorithm dcm which estimates the global covariance matrix by merging the local and cross covariances that are distributed at different nodes / sites .",
    "experimental results show that the result of dcm is exactly same as the centralized approach with good speed - up .",
    "the final output of dcm is same as centralized approach because we are not losing any data .",
    "the computational time of dcm is decreasing along the increased number of partitions .",
    "dcm is also capable of handling large data sets based on parallel calculations of vertical partitions , hence scalable .",
    "the speed - up can be further increased by making number of columns equal at every node / site and also computing the cross covariances parallely within the node / site .",
    "we are thankful for the support provided by the department of csis , bits - pilani , k.k .",
    "birla goa campus to carry out the experimental analysis and also to sreejith.v , bits - pilani , k.k.birla goa campus for useful discussions .",
    "kanishka bhaduri , kamalika das , kirk borne et al _ scalable , asynchronous , distributed eigen - monitoring of astronomy data streams _",
    "1em plus 0.5em minus 0.4emproceedings of the 2009 siam international conference on data mining .",
    "pp 247 - 258 .",
    "m weske , m shacid , c godart(eds ) _ data in astronomy : from the pipeline to the virtual observatory 1em plus 0.5em minus 0.4emwise 2007 workshops , lncs 4832 , pp 52 - 62 .",
    "_ h dutta , c giannella , k borne et al . _",
    "distributed top - k outlier detection from astronomy catalogs using the demac system _ 1em plus 0.5em minus 0.4emin proceedings of sdm07 , 2007 , pp 473 - 478 .",
    "cj hsieh , is dhillon , p ravikumar , a banerjee _ _ a divide - and - conquer procedure for sparse inverse covariance estimation__1em plus 0.5em minus 0.4emadvances in neural information processing systems 25 , 2012 pages 2330 - 2338 .",
    "nikolaus hautsch1 , lada m. kyj and roel c. a. oomen _ a blocking and regularization approach to high - dimensional realized covariance estimation_.1em plus 0.5em minus 0.4emjournal of applied econometrics volume 27 , issue 4 , pages 625645 , june / july 2012 .",
    "hsieh , cho - jui and sustik , matyas a and dhillon , inderjit s and ravikumar , pradeep k and poldrack , russell _ sparse inverse covariance estimation for a million variables _ 1em plus 0.5em minus 0.4emadvances in neural information processing systems 26 , 2013 .",
    "aruna govada , bhavul gauri , sanjay.k.sahay _ distributed multi - class svm for large data sets _",
    "1em plus 0.5em minus 0.4emproceedings of the third international symposium on women in computing and informatics cochi , india august 10 - 13,2015 , pages 54 - 58 published by acm ."
  ],
  "abstract_text": [
    "<S> the major sources of abundant data is constantly expanding with the available data collection methodologies in various applications - medical , insurance , scientific , bio - informatics and business . </S>",
    "<S> these data sets may be distributed geographically , rich in size and as well as dimensions also . to analyze these data sets to find out the hidden patterns , it is required to download the data to a centralized site which is a challenging task in terms of the limited bandwidth available and computationally also expensive . </S>",
    "<S> the covariance matrix is one of the method to estimate the relation between any two dimensions . in this paper </S>",
    "<S> we propose a communication efficient algorithm to estimate the covariance matrix in a distributed manner . </S>",
    "<S> the global covariance matrix is computed by merging the local covariance matrices using a distributed approach . </S>",
    "<S> the results show that it is exactly same as centralized method with good speed - up in terms of computation . </S>",
    "<S> the reason for speed - up is because of the parallel construction of local covariances and distributing the cross covariances among the nodes so that the load is balanced . </S>",
    "<S> the results are analyzed by considering mfeat data set on the various partitions which addresses the scalability also .    </S>",
    "<S> * keyword * : _ parallel / distributed computing , covariance matrix , vertical partition _ </S>"
  ]
}