{
  "article_text": [
    "identifying the links between the variables in multivariate datasets is a fundamental and recurrent problem in many engineering applications . to this end",
    ", the use of graphs especially in neuroimaging applications has become very popular because they allow to study and represent the interactions between variables in a concise manner @xcite .    a particular class of graphs , called _ graphical models _",
    ", encodes information about dependence between the variables conditioned on all the other variables , or _",
    "conditional dependence _ @xcite . for _ static _ models this information",
    "is contained in the inverse of the covariance matrix also known as the _ precision _ matrix , and can be estimated by solving the covariance selection problem @xcite .",
    "additionally , sparsity and/or low - rank structural constraints can be imposed to the precision matrix estimation .",
    "the sparsity constraint results from the parsimony principle in model fitting , i.e. , one assumes few direct interactions between the variables , and is obtained through @xmath0-norm regularizers @xcite . the low - rank structure , induced through nuclear - norm regularizers , models the presence of _ latent variables _ that are not observed but generate a common behavior in all the observed variables @xcite .",
    "the low - rank modeling is inspired from what is done in classical component analysis techniques , and leads to models that are simpler and more interpretable @xcite .",
    "an example of graphical model is given in fig .",
    "1    $ ] with few interactions among them ( sparsity ) and one latent variable @xmath1 ( low - rank structure).,scaledwidth=50.0% ]    in _ dynamical _ models , the additional information of the ordering of the data is taken into account and datasets are seen as time series .",
    "a widely used class of models encoding this information are autoregressive ( ar ) models which are characterized by their power spectral density , the dynamic equivalent of the covariance matrix @xcite . as in the static case",
    ", it has been shown that a zero in the inverse power spectral density corresponds to conditional independence between two variables @xcite . in the dynamic case the ( inverse )",
    "power spectral density is encoded in a block toeplitz matrix .",
    "because of this particular structure the classical @xmath0-norm can not be used to induce sparsity in the inverse power spectral density .",
    "this problem is solved by introducing an alternate regularization proposed in @xcite .",
    "finally , @xcite presents a unifying framework allowing sparse plus low - rank identification of inverse power spectral densities in multivariate time series .    in this paper",
    "we adapt the problem formulated in @xcite to the alternating direction method of multipliers ( admm ) framework of @xcite in order to scale it with larger datasets for which the cvx matlab toolbox of @xcite is computationally expensive .",
    "in particular , we exploit separability of constraints of the admm framework to decouple the sparsity and the low - rank constraints .",
    "the first update is a projective gradient update similar to the one proposed in @xcite and the second update is a well known projection onto the cone of positive semidefinite matrices . in the numerical examples we show the efficacy of our proposed algorithm on a real neuroimaging dataset .",
    "we also provide further insight into the information encoded in the low - rank structure of our model by applying the proposed algorithm to datasets with different spatio - temporal structures , which is shown to be at least partially recovered in the latent variables .",
    "the paper is organized as follows .",
    "we present the optimization problem leading to this sparse plus low - rank decomposition in section ii and we explain how we use admm to efficiently solve it in section iii .",
    "we then show the results of our approach on synthetic and real data in section iv and conclude .",
    "we first introduce some basic notions , explain the motivation of the sparse plus low - rank ( s+l ) graphical models and then formally deduce the corresponding optimization problem .",
    "finally , we define _ latent components _ that we use in the numerical examples in order to characterize information encoded in the low - rank part of this decomposition .",
    "consider a @xmath2-dimensional autoregressive ( ar ) gaussian process @xmath3^t$ ] of order @xmath4 @xmath5 where @xmath6 , @xmath7 , @xmath8 and @xmath9 is white gaussian noise with covariance matrix @xmath10 .",
    "@xmath11 is completely characterized by its spectral density @xmath12 which encodes information about dependence relations between the @xmath2 variables @xcite . on the contrary",
    "the _ inverse _ power spectral density @xmath13 encodes _ conditional _ dependence relations between variables @xcite .",
    "that is , two variables @xmath14 and @xmath15 are independent , conditionally on the other @xmath16 variables of @xmath11 over @xmath17 , if and only if @xmath18 , \\label{eq : sparsity}\\end{aligned}\\ ] ] the nodes of the corresponding graphical model are the @xmath2 variables of @xmath11 and there is no edge between the two nodes @xmath14 and @xmath15 if and only if ( [ eq : sparsity ] ) is satisfied .",
    "assume that @xmath19^t$ ] where @xmath20^t \\in \\mathbb{r}^m$ ] contains _ manifest _ variables , that is variables accessible to observations , and @xmath21^t \\in \\mathbb{r}^l$ ] contains _ latent _ variables , not accessible to observations .",
    "the power spectrum of @xmath11 can be expressed using the following block decomposition    @xmath22    where the @xmath23 denotes the conjugate transpose operation .    in order to better characterize the conditional dependence relations between the manifest variables , from ( [ eq : block ] )",
    "we obtain the following decomposition of @xmath24 using the schur complement @xcite @xmath25    the main modeling assumption here is that @xmath26 and that the conditional dependencies relations among the @xmath27 manifest variables encoded in @xmath24 can be largely explained through few latent variables .",
    "the corresponding graphical model has few edges between the manifest variables and few latent nodes , as in fig .",
    "1 . this leads to a s+l structure for @xmath24 following ( [ eq : decsl ] ) : @xmath28 , where @xmath29 is sparse because it encodes conditional dependence relations between the manifest variables and @xmath30 is low rank .",
    "since @xmath11 is an ar process of order @xmath4 , we can assume that @xmath10 and @xmath31 belong to the family of matrix pseudo - polynomials @xmath32    following @xcite we further rewrite @xmath10 and @xmath31 as @xmath33 where @xmath34 is a shift operator @xmath35 $ ] and @xmath36 and @xmath37 are now matrices belonging to @xmath38 which is the set of symmetric matrices of size @xmath39 .    finally , @xmath40 is the vector space of matrices @xmath41 $ ] with @xmath42 and @xmath43 .",
    "the linear mapping @xmath44 outputs a symmetric block toeplitz matrix from the blocks of @xmath45 as @xmath46    the adjoint operator of @xmath47 is the linear mapping @xmath48 defined for a matrix @xmath49 partitioned in square blocks of size @xmath50 as @xmath51    following this partition , @xmath52 is given by @xmath53      assume that we have a finite length realization @xmath54 of the manifest process @xmath55 .",
    "it should be emphasized that no data is available regarding the latent process @xmath56 , and its dimension is not even known .",
    "our goal is to recover the s+l model defined in the previous section which best explains the collected data .",
    "therefore , one estimate of @xmath36 and @xmath37 ( hence of @xmath10 and @xmath31 ) is given by solving the regularized maximum entropy problem @xcite for which the primal is @xmath57 where    * @xmath58 and @xmath59 are weighting parameters leading to a sparse @xmath10 and a low - rank @xmath31 , * @xmath60 where @xmath61 are the @xmath62 first sample covariance lags @xmath63 @xcite , * @xmath64 is the inner product associated with @xmath65 , * @xmath66 is the following function chosen to encourage a structured sparse solution @xmath67    @xmath68 which is convex but non smooth .",
    "this limitation was contoured in similar works @xcite by solving the corresponding dual problem . in the present case , it can be shown based on @xcite and @xcite that the dual of ( [ primal ] ) is    @xmath69    where @xmath70 is defined as @xmath71 and @xmath72 is partitioned as in ( [ eqx ] ) .      the optimal primal variables ( @xmath73,@xmath74 ) are recovered from the optimal solution @xmath75 following @xcite from which @xmath76 and @xmath77 are computed by ( [ eq : sl ] ) .",
    "@xmath77 is a square matrix function of size @xmath27 , defined over the unit circle and for which we consider the pointwise singular value decomposition since @xmath77 is low - rank for all @xmath78 $ ] @xmath79    the @xmath80-th column of @xmath81 contains the strength of the conditional dependence relation between the @xmath80-th unobserved latent variable and each of the @xmath27 manifest variables .",
    "following component analysis nomenclature @xcite we call the @xmath80-th column of @xmath81 the @xmath80-th _ latent component _ and denote it @xmath82 . in other words , @xmath83 with @xmath84 $ ] and @xmath85 $ ] represents the weight of the conditional dependence between the latent variable @xmath86 and the manifest variable @xmath87 in the expression of @xmath87 . in the static case , @xmath82 reduces to a constant vector because @xmath88 in ( [ eq : sl ] ) and @xmath31 does not depend on @xmath89 . on the contrary , in the dynamic case",
    "each latent component is a function of @xmath78 $ ] .",
    "we use the alternating direction method of multipliers ( admm ) of @xcite to solve ( [ dual ] ) .",
    "this choice is motivated by the fact that this algorithm both inherits the strong convergence properties of the method of multipliers and exploits decomposability of the dual problem formulation leading to efficient partial updates of the variables .",
    "we show how we rewrite ( [ dual ] ) in the admm format by separating the sparse and low - rank constraints , then explain how we choose an adequate stopping criterion and recover the primal variables .    in order to decouple the constraints related to sparsity and low - rank we introduce the new variable @xmath90 and reformulate ( [ dual ] ) as @xmath91 where the first constraint ( @xmath92 ) gathers the first two constraints on @xmath93 of ( [ dual ] ) and @xmath94 and @xmath95",
    "are defined accordingly ; and ( @xmath96 ) is the last constraint of ( [ dual ] ) imposing positive semidefiniteness of the new variable @xmath97 . using the _ augmented lagrangian _ formulation ,",
    "we introduce @xmath98 defined by @xmath99 where @xmath100 is the frobenius norm .",
    "subsequently , the admm updates are @xmath101    it should be noted that ( s1 ) has no closed form solution and corresponds to the sparsity set of constraints of @xcite .",
    "we approximate the solution by a projective gradient step as in @xcite . following this approach , @xmath102 where    * @xmath103 * @xmath104 * @xmath105 is found from the armijo conditions , * @xmath106 is the projection onto @xmath92 which reduces to a projection onto the @xmath107-norm ball @xcite .",
    "the optimization problem ( s2 ) has a closed form solution and is computed as @xmath108 where @xmath109 is the projection onto the cone of symmetric positive semidefinite matrices of size @xmath39 , which is done by selecting the eigenvectors corresponding to _ positive _ eigenvalues .",
    "this leads to the final updates of the admm algorithm .",
    "+    ' '' ''    * admm for sparse plus low - rank inverse power spectral density estimation . * initialize @xmath110 , @xmath111 , @xmath112 ; set @xmath113 ; and successively update variables as follows : + @xmath114    ' '' ''    following @xcite , a stopping criterion for ( [ admm_update ] ) is based on the primal and dual _ residuals _ @xmath115 and @xmath116 that respectively measure satisfaction of the equality constraint of ( [ opt ] ) and the distance between two successive iterates of the additional variable @xmath97 . @xmath115 and @xmath116 should satisfy @xmath117 and @xmath118 where @xmath119 and @xmath120 are defined as @xmath121    here @xmath122 and @xmath123 are the predefined absolute and relative tolerances for the problem .",
    "a variation is obtained when @xmath124 is multiplied by a factor of @xmath125 at each iteration up to a maximum value @xmath126 starting from a value @xmath127 depending on the application .",
    "convergence analysis of the admm algorithm follows from ( * ? ? ?",
    "* section 3.2)@xcite .",
    "the computational cost per iteration of the updates ( [ admm_update ] ) depends on the projections onto @xmath92 and @xmath96 , the gradient evaluation of @xmath98 , and the linear mappings @xmath47 and @xmath128 leading to a final complexity @xmath129 .",
    "in this section we apply the proposed admm algorithm to solve the sparse plus low - rank decomposition on synthetic and real datasets and explore the type of information encoded in the identified latent _",
    "components_. the matlab code for the algorithm is available from the webpage http://www.montefiore.ulg.ac.be/~rliegeois/      this synthetic dataset consists of time series corresponding to a first order ar model ( dynamic model , @xmath130 ) with the interaction graph presented in fig .",
    "[ interaction ] .",
    "the interaction graphs of the manifest variables ( support of @xmath10 ) identified for different values of @xmath131 and @xmath132 are represented in fig .",
    "[ ex1 ] as well as @xmath133 , the number of latent components ( rank of @xmath31 ) that were identified .",
    "in order to discriminate between models we compute a score function @xmath134 , defined in @xcite , taking into account fitting to the data and complexity of the model .",
    "and @xmath132 using a first order ar model ( @xmath130 ) .",
    "( b ) true interaction graph .",
    "( c ) optimal interaction graph of estimated model obtained in the static case ( @xmath135).,scaledwidth=50.0% ]    as expected , higher values of @xmath131 promote models with less latent components , and higher values of @xmath132 favor models with few interactions between the manifest variables .",
    "the model with the best ( lowest ) score function recovers the true interaction graph with the correct number of latent components ( fig . [",
    "the optimal interaction graph in the static case ( fig .",
    "[ ex1]c ) , on the contrary , does not recover exactly the true interaction graph .    the stopping criterion based on the primal and dual residuals",
    "is illustrated in fig .",
    "[ conv ] using this dataset .",
    "the algorithm stops when @xmath136 and @xmath137 are both satisfied .",
    ", @xmath138 , @xmath139 , @xmath140 , and @xmath141.,scaledwidth=50.0% ]      the interest in considering a non - linear generative model is that it can produce endogenous sustained oscillations in networks similarly to what is observed in neuroimaging data such as fmri time series .",
    "a popular non - linear model is the hopfield model widely used in neural networks @xcite : @xmath142        where @xmath143 denotes the level of activity of the variable @xmath80 , @xmath144 is the strength of the connection from @xmath145 to @xmath143 , and @xmath146 is a sigmodal saturation function .",
    "[ model ] shows how we generate oscillations in two clusters . in the first case ( fig .",
    "[ model]b ) the oscillations are coupled , leading to dephased oscillations of the same frequency whereas in the second case the clusters are decoupled ( fig .",
    "[ model]c ) , leading to oscillations of different frequencies in the two clusters .",
    "we finally generate three different datasets for each configuration by sampling these time series at different frequencies . the first dataset is produced by using the original time series ( no sampling ) , the second and third datasets are obtained by sampling the time series with a period of t@xmath147 and t@xmath148 to generate synthetic data with higher frequency content .",
    "[ same_f ] shows the static and dynamic latent components as defined in section [ lat_comp ] that are identified in the optimal models from the synchronous oscillations datasets of fig .",
    "[ model]b .",
    "since the results are very similar within the nodes of each cluster and for clarity purposes we plot only the value of the latent components in node @xmath147 ( @xmath149 ) and in node @xmath150 ( @xmath151 ) .",
    "b).,scaledwidth=50.0% ]    in the static case two latent components corresponding to the two clusters of the generative model are identified .",
    "there is no significant difference for the three input datasets suggesting that the frequency content of original data is not encoded in the static latent components . in the dynamic case ( @xmath130 ) , the latent components are encoded in @xmath83 and a single latent component",
    "is identified in the optimal model .",
    "interestingly , the t2-sampled synthetic data ( high - frequency time series ) leads to a latent component showing a strong high - frequency content whereas the original dataset leads to a latent component with dominant low - frequency content .",
    "c).,scaledwidth=50.0% ]    having the same approach on asynchronous oscillations , we get the results shown in fig . [ diff_f ] . in the static case",
    "we still obtain two latent components recovering the two clusters with no distinction between the three starting datasets . in the dynamic case @xmath152 , however , the optimal model now has two latent components , each one capturing the oscillations in one of the two clusters .",
    "as in the previous case the frequency content of the synthetic data is encoded in the frequency content of the latent component .",
    "identifying two latent components probably comes from the fact that the frequency of oscillation in the two clusters are different , suggesting that the dynamic latent component identifies a _ spatio - temporal _ subspace of variation common to different manifest variables .",
    "we illustrate the application of our proposed algorithm on real neuroimaging data consisting of functional magnetic resonance imaging time series in 90 brain regions collected on 17 patients during rest @xcite . it should be noted that this problem dimension is not tractable with a standard optimization tool such as the cvx toolbox of @xcite .",
    "the classical approach is to use component analysis to extract neuronal networks . during rest",
    ", three networks are robustly identified using component analysis : the visual network ( vn ) , the default mode network ( dmn ) and the executive control network ( ecn ) that are represented in fig [ 3net ] .        by fitting a first order ar model ( @xmath130 ) to this dataset we identify latent components corresponding to these networks . in fig .",
    "[ dyn_net ] we plot only these components for @xmath153 , a low - frequency contribution called @xmath154 from the definition of section [ lat_comp ] , and @xmath155 , a high - frequency contribution called @xmath156 .",
    "indeed , for a first order model the latent components are characterized by these two extreme values @xmath157 and @xmath158 .        in 14 subjects out of 17",
    "we observe that vn is recovered in one latent component , with no significant differences between @xmath157 and @xmath158 . on the other hand , in 12 subjects the dmn and ecn networks",
    "are gathered in a unique latent variable , the dmn corresponding to @xmath157 and ecn to @xmath158 .",
    "this interestingly echoes some recent results suggesting that the dmn and the ecn are both consciousness related processes and are anti - correlated , oscillating at similar frequencies @xcite .",
    "the contribution of our work is twofold .",
    "first , we reformulated the sparse plus low - rank autoregressive identification problem into the admm framework in order to scale it to larger datasets encountered in neuroimaging applications .",
    "second , we presented a deeper exploration of the type of information encoded in the latent components identified in the low - rank part of the decomposition .",
    "[ same_f ] & [ diff_f ] suggest that this information is richer in dynamic models than in static models in the sense that dynamic latent components recover _ spatio - temporal _ properties of the original time series such as common spectral content .",
    "applied to a neuroimaging dataset , this interpretation led to a novel characterization of the dynamical interplay between two neuronal networks mediating consciousness , echoing recent experimental results .",
    "as a future research direction we intend to explore whether additional information can be recovered in higher order dynamical latent components ( @xmath159 ) .",
    "it would also be interesting to study the mathematical link between the latent components of the proposed framework and the widely used static principal components that also capture a ` common subspace ' of variation of many variables from the covariance matrix instead of the precision matrix .",
    "finally , in addition to using separability into sparse and low - rank constraints , it could be beneficial to exploit these structures in the algorithm updates in order to scale it to even larger problems .",
    "banerjee , o. , el ghaoui , l. , and daspremont , a. , 2008 .",
    "model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data .",
    "_ j. mach .",
    "_ , vol . 9 , pp . 485 - 516 .",
    "boyd , s. , parikh , n. , chu , e. , peleato , b. , and eckstein , j. , 2010 . distributed optimization and statistical learning via the alternating direction method of multipliers .",
    "1 , pp . 1 - 122 .",
    "vanhaudenhuyse , a. , demertzi , a. , schabus , m. , noirhomme , q. , bredart , s. , boly , m. , phillips , c. , soddu , a. , moonen , g. , and laureys , s. , 2011 .",
    "two distinct neuronal networks mediate the awareness of environment and of self .",
    "_ j. cogn .",
    "23 , pp . 570 - 8 ."
  ],
  "abstract_text": [
    "<S> this paper considers the problem of identifying multivariate autoregressive ( ar ) sparse plus low - rank graphical models . based on the corresponding problem formulation recently presented , we use the alternating direction method of multipliers ( admm ) to efficiently solve it and scale it to sizes encountered in neuroimaging applications . </S>",
    "<S> we apply this decomposition on synthetic and real neuroimaging datasets with a specific focus on the information encoded in the low - rank structure of our model . in particular , we illustrate that this information captures the _ spatio - temporal structure _ of the original data , generalizing classical component analysis approaches . </S>"
  ]
}