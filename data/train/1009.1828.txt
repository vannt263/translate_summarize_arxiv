{
  "article_text": [
    "interpreting the patterns of activity fired by populations of neurons is one of the central challenges of modern systems neuroscience .",
    "the design of decoding algorithms capable of millisecond - by - millisecond readout of sensory or behavioural correlates of neuronal activity patterns would be a valuable step in this direction .",
    "such decoding algorithms , as well as helping us to understand the neural code , may have further practical application , as the basis of communication neural prostheses for severely disabled patients such as those with `` locked in '' syndrome .    at",
    "the heart of such a decoding algorithm must lie - whether explicit or implicit - a description of the conditional probability distribution of activity patterns given stimuli or behaviours .",
    "making this description is nontrivial , as the brain , like other biological systems , exhibits enormous complexity .",
    "this results in a very large number of possible states or configurations exhibited by the system , making the description of such systems by simply measuring the probabilities of each state unfeasible . except for very small patterns ,",
    "a model - based approach of some kind is essential .",
    "new technologies in neuroscience such as high - density multi - electrode array recording and multi - photon calcium imaging now make it possible to monitor the activity of large numbers of neurons simultaneously .",
    "analysis tools for such high dimensional data have however lagged behind the experimental technology , as most approaches are limited to very small population sizes . while considerable advances have been made in the use of information - theoretic approaches to characterise the statistical structure of small neural ensembles  @xcite ,",
    "finite sampling limitations have made results for larger ensembles much more difficult to obtain .",
    "for the statistical description of multivariate neural spike train data , parametric models able to capture most of the interesting features of real data while still being of empirically accessible dimensionality are highly desirable .",
    "one promising approach has emerged from statistical mechanics : the use of ising ( or ising - like ) models , exploiting an analogy between populations of spike trains and ensembles of interacting magnetic spins @xcite .",
    "our aim here is to devise an algorithm for  millisecond - by - millisecond \" neural decoding , on the basis that information processing in the nervous system appears to make use of such fine temporal scales @xcite .",
    "the timescale of the  symbols \" used in information processing is thus likely to be somewhere between 1 and 20 ms for most purposes @xcite . for time bins on this scale",
    ", neural spike trains are effectively binarized , and the simplest binary model ( in the maximum entropy sense ) that captures pairwise correlations is the ising model .",
    "the ising model is thus a natural way to describe the statistics of neural spike patterns at the timescale of interest .",
    "fitting of such a model to the observed neural data has the advantage that it does not implicitly assume some non - measured structure in the data , i.e. maximum entropy models express the most uncertainty about the modelled data given the ( explicit ) chosen constraints ( e.g. that certain moments of the measured distribution agree with the model distribution ) @xcite .",
    "it can be shown that this is mathematically equivalent to maximizing the likelihood of the model parameters to explain the observed data @xcite . by using this approach to fit a model to the conditional activity pattern distribution , in conjunction with maximum a posteriori decoding @xcite , it is possible to train a decoder which takes as its input a pattern of spiking activity , and gives as its output the stimulus that it determines to have elicited that spike pattern .",
    "a major obstacle to the use of ising models for neural decoding , is that , in general , it is necessary to compute a partition function ( or normalization factor ) , involving a sum over all possible states .",
    "this can be numerically challenging , and for large numbers of neurons , unfeasible . in the present study",
    ", we adopted several approaches for circumventing this problem .",
    "firstly , we make use of mean field approximations , including both the ` naive ' mean field approximation and the ( tap ) extension to it , following .",
    "secondly , we compare this with the recently proposed minimum probability flow method @xcite for learning model parameters .",
    "to assess the relative performance of these approaches in the context of a discrete decoding problem , we simulated the activity of a population of neurons in layer v of the mouse visual cortex during an experiment in which a discrete set of orientation stimuli were presented . using this simulation",
    ", we evaluated the relative performance characteristics of the different decoding algorithms in the face of limited data , exploring decoding regimes with up to 1000 neurons .",
    "we demonstrate , for the first time , the use of the ising model to effectively decode discrete stimulus states from large - scale simulated neural population activity .",
    "activity states in an ising model are boltzmann distributed , i.e. they are distributed according to the negative exponential of the `` energy '' associated with each state .",
    "this distribution , + @xmath0 , is the maximum entropy distribution subject to the set of constraints imposed by lagrange multiplers @xmath1 on variables @xmath2 .",
    "imposing these constraints upon firing rates and pairwise correlations gives @xmath3 where @xmath4 and each binary response variable @xmath5 indicates the firing / not firing of neuron @xmath6 in the observed time interval .",
    "the parameters @xmath7 ( known in statistical physics as ` external fields ' ) and @xmath8 ( ` pairwise couplings ' ) have to be fit to the data such that the model displays the same means and pairwise correlations as the data :    @xmath9    where @xmath10 denotes expectation with respect to the specified distribution .",
    "@xmath11 is the partition function , which acts as a normalisation factor .",
    "i.e. : @xmath12 note that the first sum is over all _ possible _ ( as opposed to observed ) responses , given by the set @xmath13 .    in statistical physics",
    "it is more common to use a symmetric representation @xmath14 for the ` spins ' that describe the activation of neuron @xmath6 ( with @xmath15 indicating ` no spike ' and @xmath16 indicating ` spike ' ) , which simply corresponds to a change of variables @xmath17 .",
    "accordingly the fields , couplings and partition functions change . as it is occasionally more convenient to work in one or the other representation we will denote the fields and couplings in the spin representation with @xmath18 and @xmath19 .    standard monte carlo techniques for fitting these model parameters , such as boltzmann learning , which can in principle provide an exact solution - given the number of samples is high enough - become computationally very expensive if not intractable as the number of cells increases .",
    "we have found in previous work @xcite , that the boltzmann learning approach becomes computationally too expensive in our case for ensemble sizes larger than 30 cells .",
    "this poor scaling behaviour is mainly due to the exponentially increasing number of states with the number of cells . speeding up",
    "the model fitting process is hence an essential requirement to utilize ising models for studies with large ensembles of neurons .",
    "solutions to speed up the `` classical '' boltzmann learning approach have been suggested @xcite .",
    "however these are still associated with a high computational cost .      in this paper",
    "we consider the problem of decoding which of a number of different stimuli has elicited a neural spike pattern .",
    "this can be seen as a discrete classification task : we have a set of @xmath20 stimuli @xmath21 .",
    "decoding in this scenario means that we have to provide a decision rule that estimates which stimulus has been the input to the system , given an observed spike pattern @xmath22 .",
    "the particular example to which we apply this is a simulation of the spike pattern responses elicited by visual stimuli across the receptive field of a visual cortical neuron : in this case each stimulus @xmath23 represents a different orientation of a sinusoidal grating .",
    "our main aim with this simulation was to validate our methodology in a neurophysiologically realistic coding regime , relevant to datasets to which our methodology might be applied .",
    "as a supplementary goal , we hoped to gain some insight into whether some aspects of the model affect decoding performance - such as heterogeneity of tuning , observed in real neural recordings but often ignored in population coding models .    for decoding",
    ", we use the maximum a posteriori ( map ) rule : @xmath24 where the second step is the application of bayes theorem and the third equality holds because @xmath25 is independent of @xmath26 and is hence irrelevant for maximising the given expression , i.e. just a constant factor with respect to @xmath26 that scales the maximum accordingly . in the case",
    "we examine here we assume we are in control of the stimulus distribution @xmath27 , and thus we can choose it to be uniform , i.e. to exhibit the same constant probability for each stimulus and therefore be independent of @xmath26 , as well .",
    "hence our decoding rule simplifies further to the maximum likelihood ( ml ) rule : @xmath28 within this setting , the task of creating a neural decoder reduces to the modelling of the stimulus dependent distributions @xmath29 .",
    "once these are obtained , we can apply our ml decoding rule ( equation [ ml_rule ] ) to estimate the given input stimulus @xmath26 .",
    "we have used two different statistical models to fit the observed spike patterns for each stimulus .",
    "firstly , we have used an ising model for @xmath29 , i.e. we assume that for each stimulus , the spike pattern distribution can be described by a ( different ) ising model .",
    "secondly , we have used an independent model distribution @xmath30 , assuming that given a stimulus , each cell is independent of the others : @xmath31 the independent model is the binary maximum entropy model of first order , i.e. it takes into account only the first order moments ( the constraints on the means given by equation [ constraints1 ] ) and is therefore a natural comparison for the ising model . as it is very easy to fit the independent model , we used this as a control method , to test whether the more complex ising model could enhance decoding performance .",
    "note that the numerical values for the probabilities can get very small for large cell ensembles , and therefore to evade finite precision problems we use in this case an equivalent log - likelihood decoding rule instead of the ml rule , i.e. maximise the logarithm of the likelihood instead of maximising the likelihood directly .      to train and test the decoders , we proceed as follows :    1 .   for each stimulus",
    "we simulate a set of possible response vectors .",
    "the details of the simulation are described in the following subsection .",
    "we separate the simulated response patterns into training data , which is used to fit the model and test data which we use to evaluate the decoding performance of the obtained models . 3 .",
    "the whole testing procedure is performed with 10 fold cross - validation , i.e. we divide the whole data for each stimulus into 10 equally sized parts .",
    "we then use 9 parts of the data to train our model and the remaining one for testing .",
    "we repeat this process again with all 10 possible test / training data set combinations of this kind to reveal if our results generalize to the whole dataset .    [ cols=\"<,^,<\",options=\"header \" , ]      we simulated the transient response patterns of activity evoked by visual ( orientation ) stimuli in layer v pyramidal neurons of the anaesthetized mouse visual cortex .",
    "the orientation direction were chosen to be @xmath32 , where s is the number of stimuli and @xmath33 .",
    "the properties of our simulation are motivated by the results reported by .",
    "we simulated different models by augmenting a basic model with mostly homogeneous response characteristics , with some come controlled heterogeneous characteristics .",
    "our model is defined as follows , with parameters specified in table  [ tab : simsett ] .",
    "the spontaneous activity of each neuron was set to 1.7 spikes per second , corresponds to the reported median value for layer v neurons in @xcite .",
    "we assumed that neuronal direction preferences were uniformly spaced around the circle .",
    "each neuron s tuning curve was defined by a von mises function ( circular gaussian ) with half width at half maximum ( hwhm ) fit to experimental data @xcite . the direction selectivity index ( dsi )",
    "was set to 0.1 for all layer 5 neurons in our model .",
    "sustained firing rates were fit to the distributions reported in @xcite . to reflect that we are considering a situation in which a stimulus is decoded from a short time window ( 20 ms ) of data , we multiplied these evoked rates by a fixed transient - to - sustained ratio of 1.5 , taken to reflect the onset response of the neuron s response to a flashed stimulus .",
    "as our model is fit to data from directional ( drifting grating ) stimuli , we took the arithmetic mean value of the two corresponding diametrically opposite directions for each neuron to compute the model response to a flashed orientation .",
    "the characteristics of the basic model were modulated via two inhomogeneity parameters @xmath34 $ ] , to introduce a heterogeneous distribution of firing rates and the tuning widths respectively , as described in table  [ tab : simsett ] .",
    "we neglected inhomogeneity in other parameters .",
    "thus the parameter @xmath35 regulates firing rate heterogeneity , where @xmath36 corresponded to the basic homogeneous mode , l and @xmath37 to the most heterogeneous firing rate setting .",
    "the parameter @xmath38 was used analogously to regulate heterogeneity in the tuning widths of the neurons .",
    "the effects of the two heterogeneity parameters @xmath39 are illustrated in figure [ fig:2 ] .",
    "patterns of spikes fired by the neural population were simulated using a dichotomized gaussian approach @xcite .",
    "since we can not estimate covariance matrices from experimental data directly , and not every positive definite symmetric matrix can be used as the covariance matrix of a multivariate binary distribution , we adapted the following approach .",
    "first we compute upper and lower covariance bounds for each pair of neurons , according to @xcite @xmath40 where @xmath41 and @xmath42 are the means ( mean spiking probabilities ) of neuron @xmath6 and @xmath43 , respectively .",
    "we then choose a random symmetric matrix @xmath44 that lies between these bounds .",
    "as in general this choice does not result in a permissible correlation matrix for the underlying gaussian , a correction is applied to find the closest correlation matrix possible for the latent gaussian ( cf . ) , to which we finally arithmetically add a random correlation matrix with uniformly distributed eigenvalues to adjust the mean correlation strength . having established a dichotomized gaussian model we can thus draw samples with high efficiency .            where not otherwise stated in the text , 10000 trials per stimulus were simulated , allowing 9000 training samples and 1000 test samples with 10-fold crossvalidation . in the absence of a detailed characterization of the correlation structure of neural responses in the mouse visual cortex , we assumed that the correlation in firing between each pair of neurons was weak and positive .",
    "our simulation results in a mean correlation of 0.11 and a standard deviation of 0.040 ( measured with 100000 samples for different ensemble sizes ) for our basic model and similar levels of correlation for nonzero @xmath45 .",
    "due to limitations of the dichotomized gaussian simulation , we were not able to specify the correlations of the spike trains / between the individual neurons exactly , thus all reported correlations are measured and may be prone to small variations .",
    "however , such limitations would be inherent to any simulation approach , as i ) the covariance structure of a multivariate binary distribution is always constrained by the firing probabilities of the cells and can not be chosen independently of these firing rates @xcite , and ii ) finite sampling effects will always affect the simulated data , resulting in fluctuations in the correlation structure .",
    "we were able to vary the ( measured ) mean absolute correlation level in some simulations , allowing an assessment of the relative effects of correlation strength for decoding . to do this",
    "we proceeded as follows : having established the correlation matrix of the latent gaussian as described before allows us to sample in a regime with correlations around @xmath46 . likewise",
    "a latent gaussian with a correlation matrix given by the identity matrix would correspond to the case where there are no correlations in the latent gaussian and thus in the simulated spike trains itself , which means that the simulated spike patterns for each neuron are independent ( note that due to the model used in the correlation matrix and the covariance matrix of the latent gaussian are the same ) .",
    "finite sampling effects might still introduce some nonzero measured correlations in the spike patterns , but the underlying distribution would still be with independent neurons .",
    "thus , by interpolating between these two cases we can reduce the effective correlations in the data in a controlled way , where at one end we yield our original model and at the other end we yield a set of independent neurons .      for fitting the model parameters in the ising model case we use two different strategies : mean field approximations and minimum probability flow learning .",
    "in earlier work we used boltzmann learning @xcite , however as this becomes rapidly computationally intractable with an increasing number of neurons , we have not reported it here .",
    "the suitability of different mean field approaches for fitting the parameters of an ising model have been recently assessed by . in their work ,",
    "roudi et al .",
    "compared the learned model parameter as inferred by boltzmann learning , with the parameters inferred by a number of different approximative algorithms . here",
    "we examine the utility for decoding of using successively higher order mean field approximations .",
    "tanaka @xcite demonstrated how to systematically obtain mean field approximations of increasing order based on a plefka series expansion @xcite of the gibbs free energy . by truncating these series to terms up to n - th order , and using the linear response correction",
    ", it is possible to derive an n - th order mean field approximation , yielding @xmath47 equations for the external fields , and @xmath48 equations for the pairwise couplings ( cf . ) .",
    "these equations can then be solved with respect to @xmath19 and @xmath18 . for higher order approximations",
    ", these equations can have more than one solution .",
    "this problem can be resolved by considering that the plefka series expansion is effectively a taylor series expansion and continuity of the solutions is expected when higher order terms are gradually increased @xcite .",
    "tanaka further provided an explanation for the `` diagonal weight trick '' as used by . with",
    "this trick one introduces @xmath47 extra equations for the pairwise `` self - couplings '' @xmath49 , which can be used to refine the respective approximations .",
    "the success of this trick can be explained @xcite by considering that using this diagonal weight trick in an n - th order method , is effectively incorporating the dominant terms of the next higher @xmath50 order expansion .    for fitting the parameters ,",
    "we have compared different mean field approximations from zeroth to second order with and without the diagonal weight trick , thus incorporating with our highest approximations all second order terms and the dominant third order terms of the free energy expansion .",
    "the zeroth order method is thereby equivalent to the independent model , thus providing another way of thinking about the independent decoder .",
    "[ [ first - order - methods - naive - mean - field - approximation . ] ] first order methods ( naive mean field approximation ) . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for the first order or naive mean field approximation , the equations for the external fields and pairwise couplings become :    @xmath51    where @xmath52 is the estimated coupling matrix with elements @xmath53 , the ` magnetization ' @xmath54 , and the covariance matrix @xmath55 is defined by @xmath56 . in the following we will denote this naive mean field method by nmf .    by incorporating the diagonal weight trick",
    "the above equations change slightly into the following :    @xmath57    where in addition to the matrices defined above , we have defined the diagonal matrix @xmath58 , with @xmath59 being the kronecker delta .",
    "note that in the nomenclature of , this has been called `` naive mean field method '' .",
    "however we will in the following refer to it as naive mean field method with diagonal weight trick ( nmfwd ) .",
    "[ [ second - order - methods - tap - approximation . ] ] second order methods ( tap approximation ) .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the equations of the second order method are also known as the tap equations @xcite and can be seen as a correction to the naive mean field methods . using the tap approach",
    "the equations for the model parameters read :    @xmath60    where the first equation can be solved for the pairwise couplings @xmath8 .",
    "as mentioned previously the correct solution has to be chosen according to continuity conditions outlined in , from which then the external fields @xmath7 can be computed .",
    "more precisely , if @xmath61 we choose the solution , which is closer to the original first order mean field solution . if @xmath62 we use the first order mean field solution directly .",
    "we use this procedure as it avoids pairwise couplings becoming complex , and respects the continuity of the inverse ising problem for @xmath8 as a function of @xmath63 . in the following this method",
    "is denoted by tap .",
    "[ [ incorporating - third - order - terms . ] ] incorporating third order terms .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the second order method can also be augmented by the diagonal weight trick , hence incorporating the leading third order terms of the free energy expansion .",
    "the equations for the tap method with diagonal weight trick ( tapwd ) are given by :    @xmath64    where we have solved the equations in an analogous fashion to that described for the normal tap approach .",
    "recently proposed the minimum probability flow learning ( mpfl ) technique , which provides a general framework for learning model parameters .",
    "as this technique is also applicable to the ising model , we have used it to learn external fields and pairwise couplings for our model . however , as the sampling regime usually feasible in neurophysiological experiments dictates a small number of samples compared to the number of parameters in the model ( which is @xmath65 with @xmath47 cells ) , the learning problem for the parameters becomes under - constrained already at intermediate neural ensemble sizes , i.e. we are likely to have more parameters to fit than there are samples .",
    "we therefore introduced a regularization term to their original objective function to penalize model parameters growing to large numbers , i.e. to avoid overfitting .",
    "given the original objective function @xmath66 with @xmath67 being the parameters of our model , our regularized objective function reads : @xmath68 where @xmath69 is the @xmath70 norm , which is a common choice of regularization term @xcite .",
    "so for the ising model case we have : @xmath71 for the present work , the regularization parameter was set to @xmath72 , after systematically assessing different settings for an ensemble size of @xmath73 cells via cross - validation .",
    "we refer to this learning algorithm as rmpfl ( regularized mpfl ) in the rest of the manuscript .",
    "other choices for the regularization term are possible and might even result in better performance for decoding purposes , e.g. two independent penalty terms for the external fields and the pairwise couplings .",
    "however an extensive assessment of different parameter settings would be very time consuming due to the cost of calculating the invoked partition function .",
    "we therefore have not performed an exhaustive analysis of regularization . in a real experiment",
    "the regularization term should however be adapted to yield the best possible performance for the specific number of cells .",
    "we not that while it is not necessary to compute the partition function for some applications of mpfl ( e.g. if learning the @xmath8 parameters is the end in itself ) , it is required for decoding .",
    "estimating the partition function is a computationally expensive task , since the set of possible responses @xmath13 grows exponentially with the number of cells @xmath47 , rendering an analytical computation ( equation [ part_fc ] ) intractable for large neural ensembles .",
    "as mpfl learning does not provide an estimate of the partition function , we used the ogata - tanemura partition function estimator @xcite , which is based on markov chain monte carlo ( mcmc ) techniques .",
    "however mcmc is still a very time consuming technique . to speed up the model fitting process",
    ", we estimated the partition function for each stimulus only once in a 10 fold cross - validation run when using mpfl , as ideally all samples for a specific stimulus should come from the same distribution , thus approximately sharing the same partition function .",
    "we have examined the effect of this approximation ( see results ) .    when fitting the model parameters with mean field theoretic approaches , we computed the ( true ) partition function @xmath74 in the mean field approximation , as reported in , , and .",
    "for the first order mean field approach this yields : @xmath75 with @xmath76 here each of the parameters is actually a function of the stimulus @xmath26 , which we omit for clarity .    for the second order methods ,",
    "the corresponding equation becomes : @xmath77 with @xmath78      the fraction of correctly decoded trials was the principal method used to assess decoding performance .",
    "however , as the fraction correct or accuracy does not by itself provide a complete description of decoder performance , we sought used additional performance measures .",
    "the performance of a decoder is fully described by its confusion matrix , and we show how directly examining this matrix can yield insight into its behaviour . however , it is advantageous to be able to reduce this to a single number in many cirumstances .",
    "we therefore additionally computed the mutual information between the encoded and decoded stimulus @xcite to characterise the performance further .",
    "this provides a compact summary of the information content of the decoding confusion matrix .",
    "we can write the mutual information ( measured in bits ) as : @xmath79 where @xmath80 is the entropy of the encoded stimulus @xmath81 and @xmath82 is the conditional entropy describing the distribution of stimuli @xmath26 that have been observed to elicit each decoded state @xmath83 , @xmath84 since we have in the current study opted for a uniform stimulus distribution , the entropy @xmath80 is simply given by @xmath85 in general the conditional entropy @xmath82 has to be computed from the confusion matrix .",
    "we note that if we were to assume that the correctly decoded stimuli and errors are uniformly distributed for all stimuli , i.e. that the conditional distribution @xmath86 is of the form @xmath87 then the conditional entropy simplifies to @xmath88 this simplified expression has been used to characterize decoder performance in the brain computer interface literature @xcite . here",
    ", we present this equation only to make apparent the scaling behaviour , and compute the decoded information using the more general expression ( eqn .",
    "[ eq : mutinfo ] ) .",
    "we performed computer simulations as described in methods , to generate datasets for training and testing decoding algorithms . for all settings ,",
    "20 simulations were performed with different random number seeds , in order to characterize decoding performance .",
    "a number of metrics , including the fraction of correct decodings ( accuracy ) and mutual information between decoded and presented stimulus distributions , were used in order to characterize and compare decoding performance .",
    "the ising model based decoders show better performance than the independent decoder in nearly all cases ( fig .",
    "[ fig:4]a ) , in terms of the average fraction of correctly decoded stimuli .",
    "the performance of the standard ( non - regularized ) mpfl technique , however , in our hands falls away relatively quickly as the number of cells increases , failing to better the independent decoder after approximately 100 cells .",
    "this behaviour can be explained by considering that the problem of parameter estimation becomes more and more underconstrained as the number of cells increases while holding the number of training samples fixed .",
    "falsely learned model parameters moreover affect the decoding performance by influencing the estimated partition function and thus worsen the decoder performance .",
    "as we have only estimated the partition function once per stimulus when using mpfl , large fluctuations in the training dataset can potentially have a big effect . to compensate for this behaviour , a regularization term",
    "can be included , which can stabilize the performance up to a significantly larger number of neurons ( as described in methods ) .",
    "our regularized version of mpfl still decreases in performance after about 110 cells .",
    "however , we have used a fixed value for the regularization parameter for all simulations here , whereas ideally the regularization term should be adapted to the number of cells and number of samples in the specific setting .",
    "using such an approach would most likely result in a better performance for larger numbers of cells . as an example we have tested for an optimized @xmath89 parameter for 150 cells and found that we could increase performance to @xmath90 in this case ( average over 10 trials , not shown in figure ) .",
    "one of the assumptions made for much of this paper is that the partition function can be estimated only once in each 10-fold crossvalidation run , thus speeding up training .",
    "we studied the effect of this approximation on performance , finding , as shown in figure [ fig:5 ] , that the effect is marginal , at least under our operating conditions .",
    "no statistically significant difference between the two approaches was observed . in a decoding regime where the rmpfl method starts to infer the wrong model parameters ( e.g. due to overfitting ) one would expect these effects to become more pronounced .",
    "however , in such a scenario where the rmpfl method becomes unreliable , a different regularization term or a different inference method should be considered .",
    "the decoded information analysis reveals that the difference in the decoding performance of the independent and ising ( as exemplified by tap , tapwd and rmpfl ) models becomes more pronounced as the number of stimuli is increased , as shown in fig .",
    "[ fig:4]b , c . as the number of stimuli increase ,",
    "the independent and ising decoder curves separate , indicating not only a difference in the accuracy of both decoders , but also a difference in the confusion matrices , i.e. in the distribution of errors between the two approaches .",
    "this is considered in more detail in section  [ sec : confmat ] .",
    "a further interesting behavior of the ising decoder is apparent in fig .",
    "[ fig:4]c : as the number of stimuli increases , the relative decoding gain @xmath91 , ( which we define as the ratio between the  actual \" and  chance \" fraction correct ) keeps increasing with the number of stimuli for the ising model case , whereas it saturates for the independent decoder .",
    "this effect can be explained as follows : the independent decoder relies on the fact that two different stimuli will result in different firing rates for each cell . with an increasing number of stimuli however",
    ", the difference between two adjacent stimuli becomes smaller and smaller , and thus the difference in the firing rates discriminating two adjacent stimuli becomes smaller and smaller .",
    "therefore the decoder performance of the independent decoder rapidly decreases as the number of stimuli increases . as two adjacent stimuli can result in neural responses that have quite different correlation structure",
    "despite having very close firing rates , the ising decoder can additionally make use of this information in the data and thus yield better performance .",
    "this suggests that the ising decoder may be particularly advantageous as the decoding problem becomes more difficult and not easily discriminable in terms of the neural firing rates .",
    "real world performance will of course be dependent upon the level of systematic difference in correlation structure induced by stimuli in any given dataset .",
    "performance of the ising decoder is strongly dependent on the number of training trials available ( fig .",
    "[ fig:4]d i ) .",
    "here we found , for 70 neurons , that around 400 training samples were required to allow the ising decoder to outperform independent decoding .",
    "the independent decoder will necessarily have better sampling performance , as it relies only upon lower order response statistics .",
    "( it is worth recalling that a  full \" decoder , which made use of all aspects of spike pattern structure , would have far worse scaling behaviour than either ) .",
    "another way of looking at this is to examine the estimated covariance matrix from the simulated data ( fig .",
    "[ fig:4]d ii ) .",
    "we define the mean relative error of the covariance matrix as @xmath92 where @xmath93 is the estimated covariance between unit @xmath6 and @xmath43 under stimulus @xmath26 from the ( finite ) simulated data ( normally 10000 samples ) , and @xmath94 is the asymptotic covariance , defined in the same way but computed with 100,000 samples .",
    "this error provides a measure of the finite sampling bias we encounter for fitting the model .    ) .",
    "comparison of fraction correct vs. number of cells for basic model and fully heterogeneous model .",
    "the tapwd algorithm was used to train the decoder . ]      the performance characteristics in a heterogeneous scenario , i.e. with nonzero @xmath95 , are for the most part broadly similar to the homogeneous case , so we report here only on the observed differences .",
    "the overall classification performance both in terms of fraction correct and mutual information , is slightly improved for both independent and ising decoders with heterogeneous neural ensembles . as an example , the accuracy as a function of ensemble size is compared for the basic model and the `` fully heterogeneous '' ( @xmath96 ) models in fig.[fig:6 ] , for both tapwd and independent decoders . the greater performance for nonzero @xmath45 can be explained by the greater variability of cell properties , allowing more specific response patterns than in the homogeneous scenario .",
    "such a scenario is presumably relevant to many real - world decoding problems , suggesting that decoder performance analysed with homogeneous test data may slightly under - represent real - world performance .    . with increasing @xmath35",
    "the decoding performance is enhanced .",
    "the tuning width heterogeneity as specified by @xmath38 reduces the performance of the decoder ( values from upper left to lower right curve @xmath97 ) .",
    "data for 70 cells , tapwd algorithm used to train the decoder . ]",
    "we also assessed the relative influence of the individual parameters @xmath45 on the decoding behavior as shown in figure [ fig:7 ] .",
    "our analysis shows that , while the heterogeneity in the firing rates as specified by @xmath35 has a positive effect on the decoding performance , an increased tuning width heterogeneity slightly decreases the performance of the decoder .",
    "however , the relative influence of the parameter @xmath38 is small .      as the advantage of the ising decoder over the independent decoder stems from its ability to take advantage of information contained in pairwise correlations",
    ", we examined the dependence of this advantage on the average strength of correlation . although we have set the average level of correlation to what has traditionally been thought to be a reasonable level for cells in the same neighbourhood @xcite , there is an ongoing debate about the level and stimulus dependence of correlation relevant for cortical function @xcite .",
    "this is of course critical for the performance of the ising decoder .",
    "if there were no ( noise ) correlations present in the data , i.e. an independent decoder were the correct model for the data , there would be no benefit to using any decoder including correlation such as the ising decoder .",
    "in fact , any decoder including correlations would most likely perform worse in practice than an independent decoder , as due to finite sampling effects one would most likely falsely estimate some small correlation in the data . overfitting to these falsely learned correlations would then result in a performance decrease compared to an independent decoder , which by construction does not include any correlations , i.e. would implement the correct model .",
    "this effect be seen in figure [ fig:8 ] , where we have varied the mean absolute correlation strength as outlined in the methods section .",
    "it can be seen that as the level of correlation increases for specified spike count , the independent decoder loses some discriminative capacity .",
    "the ising decoders , however , take advantage of the higher level and spread of correlation values for discrimination between stimuli .",
    "as described in methods , at the lower end of this curve the underlying latent gaussian has an identity correlation matrix , and thus the individual neurons are actually independent , although the average measured absolute correlation does not completely decrease to zero .",
    "this provides an explanation of why the ising decoder fails to beat the independent decoder : by assuming the measured correlations are due to the true distribution , it overfits to these correlations , and thus performs worse compared to the ( by construction correct ) independent model .",
    "it should be noted , however , that the performance drop of the ising decoder compared to the independent decoder is small even for a regime where cells are effectively independent .        to mimic the richer",
    "correlation structures potentially found in real neural recordings , we performed further testing .",
    "we simulated larger ensembles of neurons , while keeping the number of observed cells fixed , thus effectively creating `` hidden '' neurons .",
    "the visible neurons were simulated as described in methods , while for every unobserved cell , the preferred tuning direction was chosen randomly according to a uniform distribution . by using this scheme",
    "we could assure that the observed @xmath98 cells always had the same characteristics except the correlation structure , which was effectively changed by the introduction of @xmath99 unobserved neurons .",
    "it should be noted that introducing hidden cells alters not only the second order statistics but also higher order correlations in the data , thus providing a much richer statistical structure in the data .    the different decoding ( and training )",
    "methods vary in their robustness towards the introduction of such higher order correlation structure ( fig .",
    "[ fig:9 ] ) .",
    "while the independent decoder is relatively unaffected by the introduction of additional unobserved cells , the ising decoder model is more sensitive to such changes .",
    "however , significant differences between the different training strategies can be observed : in our case the rmpfl method showed the least performance drop ( about 10% ) in a scenario where approximately 100 out of 500 cells were observed , while the fraction correct for the standard tap approach drops by more than 15% in this scenario .",
    "tapwd is less affected than tap , consistent with its inclusion of the leading terms for the next order expansion .     for 100 cells , normalized to the fraction correct @xmath100 where 100 out of 100 cells are visible ,",
    "is displayed as a function of the ratio between @xmath99 hidden cells and @xmath98 observed cells . for simulations",
    "the basic model was used . ]",
    "the confusion matrix provides complete information about the decoding error distribution .",
    "confusion matrices for the ising decoder and the independent decoder respectively are shown in fig .",
    "[ fig:10 ] .",
    "the ising decoder has higher diagonal terms , corresponding to better decoding accuracy .",
    "the overall appearance of the ising decoder confusion matrix is fairly similar to the independent decoder .",
    "however , by comparing the two confusion matrices it can be seen that the ising decoder mainly gains its performance benefit over the independent decoder by avoiding confusion of adjacent stimuli .",
    "this shows that as the difference in adjacent stimulus directions becomes less with an increasing number of stimuli , the ising model decoder can utilize the correlation patterns to enhance the decoding accuracy , i.e. to distinguish between adjacent stimulus directions more precisely .",
    "this effect of course depends on the correlation model used - here the correlation between each pair of neurons was resampled for each stimulus in the simulation .",
    "if ( noise ) correlations were not at all stimulus - dependent , or if the model was quite different , then the ising decoder may not be able to take advantage of this potential performance advantage .",
    "while most work in the brain - machine interface literature has focused on continuous decoders , there has been some work on the discrete decoding problem , although to date with a focus on the analysis of either continuous data , such as electroencephalographic ( eeg ) data @xcite , or on longer time windows of multi - electrode array data , in which spike counts are far from binary @xcite .",
    "as the discrete decoding problem can be viewed as a classification problem ( in the same sense as the continuous decoding problem can be seen as a regression problem ) , it is of interest to compare the performance of our approach with traditional classification approaches such as the optimal linear classifer ( olc ) .    following , each stimulus class can be described by its own linear model , so that @xmath101 where @xmath102 . using a 1-of - s binary coding scheme ( i.e. we denote stimulus class @xmath23 by a `` target '' column vector @xmath103 with all zeros except the i - th entry , which is one ) the weights @xmath104 can be trained such as to minimize a sum of squares error function for the target stimulus vector .    this",
    "is done in fig .",
    "[ fig:11 ] .",
    "it can be seen that , under the conditions we test here ( 10000 trials , simulated data as described previously for the basic model ) , the olc underperforms both the ising ( as exemplified by the tapwd approach ) and independent classifiers . the former is not unexpected , but it may seem initially counter - intuitive that the olc does not yield identical performance to the independent decoder , as the latter is in effect performing a linear classification .    however , the following differences must be noted for these two algorithms . as their implementation details differ , they may have markedly different sensitivity to limited sampling - the independent decoder , as we have constructed it here ( product of marginals ) has remarkable sampling efficiency .",
    "most importantly however , the olc decoder assumes by construction a gaussian error in the stimulus class target vectors , i.e. it corresponds to a maximum likelihood decoding when assuming that the target vectors follow a gaussian conditional distribution @xcite .",
    "this assumption is clearly not valid in the case of binary target vectors .",
    "therefore the failure of the olc decoder should not be surprising .",
    "we have demonstrated , for the first time , the use of the ising model to decode discrete stimulus states from simulated large - scale neural population activity . to do this",
    ", we have had to overcome several technical obstacles , namely the poor scaling properties of previously used algorithms for learning model parameters , and similarly the poor scaling behavior of methods for estimating the partition function , which although not necessary for some applications of the ising model in neuroscience , is required for decoding .",
    "the ising model has one particular advantage over a simpler independent decoding algorithm : it can take advantage of stimulus dependence in the correlation structure of neuronal responses , where it exists . with the aid of a statistical simulation of neuronal ensemble spiking responses in the mouse visual cortex",
    ", we have demonstrated that correlational information can be taken advantage of for decoding the activity of neuronal ensembles of size in the hundreds by several algorithms , including mean field methods from statistical physics and the rmpfl algorithm .",
    "ising models have gained much attraction recently in neuroscience to describe the spike train statistics of neural ensembles @xcite .",
    "however , these findings have largely been made only in relatively small neural ensembles ( typically a few tens of cells ) , from which an extrapolation to larger ensemble sizes might not be wise @xcite .",
    "the principal reason for this limit has been the poor scaling of the computational load of fitting the ising model parameters , when algorithms such as boltzmann learning are used .",
    "moreover , new findings suggests that pairwise correlations ( and thus ising models ) might not be sufficient to predict spike patterns of small scale local clusters of neurons ( @xmath105 apart ) , which have been observed to provide evidence of higher order interactions @xcite .",
    "while the formalism for higher order models may be similar , scaling properties are guaranteed to be even worse .",
    "there is thus the pressing need to develop better algorithms for learning the parameters of ising and ising - like models .",
    "it should be noted that while ising models might not necessarily provide an exact description of neural spike train statistics , for decoding purposes we only require that we can approximate their distribution well enough to achieve good decoding performance .",
    "the development of discrete neural population decoding algorithms has two motivations .",
    "the first motivation is the desire to develop brain - computer communication devices for cognitively intact patients with severe motor disabilities @xcite . in this type of application ,",
    "an algorithm such as those we describe could be used together with multi - electrode brain recordings to allow the user to select one of a number of options ( for instance a letter from a virtual keyboard ) , or even in the longer term to communicate sequences of symbols from an optimized code directly into a computer system or communications protocol .",
    "given the short timescale to which the ising decoder can be applied ( we have fixed this at 20 ms here ) , and sufficiently large recorded ensembles to saturate decoder performance , very high bit rates could potentially be achieved .",
    "the second motivation is more scientific : to use such decoding algorithms to probe the organization and mechanisms of information processing in neural systems",
    ". it should be immediately be apparent that the ising decoder and related models can be used to ask questions about the neural representation of sensory stimuli , motor states or other behavioral correlates , by comparing decoding performance under different sets of assumptions ( for instance , by changing the constraints in an ising model to exclude correlations , include correlations within 50 @xmath106 m , etc ) .",
    "this ( commonly referred to as the  encoding problem `` ) is essentially the same use to which shannon information theory has been applied in neuroscience ( see e.g. for a recent example ) , with simply a different summary measure .",
    "use of decoding performance may be an intuitively convenient way to ask such questions , but it is still asking exactly the same question . however , there are other uses to which such algorithms can be applied .",
    "for instance , combining sensory and learning / memory experimental paradigms , once a decoder has been trained , it could be used subsequently to read out activity patterns in different brain states such as sleep , or following some period of time - for instance , to ' ' read out memories \" by decoding the patterns of activity that represent them .",
    "the decoding approach may thus have much to offer the study of information processing in neural circuits .",
    "our results show that decoding performance is critically dependent on the sample size used for training the decoder , as relatively precise characterization of pairwise correlations is needed to fit a model that matches the statistical structure of as - of - yet unobserved data well .",
    "in the `` encoding problem '' , such finite sampling constraints result in a biased estimate of the entropy of the system .",
    "for the decoding problem considered here , finite sampling leads to overfitting of the model to the observed training data , so that it does not generalize well to the unobserved data , and accordingly fails to predict stimulus classes correctly during test trials .",
    "such finite sampling constraints mean that below a particular sampling size - which we found to be 400 trials for 70 neurons in one particular example we studied - there is no point in using a model which attempts to fit pairwise ( or above ) correlations , one may as well just use an independent model .",
    "this has implications for experimental design .",
    "however , it should be noted that the real brain has no such limitation - in effect , many thousands or millions of trials are available over development , and so a biological system should certainly be capable of learning the correct correlations from the data @xcite and thus may well be able to operate in a regime where decoding benefits substantially from known correlation structure .    we have shown that incorporating correlations in the decoding process might be especially relevant for ` hard ' decoding problems , i.e. multi - class discrimination problems in which stimuli are not easily distinguishable by just observing individual neuronal firing rates . in this scenario including correlations could be a means to enhance the precision of the decoding process by increasing the discriminability between adjacent or similar stimuli .",
    "including correlations could make the pattern distribution more flat , or uniform , with low firing rates , leading to greater energy efficiency of information coding @xcite .",
    "we must insert a note of caution concerning the presence of higher order correlations in data to be fitted .",
    "such higher order correlations might arise simply from the presence of a large number of `` hidden '' neurons whose activity has not been recorded , but which have a substantial influence on the activity of those neurons recorded .",
    "this is likely to occur frequently in real neurophysiological situations .",
    "the performance of pairwise correlation decoder models , such as the ising model , is necessarily affected detrimentally by such effects , as shown in fig .",
    "interestingly , an independent decoder is less affected ( although it may also be capturing less information anyway ) . obviously , it is possible to make use of higher order models to alleviate this problem , but a penalty then has to be paid in sampling terms .",
    "we note that the primary use of the ising model in neuroscience so far has been to model the empirical statistics of neural spike train ensembles @xcite .",
    "there is of course no requirement that a good decoder is also a good model of neural spike train statistics - what matters is its performance on the test dataset . nevertheless , knowing how well the model captures spike train pattern structure may help to build better decoders , and of course may be of particular value when those decoders are used to study neural information processing ( as opposed to being used for a practical purpose such as brain - machine interface development ) .",
    "unfortunately , a direct comparison of empirical to model spike pattern probabilities is not experimentally feasible for large ensemble sizes - while this is relatively simple for 15 cells , it is far from viable for 500 cells .",
    "further work is needed to determine how best to evaluate the performance of decoders at predicting empirical spike patterns , when decoding very large neural ensembles .",
    "one caveat to the advantage provided by correlations of using ising over independent decoders is that it depends entirely upon the extent to which correlations are found to depend upon the stimulus variables of interest . while a previous study at longer timescales has found correlations to improve neural decoding @xcite , the jury is still out on the prevalence of stimulus dependence of pairwise and higher order correlations in the mammalian cortex .",
    "stimulus - dependent correlations have been found in mouse @xcite , cat @xcite and monkey @xcite , where they have been shown to contribute to information encoding @xcite , but most recordings to date have sampled relatively sparsely from the local cortical circuit , due to limitations in multi - electrode array hardware .",
    "it is possible that if one were to be able to record from a greater proportion of neurons in the local circuit , then stronger stimulus - dependent correlations might be observable .    in an experimental setting the performance could also be enhanced by choosing an optimal timebin - width , a question that we have not addressed in this paper .",
    "however care is needed for choosing the right bin - width .",
    "as shown by roudi et al .",
    "@xcite , a small bin - width is likely to yield a good fit , however choosing a too small bin - width invalidates the underlying assumption of uncorrelated time - bins .",
    "choosing a too large time - bin makes it however harder to find a good fit for the model and additionally may violate the assumptions of binary responses .",
    "a number of avenues present themselves for future development of decoding algorithms .",
    "firstly , algorithms for reducing model dimensionality without losing discriminatory power , may prove advantageous",
    ". these may include graph and hypergraph theoretic techniques @xcite for pruning out uninformative dimensions ( edges and nodes ) , and factor analysis methods for modeling conditional dependencies @xcite .",
    "such an approach may be particularly advantageous when experimental trials are limited , as the dimensionality of the parameter set is the main reason for ising model performance not exceeding the independent model for limited trials .",
    "one difficulty with the use of graph pruning approaches is that the usual pairwise correlation matrix of neural recordings , unlike the graph in many network analysis problems , tends not to be sparse .",
    "it is of course a functional , as opposed to synaptic , connectivity matrix , and one reason for this lack of sparseness is its symmetric nature .",
    "it has recently been proposed that the symmetry property of the @xmath8 matrix can be relaxed in the context of the ( non - equilibrium ) kinetic ising model , which also provides a convenient way to take into account space - time dependencies , or causal relationships @xcite .",
    "use of the kinetic ising model framework for decoding would appear to be an interesting future direction to pursue .",
    "new experimental technologies are yielding increasingly high dimensional multivariate neurophysiological datasets , usually without concomitant increases in the duration of data that can be collected .",
    "however , there is some reason for optimism that we will be able to develop new data analysis methods capable of taking advantage of this data .",
    "maximum entropy approaches to the fitting of structured parametric models such as the ising model and its extensions would appear to be one approach likely to yield progress .      we thank phil bream , hlne seiler and yang zhang for their contributions to earlier work leading up to that reported here , and aman saleem for useful discussions and comments on this manuscript .",
    "we also thank yasser roudi for useful comments on the tap equations , and jascha sohl - dickstein , peter battaglino , and michael r deweese for useful matlab code and discussion of the mpfl technique .",
    "this work was supported by epsrc grant ep / e002331/1 to srs .",
    "baddeley , r. , abbott , l.  f. , booth , m.  c. , sengpiel , f. , freeman , t. , wakeman , e.  a.  rolls , e.  t. 1997 , ` responses of neurons in primary and inferior temporal visual cortices to natural scenes . ' , _ proc biol sci _ * 264*(1389 ) ,  177583 .",
    "butts , d.  a. , weng , c. , jin , j. , yeh , c .- i .",
    "i. , lesica , n.  a. , alonso , j .-",
    "m.  stanley , g.  b. 2007 , ` temporal precision in the neural code and the timescales of natural vision . ' , _ nature _ * 449*(7158 ) ,  925 .",
    "panzeri , s. , schultz , s.  r. , treves , a.  rolls , e.  t. 1999 , ` correlations and the encoding of information in the nervous system ' , _ proceedings of the royal society of london .",
    "series b : biological sciences _ * 266*(1423 ) ,  10011012 .",
    "plefka , t. 2006 , ` expansion of the gibbs potential for quantum many - body systems : general formalism with applications to the spin glass and the weakly nonideal bose gas ' , _ phys .",
    "e _ * 73*(1 ) ,  016129 .",
    "pola , g. , thiele , a. , hoffmann , k.  panzeri , s. 2003 , ` an exact method to quantify the information transmitted by different mechanisms of correlational coding ' , _ network - computation in neural systems _ * 14*(1 ) ,  3560 .",
    "schultz , s.  r. , kitamura , k. , post - uiterweer , a. , krupic , j. hausser , m. 2009 , ` spatial pattern coding of sensory information by climbing fiber - evoked calcium signals in networks of neighboring cerebellar purkinje cells ' , _ j. neurosci .",
    "_ * 29*(25 ) ,  80058015 .",
    "shlens , j. , field , g.  d. , gauthier , j.  l. , greschner , m. , sher , a. , litke , a.  m.  chichilnisky , e.  j. 2009 , ` the structure of large - scale synchronized firing in primate retina ' , _ j. neurosci .",
    "_ * 29*(15 ) ,  50225031 .",
    "shlens , j. , field , g.  d. , gauthier , j.  l. , grivich , m.  i. , petrusca , d. , sher , a. , litke , a.  m.  chichilnisky , e.  j. 2006 , ` the structure of multi - neuron firing patterns in primate retina ' , _ j. neurosci .",
    "_ * 26*(32 ) ,  82548266 ."
  ],
  "abstract_text": [
    "<S> the ising model has recently received much attention for the statistical description of neural spike train data . in this paper , we propose and demonstrate its use for building decoders capable of predicting , on a millisecond timescale , the stimulus represented by a pattern of neural activity . after fitting to a training dataset , the ising decoder can be applied  online \" for instantaneous decoding of test data . </S>",
    "<S> while such models can be fit exactly using boltzmann learning , this approach rapidly becomes computationally intractable as neural ensemble size increases . </S>",
    "<S> we show that several approaches , including the thouless - anderson - palmer ( tap ) mean field approach from statistical physics , and the recently developed minimum probability flow learning ( mpfl ) algorithm , can be used for rapid inference of model parameters in large - scale neural ensembles . </S>",
    "<S> use of the ising model for decoding , unlike other problems such as functional connectivity estimation , requires estimation of the partition function . </S>",
    "<S> as this involves summation over all possible responses , this step can be limiting . </S>",
    "<S> mean field approaches avoid this problem by providing an analytical expression for the partition function . </S>",
    "<S> we demonstrate these decoding techniques by applying them to simulated neural ensemble responses from a mouse visual cortex model , finding an improvement in decoder performance for a model with heterogeneous as opposed to homogeneous neural tuning and response properties . </S>",
    "<S> our results demonstrate the practicality of using the ising model to read out , or decode , spatial patterns of activity comprised of many hundreds of neurons . </S>"
  ]
}