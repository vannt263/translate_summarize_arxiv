{
  "article_text": [
    "the classical multi - armed bandit problem provides an elegant model to study the tradeoff between collecting rewards in the present based on the current state of knowledge ( exploitation ) versus deferring rewards to the future in favor of gaining more knowledge ( exploration )  @xcite .",
    "specifically , in this model a user has a choice of bandit - arms to play , and at each time step it must decide which arm to play .",
    "the expected reward from playing a bandit - arm depends on the state of the bandit - arm where the state represents a `` prior '' belief on the bandit - arm . each time a bandit - arm is played , this prior",
    "gets updated according to some transition matrix defined on the state space .",
    "for instance , a typical assumption on the bandit - arms is that they have @xmath1-priors : the success probability of an @xmath1-bandit - arm is @xmath2 ; in case of a success a reward of 1 is obtained and @xmath3 gets incremented , whereas in case of a failure no reward is obtained and @xmath4 gets incremented .",
    "the user wishes to maximize the total expected discounted reward over time .",
    "this simple setting effectively models many applications .",
    "a canonical example is exploring the effectiveness of different treatments in clinical trials while maximizing the benefit received by patients .",
    "the discount factor in a multi - armed bandit problem may be viewed as modulating the horizon over which the strategy explores to identify the bandit - arm with maximum expected reward , before switching to exploitation .",
    "this facet of the multi - armed bandit problem is explicitly captured by the _ budgeted learning problem _ , recently studied by guha and munagala  @xcite .",
    "the input to the budgeted learning problem is the same as for the multi - armed bandit problem , except the discount factor is replaced by a horizon @xmath5 .",
    "the goal is to identify the bandit - arm with maximum expected reward using at most @xmath5 steps of exploration .",
    "the work of  @xcite gives a constant factor approximation for the budgeted learning problem via a linear programming based approach that determines the allocation of exploration and exploitation budgets across the various arms .",
    "the budgeted learning problem is the main object of study in this paper .",
    "the multi - armed bandit problem admits an elegant solution : compute a score for each bandit - arm using only the current state of the bandit - arm and the discount factor , _ independent _ of all other bandit - arms in the system , and then play the bandit - arm with the highest score .",
    "this score is known as the _ gittins index _ , and many proofs are known to show this is an optimal strategy ( e.g. , see  @xcite ) .",
    "the optimality of this `` index - based '' strategy implies that this problem exhibits a `` separability '' property whereby the optimal decision at each step is obtained by computations performed _ separately _ for each bandit - arm .",
    "this structural insight translates into efficient decision making algorithms .",
    "in fact , for commonly used prior update rules and discount rates , extensive collections of pre - computed gittins indices exist , enabling in principle , a simple lookup - based approach for optimal decision - making .",
    "there are multiple definitions of what it means for a problem to have an `` index '' .",
    "we will use the term index in its strongest form , i.e. , where the index of an arm depends _ only on the state of that arm_. this is also sometimes called a decomposable index ( eg .",
    "@xcite ) .",
    "the inherent appeal and efficiency of index - based policies is the unifying theme underlying our work .",
    "we show that many interesting and non - trivial variations of the multi - armed bandit problem , including the budgeted learning problem and the finite horizon problem , can all be well - approximated by index - based policies",
    ". moreover , our approach gives decision strategies that are _ oblivious _ to parameters such as the underlying horizon or the discount factor while being constant - factor competitive to optimal strategies that are fully aware of these parameters .",
    "we will study this problem when the state space of each arm satisfies the `` martingale property '' , i.e. , if we play an arm multiple times , the sequence of expected rewards is a martingale .",
    "this is a natural assumption for multi - armed bandit and related problems , e.g. the commonly used @xmath1 priors satisfy this property .",
    "[ [ an - index - for - budgeted - learning - problems ] ] an index for budgeted learning problems : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    our first result is that the budgeted learning problem admits an approximate index , which we call the _ ratio index_. informally speaking , given a single bandit - arm and an exploration budget of @xmath5 steps , the ratio index for that arm is the maximum expected exploitation reward per unit of the exploration and exploitation budget utilized .",
    "the ratio index suggests the following natural algorithm : at each step , play the arm with the highest ratio index .",
    "we show that this simple greedy algorithm gives a constant factor approximation to the budgeted learning problem .",
    "an @xmath0-approximation algorithm for this problem is already known  @xcite . however , the algorithm of  @xcite is based on solving a _ coupled _ lp over all the arms , whereas the ratio index can be computed for each arm in isolation , much like the gittins index .",
    "the ratio index has many other interesting properties .",
    "for example : +   + ( 1 ) we show that the gittins index with discount factor @xmath6 and the ratio index over horizon @xmath5 are within a constant factor of each other .",
    "this gives the following surprising result :    [ thm : introgittins ] given an exploration budget @xmath5 , playing at each step the arm with the highest gittins index , with discount factor @xmath7 , yields a constant factor approximation to the budgeted learning problem .",
    "the proof relies on comparing the `` decision - trees '' of the ratio index and gittins index strategies .",
    "even in retrospect , it is not clear to us how such a result could be derived using an lp - based formulation such as the one used by guha and munagala  @xcite .",
    "interestingly , the policy described in theorem  [ thm : introgittins ] is known to often work well in practice  @xcite . nonetheless , before the work of guha and munagala  @xcite , we do not know of any provable guarantees for polynomial time algorithms in this setting . and until now , we do nt know of any formal guarantees that relate the exponential discounting approach ( which yields the gittins index ) and the budgeted learning approach .",
    "+ ( 2 ) the ratio index can be computed in time which is strongly polynomial in the size of the state space ( independent of @xmath5 ) of each arm if the state space is acyclic , and strongly polynomial in the size of the state space and @xmath5 if the state space is general .",
    "our proof of this fact involves recursively analyzing the basic feasible solutions of an underlying lp for computing optimum single arm strategies and using the structure of the basic feasible solutions to prove that these strategies have a simple form .",
    "[ [ finite - horizon - and - discount - oblivious - multi - armed - bandits ] ] finite horizon and discount - oblivious multi - armed bandits : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we next study an important and natural variation of the budgeted learning problem , called the _",
    "finite horizon _ multi - armed bandit problem .",
    "we are given a finite horizon @xmath5 , and the goal is to maximize the expected reward collected during the horizon .",
    "thus , in contrast to the budgeted learning problem , the horizon @xmath5 is being used for both exploration and exploitation , and no payoffs are obtained after time @xmath5 .",
    "we show the following result using the ratio index :    [ thm : finite_horizon ] there is an index - based policy that gives a constant factor approximation to the finite horizon multi - armed bandit problem .",
    "finally , we study the role of the discount factor in the design of an optimal strategy for the exploration - exploitation tradeoff . small variations in discount factors can alter the choice of bandit - arm played at any step , highlighting the sensitivity of the gittins index to the discount rate .",
    "we study the `` discount - oblivious '' multi - armed bandit problem where the underlying discount factor is not known , and in fact , may even vary from one time step to the next .",
    "a finite horizon problem can be viewed as a special case of this general setting where the discount factor is @xmath8 for the first @xmath5 steps and is @xmath9 for all subsequent steps .",
    "there is a useful relationship between the finite horizon and discount oblivious versions of the multi - armed problem : a strategy is @xmath10-approximate for the discount - oblivious multi - armed bandit problem iff it is @xmath10-approximate ( _ simultaneously _ ) for all finite horizons . using this connection , and building on theorem  [ thm : finite_horizon ] , we show the following result :    [ thm : intro1 ] there is an index - based policy that gives a constant factor approximation for the multi - armed bandit problem with respect to all possible discount factors simultaneously .",
    "our proof of both of these results is based on the following easy consequence of the ratio index approach to the budgeted learning problem . for any constant @xmath4 ,",
    "the expected profit of the optimal @xmath11-horizon strategy is an @xmath12-fraction of the expected profit of an optimal @xmath5-horizon strategy . using this result",
    ", we design an algorithm that alternates between budgeted exploration and exploitation , using geometrically increasing horizons ; each increasing horizon competing against a lower discount rate on future rewards .",
    "it is worth noting that this result can also be shown using the lp - based proof of guha and munagala . however , the following corollary is a consequence of our index - based approach and the relation between ratio and gittins indices .",
    "[ cor : gittins ] the strategy that alternates between exploring the arm with the highest gittins index , and exploiting the arm with the highest reward , in phases of geometrically increasing length ( and discount factor @xmath13 during a phase of length @xmath14 ) provides a constant factor approximation to the multi - armed bandit problem _ simultaneously _ for all finite horizons and for all discount factors .",
    "we first show that there exists a single strategy that is an @xmath0-approximation to the optimum multi - armed bandit strategy , simultaneously for all discount factors .",
    "our existence proof is effective , also resulting in an efficient algorithm for solving this problem .",
    "the proof involves analyzing finite horizon strategies , which optimize the payoff from a fixed number of steps without a discount factor .",
    "in particular , we first show that , for any constant @xmath4 , the expected profit of the optimum @xmath15-horizon strategy is at least a constant fraction of the expected profit of the optimum @xmath16-horizon strategy .",
    "our proof of this lemma involves first scaling and then rounding a recent lp proposed by guha and munagala  @xcite for budgeted learning problems , which in turn builds on dean , goemans , and vondrak  @xcite .",
    "we believe this result offers fundamental insight into the multi - armed bandit problem ; we find it surprising that the discount factor makes only a constant factor difference .",
    "our result also holds if the discount factor @xmath17 is not constant but varies from step to step .",
    "this further supports the intuition mentioned earlier : perhaps the frustration probabilities , discount factors etc are not integral to this problem , at least from an approximations point of view .",
    "optimizing our running time and the constant factors in our approximations remain important open problems . however , we believe that it is even more important to gain further insight into the list ranking problems and their relation to each other",
    ". in particular , it would be very interesting to obtain an approximate index such that if items are always displayed in decreasing order of this index , we are guaranteed a constant factor approximation regardless of the type of user ; such an index may render the question of efficiency moot .",
    "there are many sources for the canonical work on gittins indices , particularly with reference to @xmath18 bandits and bernoulli bandit processes  @xcite .",
    "glazebrook and others have studied approximation algorithms for other extensions to multi - armed bandit problems  @xcite .",
    "their approach builds upon the concept of achievable regions and general conservation laws and a related linear programming approach built by tsoucas , bertsimas , nino - mora , and others  @xcite .",
    "relaxed linear programming based approaches to extensions of the multi - armed bandit problem have also been developed , e.g. for restless bandits  @xcite .",
    "our work on the ratio index builds on the insights obtained from the lp relaxation based approach of guha and munagala  @xcite as well as related work in model - driven optimization  @xcite and stochastic packing  @xcite .",
    "additionally , related lp formulations have been developed for multi - stage stochastic optimization  @xcite .",
    "in the theoretical computer science community , multi - armed bandits have primarily been studied in an adversarial setting , with the goal being to minimize the regret ( see  @xcite for a nice overview ) . a typical guarantee in these settings",
    "is that the _ total regret _",
    "after @xmath19 steps grows as @xmath20 where @xmath21 is the number of alternatives , assuming the partial information model ( i.e. only the reward for the alternative that is actually played is revealed ) , which corresponds well to our setting .",
    "these results assume no prior beliefs , unlike our decision theoretic framework .",
    "however , the regret based bounds in the adversarial setting are meaningless unless @xmath22 .",
    "the decision theoretic framework which has a rich history ( starting perhaps with wald s work in 1947  @xcite ) is more suited to the situation where the number of exploration steps is drastically limited , as is often the case .",
    "a typical setting , for example , is one where an advertiser that can advertise on 100,000 possible phrases and is willing to pay for 100 clicks to decide which keyword attracts visitors that convert into paid customers .",
    "so a traditional regret based bound may not be very meaningful in this setting .",
    "notably , glazebrook and garbe show that if you assume the added constraint that the transition matrix for each arm is irreducible , then choosing arms according to the gittins index in this environment is within an additive @xmath23 of optimal .    in section  [ sec : ratio ]",
    "we define the budgeted learning problem and the ratio index , and prove that the ratio index is a constant factor approximation to the budgeted learning problem .",
    "section  [ sec : relate ] establishes that the gittins and ratio indices are constant factor approximations of each other .",
    "we also show here that playing the arm with the largest gittins index ( with a suitable discount factor ) , gives a constant factor approximation to the budgeted learning problem .",
    "section  [ sec : multi - armed ] presents index - based policies for finite horizon and discount oblivious versions of the multi - armed bandit problem . in section  [ sec : compute ] , we present a strongly polynomial algorithm to compute the ratio index as well as several useful insights into its structural properties .",
    "_ we are given @xmath24 arms .",
    "arm @xmath25 has state space @xmath26 , with initial state @xmath27 . experimenting on an arm @xmath25 in state @xmath28 results in the arm entering state @xmath29 with known probability @xmath30 .",
    "the payoff of state @xmath31 is given as @xmath32 .",
    "given an experimentation budget @xmath5 , we are interested in finding the optimal policy , @xmath33 , so that @xmath34 $ ] is maximum among all policies , where @xmath35 is the state of arm @xmath25 after the policy has been executed ( the number of experiments can not exceed @xmath5 ) .",
    "_    we will use @xmath19 to denote @xmath36 . for convenience",
    ", we will assume that the @xmath26 are disjoint and that @xmath37 if @xmath31 and @xmath38 are in the state spaces of different bandit - arms ; this can be easily enforced by duplicating any shared states .",
    "the initial states represent a prior belief on the payoff from the bandit - arms .",
    "we will assume that the expected payoff is a martingale , i.e. , @xmath39 ; the martingale assumption is crucial to our results .",
    "we will also assume without loss of generality that the state space of any arm is acyclic and truncated at depth @xmath5 .",
    "the martingale property has some useful and easy consequences which we will use repeatedly :    1",
    ".   for an arbitrary policy let @xmath40 denote its expected payoff if it is terminated after @xmath14 experiment steps .",
    "then , @xmath40 is non - decreasing in @xmath14 .",
    "in other words , extra experiments can never hurt .",
    "2 .   given a _",
    "single arm _",
    ", no policy can have a higher expected payoff than the one which does no exploration and simply chooses the initial state as the winner ; in other words , extra experiments can never help given just one arm .",
    "the proof of the following theorem is deferred to appendix  [ append : ratio ] .",
    "it is conceivable ( though not obvious to us ) that this theorem can also be obtained via the `` indexability characterization '' of @xcite . in any case",
    ", the proof is quite elementary and provides useful intuition .",
    "[ thm : noindexbudget ] there is no exact index for the budgeted learning problem .",
    "we will now define the ratio index , which is an approximate index for this problem . at any given time , the _ current state _ of the system is denoted by @xmath41 , which captures the current states of all the arms , and the budget left ( i.e. the number of experimentation steps that are still remaining ) , @xmath42 .",
    "the initial state of the system has all the arms in their initial states , and @xmath43 .",
    "since we use the term state for both the system and an arm , we will disambiguate where necessary by referring to these as `` system - state '' and `` arm - state '' respectively .",
    "a policy @xmath44 is a function which takes as input a system - state @xmath45 and either returns an arm @xmath25 for experimentation ( i.e. explores the arm - state @xmath46 ) , or chooses an arm @xmath25 as a winner and terminates ( i.e. exploits the arm - state @xmath46 ) , or simply terminates ( abandons ) . if @xmath47 then the only options are to abandon or exploit .",
    "the martingale property ( see the comment at the end of section  [ sec : ratiodef ] ) implies that there always exists an optimal policy which explores some arm iff @xmath48 and exploits some arm iff @xmath47 .",
    "we now introduce two vectors @xmath49 and @xmath50 . the probability that arm - state @xmath31 is the final exploited state by policy @xmath44 is given by @xmath51 .",
    "the probability that arm - state @xmath31 is explored by policy @xmath44 is given by @xmath52 .",
    "we define the _ cost _ of policy @xmath44 as @xmath53 observe that @xmath54 , for any policy @xmath44 .",
    "the profit of policy @xmath44 is defined as @xmath55 observe that our definition of policy is an adaptive one ; the decisions made in step @xmath56 depend on the entire system - state at time @xmath57 and hence on the outcome of previous experimentation steps .",
    "further , it is easy to see that randomized strategies can not do any better than deterministic strategies .",
    "if we drop the requirement that a policy must either exploit or abandon when the remaining budget @xmath42 is 0 , we obtain what we call a _ pseudo - policy_. a single arm policy is one which makes all its decisions based only on the state of a single pre - determined arm @xmath25 , ignoring all other arms .",
    "we are now ready to define the ratio index and prove that it leads quite naturally to an approximation of the budgeted learning problem . + * ratio index .",
    "* _ the ratio index @xmath58 of a bandit - arm ( say arm @xmath25 ) in initial state @xmath31 and with experimentation budget @xmath5 , is defined as @xmath59 where the @xmath60 is over all single arm pseudo - policies @xmath44 which have initial arm - state @xmath31 , budget @xmath5 , state space @xmath26 , and cost @xmath61 .",
    "we refer to a policy which yields the ratio index as a ratio index policy for state @xmath31 , denoted @xmath62 .",
    "_    even though we allow pseudo - policies in the definition of the ratio index , any ratio index policy respects the budget constraint :    [ lem : ratio - not - pseudo ] any ratio index policy for state @xmath31 has cost at most @xmath8 .",
    "because of the martingale property , no single arm policy starting from arm - state @xmath31 can obtain profit more than @xmath32 .",
    "hence , any single arm policy @xmath44 that has cost more than 1 must have a smaller ratio ( of expected profit to expected cost ) than the single arm policy which exploits in state @xmath31",
    ".    * greedy algorithm . *",
    "_ suppose the initial experimentation budget is @xmath5 , and the current system - state is given by @xmath63 .",
    "if @xmath48 , the greedy algorithm explores the arm @xmath25 with the maximum ratio index , @xmath64 , with ties broken arbitrarily but consistently . if @xmath47 the greedy algorithm exploits the arm @xmath25 with maximum current expected reward @xmath65 .",
    "we denote the greedy algorithm by @xmath66 .",
    "_    * note : * the greedy algorithm uses the same @xmath5 at every step to compute the ratio index .",
    "hence , given a table of the ratio index of every state in @xmath19 ( which can be pre - computed efficiently as specified in the section  [ sec : compute ] ) , we can implement this algorithm using a simple min - heap and the complexity of each step would be just @xmath67 , which is much better than solving a coupled lp with @xmath68 variables .",
    "we now show that the greedy algorithm gives an @xmath0-approximation to the budgeted learning problem .",
    "[ lem : ratio - increases ] a ratio index policy for arm - state @xmath31 , @xmath62 , does not abandon any arm - state @xmath38 with @xmath69 and does not explore or exploit any arm - state @xmath38 with @xmath70 .",
    "the proof of the above lemma is deferred to appendix  [ append : calc ] ( as corollary  [ cor : ratioincrease ] ) .",
    "now consider the following algorithm , which we call the _ persistent _ algorithm , denoted @xmath71 :    * the persistent algorithm * @xmath72 : : :    given a system - state @xmath73 , let @xmath25 be    the arm with the highest ratio index @xmath74 where    @xmath46 denotes the current state of arm @xmath25 .",
    "play arm @xmath25 in accordance with the policy    @xmath75 until the policy chooses to exploit or    abandon .",
    "if @xmath75 abandons , let    @xmath76 be the resulting system - state .",
    "repeat the    process starting with @xmath76 .",
    "if at any time , the    system - state is such that @xmath47 , immediately exploit    the arm that has the highest current ratio index .",
    "observe that as for the greedy algorithm , the ratio index used by the persistent algorithm @xmath71 is computed using a fixed budget @xmath5 ; the number of remaining exploration steps @xmath42 is used only to terminate @xmath71 .",
    "[ lem : greedy - persistent ] the expected profit of the greedy algorithm @xmath66 is at least as much as the expected profit of the persistent algorithm @xmath71 .",
    "couple the greedy and the persistent algorithms such that if they both explore an arm in a given state , that arm transitions to the same state for both strategies .",
    "let @xmath77 denote the sequence of arms explored by @xmath71 before exploitation ; here @xmath78 since @xmath71 can exploit early .",
    "let @xmath79 denote the set of arms explored by @xmath66 .",
    "by lemma  [ lem : ratio - increases ] , we can conclude that @xmath80 is a prefix of @xmath81 . by the martingale property",
    ", early termination can never result in increased profit ; the lemma follows .",
    "thus it suffices to analyze @xmath71 . given two single arm pseudo - policies @xmath82 and @xmath44 for an arm @xmath25 , we say that @xmath83 , if for all arm - states @xmath84 for which @xmath44 explores ( exploits ) arm @xmath25 , @xmath82 also explores ( exploits ) the arm @xmath25 .",
    "notice that @xmath82 might choose to continue exploration / exploitation when @xmath44 abandons an arm - state .",
    "informally , @xmath83 means that policy @xmath82 can be played after policy @xmath44 has been played to completion .",
    "we will now state a useful technical lemma ; the proof is in appendix  [ append : ratio ] :    [ lem : submodular ] given two arbitrary single arm pseudo - policies @xmath44 , @xmath85 for arm @xmath25 in initial arm - state @xmath31 , there exists another single arm pseudo - policy @xmath82 starting in the same initial arm - state @xmath31 such that , ( 1 ) @xmath83 , ( 2 ) @xmath86 , and ( 3 ) @xmath87 .    the above property is akin to submodularity .",
    "we now state our main theorem , which says that the greedy algorithm @xmath66 gives a constant factor approximation to the optimal policy .",
    "let @xmath88 denote the expected profit obtained by strategy @xmath44 for the budgeted learning problem run with budget @xmath5 and initial system - state @xmath89 , and let @xmath90 denote the expected profit obtained by an optimum strategy with the same parameters .",
    "we will omit the system - state when it is the same for all the strategies involved .",
    "[ thm : ratio ] @xmath91    from lemma  [ lem : greedy - persistent ] , it suffices to analyze the persistent algorithm @xmath71 rather than the greedy algorithm @xmath66 .",
    "we divide the persistent algorithm into stages , starting from stage 1 .",
    "let @xmath92 be the arm with the highest ratio index at the beginning of stage 1 ( and hence the arm that will be played by @xmath71 at the first step ) . since the arms evolve probabilistically , the first stage ( as well as subsequent stages ) will result in a distribution over system - states .",
    "let @xmath93 denote the system - state at the start of stage @xmath57 , and let @xmath94 denote the distribution of @xmath93 .",
    "let @xmath95 be the arm - state with the highest ratio index among the arm - states which have a non - zero probability , say @xmath96 , in @xmath94 , and let @xmath97 be the corresponding arm .",
    "the @xmath57-th stage of @xmath71 is to simply move to the next stage if the arm @xmath97 is not in state @xmath95 ( which happens with probability @xmath98 ) ; we call this stage `` empty '' in this case .",
    "if the arm @xmath97 is in state @xmath95 , then the @xmath57-th stage of @xmath71 is to mimic an optimum ratio index policy for state @xmath95 . if the exploration budget gets exhausted during this mimicking process , then the @xmath57-th stage exploits arm @xmath97 right away and the policy terminates",
    "; the cost of the extra exploitation is not charged to this stage of the policy . by the martingale property , this early termination can only increase the expected profit of the @xmath57-th stage .",
    "if the @xmath57-th stage exploits an arm , then the persistent algorithm terminates as well .",
    "let @xmath99 denote the policy corresponding to the @xmath57-th stage .",
    "let @xmath100 and @xmath101 be the cumulative expected profit and expected cost of the first @xmath57 stages .",
    "use @xmath102 and @xmath103 to denote the expected profit and the expected cost of the @xmath57-th stage , conditioned on this stage being played ( i.e. the @xmath57-th stage being non - empty and the persistent algorithm not terminating before reaching the @xmath57-th stage ) .",
    "the following statement is a corollary of lemma  [ lem : submodular ] : the proof is a digression from the current theorem and is deferred to appendix  [ append : ratio ] .",
    "[ cor : incremental ] at the beginning of stage @xmath57 , there exists a single arm pseudo - policy with profit to cost ratio at least @xmath104 .",
    "since the persistent algorithm follows an optimum ratio index policy we are guaranteed that @xmath105 . by markov",
    "s inequality , the probability that the budget has not been exhausted before stage @xmath57 starts is at least @xmath106 . also , recall that @xmath96 is the probability that the @xmath57-th stage is non - empty .",
    "the expected unconditioned profit of the @xmath57-th stage is at least @xmath107 .",
    "the expected unconditioned cost of the @xmath57-th stage is at most @xmath108 .",
    "hence , we get @xmath109 thus , the profit obtained by the persistent algorithm is more than the one attained by the following differential process , where @xmath110 is the cumulative profit and @xmath111 is the cumulative cost , and @xmath112 ( view the process as increasing the expected cost from @xmath9 to @xmath8 ) : @xmath113 integrating from @xmath114 to @xmath115 , we get that the expected profit is at least @xmath116    thus , we have shown the existence of a simple index which yields almost as good an approximation ratio as the lp - based approach of guha and munagala .",
    "the results above assume that each exploration step has the same cost , but can easily be extended to the weighted exploration cost case .",
    "we can also modify the proof slightly to obtain the following corollary :    [ cor : scale ] @xmath117 .",
    "combining corollary  [ cor : scale ] and theorem  [ thm : ratio ] , we obtain the following corollary :    [ cor : ratio - scale ] @xmath118 .",
    "we will use @xmath119 to denote a standard ( stationary ) bandit - arm with a fixed reward of @xmath42 .",
    "we will use @xmath120 to denote a given bandit - arm in some initial state @xmath31 .",
    "a _ gittins index strategy _",
    "@xmath121 takes as input an arm @xmath120 with an unknown reward distribution ( but a known initial state ) and a standard bandit - arm @xmath119 for some @xmath122 , and gives a strategy for maximizing the discounted reward for a multi - armed bandit with @xmath120 and @xmath119 as its two bandit - arms .",
    "thus each node in the decision tree of @xmath121 is labeled as playing either the given arm @xmath120 or the standard bandit @xmath119 .",
    "we can assume w.l.o.g .",
    "that once the strategy @xmath121 plays the standard bandit at a node in the tree , it plays it forever from here onwards .",
    "gittins index _ of an arm @xmath120 is defined to be the least @xmath42 such that the gittins index strategy with input arms @xmath120 and @xmath119 is indifferent between playing either one of them at time @xmath9 .",
    "we will assume @xmath31 to be the initial state of @xmath120 in the remainder of this section , and drop its explicit mention .",
    "let @xmath123 denote the ratio index for @xmath120 when the horizon is limited to @xmath5 .",
    "let @xmath124 denote the gittins index for @xmath120 when the discount factor is uniform for some @xmath125 .",
    "the following lemmas show that the gittins and the ratio indices are constant factor approximations of each other .",
    "the proofs involve transforming the gittins index strategy to the ratio index strategy ( and vice verse ) and are in appendix  [ append : relate ] .",
    "[ lem : relate1 ] for any @xmath126 , @xmath127 where @xmath128 .",
    "thus as @xmath129 , @xmath130 .",
    "[ lem : relate2 ] for any @xmath126 , @xmath131 where @xmath132 .",
    "[ lem : relate3 ] let @xmath133 denote the gittins index of arm @xmath25 at time @xmath14 , where the discount factor @xmath17 is @xmath7 . playing the arm with the highest value of @xmath133 for @xmath134 and then picking the arm with the highest expected payoff at time @xmath14 results in a constant factor approximation to the budgeted learning problem .    while the constants in our proofs are large , the algorithms are simple and intuitive . for instance , schnieder and moore  @xcite and madani , lizotte and greiner",
    "@xcite have studied the policy defined in lemma  [ lem : relate3 ] and other similar policies , and found that they often work well in practice .",
    "we will now prove the lemmas from section  [ sec : relate ] .",
    "_ ( same as lemma  [ lem : relate1 ] ) _ for any @xmath126 , @xmath255 where @xmath128 .",
    "thus as @xmath256 , @xmath130 .    consider a strategy @xmath257 that computes the ratio index @xmath123 for arm @xmath120 with horizon limited to @xmath5 .",
    "let @xmath258 .",
    "we will modify the strategy @xmath257 in to another strategy @xmath259 that will certify that the gittins index for arm @xmath120 is at least @xmath260 for some suitable constant @xmath261 .",
    "the decision tree for strategy @xmath259 is obtained by modifying the decision tree for @xmath257 as follows :    1 .",
    "when the decision tree for strategy @xmath257 visits a node labeled `` abandon '' , the tree for @xmath259 changes it to playing the standard bandit - arm @xmath262 and terminates this branch of the tree .",
    "2 .   when the decision tree for strategy @xmath257 visits a node labeled `` exploit '' , the tree for @xmath259 replaces it to playing the arm @xmath120 forever .",
    "let @xmath263 denote the expected reward of the bandit - arm @xmath120 conditioned on exploitation by the strategy @xmath257 . also , let @xmath264 and @xmath265 denote the exploration and the exploitation budget , respectively , of the strategy @xmath257 .",
    "thus , by definition , the ratio index is given by    @xmath266    we can now lower bound the difference @xmath267 in the expected reward generated by the strategy @xmath259 over the standard arm @xmath262 to be at least    @xmath268    substituting @xmath269 , we get    @xmath270    thus @xmath271 if @xmath272 , establishing that @xmath273 .    _",
    "( same as lemma  [ lem : relate2 ] ) _ for any @xmath126 , @xmath274 where @xmath275 .",
    "let @xmath276 and @xmath277 . the optimum discounted reward strategy given a bandit - arm @xmath120 and the standard bandit - arm @xmath278 is indifferent at time 0 between playing the two bandit - arms .",
    "let @xmath257 be a gittins index strategy for arm @xmath120 and the standard bandit - arm @xmath278 such that @xmath257 plays @xmath120 at time 0 .",
    "each node in the decision tree of @xmath257 is labeled as either playing the standard bandit @xmath278 or playing the arm @xmath120 .",
    "we will modify the strategy @xmath257 into another strategy @xmath259 that will certify that the ratio index for arm @xmath120 is at least @xmath279 when horizon is restricted to @xmath5 ; here @xmath280 is a suitable constant to be specified later . note that unlike strategy @xmath257 , the strategy @xmath259 does not have access to the standard bandit @xmath278 .",
    "however , since the standard bandit @xmath278 is in a stationary state , when we say a state @xmath281 in @xmath257 , we will assume that it refers only to the state of the arm @xmath120 .",
    "the decision tree for strategy @xmath259 is obtained by modifying the decision tree for @xmath257 as follows :",
    "when the decision tree for strategy @xmath257 visits a state that is labeled as playing @xmath278 and the depth is at most @xmath5 , the strategy @xmath259 abandons . 2 .   when the decision tree for strategy @xmath257 visits a state @xmath281 labeled as playing the arm @xmath120 , then 1",
    "if the depth is less than @xmath5 , then the strategy @xmath259 explores if the expected reward of _ playing the arm @xmath120 in state @xmath281 _ is less than @xmath282 , and it exploits otherwise .",
    "if the depth is exactly @xmath5 , strategy @xmath259 exploits .    clearly , the strategy @xmath259 as defined above has its horizon restricted to @xmath5",
    ".    we will use @xmath283 and @xmath284 to denote respectively the label and depth of a state @xmath281 in the decision tree of strategy @xmath259 .",
    "a state @xmath281 is assigned a label of `` r '' if @xmath259 explores in this state , and a label of `` t '' if it exploits in this state .",
    "finally , we will denote the probability of reaching a state @xmath281 in @xmath257 by @xmath285 , and the reward at a state @xmath281 in the strategies @xmath257 and @xmath259 by @xmath286 and @xmath287 respectively . note that @xmath288 for any state @xmath281 where strategy @xmath257 plays the arm @xmath120 .",
    "the expected reward @xmath263 collected by @xmath259 , as well as the exploration and the exploitation budget of @xmath259 , denoted by @xmath264 and @xmath265 respectively , are given by    @xmath289    we now lower bound the total reward _ deficit _ that is accumulated by the strategy @xmath257 in the states that are labeled as `` exploring '' in the strategy @xmath259 , as compared to the reward of playing the standard bandit @xmath278 .",
    "@xmath290    since @xmath291 for @xmath292 .    on the other hand , the total reward _ surplus _ accumulated by the strategy @xmath257 above the reward of playing the standard bandit @xmath278 in the subtrees of the states labeled as `` exploiting '' in @xmath259 , can be upper bounded by    @xmath293    the bound above follows from the martingale property which implies that for any state @xmath281 , we must satisfy the relationship @xmath294 . to see this ,",
    "observe that for any state @xmath281 , by martingale property we have @xmath295 , where @xmath296 denotes the set of descendants of a states @xmath281 , and @xmath297 denotes the probability of transitioning from state @xmath281 to state @xmath38 when arm @xmath120 is played in states @xmath281 .",
    "on the other hand , @xmath298    since the total reward surplus in @xmath257 w.r.t .",
    "the standard bandit @xmath278 must be at least as large as the total reward deficit w.r.t .",
    "to @xmath278 , it follows that    @xmath299    thus    @xmath300    let @xmath301 , and @xmath302 . clearly ,    @xmath303    finally , we observe that the gittins index policy @xmath257 never plays the bandit - arm @xmath120 in a state @xmath281 if it ever plays the standard bandit in any ancestor of state @xmath281 . therefore , for any @xmath304 ,",
    "@xmath305    averaging over all @xmath306 , we get    @xmath307    we can now lower bound the ratio index @xmath308 by the expected reward to exploration and exploitation budget ratio achieved by the policy @xmath259 :    @xmath309    _ ( same as lemma  [ lem : relate3 ] ) _ let @xmath133 denote the gittins index of arm @xmath25 at time @xmath14 , where the discount factor @xmath17 is @xmath7 . playing the arm with the highest value of @xmath133 for @xmath134 and then picking the arm with the highest expected payoff at time @xmath14 results in a constant factor approximation to the budgeted learning problem .",
    "consider the strategy @xmath310 described in the proof of lemma  [ lem : relate2 ] above . by the proof of lemma  [ lem : relate2 ]",
    ", the profit to cost ratio of this strategy is @xmath311 , which by lemma  [ lem : relate1 ] is @xmath312 .",
    "we will call @xmath310 the truncated gittins policy . using the truncated gittins policy instead of the ratio index policy in the proof of theorem  [ thm : ratio ]",
    ", we can conclude that repeatedly choosing the arm with the highest gittins index and playing the truncated gittins policy for this arm ( the entire policy , not just for a single step ) till it abandons or exploits gives a constant factor approximation to the budgeted learning problem .",
    "we will call this the tg algorithm .    now consider the strategy which repeatedly plays the arm with the highest gittins index for one step and",
    "then at time @xmath5 , picks the arm with the highest payoff ; we will call this the gittins strategy .",
    "the gittins strategy is identical to the tg algorithm as long as the tg algorithm continues exploring some arm .",
    "the tg algorithm may choose to exploit before the entire budget is exhausted , in which case making an arbitrary set of additional exploration steps can not hurt ( by the martingale property ) .",
    "the tg algorithm may also choose to exploit an arm with a smaller current expected payoff than some other arm ; again , choosing the arm with the highest expected current payoff can not hurt . in either case , the expected profit of the gittins strategy is no worse than that of the tg algorithm , and hence , the gittins strategy is also a constant factor approximation to the budgeted learning problem .",
    "in the traditional multi - armed bandit problem , we are given a fixed discount factor @xmath135 and allowed to play one arm at each time .",
    "if the reward at time @xmath14 is @xmath136 then the total discounted reward is @xmath137 . always playing the arm with the currently highest gittins index maximizes the expected total discounted reward ; however the gittins index of an arm depends crucially on the parameter @xmath17 . in this section",
    ", we discuss both _",
    "finite horizon _ and _ discount oblivious _ versions of the multi - armed bandit problem",
    ".    in the finite horizon multi - armed bandit problem , we are given a fixed number of steps , @xmath5 , as in the budgeted learning problem .",
    "however , unlike the budgeted learning problem , the objective of the finite horizon problem is to maximize the total ( undiscounted ) expected reward obtained _ during _ the first @xmath5 steps .",
    "this models many important problems such as optimally placing bets with a fixed number of chips , and optimally assigning impressions to advertisers  @xcite .",
    "in the discount oblivious multi - armed bandit problem , we want to find a strategy that provides a constant factor approximation to the optimum reward for all @xmath135 _ simultaneously_. it is not clear up front that such a strategy exists . in fact , we will allow the discounts to be even more general .",
    "let @xmath138 be an infinite sequence of discount factors that satisfies the property @xmath139 and where @xmath140 as @xmath141 .",
    "we will call such a sequence a discount factor sequence .",
    "let the system - state @xmath89 denote the vector of all the arm - states .",
    "we will use @xmath142 to denote the total expected discounted reward of any strategy @xmath44 for discount factor sequence @xmath143 starting from system - state @xmath89 . if strategy @xmath44 obtains reward @xmath136 at time @xmath14 when started in initial system - state @xmath144 , then @xmath145 . setting @xmath146 leads to the standard multi - armed bandit problem . setting @xmath147 for @xmath148 and @xmath149",
    "otherwise leads to a fixed horizon problem where we only get the reward from the first @xmath5 time steps .",
    "we will use @xmath150 to denote the total ( undiscounted ) expected reward over a window of @xmath5 steps of any strategy @xmath44 , starting from @xmath89 .",
    "we will use @xmath151 , @xmath152 to denote the optimum values for the two problems .",
    "we will omit the parameter @xmath89 when it is the same for all strategies under discussion .",
    "all proofs are deferred to appendix  [ append : multi - armed ] .",
    "recall ( from section  [ sec : ratio ] ) that @xmath153 and @xmath154 denote the expected profit of the greedy algorithm ( which always explores the arm with the largest ratio index ) and the optimum strategy respectively , for the budgeted multi - armed bandit problem .",
    "we first relate the budgeted learning and finite horizon problems :    [ lem : fvsr ] for any positive integer @xmath5 , we have @xmath155    we will now define two index - based strategies for the finite horizon problem , assuming horizon @xmath5 and initial system - state @xmath45 : .    1 .   for the first @xmath156 steps , play the arm with the highest ratio index , where the ratio index is computed assuming a budget of @xmath157 . for the remaining @xmath158 steps , play the arm with the highest expected reward .",
    "we will denote this strategy as @xmath159 since it switches from using the ratio index ( in the first half ) to using the expected profit as an index in the second half . 2 .",
    "similarly , define @xmath160 as the strategy which plays the arm with the highest gittins index ( assuming a discount factor @xmath161 ) during the first @xmath156 steps , and then switches to using the arm with the highest expected reward .",
    "observe that both strategies use an index at each step , and the choice of index does not depend on the state of the system ; it only depends on the time step . as before",
    ", we will omit the system - state parameter @xmath45 when is it is the same for all strategies under discussion .",
    "@xmath162 [ thm : switchr ]    combining lemma  [ lem : relate3 ] with the proof of theorem  [ thm : switchr ] , we obtain :    [ thm : switchg ] @xmath163    to the best of our knowledge ,",
    "this is the first index for the finite horizon problem with provable approximation guarantees",
    ". it would be interesting to obtain a smooth version of @xmath164 which does not need to make the discrete jump from a discount factor of @xmath165 in the first half to playing the arm with the highest expected reward ( i.e. to a discount factor of 0 ) in the second half .",
    "we will first establish a connection between the discount oblivious and finite horizon problems and then use this connection to obtain a simple index - based approximation algorithm for the discount oblivious problem .",
    "[ lem : equivalent - oblivious ] for any @xmath10 , a strategy gives a @xmath10-approximation simultaneously for all discount factor sequences @xmath143 iff it gives a @xmath10-approximation simultaneously to the fixed horizon problems with all horizons @xmath166 .",
    "let @xmath167 be the following discount oblivious strategy : play in sequence the strategies @xmath168 @xmath169 @xmath170 @xmath171 , where each @xmath172 is started from the state of the system after time @xmath173 , denoted @xmath174 ; this is the state in which the arms are left by the previous @xmath175 strategy . @xmath176 is the initial state of the system .    like @xmath175 ,",
    "@xmath167 is also an index - based strategy ; the index used at any time step @xmath14 depends only on @xmath14 .",
    "analogously , @xmath177 plays the sequence @xmath178 @xmath179 @xmath180 @xmath181 .",
    "since the state of the system at the start of @xmath182 depends on the outcomes of the previous steps , the following technical lemma , which is an easy consequence of the martingale property , will be useful .",
    "this lemma states that performing an arbitrary sequence of extra explorations at the beginning can not hurt the optimum solution for the budgeted learning problem .",
    "observe that the state @xmath183 is itself a random variable in this lemma ; the expectation is over all values of @xmath183 .",
    "[ lem : restart ] let @xmath184 be any arbitrary finite sequence of explorations starting from system - state @xmath45 .",
    "let @xmath183 be the system - state at the end of @xmath184 .",
    "let @xmath185 be an optimum @xmath5 step strategy for the budgeted learning problem starting from the system - state @xmath183",
    ". then @xmath186 \\ge b^*(h,\\mathbf{s})$ ] .    using the above lemma",
    ", we can show the following :    [ lem : obliviousr ] for any positive integer @xmath187 , the expected reward of the discount oblivious strategy @xmath167 in the first @xmath5 steps is @xmath188 .",
    "[ lem : obliviousg ] for any positive integer @xmath187 , the expected reward of the discount oblivious strategy @xmath177 in the first @xmath5 steps is @xmath188 .",
    "invoking lemma  [ lem : equivalent - oblivious ] now gives us :    [ thm : oblivious ] strategies @xmath189 and @xmath190 both give a constant factor approximation to the multi - armed bandit problem simultaneously for all discount factor sequences @xmath143 .",
    "we will now provide the proofs from section  [ sec : multi - armed ] .",
    "lemma  [ lem : fvsr ] consider the fixed horizon strategy ( for horizon @xmath5 ) that first solves the budgeted learning problem with budget @xmath156 and then exploits the winner from this budgeted learning problem for the remaining @xmath313 time steps .",
    "this strategy has expected pay off at least @xmath314 and hence , @xmath315 .",
    "now consider the budgeted learning strategy ( with budget @xmath5 ) that emulates the optimum fixed horizon strategy ( for horizon @xmath5 ) but only for @xmath14 steps , where @xmath14 is an integer chosen uniformly at random from the set @xmath316 , and then declares the arm that the optimum fixed horizon strategy was about to play in step @xmath317 as the winner .",
    "this budgeted learning strategy has expected payoff exactly @xmath318 and hence , @xmath319    theorem  [ thm : switchr ] assume , for simplicity , that @xmath5 is even ; the odd case is very similar .",
    "now , @xmath320}\\\\      & = & \\omega(f^*(h ) )   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\      \\mbox{[from lemma~\\ref{lem : fvsr } ] }    \\end{aligned}\\ ] ]    lemma  [ lem : equivalent - oblivious ] the `` only if '' part is trivial since a fixed horizon problem can be modeled using a discount factor sequence , as explained earlier .",
    "for the `` if '' part , consider a strategy @xmath44 that offers expected reward @xmath136 at time @xmath14 .",
    "then the expected discounted reward , @xmath321 of @xmath44 given a discount factor @xmath143 can be written as @xmath322 .",
    "observe that @xmath323 by the definition of a discount factor sequence .",
    "if we simultaneously @xmath10-approximate each of the finite horizon rewards @xmath324 we also simultaneously @xmath10-approximate any non - negative linear combination , and hence the optimum expected discounted reward for any discount factor sequence .",
    "lemma  [ lem : obliviousr ] if @xmath325 , @xmath326 simply plays the arm with the highest expected profit , and hence obtains profit @xmath327 . if @xmath328 , it is easy to see that @xmath329 and hence the lemma holds .",
    "we will now assume that @xmath330 .",
    "let @xmath14 be the largest power of 2 which is no larger than @xmath331 ; hence @xmath332 .",
    "since @xmath333 , we have @xmath334 .",
    "the strategy @xmath167 is guaranteed to execute the strategy @xmath335 sometime during the first @xmath5 steps ( in fact during steps @xmath336 ) . using lemma  [ lem : restart ]",
    ", we can repeat the steps in the proof of theorem  [ thm : switchr ] to claim that @xmath337    = \\omega(f^*(t,\\mathbf{s_0 } ) ) = \\omega(f^*(h,\\mathbf{s_0}))$ ] .",
    "the proof of lemma  [ lem : obliviousg ] is similar to the above proof .",
    "we will now sketch how the ratio index can be computed . in the process",
    ", we will also get several useful insights into its structural properties . given a single bandit - arm @xmath25 , an initial state @xmath191 for @xmath25 , an exploration budget of @xmath5 , and a state space @xmath26 truncated to depth @xmath5 , we view @xmath26 as a layered dag of depth @xmath5 , which is to say that for any arm - state , @xmath31 , in layer @xmath57 , if @xmath192 , then @xmath38 must be in layer @xmath193 . as explained in section  [ sec : compalg ] , this is without loss of generality .",
    "we let @xmath194 be the number of nodes in the layered dag .",
    "additionally , for any state @xmath31 in @xmath26 , we use @xmath195 to denote the sub - dag of @xmath26 with root @xmath31 ; thus @xmath196 .    for the purposes of this section",
    ", we require the use of _ randomized single arm policies_. whereas a _ deterministic _ single arm policy ( corresponding to arm - state @xmath38 ) will always either explore @xmath38 , exploit @xmath38 , or abandon with probability 1 , a randomized policy , @xmath44 , selects @xmath197 where @xmath198 represents the probability @xmath44 explores in this state , @xmath199 represents the probability @xmath44 exploits in this state , and @xmath200 represents the probability @xmath44 abandons in this state .",
    "the vectors @xmath49 and @xmath50 are defined for randomized policies as for deterministic policies , as are the profit @xmath201 and cost @xmath202 of the policy . our approach below will calculate the ratio index @xmath58 for all @xmath28 as well as the entire _ profit curve _",
    "@xmath203 for all @xmath31 where @xmath204 where the @xmath60 is over all randomized single arm policies @xmath44 with initial state @xmath31 and @xmath205 .",
    "we show that there exists a deterministic policy that induces the maximum @xmath206 over all @xmath207 ( and in fact our algorithm will find such a policy ) .",
    "thus , the value @xmath208 is the ratio index for @xmath31 given @xmath5 , i.e. , @xmath58 .",
    "our algorithm relies heavily upon the following theorem on the structure of the profit curve .",
    "[ thm : profitcurvesegments ] the profit curve , @xmath203 , for any given state @xmath31 is concave and piecewise linear with at most @xmath209 segments where @xmath210 represents the number of states in @xmath195 .    the proof of this theorem involves several steps and is deferred to appendix  [ append : calc ] . towards proving the theorem ,",
    "we show that as the budget increases along the profit curve for @xmath31 , a _",
    "monotonicity property _ holds that for every state @xmath211 , both @xmath199 and @xmath212 are non - decreasing .    [",
    "lem : mono ] for any @xmath213 , with @xmath214 , there exist optimal solutions @xmath215 and @xmath216 to @xmath217 and @xmath218 respectively such that @xmath219 and @xmath220 for all @xmath38 in @xmath195 .",
    "we further characterize the intersection of line segments of the profit curve as `` corner '' solutions and show that at these points @xmath221 and @xmath222 for _ all _ states in @xmath195 .",
    "thus , these points of the curve are induced by deterministic policies .",
    "thus , the policy which induces the `` corner '' solution at the end of the first segment of the profit curve is a deterministic ratio index policy .",
    "the algorithm for computing the profit curve ( and hence the ratio index ) involves recursively calculating the profit curve for a state @xmath31 given the profit curves for all of its successor states .",
    "we begin by constructing an _ exploration profit curve _ for @xmath31 , @xmath223 , which denotes the optimal profit for any given cost conditioned on the fact that we are exploring at @xmath31 ( i.e. @xmath224 ) .",
    "we then take the concave envelope over this curve combined with the abandonment policy and the exploitation policy .",
    "figure  [ fig : curves ] in appendix  [ append : calc ] shows a typical example of the relationship between these two curves .",
    "superficially , it might seem that the number of segments of the profit curves could increase exponentially as we perform this process up the dag .",
    "however , theorem  [ thm : profitcurvesegments ] guarantees that the number of segments remains bounded and the entire curve for @xmath31 can be computed in time @xmath225 given the successor curves , where @xmath226 represents the maximum number of immediate descendants for any node .",
    "thus , this algorithm is strongly polynomial ( in @xmath194 ) for computing the entire profit curve of a state in the layered dag , and hence , the ratio index .",
    "if the underlying state space of the bandit - arm is an unlayered dag , we can make it layered by multiplying the number of states by at most @xmath194 , so the algorithm is still strongly polynomial in @xmath194 .",
    "if the underlying state space is not a dag , we can convert it into a layered dag by multiplying the number of states by at most @xmath5 .",
    "details of the algorithm and the analysis are in appendix  [ append : calc ] .",
    "the authors would like to thank rajat bhattacharjee and sudipto guha for helpful discussions , as well as anonymous referees for pointing us to the citations  @xcite .",
    "99 p. bhattacharya , l. georgiadis , and p. tsoucas .",
    "extended polymatroids , properties , and optimization . _ integer programming and combinatorial optimization , ipco2 _ , ed .",
    "e. bala , g cornnejols , and r. kannan , carnegie - mellon university 298 - 315 , 1992 .",
    "r. bellman . a problem in the sequential design of experiments .",
    "_ sankhia _ , 16:221 - 229 , 1956 .",
    "d. bertsimas and j. nino - mora .",
    "conservation laws , extended polymatroids and multi - armed bandit problems . _ mathematics of operations research _ , 21:257 - 306 , 1996 .",
    "d. bertsimas and j. nino - mora .",
    "restless bandits , linear programming relaxations , and a primal - dual index heuristic . _ operations research _ , 48:80 - 90 , 2000 .",
    "a. blum and y. mansour .",
    "learning , regret minimization , and equilibria .",
    "_ algorithmic game theory _",
    "n. nisan , t. roughgarden , e. tardos , and v. vazirani .",
    "79 - 102 , 2007 .",
    "m. charikar , c. chekuri , and m. pal . sampling bounds for stochastic optimization .",
    "_ approx - random _ 257 - 269 , 2005 .",
    "b. c. dean , m. x. goemans , and j. vondrak .",
    "approximation the stochastic knapsack problem : the benefit of adaptivity . _ focs 04 : proceedings of the 45th annual ieee symposium on foundations of computer science _ , 208 - 217 , 2004 .",
    "b. c. dean , m. x. goemans , and j. vondrak .",
    "adaptivity and approximation for stochastic packing problems .",
    "16th acm - siam symp . on discrete algorithms _ , 395 - 404 , 2005 .",
    "e. frostig and g. weiss .",
    "four proofs of gittins multiarmed bandit theorem .",
    "_ applied probability trust _ ,",
    "november 10 , 1999 .",
    "gittins and d. m. jones . a dynamic allocation index for the sequential design of experiments .",
    "_ progress in statistics : european meeting of statisticians , budapest , 1972 _ , ed .",
    "j. ganu , k. sarkadi , and i. vince .",
    "241 - 266 , 1974 .",
    "j. c. gittins .",
    "bandit processes and dynamic allocation indices .",
    "_ j royal statistical societe series b _ , 14:148 - 167 , 1979 .",
    "j. c. gittins . _ multiarmed bandits allocation indices _ , wiley , new york , 1989 .",
    "k. d. glazebrook and r. garbe .",
    "almost optimal policies for stochastic systems which almost satisfy conservation laws . _",
    "annals of operations research _ , 92:19 - 43 , 1999 . k. d. glazebrook and d. j. wilkinson .",
    "index - based policies for discounted multi - armed bandits on parallel machines . _",
    "the annals of applied probability _",
    ", 10:3:877 - 896 , 2000 .",
    "a. goel , s. guha , and k. munagala .",
    "asking the right questions : model - driven optimization using prbes .",
    "acm symp . on principles of database systems _ , 2006 .",
    "a. goel and p. indyk .",
    "stochastic load balancing and related problems .",
    "_ proc . symp . on foundations of computer science _ , 1999 .",
    "s. guha and k. munagala .",
    "model driven optimization using adaptive probes .",
    "acm - siam symp . on discrete algorithms _ , 2007 .",
    "s. guha and k. munagala .",
    "approximation algorithms for budgeted learning problems .",
    "_ stoc _ , 2007 .",
    "j. kleinberg , y. rabini , and e. tardos .",
    "allocation bandwidth for bursty connections .",
    "_ siam j. comput _",
    "30(1 ) , 2000 .    d. luenberger . _ linear and nonlinear programming _ , reading , ma , 1984 .",
    "o. madani , d. lizotte , and r. greiner . active model selection .",
    "_ proceedings of the 20th conference on uncertainty in artificial intelligence _ , 357 - 365 , 2004 .",
    "p. rusmevichientong and d. williamson .",
    "an adaptive algorithm for selecting profitable keywords for search - based advertising services .",
    "_ proceedings of the 7th acm conference on electronic commerce , _ 260 - 269 , 2006 .",
    "j. schneider and a. moore .",
    "active learning in discrete input spaces .",
    "_ proceedings of the 34th interface symposium _ , 2002 .",
    "d. shmoys and c. swamy .",
    "stochastic optimization is ( almost ) as easy as discrete optimization .",
    "45th symp . on foundations of computer science _ 228 - 237 , 2004 .",
    "p. tsoucas .",
    "the region of achievable performance in a model of klimov .",
    "research report rc16543 , ibm t.j .",
    "watson research center , yorktown heights , new york , 1991 .",
    "_ sequential analysis _",
    ", j. wiley & sons , new york , 212p , 1947 .",
    "p. whittle .",
    "restless bandits : activity allocation in a changing world .",
    "_ a celebration of applied probability .",
    "j. applied probability _",
    ", 25a:287 - 298 , 1988 .",
    "we will now provide the missing proofs from section  [ sec : ratio ] :    theorem  [ thm : noindexbudget ] consider three arms @xmath227 with @xmath1 priors : @xmath228 , @xmath229 , @xmath230 .",
    "assume the horizon @xmath5 is just 1 .",
    "first consider the scenario where arms @xmath231 and @xmath232 are the only ones present .",
    "observe that @xmath233 .",
    "so if we play arm @xmath232 once and the trial is a failure , arm @xmath232 will still be more profitable than arm @xmath231 .",
    "hence , playing arm @xmath232 gives an expected profit of @xmath234 since arm @xmath232 will be chosen as the winner regardless of the outcome .",
    "also , @xmath235 so if we play arm @xmath231 once and the trial is a success , arm @xmath231 becomes more profitable than arm @xmath232 .",
    "hence , exploring @xmath231 first gives an expected profit of @xmath236 .",
    "therefore , if there exists an index for the budgeted learning problem , the index of arm @xmath231 must be higher than that of arm @xmath232 .",
    "now consider the scenario where arms @xmath237 are all present . playing arm @xmath231 first gives the same expected profit as before : @xmath238 .",
    "if we play arm @xmath232 , and the trial is a failure , arm @xmath239 will be chosen as the winner , giving an expected profit of @xmath240 .",
    "hence , arm @xmath232 must have a higher index than arm @xmath231 , which is a contradiction .",
    "lemma  [ lem : submodular ] without loss of generality , assume that the state space @xmath26 is a tree .",
    "define @xmath82 as follows : @xmath82 makes the same choices as @xmath44 for all arm - states which are either explored or exploited by @xmath44 .",
    "this ensures that condition 1 in the lemma is satisfied .",
    "further , the total cost of @xmath82 on these states is merely the total cost of @xmath44 .    for an arm - state @xmath31 that is abandoned by @xmath44",
    ", @xmath82 does the following :    1 .",
    "if any ancestor of @xmath31 in @xmath26 gets abandoned by @xmath85 , then @xmath82 abandons @xmath31 .",
    "if any ancestor of @xmath31 gets exploited by @xmath85 , then @xmath82 exploits @xmath31 .",
    "else , @xmath82 makes the same choice as @xmath85 on @xmath31 and all the descendants of @xmath31 .    in order to prove condition 2 in the lemma",
    ", we have to bound ( by charging to @xmath85 ) the cost incurred ( by @xmath82 ) in exploring / exploiting those arm - states @xmath31 ( and their descendants ) that are abandoned by @xmath44 . in case ( a ) above , @xmath31 is abandoned and no extra cost is incurred . in case",
    "( b ) , let @xmath38 be the ancestor arm - state of @xmath31 that was exploited by @xmath85 .",
    "the cost @xmath241 of exploiting @xmath31 is the cost @xmath242 of exploiting @xmath38 times the probability of @xmath44 reaching @xmath31 conditioned on @xmath44 reaching @xmath38 .",
    "since the total probability that @xmath44 abandons a descendant of @xmath38 conditioned on @xmath44 reaching @xmath38 is at most 1 , the incremental cost for @xmath82 for all descendants of @xmath38 can be no more than @xmath242 and thus there can be no overcharging . in case ( c ) , the charging is quite straight - forward since @xmath82 just mimics @xmath85 .    in order to prove condition 3 in the lemma , consider any arm - state @xmath31 that is exploited by @xmath85 .",
    "if that state is exploited by @xmath44 , then it is also exploited by @xmath82 .",
    "if that state is explored by @xmath44 , then eventually , @xmath44 must either abandon or exploit a descendant along every path in the state space starting from @xmath31 .",
    "descendants that get abandoned by @xmath44 will get exploited by @xmath82 by property ( b ) . by the martingale property",
    ", the total profit obtained by @xmath82 from all the descendant states of @xmath31 is the same as the profit obtained by @xmath85 from @xmath31 .",
    "if the arm - state @xmath31 is abandoned or not reached by @xmath44 , then @xmath82 mimics @xmath85 according to property @xmath243 . hence , @xmath82 gets at least as much profit as @xmath85 .",
    "corollary  [ cor : incremental ] let @xmath33 denote the optimum policy . define a new policy that we call the restriction of @xmath33 to arm @xmath25 , denoted @xmath244 , as follows : @xmath244 follows @xmath33 when it explores / exploits arm @xmath25 , and simulates @xmath33 ( without really playing it ) on other states .",
    "a simple coupling argument shows that the total expected cost of all these single arm policies is equal to the expected cost of @xmath33 , and the total expected profit of all these single arm policies is equal to the expected profit of @xmath33 .",
    "similarly , let @xmath245 denote the greedy algorithm over the first @xmath246 stages , and let @xmath247 denote the restriction of @xmath245 to arm @xmath25 .",
    "the following is now immediate : @xmath248 and @xmath249 .",
    "hence , there exists some @xmath25 such that @xmath250 .",
    "applying lemma  [ lem : submodular ] with @xmath247 serving the role of @xmath44 and @xmath244 serving the role of @xmath85 proves the corollary .",
    "corollary  [ cor : scale ] if we take the integral in the proof of the above theorem with @xmath111 going from @xmath9 to @xmath251 instead of from @xmath9 to @xmath8 , we get an approximation factor of @xmath252 . hence , the optimum reward from the budgeted learning problem with budget @xmath253 is at least @xmath254 times the optimum reward with budget @xmath5 .",
    "we will now present the full details of the proof of theorem  [ thm : profitcurvesegments ] as well as the full algorithm to compute the profit curves and ratio indices for all states up to depth @xmath5 for a given bandit - arm .",
    "below , we prove a series of claims that together imply theorem  [ thm : profitcurvesegments ] .",
    "we begin by considering two methods of calculating @xmath338 that will be used in our discussions .",
    "the first is a recursive equation that can be used to calculate @xmath339 for a given state @xmath31 and budget @xmath340 provided that we have the entire profit curves of all successor states .",
    "this equation is          @xmath345 is the set of immediate descendants of @xmath31 .",
    "the decision variables @xmath346 and @xmath347 represent the probability of exploiting and exploring in @xmath31 respectively ( as in the definition of a randomized policy ) .",
    "the vector @xmath348 represents the budgets we would allocate to each of the immediate descendants of @xmath31 should we visit them . recall that @xmath30 is the probability of transitioning to state @xmath38 given we are experimenting in state @xmath31 .",
    "we assume @xmath349 if @xmath350 .",
    "this is a linear program where each decision variable , @xmath353 , selects some fraction of the @xmath25th segment of the profit curve of @xmath38 .",
    "@xmath354 represents the collection of all such variables .",
    "as the profit curve is concave",
    "( by claim  [ app : concaveprofitcurve ] ) , @xmath355 @xmath356 .",
    "thus , there exists an optimal solution which only assigns @xmath357 if @xmath358 for any @xmath359 .",
    "further , through inspection we can see that the optimal solution for any @xmath340 is to select the segments in order of decreasing slope ( where the slope of the @xmath25th segment of @xmath360 is @xmath361 ) until all budget is exhausted . by ordering segments thus",
    ", we can easily construct @xmath362 .",
    "the algorithm below orders the segments of the elements of @xmath345 and builds @xmath362 , storing the costs ( @xmath363 ) and budgets allocated to all descendants ( @xmath364 ) for each `` corner '' solution of @xmath362 .",
    "+ 1 ) for ll @xmath365 + /*compute profit , cost , and slope for each line segment of @xmath360*/ + set @xmath366 + set @xmath367 + set @xmath368 + 2 ) sort the @xmath369 elements of the form @xmath370 from largest to smallest + let @xmath371 index the node for the @xmath57th largest element in the list + let @xmath372 indicate which segment of @xmath373 this is + /*@xmath374 slope of the @xmath57th line segment in the ordered list.*/ + 3 ) set @xmath375 /*fixed cost*/ , @xmath376 /*initial profit*/ , @xmath377 @xmath378 + /*initial budgets for descendants*/ + 4 ) for =",
    "@xmath379 to @xmath369 + /*add next segment to the curve*/ + /*compute current total profit ( @xmath380 and cost ( @xmath381)*/ + let @xmath382 + let @xmath383 + /*compute budgets allocated to descendants ( @xmath384 ) at the end*/ + /*of each segment ( needed to represent the policy)*/ + let @xmath385 + let @xmath386 @xmath387 + 5 ) set @xmath388 , @xmath389 + 6 ) for = @xmath390 to @xmath369 + /*find changes in slope on the exploration profit curve*/ + if = @xmath391 + @xmath392 + @xmath393 + 7 ) for = @xmath394 to @xmath24 + /*merge together segments of the exploration profit*/ + /*curve with the same slope*/ + @xmath395 , @xmath396 + 8) @xmath397    algorithm computeexplorationprofitcurve represents the exploration profit curve for state @xmath31 by returning the number of line segments of the curve ( not including the zero slope segment from cost of 0 to @xmath398 ) , @xmath399 , as well as the cost ( @xmath400 ) , profit ( @xmath401 ) , and vector of budgets to allocate to all immediate descendants ( @xmath402 ) corresponding to the endpoint of each segment .",
    "given that the profit curves of all descendants are concave , the sorting of line segments in step 2 ) equates to simply interleaving the segments of the different states and can be performed in @xmath403 time using a simple min - heap , where @xmath226 is the maximum number of immediate descendants for a node . after sorting these segments , steps 3 ) and 4 )",
    "then determine the cost and profit associated with adding each segment to the exploration profit curve .",
    "finally , as there may and likely will be duplicates in the sorted list of slopes , steps 5 ) through 7 ) merge all segments of the curve with the same slope .    given the exploration profit curve , it is much easier to calculate the profit curve for a state . from lemma  [ app : concaveprofitcurve ] , we know that @xmath404 .",
    "further , this must be the only `` corner '' solution corresponding to exploiting at @xmath31 ( @xmath405 ) .",
    "all other  corner ",
    "solutions must thus correspond to exploring at @xmath31 ( @xmath224 ) and thus must correspond to points on @xmath362 . as we know",
    "@xmath406 is concave , we can simply take the concave envelope of the points @xmath407 , @xmath408 , and @xmath409 .",
    "the algorithm below does this .",
    "+ 1 ) find @xmath410 where @xmath411 + 2a ) if = @xmath412 /*the ratio index policy exploits immediately*/ + set @xmath413 @xmath414 + 2b ) else = /*the ratio index policy explores*/ + set @xmath415 + set @xmath416 . +",
    "while = @xmath417 + /*greater marginal return to explore than exploit*/ + @xmath418 + @xmath419 + @xmath420 + @xmath421 + set @xmath422 .",
    "algorithm computeprofitcurve runs in @xmath423 time .",
    "step 1 ) computes the value @xmath424 for each segment of @xmath362 , which represents to slope of the line segment from the origin to the point @xmath425 . in the event that @xmath412",
    ", the ratio index policy is to exploit immediately and we are done determining the profit curve for @xmath31 ( step 2a ) .",
    "otherwise , once we have found the ratio index policy , we continue to look at higher budget exploration policies to determine subsequent segments of the profit curve ( step 2b ) .",
    "the quantity @xmath426 represents the budgets allocated to each of the immediate descendants at the end of the @xmath25th segment of the profit curve .",
    "these values are only required to represent the actual policy , not calculate the ratio index or profit curve of any state .",
    "the quantity @xmath427 is the marginal ratio of transitioning from @xmath428 to exploitation at @xmath31 .",
    "once the slopes of the segments of @xmath362 are no larger than this , it is optimal to transition to exploitation at @xmath31 ."
  ],
  "abstract_text": [
    "<S> in the budgeted learning problem , we are allowed to experiment on a set of alternatives ( given a fixed experimentation budget ) with the goal of picking a single alternative with the largest possible expected payoff . </S>",
    "<S> approximation algorithms for this problem were developed by guha and munagala by rounding a linear program that couples the various alternatives together . in this paper </S>",
    "<S> we present an index for this problem , which we call the ratio index , which also guarantees a constant factor approximation . </S>",
    "<S> index - based policies have the advantage that a single number ( i.e. the index ) can be computed for each alternative irrespective of all other alternatives , and the alternative with the highest index is experimented upon . </S>",
    "<S> this is analogous to the famous gittins index for the discounted multi - armed bandit problem .    </S>",
    "<S> the ratio index has several interesting structural properties . </S>",
    "<S> first , we show that it can be computed in strongly polynomial time . </S>",
    "<S> second , we show that with the appropriate discount factor , the gittins index and our ratio index are constant factor approximations of each other , and hence the gittins index also gives a constant factor approximation to the budgeted learning problem . </S>",
    "<S> finally , we show that the ratio index can be used to create an index - based policy that achieves an @xmath0-approximation for the finite horizon version of the multi - armed bandit problem . moreover , the policy does not require any knowledge of the horizon ( whereas we compare its performance against an optimal strategy that is aware of the horizon ) . </S>",
    "<S> this yields the following surprising result : there is an index - based policy that achieves an @xmath0-approximation for the multi - armed bandit problem , oblivious to the underlying discount factor . </S>"
  ]
}