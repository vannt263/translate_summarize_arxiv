{
  "article_text": [
    "let @xmath32 , and @xmath33 _",
    "( so that @xmath34 for @xmath35 and @xmath36 otherwise ) _ let @xmath37 and @xmath38   _ ( the fw direction ) _ let @xmath39 and @xmath40 _ ( the away direction ) _ * if * @xmath41 * then * * return * @xmath14 _ ( fw gap is small enough , so return ) _ @xmath42 , and @xmath43 _ ( choose the fw direction ) _",
    "@xmath44 , and @xmath45 _",
    "( choose away direction ; maximum feasible step - size ) _ line - search : @xmath46 } \\textstyle f\\left({\\bm{x}}^{(t ) } + { \\gamma}{\\bm{d}}_t\\right)$ ] update @xmath47 _ ( and accordingly for the weights @xmath48 , see text ) _",
    "update @xmath49    @xmath50 as in algorithm  [ alg : afw ] , except replacing lines  6 to  10 by : @xmath51 , and @xmath52 .",
    "[ [ away - steps - frank - wolfe . ] ] away - steps frank - wolfe . + + + + + + + + + + + + + + + + + + + + + + +    to address the zig - zagging problem of fw , @xcite proposed to add the possibility to move _ away _ from an active atom in @xmath20 ( see middle of figure  [ fig : fwzigzag ] ) ; this simple modification is sufficient to make the algorithm linearly convergent for strongly convex functions .",
    "we describe the away - steps variant of frank - wolfe in algorithm  [ alg : afw ] . with linear inequalities and called it the modified frank - wolfe ( mfw ) algorithm .",
    "our description in algorithm  [ alg : afw ] extends it to the more general setup of  . ]",
    "the _ away _ direction @xmath53 is defined in line  4 by finding the atom @xmath54 in  @xmath20 that maximizes the potential of descent given by @xmath55 .",
    "note that this search is over the ( typically small ) active set @xmath20 , and is fundamentally easier than the linear oracle @xmath13 .",
    "the maximum step - size @xmath56 as defined on line  9 ensures that the new iterate @xmath57 stays in @xmath8 .",
    "in fact , this guarantees that the convex representation is maintained , and we stay inside @xmath58 .",
    "when @xmath8 is a simplex , then the barycentric coordinates are unique and @xmath59 truly lies on the boundary of @xmath8 . on the other hand ,",
    "if @xmath60 ( e.g. for the cube ) , then it could hypothetically be possible to have a step - size bigger than @xmath56 which is still feasible .",
    "computing the true maximum feasible step - size would require the ability to know when we cross the boundary of  @xmath8 along a specific line , which is not possible for general  @xmath8 .",
    "using the conservative maximum step - size of line  9 ensures that we do not need this more powerful oracle .",
    "this is why algorithm  [ alg : afw ] requires to maintain @xmath20 ( unlike standard fw ) .",
    "finally , as in classical fw , the fw gap @xmath61 is an upper bound on the unknown suboptimality , and can be used as a stopping criterion : @xmath62    if @xmath63 , then we call this step a _ drop step _ , as it fully removes the atom @xmath54 from the currently active set of atoms @xmath20 ( by settings its weight to zero ) . the weight updates for lines  12 and  13 are of the following form : for a fw step , we have @xmath64 if @xmath65 ; otherwise @xmath66 .",
    "also , we have @xmath67 and @xmath68 for @xmath69 . for an away step",
    ", we have @xmath70 if @xmath63 ( a _ drop step _ ) ; otherwise @xmath71 . also , we have @xmath72 and @xmath73 for @xmath74 .    [",
    "[ pairwise - frank - wolfe . ] ] pairwise frank - wolfe .",
    "+ + + + + + + + + + + + + + + + + + + + +    the next variant that we present is inspired by an early algorithm by  @xcite , called the mdm algorithm , originally invented for the polytope distance problem . here",
    "the idea is to only move weight mass between two atoms in each step .",
    "more precisely , the generalized method as presented in algorithm  [ alg : pfw ] moves weight from the away atom  @xmath54 to the fw atom  @xmath15 , and keeps all other @xmath75 weights un - changed .",
    "we call such a swap of mass between the two atoms a _ pairwise fw _ step , i.e. @xmath76 and @xmath77 for some step - size @xmath78 .",
    "in contrast , classical fw shrinks all active weights at every iteration",
    ".    the pairwise fw direction will also be central to our proof technique to provide the first global linear convergence rate for away - steps fw , as well as the fully - corrective variant and wolfe s min - norm - point algorithm .",
    "as we will see in section  [ sec : theorems ] , the rate guarantee for the pairwise fw variant is more loose than for the other variants , because we can not provide a satisfactory bound on the number of the problematic _ swap steps _ ( defined just before theorem  [ thm : megaconvergencetheorem ] ) .",
    "nevertheless , the algorithm seems to perform quite well in practice , often outperforming away - steps fw , especially in the important case of sparse solutions , that is if the optimal solution @xmath26 lies on a low - dimensional face of @xmath8 ( and thus one wants to keep the active set @xmath20 small ) .",
    "the pairwise fw step is arguably more efficient at pruning the coordinates in @xmath79 .",
    "in contrast to the away step which moves the mass back _ uniformly _ onto all other active elements @xmath20 ( and might require more corrections later ) , the pairwise fw step only moves the mass onto the ( good ) fw atom @xmath15 .",
    "a slightly different version than algorithm  [ alg : pfw ] was also proposed by  @xcite , though their convergence proofs were incomplete ( see  appendix  [ sec : pfwdetails ] ) .",
    "the algorithm is related to classical working set algorithms , such as the smo algorithm used to train svms  @xcite .",
    "we refer to  @xcite for an empirical comparison for svms , as well as their section  5 for more related work .",
    "see also appendix  [ sec : pfwdetails ] for a link between pairwise fw and  @xcite .",
    "[ [ fully - corrective - frank - wolfe - and - wolfes - min - norm - point - algorithm . ] ] fully - corrective frank - wolfe , and wolfe s min - norm point algorithm .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    when the linear oracle is expensive , it might be worthwhile to do more work to optimize over the active set @xmath20 in between each call to the linear oracle , rather than just performing an away or pairwise step .",
    "we give in algorithm  [ alg : fcfw ] the fully - corrective frank - wolfe ( fcfw ) variant , that maintains a correction polytope defined by a set of atoms @xmath80 ( potentially larger than the active set @xmath20 ) . rather than obtaining the next iterate by line - search",
    ", @xmath16 is obtained by re - optimizing @xmath5 over @xmath81 .",
    "depending on how the correction is implemented , and how the correction atoms @xmath80 are maintained , several variants can be obtained .",
    "these variants are known under many names , such as the extended fw method by  @xcite or the simplicial decomposition method  @xcite .",
    "wolfe s min - norm point ( mnp ) algorithm  @xcite for polytope distance problems is often confused with fcfw for quadratic objectives .",
    "the major difference is that standard fcfw optimizes @xmath5 over @xmath81 , whereas mnp implements the correction as a sequence of affine projections that potentially yield a different update , but can be computed more efficiently in several practical applications  @xcite .",
    "we describe precisely in appendix  [ sec : mnpdetails ] a generalization of the mnp algorithm as a specific case of the correction subroutine from step  7 of the generic algorithm  [ alg : fcfw ] .",
    "* input : * set of atoms @xmath9 , active set @xmath82 , starting point @xmath83 , stopping criterion @xmath84 .",
    "let @xmath85 ( optionally , a bigger @xmath86 could be passed as argument for a warm start ) let @xmath37 _ ( the fw atom ) _ let @xmath38 and @xmath87 _",
    "( fw gap ) _ * if * @xmath88 * then * * return * @xmath14 @xmath89 _ ( approximate correction step ) _",
    "return @xmath90 with the following properties : @xmath91 is the active set for @xmath16 and @xmath92 .",
    "@xmath93 } \\textstyle f\\left({\\bm{x}}^{(t ) } + { \\gamma}({\\bm{s}}_t - { \\bm{x}}^{(t)})\\right)$ ] _ ( make at least as much progress as a fw step ) _ @xmath94 _ ( the away gap is small enough ) _",
    "the original convergence analysis of the fcfw algorithm  @xcite ( and also mnp algorithm  @xcite ) only showed that they were finitely convergent , with a bound on the number of iterations in terms of the cardinality of @xmath9 ( unfortunately an exponential number in general ) .",
    "@xcite also argued that fcfw had an asymptotic linear convergence based on the flawed argument of  @xcite .",
    "as far as we know , our work is the first to provide global linear convergence rates for fcfw and mnp for general strongly convex functions .",
    "moreover , the proof of convergence for fcfw does not require an exact solution to the correction step ; instead , we show that the weaker properties stated for the approximate correction procedure in algorithm  [ alg : correction ] are sufficient for a global linear convergence rate ( this correction could be implemented using away - steps fw , as done for example in  @xcite ) .",
    "we first give the general intuition for the linear convergence proof of the different fw variants , starting from the work of  @xcite .",
    "we assume that the objective function @xmath5 is smooth over a compact set @xmath8 , i.e. its gradient is lipschitz continuous with constant @xmath7 . also let @xmath95 .",
    "let @xmath96 be the direction in which the line - search is executed by the algorithm ( line  11 in algorithm  [ alg : afw ] ) . by the standard descent lemma  ( see e.g. ( 1.2.5 ) in",
    "* ) , we have : @xmath97.\\ ] ] we let @xmath98 and let @xmath99 be the suboptimality error . supposing for now that @xmath100 ) .",
    "we can set @xmath101 to minimize the rhs of  , subtract @xmath102 on both sides , and re - organize to get a lower bound on the progress : @xmath103 where we use the ` hat ' notation to denote normalized vectors : @xmath104 .",
    "let @xmath105 be the error vector .",
    "by @xmath6-strong convexity of @xmath5 , we have : @xmath106.\\vspace{-0.5mm}\\ ] ] the rhs is lower bounded by its minimum as a function of @xmath107 ( unconstrained ) , achieved using @xmath108 .",
    "we are then free to use any value of @xmath107 on the lhs and maintain a valid bound .",
    "in particular , we use @xmath109 to obtain @xmath102 . again re - arranging ,",
    "we get : @xmath110 the inequality   is fairly general and valid for any line - search method in direction @xmath96 . to get a linear convergence rate , we need to lower bound ( by a positive constant ) the term in front of @xmath111 on the rhs , which depends on the angle between the update direction @xmath96 and the negative gradient  @xmath112 .",
    "if we assume that the solution @xmath26 lies in the relative interior of @xmath8 with a distance of at least @xmath113 from the boundary , then @xmath114 for the fw direction @xmath115 , and by combining with @xmath116 , we get a linear rate with constant @xmath117 ( this was the result from  @xcite ) . on the other hand , if  @xmath26 lies on the boundary , then @xmath118 gets arbitrary close to zero for standard fw ( the zig - zagging phenomenon ) and the convergence is sublinear .",
    "[ [ proof - sketch - for - afw . ] ] proof sketch for afw .",
    "+ + + + + + + + + + + + + + + + + + + + +    the key insight to prove the global linear convergence for afw is to relate @xmath119 with the _ pairwise fw _ direction @xmath120 .",
    "by the way the direction @xmath96 is chosen on lines  6 to  10 of algorithm  [ alg : afw ] , we have : @xmath121 we thus have @xmath122 .",
    "now the crucial property of the pairwise fw direction is that for any potential negative gradient direction @xmath112 , the worst case inner product @xmath123    r3.5 cm   [ fig : pwidth ]    can be lower bounded away from zero by a quantity depending only on the geometry of @xmath8 ( unless we are at the optimum ) .",
    "we call this quantity the _ pyramidal width _ of @xmath9 .",
    "the figure on the right shows the six possible pairwise fw directions @xmath124 for a triangle domain , depending on which colored area the @xmath112 direction falls into .",
    "we will see that the pyramidal width is related to the smallest width of pyramids that we can construct from @xmath9 in a specific way related to the choice of the away and towards atoms @xmath54 and @xmath15 .",
    "see   and our main theorem  [ thm : mufdirwinterpretation ] in section  [ sec : pwidth ] .",
    "this gives the main argument for the linear convergence of afw for steps where @xmath125 .",
    "when  @xmath56 is too small , afw will perform a _ drop step _ , as the line - search will truncate the step - size to @xmath63 .",
    "we can not guarantee sufficient progress in this case , but the drop step decreases the active set size by one , and thus they can not happen too often ( not more than half the time ) .",
    "these are the main elements for the global linear convergence proof for afw .",
    "the rest is to carefully consider various boundary cases .",
    "we can re - use the same techniques to prove the convergence for pairwise fw , though unfortunately the latter also has the possibility of problematic _",
    "swap steps_. while their number can be bounded , so far we only found the extremely loose bound quoted in theorem  [ thm : megaconvergencetheorem ] .",
    "[ [ proof - sketch - for - fcfw . ] ] proof sketch for fcfw .",
    "+ + + + + + + + + + + + + + + + + + + + + +    for fcfw , by line  4 of the correction algorithm  [ alg : correction ] , the away gap satisfies @xmath126 at the beginning of a new iteration . supposing that the algorithm does not exit at line  6 of algorithm  [ alg : fcfw ] , we have @xmath127 and therefore @xmath128 using a similar argument as in  .",
    "finally , by line  3 of algorithm  [ alg : correction ] , the correction is guaranteed to make at least as much progress as a line - search in direction @xmath115 , and so the progress bound   applies also to fcfw .",
    "we now give the global linear convergence rates for the four variants of the fw algorithm : away - steps fw ( afw alg .",
    "[ alg : afw ] ) ; pairwise fw ( pfw alg .",
    "[ alg : pfw ] ) ; fully - corrective fw ( fcfw alg .",
    "[ alg : fcfw ] with approximate correction alg .",
    "[ alg : correction ] ) ; and wolfe s min - norm point algorithm ( alg .",
    "[ alg : fcfw ] with mnp - correction as alg .",
    "[ alg : mnp ] in appendix  [ sec : mnpdetails ] ) . for the afw , mnp and pfw algorithms ,",
    "we call a _ drop step _ when the active set shrinks @xmath129 . for the pfw algorithm",
    ", we also have the possibility of a _ swap step _ where @xmath63 but @xmath130 ( i.e. the mass was fully swapped from the away atom to the fw atom ) .",
    "a nice property of fcfw is that it does not have any drop step ( it executes both fw steps and away steps simultaneously while guaranteeing enough progress at every iteration ) .",
    "[ thm : megaconvergencetheorem ] suppose that @xmath5 has @xmath7-lipschitz gradient is @xmath7-lipschitz over the larger domain @xmath131 .",
    "] and is @xmath6-strongly convex over @xmath11 .",
    "let @xmath132 and @xmath133 as defined by  .",
    "then the suboptimality  @xmath111 of the iterates of all the four variants of the fw algorithm decreases geometrically at each step that is not a drop step nor a swap step ( i.e. when @xmath134 , called a ` good step ' ) , that is @xmath135    let @xmath136 be the number of ` good steps ' up to iteration @xmath0",
    ". we have @xmath137 for fcfw ; @xmath138 for mnp and afw ; and @xmath139 for pfw ( because of the swap steps ) .",
    "this yields a global linear convergence rate of @xmath140 for all variants . if @xmath141 ( general convex ) , then @xmath142 instead .",
    "see theorem  [ thm : megaconvergencetheorem2 ] in appendix  [ sec : conv ] for an affine invariant version and proof .",
    "note that to our knowledge , none of the existing linear convergence results showed that the duality gap was also linearly convergent .",
    "the result for the gap follows directly from the simple manipulation of  ; putting the fw gap to the lhs and optimizing the rhs for @xmath143 $ ] .",
    "[ thm : gaptheorem ] suppose that @xmath5 has @xmath7-lipschitz gradient over @xmath8 with @xmath144 .",
    "then the fw gap @xmath61 for _ any _ algorithm is upper bounded by the primal error @xmath111 as follows : @xmath145",
    "we now describe the claimed lower bound on the angle between the negative gradient and the pairwise fw direction , which depends only on the geometric properties of  @xmath8 . according to our argument about the progress bound   and the pfw gap  , our goal is to find a lower bound on @xmath146 .",
    "first note that @xmath147 where @xmath20 is a possible active set for  @xmath14 .",
    "this looks like the _ directional width _ of a pyramid with base  @xmath20 and summit @xmath15 . to be conservative ,",
    "we consider the worst case possible active set for  @xmath14 ; this is what we will call the _ pyramid directional width _  @xmath148 .",
    "we start with the following definitions .",
    "[ [ directional - width . ] ] directional width .",
    "+ + + + + + + + + + + + + + + + + +    the directional width of a set @xmath9 with respect to a direction @xmath149 is defined as @xmath150 .",
    "the _ width _ of  @xmath9 is the minimum directional width over all possible directions in its affine hull .",
    "[ [ pyramidal - directional - width . ] ] pyramidal directional width .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    we define the pyramidal directional width of a set @xmath9 with respect to a direction @xmath149 and a base point @xmath151 to be @xmath152 where @xmath153 such that @xmath154 is a proper convex combination of all the elements in @xmath155 , and @xmath156 is the fw atom used as a summit .",
    "[ [ pyramidal - width . ] ] pyramidal width .",
    "+ + + + + + + + + + + + + + + +    to define the pyramidal width of a set , we take the minimum over the cone of possible _ feasible _ directions  @xmath149 ( in order to avoid the problem of zero width ) .",
    "+ a direction  @xmath149 is _ feasible _ for @xmath9 from @xmath154 if it points inwards @xmath157 , ( i.e. @xmath158 ) .",
    "+ we define the _ pyramidal width _ of a set @xmath9 to be the smallest pyramidal width of all its faces , i.e. @xmath159    [ thm : mufdirwinterpretation ] let @xmath160 be a suboptimal point and @xmath161 be an active set for @xmath154 .",
    "let  @xmath26 be an optimal point and corresponding error direction @xmath162 , and negative gradient @xmath163 ( and so @xmath164 ) .",
    "let @xmath165 be the pairwise fw direction obtained over @xmath9 and @xmath161 with negative gradient @xmath149 . then @xmath166      [ [ examples - of - values . ] ] examples of values .",
    "+ + + + + + + + + + + + + + + + + + +    the pyramidal width of a set @xmath9 is lower bounded by the minimal width over all subsets of atoms , and thus is strictly greater than zero if the number of atoms is finite . on the other hand",
    ", this lower bound is often too loose to be useful , as in particular , vertex subsets of the unit cube in dimension @xmath167 can have exponentially small width @xmath168  ( see corollary 27 in * ? ? ?",
    "* ) . on the other hand , as we show here , the pyramidal width of the unit cube is actually @xmath169 , justifying why we kept the tighter but more involved definition  .",
    "see appendix  [ app : cubewidth ] for the proof .",
    "[ lem : cubewidth ] the pyramidal width of the unit cube in @xmath170 is @xmath169 .    for the probability simplex with @xmath167 vertices ,",
    "the pyramidal width is actually the same as its width , which is @xmath171 when @xmath167 is even , and @xmath172 when @xmath167 is odd  @xcite ( see appendix  [ app : cubewidth ] ) .",
    "in contrast , the pyramidal width of an infinite set can be zero .",
    "for example , for a curved domain , the set of active atoms @xmath161 can contain vertices forming a very narrow pyramid , yielding a zero width in the limit .",
    "[ [ condition - number - of - a - set . ] ] condition number of a set .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + +    the inverse of the rate constant @xmath173 appearing in theorem  [ thm : megaconvergencetheorem ] is the product of two terms : @xmath174 is the standard _ condition number _ of the objective function appearing in the rates of gradient methods in convex optimization .",
    "the second quantity @xmath175 ( diameter over pyramidal width ) can be interpreted as a _ condition number _ of the domain @xmath8 , or its _",
    "eccentricity_. the more eccentric the constraint set ( large diameter compared to its pyramidal width ) , the slower the convergence .",
    "the best condition number of a function is when its level sets are spherical ; the analog in term of the constraint sets is actually the regular simplex , which has the maximum width - to - diameter ratio amongst all simplices  ( see corollary 1 in * ? ? ?",
    "its eccentricity is ( at most ) @xmath176 .",
    "in contrast , the eccentricity of the unit cube is @xmath177 , which is much worse .",
    "we conjecture that the pyramidal width of a set of _ vertices _",
    "( i.e. extrema of their convex hull ) is _ non - increasing _ when another vertex is added ( assuming that all previous points remain vertices ) .",
    "for example , the unit cube can be obtained by iteratively adding vertices to the regular probability simplex , and the pyramidal width thereby decreases from @xmath171 to @xmath169 .",
    "this property could provide lower bounds for the pyramidal width of more complicated polytopes , such as @xmath169 for the @xmath167-dimensional marginal polytope , as it can be obtained by removing vertices from the unit cube .",
    "[ [ complexity - lower - bounds . ] ] complexity lower bounds .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    combining the convergence theorem  [ thm : megaconvergencetheorem ] and the condition number of the unit simplex , we get a complexity of @xmath178 to reach @xmath84-accuracy when optimizing a strongly convex function over the unit simplex . here",
    "the linear dependence on @xmath167 should not come as a surprise , in view of the known lower bound of @xmath27 for @xmath179 for frank - wolfe type methods  @xcite .",
    "[ [ applications - to - submodular - minimization . ] ] applications to submodular minimization .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    see  appendix  [ app : submodular ] for a consequence of our linear rate for the popular mnp algorithm for submodular function optimization ( over the base polytope ) .",
    "first consider a point @xmath154 in the interior of the cube , and let @xmath149 be the unit length direction achieving the smallest pyramidal width for @xmath154 .",
    "let @xmath231 ( the fw atom in direction  @xmath149 ) .",
    "without loss of generality , by symmetry , we can rotate the cube so that @xmath232 lies at the origin .",
    "this implies that each coordinate of @xmath149 is non - positive .",
    "represent a vertex @xmath233 of the cube as its set of indices for which @xmath234 . then @xmath235 .",
    "consider any possible active set @xmath161 ; as @xmath154 has all its coordinate strictly positive , for each dimension @xmath236 , there must exist an element of @xmath161 with its @xmath236 coordinate equals to 1 .",
    "this means that @xmath237 .",
    "but as  @xmath149 has unit euclidean norm , then @xmath238 .",
    "now consider @xmath154 to lie on a facet of the cube ( i.e. the active set @xmath161 is lower dimensional ) ; and let @xmath239 . since @xmath149 has to be feasible from @xmath154 , for each @xmath240 , we can not have @xmath241 and thus there exists an element of the active set with its @xmath242 coordinate equal to 1 .",
    "we thus have that @xmath243 . using the same argument on a lower dimensional @xmath244",
    "give a lower bound of @xmath245 which is bigger .",
    "these cover all the possibilities appearing in the definition of the pyramidal width , and thus the lower bound is correct .",
    "it is achieved by choosing an @xmath154 in the interior , the canonical basis as the active set @xmath161 , and the direction defined by @xmath246 for each @xmath236 .",
    "we note that both the active set definition @xmath161 and the feasibility condition on @xmath149 were crucially used in the above proof to obtain such a large value for the pyramidal width of the unit cube , thus justifying the somewhat involved definition appearing in  . on the other hand",
    ", the astute reader might have noticed that the important quantity to lower bound for the linear convergence rate of the different fw variants is @xmath247 ( as in  ) , rather than the looser value @xmath248 that we used to handle the proof of the difficult theorem  [ thm : mufdirwinterpretation ] ( where we recall that @xmath249 is the diameter of @xmath8 ) .",
    "one could thus hope to get a tighter measure for the condition number of a set by considering @xmath250 ( with @xmath232 and @xmath233 the minimizing witnesses for the pyramidal width ) instead of the diameter @xmath249 in the ratio diameter / pyramidal width .",
    "this might give a tighter constant for general sets , but in the case of the cube , it does not change the general @xmath251 dependence for its condition number . to see this ,",
    "suppose that @xmath167 is even and let @xmath252 .",
    "consider the direction @xmath149 with @xmath253 for @xmath254 , and @xmath255 for @xmath256 .",
    "we thus have that the fw atom @xmath257 is the origin as before .",
    "consider @xmath154 such that @xmath258 for @xmath254 , and @xmath259 for @xmath256 , that is , @xmath154 is the uniform convex combination of the @xmath260 vertices which has only one non - zero in the first @xmath260 coordinates , and the last @xmath260 coordinates all equal to @xmath225 .",
    "we have that @xmath149 is a feasible direction from @xmath154 , and that all vertices in the active set for @xmath154 have the same inner product with @xmath149 : @xmath261 .",
    "we thus have : @xmath262 squaring the inverse , we thus get that the condition number of the cube is at least @xmath263 even using this tighter definition , thus not changing the @xmath251 dependence .      for any @xmath154 in the relative interior of the probability simplex on @xmath167 vertices",
    ", we have that @xmath264 , and thus the pyramidal directional width   in the feasible direction @xmath149 with base point @xmath154 is the same as the standard directional width .",
    "moreover , any face of the probability simplex is just a probability simplex in lower dimensions ( with bigger width ) .",
    "this is why the pyramidal width of the probability simplex is the same as its standard width .",
    "the width of a regular simplex was implicitly given in  @xcite ; we provide more details here on this citation .",
    "@xcite considers a regular simplex with @xmath260 vertices and side length @xmath265 . for any partition of the @xmath260 points into a set of @xmath266 and @xmath267 points ( for @xmath268 )",
    ", one can compute the distance @xmath269 between the flats ( affine hulls ) of the two sets ( which also corresponds to a specific directional width ) .",
    "alexander gives a formula on the third line of p.  91 in  @xcite for the square of this distance : @xmath270 the width of the regular simplex is obtained by taking the minimum of   with respect to @xmath271 .",
    "as   is a decreasing function up to @xmath272 , we obtain its minimum by substituting @xmath273 . by using @xmath274 , @xmath275 and @xmath276 in",
    ", we get that the width for the probability simplex on @xmath167 vertices is @xmath171 when @xmath167 is even and the slightly bigger @xmath277 when @xmath167 is odd .    from the pyramidal width perspective",
    ", one can obtain these numbers by considering any relative interior point of the probability simplex as @xmath154 and considering the following feasible @xmath149 . for @xmath167",
    "even , we let @xmath278 for @xmath279 and @xmath253 for @xmath280 .",
    "note that @xmath281 and thus @xmath149 is a feasible direction for a point in the relative interior of the probability simplex .",
    "then @xmath282 as claimed .",
    "for @xmath167 odd , we can choose @xmath283 for @xmath284 , and @xmath285 for @xmath286 . then @xmath287 and @xmath288 as claimed .",
    "showing that these obtained values were the minimum possible ones is non - trivial though , which is why we appealed to the width of the regular simplex computed from  @xcite .",
    "thm : mufdirwinterpretation let @xmath160 be a suboptimal point and @xmath161 be an active set for @xmath154 .",
    "let @xmath26 be an optimal point and corresponding error direction @xmath162 , and negative gradient @xmath163 ( and so @xmath164 ) .",
    "let @xmath289 be the pairwise fw direction obtained over @xmath9 and @xmath161 with negative gradient @xmath149 .",
    "then we have : @xmath290        by cauchy - schwarz , the denominator of   is at most @xmath291 .",
    "if @xmath149 is a feasible direction from @xmath154 for @xmath9 , then the lhs is lower bounded by @xmath292 as @xmath289 is included as a possible @xmath293 direction considered in the definition of @xmath294  .",
    "if @xmath149 is not feasible from @xmath154 , this means that @xmath154 lies on the boundary of @xmath8 .",
    "one can then show that the potential @xmath26 that can maximize @xmath295 has also to lie on a facet  @xmath244 of  @xmath8 containing @xmath154 ( see lemma  [ lem : minangle ] below ) .",
    "the idea is then to project  @xmath149 onto  @xmath244 , and re - do the argument with @xmath244 replacing @xmath8 and show that the inequality is in the right direction .",
    "this explains why all the subfaces of @xmath8 are considered in the definition of the pyramidal width  , and that only _ feasible _ directions are considered .",
    "[ lem : minangle ] let @xmath154 be at the origin , inside a polytope @xmath244 and suppose that @xmath296 is not a feasible direction for @xmath244 from @xmath154 ( i.e. @xmath297 ) .",
    "then a feasible direction in @xmath244 minimizing the angle with @xmath149 lies on a facet ( a @xmath260-dimensional face of @xmath8 ) a set @xmath244 such that @xmath298 for some normal vector @xmath149 and fixed reference point @xmath299 with the additional property that @xmath8 lies on one side of the given half - space determined by @xmath149 i.e. @xmath300 @xmath301 .",
    "@xmath260 is the dimensionality of the affine hull of @xmath244 .",
    "we call a @xmath260-face of dimensions @xmath302 , @xmath225 , @xmath303 and @xmath304 a _ vertex _ , _ edge _ , _ ridge _ and _ facet _ respectively .",
    "@xmath8 is a @xmath260-face of itself with @xmath305 .",
    "see definition 2.1 in the book of  , which we also recommend for more background material on polytopes . ]",
    "@xmath306 of @xmath244 that includes the origin @xmath154 .",
    "that is : @xmath307 where @xmath306 contains @xmath154 , @xmath308 is the euclidean norm and @xmath309 is defined as the orthogonal projection of  @xmath149 on  @xmath310 .    .",
    "if @xmath297 , but @xmath296 , then the unit vector direction  @xmath311 minimizing the angle with  @xmath149 is generated by a point  @xmath26 lying on a facet  @xmath306 of the polytope  @xmath244 that contains  @xmath154 . ]",
    "this seems like an obvious geometric fact ( see figure  [ fig : rfig ] ) , but we prove it formally , as sometimes high dimensional geometry is tricky ( for example , the result is false without the assumption that @xmath296 or if @xmath312 is not the euclidean norm ) .",
    "rewrite the optimization variable on the lhs of   as @xmath313 .",
    "the optimization domain for @xmath314 is thus the intersection between the unit sphere and @xmath315 .",
    "we now show that any maximizer @xmath311 can not lie in the relative interior of @xmath315 , and thus it has to lie on a _ facet _ of @xmath315 , implying then that a corresponding maximizer @xmath316 is lying on a facet of @xmath244 containing @xmath154 , concluding the proof for the first equality in  .",
    "first , as @xmath296 , we can consider without loss of generality that @xmath315 is full dimensional by projecting on its affine hull if needed .",
    "we want to solve @xmath317 s.t . @xmath318 and @xmath319 . by contradiction",
    ", we suppose that @xmath311 lies in the interior of @xmath315 , and so we can remove the polyhedral cone constraint .",
    "the gradient of the objective is the constant @xmath149 and the gradient of the equality constraint is @xmath320 . by the karush - kuhn - tucker ( kkt )",
    "necessary conditions for a stationary point to the problem with the only equality constraint @xmath318 ( see e.g.  ) , then the gradient of the objective is collinear to the gradient of the equality constraint , i.e. we have @xmath321 . since @xmath322 is not feasible , then @xmath323 , which is actually a local _ minimum _ of the inner product by cauchy - schwarz .",
    "we thus conclude that the maximizing @xmath311 lies on the boundary of @xmath315 , concluding the proof for the first equality in  .",
    "let @xmath325 be the normalized error vector .",
    "we consider the worst - case possibility for @xmath26 .",
    "as @xmath154 is not optimal , we require that @xmath326 .",
    "we recall that by definition of the pairwise fw direction : @xmath327 by cauchy - schwarz , we always have @xmath328 .",
    "if @xmath149 is a feasible direction from  @xmath154 in  @xmath157 , then @xmath149 appears in the set of directions considered in the definition of the pyramidal width   for @xmath9 and so from  , we have that the inequality   holds",
    ".    if @xmath149 is not a feasible direction , then we iteratively project it on the faces of @xmath8 until we get a feasible direction @xmath309 , obtaining a term @xmath329 for some face @xmath244 of @xmath8 as appearing in the definition of the pyramidal width  .",
    "the rest of the proof formalizes this process . as @xmath154 is fixed , we work on the centered polytope at @xmath154 to simplify the statements , i.e. let @xmath330 .",
    "we have the following worst case lower bound for  : @xmath331 the first term on the rhs of   just comes from the definition of @xmath289 ( with equality ) , whereas the second term is considering the worst case possibility for @xmath26 to lower bound the lhs .",
    "note also that the second term has to be strictly greater to zero since @xmath154 is not optimal .    without loss of generality",
    ", we can assume that @xmath332 ( otherwise , just project it ) , as any orthogonal component would not change the inner products appearing in  .",
    "if ( this projected ) @xmath149 is feasible from @xmath154 , then @xmath333 , and we again have the lower bound   arising in the definition of the pyramidal width .",
    "we thus now suppose that @xmath149 is not feasible . by the lemma  [ lem : minangle ]",
    ", we have the existence of a facet  @xmath306 of  @xmath334 that includes the origin @xmath154 such that : @xmath335 where  @xmath309 is the result of the orthogonal projection of  @xmath149 on  @xmath310 .",
    "we now look at how the numerator of   transforms when considering @xmath309 and @xmath306 : @xmath336 to go from the first to the second line , we use the fact that the first term yields an inequality as @xmath337 .",
    "also , since @xmath154 is in the relative interior of @xmath338 ( as @xmath154 is a _ proper _ convex combination of elements of @xmath161 by definition ) , we have that @xmath339 for any face  @xmath244 of  @xmath334 containing the origin @xmath154 .",
    "thus @xmath340 , and the second term on the first line actually yields an equality for the second line .",
    "the third line uses the fact that @xmath324 is orthogonal to members of @xmath306 , as @xmath309 is obtained by orthogonal projection .    plugging   and   into the inequality  ,",
    "we get : @xmath341 we are back to a similar situation to  , with the lower dimensional  @xmath306 playing the role of the polytope  @xmath334 , and @xmath342 playing the role of @xmath149 .",
    "if @xmath309 is feasible from @xmath154 in @xmath306 , then re - using the previous argument , we get @xmath343 as the lower bound , which is part of the definition of the pyramidal width of @xmath9 ( note that we have @xmath344 as @xmath306 is a face of the _ centered _ polytope @xmath334 ) .",
    "otherwise ( if @xmath345 ) , then we use lemma  [ lem : minangle ] again to get a facet  @xmath346 of  @xmath306 as well as a new direction  @xmath347 which is the orthogonal projection of  @xmath309 on  @xmath348 such that we can re - do the manipulations for   and  , yielding @xmath349 as a lower bound if  @xmath347 is feasible from  @xmath154 in  @xmath346 . as long as we do not obtain a feasible direction , we keep re - using lemma  [ lem : minangle ] to project the direction on a lower dimensional face of @xmath334 that contains @xmath154 .",
    "this process must stop at some point ; ultimately , we will reach the lowest dimensional face  @xmath350 that contains  @xmath154 . as  @xmath154 lies in the relative interior of  @xmath350 , then all directions in  @xmath351 are feasible , and so the projected @xmath149 will have to be feasible . moreover , by stringing together the equalities of the type   for all the projected directions , we know that @xmath352 ( as we originally had @xmath164 ) , and thus @xmath350 is at least one - dimensional and we also have @xmath353 ( this last condition is crucial to avoid having a lower bound of zero ! ) .",
    "this concludes the proof , and also explains why in the definition of the pyramidal width  , we consider the pyramidal directional width for all the faces of @xmath157 and respective non - zero feasible direction @xmath149 .",
    "building on the work of  @xcite and  @xcite , we can generalize our global linear convergence results for all frank - wolfe variants for the more general case where @xmath180 , for @xmath181 , @xmath182 and where @xmath183 is @xmath184-strongly convex and continuously differentiable over @xmath185 .",
    "we note that for a general matrix @xmath186 , @xmath5 is convex but not necessarily _ strongly _ convex . in this case",
    ", the linear convergence still holds but with the constant @xmath6 appearing in the rate of theorem  [ thm : megaconvergencetheorem ] replaced with the generalized constant @xmath187 appearing in lemma  [ lem : generalizedstrongconvexity ] in appendix  [ app : nonstronglyconvex ] .          here",
    "we will study the generalized setting with objective @xmath561 where @xmath183 is @xmath184-_strongly convex _ w.r.t .",
    "the euclidean norm over the domain @xmath185 with strong convexity constant @xmath562 .",
    "let @xmath538 be the hoffman constant ( see  ( * ? ? ?",
    "* lemma 2.2 ) ) associated with the matrix @xmath565 = \\left(\\substack{{\\bm{a}}\\\\ { \\bm{b}}^\\top \\\\ { \\bm{b}}}\\right)$ ] , where the rows of @xmath566 are the linear inequality constraints defining the set @xmath8 .              plugging   into   and adding to  , we get : @xmath574 where @xmath575 . for the last inequality",
    ", we used inequality  ( 2.1 ) in  @xcite that made use of the hoffman s lemma ( see  ( * ? ? ?",
    "* lemma 2.2 ) ) , where @xmath538 is the hoffman constant associated with the matrix @xmath576 $ ] . in this case , @xmath566 is the matrix with rows containing the linear inequality constraints defining @xmath8 .",
    "we now define the following generalization of the geometric strong convexity constant , that we now call @xmath577 : @xmath578 notice the new inner _ supremum _ over the solution set @xmath567 compared to the original definition , the factor of @xmath579 in front of the gradient , and the different overall scaling to have a similar form as in the previous linear convergence theorem .",
    "this new quantity @xmath577 is still _ affine invariant _ , but unfortunately now depends on the location of the solution set @xmath567 .",
    "we now present the generalization of theorem  [ thm : mufdirwinterpretation2 ] .",
    "[ thm : generalizedpwidth ] let @xmath561 where @xmath183 is @xmath184-_strongly convex _ w.r.t .  the euclidean norm over the domain @xmath185 with strong convexity constant @xmath562",
    "let @xmath187 be the corresponding generalized strong convexity constant coming from lemma  [ lem : generalizedstrongconvexity ] .",
    "then @xmath580      we use the generalized strong convexity notion @xmath187 from lemma  [ lem : generalizedstrongconvexity ] for the particular reference point  @xmath26 in the third line below to get : @xmath583 we can do this for each non - optimal @xmath154 .",
    "we thus obtain : @xmath584 and we are back to the same situation as in the proof of our earlier theorem  [ thm : mufdirwinterpretation2 ] , the only change being that we now have equation   holding for the general strong convexity constant @xmath187 instead of its classical analogue  @xmath6 .    having this tool at hand",
    ", the linear convergence of all frank - wolfe algorithm variants now holds with the earlier @xmath229 complexity constant replaced with  @xmath577 .",
    "the factor of 2 in the denominator of   is to ensure the same scaling .",
    "again , as we have shown in theorem [ thm : generalizedpwidth ] , we have that our condition @xmath585 leading to linear convergence is slightly weaker than generalized strong convexity in the hoffman sense ( it is implied by it ) .",
    "[ thm : megaconvergencetheoremhoffman ] suppose that @xmath5 has smoothness constant @xmath230 ( @xmath354 for fcfw and mnp ) , as well as generalized geometric strong convexity constant  @xmath577 as defined in  .",
    "then the suboptimality error  @xmath111 of the iterates of all the four variants of the fw algorithm ( afw , fcfw , mnp and pfw ) decreases geometrically at each step that is not a drop step nor a swap step ( i.e. when @xmath134 ) , with the same constants as in theorem  [ thm : megaconvergencetheorem2 ] , except that @xmath229 is replaced by  @xmath577 .        from here , we will now mirror our earlier derivation for an upper bound on the suboptimality as a function of the gap @xmath436 , as given in  . as an optimal reference point @xmath26 in , we will choose a @xmath588 attaining the supremum in , given @xmath14 .",
    "therefore @xmath591 when writing @xmath592 , which is always upper bounded for the choice of numbers @xmath593 and @xmath594 . ] by @xmath595 which is exactly the bound   as in the classical case , with the denominator being  @xmath596 instead of  @xmath597 .",
    "we illustrate the performance of the presented algorithm variants in two numerical experiments , shown in figure  [ fig : experiments ] .",
    "the first example is a constrained lasso problem ( @xmath188-regularized least squares regression ) , that is @xmath189 , with @xmath190 a scaled @xmath191-ball .",
    "we used a random gaussian matrix @xmath192 , and a noisy measurement @xmath193 with @xmath26 being a sparse vector with 50 entries @xmath194 , and @xmath195 of additive noise . for the @xmath191-ball , the linear minimization oracle @xmath13 just selects the column of @xmath186 of best inner product with the residual vector .",
    "the second application comes from video co - localization .",
    "the approach used by  @xcite is formulated as a quadratic program ( qp ) over a flow polytope , the convex hull of paths in a network . in this application ,",
    "the linear minimization oracle is equivalent to finding a shortest path in the network , which can be done easily by dynamic programming . for the @xmath13 , we re - use the code provided by  @xcite and their included dataset resulting in a qp over 660 variables . in both experiments",
    ", we see that the modified fw variants ( away - steps and pairwise ) outperform the original fw algorithm , and exhibit a linear convergence .",
    "in addition , the constant in the convergence rate of theorem  [ thm : megaconvergencetheorem ] can also be empirically shown to be fairly tight for afw and pfw by running them on an increasingly obtuse triangle ( see appendix  [ app : triangle ] ) .",
    "building on a preliminary version of our work  @xcite , @xcite also proved a linear rate for away - steps fw , but with a simpler lower bound for the lhs of   using linear duality arguments .",
    "however , their lower bound ( see e.g. lemma 3.1 in * ? ? ?",
    "* ) is looser : they get a @xmath177 constant for the eccentricity of the regular simplex instead of the tighter @xmath167 that we proved .",
    "finally , the recently proposed generic scheme for _ accelerating _ first - order optimization methods in the sense of nesterov from  @xcite applies directly to the fw variants given their global linear convergence rate that we proved .",
    "this gives for the first time first - order methods that _ only use linear oracles _ and obtain the `` near - optimal '' @xmath196 rate for smooth convex functions , or the accelerated @xmath197 constant in the linear rate for strongly convex functions .",
    "given that the constants also depend on the dimensionality , it remains an open question whether this acceleration is practically useful .",
    "the appendix is organized as follows : in appendix [ sec : fwvariants ] , we discuss some of the frank - wolfe algorithm variants in more details and related work ( including the mnp algorithm in appendix  [ sec : mnpdetails ] and its application to submodular minimization in appendix [ app : submodular ] ) .    in appendix",
    "[ sec : pyramidal ] , we discuss the pyramidal width for some particular cases of sets ( such as the probability simplex and the unit cube ) , and then provide the proof of the main theorem  [ thm : mufdirwinterpretation ] relating the pyramidal width to the progress quantity essential for the linear convergence rate .",
    "section  [ sec : invariance ] presents an affine invariant version of the complexity constants .    in the following section  [ sec : conv ] ,",
    "we show the main linear convergence result for the four variants of the fw algorithm , and also discuss the sublinear rates for general convex functions .",
    "section  [ app : triangle ] presents a simple experiment demonstrating the empirical tightness of the theoretical linear convergence rate constant . finally , in appendix",
    "[ app : nonstronglyconvex ] we discuss the generalization of the linear convergence to some cases of non - strongly convex functions in more details .",
    "a generalization of wolfe s min - norm point ( mnp ) algorithm  @xcite for general convex functions is to run algorithm  [ alg : fcfw ] with the correction subroutine in step  7 implemented as presented below in algorithm  [ alg : mnp ] . in wolfe s",
    "paper  @xcite , the correction step is called the minor cycle ; whereas the fw outer loop is called the major cycle .    as we have mentioned in section [ sec : variants ] , mnp for polytope distance is often confused with fully - corrective fw as presented in algorithm  [ alg : fcfw ] , for quadratic objectives .",
    "in fact , standard fcfw optimizes @xmath5 over @xmath81 , whereas mnp implements the correction as a sequence of _ affine _ projections on the active set that potentially yield a different update .",
    "let @xmath198 , and @xmath199 .",
    "note that @xmath200 and we assume that the elements of @xmath80 are _ affinely independent_. let @xmath201 be the minimizer of @xmath5 on the affine hull of @xmath202 * return * @xmath203 _ ( @xmath202 is active set for @xmath201 ) _ let @xmath204 be the solution of doing line - search from @xmath205 to @xmath201 .",
    "_ ( note that @xmath204 now lies on the boundary of @xmath206 , and so some atoms were removed ) _ let @xmath207 be the ( affinely independent ) active atoms in the expansion of @xmath204 .",
    "there are two main differences between fcfw and the mnp algorithm .",
    "first , after a correction step , mnp guarantees that @xmath16 is _ both _ the minimizer of @xmath5 over the _ affine hull _ of @xmath208 and also @xmath209 ( where @xmath208 might be much smaller than @xmath210 ) , whereas fcfw guarantees that @xmath16 is the minimizer of @xmath5 over @xmath211  this is usually not the case for mnp unless at most one atom was dropped from the correction polytope , as is apparent from our convergence proof .",
    "secondly , the correction atoms @xmath80 are always affinely independent for mnp and are identical to the active set @xmath20 , whereas fcfw can use both redundant as well as inactive atoms .",
    "the advantage of the mnp implementation using affine hull projections is that the correction can be efficiently implemented when @xmath5 is the euclidean norm , especially when a triangular array representation of the active set is maintained ( see the careful implementation details in wolfe s original paper  @xcite ) .",
    "the mnp variant indeed only makes sense when the minimization of @xmath5 over the affine hull of @xmath8 is well - defined ( and is efficient ) .",
    "note though that the line - search in step 7 does not require any new information about @xmath8 , as it is made only with respect to @xmath206 , for which we have an explicit list of vertices .",
    "this line - search can be efficiently computed in @xmath212 , and is well described for example in step 2(d)(iii ) of algorithm 1 of .",
    "an interesting consequence of our global linear convergence result for fw algorithm variants here is the potential to reduce the gap between the known theoretical rates and the impressive empirical performance of mnp for submodular function minimization ( over the base polytope ) .",
    "while @xcite already showed convergence of fw in this case , later gave a weaker convergence rate for wolfe s mnp variant .",
    "for exact submodular function optimization , the overall complexity by was @xmath213 ( with some corrections for mnp .",
    "however , this fell short of the earlier result of @xcite for classic fw in the submodular minimization case , which was better by two  @xmath214 factors .",
    "counted @xmath215 per iteration of the mnp algorithm whereas wolfe had provided a @xmath216 implementation ; and they missed that there were at least @xmath217 good cycles ( ` non drop steps ' ) after @xmath0 iterations , rather than @xmath218 as they have used . ] ) , where @xmath219 is the maximum absolute value of the integer - valued submodular function .",
    "this is in contrast to @xmath220 for the fastest algorithms  . using our linear convergence ,",
    "the @xmath219 factor can be put back in the @xmath221 term for mnp , , which remains to be proven . ] matching their empirical observations that the mnp algorithm was not too sensitive to @xmath219 .",
    "the same follows for afw and fcfw , which is novel .",
    "our new analysis of the pairwise frank - wolfe variant as introduced in section [ sec : variants ] is motivated by the work of  @xcite , who provided the first variant of frank - wolfe with a global linear convergence rate with explicit constants that do not depend on the location of the optimum @xmath26 , for a more complex extension of such a pairwise algorithm .",
    "an important contribution of the work of  @xcite was to define the concept of _ local linear oracle _ , which ( approximately ) minimizes a linear function on the intersection of @xmath8 and a small ball around @xmath14 ( hence the name _ local _ ) .",
    "they showed that if such a local linear oracle was available , then one could replace the step that moves towards @xmath15 in the standard fw procedure with a constant step - size move towards the point returned by the local linear oracle to obtain a globally linearly convergent algorithm .",
    "they then demonstrated how to implement such a local linear oracle by using only one call to the linear oracle ( to get @xmath15 ) , as well as sorting the atoms in @xmath20 in decreasing order of their inner product with @xmath222 ( note that the first element then is the away atom @xmath54 from algorithm  [ alg : afw ] ) .",
    "the procedure implementing the local linear oracle amounts to iteratively swapping the mass from the away atom  @xmath54 to the fw atom  @xmath15 until enough mass has been moved ( given by some precomputed constants ) .",
    "if the amount of mass to move is bigger than @xmath223 , then one sets @xmath223 to zero and start moving mass from the _ second _ away atom , and so on , until enough mass has been moved ( which is why the sorting is needed ) .",
    "we call such a swap of mass between the away atom and the fw atom a _ pairwise fw _ step , i.e. @xmath76 and @xmath77 for some step - size @xmath78",
    ". the local linear oracle is implemented as a sequence of pairwise fw steps , always keeping the same fw atom  @xmath15 as the target , but updating the away atom to move from as we set their coordinates to zero .",
    "a major disadvantage of the algorithm presented by  @xcite is that their algorithm is _ not adaptive _ : it requires the computation of several ( loose ) constants to determine the step - sizes , which means that the behavior of the algorithm is stuck in its worst - case analysis .",
    "the pairwise frank - wolfe variant is obtained by simply doing one line - search in the pairwise frank - wolfe direction @xmath224 ( see algorithm  [ alg : pfw ] ) .",
    "this gives a fully adaptive algorithm , and it turns out that this is sufficient to yield a global linear convergent rate .",
    "we here point out some corrections to the convergence proofs given in  @xcite for a variant of pairwise fw that chooses between a standard fw step and a pairwise fw step by picking the one which makes the most progress on the objective after a line - search .",
    "? * proposition  @xmath225 ) states the global convergence of their algorithm by arguing that @xmath226 and then stating that they can re - use the same pattern as the standard fw convergence proof but with the direction @xmath124 .",
    "but this is forgetting the fact that the maximal step - size @xmath227 for a pairwise fw step can be too small to make sufficient progress .",
    "their global convergence statement is still correct as every step of their algorithm makes more progress than a fw step , which already has a global convergence result , but this is not the argument they made .",
    "similarly , they state a global linear convergence result in their proposition  4 , citing a proof from  . on the other hand , the relevant used proposition  3 in   forgot to consider the possibility of problematic _ swap steps _ that we had to painfully bound in our convergence theorem  [ thm : megaconvergencetheorem2 ] ; they only considered drop steps or ` good steps ' , thereby missing a bound on the number of swap steps to get a valid global bound .      very recently , following the earlier workshop version of our article  @xcite , @xcite presented an alternative geometric quantity measuring the linear convergence speed of the afw algorithm variant .",
    "their approach is motivated by a special case of the frank - wolfe method , the von neumann algorithm .",
    "their complexity constant  called the restricted width  is also bounded away from zero , but its value does depend on the location of the optimal solution , which is a disadvantage shared with the earlier existing results of  @xcite , as well as the line of work of   @xcite that relies on robinson s condition @xcite .",
    "more precisely , the bound on the constant given in  ( * ? ? ?",
    "* theorem 4 ) applies to the translated atoms @xmath228 relative to the optimum point .",
    "the constant is not affine - invariant , whereas the constants @xmath229   and @xmath230   in our setting are so , see the discussion in section  [ sec : invariance ] .",
    "it would still be interesting to compare the value of our respective constants on standard polytopes .",
    "here we provide linear convergence proofs in terms of affine invariant quantities , since all the frank - wolfe algorithm variants presented in this paper are affine invariant .",
    "the statements presented in the main paper above are special cases of the following more general theorems , by using the bounds   for the curvature constant @xmath354 , and theorem  [ thm : mufdirwinterpretation2 ] for the affine invariant strong convexity  @xmath229 .",
    "an optimization method is called _ affine invariant _ if it is invariant under affine transformations of the input problem : if one chooses any re - parameterization of the domain  @xmath8 by a _",
    "surjective _ linear or affine map @xmath355 , then the `` old '' and `` new '' optimization problems @xmath356 and @xmath357 for @xmath358 look completely the same to the algorithm .",
    "more precisely , every `` new '' iterate must remain exactly the transform of the corresponding old iterate ; an affine invariant analysis should thus yield the convergence rate and constants unchanged by the transformation .",
    "it is well known that newton s method is affine invariant under invertible  @xmath186 , and the frank - wolfe algorithm and all the variants presented here are affine invariant in the even stronger sense under arbitrary surjective @xmath186  @xcite .",
    "( this is directly implied if the algorithm and all constants appearing in the analysis only depend on inner products with the gradient , which are preserved since @xmath359 . )",
    "note however that the property of being an extremum point ( vertex ) of  @xmath8 is _ not _ affine invariant ( see  ( * ? ? ?",
    "* section  3.1 ) for an example ) .",
    "this explains why we presented all algorithms here as working with atoms @xmath9 rather than vertices of the domain , thus maintaining the affine invariance of the algorithms as well as their convergence analysis .",
    "the affine invariant convergence analysis of the standard frank - wolfe algorithm by @xcite crucially relies on the following measure of non - linearity of the objective function @xmath5 over the domain @xmath8 .",
    "the ( upper ) _ curvature constant _ @xmath360 of a convex and differentiable function @xmath361 , with respect to a compact domain @xmath8 is defined as @xmath362,\\\\                        { \\bm{y}}= { \\bm{x}}+{\\gamma}({\\bm{s}}-{\\bm{x } } ) } } \\textstyle             \\frac{2}{{\\gamma}^2}\\big ( f({\\bm{y}})-f({\\bm{x}})-{\\left\\langle \\nabla f({\\bm{x } } ) , { \\bm{y}}-{\\bm{x}}\\right\\rangle}\\big ) \\ .\\ ] ]    the definition of @xmath354 closely mimics the fundamental descent lemma  .",
    "the assumption of bounded curvature @xmath354 closely corresponds to a lipschitz assumption on the gradient of  @xmath5 .",
    "more precisely , if  @xmath363 is @xmath7-lipschitz continuous on @xmath8 with respect to some arbitrary chosen norm @xmath364 in dual pairing , i.e. @xmath365 , then @xmath366 where @xmath367 denotes the @xmath364-diameter , see ( * ? ? ?",
    "* lemma 7 ) .",
    "while the early papers @xcite on the frank - wolfe algorithm relied on such lipschitz constants with respect to a norm , the curvature constant @xmath354 here is affine invariant , does not depend on any norm , and gives tighter convergence rates .",
    "the quantity @xmath354 combines the complexity of the domain @xmath8 and the curvature of the objective function @xmath5 into a single quantity .",
    "the advantage of this combination is well illustrated in  ( * ? ? ?",
    "* lemma a.1 ) , where frank - wolfe was used to optimize a quadratic function over product of probability simplices with an exponential number of dimensions . in this case , the lipschitz constant could be exponentially worse than the curvature constant which does take the simplex geometry of @xmath8 into account .    [",
    "[ an - affine - invariant - notion - of - strong - convexity - which - depends - on - the - geometry - of - mathcalm . ] ] an affine invariant notion of strong convexity which depends on the geometry of @xmath8 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we now present the affine invariant analog of the strong convexity bound  , which could be interpreted as the _ lower _ curvature @xmath229 analog of @xmath354 .",
    "the role of @xmath107 in the definition   of  @xmath354 was to define an affine invariant scale by only looking at proportions over lines ( as line segments between @xmath154 and  @xmath232 in this case ) .",
    "the trick here is to use anchor points in  @xmath9 in order to define standard lengths ( by looking at proportions on lines ) .",
    "these anchor points ( @xmath368 and @xmath369 defined below ) are motivated directly from the fw atom and the away atom appearing in the away - steps fw algorithm .",
    "specifically , let @xmath26 be a potential optimal point and @xmath154 a non - optimal point ; thus we have @xmath370 ( i.e. @xmath371 is a strict descent direction from @xmath154 for @xmath5 ) .",
    "we then define the positive step - size quantity : @xmath372 this quantity is motivated from both   and the linear rate inequality  , and enables to transfer lengths from the error @xmath373 to the pairwise fw direction @xmath374 .",
    "more precisely , @xmath375@xmath376 is the standard fw atom . to define the away - atom",
    ", we consider all possible expansions of @xmath154 as a convex combination of atoms .",
    "we recall that set of possible active sets is @xmath153 such that @xmath154 is a proper convex combination of all the elements in @xmath155 . for",
    "a given set  @xmath161 , we write @xmath377 for the away atom in the algorithm supposing that the current set of active atoms is @xmath161 .",
    "finally , we define @xmath378 to be the worst - case away atom ( that is , the atom which would yield the smallest away descent ) .",
    "[ [ lower - bound - for - the - geometric - strong - convexity - constant - mu_hspace-0.08emfhspace0.06emtextnormalscriptsize - a ] ] lower bound for the geometric strong convexity constant @xmath229 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    the geometric strong convexity constant @xmath229 , as defined in ( [ eq : muf ] ) , is affine invariant , since it only depends on the inner products of feasible points with the gradient .",
    "also , it combines both the complexity of the function  @xmath5 and the geometry of the domain @xmath8 .",
    "theorem  [ thm : mufdirwinterpretation2 ] allows us to lower bound the constant @xmath229 in terms of the strong convexity of the objective function , combined with a purely geometric complexity measure of the domain @xmath8 ( its pyramidal width @xmath380  ) . in the following section  [ sec : conv ] below",
    ", we will show the linear convergence of the four variants of the fw algorithm presented in this paper under the assumption that @xmath381 .    in view of the following theorem  [ thm : mufdirwinterpretation2 ]",
    ", we have that the condition @xmath381 is slightly weaker than the strong convexity of the objective function , consider @xmath382 where @xmath183 is @xmath184-strongly convex , but the matrix @xmath186 is rank deficient . then by using the affine invariance of the definition of  @xmath229 and using theorem   applied on the equivalent problem on  @xmath183 with domain  @xmath383 , we get @xmath384 . ] over a polytope domain ( it is implied by strong convexity ) .",
    "[ thm : mufdirwinterpretation2 ] let @xmath5 be a convex differentiable function and suppose that @xmath5 is @xmath6-_strongly convex _ w.r.t . to the euclidean norm @xmath385 over the domain @xmath11 with strong - convexity constant @xmath386 .",
    "then @xmath387    by definition of strong convexity with respect to a norm , we have that for any @xmath388 , @xmath389 using the strong convexity bound   with @xmath390 on the right hand side of equation   ( and using the shorthand @xmath391 ) , we thus get : @xmath392 where @xmath393 is the unit length feasible direction from @xmath154 to @xmath26 .",
    "we are thus taking an infimum over all possible feasible directions starting from @xmath154 ( i.e. which moves within @xmath8 ) with the additional constraint that it makes a positive inner product with the negative gradient @xmath394 , i.e. it is a strict descent direction .",
    "this is only possible if @xmath154 is not already optimal , i.e. @xmath395 where @xmath396 is the set of optimal points .",
    "we note that @xmath397 is a valid pairwise fw direction for a specific active set @xmath161 for @xmath154 , and so we can re - use   from theorem  [ thm : mufdirwinterpretation ] for the right hand side of to conclude the proof .",
    "because of the additional possibility of the away step in algorithm  [ alg : afw ] , we need to define the following slightly modified additional curvature constant , which will be needed for the linear convergence analysis of the algorithm : @xmath398,\\\\                        { \\bm{y}}= { \\bm{x}}+{\\gamma}({\\bm{s}}-{\\bm{v } } ) } }             \\frac{2}{{\\gamma}^2}\\big ( f({\\bm{y}})-f({\\bm{x}})-{\\gamma}\\langle \\nabla f({\\bm{x } } ) , { \\bm{s}}-{\\bm{v}}\\rangle \\big ) \\ .\\ ] ] by comparing with @xmath354  , we see that the modification is that @xmath399 is defined with any direction @xmath400 instead of a standard fw direction @xmath401 .",
    "this allows to use the away direction or the pairwise fw direction even though these might yield some @xmath399 s which are outside of the domain @xmath8 when using @xmath402 ( in fact , @xmath403 in the minkowski sense ) . on the other hand , by re - using a similar argument as in (",
    "* lemma 7 ) , we can obtain the same bound   for @xmath230 , with the only difference that the lipschitz constant @xmath7 for the gradient function has to be valid on @xmath404 instead of just @xmath8 .",
    "let @xmath154 be a vertex of @xmath8 , so that @xmath407 . then @xmath408 . pick @xmath409 and substitute in the definition for  @xmath229  .",
    "then @xmath410 and so we have @xmath411 with @xmath109 which can also be used in the definition of  @xmath354  .",
    "thus , we have @xmath412 .",
    "we now give the global linear convergence rates for the four variants of the fw algorithm : away - steps fw ( afw algorithm  [ alg : afw ] ) ; pairwise fw ( pfw algorithm  [ alg : pfw ] ) ; fully - corrective fw ( fcfw algorithm  [ alg : fcfw ] with approximate correction as per algorithm  [ alg : correction ] ) ; and wolfe s min - norm point algorithm ( algorithm  [ alg : fcfw ] with mnp - correction given in algorithm  [ alg : mnp ] ) .",
    "for the afw , mnp and pfw algorithms , we call a _ drop step _ when the active set shrinks , i.e. @xmath129 . for the pfw algorithm , we also have the possibility of a _ swap step _ , where @xmath63 but the size of the active set stays constant @xmath130 ( i.e. the mass gets fully swapped from the away atom to the fw atom ) .",
    "we note that a nice property of the fcfw variant is that it does not have any drop steps ( it executes both fw steps and away steps simultaneously while guaranteeing enough progress at every iteration ) .",
    "[ thm : megaconvergencetheorem2 ] suppose that @xmath5 has smoothness constant @xmath230 ( @xmath354 for fcfw and mnp ) , as well as geometric strong convexity constant  @xmath229 as defined in  .",
    "then the suboptimality @xmath99 of the iterates of all the four variants of the fw algorithm decreases geometrically at each step that is not a drop step nor a swap step ( i.e. when @xmath134 , called a ` good step ' can also be considered a ` good step ' , even if @xmath63 , as is apparent from the proof .",
    "the problematic steps arise only when @xmath413 . ] ) , that is @xmath414 where : @xmath415 moreover , the number of drop steps up to iteration @xmath0 is bounded by @xmath217 .",
    "this yields the global linear convergence rate of @xmath416 for the afw and mnp variants .",
    "fcfw does not need the extra @xmath417 factor as it does not have any bad step .",
    "finally , the pfw algorithm has at most @xmath418 swap steps between any two ` good steps ' .",
    "if @xmath419 ( i.e. the case of general convex objectives ) , then all the four variants have a @xmath420 convergence rate where @xmath136 is the number of ` good steps ' up to iteration @xmath0 .",
    "more specifically , we can summarize the suboptimality bounds for the four variants as : @xmath421 where @xmath422 for afw ; @xmath423 for fcfw with approximate correction ; @xmath424 for mnp ; and @xmath425 for pfw .",
    "the number of good steps is @xmath137 for fcfw ; it is @xmath138 for mnp and afw ; and @xmath139 for pfw .    * proof for afw . * the general idea of the proof is to use the definition of the geometric strong convexity constant to upper bound @xmath111 , while using the definition of the curvature constant @xmath230 to lower bound the decrease in primal suboptimality @xmath426 for the ` good steps ' of algorithm  [ alg : afw ] .",
    "then we upper bound the number of ` bad steps ' ( the drop steps ) .",
    "_ upper bounding @xmath111 . _ in the whole proof , we assume that @xmath14 is not already optimal , i.e. that @xmath427 . if @xmath428 , then because line - search is used , we will have @xmath429 and so the geometric rate of decrease is trivially true in this case .",
    "let @xmath26 be an optimum point ( which is not necessarily unique ) .",
    "as @xmath427 , we have that @xmath430 .",
    "we can thus apply the geometric strong convexity bound   at the current iterate @xmath431 using @xmath26 as an optimum reference point to get ( with @xmath432 as defined in ):",
    "@xmath433 where we define @xmath434 ( note that @xmath435 and so @xmath436 also gives a primal suboptimality certificate ) . for the third line",
    ", we have used the definition of @xmath369 which implies @xmath437 .",
    "therefore @xmath438 , which is always upper bounded for the choice of numbers @xmath439 and @xmath440 . an alternative way to obtain the bound",
    "is to look at the unconstrained maximum of the rhs which is a concave function of @xmath441 by letting @xmath442 , as we did in the main paper to obtain the upper bound on @xmath111 in  .",
    "] by @xmath443    _ lower bounding progress @xmath444 .",
    "_ we here use the key aspect in the proof that we had described in the main text with  . because of the way the direction @xmath96 is chosen in the afw algorithm  [ alg : afw ]",
    ", we have @xmath445 and thus @xmath436 characterizes the quality of the direction @xmath96 . to see this , note that @xmath446 .",
    "we first consider the case @xmath447 .",
    "let @xmath448 be the point obtained by moving with step - size @xmath107 in direction @xmath96 , where @xmath96 is the one chosen by algorithm  [ alg : afw ] . by using @xmath449 ( a feasible point as @xmath447 ) , @xmath450 and @xmath451 in the definition of the curvature constant  @xmath354  , and solving for @xmath452 , we get the affine invariant version of the descent lemma  : @xmath453$}.\\ ] ] as  @xmath454 is obtained by line - search and that @xmath455 \\subseteq [ 0,{\\gamma}_\\textrm{max}]$ ] , we also have that @xmath456 @xmath457 $ ] .",
    "combining these two inequalities , subtracting @xmath102 on both sides , and using @xmath458 to simplify the possibilities yields @xmath459 .",
    "using the crucial gap inequality  , we get @xmath460 , and so : @xmath461.\\ ] ] we can minimize the bound   on the right hand side by letting @xmath462 . supposing that @xmath463 , we then get @xmath464 ( we cover the case @xmath465 later ) . by combining this inequality with the one from geometric strong convexity  , we get @xmath466 implying that we have a geometric rate of decrease @xmath467 ( this is a ` good step ' )",
    ".    _ boundary cases .",
    "_ we now consider the case @xmath465 ( with @xmath447 still ) .",
    "the condition @xmath465 then translates to @xmath468 , which we can use in   with @xmath109 to get @xmath469 .",
    "combining this inequality with @xmath435 gives the geometric decrease @xmath470 ( also a ` good step ' ) .",
    "@xmath471 is obtained by considering the worst - case of the constants obtained from @xmath465 and @xmath463 .",
    "( note that @xmath472 by remark  [ rem : mfafwsmallerthancf ] , and thus @xmath473 ) .",
    "finally , we are left with the case that @xmath474 .",
    "this is thus an away step and so @xmath475 . here",
    ", we use the away version @xmath230 : by letting @xmath476 , @xmath477 and @xmath451 in  , we also get the bound @xmath478 , valid @xmath479 $ ] ( but note here that the points @xmath480 are not feasible for @xmath481  the bound considers some points outside of @xmath8 ) .",
    "we now have two options : either @xmath482 ( a drop step ) or @xmath483 . in the case",
    "@xmath484 ( the line - search yields a solution in the interior of @xmath485 $ ] ) , then because @xmath452 is convex in @xmath107 , we know that @xmath486 } f({\\bm{x}}_{\\gamma } ) = \\min_{{\\gamma}\\geq 0 } f({\\bm{x}}_{\\gamma})$ ] and thus @xmath486 } f({\\bm{x}}_{\\gamma } ) = f({\\bm{x}}^{(t+1 ) } ) \\leq f({\\bm{x}}_{\\gamma})$ ] @xmath479 $ ] .",
    "we can then re - use the same argument above equation   to get the inequality  , and again considering both the case @xmath463 ( which yields inequality  ) and the case @xmath465 ( which yields @xmath487 as the geometric rate constant ) , we get a ` good step ' with @xmath488 as the worst - case geometric rate constant .",
    "finally , we can easily bound the number of drop steps possible up to iteration @xmath0 with the following argument ( the drop steps are the ` bad steps ' for which we can not show good progress ) .",
    "let @xmath489 be the number of steps that added a vertex in the expansion ( only standard fw steps can do this ) and let @xmath490 be the number of drop steps .",
    "we have that @xmath491 .",
    "we thus have @xmath492 , implying that @xmath493 , as stated in the theorem .      in the case of fcfw",
    ", we do not need to consider away steps : by the quality of the approximate correction in algorithm  [ alg : correction ] ( as specified in line  4 ) , we know that at the beginning of a new iteration , the away gap @xmath126 . supposing that the algorithm does not exit at line  6 of algorithm  [ alg : fcfw ] , then @xmath127 and thus we have that @xmath128 using a similar argument as in   ( i.e. if one would be to run the afw algorithm at this point , it would take a fw step ) .",
    "finally , by property of the line  3 of the approximate correction algorithm  [ alg : correction ] , the correction is guaranteed to make at least as much progress as a line - search in direction @xmath115 , and so the lower bound   can be used for fcfw as well ( but using @xmath354 as the constant instead of @xmath230 given that it was a fw step ) .",
    "after a correction step in the mnp algorithm , we have that the current iterate is the minimizer over the active set , and thus @xmath494 .",
    "we thus have @xmath495 , which means that a standard fw step would yield a geometric decrease of error .",
    "relating @xmath496 and @xmath436 unlike in the afw and approximate fcfw case , we can remove the factor of  @xmath497 in front of  @xmath436 in  , removing the factor of  @xmath498 appearing in  , and also giving a geometric decrease with factor @xmath499 when @xmath465 . ]",
    "it thus remains to show that the mnp - correction is making as much progress as a fw line - search .",
    "consider @xmath500 as defined in algorithm  [ alg : mnp ] .",
    "if it belongs to @xmath501 , then it has made more progress than a fw line - search as @xmath15 and @xmath14 belongs to @xmath501 .",
    "the next possibility is the crucial step in the proof : suppose that exactly one atom was removed from the correction polytope and that @xmath500 does not belong to @xmath501 ( as this was covered in the above case ) .",
    "this means that @xmath502 belongs to _ the relative interior _ of @xmath503 .",
    "because @xmath502 is by definition the affine minimizer of @xmath5 on @xmath503 , the negative gradient @xmath504 is pointing away to the polytope @xmath503 ( by the optimality condition ) .",
    "but @xmath503 is a _ facet _ of @xmath501 ,",
    "this means that @xmath504 determines a facet of @xmath501 ( i.e. @xmath505 for all @xmath506 ) .",
    "this means that @xmath502 is also the minimizer of @xmath5 on @xmath501 and thus has made more progress than a fw line - search .    in the case",
    "that two atoms are removed from @xmath501 , we can not make this argument anymore ( it is possible that @xmath507 makes less progress than a fw line - search ) ; but in this case , the size of the active set is reduced by one ( we have a drop step ) , and thus we can use the same argument as in the afw algorithm to bound the number of such steps .      in this case , @xmath508 , so we do not even need a factor of 2 to relate the gaps ( with the same consequence as in mnp in getting slightly bigger constants ) .",
    "we can re - use the same argument as in the afw algorithm to get a geometric progress when @xmath134 . when @xmath63 we can either have a drop step if @xmath15 was already in @xmath20 , or a swap step if @xmath15 was also added to @xmath20 and so @xmath509 .",
    "the number of drop steps can be bounded similarly as in the afw algorithm . on the other hand ,",
    "in the worst case , there could be a very large number of swap steps .",
    "we provide here a very loose bound , though it would be interesting to use other properties of the objective to prove that this worst case scenario can not happen .",
    "we thus bound the maximum number of swap steps between two ` good steps ' ( very loosely ) .",
    "let @xmath510 be the number of possible atoms , and let @xmath266 be the size of the current active set @xmath511 .",
    "when doing a drop step @xmath512 , there are two possibilities : either we move all the mass from @xmath54 to a new atom @xmath513 i.e. @xmath514 and @xmath515 ( a swap step ) ; or we move all the mass from @xmath54 to an old atom @xmath516 i.e. @xmath517 ( a ` full drop step ' ) .",
    "when doing a swap step , the set of _ possible values _ for the coordinates @xmath518 _ do not change _ , they are only ` swapped around ' amongst the @xmath519 possible slots .",
    "the maximum number of possible consecutive swap steps without revisiting an iterate already seen is thus bounded by the number of ways we can assign @xmath266 numbers in @xmath519 slots ( supposing the @xmath266 coordinates were all distinct in the worst case ) , which is @xmath520 . note that because the algorithm does a line - search in a strict descent direction at each iteration , we always have @xmath521 unless @xmath14 is already optimal .",
    "this means that the algorithm can not revisit the same point unless it has converged .",
    "when doing a ` full drop step ' , the set of coordinates changes , but the size of the active set is reduced by one ( thus @xmath266 reduced by one ) . in the worst case , we will do a maximum number of swap steps , followed by a full drop step , repeated so on all the way until we reach an active set of only one element ( in which case there is a maximum number of @xmath519 swap steps ) .",
    "starting with an active set of @xmath266 coordinates , the maximum number of swap steps @xmath522 without doing any ` good step ' ( which would also change the set of coordinates ) , is thus upper bounded by : @xmath523 as claimed .",
    "[ [ proof - of - sublinear - convergence - for - general - convex - objectives - i.e .- when - mu_hspace-0.08emfhspace0.06emtextnormalscriptsize - a0 . ] ] proof of sublinear convergence for general convex objectives ( i.e. when @xmath524 ) .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for the good steps of the mnp algorithm and the pairwise fw algorithm , we have the reduction of suboptimality given by   without the factor of @xmath497 in front of @xmath525 .",
    "this is the standard recurrence that appears in the convergence proof of frank - wolfe ( see for example equation  ( 4 ) in  ( * ? ? ?",
    "* proof of theorem  1 ) ) , yielding the usual convergence : @xmath526 where @xmath136 is the number of good steps up to iteration @xmath0 , and @xmath527 for mnp and @xmath528 for pfw .",
    "the number of good steps for mnp is @xmath138 , while for pfw , we have the ( useless ) lower bound @xmath139 .",
    "for fcfw with exact correction , the rate   was already proven in  @xcite with @xmath137 . on the other hand , for fcfw with approximate correction , and for afw , the factor of @xmath497 in front of the gap  @xmath436 in the suboptimality bound",
    "somewhat complicates the convergence proof .",
    "the recurrence we get for the suboptimality is the same as in equation  ( 20 ) of  ( * ? ? ?",
    "* proof of theorem  c.1 ) , with @xmath529 and @xmath530 , giving the following suboptimality bound : @xmath531 where @xmath532 for afw and @xmath533 for fcfw with approximate correction .",
    "moreover , the number of good steps is @xmath138 for afw , and @xmath137 for fcfw .",
    "a weaker ( logarithmic ) dependence on the initial error  @xmath534 can also be obtained by following a tighter analysis ( see  ( * ? ? ?",
    "* theorem  c.4 ) or  ( * ? ? ?",
    "* lemma  d.5 and theorem  d.6 ) ) , though we only state the simpler result here .",
    "we describe here a simple experiment to test how tight the constant in the linear convergence rate of theorem  [ thm : megaconvergencetheorem2 ] is .",
    "we test both afw and pfw on the triangle domain with corners at the locations @xmath535 , @xmath536 and @xmath537 , for increasingly small @xmath538 ( see figure  [ fig : triangle ] ) .",
    "the pyramidal width @xmath540 becomes vanishingly small as @xmath541 ; the diameter is @xmath542 .",
    "we consider the optimization of the function @xmath539 with @xmath543 on one edge of the domain .",
    "note that the condition number of @xmath5 is @xmath544 .",
    "the bound on the linear convergence rate @xmath173 according to theorem  [ thm : megaconvergencetheorem2 ] ( using @xmath545   and @xmath546  ) is @xmath547 for pfw and @xmath548 for afw .",
    "the theoretical constant here is thus @xmath549 .",
    "we consider @xmath538 varying from @xmath550 to @xmath551 , and thus theoretical rates varying on a wide range from @xmath552 to @xmath553 .",
    "we compare the theoretical rate @xmath173 with the empirically observed one by estimating @xmath554 in the relationship @xmath555 ( using linear regression on the semilogarithmic scale ) .",
    "for each @xmath538 , we run both afw and pfw for 2000 iterations starting from 20 different random starting points is obtained by taking a random convex combination of the corners of the domain . ] in the interior of the triangle domain .",
    "we disregard the starting points that yield a drop step ( as then the algorithm converges in one iteration ; these happen for about @xmath195 of the starting points ) . note that as there is no drop step in our setup , we do not need to divide by two the effective rate as is done in theorem  [ thm : megaconvergencetheorem2 ] ( the number of ` good steps ' is @xmath137 ) .    .",
    "we plot median values over 20 random starting points ; the error bars represent the @xmath556 and @xmath557 quantiles .",
    "the empirical rate for pfw is closely following the theoretical one.,title=\"fig : \" ] ( a ) ratio of empirical rate vs. theoretical one    .",
    "we plot median values over 20 random starting points ; the error bars represent the @xmath556 and @xmath557 quantiles .",
    "the empirical rate for pfw is closely following the theoretical one.,title=\"fig : \" ] + ( b ) ratio of empirical rate of pfw vs. afw    figure  [ fig : triangleresults ] presents the results . in figure",
    "[ fig : triangleresults](a ) , we plot the ratio of the estimated rate over the theoretical rate @xmath558 for both pfw and afw as @xmath538 varies .",
    "note that the ratio is very stable for pfw ( around 10 ) , despite the rate changing through six orders of magnitude , demonstrating the empirical tightness of the constant for this domain .",
    "the ratio for afw has more fluctuations , but also stays within a stable range .",
    "we can also do a finer analysis than the pyramidal width and consider the finite number possibilities for the worst case angles for @xmath559 .",
    "this gives the tighter constant @xmath560 for our triangle domain , gaining a factor of about  4 , but still not matching yet the empirical observation for pfw .    in figure",
    "[ fig : triangleresults](b ) , we compare the empirical rate for pfw vs. the one for afw . for bigger theoretical rates , pfw appears to converge faster",
    ". however , afw gets a slightly better empirical rate for very small rates ( small angles ) ."
  ],
  "abstract_text": [
    "<S> the frank - wolfe ( fw ) optimization algorithm has lately re - gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications . </S>",
    "<S> however , its convergence rate is known to be slow ( sublinear ) when the solution lies at the boundary . </S>",
    "<S> a simple less - known fix is to add the possibility to take ` away steps ' during optimization , an operation that importantly _ does not _ require a feasibility oracle . in this paper , we highlight and clarify several variants of the frank - wolfe optimization algorithm that have been successfully applied in practice : away - steps fw , pairwise fw , fully - corrective fw and wolfe s minimum norm point algorithm , and prove for the first time that they all enjoy global linear convergence , under a weaker condition than strong convexity of the objective . </S>",
    "<S> the constant in the convergence rate has an elegant interpretation as the product of the ( classical ) condition number of the function with a novel geometric quantity that plays the role of a ` condition number ' of the constraint set . </S>",
    "<S> we provide pointers to where these algorithms have made a difference in practice , in particular with the flow polytope , the marginal polytope and the base polytope for submodular optimization .    </S>",
    "<S> the frank - wolfe algorithm @xcite ( also known as _ conditional gradient _ ) is one of the earliest existing methods for constrained convex optimization , and has seen an impressive revival recently due to its nice properties compared to projected or proximal gradient methods , in particular for sparse optimization and machine learning applications .    on the other hand , the classical projected gradient and proximal methods </S>",
    "<S> have been known to exhibit a very nice adaptive acceleration property , namely that the the convergence rate becomes linear for strongly convex objective , i.e. that the optimization error of the same algorithm after @xmath0 iterations will decrease geometrically with @xmath1 instead of the usual @xmath2 for general convex objective functions . </S>",
    "<S> it has become an active research topic recently whether such an acceleration is also possible for frank - wolfe type methods .    </S>",
    "<S> [ [ contributions . ] ] contributions . </S>",
    "<S> + + + + + + + + + + + + + +    we clarify several variants of the frank - wolfe algorithm and show that they all converge linearly for any strongly convex function optimized over a polytope domain , with a constant bounded away from zero that only depends on the geometry of the polytope . </S>",
    "<S> our analysis does _ not _ depend on the location of the true optimum with respect to the domain , which was a disadvantage of earlier existing results such as @xcite , and the newer work of @xcite , as well as the line of work of @xcite which rely on robinson s condition @xcite . </S>",
    "<S> our analysis yields a weaker sufficient condition than robinson s condition ; in particular we can have linear convergence even in some cases when the function has more than one global minima , and is not globally strongly convex . </S>",
    "<S> the constant also naturally separates as the product of the condition number of the function with a novel notion of condition number of a polytope , which might have applications in complexity theory .    </S>",
    "<S> [ [ related - work . ] ] related work . </S>",
    "<S> + + + + + + + + + + + + +    for the classical frank - wolfe algorithm , @xcite showed a linear rate for the special case of quadratic objectives when the optimum is in the strict interior of the domain , a result already subsumed by the more general  @xcite . </S>",
    "<S> the early work of  @xcite showed linear convergence for _ strongly convex constraint sets _ , under the strong requirement that the gradient norm is not too small ( see  @xcite for a discussion ) . </S>",
    "<S> the away - steps variant of the frank - wolfe algorithm , that can also remove weight from ` bad ' atoms in the current active set , was proposed in @xcite , and later also analyzed in  @xcite . </S>",
    "<S> the precise method is stated below in algorithm  [ alg : afw ] . </S>",
    "<S> @xcite showed a ( local ) linear convergence rate on polytopes , but the constant unfortunately depends on the distance between the solution and its relative boundary , a quantity that can be arbitrarily small . </S>",
    "<S> more recently , @xcite have obtained linear convergence results in the case that the optimum solution satisfies robinson s condition @xcite . in a different recent line of work , </S>",
    "<S> @xcite have studied a variation of fw that repeatedly moves mass from the worst vertices to the standard fw vertex until a specific condition is satisfied , yielding a linear rate on strongly convex functions . </S>",
    "<S> their algorithm requires the knowledge of several constants though , and moreover is not adaptive to the best - case scenario , unlike the frank - wolfe algorithm with away steps and line - search . </S>",
    "<S> none of these previous works was shown to be affine invariant , and most require additional knowledge about problem specific parameters .    [ </S>",
    "<S> [ setup . ] ] setup . </S>",
    "<S> + + + + + +    we consider general constrained convex optimization problems of the form : @xmath3 where @xmath4 is a _ finite _ set of vectors that we call _ atoms _ .. ] we assume that the function  @xmath5 is @xmath6-strongly convex with @xmath7-lipschitz continuous gradient over  @xmath8 </S>",
    "<S> . we also consider weaker conditions than strong convexity for  @xmath5 in section  [ sec : nonstronglyconvex ] . as  </S>",
    "<S> @xmath9 is finite , @xmath8 is a ( convex and bounded ) polytope . </S>",
    "<S> the methods that we consider in this paper only require access to a _ linear minimization oracle _ @xmath10 associated with the domain  @xmath8 through a generating set of atoms  @xmath9 . </S>",
    "<S> this oracle is defined as to return a minimizer of a linear subproblem over  @xmath11 , for any given direction @xmath12 .    </S>",
    "<S> [ [ examples . ] ] examples . </S>",
    "<S> + + + + + + + + +    optimization problems of the form appear widely in machine learning and signal processing applications . </S>",
    "<S> the set of atoms @xmath9 can represent combinatorial objects of arbitrary type . </S>",
    "<S> efficient linear minimization oracles often exist in the form of dynamic programs or other combinatorial optimization approaches . as an example from tracking in computer vision </S>",
    "<S> , @xmath9 could be the set of integer flows on a graph  @xcite , where @xmath13 can be efficiently implemented by a minimum cost network flow algorithm . in this case , @xmath8 can also be described with a polynomial number of linear inequalities . </S>",
    "<S> but in other examples , @xmath8 might not have a polynomial description in terms of linear inequalities , and testing membership in @xmath8 might be much more expensive than running the linear oracle . </S>",
    "<S> this is the case when optimizing over the _ base polytope _ </S>",
    "<S> , an object appearing in submodular function optimization  @xcite . there </S>",
    "<S> , the @xmath13 oracle is a simple greedy algorithm . </S>",
    "<S> another example is when @xmath9 represents the possible consistent value assignments on cliques of a markov random field ( mrf ) ; @xmath8 is the _ marginal polytope _  </S>",
    "<S> @xcite , where testing membership is np - hard in general , though efficient linear oracles exist for some special cases  @xcite . </S>",
    "<S> optimization over the marginal polytope appears for example in structured svm learning  @xcite and variational inference  @xcite .    </S>",
    "<S> [ [ the - original - frank - wolfe - algorithm . ] ] the original frank - wolfe algorithm . </S>",
    "<S> + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the frank - wolfe ( fw ) optimization algorithm  @xcite , also known as _ conditional gradient _  @xcite , </S>",
    "<S> is particularly suited for the setup   where @xmath8 is only accessed through the linear minimization oracle . </S>",
    "<S> it works as follows : at a current iterate  @xmath14 , the algorithm finds a feasible search atom @xmath15 to move towards by minimizing the linearization of the objective function @xmath5 over @xmath8 ( line  3 in algorithm  [ alg : afw ] )  this is where the linear minimization oracle @xmath13 is used . </S>",
    "<S> the next iterate @xmath16 is then obtained by doing a line - search on @xmath5 between @xmath14 and @xmath15 ( line  11 in algorithm  [ alg : afw ] ) . </S>",
    "<S> one reason for the recent increased popularity of frank - wolfe - type algorithms is the sparsity of their iterates : in iteration @xmath0 of the algorithm , the iterate can be represented as a sparse convex combination of at most @xmath17 atoms @xmath18 of the domain  @xmath8 , which we write as @xmath19 . </S>",
    "<S> we write @xmath20 for the _ active set _ , containing the previously discovered search atoms @xmath21 for @xmath22 that have non - zero _ weight _ @xmath23 in the expansion ( potentially also including the starting point  @xmath24 ) . </S>",
    "<S> while tracking the active set @xmath20 is not necessary for the original fw algorithm , the improved variants of fw that we discuss will require that @xmath20 is maintained .    </S>",
    "<S> [ [ zig - zagging - phenomenon . ] ] zig - zagging phenomenon . </S>",
    "<S> + + + + + + + + + + + + + + + + + + + + + + +    when the optimal solution lies at the boundary of @xmath8 , the convergence rate of the iterates is slow , i.e. sublinear : @xmath25 , for @xmath26 being an optimal solution  @xcite . </S>",
    "<S> this is because the iterates of the classical fw algorithm start to zig - zag between the vertices defining the face containing the solution @xmath26 ( see left of figure  [ fig : fwzigzag ] ) . </S>",
    "<S> in fact , the @xmath27 rate is tight for a large class of functions : @xcite showed ( roughly ) that @xmath28 for any @xmath29 when @xmath26 lies on a face of @xmath8 with some additional regularity assumptions . </S>",
    "<S> note that this lower bound is different than the @xmath30 one presented in ( * ? ? ? </S>",
    "<S> * lemma 3 ) which holds for all one - atom - per - step algorithms but assumes high dimensionality @xmath31 . </S>"
  ]
}