{
  "article_text": [
    "in this paper , we study the problem of computing a low rank approximation to a given matrix @xmath0 with @xmath1 rows and @xmath2 columns .",
    "a @xmath3-rank approximation matrix is a matrix of rank @xmath3 ( i.e. , the space spanned by the rows of the matrix is of dimension @xmath3 ) .",
    "the standard measure of the quality of approximation of a matrix @xmath0 by a matrix @xmath6 is the squared frobenius norm @xmath10 , which is the sum of the squared entries of the matrix @xmath11 .",
    "the optimal @xmath3-rank approximation can be computed by in time @xmath12 . however ,",
    "if @xmath2 and @xmath1 are large ( conceptually , consider the case @xmath13 ) , then this is unacceptably slow . since one has to read the input at least once , a running time of @xmath14 is required for any matrix approximation algorithm .",
    "since @xmath8 is the size of the input , we will refer to running time of @xmath5 as being linear .",
    "[ [ previous - work . ] ] previous work",
    ". + + + + + + + + + + + + + +    frieze _",
    "et  al . _",
    "@xcite showed how to sample the rows of the matrix , so that the resulting sample span a matrix which is a good low rank approximation .",
    "the error of the resulting approximation has an additive term that depends on the norm of the input matrix .",
    "achlioptas and mcsherry @xcite came up with an alternative sampling scheme that yields a somewhat similar result .",
    "note , that a multiplicative approximation error which is proportional to the error of the optimal approximation is more desirable as it is potentially significantly smaller than the additive error .",
    "however , computing quickly a small multiplicative low - rank approximation remained elusive .",
    "recently , deshpande _",
    "et  al . _",
    "@xcite , building on the work of frieze _",
    "et  al . _",
    "@xcite , made a step towards this goal .",
    "they showed a multipass algorithm with an additive error that decreases exponentially with the number of passes .",
    "they also introduced the intriguing concept of volume sampling .",
    "bdoiu and indyk @xcite had recently presented a linear time algorithm for this problem ( i.e. , @xmath15 ) , for the special case @xmath16 , where @xmath8 .",
    "they also mention that , for a fixed @xmath3 , a running time of @xmath17 is doable .",
    "in fact , using the lanczos method or power method yields a @xmath18-approximation in time roughly @xmath19 . for @xmath16 , this was proved in the work of kuczynski and wozniakowski @xcite , and the proof holds in fact for any fixed @xmath3 .",
    "we also point out another algorithm , with similar performance , using known results , see remark  [ rem : slow : but : easy ] .",
    "thus , the challenge in getting a linear running time is getting rid of the @xmath20 factor in the running time .    in practice",
    ", the power method requires @xmath21 time , where @xmath22 is the number of iterations performed , which depends on the distribution of the eigenvalues , and how close they are to each other .",
    "in fact , the @xmath23 term can be replaced by the number of non - zero entries in the matrix . in particular , if the matrix is sparse ( and @xmath22 is sufficiently small ) the running time to compute a low - rank approximation to the matrix is sublinear .",
    "one can interpret each row of the matrix @xmath0 as a point in @xmath24 .",
    "this results in a point - set @xmath25 of @xmath1 points in @xmath24 .",
    "the problem now is to compute a @xmath3-flat ( i.e. , a @xmath3-dimensional linear subspace ) such that the sum of the squared distances of the points of @xmath25 to the @xmath3-flat is minimized .",
    "this problem is known as the @xmath26-fitting problem .",
    "the related @xmath27-fitting problem was recently studied by clarkson @xcite and the @xmath28-fitting problem was recently studied by har - peled and varadarajan @xcite ( see references therein for the history of these problems ) .",
    "[ [ our - results . ] ] our results .",
    "+ + + + + + + + + + + +    the extensive work on this problem mentioned above still leaves open the nagging question of whether one can get a multiplicative low - rank approximation to a matrix in linear time ( which is optimal in this case ) . in this paper , we answer this question in the positive and show a linear - time algorithm with small multiplicative error , using a more geometric interpretation of this problem . in particular",
    ", we present an algorithm that with constant probability computes a matrix @xmath6 such that @xmath29 , where @xmath30 is the minimum error of a @xmath3-rank approximation to @xmath0 , and @xmath31 is a prespecified error parameter . for input of size @xmath8 the running time of the new algorithm",
    "is ( roughly ) @xmath32 ; see theorem  [ theo : main ] for details .",
    "our algorithm relies on the observation that a small random sample spans a flat which is good for most of the points .",
    "this somewhat intuitive claim is proved by using @xmath4-nets and vc - dimension arguments performed ( conceptually ) on the optimal ( low - dimensional ) @xmath3-flat ( see lemma  [ lemma : r : sample ] below ) .",
    "next , we can filter the points , and discover the outliers for this random flat ( i.e. , those are the points which are `` far '' from the random flat ) . since the number of outliers is relatively small , we can approximate them quickly using recursion . merging the random flat together with the flat returned from the recursive call results in a flat that approximates well all the points , but its rank is too high .",
    "this can be resolved by extracting the optimal solution on the merged flat , and improving it into a good approximation using the techniques of frieze _",
    "_ @xcite and deshpande _",
    "a @xmath3- @xmath33 is a linear subspace of @xmath24 of dimension @xmath3 ( in particular , the origin is included in @xmath33 ) .    for a point @xmath34 and a flat @xmath33 ,",
    "let @xmath35 denote the of @xmath34 to @xmath33 .",
    "formally , @xmath36 . we will denote the squared distance by @xmath37 .",
    "the of fitting a point set @xmath25 of @xmath1 points in @xmath24 to a flat @xmath33 is @xmath38 the @xmath3-flat realizing @xmath39 is denoted by @xmath40 , and its price is @xmath30 .",
    "the of a set @xmath41 , denoted by @xmath42 , is the smallest linear subspace that contains @xmath43 .",
    "the input is a set @xmath25 of @xmath1 points in @xmath24 and its size is @xmath44 .",
    "let @xmath33 be a flat .",
    "for a parameter @xmath46 , a from @xmath25 is generated by picking point @xmath47 to the random sample with probability @xmath48 where @xmath49 is an appropriate constant , and the size of the sample is at least @xmath46 .",
    "it is not immediately clear how to compute a @xmath45-sample efficiently since we have to find the right value of @xmath49 such that the sample generated is of the required size ( and not much bigger than this size ) .",
    "however , as described by frieze _ et  al . _",
    "@xcite , this can be achieved by bucketing the basic probabilities ( i.e. , @xmath50 for a point @xmath51 ) , such that each bucket corresponds to points with probabilities in the range @xmath52 $ ] .",
    "it is now easy to verify that given a target size @xmath46 , one can compute the corresponding @xmath49 in linear time , such that the expected size of the generated sample is , say , @xmath53 .",
    "we also note , that by the chernoff inequality , with probability @xmath54 , the sample is of size in the range @xmath55 $ ] .",
    "[ lemma : weighted : sample ] given a flat @xmath33 and a parameter @xmath46 , one can compute a @xmath45-sample from a set @xmath25 of @xmath1 points in @xmath24 in @xmath56 time .",
    "the sample is of size in the range @xmath55 $ ] with probability @xmath54 .",
    "furthermore , if we need to generate @xmath57 independent @xmath45-samples , all of size in the range @xmath55 $ ] , then this can be done in @xmath58 time , and this bound on the running time holds with probability @xmath59 .",
    "the first claim follows from the above description . as for the second claim , generate @xmath60 samples independently .",
    "note , that we need to compute the distance of the points of @xmath25 to @xmath33 only once before we compute these @xmath61 samples . by the chernoff inequality ,",
    "the probability that less than @xmath57 samples , out of the @xmath60 samples generated , would be in the valid size range is smaller than @xmath62 .",
    "[ lemma : flat : extract ]",
    "let @xmath33 be a @xmath63-flat in @xmath24 , and let @xmath25 be a set of @xmath1 points in @xmath24 .",
    "then , one can extract the @xmath3-flat @xmath64 that minimizes @xmath65 in @xmath66 time , where @xmath67 .",
    "we project the points of @xmath25 onto @xmath33 .",
    "this takes @xmath68 time , and let @xmath69 denote the projected set .",
    "next , we compute , using , the best approximation to @xmath69 by a @xmath3-flat @xmath70 inside @xmath33 . since @xmath33 is @xmath63 dimensional , we can treat the point of @xmath69 as being @xmath63 dimensional and this can be done in @xmath71 time overall .    for a point @xmath51 ,",
    "let @xmath72 be its projection onto @xmath33 and let @xmath73 be the projection of @xmath74 onto @xmath70 . by elementary argumentation , we know that the projection of @xmath72 onto @xmath70 is @xmath73 . as such",
    ", @xmath75 implying that @xmath76 .",
    "since we computed the flat @xmath70 that minimizes @xmath77 , it follows that we computed the @xmath3-flat @xmath70 lying inside @xmath33 that minimizes the approximation price @xmath65 .",
    "[ sec : fast : additive ]    we need the following result ( the original result of @xcite is in fact slightly stronger ) .",
    "[ theo : sample : a ] let @xmath25 be a set of @xmath1 points in @xmath24 , @xmath33 be a @xmath63-flat , @xmath78 be a @xmath45-sample of size @xmath79 , and let @xmath80 .",
    "let @xmath81 be the @xmath3-flat lying inside @xmath70 that minimizes @xmath82 .",
    "then , we have that @xmath83 } } \\leq { \\mu_{\\mathrm{opt}}{\\mleft(p , k\\mright ) } } + \\frac{k}{r }      { \\mu_{{\\euscript{f}}}{\\mleft(p\\mright)}}.      \\end{aligned}\\ ] ] furthermore , @xmath81 can be computed in @xmath84 time .",
    "the correctness follows immediately from the work of deshpande _",
    "_ @xcite .    as for the running time",
    ", we compute the distance of the points of @xmath25 to @xmath33 .",
    "this takes @xmath85 time .",
    "next , we compute @xmath80 . finally , we compute the best @xmath3-flat @xmath86 that approximates @xmath25 using lemma  [ lemma : flat : extract ] , which takes @xmath87 since @xmath88 .    we need a probability guaranteed version of theorem  [ theo : sample : a ] , as follows .",
    "[ lemma : guaranteed ] let @xmath25 be a set of @xmath1 points in @xmath24 , @xmath33 be a @xmath89-flat , and parameters @xmath90 and @xmath31 .",
    "then , one can compute a @xmath3-flat @xmath81 , such that , with probability at least @xmath91 , we have @xmath92 , where @xmath93 the running time of the algorithm is @xmath94 .",
    "the algorithm succeeds with probability @xmath95 .",
    "set @xmath96 .",
    "generate @xmath97 independent @xmath45-samples , of size in the range @xmath55 $ ] , using the algorithm of lemma  [ lemma : weighted : sample ] .",
    "next , for each random sample @xmath98 , we apply the algorithm of theorem  [ theo : sample : a ] to it , generating a @xmath3-flat @xmath99 , for @xmath100 . by the markov inequality",
    ", we have @xmath101 } } } \\mright ] } } \\leq \\frac{1}{1+{\\varepsilon}/4}.      \\end{aligned}\\ ] ] as such , with probability @xmath102 we have @xmath103 clearly , the best @xmath3-flat computed , which realizes the price @xmath104 , has a price which is at most @xmath105 , and this holds with probability @xmath106 .",
    "note , that the time to generate the samples is @xmath107 , and this bounds holds with probability @xmath108 . as such , the overall running time of the algorithm is @xmath109 .",
    "[ lemma : easy ] let @xmath33 be a given @xmath89-flat , @xmath25 a set of points @xmath1 in @xmath24 , and @xmath110 a given parameter .",
    "assume that @xmath111 .",
    "then , one can compute a @xmath3-flat @xmath70 such that @xmath112 the running time of the algorithm is @xmath113 and it succeeds with probability @xmath95 .",
    "let @xmath114 , and set @xmath115 . in the @xmath116thstep of the algorithm , we generate a @xmath117-sample @xmath98 with @xmath118 .",
    "if the generated sample is not of size in the range @xmath55 $ ] , we resample .",
    "next , we apply theorem  [ theo : sample : a ] to @xmath119 and @xmath98 .",
    "if the new generated flat @xmath120 is more expensive than the @xmath119 then we set @xmath121 . otherwise , we set @xmath122 .",
    "we perform this improvement loop @xmath57 times .",
    "note , that if @xmath123 then @xmath124 } } \\leq { \\mu_{\\mathrm{opt}}{\\mleft(p , k\\mright ) } } + \\frac{k}{20k }      { \\mu_{{\\euscript{f}}_{i-1}}{\\mleft(p\\mright ) } } \\leq { \\mleft ( \\frac{1}{8 } +         \\frac{1}{20}\\mright)}{\\mu_{{\\euscript{f}}_{i-1}}{\\mleft(p\\mright ) } } \\leq      \\frac{1}{4}{\\mu_{{\\euscript{f}}_{i-1}}{\\mleft(p\\mright ) } } ,      \\end{aligned}\\ ] ] by theorem  [ theo : sample : a ] . by markov s inequality",
    ", it follows that @xmath125 } } \\geq 1/2 $ ] .",
    "namely , with probability half , we shrink the price of the current @xmath3-flat by a factor of @xmath126 ( in the worst case the price remains the same ) .",
    "since each iteration succeeds with probability half , and we perform @xmath57 iterations , it follows that with probability larger than @xmath127 ( by the chernoff inequality ) , the price of the last flat @xmath128 computed is @xmath129 .",
    "similarly , we know by the chernoff inequality that the algorithm performed at most @xmath130 sampling rounds till it got the required @xmath57 good samples , and this holds with probability @xmath131 .",
    "thus , computing @xmath128 takes @xmath132 time .",
    "finally , we apply lemma  [ lemma : guaranteed ] to @xmath128 with @xmath133 and success probability @xmath134 .",
    "the resulting @xmath3-flat has price @xmath135 as required .",
    "note , that overall this algorithm succeeds with probability @xmath91 .",
    "[ rem : slow : but : easy ] har - peled and varadarajan @xcite presented an algorithm that computes , in @xmath136 time , a @xmath3-dimensional affine subspace ( i.e. , this subspace does not necessarily contains the origin ) which approximates , up to a factor of @xmath137 , the best such subspace that minimizes the maximum distance of a point of @xmath25 to such a subspace .",
    "namely , this is an approximation algorithm for the @xmath28-fitting problem , and let @xmath33 denote the computed @xmath138-flat ( i.e. , we turn the @xmath3-dimensional affine subspace into a @xmath138-flat by adding the origin to it ) .",
    "it is easy to verify that @xmath33 provides a @xmath139 approximation to the optimal @xmath3-flat realizing @xmath30 ( i.e. , the squared @xmath26-fitting problem ) . to see that , we remind the reader that the @xmath28 and @xmath26 metrics are the same up to a factor which is bounded by the dimension ( which is @xmath1 in this case ) .",
    "we can now use lemma  [ lemma : easy ] starting with @xmath33 , with @xmath140 .",
    "we get the required @xmath18-approximation , and the running time is @xmath141 .",
    "the algorithm succeeds with probability @xmath95 .",
    "[ lemma : distance:2 ] let @xmath142 be the squared distance function of a point @xmath143 to a given @xmath63-flat @xmath33 .",
    "let @xmath144 be any two points .",
    "we have :    * for any @xmath145 , @xmath146 .",
    "that is , @xmath147 is convex .",
    "* let @xmath148 be the line through @xmath143 and @xmath149 , and let @xmath150 be a point on @xmath148 such that @xmath151 . then , @xmath152    the claims of this lemma follow by easy elementary arguments and the proof is included only for the sake of completeness .",
    "\\(i ) rotate and translate space , such that @xmath33 becomes the @xmath63-flat spanned by the first @xmath63 unit vectors of the standard orthonormal basis of @xmath24 , denoted by @xmath153 .",
    "let @xmath154 and @xmath155 be the images of @xmath143 and @xmath149 , respectively . consider the line @xmath156 .",
    "we have that @xmath157 , which is a convex function since it is a sum of convex functions , where @xmath158 and @xmath159 .",
    "as such , @xmath160 and @xmath161 , and @xmath162 . by the convexity of @xmath163 , we have @xmath164 , as claimed .",
    "\\(ii ) let @xmath165 be the projection of @xmath143 to @xmath33 , and let @xmath166 be the parallel line to @xmath148 passing through @xmath165 .",
    "let @xmath167 and @xmath168 be the translation of @xmath149 and @xmath150 onto @xmath166 by the vector @xmath169 . by similarity of triangles , we have @xmath170    thus , @xmath171 since @xmath172 .",
    "[ lemma : extract : k : flat ] let @xmath25 be a set of @xmath1 points in @xmath24 , and let @xmath33 be a flat in @xmath24 , such that @xmath173 , where @xmath174 is a constant .",
    "then , there exists a @xmath3-flat @xmath70 that lies inside @xmath33 such that @xmath175 .",
    "let @xmath70 be the projection of @xmath40 onto @xmath33 , where @xmath40 denotes the optimal @xmath3-flat that approximates @xmath25 .",
    "let @xmath69 be the projection of @xmath25 onto @xmath33 .",
    "we have that @xmath176 . on the other hand , @xmath177 where @xmath72 denotes the projection of a point @xmath51 onto @xmath33 .",
    "furthermore , @xmath178 we conclude that @xmath179 .",
    "the following lemma , testifies that a small random sample induces a low - rank flat that is a good approximation for most points .",
    "the random sample is picked by ( uniformly ) choosing a subset of the appropriate size from the input set , among all subsets of this size .",
    "[ lemma : r : sample ] let @xmath25 be a set of @xmath1 points in @xmath24 , @xmath90 a parameter , and let @xmath78 be a random sample of @xmath25 of size @xmath180 . consider the flat @xmath181 , and let @xmath182 be the set of the @xmath183 points of @xmath25 which are closest to @xmath33 .",
    "then @xmath184 with probability @xmath95 .",
    "let @xmath185 be the average contribution of a point to the sum @xmath186 , where @xmath40 is the optimal @xmath3-flat approximating @xmath25 .",
    "let @xmath187 be the points of @xmath25 that contribute at most @xmath188 to @xmath189 ; namely , those are the points in distance at most @xmath190 from @xmath40 . by markov s inequality , we have @xmath191 .",
    "next , consider the random sample when restricted to the points of @xmath187 ; namely , @xmath192 . by the chernoff inequality , with probability at least @xmath193",
    ", we have that @xmath194 .",
    "next , consider the projections of @xmath195 and @xmath187 onto @xmath40 , denoted by @xmath196 and @xmath197 , respectively .",
    "let @xmath198 be the largest volume ellipsoid , contained in @xmath40 that is enclosed inside @xmath199 , where @xmath199 denotes the convex - hull of @xmath196 .",
    "let @xmath200 be the expansion of @xmath198 by a factor of @xmath3 . by john",
    "s theorem @xcite , we have @xmath201 .    since @xmath196 is a large enough random sample of @xmath197 , we know that @xmath196 is a @xmath202-net for ellipsoids for the set @xmath197 .",
    "this follows as ellipsoids at euclidean space @xmath203 have vc - dimension @xmath204 , as can be easily verified .",
    "( one way to see this is by lifting the points into @xmath204 dimensions , where an ellipsoid in the original space is mapped into a half space . ) as such , we can use the @xmath4-net theorem of haussler and welzl @xcite , which implies that @xmath205 , with probability at least @xmath206 .",
    "this holds since no points of @xmath196 falls outside @xmath200 , and as such only @xmath207 points of @xmath197 might fall outside @xmath200 by the @xmath4-net theorem .",
    "( strictly speaking , the vc - dimension argument is applied here to ranges that are complements of ellipsoids . since the vc dimension is persevered under complement the claim still holds . )    for @xmath208 , let @xmath209 denote the squared distance of @xmath72 to @xmath33 . if @xmath210 then the corresponding original point @xmath211 and as such @xmath212 .",
    "implying that @xmath213 since @xmath214 . by lemma  [ lemma : distance:2 ] ( i ) , the function @xmath215 is convex and , for any @xmath216 , we have @xmath217 since @xmath218 .",
    "let @xmath219 denote the center of @xmath198 , and consider any point @xmath220 .",
    "consider the line @xmath148 connecting @xmath219 to @xmath72 , and let @xmath63 be one of the endpoints of @xmath221 .",
    "observe that @xmath222 .",
    "this implies by lemma  [ lemma : distance:2 ] ( ii ) that @xmath223 in particular , let @xmath224 be the set of points of @xmath187 such that their projection onto @xmath40 lies inside @xmath200 . for @xmath225 , we have that @xmath226 where @xmath72 is its the projection of @xmath74 onto @xmath40 .",
    "furthermore , @xmath227 .",
    "specifically , let @xmath182 be the set of @xmath183 points of @xmath25 closest to @xmath33 .",
    "since @xmath228 , we have that for any @xmath229 , it holds @xmath230 and as such @xmath231 .",
    "[ fig : algorithm ]    the new algorithm is depicted in figure  [ fig : algorithm ] .",
    "[ theo : main ] for a set @xmath25 of @xmath1 points in @xmath24 , and parameters @xmath4 and @xmath232 , the algorithm @xmath233 computes a @xmath3-flat @xmath234 of price @xmath235 .",
    "if @xmath236 , the running time of this algorithm is @xmath237 and it succeeds with probability @xmath91 , where @xmath44 is the input size .",
    "let us first bound the price of the generated approximation .",
    "the proof is by induction . for @xmath238",
    "the claim trivially holds . otherwise , by induction , we know that @xmath239 . also , @xmath240 , by lemma  [ lemma : r : sample ] . as such , @xmath241 . by lemma  [ lemma :",
    "extract : k : flat ] , the @xmath3-flat @xmath81 is of price @xmath242 .",
    "thus , by lemma  [ lemma : easy ] , we have @xmath243 , as required .",
    "next , we bound the probability of failure .",
    "we picked @xmath78 such that lemma  [ lemma : r : sample ] holds with probability @xmath244 . by induction",
    ", the recursive call succeeds with probability @xmath245 .",
    "finally , we used the algorithm of lemma  [ lemma : easy ] so that it succeeds with probability @xmath244 . putting everything together",
    ", we have that the probability of failure is bounded by @xmath246 , as required .    as for the running time",
    ", we have @xmath247 .",
    "next , @xmath248 assuming @xmath236 .",
    "if @xmath249 then we just use the standard algorithm to extract the best @xmath3-flat in time @xmath250 , which is better than the performance guaranteed by theorem  [ theo : main ] in this case .",
    "note , that for a fixed quality of approximation and constant confidence , the algorithm of theorem  [ theo : main ] runs in @xmath251 time , and outputs a @xmath3-flat as required .    also , by putting all the random samples used by theorem  [ theo : main ] together .",
    "we get a sample @xmath78 of size @xmath252 which spans a flat that contains a @xmath3-flat which is the required @xmath18-approximation .",
    "if the input is given as a matrix @xmath0 with @xmath1 rows and @xmath2 columns , we transform it ( conceptually ) to a point set @xmath25 of @xmath1 points in @xmath24 .",
    "the algorithm of theorem  [ theo : main ] applied to @xmath25 computes a @xmath3-flat @xmath234 , which we can interpret as a @xmath3-rank matrix approximation to @xmath0 , by projecting the points of @xmath25 onto @xmath234 , and writing down the @xmath116thprojected point as the @xmath116throw in the generated matrix @xmath6 .",
    "in fact , theorem  [ theo : main ] provides an approximation to the optimal @xmath3-rank matrix under the frobenius norm and not only the frobenius norm . specifically , given a matrix @xmath0 , theorem  [ theo : main ] computes a matrix @xmath6 of rank @xmath3 , such that @xmath253",
    "in this paper we presented a linear - time algorithm for low - rank matrix approximation .",
    "we believe that our techniques and more geometric interpretation of the problem is of independent interest .",
    "note , that our algorithm is not pass efficient since it requires @xmath254 passes over the input ( note however that the algorithm is io efficient ) .",
    "we leave the problem of how to develop a pass efficient algorithm as an open problem for further research .    surprisingly , the current bottleneck in our algorithm is the sampling lemma ( lemma  [ lemma : r : sample ] ) .",
    "it is natural to ask if it can be further improved .",
    "in fact , the author is unaware of a way of proving this lemma without using the ellipsoid argument ( via @xmath4-net or random sampling @xcite techniques ) , and an alternative proof avoiding this might be interesting .",
    "we leave this as an open problem for further research .",
    "this result emerged during insightful discussions with ke chen .",
    "the author would also like to thank muthu muthukrishnan and piotr indyk for useful comments on the problem studied in this paper .",
    "http://cm.bell-labs.com/who/clarkson/[k .",
    "l. clarkson ] .",
    "subgradient and sampling algorithms for @xmath255 regression . in _ proc .",
    "16th acm - siam sympos",
    ". discrete algs . _ _ _ , pages 257266 , philadelphia , pa , usa , 2005 . society for industrial and applied mathematics ."
  ],
  "abstract_text": [
    "<S> given a matrix @xmath0 with @xmath1 rows and @xmath2 columns , and fixed @xmath3 and @xmath4 , we present an algorithm that in linear time ( i.e. , @xmath5 ) computes a @xmath3-rank matrix @xmath6 with approximation error @xmath7 , where @xmath8 is the input size , and @xmath9 is the minimum error of a @xmath3-rank approximation to @xmath0 .    </S>",
    "<S> this algorithm succeeds with constant probability , and to our knowledge it is the first linear - time algorithm to achieve multiplicative approximation . </S>"
  ]
}