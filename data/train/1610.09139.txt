{
  "article_text": [
    "when analysing data , it is common practice to first explore the options available using various data plotting techniques . for regression models ,",
    "a key tool is to construct a plot of the model residuals in absolute value against fitted values .",
    "if there is only one covariate , we can use a plot of the residuals in absolute value against that covariate .",
    "this technique helps determine whether or not theoretical requirements for certain statistical procedures are satisfied , in particular whether or not the variation in the errors remains constant across values of the covariate .",
    "this is an important assumption , which we want to examine more closely .",
    "we will therefore consider the model with constant error variance @xmath1 , the _",
    "homoskedastic model _",
    "@xmath2 the function @xmath3 is the regression function and @xmath4 a positive constant .",
    "we consider a response variable @xmath5 , a covariate _ vector _",
    "@xmath6 and assume that @xmath6 and the random variable @xmath7 are independent , where @xmath7 has mean equal to zero and variance equal to one . when the variation in the data is not constant across the covariate values the _ heteroskedastic model _ is adequate : @xmath8 here @xmath9 is a scale function with @xmath10 = \\sigma_0 ^ 2 $ ] .",
    "model ( [ hetero ] ) contains the homoskedastic regression model as a ( degenerate ) special case with @xmath11 , a constant function . in order to be able to discriminate between both models",
    "we assume that @xmath9 is non - constant in the heteroskedastic case , i.e.  it varies with the values of the covariates @xmath6 .",
    "testing for heteroskedasticity is of great importance : many procedures lead to inconsistent and inaccurate results if the heteroskedastic model is appropriate but not properly handled .",
    "consider model ( [ hetero ] ) with a parametric regression function , e.g.  linear regression with @xmath12 .",
    "the ordinary least squares estimator @xmath13 of the parameter vector @xmath14 , which is constructed for the homoskedastic model , will still be consistent under heteroskedasticity",
    ". however it will be less accurate than a version that puts more weight on observations @xmath15 with small variance @xmath16 ( and less weight when the variance is large ) .",
    "the estimated variance of @xmath13 will be biased if the model is in fact heteroskedastic and testing hypotheses based on @xmath13 may lead to invalid conclusions .",
    "the relationship between the homoskedastic and the heteroskedastic models can be expressed in terms of statistical hypotheses : @xmath17 here @xmath18 is the distribution function of the covariates @xmath6 and @xmath19 is a space of scale functions .",
    "the null hypothesis corresponds to the homoskedastic model and the alternative hypothesis to the heteroskedastic model .",
    "rejection of the null hypothesis would imply that sufficient statistical evidence is gathered in the data to declare the homoskedastic model inappropriate .",
    "tests for heteroskedasticity are well studied for various regression models .",
    "glejser ( 1969 ) forms a test using the absolute values of the residuals of a linear regression fitted by ordinary least - squares .",
    "white ( 1980 ) constructs an estimator of the covariance matrix of the ordinary least - squares estimator in linear regression and proposes a test based on this estimator .",
    "cook and weisberg ( 1983 ) derive a score test for a parametric form of the scale function of the errors in a linear regression . eubank and thomas ( 1993 )",
    "study a test for heteroskedasticity , which is related to the score test , for the nonparametric regression model with normal errors .",
    "dette and munk ( 1998 ) and dette ( 2002 ) also consider nonparametric regression .",
    "dette and munk create tests based on an approximation of the variance function ; in the 2002 paper dette proposes a residual - based test using kernel estimators .",
    "this approach is extended to the case of a partially linear regression by you and chen ( 2005 ) and lin and qu ( 2012 ) .",
    "dette , neumeyer and van keilegom ( 2007 ) construct a test for a parametric form of a scale function of the errors from a nonparametric regression using a bootstrap approach .",
    "dette and hetzler ( 2009 ) construct a test for a parametric form of a scale function of a heteroskedastic nonparametric regression using an empirical process .",
    "these approaches either require strict modelling of the scale function , but are consistent at the `` parametric '' root-@xmath20 rate of convergence , or favour more parsimonious modelling conditions but converge at a slower rate .",
    "this means a trade - off between modelling conditions and how much data is required for meaningful statistical inference .",
    "in contrast to some of the above articles we will not require a specific ( parametric ) model for the unknown variance function . our approach is new in that our proposed test statistic for the nonparametric regression model converges at the root-@xmath20 rate .",
    "moreover , we allow @xmath6 to be multivariate , which is also new .",
    "the tests proposed by dette ( 2002 ) for the nonparametric regression model are probably closest to our approach .",
    "however , dette only considers the case where @xmath6 is univariate and his tests converge with rates slower than root-@xmath20 .",
    "the tests introduced in this article are inspired by koul , mller and schick ( 2012 ) , who develop tests for linearity of a semiparametric regression function for fully observed data and for a missing data model .",
    "these approaches are in the spirit of stute ( 1997 ) , who studies a test for a parametric regression against nonparametric alternatives and , in particular , of stute , xu and zhu ( 2008 ) , who propose a related test suitable for high - dimensional covariates .",
    "the test statistics are based on weighted empirical distribution functions .",
    "the form of these statistics is strikingly simple and their associated limiting behaviour is obtained by considering the related weighted empirical process .",
    "we consider detecting heteroskedasticity ( represented by the non - constant scale function @xmath9 ) by using some ( non - constant ) `` detection function '' @xmath21 in the space @xmath22 .",
    "to explain the idea , we consider the weighted error distribution function @xmath23\\big ] ,   \\qquad t \\in \\r.\\ ] ] if the null hypothesis is true , we can write @xmath24\\big ]   = e\\big [   e\\big[\\omega(x)\\big]\\1[\\sigma_0\\err \\leq t]\\big ] ,   \\qquad t \\in \\r.\\ ] ] here we have also used that under the null hyptothesis the covariates @xmath6 and the errors @xmath25 are independent .",
    "this motivates a test based on the difference between the two quantities ( which is zero only under @xmath26 ) , i.e.  on @xmath27\\big\\ }   \\1[\\sigma_0\\err \\leq t]\\big ] ,   \\qquad t \\in \\r.\\ ] ] we can estimate the outer expectation by its empirical version , which yields a test based on @xmath28\\big\\ }   \\1[\\sigma_0 \\err_j \\leq t ] ,   \\qquad t \\in \\r.\\ ] ] this is a process in the skorohod space @xmath29 . to move this process to the more convenient space",
    "@xmath30 $ ] , we define the familiar limit @xmath31 and the limit @xmath32\\big\\}.\\ ] ] since the variance of @xmath33 equals the variance of @xmath34 it is clear the asymptotic distribution of @xmath35 will depend on @xmath36 , which is not desirable for obtaining a standard distribution useful for statistical inference .",
    "therefore , we standardise @xmath37 and obtain the weighted empirical process @xmath38 ,   \\qquad t \\in \\r,\\ ] ] with weights @xmath39 } { \\sqrt{\\operatorname*{var}[\\omega(x_j ) ] } } = \\frac{\\omega(x_j ) - e\\big[\\omega(x_j)\\big ] }    { \\sqrt{e\\big[\\big\\{\\omega(x_j ) - e\\big[\\omega(x_j)\\big]\\big\\}^2\\big ] }    } , \\qquad j=1,\\ldots , n.\\ ] ] the process @xmath40 can not be used for testing because it depends on unknown quantities .",
    "our final test statistic @xmath41 will therefore be based on an estimated version of @xmath40 with the errors estimated by residuals @xmath42 , @xmath43 , from a sample of @xmath20 i.i.d.random variables @xmath44 . here",
    "@xmath45 is a suitable estimator of the regression function . in this article",
    "we assume a nonparametric regression model and estimate the unknown smooth regression function @xmath3 using a nonparametric function estimator ; see section [ fullmodel ] for details .",
    "when @xmath46 is a constant function ( the null hypothesis is true ) , we expect the estimated process to behave like @xmath47 and exhibit a standard limiting behaviour . however , if @xmath9 is non - constant ( the alternative hypothesis is true ) , the residuals @xmath48 will estimate @xmath49 ( and the weights @xmath50 and the errors @xmath51 will not be independent ) .",
    "we expect the estimated process will show a different limiting behaviour in this case .",
    "note that our test exploits just the independence  dependence structure between the covariates and the errors .",
    "the choice of the weights , i.e.  of the detection function @xmath52 , is important to guarantee that the tests are powerful : it is clear that @xmath52 must be non - constant to detect heteroskedasticity .",
    "if the alternative hypothesis is true , it will be advantageous to have weights that are highly correlated with the scale function @xmath53 to increase the power of the test .",
    "we give reasons for this behaviour at the end of section [ fullmodel ] , where we also construct weights based on an estimate @xmath54 of @xmath9 .",
    "we are interested in both the case when all data are completely observed , the `` full model '' , and the case when responses @xmath5 are missing at random ( mar ) , the `` mar model '' . here",
    "the observed data can be written as independent copies @xmath55 of a base observation @xmath56 , where @xmath57 is an indicator which equals one if @xmath5 is observed and zero otherwise . assuming that responses are _ missing at random _ means the distribution of @xmath57 given the pair @xmath15 depends only on the covariates @xmath6 ( which are always observed )",
    ", i.e.   @xmath58 this implies that @xmath5 and @xmath57 are conditionally independent given @xmath6 . assuming that responses are missing at random is often reasonable",
    "; see little and rubin ( 2002 , chapter 1 ) . working with this missing data model is advantageous because the missingness mechanism is ignorable , i.e.  @xmath59 can be estimated .",
    "it is therefore possible to draw valid statistical conclusions without auxiliary information , in contrast to the model with data that are `` not missing at random '' ( nmar ) .",
    "note how the mar model covers the full model as a special case with all indicators @xmath57 equal to @xmath60 , hence @xmath61 .    in this article",
    ", we will show that our test statistics @xmath41 , defined in ( [ tn ] ) for the full model , and @xmath62 , defined in ( [ tc ] ) for the mar model , may be used to test for the presence of heteroskedasticity .",
    "the subscript `` c '' indicates that our test statistic @xmath62 uses only the completely observed data ; i.e.  we use only observations @xmath15 where @xmath57 equals one , called the _",
    "complete cases_. in particular , we use only the available residuals @xmath63 , where @xmath64 is a suitable complete case estimator of the regression function @xmath3 . demonstrating this",
    "will require two steps .",
    "first , we study the full model and provide the limiting distribution of the test statistic @xmath41 under the null hypothesis in theorem [ thmtn ] .",
    "then we apply the _ transfer principle _ for complete case statistics ( given in koul et al .",
    "2012 ) to adapt the results of theorem [ thmtn ] to the mar model .",
    "since residuals can only be computed for data @xmath15 that are completely observed , the transfer principle is useful for developing residual - based statistical procedures in mar regression models .",
    "our proposed ( residual - based ) tests are asymptotically distribution free .",
    "this means that inference based on the limiting distribution of the test statistic does not depend on parameters of the underlying distribution .",
    "the transfer principle guarantees , in this case , that the test statistic and its complete case version have the same limiting distribution ( under a mild condition ) , i.e.  one can simply omit the incomplete cases and work with the same quantiles as in the full model , which is desirable due to its simplicity .",
    "this article is structured as follows .",
    "section [ fullmodel ] contains the statement of the test statistic and the asymptotic results for the full model .",
    "section [ marmodel ] extends the results of the full model to the mar model .",
    "simulations in section [ simstudy ] investigate the performance of these tests .",
    "technical arguments supporting the results in section [ fullmodel ] are given in section [ aux ] .",
    "we begin with the full model and require the following standard condition ( which guarantees good performance of nonparametric function estimators ) :    [ assumpg ] the covariate vector @xmath6 is quasi - uniform on the cube @xmath65^m$ ] ; i.e.@xmath6 has a density that is bounded and bounded away from zero on @xmath65^m$ ] .    as in mller , schick and wefelmeyer ( 2009 )",
    ", we require the regression function to be in the hlder space @xmath66 , i.e.it has continuous partial derivatives of order @xmath67 ( or higher ) and the partial @xmath67-th derivatives are hlder with exponent @xmath68 $ ] .",
    "we estimate the regression function @xmath3 by a local polynomial smoother @xmath45 of degree @xmath67 .",
    "the choice of @xmath67 will not only depend on the number of derivatives of @xmath3 , but also on the dimension @xmath69 of the covariate vector .",
    "( we will need more smoothness if @xmath69 is large . )",
    "we write @xmath70 and @xmath71 for the distribution function and the density of the errors @xmath72 which will have to satisfy certain smoothness and moment conditions .    in order to describe the local polynomial smoother ,",
    "let @xmath73 be a multi - index and @xmath74 be the set of multi - indices that satisfy @xmath75 .",
    "then @xmath45 is defined as the component @xmath76 corresponding to the multi - index @xmath77 of a minimiser @xmath78 where @xmath79^m,\\ ] ] @xmath80 is a product of densities and @xmath81 is a bandwidth .",
    "the estimator @xmath45 was studied in mller et al .",
    "( 2009 ) , who provide a uniform expansion of an empirical distribution function based on residuals @xmath82 the proof uses results from a crucial technical lemma , lemma 1 in that article ( written as lemma [ lemrhat ] in section [ aux ] ) , which gives important asymptotic properties of @xmath45 .",
    "we will use these properties in section [ aux ] to derive the limiting distribution of our test statistic , which is based on a weighted version of the empirical distribution function proposed by mller et al .",
    "for the full model , the test statistic is given as @xmath83 \\bigg|\\ ] ] with @xmath84^{1/2 } , \\quad \\omega \\in \\sigma,\\ ] ] for @xmath85 .",
    "the term in absolute brackets of ( [ tn ] ) is an approximation ( under @xmath26 ) of the process @xmath47 from the introduction , now with the standardised weights @xmath50 from ( [ wj ] ) replaced by empirically estimated weights @xmath86 .",
    "recall that @xmath52 must be a non - constant function , i.e.   @xmath87 , which is crucial to guarantee that the test is able to detect heteroskedasticity .",
    "we arrive at our main result , the limiting distribution for the test statistic @xmath41 in the fully observed model .",
    "[ thmtn ] let the distribution @xmath18 of the covariates @xmath6 satisfy assumption [ assumpg ] .",
    "suppose the regression function @xmath3 belongs to the hlder space @xmath66 with @xmath88 ; the distribution @xmath70 of the error variable @xmath89 has mean zero , a finite moment of order @xmath90 and a lebesgue density @xmath71 that is both uniformly continuous and bounded ; the densities @xmath91 are @xmath92times continuously differentiable and have compact support @xmath93 $ ] .",
    "let @xmath94 .",
    "let the null hypothesis hold",
    ". then @xmath95 \\bigg|\\ ] ] with @xmath86 specified in ( [ hwj ] ) above , converges in distribution to @xmath96 } |b_0(t)|$ ] , where @xmath97 denotes the standard brownian bridge .",
    "the proof of theorem [ thmtn ] is given in section [ aux ] .",
    "we note the distribution of @xmath98 } |b_0(t)|$ ] is a standard distribution , whose upper @xmath99-quantiles @xmath100 can be calculated using the formula @xmath101 } \\big|b_0(t)\\big| > b_\\alpha\\bigg )    = \\exp\\big(-2b_\\alpha^2\\big),\\ ] ] i.e.  @xmath102 ; see page 37 of shorack and wellner ( 2009 ) .",
    "for example , the critical value of a 5% level test is approximately @xmath103 .",
    "[ remtnpower ] to derive the power of the test under local alternatives of the form @xmath104 , @xmath105 we use le cam s third lemma .",
    "this result states that a local shift @xmath106 away from the null hypothesis results in an additive shift of the asymptotic distribution of @xmath41 ; see e.g.  page 90 of van der vaart ( 1998 ) .",
    "the shift is calculated as the covariance between @xmath41 and @xmath107 under @xmath26 . here",
    "a brief sketch gives @xmath109 & = e\\bigg [ n^{-1/2}\\sj \\bigg\\ {    w_j\\1\\big[\\sigma_0\\err_j \\leq t\\big ] \\bigg\\}\\bigg\\ {   n^{-1/2}\\delta(x_j)\\frac{f'(\\sigma_0\\err_j)}{f(\\sigma_0\\err_j ) }   \\bigg\\ } \\bigg ] + \\op \\\\ & = e(w\\delta)\\int_{-\\infty}^t \\frac{f'(s)}{f(s)}\\,f(ds ) + \\op \\\\ & = f(t)e(w\\delta ) + \\op.\\end{aligned}\\ ] ] hence , under a contiguous alternative @xmath110 , the distribution of the test statistic @xmath41 limits to @xmath98}|b_0(t ) + \\{f \\circ f^{-1}(t)\\}e(w\\delta)|$ ] , writing @xmath111 for the quantile function of @xmath70 .",
    "since the weights in our test statistic are standardised , only the shape of @xmath52 may have an effect on the behaviour of the statistic  location and scale have no influence . from remark [ remtnpower ]",
    ", we find the power of our test increases with @xmath112 .",
    "so it can be expected that our test will perform best when @xmath52 is a linear transformation of the scale function @xmath53 .",
    "this suggests simply using an estimator @xmath113 of the scale function @xmath53 in order to obtain a powerful test .",
    "we expect that this will not change the asymptotic distribution of the test statistic under the null hypothesis .",
    "we have studied this more closely , assuming that @xmath53 is in the same hlder class as @xmath3 , that is , @xmath114 .",
    "then we can estimate @xmath53 by a local polynomial estimator @xmath115 . here",
    "@xmath116 is a local polynomial estimate of the second conditional moment @xmath117 of @xmath5 given @xmath6 , which is defined in the same way as @xmath45 , but with @xmath118 replaced by @xmath119 .",
    "our estimated weights are then given by @xmath120^{1/2}\\ ] ] for @xmath85 .",
    "using similar donsker class arguments as in the proofs of theorem [ thmtn ] and lemma [ lemtnmodulus ] in section [ aux ] , it is straightforward but lengthy to verify that the asymptotic statements from theorem [ thmtn ] continue to hold for this choice of weights .",
    "we therefore omit the proofs .",
    "the formal result is given in theorem [ thmtnwhat ] below .",
    "the last part of theorem [ thmtnwhat ] concerning the power of the test follows from remark [ remtnpower ] .",
    "[ thmtnwhat ] suppose the assumptions of theorem [ thmtn ] are satisfied with , additionally , the error variable @xmath121 having a finite moment of order greater than @xmath122 . assume the alternative hypothesis restricts @xmath9 to the hlder class @xmath66 , with @xmath66 as in theorem [ thmtn ] .",
    "then , under the null hypothesis , @xmath123 \\bigg|\\ ] ] with @xmath124 specified in ( [ twj ] ) above , converges in distribution to @xmath125 , where @xmath97 denotes the standard brownian bridge , and @xmath126 is asymptotically most powerful for detecting alternative hypotheses of the form @xmath127 , where @xmath128 .",
    "we now consider the mar model .",
    "the complete case test statistic is given by @xmath129 \\bigg| ,   \\quad \\mbox { with } \\",
    ", \\hve_{j , c } = y_j - \\rhat_c(x_j).\\end{aligned}\\ ] ] here @xmath130 is the number of complete cases and @xmath131 denotes the weights from equation ( [ hwj ] ) in the previous section , which are now constructed using only the complete cases . the estimator @xmath64 is the complete case version of @xmath45 ; i.e.  the component @xmath132 corresponding to the multi - index @xmath77 of a minimiser @xmath133 which is defined as in the previous section , but now also involves the indicator @xmath134 .    the transfer principle for complete case statistics ( koul et al . , 2012 ) states that if the limiting distribution of a statistic in the full model is @xmath135 , with @xmath136 the joint distribution of @xmath15 , then the distribution of its complete case version in the mar model will be @xmath137 , where @xmath138 is the conditional distribution of @xmath15 given @xmath139 .",
    "the implication holds provided @xmath138 satisfies the same model assumptions as @xmath136 . for our problem",
    "this means that @xmath138 must meet the assumptions imposed on @xmath136 by theorem [ thmtn ] .",
    "it is easy to see how this affects only the covariate distribution @xmath18 . due to the independence of @xmath6 and @xmath7 , the distribution @xmath136 of @xmath15 factors into the marginal distribution @xmath18 of @xmath6 and the conditional distribution of @xmath5 given @xmath6 , i.e.  the distribution @xmath70 of the errors",
    "this means we can write @xmath140 .",
    "the mar assumption implies that @xmath7 and @xmath57 are independent .",
    "hence the distribution @xmath70 of the errors remains unaffected when we move from @xmath136 to the conditional distribution @xmath138 given @xmath139 , and we have @xmath141 , where @xmath142 is the distribution of @xmath6 given @xmath143 .",
    "thus , assumption [ assumpg ] about @xmath18 must be restated ; we also have to assume the detection function @xmath52 is square - integrable with respect to @xmath142 .",
    "[ assumpg1 ] the conditional distribution @xmath142 of the covariate vector @xmath6 given @xmath139 is quasi - uniform on the cube @xmath65^m$ ] ; i.e.  it has a density that is bounded and bounded away from zero on @xmath65^m$ ] .    the limiting distribution @xmath135 of the test statistic in the full model in theorem [ thmtn ] is given by @xmath98 }    @xmath136 of @xmath15 ( or on unknown parameters ) .",
    "this makes the test particularly interesting for the mar model , since the limiting distribution of the complete case statistic @xmath137 is the same as the distribution of the full model statistic , @xmath144 , i.e.  it is also given by @xmath98}|b_0(t)|$ ] .",
    "combining these arguments already provides proof for the main result for the mar model .",
    "[ thmtc ] let the null hypothesis hold .",
    "suppose the assumptions of theorem [ thmtn ] are satisfied , with assumption [ assumpg1 ] in place of assumption [ assumpg ] , and let @xmath145 be positive and non - constant .",
    "write @xmath146^{1/2}\\ ] ] and @xmath63",
    ". then @xmath147 \\bigg|\\ ] ] converges in distribution to @xmath98 } |b_0(t)|$ ] , where @xmath97 denotes the standard brownian bridge .",
    "this result is very useful : if the assumptions of the mar model are satisfied it allows us to simply delete the incomplete cases and implement the test for the full model ; i.e.  we may use the same quantiles .",
    "[ remtcwhat ] following the discussions above and preceding theorem [ thmtnwhat ] in the previous section , we can construct estimated weights based on complete cases as follows .",
    "the second conditional moment of @xmath5 given @xmath6 can be estimated by a complete case estimator @xmath148 , which is computed in the same way as @xmath64 , but now with @xmath118 replaced by @xmath119 .",
    "hence , @xmath149 is a consistent complete case estimator of @xmath150 ( which optimises the power of the test ) . the complete case version of the test statistic @xmath126 is @xmath151 \\bigg|,\\ ] ] where the weights @xmath152 are complete case versions of @xmath124 ; see ( [ twj ] ) .",
    "the transfer principle then implies that the results of theorem [ thmtnwhat ] continue to hold for @xmath153 , i.e.@xmath153 tends under the null hypothesis in distribution to @xmath98 } |b_0(t)|$ ] and is asymptotically most powerful for detecting smooth local alternatives .",
    "a brief simulation study demonstrates the effectiveness of a hypothesis test using the test statistics given above for the full model and the mar model .",
    "* example 1 : testing for heteroskedasticity with one covariate .",
    "* for the simulations we chose the regression function as @xmath154 to preserve the nonparametric nature of the model .",
    "the covariates were generated from a uniform distribution and errors from a standard normal distribution : @xmath155 and @xmath156 for @xmath43 .",
    "finally , the indicators @xmath134 have a bernoulli@xmath157 distribution , with @xmath158 . in this study",
    ", we use a logistic distribution function for @xmath159 with a mean of 0 and a scale parameter of 1 . as a consequence ,",
    "the average amount of missing data is around 50% , ranging between 27% and 73% .",
    "we work with @xmath160 , the locally linear smoother , sample sizes @xmath161 , @xmath162 , @xmath163 and @xmath164 , and bandwidths @xmath165 .                in order to investigate the level and power of the test in the full model and in the mar model , we consider the following scale functions : @xmath166 the constant scale function @xmath4 allows for the ( 5% ) level of the test to be checked .",
    "the simulations based on ( non - constant ) scale functions @xmath167 , @xmath168 and @xmath169 give an indication of the power of the test in different scenarios .",
    "in particular , we consider the power of the test against the local alternative @xmath169 .",
    "the power of the test is maximised if @xmath52 equals the scale function @xmath53 ( or is a linear transformation of @xmath53 ) , as explained at the end of section [ fullmodel ] .",
    "we constructed estimated weights based on the assumption that @xmath167 , @xmath168 and @xmath169 are all continuously differentiable and their derivatives satisfy a hlder condition .",
    "this allows us to construct suitable local - constant estimators , which we use in our weights .",
    "the bandwidth is chosen automatically by the function _ loess _ in _",
    "r_. we chose @xmath113 as the the square root of the variance estimate at each value of the covariate ; see remark [ remtnpower ] in section [ fullmodel ] and the discussion following it .",
    "the critical value for the 5% level of each test is approximately @xmath103 .    as an illustration",
    ", we generated a random dataset of size 100 for each scenario .",
    "a scatter plot of the residuals ( in absolute value ) from the nonparametric regression is given for each dataset ( figure [ figure1 ] ) .",
    "the plots also show the underlying scale functions in black ( solid line ) and estimated scale functions in red ( dashed line ) .    to check the performance of our test we conducted simulations of 1000 runs .",
    "table [ table1 ] shows the test results using @xmath41 ( fully observed data ) and @xmath62 ( missing data ) .",
    "the figures corresponding to the null hypothesis ( @xmath4 ) show type i error rates near the desired 5% .",
    "the results for the test using @xmath62 are more conservative than the results for full model based on @xmath41 .",
    "& & + n & 50 & 100 & 200 & 1000 & 50 & 100 & 200 & 1000 + @xmath4 & 0.054 & 0.050 & 0.047 & 0.033 & 0.046 & 0.045 & 0.037 & 0.023 + @xmath167 & 0.565 & 0.979 & 1.000 & 1.000 & 0.098 & 0.511 & 0.969 & 1.000 + @xmath168 & 0.141 & 0.643 & 0.993 & 1.000 & 0.031 & 0.160 & 0.595 & 1.000 + @xmath169 & 0.079 & 0.190 & 0.364 & 0.724 & 0.007 & 0.047 & 0.128 & 0.287 +    we now consider the power of each test .",
    "the figures corresponding to @xmath167 and @xmath168 each show the procedure @xmath41 to perform well at moderate and larger sample sizes . using @xmath41",
    ", we rejected the null hypothesis 100% ( @xmath167 ) and 99.3% ( @xmath168 ) of the time for samples of size 200 .",
    "similar results were obtained for the test using @xmath62 , but they are ( as expected ) less impressive . for the figures corresponding to @xmath169 ,",
    "both test procedures have difficulty rejecting in small samples . using @xmath41",
    ", we rejected the null hypothesis 72.4% of the time for samples of size 1000 , but only 7.9% of the time for samples of size 50 .",
    "again , the results are similar for missing data . in conclusion , each test performs well and the procedures @xmath41 and @xmath62 proposed in this article appear particularly promising for detecting heteroskedasticity .",
    "it seems the test is affected by the amount of smoothing used to construct the regression function estimator .",
    "if the data are under - smoothed , the regression estimate is attempting to explain too much of the model ( the errors as well as the underlying regression function ) . in this case",
    "the estimate shows a large variation and residuals will have smaller than expected magnitudes",
    ". the test will then be conservative because the residual - based empirical distribution function will have lighter tails than if more smoothing were used .",
    "however , if the data are over - smoothed then the regression estimate does not explain enough of the model . in this case",
    "the estimate has a large bias and residuals will have larger than expected magnitudes .",
    "the test will then be liberal because the residual - based empirical distribution function will have heavier tails than it would have if less smoothing were used . since the bandwidth is @xmath170 , we considered constants @xmath171 between 1.5 ( chosen so that @xmath172 ) and 2.5 . using only a multiplier of 1.5 for each sample size produced similar results to those of table [ table1 ] above , but they were more conservative ( due to under - smoothing the regression function ) . using only a multiplier of 2.5 for each sample size yielded figures similar to those of table [ table1 ] , but they were more liberal ( due to over - smoothing the regression function ) .",
    "* example 2 : testing for heteroskedasticity with two covariates .",
    "* throughout this example we work with the regression function @xmath173 which again preserves the nonparametric nature of the study .",
    "the covariates @xmath174 and @xmath175 are each independently generated from a uniform distribution on the interval @xmath176 $ ] .",
    "as above we generate the model errors from a standard normal distribution .",
    "we do not consider missing data , because we expect the conclusions to mirror those of the first simulation study .",
    "here we are interested in the differences in performance of our test @xmath41 , for the full model , when we select different weights .",
    "we work with @xmath177 , the locally cubic smoother , and bandwidths @xmath178 .",
    "the level of the test is 5% as in example 1 above .    for the simulations we use three scale functions : @xmath179 , @xmath180 and @xmath181 .",
    "our weights are constructed based on detection functions @xmath182 , @xmath183 and @xmath184 is an estimated scale function similar to the procedure in the first example .",
    "we choose the constant in the bandwidth @xmath81 of the locally cubic smoother to be @xmath185 for the two cases of known ( fixed ) weights ( @xmath186 and @xmath187 ) and @xmath188 for the case of estimated weights ( @xmath184 ) .",
    "the estimated weights are based on a kernel smoothing of the squared residuals of each nonparametric regression and also require choosing a bandwidth ( @xmath189 ) .",
    "a practical choice is one that minimises the asymptotic mean squared error .",
    "we choose a product of tricubic kernels with a single bandwidth @xmath190 ( see , for example , hrdle and mller , 2000 ) , which is different from the bandwidths used for the locally cubic smoother",
    ".    from the discussion above it is clear that @xmath191 will provide the largest power for detecting @xmath167 but not necessarily for detecting @xmath168 . the choices @xmath187 and @xmath184",
    "will then illustrate the test performance when we choose ( or guess ) some reasonable non - constant detection function and when we use an estimator of the scale function to increase the power of the test .",
    "& & & + n & @xmath186 & @xmath187 & @xmath184 & @xmath186 & @xmath187 & @xmath184 & @xmath186 & @xmath187 & @xmath184 + 100 & 0.002 & 0.008 & 0.005 & 0.707 & 0.146 & 0.240 & 0.808 & 0.179 & 0.287 + 200 & 0.003 & 0.009 & 0.006 & 1.000 & 0.520 & 0.963 & 1.000 & 0.524 & 0.971 + 500 & 0.009 & 0.016 & 0.021 & 1.000 & 0.977 & 1.000 & 1.000 & 0.989 & 1.000 + 1000 & 0.036 &",
    "0.040 & 0.045 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 +    we conducted simulations consisting of 1000 runs , now using sample sizes 100 , 200 , 500 and 1000 .",
    "the results are displayed in table [ table2 ] .",
    "the figures in the left panel ( @xmath179 ) corresponding to the test level ( 5% ) show the tests are all highly conservative and only reach adequate levels at samples of size 1000 .",
    "nevertheless , when we consider the figures in the remaining panels , corresponding to the powers of each test , we find considerable differences between the tests .",
    "it is clear that testing using @xmath182 provides the best results in the second column referring to @xmath167 ( best weights ) .",
    "since @xmath167 and @xmath168 are similar in shape , the results for the test based on @xmath182 are also quite convincing when @xmath168 is the underlying scale function ( third column ) .",
    "testing using @xmath184 ( estimated weights ) provides comparable results to those of @xmath186 .",
    "the only notable difference between the two procedures occurs when the sample size is small ( 100 observations ) . here",
    "we find the test using @xmath186 gives powers 0.707 ( @xmath167 ) and 0.808 ( @xmath168 ) while the test using @xmath184 only gives powers 0.240 ( @xmath167 ) and 0.287 ( @xmath168 ) .",
    "this difference in behavior is expected .",
    "when we consider the test using @xmath187 , we see a considerable decrease in power at smaller sample sizes . at 200 observations ,",
    "the tests using @xmath186 and @xmath184 already have powers at or near 1.000 , but the test using @xmath187 only gives powers 0.520 ( @xmath167 ) and 0.524 ( @xmath168 ) . only at very large sample sizes",
    "are all three testing procedures similar . in conclusion ,",
    "we find the test using an arbitrary non - constant weight function is useful but will normally be outperformed by the test using estimated weights that attempt to optimise the power of the test .",
    "all three test procedures are fairly conservative .",
    "in this section we present the proof of theorem [ thmtn ] and some auxiliary results .",
    "as explained earlier , it suffices to consider the full model and the test statistic @xmath41 .",
    "our approach consists of two steps .",
    "our first step will be to use theorem 2.2.4 in koul s 2002 book on weighted empirical processes to obtain the limiting distribution of an asymptotically linear statistic ( i.e.  a sum of i.i.d .",
    "random variables ) that is related to @xmath41",
    ". then we review some results from mller , schick and wefelmeyer ( 2009 ) , who propose local polynomial smoothers to estimate a regression function of many covariates . using these results",
    ", we will show that the statistic @xmath41 and the asymptotically linear statistic are indistinguishable for large samples , i.e.  they have the same limiting distribution .",
    "the asymptotically linear statistic , which is an empirical process related to @xmath41 , is defined similarly to @xmath41 as @xmath192 - f(t ) \\big\\ }   \\bigg|,\\ ] ] where @xmath193 is the unobserved `` model error '' from the null hypothesis and @xmath194 are the standardised weights given in ( [ wj ] ) .",
    "we will now demonstrate that the requirements for koul s theorem are satisfied .",
    "the asymptotic statement is given afterwards in corollary [ coraslinstat ] .",
    "theorem 2.2.4 of koul ( 2002 ) states that @xmath195 - k(t ) \\big\\ }   \\xrightarrow[]{d } \\xi \\big\\ { b_0 \\circ k(t ) \\big\\ } ,   \\qquad t \\in \\r,\\text { as } n \\to \\infty,\\ ] ] where @xmath97 is the standard brownian bridge in the skorohod space @xmath196 $ ] , independent of a random variable @xmath197 .",
    "the roles of his random variable @xmath198 and the square integrable random variable @xmath199 , which are assumed to be independent , are now played by @xmath200 and @xmath50 , @xmath85 .",
    "the distribution function @xmath201 corresponds to our error distribution function @xmath70 and is assumed to have a uniformly continuous lebesgue density .",
    "the random variable @xmath197 from above comes from koul s requirement that @xmath202 here we work with @xmath50 , in place of @xmath199 , with @xmath203 .",
    "therefore , by the law of large numbers , @xmath204 and , using the continuous mapping theorem , @xmath205 , i.e.  @xmath206 .",
    "hence we have @xmath207 - f(t ) \\big\\ }   \\xrightarrow[]{d } b_0 \\circ f(t ) ,   \\qquad t \\in \\r , \\text { as } n \\to \\infty.\\ ] ] taking the supremum with respect to @xmath208 , the right - hand side becomes @xmath209 }    asymptotically linear statistic ( [ aslinstat ] ) .",
    "note that koul s theorem also provides the limiting distribution for a shifted version @xmath210 of @xmath211 that involves random variables @xmath212 .",
    "since we only need the simpler result for @xmath211 , we do not need to verify the more complicated assumptions regarding the @xmath213 s .",
    "this shows the conditions of theorem 2.2.4 in koul ( 2002 ) are indeed satisfied .",
    "we will formulate this result as a corollary .",
    "since we only require the weights to be square - integrable functions of @xmath214 with @xmath215 , we will not require the explicit form ( [ wj ] ) .",
    "[ coraslinstat ] consider the homoskedastic nonparametric regression model @xmath216 .",
    "assume the distribution function @xmath70 of the errors has a uniformly continuous lebesgue density @xmath71 that is positive almost everywhere .",
    "further , let @xmath50 be a square integrable function of @xmath214 satisfying @xmath215 , @xmath85 . then @xmath217 - f(t )",
    "\\big\\ }   \\bigg| \\xrightarrow[]{d } \\sup_{t \\in [ 0,1 ] } |b_0(t)| ,   \\qquad\\text{as } n \\to \\infty,\\end{gathered}\\ ] ] where @xmath97 denotes the standard brownian bridge .    for our second step",
    ", we will show that @xmath41 and the asymptotically linear statistic ( [ aslinstat ] ) are asymptotically equivalent . to begin we rewrite @xmath41 , using the identity ( under @xmath26 ) @xmath218 , as @xmath219\\bigg|   = \\sup_{t \\in \\r}\\bigg| n^{-1/2}\\sj    \\hw_j\\1\\big[\\sigma_0\\err_j \\leq t + \\rhat(x_j ) - r(x_j)\\big ] \\bigg|.\\ ] ]",
    "we will first consider the shift in the indicator function from @xmath220 to @xmath221 , which comes in because @xmath41 involves an estimator @xmath222 of the regression function .",
    "consider now the hlder space @xmath66 from section [ fullmodel ] , i.e.  the space of functions that have partial derivatives of order @xmath67 that are hlder with exponent @xmath223 $ ] .",
    "for these functions we define the norm @xmath224^m }   \\big|d^ih(x)\\big| + \\max_{i \\in i(d ) }   \\sup_{x,\\,y \\in [ 0,1]^m,~ x \\neq y }   \\frac{|d^ih(y ) - d^ih(x)|}{\\|x - y\\|^\\gamma},\\ ] ] where @xmath225 is the euclidean norm of a real - valued vector @xmath226 and @xmath227^m.\\ ] ] write @xmath228 for the unit ball of @xmath66 using this norm .",
    "these function spaces are particularly useful for studying local polynomial smoothers @xmath45 as defined in section [ fullmodel ] .",
    "mller et al .  ( 2009 ) make use of these spaces to derive many useful facts concerning regression function estimation using local polynomials .",
    "we will use some of their results to prove theorem [ thmtn ] ; see lemma [ lemrhat ] below .",
    "[ lemrhat ] let the local polynomial smoother @xmath45 , the regression function @xmath3 , the covariate distribution @xmath18 and the error distribution @xmath70 satisfy the assumptions of theorem [ thmtn ] .",
    "then there is a random function @xmath229 such that , for some @xmath230 , @xmath231^m } \\big|\\rhat(x ) - r(x ) - \\ahat(x)\\big| = \\opn.\\end{gathered}\\ ] ]    we now use these results to show the difference between the asymptotically linear statistic ( [ aslinstat ] ) and an empirical process related to the shifted version of @xmath41 ( called @xmath232 in lemma [ lemtnmodulus ] below ) are asymptotically negligible . note : an unweighted version of that difference is considered in the proof of theorem 2.2 of mller , schick and wefelmeyer ( 2007 ) .",
    "[ lemtnmodulus ] let the null hypothesis hold .",
    "suppose the assumptions of theorem [ thmtn ] on @xmath222 , @xmath3 , @xmath18 and @xmath70 are satisfied .",
    "let @xmath50 be a square integrable function of @xmath214 satisfying @xmath233 < \\infty$ ] , @xmath85 .",
    "then @xmath234 , where @xmath235   - \\1[\\sigma_0\\err_j \\leq t ] - f\\big(t + \\rhat(x_j ) - r(x_j)\\big )   + f(t)\\big\\}.\\ ] ] if , additionally , @xmath236 = 0 $ ] , @xmath85 , then @xmath237    in the following we will write , for any function @xmath238 from @xmath239^m$ ] to @xmath240 , @xmath241^m }    @xmath240 , @xmath242 .",
    "we begin by noting that the assumptions of lemma [ lemrhat ] are satisfied .",
    "hence , by property ( [ ahatinh1 ] ) of lemma [ lemrhat ] , there is a random function @xmath229 which approximates @xmath243 and therefore still depends on the data @xmath244 .",
    "also there is an @xmath230 such that @xmath245",
    ". we will first show an auxiliary statement , namely that the ( simpler ) class of functions @xmath246 - f\\big(t + a(x)\\big ) \\big\\ }   \\,:\\,t\\in\\r,~a\\in h_1(m,\\alpha)\\bigg\\}\\ ] ] is @xmath247donsker . to prove this , it suffices to verify dudley s entropy integral condition : @xmath248}\\big((\\epsilon,\\fclass , l_2(g \\otimes f)\\big ) } \\ ,   d\\epsilon < \\infty\\ ] ] ( see theorem 2.5.6 of van der vaart and wellner , 1996 ) .",
    "here @xmath249}(\\epsilon,\\fclass , l_2(g \\otimes f))$ ] is the number of brackets of length no greater than @xmath250 required to cover @xmath251 and @xmath252 is the @xmath253norm with respect to the measure @xmath247 .",
    "bracketing numbers are a measure of the amount of entropy residing in the class @xmath251",
    ".    we will proceed similarly to the proof of lemma a.1 of van keilegom and akritas ( 1999 ) and find a suitable function of @xmath250 that satisfies the above integral condition concerning the the bracketing numbers @xmath249}(\\epsilon,\\fclass , l_2(g \\otimes f))$ ] .",
    "let @xmath254 .",
    "since functions in @xmath251 are sums of two terms , we will show the first space @xmath255\\,:\\,t\\in\\r,~a\\in h_1(m,\\alpha)\\}$ ] satisfies the integral condition above .",
    "the proof for the second space @xmath256 is similar and therefore omitted . by theorem 2.7.1 of van der vaart and wellner ( 1996 ) it follows , for some positive constant @xmath201 , that @xmath257}(\\epsilon^2/(2\\|f\\|_{t,\\infty}e[w^2 ] ) , h_1(m,\\alpha ) , \\|\\cdot\\|_{x,\\infty } ) \\leq \\exp\\{k\\epsilon^{-(2m)/(m+\\alpha)}\\}$ ] .",
    "let @xmath258 be the functions defining the @xmath259 brackets for @xmath260 based on the @xmath261norm .",
    "it then follows for the brackets @xmath262 to satisfy @xmath263",
    "\\leq \\epsilon^2/(2\\|f\\|_{t,\\infty})$ ] , @xmath264 .",
    "the random variable @xmath265 can be either positive  valued or negative  valued .",
    "we can then construct brackets for @xmath240 based on this information and the @xmath259 brackets for @xmath266 as follows .",
    "for every fixed @xmath208 and @xmath264 , we have , writing @xmath267 $ ] and @xmath268 $ ] , @xmath269   \\leq w^-\\1\\big[\\sigma_0\\err \\leq t + a(x)\\big ]   \\leq w^-\\1\\big[\\sigma_0\\err \\leq t + a_{li}(x)\\big]\\end{aligned}\\ ] ] and @xmath270   \\leq w^+\\1\\big[\\sigma_0\\err \\leq t + a(x)\\big ]   \\leq w^+\\1\\big[\\sigma_0\\err \\leq t + a_{ui}(x)\\big].\\end{aligned}\\ ] ]    now define @xmath271 for the conditional probability that @xmath89 is at most @xmath272 given the covariates @xmath6 .",
    "further , let @xmath273 , for @xmath274 , partition @xmath275 into segments having @xmath276probability at most @xmath277)$ ] .",
    "similarly , define @xmath278 and let @xmath279 , for @xmath280 , partition @xmath275 into segments having @xmath281probability at most @xmath277)$ ] . hence , we obtain the following bracket for @xmath220 : @xmath282 , where @xmath283 is the largest @xmath273 that is less than or equal to @xmath220 and @xmath284 is the smallest @xmath279 that is larger than or equal to @xmath220 .",
    "we will now show the brackets for @xmath285 are given by @xmath286   + w^+\\1\\big[\\sigma_0\\err \\leq t^-_{lij_1 } + a_{li}(x)\\big ] \\\\ & \\leq w\\1\\big[\\sigma_0\\err \\leq t + a(x)\\big ] \\\\ & \\leq w^-\\1\\big[\\sigma_0\\err \\leq t^-_{lij_1 } + a_{li}(x)\\big ]   + w^+\\1\\big[\\sigma_0\\err \\leq t^+_{uij_2 } + a_{ui}(x)\\big].\\end{aligned}\\ ] ] the squared length of the proposed brackets above is equal to @xmath287   - \\1\\big[\\sigma_0\\err \\leq t^+_{uij_2 } + a_{ui}(x)\\big]\\big\\ } \\\\ & \\phantom{e\\bigg[\\big ( }   + w^+\\big\\{\\1\\big[\\sigma_0\\err \\leq t^+_{uij_2 } + a_{ui}(x)\\big ]   - \\1\\big[\\sigma_0\\err \\leq t^-_{lij_1 } + a_{li}(x)\\big]\\big\\ }   \\big)^2\\bigg ] \\\\ & = e\\big[w^2\\big\\{\\1[w < 0 ] + \\1[w \\geq 0]\\big\\ }   \\big\\{\\1\\big[\\sigma_0\\err \\leq t^+_{uij_2 } + a_{ui}(x)\\big ]   - \\1\\big[\\sigma_0\\err \\leq t^-_{lij_1 } + a_{li}(x)\\big]\\big\\ }   \\big ] \\\\ & = e\\big[w^2\\big\\ {   f_{ui}\\big(t_{uij_2}^+\\big ) - f_{li}\\big(t_{lij_1}^-\\big )   \\big\\}\\big],\\end{aligned}\\ ] ] which is bounded by @xmath288   + \\frac{\\epsilon^2}{2}.\\ ] ] consider the first term .",
    "since the distribution function @xmath70 has a bounded density function @xmath71 , the inequality above for @xmath289 $ ] implies @xmath290    \\leq \\|f\\|_{t,\\infty}e\\big[w^2\\{a_{ui}(x ) - a_{li}(x)\\}\\big ]   \\leq \\frac{\\epsilon^2}{2}.\\end{aligned}\\ ] ] hence , the @xmath252lengths of our proposed brackets are bounded by @xmath250 as desired .",
    "it then follows , for every @xmath291 , that the number of brackets is at most @xmath292 and , for @xmath293 , one bracket suffices .",
    "therefore , we can choose appropriate positive constants @xmath294 and @xmath295 to find @xmath296}\\big(\\epsilon , \\fclass_1 , l_2(g \\otimes f)\\big ) } \\ ,   d\\epsilon   = \\int_{0}^{1 } \\sqrt { \\log   n_{[\\,]}\\big(\\epsilon , \\fclass_1 , l_2(g \\otimes f)\\big ) } \\ ,   d\\epsilon   \\leq c_1 + c_2\\frac{m + \\alpha}{\\alpha}.\\ ] ] since @xmath230 , the bound above is finite and dudley s entropy integral condition holds .",
    "this shows the class @xmath285 is @xmath297donsker , which combined with the statement for @xmath256 implies the class @xmath251 is @xmath247donsker .",
    "it follows from corollary 2.3.12 of van der vaart and wellner ( 1996 ) that empirical processes ranging over the donsker class @xmath251 are asymptotically equicontinuous , i.e.  we have , for any @xmath298 , @xmath299 we are interested in the case that involves the approximation @xmath300 in place of @xmath266 , where the corresponding class of functions is , in general , no longer donsker ( and the equicontinuity property does not hold ) .",
    "however , we can assume that @xmath300 is in @xmath301 , which holds on an event that has probability tending to one .",
    "this together with the following negligibility condition on the variance guarantees that the extended class of processes involving @xmath300 is also equicontinuous . to prove that variance condition",
    ", we fix the function @xmath229 by conditioning on the observed data @xmath302 .",
    "the variation of a function from the extension of @xmath251 , i.e.  now involving @xmath229 instead of @xmath266 , is equal to @xmath303   - \\1[\\sigma_0\\err \\leq t ] - f\\big(t + \\ahat(x)\\big )   + f(t)\\big\\}\\,\\big|\\,\\dset\\big ] \\\\ & = e\\bigg[w^2\\big\\{\\1\\big[\\sigma_0\\err \\leq t + \\ahat(x)\\big ]   - \\1[\\sigma_0\\err \\leq t ] - f\\big(t + \\ahat(x)\\big )   + f(t)\\big\\}^2\\,\\bigg|\\,\\dset\\bigg ] \\\\ &",
    "= e\\big[w^2\\big\\{\\1\\big[\\sigma_0\\err \\leq t + \\ahat(x)\\big ]   - 2\\1\\big[\\sigma_0\\err \\leq \\min\\{t,\\,t+\\ahat(x)\\}\\big ] \\\\ & \\phantom{= e\\big [ }   - 2\\1\\big[\\sigma_0\\err \\leq t + \\ahat(x)\\big]f\\big(t + \\ahat(x)\\big )   + 2\\1\\big[\\sigma_0\\err \\leq t + \\ahat(x)\\big]f(t ) \\\\ & \\phantom{= e\\big [ }   + \\1[\\sigma_0\\err \\leq t ]   + 2\\1[\\sigma_0\\err \\leq t]f\\big(t + \\ahat(x)\\big )   - 2\\1[\\sigma_0\\err \\leq t]f(t ) \\\\ & \\phantom{= e\\big [ }   + f^2\\big(t + \\ahat(x)\\big )   - 2f\\big(t + \\ahat(x)\\big)f(t )   + f^2(t)\\big\\}\\,\\big|\\,\\dset\\big ] \\\\ & = e\\bigg[w^2\\bigg\\ {   f\\big(t + \\ahat(x)\\big ) - f\\big(\\min\\{t,\\,t+\\ahat(x)\\}\\big )   + f(t ) - f\\big(\\min\\{t,\\,t+\\ahat(x)\\}\\big ) \\\\ & \\phantom{= e\\bigg[w^2\\bigg\\ { }   - \\big\\{f\\big(t +",
    "\\ahat(x)\\big ) - f(t)\\big\\}^2   \\bigg\\}\\,\\bigg|\\,\\dset\\bigg ] \\\\ & = e\\bigg[w^2\\bigg\\ {   f\\big(\\max\\{t,\\,t+\\ahat(x)\\}\\big )   - f\\big(\\min\\{t,\\,t+\\ahat(x)\\}\\big ) \\\\ & \\phantom{= e\\bigg[w^2\\bigg\\ { }   - \\big\\{f\\big(\\max\\{t,\\,t+\\ahat(x)\\}\\big )   - f\\big(\\min\\{t,\\,t+\\ahat(x)\\}\\big)\\big\\}^2   \\bigg\\}\\,\\bigg|\\,\\dset\\bigg],\\end{aligned}\\ ] ] which is bounded by @xmath304 \\\\ & \\leq \\|f\\|_{t,\\infty}e\\big[w^2\\big(\\max\\{t,\\,t+\\ahat(x)\\ }   - \\min\\{t,\\,t+\\ahat(x)\\}\\big)\\,\\big|\\,\\dset\\big ] \\\\ & = \\|f\\|_{t,\\infty}e\\big[w^2\\big|\\ahat(x)\\big|\\,\\big|\\,\\dset\\big ] \\\\ &",
    "\\leq \\|f\\|_{t,\\infty}e\\big[w^2\\big]\\|\\ahat\\|_{x,\\infty } \\\\ & = \\op,\\end{aligned}\\ ] ] i.e.  the variance is asymptotically negligible .",
    "here we used @xmath305 ; see page 961 of the proof of lemma 1 in mller et al .",
    "hence we have asymptotic equicontinuity and therefore @xmath306   - \\1[\\sigma_0\\err_j \\leq t ]    - f\\big(t + \\ahat(x_j)\\big ) + f(t)\\big\\}\\bigg| = \\opn.\\ ] ]    we now decompose @xmath232 from the first assertion as the sum of @xmath307   - \\1[\\sigma_0\\err_j \\leq t ]    - f\\big(t + \\ahat(x_j)\\big ) + f(t)\\big\\}\\ ] ] and @xmath308   - f\\big(t + \\rhat(x_j ) - r(x_j)\\big)\\big\\ } \\\\",
    "\\nonumber & - \\avj w_j\\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t + \\ahat(x_j)\\big ]   - f\\big(t + \\ahat(x_j)\\big ) \\big\\}.\\end{aligned}\\ ] ] we have already shown the first term is @xmath309 , uniformly in @xmath310 . by property ( [ rhatrahatneg ] ) of lemma [ lemrhat ] , @xmath311 .",
    "the decomposition of @xmath50 into @xmath312 , @xmath85 , yields the following bounds for the weighted indicator functions , for each @xmath43 : @xmath313   \\leq w_j^-\\1\\big[\\sigma_0\\err_j \\leq t - a_n + \\ahat(x_j)\\big],\\ ] ] @xmath314   \\geq w_j^-\\1\\big[\\sigma_0\\err_j \\leq t + a_n + \\ahat(x_j)\\big],\\ ] ] @xmath315   \\leq w_j^+\\1\\big[\\sigma_0\\err_j \\leq t + a_n + \\ahat(x_j)\\big].\\ ] ] and @xmath316   \\geq w_j^+\\1\\big[\\sigma_0\\err_j \\leq t - a_n + \\ahat(x_j)\\big].\\ ] ] this implies that we can find a bound for ( [ modrhatrahat ] ) by calculating @xmath317   - f\\big(t + \\rhat(x_j ) - r(x_j)\\big ) \\big\\ } \\\\ & \\quad - \\avj w_j \\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t + \\ahat(x_j)\\big ]   - f\\big(t + \\ahat(x_j)\\big ) \\big\\ } \\\\ & = \\avj w_j^- \\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t + \\rhat(x_j ) - r(x_j)\\big ]   - \\1\\big[\\sigma_0\\err_j \\leq t + \\ahat(x_j)\\big ] \\big\\ } \\\\ & \\quad + \\avj w_j^+ \\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t + \\rhat(x_j ) - r(x_j)\\big ]   - \\1\\big[\\sigma_0\\err_j \\leq t + \\ahat(x_j)\\big ] \\big\\ } \\\\ & \\quad - \\avj w_j \\big\\ {   f\\big(t + \\rhat(x_j ) - r(x_j)\\big )   - f\\big(t + \\ahat(x_j)\\big ) \\big\\ } \\\\ & \\leq \\avj w_j^- \\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t - a_n + \\ahat(x_j)\\big ]   - f\\big(t - a_n + \\ahat(x_j)\\big ) \\big\\ } \\\\ & \\quad - \\avj w_j^- \\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t + a_n + \\ahat(x_j)\\big ]   - f\\big(t + a_n + \\ahat(x_j)\\big ) \\big\\ } \\\\ & \\quad + \\avj w_j^- \\big\\ {   f\\big(t - a_n + \\ahat(x_j)\\big )   - f(t + a_n + \\ahat(x_j)\\big ) \\big\\ } \\\\ & \\quad + \\avj w_j^+ \\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t + a_n + \\ahat(x_j)\\big ]   - f\\big(t + a_n + \\ahat(x_j)\\big ) \\big\\ } \\\\ & \\quad - \\avj w_j^+ \\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t - a_n + \\ahat(x_j)\\big ]   - f\\big(t - a_n + \\ahat(x_j)\\big ) \\big\\ } \\\\ & \\quad + \\avj w_j^+ \\big\\ {   f\\big(t + a_n + \\ahat(x_j)\\big )   - f\\big(t - a_n + \\ahat(x_j)\\big ) \\big\\ } \\\\ &",
    "\\quad - \\avj w_j \\big\\ {   f\\big(t + \\rhat(x_j ) - r(x_j)\\big )   - f\\big(t + \\ahat(x_j)\\big ) \\big\\ } \\\\ & = \\avj \\big\\ { w_j^+ - w_j^- \\big\\ } \\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t + a_n + \\ahat(x_j)\\big ]   - f\\big(t + a_n + \\ahat(x_j)\\big ) \\big\\ } \\\\ & \\quad - \\avj \\big\\ { w_j^+ - w_j^- \\big\\ } \\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t - a_n + \\ahat(x_j)\\big ]   - f\\big(t - a_n + \\ahat(x_j)\\big ) \\big\\ } \\\\ & \\quad + \\avj \\big\\ { w_j^+ - w_j^- \\big\\ }",
    "\\big\\ {   f\\big(t + a_n + \\ahat(x_j)\\big )   - f\\big(t - a_n + \\ahat(x_j)\\big ) \\big\\ } \\\\ & \\quad - \\avj w_j \\big\\ {   f\\big(t + \\rhat(x_j ) - r(x_j)\\big )   - f\\big(t + \\ahat(x_j)\\big ) \\big\\ } \\\\ & = \\avj \\big|w_j\\big|\\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t + a_n + \\ahat(x_j)\\big ]   - f\\big(t + a_n + \\ahat(x_j)\\big)\\big\\ } \\\\ & \\quad - \\avj \\big|w_j\\big|\\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t - a_n + \\ahat(x_j)\\big ]   - f\\big(t - a_n + \\ahat(x_j)\\big)\\big\\ } \\\\ & \\quad + \\avj \\big|w_j\\big|\\big\\ {   f\\big(t + a_n + \\ahat(x_j)\\big )   - f\\big(t - a_n + \\ahat(x_j)\\big)\\big\\ } \\\\ & \\quad - \\avj w_j\\big\\ {   f\\big(t + \\rhat(x_j ) - r(x_j)\\big )   - f\\big(t + \\ahat(x_j)\\big)\\big\\}.\\end{aligned}\\ ] ] hence , the first assertion follows from showing @xmath318   - f\\big(t + a_n + \\ahat(x_j)\\big)\\big\\ } \\\\",
    "\\nonumber & - \\avj \\big|w_j\\big|\\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq t - a_n + \\ahat(x_j)\\big ]   - f\\big(t - a_n + \\ahat(x_j)\\big)\\big\\ }   \\bigg| = \\opn,\\end{aligned}\\ ] ] @xmath319 and @xmath320    beginning with ( [ modan ] ) , since the random variables @xmath321 are square integrable , the class of functions @xmath322   - f\\big(t + a(x)\\big ) \\big\\ }   \\,:\\,t \\in \\r,~a \\in h_1(m,\\alpha ) \\bigg\\}\\ ] ] is also @xmath247donsker",
    ". therefore the asymptotic equicontinuity property holds for empirical processes ranging over @xmath323 , i.e.  ( [ equicon ] ) holds with @xmath323 in place of @xmath251 .",
    "however , rather than investigating the situation where @xmath229 is limiting toward zero , as we did above , we will consider two sequences of real numbers @xmath324 and @xmath325 satisfying @xmath326 , which corresponds to the case of random sequences @xmath327 conditional on the data @xmath302 .",
    "analogously to the calculations following ( [ equicon ] ) , we consider the variation condition under the norm in ( [ equicon ] ) , now for the function @xmath328 - f(t_n + a(x ) ) + f(s_n + a(x))$ ] , which is equal to @xmath329   - \\1\\big[\\sigma_0\\err \\leq s_n + a(x)\\big ]   - f\\big(t_n + a(x)\\big )   - f\\big(s_n + a(x)\\big ) \\big\\}\\big ] \\\\ & = e\\bigg[w^2\\bigg\\ {   f\\big(\\max\\{t_n + a(x),\\,s_n + a(x)\\}\\big )   - f\\big(\\min\\{t_n + a(x),\\,s_n + a(x)\\}\\big ) \\\\ & \\phantom{= e\\bigg[w^2\\bigg\\ { }   - \\big\\{f\\big(\\max\\{t_n + a(x),\\,s_n + a(x)\\}\\big )   - f\\big(\\min\\{t_n + a(x),\\,s_n + a(x)\\}\\big)\\big\\}^2   \\bigg\\}\\bigg].\\end{aligned}\\ ] ] this is bounded by @xmath330 \\\\ & \\leq \\|f\\|_{t,\\infty}e\\big[w^2\\big]\\big|t_n - s_n\\big|.\\end{aligned}\\ ] ] since the bound above is @xmath331 , equicontinuity now implies , for any @xmath332 and sequences of real numbers @xmath333 and @xmath325 satisfying @xmath334 , @xmath335   - f\\big(t_n + a(x_j)\\big ) \\big\\ } \\\\ & \\phantom{\\sup_{t \\in \\r } \\bigg| }   - \\avj \\big|w_j\\big|\\big\\ {   \\1\\big[\\sigma_0\\err_j \\leq s_n + a(x_j)\\big ]   - f\\big(s_n + a(x_j)\\big ) \\big\\ } \\bigg| = \\opn.\\end{aligned}\\ ] ] as before we may assume that @xmath229 belongs to @xmath260 .",
    "now conditioning on @xmath302 and letting @xmath336 and @xmath337 , which now satisfies @xmath338 , we find @xmath339   - \\1\\big[\\sigma_0\\err \\leq s_n + \\ahat(x)\\big ] \\\\ & \\quad\\phantom{\\operatorname*{var}\\big[|w|\\big\\ { }   - f\\big(t_n + \\ahat(x)\\big )   - f\\big(s_n + \\ahat(x)\\big ) \\big\\}\\,\\big|\\,\\dset\\big ] \\\\ & \\leq 2\\|f\\|_{t,\\infty}e\\big[w^2\\big]a_n.\\end{aligned}\\ ] ] since @xmath340 , in the same way as before , with @xmath41 and @xmath40 conditional on @xmath302 playing the roles of @xmath341 and @xmath342 above , the negligibility condition on the variance is satisfied .",
    "this combined with the fact that @xmath323 is a donsker class ( and the corresponding class of processes is equicontinuous ) therefore implies that ( [ modan ] ) is satisfied .    turning our attention now to ( [ modanrem ] )",
    ", we find that @xmath343 is consistent for @xmath344 , and we have @xmath345 < \\infty$ ] .",
    "since both @xmath346 and @xmath347 are positive - valued for each @xmath348 , we can find a bound for ( [ modanrem ] ) by calculating @xmath349 since @xmath350 by lemma [ lemrhat ] and since @xmath351 is consistent for @xmath344 , we obtain for the bound above to also be @xmath309 , i.e.  ( [ modanrem ] ) holds .",
    "we can bound the left - hand side of ( [ modfrhatrahatrem ] ) by @xmath352 .",
    "using again @xmath350 and the consistency of @xmath353 , this bound is @xmath309",
    ". therefore ( [ modfrhatrahatrem ] ) holds , which concludes the proof of the first assertion that @xmath354 .",
    "we will now prove the second assertion that @xmath355 . in the same way as above",
    ", we will incorporate the approximation @xmath229 to separate the stochastic process into two parts and then argue each part is negligible at the @xmath356 rate of convergence . the main difference between the proof technique used above , where we additionally required separating the process using the decomposition of the weights into their respective positive and negative components , and the technique used now comes from one remainder term",
    ": we now require that the random variables @xmath194 each have mean zero , which allows us to use the central limit theorem in combination with the result @xmath305 .",
    "this means we can write @xmath357 as @xmath358 \\big\\ } \\\\ & \\quad + e\\big[f\\big(t + \\ahat(x)\\big ) - f(t )   \\,\\big|\\,\\dset\\big]\\bigg(\\avj w_j\\bigg ) \\\\ & \\quad + \\avj w_j\\big\\ {   f\\big(t + \\rhat(x_j ) - r(x_j)\\big ) - f\\big(t + \\ahat(x_j)\\big )   \\big\\}.\\end{aligned}\\ ] ] this shows that @xmath359 is bounded by three terms : @xmath360   \\big\\ } \\bigg|,\\ ] ] @xmath361 \\bigg|   \\bigg| \\avj w_j \\bigg|,\\ ] ] and the third term is the left",
    " hand side of ( [ modfrhatrahatrem ] ) , which we have already shown is @xmath309 . from the arguments",
    "above , it follows for the class of functions @xmath362",
    "\\big\\}\\,:\\,t \\in \\r,~a \\in h_1(m,\\alpha ) \\bigg\\}\\ ] ] to be @xmath18donsker .",
    "therefore , empirical processes ranging over @xmath256 are asymptotically equicontinuous as in ( [ equicon ] ) , but now without @xmath363 and with @xmath256 in place of @xmath251 . as before",
    ", we can assume that @xmath229 belongs to @xmath260 .",
    "we will now show the variance condition is satisfied for the function @xmath364\\}$ ] .",
    "this variance is equal to @xmath365   + e\\big[w^2\\big]e^2\\big",
    "[   f\\big(t + \\ahat(x)\\big ) - f(t)\\,\\big|\\,\\dset\\big ] \\\\ & - 2e\\big[w^2 \\big\\ { f\\big(t + \\ahat(x)\\big ) - f(t )   \\big\\}\\,\\big|\\,\\dset\\big ]   e\\big[f\\big(t + \\ahat(x)\\big ) - f(t)\\,\\big|\\,\\dset\\big],\\end{aligned}\\ ] ] and is bounded by @xmath366\\big\\ {   f\\big(t + \\|\\ahat\\|_{x,\\infty}\\big ) - f(t )   \\big\\}^2    \\leq 2\\|f\\|_{t,\\infty}^2 e\\big[w^2\\big]\\|\\ahat\\|_{x,\\infty}^2.\\end{aligned}\\ ] ] since we have already used that @xmath305 , the bound above is @xmath367 , i.e.  the variance is asymptotically negligible .",
    "this combined with equicontinuity implies that the term in ( [ modfahat ] ) has rate @xmath309 , as desired .",
    "finally we can bound ( [ modfahatrem ] ) by @xmath368 the central limit theorem combined with @xmath236 = 0 $ ] @xmath85 , gives @xmath369 . since @xmath305 this shows that the bound above , and also ( [ modfahatrem ] ) , is of order @xmath309 .",
    "this completes the proof of the second assertion that @xmath370 .    using the results of lemma [ lemtnmodulus ]",
    ", we will now show that the test statistic @xmath41 and the asymptotically linear statistic above are asymptotically equivalent .",
    "this will imply the limiting distribution of @xmath41 is the same as that of the asymptotically linear statistic ( [ aslinstat ] ) , which we have already investigated ; see corollary [ coraslinstat ] .",
    "consider the asymptotically linear statistic from ( [ aslinstat ] ) , @xmath371 - f(t ) \\big\\},\\ ] ] with @xmath50 given in ( [ wj ] ) .",
    "it follows , by the arguments preceding corollary [ coraslinstat ] , for this statistic to have the limiting distribution @xmath372 , where @xmath97 is the brownian bridge .",
    "we will now show that @xmath373   - \\avj w_j\\big\\{\\1[\\sigma_0\\err_j \\leq t ] - f(t)\\big\\ } \\bigg|   = \\opn.\\ ] ] combining the above , the desired statement of theorem [ thmtn ] concerning the limiting distribution of the test statistic @xmath41 follows , i.e.@xmath374 \\bigg|    \\xrightarrow[]{d } \\sup _ { t \\in [ 0,1 ] } |b_0(t)|,\\ ] ] it follows from @xmath375 that we can decompose the difference in ( [ diff ] ) into the following sum of five remainder terms : @xmath376 , where @xmath232 and @xmath357 ( which is part of @xmath377 ) are the remainder terms of lemma [ lemtnmodulus ] , and where the other terms are defined as follows , @xmath378 } {   \\avj \\{\\omega(x_j ) - \\avk \\omega(x_k)\\}^2 } \\bigg)^{1/2 }   r_2,\\ ] ] @xmath379 } {   \\avj \\{\\omega(x_j ) - \\avk \\omega(x_k)\\}^2 } \\bigg)^{1/2 }   - 1\\bigg ) \\\\ & \\quad \\times \\bigg(\\avj w_j \\big\\ {   \\1\\big[\\sigma_0\\err \\leq t + \\rhat(x_j ) - r(x_j)\\big ]   - f\\big(t + \\rhat(x_j ) - r(x_j)\\big ) \\big\\}\\bigg),\\end{aligned}\\ ] ] @xmath380 } {   \\avj \\{\\omega(x_j ) - \\avk \\omega(x_k)\\}^2 } \\bigg)^{1/2 }   \\bigg(\\avj w_j\\bigg ) \\\\ & \\quad \\times \\bigg ( \\avj \\big\\ {   \\1\\big[\\sigma_0\\err \\leq t + \\rhat(x_j ) - r(x_j)\\big ]   - f\\big(t + \\rhat(x_j ) - r(x_j)\\big ) \\big\\ } \\bigg),\\end{aligned}\\ ] ] and @xmath381 } {   \\avj \\{\\omega(x_j ) - \\avk \\omega(x_k)\\}^2 } \\bigg)^{1/2 }   \\bigg(\\avj w_j\\bigg ) \\\\   & \\quad \\times \\bigg (   \\avj \\big\\ { f\\big(t + \\rhat(x_j ) - r(x_j)\\big ) - f(t ) \\big\\ }   \\bigg).\\end{aligned}\\ ] ] it remains to show @xmath382 , @xmath383 , which will conclude the proof .",
    "the statement for @xmath384 holds true by the first part of lemma [ lemtnmodulus ] .",
    "note that the assumptions of both statements of lemma [ lemtnmodulus ] are satisfied for our choice of weights @xmath194 .",
    "the statement for @xmath385 follows from the second statement of the same lemma regarding @xmath357 and and from the fact that the first quantity of @xmath377 is a consistent estimator of one .    to show @xmath386",
    ", we only need to demonstrate that @xmath387   - f\\big(t + \\rhat(x_j ) - r(x_j)\\big ) \\big\\ } \\bigg|   = \\opn,\\ ] ] because the first term of @xmath388 both does not depend on @xmath220 and is asymptotically negligible . to verify ( [ r2 ] ) , combine the statement for @xmath232 with the limiting result ( [ coraslinstatresult ] ) from corollary [ coraslinstat ] for the asymptotically linear statistic , which shows @xmath389 - f(t ) \\ } = o_p(n^{-1/2})$ ] , uniformly in @xmath208 .",
    "now consider @xmath390 and remember that both corollary [ coraslinstat ] and the first statement of lemma [ lemtnmodulus ] cover the special case where all of the weights are equal to one , i.e.  ( [ r2 ] ) holds with @xmath391 , @xmath392 .",
    "therefore , the third term of @xmath390 is @xmath393 , uniformly in @xmath208 .",
    "it is clear for the product of the first and second terms of @xmath390 to be @xmath367 .",
    "it then follows that @xmath394 .",
    "we find that @xmath395 is bounded by @xmath396 } {   \\avj \\{\\omega(x_j ) - \\avk \\omega(x_k)\\}^2 } \\bigg)^{1/2 } \\\\ & \\times \\bigg ( \\sup_{x \\in [ 0,\\,1]^m } \\big|\\ahat(x)\\big|   + \\sup_{x \\in [ 0,\\,1]^m } \\big|\\rhat(x ) - r(x ) - \\ahat(x)\\big| \\bigg )    \\bigg|\\avj w_j\\bigg|.\\end{aligned}\\ ] ] the second term in the bound above is a consistent estimator of one . as in the proof of lemma [ lemtnmodulus ] , we use @xmath397^m } |\\ahat(x)| = \\op$ ] and @xmath398^m } |\\rhat(x ) - r(x ) - \\ahat(x)| = \\op$ ] , e.g.  see property ( [ rhatrahatneg ] ) of lemma [ lemrhat ] .",
    "hence , the third term in the bound above is @xmath367 .",
    "we can apply the central limit theorem to treat the fourth quantity and find it is @xmath393 .",
    "combining these findings yields the bound above is @xmath309 .",
    "this implies @xmath399 .",
    "justin chown acknowledges financial support from the contract ` projet dactions de recherche concertes ' ( arc ) 11/16 - 039 of the ` communaut franaise de belgique ' , granted by the ` acadmie universitaire louvain ' , the iap research network p7/06 of the belgian government ( belgian science policy ) and the collaborative research center `` statistical modeling of nonlinear dynamic processes '' ( sfb 823 , teilprojekt c4 ) of the german research foundation ( dfg ) .",
    "dette , h. , neumeyer , n.  and van keilegom , i.  ( 2007 ) . a new test for the parametric form of the variance function in nonparametric regression . _ journal of the royal statistical society , series b _ , * 69 * , 903 - 917 .",
    "hrdle , w.  and mller m.  ( 2000 ) .",
    "multivariate and semiparametric kernel regression . in : _ smoothing and regression : approaches , computation , and application _ ( ed m.g .",
    "schimek ) , john wiley & sons , hoboken , nj , usa .              mller , u.u . ,",
    "schick , a.  and wefelmeyer , w.  ( 2009 ) . estimating the error distribution function in nonparametric regression with multivariate covariates . _ statistics and probability letters _ , * 79 * , 957 - 964 ."
  ],
  "abstract_text": [
    "<S> heteroskedastic errors can lead to inaccurate statistical conclusions if they are not properly handled . </S>",
    "<S> we introduce a test for heteroskedasticity for the nonparametric regression model with multiple covariates . </S>",
    "<S> it is based on a suitable residual - based empirical distribution function . </S>",
    "<S> the residuals are constructed using local polynomial smoothing . </S>",
    "<S> our test statistic involves a  detection function \" that can verify heteroskedasticity by exploiting just the independence - dependence structure between the detection function and model errors , i.e.  we do not require a specific model of the variance function . </S>",
    "<S> the procedure is asymptotically distribution free : inferences made from it do not depend on unknown parameters . </S>",
    "<S> it is consistent at the parametric ( root - n ) rate of convergence . </S>",
    "<S> our results are extended to the case of missing responses and illustrated with simulations .    </S>",
    "<S> _ ruhr - universitt bochum , fakultt fr mathematik , lehrstuhl fr stochastik , 44780 bochum , de _ + @xmath0_department of statistics , texas a&m university , college station , tx 77843 - 3143 , usa _ ]    _ keywords : _ heteroskedastic nonparametric regression , local polynomial smoother , missing at random , transfer principle , weighted empirical process    _ 2010 ams subject classifications : _ primary : 62g08 , 62g10 ; secondary : 62g20 , 62g30 . </S>"
  ]
}