{
  "article_text": [
    "linear regression for interval - valued data has been attracting increasing interests among researchers .",
    "see @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , for a partial list of references .",
    "however , issues such as interpretability and computational feasibility still remain .",
    "especially , a commonly accepted mathematical foundation is largely underdeveloped , compared to its demand of applications . by proposing our new model",
    ", we continue to build up the theoretical framework that deeply understands the existing models and facilitates future developments .    in the statistics literature ,",
    "the interval - valued data analysis is most often studied under the framework of random sets , which includes random intervals as the special ( one - dimensional ) case .",
    "the probability - based theory for random sets has developed since the publication of the seminal book of @xcite .",
    "see @xcite for a relatively complete monograph . to facilitate the presentation of our results",
    ", we briefly introduce the basic notations and definitions in the random set theory .",
    "let @xmath1 be a probability space .",
    "denote by @xmath2 or @xmath3 the collection of all non - empty compact subsets of @xmath4 . in the space @xmath3 , a linear structure is defined by minkowski addition and scalar multiplication , i.e. , @xmath5 @xmath6 and @xmath7 .",
    "a natural metric for the space @xmath3 is the hausdorff metric @xmath8 , which is defined as @xmath9 where @xmath10 denotes the euclidean metric .",
    "a random compact set is a borel measurable function @xmath11 , @xmath3 being equipped with the borel @xmath12-algebra induced by the hausdorff metric . for each @xmath13 ,",
    "the function defined on the unit sphere @xmath14 : @xmath15 is called the support function of x. if @xmath16 is convex almost surely , then @xmath17 is called a random compact convex set .",
    "( see @xcite , p.21 , p.102 . )",
    "the collection of all compact convex subsets of @xmath4 is denoted by @xmath18 or @xmath19 .",
    "when @xmath20 , the corresponding @xmath19 contains all the non - empty bounded closed intervals in @xmath0 .",
    "a measurable function @xmath21 is called a random interval .",
    "much of the random sets theory has focused on compact convex sets .",
    "let @xmath22 be the space of support functions of all non - empty compact convex subsets in @xmath19 .",
    "then , @xmath22 is a banach space equipped with the @xmath23 metric @xmath24^{\\frac{1}{2}},\\ ] ] where @xmath25 is the normalized lebesgue measure on @xmath14 . according to the embedding theorems ( see @xcite , @xcite ) ,",
    "@xmath19 can be embedded isometrically into the banach space @xmath26 of continuous functions on @xmath14 , and @xmath22 is the image of @xmath27 into @xmath26 .",
    "therefore , @xmath28 , @xmath29 , defines a metric on @xmath27 .",
    "particularly , let @xmath30=[x^c - x^r , x^c+x^r]\\ ] ] be an bounded closed interval with center @xmath31 and radius @xmath32 , or lower bound @xmath33 and upper bound @xmath34 , respectively .",
    "then , the @xmath35-metric of @xmath36 is @xmath37 and the @xmath35-distance between two intervals @xmath36 and @xmath38 is @xmath39^{\\frac{1}{2}}\\\\    & = & \\left[\\left(x^c - y^c\\right)^2+\\left(x^r - y^r\\right)^2\\right]^{\\frac{1}{2}}.\\end{aligned}\\ ] ]    existing literature on linear regression for interval - valued data mainly falls into two categories . in the first",
    ", separate linear regression models are fitted to the center and range ( or the lower and upper bounds ) , respectively , treating the intervals essentially as bivariate vectors .",
    "examples belonging to this category include the center method by @xcite , the minmax method by @xcite , the ( constrained ) center and range method by @xcite , and the model m by @xcite .",
    "these methods aim at building up model flexibility and predicting capability , but without taking the interval as a whole .",
    "consequently , their geometric interpretations are prone to different degrees of ambiguity .",
    "take the constrained center and range method ( ccrm ) for example . adopting the notations in @xcite ,",
    "it is specified as @xmath40 where @xmath41 and @xmath42 .",
    "it follows that @xmath43 ^ 2+\\left[\\beta_1^r\\left(x_i^r - x_j^r\\right)\\right]^2.\\end{aligned}\\ ] ] because @xmath44 in general , a constant change in @xmath45 does not result in a constant change in @xmath46 .",
    "in fact , a constant change in any metric of @xmath36 as an interval does not lead to a constant change in the same metric of @xmath38 .",
    "this essentially means that the model is not linear in intervals .    in the second category ,",
    "special care is given to the fact that the interval is a non - separable geometric unit , and their linear relationship is studied in the framework of random sets .",
    "investigation in this category began with @xcite developing a least squares fitting of compact set - valued data and considering the interval - valued input and output as a special case .",
    "precisely , he gave analytical solutions to the real - valued numbers @xmath47 and @xmath48 under different circumstances such that @xmath49 is minimized on the data .",
    "the pioneer idea of @xcite was further studied in @xcite , where the @xmath35-metric was extended to a more general metric called @xmath50-metric originally proposed by @xcite .",
    "the advantage of the @xmath50-metric lies in the flexibility to assign weights to the radius and midpoints in calculating the distance between intervals .",
    "so far the literature had been focusing on finding the affine transformation @xmath51 that best fits the data , but the data are not assumed to fulfill such a transformation . a probabilistic model along this direction kept missing until @xcite , and simultaneously @xcite , proposed the same simple linear regression model for the first time .",
    "the model essentially takes on the form of @xmath52 with @xmath53 and @xmath54 , c\\in\\mathbb{r}$ ] .",
    "this can be written equivalently as @xmath55 it leads to the following equation that clearly shows linearity in @xmath19 : @xmath56    some advances have been made regarding this model and the associated estimators .",
    "@xcite derived least squares estimators for the model parameters and examined them from a theoretical perspective . @xcite",
    "established a test of linear independence for interval - valued data",
    ". however , many problems still remain open such as biases and asymptotic distributions , as anticipated in @xcite .",
    "this paper presents a continuous development addressing some issues and open problems in the direction of model ( [ mod - gil ] ) .",
    "first , we relax the restriction of model ( [ mod - gil ] ) that the hukuhara difference @xmath57 must exist ( see @xcite ) and generalize the univariate model to the multiple case .",
    "we also give analytical least squares ( ls ) solutions to the model parameters .",
    "second , we show that our model and ls estimation together accommodate a decomposition of the sums of squares in @xmath19 analogous to that of the classical linear regression .",
    "third , we derive explicit formulas of the ls estimates for the univariate model , which exist with probability going to one .",
    "the ls estimates are further shown to be asymptotically unbiased .",
    "a simulation study is carried out to validate our theoretical findings , as well as compare our model to ccrm .",
    "finally , we apply our model to a climate data set to illustrate the applicability of our model .",
    "the rest of the paper is organized as follows : section 2 formally introduces our model and the associated ls estimators .",
    "then , the sums of squares and coefficient of determination in @xmath19 are defined and discussed .",
    "section 3 presents the theoretical properties of the ls estimates for the univariate model .",
    "the simulation study is reported in section 4 , and the real data application is presented in section 5 .",
    "we give concluding remarks in section 6 . technical proofs and useful lemmas",
    "are deferred to the appendices .",
    "we consider an extension of model ( [ mod - gil ] ) to the form @xmath58 where @xmath59=[-c , c]$ ] , @xmath60 .",
    "it is equivalently expressed as @xmath61 this leads to the following center - radius specification @xmath62 where @xmath63 , @xmath64 , and the signs ",
    "@xmath65 \" correspond to the two cases in ( [ mod - cases ] ) . define @xmath66 our model",
    "is specified as @xmath67 where @xmath68 , @xmath69 $ ] , @xmath70 , and @xmath71 .    to model the outcome intervals @xmath72 $ ] by @xmath73 interval - valued predictors",
    "@xmath74 $ ] , @xmath75 ; @xmath76 , we consider the multivariate extension of ( [ mod-1 ] ) : @xmath77 which leads to the following center - radius specification @xmath78 where @xmath68 , @xmath69 $ ] , @xmath79 , and @xmath80 .",
    "we have assumed @xmath81 and @xmath82 are independent in this paper to simplify the presentation .",
    "the model that includes a covariance between @xmath81 and @xmath82 can be implemented without much extra difficulty .",
    "least squares method is widely used in the literature to estimate the interval - valued regression coefficients ( @xcite , @xcite , @xcite ) .",
    "it minimizes @xmath83 on the data with respect to the parameters .",
    "denote @xmath84 then the sum of squared @xmath35-distance between @xmath85 and @xmath86 is written as @xmath87\\\\      & = & \\sum_{i=1}^{n}\\left[\\left(b+\\sum_{j=1}^{p}a_jx_{j , i}^c - y_i^c\\right)^2+\\left(\\sum_{j=1}^{p}\\left|a_j\\right|x_{j , i}^r+\\mu - y_i^r\\right)^2\\right].\\end{aligned}\\ ] ] therefore , the lse of @xmath88 is defined as @xmath89 let @xmath90 be the sample covariances of the centers and radii of @xmath91 and @xmath92 , respectively .",
    "especially , when @xmath93 , we denote by @xmath94 and @xmath95 the corresponding sample variances .",
    "in addition , define @xmath96 as the sample covariances of the centers and radii of @xmath91 and @xmath38 , respectively .",
    "then , the minimization problem ( [ def - ls ] ) is solved in the following proposition .",
    "[ prop : ls_solu ] the least squares estimates of the regression coefficients @xmath97 , if they exist , are solution of the equation system : @xmath98 and then , @xmath99 are given by @xmath100      the variance of a compact convex random set @xmath36 in @xmath4 is defined via its support function as @xmath102 where the expectation is defined by aumann integral ( see @xcite , @xcite ) as @xmath103 see @xcite . for the case",
    "@xmath20 , it is shown by straightforward calculations that @xmath104,\\\\    & & \\text{var}(x)=\\text{var}\\left(x^c\\right)+\\text{var}\\left(x^r\\right).\\end{aligned}\\ ] ] this leads us to define the sums of squares in @xmath105 to measure the variability of interval - valued data .",
    "a definition of the coefficient of determination @xmath101 in @xmath105 follows immediately , which produces a measure of goodness - of - fit .",
    "the total sum of squares ( sst ) in @xmath27 is defined as @xmath106.\\ ] ]    the explained sum of squares ( sse ) in @xmath27 is defined as @xmath107.\\ ] ]    the residual sum of squares ( ssr ) in @xmath27 is defined as @xmath108.\\ ] ]    the coefficient of determination ( @xmath101 ) in @xmath27 is defined as @xmath109 where @xmath110 and @xmath111 are defined in ( [ def : sst ] ) and ( [ def : ssr ] ) , respectively .",
    "analogous to the classical theory of linear regression , our model ( [ mmod-1**])-([mmod-2 * * ] ) together with the ls estimates ( [ def - ls ] ) accommodates the partition of @xmath110 into @xmath112 and @xmath111 . as a result , the coefficient of determination ( @xmath101 )",
    "can also be calculated as the ratio of @xmath112 and @xmath110 .",
    "the partition has a series of important implications of the underlying model , one of which being that the residual @xmath113/@xmath114 and the predictor @xmath115 are empirically uncorrelated in @xmath116 .    [ thm : ss ] assume model ( [ mmod-1**])-([mmod-2 * * ] ) .",
    "let @xmath117 and @xmath118 in ( [ exp - c])-([exp - r ] ) be calculated according to the ls estimates @xmath119 in ( [ def - ls ] ) .",
    "then , @xmath120 it follows that the coefficient of determination in @xmath19 is equivalent to @xmath121      it is possible to get negative values of @xmath122 by its definition ( [ exp - r ] ) .",
    "theorem [ thm : pred - adjust ] gives an upper bound of the probability of this unfortunate event . if the model largely explains the variability of @xmath123 , @xmath124 should be very small and so is this bound .",
    "then , the rare cases of negative @xmath122 can be rounded up to 0 since @xmath118 is nonnegative . otherwise , if most of the variability of @xmath123 lies in the random error , the probability of getting negative predicts may not be ignorable , but it is essentially due to the insufficiency of the model and a different model should be pursued anyway .",
    "[ thm : pred - adjust ] consider model ( [ mmod-1**])-([mmod-2 * * ] ) .",
    "let @xmath86 be defined in ( [ exp - c])-([exp - r ] ) .",
    "then , @xmath125",
    "in this section , we study the theoretical properties of the lse for the univariate model ( [ mod-1**])-([mod-2 * * ] ) . applying proposition [ prop : ls_solu ] to the case @xmath126",
    ", we obtain the two sets of half - space solutions , corresponding to @xmath127 and @xmath128 , respectively , as follows : @xmath129 and @xmath130 the final formula for the ls estimates falls in three categories . in the first",
    ", there is one and only one set of existing solution , which is defined as the lse . in the second ,",
    "both sets of solutions exist , and the lse is the one that minimizes @xmath131 . in the third situation , neither solution exists , but this only happens with probability going to @xmath132 .",
    "we conclude these findings in the following theorem .",
    "[ thm : ls_solu ] assume model ( [ mod-1**])-([mod-2 * * ] ) .",
    "let @xmath133 be the least squares solution defined in ( [ def - ls ] ) .",
    "if @xmath134 , then there exists one and only one half - space solution .",
    "more specifically , + * i. * if in addition @xmath135 , then the ls solution is given by @xmath136    * ii .",
    "* if instead @xmath137 , then the ls solution is given by @xmath138 otherwise , @xmath139 , and then either both of the half - space solutions exist , or neither one exists .",
    "in particular , + * iii . *",
    "if in addition @xmath140 , then both of the half - space solutions exist , and @xmath141    * iv . *",
    "if instead @xmath142 , then the ls solution does not exist , but this happens with probability converging to 0 .    unlike the classical linear regression , ls estimates for the model ( [ mod-1**])-([mod-2 * * ] )",
    "are biased .",
    "we calculate the biases explicitly in proposition [ prop : ls_exp ] , which are shown to converge to zero as the sample size increases to infinity",
    ". therefore , the ls estimates are asymptotically unbiased .",
    "[ prop : ls_exp ] let @xmath143 be the least squares solution in theorem [ thm : ls_solu ] . then ,",
    "@xmath144,\\end{aligned}\\ ] ] @xmath145.\\end{aligned}\\ ] ]    [ thm : ls_consist ] consider model ( [ mod-1**])-([mod-2 * * ] ) .",
    "assume @xmath146 and @xmath147 .",
    "then , the least squares solution @xmath143 in theorem [ thm : ls_solu ] is asymptotically unbiased , i.e. @xmath148 as @xmath149 .",
    "we carry out a systematic simulation study to examine the empirical performance of the least squares method proposed in this paper .",
    "first , we consider the following three models : +    * model 1 : @xmath150 , @xmath151 , @xmath152 , @xmath153 , @xmath154 ; * model 2 : @xmath155 , @xmath151 , @xmath152 , @xmath153 , @xmath156 ; * model 3 : @xmath150 , @xmath151 , @xmath157 , @xmath153 , @xmath154 ;    where data show a positive correlation , a negative correlation , and a positive correlation with a negative @xmath25 , respectively .",
    "a simulated dataset from each model is shown in figure [ fig : sim - data ] , along with its fitted regression line .     +    to investigate the asymptotic behavior of the ls estimates , we repeat the process of data generation and parameter estimation 1000 times independently using sample size @xmath158 for all the three models",
    ". the resulting 1000 independent sets of parameter estimates for each model / sample size are evaluated by their mean absolute error ( mae ) and mean error ( me ) .",
    "the numerical results are summarized in table [ tab : sim ] .",
    "consistent with proposition [ prop : ls_exp ] , @xmath159 tends to underestimate @xmath47 when @xmath160 and overestimate @xmath47 when @xmath128 .",
    "this bias also causes a positive and negative bias in @xmath161 , when @xmath160 and @xmath128 , respectively .",
    "similarly , a positive bias in @xmath162 is induced by the negative bias in @xmath163 .",
    "all the biases diminish to 0 as the sample size increases to infinity , which confirms our finding in theorems [ thm : ls_consist ] .",
    ".evaluation of parameter estimation [ cols=\"^ , > , > , > , > , > , > , > \" , ]     [ tab : sim - com ]",
    "in this section , we apply our model to analyze the average temperature data for large us cities , which are provided by national oceanic and atmospheric administration ( noaa ) and are publicly available .",
    "the three data sets we obtained specifically are average temperatures for 51 large us cities in january , april , and july .",
    "each observation contains the averages of minimum and maximum temperatures based on weather data collected from 1981 to 2010 by the noaa national climatic data center of the united states .",
    "july in general is the hottest month in the us . by this analysis",
    ", we aim to predict the summer ( july ) temperatures by those in the winter ( january ) and spring ( april ) .",
    "figure [ fig : real - data ] plots the july temperatures versus those in january and april , respectively .",
    "the parameters are estimated according to ( [ eqn : lse-1])-([eqn : lse-3 ] ) as @xmath164 denote by @xmath165 , @xmath166 , and @xmath167 , the average temperatures in a us city in january , april , and july , respectively .",
    "the prediction for @xmath167 based on @xmath165 and @xmath166 is given by @xmath168 the three sums of squares are calculated to be @xmath169 therefore , the coefficient of determination is @xmath170 finally , the variance parameters can be estimated as @xmath171 thus , by theorem [ thm : pred - adjust ] , an upper bound of @xmath172 on average is estimated to be @xmath173 which is very small and reasonably ignorable .",
    "we calculate @xmath174 for the entire sample and all of them are well above @xmath132 .",
    "so , for this data , although @xmath175 and it is possible to get negative predicted radius , it in fact never happens because the model has captured most of the variability .",
    "the empirical distributions of residuals are shown in figure [ fig : real - residual ] .",
    "both distributions are centered at 0 , with the center residual having a slightly bigger tail .",
    "we have rigorously studied linear regression for interval - valued data in the metric space @xmath176 .",
    "the new model we introduces generalizes previous models in the literature so that the hukuhara difference @xmath177 needs not exist .",
    "analogous to the classical linear regression , our model together with the ls estimation leads to a partition of the total sum of squares ( ssr ) into the explained sum of squares ( sse ) and the residual sum of squares ( ssr ) in @xmath176 , which implies that the residual is uncorrelated with the linear predictor in @xmath176 .",
    "in addition , we have carried out theoretical investigations into the least squares estimation for the univariate model .",
    "it is shown that the ls estimates in @xmath176 are biased but the biases reduce to zero as the sample size tends to infinity .",
    "therefore , a bias - correction technique for small sample estimation could be a good future topic .",
    "the simulation study confirms our theoretical findings and shows that the least squares estimators perform satisfactorily well for moderate sample sizes .    99    artstein z , vitale , ra . a strong law of large numbers for random compact sets .",
    "annals of probability . 1975;5:879882 .",
    "aumann rj .",
    "integrals of set - valued functions . j. math .",
    "1965;12:112 .",
    "billard l , diday e. regression analysis for interval - valued data . in : data",
    "analysis , classification and related methods , proceedings of the seventh conference of the international federation of classification societies ( ifcs00 ) .",
    "springer , belgium ; 2000.p.369374 .",
    "billard l , diday e. symbolic regression analysis . in : classification , clustering and data",
    "analysis , proceedings of the eighth conference of the international federation of classification societies ( ifcs02 ) .",
    "springer , poland ; 2002.p.281288 .",
    "billard l. dependencies and variation components of symbolic interval - valued data . in : selected contributions in data analysis and classification .",
    "springer , berlin heidelberg ; 2007.p.312 .",
    "blanco - fernndez a , corral n , gonzlez - rodrguez g. estimation of a flexible simple linear model for interval data based on set arithmetic .",
    "computational statistics & data analysis .",
    "2011;55:25682578 .",
    "blanco - fernndez a , colubi a , gonzlez - rodrguez g. confidence sets in a linear regression model for interval data . journal of statistical planning and inference .",
    "2012;142:13201329 .",
    "carvalho fat , lima neto ea , tenorio , cp . a new method to fit a linear regression model for interval - valued data .",
    "lecture notes in computer sciences .",
    "2004;3238 : 295306 .",
    "cattaneo megv , wiencierz a. likelihood - based imprecise regression . international journal of approximate reasoning .",
    "2012;53:11371154 .",
    "diamond p. least squares fitting of compact set - valued data . j. math .",
    "1990;147:531544 .",
    "gil ma , lopez mt , lubiano ma , and montenegro m. regression and correlation analyses of a linear relation between random intervals .",
    "2001;10,1:183201 .",
    "gil ma , lubiano ma , montenegro m , lopez mt .",
    "least squares fitting of an affine function and strength of association for interval - valued data . metrika .",
    "2002;56 : 97111 .",
    "gil ma , gonzlez - rodrguez g , colubi a , montenegro m. testing linear independence in linear models with interval - valued data .",
    "computational statistics & data analysis .",
    "2007;51:30023015 .",
    "gonzlez - rodrguez g , blanco a , corral n , colubi a. least squares estimation of linear regression models for convex compact random sets .",
    "advances in data analysis and classification . 2007;1:6781 .    hrmander h. sur la fonction dappui des ensembles convexes dans un espace localement convexe .",
    "arkiv fr mat .",
    "1954;3:181186 .",
    "hukuhara m. integration des applications mesurables do nt la valeur est un compact convexe .",
    "funkcialaj ekvacioj .",
    "1967;10:205223 .",
    "kendall dg .",
    "foundations of a theory of random sets . in : harding ef and kendall dg ( ed ) stochastic geometry .",
    "john wiley & sons , new york ; 1974 .",
    "krner r. a variance of compact convex random sets .",
    "institut fr stochastik , bernhard - von - cotta - str .",
    "2 09599 freiberg ; 1995 .",
    "krner r. on the variance of fuzzy random variables .",
    "fuzzy sets and systems . 1997;92:8393 .",
    "krner r , nther w. linear regression with random fuzzy variables : extended classical estimates , best linear estimates , least squares estimates .",
    "information sciences .",
    "1998;109 : 95118 .",
    "lyashenko nn . limit theorem for sums of independent compact random subsets of euclidean space .",
    "journal of soviet mathematics .",
    "1982;20:21872196 .",
    "lyashenko nn . statistics of random compacts in euclidean space .",
    "journal of soviet mathematics .",
    "1983;21:7692 .",
    "manski cf , tamer t. inference on regressions with interval data on a regressor or outcome .",
    "2002;70:519546 .",
    "matheron g. random sets and integral geometry .",
    "john wiley & sons , new york ; 1975 .",
    "molchanov i. theory of random sets .",
    "springer , london ; 2005 .",
    "lima neto ea , carvalho fat .",
    "centre and range method for fitting a linear regression model to symbolic interval data .",
    "computational statistics & data analysis . 2008 ; 52:15001515 .",
    "lima neto ea , carvalho fat . constrained linear regression models for symbolic interval - valued variables .",
    "computational statistics & data analysis . 2010;54:333347 .",
    "radstrm h. an embedding theorem for spaces of convex sets .",
    "differentiating @xmath131 with respect to @xmath25 , @xmath48 , and @xmath178 , @xmath76 , respectively , and setting the derivatives to zero , we get @xmath179 equations ( [ lse-1])-([lse-2 ] ) yield @xmath180 equations ( [ eqn : lse-1 ] ) are obtained by plugging ( [ lse-4])-([lse-5 ] ) into ( [ lse-3 ] ) , and equations ( [ eqn : lse-2])-([eqn : lse-3 ] ) follow from ( [ lse-4])-([lse-5 ] ) .",
    "this completes the proof .      according to definitions ( [ def : sst])-([def : ssr ] ) , @xmath181\\nonumber\\\\    & = & sse+ssr+2\\sum_{i=1}^{n}\\left[\\left(y_i^c-\\hat{y}_i^c\\right)\\left(\\hat{y}_i^c-\\overline{y^c}\\right )    + \\left(y_i^r-\\hat{y}_i^r\\right)\\left(\\hat{y}_i^r-\\overline{y^r}\\right)\\right]\\nonumber\\\\    &",
    "= & sse+ssr+2\\sum_{i=1}^{n}\\left[\\left(y_i^c-\\hat{y}_i^c\\right)\\hat{y}_i^c+\\left(y_i^r-\\hat{y}_i^r\\right)\\hat{y}_i^r\\right].\\label{ss : eqn-1}\\end{aligned}\\ ] ] the last equation is due to ( [ lse-1])-([lse-2 ] ) .",
    "further in view of ( [ exp - c])-([exp - r ] ) and ( [ lse-3 ] ) , we have @xmath182\\\\    & = & \\sum_{i=1}^{n}\\left[\\left(y_i^c-\\hat{y}_i^c\\right)\\sum_{j=1}^{p}a_jx_{j , i}^c+\\left(y_i^r-\\hat{y}_i^r\\right)\\sum_{j=1}^{p}|a_j|x_{j , i}^r\\right]\\\\    & = & \\sum_{j=1}^{p}a_j\\sum_{i=1}^{n}\\left[\\left(y_i^c-\\hat{y}_i^c\\right)x_{j , i}^c+\\left(y_i^r-\\hat{y}_i^r\\right)sgn(a_j)x_{j , i}^r\\right]\\\\    & = & 0.\\end{aligned}\\ ] ] this together with ( [ ss : eqn-1 ] ) completes the proof .",
    "notice that @xmath183 an application of markov s inequality completes the proof .",
    "parts * i * , * ii * and * iii * are obvious from proposition [ prop : ls_solu ] .",
    "part * iv * follows from lemma [ lem : cov_r ] in appendix ii .",
    "we prove the cases @xmath127 and @xmath128 separately .",
    "to simplify notations , we will use @xmath184 throughout the proof , but the expectation should be interpreted as being conditioned on @xmath36 . + * case i : @xmath127*. + from lemma [ lem : cov_est ] , we have @xmath185 + \\sum_{i < j}(x_i^r - x_j^r ) \\left [ ( y_i^r - y_j^r)-a(x_i^r - x_j^r ) \\right]}{\\sum_{i",
    "< j}(x_i^c - x_j^c)^2+\\sum_{i < j}(x_i^r - x_j^r)^2}\\\\ = & \\frac{\\sum_{i < j}(x_i^c - x_j^c)(\\lambda_i-\\lambda_j)+\\sum_{i < j}(x_i^r - x_j^r)(\\eta_i-\\eta_j ) } { \\sum_{i < j}(x_i^c - x_j^c)^2+\\sum_{i < j}(x_i^r - x_j^r)^2}.\\\\\\end{aligned}\\ ] ] this immediately yields @xmath186 similarly , @xmath187 } { \\sum_{i < j}(x_i^c - x_j^c)^2+\\sum_{i < j}(x_i^r - x_j^r)^2},\\ ] ] and consequently , @xmath188 notice now @xmath189\\\\ & & -\\left[\\int_{\\{\\hat{a}=a^-\\}}(a^+-a ) \\mathrm{d}\\mathbb{p}+\\int_{\\{\\hat{a}=a^-\\}}(a^--a ) \\mathrm{d}\\mathbb{p } \\right]\\nonumber\\\\ & = & e\\left(a^+-a\\right)-\\int_{\\{\\hat{a}=a^-\\}}(a^+-a^- ) \\mathrm{d}\\mathbb{p}\\nonumber\\\\ & = & -e(a^+-a^-)i_{\\{\\hat{a}=a^-\\}}\\label{eqn-3}.\\end{aligned}\\ ] ] here , equation ( [ eqn-3 ] ) is due to ( [ eqn-1 ] ) . recall that @xmath190 } { \\sum_{i < j}(x_i^c - x_j^c)^2+\\sum_{i < j}(x_i^r - x_j^r)^2},\\label{a+-a-}\\end{aligned}\\ ] ] since @xmath191 .",
    "therefore , @xmath192 } { \\sum_{i < j}(x_i^c - x_j^c)^2+\\sum_{i < j}(x_i^r - x_j^r)^2}\\right\\}i_{\\{\\hat{a}=a^-\\}}\\nonumber\\\\ & = & -\\frac{2\\sum_{i < j}\\left[|a|(x_i^r - x_j^r)^2p(\\hat{a}=a^-)+(x_i^r - x_j^r)e(\\eta_i-\\eta_j)i_{\\{\\hat{a}=a^-\\}}\\right ] } { \\sum_{i < j}(x_i^c - x_j^c)^2+\\sum_{i < j}(x_i^r - x_j^r)^2}\\nonumber\\\\ & = & -\\frac{2\\sum_{i < j}(x_i^r - x_j^r)^2p(\\hat{a}=a^-)}{\\sum_{i < j}(x_i^c - x_j^c)^2+\\sum_{i < j}(x_i^r - x_j^r)^2}\\nonumber\\\\ & = & -\\frac{2as^2(x^r)}{s^2(x^c)+s^2(x^r)}p(\\hat{a}=a^-).\\label{rst-1}\\end{aligned}\\ ] ] similar to the preceding arguments , @xmath193 recall again that @xmath194}{s^2\\left(x^c\\right)+s^2\\left(x^r\\right)}.\\label{a++a-}\\end{aligned}\\ ] ] it follows that @xmath195    * case ii : @xmath128 * + in this case , we have @xmath196}{s^2(x^c)+s^2(x^r)},\\\\ & a^--a = \\frac{\\sum_{i < j}(x_i^c - x_j^c)(\\lambda_i-\\lambda_j)-\\sum_{i < j}(x_i^r - x_j^r)(\\eta_i-\\eta_j)}{s^2(x^c)+s^2(x^r)}.\\\\\\end{aligned}\\ ] ] these imply @xmath197 similar to the case of @xmath127 , we obtain @xmath198 these , together with ( [ a+-a- ] ) and ( [ a++a- ] ) , imply , @xmath199 the desired result follows from ( [ rst-1 ] ) , ( [ rst-2 ] ) , ( [ rst-3 ] ) and ( [ rst-4 ] ) .      from ( [ b+ ] ) and ( [ b- ] ) , @xmath200 similarly , from ( [ mu+ ] ) and ( [ mu- ] ) , @xmath201 hence , the desired result follows by proposition [ prop : ls_exp ] and lemma [ lem : sign - consist ] in the appendix .",
    "[ lem : cov_r ] assume model ( [ mod-1**])-([mod-2 * * ] ) and @xmath202 . then @xmath203 . consequently , @xmath204 with probability converging to 1 .    according to ( [ mod-2 * * ] ) , @xmath205-e\\left(x^r\\right)e\\left(|a|x^r+\\eta_1\\right)\\nonumber\\\\    & = & |a|e\\left(x^r\\right)^2+\\mu e\\left(x^r\\right)-|a|\\left[e\\left(x^r\\right)\\right]^2-\\mu e\\left(x^r\\right)\\nonumber\\\\    & = & |a|\\text{var}\\left(x^r\\right)\\nonumber\\\\    & \\geq & 0,\\label{cov_true}\\end{aligned}\\ ] ] provided that @xmath202 . by the slln , @xmath206 ( [ cov_sample ] ) together with ( [ cov_true ] ) completes the proof .      to prove , @xmath209\\\\ & = n\\sum_{i=1}^nx_i^vy_i^v-(\\sum_{i=1}^nx_i^v)(\\sum_{i=1}^ny_i^v)=n^2s\\left(x^v , y^v\\right).\\\\\\end{aligned}\\ ] ] follows by replacing @xmath210 with @xmath211 and @xmath210 with @xmath212 in the above calculations .",
    "[ lem : sign - consist ] assume model ( [ mod-1**])-([mod-2 * * ] ) .",
    "assume in addition that @xmath146 and @xmath147 .",
    "let @xmath133 be the least squares solution defined in ( [ def - ls ] ) .",
    "then @xmath213 as @xmath149 .",
    "we prove the case @xmath127 only .",
    "the case @xmath128 can be proved similarly . under the assumption that @xmath127 , @xmath214 and consequently , @xmath215 . according to theorem [ thm : ls_solu ] , the only other circumstance under which @xmath216 is when @xmath217 and @xmath218 simultaneously .",
    "it is therefore sufficient to show that @xmath219 notice @xmath220\\\\    & & + \\frac{1}{n}\\sum_{i=1}^{n}\\left[\\left(a^+x_i^r+\\mu - y_i^r\\right)^2-\\left(a^-x_i^r+\\mu - y_i^r\\right)^2\\right]\\\\    : = & & \\frac{1}{n}\\left(i+ii\\right).\\end{aligned}\\ ] ] the first term @xmath221\\\\    & = & \\sum_{i=1}^{n}\\left[\\left(a^{+}-a\\right)^2\\left(x_i^c-\\overline{x^c}\\right)^2+\\left(\\lambda_i-\\overline{\\lambda}\\right)^2    -2\\left(a^{+}-a\\right)\\left(x_i^c-\\overline{x^c}\\right)\\left(\\lambda_i-\\overline{\\lambda}\\right)\\right]\\\\    & & -\\sum_{i=1}^{n}\\left[\\left(a^{-}-a\\right)^2\\left(x_i^c-\\overline{x^c}\\right)^2+\\left(\\lambda_i-\\overline{\\lambda}\\right)^2    -2\\left(a^{-}-a\\right)\\left(x_i^c-\\overline{x^c}\\right)\\left(\\lambda_i-\\overline{\\lambda}\\right)\\right]\\\\    & = & \\left[\\left(a^{+}-a\\right)^2-\\left(a^{-}-a\\right)^2\\right]\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)^2\\\\    & & -2\\left(a^{+}-a^{-}\\right)\\sum_{i=1}^{n }    \\left(x_i^c-\\overline{x^c}\\right)\\left(\\lambda_i-\\overline{\\lambda}\\right)\\\\    & = & \\left(a^{+}-a^{-}\\right)\\left[\\left(a^{+}+a^{-}-2a\\right)\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)^2    -2\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)\\left(\\lambda_i-\\overline{\\lambda}\\right)\\right].\\end{aligned}\\ ] ] from this , and the assumption that @xmath217 , we see that @xmath222 is equivalent to @xmath223 on the other hand , @xmath224\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)^2    -\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)\\left(\\lambda_i-\\overline{\\lambda}\\right)\\\\    & & = \\left[\\frac{\\sum_{i < j}\\left(x_i^c - x_j^c\\right)\\left(\\lambda_i-\\lambda_j\\right)}{\\sum_{i < j}\\left(x_i^c - x_j^c\\right)^2+\\sum_{i < j}\\left(x_i^r - x_j^r\\right)^2 }    -a\\frac{s^2\\left(x^r\\right)}{s^2\\left(x^c\\right)+s^2\\left(x^r\\right)}\\right]\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)^2\\\\    & & -\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)\\left(\\lambda_i-\\overline{\\lambda}\\right)\\\\    & & = \\frac{\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)^2}{\\sum_{i < j}\\left(x_i^c - x_j^c\\right)^2+\\sum_{i < j}\\left(x_i^r - x_j^r\\right)^2 }    \\sum_{i < j}\\left(x_i^c - x_j^c\\right)\\left(\\lambda_i-\\lambda_j\\right)\\\\    & & -\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)\\left(\\lambda_i-\\overline{\\lambda}\\right )    -a\\frac{s^2\\left(x^r\\right)}{s^2\\left(x^c\\right)+s^2\\left(x^r\\right)}\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)^2\\\\    & & = \\frac{\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)^2}{\\sum_{i < j}\\left(x_i^c - x_j^c\\right)^2+\\sum_{i < j}\\left(x_i^r - x_j^r\\right)^2 }    \\left[n\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)\\left(\\lambda_i-\\overline{\\lambda}\\right)\\right]\\\\    & & -\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)\\left(\\lambda_i-\\overline{\\lambda}\\right )    -a\\frac{s^2\\left(x^r\\right)}{s^2\\left(x^c\\right)+s^2\\left(x^r\\right)}\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)^2\\\\    & & = \\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)\\left(\\lambda_i-\\overline{\\lambda}\\right )    \\left[\\frac{s^2\\left(x^c\\right)}{s^2\\left(x^c\\right)+s^2\\left(x^r\\right)}-1\\right]\\\\    & & -a\\frac{s^2\\left(x^r\\right)}{s^2\\left(x^c\\right)+s^2\\left(x^r\\right)}\\sum_{i=1}^{n}\\left(x_i^c-\\overline{x^c}\\right)^2\\\\    & & = -\\frac{s^2\\left(x^r\\right)}{s^2\\left(x^c\\right)+s^2\\left(x^r\\right ) }    n\\left[as^2\\left(x^c\\right)+s\\left(x^c , \\lambda\\right)\\right],\\end{aligned}\\ ] ] where @xmath225 denotes the sample covariance of the random variables @xmath31 and @xmath226 , which converges to @xmath132 almost surely by the independence assumption .",
    "therefore , @xmath227\\nonumber\\\\    & & \\to c_1<0\\label{eqn : consist-2}\\end{aligned}\\ ] ] almost surely , as @xmath149 . + by the similar calculation , we have that the second term @xmath228\\nonumber\\\\    & & \\to c_2<0\\label{eqn : consist-3}\\end{aligned}\\ ] ] almost surely , as @xmath149 .",
    "( [ eqn : consist-2 ] ) and ( [ eqn : consist-3 ] ) together imply that @xmath229 this completes the proof ."
  ],
  "abstract_text": [
    "<S> it has been some time since interval - valued linear regression was investigated . in this paper , we focus on linear regression for interval - valued data within the framework of random sets . </S>",
    "<S> the model we propose generalizes a series of existing models . </S>",
    "<S> we establish important properties of the model in the space of compact convex subsets of @xmath0 , analogous to those for the classical linear regression . </S>",
    "<S> furthermore , we carry out theoretical investigations into the least squares estimation that is widely used in the literature . a simulation study is presented that supports our theorems . </S>",
    "<S> finally , an application to a climate data set is provided to demonstrate the applicability of our model . </S>"
  ]
}