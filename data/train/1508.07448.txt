{
  "article_text": [
    "predictive distributions play a prominent role in bayesian theory ; in fact , sequences of predictive densities fully characterize a bayesian model via the well known de  finetti representation theorem , as discussed in de  finetti ( 1937 ) and hewitt and savage ( 1955 ) .",
    "the bayesian predictive density is obtained by updating the prior to the posterior and then marginalizing over the model parameters . in particular ,",
    "if @xmath0 is the statistical model for iid real - valued data @xmath1 and @xmath2 is the prior distribution for the parameter @xmath3 , then the predictive density for @xmath4 , given @xmath5 , is given by @xmath6 where the posterior distribution @xmath7 for @xmath3 , given @xmath5 , is @xmath8 intuitively , this bayesian approach should be ideally suited to the accurate and coherent updating of information .",
    "however , by examining ( [ pred ] ) , one can see that there is no obvious route to quickly update the predictive directly : when a new observation is received , one first updates the posterior and then computes the integral to obtain the predictive .",
    "this can be especially prohibitive in complex problems where monte carlo methods are needed to compute the posterior .",
    "the goal of this paper is to show that the bayesian predictive distribution update can , indeed , be expressed in a recursive form , making fast online bayesian prediction possible , even in complex nonparametric models .    to show that the bayesian predictive @xmath9 in can be updated without directly passing through the posterior , alleviating the need for monte carlo methods in bayesian prediction ,",
    "our jumping off point is a new observation that the predictive updates can be expressed in terms of a sequence of bivariate copula densities ( e.g. , nelson 1999 ) .",
    "this observation is interesting for at least three reasons :    * according to de  finetti s representation theorem , this sequence of copula densities provides an alternative characterization of the bayesian model itself ; * in cases where this sequence of copula densities can be identified analytically , this representation provides fast recursive updates to the bayesian predictive ; * and , even in cases where the sequence of copula densities can not be written down analytically , the copula representation provides new insights on how to approximate the recursive updates .",
    "the latter point above leads to the main contribution of the paper .",
    "certain applications , such as color modeling and tracking of objects ( e.g. , elgammal et al .",
    "2003 ; han et al .",
    "2008 ) , require both flexible nonparametric modeling and fast online estimation .",
    "however , the challenges in updating the bayesian predictive are most acute in nonparametric problems , so kernel - based densities estimates ( e.g. , raykar et al .",
    "2010 ; nakamura and hasegawa 2013 ) are often preferred over bayesian methods in these applications .",
    "a commonly used bayesian nonparametric model is the mixture of dirichlet processes ( e.g. , escobar 1988 ; escobar and west 1995 ) , but the need for markov chain monte carlo methods to compute the posterior motivated newton and zhang ( 1999 ) and newton ( 2002 ) to propose a _",
    "predictive recursion _ algorithm for estimating the posterior ; see , also , tokdar et al .",
    "( 2009 ) and martin and tokdar ( 2009 , 2011 ) . despite its name",
    ", the predictive recursion algorithm is not fully satisfactory for estimating the predictive distribution : it targets the posterior instead of the predictive , so integration is needed to compute normalizing constants , etc .",
    "our copula characterization of the predictive update remains valid in nonparametric problems , but it may not be possible to derive the sequence of copula densities in closed - form .",
    "it does , however , suggest a new version of newton s predictive recursion algorithm that targets the predictive density directly , avoiding the difficult problem of computing normalizing constants . besides being intuitively clear and fast to compute",
    ", we show both theoretically and numerically the accuracy of our proposed recursive predictive density estimate .",
    "the layout of the paper is as follows . in section",
    "2 we provide the details of our representation of the predictive via copula models and identify the particular sequence of copula densities for some common bayesian models .",
    "our investigation of the mixture of dirichlet processes model lays the foundation for our recursive algorithm directly targeting the predictive densities presented in section  3 . after giving a few illustrative examples ,",
    "we establish , in section  4 , both weak and kullback  leibler consistency of the predictive distribution sequence .",
    "numerical examples given in section  5 demonstrate that the recursive copula approach is competitive with other common density estimation methods , i.e. , bayesian gaussian mixture models and kernel density estimation methods , on prediction tasks .",
    "section  6 provides some concluding remarks and the appendix provides some technical details .",
    "to characterize the bayesian predictive updates , we take a sequential point of view .",
    "that is , if @xmath10 is the predictive density for @xmath11 based on observations @xmath12 , then we want an update @xmath13 for the predictive density for @xmath4 based on observations @xmath5 .",
    "consider the bivariate function @xmath14 that satisfies @xmath15 therefore , @xmath16 which is symmetric in @xmath17 , since @xmath18 the function @xmath19 in ( [ kstuff ] ) is easily seen to be a bivariate copula density function ; that is , for some symmetric copula density @xmath20 , which depends only on the sample through the sample size , we have @xmath21 where @xmath22 is a symmetric copula density , and @xmath23 is the distribution function corresponding to the predictive density @xmath10",
    ".    we can now write the update @xmath13 as @xmath24 and for each bayesian model there is a unique sequence @xmath20 .",
    "now ( [ copula ] ) allows for the direct update of the predictive and moreover it can be seen that all one needs to direct a sequence of predictive densities is to define a sequence of copula functions @xmath20 , the key to which is that @xmath25 as @xmath26 , i.e. the sequence of copula converges to the independent copula as the sample size increases . to put into context , the de  finetti characterization of a bayesian model is in terms of a ( dependent ) joint distribution over all future observables @xmath27 and such a joint distribution can always be expressed in compositional form @xmath28 , etc .",
    "additionally , sklar s theorem ( sklar 1959 ) tells us that any joint distribution can be represented in copula form .",
    "these elements are familiar .",
    "this paper focuses on the computational properties of a copula representation for the bivariate conditional distribution @xmath29 , as given in ( [ copula0 ] ) , which will lead to a novel approximation of the predictive update in ( [ copula ] ) .",
    "in this section we consider some standard bayesian models and where the focus is on establishing the corresponding sequence @xmath20 of copula densities .    here",
    "we consider the model and prior as @xmath30 respectively .",
    "then standard calculations give @xmath31 where @xmath32 , and @xmath33 therefore , @xmath34 which can be seen to be symmetric in @xmath17 .",
    "now @xmath35 and so @xmath36.\\ ] ] therefore , @xmath37^{n+2 } } \\ ] ] and so we have the clayton copula ( clayton 1978 ) , i.e. , @xmath38 with parameter @xmath39 , describing the sequence of predictive distributions . note that , as @xmath26 , @xmath20 converges to the independence copula .",
    "the calculations in example  1 can be generalized to cover an exponential family model with conjugate prior , i.e. , @xmath40 and @xmath41 .",
    "a by - product of this argument is the identification of a new and general class of copula that contains the archimedean class .",
    "details are provided in appendix  a.1 .",
    "here we consider a normal model @xmath42 and a conjugate prior @xmath43 .",
    "the math for this is quite straightforward and it is no surprise we recover a gaussian copula with correlation parameter @xmath44 . to see this , we have @xmath45 where @xmath46 , and write @xmath47 . hence @xmath48 where @xmath49 .",
    "writing the copula density in the form of a gaussian copula with only the key exponential component ; we have , since @xmath50 , @xmath51-\\frac{2\\rho}{1-\\rho^2}\\bigl(\\frac{y-\\mu_n}{\\sigma_n}\\bigr)\\,\\bigl(\\frac{x-\\mu_n}{\\sigma_n}\\bigr).\\ ] ] the derived term by writing down @xmath52 is given by @xmath53 these are equal up to constant terms when @xmath44 . hence , @xmath54 where @xmath55 stands for gaussian copula density with correlation @xmath56 , i.e. , @xmath57 with @xmath58 the standard bivariate normal density , with correlation @xmath56 , and @xmath59 the @xmath60 distribution function",
    ". note that if the model were @xmath61 , with @xmath62 , and we put a standard conjugate prior on the variance parameter @xmath63 , then we would recover the student - t copula for the update .    here",
    "we consider the model where @xmath64 and @xmath65 with @xmath66 .",
    "this model is well known and @xmath67 where @xmath68 and @xmath69 .",
    "therefore , to recognize this copula , we find the copula distribution @xmath70 which in more detail is @xmath71 where @xmath72 and @xmath73 .",
    "thus @xmath74 is a mixture of the frechet  hoeffding copula , @xmath75 , and the independence copula , @xmath76 .",
    "here we consider a nonparametric model , namely , a mixture of dirichlet processes model as considered in escobar ( 1988 ) and escobar and west ( 1995 ) , given by @xmath77 where @xmath78 is a given kernel and the prior assigned to @xmath79 is a dirichlet process prior @xmath80 , where @xmath81 is the base measure and @xmath82 is the precision parameter ( ferguson , 1973 ) .",
    "this model was first introduced in lo ( 1986 ) and the constructive definition of the dirichlet process , see sethuraman ( 1994 ) , means we can write @xmath83 where the @xmath84 are iid @xmath81 and the weights @xmath85 follow a stick - breaking construction , i.e. , @xmath86 and , for @xmath87 , @xmath88 , with @xmath89 iid @xmath90 .",
    "hjort et al .",
    "( 2010 ) give details on this model and inference procedures using markov chain monte carlo .",
    "let us assume that @xmath91 and @xmath81 is @xmath92 , as in example 2 .",
    "we can extend this to a prior on the variance and we will recover the student - t copula instead of the gaussian copula .",
    "now , for the first update , we can compute the copula density ; it is given by @xmath93 where , @xmath94 , which is known , @xmath95 and @xmath96 .",
    "hence , the copula is a mixture of the gaussian copula , @xmath97 , in with @xmath98 as in example  2 , and the independence copula .",
    "this yields @xmath99 the calculations carried out above can not be done explicitly for the general update from @xmath100 , since the copula density is too difficult to compute .    in the posterior framework ,",
    "the complexity of the posterior inspired newton to propose a fast iterative method that replicates the update for the @xmath101 case for general @xmath102 .",
    "given an updating rule for the posterior or , more precisely , for the mixing distribution , the predictive can be obtained via integration .",
    "although newton s updates of the mixing distribution and of the corresponding predictive are fast in principle , there may be difficulties in practice due to intractable normalizing constants .",
    "such a normalizing constant does not arise when we work directly with the predictive .",
    "motivated by the calculations for the mixture of dirichlet processes model in section  [ ss : dpmix ] , we propose the following recursive algorithm for directly updating the predictive , completely avoiding the posterior .",
    "in particular , fix an initial guess @xmath103 , with density @xmath104 , and a sequence of weights @xmath105 . then , sequentially compute @xmath106 where @xmath55 is the gaussian copula density in .",
    "the sequence @xmath107 is based on stick breaks which are iid @xmath108 .",
    "therefore , they look like roughly @xmath39 , which is effectively what newton took them to be ; see below",
    ". we will take @xmath56 to be a constant close to 1 . in the next two sections",
    ", we will demonstrate empirically and theoretically the remarkable estimating abilities of ( [ mdp ] ) .    here",
    "we make two remarks . in the gaussian copula model in example  2 ,",
    "the sample size was captured by @xmath109 . in ( [ mdp ] ) ,",
    "the @xmath56 is held fixed and the sample size is now carried by @xmath110 .",
    "indeed , it is @xmath110 going to 0 that takes us to the independence copula .",
    "also , the coherence property enjoyed by the `` correct '' bayesian update , i.e. , @xmath111 comes at a price ",
    "it can not be computed recursively . on the other hand , by sacrificing coherence , we can get a fast update which is still theoretically and numerically accurate .",
    "to elaborate , the update @xmath104 to @xmath112 is the exact bayesian update and , therefore , must be good ; our proposal is to replicate this `` good '' update for all @xmath102 .",
    "we lose the coherence property above , but this has no effect on accuracy .",
    "a few words should also be said about the implementation .",
    "it is actually simpler to work on the distribution function scale , where the algorithm looks like @xmath113 where @xmath114 the computational strategy is to take a fixed grid of points , @xmath115 , in @xmath116 and compute the sequence @xmath117 for each @xmath118 .",
    "then the distribution function @xmath119 can be plotted by interpolation . from this , the density @xmath120 can be obtained by approximating the derivative by a difference ratio . given the distribution function or density evaluated on a fine grid of points , features of the predictive distribution , such as the mean or quantiles , can be readily obtained .      here",
    "we give three illustrations of the recursive predictive distribution estimator described in section  3.1 .",
    "in particular , we take @xmath121 samples from a given @xmath122 and display , in figure  1 , the predictive distribution function estimate @xmath123 for a selected initial guess @xmath103 . in each case , the weight sequence @xmath107 is taken as @xmath124 .",
    "consider a two - component normal mixture , @xmath125 , and take the initial guess as @xmath126 .",
    "the estimate @xmath123 is displayed in figure  1(a ) and we see that flatter portion of the distribution near zero has been clearly identified .",
    "indeed , the estimate @xmath123 displayed in black and @xmath122 displayed in gray are indistinguishable .",
    "next , we take data from a similar three - component normal mixture , @xmath127 , and the initial guess @xmath103 just as in illustration  1 . the estimate is shown in figure  1(b ) and , again , the non - normal shape is clearly identified .    to see the versatility of the procedure , here we take data from a gamma distribution , @xmath128 , and take the initial guess as @xmath129 .",
    "the estimate is shown in figure  1(c ) and , like in the previous illustrations , we see that the shape of the true @xmath122 is identified .",
    "this is in spite of the fact that the recursive scheme is essentially gaussian - based .",
    "here we investigate the asymptotic convergence properties of the recursive estimator @xmath123 of the predictive distribution of @xmath4 , given @xmath1 , presented in . recall that this algorithm is based on a gaussian copula via the function @xmath130 in , and throughout we take the copula correlation parameter @xmath131 to be fixed .",
    "we will also require that the weight sequence @xmath107 satisfies @xmath132 of course , is satisfied if @xmath133 , as in our illustrations in section  3.2 , but sequences that vanish a bit slower are also allowed , e.g. , @xmath134 , @xmath135 $ ] .    in this section , we prove two successively stronger convergence results for @xmath123 , namely weak and kullback  leibler ( or @xmath136 ) convergence to the true distribution @xmath122 with probability  1 . for weak consistency ,",
    "we need two preliminary results .",
    "the first establishes that @xmath123 converges weakly to some distribution function @xmath137 .",
    "the proof given in appendix  [ s : proofs ] requires a careful tightness argument .",
    "[ lem : tight ] given , there exists a distribution function @xmath137 such that @xmath138 weakly @xmath122-almost surely .",
    "next , for @xmath130 in , define a mapping @xmath139 as @xmath140 this takes a distribution function @xmath141 to a new distribution function @xmath142 .",
    "an essential part of our consistency proofs , and the second of our preliminary results , is the representation of the true distribution @xmath122 as the unique fixed point of the mapping @xmath143 .",
    "[ lem : fixed.point ] when restricted to distribution functions @xmath141 supported on @xmath116 , the only fixed point of @xmath143 is @xmath122 , i.e. , @xmath144 if and only if @xmath145 .    fixed point arguments have been used previously in the convergence analysis of newton s predictive recursion algorithm ( martin and ghosh 2008 ; martin and tokdar 2009 ; walker 2015 ) and a related algorithm in shayamalkumar ( 1996 ) .",
    "the novelty here is that the fixed point representation is in terms of the predictive and not the posterior .",
    "we can now state and prove the first of our two main consistency results , namely , that the recursive estimator @xmath123 converges weakly to the true distribution @xmath122 with probability  1 under a mild continuity condition on the limit @xmath122 .",
    "this result implies the convergence of @xmath123 observed in the three illustrations in figure  1 .",
    "[ thm : weak.limit ] if @xmath122 is continuous and the weight sequence @xmath107 satisfies , then the predictive distribution @xmath123 satisfies @xmath146 weakly @xmath122-almost surely .",
    "it remains to show that the limit @xmath137 in lemma  [ lem : tight ] is @xmath122 .",
    "for the mapping @xmath143 given in , define a new sequence of random functions @xmath147 as @xmath148 the key property is that , for each fixed @xmath149 , @xmath150 is a bounded martingale difference sequence .",
    "now @xmath151 and since @xmath150 is bounded and @xmath152 , the martingale convergence theorem implies the last term on the right - hand - side of ( [ tele ] ) converges @xmath122-almost surely to a random variable with finite mean .",
    "since @xmath119 converges for all @xmath149 , so too must the central term of ( [ tele ] ) .",
    "since @xmath153 , @xmath154 must have @xmath155 as an accumulation point for all @xmath149 . now , since @xmath156 has a limit @xmath137 , we can conclude that @xmath137 is a fixed point of @xmath143 .",
    "therefore , @xmath157 by lemma  [ lem : fixed.point ] , completing the proof .    towards a stronger mode of convergence ,",
    "consider the algorithm for the predictive density @xmath120 given by @xmath158 , \\end{aligned}\\ ] ] where @xmath159 is the bivariate gaussian copula density with correlation parameter @xmath160 and @xmath103 is an initial guess .",
    "let @xmath161 denote the kullback  leibler divergence , and @xmath162 the true data - generating density .",
    "the goal is to provide condition such that @xmath163 @xmath122-almost surely .",
    "our analysis here is based on that in martin and tokdar ( 2009 ) for proving consistency of newton s original predictive recursion algorithm .",
    "however , since there is no natural mixture model structure , some new ideas are needed .",
    "the main ingredient is a representation of the gaussian copula density as a sort of mixture .",
    "to start , write @xmath164 { p^\\star}(y ) \\,\\d y.\\end{aligned}\\ ] ] for @xmath165 away from @xmath166 , i.e. , @xmath167 , the following inequality holds : @xmath168 this inequality can be applied in our case , since @xmath169 and @xmath170 , and it gives @xmath171 where the `` remainder term '' @xmath172 is given by @xmath173 taking conditional expectation with respect to @xmath174 , we get @xmath175 if the double integral above is positive , and the remainder term is negligible , then @xmath176 is an `` almost supermartingale '' ( robbins and siegmund 1971 ) and converges to an almost sure limit , say , @xmath177 . to handle the double integral , and to show that the limit is almost surely zero , some manipulation of the copula density @xmath55 is needed .",
    "traditionally , the copula density is written as in equation above , which has a relatively simple closed - form expression that is used for practical implementation .",
    "however , for our theoretical analysis , it will be convenient to rewrite the copula density as @xmath178 where @xmath179 is a ratio of normal densities , @xmath180 this follows from routine calculations using normal convolutions .",
    "the point is that the gaussian copula has a sort of mixture or `` conditionally iid '' representation .",
    "kullback  leibler consistency also requires two preliminary results ; see appendix  [ s : proofs ] for the proofs . for",
    "the first , write @xmath181 for that double integral in the conditional expectation above , where @xmath182 and @xmath183 is a generic density with distribution function @xmath141 .",
    "if we plug in the alternative representation of the copula density into the formula for @xmath184 and interchange the order of integration , we get @xmath185    [ lem : positive ] consider a density @xmath183 whose support contains that of @xmath162",
    ". then @xmath186 with equality if and only if @xmath187 lebesgue - almost everywhere .",
    "our second preliminary result demonstrates that the remainder term @xmath172 is negligible , i.e. , it vanishes sufficiently fast that it does not disrupt the supermartingale - like dynamics of kullback  leibler sequence @xmath188 .",
    "[ lem : bound ] write @xmath189 .",
    "suppose that @xmath162 is continuous and satisfies @xmath190 if @xmath107 satisfies , then @xmath191 @xmath122-almost surely .",
    "the integrability condition can be understood as a tail condition on the true density @xmath162 .",
    "for example , if @xmath162 has exponential tails , then so does @xmath122 and @xmath192 , so condition requires a suitable balance of the constants in the tail exponents .",
    "it is straightforward to check that condition holds for a variety of distributions @xmath122 , including weibull , pareto , normal , etc , for any correlation @xmath131 .",
    "[ thm : limit ] let @xmath9 be the predictive density for @xmath4 , given @xmath1 defined above , with correlation parameter @xmath131 and with weight sequence @xmath107 that satisfies .",
    "if the true density @xmath162 is continuous and satisfies , and if the support of the initial guess @xmath104 contains that of @xmath162 , then @xmath163 @xmath122-almost surely .    from the expression for @xmath193 , and lemmas  [ lem :",
    "positive][lem : bound ] , it follows from robbins and siegmund ( 1971 ) that @xmath194 it remains to show that @xmath195 @xmath122-almost surely .",
    "suppose , to the contrary , that @xmath196 with positive probability .",
    "then @xmath9 is away from @xmath162 ( in the kullback ",
    "leibler sense ) for all but finitely many @xmath102 with positive probability .",
    "more precisely , there is a set of positive lebesgue measure on which @xmath197 . by lemma  [ lem : positive ] , this implies @xmath198 for all but finitely many @xmath102 . since @xmath181 is bounded away from zero , we get @xmath199 with positive probability , which contradicts the second conclusion in the above display",
    ". therefore , we must have @xmath195 almost surely , completing the proof .",
    "keeping with our focus on predictive distributions , this section evaluates the recursive estimator in terms of a predictive loss , measuring the difference between a prediction and a future realization of an observable variable .",
    "specifically , we consider a finite vector of quantiles , defining a vector valued check - loss function .",
    "the check loss function is a piece - wise linear loss function which can be expressed as @xmath200 check loss gets its name from the check - shaped graph of the function , as shown in figure  [ check_loss ] .",
    "check loss can be justified intuitively in terms of asymmetric costs . to take a simple example ,",
    "consider a restaurant : too much inventory leads to waste via spoilage at some cost per unit ( purchase price ) , while too little inventory leads to foregone sales due to unfulfillable orders at a distinct cost per unit ( because orders for multiple items are canceled in their entirety ) .",
    "check loss is intimately related to quantile estimation as follows : it is straightforward to show that for any density function @xmath201 with distribution function @xmath202 , the integrated ( expected ) check loss is minimized at @xmath203 . in our simulation study , we use a vector - valued check loss function defined by a vector parameter @xmath204 ; specifically we consider @xmath205 .      for our first simulation study , we compare the performance of our recursive approximation of the predictive density to the predictive distribution arising from the posterior of a bayesian finite mixture model with @xmath206 normal mixture components with unknown means @xmath207 , variances @xmath208 and mixture weights @xmath209 . more concretely",
    ", we use the rnmixgibbs function from the bayesm package in the r statistical language , by peter rossi , which employs a gibbs sampling approach to generate samples from the posterior .",
    "we use the default prior specification to generate 5,000 posterior samples .",
    "our exact function call is :    ....    rnmixgibbs(data = list(y = as.matrix(x ) ) , prior = list(ncomp=10 ) ,                mcmc = list(r=5000 ) ) ....    where x is the vector of observed data . with our method",
    "we use @xmath210 , @xmath211 and @xmath104 a standard cauchy distribution .",
    "we generate the data , @xmath212 , according to a two component mixture of t - distributions with 5 degrees of freedom .",
    "one of these components is a fixed to have location parameter 1 and scale parameter 1 .",
    "the second component has mean @xmath213 and scale @xmath214 .",
    "we simulate 500 independent samples from this distribution of size @xmath215 . for each sample ,",
    "the values of @xmath213 , @xmath216 and the mixing proportion @xmath217 , are drawn at random according to @xmath218 , @xmath219 and @xmath220 . to evaluate each method",
    ", we compute the mean check loss on a monte carlo sample of size 100,000 from the true distribution , using the optimal action according to the inferred predictive distribution using each method , which we denote @xmath221 and @xmath222 respectively .",
    "we also compute @xmath223 which is the check loss minimizer according to the true data generating distribution .",
    "finally , we consider the scaled difference of integrated check loss : @xmath224 we evaluate @xmath225 for @xmath226 trials .",
    "figure [ result ] depicts the distribution of @xmath227 for each value of @xmath204 ; table [ results ] reports summary statistics of the distribution of @xmath228 across trials .     observations .",
    "negative numbers mean the recursive method outperformed the bayesian finite mixture model .",
    "units are in percentage of the theoretical optimal check loss.,width=528 ]    .summary statistics of the distribution of @xmath229 across simulations for @xmath215 observations .",
    "recall that @xmath230 implies that the recursive method has lower loss than the bayesian finite mixture approach .",
    "the table reveals that the recursive method performs at least as good as the finite mixture model at all values of @xmath204 considered ; the recursive approach is comparatively better further away from the median .",
    "[ cols=\">,>,>,>\",options=\"header \" , ]     as with the batch simulation study , our claim is not that the bivariate recursive method is outright superior to these alternatives .",
    "however , these simulations highlight certain virtues of the approach  speed and ease - of - implementation  while demonstrating that the performance is broadly competitive .",
    "it is worth emphasizing that simulation studies such as those reported here are inherently sensitive to prior specification : after all , attempting to infer the 10%tile based on only fifty observations is a difficult task that will benefit from wise choices of prior .",
    "that said , we argue that the recursive bivariate copula approach has an advantage in terms of being relatively transparent in terms of its prior specification ( the initial distribution function @xmath104 can be a convenient parametric form ) and its hyper - parameters ( which are easy to elicit and with default ranges @xmath231 , @xmath232 that give good empirical performance . ) .",
    "mixture models of any stripe do not boast this advantage .",
    "in this paper , we have identified an interesting new connection between bayesian predictive updates and the well - known bivariate copulas . besides the new light cast on this previously unknown connection between bayesian inference and copulas , which can provide further and deeper insights and understanding about both",
    ", this development makes clear that bayesian predictive updates need not require posterior computations .",
    "this opens the door for online bayesian prediction , as well as for bayesian predictive analysis for researchers who are uncomfortable with the implementation and/or slow speeds of markov chain monte carlo methods .",
    "the new recursive algorithm developed here is important because it provides a direct attack on the predictive density , which can simplify both the modeling and the computational aspects in applications .",
    "first , if the predictive is the goal , then needing to specify a mixture model , especially , a support for the mixing distribution , is undesirable , and the new algorithm circumvents this .",
    "second , newton s original algorithm requires computation of a normalizing constant at each iteration , and these are never available in closed - form . for mixing distributions supported on one- or two - dimensional spaces this can easily be handled with quadrature but , to date , there is no efficient strategy for computing these normalizing constants for higher - dimensional spaces .",
    "again , the new version that directly attacks the predictive distribution avoids all of these difficulties .",
    "the authors research is partially supported by the u.  s.  national science foundation , grants dms1507073 and dms1506879 , and by the u.  s.  army research offices , award # w911nf-15 - 1 - 0154 .",
    "write @xmath233 for the distribution function corresponding to the model density @xmath234 .",
    "suppose that @xmath235 can be written as @xmath236 for non - negative functions @xmath237 and @xmath238 .",
    "then a symmetric copula distribution can be defined via @xmath239 for some function @xmath240 , where @xmath241 , provided @xmath242 define @xmath243 so @xmath244 where @xmath245 . also , @xmath246 the aim now is to show that for the functions @xmath247 and @xmath248 with @xmath249 we get that @xmath250 satisfy ( [ condi ] ) . to this end ,",
    "we have that @xmath251 is given by @xmath252 which is easily seen to be @xmath253 , and hence ( [ condi ] ) holds .",
    "thus , the new class of symmetric copula is given by @xmath254 with @xmath255 the corresponding copula density function is given by @xmath256 where @xmath257 .",
    "to see this more clearly , let @xmath258 , so @xmath259 and now @xmath238 is the density function corresponding to @xmath235 .",
    "if @xmath260 is negative , i.e. @xmath235 is concave , then @xmath261 , giving @xmath262 this is an archimedean copula and hence the new class of copula provides a generalization .",
    "note that for the exponential model considered in example 1 , we did have @xmath263 and hence we recovered an archimedean copula .      for each fixed @xmath149",
    ", we have @xmath264 , so , by , @xmath265 therefore , @xmath266 is a cauchy sequence , and completeness of the closed unit interval , @xmath267 $ ] , implies that @xmath268 for some @xmath269 $ ] for all @xmath149 .    to show that the limit @xmath137 is a proper distribution function , we need some auxiliary results . for @xmath130 in , define the related function @xmath270 the following simple but useful properties of @xmath130 and @xmath271",
    "will be needed :    * for all @xmath272 , @xmath273 ; this follows from monotonicity of @xmath274 for fixed @xmath275 . * if @xmath276 , then @xmath277 for @xmath278 ; this follows since @xmath279 is negative and @xmath280 is decreasing . * for sufficiently small @xmath275 , @xmath281 ; for all @xmath276 , we have @xmath282 and the inequality maintains but on a smaller interval if the line gets steeper .",
    "returning to our main proof , we want to show that @xmath137 is a proper distribution function or , equivalently , that @xmath156 is tight . in this case , since each @xmath123 is non - decreasing and takes all values in @xmath283 , if tightness fails , then @xmath137 is a constant function , equal to either 0 or 1 .",
    "so , to prove tightness , we need to show that constant @xmath137 is a probability  0 event . towards this , we proceed by contradiction .",
    "assume that @xmath284 with positive probability ; the case @xmath285 can be handled similarly .",
    "on the event where @xmath284 , we can be sure that eventually @xmath119 will be arbitrarily small . in this case , by property  a above , we have @xmath286 let @xmath287 and apply property  b to get @xmath288 now , by property  c , we get @xmath289 the latter inequality being a consequence of @xmath290 .",
    "factor out @xmath291 and apply a telescoping product to get @xmath292 we assumed that @xmath284 with positive probability , which implies that the above product converges to 0 with positive probability . from standard results on convergence of an infinite product , we know that the product vanishes if and only if @xmath293 but this is a sum of independent mean - zero random variables and , since @xmath294 , we get that @xmath295 according to billinsgley ( 1995 ) , theorem  22.6 , this implies that the series @xmath296 converges with probability  1 , contradicting the assumption that @xmath284 with positive probability . therefore , with probability  1 , there exists @xmath149 such that @xmath297 , which implies that @xmath156 is tight and the limit @xmath137 is a proper distribution function .",
    "that @xmath298 can be seen by a simple change of variables , @xmath299 . to see that @xmath122 is the only fixed point of @xmath143 , note that @xmath300 is a conditional distribution function in @xmath149 , for fixed @xmath301 , corresponding to a joint distribution with both marginals equal to @xmath141 .",
    "so , the only way that @xmath302 can be recovered by integrating the conditional with respect to @xmath122 is if @xmath145 .",
    "so , @xmath122 is the only fixed point of @xmath143 and , therefore , must be the weak limit of the sequence @xmath123 , proving the claim .    that @xmath184 is non - negative is clear .",
    "moreover , we have @xmath303 it is easy to check , using the formula for @xmath179 and a change of variable , that @xmath304 . that @xmath187 is the unique ( almost everywhere ) solution follows from completeness of the normal mean family",
    "indeed , if we make a change of variable @xmath305 , then @xmath306 the condition that the integral above equals 1 for lebesgue - almost all @xmath3 implies , by completeness of the normal mean family @xmath307 , that the ratio in the integrand equals 1 for lebesgue - almost all @xmath308 , that is , @xmath309 the claim that @xmath187 lebesgue - almost everywhere follows immediately .",
    "recall that the remainder term @xmath172 is given by @xmath310 ^ 2 { p^\\star}(y ) \\,\\d y.\\ ] ] to get a handle on the conditional expectation @xmath311 , it suffices to bound @xmath312 using the formula for @xmath55 and cauchy  schwartz , we have @xmath313 write @xmath314 and note that , since @xmath315 and @xmath316 , we have @xmath317 where @xmath318 .",
    "it follows from inglot ( 2010 , theorem 2.1 ) that @xmath319 applying this to the previous expression , we have that @xmath320 and , consequently , @xmath321 ^ 2.\\ ] ] from theorem  [ thm : weak.limit ] we have that @xmath322 pointwise , almost surely .",
    "but since these are distribution functions , and @xmath122 is continuous , the mode of convergence is uniform .",
    "we have assumed in that the limit is integrable , so uniform convergence implies we can interchange the order of limit and integral .",
    "therefore , for large @xmath102 , @xmath323 is bounded almost surely by a constant times @xmath324 ^ 2,\\ ] ] since this bound is finite according to , the summation condition on @xmath107 guarantees that @xmath325 converges @xmath122-almost surely , proving the claim .",
    "bernardo , j.  m.  and smith , a.  f.  m.  ( 1994 ) .",
    "_ bayesian theory_. wiley .",
    "elgammal , a. , duraiswami , r. and davis , l.s .",
    "efficient kernel density estimation using the fast gauss transform with applications to color modeling and tracking . _ ieee transactions on pattern analysis and machine intelligence _ * 25 * , 14991504 .",
    "escobar , m.  d.  ( 1988 ) .",
    "_ estimating the means of several normal populations by nonparametric estimation of the distribution of the means_. unpublished phd dissertation , department of statistics , yale university .",
    "han , b. , comaniciu , d. , zhu , y. and davis , l.  s.  ( 2008 ) .",
    "sequential kernel density approximation and its application to real time visual tracking .",
    "_ ieee transactions on pattern analysis and machine intelligence _ * 7 * , 11861197 .",
    "robbins , h.  and siegmund , d.  ( 1971 ) . a convergence theorem for non negative almost supermartingales and some applications .",
    "in _ optimizing methods in statistics @xmath327proc .",
    "sympos . , ohio state univ . ,",
    "columbus@xmath328 _ , 233257 . academic press , new york .",
    "taddy , m.a .",
    "autoregressive mixture models for dynamic spatial poisson processes : application to tracking intensity of violent crime . _ journal of the american statistical association _ * 105 * , 14031417"
  ],
  "abstract_text": [
    "<S> a bayesian framework is attractive in the context of prediction , but a fast recursive update of the predictive distribution has apparently been out of reach , in part because monte carlo methods are generally used to compute the predictive . </S>",
    "<S> this paper shows that online bayesian prediction is possible by a characterizing the bayesian predictive update in terms of a bivariate copula , making it unnecessary to pass through the posterior to update the predictive . in standard models </S>",
    "<S> , the bayesian predictive update corresponds to familiar choices of copula but , in nonparametric problems , the appropriate copula may not have a closed - form expression . </S>",
    "<S> in such cases , our new perspective suggests a fast recursive approximation to the predictive density , in the spirit of newton s predictive recursion algorithm , but without requiring evaluation of normalizing constants . </S>",
    "<S> consistency of the new algorithm is shown , and numerical examples demonstrate its quality performance in finite - samples compared to fully bayesian and kernel methods .    _ keywords and phrases : _ copula ; density estimation ; nonparametric bayes ; prediction ; recursive estimation . </S>"
  ]
}