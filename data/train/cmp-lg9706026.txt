{
  "article_text": [
    "over the past decade , researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation @xcite .",
    "however , the ibm models , which attempt to capture a broad range of translation phenomena , are computationally expensive to apply .",
    "table look - up using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications , including `` crummy '' mt on the world wide web @xcite , certain machine - assisted translation tools ( e.g. @xcite ) , concordancing for bilingual lexicography @xcite , computer - assisted language learning , corpus linguistics @xcite , and cross - lingual information retrieval @xcite .    in this paper",
    ", we present a fast method for inducing accurate translation lexicons .",
    "the method assumes that words are translated one - to - one .",
    "this assumption reduces the explanatory power of our model in comparison to the ibm models , but , as shown in section  [ linkalg ] , it helps us to avoid what we call indirect associations , a major source of errors in other models .",
    "section  [ linkalg ] also shows how the one - to - one assumption enables us to use a new greedy competitive linking algorithm for re - estimating the model s parameters , instead of more expensive algorithms that consider a much larger set of word correspondence possibilities .",
    "the model uses two hidden parameters to estimate the confidence of its own predictions .",
    "the confidence estimates enable direct control of the balance between the model s precision and recall via a simple threshold .",
    "the hidden parameters can be conditioned on prior knowledge about the bitext to improve the model s accuracy .",
    "with the exception of @xcite , previous methods for automatically constructing statistical translation models begin by looking at word co - occurrence frequencies in bitexts @xcite .",
    "a * bitext * comprises a pair of texts in two languages , where each text is a translation of the other .",
    "word co - occurrence can be defined in various ways .",
    "the most common way is to divide each half of the bitext into an equal number of segments and to align the segments so that each pair of segments @xmath0 and @xmath1 are translations of each other @xcite .",
    "then , two word tokens @xmath2 are said to * co - occur * in the aligned segment pair @xmath3 if @xmath4 and @xmath5 . the co - occurrence relation can also be based on distance in a bitext space , which is a more general representations of bitext correspondence @xcite , or it can be restricted to words pairs that satisfy some matching predicate , which can be extrinsic to the model @xcite .",
    "our translation model consists of the hidden parameters @xmath6 and @xmath7 , and likelihood ratios @xmath8 .",
    "the two hidden parameters are the probabilities of the model generating true and false positives in the data .",
    "@xmath8 represents the likelihood that @xmath9 and @xmath10 can be mutual translations . for each co - occurring pair of word types @xmath9 and @xmath10 , these likelihoods are initially set proportional to their co - occurrence frequency @xmath11 and inversely proportional to their marginal frequencies @xmath12 and @xmath13 , , which is _ not _ the same as the frequency of * u * , because each token of * u * can co - occur with several different*v * s . ] following @xcite @xcite or the dice coefficient @xcite . ] .",
    "when the @xmath8 are re - estimated , the model s hidden parameters come into play .",
    "after initialization , the model induction algorithm iterates :    1 .",
    "find a set of `` links '' among word tokens in the bitext , using the likelihood ratios and the competitive linking algorithm .",
    "2 .   use the links to re - estimate @xmath6 , @xmath7 , and the likelihood ratios .",
    "3 .   repeat from step 1 until the model converges to the desired degree .",
    "the competitive linking algorithm and its one - to - one assumption are detailed in section  [ linkalg ] .",
    "section  [ paramest ] explains how to re - estimate the model parameters .",
    "the competitive linking algorithm is designed to overcome the problem of indirect associations , illustrated in figure  [ dep ] .",
    "the sequences of @xmath14 s and @xmath15 s represent corresponding regions of a bitext .",
    "if @xmath16 and @xmath17 co - occur much more often than expected by chance , then any reasonable model will deem them likely to be mutual translations .",
    "if @xmath16 and @xmath17 are indeed mutual translations , then their tendency to co - occur is called a * direct association*. now , suppose that @xmath16 and @xmath18 often co - occur within their language .",
    "then @xmath17 and @xmath19 will also co - occur more often than expected by chance .",
    "the arrow connecting @xmath17 and @xmath19 in figure  [ dep ] represents an * indirect association * , since the association between @xmath17 and @xmath19 arises only by virtue of the association between each of them and @xmath16 .",
    "models of translational equivalence that are ignorant of indirect associations have `` a tendency ... to be confused by collocates '' @xcite .    [ paramest ]     between word types @xmath9 and @xmath10 + @xmath20 & = & @xmath21 total number of co - occurrences in the bitext + @xmath22 & = & frequency of links between word types @xmath9 and @xmath10 + @xmath23 & = & @xmath24 total number of links in the bitext + @xmath25 & = & @xmath26 ( mutual translations @xmath27 co - occurrence ) + @xmath28 & = & @xmath26 ( link @xmath27 co - occurrence ) + @xmath6 & = & @xmath26 ( link @xmath27 co - occurrence of mutual translations ) + @xmath7 & = & @xmath26 ( link @xmath27 co - occurrence of not mutual translations ) + @xmath29 & = & @xmath26 ( @xmath30 ) , where @xmath31 has a binomial distribution with parameters @xmath32 and @xmath33 +   +    fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based @xcite .",
    "the majority of indirect associations can be filtered out by a simple competition heuristic : whenever several word tokens @xmath34 in one half of the bitext co - occur with a particular word token @xmath15 in the other half of the bitext , the word that is most likely to be @xmath15 s translation is the one for which the likelihood @xmath8 of translational equivalence is highest .",
    "the competitive linking algorithm implements this heuristic :    1 .",
    "discard all likelihood scores for word types deemed unlikely to be mutual translations , i.e. all @xmath35 .",
    "this step significantly reduces the computational burden of the algorithm .",
    "it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values @xcite . to retain word type pairs that are at least twice as likely to be mutual translations than",
    "not , the threshold can be raised to  2 .",
    "conversely , the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly .",
    "sort all remaining likelihood estimates @xmath36 from highest to lowest .",
    "3 .   find @xmath9 and @xmath10 such that the likelihood ratio @xmath37 is highest .",
    "token pairs of these types would be the winners in any competitions involving @xmath9 or @xmath10 .",
    "4 .   link all token pairs @xmath38 in the bitext .",
    "the one - to - one assumption means that linked words can not be linked again .",
    "therefore , remove all linked word tokens from their respective texts .",
    "6 .   if there is another co - occurring word token pair @xmath39 such that @xmath37 exists , then repeat from step  3 .",
    "the competitive linking algorithm is more greedy than algorithms that try to find a _ set _ of link types that are jointly most probable over some segment of the bitext . in practice ,",
    "our linking algorithm can be implemented so that its worst - case running time is @xmath40 , where @xmath41 and @xmath42 are the lengths of the aligned segments .",
    "the simplicity of the competitive linking algorithm depends on the * one - to - one assumption * : each word translates to at most one other word .",
    "certainly , there are cases where this assumption is false .",
    "we prefer not to model those cases , in order to achieve higher accuracy with less effort on the cases where the assumption is true .",
    "the purpose of the competitive linking algorithm is to help us re - estimate the model parameters .",
    "the variables that we use in our estimation are summarized in figure  [ vars ] .",
    "the linking algorithm produces a set of links between word tokens in the bitext .",
    "we define a * link token * to be an ordered pair of word tokens , one from each half of the bitext . a * link type * is an ordered pair of word types .",
    "let @xmath43 be the co - occurrence frequency of @xmath44 and @xmath10 and @xmath45 be the number of links between tokens of @xmath9 and @xmath10 depends on the linking algorithm , but @xmath43 is a constant property of the bitext . ] .",
    "an important property of the competitive linking algorithm is that the ratio @xmath46 tends to be very high if @xmath9 and @xmath10 are mutual translations , and quite low if they are not .",
    "the bimodality of this ratio for several values of @xmath43 is illustrated in figure  [ bimodal ] .",
    "this figure was plotted after the model s first iteration over    300000 aligned sentence pairs from the canadian hansard bitext .",
    "note that the frequencies are plotted on a log scale  the bimodality is quite sharp .",
    "the linking algorithm creates all the links of a given type independently of each other , so the number @xmath47 of links connecting word types @xmath9 and @xmath10 has a binomial distribution with parameters @xmath43 and @xmath48 .",
    "if @xmath9 and @xmath10 are mutual translations , then @xmath49 tends to a relatively high probability , which we will call @xmath6 .",
    "if @xmath9 and @xmath10 are not mutual translations , then @xmath48 tends to a very low probability , which we will call @xmath7 .",
    "@xmath6 and @xmath7 correspond to the two peaks in the frequency distribution of @xmath46 in figure  2 .",
    "the two parameters can also be interpreted as the percentage of true and false positives .",
    "if the translation in the bitext is consistent and the model is accurate , then @xmath6 should be near 1 and @xmath7 should be near 0 .    to find the most probable values of the hidden model parameters @xmath6 and @xmath7 , we adopt the standard method of maximum likelihood estimation , and find the values that maximize the probability of the link frequency distributions .",
    "the one - to - one assumption implies independence between different link types , so that @xmath50 the factors on the right - hand side of equation  [ prdata ] can be written explicitly with the help of a mixture coefficient .",
    "let @xmath25 be the probability that an arbitrary co - occurring pair of word types are mutual translations .",
    "let @xmath51 denote the probability that @xmath31 links are observed out of @xmath32 co - occurrences , where @xmath31 has a binomial distribution with parameters @xmath32 and @xmath33 .",
    "then the probability that @xmath9 and @xmath10 are linked @xmath52 times out of @xmath11 co - occurrences is a mixture of two binomials : @xmath53    one more variable allows us to express @xmath25 in terms of @xmath6 and @xmath7 : let @xmath28 be the probability that an arbitrary co - occuring pair of word tokens will be linked , regardless of whether they are mutual translations . since @xmath25 is constant over all word types , it also represents the probability that an arbitrary co - occurring pair of word _",
    "tokens _ are mutual translations .",
    "therefore , @xmath54 @xmath28 can also be estimated empirically .",
    "let @xmath23 be the total number of links in the bitext and let @xmath20 be the total number of co - occuring word token pairs : @xmath55 , @xmath56 . by definition , @xmath57 equating the right - hand sides of equations ( [ l1 ] ) and ( [ l2 ] ) and rearranging the terms , we get : @xmath58 since @xmath25 is now a function of @xmath6 and @xmath7 , only the latter two variables represent degrees of freedom in the model .    the probability function expressed by equations  [ prdata ] and  [ twobin ] has many local maxima . in practice , these local maxima are like pebbles on a mountain , invisible at low resolution .",
    "we computed equation  [ prdata ] over various combinations of @xmath6 and @xmath7 after the model s first iteration over 300000 aligned sentence pairs from the canadian hansard bitext .",
    "figure  4 shows that the region of interest in the parameter space , where @xmath59 , has only one clearly visible global maximum .",
    "this global maximum can be found by standard hill - climbing methods , as long as the step size is large enough to avoid getting stuck on the pebbles .",
    "given estimates for @xmath6 and @xmath7 , we can compute @xmath60 and @xmath61 .",
    "these are probabilities that @xmath52 links were generated by an algorithm that generates correct links and by an algorithm that generates incorrect links , respectively , out of @xmath11 co - occurrences .",
    "the ratio of these probabilities is the likelihood ratio in favor of @xmath9 and @xmath62 being mutual translations , for all @xmath9 and @xmath10 : @xmath63",
    "in the basic word - to - word model , the hidden parameters @xmath6 and @xmath7 depend only on the distributions of link frequencies generated by the competitive linking algorithm .",
    "more accurate models can be induced by taking into account various features of the linked tokens .",
    "for example , frequent words are translated less consistently than rare words @xcite . to account for this difference",
    ", we can estimate separate values of @xmath6 and @xmath7 for different ranges of @xmath64 .",
    "similarly , the hidden parameters can be conditioned on the linked parts of speech .",
    "word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences . just as easily , we can model links that coincide with entries in a pre - existing translation lexicon separately from those that do not .",
    "this method of incorporating dictionary information seems simpler than the method proposed by brown et al . for their models",
    "when the hidden parameters are conditioned on different link classes , the estimation method does not change ; it is just repeated for each link class .",
    "a word - to - word model of translational equivalence can be evaluated either over types or over tokens .",
    "it is impossible to replicate the experiments used to evaluate other translation models in the literature , because neither the models nor the programs that induce them are generally available . for each kind of evaluation",
    ", we have found one case where we can come close .",
    "we induced a two - class word - to - word model of translational equivalence from 13 million words of the canadian hansards , aligned using the method in @xcite .",
    "one class represented content - word links and the other represented function - word links .",
    "link types with negative log - likelihood were discarded after each iteration .",
    "both classes parameters converged after six iterations .",
    "the value of class - based models was demonstrated by the differences between the hidden parameters for the two classes .",
    "@xmath65 converged at ( .78,.00016 ) for content - class links and at ( .43,.000094 ) for function - class links .",
    "the most direct way to evaluate the link types in a word - level model of translational equivalence is to treat each link type as a candidate translation lexicon entry , and to measure precision and recall .",
    "this evaluation criterion carries much practical import , because many of the applications mentioned in section  1 depend on accurate broad - coverage translation lexicons . machine readable bilingual dictionaries , even when they are available , have only limited coverage and rarely include domain - specific terms @xcite .",
    "we define the recall of a word - to - word translation model as the fraction of the bitext vocabulary represented in the model .",
    "translation model precision is a more thorny issue , because people disagree about the degree to which context should play a role in judgements of translational equivalence .",
    "we hand - evaluated the precision of the link types in our model in the context of the bitext from which the model was induced , using a simple bilingual concordancer .",
    "a link type @xmath66 was considered correct if @xmath14 and @xmath15 ever co - occurred as direct translations of each other . where the one - to - one assumption failed , but a link type captured part of a correct translation , it was judged `` incomplete . '' whether incomplete links are correct or incorrect depends on the application .",
    "we evaluated five random samples of 100 link types each at three levels of recall . for our bitext ,",
    "recall of 36% , 46% and 90% corresponded to translation lexicons containing 32274 , 43075 and 88633 words , respectively .",
    "figure  [ results ] shows the precision of the model with 95% confidence intervals .",
    "the upper curve represents precision when incomplete links are considered correct , and the lower when they are considered incorrect . on the former metric ,",
    "our model can generate translation lexicons with precision and recall both exceeding 90% , as well as dictionary - sized translation lexicons that are over 99% correct .",
    "though some have tried , it is not clear how to extract such accurate lexicons from other published translation models .",
    "part of the difficulty stems from the implicit assumption in other models that each word has only one sense .",
    "each word is assigned the same unit of probability mass , which the model distributes over all candidate translations .",
    "the correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation . this imbalance foils thresholding strategies , clever as they might be @xcite .",
    "the likelihoods in the word - to - word model remain unnormalized , so they do not compete .",
    "the word - to - word model maintains high precision even given much less training data .",
    "resnik & melamed ( 1997 ) report that the model produced translation lexicons with 94% precision and 30% recall , when trained on french / english software manuals totaling about 400,000 words .",
    "the model was also used to induce a translation lexicon from a 6200-word corpus of french / english weather reports .",
    "nasr ( 1997 ) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90% .",
    "recall drops when there is less training data , because the model refuses to make predictions that it can not make with confidence .",
    "for many applications , this is the desired behavior .",
    "._erroneous link tokens generated by two translation models . _ [ errors ] [ cols=\"^,^,^,^ \" , ]     the most detailed evaluation of link tokens to date was performed by @xcite , who trained brown et al.s model 2 on 74 million words of the canadian hansards .",
    "these authors kindly provided us with the links generated by that model in 51 aligned sentences from a held - out test set .",
    "we generated links in the same 51 sentences using our two - class word - to - word model , and manually evaluated the content - word links from both models .",
    "the ibm models are directional ; i.e. they posit the english words that gave rise to each french word , but ignore the distribution of the english words .",
    "therefore , we ignored english words that were linked to nothing .",
    "the errors are classified in table  [ errors ] .",
    "the `` wrong link '' and `` missing link '' error categories should be self - explanatory .",
    "`` partial links '' are those where one french word resulted from multiple english words , but the model only links the french word to one of its english sources . `` class conflict '' errors resulted from our model s refusal to link content words with function words .",
    "usually , this is the desired behavior , but words like english auxiliary verbs are sometimes used as content words , giving rise to content words in french .",
    "such errors could be overcome by a model that classifies each word token , for example using a part - of - speech tagger , instead of assigning the same class to all tokens of a given type .",
    "the bitext pre - processor for our word - to - word model split hyphenated words , but macklovitch & hannan s preprocessor did not .",
    "in some cases , hyphenated words were easier to link correctly ; in other cases they were more difficult . both models made some errors because of this tokenization problem , albeit in different places .",
    "the `` paraphrase '' category covers all link errors that resulted from paraphrases in the translation .",
    "neither ibm s model 2 nor our model is capable of linking multi - word sequences to multi - word sequences , and this was the biggest source of error for both models .",
    "the test sample contained only about 400 content words , and the links for both models were evaluated post - hoc by only one evaluator .",
    "nevertheless , it appears that our word - to - word model with only two link classes does not perform any worse than ibm s model 2 , even though the word - to - word model was trained on less than one fifth the amount of data that was used to train the ibm model .",
    "since it does nt store indirect associations , our word - to - word model contained an average of 4.5 french words for every english word .",
    "such a compact model requires relatively little computational effort to induce and to apply .",
    "in addition to the quantitative differences between the word - to - word model and the ibm model , there is an important qualitative difference , illustrated in figure  [ links ] . as shown in table  [ errors ] , the most common kind of error for the word - to - word model was a missing link , whereas the most common error for ibm s model 2 was a wrong link .",
    "missing links are more informative : they indicate where the model has failed . the level at which the model trusts its own judgement can be varied directly by changing the likelihood cutoff in step 1 of the competitive linking algorithm .",
    "each application of the word - to - word model can choose its own balance between link token precision and recall .",
    "an application that calls on the word - to - word model to link words in a bitext could treat unlinked words differently from linked words , and avoid basing subsequent decisions on uncertain inputs .",
    "it is not clear how the precision / recall trade - off can be controlled in the ibm models .",
    "one advantage that brown et al.s model 1 has over our word - to - word model is that their objective function has no local maxima . by using the em algorithm @xcite , they can guarantee convergence towards the globally optimum parameter set .",
    "in contrast , the dynamic nature of the competitive linking algorithm changes the @xmath67 in a non - monotonic fashion .",
    "we have adopted the simple heuristic that the model `` has converged '' when this probability stops increasing .",
    "many multilingual nlp applications need to translate words between different languages , but can not afford the computational expense of modeling the full range of translation phenomena . for these applications ,",
    "we have designed a fast algorithm for estimating word - to - word models of translational equivalence .",
    "the estimation method uses a pair of hidden parameters to measure the model s uncertainty , and avoids making decisions that it s not likely to make correctly .",
    "the hidden parameters can be conditioned on information extrinsic to the model , providing an easy way to integrate pre - existing knowledge .",
    "so far we have only implemented a two - class model , to exploit the differences in translation consistency between content words and function words .",
    "this relatively simple two - class model linked word tokens in parallel texts as accurately as other translation models in the literature , despite being trained on only one fifth as much data . unlike other translation models",
    ", the word - to - word model can automatically produce dictionary - sized translation lexicons , and it can do so with over 99% accuracy .    even better accuracy can be achieved with a more fine - grained link class structure .",
    "promising features for classification include part of speech , frequency of co - occurrence , relative word position , and translational entropy @xcite .",
    "another interesting extension is to broaden the definition of a `` word '' to include multi - word lexical units @xcite .",
    "if such units can be identified _ a priori _ , their translations can be estimated without modifying the word - to - word model . in this manner ,",
    "the model can account for a wider range of translation phenomena .",
    "the french / english software manuals were provided by gary adams of sun microsystems laboratories .",
    "the weather bitext was prepared at the university of montreal , under the direction of richard kittredge .",
    "thanks to alexis nasr for hand - evaluating the weather translation lexicon . thanks",
    "also to mike collins , george foster , mitch marcus , lyle ungar , and three anonymous reviewers for helpful comments .",
    "this research was supported by an equipment grant from sun microsystems and by arpa contract # n66001 - 94c-6043 .",
    "p. f. brown , j. cocke , s. della pietra , v. della pietra , f. jelinek , r. mercer , & p. roossin , `` a statistical approach to language translation , '' _ proceedings of the 12th international conference on computational linguistics _ ,",
    "budapest , hungary , 1988 .",
    "p. f. brown , j. cocke , s. della pietra , v. della pietra , f. jelinek , r. mercer , & p. roossin , `` a statistical approach to machine translation , '' _ computational linguistics 16_(2 ) , 1990 .",
    "p. f. brown , v. j. della pietra , s. a. della pietra & r. l. mercer , `` the mathematics of statistical machine translation : parameter estimation , '' _ computational linguistics 19_(2 ) , 1993 .",
    "f. brown , s. a. della pietra , v. j. della pietra , m. j. goldsmith , j. hajic , r. l. mercer & s. mohanty , `` but dictionaries are data too , '' _ proceedings of the arpa hlt workshop _ , princeton , nj , 1993 .",
    "r. catizone , g. russell & s. warwick `` deriving translation data from bilingual texts , '' _ proceedings of the first international lexical acquisition workshop _ , detroit , mi , 1993 . s. chen , _ building probabilistic models for natural language _ , ph.d . thesis , harvard university , 1996 .",
    "k. w. church & e. h. hovy , `` good applications for crummy machine translation , '' _ machine translation 8 _ , 1993 .",
    "i. dagan , k. church , & w. gale , `` robust word alignment for machine aided translation , '' _ proceedings of the workshop on very large corpora : academic and industrial perspectives _ ,",
    "columbus , oh , 1993 .",
    "a. p. dempster , n. m. laird & d. b. rubin , `` maximum likelihood from incomplete data via the em algorithm , '' _ journal of the royal statistical society 34(b ) _ , 1977 .",
    "t. dunning , `` accurate methods for the statistics of surprise and coincidence , '' _ computational linguistics 19_(1 ) , 1993 .",
    "p. fung , `` compiling bilingual lexicon entries from a non - parallel english - chinese corpus , '' _ proceedings of the third workshop on very large corpora _ , boston , ma , 1995a .",
    "p. fung , `` a pattern matching method for finding noun and proper noun translations from noisy parallel corpora , '' _ proceedings of the 33rd annual meeting of the association for computational linguistics _ ,",
    "boston , ma , 1995b .",
    "w. gale & k. w. church , `` a program for aligning sentences in bilingual corpora '' _ proceedings of the 29th annual meeting of the association for computational linguistics _ , berkeley , ca , 1991",
    ". w. gale & k. w. church , `` identifying word correspondences in parallel texts , '' _ proceedings of the darpa snl workshop _ , 1991 .",
    "a. kumano & h. hirakawa , `` building an mt dictionary from parallel texts based on linguistic and statistical information , '' _ proceedings of the 15th international conference on computational linguistics , _ kyoto , japan , 1994 .",
    "e. macklovitch `` using bi - textual alignment for translation validation : the transcheck system , '' _ proceedings of the 1st conference of the association for machine translation in the americas _ , columbia , md , 1994 .",
    "e. macklovitch & m .-",
    "hannan , `` line em up : advances in alignment technology and their impact on translation support tools , '' _",
    "2nd conference of the association for machine translation in the americas _ ,",
    "montreal , canada , 1996 .",
    "i. d. melamed `` automatic evaluation and uniform filter cascades for inducing @xmath20-best translation lexicons , '' _ proceedings of the third workshop on very large corpora _ , boston , ma , 1995 .",
    "i. d. melamed , `` a geometric approach to mapping bitext correspondence , '' _ proceedings of the first conference on empirical methods in natural language processing _",
    ", philadelphia , pa , 1996a . i. d. melamed `` automatic detection of omissions in translations , '' _ proceedings of the 16th international conference on computational linguistics , _ copenhagen , denmark , 1996b .",
    "i. d melamed , `` automatic construction of clean broad - coverage translation lexicons , '' _",
    "2nd conference of the association for machine translation in the americas _ ,",
    "montreal , canada , 1996c .",
    "i. d. melamed , `` measuring semantic entropy , '' _ proceedings of the siglex workshop on tagging text with lexical semantics _",
    ", washington , dc , 1997a .",
    "i. d. melamed , `` a portable algorithm for mapping bitext correspondence , '' _ proceedings of the 35th conference of the association for computational linguistics _",
    ", madrid , spain , 1997b .",
    "( in this volume ) a. melby , `` a bilingual concordance system and its use in linguistic studies , '' _ proceedings of the english lacus forum _ ,",
    "columbia , sc , 1981 .",
    "a. nasr , personal communication , 1997 .",
    "p. resnik & i. d. melamed , `` semi - automatic acquisition of domain - specific translation lexicons , '' _ proceedings of the 7th acl conference on applied natural language processing _",
    ", washington , dc , 1997 .",
    "d. w. oard & b. j. dorr ,  a survey of multilingual text retrieval , _",
    "umiacs tr-96 - 19 _ , university of maryland , college park , md , 1996 .",
    "f. smadja , `` how to compile a bilingual collocational lexicon automatically , '' _ proceedings of the aaai workshop on statistically - based nlp techniques _ ,",
    "d. wu & x. xia , `` learning an english - chinese lexicon from a parallel corpus , '' _ proceedings of the first conference of the association for machine translation in the americas _ ,",
    "columbia , md , 1994 ."
  ],
  "abstract_text": [
    "<S> many multilingual nlp applications need to translate words between different languages , but can not afford the computational expense of inducing or applying a full translation model . for these applications , </S>",
    "<S> we have designed a fast algorithm for estimating a partial translation model , which accounts for translational equivalence only at the word level . </S>",
    "<S> the model s precision / recall trade - off can be directly controlled via one threshold parameter . </S>",
    "<S> this feature makes the model more suitable for applications that are not fully statistical . the model s </S>",
    "<S> hidden parameters can be easily conditioned on information extrinsic to the model , providing an easy way to integrate pre - existing knowledge such as part - of - speech , dictionaries , word order , etc .. our model can link word tokens in parallel texts as well as other translation models in the literature . unlike other translation models </S>",
    "<S> , it can automatically produce dictionary - sized translation lexicons , and it can do so with over 99% accuracy .    </S>",
    "<S> = 10000 = 10000 </S>"
  ]
}