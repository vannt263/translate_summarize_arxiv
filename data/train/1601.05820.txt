{
  "article_text": [
    "robins et al , 2008 , published a theory of higher order influence functions for inference in semi- and non - parametric models .",
    "this paper is a comprehensive manuscript from which robins et al , was drawn .",
    "the current paper includes many results and proofs that were not included in robins et al due to space limitation .",
    "particular results contained in the present paper that were not reported in robins et al include the following . given a set of functionals and their corresponding higher order influence functions ,",
    "we show how to derive the higher order influence function of their product .",
    "we apply this result to obtain higher order influence functions and associated estimators for the mean of a response @xmath0 subject to monotone missingness under missing at random .",
    "these results also apply to estimating the causal effect of a time dependent treatment on an outcome @xmath0 in the presence of time - varying confounding .",
    "finally , we include an appendix that contains proofs for all theorems that were stated without proof in robins et al , 2008 . the initial part of the paper is closely related to robins et al , , the latter parts differ .",
    "we have developed a theory of point and interval estimation for nonlinear functionals @xmath2 in parametric , semi- , and non - parametric models based on higher order likelihood scores and influence functions that applies equally to both @xmath1 and non-@xmath1 problems ( robins 2004 , sec .",
    "9 ; li et al , 2006 , tchetgen et al , 2006 , robins et al , 2007 ) .",
    "the theory reproduces results previously obtained by the modern theory of non - parametric inference , produces many new non-@xmath1 results , and most importantly opens up the ability to perform non-@xmath1 inference in complex high dimensional models , such as models for the estimation of the causal effect of time varying treatments in the presence of time varying confounding and informative censoring .",
    "see tchetgen et al .",
    "( 2007 ) for examples of the latter .",
    "higher order influence functions are higher order u - statistics .",
    "our theory extends the first order semiparametric theory of bickel et al .",
    "( 1993 ) and van der vaart ( 1991 ) by incorporating the theory of higher order scores and bhattacharrya bases considered by pfanzagl ( 1990 ) , small and mcleish ( 1994 ) and lindsay and waterman ( 1996 ) .",
    "the purpose of this paper is to demonstrate the scope and flexibility of our methodology by deriving rate - optimal point and interval estimators for various functionals that are of central importance to biostatistics .",
    "we now describe some of these functionals .",
    "we suppose we observe i.i.d copies of a random vector @xmath3 with unknown distribution @xmath4 on each of @xmath5 study subjects . in this paper",
    ", we largely study non - parametric models that place no restrictions on @xmath6 other than bounds on both the @xmath7 norms and on the smoothness of certain density and conditional expectation functions .",
    "the variable @xmath8 represents a random vector of baseline covariates such as age , height , weight , hematocrit , and laboratory measures of lung , renal , liver , brain , and heart function .",
    "@xmath8 is assumed to have compact support and a density @xmath9 with respect to the lebesgue measure in @xmath10where , in typical applications , @xmath11 is in the range 5 to 100 .",
    "@xmath12 is a binary treatment and @xmath0 is a response , higher values of which are desirable .",
    "then , in the absence of confounding by additional unmeasured factors , the functional @xmath13   \\right\\ }   -e\\left\\ {   e\\left [   y|a=0,x\\right ]   \\right\\ }   $ ] is the mean effect of treatment in the total study population .",
    "our results for @xmath14   \\right\\ }   -e\\left\\ {   e\\left [ y|a=0,x\\right ]   \\right\\ }   $ ] follow from results for the functional @xmath15   \\right\\ }   $ ] based on data @xmath16 rather than @xmath17 if @xmath0 is missing for some study subjects , and @xmath12 is now the indicator that takes the value @xmath18 when @xmath0 is observed and zero otherwise , then the functional @xmath14   \\right\\ }   $ ] is the marginal mean of @xmath0 under the missing at random assumption that the probability @xmath19   = p\\left [   a=0|x\\right ]   \\ $ ] that @xmath0 is missing does not depend on the unobserved @xmath20    returning to data @xmath21 the functional @xmath22 \\\\ &   = e\\left [   w\\left (   x\\right )   \\left\\ {   e\\left [   y|a=1,x\\right ]   -e\\left [ y|a=0,x\\right ]   \\right\\ }   \\right]\\end{aligned}\\ ] ] with @xmath23   \\ $ ] is the variance weighted average treatment effect . our results for @xmath24   $ ] are derived from results for the functionals @xmath25 and @xmath26   .$ ]    we note that robins and van der vaart s ( 2006 ) construction of an adaptive confidence set for a regression function @xmath27 depended on being able to construct a confidence interval for @xmath28   .$ ] they constructed an interval for @xmath29   $ ] when the marginal distribution of @xmath8 was known . in this paper , we construct a confidence interval for @xmath30   $ ] when the marginal of @xmath8 is unknown and , in section [ adaptive_section ] , use it to obtain an adaptive confidence set for @xmath27 .    the functional @xmath31 is the functional @xmath32 in the special case in which @xmath33 wp1 .",
    "minimax estimation of @xmath34 has recently been discussed by wang et al .",
    "( 2006 ) and cai et al .",
    "( 2006 ) in the setting of non - random @xmath8 .",
    "the function @xmath35   -e\\left [ y|a=0,x = x\\right ]   $ ] is the effect of treatment on the subgroup with @xmath36 it is important to estimate the function @xmath37 , in addition to the average treatment effect in the total population , because treatment should be given , since beneficial , to those subjects with @xmath38 but withheld , since harmful , from subjects with @xmath39 we show that one can obtain adaptive confidence sets for @xmath37 if one can set confidence intervals for the functional @xmath40   $ ] .",
    "we construct intervals for @xmath41   $ ] under the additional assumption that the data @xmath42 came from a randomized trial . in a randomized trial , in contrast to an observational study ,",
    "the randomization probabilities , @xmath43 are known by design .",
    "we plan to report confidence intervals for @xmath44   $ ] with @xmath45 unknown elsewhere .",
    "all of the above functionals @xmath2 have a positive semiparametric information bound ( sib ) and thus a ( first order ) efficient influence function with a finite variance .",
    "in fact all the functionals @xmath2 have efficient influence function @xmath46 where @xmath47 are monotone functions of certain conditional expectations , and , for any @xmath48@xmath49   = e_{f}\\left [   h_{1}\\left (   o\\right )   \\left\\ {   b^{\\ast}\\left ( x\\right )   -b\\left (   x;f\\right )   \\right\\ }   \\left\\ {   p^{\\ast}\\left (   x\\right ) -p\\left (   x;f\\right )   \\right\\ }   \\right]\\ ] ] where @xmath50 is a known function .",
    "we refer to functionals in our class as doubly - robust to indicate that @xmath51 continues to have mean zero when either ( but not both ) @xmath52 is misspecified as @xmath53 or @xmath54 is misspecified as @xmath55  the functions @xmath56 and @xmath50 differ depending on the functional @xmath57 of interest",
    ".    as the functionals @xmath2 are all closely related , we shall use @xmath31 as a prototype in this introduction . for @xmath58 @xmath59 @xmath60@xmath61 and @xmath62 .",
    "whenever a functional @xmath2 has a non - zero sib , given sufficiently stringent bounds on @xmath7 norms and on smoothness , it is possible to use the estimated first order influence function to construct regular estimators and honest asymptotic confidence intervals whose width shrinks at the usual parametric rate of @xmath63 @xmath64 we recall that , by definition , regular estimators are @xmath65-consistent .",
    "when @xmath8 is high dimensional , the apriori smoothness restrictions on @xmath66 and @xmath67 necessary for point or interval estimators of @xmath31 to achieve the parametric rate of @xmath63 are so severe as to be substantively implausible . as a consequence ,",
    "we replace the usual approach based on first order influence functions by one based on higher order influence functions .    to provide quantitative results , we require a measure of the maximal possible complexity ( e.g. smoothness ) of @xmath68 and @xmath69 believed substantively plausible.@xmath70    @xmath70we use hlder balls for concreteness , although our methods extend to other measures of complexity .",
    "a function @xmath71 lies in the hlder ball @xmath72 with hlder exponent @xmath73 and radius @xmath74 if and only if @xmath71 is bounded in supremum norm by @xmath75 and all partial derivatives of @xmath76 up to order @xmath77 exist , and all partial derivatives of order @xmath77 are lipschitz with exponent @xmath78and constant @xmath75 .",
    "we make the assumption that @xmath79lie in given hlder balls @xmath80 @xmath81 furthermore , it turns out we must also make assumptions about the complexity of the function @xmath82 f_{x}\\left (   x\\right )   , $ ] which we take to lie in a given @xmath83 for @xmath84    using higher order influence functions , we construct regular estimators and honest ( i.e uniform over our model ) asymptotic confidence intervals for functionals @xmath2 in our class whose width shrinks at the usual parametric rate of @xmath63 whenever @xmath85 and @xmath86 this result can not be improved on , since even when @xmath87 is known apriori , @xmath88 is necessary for a regular estimator to exist .    when @xmath89 and @xmath87 is known apriori@xmath90 we have shown using arguments similar to those of birge and massart ( 1995 ) that the minimax rate of convergence for an estimator and minimax rate of shrinkage of a confidence interval is @xmath91 when @xmath87 is unknown , we construct point and interval estimators with this same rate of @xmath92 whenever @xmath93 where @xmath94 for example if @xmath95 we require @xmath96exceed @xmath97 to achieve the rate @xmath98 when the previous inequality does not hold and @xmath99 we have constructed , in a yet unpublished paper , estimators that converge at rate @xmath100   ^{1/2}-\\left (   1 + 2\\beta / d\\right )   \\right )   \\rceil .\\nonumber\\end{aligned}\\ ] ] we conjecture that this rate is minimax , possibly up to log factors . in this paper , however , the estimators we construct are inefficient when the previous inequality fails to hold , converging at rates less than the conjectured minimax rate of eq @xmath101 .",
    "let us return to the case where @xmath33 @xmath102@xmath103@xmath103@xmath103wp1 .",
    "then @xmath104 and @xmath105 @xmath106",
    "so @xmath107 now , for fixed @xmath108 eq @xmath101 converges to @xmath109 as @xmath110 which agrees ( up to a log factor ) with the minimax rate of @xmath111 given by wang et al .",
    "( 2006 ) and cai et al .",
    "( 2006 ) under the semiparametric homoscedastic model @xmath112 with equal - spaced non - random @xmath113 this result might suggest that @xmath114being random rather than equal - spaced can result in faster rates of convergence only when the density of @xmath8 has some smoothness , as quantified here by @xmath86 but this suggestion is not correct . recall that we obtained the rate @xmath115 for @xmath104 as @xmath116 under a non - parametric model . in section [ minimax_section ]",
    ", we construct a simple estimator of @xmath117 under the homoscedastic model with @xmath8 random with unknown density that , for @xmath118 , @xmath119 and without smoothness restrictions on @xmath9 , converges at the rate @xmath120 which is faster than the equal - spaced non - random minimax rate of @xmath121    the paper is organized as follows . in section 2 , we define the higher order ( estimation ) influence functions of a functional @xmath2 for @xmath4 contained in a model @xmath122 and prove two fundamental theorems - the extended information equality theorem and the efficient estimation influence function theorem .",
    "further , in the context of a parametric model whose dimension increases with sample size , we outline why estimators based on higher order influence can outperform those based on first order influence functions in high - dimensional models . in section 3",
    ", we introduce the class of functionals we study in the remainder of the paper and describe their importance in biostatistics .",
    "the theory of section 2 , however , is not directly applicable to these functionals because they have first order but not higher order influence functions .",
    "we show that higher order influence functions fail to exist precisely because the dirac delta function is not an element of the hilbert space @xmath123 of square integrable functions .",
    "we describe two approaches to overcoming this difficulty .",
    "the first approach is based on approximating the dirac delta function by a projection operator onto a subspace of @xmath123 of dimension @xmath124 where @xmath125 can be as large as the square of the sample size @xmath126 the second approach is based on approximating the functional @xmath2 by a truncated functional @xmath127 .",
    "the truncated functional has influence functions of all orders , is equal to @xmath2 if either a @xmath128 dimensional working parametric model ( with @xmath129 for the function @xmath130 or the function @xmath131 in eq .",
    "@xmath132 is correct , and remains close to @xmath2 even if both working models are misspecified .",
    "we then use higher order influence function based estimators of @xmath133 as estimators of @xmath57 .",
    "these estimators @xmath134 are asymptotically normal with variance and bias for @xmath2 depending both on the choice of the dimension @xmath128 of the working models and on the order @xmath135 of the influence function of @xmath136 we show that these same estimators @xmath134 can also be obtained under the approximate dirac delta function approach .",
    "we derive the optimal estimator @xmath137 in the class as a function of the hlder balls in which the functions @xmath138 and @xmath139 are assumed to lie .",
    "finally we conclude section 3 by showing that the estimators @xmath140 have a multi - robustness property that extends the double - robustness property of the first order influence function estimator @xmath141    in section 4 , we consider whether the estimators @xmath142 are rate - minimax .",
    "we show that whenever @xmath143and @xmath144,@xmath145 is not only rate minimax but is semiparametric efficient .",
    "further , by letting the order @xmath146 of the u - statistic depend on sample size , we construct a single estimator @xmath147 that is semiparametric efficient for all @xmath88 even when @xmath148 can not be estimated at an algebraic rate .",
    "we show , however , that when @xmath149 does not in general converge at the minimax rate . in section 4.1 , however , we construct a new estimator @xmath150 that converges at the minimax rate of @xmath151 whenever eq .",
    "@xmath152 holds . in section 5",
    ", we use the results obtained in earlier sections to construct adaptive confidence intervals for a regression function @xmath153   $ ] when the marginal of @xmath8 is unknown and for the treatment effect function and optimal treatment regime in a randomized clinical trial . in section 6.1 , we discuss how to obtain higher order u - statistic point estimators and confidence intervals for functionals @xmath154 that are implicitly defined as the solution to an equation @xmath155 in section 6.2 , we define higher order testing influence functions and efficient scores and describe their relationship to the higher order estimation influence functions and efficient influence functions of section 2 . finally ,",
    "in section 6.3 , we discuss the relationship between the higher order u - statistic point estimators of an implicitly defined functional @xmath154 and higher order testing influence functions .    before proceeding ,",
    "several additional comments are in order . in this paper",
    ", we investigate the asymptotic properties of our higher order u - statistic point and interval estimators .",
    "the reader is referred to li et al ( 2006 ) for an investigation of the finite sample properties of our procedures through simulation .",
    "in addition , precise regularity conditions are sometimes omitted from both the statements and the proofs of various theorems .",
    "this reflects the fact that the goal of this paper is to provide a broad overview of our theory as it currently stands .",
    "different subject matter experts will clearly disagree as to the maximum possible complexity of @xmath156,@xmath157 and @xmath158 thus it is important to have methods that adapt to the actual smoothness of these functions .",
    "elsewhere , we plan to provide point estimators that optimally adapt to unknown smoothness .",
    "in contrast to point estimators , however , for honest confidence intervals , the degree of possible adaption to unknown smoothness is small .",
    "therefore we propose that an analyst should report a mapping from apriori smoothness assumptions encoded in the exponents and radii of hlder balls ( or in other measures of complexity ) to the associated optimal @xmath159 honest confidence intervals proposed in this paper .",
    "such a mapping is finally only useful if substantive experts can approximately quantify their informal opinions concerning the shape and wiggliness of @xmath160and @xmath139 using the measure of complexity on offer by the analyst .",
    "it is an open question which , if any , complexity measure is suitable for this purpose .",
    "finally , most of our mathematical results concern rates of convergence .",
    "we offer only a few results on the constants in front of those rates .",
    "this is not because the constant is less important than the rate in predicting how a proposed procedure will perform in the moderate sized samples occurring in practice .",
    "rather , at present , we do not possess the mathematical tools necessary to obtain useful results concerning constants .",
    "a more extended discussion of the issue is found in section 3 of li et al .",
    "( 2006 ) .    in the following , we use @xmath161 to mean @xmath162 and @xmath163 @xmath164 to mean @xmath165 @xmath166 and @xmath167 @xmath168 to respectively mean @xmath169 @xmath170 as @xmath171",
    "given @xmath5 i.i.d observations @xmath172 from a model @xmath173 we consider inference on a nonlinear functional @xmath174 in general , @xmath175 can be infinite dimensional but for now we only consider the one dimensional case . in the following all quantities",
    "can depend on the sample size @xmath176 including the support of @xmath177 the parameter space @xmath178 and the functional @xmath175 .",
    "we generally suppress the dependence on @xmath5 in the notation",
    ". we will be particularly interested in models in which the parameter @xmath179 is infinite dimensional and @xmath180 and @xmath181 do not depend on @xmath126 we also briefly discuss models in which subvectors of @xmath179 are finite - dimensional parameters whose dimension @xmath182 increases as power @xmath183 ( often @xmath184 of @xmath5 and thus @xmath185 and @xmath186 depend on @xmath126    our first task is to define higher order influence functions .  before proceeding we recall some facts about @xmath187statistics .",
    "consider a function @xmath188 where we often suppress @xmath189 subscript @xmath190 for integers @xmath191lying in @xmath192 we define @xmath193 and@xmath194   \\mathbf{\\equiv}\\frac{(n - m)!}{n!}\\sum\\limits_{i_{1}\\neq i_{2} ... \\neq i_{m}}b_{m , i_{1}, ... ,i_{m}}.\\ ] ]    in an abuse of notation , we will consider the following expressions to be equivalent @xmath195   \\mathbf{\\equiv}\\mathbb{v}_{n}\\left [ b_{m,,i_{1}, ... ,i_{m}}\\right ]   \\mathbf{\\equiv}\\mathbb{v}_{n}\\left [ b_{m\\ } \\right ]   .\\ ] ] thus @xmath196   $ ] is a @xmath197 order u - statistic with kernel @xmath198 .",
    "we do not assume that @xmath198 is symmetric .",
    "we will write @xmath199   $ ] as @xmath200 so , suppressing the dependence on @xmath176 @xmath201   $ ] .",
    "any @xmath202has a unique ( up to permutation ) decomposition @xmath203 under any @xmath204 as a sum of degenerate u - statistics @xmath205 where degeneracy of @xmath206 means that @xmath207 satisfies @xmath208   = 0,l=1, .... ,s\\ ] ] where upper and lower case letters , respectively , denote random variables and their possible realizations .",
    "let @xmath209 be the hilbert space of all @xmath187statistics of order @xmath135 with mean zero and finite variance with inner product defined by covariances with respect to the @xmath5-fold product measure @xmath210 note that any @xmath187statistic @xmath211 of order @xmath212 , @xmath213 is also an mth order @xmath187 statistic with @xmath214 identically zero for @xmath215 .    since any two degenerate @xmath187 statistics of different orders",
    "are uncorrelated , the @xmath209-hilbert space projection of @xmath216 on @xmath217 is @xmath218 for @xmath219 thus a @xmath187statistic @xmath216 is degenerate @xmath220 @xmath221 @xmath222 @xmath220 @xmath223   = 0\\leftrightarrow\\mathbb{b}_{m}\\in\\mathcal{u}_{m-1}\\left (   \\theta\\right )   ^{\\perp_{m,\\theta}},\\ $ ] where @xmath224   \\mathbf{\\equiv}\\pi_{\\theta , m}\\left [   \\cdot|\\cdot\\right ]   $ ] is the projection operator of the hilbert space @xmath225 ( with the dependence on @xmath135 suppressed when no ambiguity can arise ) and , for any linear subspace @xmath226 of @xmath227 @xmath228 is its orthocomplement in the hilbert space @xmath229 given any @xmath230   , $ ] @xmath231 is explicitly given by @xmath232   $ ] where @xmath233  maps @xmath234 to @xmath235 define for @xmath236 @xmath237 with @xmath238 where the @xmath239 symbol denotes differentiation by the variables occurring to its right and the overbar @xmath240 denotes the vector @xmath241 .",
    "given a sufficiently smooth @xmath242dimensional parametric submodel @xmath243 mapping @xmath244 injectively into @xmath245 , define for @xmath179 in the range of @xmath246 @xmath247 and @xmath248 where @xmath249 is the density of @xmath250wrt a dominating measure .",
    "that is @xmath251 and @xmath252 are higher order derivatives of @xmath253and @xmath254 under a parametric submodel @xmath255 where the model @xmath256 has been suppressed in the notation .",
    "an @xmath257 order score associated with the submodel @xmath243 is defined to be @xmath258 where @xmath259 is a u - statistic of order @xmath260 to understand why @xmath259 is a @xmath187statistic we provide formulae for an arbitrary score @xmath259 in terms of the subject specific scores @xmath261",
    "@xmath262 for @xmath263  suppressing the @xmath264dependence , results in waterman and lindsay ( 1996 ) imply@xmath265 @xmath266 @xmath267    note these equations express each @xmath268 as a sum of degenerate u - statistics .",
    "we now define a @xmath197 order estimation influence function @xmath269 for @xmath270 where we suppress the dependence on @xmath271 when no ambiguity will arise .",
    "a u - statistic @xmath272 of order @xmath135 and finite variance is said to be an @xmath197 order estimation influence function for @xmath175 if ( i ) @xmath273   = 0,$ ] @xmath274 and ( ii ) for @xmath275 and every suitably smooth and regular ( see appendix ) @xmath276 dimensional parametric submodel @xmath277@xmath278   .\\ ] ]     estimation influence functions need not always exist , but when they do they are useful for deriving point estimators of @xmath271 with small bias and for deriving confidence interval estimators centered on an estimate of @xmath279 we will generally refer to estimation influence functions simply as influence functions .",
    "we remark that @xmath272 is an influence function under the above definition if and only if it is one under the modified version in which the dimension of the parametric submodel @xmath243 is unrestricted .",
    "a key result is the following theorem which is related to results of small and mcleish ( 1994 ) .",
    "[ eiet]*extended information equality theorem : * given a @xmath197 order influence function @xmath280 for any smooth and regular submodels @xmath243 and @xmath281@xmath282   /\\partial\\zeta_{l_{1}} ... \\partial \\zeta_{l_{_{s}}}|_{\\zeta=\\widetilde{\\theta}^{-1}\\left\\ {   \\theta\\right\\ } } = -\\psi_{\\backslash\\overline{l}_{s}}\\left (   \\theta\\right)\\ ] ] thus , if the functionals @xmath283   $ ] and @xmath284   $ ] have bounded frchet derivatives w.r.t .",
    "@xmath285 to order @xmath286 for a norm",
    "@xmath287 @xmath288 = -\\left [   \\psi\\left (   \\theta+\\delta\\theta\\ \\right )   -\\psi\\left ( \\theta\\right )   \\right ]   + \\text { } o\\left (   \\left\\vert \\left\\vert \\delta \\theta\\ \\right\\vert \\right\\vert ^{m+1}\\right)\\ ] ] since the functions @xmath289   $ ] and @xmath290   $ ] of @xmath285 have the same taylor expansion around @xmath179 up to order @xmath190    the proof is in the appendix .",
    "define the @xmath135th order tangent space @xmath291 at @xmath179 for the model @xmath292 to be the subspace of @xmath293 formed by taking the closed linear span of all scores of order @xmath135 or less as we vary over all regular parametric submodels @xmath294 ( whose range includes @xmath295 of our model @xmath296 we say a model is ( locally ) nonparametric for @xmath197 order inference if @xmath297    given any @xmath197 order estimation influence function @xmath298 define the mth order efficient estimation influence function to be @xmath299\\ ] ] @xmath70where @xmath300   \\mathbf{\\equiv}\\pi _ { \\theta , m}\\left [   \\cdot|\\cdot\\right ]   $ ] is the @xmath301projection operator . in the appendix , we prove the following :    [ eift]*efficient estimation influence function theorem :*    1 .",
    "@xmath302 is unique in the sense that for any two mth order influence functions @xmath303",
    "\\text { a.s.}\\ ] ] *  * 2 .",
    "@xmath302 is a mth order estimation influence function and has variance less than or equal to any other @xmath197 order estimation influence function .",
    "@xmath272 is a @xmath197 order estimation influence function if and only if @xmath70@xmath304 where @xmath305 is the ortho - complement of @xmath306 in @xmath229 4 .",
    "if @xmath272 exists then @xmath307exists for @xmath308 and @xmath309   = \\mathbb{if}_{s}^{eff}\\left (   \\theta\\right )   .$ ] 5 .   if the model @xmath310 is ( locally ) nonparametric , then 1 .",
    "there is at most one @xmath135th order estimation influence function @xmath272 for @xmath311 2 .",
    "@xmath312 where@xmath313\\ ] ] and @xmath314 is a degenerate @xmath197 order u - statistic and thus @xmath315   = 0.\\ ] ] 3 .",
    "( i ) : suppose , for a given @xmath316 @xmath317 exists and a kernel @xmath318 of @xmath319 has a first order influence function with kernel @xmath320 for all @xmath321 in a set @xmath322 which has probability @xmath18 under @xmath323 then @xmath272 exists and @xmath324   \\right ) \\label{mm1}\\ ] ] where the operator @xmath325 is given in @xmath326 + \\(ii ) conversely , if @xmath327 exists then the symmetric kernel @xmath328 of @xmath329 has a first order influence function for all @xmath321 in a set @xmath322 which has probability @xmath18 under @xmath330 further @xmath331 = if_{m , m}^{sym}\\left (   o_{i_{1}}, ... ,o_{i_{m}};\\theta\\right )   .\\ ] ]    pfanzagl ( 1990 ) previously proved part 5.c(i ) for @xmath332 our theorem offers a generalization of his result .",
    "note , in part ( i ) of 5(c ) , we can always take the kernel to be the symmetric kernel .",
    "[ proj]provided one knows how to calculate first order influence functions , one can obtain @xmath333 recursively using part @xmath334 an example of such a calculation is given in section [ dhoif_subsection ] below .",
    "thus part @xmath335 has the interesting implication that even though higher order influence functions are defined in terms of their inner products with higher order scores @xmath336 nevertheless , in ( locally ) nonparametric models , one can derive all the higher order influence functions of a functional @xmath175 without even knowing how to compute the scores @xmath268 for any @xmath337 in fact , one need not even be aware of the structure of the scores @xmath268 in terms of the subject - specific higher order scores @xmath338 in contrast , in parametric or semiparametric models whose tangent space @xmath339  does not equal the set @xmath225 of all @xmath135th order @xmath340 , one can often ( but not always ) still obtain an inefficient influence @xmath341  by applying part @xmath335 of the theorem .",
    "however , calculation of the efficient influence function @xmath342   $ ] by projection generally requires explicit knowledge of the scores @xmath343to derive @xmath344 for this reason ,  it can be considerably more difficult to analyze certain parametric models ( with dimension increasing with sample size ) than to analyze ( locally ) nonparametric models .",
    "we will consider derivation of and projections onto @xmath345 in a forthcoming paper . in the current paper , however , we do calculate @xmath346 in one model that is not ( locally ) nonparametric so as to provide some sense of the issues that arise . specifically in section [ minimax_section ] , we calculate @xmath347 for the functional @xmath348   \\right\\ }   ^{2}\\right ] $ ] in a model that assumes the marginal distribution of @xmath8 is known .",
    "*  [ jj1 ] : *  * implications of theorem * @xmath349 * *  for the variance of unbiased estimators : * *  suppose we have @xmath5 iid draws @xmath350 from @xmath351and a u - statistic @xmath352 of order @xmath353 with @xmath354   <",
    "\\infty$ ] for @xmath355satisfying @xmath356   = \\psi\\left (   \\theta\\right )   $ ] for all @xmath357 that is , @xmath352 is unbiased for @xmath175 .",
    "we will use theorem @xmath358 to generalize a number of well - known results on minimum variance unbiased estimation to arbitrary models .",
    "by @xmath356   = \\psi\\left (   \\theta\\right ) , $ ] we immediately conclude that , viewing @xmath359as a kth order u - statistic , @xmath360 is a @xmath361 order estimation influence function for @xmath175 for @xmath362 by theorem @xmath363",
    "@xmath364   \\geq var_{\\theta}\\left [   \\mathbb{if}_{m}^{eff}\\left (   \\theta\\right )   \\right ]   .$ ] we refer to @xmath365   $ ] as the @xmath197 order bhattacharyya variance bound at @xmath179 for the parameter @xmath270 in model @xmath366 as this definition , in a precise analogy to bickel et al .",
    "( 1993 ) s generalization of the cramer - rao variance bound , generalizes bhattacharyya s ( 1947 ) variance bound to arbitrary semi- and non- parametric models .",
    "indeed our 1st order bhattacharyya bound is precisely bickel et al.s ( 1993 ) generalization of the cramer - rao variance bound .",
    "we shall refer to an mth order u - statistic estimator @xmath352 as mth order ` unbiased locally efficient ' at @xmath285 for @xmath270 in model @xmath310 if it is unbiased for @xmath175 under the model with variance at @xmath285 equal to the @xmath197 order bhattacharyya bound at @xmath367 if @xmath352 is ` unbiased locally efficient ' at @xmath368 for all @xmath369 we say it is ` unbiased globally efficient ' . by theorem @xmath363",
    "@xmath370   \\geq var_{\\theta}\\left [ \\mathbb{if}_{m}^{eff}\\left (   \\theta\\right )   \\right ]   $ ] for @xmath371 as a consequence if an mth order ` unbiased locally efficient ' estimator @xmath372 exists at @xmath285 then , for @xmath373 @xmath374 so the @xmath197 and @xmath375 order bhattacharyya bounds are equal at @xmath285 and @xmath376 is also kth order ` unbiased locally efficient ' at @xmath285 .    from the fact that for , an unbiased estimator @xmath377 @xmath360 is an @xmath197 order influence function ,",
    "we conclude that the variance of @xmath352 attains the the bound @xmath378   $ ] at @xmath285 if and only if @xmath379 it follows that @xmath352 is ` unbiased globally efficient ' if and only if @xmath380 for all @xmath357 we thus have proved the following theorem in the @xmath381 direction .",
    "the @xmath382 direction is immediate .",
    "[ global_eff ] * : * in a model @xmath366 there exists an mth order unbiased globally efficient u - statistic estimator of @xmath311 if and only if , for all @xmath383 @xmath384 is a function @xmath372 of the data @xmath385 not depending on @xmath386 in that case , @xmath372 is the unique unbiased globally efficient estimator .    in a locally nonparametric model all unbiased mth order estimators are unbiased globally efficient , as there is a unique @xmath197 order influence function .",
    "for example , the usual unbiased estimator @xmath387 of the variance of a random variable @xmath8 is a second order u - statistic and thus is a kth order unbiased globally efficient u - statistic for @xmath388 in the locally nonparametric model consisting of all distributions under which @xmath389 has a finite variance .    in section [ minimax_section ]",
    "we use the results from this remark to compare the relative efficiencies of competing rate - optimal unbiased estimators in a model which is not locally nonparametric .",
    "we now describe the main heuristic idea behind using higher order influence functions .",
    "technical details are suppressed .",
    "consider the estimator @xmath390 based on a sample size @xmath176 where @xmath391 is an initial rate optimal estimator of @xmath179 from a separate independent training sample .",
    "that is we assume that our actual sample size is @xmath392 and we randomly split the @xmath392 observations into two samples : an analysis sample of size @xmath5 and a training sample of size @xmath393 where @xmath394 @xmath90 @xmath395we obtain our initial estimate @xmath391 from the training sample data .",
    "sample splitting has no effect on optimal rates of convergence , although in the form described here does affect constants. throughout the paper , we derive the properties of our estimators conditional on the data in the training sample . in a later section ,",
    "we describe how one can sometimes obtain an optimal constant by choosing @xmath396 rather than @xmath397    [ semi ] note that sample splitting is avoided in most statistical applications by using modern empirical process theory  to prove that ` plug - in ' estimators such as @xmath398 that estimate @xmath179 from the same sample used to calculate @xmath399 have nice statistical properties .",
    "however empirical process theory is not applicable in our setting because we are interested in function classes whose size ( entropy ) is so large that they fail to be donsker .",
    "for this reason we initially believed that explicit sample splitting would be difficult to avoid in our methodology .",
    "however , in robins et al .",
    "( 2007 ) , we describe a new method , more analogous to the jackknife than to sample splitting , that effectively allows one to use all the data for estimator construction .    expanding and evaluating conditionally on the training sample ( or equivalently on @xmath400",
    "we find by theorem @xmath401 that the conditional bias @xmath402\\ ] ] is @xmath403which decreases with @xmath404provided @xmath405    in theorem [ 3.19 ] below , we show that if @xmath406 as @xmath407 where @xmath408 is the density of @xmath409 under @xmath179 and @xmath410 has probability one under all @xmath383 then @xmath411   \\equiv var_{\\theta}\\left [   \\mathbb{if}_{m}^{eff}\\left (   \\widehat{\\theta}\\right )     now , by theorem [ eift ] , @xmath412   $ ] increases with @xmath135 .",
    "further , @xmath413",
    "\\asymp1/n$ ] , since , conditional on @xmath414 @xmath415 is the sample average of @xmath416 random variables .    to proceed further",
    "we shall need to be more explicit about the model @xmath296 for now , we consider finite - dimensional parametric models whose dimension @xmath128 increases with sample size .",
    "that is @xmath417 depends on @xmath5 and the dimension of @xmath418 is @xmath128 .",
    "suppose @xmath419 let @xmath420 be the maximum likelihood estimator of @xmath386 if @xmath128 increases slower than the sample size ( i.e. , @xmath421 then , a ) under regularity conditions , @xmath422 with @xmath423 the usual euclidean norm in @xmath424 and b ) @xmath412   , \\ $ ] although increasing with @xmath425 remains order @xmath426 as a consequence , if @xmath404is chosen greater than the solution @xmath427 to @xmath428 the bias of @xmath352 will be @xmath429 the rate of convergence will be the usual parametric rate of @xmath430 and thus , for @xmath5 sufficiently large , the squared bias of @xmath352 will be less than the variance . as a consequence , as discussed in section [ dr_ci_section ] , we can construct honest ( i.e uniform over @xmath431 asymptotic confidence intervals centered at @xmath432 with width of order @xmath433 here is a concrete example .",
    "* example : * suppose @xmath434 with @xmath0 bernouilli and with @xmath114having a density with respect to the uniform measure @xmath435 on the unit cube @xmath436   ^{d}$ ] in  @xmath437 suppose @xmath438   \\right )   ^{2}\\right ]   .$ ] let @xmath439 be a countable , linearly independent , sequence of either spline , polynomial , or compact wavelet basis functions dense in @xmath440 set @xmath441 we assume @xmath442{c}b\\left (   x;\\overline{\\eta}_{k^{\\ast}\\left (   n\\right )   } \\right )   \\equiv\\left [ 1+\\exp\\left (   -\\overline{\\eta}_{k^{\\ast}\\left (   n\\right )   } ^{t}\\overline { z}_{k^{\\ast}\\left (   n\\right )   } \\left (   x\\right )   \\right )   \\right ]   ^{-1};\\\\ \\overline{\\eta}_{k^{\\ast}\\left (   n\\right )   } \\in\\mathcal{n}_{k^{\\ast}\\left ( n\\right )   } \\end{array } \\right\\ }   , \\ ] ] @xmath443{c}f\\left (   x;\\overline{\\omega}_{k^{\\ast\\ast}\\left (   n\\right )   } \\right )   \\equiv c\\left (   \\overline{\\omega}_{k^{\\ast\\ast}\\left (   n\\right )   }",
    "\\right ) \\exp\\left [   \\overline{\\omega}_{k^{\\ast\\ast}\\left (   n\\right )   } ^{t}\\overline { z}_{k^{\\ast\\ast}\\left (   n\\right )   } \\left (   x\\right )   \\right ]   ; \\\\ \\overline{\\omega}_{k^{\\ast\\ast}\\left (   n\\right )   } \\in\\mathcal{w}_{k^{\\ast\\ast } \\left (   n\\right )   } \\end{array } \\right\\}\\ ] ] where @xmath444 is a normalizing constant and @xmath445 and @xmath446 are open bounded subsets of @xmath447 and @xmath448 hence , @xmath449 has dimension @xmath450and @xmath451    he ( 2000 ) and portnoy ( 1988 ) prove that , under regularity conditions , @xmath452 when @xmath453 below we shall see that @xmath454   \\asymp1/n$ ] for @xmath455    consider next models whose dimension @xmath456 increases faster than @xmath5 ( i.e. , @xmath457 in such models , the mle @xmath420 is generally inconsistent and indeed there may exist no consistent estimator of @xmath458 . in that case , @xmath459fails to be @xmath460 and the conditional bias @xmath461exist@xmath90 it is necessary to place further apriori restrictions on the complexity of @xmath462 typical examples of complexity - reducing assumptions would be an @xmath463sparseness assumption that only @xmath464 of the @xmath128 parameters are non - zero or a smoothness assumption that specifies that the rate of decrease of the @xmath465component of @xmath458 is equal to @xmath466 raised to a given ( positive ) power . even after imposing such complexity - reducing assumptions , @xmath467 may not be estimable at rate @xmath63 .",
    "for instance consider the previous example but now with @xmath468 and @xmath469 exceeding @xmath470 so @xmath471 , @xmath472 and @xmath473with @xmath474 consider the norms @xmath475 = @xmath476@xmath477 and @xmath478 .",
    "suppose , under a particular smoothness assumption , optimal rate estimators @xmath479 and @xmath480 of @xmath481 and @xmath482 satisfy @xmath483 and @xmath484for some @xmath485 and all @xmath486 hence , @xmath487 for @xmath488 based on arguments given later , we expect that @xmath489   \\asymp\\frac{n^{\\left (   \\gamma -1\\right )   \\left (   m-1\\right )   } } { n}$ ] and @xmath490   = o_{p}\\left ( \\left\\vert \\widehat{\\overline{\\eta}}_{k^{\\ast}\\left (   n\\right )   } -\\overline{\\eta}_{k^{\\ast}\\left (   n\\right )   } \\right\\vert ^{2}\\left\\vert \\widehat{\\overline{\\omega}}_{k^{\\ast\\ast}\\left (   n\\right )   } -\\overline{\\omega } _ { k^{\\ast\\ast}\\left (   n\\right )   } \\right\\vert _ { m-1}^{m-1}\\right ) = o_{p}\\left (   n^{-2\\gamma_{\\eta}-\\left (   m-1\\right )   \\gamma_{\\omega}}\\right ) $ ]    @xmath491    to find the estimator @xmath492 in the class @xmath352with optimal rate of convergence@xmath90 let @xmath493 be the value of @xmath135 that equates the order @xmath494 of the squared bias and the order @xmath495 of the variance .",
    "then @xmath496 if the order @xmath497of the mean squared error@xmath70at @xmath498 is less than or equal to that at @xmath499 otherwise , @xmath500 .",
    "the rate of convergence of @xmath492 will often be slower than @xmath433 note @xmath501 whenever @xmath502 regardless of @xmath503 and @xmath504    by using the estimator @xmath505 rather than @xmath506we can guarantee that the variance asymptotically dominates bias and construct honest ( i.e uniform over @xmath431 asymptotic confidence intervals centered at @xmath507 of course , the sample size @xmath5 at which , for all @xmath508 the finite sample coverage of the intervals discussed above is close to the asymptotic ( i.e. nominal ) coverage is generally unknown and could be very large .",
    "for this reason , a better , but unfortunately as yet technically out of reach , approach to confidence interval construction is discussed in section [ dr_ci_section ] .",
    "in contrast to the case of parametric models of increasing dimension , in the infinite dimensional models which we consider in the following section , the functionals @xmath175 of interest have first order influence functions @xmath509 but do not have higher order influence functions . as a consequence ,",
    "an initial truncation step is needed before we can apply the approach outlined in the preceding paragraph .",
    "finally , even in the case of parametric models with @xmath510 and complexity reducing assumptions imposed , , when the minimax rate for estimation of @xmath175 is slower than @xmath430 the optimal estimator @xmath492 in the class @xmath511 will generally not be rate minimax .",
    "see section 3.2.6 and sections 4.1.1 for additional discussion .",
    "in this section we consider models in which the parameter @xmath179 is infinite dimensional and @xmath180 and @xmath181 do not depend on @xmath126 we make the following 4 assumptions @xmath512 :     ai ) the data @xmath513includes a vector @xmath514where , for all @xmath515 the distribution of @xmath114 is supported on the unit cube @xmath436 ^{d}$ ] ( or more generally a compact set ) in  @xmath516and has a density @xmath517 with respect to the lebesgue measure .",
    "further @xmath518 where @xmath519 governs the marginal law of @xmath8 and @xmath520 governs the conditional distribution of @xmath521    aii ) the parameter @xmath522 contains components @xmath523 and @xmath524 , @xmath525   ^{d}\\mathcal{\\rightarrow r}$ ] and @xmath526   ^{d}\\mathcal{\\rightarrow r},$ ] such that the functional @xmath175 of interest has a first order influence function @xmath527   , $ ] where @xmath528 and the known functions @xmath529do not depend on @xmath386    aiii )    \\a ) @xmath530 where @xmath531 and @xmath532 are the parameter spaces for the functions @xmath533 and @xmath534 .",
    "furthermore the sets @xmath535 and @xmath532 are dense in @xmath536 at each @xmath537    or    \\b ) @xmath538 , @xmath539and @xmath540 is dense in @xmath541 at each @xmath537    * remark : * aiiib ) can be viewed as a special case of aiiia ) as discussed in example 1a below , so we need only prove results under assumption aiiia ) .",
    "assumptions ai)-aiii ) have a number of important implications that we summarize in a theorem and two lemmas .",
    "[ dr]double - robustness : assume ai)-aiii ) hold , and recall @xmath542and @xmath533 are elements of @xmath386 then @xmath543   = e_{\\theta}\\left [ h\\left (   b^{\\ast},p\\right )   \\right ]   = e_{\\theta}\\left [   h\\left (   b , p\\right ) \\right ]   = \\psi\\left (   \\theta\\right)\\ ] ] for all @xmath544    : @xmath545   -e_{\\theta}\\left [ h\\left (   b , p\\right )   \\right ]   = e_{\\theta}\\left [   \\left\\ {   h_{1}p\\left ( x\\right )   + h_{2}\\right\\ }   \\left\\ {   b\\left (   x\\right )   -b^{\\ast}\\left ( x\\right )   \\right\\ }   \\right ]   $ ] and @xmath546   -e_{\\theta}\\left [   h\\left (   b , p\\right )   \\right ] = e_{\\theta}\\left [   \\left\\ {   h_{1}b\\left (   x\\right )   + h_{3}\\right\\ }   \\left\\ { p\\left (   x\\right )   -p^{\\ast}\\left (   x\\right )   \\right\\ }   \\right ]   .$ ]  the theorem then follows from part 1 ) of the following lemma .",
    "theorem [ dr ] states that @xmath547  has mean @xmath175  under @xmath548  even when @xmath549  is misspecified as @xmath53  or @xmath533  is misspecified as @xmath550  we refer to the functional @xmath175  as doubly robust because of this property .",
    "the next lemma shows that @xmath551  is not unbiased if both @xmath533  and @xmath534  are simultaneously misspecified .",
    "that is , @xmath552   \\neq\\psi\\left (   \\theta\\right )   .$ ]    [ conde ] assume ai)-aiii ) hold . then    1",
    ".   @xmath553   = e_{\\theta } \\left [   \\left\\ {   h_{1}p+h_{2}\\right\\ }   |x\\right ]   = 0 $ ] 2 .",
    "@xmath554 -e_{\\theta}\\left [   h\\left (   b , p\\right )   \\right ]   = e_{\\theta}\\left [   \\left ( b - b^{\\ast}\\right )   \\left (   p - p^{\\ast}\\right )   h_{1}\\right ]   $ ] + and @xmath555   = e_{\\theta}\\left [   -bph_{1}+h_{4}\\right ]   $ ]    part 1 ) : by assumptions @xmath556 and @xmath557 we have paths @xmath558 in our model with @xmath559 and@xmath560 for @xmath561 where the sequence @xmath562 is dense in @xmath563   .$ ] let @xmath564be the score for path @xmath565 at @xmath566 then by @xmath567   $ ] @xmath568 \\\\ &   + e_{\\theta}\\left [   h\\left (   b , p\\right )   s_{l}\\left (   \\theta\\right ) \\right]\\end{aligned}\\ ] ] by @xmath569=@xmath570@xmath571\\ ] ] thus @xmath572 = 0.$ ] but @xmath573 is dense in @xmath574   \\ $ ]  so @xmath575   = 0\\ ] ] an analogous argument proves latexmath:[$e_{\\theta}\\left [   \\left\\ {   h_{1}p+h_{2}\\right\\ }    @xmath552   -e_{\\theta}\\left [   h\\left (   b , p\\right )   \\right ]   = $ ] @xmath577 \\\\ &   = e_{\\theta}\\left [   \\left (   b^{\\ast}p^{\\ast}-bp\\right )   h_{1}-\\left ( b^{\\ast}-b\\right )   ph_{1}-\\left (   p^{\\ast}-p\\right )   bh_{1}\\right ] \\\\ &   = e_{\\theta}\\left [   \\left (   b - b^{\\ast}\\right )   \\left (   p - p^{\\ast}\\right ) h_{1}\\right]\\end{aligned}\\ ] ] where the second equality is by part 1 ) .",
    "choosing @xmath578 wp1 completes the proof of the theorem since then @xmath579   = e_{\\theta}\\left [   h_{4}\\right ]   $ ] .",
    "below we will need the following partial converse to lemma [ conde ] .",
    "[ if1 ] let @xmath580 and @xmath245 and @xmath581 be as defined in ai)- aiiia ) .",
    "@xmath582   = e_{\\theta}\\left [ \\left\\ {   h_{1}p+h_{2}\\right\\ }   |x\\right ]   = 0\\ ] ] and @xmath583   .$ ] then @xmath584   \\ $ ] is the first order influence function of @xmath174    : the influence function of the functional @xmath579   $ ] for known functions @xmath585 is @xmath586   \\right ]   .$ ] thus by the linearity of first order influence functions , the lemma is true if and only if for each @xmath587 the functional @xmath588   $ ] with @xmath589 fixed has influence function equal to @xmath590 wp1 at @xmath591 that the influence function is equal to @xmath590 follows from the fact that , under the assumptions of the lemma , for sets @xmath592 and @xmath593 dense in @xmath594   $ ] , @xmath595 /dt_{|t=0}\\\\ &   = e_{\\theta}\\left [   \\left\\ {   h_{1}b_{0}\\left (   x\\right )   + h_{3}\\right\\ } d_{l}\\left (   x\\right )   \\right ]   + e_{\\theta}\\left [   \\left\\ {   h_{1}p_{0}\\left ( x\\right )   + h_{2}\\right\\ }   c_{l}\\left (   x\\right )   \\right ]   = 0\\end{aligned}\\ ] ]    results of ritov and bickel ( 1990 ) and robins and ritov ( 1997 ) imply it is not possible to construct honest asymptotic confidence intervals for @xmath270 whose width shrinks to @xmath590 as @xmath596 if @xmath597  and @xmath598 are too rough@xmath64  therefore we also place apriori bounds on their roughness .",
    "our bounds will be based on the following definition .    a function @xmath599 with domain @xmath436   ^{d}$ ]",
    "is said to belong to a hlder ball @xmath72 with hlder exponent @xmath600 and radius @xmath74 if and only if @xmath71 is uniformly bounded by @xmath75 , all partial derivatives of @xmath599 up to order @xmath77 exist and are bounded , and all partial derivatives @xmath601 of order @xmath77 satisfy @xmath602   ^{d}}\\left\\vert \\nabla^{\\left\\lfloor \\beta\\right\\rfloor } h(x+\\delta x)-\\nabla^{\\left\\lfloor \\beta\\right\\rfloor } h(x)\\right\\vert \\leq c||\\delta x||^{\\beta-\\left\\lfloor \\beta\\right\\rfloor } .\\ ] ]    we note that the @xmath603 and @xmath604 rates of convergence for estimation of a marginal density or conditional expectation @xmath605 @xmath606 are @xmath607 and @xmath608 respectively@xmath64 we refer to an estimator attaining these rates as rate optimal .",
    "aiv ) we assume @xmath609 and g@xmath610 lie in given hlder balls @xmath80 @xmath611 @xmath612 where @xmath613    furthermore we assume @xmath614 wp1 . finally we assume",
    ", as can always be arranged by a suitable choice of estimator , that the initial training sample estimators @xmath615 and @xmath616 are rate optimal , have more than max@xmath617 derivatives , and have @xmath604 norm bounded by a constant @xmath618 further inf@xmath619   ^{d}}$ ] @xmath620 the reason for the restrictions on @xmath148 will become clear below .    the restrictions @xmath512 are the only restrictions common to all functionals and models in the class . additional model and/or functional specific restrictions will be given below .    to motivate our interest in such a class of functionals and models we provide a number of examples .",
    "in each case , one can use lemma @xmath621 to verify that the influence function of @xmath270 is as given .",
    "all but examples 3 and 4 are examples of ( locally ) nonparametric models .",
    "* example 1",
    ": * suppose @xmath409=@xmath622with @xmath12 and @xmath0 univariate random variables .",
    "* example 1a : * * expected product of conditional expectations : * let * *  * * @xmath623   $ ] where @xmath624   , p\\left (   x\\ \\right )   = e_{\\theta}\\left [ a|x\\right ]   .$ ] in this model @xmath625 so @xmath626    we also consider the special case of this model where @xmath627 with probability one @xmath628 . then , as in assumption @xmath629 @xmath630 @xmath631 @xmath632 wp1",
    ". then @xmath633   .$ ] in section [ adaptive_section ] , we show how our confidence interval for @xmath634   $ ] can be used to obtain an adaptive confidence interval for the regression function @xmath635 .",
    "* example 1b :* * expected conditional covariance * @xmath636   \\ -e_{\\theta}\\left [ p\\left (   x\\ \\right )   b\\left (   x\\ \\right )   \\right ]   = e_{\\theta}\\left [ cov_{\\theta}\\left\\ {   y , a|x\\right\\ }   \\right]\\ ] ] has influence function @xmath637 so @xmath638 .",
    "the next example 1c shows that a confidence interval and point estimators for @xmath639   $ ] can be used to obtain confidence intervals and point estimator for the variance weighted average treatment effect in an observational study .",
    "* example 1c : * * variance - weighted average treatment effect : *  suppose , in an observational study , @xmath640 , @xmath12 is a binary treatment taking values in @xmath641 , @xmath642 is a univariate response and @xmath8 is a vector of pretreatment covariates . consider the parameter @xmath643 given by:@xmath644   } { e_{\\theta}\\left [   \\ var_{\\theta}(a|x)\\right ]   } = \\frac{e_{\\theta}\\left [   \\ cov_{\\theta}(y^{\\ast},a|x)\\right ]   } { e_{\\theta } \\left [   \\ \\pi\\left (   x\\right )   \\left\\ {   1-\\pi\\left (   x\\right )   \\right\\ } \\right ]   } , \\text { } \\ ] ] where @xmath645 is often referred to as the propensity score .",
    "we are interested in @xmath646 for several reasons .",
    "first , in the absence of confounding by unmeasured factors , @xmath643 is the variance - weighted average treatment effect since @xmath643 can be rewritten as @xmath647   $ ] where @xmath648   } $ ] and @xmath649 is the average conditional treatment effect at level @xmath650 of the covariates .",
    "second , under the semiparametric model @xmath651 that assumes the treatment effect does not depend on @xmath652 @xmath653 however since the model @xmath654 may not hold and therefore the parameter @xmath655 may be undefined , we choose to make inference on @xmath646 without imposing @xmath656 .     now if for any @xmath657 we define @xmath658 to be @xmath659\\ ] ]  with @xmath660 it is easy to verify that @xmath643 may also be characterized as the solution @xmath661 to the equation @xmath662 .",
    "thus inference on @xmath646 is easily obtained from inference on @xmath663 in particular a @xmath664confidence set for @xmath643 is the set of @xmath665such that a @xmath664ci interval for @xmath666 contains @xmath667therefore , with no loss of generality , we consider the construction of a @xmath664ci for @xmath668 for a fixed value @xmath669 , and write @xmath670 and @xmath671 thus @xmath672   $ ] and we are in the setting of example 1b .",
    "in section [ testing_section]@xmath90 we show the rates at which the width of the confidence sets for @xmath673 and for @xmath643 shrink with @xmath5 are equal@xmath64    * example 2a : * * missing at random : * suppose @xmath674 where @xmath0 is an outcome that is not always observed , @xmath675is the binary missingness indicator , @xmath8 is a @xmath676dimensional vector of always observed continuous covariates , and let @xmath677 be the propensity score , and @xmath678 .",
    "we suppose @xmath679and define @xmath680   = e_{\\theta}\\left [   b\\left (   x\\right )   \\right]\\ ] ] interest in @xmath175 lies in the fact that @xmath270 is the marginal mean of @xmath0 under the missing ( equivalently , coarsening ) at random ( mar ) assumption that @xmath681 in this model @xmath682 so @xmath683 .",
    "note that if one has assumed apriori that @xmath684 and @xmath685 lay in hlder balls with respective exponents @xmath686 and @xmath687 then @xmath688 would be @xmath689 since @xmath690 .",
    "* example 2b : * * missing not - at random : * consider again the setting of example 2a but we no longer assume mar .",
    "rather we assume @xmath691   \\right\\ }   \\right\\ }   ^{-1}\\ ] ] may depend on @xmath0 , where now @xmath692 is an unknown function and @xmath693 is a known constant ( to be later varied in a sensitivity analysis ) .",
    "in this case the marginal mean of @xmath0 is given by @xmath694   \\right\\ }   \\right ]   \\right )   .$ ] robins and rotnitzky @xmath695 proved this model places no restrictions on @xmath696 and derived @xmath697 where , now , @xmath698 /e\\left [   \\exp\\left\\ {   -\\alpha y\\right\\ }   |a=1,x\\right]\\ ] ] @xmath70 and @xmath699",
    ". thus @xmath700 and @xmath701 when @xmath702 this provides an alternate parametrization of example 2a .",
    "* example 3 : marginal structural models and the average treatment effect : * consider the set - up of example 1c including the non - identifiable assumption of no unmeasured confounders , except now @xmath12 is discrete with possibly many levels and @xmath703 wp1 .",
    "a marginal structural model assumes @xmath704 where @xmath705 is a known function and @xmath706 is an unknown vector parameter of dimension @xmath707 when @xmath12 is dichotomous with @xmath708 and @xmath709 , then @xmath710 is the average treatment effect parameter .",
    "let @xmath711 be any density with the same support as @xmath12 and let @xmath712 be a @xmath713-vector function , both chosen by the analyst .",
    "then @xmath714is identified as the ( assumed ) unique value of @xmath715 satisfying @xmath716   = 0,\\ ] ] where @xmath717 .",
    "thus a @xmath664confidence set for @xmath655 is the set of vectors @xmath718such that a @xmath664ci for @xmath719 contains @xmath667therefore , with no loss of generality , we consider the construction of a @xmath664ci for the @xmath676vector functional @xmath720 for a fixed value @xmath721 and define @xmath722 and @xmath723   .$ ] then @xmath724 has influence function @xmath725 next define @xmath726   $ ] . then @xmath727 is the  integral @xmath728    it follows that @xmath727 is a integral over @xmath729 of influence functions @xmath730 for parameters @xmath731 in our class with @xmath732 .",
    "thus we can estimate @xmath175 by @xmath733 where @xmath734 is an estimator of @xmath735 if the support of @xmath12 is of greater cardinality than @xmath736 the model is not locally nonparametric .",
    "different choices for @xmath712 and @xmath737 for which @xmath738   $ ] is invertible may result in difference influence functions .",
    "all yield the same rate of convergence , although the constants differ .",
    "see remark [ proj ] above .",
    "extension of our methods to continuous @xmath12 will be treated elsewhere .",
    "* example 4 : confidence intervals for the optimal treatment strategy in a randomized clinical trial : * consider a randomized clinical trial with data @xmath739 , @xmath12 is a binary treatment taking values in @xmath740 , @xmath642 and @xmath0 are univariate responses , @xmath8 is a vector of pretreatment covariates . in a randomized trial , the randomization probabilities @xmath741",
    "are known by design .",
    "let @xmath742 and @xmath743 be the average treatment effects at level @xmath744 on @xmath642 and @xmath20 we assume @xmath745  and @xmath642 have been coded so that positive treatment effects are desirable .",
    "let @xmath746   .$ ] because the model is not locally nonparametric there exists more than a single first order influence function . indeed , for any given function @xmath747 @xmath748 \\\\ &   \\times\\left\\ {   a-\\pi_{0}\\left (   x\\right )   \\right\\ }   \\sigma_{0}^{-2}\\left ( x\\right )   + c\\left (   x\\right )   \\left\\ {   a-\\pi_{0}\\left (   x\\right )   \\right\\}\\end{aligned}\\ ] ] with @xmath749 is an influence function in our class [ provided it is square integrable ] with @xmath750 @xmath751 @xmath752 @xmath753 .",
    "as @xmath754 is varied , one obtains all first order influence functions .",
    "we do not discuss the efficient choice of @xmath754 in this paper .",
    "our interest lies in the special case where @xmath755 wp1 ( so there is but one response of interest ) and thus , as in assumption @xmath629 @xmath756and we construct confidence interval for @xmath757   .$ ] in section [ adaptive_section ] we describe how we can use a confidence interval for @xmath758   $ ] to obtain confidence intervals for the treatment effect function @xmath759 and , most importantly , for the optimal treatment strategy @xmath760   $ ] under which a subject with covariate value @xmath650 is treated if and only if the treatment effect @xmath761 is positive ( i.e. , @xmath762 .        in all of our examples",
    "the functions @xmath598 and @xmath763 are functions of conditional expectations given the continuous random variable @xmath113  it is well known that the associated point - evaluation functional @xmath764 and @xmath759 do not have first order influence functions .",
    "it then follows from part 5c of theorem [ eift ] and the dependence of @xmath765   $ ] on @xmath130 and @xmath598 evaluated at the point @xmath8 that , in none of our examples , does @xmath175 have a second ( or higher ) order influence function .",
    "as a precise understanding of the reason for the nonexistence of higher order influence functions for @xmath175 is fundamental to our approach , we now use part 5c of theorem [ eift ] to prove that @xmath766 does not exist by showing that the functional @xmath767 does not have a first order influence function @xmath768   .$ ] in this proof , we do not assume that @xmath130 and @xmath598 are functions of conditional expectations .",
    "rather we only assume that our functional satisfies assumptions a(i - iv ) .",
    "let @xmath769 and @xmath770 denote the marginal cdf and density of @xmath113    consider paths ( parametric submodels ) @xmath771 such that @xmath772satisfying @xmath773 where the sequences @xmath562 and @xmath774 are each dense in @xmath775   .$ ] let @xmath776 @xmath777 and @xmath778 denote the overall , conditional , and marginal scores@xmath779    by linearity , @xmath767 has an influence function only if the functionals @xmath759 and @xmath764 have one as well .",
    "now by differentiating the identity @xmath780   = 0\\ ] ] wrt to @xmath781 and evaluating at @xmath782 we have @xmath783   = e_{\\mathbb{\\theta}}\\left [ h_{1}|x = x\\right ]   a_{l}\\left (   x\\right)\\ ] ] however , by definition , @xmath759 has an influence function @xmath784 $ ] at @xmath785only if for @xmath786 both @xmath787 equals @xmath788   $ ] and @xmath789   = 0.$ ] thus if @xmath790 exists , it must satisfy @xmath791 \\\\ &   = e_{\\theta}\\left [   h_{1}|x = x\\right ]   e_{\\theta}\\left [   if_{1,b\\left ( x\\right )   \\ } \\left (   o;\\theta\\right )   s_{l}\\left (   o;\\theta\\right )   \\right]\\end{aligned}\\ ] ] without loss of generality , suppose @xmath792 wp 1@xmath64 now if we could find a kernel @xmath793 such that@xmath794 \\nonumber\\\\ &   \\equiv\\int k_{f_{x},\\infty}\\left (   x , x^{\\ast}\\right )   r\\left (   x^{\\ast } \\right )   f_{x}\\left (   x^{\\ast}\\right )   dx^{\\ast}\\text { for all } r\\left ( \\cdot\\right )   \\in l_{2}\\left (   f_{x}\\right ) \\label{dirac2}\\ ] ] then @xmath795{c}\\left\\ {   e_{\\theta}\\left [   h_{1}|x = x\\right ]   \\right\\ }   ^{-1/2}k_{f_{x},\\infty } \\left (   x , x\\ \\right ) \\\\ \\times\\left\\ {   e_{\\theta}\\left [   h_{1}|x\\right ]   \\right\\ }   ^{-1/2}\\left\\ { h_{1}b\\left (   x\\ \\right )   + h_{3}\\right\\ } \\end{array } \\right]\\ ] ] would be an influence function since @xmath796   e_{\\theta}\\left [ \\begin{array } [ c]{c}-\\left\\ {   e_{\\theta}\\left [   h_{1}|x = x\\right ]   \\right\\ }   ^{-1/2}k_{f_{x},\\infty}\\left (   x , x\\right )   \\times\\\\ \\left\\ {   e_{\\theta}\\left [   h_{1}|x\\right ]   \\right\\ }   ^{-1/2}\\left\\ { h_{1}b\\left (   x\\ \\right )   + h_{3}\\right\\ }   s_{l}\\left (   o;\\theta\\right ) \\end{array } \\right ] \\\\ &   = e\\left [   h_{1}|x = x\\right ]   ^{1/2}\\ e_{\\theta}\\left [ \\begin{array } [ c]{c}-k_{f_{x},\\infty}\\left (   x , x\\right )   \\left\\ {   e_{\\theta}\\left [   h_{1}|x\\right ]   \\right\\ }   ^{-1/2}\\\\ \\times\\left\\ {   h_{1}b\\left (   x\\ \\right )   + h_{3}\\right\\ }   \\left\\ {   s_{l}\\left ( o|x\\right )   + s_{l}\\left (   x\\right )   \\right\\ } \\end{array } \\right ] \\\\ &   = e\\left [   h_{1}|x = x\\right ]   ^{1/2}\\ e_{f_{x}}\\left\\ {   e_{\\theta}\\left [ \\begin{array } [ c]{c}-k_{f_{x},\\infty}\\left (   x , x\\right )   \\left\\ {   e_{\\theta}\\left [   h_{1}|x\\right ]   \\right\\ }   ^{-1/2}\\times\\\\ \\left\\ {   h_{1}b\\left (   x\\ \\right )   + h_{3}\\right\\ }   s_{l}\\left (   o|x\\right )   |x \\end{array } \\right ]   \\right\\ } \\\\ &   = -e_{\\theta}\\left [   \\left (   h_{1}b\\left (   x\\right )   + h_{3}\\right ) s_{l}\\left (   o|x\\right )   |x = x\\right]\\end{aligned}\\ ] ] by an analogous argument @xmath797{c}\\left\\ {   e_{\\theta}\\left [   h_{1}|x = x\\right ]   \\right\\ }   ^{-1/2}k_{f_{x},\\infty } \\left (   x , x\\right ) \\\\ \\times\\left\\ {   e_{\\theta}\\left [   h_{1}|x\\right ]   \\right\\ }   ^{-1/2}\\left\\ { h_{1}p\\left (   x\\ \\right )   + h_{2}\\right\\ } \\end{array } \\right]\\ ] ] would be an influence function .    indeed since the sequences @xmath798  and @xmath799 are dense the existence of such a kernel is also a necessary condition for @xmath800  and @xmath801 to exist and thus for @xmath802 to exist .",
    "a kernel satisfying eq.@xmath803 is referred to as the dirac delta function wrt to the measure @xmath804 and would clearly have to satisfy @xmath805 were it to exist . of course a kernel satisfying eq .",
    "( [ dirac2 ] ) is known not to exist in @xmath806   \\times l_{2}\\left [   f_{x}\\right ] .$ ] we conclude that @xmath767 does not have an influence function and therefore @xmath807 does not exist .    * a formal approach : * to motivate how one might overcome this difficulty , we note that kernels satisfying eq .",
    "( [ dirac2 ] ) exist as generalized functions or kernels ( also known as schwartz functions or distributions ) .",
    "we shall `` formally derive higher order influence functions that appear to be elements of the space of generalized functions .",
    "however , we use these calculations only as motivation for statistical procedures based on ordinary kernels living in @xmath806   \\times l_{2}\\left [ f_{x}\\right ]   .$ ] thus it does not matter whether these formal calculations could be made rigorous with appropriate redefinitions . rather we can simply regard the following as results obtained by applying a ' ' formal calculus \" to part 5c of theorem [ eift ] that adds to the usual calculus additional identities licensed by eqs .",
    "( [ dirac2 ] ) and ( [ diracpoint])@xmath64    we will need the fact that@xmath90 for any function @xmath808 eq .",
    "@xmath809implies that @xmath810    we now show that@xmath811   = \\pi_{\\theta , 2}\\left [   \\mathbb{v}\\left [   if_{1,if_{1,\\psi}\\left (   o_{i_{1}};\\cdot\\right ) \\ } \\left (   o_{i_{2}};\\theta\\right )   /2\\right ]   |\\mathcal{u}_{1}^{\\perp _ { 2,\\theta}}\\left (   \\theta\\right )   \\right]\\ ] ] would formally have u - statistic kernel @xmath812{c}\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   e_{\\theta}\\left [   h_{1}|x_{i_{1}}\\right ]   ^{-\\frac{1}{2}}k_{f_{x},\\infty}\\left (   x_{i_{1}},x_{i_{2}}\\right ) \\\\ e_{\\theta}\\left [   h_{1}|x_{i_{2}}\\right ]   ^{-\\frac{1}{2}}\\varepsilon_{p , i_{2}}\\left (   \\theta\\right ) \\end{array } \\right ]   , \\label{infin2}\\\\ \\text{with } \\varepsilon_{b , i_{1}}\\left (   \\theta\\right )    &   = \\ \\left\\ { b_{i_{1}}h_{1,i_{1}}+h_{3,i_{1}}\\right\\ }   , \\text { } \\varepsilon_{p , i_{2}}\\left (   \\theta\\right )   = \\",
    "\\left\\ {   h_{1,i_{2}}p_{i_{2}}+h_{2,i_{2}}\\right\\ } .\\nonumber\\end{aligned}\\ ] ] to show eq [ infin2 ] note , by @xmath813 @xmath70and@xmath814 we have @xmath815 where @xmath816   ^{-\\frac{1}{2}}\\\\ &   \\times k_{f_{x},\\infty}\\left (   x_{i_{1}},x_{i_{2}}\\right )   e_{\\theta } \\left [   h_{1}|x_{i_{2}}\\right ]   ^{-\\frac{1}{2}}\\left\\ {   p_{i_{2}}h_{1,i_{2}}+h_{2,i_{2}}\\right\\ } \\\\ &   = -\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   e_{\\theta}\\left [ h_{1}|x_{i_{1}}\\right ]   ^{-\\frac{1}{2}}k_{f_{x},\\infty}\\left (   x_{i_{1}},x_{i_{2}}\\right )   e_{\\theta}\\left [   h_{1}|x_{i_{2}}\\right ]   ^{-\\frac{1}{2}}\\varepsilon_{p , i_{2}}\\left (   \\theta\\right ) \\\\ q_{2,b,\\overline{i}_{2}}\\left (   \\theta\\right )    &   \\equiv\\left\\ {   p_{i_{1}}h_{1,i_{1}}+h_{2,i_{1}}\\right\\ }   if_{1,b\\left (   x_{i_{1}}\\right )   \\ } \\left ( o_{i_{2}};\\theta\\right ) \\\\ &   = -\\varepsilon_{b , i_{2}}\\left (   \\theta\\right )   e_{\\theta}\\left [ h_{1}|x_{i_{2}}\\right ]   ^{-\\frac{1}{2}}k_{f_{x},\\infty}\\left (   x_{i_{2}},x_{i_{1}}\\right )   e_{\\theta}\\left [   h_{1}|x_{i_{1}}\\right ]   ^{-\\frac{1}{2}}\\varepsilon_{p , i_{1}}\\left (   \\theta\\right)\\end{aligned}\\ ] ]    thus , by part 5c of theorem [ eift ] @xmath70@xmath817 \\\\ &   = \\frac{1}{2}\\left\\ {   \\mathbb{q}_{2,p,\\overline{i}_{2}}\\left ( \\theta\\right )   + \\mathbb{q}_{2,b,\\overline{i}_{2}}\\left (   \\theta\\right ) \\right\\ }   = \\mathbb{q}_{2,p,\\overline{i}_{2}}\\left (   \\theta\\right ) \\equiv\\mathbb{v}\\left [   rhs\\text { of eq}.(\\ref{infin2})\\right]\\end{aligned}\\ ] ] since @xmath818 is a function of only one subject s data and @xmath819 and @xmath820 are the same up to a permutation that exchanges @xmath821 with @xmath822    to obtain @xmath823 one must derive the influence function @xmath824of @xmath825 the formula for @xmath826 is given in eq . @xmath827",
    "a detailed derivation is given in the appendix .",
    "here we simply note that the only essentially new point is that we now require the influence function of @xmath828 , which , as shown next , is given by @xmath829{c}k_{f_{x},\\infty}\\left (   x_{i_{1}},x_{i_{3}}\\right )   k_{f_{x},\\infty}\\left ( x_{i_{3}},x_{i_{2}}\\right ) \\\\",
    "-k_{f_{x},\\infty}\\left (   x_{i_{1}\\ } , x_{i_{2}}\\right ) \\end{array } \\right\\ } \\label{kernelif}\\ ] ]    to see that if eq.@xmath803 held , eq.@xmath830 would hold , note that for any path @xmath831 with @xmath832 @xmath833   .\\ $ ] differentiating wrt to @xmath781 and evaluating at @xmath834 , we have @xmath835   + e_{_{\\theta}}\\left [   \\left\\ {   \\frac { \\partial}{\\partial t}k_{\\widetilde{\\mathbb{\\theta}}\\left (   t\\right )   , \\infty } \\left (   x , x_{i_{1}}\\right )   _ { |t=0}\\right\\ }   h\\left (   x_{i_{1}}\\right ) \\right]\\ ] ] hence it suffices to show that @xmath836 \\\\ &   = e_{_{\\theta}}\\left [   \\left\\ {   e_{\\theta}\\left\\ {   -k_{f_{x},\\infty}\\left ( x , x_{i_{2}}\\right )   k_{f_{x},\\infty}\\left (   x_{i_{2}},x_{i_{1}}\\right ) s_{i_{2}}\\left (   \\theta\\right )   |x_{i_{1}}\\right\\ }   \\right\\ }   h\\left ( x_{i_{1}}\\right )   \\right]\\end{aligned}\\ ] ] but , by eq.@xmath837 @xmath838 \\\\ &   = e_{_{\\theta}}\\left [   -k_{f_{x},\\infty}\\left (   x , x_{i_{1}}\\right ) s_{i_{1}}\\left (   \\theta\\right )   h\\left (   x_{i_{1}}\\right )   \\right ]   .\\end{aligned}\\ ] ]    *  feasible estimators : * these `` formal '' calculations motivate a `` truncated dirac '' approach to estimate @xmath174 let @xmath839 be a countable sequence of known basis functions with dense span in @xmath840 and define @xmath841 define @xmath842   \\right\\ } ^{-1}\\overline{z}_{k}\\left (   x_{i_{2}}\\right)\\ ] ] to be the projection kernel in @xmath840 onto the subspace @xmath843 spanned by the elements of @xmath844 that is , for any @xmath845@xmath846 \\\\ &   = e_{f_{x}}\\left [   k_{f_{x},k}\\left (   x , x\\right )   h\\left (   x\\right )   \\right ] \\\\ &   = \\overline{z}_{k}\\left (   x\\right )   ^{t}\\left\\ {   e_{f_{x}}\\left [ \\overline{z}_{k}\\left (   x\\ \\right )   \\overline{z}_{k}\\left (   x\\right ) ^{t}\\right ]   \\right\\ }   ^{-1}e_{f_{x}}\\left [   \\overline{z}_{k}\\left ( x\\ \\right )   h\\left (   x\\right )   \\right]\\end{aligned}\\ ] ] then we can view @xmath847 as a truncated at @xmath848 approximation to @xmath849 that is in @xmath806   \\times l_{2}\\left [ f_{x}\\right ]   $ ] and satisfies eq.@xmath803 for all @xmath850 then a natural idea would be to substitute @xmath851{c}-\\varepsilon_{b , i_{1}}\\left (   \\widehat{\\theta}\\right )   e_{\\widehat{\\theta}}\\left [   h_{1}|x_{i_{1}}\\right ]   ^{-\\frac{1}{2}}k_{\\widehat{f}_{x},k}\\left ( x_{i_{1}},x_{i_{2}}\\right ) \\\\",
    "\\times e_{\\widehat{\\theta}}\\left [   h_{1}|x_{i_{2}}\\right ]   ^{-\\frac{1}{2}}\\varepsilon_{p , i_{2}}\\left (   \\widehat{\\theta}\\right ) \\end{array } \\right)\\ ] ] with , for example , @xmath852 for the generalized function @xmath853based on eqs .",
    "[ infin2 ] resulting in the feasible 2nd u - statistic estimator@xmath854 where@xmath855\\ ] ]    to avoid having to do a matrix inversion it is convenient to choose @xmath856 where @xmath857 is a complete orthonormal basis wrt to lebesgue measure in @xmath437 then @xmath858   = i_{k\\times k}$ ] so @xmath859 this choice corresponds to having taken @xmath860 in our formal calculations where @xmath861 is the dirac delta function wrt to lebesgue measure . in that case with @xmath862   $ ] and @xmath863   , $ ] @xmath864 in the appendix , we show one can proceed by induction to formally obtain that for @xmath865@xmath866{c}\\sum_{j=0}^{m-2}c(m , j)\\times\\\\ \\prod\\limits_{s=1}^{j}\\frac{h_{1,i_{s+1}}}{g\\left (   x_{i_{s+1}}\\right ) } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right ) \\\\",
    "\\times k_{leb,\\infty}\\left (   x_{i_{j+1}},x_{i_{m}}\\right ) \\end{array } \\right ]   g\\left (   x_{i_{m}}\\right )   ^{-\\frac{1}{2}}\\varepsilon_{p , i_{m}}\\left (   \\theta\\right ) \\label{imm}\\ ] ] where @xmath867 which we then use to obtain @xmath868and @xmath869    * statistical properties : * we shall prove below that the estimator @xmath870 has variance @xmath871 \\asymp\\left (   \\frac{1}{n}\\max\\left [   1,\\left (   \\frac{k}{n}\\right ) ^{m-1}\\right ]   \\right)\\ ] ] when @xmath872 is a compact wavelet basis@xmath64 ( robins et al .",
    "( 2007 ) proves this result for more general bases ) .",
    "we also prove that the bias @xmath873",
    "-\\psi\\left ( \\theta\\right )   = tb_{k}\\left (   \\theta\\right )   + eb_{m}\\left (   \\theta\\right )   , \\ ] ] of @xmath870 is the sum of a truncation bias term of order @xmath874 ( for a basis @xmath872 that provides optimal rate approximation for hlder balls ) and an estimation bias term of order @xmath875 the truncation bias is of this order only if @xmath139 has smoothness exceeding @xmath876 .",
    "this restriction on @xmath139 is removed later by using kernels based on eq .",
    "note this estimation bias is @xmath878 it gets its name from the fact that , unlike the truncation bias , it would be exactly zero if the initial estimator @xmath391 happened to equal @xmath386 thus , the u - statistic estimator @xmath870 for our functional @xmath175 ( which does not admit a second order influence function ) differs from the u - statistic estimators @xmath352 of eq .",
    "( [ est ] ) for functionals that admit second order influence functions in that , owing to truncation bias , the total bias of @xmath879 is not @xmath880 the choice of @xmath881determines the trade - off between the variance and truncation bias . as @xmath882 with @xmath5 fixed@xmath90 @xmath883 \\rightarrow\\infty$ ] and @xmath884",
    "thus , we can heuristically view the non - existent estimator @xmath885 as the choice of @xmath848 that results in no truncation bias [ and therefore a total bias of @xmath886 $ ] at the expense of an infinite variance .",
    "writing @xmath887 the order of the asymptotic mse of @xmath870 is minimized at the value of @xmath888for which order of the variance equals the order of the sum of the truncation and estimation bias .    [ 1proj]the models of examples 1 - 4 exhibit a spectrum of different likelihood functions and therefore a spectrum of different first order and higher order scores .",
    "nonetheless , because the first order influence functions of the functionals @xmath175 share a common structure , we were able to use part 5c of theorem [ eift ] to formally derive @xmath889 and , thus , the feasible @xmath890 in examples 1 - 4 in a unified manner without needing to consult the full likelihood function for any of the models",
    ". see remark ( [ proj ] )  above for a closely related discussion .    * a critical non - uniqueness : *",
    "we have as yet neglected a critical non - uniqueness in our definition of @xmath891 and thus @xmath870 that poses a significant problem for our `` truncated dirac '' approach .",
    "for instance , when @xmath892 the two generalized @xmath893 kernels @xmath894 of eq [ imm ] and @xmath895{c}\\frac{h_{1,i_{2}}}{g\\left (   x_{i_{2}}\\right )   } k_{leb,\\infty}\\left (   x_{i_{1}},x_{i_{2}}\\right ) \\\\ -e_{\\theta}\\left [   \\frac{k_{leb,\\infty}\\left (   x_{i_{1}},x_{i_{2}}\\right ) } { f\\left (   x_{i_{2}}\\right )   } |x_{i_{1}}\\right ] \\end{array } \\right ] \\\\ &   \\times k_{leb,\\infty}\\left (   x_{i_{1}},x_{i_{3}}\\right )   \\frac { \\varepsilon_{p , i_{3}}\\left (   \\theta\\right )   } { g\\left (   x_{i_{3}}\\right ) ^{\\frac{1}{2}}}\\ ] ] are precisely equal , by eq .",
    "@xmath896 ; nonetheless , upon truncation , they result in different feasible kernels ; @xmath897{c}\\frac{h_{1,i_{2}}}{\\widehat{g}\\left (   x_{i_{2}}\\right )   } k_{leb , k}\\left ( x_{i_{1}},x_{i_{2}}\\right )   k_{leb , k}\\left (   x_{i_{2}},x_{i_{3}}\\right ) \\\\",
    "-k_{leb , k}\\left (   x_{i_{1}},x_{i_{3}}\\right ) \\end{array } \\right ]   \\times\\frac{\\widehat{\\varepsilon}_{p , i_{3}}\\left (   \\theta\\right ) } { \\widehat{g}\\left (   x_{i_{3}}\\right )   ^{\\frac{1}{2}}}\\ ] ] and @xmath898{c}\\frac{h_{1,i_{2}}}{\\widehat{g}\\left (   x_{i_{2}}\\right )   } k_{leb , k}\\left ( x_{i_{1}},x_{i_{2}}\\right ) \\\\ -e_{\\widehat{\\theta}}\\left [   \\frac{k_{leb , k}\\left (   x_{i_{1}},x_{i_{2}}\\right )   } { \\widehat{f}\\left (   x_{i_{2}}\\right )   } |x_{i_{1}}\\right ] \\end{array } \\right ] \\\\ &   \\times k_{leb , k}\\left (   x_{i_{1}},x_{i_{3}}\\right )   \\frac { \\widehat{\\varepsilon}_{p , i_{3}}\\left (   \\theta\\right )   } { \\widehat{g}\\left ( x_{i_{3}}\\right )   ^{\\frac{1}{2}}}\\ ] ] with different orders of bias . for simplicity",
    ", we consider the case where @xmath899 as in examples * 1a*-**1c .  *",
    "* let @xmath900 @xmath901 @xmath902 and @xmath903 then,@xmath904 \\\\ &   = e_{\\theta}\\left [ \\begin{array } [ c]{c}\\frac{\\delta b_{i_{1}}}{\\widehat{f}\\left (   x_{i_{1}}\\right )   ^{\\frac{1}{2}}}\\times\\\\ e_{\\mu}\\left [   \\left (   \\frac{f\\left (   x_{i_{2}}\\right )   } { \\widehat{f}\\left ( x_{i_{2}}\\right )   } -1\\right )   \\overline{\\varphi}_{k}\\left (   x_{i_{2}}\\right ) ^{t}\\right ]   \\overline{\\varphi}_{k}\\left (   x_{i_{1}}\\right ) \\\\",
    "\\times e_{\\theta}\\left [   \\frac{\\delta p_{i_{3}}}{\\widehat{f}\\left (   x_{i_{3}}\\right )   ^{\\frac{1}{2}}}\\overline{\\varphi}_{k}\\left (   x_{i_{3}}\\right ) ^{t}\\right ]   \\overline{\\varphi}_{k}\\left (   x_{i_{1}}\\right ) \\end{array } \\right ] \\\\ &   = e_{\\widehat{\\theta}}\\left [ \\begin{array } [ c]{c}\\left (   \\frac{f\\left (   x_{i_{1}}\\right )   } { \\widehat{f}\\left (   x_{i_{1}}\\right )   } -1 + 1\\right )   \\widehat{f}\\left (   x_{i_{1}}\\right )   ^{\\frac{1}{2}}\\delta b_{i_{1}}\\times\\\\ e_{\\widehat{\\theta}}\\left [   \\left (   \\frac{f\\left (   x_{i_{2}}\\right ) } { \\widehat{f}\\left (   x_{i_{2}}\\right )   } -1\\right )   \\widehat{f}\\left ( x_{i_{2}}\\right )   ^{-\\frac{1}{2}}\\frac{\\overline{\\varphi}_{k}\\left (   x_{i_{2}}\\right )   ^{t}}{\\widehat{f}\\left (   x_{i_{2}}\\right )   ^{\\frac{1}{2}}}\\right ] \\frac{\\overline{\\varphi}_{k}\\left (   x_{i_{1}}\\right )   } { \\widehat{f}\\left ( x_{i_{1}}\\right )   ^{\\frac{1}{2}}}\\\\ \\times e_{\\widehat{\\theta}}\\left [   \\left (   \\frac{f\\left (   x_{i_{3}}\\right ) } { \\widehat{f}\\left (   x_{i_{3}}\\right )   } -1 + 1\\right )   \\delta p_{i_{3}}\\frac{\\overline{\\varphi}_{k}\\left (   x_{i_{3}}\\right )   ^{t}}{\\widehat{f}\\left ( x_{i_{3}}\\right )   ^{\\frac{1}{2}}}\\right ]   \\frac{\\overline{\\varphi}_{k}\\left ( x_{i_{1}}\\right )   } { \\widehat{f}\\left (   x_{i_{1}}\\right )   ^{\\frac{1}{2}}}\\end{array } \\right ] \\\\ &   = e_{\\widehat{\\theta}}\\left [ \\begin{array } [ c]{c}\\widehat{f}\\left (   x_{i_{1}}\\right )   ^{\\frac{1}{2}}\\delta b_{i_{1}}e_{\\widehat{\\theta}}\\left [   \\delta f\\left (   x_{i_{2}}\\right )   \\widehat{f}\\left (   x_{i_{2}}\\right )   ^{-\\frac{1}{2}}\\overline{z}_{k , i_{2}}^{t}\\right ] \\overline{z}_{k , i_{1}}\\\\ \\times e_{\\widehat{\\theta}}\\left [   \\delta p_{i_{3}}\\overline{z}_{k , i_{3}}^{t}\\right ]   \\overline{z}_{k , i_{1}}\\end{array } \\right ] \\\\ &   + o_{p}\\left (   \\left\\ {   b-\\widehat{b}\\right\\ }   \\left\\ {   p-\\widehat{p}\\right\\ }   \\left\\ {   g-\\widehat{g}\\right\\ }   ^{2}\\right)\\end{aligned}\\ ] ] and@xmath905 \\\\ &   = e_{\\mu}\\left [ \\begin{array } [ c]{c}e_{\\theta}\\left [   \\frac{\\delta b_{i_{1}}}{\\widehat{f}\\left (   x_{i_{1}}\\right ) ^{\\frac{1}{2}}}\\overline{\\varphi}_{k}\\left (   x_{i_{1}}\\right )   ^{t}\\right ] \\overline{\\varphi}_{k}\\left (   x_{i_{2}}\\right )   \\left (   \\frac{f\\left ( x_{i_{2}}\\right )   } { \\widehat{f}\\left (   x_{i_{2}}\\right )   } -1\\right ) \\\\ \\times\\overline{\\varphi}_{k}\\left (   x_{i_{2}}\\right )   ^{t}e_{\\theta}\\left [ \\frac{\\delta p_{i_{3}}}{\\widehat{f}\\left (   x_{i_{3}}\\right )   ^{\\frac{1}{2}}}\\overline{\\varphi}_{k}\\left (   x_{i_{3}}\\right )   \\right ] \\end{array } \\right ] \\\\ &   = e_{\\widehat{\\theta}}\\left [ \\begin{array } [ c]{c}e_{\\widehat{\\theta}}\\left [   \\left (   \\delta f\\left (   x_{i_{1}}\\right ) + 1\\right )   \\delta b_{i_{1}}\\overline{z}_{k , i_{1}}^{t}\\right ] \\\\",
    "\\times\\overline{z}_{k , i_{2}}\\left (   \\frac{f\\left (   x_{i_{2}}\\right ) } { \\widehat{f}\\left (   x_{i_{2}}\\right )   } -1\\right ) \\\\",
    "\\times\\overline{z}_{k , i_{2}}^{t}e_{\\widehat{\\theta}}\\left [   \\left (   \\delta f\\left (   x_{i_{3}}\\right )   + 1\\right )   \\delta p_{i_{3}}\\overline{z}_{k , i_{3}}\\right ] \\end{array } \\right ] \\\\ &   = e_{\\widehat{\\theta}}\\left [ \\begin{array } [ c]{c}e_{\\widehat{\\theta}}\\left [   \\delta b_{i_{1}}\\overline{z}_{k , i_{1}}^{t}\\right ] \\overline{z}_{k , i_{2}}\\left (   \\frac{f\\left (   x_{i_{2}}\\right )   } { \\widehat{f}\\left (   x_{i_{2}}\\right )   } -1\\right ) \\\\",
    "\\times\\overline{z}_{k , i_{2}}^{t}e_{\\widehat{\\theta}}\\left [   \\delta p_{i_{3}}\\overline{z}_{k , i_{3}}\\right ] \\end{array } \\right ] \\\\ &   + o_{p}\\left (   \\left\\ {   b-\\widehat{b}\\right\\ }   \\left\\ {   p-\\widehat{p}\\right\\ }   \\left\\ {   g-\\widehat{g}\\right\\ }   ^{2}\\right)\\end{aligned}\\ ] ] that is , @xmath904   -e_{\\theta}\\left [   if_{3,3,\\psi , i_{1},i_{2},i_{3}}^{\\left (   k\\right )   } \\left (   \\widehat{\\theta}\\right ) \\right ] \\\\ &   = e_{\\widehat{\\theta}}\\left [ \\begin{array } [ c]{c}\\widehat{f}\\left (   x_{i_{1}}\\right )   ^{\\frac{1}{2}}\\delta b_{i_{1}}e_{\\widehat{\\theta}}\\left [   \\delta f\\left (   x_{i_{2}}\\right )   \\widehat{f}\\left (   x_{i_{2}}\\right )   ^{-\\frac{1}{2}}\\overline{z}_{k , i_{2}}^{t}\\right ] \\overline{z}_{k , i_{1}}\\\\ e_{\\widehat{\\theta}}\\left [   \\delta p_{i_{3}}\\overline{z}_{k , i_{3}}^{t}\\right ] \\overline{z}_{k , i_{1}}\\end{array } \\right ] \\\\ &   -e_{\\widehat{\\theta}}\\left [ \\begin{array } [ c]{c}e_{\\widehat{\\theta}}\\left [   \\delta b_{i_{1}}\\overline{z}_{k , i_{1}}^{t}\\right ] \\overline{z}_{k , i_{2}}\\delta f\\left (   x_{i_{2}}\\right ) \\\\ \\times\\overline{z}_{k , i_{2}}^{t}e_{\\widehat{\\theta}}\\left [   \\delta p_{i_{3}}\\overline{z}_{k , i_{3}}\\right ] \\end{array } \\right ] \\\\ &   + o_{p}\\left (   \\left\\ {   b-\\widehat{b}\\right\\ }   \\left\\ {   p-\\widehat{p}\\right\\ }   \\left\\ {   g-\\widehat{g}\\right\\ }   ^{2}\\right)\\end{aligned}\\]]@xmath906@xmath907   \\widehat{f}\\left (   x\\right )   ^{\\frac{1}{2}}\\delta b\\pi_{\\widehat{\\theta}}\\left [   \\delta f\\left (   x\\right )   \\widehat{f}\\left ( x\\right )   ^{-\\frac{1}{2}}|\\overline{z}_{k}\\right ]   \\right ] \\\\ &   -e_{\\widehat{\\theta}}\\left [   \\pi_{\\widehat{\\theta}}\\left [   \\delta p|\\overline{z}_{k}\\right ]   \\widehat{f}\\left (   x\\right )   ^{\\frac{1}{2}}\\delta f\\left (   x\\right )   \\widehat{f}\\left (   x\\right )   ^{-\\frac{1}{2}}\\pi _ { \\widehat{\\theta}}\\left [   \\delta b|\\overline{z}_{k}\\right ]   \\right ] \\\\ &   + o_{p}\\left (   \\left\\ {   b-\\widehat{b}\\right\\ }   \\left\\ {   p-\\widehat{p}\\right\\ }   \\left\\ {   g-\\widehat{g}\\right\\ }   ^{2}\\right)\\end{aligned}\\]]@xmath906@xmath908{c}\\pi_{\\widehat{\\theta}}\\left [   \\delta p|\\overline{z}_{k}\\right ]   \\widehat{f}\\left (   x\\right )   ^{\\frac{1}{2}}\\times\\\\ \\left\\ { \\begin{array } [ c]{c}\\pi_{\\widehat{\\theta}}^{\\bot}\\left [   \\delta b|\\overline{z}_{k}\\right ] \\pi_{\\widehat{\\theta}}\\left [   \\delta f\\left (   x\\right )   \\widehat{f}\\left ( x\\right )   ^{-\\frac{1}{2}}|\\overline{z}_{k}\\right ] \\\\ -\\pi_{\\widehat{\\theta}}\\left [   \\delta b|\\overline{z}_{k}\\right ] \\pi_{\\widehat{\\theta}}^{\\bot}\\left [   \\delta f\\left (   x\\right )   \\widehat{f}\\left (   x\\right )   ^{-\\frac{1}{2}}|\\overline{z}_{k}\\right ] \\end{array } \\right\\ } \\end{array } \\right ] \\\\ &   + o_{p}\\left (   \\left\\ {   b-\\widehat{b}\\right\\ }   \\left\\ {   p-\\widehat{p}\\right\\ }   \\left\\ {   g-\\widehat{g}\\right\\ }   ^{2}\\right)\\end{aligned}\\ ] ] where @xmath909   $ ] and @xmath910 in @xmath911 of @xmath912 on the @xmath848 dimensional linear subspace @xmath913 spanned by the components of the vector @xmath914 and the projection on the orthocomplement of this subspace .",
    "since the basis @xmath872 provides optimal rate approximation for hlder balls , it is easy to verify that the difference is of order @xmath915{c}n^{-\\frac{\\beta_{p}/d}{1 + 2\\beta_{p}/d}-\\frac{\\beta g / d}{1 + 2\\beta_{g}/d}}k^{-\\beta_{b}/d}+n^{-\\frac{\\beta_{p}/d}{1 + 2\\beta_{p}/d}-\\frac{\\beta_{b}/d}{1 + 2\\beta_{b}/d}}k^{-\\beta_{g}/d}\\\\ + n^{-\\frac{\\beta_{p}/d}{1 +",
    "2\\beta_{p}/d}-\\frac{\\beta_{b}/d}{1 + 2\\beta_{b}/d}-\\frac{2\\beta g / d}{1 + 2\\beta_{g}/d}}\\end{array } \\right)\\ ] ] provided @xmath139 has smoothness exceeding @xmath916    for concreteness , we shall look at an example .",
    "suppose @xmath917 and @xmath918 thus , by choosing @xmath919 @xmath920 converges to @xmath270 at rate @xmath921 in contrast , the order , @xmath922 of the optimal root mean squares error of @xmath923that uses @xmath924 is @xmath925  thus @xmath926converges to @xmath175 at a slower rate than @xmath927 which uses @xmath928 nothing in our development up to this point provides any guidance as to which of the many equivalent generalized u - statistic kernels should be selected for truncation . to provide some guidance",
    ", we introduce an alternative approach to the estimation of @xmath175 based on truncated parameters that admit higher order influence functions .",
    "the class of estimators we derive using this alternative approach includes members algebraically identical to the estimators @xmath870 but does not include estimators equivalent to less efficient estimators such as @xmath929 .    * an approach based on truncated parameters : * we introduce a class of truncated parameters @xmath930 that ( i ) depend on the sample size through a positive integer index @xmath931which we refer to as the truncation index and will be optimized below ) , ( ii ) have influence functions @xmath932 of all orders @xmath135 , ( iii ) equals @xmath270 on a large subset @xmath933 of @xmath245 and ( iv ) the initial estimator @xmath934  is an element of @xmath935so that the plug - ins @xmath936and @xmath937  are equal . to prepare we introduce a simplified notation . for functions",
    "@xmath938 or @xmath939 of @xmath940 we will often write @xmath941 and @xmath942 as @xmath943 and @xmath944 and @xmath945   $ ] as @xmath946   $ ] . similarly , we often write @xmath947 and @xmath948 as @xmath949and @xmath950 and @xmath951   $ ] as @xmath952   .$ ] further we shall introduce slightly different definitions of truncation and estimation bias .",
    "define the estimator @xmath953or , equivalently , @xmath954 .  then the conditional bias @xmath955",
    "-\\psi$ ] of @xmath956 is @xmath957 where the truncation bias @xmath958 is zero for @xmath959 and does not depend on @xmath135 and the estimation bias @xmath960 -\\widetilde{\\psi}_{k}$ ] is @xmath961 does not depend on @xmath962 we will abbreviate @xmath963 as @xmath964 suppressing the dependence on @xmath965 under minimal conditions , the conditional variance of @xmath956 is of the order of @xmath966   $ ] whenever @xmath967 the rate of convergence of @xmath956 to @xmath271 can depend on the choice of @xmath968  nevertheless , many different choices @xmath969 result in estimators @xmath956 that achieve what we conjecture to be the optimal rate for estimators in our class of the form @xmath956 .",
    "we choose , among all such @xmath970 the class that minimizes the computational complexity of @xmath971 specifically for all @xmath969 in our chosen class and all @xmath972 @xmath973 consists of a single term rather than a sum of many terms .",
    "we conjecture this appealing property does not hold for any @xmath969 outside our class .",
    "we now describe this choice .",
    "the parameter @xmath974is defined in terms of @xmath975dimensional working linear parametric submodels for @xmath976 and @xmath130 depending on unknown parameters @xmath977and @xmath978 through the basepoints @xmath979 and @xmath980 where @xmath979 and @xmath981 are initial estimators from the training sample .",
    "specifically let @xmath982 and @xmath983 be arbitrary bounded known functions chosen by the analyst satisfying eqs @xmath984 below .",
    "@xmath985 \\geq0\\text { } w.p.1\\label{dot1}\\\\ \\left\\vert \\left\\vert \\frac{\\dot{p}\\left (   x\\right )   } { \\dot{b}\\left ( x\\right )   } \\right\\vert \\right\\vert _ { \\infty}<c^{\\ast},\\left\\vert \\left\\vert \\frac{\\dot{b}\\left (   x\\right )   } { \\dot{p}\\left (   x\\right )   } \\right\\vert \\right\\vert _ { \\infty}<c^{\\ast}\\label{dot2}\\\\ \\frac{\\dot{p}\\left (   x\\right )   } { \\dot{b}\\left (   x\\right )   } \\text { has at least } \\left\\lceil max\\left\\ {   \\beta_{b},\\ \\beta_{p}\\right\\ }   \\right\\rceil \\text { derivatives}\\label{dot3}\\ ] ] particular choices of @xmath982 and @xmath986 can make the form of @xmath987 more aesthetic .",
    "the choice has no bearing on the rate of convergence of the estimator @xmath956 to @xmath988 often there are fairly natural choices for @xmath989 and @xmath990 see remark [ q ] below for examples@xmath64 let @xmath991 be @xmath992vectors of unknown parameters and consider the working linear models @xmath993    we define the parameters @xmath994 and @xmath995 respectively to be the solution",
    "to@xmath996   = e_{\\theta}\\left [   \\left\\ { h_{1}b^{\\ast}\\left (   x,\\overline{\\eta}_{k}\\right )   + h_{3}\\right\\ }   \\dot { p}\\overline{z}_{k}\\right ] \\label{eta}\\\\ 0   &   = e_{\\theta}\\left [   \\partial h\\left (   b^{\\ast}\\left (   x,\\overline{\\eta } _ { k}\\right )   , p^{\\ast}\\left (   x,\\overline{\\alpha}_{k}\\right )   \\right ) /\\partial\\overline{\\eta}_{k}\\right ]   = e_{\\theta}\\left [   \\left\\ {   h_{1}p^{\\ast } \\left (   x,\\overline{\\alpha}_{k}\\right )   + h_{2}\\right\\ }   \\dot{b}\\overline { z}_{k}\\right ]   .\\label{alpha}\\ ] ]    the solution to @xmath997 and @xmath998 exist in closed form as @xmath999   ^{-1}e_{\\theta}\\left [   \\overline{z}_{k}\\dot{p}\\left\\ {   h_{1}\\widehat{b}+h_{3}\\right\\ }   \\right ] \\label{eta1}\\\\ \\widetilde{\\overline{\\alpha}}_{k}\\left (   \\theta\\right )    &   = -e_{\\theta } \\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] ^{-1}e_{\\theta}\\left [   \\overline{z}_{k}\\dot{b}\\left\\ {   h_{1}\\widehat{p}+h_{2}\\right\\ }   \\right ]   .\\label{alpha1}\\ ] ]    next define @xmath1000  and @xmath1001and @xmath1002\\ ] ]    note the models @xmath1003 and @xmath1004 are used only to define the truncated parameter @xmath1005 they are not assumed to be correctly specified . in particular , the training sample estimates @xmath1006 @xmath1007 need not be based on the models @xmath1003,@xmath1008we now compare our truncated parameter @xmath930 with @xmath1009 and calculate the truncation bias .",
    "it is important to keep in mind that @xmath1010 are components of the unknown @xmath785while @xmath1011,@xmath1012are regarded as known functions .",
    "if our model satisfies @xmath1013 @xmath70and @xmath1014 then @xmath1015    further @xmath1016   $ ]    immediate from theorem [ dr ] and lemma [ conde ] .    we know from the above theorem that @xmath1017 for @xmath1018 however to control the truncation bias in forming confidence intervals for @xmath175 we will need to know how fast sup@xmath1019 decreases as @xmath848 increases@xmath1020the following theorem is a key step towards determining an upper bound .    [",
    "tbformula]suppose @xmath983 and @xmath1021 are chosen so that @xmath1022   \\geq0 $ ] wp1 .",
    "let @xmath1023 \\right\\ }   ^{1/2}\\ ] ] and @xmath1024   $ ] and @xmath1025   $ ] be , respectively , the projection in @xmath1026 of @xmath912 on the @xmath848 dimensional linear subspace @xmath1027 spanned by the components of the vector @xmath1028 and the projection on the orthocomplement of this subspace .",
    "then if @xmath1013 are satisfied @xmath90@xmath1029   \\pi^{\\perp}\\left [   \\left ( \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   q|q\\overline{z}_{k}\\right ]   \\right]\\ ] ]    [ q ] to simplify various formulae it is often convenient and aesthetically pleasing to have @xmath1030 we can choose @xmath1031 and @xmath1032 to guarantee @xmath1033 wp1@xmath64 for the functional @xmath1034   $ ] of example 1a , @xmath1035 wp1 . thus choosing @xmath1031 and @xmath1036 equal to @xmath18 and @xmath1037 , respectively , @xmath1038 makes @xmath1033 wp1 . in the missing data example 2a , the function @xmath1039",
    "so @xmath1040   = 1/\\widehat{p}$ ] and thus the choice @xmath1041makes @xmath1033 wp1 . note since inference on @xmath175 is conditional on the training sample data , we view the initial estimator @xmath979 of @xmath976 from the training sample as known and thus an analyst is free to choose @xmath1032 to be @xmath1042    *  examples continued .",
    "* in * example * * 1a , * recall @xmath1043   .$ ] choose @xmath1044 @xmath1038 so @xmath1045 and take @xmath1046 then @xmath1047   , \\\\ tb_{k }   &   = e\\left\\ {   \\left [   \\pi^{\\perp}\\left [   b|\\overline{z}_{k}\\right ] \\pi^{\\perp}\\left [   p|\\overline{z}_{k}\\right ]   \\right ]   \\right\\ }   , \\\\ \\widetilde{\\psi}_{k }   &   = \\psi - tb_{k}=e\\left\\ {   \\pi\\left [   b|\\overline{z}_{k}\\right ]   \\pi\\left [   p|\\overline{z}_{k}\\right ]   \\right\\}\\end{aligned}\\ ] ]    thus @xmath969 appears to be the natural choice for a truncated parameter .    in the * example 2a * with @xmath1048   , \\dot{b}=-1,\\dot { p}=\\widehat{p}=1/\\widehat{\\pi},$ ]",
    "@xmath1049 we obtain@xmath1050{c}\\pi^{\\perp}\\left [   \\widehat{\\pi}\\left (   \\frac{1}{\\pi}-\\frac{1}{\\widehat{\\pi}}\\right )   \\left\\ {   \\frac{\\pi}{\\widehat{\\pi}}\\right\\ }   ^{1/2}|\\left\\ { \\frac{\\pi}{\\widehat{\\pi}}\\right\\ }   ^{1/2}\\overline{z}_{k}\\right ] \\\\ \\times\\pi^{\\perp}\\left [   \\left\\ {   \\frac{\\pi}{\\widehat{\\pi}}\\right\\ } ^{1/2}\\left (   b-\\widehat{b}\\right )   |\\left\\ {   \\frac{\\pi}{\\widehat{\\pi}}\\right\\ }   ^{1/2}\\overline{z}_{k}\\right ] \\end{array } \\right]\\ ] ]    thus the truncated parameter @xmath1051 does not seem to be a particular natural or obvious choice .",
    "the complexity of @xmath969 is not simply due to the fact that we chose @xmath1052 rather than @xmath1053 as we now demonstrate .    in * example 2a * with @xmath1054 @xmath1055@xmath1050{c}\\pi^{\\perp}\\left [   \\left (   \\frac{1}{\\pi}-\\frac{1}{\\widehat{\\pi}}\\right ) \\pi^{1/2}|\\pi^{1/2}\\overline{z}_{k}\\right ]   \\times\\\\ \\pi^{\\perp}\\left [   \\left\\ {   \\frac{\\pi}{\\widehat{\\pi}}\\right\\ }   ^{1/2}\\left ( b-\\widehat{b}\\right )   |\\pi^{1/2}\\overline{z}_{k}\\right ] \\end{array } \\right]\\ ] ]    nonetheless we will see that , for either choice of @xmath1056 the parameter @xmath969 will result in estimators with good properties .",
    "henceforth , given @xmath1057 @xmath1058 will always denote a complete orthonormal basis wrt to lebesgue measure in @xmath1059 or in the unit cube in @xmath1059 that provides optimal rate approximation for hlder balls @xmath1060 , i.e.@xmath1061 the basis consisting of @xmath1062 tensor products of univariate orthonormal polynomials satisfies @xmath1063 for all @xmath1064 the basis consisting of @xmath1062 tensor products of a univariate daubechies compact wavelet basis with mother wavelet @xmath1065 satisfying @xmath1066 also satisfies @xmath1063 for @xmath1067    [ tbrate]suppose that @xmath512 are satisfied , that @xmath986 and @xmath982 satisfy @xmath1068 and in the remainder of the paper , unless stated otherwise , we take @xmath1069 where recall that @xmath1070   \\right\\ }   .$ ] then @xmath1071      we begin by proving that the first order influence functions of @xmath974and @xmath271 are identical except with @xmath1072 replacing @xmath1073    [ foif]@xmath1074\\ ] ] with@xmath1075    since @xmath1076   $ ] , @xmath1077   if_{1,\\widetilde{\\overline{\\eta}}_{k}\\left (   \\cdot\\right ) } \\left (   \\theta\\right ) \\\\ &   + e\\left [   h\\left (   b^{\\ast}\\left (   x,\\widetilde{\\overline{\\eta}}_{k}\\left ( \\theta\\right )   \\right )   , p^{\\ast}\\left (   x,\\widetilde{\\overline{\\alpha}}_{k}\\left (   \\theta\\right )",
    "\\right )   \\right )   /\\partial\\overline{\\alpha}_{k}^{t}\\right ]   if_{1,\\widetilde{\\overline{\\alpha}}_{k}\\left (   \\cdot\\right ) } \\left (   \\theta\\right)\\end{aligned}\\ ] ] but , by definition of @xmath1078 and @xmath1079 both expectations are zero .",
    "note that @xmath1078 and @xmath1080 are not maximizers of the expected log - likelihood for @xmath1081 and @xmath1082 this choice was deliberate . had we defined @xmath1083 and @xmath1080 as the maximizers of the expected log - likelihood , then @xmath1084 would have had additional terms since the expectations in the preceding proof would not be zero .",
    "the existence of these extra terms would translate to many extra terms in @xmath932 for large @xmath135 leading to computational difficulties .",
    "similarly had we chosen models @xmath1085 and @xmath1086 with @xmath1087 a non - linear inverse - link function , @xmath932 would also have had many extra terms without an improvement in the rate of convergence .",
    "the following is proved in the appendix .",
    "[ drhoif]@xmath1088 where @xmath973=@xmath1089   $ ] is a jth order degenerate @xmath187statistic given by @xmath1090{c}\\left [   \\left (   h_{1}\\widetilde{p}+h_{2}\\right )   \\dot{b}\\overline{z}_{k}^{t}\\right ]   _ { i_{1}}\\left\\ {   e\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\\\ \\times\\left [   \\overline{z}_{k}\\left (   h_{1}\\widetilde{b}+h_{3}\\right )   \\dot { p}\\right ]   _ { i_{2}}\\end{array } \\right\\ } \\\\ if_{jj,\\widetilde{\\psi}_{k},\\overline{i}_{j } }   &   = \\left (   -1\\right ) ^{j-1}\\left [   \\left (   h_{1}\\widetilde{p}+h_{2}\\right )   \\dot{b}\\overline{z}_{k}^{t}\\right ]   _ { i_{1}}\\\\ \\times &   \\left [ \\begin{array } [ c]{c}{\\displaystyle\\prod\\limits_{s=3}^{j } } \\left\\ {   e\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\\\ \\left\\ {   \\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right )   _ { i_{s}}-e\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline { z}_{k}^{t}\\right ]   \\right\\ } \\end{array } \\right ] \\\\ &   \\times\\left\\ {   e\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left [   \\overline{z}_{k}\\left (   h_{1}\\widetilde{b}+h_{3}\\right )   \\dot{p}\\right ]   _ { i_{2}}\\ ] ]      we can now calculate the estimator @xmath1092by substitution of @xmath391 for @xmath179 in @xmath1093 to obtain the following .",
    "suppose @xmath877 holds .",
    "then @xmath1094 where @xmath1095   _ { i_{1}}\\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right )   \\dot { p}\\right ]   _ { i_{2}}\\\\ \\widehat{if}_{jj,\\widetilde{\\psi}_{k},\\overline{i}_{j } } &   = \\left (   -1\\right ) ^{j-1}\\left\\ { \\begin{array } [ c]{c}\\left [   \\left (   h_{1}\\widehat{p}+h_{2}\\right )   \\dot{b}\\overline{z}_{k}^{t}\\right ]   _ { i_{1}}\\left [ { \\displaystyle\\prod\\limits_{s=3}^{j } } \\left\\ { \\begin{array } [ c]{c}\\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ) _ { i_{s}}\\\\ -i_{k\\times k}\\end{array } \\right\\ }   \\right ] \\\\ \\times\\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right )   \\dot { p}\\right ]   _ { i_{2}}\\end{array } \\right\\}\\end{aligned}\\ ] ]    by lemma [ conde ]",
    "@xmath1096   = e_{\\widehat{\\theta } } \\left [   \\left\\ {   h_{1}\\widehat{p}+h_{2}\\right\\ }   \\dot{b}\\overline{z}_{k}\\right ]   = 0.$ ] thus by eqs .",
    "@xmath1097and @xmath1098 @xmath1099= @xmath1100 so @xmath1101  and @xmath1102 .",
    "further , by eq.@xmath1103 @xmath1104   = \\widehat{e}\\left [   \\dot { p}\\dot{b}\\widehat{e}\\left [   h_{1}|x\\right ]   \\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   = \\widehat{e}\\left [   \\widehat{q}^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   = \\int\\overline{\\varphi}_{k}\\left (   x\\right ) \\overline{\\varphi}_{k}\\left (   x\\right )   ^{t}=i_{k\\times k}$ ] .",
    "it follows that by our judicious choice of @xmath1105 in eq.@xmath1106 we have avoided the need to invert a @xmath1107 matrix to compute @xmath971    the reader can easily check that when we take @xmath1108   \\right\\ } ^{1/2}\\ ] ] @xmath1109 and @xmath792 wp 1@xmath90 @xmath1110 is precisely the same as @xmath1111 of equation ( [ 22 ] ) in section 3.2.1 .    to make our procedures less abstract , we provide explicit expressions for @xmath1112 in examples 1a and 2a .",
    "* example 1a * * continued : * @xmath1043   , $ ] @xmath1113 @xmath1114 @xmath1115 @xmath1116   = i_{k\\times k}.$ ] then @xmath1117 and thus @xmath1118   _ { i_{1}}\\left [ \\overline{z}_{k}\\left (   y-\\widehat{b}\\right )   \\right ]   _ { i_{2}}\\\\ \\widehat{if}_{jj,\\widetilde{\\psi}_{k},\\overline{i}_{j } } &   = \\left (   -1\\right ) ^{j-1}\\left [   \\left (   a-\\widehat{p}\\right )   \\overline{z}_{k}^{t}\\right ] _ { i_{1}}\\left [ { \\displaystyle\\prod\\limits_{s=3}^{j } } \\left\\ {   \\overline{z}_{ki_{s}}\\overline{z}_{ki_{s}}^{t}-i_{k\\times k}\\right\\ } \\right ]   \\left [   \\overline{z}_{k}\\left (   y-\\widehat{b}\\right )   \\right ] _ { i_{2}}\\ ] ]    * example 2a continued : * @xmath1119 @xmath1120 , @xmath1048   , $ ] @xmath1121 and @xmath1122 so @xmath1123   = i_{k\\times k}.$ ]    then @xmath1124 @xmath1125 so@xmath1126   _ { i_{1}}\\left [ \\overline{z}_{k}\\frac{a}{\\widehat{\\pi}}\\left (   y-\\widehat{b}\\right )   \\right ] _ { i_{2}}\\\\ \\widehat{if}_{jj,\\widetilde{\\psi}_{k},\\overline{i}_{j } }   &   = \\left ( -1\\right )   ^{j-1}\\left [   \\left (   \\frac{a}{\\widehat{\\pi}}-1\\right ) \\overline{z}_{k}^{t}\\right ]   _ { i_{1}}\\left [ { \\displaystyle\\prod\\limits_{s=3}^{j } } \\left\\ {   \\frac{a}{\\widehat{\\pi}}\\overline{z}_{k}\\overline{z}_{k}^{t}-i_{k\\times k}\\right\\ }   _ { i_{s}}\\right ]   \\left [   \\overline{z}_{k}\\frac { a}{\\widehat{\\pi}}\\left (   y-\\widehat{b}\\right )   \\right ]   _ { i_{2}}\\ ] ]    consider example 2a with @xmath1127 , @xmath1128   = i_{k\\times k},$ ] @xmath1129   , $ ] @xmath1125 so @xmath1130   _ { i_{1}}\\left [ \\overline{z}_{k}a\\left (   y-\\widehat{b}\\right )   \\right ]   _ { i_{2}}\\\\ \\widehat{if}_{jj,\\widetilde{\\psi}_{k},\\overline{i}_{j } } &   = \\left (   -1\\right ) ^{j-1}\\left [   \\left (   \\frac{a}{\\widehat{\\pi}}-1\\right )   \\overline{z}_{k}^{t}\\right ]   _ { i_{1}}\\left [ { \\displaystyle\\prod\\limits_{s=3}^{j } } \\left\\ {   a\\overline{z}_{k}\\overline{z}_{k}^{t}-i_{k\\times k}\\right\\ }   _ { i_{s}}\\right ]   \\left [   \\overline{z}_{k}a\\left (   y-\\widehat{b}\\right )   \\right ] _ { i_{2}}\\ ] ]    our next theorem , proved in the appendix , derives the estimation bias @xmath1131   -\\widetilde{\\psi}_{k}.$ ]    [ ebrate]suppose @xmath1132 and @xmath512 hold then@xmath1133{c}e\\left [   q^{2}\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\overline{z}_{k}^{t}\\right ]   \\left\\ {   e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   -i_{k\\times k}\\right\\ }   ^{m-1}\\\\ \\times\\left\\ {   e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] \\right\\ }   ^{-1}e\\left [   \\overline{z}_{k}q^{2}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   \\right ] \\end{array } \\right\\ } \\label{eb3}\\\\    \\leq\\left\\ { \\begin{array } [ c]{c}\\left\\vert \\left\\vert \\left\\ {   \\frac{\\dot{b}}{\\dot{p}}g\\right\\ } ^{1/2}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\left\\ { \\frac{\\dot{p}}{\\dot{b}}g\\right\\ }   ^{1/2}\\right\\vert \\right\\vert _ { \\infty } \\left\\vert \\left\\vert \\delta g\\right\\vert \\right\\vert _ { \\infty}^{m-1}\\left ( 1+o_{p}\\left (   1\\right )   \\right )   \\times\\\\ \\left\\ {   \\int\\left (   p\\left (   x\\right )   -\\widehat{p}\\left (   x\\right )   \\right ) ^{2}dx\\right\\ }   ^{1/2}\\left\\ {   \\int\\left (   b\\left (   x\\right )   -\\widehat{b}\\left (   x\\right )   \\right )   ^{2}dx\\right\\ }   ^{1/2}\\end{array } \\right\\ } \\label{eb4}\\\\ = o_{p}\\left (   \\left (   \\frac{\\log n}{n}\\right )   ^{\\frac{\\left (   m-1\\right ) \\beta_{g}}{d+2\\beta_{g}}}n^{-\\left (   \\frac{\\beta_{b}}{d+2\\beta_{b}}+\\frac{\\beta_{p}}{d+2\\beta_{p}}\\right )   } \\right ) \\label{eb6}\\ ] ]    for @xmath1134  where @xmath1135    [ pnorm]at the cost of a longer proof we could have used hlder s inequality repeatedly to control @xmath1136 in the @xmath7 norm @xmath1137 with @xmath1138 to show that @xmath1139 thus , @xmath1140 is @xmath1141 consistent with the form of the bias given in our fundamental theorem [ eiet ] .    * an alternate derivation of * @xmath971 the above derivation of @xmath956 required that one have facility in calculating higher order influence functions @xmath1142as done in the proof of theorem [ drhoif ] in the appendix .",
    "however , there exists an alternate derivation of @xmath956 that does not require one learn how to calculate influence functions . specifically , we know from theorems [ eiet ] and [ eift ] that in a ( locally ) nonparametric model @xmath1143 is the unique @xmath465 order u - statistic that is degenerate under @xmath391 and satisfies @xmath1144   \\equiv eb_{j}=o_{p}\\left (   \\left\\vert \\left\\vert \\widehat{\\theta}-\\theta\\right\\vert \\right\\vert ^{j+1}\\right)\\ ] ]    with @xmath1145 -\\widetilde{\\psi}_{k}.$ ] in fact , we first derived @xmath956 by beginning with @xmath1146 , calculating @xmath1145   -\\widetilde{\\psi}_{k},$ ] and then , recursively for @xmath1147 finding @xmath1148 satisfying the above equation . for explicit details",
    "see the appendix.in fact if one did not even know how to derive @xmath1149 one could begin the recursion by obtaining @xmath1150 as the unique first order u - statistic with mean zero under @xmath391 satisfying @xmath1151   = o_{p}\\left (   \\left\\vert \\left\\vert \\widehat{\\theta}-\\theta \\right\\vert \\right\\vert ^{2}\\right )   $ ] .      in this section ,",
    "we derive the order of the variance of @xmath1152 when the orthonormal system @xmath1153 used to construct our @xmath1154-statistics are a compact wavelet basis .",
    "first consider the case where @xmath8 is univariate ; without loss of generality , assume that @xmath1155.$ ] because we are primarily interested in convergence rates , the fact that @xmath8 may not follow the uniform distribution will not affect the rate results given below , but can influence the size of the constants .",
    "we use @xmath1156 in place of @xmath1157 to indicate univariate basis functions .",
    "let @xmath1158 @xmath848 be integer powers of two with @xmath1159  .",
    "denote by @xmath1160 the @xmath992 dimensional basis vector whose first @xmath1161 components @xmath1162 are the @xmath1163vector of level @xmath1164 scaled and translated versions of a compactly supported father wavelet ( mallat , 1998 ) and whose last @xmath1165 components @xmath1166 are the associated compact mother wavelets between levels @xmath1164 and @xmath1167 . in particular , one may use periodic wavelets , folded wavelets or daubechies boundary wavelets with enough vanishing moments to obtain the optimal approximation rate of @xmath1168 for @xmath1169 @xmath64  the multiresolution analysis ( mra ) property of wavelets allows us to decompose the vector space spanned by the @xmath1170-level father wavelets @xmath1171 into the direct sum of the subspace spanned by @xmath1172-level father wavelets @xmath1173 and the span of mother wavelets for each level between @xmath1172 and @xmath1174 which we respectively write as @xmath1175 then for any integer @xmath212 with @xmath1176 we have@xmath1177 as @xmath1178 the resulting basis system is dense in @xmath1179 @xmath1180 since , in fact , @xmath8 is @xmath676dimensional we require a generalization that allows for multivariate tensor wavelet basis functions .",
    "in fact , suppose @xmath1181 is now multivariate , and we again assume @xmath1182 on @xmath436   ^{d}.$ ]  given @xmath11 univariate vector spaces @xmath1183 respectively spanned by vectors @xmath1184 so that for @xmath1185@xmath1186 and @xmath1187 one may define @xmath11 dimensional tensor vector spaces @xmath1188 such that@xmath1189 where for @xmath1190 @xmath1191 as @xmath1178 the resulting tensor basis system is dense in @xmath1192     next , suppose that we have a set of multivariate basis functions @xmath1193 such that for each @xmath1194 @xmath1195 spans @xmath1196 where @xmath1197 .",
    "define @xmath1198 as the @xmath123 norm with respect to the lebesgue measure .",
    "the following theorem is key to our derivation of the order of the variance of @xmath956    [ var_multi]for @xmath1199 @xmath1200    the following theorem is an immediate consequence of theorem ( [ var_multi ] ) obtained by taking @xmath1201  ( which implies we use the father wavelets at level log@xmath1202but no mother wavelets . )",
    "[ var_if]for all @xmath383 @xmath1203   \\asymp\\frac{1}{n}\\]]@xmath1204   \\asymp\\left (   \\frac{1}{n}\\max\\left\\ {   1,\\left ( \\frac{k}{n}\\right )   ^{j-1}\\right\\ }   \\right)\\ ] ] @xmath90 @xmath1205   \\approx var_{\\widehat{\\theta}}\\left [   \\widehat{\\mathbb{if}}_{m,\\widetilde{\\psi}_{k}}|\\widehat{\\theta}\\right ]   \\asymp\\frac{1}{n}\\max\\left\\ {   1,\\left (   \\frac{k}{n}\\right )   ^{m-1}\\right\\}\\ ] ]    we now use theorem @xmath1206 to derive the order of the conditional variance of @xmath956 given @xmath391 .",
    "[ 3.19]if sup@xmath1207as @xmath1208 then for a fixed @xmath425 @xmath1209    &   = var_{\\theta}\\left [   \\widehat{\\mathbb{if}}_{m,\\widetilde{\\psi}_{k}}|\\widehat{\\theta}\\right ] \\\\ &   = var_{\\widehat{\\theta}}\\left [   \\widehat{\\mathbb{if}}_{m,\\widetilde{\\psi } _ { k}}|\\widehat{\\theta}\\right ]   \\left (   1+o_{p}\\left (   1\\right )   \\right ) \\\\ &   \\asymp\\left (   \\frac{1}{n}\\max\\left\\ {   1,\\left (   \\frac{k}{n}\\right ) ^{m-1}\\right\\ }   \\right)\\end{aligned}\\ ] ]    the proof is in the appendix .    for a given @xmath425 the estimator @xmath1210 that minimizes the maximum asymptotic mse over the model @xmath292 @xmath70defined by @xmath512 among the candidates @xmath1211 uses the value @xmath1212 of @xmath848 that equates the order @xmath1213 of @xmath1214   \\ $ ] to the order @xmath1215   = \\\\ &   \\max\\left [ \\begin{array } [ c]{c}\\left (   \\frac{\\log n}{n}\\right )   ^{\\frac{2\\left (   m-1\\right )   \\beta_{g}}{d+2\\beta_{g}}}n^{-\\left (   \\frac{2\\beta_{b}}{d+2\\beta_{b}}+\\frac{2\\beta_{p}}{d+2\\beta_{p}}\\right )   } , \\\\",
    "k^{-\\frac{2\\left (   \\beta_{b}+\\beta_{p}\\right )   } { d}}\\end{array } \\right]\\end{aligned}\\ ] ] of the maximal squared bias .",
    "the estimator @xmath1216 that minimizes the maximum asymptotic mse over the model @xmath292 among all candidates @xmath1217is the estimator @xmath1218 which minimizes @xmath1219      we derive a consistent estimator of the variance and give the asymptotic distribution of @xmath1217  for any model and functional satisfying @xmath1220 let @xmath1221 be the upper @xmath1222quantile of a standard normal , i.e. a @xmath1223 distribution .    * [ hoci ] * :    \\a ) letting @xmath1224   $ ] , @xmath1225   , \\ ] ] for @xmath1226 and @xmath1227 where @xmath1228 is the symmetric kernel of @xmath1229    we have@xmath90 @xmath1230 &   = \\widehat{var}\\left [   \\widehat{\\mathbb{if}}_{1,\\widetilde{\\psi}_{k}}|\\widehat{\\theta}\\right ] \\\\ \\widehat{e}\\left [   \\widehat{\\mathbb{w}}_{jj,\\widetilde{\\psi}_{k}}^{2}\\right ] &   = \\widehat{var}\\left [   \\widehat{\\mathbb{if}}_{jj,\\widetilde{\\psi}_{k}}|\\widehat{\\theta}\\right ]   , \\\\",
    "\\widehat{e}\\left [   \\widehat{\\mathbb{w}}_{m,\\widetilde{\\psi}_{k}}^{2}\\right ] &   = \\widehat{var}\\left [   \\widehat{\\mathbb{if}}_{m,\\widetilde{\\psi}_{k}}|\\widehat{\\theta}\\right]\\end{aligned}\\ ] ] where @xmath1231   = var_{\\widehat{\\theta}}\\left [ \\cdot\\right ]   .$ ]    \\b ) conditional on the training sample , @xmath1232   \\right\\}\\ ] ] converges uniformly for @xmath274 to a normal distribution with finite variance as @xmath596 .",
    "the asymptotic variance is uniformly consistently estimated by @xmath1233 thus @xmath1234 \\right\\ }   /\\widehat{\\mathbb{w}}_{m,\\widetilde{\\psi}_{k_{opt}\\left ( m , n\\right )   } } \\ ] ] is converging in distribution to a standard normal distribution .",
    "\\c ) define the interval @xmath1235 suppose @xmath1236 then for @xmath1237 @xmath1238 @xmath1239   } { \\sqrt{var_{\\theta}\\left [ \\widehat{\\psi}_{2,k^{\\ast}}|\\widehat{\\theta}\\right ]   } } \\right ]   = o_{p}\\left ( 1\\right)\\ ] ] and @xmath1240 converges uniformly in @xmath274 to a @xmath1241 .",
    "moreover , @xmath1242 is a conservative uniform asymptotic @xmath1243 confidence interval for @xmath175 .",
    "\\d ) suppose we could derive a constant @xmath1244 and a constant @xmath1245  such that @xmath1246   \\right\\vert \\\\ &   = \\sup_{\\theta}\\left\\vert \\left\\ {   tb_{k_{opt}\\left (   m , n\\right )   } \\left ( \\theta\\right )   + eb_{m}\\left (   \\theta\\right )   \\right\\ }   \\right\\vert \\\\ &   \\leq c_{bias}\\left\\ {   \\frac{1}{n}\\max\\left\\ {   1,\\left (   \\frac { n^{\\rho_{_{opt}\\left (   m , n\\right )   } } } { n}\\right )   ^{m-1}\\right\\ }   \\right\\ } ^{1/2}\\ ] ]  for @xmath1247 .",
    "then @xmath1248 is a conservative uniform asymptotic @xmath1243 confidence interval for @xmath174    part a ) of the theorem is an easy calculation .",
    "the asymptotic normality of @xmath1218 is based on new results on the asymptotic distribution of higher order @xmath340 with kernels depending on @xmath5 to be published elsewhere ( robins et al , 2007 ) .",
    "part c of the theorem implies we obtain a conservative uniform asymptotic @xmath1243 confidence interval for any value of @xmath1249  exceeding @xmath1250 however , for the actual fixed sample size of  our study , say @xmath12515000@xmath90  there is no guarantee the interval of part c based on given difference @xmath1252 say @xmath1253 will provide conservative finite sample coverage .    because of this difficulty , a better approach , described in part d , would be to determine a constant @xmath1244 that can be used to bound the maximal bias under the model at a sample sizes exceeding @xmath1254 with @xmath1255 no greater than the actual fixed sample size @xmath5 of the study .",
    "then the interval @xmath1256 will be a honest conservative finite sample @xmath159  confidence interval , provided that @xmath1257  has nearly converged to its normal limit at sample size @xmath126  unfortunately as yet we do not know how to determine the constants @xmath1244  and @xmath1258 of part d as a function of our model and of our initial estimator @xmath1259 this  is an important open problem .      * a model of increasing dimension : * the previous results can also be used for the analysis of models whose dimension increases with sample size . in fact , consider the @xmath1260 @xmath1261known , that differs from model @xmath310 in that , rather than assuming @xmath759 and @xmath764 live in particular hlder balls , we instead assume the working models of eqs .",
    "[ worka ] and [ workb ] are precisely true for @xmath1262 so @xmath1263 and the dimensions of @xmath759 and @xmath1264 increase as @xmath1265 .",
    "valid point and interval estimation for @xmath1266can still be based on the estimators @xmath956 except now ( i ) there is truncation bias only when @xmath1267 , ( ii ) the variance remains of the order of @xmath1268 and ( iii ) the estimation and trunction bias ( when it exists )  orders will be determined by any additional complexity reducing restrictions placed on the fraction of non - zero components or on the rate of decay of the components of the vectors @xmath1269and @xmath1270 and , for estimation bias , by @xmath688 as well . as a consequence , @xmath1271 and @xmath1272 under model",
    "@xmath1273 will differ from their values under model @xmath296 note we need not take @xmath1274 as we did in the heurisitic discussion following remark @xmath1275 indeed @xmath492 in that discussion corresponds to the estimator in the class @xmath1276 with the fastest rate of convergence . in general , @xmath492 will have convergence rate slower than @xmath1277 furthermore , the discussion in section 4.1.1 implies that , when @xmath1278 and the minimax rate for estimation of @xmath175 is slower than @xmath430 even @xmath1279 will typically fail to converge at the minimax rate when complexity reducing restrictions have been imposed on @xmath1280and @xmath1281 .",
    "* multi - robustness and a practical data analysis strategy : * [ multirob copy(1 ) ] conditional on @xmath414 for @xmath316 @xmath1282 is zero and thus estimator @xmath956 is unbiased for @xmath969 if @xmath1283 or @xmath1284 we refer to @xmath956 as triply - robust for @xmath970 generalizing robins and rotnitzky ( 2001 ) and van der laan and robins ( 2003 ) who referred to @xmath1285 as doubly - robust because of its being unbiased for @xmath969 if either @xmath1286 or @xmath1287  in fact , for @xmath1288 we can construct a modified estimator @xmath1289 that is @xmath1290 robust as follows . let @xmath1291 @xmath1292 denote @xmath1293 additional initial estimators of @xmath1294that differ from one another and from @xmath1295 define @xmath1296 where@xmath1297 @xmath1298   _ { i_{1}}\\left\\ {   \\left (   \\dot{p}\\dot { b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right )   _ { i_{2}}-i_{k\\times k}\\right\\ } \\\\ \\times &   \\left [ \\begin{array } [ c]{c}{\\displaystyle\\prod\\limits_{s=3}^{j-1 } } \\left\\ {   \\widehat{e}_{s}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\\\ \\left\\ {   \\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right )   _ { i_{s}}-\\widehat{e}_{s}\\left [   \\dot{p}\\dot{b}h_{1}\\overline { z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ } \\end{array } \\right ] \\\\ &   \\times\\left\\ {   \\widehat{e}_{j}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\times\\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right )   \\dot{p}\\right ]   _ { i_{j}}\\ ] ] with @xmath1299 defined like @xmath1300 except with @xmath1301 replacing @xmath1302 in the appendix , we prove that @xmath1303   -\\widetilde{\\psi}_{k}$ ] is @xmath1304{c}e\\left [   \\dot{b}\\dot{p}h_{1}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right ) \\overline{z}_{k}^{t}\\right ]   \\left\\ {   e\\left [   \\dot{b}\\dot{p}h_{1}\\overline { z}_{k}\\overline{z}_{k}^{t}\\right ]   -i_{k\\times k}\\right\\ } \\\\ \\times{\\displaystyle\\prod\\limits_{s=3}^{m } } \\left\\ {   \\widehat{e}_{s}\\left [   \\dot{b}\\dot{p}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\\\ \\times\\left\\ {   e\\left [   \\dot{b}\\dot{p}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   -\\widehat{e}_{s}\\left [   \\dot{b}\\dot{p}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ } \\\\",
    "\\times\\left\\ {   e\\left [   \\dot{b}\\dot{p}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}e\\left [   \\dot{b}\\dot{p}h_{1}\\left ( \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\right ] \\end{array } \\right\\ } \\label{multirob_bias}\\ ] ] which is zero if @xmath1305 @xmath1306 or if any of the @xmath1293 @xmath1301 equals @xmath1307 ( we note that if @xmath1308 or @xmath1309 @xmath1310 and thus @xmath1289 and @xmath956 are unbiased for @xmath271 . )    in settings where the dimension @xmath11 of @xmath8 is so large ( say @xmath1311 that the above asymptotic results fail as a guide to the finite sample performance of our procedures at the moderate sample sizes , say @xmath1312 commonly found in practice , one might consider , as a practical data analysis strategy , using the @xmath1290 robust estimator @xmath1313with @xmath1314 and the @xmath1315 selected by cross - validation as in van der laan and dudoit ( 2003 ) .",
    "specifically , the training sample is split into two random subsamples - a candidate estimator subsample of size @xmath1316 and a validation subsample of size @xmath1317 where both @xmath1318 and @xmath1319 are bounded away from @xmath590 as @xmath171 a large number ( e.g. , @xmath1320 candidate parametric models of various dimensions and functional forms for @xmath160 and @xmath139 are fit to the candidate estimator subsample and the validation sample is used to find the candidate estimators @xmath1321and @xmath1322 for @xmath534 and @xmath1323 and the @xmath1324 candidate estimators @xmath616 and @xmath1325 @xmath1292 for @xmath139 with the smallest estimated risks ( with respect to an appropriate risk function such as squared error or kullback - leibler . ) an alternative approach would be to use the triply robust estimator @xmath956 with @xmath616 the candidate for @xmath139 with minimum estimated risk .",
    "we plan to explore through simulation whether @xmath1289 outperforms @xmath956 in the setting of very high dimensional @xmath113",
    "we consider a generic version in which we only assume a model and functional satisfying @xmath1220 to examine efficiency issues , we first consider the estimator @xmath1285 based on the first order influence function and sample splitting . without loss of generality we assume @xmath1326 ( otherwise simply interchange @xmath1327and @xmath1328 in what follows . )  it will be useful to consider the alternative parametrization @xmath1329 the ( conditional ) variance of @xmath1285 is of the order of @xmath1330 and the ( conditional ) bias of @xmath1285 in estimating @xmath271 is @xmath1331 if @xmath1332 and thus @xmath1333 the bias of @xmath1334 is @xmath1335 and @xmath1285 is not @xmath1336 consistent for @xmath271 when @xmath1337 at the other extreme , as @xmath1338 i.e. @xmath1339 the bias of @xmath1285 is @xmath1340which fails to be @xmath1336consistent@xmath70for any finite @xmath1341    * minimaxity with * @xmath139 * *  known : * * to further examine efficiency issues , it is instructive to first consider the estimation of @xmath271 with @xmath1342 known .",
    "if @xmath1342 were known , we could set @xmath1343 when calculating @xmath971 then @xmath1344 and @xmath1345 would therefore be an unbiased estimator of @xmath968 letting a superscript @xmath139 denote the model with @xmath139 known , it is easy to see that @xmath1346 would be @xmath1347 where @xmath1348 satisfies @xmath1349solving this , we find that when  @xmath1350 is greater than or equal to @xmath1351 we can take @xmath1352 and @xmath1353 regardless of @xmath1354 which is , of course , the minimax rate .",
    "in contrast if @xmath1350 @xmath1355 @xmath1356 and @xmath1357 in an unpublished paper , we have proved that this is the minimax rate when @xmath148 is known .",
    "this raises the question of whether the lower bounds of rate @xmath1358 for @xmath1350 @xmath1359 and/or rate @xmath1360  for @xmath1350 @xmath1361are still achievable when @xmath139 is unknown , without restrictions on the smoothness of @xmath1362    before addressing this question , we take the opportunity to compare the relative efficiencies of competing rate - optimal unbiased estimators in the case of @xmath139 known",
    ". this discussion will provide further insight into the results given in remark [ jj1 ] for models which are not locally nonparametric .    * relative efficiency of various unbiased estimator with * @xmath139 * *  known * * :    for simplicity , we restrict the following discussion to the truncated version of the parameter @xmath1363   , $ ] with @xmath1364,$ ] @xmath1342 known , and @xmath0 bernouilli . for",
    "this choice of @xmath1365 @xmath1366 is the marginal density of @xmath113 in this subsection , we assume @xmath1367is chosen equal to the known @xmath1368 so @xmath1369   = i_{k\\times k}.$ ] also we choose @xmath1370 and take @xmath1371 so @xmath1372   = e\\left [   b\\overline{z}_{k}^{t}\\right ] \\overline{z}_{k}$ ] and @xmath1373   \\right\\ }   ^{2}\\right ]   $ ] do not depend on @xmath1374 further we only concern ourselves with efficiency relative to the @xmath5 observations in the estimation sample .",
    "we thus ignore any efficiency loss from using @xmath393 observations to construct @xmath1007 .",
    "let @xmath1375 \\right\\ }   \\subset\\theta$ ] denote the subset of @xmath245 corresponding to the known @xmath139 , which consists of all functions from the unit cube in @xmath1059 to the unit interval .",
    "the model @xmath1376 is not locally nonparametric .",
    "for example , the @xmath1377 order tangent space @xmath1378 does not include first order scores for @xmath1362 its 2nd order tangent space @xmath1379 does not contain second order scores for @xmath139 or mixed scores for @xmath139 and @xmath1380 rather , @xmath1381 is the closed linear span of the first and second order scores for @xmath533",
    ". thus @xmath1382   \\right\\ }   < \\infty ; a\\in\\mathcal{a},c\\in\\mathcal{c}\\ \\}\\ ] ] where @xmath1383   , \\ ] ] and @xmath1384 and @xmath1385 are the set of one and two dimensional functions of @xmath1386 since , for @xmath1387 @xmath1388{c}\\left [   \\widehat{b}^{2}+2\\widehat{b}\\left (   y-\\widehat{b}\\right )   \\right ] _ { i}\\\\ + \\left [   \\left (   y-\\widehat{b}\\right )   \\overline{z}_{k}^{t}\\right ] _ { i}\\left [   \\overline{z}_{k}\\left (   y-\\widehat{b}\\right )   \\right ]   _ { j}\\end{array } \\right\\}\\end{aligned}\\ ] ] is unbiased for @xmath1389   \\right\\ }   ^{2}\\right ]   $ ] in model @xmath1376 , we know , by remark [ jj1 ] , that @xmath1390for @xmath1391 is the projection @xmath1392   $ ] of the 2nd order influence function @xmath1393 onto @xmath1394 now if @xmath1395 was an element of @xmath1396 @xmath1393 would equal @xmath1397 and thus be 2nd order ` unbiased locally efficient ' , at @xmath1398 as defined earlier in remark [ jj1 ]",
    ". however we show below that , when @xmath1399 for some @xmath1400 wp1 does not hold , @xmath1401 is not an element of @xmath1381 for any @xmath533 .",
    "rather , a straightforward calculation gives @xmath1402{c}\\left [   2e\\left [   b\\overline{z}_{k}^{t}\\right ]   \\overline{z}_{k}\\left ( y - b\\right )   \\right ]   _ { i}\\\\ + \\left [   \\left (   y - b\\right )   \\overline{z}_{k}^{t}\\right ]   _ { i}\\left [ \\overline{z}_{k}\\left (   y - b\\right )   \\right ]   _ { j}\\end{array } \\right\\ }   .\\ ] ]    now one can check that @xmath1403 is a function of @xmath1404 so by theorem [ global_eff ] of remark [ jj1 ] , we conclude no unbiased globally efficient estimator exists .",
    "however , we prove below that @xmath1405 and @xmath1345 have identical means .",
    "it follows that @xmath1406 is an unbiased estimator of @xmath1407   \\right )   ^{2}\\right ]   $ ] for any @xmath1408 .",
    "thus , for a given choice of @xmath1409 @xmath1405 is 2nd order unbiased locally efficient at @xmath1410 however , one can show using a proof analogous to that in theorem [ 3.19 ]  that for @xmath1411 @xmath1412   /var_{b}\\left [   \\mathbb{if}_{2,\\widetilde{\\psi}_{k}}^{eff}\\left ( b\\right )   \\right ] \\\\ &   = 1+o_{p}\\left (   \\left\\vert \\left\\vert \\widehat{b}-b\\right\\vert \\right\\vert _",
    "{ \\infty}\\right )   .\\end{aligned}\\ ] ]    henceforth we assume that @xmath533 lies in a holder ball @xmath1413 that is we consider the submodel @xmath1414and assume @xmath1415 converges to @xmath533 in sup norm at the optimal rate of @xmath1416uniformly over @xmath1417 the submodel and the model @xmath1418  have identical tangent spaces",
    ". for all @xmath1419  @xmath1420  has an asymptotic distribution with mean zero and variance equal to @xmath1421   $ ]  for all @xmath1422 _ _  _ _ in a slight abuse of language , we shall refer to @xmath1423   $ ] as the asymptotic variance of @xmath1424thus , as with standard first order theory , even when no unbiased estimator has finite sample variance that attains the bhattacharyya bound for all @xmath1425 , there can exist an unbiased estimator sequence whose asymptotic variance does attain the bound globally .",
    "we next compare the means and variances of @xmath1426 and @xmath1427 now the two estimators are algebraically related by @xmath1428   -e\\left [ \\widehat{b}^{2}\\right ]   \\right\\ }   .\\ ] ] since @xmath1429   -e\\left [   \\widehat{b}^{2}\\right ]   $ ] is unbiased for zero , we conclude that @xmath1345 and @xmath1405 have the same mean but @xmath1430   /var_{b}\\left [ \\mathbb{if}_{2,\\widetilde{\\psi}_{k}}^{eff}\\left (   b\\right )   \\right ]   > 1 $ ] except when @xmath1431 @xmath1432for some @xmath1433 thus , since @xmath1434has asymptotic variance @xmath1435   $ ] and , except when @xmath1436 @xmath1437   \\right )   \\asymp\\ n^{-1},$ ] we conclude the asymptotic variance of @xmath1345 attains the bound @xmath1438   $ ] when @xmath1439 but exceeds the bound when @xmath1440 , except when @xmath1441 .",
    "finally , for completeness , robins and van der vaart ( 2006 ) considered an alternative particularly simple rate - optimal unbiased estimator of @xmath1442   \\right\\ }   ^{2}\\right ]   $ ] given by @xmath1443   _ { i}\\left [ \\overline{z}_{k}y\\right ]   _ { j}\\right\\ }   $ ] .",
    "the hoeffding decomposition of @xmath1444 is @xmath1445   \\overline{z}_{k}y-\\widetilde{\\psi}_{k}\\left (   b\\right )   \\right ]   + \\mathbb{v}\\left\\ { \\left [   y\\overline{z}_{k}^{t}-e\\left [   b\\overline{z}_{k}^{t}\\right ]   \\right ] _",
    "{ i}\\left [   \\overline{z}_{k}y - e\\left [   b\\overline{z}_{k}\\right ]   \\right ] _ { j}\\right\\ } \\\\",
    "&   = \\mathbb{if}_{2,\\widetilde{\\psi}_{k}}^{eff}\\left (   b\\right )   + q+t\\end{aligned}\\ ] ] with @xmath1446 b-\\psi\\right\\ }   \\right ] \\\\",
    "t   &   = \\mathbb{v}\\left\\ { \\begin{array } [ c]{c}2\\left (   b_{i}\\overline{z}_{k , i}^{t}\\overline{z}_{k , j}-\\pi\\left [ b|\\overline{z}_{k}\\right ]   _ { j}\\right )   \\left (   y - b\\right )   _ { j}\\\\ + b_{i}\\overline{z}_{k , i}^{t}\\overline{z}_{k , j}b_{j}-\\pi\\left [   b|\\overline { z}_{k}\\right ]   _ { i}b_{i}-\\pi\\left [   b|\\overline{z}_{k}\\right ]   _ { j}b_{j}+\\psi \\end{array } \\right\\}\\end{aligned}\\ ] ] since , except when @xmath1447 wp1 , @xmath1448 and @xmath1449 we conclude that the asymptotic variance of @xmath1450 exceeds the bound @xmath1438   $ ] regardless of whether @xmath1451 does or does not hold except when @xmath1452 wp1 .",
    "* minimaxity with unknown * @xmath139 * *  and * * @xmath1350 * *  * * @xmath1453 we now show that the bound @xmath1358 for @xmath1350 @xmath1359 is achievable for each @xmath86 consider the estimator @xmath956 with @xmath1454 and @xmath1455 so that @xmath1456 is @xmath1457 then @xmath1458 @xmath1459 and @xmath1460 so @xmath956 will be @xmath1461consistent for @xmath279 if @xmath1462 and @xmath1463 the above expression implies that @xmath1464 for @xmath1461consistency . similarly , if @xmath1465 i.e. @xmath1339 it is necessary that @xmath1466 for @xmath1467consistency .",
    "these results imply that estimators @xmath956 in our class can always achieve @xmath1461consistency whenever @xmath144 , but for fixed @xmath1468 the order @xmath135 of the required @xmath187statistic increases without bound as the smoothness @xmath688 of @xmath139 approaches zero .",
    "* efficiency : * we now show that when @xmath1350 is strictly greater than @xmath1351 we can construct an unconditional asymptotically linear estimator based on all @xmath392 subjects with influence function @xmath1469 by having the number of the @xmath392 subjects allotted to the validation sample and analysis sample be @xmath1470 and @xmath1471 respectively , for @xmath1472 .",
    "it then follows from van der vaart@xmath1473 that the estimator is regular and semiparametric efficient .",
    "specifically , suppose @xmath1474 @xmath1475 consider the estimator @xmath1476 with @xmath1477 so that @xmath1478 is @xmath1479and @xmath1480 so that @xmath1481 and @xmath1482   = o_{p}\\left (   1/n\\right )   $ ] for @xmath1483then , by our previous results , @xmath1484 it remains to show that @xmath1485 but the lhs is @xmath1486    * adaptivity when * @xmath1350 * *  * * @xmath1487 * *  :* * we next prove that if we let @xmath1488with @xmath1489 and @xmath1490 @xmath1491 will be semiparametric efficient for each @xmath1492 , provided @xmath1493 clearly , the truncation bias is @xmath1494 the estimation bias @xmath1495 is @xmath1496   } n^{\\ -\\left (   1-\\epsilon\\right )   \\left\\ { \\frac{\\beta_{b}}{d+2\\beta_{b}}+\\frac{\\beta_{p}}{d+2\\beta_{p}}\\right\\ } } \\right )   .$ ] thus @xmath1497 if @xmath1498   } = o\\left (   n^{-\\frac{1}{2}+\\left (   1-\\epsilon\\right ) \\left\\ {   \\frac{\\beta_{b}}{d+2\\beta_{b}}+\\frac{\\beta_{p}}{d+2\\beta_{p}}\\right\\ }   } \\right )   .$ ] so we require @xmath1499   ln\\left\\ {   m\\left (   n^{\\left ( 1-\\epsilon\\right )   } \\right )   \\right\\ }   /\\left [   \\frac{1}{2}-\\left ( 1-\\epsilon\\right )",
    "\\left\\ {   \\frac{\\beta_{b}}{d+2\\beta_{b}}+\\frac{\\beta_{p}}{d+2\\beta_{p}}\\right\\ }   \\right ]   \\ln\\left (   n\\right )   \\rightarrow\\infty,\\ ] ] which is satisfied if @xmath1500 in the appendix we prove that var@xmath1501   = var_{\\widehat{\\theta}}\\left [   \\widehat{\\psi}_{m , k\\ } \\right ]   \\left\\ {   1+o_{p}\\left (   1\\right )   \\right\\ }   $ ] provided @xmath1502 now @xmath1503   = \\frac{1}{n}var_{\\widehat{\\theta}}\\left\\ {   if_{1,\\psi , i}\\left (   \\widehat{\\theta } \\right )   \\right\\ }   \\left [   o\\left (   \\sum_{l=0}^{m\\left (   n\\right )   } \\left\\ { \\ln n\\right\\ }   ^{-l}\\right )   \\right ]   .{\\huge \\ } $ ] but @xmath1504 @xmath1505   } } { 1-\\left\\ {   \\ln n\\right\\ }   ^{-1}}\\right )   = o\\left (   1+\\left\\ {   \\ln n\\right\\ }   ^{-1}\\right )   , $ ] so var@xmath1506   $ ] is @xmath1507 the proof of efficiency now proceeds as above .",
    "* alternative estimators when * @xmath1350 * *  * * @xmath1487 * *  :* * when @xmath1350 @xmath1508 there actually exist , at least for certain functionals in our class , @xmath1461consistent estimators of @xmath271 that are much simpler than our very high order u - statistic estimators.@xmath70for example consider the expected conditional covariance @xmath1509   $ ] of example 1b with @xmath1510 .",
    "* example 1b ( cont ) * : number the study subjects @xmath1511 ordered by their realized values @xmath1512 where we have not split the sample@xmath64 following wang et al .",
    "( 2006 ) , consider the difference -based estimator estimator @xmath1513 which has conditional mean given @xmath1514 of @xmath1515    hence @xmath1516 \\\\ &   = n^{-1}e\\left [   \\sum_{i=0}^{n/2 - 1}\\left\\ {   b\\left (   x_{i+1}\\right ) -b\\left (   x_{i}\\right )   \\right\\ }   \\left\\ {   p\\left (   x_{i+1}\\right )   -p\\left ( x_{i}\\right )   \\right\\ }   \\right ] \\\\",
    "&   = o_{p}\\left (   n^{-1}{\\textstyle\\sum\\limits_{i=0}^{\\frac{n}{2}-1 } } e\\left\\ {   x_{i+1}-x_{i}\\right\\ }   ^{2\\beta}\\right )   = o\\left (   n^{-2\\beta } \\right)\\end{aligned}\\ ] ] by the theory of spacings ( pyke , 1965 ) . but",
    "@xmath1517 is @xmath1518 when @xmath1519 @xmath1520 the variance of @xmath1521 is @xmath1522so @xmath1521 is @xmath1523consistent@xmath64 however , @xmath1524 so @xmath1521 is not ( semiparametric ) efficient . as discussed by arellano ( 2003 ) , by using a @xmath135th order rather than a second order difference operator and letting @xmath1525 at an appropriate rate as @xmath1526 the @xmath135th order estimator @xmath1521 can be made efficient .",
    "* minimaxity with unknown * @xmath139 * *  and * * @xmath1350 * *  * * @xmath1527 consider next whether the lower bound of @xmath1360  for @xmath1350 @xmath1528 is achievable when @xmath139 is unknown@xmath70but @xmath144 .",
    "we will show the next section that the bound @xmath1529 is achievable provided @xmath1530 i.e. , @xmath1531 to attain the bound @xmath1532 whenever eq.@xmath1533 holds , we introduce new more efficient estimators , owing to the fact that an estimator @xmath956 in our class can attain the bound @xmath1532 only in the special case where the second order estimation bias @xmath1534 is less than @xmath1535    for a fixed @xmath1536 the right hand side of eq.@xmath1537 is minimized over @xmath1538 at @xmath107 at @xmath99 eq.@xmath1537 reduces to @xmath1539    the right hand side of eq.@xmath1537 increases with @xmath1540 with asymptote equal to twice the rhs of eq.([bounde ] ) as @xmath1541 hence , in order to attain the optimal rate @xmath1532 when @xmath1542 and @xmath1543 the quantity @xmath1544 must be twice as large as when @xmath1545    in the next section , we construct an estimator with a convergence rate of @xmath1546 at the cut - point @xmath1547 in this paper we do not consider the construction of estimators that are rate optimal below this cutpoint .    however , for the special case @xmath99 in an unpublished paper li et .",
    "2007 ) have constructed estimators which converge at a rate given in eq.@xmath101 , whenever inequality @xmath1537 fails to hold .",
    "we conjecture that this rate is minimax , possibly only up to @xmath1548 factors , when inequality @xmath1537 fails to hold and @xmath107 at the cut - point @xmath1549 we obtain @xmath1550 and thus@xmath70eq.@xmath101 becomes@xmath1551 in agreement with the rate of the estimator of section [ case2 ] below . in the extreme case in which @xmath116 with @xmath1519 remaining fixed , @xmath1552 @xmath1553 which agrees ( up to a log factor ) with the rate of @xmath111 given by the simple estimator of wang et al .",
    "( 2006 ) analyzed above under `` example 1b ( cont ) '' .",
    "* improved rates of convergence with * @xmath8 * *  random in a semiparametric model : * * we now , as promised in the introduction , construct an estimator of @xmath117 under the homoscedastic model @xmath1554 = b\\left (   x\\right )   , $ ] @xmath1555   = \\sigma^{2}$ ] with @xmath8 random with unknown density that , whenever @xmath1556 and , regardless of the smoothness of @xmath9 , converges at the rate @xmath120 which is faster than equal - spaced non - random minimax rate of @xmath121 specifically we divide the support of @xmath652 i.e. , the unit cube in @xmath1557 into @xmath1558 identical subcubes with edge length @xmath1559 we continue to assume the unknown density @xmath9 is absolutely continuous wrt to lebesgue measure and both it and its inverse are bounded in sup - norm . then it is a standard probability calculation that the number of subcubes containing at least two observations is @xmath1560 we estimate @xmath117 in each such subcube by @xmath1561 where , for any subcube with @xmath1562 or more observations,@xmath1563 and @xmath1564 are chosen randomly , without replacement .",
    "our final estimator of @xmath117 is the average of our subcube - specific estimates @xmath1565 over the @xmath1566 subcubes with at least two observations .",
    "the rate of convergence of the estimator is minimized at @xmath92 by taking @xmath1567 as we now show .",
    "we note that @xmath1568 = \\sigma^{2}+\\left\\ {   b\\left (   x_{i}\\right )   -b\\left (   x_{j}\\right )   \\right\\ } ^{2}/2,$ ] @xmath1569 by @xmath119 and @xmath1570when @xmath1571 and@xmath1572 are in the same subcube .",
    "it follows that the estimator has variance @xmath1573 and bias of @xmath1574 to minimize the convergence rate we equate the orders of the variance and the squared bias by solving @xmath1575 which gives @xmath1576 our random design estimator has better bias control and hence converges faster than the optimal equal - spaced fixed @xmath8 estimator , because the random design estimator exploits the @xmath1577 random fluctuations for which @xmath1578 corresponding to two different observations are a distance of @xmath1579 apart .",
    "our estimator will not converge at rate @xmath92 to @xmath1580   $ ] in our nonparametric model , because it then no longer suffices to average estimates of @xmath1581 only over subcubes containing 2 or more observations .        in a ( locally ) nonparametric model",
    "@xmath366 the estimator @xmath1582 is essentially the unique @xmath1583 order u - statistic estimator of the truncated parameter @xmath969 for which the leading term in the bias is @xmath1584 however , when the minimax rate of convergence for @xmath271 is slower than @xmath63 , other @xmath1585 order u - statistics estimators will often converge to @xmath969 ( and thus @xmath1586 at a faster rate uniformely over the model than does any estimator @xmath956 ( constructed from an estimated higher order influence function @xmath1587 for @xmath1588 by tolerating bias at orders less than @xmath286 in exchange for a savings in variance .",
    "[ t7]a heuristic understanding as to why this is so can be gained from the following considerations .",
    "the theory of higher order influence functions as developed in theorems 2.2 and 2.3 is a theory of score functions ( derivatives ) .",
    "thus it can directly incorporate the restriction that a function , say @xmath1589 has an expansion @xmath1590 for which @xmath1591 for @xmath1592  as the restriction is equivalent to various scores being equal to zero .",
    "however the theory can not directly incorporate restrictions such as @xmath1593 or @xmath1594 that do not imply any restrictions on score functions .",
    "thus to find an optimal estimator , one must perform additional side calculations  to quantify the estimation and truncation bias of various candidate estimators under these restrictions . as the assumption that @xmath759 lies in a holder ball",
    "can be expressed in terms of such restrictions , this remark is relevant to a search for an optimal rate estimator .",
    "we now construct such estimators .",
    "we first consider the case where @xmath1595 and @xmath688 are such that the estimation bias @xmath1596 of the second order estimator is greater than @xmath1597 but the estimation bias @xmath1598 of the third order estimator is less than @xmath1599 that is @xmath1600 then the most efficient estimator @xmath1491 in our class has rate of convergence slower than @xmath1532 because @xmath1601 converges at rate @xmath1602 determined by the 2nd order estimation bias and , for @xmath1603 @xmath1210 converges at a rate no faster than @xmath1604 [ we obtained @xmath1605 as @xmath1606 where @xmath848 solves the equation @xmath1607 that equates the variance @xmath1608 of @xmath327 to the squared truncation bias @xmath1609    to describe our more efficient estimator , define for nonnegative integers @xmath1610 with @xmath1611 and @xmath1612 the @xmath187statistic @xmath1613 with @xmath1614   _ { i_{2}}-i_{\\left\\ { k\\left (   1\\right )   -k\\left (   0\\right )   \\right\\ }   \\times\\left\\ {   k^{\\ast } \\left (   1\\right )   -k^{\\ast}\\left (   0\\right )   \\right\\ }   } \\right )   \\overline { z}_{k^{\\ast}\\left (   0\\right )   , i_{3}}^{k^{\\ast}\\left (   1\\right ) } \\widehat{\\delta}_{i_{3}}\\\\ &   = \\sum_{s_{1}=k\\left (   0\\right )   + 1}^{k\\left (   1\\right )   } \\sum _ { s_{2}=k^{\\ast}\\left (   0\\right )   + 1}^{k^{\\ast}\\left (   1\\right )   } \\left\\ { \\begin{array } [ c]{c}\\widehat{\\epsilon}_{i_{1}}z_{s_{1}}\\left (   x_{i_{1}}\\right )   \\times\\\\ \\left\\ {   \\left [   \\dot{b}\\dot{p}h_{1}\\right ]   _ { i_{2}}z_{s_{1}}\\left ( x_{i_{2}}\\right )   z_{s_{2}}\\left (   x_{i_{2}}\\right )   -i\\left [   s_{1}=s_{2}\\right ]   \\right\\ }   z_{s_{2}}\\left (   x_{i_{3}}\\right )   \\widehat{\\delta } _ { i_{3}}\\end{array } \\right\\ }   , \\end{aligned}\\ ] ]    where @xmath1615    @xmath1616 with @xmath1617    as an example @xmath1618 we can identify @xmath1619 with the rectangle in @xmath1620 defined by @xmath1621 with @xmath1622 and @xmath1623 respectively , the vertices closest and furthest from the origin . thus @xmath1624 is identified with the rectangle",
    "@xmath1625 indeed we can write @xmath1626{c}\\widehat{\\epsilon}_{i_{1}}z_{s_{1}}\\left (   x_{i_{1}}\\right )   \\times\\\\ \\left\\ {   \\left [   \\dot{b}\\dot{p}h_{1}\\right ]   _ { i_{2}}z_{s_{1}}\\left ( x_{i_{2}}\\right )   z_{s_{2}}\\left (   x_{i_{2}}\\right )   -i\\left [   s_{1}=s_{2}\\right ]   \\right\\ }   z_{s_{2}}\\left (   x_{i_{3}}\\right )   \\widehat{\\delta } _ { i_{3}}\\end{array } \\right\\}\\end{aligned}\\ ] ] where , here and below ,",
    "@xmath1627 and @xmath1628 are restricted to be integers , so @xmath1629 are the lattice points of the rectangle .",
    "we next study the variance of @xmath1630 it follows from theorem [ var_multi ] above that the number of lattice points in @xmath1619 is proportional to the variance of @xmath1631 so if @xmath1632 and @xmath1633 then @xmath1634   $ ] and @xmath1635   $ ] are both of order @xmath1636 hence the order of the variance of @xmath1637 is determined by the vertex of the rectangle @xmath1638 furthest from the origin .",
    "in contrast by a theorem in the appendix , the mean @xmath1639   $ ] is @xmath1640   \\delta g\\widehat{q}^{2}\\widehat{\\pi } \\left [   \\delta p|\\overline{z}_{k^{\\ast}\\left (   0\\right )   } ^{k^{\\ast}\\left ( 1\\right )   } \\right ]   \\right )   \\left (   1+o_{p}\\left (   1\\right )   \\right)\\ ] ] with @xmath1641 @xmath1642 @xmath1643 @xmath1644 and @xmath1645 it follows that if @xmath1632 and @xmath1633 then @xmath1639   $ ] and @xmath1646   $ ] are both of order @xmath1647   .\\ ] ] to see this for @xmath1648   $ ] , we @xmath1649sup out @xmath1650 from @xmath1651 \\delta g\\widehat{q}^{2}\\widehat{\\pi}\\left [   \\delta p|\\overline{z}_{k^{\\ast } \\left (   0\\right )   } ^{k^{\\ast}\\left (   1\\right )   } \\right ]   \\right\\vert \\right ) $ ] which is @xmath1652   \\widehat{e}\\left (   \\left\\vert \\widehat{\\pi}\\left [   \\delta b|\\overline{z}_{k\\left (   0\\right )   } ^{k\\left (   1\\right )   } \\right ] \\widehat{\\pi}\\left [   \\delta p|\\overline{z}_{k^{\\ast}\\left (   0\\right ) } ^{k^{\\ast}\\left (   1\\right )   } \\right ]   \\right\\vert \\right )   .\\ ] ] we then apply cauchy schwartz to @xmath1653   \\widehat{\\pi}\\left [   \\delta p|\\overline{z}_{k^{\\ast}\\left ( 0\\right )   } ^{k^{\\ast}\\left (   1\\right )   } \\right ]   \\right\\vert \\right )   , $ ] noting that @xmath1654   \\right\\ } ^{2}\\right )   ^{1/2}=o\\left (   k\\left (   0\\right )   ^{-\\beta_{b}}\\right )   $ ] .",
    "again a more careful argument using hlder s inequality would show the log factor is unnecessary .",
    "hence the order of the mean of @xmath1637 is determined by the vertex of the rectangle @xmath1638 closest to the origin .",
    "* motivation : * with this background we are ready to motivate our new estimator . recall from section [ dr_ci_section ] , that with @xmath139 known , the choice @xmath1655 gives @xmath1656 because the truncation bias @xmath1657 and variance are of order @xmath1360 and the estimation bias is zero .",
    "any choice of @xmath848 larger than @xmath1658 will result in a slower rate of convergence .",
    "however , when @xmath139 is unknown and thus estimated , @xmath1659 does not attain the optimal rate of convergence because the estimation bias @xmath1660 exceeds @xmath1661 the estimator @xmath1662 also fails to attain the rate @xmath1360 because it has variance of the order of @xmath1663 which exceeds @xmath1664 on the other hand , @xmath1665 has bias of @xmath1666 because the truncation bias is @xmath1666 and the estimation bias @xmath1667 is also @xmath1666 under our assumption @xmath1668 .",
    "our strategy will be to try to replace the term @xmath1669 in the estimator @xmath1670 by @xmath1671{c}\\widehat{\\epsilon}_{i_{1}}z_{s_{1}}\\left (   x_{i_{1}}\\right )   z_{s_{2}}\\left ( x_{i_{3}}\\right )   \\widehat{\\delta}_{i_{3}}\\times\\\\ \\left\\ {   \\left [   \\dot{b}\\dot{p}h_{1}\\right ]   _ { i_{2}}z_{s_{1}}\\left ( x_{i_{2}}\\right )   z_{s_{2}}\\left (   x_{i_{2}}\\right )   -i\\left [   s_{1}=s_{2}\\right ]   \\right\\ } \\end{array } \\right\\}\\ ] ]    where @xmath1672 is a subset of the rectangle @xmath1673 such that @xmath1674 but the additional bias @xmath1675 \\\\ &   = e\\left [   \\widehat{\\mathbb{u}}_{3}\\left (   \\left (   _ { 0,}^{k_{opt}^{g}\\left ( 2\\ \\right )   , } \\genfrac{}{}{0pt}{}{k_{opt}^{g}\\left (   2\\ \\right )   } { 0}\\right )   \\backslash\\omega\\right )   \\right ] \\\\ &   \\equiv e\\left [   \\sum_{\\left (   s_{1},s_{2}\\right )   \\in\\left (   _ { 0,}^{k_{opt}^{g}\\left (   2\\ \\right )   , } \\genfrac{}{}{0pt}{}{k_{opt}^{g}\\left (   2\\ \\right )   } { 0}\\right )   \\backslash\\omega}\\left\\ { \\begin{array } [ c]{c}\\widehat{\\epsilon}_{i_{1}}z_{s_{1}}\\left (   x_{i_{1}}\\right )   \\left\\ { \\begin{array } [ c]{c}\\left [   \\dot{b}\\dot{p}h_{1}\\right ]   _ { i_{2}}z_{s_{1}}\\left (   x_{i_{2}}\\right ) z_{s_{2}}\\left (   x_{i_{2}}\\right ) \\\\ -i\\left [   s_{1}=s_{2}\\right ] \\end{array } \\right\\ } \\\\",
    "\\times z_{s_{2}}\\left (   x_{i_{3}}\\right )   \\widehat{\\delta}_{i_{3}}\\end{array } \\right\\ }   \\right]\\end{aligned}\\ ] ] is @xmath1676 this approach will succeed if we can chose @xmath1672 and thus @xmath1677 to be sums of rectangles ( whose number does not increase with @xmath1678 such that ( i ) each rectangle in @xmath1679 has its closest vertex to the origin , say @xmath1680 satisfying @xmath1681   \\leq n^{-\\frac { 4\\beta\\ } { 4\\beta+d}}$ ] and ( ii ) simultaneously each rectangle in @xmath1672 has its furthest vertex from the origin , say @xmath1682 satisfying @xmath1683    we index the vertices of our set of rectangles as follows . consider a natural number @xmath1684 and a set of non - negative integers @xmath1685 satisfying @xmath1686    note the elements with even subscripts increase from @xmath590 to @xmath1687 while elements with odd subscripts decrease from @xmath1037 to @xmath1688 further the smallest element with odd subscript equals the largest element with even subscript .",
    "we will use two such sets @xmath1689 and @xmath1690 with corresponding elements @xmath1691 and @xmath1692with @xmath1693.@xmath70    @xmath70set for @xmath1694 @xmath1695 we leave @xmath1684,@xmath1696 and @xmath1697 unspecified for now but derive optimal values below .",
    "let @xmath1698 be the union of rectangles @xmath1699    the points @xmath1700 for @xmath1701 lie on a hyperbola @xmath1702 in @xmath1620 defined by @xmath1703 shown in figure 1 for @xmath1704 the set @xmath1705 lies below @xmath1706        define@xmath1707    we then have    [ uy](i ) : the estimator @xmath1708 has variance of the order of @xmath1709 and bias @xmath1710 of order @xmath1711    proof : each of the @xmath1712 rectangles whose union is @xmath1713 has @xmath1714 or @xmath1715 for some @xmath1694 as the vertex furthest from the origin and thus contributes @xmath1716 to the variance of @xmath1717 the variance of @xmath1718 now @xmath1719 -e\\left [   \\widehat{\\mathbb{u}}_{3}\\left\\ {   \\left (   _ { 0,}^{k_{-1},}\\genfrac{}{}{0pt}{}{k_{-1}}{0}\\right )   \\right\\ }   \\right ]   \\right\\ } \\\\ &   = o_{p}\\left (   k_{-1}^{-\\left (   \\beta_{p}+\\beta_{b}\\right )   /d}\\right ) + o_{p}\\left (   n^{-\\left (   \\frac{2\\beta_{g}}{2\\beta_{g}+d}+\\frac{\\beta_{b}}{d+2\\beta_{b}}+\\frac{\\beta_{p}}{d+2\\beta_{p}}\\right )   } \\right ) \\\\ &   + e\\left [   \\widehat{\\mathbb{u}}_{3}\\left\\ {   \\left (   _ { 0,}^{k_{-1},}\\genfrac{}{}{0pt}{}{k_{-1}}{0}\\right )   \\backslash\\omega\\left (   \\left (   \\mathcal{k}_{pj},\\mathcal{k}_{bj}\\right )   \\right )   \\right\\ }   \\right]\\end{aligned}\\ ] ] as is evident from figure 1 , @xmath1720 is the union of rectangles @xmath1721which have @xmath1722 as the set of vertices closest to the origin , leading to the expression for the bias given in the theorem .",
    "[ uz]given @xmath1723with @xmath1724 so @xmath1725 eq.(@xmath1726 ) holds if and only if there exists @xmath1727 such that @xmath1728    if eq.(@xmath1726 ) holds , @xmath1729   = o_{p}\\left (   n^{-\\frac{4\\beta } { 4\\beta+d}}\\right )   $ ] and thus @xmath1730 when we choose @xmath1684 to be the smallest integer such that    @xmath1731 with @xmath1732    @xmath1733 , @xmath1734for @xmath1735 with @xmath1736    note @xmath1684 does not depend on the sample size @xmath126    proof : from theorem [ uy ] , for the variance of @xmath1737 to be @xmath1738 @xmath1684 can not increase with @xmath126 further for the second order truncation bias @xmath1739 and the square root of the variance @xmath1740 of @xmath1741 both to be @xmath1742we must have @xmath1743 it then follows from eqs .",
    "( @xmath1744 ) and ( [ variance3 ] ) that @xmath1745    in order for @xmath1746   = o_{p}\\left (   n^{-\\frac{4\\beta } { 4\\beta+d}}\\right )   , $ ] we require for @xmath1747 @xmath1748 substituting for @xmath1749 in eq .",
    "( [ bias2 ] ) using eq.([variance1 ] ) and recalling that @xmath1724 so @xmath1538 , we obtain @xmath1750    since @xmath1751    solving the last expression for @xmath1752,we obtain @xmath1753 which is equation eq.@xmath1754 except with a nonstrict inequality .",
    "we have just deduced that the constraint @xmath1755 was due to restriction ( [ bias2 ] ) .",
    "we have not yet considered whether the restriction @xmath1756 implies additional constraints .",
    "we now show that it does not .",
    "specifically if we set @xmath1757 for all @xmath1758 then eq.@xmath1756 is true whenever eq.@xmath1759 holds because of our assumption that @xmath1760 thus we can set @xmath1761    thus we have shown that if @xmath1762 then @xmath1763 @xmath1755 holds , and @xmath1684 must not increase with @xmath126    we next show that when the inequality is strict in @xmath1764and eq.@xmath1668 holds , we can find @xmath1765 for which @xmath1766 we then complete the proof of the theorem by showing that when @xmath1755 holds with an equality , there is no choice of @xmath1767 for which @xmath1768converges at a rate better than @xmath1769    suppose the inequality is strict in @xmath1770 since @xmath1771 eq.@xmath1772 applied recursively suggests we define @xmath1773 for @xmath1774 and take @xmath1775 however , this will not generally give @xmath1776 as required when @xmath1761 instead we use the modified algorithm given in the statement of the theorem which insures that @xmath1777 , as required .",
    "since @xmath1684 is not a function of @xmath176 in order to show @xmath1778 converges at rate @xmath1779 we only need to check the bias",
    ".    now @xmath1780 @xmath1781 since @xmath1782so the bias of @xmath1783 is @xmath1784 as required .",
    "suppose now the equality holds in eq.@xmath1755 so @xmath1785 and continue to assume eq.@xmath1668 holds .",
    "we now construct an estimator @xmath1783 that converges at rate @xmath1786and show that no estimator in our class @xmath1783 converges at a faster rate .",
    "we conjecture this rate is minimax when the equality in eq.@xmath1755 holds . again",
    "@xmath1787 and by the previous arguments , @xmath1788 @xmath1789 we can suppose that @xmath1790it remains to determine @xmath1791 and @xmath1792 we know@xmath1793 must satisfy @xmath1794 the variance of @xmath1783 is of order @xmath1795 thus the order of the bias will still equal that of the variance provided we multiply the rhs of eq.@xmath1796 by @xmath1797 . then eq.@xmath1798 becomes @xmath1799 since , @xmath1800and @xmath1801 we substitute @xmath1802 for @xmath1803 in the modified eq.@xmath1798 which gives @xmath1804 hence @xmath1805 which implies that@xmath64 @xmath1806   \\right ) \\label{log}\\ ] ]    to minimize the variance , we want the slowest growing function of @xmath5 that satisfies eq.@xmath1807 which is @xmath1808 as claimed .",
    "in this section we no longer assume that the estimation bias @xmath1809of a third order estimator is less than @xmath1535  then even when eq.@xmath1810 holds with a strict inequality , @xmath1778 does not achieve a @xmath1532 rate of convergence because the fourth order bias @xmath1811exceeds @xmath1535  however , we will now construct an estimator @xmath1812 that under our assumptions @xmath512 does converge at rate @xmath1813 whenever @xmath1814 given in assumption @xmath1815 satisfy eq.@xmath1755 with a strict inequality . because the estimator is very complicated ,",
    "we have chosen to only define the estimator and give its properties in the text .",
    "the motivating ideas for and the formal proofs of these properties are provided in the appendix .    to define the estimator",
    ", we need some additional notation .",
    "define@xmath1816    where , @xmath1817 @xmath1818 with @xmath1617    then define @xmath1819 as @xmath1820 @xmath1821  is defined as @xmath1822 with @xmath1823 for @xmath1824 and @xmath1825 @xmath1826 next  @xmath1827 is defined as @xmath1822 with @xmath1823 for @xmath1828 and @xmath1829 @xmath1830 @xmath1831 @xmath1832 @xmath1833 we will use this notation for @xmath892 even though @xmath1834 does not depend on @xmath1835 and is equal to @xmath1836 of the previous subsection .",
    "finally define@xmath1837    [ beyond4th]given @xmath1814 satisfying eq.[ne2 ] with a strict inequality@xmath90 define @xmath1838 @xmath70to be the smallest integer such that @xmath1839 where @xmath1840 let @xmath1767 , @xmath1841 @xmath1778 be as in theorem [ uz ]  and define @xmath1842    then@xmath1843{c}k_{-1}^{2\\beta / d},\\left (   \\frac{\\log n}{n}\\right )   ^{-\\frac{\\beta_{g}}{d+2\\beta_{g}}}k_{2s}^{-\\beta_{b}/d}k_{2s+1}^{-\\beta_{p}/d},\\left ( \\frac{\\log n}{n}\\right )   ^{-\\frac{\\beta_{g}}{d+2\\beta_{g}}}k_{2s+1}^{-\\beta_{b}/d}k_{2s}^{-\\beta_{p}/d},\\\\ \\left (   \\frac{\\log n}{n}\\right )   ^{-\\frac{2\\beta_{g}}{d+2\\beta_{g}}}k_{0}^{-2\\beta / d},\\left (   \\frac{\\log n}{n}\\right )   ^{-\\frac{\\left ( m-1\\right )   \\beta_{g}}{d+2\\beta_{g}}}n^{-\\frac{\\beta_{b}}{d+2\\beta_{b}}-\\frac{\\beta_{p}}{d+2\\beta_{p}}},\\\\ \\left (   \\frac{\\log n}{n}\\right )   ^{-\\frac{2\\beta_{g}}{d+2\\beta_{g}}}\\underset{1\\leq s\\leq j}{\\max}\\left (   k_{2s}^{-\\beta_{b}/d}k_{0}^{-\\beta _ { p}/d},k_{0}^{-\\beta_{b}/d}k_{2s}^{-\\beta_{p}/d}\\right ) \\end{array } \\right ]   \\right ) \\\\ &   = o_{p}\\left (   \\max\\left [ \\begin{array } [ c]{c}k_{-1}^{2\\beta / d},\\left (   \\frac{\\log n}{n}\\right )   ^{-\\frac{\\beta_{g}}{d+2\\beta_{g}}}k_{2s}^{-\\beta_{b}/d}k_{2s+1}^{-\\beta_{p}/d},\\left ( \\frac{\\log n}{n}\\right )   ^{-\\frac{\\beta_{g}}{d+2\\beta_{g}}}k_{2s+1}^{-\\beta_{b}/d}k_{2s}^{-\\beta_{p}/d},\\\\ \\left (   \\frac{\\log n}{n}\\right )   ^{-\\frac{2\\beta_{g}}{d+2\\beta_{g}}}k_{0}^{-2\\beta / d},\\left (   \\frac{\\log n}{n}\\right )   ^{-\\frac{\\left ( m-1\\right )   \\beta_{g}}{d+2\\beta_{g}}}n^{-\\frac{\\beta_{b}}{d+2\\beta_{b}}-\\frac{\\beta_{p}}{d+2\\beta_{p}}}\\end{array } \\right ]   \\right ) \\\\ &   = o_{p}\\left (   n^{-\\frac{4\\beta}{d+4\\beta}}\\right)\\end{aligned}\\ ] ]    and @xmath1844    * inference : * elsewhere , we prove that @xmath1845 is asymptotically normal . here , to avoid the problem of unknown constants for confidence interval construction that we discussed in section [ dr_ci_section ] , we will construct nearly optimal rather than optimal confidence intervals .",
    "we suppose that eq .",
    "@xmath1755 holds with strict equality for the @xmath1846associated with the parameter space @xmath245 .",
    "then there exists @xmath1847 such that for all @xmath1848 @xmath1849 satisfies eq @xmath1755 with strict equality@xmath90 @xmath1850   } { var_{\\theta}\\left [   \\widehat{\\psi } _ { \\mathcal{k}_{j}}^{eff}\\left (   \\beta_{g},\\beta_{b}-\\sigma,\\beta_{p}-\\sigma\\right )   |\\widehat{\\theta}\\right ]   } \\right ]   = o_{p}\\left (   1\\right)\\ ] ] and @xmath1851   \\right\\ }   \\asymp n^{-\\frac{8\\left ( \\beta-\\sigma\\right )   } { d+4\\left (   \\beta-\\sigma\\right )   } } .\\ ] ] let @xmath1852   $ ] be a uniformly consistent estimator of ( the properly standardized ) @xmath1853 .",
    "then , for all @xmath1854 @xmath1855 \\right )   ^{-1}\\ ] ] converges uniformly in @xmath274 to a @xmath1241 .",
    "moreover , @xmath1856\\ ] ] is a conservative uniform asymptotic @xmath1243 confidence interval for @xmath175 with diameter of the order of @xmath1857    if eq.[ne2 ] holds with an equality and @xmath1767 , @xmath1841 @xmath1778 are as in the final paragraph of the preceding subsection then the proof of theorem [ beyond4th ] in the appendix implies @xmath1858",
    "in this section we describe how to construct adaptive confidence intervals ( i ) for a regression function @xmath1859   $ ] when the marginal of @xmath8 is unknown and ( ii ) for the treatment effect function and optimal treatment regime in a randomized clinical trial .",
    "* example 1a continued : * consider the case @xmath1860 , @xmath1861with @xmath1862 as usual , we assume for all @xmath383 @xmath130 and the density @xmath148 of @xmath8 are contained in known hlder balls @xmath1863 and @xmath1864 redefine @xmath1865   $ ] where @xmath1322 is an adaptive estimate of @xmath763 from the training sample and expectations and probabilities remain conditional on the training sample .",
    "adaptivity of @xmath1866implies that if @xmath1867 is also contained in a smaller hlder ball @xmath1868 @xmath1869 , then @xmath1322 will converge to @xmath130 under @xmath1870 at rate @xmath1871 robins and van der vaart ( 2006 ) showed that , when the marginal density @xmath87 of @xmath8 is known , the key to constructing optimal ( rate ) adaptive confidence balls for @xmath1872 was to find a rate optimal estimator of @xmath1873   .$ ] we shall show that their approach fails when the marginal of @xmath8 is unknown , but that a modification described below succeeds . specifically , if @xmath1874 lies in a smaller hlder ball @xmath1875 @xmath1876 our modification results in honest asymptotic confidence balls under @xmath1877 @xmath1878 whose diameter is ( essentially ) of the same order @xmath1879 as the diameter of robins and van der vaart s optimal adaptive region or ball , provided either @xmath1880 @xmath1881 and @xmath1882 or @xmath1883 @xmath1884 and eq.@xmath1885 holds with @xmath1886 this order is the maximum of the minimax rate @xmath1887 of convergence of @xmath1888 to @xmath1889 were @xmath1890 known to lie in @xmath1891 and the square root of the minimax rate of convergence of an estimator of @xmath1892 \\ $ ] in the larger model @xmath310 with @xmath763 and @xmath1342 only known to lie in @xmath1893and @xmath1864    the case where @xmath1884 and eq.@xmath1537 does not hold will be considered elsewhere .    now , since @xmath1894   = e_{\\theta}\\left [   \\widehat{b}\\left (   x\\right )   y\\right ]   , $ ]",
    "@xmath1895   = e_{\\theta}\\left [ \\left\\ {   b\\left (   x\\right )   \\right\\ }   ^{2}\\right ]   -2e_{\\theta}\\left [ \\widehat{b}\\left (   x\\right )   b\\left (   x\\right )   \\right ]   + e_{\\theta}\\left [ \\left\\ {   \\widehat{b}\\left (   x\\right )   \\right\\ }   ^{2}\\right]\\ ] ] has first order influence function @xmath1896 \\ $ ] where @xmath1897   -2\\widehat{b}\\left (   x\\right )   y+\\widehat{b}^{2}\\left (   x\\right )   , \\ ] ] @xmath70so @xmath1898 thus @xmath1899 for @xmath1900   $ ] differs from @xmath1901 for @xmath1902   $ ] only in @xmath1903 since the truncation bias @xmath1904 , higher order influence functions of @xmath930 and estimation bias do not depend on @xmath1905 , it follows that @xmath1906,@xmath1907 and @xmath1908 are identical for @xmath1909   $ ] and @xmath1910   .$ ] in contrast , @xmath1911 is identically zero for @xmath1865   $ ] but not for @xmath1912   .$ ] thus , by theorem @xmath1913  for @xmath1865   , $ ]  var@xmath1914   \\asymp\\frac{1}{n}\\left (   \\frac{k}{n}\\right )   ^{m-1}$ ]  if @xmath1915  and @xmath1916  and var@xmath1914   = 0 $ ]  if @xmath1440 and @xmath1917 in the case when @xmath1918  and @xmath1916 by the hoeffding decomposition,@xmath1919 = var_{\\theta}\\left (   \\sum_{s=1}^{m}\\left (   \\mathbb{d}_{s}^{\\left ( \\widehat{\\mathbb{\\psi}}_{m,\\widetilde{\\psi}_{k}}\\right )   } \\left ( \\theta\\right )   \\right )   \\right)\\ ] ] where @xmath1920 is a @xmath212th order degenerate u - statistic .",
    "further by theorem @xmath1913 we have@xmath1919 \\asymp\\max\\left (   var_{\\theta}\\left (   \\mathbb{d}_{1}^{\\left ( \\widehat{\\mathbb{\\psi}}_{m,\\widetilde{\\psi}_{k}}\\right )   } \\right ) , var_{\\theta}\\left (   \\mathbb{d}_{2}^{\\left (   \\widehat{\\mathbb{\\psi}}_{m,\\widetilde{\\psi}_{k}}\\right )   } \\right )   \\right)\\ ] ] as @xmath1921 for any @xmath1922 moreover , @xmath1923 since the kernel of @xmath1924 is of order @xmath1925 in summary @xmath1926 &   \\asymp\\max\\left (   \\frac{\\left\\vert \\left\\vert b\\left (   x\\right ) -\\widehat{b}\\left (   x\\right )   \\right\\vert \\right\\vert _ { 2}^{2}}{n},\\frac { k}{n^{2}}\\right ) \\\\ &   = \\max\\left (   n^{-\\frac{2\\beta_{b}/d}{1 + 2\\beta_{b}/d}-1},\\frac{k}{n^{2}}\\right)\\end{aligned}\\ ] ]  if @xmath1918  and @xmath1927 ( in contrast , for @xmath1928   , $ ]  var@xmath1929   \\asymp \\max\\left (   \\frac{1}{n},\\frac{k}{n^{2}}\\right )   = \\frac{1}{n}$ ]  if @xmath1440 ) .",
    "thus if @xmath1930 @xmath1880 @xmath1931 has @xmath1932 of @xmath1933 where @xmath1934 comes from equating the order @xmath1935 of @xmath1936 to the order @xmath1937 of the variance ( @xmath1938 for @xmath1939 @xmath1940 ) and @xmath1941 is the smallest integer @xmath135 such that the order @xmath1942 of @xmath1943is less than the order @xmath1944 of the standard error@xmath64 it follows that , for @xmath1930 in contrast to @xmath1910   , $ ] we can estimate @xmath1945   $ ] at ( the minimax ) rate @xmath1946 which is faster ( i.e. , less ) than the usual parametric rate of @xmath433    when @xmath1947 , the minimax rates for @xmath1948   $ ] and @xmath1949   $ ] are identical and , when eq.@xmath1537 holds , it follows from theorem [ beyond4th ]  that @xmath1950 achieves the minimax rate of @xmath1951 .    henceforth assume either @xmath1952 or @xmath1953 and eq.@xmath1537 holds .",
    "pick an @xmath1954 so that eq.@xmath1537 holds for @xmath1955 let @xmath1956 and define @xmath1957 and @xmath1958if @xmath1881 and @xmath1959 and @xmath1960   \\ $ ] if @xmath1961 note @xmath1962 is @xmath1963 uniformly over @xmath178 where @xmath245 is the parameter space with smoothness parameters @xmath1964 then , by eq.@xmath1537 and results in section [ case2 ] , as @xmath1965 @xmath1966pr@xmath1967 \\geq1-\\alpha.$ ]  thus , if @xmath175 were a function of @xmath179 only through @xmath597so @xmath1968 , the set @xmath1969 would be an uniform asymptotic @xmath1243 confidence region for @xmath1970 however , for @xmath1971   , $ ] this approach fails because @xmath270 also depends on @xmath179 through the unknown density @xmath87 of @xmath113 this approach succeeded in robins and van der vaart ( 2006 ) because @xmath87 was assumed known .",
    "we consider two solutions .",
    "the first gives ( near ) optimal adaptive honest intervals .",
    "the second would give honest , but non - optimal , intervals .",
    "the first solution is to replace @xmath175 with its empirical mean @xmath1972   \\ $ ] in eq.([cia])@xmath64 @xmath1973   n^{-1/2}\\right )   = o_{p}\\left (   n^{-\\left (   \\frac{2\\beta_{b}}{d+2\\beta_{b}}+\\frac{1}{2}\\right )   } \\right)\\ ] ] uniformly in @xmath357 it is straightforward to check that for all @xmath1974 @xmath1975 thus , for @xmath1854 @xmath1976 uniformly over @xmath1977 so @xmath1966pr@xmath1978   \\geq1-\\alpha$ ] and @xmath1979 \\leq\\widehat{\\psi}_{\\ } ^{\\ast}+z_{\\alpha}\\widehat{\\mathbb{w}}^{\\ast}\\right\\ } \\label{citrue}\\ ] ] is a uniform asymptotic @xmath1243 confidence region for @xmath1970 moreover , if @xmath1867 lies in a smaller hlder ball @xmath1868 @xmath1876 then , under @xmath1980 the diameter @xmath1981 since @xmath1982 and @xmath1983 and @xmath1962 are @xmath1984    the second , non - optimal , solution would be to replace the functional @xmath1865   $ ] with @xmath1985 the functional @xmath1986 is the first functional we have considered that is not in our doubly robust class of functionals . arguing as above ,",
    "if we can construct an asymptotically normal higher order @xmath187 statistic estimator @xmath1987 that converges to @xmath1986 at rate @xmath1988 on @xmath310 and a consistent estimator @xmath1989of its standard error , then @xmath1990 would be an honest adaptive confidence interval of diameter @xmath1991 we conjecture , based on arguments given elsewhere , that the minimax rate for estimation of @xmath1992 exceeds @xmath1993   $ ] whenever @xmath1994 . since @xmath1995for all",
    "@xmath1996 it follows that , when the marginal of @xmath8 is unknown and @xmath1997 , intervals based on @xmath1998 $ ] will , but intervals based on @xmath1999 will not , have diameter of the same order as the optimal interval with the marginal of @xmath8 known .",
    "* example 4 continued : * consider the case @xmath1860 , @xmath755 wp1 so we have data @xmath2000 , where @xmath12 is a binary treatment , @xmath0 is the response , and @xmath8 is a vector of prerandomization covariates .",
    "the randomization probabilities @xmath741 are known by design and @xmath2001 is the average treatment effects function . for @xmath1977 @xmath130 and the density @xmath1366 of @xmath8",
    "are contained in known hlder balls @xmath2002 and @xmath1864 suppose we have an adaptive estimator @xmath1322 of @xmath2003based on the training sample constructed as described below .",
    "now , since @xmath1894   = e_{\\theta}\\left [   \\widehat{b}\\left (   x\\right )   y|a=1\\right ] -e_{\\theta}\\left [   \\widehat{b}\\left (   x\\right )   y|a=0\\right ]   $ ] has influence function @xmath2004 = \\left (   a-\\pi_{0}\\left (   x\\right )   \\right )   \\sigma_{0}^{-2}\\left (   x\\right ) y\\widehat{b}\\left (   x\\right )   -e_{\\theta}\\left [   \\widehat{b}\\left (   x\\right ) b\\left (   x\\right )   \\right ]   , $ ] where @xmath2005 @xmath1865   $ ] has first order influence functions , indexed by arbitrary functions @xmath2006 @xmath2007   \\ $ ] with @xmath2008 thus @xmath1899 for @xmath2009   $ ] differs from @xmath1901 for @xmath2010   $ ] only in @xmath1903 it follows that all the properties of the confidence ball [ citrue ] for @xmath2011 in the setting of the last subsection remain true for @xmath2012 in the setting of this subsection .",
    "now define @xmath2013   .$ ] then it then follows that an honest @xmath159 uniform asymptotic confidence set for the optimal treatment regime @xmath2014   $ ] is given by @xmath2015 \\leq\\widehat{\\psi}_{\\ } ^{\\ast}+z_{\\alpha}\\widehat{\\mathbb{w}}^{\\ast}\\right\\ } .$ ]    * adaptive estimator of the treatment effect function : * one among many approaches to constructing a rate - adaptive estimator of @xmath763 is as follows .",
    "split the training sample into two random subsamples - a candidate estimator subsample of size @xmath1316 and a validation subsample of size @xmath1317 where both @xmath1318 and @xmath1319 are bounded away from @xmath590 as @xmath171 noting that @xmath2016   $ ] for all @xmath2017 we construct candidate estimators of @xmath130 as follows . for @xmath2018 @xmath2019 let @xmath2020 be the solution , if any , to the @xmath212 equations @xmath2021\\ ] ] where @xmath857 is a complete basis@xmath70wrt to lebesgue measure in @xmath1059 that provides optimal rate approximation for hlder balls and @xmath2022 is the empirical measure for the candidate estimator subsample .",
    "our candidates for @xmath1890 are the @xmath2023 robins ( 2004 ) proved that @xmath130 is the unique function @xmath2024 * *  * * minimizing @xmath2025   b^{\\ast}\\left ( x\\right )   \\right\\ }   ^{2}\\right ]   .$ ] in fact , the candidate @xmath2026 in our set for which @xmath2027is smallest is also the candidate that minimizes @xmath2028   $ ] since @xmath2029   .",
    "$ ] specifically , @xmath2030{c}\\sigma_{0}^{-2}\\left (   x\\right )   \\left\\ {   y-\\left [   a-\\pi_{0}\\left (   x\\right ) \\right ]   \\widehat{b}^{\\left (   s\\right )   } \\left (   x\\right )   \\right\\ }   ^{2}\\\\ -\\sigma_{0}^{-2}\\left (   x\\right )   \\left\\ {   y-\\left [   a-\\pi_{0}\\left ( x\\right )   \\right ]   b\\left (   x\\right )   \\right\\ }   ^{2}\\end{array } \\right ] \\\\ &   = e\\left [ \\begin{array } [ c]{c}\\sigma_{0}^{-2}\\left (   x\\right )   \\left (   a-\\pi_{0}\\left (   x\\right )   \\right ) \\left (   b\\left (   x\\right )   -\\widehat{b}^{\\left (   s\\right )   } \\left (   x\\right ) \\right )   \\times\\\\ \\left (   2\\left (   ab\\left (   x\\right )   -e\\left (   y|a=0,x\\right )   \\right ) -\\left (   a-\\pi_{0}\\left (   x\\right )   \\right )   \\left (   b\\left (   x\\right ) + \\widehat{b}^{\\left (   s\\right )   } \\left (   x\\right )   \\right )   \\right ) \\end{array } \\right ] \\\\ &   = e\\left (   \\sigma_{0}^{-2}\\left (   x\\right )   \\left (   a-\\pi_{0}\\left ( x\\right )   \\right )   a\\left (   b\\left (   x\\right )   -\\widehat{b}^{\\left (   s\\right ) } \\left (   x\\right )   \\right )   ^{2}\\right ) \\\\ &   = e\\left [   \\left (   b\\left (   x\\right )   -\\widehat{b}^{\\left (   s\\right ) } \\left (   x\\right )   \\right )   ^{2}\\right]\\end{aligned}\\ ] ]    we use these results to select among our candidates by cross - validation .",
    "let @xmath1322 be the @xmath2031 minimizing @xmath2032 \\widehat{b}^{\\left (   s\\right )   } \\left (   x\\right )   \\right\\ }   ^{2}\\right ]   $ ] over @xmath2033 , where @xmath2034 is the validation subsample empirical measure .",
    "if @xmath130 were known to lie in a hlder ball @xmath2035 it is easy to check that the candidate @xmath2036 with @xmath2037 obtains the optimal rate of @xmath2038 for estimating @xmath2039   .$ ] since the number of candidates at sample size @xmath5 is less than @xmath176 it then follows at once from van der laan and dudoit s ( 2003 ) results on model selection by cross validation that @xmath1322 is adaptive over hlder balls .",
    "in example @xmath2040 of section 3.1@xmath90 we considered the following problem .",
    "we were given a functional @xmath658 indexed by a real number @xmath2041 and the parameter @xmath357 the implicitly defined - functional @xmath643 was the assumed unique solution to @xmath2042 we noted that a @xmath664confidence set for @xmath643 is the set of @xmath665such that a @xmath664ci interval for @xmath658 contains @xmath2043 in the following subsection we derive the width of the confidence set for @xmath2044 we then generalize the problem in the second subsection by introducing the notions of the testing tangent space , a testing influence function , and the higher order efficient testing score . in the final subsection , we show how the two earlier subsections are related .      to derive the order of the length of the confidence interval for the parameter @xmath643  in example 1c , we can use the next theorem as follows .",
    "assume eq ( [ ne11 ] )  holds and @xmath2045 .",
    "then we can take the estimator @xmath2046  and rate @xmath2047 in the theorem to be the estimator @xmath2048  and rate @xmath2049  for a very small positive @xmath2050  and conclude that the length of the confidence interval for @xmath643  in example 1c to be @xmath2051    [ j1 ] * : * suppose for an estimator @xmath2052 and functional @xmath2053 there is a scale estimator @xmath2054 such that @xmath2055 in @xmath264probability @xmath2056 and @xmath2057 converges in law to @xmath2058uniformly for @xmath274 , @xmath2059 .",
    "then , ( i ) with @xmath1221 the @xmath1222quantile and @xmath2060 the cdf of a @xmath1223 the confidence set @xmath2061 is a uniform asymptotic @xmath159 confidence set for the ( assumed ) unique solution @xmath643 to @xmath2062 @xmath1883 the probability under @xmath179 that a sequence @xmath2063satisfying @xmath2064 is contained in @xmath2065 converges to @xmath18",
    "when @xmath2066 is @xmath2067 when @xmath2068 and converges to @xmath2069when @xmath2070 @xmath2071 if @xmath666 is uniformly twice continuously differentiable in @xmath2041 and _ _  _ _ @xmath2072 _ _  _ _ and _ _  _ _ @xmath2073 _ _  _ _ for constants _ _  _ _",
    "@xmath2074 then @xmath2075 holds for a sequence @xmath2076 satisfying @xmath2077    ( i ) : that @xmath2065 is a uniform asymptotic @xmath159 confidence set is immediate .",
    "( ii ) : now @xmath2078 ( iii ) : since @xmath2079 for some @xmath2080 between @xmath643 and @xmath2081 we have that @xmath2082 satisfies the assumption in ( ii ) .",
    "* remark : *  under some further regularity conditions , the solution @xmath2083 to @xmath2084 is asymptotically normal with mean @xmath643 and variance @xmath2085   $ ] uniformly over @xmath274,@xmath2086      in the following , we repeatedly use definitions from sec .",
    "2 , which might usefully be reviewed at this point .    *",
    "*  m**@xmath2087 *  order testing nuisance tangent space , testing tangent space , testing influence functions , efficient score , efficient information , and efficient testing variance : * given a model @xmath310 with parameter space @xmath245 and a functional @xmath2088 define @xmath2089 to be the submodel with parameter space @xmath2090 . thus @xmath2091 is the submodel with @xmath643 equal to @xmath2092 define , for @xmath2093 the @xmath135th order ( i ) testing nuisance tangent space @xmath2094 to be the @xmath1585 order tangent space for the submodel @xmath2095 ( ii ) testing tangent space @xmath2096 to be the closed linear span of @xmath2097 ( iiia ) set @xmath2098 of testing influence functions to be the orthocomplement of @xmath2094 in @xmath209 @xmath90 ( iiib ) set @xmath2099 of standardized testing influence functions to be @xmath2100   = var_{\\theta}\\left [   \\mathbb{if}_{1,\\tau\\left (   \\cdot\\right )   } ^{eff}\\left (   \\theta\\right )   \\right ]   \\right\\ }   , \\ ] ] @xmath2101 efficient testing score @xmath2102 to be @xmath2103   \\equiv \\pi_{\\theta}\\left [   \\mathbb{es}_{1}^{test}\\left (   \\theta\\right )   |\\gamma _ { m}^{nuis , test,\\perp}\\left (   \\theta,\\tau^{\\dagger}\\right )   \\right]\\ ] ] where @xmath2104 @xmath2105 efficient testing information to be @xmath2106 and @xmath2107 the efficient testing variance to be @xmath2108   ^{-1}.$ ]    further define , for @xmath383 the @xmath135th order ( i ) estimation nuisance tangent space @xmath2109 to be @xmath2110 @xmath2111 = 0\\right\\ }   , $ ] and ( ii ) efficient estimation variance to be @xmath2112   $ ] .    *",
    "remark : * for @xmath2113 the testing and estimation nuisance tangent spaces @xmath2114 and @xmath2115 are identical .",
    "however for @xmath1916 @xmath2114 is generally a strict subset of @xmath2116 for example , if the model can be parametrized as @xmath2117 and @xmath245 is the product of the parameter spaces for @xmath2041 and @xmath2118 the @xmath2114 is the space of mth order scores for @xmath2119 however , @xmath2120 also includes the mixed scores that have @xmath212 derivatives in the direction @xmath2041 and @xmath2121 derivatives in @xmath2122 directions .",
    "it is this strict inclusion that gives rise to higher order phenomena that do not occur in the first order theory .",
    "[ gg ] * : * suppose * *  * * @xmath2123 exists in @xmath229 then for @xmath2093 ( i ) the set of estimation nuisance scores @xmath2109 includes the set of testing nuisance scores @xmath2094 with equality of the sets when @xmath2124 , ( ii ) @xmath2125 is standardized if and only if @xmath2126   = 1\\ $ ] if and only if @xmath2127   = 1,$ ] ( iii)@xmath2128   ^{-1}\\mathbb{if}_{m,\\tau\\left (   \\cdot\\right )   } ^{test};\\text { } \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right )   } ^{test}\\in\\left\\ {   \\mathbb{if}_{m,\\tau\\left ( \\cdot\\right )   } ^{test}\\right\\ }   \\right\\ }   , \\ ] ] ( iv ) the set @xmath2129 of all mth order estimation influence functions is contained in @xmath2130 with equality of the sets when @xmath2113 ( v)@xmath2131   = \\left\\ {   var\\left [   \\mathbb{es}_{m}^{test}\\left (   \\theta\\right ) \\right ]   \\right\\ }   ^{-1}\\mathbb{es}_{m}^{test}\\left (   \\theta\\right )   , \\ ] ] ( vi ) @xmath2132   \\right\\ }   ^{-1}\\mathbb{es}_{m}^{test}\\left ( \\theta\\right )   \\in\\left\\ {   \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right ) } ^{std , test}\\right\\ }   $ ] and has the minimum variance @xmath2133   \\right\\ } ^{-1}$ ] among members of @xmath2134 . in particular @xmath2135   \\right\\ }   ^{-1}\\leq var_{\\theta}\\left [   \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right )   } ^{eff}\\left ( \\theta\\right )   \\right ]   $ ] with equality when @xmath2113 ( vii ) given @xmath2136,any smooth submodel @xmath243 with range containing @xmath179 and contained in @xmath2137and @xmath281 we have @xmath2138 /\\partial\\zeta_{l_{1}} ...",
    "\\partial\\zeta_{l_{_{s}}}|_{\\zeta=\\widetilde{\\theta } ^{-1}\\left\\ {   \\theta\\right\\ }   } = 0.\\ ] ] thus , if @xmath2139   $ ] is frchet differentiable w.r.t .",
    "@xmath285 to order @xmath286 for a norm",
    "@xmath2140 @xmath2141   = o\\left ( \\left\\vert \\left\\vert \\delta\\theta\\ \\right\\vert \\right\\vert ^{m+1}\\right )   $ ] for @xmath179 and @xmath2142 in an open neighborhood contained in @xmath2143 , since the taylor expansion of @xmath2144   \\ $ ] around @xmath179 through order @xmath135 is identically zero@xmath64    the proof of the theorem will use the following two lemmas :    [ a ] : for any @xmath2145@xmath2146 = e_{\\theta}\\left [   \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right )   } ^{test}\\left ( \\theta\\right )   \\mathbb{es}_{m}^{test}\\left (   \\theta\\right )   \\right]\\ ] ]    @xmath2147 \\\\ &   = e_{\\theta}\\left [   \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right )   } ^{test}\\pi_{\\theta}\\left [   \\mathbb{es}_{1}^{test}\\left (   \\theta\\right )   |\\gamma _ { m}^{nuis , test,\\perp}\\left (   \\theta,\\tau^{\\dagger}\\right )   \\right ]   \\right ] = e_{\\theta}\\left [   \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right )   } ^{test}\\mathbb{es}_{1}^{test}\\left (   \\theta\\right )   \\right ]   , \\end{aligned}\\ ] ]    where the last equality holds by @xmath2148    [ b ]  for any @xmath2149 @xmath2150 \\\\ &   = e\\left [   \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right )   } ^{test}\\left ( \\theta\\right )   \\mathbb{es}_{m}^{test}\\left (   \\theta\\right )   \\right ]   \\left\\ { var\\left [   \\mathbb{es}_{m}^{test}\\left (   \\theta\\right )   \\right ]   \\right\\ } ^{-1}\\mathbb{es}_{m}^{test}\\left (   \\theta\\right ) \\\\ &   = e\\left [   \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right )   } ^{test}\\left ( \\theta\\right )   \\mathbb{es}_{1}^{test}\\left (   \\theta\\right )   \\right ]   \\left\\ { var\\left [   \\mathbb{es}_{m}^{test}\\left (   \\theta\\right )   \\right ]   \\right\\ } ^{-1}\\mathbb{es}_{m}^{test}\\left (   \\theta\\right)\\end{aligned}\\ ] ]    @xmath2151 thus , by @xmath2152 @xmath2153    &   = \\pi_{\\theta}\\left [   \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right ) } ^{test}\\left (   \\theta\\right )   |\\left\\ {   c\\mathbb{es}_{m}^{test}\\left ( \\theta\\right )   ; c\\in r^{1}\\right\\ }   \\right ] \\\\ &   = e\\left [   \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right )   } ^{test}\\left ( \\theta\\right )   \\mathbb{es}_{m}^{test}\\left (   \\theta\\right )   \\right ]   \\left\\ { var\\left [   \\mathbb{es}_{m}^{test}\\left (   \\theta\\right )   \\right ]   \\right\\ } ^{-1}\\mathbb{es}_{m}^{test}\\left (   \\theta\\right )   .\\end{aligned}\\ ] ] now apply lemma [ a ] .",
    "( theorem [ gg ] ) ( i ) is immediate from the definitions .",
    "( ii ) and ( iiii ) follow from @xmath2154    & = 1\\leftrightarrow e\\left [   \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right )   } ^{test}\\left (   \\theta\\right )   \\mathbb{es}_{1}^{test}\\left (   \\theta\\right ) \\right ]   = 1\\\\ &   \\leftrightarrow e_{\\theta}\\left [   \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right ) } ^{test}\\mathbb{if}_{1,\\tau\\left (   \\cdot\\right )   } ^{eff}\\left (   \\theta\\right ) \\right ]   = var_{\\theta}\\left [   \\mathbb{if}_{1,\\tau\\left (   \\cdot\\right )   } ^{eff}\\left (   \\theta\\right )   \\right ]   , \\end{aligned}\\ ] ] where we have used lemma [ a ] . for",
    "( iv ) , note @xmath2155 follows from the fact that every smooth submodel through @xmath179 in model @xmath2091 is a smooth submodel through @xmath179 in model @xmath310 .",
    "thus it remains to prove that @xmath2156 is standardized .",
    "but , by part 4 of theorem [ eift ] , @xmath2157   = var_{\\theta}\\left [   \\mathbb{if}_{1,\\tau\\left (   \\cdot\\right )   } ^{eff}\\left (   \\theta\\right )   \\right ]   $ ] .",
    "( v ) follows at once from lemma [ a ] and part ( ii ) . for ( vi ) , note that @xmath2158   \\right\\ }   ^{-1}\\mathbb{es}_{m}^{test}\\left (   \\theta\\right ) \\in\\left\\ {   \\mathbb{if}_{m,\\tau\\left (   \\cdot\\right )   } ^{std , test}\\right\\ }   $ ] by definition .",
    "thus @xmath2159 ^{-1}\\mathbb{if}_{m,\\tau\\left (   \\cdot\\right )   } ^{test}\\right\\ }   \\geq\\left\\ { var_{\\theta}\\left [   \\mathbb{es}_{m}^{test}\\left (   \\theta\\right )   \\right ] \\right\\ }   ^{-1}\\ ] ] follows from ( v ) .",
    "the result then follows from part ( iii ) .",
    "part ( vii ) is proved analogously to theorem [ eiet ] except now all scores lie in @xmath2109 by range @xmath2160 in @xmath2161    in the case of ( locally ) nonparametric models , we can explicitly characterize @xmath2162 .",
    "let @xmath2163 be the set of all @xmath2164   $ ] with the @xmath2165 indexed by constants @xmath2166 and functions @xmath2167 satisfying @xmath2168   = 0.$ ] we remark that the subset of @xmath2169 comprised of all @xmath1564th order degenerate u - statistics can be written @xmath2170   \\right\\ }   $ ] .",
    "thus @xmath2163 simply restricts one of the functions @xmath2171 to be @xmath2172    [ ff ] if the model @xmath310 is ( locally ) nonparametric , then @xmath2173 .    * *  * * since the model is locally nonparametric @xmath2174 includes the set of all mean zero first order @xmath187statistics @xmath2175and thus any element of @xmath2162 must be a sum of degenerate @xmath340 of orders @xmath2176 through @xmath190 we continue by induction .",
    "first we prove the theorem for @xmath332 @xmath70now , @xmath2177 where @xmath2178 is the closed linear span of the 2nd order degenerate part @xmath2179 of 2nd order scores @xmath2180 in model @xmath2181 where @xmath2179 is a sum of products @xmath2182 of first order scores in model @xmath2183 for two different subjects . by model @xmath292 being ( locally )",
    "nonparametric , the set of first order scores in model @xmath2091 is precisely the set of random variables @xmath2184 orthogonal to @xmath2185 but the set of degenerate @xmath340 of order @xmath2176 orthogonal to the product of two scores in @xmath2186 is clearly @xmath2187 suppose now the theorem is true for @xmath2188 we show it is true for @xmath2189 by @xmath310 ( locally ) nonparametric and the induction assumption , @xmath2190 where @xmath2191 is the closed linear span of the sum of products of first order scores in model @xmath2192 for @xmath286 different subjects .",
    "but @xmath2193 is the set of set of degenerate @xmath340 of order @xmath286 orthogonal to @xmath2194      in the following theorem we show that estimation influence functions @xmath2195 for the parameter @xmath2196 evaluated at the solution @xmath643 to @xmath2197 is contained in the set @xmath2198 of testing influence functions for @xmath2044 we also derive the estimation influence functions @xmath2199 for @xmath643 in terms of the estimation influence functions @xmath2200 for @xmath2196 and their derivatives with respect to @xmath2201    [ tt ] let @xmath643 be the assumed unique functional defined by @xmath2202 then , for @xmath2203 , whenever @xmath2204 and @xmath2205 exist , ( i ) @xmath2206 ( ii ) @xmath2207 where @xmath2208 ( iii ) @xmath2209where @xmath2210 for @xmath2211 @xmath2212{c}\\left ( \\begin{array } [ c]{c}\\frac{\\partial if_{1,\\psi\\left (   \\tau^{\\dagger},\\cdot\\right )   , i_{1}}\\left ( \\theta\\right )   } { \\partial\\tau}\\\\ -e_{\\theta}\\left [   \\frac{\\partial if_{1,\\psi\\left (   \\tau^{\\dagger},\\cdot\\right )   , i_{1}}\\left (   \\theta\\right )   } { \\partial\\tau}\\right ] \\end{array } \\right )   if_{1,\\tau\\left (   \\cdot\\right )   , i_{2}}\\ \\left (   \\theta\\right ) \\\\ + \\left ( \\begin{array } [ c]{c}\\frac{\\partial if_{1,\\psi\\left (   \\tau^{\\dagger},\\cdot\\right )   , i_{2}}\\left ( \\theta\\right )   } { \\partial\\tau}\\\\ -e_{\\theta}\\left [   \\frac{\\partial if_{1,\\psi\\left (   \\tau^{\\dagger},\\cdot\\right )   , i_{2}}\\left (   \\theta\\right )   } { \\partial\\tau}\\right ] \\end{array } \\right )   if_{1,\\tau , i_{1}}\\left (   \\theta\\right ) \\end{array } \\right ] \\label{q22}\\ ] ] where @xmath2213 .",
    "@xmath2214 is given in the appendix as well as the general formula .",
    "\\(i ) for @xmath2215 consider any suitably smooth @xmath2216dimensional parametric submodel @xmath243 with range containing @xmath179 and contained in @xmath2161 let @xmath2217 be any associated @xmath2218 score @xmath2219 . by definition of @xmath2220 ,",
    "@xmath2221 hence , @xmath2222 now we expand the rhs using the chain rule and note that the only non - zero term is the term @xmath2223 in which all @xmath2224derivatives are taken with respect to the second @xmath2225 in @xmath2226 all other terms include derivatives of @xmath2227which are zero by range @xmath2228 further @xmath2229   $ ] by the definition of the estimation influence function @xmath2230 .",
    "we conclude that @xmath2204 is in @xmath2231 ( ii ) @xmath2232 @xmath2233 is straightforward .",
    "that @xmath2232 is contained in @xmath2234 follows by part ( iv ) of theorem [ gg ] .",
    "( iii ) see appendix for proof .",
    "we now provide an example to show that , contrary to what one might expect based on part ( vi ) of theorem [ gg ] , inference concerning @xmath2220 may be more efficient when based on an inefficient member of the set @xmath2235 such as @xmath2236 than when based on the efficient score @xmath2237 without loss of generality , it is sufficient to consider the case @xmath2238 . in the following example",
    "it is @xmath2239and @xmath2240 that play the role of @xmath643 and @xmath2241 in the preceding theorem , because @xmath2242 and @xmath2243 have , but @xmath643 and @xmath2244 do not have , higher order estimation and testing influence functions .",
    "* example 1c ( continued ) : * in this example , with @xmath2245 @xmath675and _ _  _ _ @xmath642 _ _  _ _ binary , @xmath659\\ ] ] and @xmath643 satisfies @xmath2246 .",
    "let @xmath2247 satisfy @xmath2248 where @xmath2249   -e_{\\theta}\\left\\ {   \\left [   \\pi_{\\theta}\\left [ b\\left (   \\tau\\right )   |\\overline{z}_{k}\\right ]   \\pi_{\\theta}\\left [ p|\\overline{z}_{k}\\right ]   \\right ]   \\right\\ }   $ ] is defined in section 3.1 with @xmath2041 a real - valued index and @xmath2250 . note @xmath2251",
    "-e_{\\theta}\\left [   \\left\\ {   \\pi_{\\theta } \\left [   p|\\overline{z}_{k}\\right ]   \\right\\ }   ^{2}\\right ]   \\right\\ }   , $ ] @xmath2252   , $ ] @xmath2253 below we freely use results of theorems [ tbrate ] , [ drhoif ] , and [ ebrate ] .",
    "we suppose that @xmath2254 and @xmath2255   < c$ ] for some @xmath2074 @xmath2256 choose @xmath2257 so the truncation bias of @xmath2258 is @xmath2259and @xmath2260   \\asymp k / n^{2}=n^{-2\\left (   \\frac{4\\beta}{4\\beta+d}+\\sigma\\right )   } .$ ] we assume the given @xmath1814 are such that the order @xmath2261   $ ] of the estimation bias of @xmath2262 is @xmath2263 then @xmath2264 and @xmath2265 are @xmath2266 which just exceeds the minimax rate @xmath2267 for @xmath2050 very small .",
    "our goal is to compare the coverage and length of confidence intervals for @xmath2242 and @xmath2268 based on@xmath70 _ _  _ _ @xmath2269 where @xmath2270 are appropriate variance estimators , @xmath391 is our usual split sample initial estimator , and @xmath2271 is an initial split sample estimator depending on @xmath2272 that satisfies @xmath2273 if @xmath2274 , i.e. , @xmath2275   = \\tau^{\\dagger},$ ] we assume that if @xmath2276 then the convergence rate under @xmath179 of our estimator of @xmath2277 for any @xmath2278 remains @xmath2279 . now",
    "the assumption @xmath2280   , $ ] @xmath2281   < c$ ] implies @xmath2282is uniformly bounded away from zero and infinity .",
    "it then follows from earlier results on @xmath2283 the assumption @xmath2284   , e_{\\theta}\\left [   a^{2}\\right ]   < c,$ ] and theorem [ tt ] that @xmath2285 is a uniform asymptotic @xmath159 confidence interval for both @xmath643 and @xmath2247 of length @xmath2286    the next theorem gives explicit formulae for @xmath2287 @xmath2288 and @xmath2289 using these formulae we calculate the biases and variances necessary to compare the coverage of the three intervals .",
    "this comparison requires each of our three candidate procedures to be on the same scale .",
    "therefore we used standardized versions of the relevant statistics .",
    "[ aa ] suppose the assumptions described in the preceding example hold . then    ( i)@xmath2290 \\\\ &   + \\mathbb{v}\\left [   \\left\\ {   \\left [   y^{\\ast}\\left (   \\tau\\right ) -\\widehat{b}\\left (   x,\\tau\\right )   \\right ]   \\overline{z}_{k}^{t}\\right\\ } _ { i_{1}}\\left\\ {   \\overline{z}_{k}\\left [   a-\\widehat{p}\\left (   x\\right ) \\right ]   \\right\\ }   _ { i_{2}}\\right]\\end{aligned}\\ ] ] where @xmath2291 @xmath2292    ( ii ) : let @xmath2293 denote @xmath2294 and @xmath2295 denote @xmath2296 thus,@xmath2297   -var\\left [ \\mathbb{u}_{2,2,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right )   } ^{\\ast , test,\\perp } \\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   , \\tau^{\\dagger } \\right )   \\right ]   \\right\\ }   ^{-1}\\\\ &   \\times\\left\\ {   \\mathbb{if}_{2,\\widetilde{\\psi}_{k}\\left (   \\tau^{\\dagger } , \\cdot\\right )   } \\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right ) \\right )   -\\mathbb{u}_{2,2,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right )   } ^{\\ast , test,\\perp}\\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right ) , \\tau^{\\dagger}\\right )   \\right\\}\\end{aligned}\\ ] ] where@xmath2298   \\right )   ^{-1}\\times\\widehat{\\epsilon}_{i}\\widehat{\\delta}_{i}\\\\ &   \\left\\ { \\begin{array } [ c]{c}-\\left\\ { \\begin{array } [ c]{c}\\left (   e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta } _ { i}^{2}\\right ]   \\right )   ^{-1}e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon } \\widehat{\\delta}^{2}\\overline{z}_{k}^{t}\\right ] \\\\",
    "\\times e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}^{2}\\widehat{\\delta } \\overline{z}_{k}^{t}\\right ]   \\widehat{\\epsilon}_{j}\\widehat{\\delta}_{j}\\end{array } \\right\\ } \\\\ + e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta}_{i}\\overline{z}_{k , i}^{t}\\right ]   \\overline{z}_{k , j}\\widehat{\\delta}_{j}\\\\ + e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}\\widehat{\\delta}_{i}^{2}\\overline{z}_{k , i}^{t}\\right ]   \\overline{z}_{k , j}\\widehat{\\epsilon}_{j}\\end{array } \\right\\}\\end{aligned}\\ ] ] and @xmath2299\\ ] ] also , @xmath2300 ( iii ) @xmath2301 where @xmath2302   \\right\\}\\ ] ] with @xmath2303    @xmath103@xmath103@xmath2304   $ ] @xmath103where @xmath2305{c}\\left [   \\left\\ {   a-\\widehat{p}\\left (   x\\right )   \\right\\ }   _ { i_{1}}^{2}-v\\left (   \\widehat{\\theta}\\right )   \\right ]   \\left [   \\left\\ { y-\\widehat{b}\\left (   x\\right )   \\right\\ }   \\left\\ {   a-\\widehat{p}\\left ( x\\right )   \\right\\ }   \\right ]   _ { i_{2}}+\\\\ \\left [   \\left\\ {   a-\\widehat{p}\\left (   x\\right )   \\right\\ }   _ { i_{2}}^{2}-v\\left (   \\widehat{\\theta}\\right )   \\right ]   \\left [   \\left\\ { y-\\widehat{b}\\left (   x\\right )   \\right\\ }   \\left\\ {   a-\\widehat{p}\\left ( x\\right )   \\right\\ }   \\right ]   _ { i_{1}}\\end{array } \\right]\\end{aligned}\\ ] ]    the proof of @xmath1880 was given earlier . the proofs of @xmath2075 and ( iii )",
    "are in the appendix .",
    "[ bb]@xmath64 * *  * * suppose @xmath2306 and the assumptions of the preceding theorem hold .",
    "@xmath2307@xmath2308then    \\(i ) @xmath2309   = o\\left (   \\frac{1}{n}\\right )   , $ ] @xmath2310   \\times\\left [ var_{\\theta}\\left\\ {   var_{\\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right ) } \\left\\ {   \\mathbb{es}_{2,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right )   } ^{test}\\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   \\right ) \\right\\ }   ^{-1}\\mathbb{es}_{2,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right ) } ^{test}\\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   \\right ) \\right\\ }   \\right ]   ^{-1}\\\\ &   = 1+o_{p}\\left (   1\\right)\\end{aligned}\\ ] ]    \\(ii ) @xmath2311    &   = o\\left (   \\frac{1}{n}\\right ) , \\\\ var_{\\theta}\\left [   v\\left (   \\widehat{\\theta}\\right )   ^{-1}\\ \\psi _ { 2,k\\ } \\left (   \\tau,\\widehat{\\theta}\\right )   \\right ]   /var_{\\theta}\\left\\",
    "{ \\tau_{2,k\\ } \\left (   \\widehat{\\theta}\\right )   -\\tau^{\\dagger}\\right\\ }    & = 1+o_{p}\\left (   1\\right)\\end{aligned}\\ ] ]    @xmath2071@xmath2312    &   = o_{p}\\left\\ {   \\left ( p-\\widehat{p}\\right )   \\left (   b\\left (   \\tau^{\\dagger}\\right )   -\\widehat{b}\\left (   \\tau^{\\dagger}\\right )   \\right )   \\left (   \\frac{g\\left (   x\\right ) } { \\widehat{g}\\left (   x\\right )   } -1\\right )   \\right\\ } \\\\ &   = o_{p}\\left (   n^{-\\left (   \\frac{\\beta_{g}}{2\\beta_{g}+d}+\\frac{\\beta_{b}}{d+2\\beta_{b}}+\\frac{\\beta_{p}}{d+2\\beta_{p}}\\right )   } \\right)\\end{aligned}\\ ] ]    \\(iv ) @xmath2313 \\\\ &   = o_{p}\\left\\ {   \\left (   p-\\widehat{p}\\right )   \\left (   b\\left (   \\tau ^{\\dagger}\\right )   -\\widehat{b}\\left (   \\tau^{\\dagger}\\right )   \\right )   \\left [ \\left (   \\frac{g\\left (   x\\right )   } { \\widehat{g}\\left (   x\\right )   } -1\\right ) + \\left (   p-\\widehat{p}\\right )   + \\left (   b\\left (   \\tau^{\\dagger}\\right ) -\\widehat{b}\\left (   \\tau^{\\dagger}\\right )   \\right )   \\right ]   \\right\\ } \\\\ &   = o_{p}\\left [   \\max\\left\\ {   n^{-\\left (   \\frac{\\beta_{g}}{2\\beta_{g}+d}+\\frac{\\beta_{b}}{d+2\\beta_{b}}+\\frac{\\beta_{p}}{d+2\\beta_{p}}\\right ) } , n^{-\\left (   \\frac{\\beta_{b}}{d+2\\beta_{b}}+\\frac{2\\beta_{p}}{d+2\\beta_{p}}\\right )   } , n^{-\\left (   \\frac{2\\beta_{b}}{d+2\\beta_{b}}+\\frac{\\beta_{p}}{d+2\\beta_{p}}\\right )   } \\right\\ }   \\right]\\end{aligned}\\ ] ]    ( v ) @xmath2314    &   = o_{p}\\left\\ { \\begin{array } [ c]{c}\\left (   p-\\widehat{p}\\right )   \\left (   \\frac{g\\left (   x\\right )   } { \\widehat{g}\\left (   x\\right )   } -1\\right )   \\left (   b-\\widehat{b}\\right )   + \\\\ \\left (   p-\\widehat{p}\\right )   ^{2}\\left (   p-\\widehat{p}\\right )   \\left ( b-\\widehat{b}\\right ) \\\\ + \\left (   \\frac{g\\left (   x\\right )   } { \\widehat{g}\\left (   x\\right )   } -1\\right ) ^{2}\\left [   \\left (   p-\\widehat{p}\\right )   + \\left (   \\frac{g\\left (   x\\right ) } { \\widehat{g}\\left (   x\\right )   } -1\\right )   + \\left (   b-\\widehat{b}\\right ) \\right ] \\end{array } \\right\\ } \\\\ &   = o_{p}\\left\\ {   \\max\\left\\ { \\begin{array } [ c]{c}n^{-\\left (   \\frac{\\beta_{g}}{2\\beta_{g}+d}+\\frac{\\beta_{p}}{d+2\\beta_{p}}+\\frac{\\beta_{b}}{d+2\\beta_{b}}\\right )   } , \\\\",
    "n^{-\\left (   \\frac{2\\beta_{p}}{d+2\\beta_{p}}\\right )   } n^{-\\frac{\\beta_{p}}{d+2\\beta_{p}}}n^{-\\frac{\\beta_{b}}{d+2\\beta_{b}}},\\\\ n^{-\\frac{2\\beta_{g}}{2\\beta_{g}+d}}\\left\\ {   n^{-\\frac{\\beta_{b}}{d+2\\beta _ { b}}}+n^{-\\frac{\\beta_{p}}{d+2\\beta_{p}}}++n^{-\\frac{\\beta_{g}}{2\\beta_{g}+d}}\\right\\ } \\end{array } \\right\\ }   \\right\\}\\end{aligned}\\ ] ]    the proof of part ( iii ) was given earlier .",
    "the remaining parts are proved in the appendix .",
    "we conclude from this theorem that the savings in variance that comes with using @xmath2315 rather than @xmath2316 is asymptotically negligible even in regard to constants .",
    "similarly , we conclude that the difference in variance that comes with using @xmath2317 rather than @xmath2318 is asymptotically negligible , again even in regard to constants .",
    "further , because @xmath2319   $ ] and @xmath2320   $ ] are of the order of @xmath2321 as their first order degenerate kernels are both of order @xmath2322 and @xmath2323   \\right\\ }   $ ] is asymptotically normal , we conclude that @xmath2324   \\right\\ }   , \\\\ &   n^{\\frac{4\\beta}{4\\beta+d}-\\sigma}\\left\\ {   \\mathbb{es}_{2,\\widetilde{\\tau } _ { k}\\left (   \\cdot\\right )   } ^{test}\\left (   \\widehat{\\theta}\\left ( \\tau^{\\dagger}\\right )   \\right )   \\right\\ }   ^{-1}\\left [   \\mathbb{es}_{2,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right )   } ^{test}\\left (   \\widehat{\\theta } \\left (   \\tau^{\\dagger}\\right )   \\right )   -e_{\\theta}\\left [   \\mathbb{es}_{2,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right )   } ^{test}\\left (   \\widehat{\\theta } \\left (   \\tau^{\\dagger}\\right )   \\right )   \\right ]   \\right]\\end{aligned}\\ ] ] and @xmath2325   \\right\\ } $ ] are all asymptotically normal with the same asymptotic variance .",
    "it then follows that a necessary condition for the intervals based on @xmath2326 @xmath2327 and @xmath2328 to cover @xmath2306 at the nominal @xmath159 level as @xmath596 is that @xmath2329   , \\\\ &   var_{\\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   } \\left\\ { \\mathbb{es}_{2,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right )   } ^{test}\\left ( \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   \\right )   \\right\\ } ^{-1}e_{\\theta}\\left [   \\ \\mathbb{es}_{2,\\widetilde{\\tau}_{k}\\ } ^{test}\\left ( \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   \\right )   \\right]\\end{aligned}\\ ] ] and @xmath2330   $ ] are @xmath2331    now we know under the assumptions of theorem [ bb ] that this necessary condition holds for @xmath2332   $ ] since @xmath2333 is bounded away from zero and one and , by assumption , @xmath2334 however this necessary condition need not hold for either @xmath2335\\ ] ] or @xmath2330   .$ ] for example , consider the following specification consistent with our assumptions : @xmath2336 , @xmath2337 then @xmath2338 so @xmath2339 however , @xmath2340 \\ $ ] converges to zero at rate @xmath2341 .",
    "next @xmath2342 \\\\ &   = o_{p}\\left (   n^{-\\left (   \\frac{\\beta_{b}}{d+2\\beta_{b}}+\\frac{2\\beta_{p}}{d+2\\beta_{p}}\\right )   } \\right )   = n^{-1/6}>>o_{p}\\left (   n^{-\\frac{4\\beta } { 4\\beta+d}+\\sigma}\\right )   = n^{-1/3+\\sigma}\\ ] ] for small @xmath2343 we conclude that the intervals based on @xmath2344 and @xmath2345 fail to cover @xmath2306 at the nominal @xmath159 level uniformly over @xmath245 as @xmath596 .",
    "we reach the identical conclusion with regard to the parameter @xmath643 because under our assumptions @xmath2346    furthermore , by the argument used in the proof of theorem [ tt ] , it is easy to see that the length of each interval is @xmath2347 it follows that if we try to improve the coverage of the intervals based on @xmath2344 and @xmath2328 by further increasing @xmath962 the length of the intervals will increase beyond @xmath2331 we conclude that the interval based on @xmath2348 is strictly preferred to the other two intervals when @xmath2349 , @xmath2350 and is never worse in terms of shrinkage rate and coverage than the other two intervals whatever be @xmath2351 , @xmath2352 and @xmath2353 .",
    "we reach the identical conclusion with regard to the coverage of the parameter @xmath643 because , under our assumptions including our choice of @xmath848 , @xmath2354 and @xmath2355 the order of the interval lengths .",
    "these results translate directly into analogous results concerning the associated estimators . under our assumptions",
    "the estimator solving @xmath2356 converges to both @xmath643 and @xmath2357 at rate @xmath2331 in contrast the rate of convergence of @xmath2358 and the estimator solving @xmath2359 converge to @xmath643 and @xmath2360 at the rates given in ( iv ) and ( v ) of theorem [ bb ] .",
    "what is the intuition behind the above findings ?",
    "first note that , as promised by theorem [ eiet ] and part ( vii ) of the theorem in the last subsection , the bias away from zero of @xmath2361   , e_{\\theta}\\left [   \\tau_{2,k}\\left (   \\widehat{\\theta}\\right ) -\\widetilde{\\tau}_{k}\\left (   \\theta\\right )   \\right ]   , $ ] and @xmath2362   $ ] are all @xmath2363 however the nature and convergence rate of the @xmath2364 term can vary markedly between estimators , attaining a minimum for @xmath2365   .$ ] now it is not surprising that , for the same order of variance , the order of @xmath2366   $ ] often exceeds that of @xmath2365   .$ ] confidence intervals for @xmath2247 based on @xmath2367 are centered at ( i.e are symmetric around ) @xmath2368 which is a quite stringent constraint on the form of the interval . in that sense , intervals based on @xmath2358 are a higher order generalization of the first order asymptotic wald intervals for @xmath2369 it is well known that when @xmath2242 is an implicit parameter that sets a functional such as @xmath2370 to zero , first - order wald confidence intervals are often outperformed in finite samples by confidence sets obtained by inverting a score - like test based on first order estimating functions for the functional that depend on the parameter @xmath2371 and , frequently , on estimated nuisance parameters as well , although this fact is not reflected in the first order asymptotics .",
    "our example is higher order version of this phenomenon , where the benefit of the interval @xmath2372 obtained by inverting tests based on the estimating function @xmath2348 for the functional @xmath2373 is clearly and quantitatively revealed by the asymptotics .",
    "note that , like first order wald intervals , the interval based on @xmath2374 will differ from the interval for @xmath2242 based on applying an inverse nonlinear monotone transform @xmath2375 to the end points of a wald interval for the transformed parameter @xmath2376 that is centered on @xmath2377 in contrast , like first order score - based intervals , the intervals based on @xmath2348 and @xmath2378 are invariant to monotone transformations of the parameter @xmath2242 .",
    "more interesting and perhaps more surprising is that , for the same order of variance , the order of @xmath2379   $ ] exceeds that of @xmath2365   .$ ] the surprise derives from a failure to recognize that the theorem [ gg ] is simply too general to help select among competing procedures .",
    "for example , this theorem implies that under law @xmath2380 ( a ) the variance @xmath2381 of @xmath2382   \\right ]   $ ] is less ( and generally strictly less ) than the variance of @xmath2383   , $ ] while ( b ) both have bias of @xmath2384 at first blush , this might suggest that the estimator solving @xmath2385 would likely have the same bias but smaller variance than the estimator solving @xmath2386 but we have seen that just the opposite is true .",
    "the reason is that the difference between the variances in ( a ) is negligible in the sense that their ratio is @xmath2387 , while the @xmath2388 biases are often of quite different orders with that of @xmath2383   $ ] always a minimum .",
    "more generally , whenever the functional @xmath658  is in our doubly robust class , eq [ ne11 ] holds so @xmath2389  is rate minimax ( or near minimax if @xmath2050 is chosen positive ) , and the suppositions of theorem [ j1 ] hold for @xmath2390  @xmath2391  theorem [ j1 ] then implies the width of the interval estimator for @xmath643  based on @xmath2392  converges to zero at the convergence rate of @xmath2393  to @xmath663",
    "in this section , we discuss the construction of higher order influence functions for a more general class of functionals than the one thus far considered .",
    "an important application of this construction is in the derivation of higher order influence functions in monotone missing data problems .  to begin",
    ", we must learn to construct higher order influence functions for a functional with the product form:@xmath2394 here @xmath2395 @xmath2396 are known to be higher order pathwise differentiable functionals and @xmath2397 is a known constant .    the following lemma gives the general form of higher order influence functions of @xmath2398 as a function of influence functions of @xmath2399  before stating our lemma , we need additional notation .",
    "given an ordered set of @xmath135 positive integers @xmath2400 for any @xmath276 non - negative integers @xmath2401 , satisfying @xmath2402 we define @xmath2403 to be an ordered partition of degree @xmath135 and order @xmath276 if and only if for @xmath2404 and @xmath2405 we have@xmath2406 @xmath2407{c}i_{j(s^{\\ast})+1},i_{j(s^{\\ast})+2}, ... ,i_{j(s^{\\ast})+t_{s^{\\ast}}}:\\\\ j(s^{\\ast})=\\\\ \\left\\ { \\begin{array } [ c]{c}\\sum\\limits_{q=1}^{s^{\\ast}-1}t_{q}\\text { \\ \\ \\ \\ \\ for } 1<s^{\\ast}\\leq r\\\\ 0\\text{\\ \\ \\ \\ \\ \\ \\ \\ \\ for \\ \\ } s^{\\ast}=1 \\end{array } \\right .",
    "\\end{array } \\right\\}\\ ] ] and for all @xmath2408 we have @xmath2409  any such ordered partition satisfies@xmath2410    [ hoiprod]let @xmath2411 for @xmath2412 be the @xmath2413 order influence function of @xmath2395 and define @xmath2414 then the @xmath197 order influence function of @xmath2398 is given by:@xmath2415\\end{aligned}\\ ] ] where @xmath2416   = \\mathbb{v}\\left [ { \\displaystyle\\sum\\limits_{\\left\\ {   t_{1}, ... t_{\\zeta}\\right\\ }   \\in \\upsilon_{\\zeta;j } } } { \\displaystyle\\prod\\limits_{s=1}^{\\zeta } } if_{\\psi_{s}\\left (   \\theta\\right )   ; t_{s},t_{s};\\overline{i}_{s , t_{s}}}\\left ( \\theta\\right )   \\right]\\]]@xmath2417    it is easy to generalize this lemma to functionals of the more general form @xmath2418 where both @xmath2419 are known constants ,  since the higher order influence function of a linear combination of functionals is the linear combination of the influence functions of the functionals .",
    "we next turn to the analysis of missing data models .",
    "suppose that we have derived the @xmath197 order influence function @xmath2420 @xmath2421 for a truncated parameter @xmath969 of a parameter @xmath175 in our doubly robust class of functionals based on i.i.d full data @xmath2422 ; then , according to theorem [ hoiprod ] the estimated @xmath2423 @xmath2424 is of the form @xmath2425    &   = \\sum_{1\\leq l\\leq k^{\\left (   j-1\\right )   } } \\left\\ {   \\mathbb{v}\\left (   \\prod\\limits_{s=1}^{j}u_{l , s}^{(j)}\\left (   l_{i_{s}}^{full};\\widehat{\\theta}\\right )   \\right ) \\right\\ } \\\\ &   = \\sum_{1\\leq l\\leq k^{\\left (   j-1\\right )   } } \\mathbb{v}\\left [ \\prod\\limits_{s=1}^{j}\\widehat{u}_{l , s , i_{s}}^{(j)}\\right]\\end{aligned}\\ ] ] where @xmath2426 @xmath2427 depends of one subject s data and has a functional form which depends on the form of @xmath547 corresponding to the functional of interest .",
    "for example , @xmath2428 _ { i_{1}}$]corresponds to the first component of the vector contributed by person @xmath2429 to @xmath2430 .",
    "note that @xmath2431   = 0.$ ]  next , suppose that for a subject some of the data may be missing , so that we observe @xmath2432 where @xmath2433   $ ] instead of the full data @xmath2434 for now , @xmath2435 is a known function of the full data and @xmath2436 is always observed .",
    "below we shall extend our results to monotone missing data .",
    "define @xmath2437 and we suppose @xmath2438if  we know @xmath2439 we can use @xmath2440\\ ] ] instead of @xmath2441   $ ] , where @xmath2442 is an arbitrary function with finite variance which satisfies @xmath2443   = 0.$ ]  it is easy to verify that this statistic is a function of the observed data and an unbiased estimator of@xmath2444   \\right]\\ ] ] so that in fact , in terms of rates , our observed data statistic has the same bias and variance properties as the original full data higher order influence function .  moreover , the most efficient ( in terms of constants ) choice for @xmath2445 in our class of estimators is @xmath2446 while @xmath2447 is an unknown function to be estimated from the observed data .",
    "this last two observations motivate our proposed strategy for mapping higher order influence functions in the full data model to higher order influence functions in the observed data model when the missingness mechanism is unknown .",
    "first , define @xmath2448   \\right ] \\\\ &   = e_{\\theta}\\left [   \\mathbb{v}\\left [   \\prod\\limits_{s=1}^{j}\\left ( \\frac{r_{i_{s}}\\left [   \\widehat{u}_{l , s , i_{s}}^{(j)}-b_{l , s}^{(j)}\\left ( l_{obs , i_{s}};\\theta\\right )   \\right ]   } { \\pi\\left (   l_{obs,,i_{s}};\\theta\\right )   } + b_{l , s}^{(j)}\\left (   l_{obs , i_{s}};\\theta\\right )   \\right ) \\right ]   \\right]\\end{aligned}\\ ] ] which we notice is of the product form that was discussed earlier in this section .",
    "in order to construct an @xmath197 order influence function for @xmath970 it is now apparent that we must first succeed in constructing @xmath197 order influence functions for the set of parameters @xmath2449",
    "so as to guarantee an estimation bias of order @xmath2450  now , for each @xmath2451 and @xmath212 , @xmath2452 is itself a member of our general doubly robust class so that @xmath2453   $ ] where @xmath2454 @xmath2455and @xmath2456 .",
    "thus , it immediately follows that neither @xmath2457 nor @xmath969 have higher order influence functions .",
    "we proceed by constructing influence functions for the set of truncated parameters @xmath2458 where @xmath2459 are appropriately truncated versions of @xmath2460 as in section 3.2.2 .",
    "we then define the truncated parameter @xmath2461 @xmath2462 note that @xmath2463 differs from the truncated parameters @xmath930 of section 3.2.2 , in that it depends on the order of the influence function we plan to base our inference on .",
    "the next theorem gives the @xmath197 order influence function of @xmath2464    [ map]let @xmath2465the @xmath197 order influence function of @xmath2463 is given by@xmath2466\\ ] ] where @xmath2467@xmath2468    the proof follows directly from the lemma [ hoiprod ] applied to each element @xmath2469    the @xmath197 order estimated influence function of @xmath2463 is given by@xmath2470\\ ] ] where @xmath2471\\]]@xmath2472    this result follows immediately from theorem [ map ] and the fact that @xmath2473 by definition .",
    "we are now ready to use our theorem to derive higher order influence functions for a functional @xmath175 from monotone missing data .",
    "we begin with the simple two - occasion case .",
    "let @xmath2474 @xmath2475 @xmath2476 be @xmath5 @xmath2477 copies constitute the full data .",
    "the outcome of interest , @xmath2478 is univariate . @xmath2479 and @xmath2480 are vectors of continuous covariates with dimensions @xmath2481 and @xmath2482 respectively .  the observed data @xmath2483 where both @xmath2484 and @xmath2485 are binary missing indicators .",
    "the outcome @xmath0  is observed if and only if @xmath2486",
    "@xmath2487 = @xmath470 while @xmath2488 is observed if and only if @xmath2486 @xmath2489 .",
    "the data is assumed to be missing at random , that is : @xmath2490 and @xmath2491  under this monotone missing - data pattern , the parameter of interest is given by @xmath2492    we impose the following assumptions : @xmath2493 @xmath2494   \\in h\\left (   \\beta_{b_{0}},c_{b_{0}}\\right )   ; $ ] @xmath2495 @xmath2496   \\in h\\left (   \\beta_{b_{1}},c_{b_{1}}\\right )   ; $ ] @xmath2497 @xmath2498 and @xmath2499 w.p . 1 @xmath2500 @xmath2501 and @xmath2502 w.p .",
    "1@xmath64 @xmath2503 the marginal density of @xmath2504 @xmath2505 falls in a hlder ball @xmath2506 and @xmath2507 w.p .",
    "1 , @xmath2508 @xmath2509 the marginal density of @xmath2510 @xmath2511 falls in a hlder ball @xmath2512 and @xmath2513 w.p .",
    "1 , @xmath2514    we define @xmath2515 , @xmath2516 with corresponding hlder exponents @xmath2517 and @xmath2518 respectively",
    ".    we next show how to apply theorem [ map ] in a nested fashion in order to derive higher order u - statistic estimators @xmath2519 in the observed data model .  in the first step of our procedure ,",
    "we derive higher order influence functions for a truncated parameter in the artificial missing data problem in which the observed data is given by @xmath2520 rather than @xmath2521  in the second step , we construct influence functions from a second artificial missing data problem with @xmath2522 now the full data and @xmath2523 the observed data .",
    "this final influence function is , in fact , the influence function for a truncated parameter in the original monotone missing data model .    to follow this nested procedure ,",
    "we derive a first stage class of estimators @xmath2524 which are functions of @xmath2525  in this model , @xmath2526 is the full data , @xmath2522 is the observed data , @xmath2484 is the missing indicator , @xmath2527 is a vector of always observed covariates , and @xmath2528 is the outcome which might be missing .",
    "since the parameter of interest @xmath175 is the marginal mean of @xmath2529 this is example 2a * *  * * of section 3.1**. * * therefore results from this example may be applied , hence @xmath2530 and @xmath2531    moreover , @xmath2532 so that we can define @xmath2533   ^{-1}e_{\\theta}\\left [   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{0}\\right )   \\overline{z}_{k_{0}}\\right ] \\\\ \\widetilde{\\pi}_{0}^{-1 } &   = \\widehat{\\pi}_{0}^{-1}\\left (   1-\\overline { z}_{k_{0}}^{t}e_{\\theta}\\left [   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline { z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right ]   ^{-1}e_{\\theta}\\left [   \\left ( \\frac{r_{0}}{\\widehat{\\pi}_{0}}-1\\right )   \\overline{z}_{k_{0}}\\right ]   \\right ) \\\\ \\widetilde{\\psi}^{\\dag}\\left (   \\theta\\right )    &   = e_{\\theta}\\left [ \\frac{r_{0}}{\\widetilde{\\pi}_{0}}\\left (   y-\\widetilde{b}_{0}\\right ) + \\widetilde{b}_{0}\\right]\\end{aligned}\\ ] ] with @xmath2534 and @xmath2535    moreover@xmath2536  @xmath2537 are rate optimal nonparametric estimators of @xmath2538 respectively estimated from the training sample and @xmath2539    from theorem [ tbrate ] and eq.([eb6 ] ) we also know that:@xmath2540   \\right)\\end{gathered}\\ ] ]    next , we proceed with the second step of our procedure , where @xmath2522 is now the full data and @xmath2523 becomes the observed data",
    ".  then , @xmath2541 if @xmath2542 or @xmath2543 therefore we may define a new missing indicator @xmath2544 with @xmath2545 in contrast with the first phase of our procedure , the full data influence function now has non - zero higher order contributions ; thus we can proceed with the strategy layed out in the first part of this section .",
    "we can put @xmath2546 in the format of eq.([sum ] ) as @xmath2547{c}\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{0}\\right ) z_{s_{1}}\\right )   _ { i_{1}}\\left (   \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}-1\\right )   z_{s_{j-1}}\\right )   _ { i_{j}}\\times\\\\{\\textstyle\\prod\\limits_{t=2}^{j-1 } } \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}z_{s_{t-1}}z_{s_{t}}-i\\left ( s_{t-1}=s_{t}\\right )   \\right )   _ { i_{t}}\\end{array } \\right\\ } \\\\ &   = \\left (   -1\\right )   ^{j-1}\\sum_{l=1}^{k_{0}^{j-1}}\\left\\ { \\begin{array } [ c]{c}\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{0}\\right ) z_{n\\left (   l,1\\right )   } \\right )   _ { i_{1}}\\left (   \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}-1\\right )   z_{_{n\\left (   l , j-1\\right )   } } \\right )   _ { i_{j}}\\times\\\\{\\textstyle\\prod\\limits_{t=2}^{j-1 } } \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}z_{n\\left (   l , t-1\\right )   } z_{n\\left ( l , t\\right )   } -i\\left (   n\\left (   l , t-1\\right )   = n\\left (   l , t\\right )   \\right ) \\right )   _ { i_{t}}\\end{array } \\right\\}\\end{aligned}\\ ] ]    where @xmath2548 is a one - to - one mapping that indexes all permutations of @xmath2549 and @xmath2550 is the @xmath781th entry of @xmath2551 we define  @xmath1939 @xmath2552@xmath2553{c}$\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{0}\\right )   z_{n\\left ( l,1\\right )   } $ \\ \\ \\ for{}{}{}$s=1$\\\\ $ \\left\\ { \\begin{array } [ c]{c}\\frac{r_{0}}{\\widehat{\\pi}_{0}}z_{n\\left (   l , s-1\\right )   } z_{n\\left ( l , s\\right )   } \\\\",
    "-i\\left (   n\\left (   l , s-1\\right )   = n\\left (",
    "l , s\\right )   \\right ) \\end{array } \\right\\ }   $ \\ for { } { } $ 1<s < j$\\\\ $ \\left (   -1\\right )   ^{j-1}\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}-1\\right ) z_{_{n\\left (   l , j-1\\right )   } } $ \\ { } for { } $ s = j$\\end{tabular } \\ \\right .",
    "\\\\ \\tau_{l , s}^{\\left (   j\\right )   } \\left (   \\theta\\right )    &   \\equiv e_{\\theta } \\left (   \\widehat{u}_{l , s}^{\\left (   j\\right )   } \\right)\\end{aligned}\\ ] ]    let @xmath2554 which is always observed",
    ". then @xmath2555{c}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{0}\\right )   z_{n\\left ( l,1\\right )   } -\\\\ e\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{0}\\right ) z_{n\\left (   l,1\\right )   } |v\\right ) \\end{array } \\right ]   + e_{\\theta}\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left ( y-\\widehat{b}_{0}\\right )   z_{n\\left (   l,1\\right )   } |v\\right )   \\right ) \\\\ = e_{\\theta}\\left (   \\frac{r_{1}}{\\pi_{1}\\left (   \\theta\\right )   } \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   yz_{n\\left (   l,1\\right )   } -b_{1}\\left ( \\theta\\right )   z_{n\\left (   l,1\\right )   } \\right )   + \\frac{r_{0}}{\\widehat{\\pi } _ { 0}}\\left (   b_{1}\\left (   \\theta\\right )   z_{n\\left (   l,1\\right ) } -\\widehat{b}_{0}z_{n\\left (   l,1\\right )   } \\right )   \\right)\\end{gathered}\\ ] ]    thus , @xmath2556 falls into the doubly - robust @xmath581 class of functionals with @xmath2557 @xmath2558 and corresponding @xmath2559 @xmath2560 @xmath2561 @xmath2562 for any @xmath2563 @xmath2564 is a known function of the observed data , thus @xmath2565 and @xmath2566 for any @xmath337 moreover , choosing @xmath2567 @xmath2568",
    "so that : @xmath2569{c}\\widehat{b}_{1}z_{n\\left (   l,1\\right )   } + \\overline{w}_{k_{1}}^{t}e_{\\theta } \\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right )   ^{-1}\\\\ \\times e_{\\theta}\\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{1}\\right )   z_{n\\left (   l,1\\right ) } \\overline{w}_{k_{1}}\\right ) \\end{array } \\right\\ } \\\\ \\widetilde{\\pi}_{1}^{-1 } &   = \\left\\ {   \\widehat{\\pi}_{1}^{-1}\\left ( \\begin{array } [ c]{c}1-\\overline{w}_{k_{1}}^{t}e_{\\theta}\\left [   \\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right ]   ^{-1}\\\\ \\times e_{\\theta}\\left [   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}-1\\right )   \\overline{w}_{k_{1}}\\right ] \\end{array } \\right )   \\right\\}\\end{aligned}\\ ] ] where @xmath2570 and @xmath2571 is a @xmath2572dimensional vector of tensor product basis for functions of @xmath2573 from theorem [ map ] , for @xmath2574 @xmath2575{c}r_{1}\\widetilde{\\pi}_{1}^{-1}\\left (   \\theta\\right )   \\frac{r_{0}}{\\widehat{\\pi } _ { 0}}\\left (   yz_{n\\left (   l,1\\right )   } -\\widetilde{b_{1}\\left ( \\theta\\right )   z_{n\\left (   l,1\\right )   } } \\right ) \\\\ + \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\widetilde{b_{1}\\left (   \\theta\\right ) z_{n\\left (   l,1\\right )   } } -\\widehat{b}_{0}z_{n\\left (   l,1\\right )   } \\right ) \\end{array } \\right)\\end{aligned}\\ ] ] have higher order influence functions @xmath2576 .",
    "let @xmath2577 for any @xmath2578 and @xmath2579 be the higher order influence functions of @xmath2580 .    for @xmath2581",
    "define @xmath2582    @xmath2583 also belongs to the @xmath2584 class of models with @xmath2585 @xmath2586 @xmath2587 @xmath2588 @xmath2589 @xmath2590 we may choose @xmath2591 @xmath2592so that @xmath2593{c}\\widehat{b}_{1}+\\overline{w}_{k_{1}}^{t}e_{\\theta}\\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right )   ^{-1}\\\\ \\times e_{\\theta}\\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{1}\\right )   \\overline{w}_{k_{1}}\\right ) \\end{array } \\right\\ } \\\\ \\widetilde{\\pi}_{1}^{-1 }   &   = \\left\\ {   \\widehat{\\pi}_{1}^{-1}\\left ( \\begin{array } [ c]{c}1-\\overline{w}_{k_{1}}^{t}e_{\\theta}\\left [   \\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right ]   ^{-1}\\\\ \\times e_{\\theta}\\left [   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}-1\\right )   \\overline{w}_{k_{1}}\\right ] \\end{array } \\right )   \\right\\}\\end{aligned}\\ ] ] and@xmath2594 has higher order ifs @xmath2595    define @xmath2596 then @xmath1939 @xmath2597@xmath2598    with @xmath2599{c}\\frac{r_{1,i}}{\\widehat{\\pi}_{1,i}}\\frac{r_{0,i}}{\\widehat{\\pi}_{0,i}}\\left ( y_{i}-\\widehat{b}_{1,i}\\right )   + \\\\",
    "\\frac{r_{0,i}}{\\widehat{\\pi}_{0,i}}\\left (   \\widehat{b}_{1,i}-\\widehat{b}_{0,i}\\right )   + \\widehat{b}_{0,i}-\\psi\\left (   \\widehat{\\theta}\\right ) \\end{array } \\right\\}\\ ] ]    and @xmath1939  @xmath2600@xmath2601{c}\\left\\ { \\begin{array } [ c]{c}\\left [   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\left ( y-\\widehat{b}_{1}\\right )   \\right ]   _ { i_{1}}\\overline{w}_{k_{1},i_{1}}^{t}\\times\\\\{\\displaystyle\\prod\\limits_{s=2}^{j-1 } } \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right )   _ { i_{s}}\\overline { w}_{k_{1},i_{j}}\\left [   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}-1\\right )   \\right ]   _ { i_{j}}\\end{array } \\right\\ } \\\\{\\textstyle\\sum\\limits_{t=2}^{j-1 } } \\left\\ { \\begin{array } [ c]{c}\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}-1\\right )   \\right )   _ { i_{j}}\\overline{w}_{k_{1},i_{j}}^{t}{\\textstyle\\prod\\limits_{l = t+1}^{j-1 } } \\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right )   _ { i_{l}}\\\\ \\times\\left [   \\overline{w}_{k_{1}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{1}\\right )   \\overline{z}_{k_{0}}^{t}\\right ]   _ { i_{1}}\\times\\\\{\\displaystyle\\prod\\limits_{s=2}^{t-1 } } \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right )   _ { i_{s}}\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}-1\\right )   _ { i_{t}}\\overline{z}_{k_{0,}i_{t}}\\end{array } \\right\\ } \\\\ + \\left\\ { \\begin{array } [ c]{c}\\left [   \\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left ( y-\\widehat{b}_{1}\\right )   + \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left ( \\widehat{b}_{1}-\\widehat{b}_{0}\\right )   \\right ]   _ { i_{1}}\\overline{z}_{k_{0},i_{1}}^{t}\\times\\\\{\\displaystyle\\prod\\limits_{s=2}^{j-1 } } \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right )   _ { i_{s}}\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}-1\\right )   _ { i_{j}}\\overline{z}_{k_{0,}i_{j}}\\end{array } \\right\\ } \\end{array } \\right]\\end{aligned}\\ ] ]    this follows directly from theorem [ map ] and the fact that @xmath2602 @xmath2603 for @xmath2604    suppose the class of estimators @xmath2605 are defined as @xmath2606    then@xmath2607{c}e_{\\theta}\\left (   \\psi\\left (   \\widehat{\\theta}\\right )   + \\mathbb{if}_{m,\\widetilde{\\widetilde{\\psi}}_{k_{0},k_{1},m}\\left (   \\theta\\right ) } \\left (   \\widehat{\\theta}\\right )   -\\widetilde{\\widetilde{\\psi}}_{k_{0},k_{1},m}\\left (   \\theta\\right )   \\right ) \\\\ + \\left (   \\widetilde{\\widetilde{\\psi}}_{k_{0},k_{1},m}\\left (   \\theta\\right ) -e_{\\theta}\\left [   \\mathbb{if}_{m,\\widetilde{\\psi}^{\\dag}\\left ( \\theta\\right )   } \\left (   \\widehat{\\theta}\\right )   + \\psi\\left (   \\widehat{\\theta } \\right )   \\right ]   \\right )",
    "\\\\ + \\left [   e_{\\theta}\\left (   \\mathbb{if}_{m,\\widetilde{\\psi}^{\\dag}}\\left ( \\widehat{\\theta}\\right )   + \\psi\\left (   \\widehat{\\theta}\\right )   \\right ) -\\widetilde{\\psi}^{\\dag}\\left (   \\theta\\right )   \\right ]   + \\left ( \\widetilde{\\psi}^{\\dag}\\left (   \\theta\\right )   -\\psi\\left (   \\theta\\right ) \\right ) \\end{array } \\right\\ } \\label{bsp}\\ ] ]    the following theorem examines the bias and variance of @xmath2608define @xmath2609 @xmath2610 and @xmath2611 for @xmath2612 let @xmath2613 and @xmath2614    [ btomm]suppose conditions @xmath2615 hold then@xmath2616\\ ] ] and @xmath1939  @xmath1927@xmath2617 where @xmath2618{c}\\left\\ { \\begin{array } [ c]{c}e_{\\theta}\\left\\ {   \\left [   q_{0}\\delta b_{0}\\right ]   \\overline{z}_{k_{0}}^{t}\\right\\ }   e_{\\theta}\\left [   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right ]   ^{-1}\\times\\\\ \\left [   e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right )   -i\\right ]   ^{m-1}e_{\\theta}\\left [   \\overline{z}_{k_{0}}\\delta p_{0}\\right ] \\end{array } \\right\\ }   _ { -\\left (   eb_{1}^{\\left (   1\\right )   } \\right )   }",
    "\\\\ + \\left\\ { \\begin{array } [ c]{c}e_{\\theta}\\left\\ {   \\left [   q_{01}\\delta b_{1}\\right ]   \\overline{w}_{k_{1}}^{t}\\right\\ }   e_{\\theta}\\left [   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right ]   ^{-1}\\times\\\\ \\left [   e_{\\theta}\\left (   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right )   -i\\right ]   ^{m-1}e_{\\theta}\\left [   \\overline{w}_{k_{1}}q_{0}\\delta p_{1}\\right ] \\end{array } \\right\\ }   _ { -\\left (   eb_{1}^{\\left (   2\\right )   } \\right )   } \\\\",
    "+ { \\displaystyle\\sum\\limits_{j=2}^{m-1 } } \\left\\ { \\begin{array } [ c]{c}e_{\\theta}\\left [   \\overline{z}_{k_{0}}\\delta p_{0}^{t}\\right ]   \\left [ e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right ) \\right ]   ^{j-2}\\times\\\\ e_{\\theta}\\left [   q_{01}\\delta b_{1}\\overline{z}_{k_{0}}\\overline{w}_{k_{1}}^{t}\\right ]   e_{\\theta}\\left [   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right ]   ^{-1}\\times\\\\ e_{\\theta}\\left (   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right ) ^{m - j}e_{\\theta}\\left [   \\overline{w}_{k_{1}}q_{0}\\delta p_{1}\\right ] \\end{array } \\right\\ }   _ { -\\left (   eb_{jj}^{\\left (   2\\right )   } \\right )   } \\\\ + e_{\\theta}\\left\\ {   \\left [   q_{0}\\delta p_{1}\\delta b_{1}\\right ]   \\overline { z}_{k_{0}}^{t}\\right\\ }   \\left [   e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right )   -i\\right ]   ^{m-2}e_{\\theta}\\left [ \\overline{z}_{k_{0}}\\delta p_{0}\\right ]   _ { -\\left (   eb_{mm}^{\\left (   2\\right ) } \\right )   } \\end{array } \\right\\ } \\label{ebm}\\]]@xmath2619{c}\\left\\ { \\begin{array } [ c]{c}\\left\\vert \\left\\vert \\widehat{f}_{0}\\right\\vert \\right\\vert _ { \\infty } \\left\\vert \\left\\vert q_{0}^{-1/2}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert q_{0}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\frac{\\widehat{f}_{0}}{f_{0}}\\right\\vert \\right\\vert",
    "_ { \\infty}\\times\\\\ \\left\\vert \\left\\vert \\delta g_{0}\\right\\vert \\right\\vert",
    "_ { \\infty}^{m-1}\\left\\ {   \\int\\left (   b_{0}-\\widehat{b}_{0}\\right )   ^{2}dl_{0}\\right\\ } ^{1/2}\\times\\\\ \\left\\vert \\left\\vert \\frac{f_{0}}{\\pi_{0}\\widehat{\\pi}_{0}}\\right\\vert \\right\\vert _ { \\infty}\\left\\ {   \\int\\left (   \\pi_{0}-\\widehat{\\pi}_{0}\\right ) ^{2}dl_{0}\\right\\ }   ^{1/2}\\end{array } \\right\\ } \\\\ + \\left\\ { \\begin{array } [ c]{c}\\left\\vert \\left\\vert \\widehat{f}_{1}\\right\\vert \\right\\vert _ { \\infty } \\left\\vert \\left\\vert q_{01}^{-1/2}\\right\\vert \\right\\vert _ { \\infty } \\left\\vert \\left\\vert q_{01}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\frac{\\widehat{f}_{1}}{f_{1}}\\right\\vert \\right\\vert _",
    "{ \\infty } \\times\\\\ \\left\\vert \\left\\vert \\delta g_{1}\\right\\vert \\right\\vert _ { \\infty}^{m-1}\\left\\ {   \\int\\left (   b_{1}-\\widehat{b}_{1}\\right )   ^{2}dl_{0}dl_{1}\\right\\ }   ^{1/2}\\\\ \\times\\left\\vert \\left\\vert \\frac{\\pi_{0}f_{1}}{\\widehat{\\pi}_{0}\\pi _ { 1}\\widehat{\\pi}_{1}}\\right\\vert \\right\\vert _ { \\infty}\\left\\ {   \\int\\left ( \\pi_{1}-\\widehat{\\pi}_{1}\\right )   ^{2}dl_{0}dl_{1}\\right\\ }   ^{1/2}\\end{array } \\right\\ } \\\\ + { \\displaystyle\\sum\\limits_{j=2}^{m-1 } } \\left\\ { \\begin{array } [ c]{c}\\left\\vert \\left\\vert \\frac{f_{1}}{\\widehat{f}_{1}}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\frac{f_{0}^{2}}{\\widehat{f}_{0}\\widehat{\\pi } _ { 0}^{2}}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\frac{\\widehat{f}_{1}}{f_{1}}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\frac{\\pi_{0}f_{1}}{\\widehat{\\pi}_{0}\\pi_{1}\\widehat{\\pi}_{1}}\\right\\vert \\right\\vert",
    "_ { \\infty}\\\\ \\left\\vert \\left\\vert q_{01}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\delta b_{1}\\right\\vert \\right\\vert",
    "_ { \\infty}\\left\\vert \\left\\vert \\delta g_{0}\\right\\vert \\right\\vert",
    "_ { \\infty}^{j-2}\\left\\ {   \\int\\left ( \\pi_{0}-\\widehat{\\pi}_{0}\\right )   ^{2}dl_{0}\\right\\ }   ^{1/2}\\times\\\\ \\left\\vert \\left\\vert q_{01}^{-1/2}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\delta g_{1}\\right\\vert \\right\\vert _ { \\infty}^{m - j}\\left\\ { \\int\\left (   \\pi_{1}-\\widehat{\\pi}_{1}\\right )   ^{2}dl_{0}dl_{1}\\right\\ }   ^{1/2}\\end{array } \\right\\ }",
    "\\\\ + \\left\\ { \\begin{array } [ c]{c}\\left\\vert \\left\\vert \\frac{f_{1}}{\\widehat{f}_{1}\\widehat{\\pi}_{1}^{2}}q_{0}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\delta b_{1}\\right\\vert \\right\\vert _ { \\infty}\\left\\ {   \\int\\left (   \\pi_{1}-\\widehat{\\pi}_{1}\\right )   ^{2}dl_{0}dl_{1}\\right\\ }   ^{1/2}\\\\ \\times\\left\\vert \\left\\vert \\frac{f_{0}^{2}}{\\widehat{f}_{0}\\widehat{\\pi}_{0}^{2}}\\right\\vert \\right\\vert \\left\\vert \\left\\vert \\delta g_{0}\\right\\vert \\right\\vert",
    "_ { \\infty}^{m-2}\\left\\ {   \\int\\left (   \\pi_{0}-\\widehat{\\pi}_{0}\\right )   ^{2}dl_{0}\\right\\ }   ^{1/2}\\end{array } \\right\\ } \\end{array } \\right ] \\label{ebm2}\\\\ &   = o_{p}\\left (   \\max\\left [ \\begin{array } [ c]{c}\\left (   \\frac{\\log n}{n}\\right )   ^{\\frac{\\left (   m-1\\right )   \\beta_{g_{0}}}{d_{0}+2\\beta_{g_{0}}}}n^{-\\left (   \\frac{\\beta_{b_{0}}}{d_{0}+2\\beta_{b_{0}}}+\\frac{\\beta_{\\pi_{0}}}{d_{0}+2\\beta_{\\pi_{0}}}\\right )   } , \\\\ \\left (   \\frac{\\log n}{n}\\right )   ^{\\frac{\\left (   m-1\\right )   \\beta_{g_{1}}}{d_{1}+2\\beta_{g_{1}}}}n^{-\\left (   \\frac{\\beta_{b_{1}}}{d_{1}+2\\beta_{b_{1}}}+\\frac{\\beta_{\\pi_{1}}}{d_{1}+2\\beta_{\\pi_{1}}}\\right )   } , \\\\{\\displaystyle\\sum\\limits_{j=2}^{m } } \\left (   \\frac{\\log n}{n}\\right )   ^{\\frac{\\beta_{b_{1}}}{d_{1}+2\\beta_{b_{1}}}+\\frac{\\left (   j-2\\right )   \\beta_{g_{0}}}{d_{0}+2\\beta_{g_{0}}}+\\frac{\\left ( m - j\\right )   \\beta_{g_{1}}}{d_{1}+2\\beta_{g_{1}}}}n^{-\\frac{\\beta_{\\pi_{0}}}{d_{0}+2\\beta_{\\pi_{0}}}-\\frac{\\beta_{\\pi_{1}}}{d_{1}+2\\beta_{\\pi_{1}}}}\\end{array } \\right ]   \\right ) \\label{ebm3}\\ ] ] and@xmath2620{c}e_{\\theta}\\left (   \\pi_{\\theta}^{\\bot}\\left (   q_{0}^{1/2}\\delta b_{0}|\\left ( q_{0}^{1/2}\\overline{z}_{k_{0}}\\right )   \\right )   \\pi_{\\theta}^{\\bot}\\left ( q_{0}^{-1/2}\\delta p_{0}|\\left (   q_{0}^{1/2}\\overline{z}_{k_{0}}\\right ) \\right )   \\right ) \\\\ + e_{\\theta}\\left (   \\pi_{\\theta}^{\\bot}\\left (   q_{01}^{1/2}\\delta b_{1}|\\left ( q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right )   \\pi_{\\theta}^{\\bot}\\left ( q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right )   \\right ) \\end{array } \\right\\ }   + i\\left (   m>2\\right )   \\times\\nonumber\\\\ &   \\left\\ { \\begin{array } [ c]{c}-e_{\\theta}\\left [ \\begin{array } [ c]{c}\\pi_{\\theta}^{\\bot}\\left [   \\left ( \\begin{array } [ c]{c}\\frac{\\pi_{1}^{1/2}}{\\widehat{\\pi}_{1}^{1/2}}\\delta b_{1}\\pi_{\\theta}\\left ( q_{0}^{1/2}\\delta p_{0}|\\left (   q_{0}^{1/2}\\overline{z}_{k_{0}}\\right ) \\right ) \\end{array } \\right )   |\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\\\ \\times\\pi_{\\theta}^{\\bot}\\left [   q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left ( q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\end{array } \\right ]   _ { -\\left (   tb\\left (   m,2\\right )   \\right )   } \\\\ + \\left (   -1\\right )   ^{m}e_{\\theta}\\left [ \\begin{array } [ c]{c}\\pi_{\\theta}^{\\bot}\\left [   \\left ( \\begin{array } [ c]{c}q_{01}^{1/2}\\delta b_{1}e_{\\theta}\\left [   \\delta p_{0}\\overline{z}_{k_{0}}^{t}\\right ]   e_{\\theta}\\left [   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right ]   ^{-1}\\\\ \\times\\left (   e_{\\theta}\\left [   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right ]   \\right )   ^{m-2}\\overline{z}_{k_{0}}\\end{array } \\right )   |\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\\\ \\times\\pi_{\\theta}^{\\bot}\\left [   q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left ( q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\end{array } \\right ]   _ { -\\left (   tb\\left (   m,3\\right )   \\right )   } \\end{array } \\right\\ } \\label{tbm}\\]]@xmath2621{c}k_{0}^{-\\left (   \\beta_{b_{0}}+\\beta_{\\pi_{0}}\\right )   /d_{0}},k_{1}^{-\\left ( \\beta_{b_{1}}+\\beta_{\\pi_{1}}\\right )   /d_{1}},k_{1}^{-\\beta_{\\pi_{1}}/d_{1}}k_{0}^{-\\beta_{\\pi_{0}}/d_{0}}\\left (   \\frac{\\log n}{n}\\right )   ^{\\frac { \\beta_{b_{1}}}{d_{1}+\\beta_{b_{1}}}}\\\\ k_{1}^{-\\left (   \\min\\left (   \\beta_{\\pi_{0}},\\beta_{b_{1}}\\right )   + \\beta _ { \\pi_{1}}\\right )   /d_{1}},\\left (   \\frac{\\log n}{n}\\right )   ^{-\\frac { \\beta_{b_{1}}}{d_{1}+\\beta_{b_{1}}}-\\frac{\\left (   m-2\\right )   \\beta_{g_{0}}}{d_{0}+\\beta_{g_{0}}}}n^{-\\frac{\\beta_{\\pi_{0}}}{d_{0}+\\beta_{\\pi_{0}}}}k_{1}^{-\\beta_{\\pi_{1}}/d_{1}}\\end{array } \\right ]   \\right ) \\label{tbm2}\\ ] ] moreover , @xmath2622    the optimal choice of @xmath2623 and @xmath2624 in this class of higher order estimators , will depend on the size of the effective smoothness exponents @xmath2625,@xmath2626 and @xmath2627 relative to the size of the exponents @xmath2628,@xmath2629 and @xmath2630  a general prescription for finding an optimal estimator in our class is to choose a pair @xmath2631 that minimizes the maximum asymptotic mse over the model among the candidate @xmath2632 here ,  the estimator @xmath2633 uses the pair @xmath2634 of @xmath135 that equates the order of the variance to the order of the maximum between the squared truncation and estimation biases ( which are given in the theorem )      theorem [ map ] can be applied in a nested fashion to the estimation of general functionals in nonparametric models with monotone missingness ( that is with an arbitrary number of occasions ) . building on the two - occasion case ,",
    "@xmath2635 where @xmath2636 is the missing indicator for the third occasion .",
    "we also write @xmath2637 and @xmath2638 let @xmath2639 and @xmath2640 is a @xmath2641dimensional vector of tensor product basis for functions of @xmath2642 with finite variance .",
    "the truncated parameter @xmath2643 is given by : @xmath2644 where for all @xmath2645 @xmath2646 @xmath2647{c}1-\\overline{v}_{k_{2}}^{t}e_{\\theta}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline { v}_{k_{2}}\\overline{v}_{k_{2}}^{t}\\right )   ^{-1}\\\\ \\times e_{\\theta}\\left [   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}-1\\right ) \\overline{v}_{k_{2}}\\right ] \\end{array } \\right )   \\right\\ } \\\\ \\widetilde{b}_{2}^{\\left (   3\\right )   }   &   = \\left\\ { \\begin{array } [ c]{c}\\widehat{b}_{2}+\\overline{v}_{k_{2}}^{t}e_{\\theta}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi } _ { 0}}\\overline{v}_{k_{2}}\\overline{v}_{k_{2}}^{t}\\right )   ^{-1}\\\\ \\times e_{\\theta}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{2}\\right )   \\overline{v}_{k_{2}}\\right ) \\end{array } \\right\\ } \\\\ \\widetilde{w_{s}b}_{2}^{\\left (   3\\right )   }   &   = \\left\\ { \\begin{array } [ c]{c}\\widehat{b}_{2}w_{s}+\\overline{v}_{k_{2}}^{t}e_{\\theta}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi } _ { 0}}\\overline{v}_{k_{2}}\\overline{v}_{k_{2}}^{t}\\right )   ^{-1}\\\\ \\times e_{\\theta}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{2}\\right )   w_{s}\\overline{v}_{k_{2}}\\right ) \\end{array } \\right\\ } \\\\",
    "\\widetilde{z_{t}w_{s}b}_{2}^{\\left (   3\\right )   }   &   = \\left\\ { \\begin{array } [ c]{c}\\widehat{b}_{2}z_{t}w_{s}+\\overline{v}_{k_{2}}^{t}e_{\\theta}\\left ( \\frac{r_{2}}{\\widehat{\\pi}_{2}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{v}_{k_{2}}\\overline{v}_{k_{2}}^{t}\\right ) ^{-1}\\\\ \\times e_{\\theta}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{2}\\right )   z_{t}w_{s}\\overline{v}_{k_{2}}\\right ) \\end{array } \\right\\ } \\\\ \\widetilde{z_{t}b}_{2}^{\\left (   3\\right )   }   &   = \\left\\ { \\begin{array } [ c]{c}\\widehat{b}_{2}z_{t}+\\overline{v}_{k_{2}}^{t}e_{\\theta}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi } _ { 0}}\\overline{v}_{k_{2}}\\overline{v}_{k_{2}}^{t}\\right )   ^{-1}\\\\ \\times e_{\\theta}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widehat{b}_{2}\\right )   z_{t}\\overline{v}_{k_{2}}\\right ) \\end{array } \\right\\}\\end{aligned}\\ ] ] and@xmath2648{c}e\\left (   \\frac{r_{2}}{\\widetilde{\\pi}_{2}^{\\left (   3\\right )   } } \\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   y-\\widetilde{b}_{2}^{\\left (   3\\right )   } \\right )   + \\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\widetilde{b}_{2}^{\\left (   3\\right )   } -\\widehat{b}_{1}\\right )   \\right ) \\\\ e\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\widehat{b}_{1}-\\widehat{b}_{0}\\right )   + \\widehat{b}_{0}-\\psi\\left (   \\widehat{\\theta}\\right )   \\right ) \\end{array } \\right\\}\\ ] ] @xmath2649{c}\\left\\ {   \\left (   a_{s}\\right )   _ { 1\\times k_{1}}\\left [   e\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\frac{r_{1}}{\\widehat{\\pi}_{1}}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right )   \\right ]   ^{j-2}e\\left [   \\overline { w}_{k_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\frac{r_{1}}{\\widehat{\\pi } _ { 1}}-1\\right )   \\right ]   \\right\\ } \\\\",
    "+ { \\textstyle\\sum\\limits_{q=2}^{j-1 } } \\left\\ { \\begin{array } [ c]{c}e\\left (   \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}-1\\right )",
    "\\right )   \\overline{w}_{k_{1}}^{t}\\right ) \\left [   e\\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right )   \\right ] ^{j - q-1}\\\\ \\times\\left (   b_{s , t}\\right )   _ { k_{1}\\times k_{0}}\\times\\\\ \\left [   e\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right )   \\right ]   ^{q-2}e\\left [   \\left (   \\frac { r_{0}}{\\widehat{\\pi}_{0}}-1\\right )   \\overline{z}_{k_{0}}\\right ] \\end{array } \\right\\ } \\\\ + \\left\\ {   \\left (   c_{t}\\right )   _ { 1\\times k_{0}}\\times\\left [   e\\left ( \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right )   \\right ]   ^{j-2}e\\left [   \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}-1\\right )   \\overline{z}_{k_{0}}\\right ]   \\right\\ } \\end{array } \\right]\\end{aligned}\\ ] ]    where @xmath2650 we only give out the final expression for @xmath2651 without any technical details ; bias and variance properties of this estimator will be published elsewhere .",
    "@xmath2652 { \\displaystyle\\prod\\limits_{q=2}^{r-1 } } \\left (   \\frac{r_{0}r_{1}r_{2}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}\\widehat{\\pi } _ { 2}}\\overline{v}_{k_{2}}\\overline{v}_{k_{2}}^{t}-i\\right )   _ { i_{q}}\\left [ \\overline{v}_{k_{2}}\\frac{r_{0}r_{1}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}-1\\right )   \\right ]   _ { i_{r}}\\\\ &   + { \\displaystyle\\sum\\limits_{m=2}^{r-1 } } \\left\\ { \\begin{array } [ c]{c}\\begin{array } [ c]{c}\\left [   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}-1\\right )   \\right ]   _ { i_{1}}\\overline{w}_{k_{1}}^{t}{\\displaystyle\\prod\\limits_{q=2}^{m-1 } } \\left (   \\frac{r_{0}r_{1}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}}\\overline { w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right )   _ { i_{q}}\\times\\\\ \\left [   \\frac{r_{0}r_{1}r_{2}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}\\widehat{\\pi } _ { 2}}\\left (   y-\\widehat{b}_{2}\\right )   \\overline{w}_{k_{1}}\\overline { v}_{k_{2}}^{t}\\right ]   _ { i_{m}}{\\displaystyle\\prod\\limits_{s = m+1}^{r-1 } } \\left (   \\frac{r_{0}r_{1}r_{2}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}\\widehat{\\pi } _ { 2}}\\overline{v}_{k_{2}}\\overline{v}_{k_{2}}^{t}-i\\right )   _ { i_{s}}\\times\\\\ \\left [   \\overline{v}_{k_{2}}\\frac{r_{0}r_{1}}{\\widehat{\\pi}_{0}\\widehat{\\pi } _ { 1}}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}-1\\right )   \\right ]   _ { i_{r}}\\end{array } \\\\ + { \\displaystyle\\sum\\limits_{j=2}^{m-1 } } \\left [ \\begin{array } [ c]{c}\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}-1\\right )   _ { i_{1}}\\overline{z}_{k_{0},i_{1}}^{t}{\\displaystyle\\prod\\limits_{s=2}^{j-1 } } \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right )   _ { i_{s}}\\left [   \\frac{r_{0}r_{1}r_{2}}{\\widehat{\\pi } _ { 0}\\widehat{\\pi}_{1}\\widehat{\\pi}_{2}}\\left (   y-\\widehat{b}_{2}\\right ) \\overline{z}_{k_{0}}\\overline{v}_{k_{2}}^{t}\\right ]   _ { i_{j}}\\\\ \\times{\\displaystyle\\prod\\limits_{q = j+1}^{r+j - m-1 } } \\left (   \\frac{r_{0}r_{1}r_{2}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}\\widehat{\\pi } _ { 2}}\\overline{v}_{k_{2}}\\overline{v}_{k_{2}}^{t}-i\\right )   _ { i_{q}}\\left [ \\overline{v}_{k_{2}}\\frac{r_{0}r_{1}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}-1\\right )   \\overline{w}_{k_{1}}^{t}\\right ]   _ { i_{r+j - m}}\\times\\\\{\\displaystyle\\prod\\limits_{q = r+j - m+1}^{r-1 } } \\left (   \\frac{r_{0}r_{1}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}}\\overline { w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right )   _ { i_{q}}\\left [   \\overline { w}_{k_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\frac{r_{1}}{\\widehat{\\pi } _ { 1}}-1\\right )   \\right ]   _ { i_{r}}\\end{array } \\right ] \\\\ + \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}-1\\right )   _ { i_{1}}\\overline{z}_{k_{0},i_{1}}^{t}{\\displaystyle\\prod\\limits_{s=2}^{m-1 } } \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right )   _ { i_{s}}\\left [   \\overline{z}_{k_{0}}\\frac{r_{0}r_{1}r_{2}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}\\widehat{\\pi}_{2}}\\left ( y-\\widehat{b}_{2}\\right )   \\overline{v}_{k_{2}}^{t}\\right ]   _ { i_{m}}\\times\\\\{\\displaystyle\\prod\\limits_{q = m+1}^{r-1 } } \\left (   \\frac{r_{0}r_{1}r_{2}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}\\widehat{\\pi } _ { 2}}\\overline{v}_{k_{2}}\\overline{v}_{k_{2}}^{t}-i\\right )   \\left [ \\overline{v}_{k_{2}}\\frac{r_{0}r_{1}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}}\\left (   \\frac{r_{2}}{\\widehat{\\pi}_{2}}-1\\right )   \\right ]   _ { i_{r}}\\end{array } \\right\\}\\end{aligned}\\]]@xmath2653{c}\\left [ \\begin{array } [ c]{c}\\frac{r_{0}r_{1}r_{2}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}\\widehat{\\pi}_{2}}\\left (   y-\\widehat{b}_{2}\\right )   + \\\\ \\frac{r_{0}r_{1}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}}\\left (   \\widehat{b}_{2}-\\widehat{b}_{1}\\right ) \\end{array } \\right ]   _ { i_{1}}\\overline{w}_{k_{1},i_{1}}^{t}{\\displaystyle\\prod\\limits_{q=2}^{r-1 } } \\left (   \\frac{r_{0}r_{1}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}}\\overline { w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right )   _ { i_{q}}\\left [   \\overline { w}_{k_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\frac{r_{1}}{\\widehat{\\pi } _ { 1}}-1\\right )   \\right ]   _ { i_{r}}\\\\ + { \\displaystyle\\sum\\limits_{j=2}^{r-1 } } \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}-1\\right )   _ { i_{1}}\\overline{z}_{k_{0},i_{1}}^{t}{\\displaystyle\\prod\\limits_{s=2}^{j-1 } } \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right )   _ { i_{s}}\\left [   \\left ( \\begin{array } [ c]{c}\\frac{r_{0}r_{1}r_{2}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}\\widehat{\\pi}_{2}}\\left (   y-\\widehat{b}_{2}\\right ) \\\\ + \\frac{r_{0}r_{1}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}}\\left (   \\widehat{b}_{2}-\\widehat{b}_{1}\\right ) \\end{array } \\right )   \\overline{z}_{k_{0}}\\overline{w}_{k_{1}}^{t}\\right ]   _ { i_{j}}\\times\\\\{\\displaystyle\\prod\\limits_{q = j+1}^{r-1 } } \\left (   \\frac{r_{0}r_{1}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}}\\overline { w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right )   _ { i_{q}}\\left [   \\overline { w}_{k_{1}}\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\frac{r_{1}}{\\widehat{\\pi } _ { 1}}-1\\right )   \\right ]   _ { i_{r}}\\\\ + \\left [   \\left ( \\begin{array } [ c]{c}\\frac{r_{0}r_{1}r_{2}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}\\widehat{\\pi}_{2}}\\left (   y-\\widehat{b}_{2}\\right ) \\\\ + \\frac{r_{0}r_{1}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}}\\left (   \\widehat{b}_{2}-\\widehat{b}_{1}\\right ) \\\\ + \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\widehat{b}_{1}-\\widehat{b}_{0}\\right ) \\end{array } \\right )   \\right ]   _ { i_{1}}\\overline{z}_{k_{0},i_{1}}^{t}{\\displaystyle\\prod\\limits_{s=2}^{r-1 } } \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right )   _ { i_{s}}\\overline{z}_{k_{0},i_{r}}\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}-1\\right )   _ { i_{r}}\\end{array } \\right\\}\\ ] ]    99 arellano m. ( 2003 ) \" _ panel data econometrics \" .",
    "_ oxford university press : advanced texts in econometrics .",
    "bhattacharyya a. ( 1947 ) `` on some analogues of the amount of information and their use in statistical estimation ii - iii ''  _ sankhy _ , vol.8 , 3 , 201218 .",
    "bickel p. , klassen c. , ritov y. , wellner j. ( 1993 ) `` _ efficient and adaptive estimation for semiparametric models _ '' .",
    "springer .",
    "bickel p. , and ritov y. ( 2003 ) `` nonparametric estimators which can be ' ' plugged - in \" .",
    "_ annals of statist .",
    "_ 31(4 ) , 103353 .    birge l. , massart p.(1995 ) `` estimation of integral functionals of a density '' .",
    "_ annals of statistics_. 23(1 ) , 11 - 29 .",
    "he , x. and shao , q.m .",
    "`` on parameters of increasing dimension '' .",
    "_ journal of multivariate analysis _ , 73(1 ) , 120 - 135 .",
    "klassen c. ( 1987 ) `` consistent estimation of the influence function of locally asymptotically linear estimators '' .",
    "_ annals of statistics .",
    "_ 15(4 ) , 1548 - 62 .    lee a.j .",
    "( 1990 ) `` _ _ u - statistics : theory and practice _ _ '' .",
    "marcel dekker , new york .",
    "lindsay r , waterman b. ( 1996 ) `` projected score methods for approximating conditional scores '' .",
    "_ biometrika _ 83(1):1 - 13",
    ".    li l. , tchetgen e. , van der vaart aw , and robins jm .",
    "( 2006 ) `` robust inference with higher order inference functions : part ii . ''",
    "2005 _ jsm proceedings . _ american statistical associations .",
    "2558 - 2565 .",
    "mallat sg .",
    "( 1998 ) `` _ _ a wavelet tour of signal processing _ _ '' .",
    "acedemic press .",
    "newey w. , hsieh f. , and robins jm .",
    "( 2004 ) `` twicing kernels and a small bias property of semiparametric estimators '' _ econometrica _ 72 , 947 - 962 .",
    "pfanzagl j. ( 1990 ) `` _ _ estimation in semiparametric models : some recent developments _ _ '' .",
    "lecture notes in statistics 31 , springer - verlag , berlin 1985 , 505 pages    portnoy s. ( 1988 ) .",
    "`` asymptotic behavior of likelihood methods for exponential families when the number of parameters tends to infinity '' .",
    "_ annals of statistics .",
    "_ 16 , 356 - 366    pyke r. , `` spacings ( with discussion ) '' , _",
    "b _ 27 ( 1965 ) , 395449 .",
    "ritov y. , bickel p. ( 1990 ) `` achieving information bounds in non- and semi - parametric models '' . _ annals of statistics .",
    "_ 18 , 925 - 938 .",
    "robins j. , ritov , y ( 1997 ) .",
    "`` toward a curse - of - dimensionality appropriate ( coda ) asymptotic theory for semiparametric models '' . _ statistics in medicine .",
    "_ 16 285 - 319 .",
    "robins jm .",
    "( 2004 ) `` optimal structural nested models for optimal sequential decisions '' . in dy lin and p. heagerty ( eds . ) _ proceedings of the second seattle symposium in biostatistics_. new york springer .",
    "robins jm , rotnitzky a. ( 2001).comment on the bickel and kwon article , `` inference for semiparametric models : some questions and an answer  statistica sinica , 11(4):920 - 936 . [ ' ' on double robustness . \" ]    robins j.m , li l , tchetgen eric , van der vaart aw ( 2007 ) , `` asymptotic normality of degenerate u - statistics '' . working paper .",
    "robins j.m , van der vaart aw .",
    "( 2006 ) `` adaptive nonparametric confidence sets '' .  _",
    "annals of statistics_. 34(1):229 - 253 .",
    "robins jm , li l , tchetgen e , van der vaart a. ( 2008 ) . higher order influence functions and minimax estimation of nonlinear functionals . probability and statistics :",
    "essays in honor of david a. freedman 2:335 - 421 .",
    "small d. , mcleish c.(1994 ) `` _ _ hilbert space methods in probability and statistical inference _ _ '' .",
    "tchetgen e. , li l. , van der vaart aw , and robins jm .",
    "( 2006 ) `` robust inference with higher order inference functions : part i. '' 2005 _ jsm proceedings .",
    "_ american statistical associations .",
    "2644 - 2651 .",
    "tchetgen e. , li l. , van der vaart aw , and robins jm .",
    "( 2007 ) `` higher order u - statistics estimators for longitudinal missing data and causal inference models '' .",
    "working paper    van der laan m and dudoit s.(2003 ) .",
    "asymptotics of cross - validated risk estimation in estimator selection and performance assessment . technical report .",
    "van der laan m , and robins jm .",
    "( 2003 ) \" _ unified methods for censored longitudinal data and causality \" _",
    "springer series in statistics .",
    "wang , l. , brown , l. d. , cai , t. levine , m. ( 2006 ) .",
    "`` effect of mean on variance function estimation in nonparametric regression '' .",
    "technical report .",
    "cai , t. , levine , m. wang , l. ( 2006 ) `` variance function estimation in multivariate nonparametric regression '' . technical report .",
    "van der vaart , aw ( 1991 ) .",
    "`` on differentiable functionals '' .",
    "statist_. 19 178 - 204 .",
    "van der vaart , aw ( 1998 ) .",
    "`` _ _ asymptotic statistics _ _ '' .",
    "cambridge series in statistical and probabilistic mathematics .",
    "in the following , we assume all parametric submodels are sufficiently smooth and regular that expectation and differentiation operators commute as needed",
    ". we also define @xmath2654 to be @xmath2655    ( theorem [ eiet ] ) define the bias function @xmath2656   $ ] of @xmath272 to be @xmath2657   .$ ] @xmath70  define @xmath2658 = \\partial^{s}b_{m}\\left [   \\widetilde{\\theta}\\left (   \\varsigma^{\\ast}\\right ) , \\widetilde{\\theta}\\left (   \\varsigma\\right )   \\right ]   /\\partial\\varsigma _ { l_{1} ... }^{\\ast}\\partial\\varsigma_{l_{j}}^{\\ast}\\partial\\varsigma _ { l_{j+1} ... }\\partial\\varsigma_{l_{s}}|_{\\varsigma^{\\ast}=\\widetilde{\\theta } ^{-1}\\left\\ {   \\theta\\right\\ }   , \\varsigma=\\widetilde{\\theta}^{-1}\\left\\ { \\theta\\right\\ }   } \\ ] ] where we reserve @xmath2659 for differentiation with respect to the first argument of @xmath2660   .$ ] thus for @xmath281 @xmath2661\\ ] ] to prove the theorem we will first need to show that : @xmath2658 = 0\\text { for } m\\geq s > j>0\\label{interinfo0}\\ ] ] to this end note that for @xmath2662 @xmath2663   /\\partial\\varsigma_{l_{j+1}}\\\\ &   = b_{m , l_{1}^{\\ast} ... l_{j}^{\\ast}l_{j+1}^{\\ast}}\\left [   \\theta , \\theta\\right ]   + b_{m , l_{1}^{\\ast} ... l_{j}^{\\ast}l_{j+1}}\\left [   \\theta , \\theta\\right ] \\\\ &   = \\psi_{\\backslash l_{1} ... l_{j+1}}\\left (   \\theta\\right )   + b_{m , l_{1}^{\\ast } ... l_{j}^{\\ast}l_{j+1}}\\left [   \\theta,\\theta\\right]\\end{aligned}\\ ] ] where the 2nd equality is by the definition of @xmath341 , the third is by the chain rule , and the fourth is again by the definition of @xmath272 .",
    "hence @xmath2664   = 0.$ ] hence for @xmath2665 @xmath2666   /\\partial\\varsigma_{l_{j+2} .. }=b_{m , l_{1}^{\\ast} ...",
    "l_{j}^{\\ast}l_{j+2}^{\\ast}l_{j+1}}\\left [   \\theta,\\theta\\right ]   + b_{m , l_{1}^{\\ast } ... l_{j}^{\\ast}l_{j+1}l_{j+2}}\\left [   \\theta,\\theta\\right ] \\\\ &   = 0+b_{m ,",
    "l_{1}^{\\ast} ... l_{j}^{\\ast}l_{j+1}l_{j+2}}\\left [   \\theta , \\theta\\right]\\end{aligned}\\ ] ] @xmath70where the last equality holds because we just proved @xmath2667   = 0 $ ] for arbitrary indices .  iterating this argument proves @xmath2668 .",
    "we complete the proof by induction on @xmath212 for some @xmath2669 given a @xmath2670 dimensional regular parametric submodel @xmath294 , @xmath2671   = 0 $ ] by assumption .",
    "hence , by regularity of the model , @xmath2672   + b_{m , l_{1}.}\\left [   \\theta,\\theta\\right ]   .$ ] therefore @xmath2673   = -\\psi_{\\backslash l_{1}}\\left (   \\theta\\right )   .$ ] now suppose the theorem if true for @xmath260 then @xmath2674 /\\partial\\varsigma_{l_{s+1}.}\\\\ &   = b_{m , l_{s+1}^{\\ast}l_{1} ... l_{s}}\\left [   \\theta,\\theta\\right ] + b_{m , l_{1} ... l_{s+1}}\\left [   \\theta,\\theta\\right ]   = 0+b_{m , l_{1} ... l_{s+1}}\\left [   \\theta,\\theta\\right ]   \\\\end{aligned}\\ ] ] where the second equality is by the induction assumption , the third by the chain rule , and the last by equation @xmath2668    ( theorem [ eift ] ) ( 1 ) : consider two influence functions @xmath2675 and @xmath2676 for @xmath988 then @xmath2677   = \\psi_{\\backslash\\overline{l}_{s}}\\left ( \\theta\\right )   -\\psi_{\\backslash\\overline{l}_{s}}\\left (   \\theta\\right ) = 0\\ $ ] for any score @xmath2678  and hence for any linear combination of scores .",
    "but , by definition , linear combinations of scores are dense in @xmath2679 thus @xmath2680 and @xmath2681 have the same projection on @xmath2682 ( 2 - 3 ) : essentially immediate from the definitions . ( 4 ) : for @xmath2683   = e_{\\theta}\\left [   \\pi_{m,\\theta } \\left [   \\mathbb{if}_{m}\\left (   \\theta\\right )   |\\mathcal{u}_{t}\\left ( \\theta\\right )   \\right ]   \\widetilde{\\mathbb{s}}_{t,\\overline{l}_{t}}\\left ( \\theta\\right )   \\right ]   $ ] for any @xmath2684 .",
    "( 5.a ) : follows from ( 1 ) .",
    "( 5.b ) : follows from ( 4 ) .",
    "degeneracy of @xmath314 follows at once from the fact that @xmath2685 in @xmath225 . proof of part ( 5.c ) requires the following .",
    "[ joel2]suppose , for @xmath1134 @xmath2686 and @xmath2687 exist w.p.1 for a kernel @xmath2688 then , ( i):@xmath2689@xmath2690 and @xmath2691 each have the same mean given @xmath2692 , ( ii ) @xmath2693   = 0,$ ] so @xmath2694        ( i):by @xmath2698 degenerate@xmath90@xmath2699   .$ ] further , by definition , @xmath2700 \\\\ &   = e_{\\theta}\\left [   if_{m , m,\\backslash l_{t},\\overline{i}_{m}}\\left ( \\theta\\right )   |o_{i_{1}}, ... ,o_{i_{m}}\\right ]   .\\end{aligned}\\ ] ] ( ii ) : by @xmath2698 degenerate @xmath2701   $ ] wp1 and so ( ii ) follows from ( i).(iii ) : ( i ) and ( ii ) imply @xmath2702 \\\\ &   = e_{\\theta}\\left [ \\begin{array } [ c]{c}e_{\\theta}\\left\\ {   if_{1,if_{m\\ , m\\ } \\left (   o_{i_{1},} .. o_{i_{m}};\\cdot\\right )   } \\left (   o_{i_{m+1}};\\theta\\right )   |\\left (   o_{i_{m+1}},o_{i_{1}}, ... ,o_{i_{m-2}}\\right )   \\right\\ } \\\\ \\times s_{l_{t}}\\left (   o_{i_{m+1}}\\right )   |o_{i_{1}}, ... ,o_{i_{m-2}}\\end{array } \\right]\\end{aligned}\\ ] ] but , by @xmath2703 an arbitrary mean zero function , @xmath2704 ( iv ) : by definition , @xmath2705 = \\mathbb{v}\\left [   \\left\\ {   i - d_{m,\\theta}\\right\\ }   \\left\\ { if_{m , m,\\backslash l_{t},\\overline{i}_{m}}\\left (   \\theta\\right )   \\right\\ } \\right ]   .$ ] the result follows by eq .",
    "@xmath2706 and part ( ii ) .    * theorem 5c(ii ) : * consider a @xmath135-dimensional parametric submodel @xmath2707 @xmath2708 with @xmath2709   = 0.$ ] since this model is linear in the @xmath2710 @xmath2711 for @xmath1927 .",
    "hence @xmath2712is degenerate of order @xmath135 , i.e. , @xmath2713 since @xmath2714 exists , on setting @xmath2715for @xmath2716 @xmath2717   .\\ ] ] differentiating the last display with respect to @xmath2718 and evaluating at @xmath2719 , we obtain @xmath2720   + e_{\\theta}\\left [   \\mathbb{if}_{m-1_{,}\\backslash l_{m}}\\left (   \\theta\\right )   \\widetilde{\\mathbb{s}}_{m-1,\\overline{l}_{m-1}}\\left (   \\theta\\right )   \\right ] \\\\ &   = e_{\\theta}\\left [   \\mathbb{if}_{m-1_{,}\\backslash l_{m}}\\left ( \\theta\\right )   \\widetilde{\\mathbb{s}}_{m-1,\\overline{l}_{m-1}}\\left ( \\theta\\right )   \\right]\\end{aligned}\\ ] ] now @xmath2721   $ ] @xmath2722   + e_{\\theta}\\left [ \\mathbb{if}_{m-1_{,}m-1,\\backslash l_{m}}\\left (   \\theta\\right ) \\widetilde{\\mathbb{s}}_{m-1,\\overline{l}_{m-1}}\\left (   \\theta\\right )   \\right ] .$ ] setting @xmath2723 @xmath2724 is degenerate of order @xmath1324 so @xmath2725 \\\\ &   = \\left (   m-1\\right )   ! e_{\\theta}\\left (   \\left [   if_{m-1,m-1}^{sym},_{\\backslash l_{m}}\\left (   o_{i_{1},} .. o_{i_{m-1}};\\theta\\right )   \\right ] \\prod\\limits_{r=1}^{m-1}a_{r}\\left (   o_{i_{r}},\\theta\\right )   \\right)\\end{aligned}\\ ] ] and",
    "@xmath2726   = 0.$ ] hence @xmath2727 now , by the assumed existence of @xmath272 , we also have @xmath2728   = m!e_{\\theta}\\left (   if_{m , m}^{sym}\\left ( o_{i_{1},} .. o_{i_{m}};\\theta\\right )   \\prod\\limits_{r=1}^{m}a_{r}\\left ( o_{i_{r}},\\theta\\right )   \\right )   $ ] .",
    "it follows that , for any choice of @xmath1324 mean zero functions @xmath2729 under @xmath940 @xmath2730{c}if_{m-1,m-1}^{sym},_{\\backslash l_{m}}\\left (   o_{i_{1},} .. o_{i_{m-1}};\\theta\\right ) \\\\ -me_{\\theta}\\left [   if_{m , m}^{sym}\\left (   o_{i_{1},} .. o_{i_{m}};\\theta\\right ) a_{m}\\left (   o_{i_{m}},\\theta\\right )   |o_{i_{1},} .. o_{i_{m-1}}\\right ] \\end{array } \\right\\ }   \\times\\prod\\limits_{r=1}^{m-1}a_{r}\\left (   o_{i_{r}},\\theta\\right ) \\right ) \\\\ &   = e_{\\theta}\\left (   r\\left (   o_{i_{1},} .. o_{i_{m-1}};\\theta\\right ) \\prod\\limits_{r=1}^{m-1}a_{r}\\left (   o_{i_{r}},\\theta\\right )   \\right)\\end{aligned}\\ ] ] where @xmath2731   -me_{\\theta}\\left [ if_{m , m}^{sym}\\left (   o_{i_{1},} .. o_{i_{m}};\\theta\\right )   a_{m}\\left ( o_{i_{m}},\\theta\\right )   |o_{i_{1},} .. o_{i_{m-1}}\\right]\\end{aligned}\\ ] ] the last equality follows from @xmath2732   $ ] orthogonal to @xmath2733 we conclude @xmath2734 with probability @xmath18 because @xmath2735 is a degenerate u - statistic kernel of order @xmath1324 and all degenerate u - statistics of order @xmath1324 have kernels that are the ( possibly infinite ) sum of products of @xmath1324 mean zero functions .",
    "it follows that , on a set @xmath322 which has probability @xmath18 under @xmath2736 @xmath2737 \\\\ &   + \\left\\ {   i - d_{m-1,\\theta}\\right\\ }   \\left [   if_{m-1,m-1}^{sym},_{\\backslash l_{m}}\\left (   o_{i_{1},} ... ,o_{i_{m-1}};\\theta\\right )   \\right ] \\\\ &   = e_{\\theta}\\left [   \\left\\ { \\begin{array } [ c]{c}m\\times if_{m , m}^{sym}\\left (   o_{i_{1},} .. o_{i_{m-1}},o;\\theta\\right ) \\\\ -\\sum_{j=1}^{m-1}if_{m-1,m-1}^{sym}\\left (   o_{i_{1},} ... ,o_{i_{j-1}},o , o_{i_{j+1}}, ... ,o_{i_{m-1}};\\theta\\right ) \\end{array } \\right\\ }   a_{m}\\left (   o,\\theta\\right )   \\right]\\end{aligned}\\ ] ] since , by parts ( i ) and ( ii ) of the lemma [ joel2 ] and eq .",
    "@xmath2739 \\\\ &   = -e_{\\theta}\\left [   \\sum_{j=1}^{m-1}if_{m-1,m-1}^{sym}\\left (   o_{i_{1},} ... ,o_{i_{j-1}},o , o_{i_{j+1}}, ... ,o_{i_{m-1}};\\theta\\right )   a_{m}\\left ( o,\\theta\\right )   \\right]\\end{aligned}\\ ] ] here @xmath2740 is the identity operator .",
    "now since the model @xmath2741 with @xmath2742 for @xmath308 has score @xmath2743 and such scores are dense in the subspace of l@xmath2744 with mean zero , it follows that @xmath2745 has influence function @xmath2746on the set @xmath322 . thus @xmath2747",
    ".$ ]          &   e_{\\theta}\\left [   \\mathbb{if}_{m,\\backslash l_{m+1}}\\left (   \\theta\\right ) \\widetilde{\\mathbb{s}}_{m,\\overline{l}_{m}}\\left (   \\theta\\right )   \\right ] \\nonumber\\\\ &   = m!e_{\\theta}\\left (   if_{m , m,\\backslash l_{m+1}}^{sym}\\left (   o_{i_{1},} .. o_{i_{l_{m}}};\\theta\\right )   \\prod\\limits_{r=1}^{m}s_{l_{r}}\\left ( o_{i_{r}},\\theta\\right )   \\right ) \\label{ccc}\\ ] ]    @xmath2754:by lemma [ joel2 ] and theorem 5c(ii)@xmath90 @xmath2755 \\\\ &   = \\mathbb{v}\\left [   me_{\\theta}\\left (   if_{m , m,\\overline{i}_{m}}^{sym}\\left (   \\theta\\right )   s_{l_{t}}\\left (   o_{i_{m}}\\right )   |o_{i_{1}}, ..",
    ",o_{i_{m-1}}\\right )   \\right ] \\\\ &   = \\mathbb{v}\\left [   me_{\\theta}\\left (   m^{-1}d_{m,\\theta}\\left\\ { if_{1,if_{m-1,m-1}^{sym}\\left (   o_{i_{1},} .. o_{i_{m-1}};\\cdot\\right )   } \\left ( o_{i_{m}};\\theta\\right )   \\right\\ }   s_{l_{t}}\\left (   o_{i_{m}}\\right )    ( iii ) of lemma [ joel2 ] and eq . @xmath2756",
    "the rhs is @xmath2757 \\\\ &   -\\mathbb{v}\\left\\ {   e\\left [   e\\left [   \\left (   m-1\\right )   e\\left [ if_{1,if_{m-1,m-1}^{sym}\\left (   o_{i_{1},} .. o_{i_{m-1}};\\cdot\\right )   } \\left ( o_{i_{m}};\\theta\\right )   |o_{i_{m}},o_{i_{1}}, .. ,o_{i_{m-2}}\\right ]   \\right ] s_{l_{t}}\\left (   o_{i_{m}}\\right )   |o_{i_{1}}, .. ,o_{i_{m-1}}\\right ]   \\right\\ } \\\\ &   = \\mathbb{v}\\left [   if_{m-1,m-1,\\backslash l_{t}}^{sym}\\left ( \\theta\\right )   \\right ]   -\\mathbb{v}\\left\\ {   \\left (   m-1\\right )   e_{\\theta } \\left [   if_{m-1,m-1,\\backslash l_{t}}^{sym}\\left (   \\theta\\right )   |o_{i_{1}}, ... ,o_{i_{m-2}}\\right ]   \\right\\}\\end{aligned}\\ ] ] on the other hand , by part ( iv ) of the lemma [ joel2 ] , @xmath2758   = \\mathbb{v}\\left [ if_{m-1,m-1,\\backslash l_{t}}\\left (   \\theta\\right )   \\right ]   -\\mathbb{v}\\left [   \\left (   m-1\\right )   e_{\\theta}\\left [   if_{m-1,m-1,\\backslash l_{t},\\overline{i}_{m-1}}^{sym}\\left (   \\theta\\right )   |o_{i_{1}}, ... ,o_{i_{m-2}}\\right ]   \\right ]   .",
    "$ ]    @xmath2759 write @xmath2760   + \\mathbb{if}_{1,\\backslash l_{t}}\\left ( \\theta\\right )   \\right\\ } \\\\ &   + \\sum_{j=2}^{m-1}\\left\\ {   \\pi\\left [   \\mathbb{if}_{j+1,j+1,\\backslash l_{t}}\\left (   \\theta\\right )   |\\mathcal{u}_{j}\\left (   \\theta\\right )   \\right ] + \\pi\\left [   \\mathbb{if}_{jj,\\backslash l_{t}}\\left (   \\theta\\right )      @xmath2763 @xmath2764   = e_{\\theta}\\left [   \\pi\\left [   \\mathbb{if}_{m , m,\\backslash l_{m+1}}\\left (   \\theta\\right )   |\\mathcal{u}_{m-1}^{\\bot } \\left (   \\theta\\right )   \\right ]   \\widetilde{\\mathbb{s}}_{m,\\overline{l}_{m}}\\left (   \\theta\\right )   \\right ]   $ ] by eq . @xmath2765",
    "but the rhs of this equation is the rhs of eq .",
    "@xmath2766    ( * theorem * * 5c(i ) ) : * by assumption @xmath2767 hence @xmath2768\\ ] ] by eq . @xmath2769 and",
    "the assumption @xmath2770 has an influence function , we obtain @xmath2771 \\\\ &   = \\left (   m-1\\right )   !",
    "e_{\\theta}\\left (   if_{1if_{m-1,m-1}^{sym}\\left ( o_{i_{1},} .. o_{i_{m-1}};\\cdot\\right )   } \\left (   o_{i_{m}},\\theta\\right ) s_{l_{m}}\\left (   o_{i_{m}},\\theta\\right )   \\prod\\limits_{r=1}^{m-1}s_{l_{r}}\\left (   o_{i_{r}},\\theta\\right )   \\right )   .\\end{aligned}\\ ] ] we conclude that @xmath2772 exists and equals @xmath2773   .$ ]      first we show that for any @xmath1564-dimensional parametric submodel @xmath255 @xmath2774 where @xmath2775 from eq ( [ interinfo0 ] ) we know that @xmath2776 for all @xmath2777 since @xmath310 is locally nonparametric , i.e. , @xmath2778 and @xmath2779 therefore we have @xmath2780    \\ii ) if @xmath327 exists , we have @xmath2781 with @xmath2782 symmetric , such that@xmath2783   \\mathbb{d}_{m}^{(\\mathbb{s}_{m,\\overline{l}_{m}})}\\right ) \\\\ &   + e_{\\theta}\\left (   \\mathbb{v}\\left (   me\\left [   if_{m}\\left (   o_{i_{1}}, ...",
    ",o_{i_{m}};\\theta\\right )   |o_{i_{1}}, ... o_{i_{m-1}}\\right ]   ^{c}\\right ) \\mathbb{d}_{m-1}^{(\\mathbb{s}_{m,\\overline{l}_{m}})}\\right ) \\\\ &   + e\\left (   \\mathbb{if}_{m-2}\\mathbb{s}_{m,\\overline{l}_{m}}\\right)\\end{aligned}\\ ] ] therefore , @xmath2784",
    "^{c}$ ] wp1@xmath64  so that the lhs has an influence function since : @xmath2785   ^{c}\\right\\ }   _ { \\backslash l_{m}}\\\\ &   = \\left\\ {   e_{\\theta}\\left [   if_{m}\\left (   o_{i_{1}}, ... ,o_{i_{m}};\\theta\\right )   |o_{i_{1}}, ... o_{i_{m-1}}\\right ]   \\right\\ }   _ { \\backslash l_{m}}\\\\ &   + \\sum_{t=0}^{m-2}\\left (   -1\\right )   ^{m-1-t}\\sum_{\\substack{i_{r_{1}}\\neq",
    "i_{r_{2}} .. \\neq i_{r_{t } } \\\\\\overline{i}_{r_{t}}\\subset\\overline{i}_{m-1 } } } e_{\\theta}\\left (   if_{m}\\left (   o_{i_{1}},o_{i_{2}}, .. o_{i_{m}};\\theta\\right )   |o_{i_{r_{1}}},o_{i_{r_{2}}}, .. o_{i_{r_{t}}}\\right ) _",
    "{ \\backslash l_{m}}\\\\ &   = \\left\\ {   e_{\\theta}\\left [   if_{m}\\left (   o_{i_{1}}, ... ,o_{i_{m}};\\theta\\right )   s_{l_{m}}\\left (   o_{i_{m}}\\right )   |o_{i_{1}}, ... o_{i_{m-1}}\\right ]   \\right\\ } \\\\ &   + \\sum_{t=0}^{m-2}\\left [ \\begin{array } [ c]{c}\\left (   -1\\right )   ^{m-1-t}\\left (   m - t\\right ) \\\\",
    "\\sum_{\\substack{i_{r_{1}}\\neq i_{r_{2}} .. \\neq i_{r_{t } } \\\\\\overline{i}_{r_{t}}\\subset\\overline{i}_{m-1}}}e\\left ( \\begin{array } [ c]{c}e\\left [   if_{m}\\left (   o_{i_{1}}, ... ,o_{i_{m}};\\theta\\right )   |o_{i_{r_{1}}},o_{i_{t}}, .. o_{i_{t+1}}\\right ]   s_{l_{t+1}}\\left (   o_{i_{t+1}}\\right ) \\\\          which means @xmath2794{c}if_{1,if_{m-1,m-1,\\theta}\\left (   o_{i_{1},} ..",
    "o_{i_{m-1}};\\cdot\\right )   } ^{c}\\left (   o_{i_{m}},\\theta\\right ) \\\\ -if_{1,if_{m-1,m-1,\\theta}\\left (   o_{i_{1},}.o_{i_{m}}.o_{i_{m-1}};\\cdot\\right )   } ^{c}\\left (   o_{i_{j}},\\theta\\right ) \\end{array } \\right ] { \\textstyle\\prod\\limits_{r=1}^{m } } s_{l_{r}}\\left (   o_{i_{r}},\\theta\\right )   \\right )   = 0\\\\ \\longleftrightarrow if_{1,if_{m-1,m-1,\\theta}\\left (   o_{i_{1},} .. o_{i_{m-1}};\\cdot\\right )   } ^{c}\\left (   o_{i_{m}},\\theta\\right ) = if_{1,if_{m-1,m-1,\\theta}\\left (   o_{i_{1},}.o_{i_{m}}.o_{i_{m-1}};\\cdot\\right )   } ^{c}\\left (   o_{i_{j}},\\theta\\right )   \\text { w.p.1 } \\ ] ]        ( proof of eq . ( [ imm ] ) ) we have proved in the text the following results that will be used repeatedly throughout the proof:@xmath2797@xmath2798   ^{-\\frac{1}{2}}k_{f_{x},\\infty } \\left (   x_{i_{1}},x_{i_{2}}\\right )   e_{\\theta}\\left [   h_{1}|x_{i_{2}}\\right ] ^{-\\frac{1}{2}}\\varepsilon_{b , i_{2}}\\left (   \\theta\\right ) \\\\ &   = -\\frac{k_{leb,\\infty}\\left (   x_{i_{1}},x_{i_{2}}\\right )   } { g\\left ( x_{i_{1}}\\right )   ^{\\frac{1}{2}}g\\left (   x_{i_{2}}\\right )   ^{\\frac{1}{2}}}\\varepsilon_{b , i_{2}}\\left (   \\theta\\right )   , \\end{aligned}\\]]@xmath2799   ^{-\\frac{1}{2}}k_{f_{x},\\infty } \\left (   x_{i_{1}},x_{i_{2}}\\right )   e_{\\theta}\\left [   h_{1}|x_{i_{2}}\\right ] ^{-\\frac{1}{2}}\\varepsilon_{p , i_{2}}\\left (   \\theta\\right ) \\\\ &   = -\\frac{k_{leb,\\infty}\\left (   x_{i_{1}},x_{i_{2}}\\right )   } { g\\left ( x_{i_{1}}\\right )   ^{\\frac{1}{2}}g\\left (   x_{i_{2}}\\right )   ^{\\frac{1}{2}}}\\varepsilon_{p , i_{2}}\\left (   \\theta\\right )   , \\end{aligned}\\ ] ] and @xmath2800    in addition , by an analogous argument , we can also show that @xmath2801   } f_{x}\\left (   x\\right ) + e_{\\theta}\\left [   h_{1}|x\\right ]   if_{1,f_{x}\\left (   x;\\cdot\\right )   } \\\\ &   = \\left (   h_{1}-e\\left [   h_{1}|x\\right ]   \\right )   k_{leb,\\infty}\\left ( x , x\\right )   + e_{\\theta}\\left [   h_{1}|x\\right ]   \\left (   k_{leb,\\infty}\\left ( x , x\\right )   -f_{x}\\left (   x\\right )   \\right ) \\\\ &   = h_{1}k_{leb,\\infty}\\left (   x , x\\right )   -g\\left (   x\\right)\\end{aligned}\\ ] ]        \\ii ) we now assume eq .",
    "( [ imm ] ) holds for @xmath135 for some @xmath316 and shall prove it is also true for @xmath2450 @xmath70by assumption,@xmath866{c}\\sum_{j=0}^{m-2}c(m , j)\\times\\\\ \\prod\\limits_{s=1}^{j}\\frac{h_{1,i_{s+1}}}{g\\left (   x_{i_{s+1}}\\right ) } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right ) \\\\",
    "k_{leb,\\infty}\\left (   x_{i_{j+1}},x_{i_{m}}\\right ) \\end{array } \\right ]   g\\left (   x_{i_{m}}\\right )   ^{-\\frac{1}{2}}\\varepsilon_{p , i_{m}}\\left (   \\theta\\right)\\end{aligned}\\ ] ]    following the results from part 5c ) of theorem [ eift ] , @xmath70  @xmath2802 exists if and only if @xmath2803 exists .  by the chain rule,@xmath2804{c}\\left ( \\begin{array } [ c]{c}h_{1,i_{1}}if_{1,\\varepsilon_{b , i_{1}}\\left (   \\cdot\\right )   } \\left ( o_{i_{m+1}}\\right )   \\varepsilon_{p , i_{m}}\\left (   \\theta\\right )   + \\\\ h_{1,i_{m}}\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   if_{1,}\\varepsilon _ { p , i_{m}}\\left (   \\cdot\\right )   \\left (   o_{i_{m+1}}\\right ) \\end{array } \\right )   g\\left (   x_{i_{1}}\\right )   ^{-\\frac{1}{2}}g\\left (   x_{i_{m}}\\right ) ^{-\\frac{1}{2}}\\\\ -\\frac{1}{2}\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   \\varepsilon_{p , i_{m}}\\left (   \\theta\\right )   g\\left (   x_{i_{1}}\\right )   ^{-\\frac{1}{2}}g\\left ( x_{i_{m}}\\right )   ^{-\\frac{1}{2}}\\times\\\\ \\left [   \\frac{if_{1,g\\left (   x_{i_{1}}\\right )   } \\left (   o_{i_{m+1}}\\right ) } { g\\left (   x_{i_{1}}\\right )   } + \\frac{if_{1,g\\left (   x_{i_{m}}\\right )   } \\left ( o_{i_{m+1}}\\right )   } { g\\left (   x_{i_{m}}\\right )   } \\right ] \\end{array } \\right\\ }   \\times\\\\ &   \\left [ \\begin{array } [ c]{c}\\sum_{j=0}^{m-2}c(m , j)\\times\\\\ \\prod\\limits_{s=1}^{j}\\frac{h_{1,i_{s+1}}}{g\\left (   x_{i_{s+1}}\\right ) } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right ) \\\\ \\times k_{leb,\\infty}\\left (   x_{i_{j+1}},x_{i_{m}}\\right ) \\end{array } \\right ] \\\\ &   -\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   \\varepsilon_{p , i_{m}}\\left ( \\theta\\right )   g\\left (   x_{i_{1}}\\right )   ^{-\\frac{1}{2}}g\\left (   x_{i_{m}}\\right )   ^{-\\frac{1}{2}}\\sum_{j=0}^{m-2}c(m , j)\\times\\\\ &   \\left [ \\begin{array } [ c]{c}\\sum_{t=1}^{j}\\frac{h_{1,i_{t+1}}k_{leb,\\infty}\\left (   x_{i_{t}},x_{i_{t+1}}\\right )   } { g^{2}\\left (   x_{i_{t+1}}\\right )   } if_{1,g\\left (   x_{i_{t+1}}\\right )   } \\left (   o_{i_{m+1}}\\right )   \\times\\\\ \\prod\\limits_{s\\neq t}\\frac{h_{1,i_{s+1}}}{g\\left (   x_{i_{s+1}}\\right ) } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right )   k_{leb,\\infty}\\left ( x_{i_{j+1}},x_{i_{m}}\\right ) \\end{array } \\right]\\end{aligned}\\]]@xmath906@xmath2805{c}-\\frac{h_{1,i_{1}}k_{leb,\\infty}\\left (   x_{i_{1}},x_{i_{m+1}}\\right ) } { g\\left (   x_{i_{1}}\\right )   g\\left (   x_{i_{m+1}}\\right )   ^{\\frac{1}{2}}g\\left (   x_{i_{m}}\\right )   ^{\\frac{1}{2}}}\\varepsilon_{b , i_{m+1}}\\left ( \\theta\\right )   \\varepsilon_{p , i_{m}}\\left (   \\theta\\right ) \\\\ -\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   \\frac{h_{1,i_{m}}k_{leb,\\infty } \\left (   x_{i_{m}},x_{i_{m+1}}\\right )   } { g\\left (   x_{i_{m}}\\right )   g\\left ( x_{i_{1}}\\right )   ^{\\frac{1}{2}}g\\left (   x_{i_{m+1}}\\right )   ^{\\frac{1}{2}}}\\varepsilon_{p , i_{m+1}}\\\\ -\\frac{1}{2}\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   \\varepsilon_{p , i_{m}}\\left (   \\theta\\right )   g\\left (   x_{i_{1}}\\right )   ^{-\\frac{1}{2}}g\\left ( x_{i_{m}}\\right )   ^{-\\frac{1}{2}}\\times\\\\ \\left [   \\frac{h_{1,i_{m+1}}k_{leb,\\infty}\\left (   x_{i_{m+1}},x_{i_{1}}\\right ) } { g\\left (   x_{i_{1}}\\right )   } + \\frac{h_{1,i_{m+1}}k_{leb,\\infty}\\left ( x_{i_{m+1}},x_{i_{m}}\\right )   } { g\\left (   x_{i_{m}}\\right )   } -2\\right ] \\end{array } \\right\\ } \\\\ \\times &   \\left [ \\begin{array } [ c]{c}\\sum_{j=0}^{m-2}c(m , j)\\times\\\\ \\prod\\limits_{s=1}^{j}\\frac{h_{1,i_{s+1}}}{g\\left (   x_{i_{s+1}}\\right ) } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right ) \\\\ \\times k_{leb,\\infty}\\left (   x_{i_{j+1}},x_{i_{m}}\\right ) \\end{array } \\right ] \\\\ &   -\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   \\varepsilon_{p , i_{m}}\\left ( \\theta\\right )   g\\left (   x_{i_{1}}\\right )   ^{-\\frac{1}{2}}g\\left (   x_{i_{m}}\\right )   ^{-\\frac{1}{2}}\\sum_{j=0}^{m-2}c(m , j)\\times\\\\ &   \\left [ \\begin{array } [ c]{c}\\sum_{t=1}^{j}\\frac{h_{1,i_{t+1}}k_{leb,\\infty}\\left (   x_{i_{t}},x_{i_{t+1}}\\right )   } { g^{2}\\left (   x_{i_{t+1}}\\right )   } \\left (   h_{1,i_{m+1}}k_{leb,\\infty}\\left (   x_{i_{m+1}},x_{i_{t+1}}\\right )   -g\\left (   x_{i_{t+1}}\\right )   \\right )   \\times\\\\ \\prod\\limits_{s\\neq t}\\frac{h_{1,i_{s+1}}}{g\\left (   x_{i_{s+1}}\\right ) } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right )   k_{leb,\\infty}\\left ( x_{i_{j+1}},x_{i_{m}}\\right ) \\end{array } \\right]\\end{aligned}\\]]@xmath906 @xmath2806{c}\\varepsilon_{b , i_{m+1}}\\left (   \\theta\\right )   g\\left (   x_{i_{m+1}}\\right ) ^{-\\frac{1}{2}}\\frac{h_{1,i_{1}}k_{leb,\\infty}\\left (   x_{i_{m+1}},x_{i_{1}}\\right )   } { g\\left (   x_{i_{1}}\\right )   } \\\\ \\prod\\limits_{s=1}^{j}\\frac{h_{1,i_{s+1}}}{g\\left (   x_{i_{s+1}}\\right ) } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right )   k_{leb,\\infty}\\left ( x_{i_{j+1}},x_{i_{m}}\\right ) \\\\",
    "\\times g\\left (   x_{i_{m}}\\right )   ^{-\\frac{1}{2}}\\varepsilon_{p , i_{m}}\\left ( \\theta\\right ) \\end{array } \\right ) \\\\ &   + \\sum_{j=0}^{m-2}\\left (   -1\\right )   ^{j}\\binom{m-2}{j}\\left ( \\begin{array } [ c]{c}\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   g\\left (   x_{i_{1}}\\right ) ^{-\\frac{1}{2}}\\prod\\limits_{s=1}^{j}\\frac{h_{1,i_{s+1}}}{g\\left (   x_{i_{s+1}}\\right )   } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right ) \\\\",
    "\\frac{h_{1,i_{m}}k_{leb,\\infty}\\left (   x_{i_{j+1}},x_{i_{m}}\\right ) } { g\\left (   x_{i_{m}}\\right )   } k_{leb,\\infty}\\left (   x_{i_{m}},x_{i_{m+1}}\\right )   ^{\\frac{1}{2}}\\\\ \\times g\\left (   x_{i_{m+1}}\\right )   ^{-\\frac{1}{2}}\\varepsilon_{p , i_{m+1}}\\end{array } \\right ) \\\\ &   + \\frac{1}{2}\\sum_{j=0}^{m-2}\\left (   -1\\right )   ^{j}\\binom{m-2}{j}\\left\\ { \\begin{array } [ c]{c}\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   \\varepsilon_{p , i_{m}}\\left ( \\theta\\right )   \\left ( \\begin{array } [ c]{c}\\frac{h_{1,i_{m+1}}k_{leb,\\infty}\\left (   x_{i_{m+1}},x_{i_{1}}\\right ) } { g\\left (   x_{i_{1}}\\right )   }",
    "\\\\ + \\frac{h_{1,i_{m+1}}k_{leb,\\infty}\\left (   x_{i_{m+1}},x_{i_{j+1}}\\right ) } { g\\left (   x_{i_{m+1}}\\right )   } \\end{array } \\right ) \\\\ \\times\\prod\\limits_{s=1}^{j}\\frac{h_{1,i_{s+1}}}{g\\left (   x_{i_{s+1}}\\right ) } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right )   \\times\\\\ k_{leb,\\infty}\\left (   x_{i_{j+1}},x_{i_{m}}\\right )   g\\left (   x_{i_{1}}\\right ) ^{-\\frac{1}{2}}g\\left (   x_{i_{m}}\\right )   ^{-\\frac{1}{2}}\\end{array } \\right\\ } \\\\ &   + if_{m , m,\\psi,\\overline{i}_{m}}\\left (   \\theta\\right ) \\\\ &   + \\sum_{j=0}^{m-2}\\left (   -1\\right )   ^{j}\\binom{m-2}{j}\\left [   \\sum _ { t=1}^{j}\\left ( \\begin{array } [ c]{c}\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   \\varepsilon_{p , i_{m}}\\left ( \\theta\\right )   g\\left (   x_{i_{1}}\\right )   ^{-\\frac{1}{2}}g\\left (   x_{i_{m}}\\right )   ^{-\\frac{1}{2}}\\times\\\\ \\frac{h_{1,i_{t+1}}k_{leb,\\infty}\\left (   x_{i_{t}},x_{i_{t+1}}\\right ) } { g\\left (   x_{i_{t+1}}\\right )   } h_{1,i_{m+1}}\\frac{k_{leb,\\infty}\\left ( x_{i_{t+1}},x_{i_{m+1}}\\right )   } { g\\left (   x_{i_{m+1}}\\right )   } \\\\ \\times\\prod\\limits_{s\\neq t}\\frac{h_{1,i_{s+1}}}{g\\left (   x_{i_{s+1}}\\right ) } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right )   k_{leb,\\infty}\\left ( x_{i_{j+1}},x_{i_{m}}\\right ) \\end{array } \\right )   \\right ] \\\\ &   -\\sum_{j=0}^{m-2}\\left (   -1\\right )   ^{j}\\binom{m-2}{j}j\\left ( \\begin{array } [ c]{c}\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   \\varepsilon_{p , i_{m}}\\left ( \\theta\\right )   g\\left (   x_{i_{1}}\\right )   ^{-\\frac{1}{2}}g\\left (   x_{i_{m}}\\right )   ^{-\\frac{1}{2}}\\times\\\\ \\prod\\limits_{s=1}^{j}\\frac{h_{1,i_{s+1}}}{g\\left (   x_{i_{s+1}}\\right ) } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right )   k_{leb,\\infty}\\left ( x_{i_{j+1}},x_{i_{m}}\\right ) \\end{array } \\right)\\end{aligned}\\ ] ]    applying the operator @xmath2807 which is defined in eq@xmath2808 on the statistic above , it is straightforward to show that @xmath2809 \\right\\ }   \\right )",
    "\\\\ = \\mathbb{v}\\left\\ {   d_{m+1,\\theta}\\left [ \\begin{array } [ c]{c}\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   g\\left (   x_{i_{1}}\\right ) ^{-\\frac{1}{2}}\\prod\\limits_{s=1}^{m-1}\\frac{h_{1,i_{s+1}}}{g\\left ( x_{i_{s+1}}\\right )   } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right ) \\\\",
    "\\times k_{leb,\\infty}\\left (   x_{i_{m}},x_{i_{m+1}}\\right )   g\\left ( x_{i_{m+1}}\\right )   ^{-\\frac{1}{2}}\\varepsilon_{p , i_{m+1}}\\left ( \\theta\\right ) \\end{array } \\right ]   \\right\\ } \\\\",
    "= \\mathbb{v}\\left\\ { \\begin{array } [ c]{c}\\varepsilon_{b , i_{1}}\\left (   \\theta\\right )   g\\left (   x_{i_{1}}\\right ) ^{-\\frac{1}{2}}\\left [ \\begin{array } [ c]{c}\\sum_{j=0}^{m-1}c(m+1,j)\\times\\\\ \\prod\\limits_{s=1}^{j}\\frac{h_{1,i_{s+1}}}{g\\left (   x_{i_{s+1}}\\right ) } k_{leb,\\infty}\\left (   x_{i_{s}},x_{i_{s+1}}\\right ) \\\\ \\times k_{leb,\\infty}\\left (   x_{i_{j+1}},x_{i_{m+1}}\\right ) \\end{array } \\right ] \\\\",
    "\\times g\\left (   x_{i_{m+1}}\\right )   ^{-\\frac{1}{2}}\\varepsilon_{p , i_{m+1}}\\left (   \\theta\\right ) \\end{array } \\right\\}\\end{gathered}\\ ] ]    ( theorem [ tbformula ] ) by eq ( [ eta1 ] ) and eq ( [ alpha1 ] ) and part a of the preceding lemma @xmath2810   ^{-1}e\\left [   \\overline{z}_{k}\\dot { p}\\left (   \\widehat{b}-b\\right )   h_{1}\\right ] \\\\",
    "\\widetilde{\\overline{\\alpha}}_{k }   &   = -e\\ \\left [   \\dot{b}\\dot{p}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   ^{-1}e\\ \\left [   \\overline { z}_{k}\\dot{b}\\left (   \\widehat{p}-p\\right )   h_{1}\\right]\\end{aligned}\\ ] ] and hence @xmath2811 ^{-1}e\\left [   \\dot{p}\\dot{b}\\overline{z}_{k}\\left (   b-\\widehat{b}\\ \\right ) \\left\\ {   \\dot{b}\\right\\ }   ^{-1}h_{1}\\right ] \\\\ \\widetilde{p}-\\widehat{p }   &   = -\\dot{p}\\overline{z}_{k}^{t}e_{\\ } \\left [ \\dot{b}\\dot{p}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   ^{-1}e\\left [ \\dot{b}\\dot{p}\\overline{z}_{k}\\left (   p-\\widehat{p}\\right )   \\left\\ {   \\dot { p}\\right\\ }   ^{-1}h_{1}\\right]\\end{aligned}\\ ] ] thus @xmath2812 ^{-1}e\\left [   q^{2}\\overline{z}_{k}\\frac{\\left (   p-\\widehat{p}\\right )   } { \\dot{p}}\\right ] \\\\ &   = \\pi\\left [   \\frac{p-\\widehat{p}}{\\dot{p}}q|q\\overline{z}_{k}\\right ] \\\\ q\\frac{\\widetilde{b}-\\widehat{b}}{\\dot{b } }   &   = -q\\overline{z}_{k}^{t}e_{\\theta}\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] ^{-1}e\\left [   q^{2}\\overline{z}_{k}\\frac{\\left (   b-\\widehat{b}\\right )   } { \\dot{b}}\\right ] \\\\ &   = \\pi\\left [   \\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   q|q\\overline { z}_{k}\\right]\\end{aligned}\\ ] ] and hence @xmath2813 \\\\ q\\left (   \\frac{b-\\widetilde{b}}{\\dot{b}}\\right )    &   = \\pi^{\\perp}\\left [ \\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   q|q\\overline{z}_{k}\\right]\\end{aligned}\\ ] ] but by the previous theorem , @xmath2814   = e\\left [   q\\left (   \\frac{b-\\widetilde{b}}{\\dot{b}}\\right )   q\\left (   \\frac{\\left (   p-\\widetilde{p}\\right )   } { \\dot{p}}\\right ) \\right]\\ ] ] proving the theorem .",
    "( theorem [ tbrate ] ) under our assumptions , the following holds uniformly for @xmath357@xmath2815   \\pi^{\\perp } \\left [   \\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   q|q\\overline{z}_{k}\\right ]   \\right ]   \\right\\ }   ^{2}\\\\ &   \\leq e\\left\\ {   \\pi^{\\perp}\\left [   \\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   q|q\\overline{z}_{k}\\right ]   ^{2}\\right\\ }   \\times e\\left\\ { \\pi^{\\perp}\\left [   \\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right ) q|q\\overline{z}_{k}\\right ]   ^{2}\\right\\}\\end{aligned}\\ ] ] by cauchy shwartz .",
    "now @xmath2816   ^{2}\\right\\ } \\\\ &   = inf_{\\varsigma_{_{l}}}\\int_{r^{d}}q^{2}\\left (   \\frac{p\\left (   x\\right ) -\\widehat{p}\\left (   x\\right )   } { \\dot{p}\\left (   x\\right )   } -\\sum_{l=1}^{k}\\varsigma_{_{l}}z_{l}\\left (   x\\right )   \\right )   ^{2}f\\left (   x\\right ) dx\\\\ &   = inf_{\\varsigma_{_{l}}}\\int_{r^{d}}q^{2}\\left (   \\frac{\\left (   p\\left ( x\\right )   -\\widehat{p}\\left (   x\\right )   \\right )   } { \\dot{p}\\left (   x\\right ) } -\\sum_{l=1}^{k}\\varsigma_{_{l}}^{\\ast}\\varphi_{l}\\left (   x\\right )   \\right ) ^{2}q^{2}f\\left (   x\\right )   dx\\\\ &   = inf_{\\varsigma_{_{l}}}\\int_{r^{d}}\\left (   \\left (   p\\left (   x\\right ) -\\widehat{p}\\left (   x\\right )   \\right )   -\\sum_{l=1}^{k}\\varsigma_{_{l}}^{\\ast } \\varphi_{l}\\left (   x\\right )   \\right )   ^{22}q^{2}\\ f\\left (   x\\right )   dx\\\\ &   \\leq\\left\\vert \\left\\vert q^{2}f\\left (   x\\right )   \\right\\vert \\right\\vert",
    "_ { \\infty}inf_{\\varsigma_{_{l}}}\\int_{r^{d}}\\left (   \\left (   p\\left (   x\\right ) -\\widehat{p}\\left (   x\\right )   \\right )   -\\sum_{l=1}^{k}\\varsigma_{_{l}}^{\\ast } \\varphi_{l}\\left (   x\\right )   \\right )   ^{2}dx\\\\ &   \\leq\\left\\vert \\left\\vert q^{2}f\\left (   x\\right )   \\right\\vert \\right\\vert",
    "_ { \\infty}o_{p}\\left (   k^{-2\\beta_{p}/d}\\right )   = o_{p}\\left (   k^{-2\\beta _ { p}/d}\\right)\\end{aligned}\\ ] ] the last equality follows from the fact that under the stated assumptions @xmath2817 .",
    "similarly @xmath2818 ^{2}\\right\\ }   = o_{p}\\left (   k^{-2\\beta_{b}/d}\\right )   .$ ]    ( theorem [ drhoif ] ) by theorem [ foif ] @xmath2819 @xmath2820and by part 5.c of theorem [ eift ] @xmath2821 = \\frac{1}{2}\\left\\ {   \\pi_{\\theta}\\left [   \\mathbb{v}\\left [ if_{1,if_{1,\\widetilde{\\psi}_{k}}\\left (   o_{i_{1}},\\cdot\\right )   , i_{2}}\\,\\left (   \\theta\\right )   \\right ]   |\\mathcal{u}_{1}^{\\perp_{\\theta,2}}\\left ( \\theta\\right )   \\right ]   \\right\\ }   .\\ ] ] now @xmath2822 where @xmath2823@xmath2824   \\right\\ } ^{-1}\\left [   \\left\\ {   h_{1}\\widetilde{b}\\left (   x,\\theta\\right ) + h_{3}\\right\\ }   \\dot{p}\\overline{z}_{k}\\right ]   _ { i_{2}},\\end{aligned}\\ ] ] and @xmath2825 \\right\\ }   ^{-1}\\left [   \\left\\ {   h_{1}\\widetilde{p}\\left (   x,\\theta\\right ) + h_{2}\\right\\ }   \\dot{b}\\overline{z}_{k}\\right ]   _ { i_{2}}.\\]]@xmath2826   \\right\\ } ^{-1}\\\\ &   \\times\\left [   \\left\\ {   h_{1}\\widetilde{b}\\left (   x,\\theta\\right ) + h_{3}\\right\\ }   \\dot{p}\\overline{z}_{k}\\right ]   _ { i_{2}}\\\\ &   -\\left\\ {   h_{1}\\widetilde{b}\\left (   x,\\theta\\right )   + h_{3}\\right\\ } _ { i_{1}}\\dot{p}_{i_{1}}\\overline{z}_{ki_{1}}^{t}\\left\\ {   e_{\\theta}\\left [ \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ } ^{-1}\\\\ &   \\times\\left [   \\left\\ {   h_{1}\\widetilde{p}\\left (   x,\\theta\\right ) + h_{2}\\right\\ }   \\dot{b}\\overline{z}_{k}\\right ]   _ { i_{2}}\\ ] ] and further @xmath2827    @xmath2828   = e_{\\theta}\\left [   \\left\\ { h_{1}\\widetilde{b}\\left (   x,\\theta\\right )   + h_{3}\\right\\ }   \\dot{p}\\overline { z}_{k}\\right ]   = 0\\ ] ] and thus @xmath2829 is degenerate .",
    "because @xmath2830 has two terms , it appears that @xmath2831 will consist of two terms .",
    "however by the symmetry upon interchange of @xmath821 and @xmath2832  and the permutation invariance of the operator @xmath2833 @xmath2834 \\\\ &   = \\mathbb{v}\\left [ \\begin{array } [ c]{c}-2\\left\\ {   h_{1}\\widetilde{p}\\left (   x,\\theta\\right )   + h_{2}\\right\\ }   _ { i_{1}}\\dot{b}_{i_{1}}\\overline{z}_{ki_{1}}^{t}\\left\\ {   e_{\\theta}\\left [   \\dot { p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\\\ \\times\\left [   \\overline{z}_{k}\\left\\ {   h_{1}\\widetilde{b}\\left ( x,\\theta\\right )   + h_{3}\\right\\ }   \\dot{p}\\right ]   _ { i_{2}}\\end{array } \\right]\\end{aligned}\\ ] ] thus we can take @xmath2835   \\right\\ }   ^{-1}\\\\ &   \\times\\left [   \\overline{z}_{k}\\left\\ {   h_{1}\\widetilde{b}\\left ( x,\\theta\\right )   + h_{3}\\right\\ }   \\dot{p}\\right ]   _ { i_{2}}\\ ] ] as was to be proved .",
    "we now complete the proof of the theorem by induction .",
    "we assume it is true for @xmath2836 and prove it is true for @xmath2837",
    "now@xmath2838   = \\frac{1}{m}\\mathbb{v}\\left [   \\pi_{\\theta}\\left [   if_{1,if_{mm,\\widetilde{\\psi}_{k},}\\,\\left (   o_{\\overline{i}_{m}},\\cdot\\right )   , i_{m+1}}\\,\\left ( \\theta\\right )   |\\mathcal{u}_{m}^{\\perp_{\\theta , m+1}}\\left (   \\theta\\right ) \\right ]   \\right]\\ ] ] now by the induction hypothesis , @xmath2839   _ { i_{1}}\\\\ &   \\times\\left [ { \\displaystyle\\prod\\limits_{s=3}^{m } } \\left\\ {   e_{\\theta}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left\\ { \\begin{array } [ c]{c}\\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ) _ { i_{s}}\\\\ -e_{\\theta}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] \\end{array } \\right\\ }   \\right ] \\\\ &   \\times\\left\\ {   e_{\\theta}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left [   \\overline{z}_{k}\\left (   h_{1}\\widetilde{b}\\left (   \\theta\\right )   + h_{3}\\right )   \\dot { p}\\right ]   _ { i_{2}}\\ ] ] the derivatives with respect to the @xmath2840 in @xmath2841and in the @xmath1324 terms @xmath2842   \\right\\ }   ^{-1}$ ] will each contribute a term to @xmath2843   .$ ]  however differentiating wrt to the @xmath179 in the @xmath1293 terms @xmath2844   $ ] will not contribute to @xmath2845   $ ] as the contribution from each of these @xmath1293 terms to @xmath2846 is only a function of @xmath135 units data and is thus an element of @xmath209 which is orthogonal to the space @xmath2847 that is projected on .. now @xmath2848   \\right\\ }   ^{-1},i_{m+1}}\\left (   \\theta\\right ) \\\\ &   = -\\left\\ {   e_{\\theta}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left\\ { \\begin{array } [ c]{c}\\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ) _ { i_{m+1}}\\\\ -e_{\\theta}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] \\end{array } \\right\\ }   \\left\\ {   e_{\\theta}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\ ] ] so upon permuting the unit indices , the contribution of each of these @xmath1324 terms to @xmath2846 is@xmath2849   _ { i_{1}}\\label{ifm+1}\\\\ &   \\times\\left [ { \\displaystyle\\prod\\limits_{s=3}^{m+1 } } \\left\\ {   e_{\\theta}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left\\ { \\begin{array } [ c]{c}\\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ) _ { i_{s}}\\\\ -e_{\\theta}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] \\end{array } \\right\\ }   \\right ] \\nonumber\\\\ &   \\times\\left\\ {   e_{\\theta}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left [   \\overline{z}_{k}\\left (   h_{1}\\widetilde{b}\\left (   \\theta\\right )   + h_{3}\\right )   \\dot { p}\\right ]   _ { i_{2}}\\nonumber\\end{aligned}\\ ] ]  which is already degenerate ( i.e. , orthogonal to @xmath2850  differentiating with respect to the @xmath2840 of @xmath2851 in @xmath2846 we obtain @xmath2852   _ { i_{1}}\\\\ &   \\left [ { \\displaystyle\\prod\\limits_{s=3}^{m } } \\left\\ {   e_{\\theta}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left\\ {   \\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right )   _ { i_{s}}-e_{\\theta}\\left [ \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ } \\right ]   \\times\\\\ &   \\left\\ {   e_{\\theta}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline { z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left [   \\overline{z}_{k}\\left ( h_{1}\\widetilde{b}\\left (   \\theta\\right )   + h_{3}\\right )   \\dot{p}\\right ] _ { i_{2}}\\\\ &   + \\left (   -1\\right )   ^{m-1}\\left [   \\left (   h_{1}\\widetilde{p}\\left ( \\theta\\right )   + h_{2}\\right )   \\dot{b}\\overline{z}_{k}^{t}\\right ]   _ { i_{1}}\\times\\\\ &   \\left [ { \\displaystyle\\prod\\limits_{s=3}^{m } } \\left\\ {   e_{\\theta}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left\\ {   \\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right )   _ { i_{s}}-e_{\\theta}\\left [ \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ } \\right ]   \\times\\\\ &   \\left [   \\overline{z}_{k}h_{1}\\dot{p}\\right ]   if_{1,\\widetilde{p}\\left ( x_{i_{2}},\\cdot\\right )   , i_{m+1}}\\,\\left (   \\theta\\right)\\end{aligned}\\ ] ] substituting in the above expressions for @xmath2853 and @xmath2854 then projecting on @xmath2855 and again permuting unit indices , we obtain two identical terms both equal to eq ( @xmath2856 .",
    "thus we obtain @xmath286 identical terms in all . upon dividing by @xmath286 ,",
    "we conclude that @xmath2857   \\ $ ] equals @xmath2833 operating on @xmath2858 proving the theorem .",
    "( theorem [ ebrate ] ) equation ( [ eb6 ] ) follows from eq .",
    "( [ eb4 ] ) by our assumption of rate optimality of the initial estimators .",
    "we next prove eq .",
    "( [ eb3 ] ) by induction .  for @xmath1917@xmath2859",
    "-\\widetilde{\\psi}_{k}\\\\ &   = e\\left [   \\widehat{b}\\widehat{p}h_{1}+\\widehat{b}h_{2}+\\widehat{p}h_{3}\\right ]   -e\\left [   \\widetilde{b}\\widetilde{p}h_{1}+\\widetilde{b}h_{2}+\\widetilde{p}h_{3}\\right ] \\\\ &   = e\\left [   \\dot{b}\\dot{p}\\overline{z}_{k}^{t}\\left (   p-\\widehat{p}\\right ) \\left\\ {   \\dot{p}\\right\\ }   ^{-1}h_{1}\\right ]   e_{\\theta}\\left [   \\dot{b}\\dot { p}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   ^{-1}\\\\ &   \\times e\\left [   \\dot{p}\\dot{b}\\overline{z}_{k}\\left (   b-\\widehat{b}\\ \\right )   \\left\\ {   \\dot{b}\\right\\ }   ^{-1}h_{1}\\right]\\end{aligned}\\ ] ]    where the last equality follows from @xmath2860   ^{-1}e\\left [   \\dot{p}\\dot{b}\\overline{z}_{k}\\left ( b-\\widehat{b}\\ \\right )   \\left\\ {   \\dot{b}\\right\\ }   ^{-1}h_{1}\\right ] \\\\ \\widetilde{p }   &   = \\widehat{p}-\\dot{p}\\overline{z}_{k}^{t}e_{\\theta}\\left [ \\dot{b}\\dot{p}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   ^{-1}e\\left [ \\dot{b}\\dot{p}\\overline{z}_{k}\\left (   p-\\widehat{p}\\right )   \\left\\ {   \\dot { p}\\right\\ }   ^{-1}h_{1}\\right]\\end{aligned}\\ ] ]    next , we proceed by induction .",
    "assume [ eb3 ] holds for @xmath2861 we next show that it holds for @xmath190@xmath2862 \\\\ &   = \\left\\ { \\begin{array } [ c]{c}\\left (   -1\\right )   ^{m-2}e\\left [   q^{2}\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\overline{z}_{k}^{t}\\right ] \\\\",
    "\\left\\ {   e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] -i_{k\\times k}\\right\\ }   ^{m-2}\\left\\ {   e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}e\\left [   q^{2}\\left ( \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   \\right ] \\end{array } \\right\\ } \\\\ &   + \\left\\ { \\begin{array } [ c]{c}\\left (   -1\\right )   ^{m-1}e\\left [   q^{2}\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\overline{z}_{k}^{t}\\right ] \\\\",
    "\\times\\left\\ {   e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] -i_{k\\times k}\\right\\ }   ^{m-2}e\\left [   q^{2}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   \\right ] \\end{array } \\right\\ } \\\\ &   = \\left (   -1\\right )   ^{m-1}e\\left [   q^{2}\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\overline{z}_{k}^{t}\\right ]   \\left\\ {   e\\left [ q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   -i_{k\\times k}\\right\\ } ^{m-1}\\\\ &   \\times\\left\\ {   e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] \\right\\ }   ^{-1}e\\left [   q^{2}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right ) \\right]\\end{aligned}\\ ] ]    finally we prove that [ eb3 ] implies [ eb4 ] . for any random variable @xmath2863 define @xmath2864   = \\widehat{q}\\overline{z}_{k}^{t}\\widehat{e}\\left [ \\delta g\\widehat{q}\\overline{z}_{k}h\\right ] \\\\",
    "\\widehat{r}^{t}\\left (   h\\right )    &   = \\widehat{r}\\circ\\widehat{r}^{t-1}\\left ( h\\right )   \\text { \\ for } t\\geq2\\end{aligned}\\ ] ]      then @xmath2867{c}e\\left [   q^{2}\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\overline{z}_{k}^{t}\\right ]   \\left\\ {   \\widehat{e}\\left [   \\delta g\\text { } \\widehat{q}^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{m-2}\\times\\\\ \\widehat{e}\\left [   \\delta g\\text { } \\widehat{q}\\overline{z}_{k}\\frac { \\widehat{q}}{q}\\pi\\left (   q\\overline{z}_{k}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   |\\left (   q\\overline{z}_{k}\\right )   \\right )   \\right ] \\end{array } \\right\\ } \\\\ &   = e\\left [   q\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\frac { q}{\\widehat{q}}\\widehat{r}^{m-1}\\left (   \\frac{\\widehat{q}}{q}\\pi\\left ( q\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   |\\left (   q\\overline{z}_{k}\\right )   \\right )   \\right )   \\right]\\end{aligned}\\]]@xmath2868 \\\\ &   \\leq\\left\\ {   \\widehat{e}\\left [   q\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\frac{qf}{\\widehat{q}\\widehat{f}}\\right ]   ^{2}\\right\\ }   ^{\\frac { 1}{2}}\\left\\ {   \\widehat{e}\\left [   \\widehat{r}^{m-1}\\left (   \\frac{\\widehat{q}}{q}\\pi\\left (   q\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   |\\left ( q\\overline{z}_{k}\\right )",
    "\\right )   \\right )   \\right ]   ^{2}\\right\\ }   ^{\\frac { 1}{2}}\\ ] ] by cauchy shwartz .    now @xmath2869   ^{2}\\\\ &   \\leq\\widehat{e}\\left [   \\left (   \\delta g\\right )   ^{2}\\left [   \\widehat{r}^{m-2}\\left (   \\frac{\\widehat{q}}{q}\\pi\\left (   q\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   |\\left (   q\\overline{z}_{k}\\right )   \\right )   \\right ) \\right ]   ^{2}\\right]\\end{aligned}\\ ] ]      iterating this calculation @xmath1324 times we find @xmath2869   ^{2}\\\\ &   \\leq\\left\\vert \\left\\vert \\delta g\\right\\vert \\right\\vert _ { \\infty } ^{2\\left (   m-1\\right )   } \\widehat{e}\\left [   \\frac{\\widehat{q}}{q}\\pi\\left ( q\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   |\\left (   q\\overline{z}_{k}\\right )   \\right )   \\right ]   ^{2}\\\\ &   \\leq\\left\\vert \\left\\vert \\delta g\\right\\vert \\right\\vert _ { \\infty } ^{2\\left (   m-1\\right )   } \\left\\vert \\left\\vert \\frac{\\widehat{g}}{g}\\right\\vert \\right\\vert _ { \\infty}e\\left (   \\pi\\left (   q\\left ( \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   |\\left (   q\\overline{z}_{k}\\right )",
    "\\right )   \\right )   ^{2}\\\\ &   \\leq\\left\\vert \\left\\vert \\delta g\\right\\vert \\right\\vert _ { \\infty } ^{2\\left (   m-1\\right )   } \\left\\vert \\left\\vert \\frac{\\widehat{g}}{g}\\right\\vert \\right\\vert _ { \\infty}\\int q^{2}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   ^{2}f\\left (   x\\right )   dx\\\\ &   \\leq\\left\\vert \\left\\vert \\delta g\\right\\vert \\right\\vert",
    "_ { \\infty } ^{2\\left (   m-1\\right )   } \\left\\vert \\left\\vert \\frac{\\widehat{g}}{g}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\frac{q^{2}f}{\\dot{p}^{2}}\\right\\vert \\right\\vert",
    "_ { \\infty}\\int\\left (   p\\left (   x\\right ) -\\widehat{p}\\left (   x\\right )   \\right )   ^{2}dx\\\\ &   = \\left\\vert \\left\\vert \\delta g\\right\\vert \\right\\vert _ { \\infty}^{2\\left ( m-1\\right )   } \\left\\vert \\left\\vert \\frac{q^{2}f}{\\dot{p}^{2}}\\right\\vert \\right\\vert _ { \\infty}\\left [   \\int\\left (   p\\left (   x\\right )   -\\widehat{p}\\left (   x\\right )   \\right )   ^{2}dx\\right ]   \\left (   1+o_{p}\\left (   1\\right ) \\right)\\end{aligned}\\ ] ]    by @xmath2871  next @xmath2872   ^{2}\\\\ &   = \\int\\frac{q^{2}f}{\\dot{b}^{2}}\\frac{g}{\\widehat{g}}\\left (   b\\left ( x\\right )   -\\widehat{b}\\left (   x\\right )   \\right )   ^{2}dx\\\\ &   \\leq\\left\\vert \\left\\vert \\frac{q^{2}f}{\\dot{b}^{2}}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\frac{g}{\\widehat{g}}\\right\\vert \\right\\vert",
    "_ { \\infty}\\int\\left (   b\\left (   x\\right )   -\\widehat{b}\\left (   x\\right )   \\right ) ^{2}dx\\\\ &   = \\left\\vert \\left\\vert \\frac{q^{2}f}{\\dot{b}^{2}}\\right\\vert \\right\\vert _ { \\infty}\\left [   \\int\\left (   b\\left (   x\\right )   -\\widehat{b}\\left (   x\\right ) \\right )   ^{2}dx\\right ]   \\left (   1+o_{p}\\left (   1\\right )   \\right)\\end{aligned}\\ ] ]",
    "then we know @xmath2873{c}\\left\\vert \\left\\vert \\delta g\\right\\vert \\right\\vert _ { \\infty}^{m-1}\\left\\vert \\left\\vert \\left (   \\frac{\\dot{b}}{\\dot{p}}g\\right )   \\right\\vert \\right\\vert _ { \\infty}^{\\frac{1}{2}}\\left\\vert \\left\\vert \\frac{\\dot{p}}{\\dot{b}}g\\right\\vert \\right\\vert _ { \\infty}^{\\frac{1}{2}}\\left ( 1+o_{p}\\left (   1\\right )   \\right )   \\times\\\\ \\left\\ {   \\int\\left (   p\\left (   x\\right )   -\\widehat{p}\\left (   x\\right )   \\right ) ^{2}dx\\right\\ }   ^{\\frac{1}{2}}\\left\\ {   \\int\\left (   b\\left (   x\\right ) -\\widehat{b}\\left (   x\\right )   \\right )   ^{2}dx\\right\\ }   ^{\\frac{1}{2}}\\end{array } \\right\\}\\ ] ]    to prove theorem [ var_multi ] , we first give the following univariate result .",
    "suppose that we have a set @xmath2874 such that for each @xmath2875 pair , @xmath2876 either spans @xmath2877 or spans @xmath2878 where @xmath2879 and @xmath2880 are both nonnegative integers and @xmath2881",
    "let @xmath2882 we first introduce an important preliminary lemma .",
    "[ var_kernel]for @xmath2883 and @xmath2884 if  @xmath2885 then @xmath2886 otherwise if @xmath2887 then @xmath2888 and @xmath2889 are both nonnegative integers and @xmath2890 .  thus , @xmath2891   \\right ) ^{-1}\\left (   \\pi_{j=1}^{m+1}k_{2j-1}\\right )   = o\\left (   1\\right)\\ ] ]        case 2 : @xmath1917  if @xmath2894 then , @xmath2895 \\\\ &   \\geq e\\left [   \\left (   k_{\\left (   1,k^{\\ast}\\right )   } \\left (   x , x\\right ) \\right )   ^{2}\\right ]   \\geq\\left (   e\\left [   k_{\\left (   1,k^{\\ast}\\right ) } \\left (   x , x\\right )   \\right ]   \\right )   ^{2}=\\left (   k^{\\ast}\\right ) ^{2}\\asymp k_{1}k_{3}.\\end{aligned}\\ ] ] similarly , if @xmath2896 and we further assume @xmath2897 wlog ,  then@xmath2895 \\\\ &   \\geq e\\left [   \\left (   k_{\\left (   \\frac{k_{1}}{2}+1,k_{1}\\right )   } \\left ( x , x\\right )   \\right )   ^{2}\\right ]   \\geq\\left (   e\\left [   k_{\\left (   \\frac{k_{1}}{2}+1,k_{1}\\right )   } \\left (   x , x\\right )   \\right ]   \\right )   ^{2}\\\\ &   = \\left (   \\frac{k_{1}}{2}\\right )   ^{2}\\asymp k_{1}k_{3};\\end{aligned}\\ ] ] otherwise , wlog , we assume that @xmath2898 then@xmath2895 \\\\ &   = e\\left [   \\sum_{r = k_{0}}^{k_{1}}\\phi_{r}^{2}\\left (   x\\right )   \\sum _ { s = k_{2}}^{k_{3}}\\phi_{s}^{2}\\left (   x\\right )   \\right ] \\\\ &   \\geq e\\left [   \\sum_{r = k_{0}}^{k_{1}}\\phi_{r}^{2}\\left (   x\\right ) \\sum_{s=\\frac{k_{3}}{2}+1}^{k_{3}}\\phi_{s}^{2}\\left (   x\\right )   \\right]\\end{aligned}\\ ] ]      let @xmath2901 indicate the correponding compactly supported father / mother wavelet on @xmath2902   , $ ] whose absolute value is bounded above by @xmath2903 by the continuity of @xmath2904 there exists a set @xmath2905 which is a union of finite number of disjoint open intervals , such that @xmath2906 is greater than @xmath2907 on @xmath12 . moreover , the lebesgue measure ( i.e. , the length ) of @xmath2908 is greater than @xmath2909 because @xmath2901 is bounded and has unit length .",
    "specifically,@xmath2910    by definition @xmath2911 is a sequence of level @xmath1164 scaled and translated father wavelets on the unit interval @xmath436   $ ] ,  therefore it is obvious that the lebesgue measure of the set @xmath2912 is greater than @xmath2913 furthermore , for @xmath2914 , the set @xmath2915 consists of multiple disjoint open intervals whose lengths are all of order @xmath2916  in contrast , the support for any level @xmath2917 scaled and translated mother wavelet @xmath2918  @xmath2919 , is of order @xmath2920 as @xmath2921 therefore ,  at least @xmath2922 proportion of the level @xmath2917 scaled and translated mother wavelets @xmath2923 have their support inside @xmath2924 hence ,  @xmath2895 \\\\ &   > e\\left [   1_{\\widetilde{a}}\\sum_{r=1}^{k^{\\ast}}\\phi_{r}^{2}\\left ( x\\right )   \\sum_{s=\\frac{k_{3}}{2}+1}^{k_{3}}\\phi_{s}^{2}\\left (   x\\right ) \\right ] \\\\ &   \\geq\\frac{k^{\\ast}}{2\\left (   u_{w}-l_{w}\\right )   } e\\left [   1_{\\widetilde{a}}\\sum_{s=\\frac{k_{3}}{2}+1}^{k_{3}}\\phi_{s}^{2}\\left (   x\\right )   \\right ] \\\\ &   > \\frac{k^{\\ast}}{2\\left (   u_{w}-l_{w}\\right )   } \\frac{1}{4m_{w}^{2}\\left ( u_{w}-l_{w}\\right )   } \\frac{k_{3}}{2}\\\\ &   \\asymp k_{1}k_{3}.\\end{aligned}\\ ] ]    case 3 : @xmath337  wlog , we assume that @xmath2925 and @xmath2926 for @xmath2927 @xmath2928 then @xmath2929 \\\\ &   \\geq e\\left [   \\left (   k_{\\left (   k_{0},k_{1}\\right )   } \\right )   ^{l_{1}}{\\displaystyle\\prod\\limits_{r=2}^{t } } \\left (   k_{\\left (   \\frac{k_{2l_{r-1}+1}}{2}+1,k_{2l_{r-1}+1}\\right )   } \\right ) ^{l_{r}-l_{r-1}}\\right ] \\\\ &   > e\\left [   \\left (   \\sum_{r = k_{0}}^{k_{1}}\\phi_{r}^{2}\\left (   x\\right ) \\right )   ^{l_{1}}{\\displaystyle\\prod\\limits_{r=2}^{t } } \\left (   \\sum_{r=\\frac{k_{2l_{r-1}+1}}{2}+1}^{k_{2l_{r-1}+1}}\\phi_{r}^{2}\\left (   x\\right )   \\right )   ^{l_{r}-l_{r-1}}\\right]\\end{aligned}\\ ] ]    by a similar argument as that in case 2 , we can show that there exists a set @xmath2930 which consists of multiple disjoint open intervals , such that @xmath2931 @xmath1939 @xmath2932 and @xmath2933 for some positive constants @xmath2934 moreover , by the multiresolution analysis ( mra ) property of compactly supported wavelets and the fact that @xmath2935 , it is obvious that at least @xmath2936 proportion of the level @xmath2937 scaled and translated mother wavelets @xmath2938 have support inside @xmath2939 hence we can find a set @xmath2940 , which is also a union of disjoint open intervals , such that @xmath2941 @xmath1939 @xmath2942 and @xmath2943 for some positive constants @xmath2944  furthermore , by applying this algorithm in a nested fashion @xmath2945 times , we can find a decreasing sequence of sets @xmath2946 such that for any @xmath2947 @xmath2948  @xmath1939 @xmath2949 and  @xmath2950 for some positive constants @xmath2951 and @xmath2952  in addition ,  @xmath2953 proportion of the level @xmath2954 scaled and translated mother wavelets @xmath2955 have support inside @xmath2956    hence , @xmath2929 \\\\ &   > \\left ( { \\displaystyle\\prod\\limits_{r=1}^{t-1 } } k_{2l_{r-1}+1}^{l_{r}-l_{r-1}}\\right )   e \\left [   \\left (   1_{a_{t-1}}\\sum_{r=\\frac{k_{2l_{t-1}+1}}{2}+1}^{k_{2l_{t-1}+1}}\\phi_{r}^{2}\\left ( x\\right )   \\right )   ^{l_{t}-l_{t-1}}\\right ] \\\\ &   \\geq\\left ( { \\displaystyle\\prod\\limits_{r=1}^{t-1 } } k_{2l_{r-1}+1}^{l_{r}-l_{r-1}}\\right )   e\\left [   \\left (   1_{a_{t-1}}\\sum_{r=\\frac{k_{2l_{t-1}+1}}{2}+1}^{k_{2l_{t-1}+1}}\\phi_{r}^{2}\\left ( x\\right )   \\right )   \\right ]   ^{l_{t}-l_{t-1}}\\\\ &   > \\left ( { \\displaystyle\\prod\\limits_{r=1}^{t-1 } } k_{2l_{r-1}+1}^{l_{r}-l_{r-1}}\\right )   \\left (   \\frac{k_{2l_{t-1}+1}}{2}{\\displaystyle\\prod\\limits_{r=1}^{t-1 } } \\frac{\\delta_{r}^{\\phi}}{2}\\right )   ^{l_{t}-l_{t-1}}\\times{\\displaystyle\\prod\\limits_{j=1}^{m+1 } } k_{2j-1}\\ ] ]        ( lemma : [ var_uni ] ) case 1:@xmath2958@xmath2959   ^{2}\\\\ &   = e\\left [   \\left (   k_{\\left (   k_{0},k_{1}\\right )   } \\left (   x_{i_{1}},x_{i_{2}}\\right )   \\right )   ^{2}\\right ] \\\\ &   = e\\left [   \\left (   k_{\\left (   k_{0},k_{1}\\right )   } \\left (   x_{i_{1}},x_{i_{1}}\\right )   \\right )   \\right ] \\\\ &   = k_{1}-k_{0}\\ ] ] which follows from the orthonormality of wavelets .",
    "consider the case @xmath2124 @xmath2962 \\\\ &   = e\\left [   \\left (   k_{\\left (   k_{0},k_{1}\\right )   } \\left (   x_{i_{2}},x_{i_{2}}\\right )   k_{\\left (   k_{2},k_{3}\\right )   } \\left (   x_{i_{2}},x_{i_{2}}\\right )   \\right )   \\right ] \\\\ &   \\leq||k_{\\left (   k_{0},k_{1}\\right )   } \\left (   x , x\\right )   ||_{\\infty } ||k_{\\left (   k_{2},k_{3}\\right )   } \\left (   x , x\\right )   ||_{\\infty}\\ ] ] if @xmath2963 then @xmath2964 .",
    "similarly , if @xmath2965 and @xmath2966 then @xmath2967 otherwise , if @xmath2965 and @xmath2968 then @xmath2969 finally ,  for @xmath1927@xmath2970 \\\\ &   = e\\left [   k_{\\left (   k_{0},k_{1}\\right )   } \\left (   x_{i_{1}},x_{i_{2}}\\right )   ^{2}\\left ( { \\displaystyle\\prod\\limits_{j=2}^{m } } k_{\\left (   k_{2j-2},k_{2j-1}\\right )   } \\left (   x_{i_{j}},x_{i_{j+1}}\\right ) \\right )   ^{2}k_{\\left (   k_{2m},k_{2m+1}\\right )   } \\left (   x_{i_{m+1}},x_{i_{m+2}}\\right )   ^{2}\\right ] \\\\ &   = e\\left [   k_{\\left (   k_{0},k_{1}\\right )   } \\left (   x_{i_{2}},x_{i_{2}}\\right )   \\left ( { \\displaystyle\\prod\\limits_{j=2}^{m } } k_{\\left (   k_{2j-2},k_{2j-1}\\right )   } \\left (   x_{i_{j}},x_{i_{j+1}}\\right ) \\right )   ^{2}k_{\\left (   j_{2m},k_{2m+1}\\right )   } \\left (   x_{i_{m+2}},x_{i_{m+2}}\\right )   \\right ] \\\\ &   \\leq||k_{\\left (   k_{0},k_{1}\\right )   } \\left (   x , x\\right )   ||_{\\infty } e\\left [   \\left ( { \\displaystyle\\prod\\limits_{j=2}^{m } } k_{\\left (   k_{2j-2},k_{2j-1}\\right )   } \\left (   x_{i_{j}},x_{i_{j+1}}\\right ) \\right )   ^{2}\\right ]   ||k_{\\left (   k_{2m},k_{2m+1}\\right )   } \\left ( x , x\\right )   ||_{\\infty}\\\\ &   = ||k_{\\left (   k_{0},k_{1}\\right )   } \\left (   x , x\\right )   ||_{\\infty } ||k_{\\left (   k_{2m},k_{2m+1}\\right )   } \\left (   x , x\\right )   ||_{\\infty}e\\left [ \\left ( { \\displaystyle\\prod\\limits_{j=2}^{m } } k_{\\left (   k_{2j-2},k_{2j-1}\\right )   } \\left (   x_{i_{j}},x_{i_{j+1}}\\right ) \\right )   ^{2}\\right ] \\\\ &   \\leq{\\displaystyle\\prod\\limits_{j=1}^{m+1 } }        we shall first show that @xmath2972 and then complete the proof by showing that @xmath2973 specifically , @xmath2974 \\\\ &   = e\\left [   k_{\\left (   1,k_{1}\\right )   } \\left (   x_{i_{3}},x_{i_{3}}\\right ) k_{\\left (   1,k_{3}\\right )   } \\left (   x_{i_{3}},x_{i_{3}}\\right ) { \\displaystyle\\prod\\limits_{j=3}^{m+1 } } \\left (   k_{\\left (   1,k_{2j-1}\\right )   } \\left (   x_{i_{j}},x_{i_{j+1}}\\right ) \\right )   ^{2}\\right ] \\\\ &   -e\\left [   k_{1}k_{3}\\left\\ {   h_{x_{i_{3}}}\\left (   x_{i_{3}}\\right ) -\\left (   \\mathcal{k}_{\\left (   1,k_{3}\\right )   } \\mathcal{\\circ}h_{x_{i_{3}}}\\right )   \\left (   x_{i_{3}}\\right )   \\right\\ } { \\displaystyle\\prod\\limits_{j=3}^{m+1 } } \\left (   k_{\\left (   1,k_{2j-1}\\right )   } \\left (   x_{i_{j}},x_{i_{j+1}}\\right ) \\right )   ^{2}\\right]\\end{aligned}\\ ] ] where @xmath2975 and @xmath2976   .$ ]    as previously shown ,  there exists a positive constant @xmath2977 such that @xmath2978  by the regularity property of compactly supported wavelets and the approximation property of the kernel @xmath2979 it is obvious that @xmath2980 thus , @xmath2030{c}k_{1}k_{3}\\left\\ {   h_{x_{i_{3}}}\\left (   x_{i_{2}}\\right )   -\\left ( \\mathcal{k}_{\\left (   1,k_{3}\\right )   } \\mathcal{\\circ}h_{x_{i_{3}}}\\right ) \\left (   x_{i_{3}}\\right )   \\right\\ } \\\\ \\times{\\displaystyle\\prod\\limits_{j=3}^{m+1 } } \\left (   k_{\\left (   1,k_{2j-1}\\right )   } \\left (   x_{i_{j}},x_{i_{j+1}}\\right ) \\right )   ^{2}\\end{array } \\right ] \\\\ &   = o\\left (   \\pi_{j=1}^{m+1}k_{2j-1}\\right)\\end{aligned}\\ ] ]    in fact , by arguing similarly as above , @xmath2981{c}k_{\\left (   1,k_{1}\\right )   } \\left (   x_{i_{3}},x_{i_{3}}\\right )   k_{\\left ( 1,k_{3}\\right )   } \\left (   x_{i_{3}},x_{i_{3}}\\right ) \\\\ \\times{\\displaystyle\\prod\\limits_{j=3}^{m+1 } } \\left (   k_{\\left (   1,k_{2j-1}\\right )   } \\left (   x_{i_{j}},x_{i_{j+1}}\\right ) \\right )   ^{2}\\end{array } \\right\\ }   \\right ]   + o\\left (   \\pi_{j=1}^{m+1}k_{2j-1}\\right ) \\\\ &   = e\\left [   \\left\\ { \\begin{array } [ c]{c}k_{\\left (   1,k_{1}\\right )   } \\left (   x_{i_{4}},x_{i_{4}}\\right )   k_{\\left ( 1,k_{3}\\right )   } \\left (   x_{i_{4}},x_{i_{4}}\\right )   k_{\\left (   1,k_{5}\\right )   } \\left (   x_{i_{4}},x_{i_{4}}\\right ) \\\\ \\times{\\displaystyle\\prod\\limits_{j=4}^{m+1 } } \\left (   k_{\\left (   1,k_{2j-1}\\right )   } \\left (   x_{i_{j}},x_{i_{j+1}}\\right ) \\right )   ^{2}\\end{array } \\right\\ }   \\right ] \\\\ &   + o\\left (   \\pi_{j=1}^{m+1}k_{2j-1}\\right ) \\\\ &   = e\\left [ { \\displaystyle\\prod\\limits_{j=1}^{m+1 } } k_{\\left (   1,k_{2j-1}\\right )   } \\left (   x , x\\right )   \\right ]   + o\\left ( \\pi_{j=1}^{m+1}k_{2j-1}\\right ) \\\\ &   \\asymp{\\displaystyle\\prod\\limits_{j=1}^{m+1 } } k_{2j-1}\\ ] ]    now , we prove eq .",
    "( [ eq1 ] ) .",
    "@xmath2982@xmath906@xmath2983 where @xmath2984 may be @xmath2985 or @xmath2986 but @xmath2987 therefore @xmath2988  as a direct consequence of the previously proved result that @xmath2989 the proof is complete .",
    "( theorem [ var_multi ] ) we note that since the linear span of @xmath2990 equals @xmath2991we have @xmath2992{c}t_{1}, ... ,t_{l}:\\\\ 1\\leq t_{u}\\leq k_{j , u}\\\\ u=1, ...",
    ",l \\end{array } \\right\\ }   } \\prod\\limits_{u=1}^{l}\\overline{\\varphi}_{t_{u}}\\left (   x_{i_{j}}^{u}\\right )   \\prod\\limits_{u^{\\prime}=1}^{l}\\overline{\\varphi}_{t_{u^{\\prime } } } \\left (   x_{i_{j+1}}^{u^{\\prime}}\\right)\\end{aligned}\\ ] ] so that @xmath2993 the remainder of the proof then follows from lemma [ var_uni ] .",
    "( theorem [ 3.19 ] )  in the proof of theorem [ 3.19 ] , the following lemma plays a central role .",
    "note that expectations and probabilities remain conditional on @xmath391 even when it is suppressed in the notation .",
    "[ 3.19_lemma]let @xmath2994 be the symmetric kernel of @xmath2995 then for any @xmath2421 and @xmath2996 @xmath2997   -var_{\\widehat{\\theta}}\\left [   \\left (   \\widehat{\\mathbb{if}}_{m , m,\\widetilde{\\psi}_{k}\\left ( \\cdot\\right )   } \\right )   ^{2}\\right ] \\nonumber\\\\ &   = o\\left (   n^{-m}\\left\\ {   e_{\\theta}\\left [   \\left (   \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\right )   ^{2}\\right ] -e_{\\widehat{\\theta}}\\left [   \\left (   \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\right )   ^{2}\\right ]   \\right\\ }   \\right ) \\nonumber\\\\ &   + o\\left (   \\frac{1}{n}\\left\\ {   e_{\\theta}\\left [   \\widehat{if}_{m , m,\\overline { i}_{m}}^{\\left (   s\\right )   } \\right ]   \\right\\ }   ^{2}\\right ) \\nonumber\\\\ &   + o\\left (   \\max\\left (   \\frac{1}{n},\\frac{k^{m-2}}{n^{m-1}}\\right )   \\right ) \\label{lemma_1}\\ ] ]    and @xmath2998   e_{\\theta}\\left [ \\widehat{if}_{m_{2},m_{2},\\overline{i}_{m}}^{\\left (   s\\right )   } \\right ] \\right )   + o\\left (   \\max\\left (   \\frac{1}{n},\\frac{k^{m_{1}-2}}{n^{m_{1}-1}}\\right )   \\right ) \\label{lemma_2}\\ ] ]      ( theorem [ 3.19 ] ) . by the degeneracy of @xmath2999 for any @xmath3000 under @xmath3001",
    "@xmath3002   -var_{\\widehat{\\theta}}\\left [   \\widehat{\\mathbb{if}}_{m,\\widetilde{\\psi}_{k}\\left (   \\cdot\\right ) } |\\widehat{\\theta}\\right ]   } { var_{\\widehat{\\theta}}\\left [ \\widehat{\\mathbb{if}}_{m,\\widetilde{\\psi}_{k}\\left (   \\cdot\\right ) } |\\widehat{\\theta}\\right ]   } \\\\ &   = \\frac{\\left\\ { \\begin{array } [ c]{c}{\\displaystyle\\sum\\limits_{t=1}^{m } } \\left (   var_{\\theta}\\left [   \\widehat{\\mathbb{if}}_{t , t,\\widetilde{\\psi}_{k}\\left (   \\cdot\\right )   } \\right ]   -var_{\\widehat{\\theta}}\\left [ \\widehat{\\mathbb{if}}_{t , t,\\widetilde{\\psi}_{k}\\left (   \\cdot\\right )   } \\right ] \\right ) \\\\ + 2{\\displaystyle\\sum\\limits_{1\\leq t_{1}<t_{2}\\leq m } } cov_{\\theta}\\left [   \\widehat{\\mathbb{if}}_{t_{1},t_{1},\\widetilde{\\psi}_{k}\\left (   \\cdot\\right )   } , \\widehat{\\mathbb{if}}_{t_{2},t_{2},\\widetilde{\\psi } _ { k}\\left (   \\cdot\\right )   } \\right ] \\end{array } \\right\\ }   } { var_{\\widehat{\\theta}}\\left [   \\widehat{\\mathbb{if}}_{m,\\widetilde{\\psi}_{k}\\left (   \\cdot\\right )   } |\\widehat{\\theta}\\right ]   } , \\end{aligned}\\ ] ]    which equals @xmath3003   -e_{\\widehat{\\theta}}\\left [ \\left (   \\widehat{if}_{t , t,\\overline{i}_{m}}^{\\left (   s\\right )   } \\right ) ^{2}\\right ]   \\right )   } { { \\displaystyle\\sum\\limits_{t=1}^{m } } n^{-t}e_{\\widehat{\\theta}}\\left [   \\left (   \\widehat{if}_{t , t,\\overline{i}_{m}}^{\\left (   s\\right )   } \\right )   ^{2}\\right ]   } \\left (   1+o\\left (   1\\right ) \\right ) \\\\ &   + \\frac{o\\left (   \\max\\left (   \\frac{1}{n},\\frac{k^{m-2}}{n^{m-1}}\\right ) \\right )   } { var_{\\widehat{\\theta}}\\left [   \\widehat{\\mathbb{if}}_{m,\\widetilde{\\psi}_{k}\\left (   \\cdot\\right )   } |\\widehat{\\theta}\\right ]   } \\ ] ]      by assumption , @xmath3004 as @xmath3005 , hence @xmath3006   -e_{\\widehat{\\theta}}\\left [ \\left (   \\widehat{if}_{t , t,\\overline{i}_{m}}^{\\left (   s\\right )   } \\right ) ^{2}\\right ]   } { e_{\\widehat{\\theta}}\\left [   \\left (   \\widehat{if}_{t , t,\\overline { i}_{m}}^{\\left (   s\\right )   } \\right )   ^{2}\\right ]   } \\\\ &   = \\frac{e_{\\widehat{\\theta}}\\left [   \\left (   \\widehat{if}_{t , t,\\overline { i}_{m}}^{\\left (   s\\right )   } \\right )   ^{2}\\left ( { \\displaystyle\\prod\\limits_{s=1}^{t } } \\frac{f\\left (   o_{i_{s}};\\theta\\right )   } { f\\left (   o_{i_{s}};\\widehat{\\theta } \\right )   } -1\\right )   \\right ]   } { e_{\\widehat{\\theta}}\\left [   \\left ( \\widehat{if}_{t , t,\\overline{i}_{m}}^{\\left (   s\\right )   } \\right )   ^{2}\\right ] } \\\\ &   \\leq\\sup_{\\mathbf{o}_{\\overline{i}_{t}}}\\left\\vert { \\displaystyle\\prod\\limits_{s=1}^{t } } \\frac{f\\left (   o_{i_{s}};\\theta\\right )   } { f\\left (   o_{i_{s}};\\widehat{\\theta } \\right )   } -1\\right\\vert \\\\ &   = o\\left (   \\underset{o\\in\\mathcal{o}}{sup}\\left\\vert f\\left ( o;\\widehat{\\theta}\\right )   -f\\left (   o;\\theta\\right )   \\right\\vert \\right ) = o\\left (   1\\right )   .\\end{aligned}\\ ] ]    furthermore , @xmath3007 } = o\\left (   1\\right )   $ ] as well since @xmath3008   \\times\\max\\left (   \\frac{1}{n},\\frac{k^{m-1}}{n^{m}}\\right )   $ ] from theorem [ var_multi ] .",
    "thus the proof of thoerem [ 3.19 ] is complete .        as proved in theorem [ on ] , for any @xmath316 @xmath3011{c}\\left [   \\left (   h_{1}\\widehat{p}+h_{2}\\right )   \\dot{b}\\overline{z}_{k}^{t}\\right ]   _ { i_{1}}\\left [ { \\displaystyle\\prod\\limits_{s=3}^{m } } \\left\\ {   \\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right )   _ { i_{s}}-i_{k\\times k}\\right\\ }   \\right ] \\\\ \\times\\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right )   \\dot { p}\\right ]   _ { i_{2}}\\end{array } \\right\\}\\end{aligned}\\ ] ]    we first consider the case when @xmath332 @xmath3012 \\\\ &   = e_{\\theta}\\left [   q^{2}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right ) \\overline{z}_{k}^{t}\\right ]   \\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right )   \\dot{p}\\right ]   _ { i_{2}}\\\\ &   = \\widehat{e}\\left [   \\frac{f\\left (   x\\right )   } { \\widehat{f}\\left (   x\\right ) } \\frac{q^{2}}{\\widehat{q}}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right ) \\widehat{q}\\overline{z}_{k}^{t}\\right ]   \\left [   \\frac{\\widehat{q}}{\\widehat{q}}\\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right )   \\dot { p}\\right ]   _ { i_{2}}\\\\ &   = \\widehat{\\pi}\\left [   \\left (   \\frac{f\\left (   x\\right )   } { \\widehat{f}\\left ( x\\right )   } \\frac{q^{2}}{\\widehat{q}}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   \\right )   |\\left (   \\widehat{q}\\overline{z}_{k}\\right )   _ { i_{2}}\\right ]   \\left (   \\frac{\\left (   h_{1}\\widehat{b}+h_{3}\\right )   \\dot{p}}{\\widehat{q}}\\right )   _ { i_{2}}\\\\ &   = \\left\\vert \\left\\vert p-\\widehat{p}\\right\\vert \\right\\vert _ { 2}\\frac { t_{c}\\left (   o_{i_{2}}\\right )   } { \\left\\vert \\left\\vert p-\\widehat{p}\\right\\vert \\right\\vert _ { 2}}\\left (   \\frac{\\left (   h_{1}\\widehat{b}+h_{3}\\right )   \\dot{p}}{\\widehat{q}}\\right )   _ { i_{2}}\\ ] ]            \\i ) if @xmath3018  then @xmath3019   \\left [ { \\displaystyle\\prod\\limits_{s=3}^{m } } \\left\\ { \\begin{array } [ c]{c}\\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ) _ { i_{s}}\\\\ -i_{k\\times k}\\end{array } \\right\\ }   \\right ] \\\\ &   \\times\\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right ) \\dot{p}\\right ]   _ { i_{2}}\\\\ &   = \\left\\ { \\begin{array } [ c]{c}\\widehat{e}\\left [   \\frac{f\\left (   x\\right )   } { \\widehat{f}\\left (   x\\right ) } q^{2}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   \\overline{z}_{k}^{t}\\right ]   \\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right )   _ { i_{3}}\\\\ -e_{\\theta}\\left [   q^{2}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right ) \\overline{z}_{k}^{t}\\right ] \\end{array } \\right\\ }   \\times\\\\ & { \\displaystyle\\prod\\limits_{s=4}^{m } } \\left\\ { \\begin{array } [ c]{c}\\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ) _ { i_{s}}\\\\ -i_{k\\times k}\\end{array } \\right\\ }   \\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right ) \\dot{p}\\right ]   _ { i_{2}}\\]]@xmath906@xmath3020{c}\\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ) _ { i_{s}}\\\\ -i_{k\\times k}\\end{array } \\right\\ }   \\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right ) \\dot{p}\\right ]   _ { i_{2}}\\\\ &   -e_{\\theta}\\left (   \\widehat{if}_{m-1,m-1,i_{1}i_{2}i_{4} ... i_{m}}|\\mathbf{o}_{-i_{1}}\\right ) \\\\ &   = \\left (   \\frac{\\dot{p}\\dot{b}h_{1}}{\\widehat{q}}\\right )   _ { i_{3}}t_{c}\\left (   o_{i_{3}}\\right )   \\overline{z}_{k , i_{3}}^{t}{\\displaystyle\\prod\\limits_{s=4}^{m } } \\left\\ { \\begin{array } [ c]{c}\\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ) _ { i_{s}}\\\\ -i_{k\\times k}\\end{array } \\right\\ } \\\\ &   \\times\\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right ) \\dot{p}\\right ]   _ { i_{2}}-e_{\\theta}\\left (   \\widehat{if}_{m-1,m-1,i_{1}i_{2}i_{4} ... i_{m}}|\\mathbf{o}_{-i_{1}}\\right)\\end{aligned}\\ ] ]    from the fact that @xmath3021 we have @xmath3022   |\\mathbf{o}_{-i_{1}}\\right )   ^{2}\\right ] \\\\ &   \\leq2e\\left [   \\left ( \\begin{array } [ c]{c}\\left (   \\frac{\\dot{p}\\dot{b}h_{1}}{\\widehat{q}}\\right )   _ { i_{3}}t_{c}\\left ( o_{i_{3}}\\right )   \\overline{z}_{k , i_{3}}^{t}\\times\\\\{\\displaystyle\\prod\\limits_{s=4}^{m } } \\left\\ { \\begin{array } [ c]{c}\\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ) _",
    "{ i_{s}}\\\\ -i_{k\\times k}\\end{array } \\right\\ }   \\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right ) \\dot{p}\\right ]   _ { i_{2}}\\end{array } \\right )   ^{2}\\right ] \\\\ &   + 2e\\left [   \\left (   e_{\\theta}\\left [   \\widehat{if}_{m-1,m-1,i_{1}i_{2}i_{4} ... i_{m}}|\\mathbf{o}_{-i_{1}}\\right ]   \\right )   ^{2}\\right]\\end{aligned}\\ ] ]    from theorem [ var_multi ] , it can be shown that @xmath3023{c}\\left (   \\frac{\\dot{p}\\dot{b}h_{1}}{\\widehat{q}}\\right )   _ { i_{3}}t_{c}\\left ( o_{i_{3}}\\right )   \\overline{z}_{k , i_{3}}^{t}\\times\\\\{\\displaystyle\\prod\\limits_{s=4}^{m } } \\left\\ { \\begin{array } [ c]{c}\\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ) _",
    "{ i_{s}}\\\\ -i_{k\\times k}\\end{array } \\right\\ }   \\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right ) \\dot{p}\\right ]   _ { i_{2}}\\end{array } \\right )   ^{2}\\right ] \\\\ &   = \\left\\vert \\left\\vert p-\\widehat{p}\\right\\vert \\right\\vert _ { \\infty } o\\left (   k^{m-2}\\right ) \\\\ &   = o\\left (   k^{m-2}\\right)\\end{aligned}\\ ] ]        \\iii ) if @xmath3027  wlog , assume @xmath3028 then@xmath3029 \\\\ &   = \\left [   \\left (   h_{1}\\widehat{p}+h_{2}\\right )   \\dot{b}\\overline{z}_{k}^{t}\\right ]   _ { i_{1}}\\left (   e_{\\theta}\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   -i\\right )   \\times\\\\ & { \\displaystyle\\prod\\limits_{s=4}^{m } } \\left\\ { \\begin{array } [ c]{c}\\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ) _ { i_{s}}\\\\ -i_{k\\times k}\\end{array } \\right\\ }   \\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right ) \\dot{p}\\right ]   _ { i_{2}}\\\\ &   = \\left [   \\left (   h_{1}\\widehat{p}+h_{2}\\right )   \\dot{b}\\overline{z}_{k}^{t}\\right ]   _ { i_{1}}\\widehat{e}\\left [   \\delta g\\text { } \\widehat{q}^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right )   _ { i_{4}}\\\\ &   \\times{\\displaystyle\\prod\\limits_{s=5}^{m } } \\left\\ {   \\left (   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right )   _ { i_{s}}-i_{k\\times k}\\right\\ }   \\left [   \\overline{z}_{k}\\left ( h_{1}\\widehat{b}+h_{3}\\right )   \\dot{p}\\right ]   _ { i_{2}}\\left (   \\equiv \\widehat{t}\\right ) \\\\ &   -e_{\\theta}\\left [   \\widehat{if}_{m-1,m-1,i_{1}i_{2}i_{3}i_{5} .. i_{m}}|\\mathbf{o}_{-i_{3}}\\right]\\end{aligned}\\ ] ]    moreover , it can be shown that @xmath3030 = o\\left (   \\left\\vert \\left\\vert \\delta g\\right\\vert \\right\\vert _ { \\infty}^{2}k^{m-2}\\right )   = o\\left (   k^{m-2}\\right )   $ ] following the proof of theorem [ var_multi ] but replacing @xmath3031 with @xmath3032   \\overline{\\phi } _ { 0}^{k}\\left (   x_{i_{2}}\\right )   .$ ] specifically , @xmath3033   \\overline{\\phi}_{0}^{k}\\left (   x_{i_{2}}\\right )   \\overline{\\phi}_{0}^{k , t}\\left (   x_{i_{2}}\\right )   \\widehat{e}\\left [   \\delta g\\text { } \\widehat{q}^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right ) \\\\ &   = \\overline{\\phi}_{0}^{k , t}\\left (   x_{i_{2}}\\right )   \\left (   \\widehat{e}\\left [   \\delta g\\text { } \\widehat{q}^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right )   ^{2}\\overline{\\phi}_{0}^{k}\\left (   x_{i_{2}}\\right ) \\\\ &   \\leq\\left\\vert \\left\\vert \\delta g\\right\\vert \\right\\vert _ { \\infty}^{2}\\overline{\\phi}_{0}^{k , t}\\left (   x_{i_{2}}\\right )   \\overline{\\phi}_{0}^{k}\\left (   x_{i_{2}}\\right)\\end{aligned}\\ ] ]    the last inequality holds because @xmath3034   $ ] is a semi - positive definite symmetric matrix . @xmath3035",
    "\\right )   ^{2}\\right ]   $ ] is of order @xmath3036 by induction assumption .",
    "now , the proof of this proposition is complete .",
    "moreover , by arguing similarly , the result above can be generalized in a straightforward manner to the following proposition .",
    "( lemma [ 3.19_lemma ] ) throughout the proof , we repeatedly use the result that @xmath3039 for any @xmath2421 is degenerate under @xmath3040 we first prove eq .",
    "( [ lemma_1 ] ) . @xmath2820@xmath2997   -var_{\\widehat{\\theta}}\\left [   \\left (   \\widehat{\\mathbb{if}}_{m , m,\\widetilde{\\psi}_{k}\\left ( \\cdot\\right )   } \\right )   ^{2}\\right ] \\\\ &   = e_{\\theta}\\left [   \\left (   \\widehat{\\mathbb{if}}_{m , m,\\widetilde{\\psi}_{k}\\left (   \\cdot\\right )   } \\right )   ^{2}\\right ]   -e_{\\widehat{\\theta}}\\left [ \\left (   \\widehat{\\mathbb{if}}_{m , m,\\widetilde{\\psi}_{k}\\left (   \\cdot\\right ) } \\right )   ^{2}\\right ]   -\\left (   e_{\\theta}\\left [   \\widehat{\\mathbb{if}}_{m , m,\\widetilde{\\psi}_{k}\\left (   \\cdot\\right )   } \\right ]   \\right )   ^{2}\\\\ &   = e_{\\widehat{\\theta}}\\left [   \\left (   \\widehat{\\mathbb{if}}_{m , m,\\widetilde{\\psi}_{k}\\left (   \\cdot\\right )   } \\right )   ^{2}\\left ( { \\textstyle\\prod\\limits_{i=1}^{n } } \\frac{f\\left (   o_{i}\\right )   } { \\widehat{f}\\left (   o_{i}\\right )   } -1\\right ) \\right ]   -\\left (   e_{\\theta}\\left [   \\widehat{\\mathbb{if}}_{m , m,\\widetilde{\\psi } _ { k}\\left (   \\cdot\\right )   } \\right ]   \\right )   ^{2}\\ ] ] can be written as a sum of four terms as below:@xmath3041   -\\left (   e_{\\theta}\\left [   \\widehat{\\mathbb{if}}_{m , m,\\widetilde{\\psi } _ { k}\\left (   \\cdot\\right )   } \\right ]   \\right )   ^{2}\\\\ &   = e_{\\widehat{\\theta}}\\left\\ { \\begin{array } [ c]{c}\\left [   \\binom{n}{m}\\right ]   ^{-2}\\left ( { \\displaystyle\\sum\\limits_{i_{1}<i_{2} .. <i_{m } } } \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\right )",
    "\\\\ \\times\\left ( { \\displaystyle\\sum\\limits_{r_{1}<r_{2} .. <r_{m } } } \\widehat{if}_{m , m,\\overline{r}_{m}}^{\\left (   s\\right )   } \\right )   \\left ( { \\textstyle\\prod\\limits_{i=1}^{n } } \\frac{f\\left (   o_{i}\\right )   } { \\widehat{f}\\left (   o_{i}\\right )   } -1\\right ) \\end{array } \\right\\ } \\\\ &   -\\left (   e_{\\theta}\\left [   \\widehat{\\mathbb{if}}_{m , m,\\widetilde{\\psi}_{k}\\left (   \\cdot\\right )   } \\right ]   \\right )   ^{2}\\\\ &   = e_{\\widehat{\\theta}}\\left\\ { \\begin{array } [ c]{c}\\left [   \\binom{n}{m}\\right ]   ^{-2}\\left [ \\begin{array } [ c]{c}{\\displaystyle\\sum\\limits_{i_{1}<i_{2} .. <i_{m } } } \\left (   \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\right ) ^{2}\\\\ + { \\displaystyle\\sum\\limits_{\\overline{i}_{m}\\cap\\overline{r}_{m}=\\emptyset } } \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\widehat{if}_{m , m,\\overline{r}_{m}}^{\\left (   s\\right )   } \\\\ + { \\displaystyle\\sum\\limits_{1\\leq\\#\\left (   \\overline{i}_{m}\\cap\\overline{r}_{m}\\right )   < m } } \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\widehat{if}_{m , m,\\overline{r}_{m}}^{\\left (   s\\right )   } \\end{array } \\right ]",
    "\\\\ \\times\\left ( { \\textstyle\\prod\\limits_{i=1}^{n } } \\frac{f\\left (   o_{i}\\right )   } { \\widehat{f}\\left (   o_{i}\\right )   } -1\\right ) \\end{array } \\right\\ } \\\\ &   -\\left (   e_{\\theta}\\left [   \\widehat{\\mathbb{if}}_{m , m,\\widetilde{\\psi}_{k}\\left (   \\cdot\\right )   } \\right ]   \\right )   ^{2}\\ ] ] where @xmath3042 is the number of elements in the intersection set @xmath3043  the first term : @xmath3044   ^{-2}{\\displaystyle\\sum\\limits_{i_{1}<i_{2} .. <i_{m } } } \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   2}\\left ( { \\textstyle\\prod\\limits_{i=1}^{n } } \\frac{f\\left (   o_{i}\\right )   } { \\widehat{f}\\left (   o_{i}\\right )   } -1\\right ) \\right ] \\\\ &   = \\left [   \\binom{n}{m}\\right ]   ^{-1}e_{\\widehat{\\theta}}\\left [ \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   2}\\left ( { \\textstyle\\prod\\limits_{i=1}^{n } } \\frac{f\\left (   o_{i}\\right )   } { \\widehat{f}\\left (   o_{i}\\right )   } -1\\right ) \\right ] \\\\ &   = \\left [   \\binom{n}{m}\\right ]   ^{-1}\\left (   e_{\\theta}\\left [   \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   2}\\right ]   -e_{\\widehat{\\theta}}\\left [   \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   2}\\right ] \\right)\\end{aligned}\\ ] ] the second term : @xmath3044   ^{-2}\\left ( { \\displaystyle\\sum\\limits_{\\overline{i}_{m}\\cap\\overline{r}_{m}=\\emptyset } } \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\widehat{if}_{m , m,\\overline{r}_{m}}^{\\left (   s\\right )   } \\right )   \\left ( { \\textstyle\\prod\\limits_{i=1}^{n } } \\frac{f\\left (   o_{i}\\right )   } { \\widehat{f}\\left (   o_{i}\\right )   } -1\\right ) \\right ] \\\\ &   = \\left [   \\binom{n}{m}\\right ]   ^{-2}e_{\\theta}\\left [ { \\displaystyle\\sum\\limits_{\\overline{i}_{m}\\cap\\overline{r}_{m}=\\emptyset } } \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\widehat{if}_{m , m,\\overline{r}_{m}}^{\\left (   s\\right )   } \\right ] \\\\ &   = \\left [   \\binom{n}{m}\\right ]   ^{-2}\\binom{n}{m}\\binom{n - m}{m}e_{\\theta } \\left [   \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\widehat{if}_{m , m,\\overline{r}_{m}}^{\\left (   s\\right )   } \\right ] \\\\ &   = \\frac{\\left (   n - m\\right )   ... \\left (   n-2m+1\\right )   } { n\\left (   n-1\\right ) ... \\left (   n - m+1\\right )   } \\left (   e_{\\theta}\\left [   \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\right ]   \\right )   ^{2}\\ ] ] substracting the fourth term from the second term , we have @xmath3045   \\left (   e_{\\theta}\\left [ \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\right ]   \\right ) ^{2}\\\\ &   = o\\left (   \\frac{1}{n}\\left (   e_{\\theta}\\left [   \\widehat{if}_{m , m,\\overline { i}_{m}}^{\\left (   s\\right )   } \\right ]   \\right )   ^{2}\\right)\\end{aligned}\\ ] ] the third term:@xmath3044   ^{-2}\\left ( { \\displaystyle\\sum\\limits_{1\\leq\\#\\left (   \\overline{i}_{m}\\cap\\overline{r}_{m}\\right )   < m } } \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\widehat{if}_{m , m,\\overline{r}_{m}}^{\\left (   s\\right )   } \\right )   \\left ( { \\textstyle\\prod\\limits_{i=1}^{n } } \\frac{f\\left (   o_{i}\\right )   } { \\widehat{f}\\left (   o_{i}\\right )   } -1\\right ) \\right ] \\\\ &   = \\sum_{t=1}^{m-1}e_{\\theta}\\left [   \\left [   \\binom{n}{m}\\right ] ^{-2}\\left ( { \\displaystyle\\sum\\limits_{\\#\\left (   \\overline{i}_{m}\\cap\\overline{r}_{m}\\right )   = t } } \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } \\widehat{if}_{m , m,\\overline{r}_{m}}^{\\left (   s\\right )   } \\right )   \\right ] \\\\ &   = \\sum_{t=1}^{m-1}\\left [   \\binom{n}{m}\\right ]   ^{-2}\\binom{n}{2m - t}\\left [ \\binom{m}{t}\\right ]   ^{2}e_{\\theta}\\left [   \\widehat{if}_{m , m , i_{1}i_{2} .. i_{t}i_{t+1} .. i_{m}}^{\\left (   s\\right )   } \\widehat{if}_{m , m , i_{1}i_{2} .. i_{t}i_{m+1} .. i_{2m - t}}^{\\left (   s\\right )   } \\right ] \\\\ &   = o\\left (   \\sum_{t=1}^{m-1}n^{-t}e_{\\theta}\\left [   \\left (   e_{\\theta}\\left [ \\widehat{if}_{m , m,\\overline{i}_{m}}^{\\left (   s\\right )   } |\\mathbf{o}_{\\overline{i}_{t}}\\right ]   \\right )   ^{2}\\right ]   \\right ) \\\\ &   = o\\left (   \\sum_{t=1}^{m-1}n^{-t}\\left\\ {   e_{\\theta}\\left [   \\left ( \\widehat{if}_{m , m,\\overline{i}_{m}}|o_{s_{1}}, ... o_{s_{t}}\\right ) ^{2}\\right ]   e_{\\theta}\\left [   \\left (   \\widehat{if}_{m , m,\\overline{i}_{m}}|o_{v_{1}}, ...",
    "o_{v_{t}}\\right )   ^{2}\\right ]   \\right\\ }   ^{1/2}\\right ) \\\\ &   = o\\left (   \\max\\left (   \\frac{1}{n},\\frac{k^{m-2}}{n^{m-1}}\\right )   \\right)\\end{aligned}\\ ] ] the last two equalities follow from cauchy - shwartz inequality and lemma [ 3.19_lemma ] .",
    "specifically , @xmath3046   \\right )   ^{2}\\\\ &   = e_{\\theta}\\left [   e_{\\theta}\\left (   \\widehat{if}_{m , m,\\overline{i}_{m}^{\\ast}}|o_{i_{1}}, ...",
    "o_{i_{t}}\\right )   e_{\\theta}\\left (   \\widehat{if}_{m , m,\\overline{r}_{m}^{\\ast}}|o_{i_{1}}, ...",
    "o_{i_{t}}\\right )   \\right ] \\\\ &   \\leq\\left\\ {   e_{\\theta}\\left [   \\left (   e_{\\theta}\\left (   \\widehat{if}_{m , m,\\overline{i}_{m}^{\\ast}}|o_{i_{1}}, ...",
    "o_{i_{t}}\\right )   \\right ) ^{2}\\right ]   \\right\\ }   ^{1/2}\\\\ &   \\times\\left\\ {   e_{\\theta}\\left [   \\left (   e_{\\theta}\\left (   \\widehat{if}_{m , m,\\overline{r}_{m}^{\\ast}}|o_{i_{1}}, ...",
    "o_{i_{t}}\\right )   \\right ) ^{2}\\right ]   \\right\\ }   ^{1/2}.\\end{aligned}\\ ] ]      next , we prove eq .",
    "[ lemma_2 ] for any @xmath3050  here we also rewrite @xmath3051 as a sum of four terms . @xmath3052   -e_{\\theta}\\left [   \\widehat{if}_{m_{1},m_{1},\\overline{i}_{m}}^{\\left (   s\\right )   } \\right ]   e_{\\theta}\\left [ \\widehat{if}_{m_{2},m_{2},\\overline{i}_{m}}^{\\left (   s\\right )   } \\right ] \\\\ &   = e_{\\theta}\\left [   \\frac{1}{\\binom{n}{m_{1}}\\binom{n}{m_{2}}}\\sum _ { i_{1}<i_{2} .. <i_{m_{1}}}\\widehat{if}_{m_{1},m_{1},\\overline{i}_{m_{1}}}^{\\left (   s\\right )   } \\sum_{r_{1}<r_{2} .. <r_{m_{2}}}\\widehat{if}_{m_{2},m_{2},\\overline{r}_{m_{2}}}^{\\left (   s\\right )   } \\right ] \\\\ &   -e_{\\theta}\\left [   \\widehat{if}_{m_{1},m_{1},\\overline{i}_{m}}^{\\left ( s\\right )   } \\right ]   e_{\\theta}\\left [   \\widehat{if}_{m_{2},m_{2},\\overline { i}_{m}}^{\\left (   s\\right )   } \\right ] \\\\ &   = e_{\\theta}\\left [   \\frac{1}{\\binom{n}{m_{1}}\\binom{n}{m_{2}}}\\left\\ { \\begin{array } [ c]{c}\\left ( { \\displaystyle\\sum\\limits_{\\overline{i}_{m_{1}}\\subset\\overline{r}_{m_{2 } } } } + { \\displaystyle\\sum\\limits_{\\overline{i}_{m_{1}}\\cap\\overline{r}_{m_{2}}=\\emptyset } } + { \\displaystyle\\sum\\limits_{1\\leq\\#\\left (   \\overline{i}_{m_{1}}\\cap\\overline { r}_{m_{2}}\\right )   < m_{1 } } } \\right ) \\\\ \\times\\widehat{if}_{m_{1},m_{1},\\overline{i}_{m_{1}}}^{\\left (   s\\right ) } \\widehat{if}_{m_{2},m_{2},\\overline{r}_{m_{2}}}^{\\left (   s\\right )   } \\end{array } \\right\\ }   \\right ] \\\\ &   -e_{\\theta}\\left [   \\widehat{if}_{m_{1},m_{1},\\overline{i}_{m}}^{\\left ( s\\right )   } \\right ]   e_{\\theta}\\left [   \\widehat{if}_{m_{2},m_{2},\\overline { i}_{m}}^{\\left (   s\\right )   } \\right]\\end{aligned}\\ ] ] the first term : @xmath3053 \\\\ &   = \\frac{1}{\\binom{n}{m_{1}}\\binom{n}{m_{2}}}\\binom{n}{m_{2}}\\binom{m_{2}}{m_{1}}e_{\\theta}\\left [   \\widehat{if}_{m_{1},m_{1},\\overline{i}_{m_{1}}}^{\\left (   s\\right )   } \\widehat{if}_{m_{2},m_{2},\\overline{r}_{m_{2}}}^{\\left ( s\\right )   } \\right ] \\\\ &   = \\frac{\\binom{m_{2}}{m_{1}}}{\\binom{n}{m_{1}}}e_{\\theta}\\left [ \\widehat{if}_{m_{1},m_{1},\\overline{i}_{m_{1}}}^{\\left (   s\\right )   } e_{\\theta } \\left [   \\widehat{if}_{m_{2},m_{2},\\overline{r}_{m_{2}}}^{\\left (   s\\right ) } |\\mathbf{o}_{\\overline{i}_{m_{1}}}\\right ]   \\right ] \\\\ &   \\leq\\frac{\\binom{m_{2}}{m_{1}}}{\\binom{n}{m_{1}}}\\left\\ {   e_{\\theta}\\left [ \\left (   \\widehat{if}_{m_{1},m_{1},\\overline{i}_{m_{1}}}^{\\left (   s\\right ) } \\right )   ^{2}\\right ]   e_{\\theta}\\left [   \\left (   e_{\\theta}\\left [ \\widehat{if}_{m_{2},m_{2},\\overline{r}_{m_{2}}}^{\\left (   s\\right ) } |\\mathbf{o}_{\\overline{i}_{m_{1}}}\\right ]   \\right )   ^{2}\\right ]   \\right\\ } ^{1/2}\\\\ &   = o\\left (   \\frac{k^{m_{1}-1}}{n^{m_{1}}}\\right )   , \\end{aligned}\\ ] ] which follows from cauchy - shwartz inequality , theorem ( [ var_multi ] ) , and lemma [ 3.19_lemma ] .    the difference between the second and the fourth terms equals@xmath3054 \\\\ &   -e_{\\theta}\\left [   \\widehat{if}_{m_{1},m_{1},\\overline{i}_{m}}^{\\left ( s\\right )   } \\right ]   e_{\\theta}\\left [   \\widehat{if}_{m_{2},m_{2},\\overline { i}_{m}}^{\\left (   s\\right )   } \\right ] \\\\ &   = \\left (   \\frac{1}{\\binom{n}{m_{1}}\\binom{n}{m_{2}}}\\binom{n}{m_{1}}\\binom{n - m_{1}}{m_{2}}-1\\right )   e_{\\theta}\\left [   \\widehat{if}_{m_{1},m_{1},\\overline{i}_{m}}^{\\left (   s\\right )   } \\right ]   e_{\\theta}\\left [ \\widehat{if}_{m_{2},m_{2},\\overline{i}_{m}}^{\\left (   s\\right )   } \\right ] \\\\ &   = o\\left (   \\frac{1}{n}e_{\\theta}\\left [   \\widehat{if}_{m_{1},m_{1},\\overline{i}_{m}}^{\\left (   s\\right )   } \\right ]   e_{\\theta}\\left [ \\widehat{if}_{m_{2},m_{2},\\overline{i}_{m}}^{\\left (   s\\right )   } \\right ] \\right)\\end{aligned}\\ ] ] the third term:@xmath3055 \\\\ &   = { \\displaystyle\\sum\\limits_{t=1}^{m_{1}-1 } } \\frac{1}{\\binom{n}{m_{1}}\\binom{n}{m_{2}}}e_{\\theta}\\left [ { \\displaystyle\\sum\\limits_{\\#\\left (   \\overline{i}_{m_{1}}\\cap\\overline { r}_{m_{2}}\\right )   = t } } \\widehat{if}_{m_{1},m_{1},\\overline{i}_{m_{1}}}^{\\left (   s\\right ) } \\widehat{if}_{m_{2},m_{2},\\overline{r}_{m_{2}}}^{\\left (   s\\right )   } \\right ] \\\\ &   = { \\displaystyle\\sum\\limits_{t=1}^{m_{1}-1 } } \\left\\ { \\begin{array } [ c]{c}\\frac{\\binom{n}{m_{2}+m_{1}-t}\\binom{m_{2}+m_{1}-t}{m_{2}}\\binom{m_{2}}{t}}{\\binom{n}{m_{1}}\\binom{n}{m_{2}}}\\times\\\\ e_{\\theta}\\left [   \\widehat{if}_{m_{1},m_{1},i_{1}i_{2} .. i_{t}i_{t+1} .. i_{m_{1}}}^{\\left (   s\\right )   } \\widehat{if}_{m_{2},m_{2},i_{1} .. i_{t}i_{m_{1}+1} .. i_{m_{1}+m_{2}-t}}^{\\left (   s\\right )   } \\right ] \\end{array } \\right\\ } \\\\ &   = o\\left (   \\max\\left (   \\frac{1}{n},\\frac{k^{m_{1}-2}}{n^{m_{1}-1}}\\right ) \\right )   , \\end{aligned}\\ ] ] which also follows from cauchy - shwartz inequality and lemma [ 3.19_lemma ] .      for @xmath2211the estimation bias is given by@xmath3057",
    "e\\left [   q^{2}\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\overline{z}_{k}\\right ]   \\right\\ } \\\\ &   + \\left\\ { \\begin{array } [ c]{c}e\\left [   q^{2}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   \\overline{z}_{k}^{t}\\right ]   \\left\\ {   e\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\\\ \\times e\\left [   \\overline{z}_{k}q^{2}\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\right ] \\end{array } \\right\\}\\end{aligned}\\]]@xmath906@xmath2805{c}e\\left [   \\left (   h_{1}\\widehat{p}+h_{2}\\right )   \\dot{b}\\overline{z}_{k}^{t}\\right ]   \\left [   \\left\\ {   e\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}-i\\right ] \\\\",
    "\\times e\\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right )   \\dot { p}\\right ] \\end{array } \\right\\ } \\\\ &   = -\\left\\ { \\begin{array } [ c]{c}e\\left [   \\left (   h_{1}\\widehat{p}+h_{2}\\right )   \\dot{b}\\overline{z}_{k}^{t}\\right ]   \\left\\ {   e\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline { z}_{k}^{t}\\right ]   -i\\right\\ } \\\\",
    "\\times e\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   ^{-1}e\\left [   \\overline{z}_{k}\\left (   h_{1}\\widehat{b}+h_{3}\\right )   \\dot{p}\\right ] \\end{array } \\right\\}\\end{aligned}\\ ] ]    suppose the bias formula holds for @xmath135 , then the bias at @xmath286 is@xmath3058{c}e\\left [   q^{2}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   \\overline{z}_{k}^{t}\\right ]   \\left\\ {   e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   -i\\right\\ } \\\\ \\times{\\displaystyle\\prod\\limits_{s=3}^{m } } \\left\\ {   \\widehat{e}_{s}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left\\ {   e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   -\\widehat{e}_{s}\\left [ q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }",
    "\\\\ \\times\\left\\ {   e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] \\right\\ }   ^{-1}e\\left [   q^{2}\\overline{z}_{k}\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\right ] \\end{array } \\right\\ } \\\\ &   + \\left (   -1\\right )   ^{m}e\\left [   q^{2}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   \\overline{z}_{k}^{t}\\right ]   \\left (   e\\left [   \\left (   \\dot{p}\\dot { b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right )   _ { i_{2}}\\right ] -i\\right )   \\times\\\\ &   \\left [ { \\displaystyle\\prod\\limits_{s=3}^{m } } \\left\\ {   \\widehat{e}_{s}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left\\ {   e\\left [   \\left (   \\dot { p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right )   _ { i_{s}}\\right ] -\\widehat{e}_{s}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   \\right ] \\\\ &   \\times\\left\\ {   \\widehat{e}_{m+1}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}e\\left [   \\overline{z}_{k}q^{2}\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\right]\\end{aligned}\\]]@xmath906@xmath3059{c}e\\left [   q^{2}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   \\overline{z}_{k}^{t}\\right ]   \\left\\ {   e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   -i\\right\\ } \\\\",
    "\\times{\\displaystyle\\prod\\limits_{s=3}^{m } } \\left\\ {   \\widehat{e}_{s}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left\\ { \\begin{array } [ c]{c}e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] \\\\ -\\widehat{e}_{s}\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] \\end{array } \\right\\ } \\\\ \\times\\left [   \\left\\ {   \\widehat{e}_{m+1}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}-\\left\\ {   e\\left [ q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\right ] \\\\ \\times",
    "e\\left [   \\overline{z}_{k}q^{2}\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\right ] \\end{array } \\right\\ } \\\\ &   = \\left (   -1\\right )   ^{m}\\left\\ { \\begin{array } [ c]{c}e\\left [   q^{2}\\left (   \\frac{p-\\widehat{p}}{\\dot{p}}\\right )   \\overline{z}_{k}^{t}\\right ]   \\left\\ {   e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   -i\\right\\ } \\\\",
    "\\times{\\displaystyle\\prod\\limits_{s=3}^{m+1 } } \\left\\ {   \\widehat{e}_{s}\\left [   \\dot{p}\\dot{b}h_{1}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   \\right\\ }   ^{-1}\\left\\ { \\begin{array } [ c]{c}e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] \\\\ -\\widehat{e}_{s}\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] \\end{array } \\right\\ } \\\\",
    "\\times\\left\\ {   e\\left [   q^{2}\\overline{z}_{k}\\overline{z}_{k}^{t}\\right ] \\right\\ }   ^{-1}e\\left [   \\overline{z}_{k}q^{2}\\left (   \\frac{b-\\widehat{b}}{\\dot{b}}\\right )   \\right ] \\end{array } \\right\\}\\end{aligned}\\ ] ] which completes the proof .",
    "since @xmath3060 we can show that @xmath3061   e_{\\theta } \\left (   \\overline{z}_{k\\left (   m-1,0\\right )   } ^{k\\left (   m-1,1\\right ) } \\widehat{\\delta}\\right )   \\right ) \\\\ &   = \\left\\ {   \\widehat{e}\\left (   \\left (   \\delta g+1\\right )   \\delta b\\overline{z}_{k\\left (   1,0\\right )   } ^{k\\left (   1,1\\right )   t}\\right ) { \\textstyle\\prod\\limits_{u=2}^{m-1 } } \\widehat{e}\\left (   \\delta g\\widehat{q}^{2}\\overline{z}_{k\\left (   u-1,0\\right ) } ^{k\\left (   u-1,1\\right )   } \\overline{z}_{k\\left (   u,0\\right )   } ^{k\\left ( u,1\\right )   t}\\right )   \\widehat{e}\\left (   \\left (   \\delta g+1\\right )   \\delta p\\overline{z}_{k\\left (   m-1,0\\right )   } ^{k\\left (   m-1,1\\right )   } \\right ) \\right\\}\\end{aligned}\\ ] ] with @xmath1641 @xmath1642 @xmath1643 @xmath3062 and @xmath1645    below we give a useful representation of @xmath3063   .\\mathit{\\ \\ } $ ] let @xmath3064   \\widehat{e}\\left (   \\delta p\\overline { z}_{k\\left (   m-1,0\\right )   } ^{k\\left (   m-1,1\\right )   } \\right )   \\right\\ } \\\\ &   b_{m+1}^{bg}\\left (   \\widehat{\\mathbb{u}}_{m}\\left (   \\left (   l\\right ) _",
    "{ k\\left (   l,0\\right )   } ^{k\\left (   l,1\\right )   } , 1\\leq l\\leq m-1\\right ) \\right ) \\\\ &   \\equiv\\left\\ {   \\widehat{e}\\left (   \\delta g\\delta b\\overline{z}_{k\\left ( 1,0\\right )   } ^{k\\left (   1,1\\right )   t}\\right ) { \\textstyle\\prod\\limits_{u=2}^{m-1 } } \\widehat{e}\\left (   \\delta g\\widehat{q}^{2}\\overline{z}_{k\\left (   u-1,0\\right ) } ^{k\\left (   u-1,1\\right )   } \\overline{z}_{k\\left (   u,0\\right )   } ^{k\\left ( u,1\\right )   t}\\right )   \\widehat{e}\\left (   \\delta p\\overline{z}_{k\\left ( m-1,0\\right )   } ^{k\\left (   m-1,1\\right )   } \\right )   \\right\\}\\end{aligned}\\]]@xmath3065        [ cp]assume @xmath3067 is a @xmath3068 dimensional matrix with @xmath3069 @xmath3070 and @xmath3071 @xmath103for @xmath1939 @xmath3072 if @xmath3073{c}\\left ( \\begin{array } [ c]{c}b_{t}\\left (   \\widehat{\\mathbb{u}}_{t-2}\\left (   \\left (   l\\right ) _ { k_{1}\\left (   l,0\\right )   } ^{k_{1}\\left (   l,1\\right )   } , 1\\leq l\\leq t-3\\right )   \\right ) \\\\ -b_{t}^{bg}\\left (   \\widehat{\\mathbb{u}}_{t-1}\\left (   \\left (   l\\right ) _ { k_{1}\\left (   l,0\\right )   } ^{k_{1}\\left (   l,1\\right )   } , \\left (   t-2\\right ) _ { k_{1}\\left (   t-2,0\\right )   } ^{k_{1}\\left (   t-2,0\\right )   } , 1\\leq l\\leq t-3\\right )   \\right ) \\end{array } \\right ) \\\\ -\\left ( \\begin{array } [ c]{c}b_{t}^{pg}\\left (   \\widehat{\\mathbb{u}}_{t-1}\\left (   \\left (   1\\right ) _ { k_{1}\\left (   0,0\\right )   } ^{k_{1}\\left (   0,1\\right )   } , \\left (   l+1\\right ) _ { k_{1}\\left (   l,0\\right )   } ^{k_{1}\\left (   l,1\\right )   } , 1\\leq l\\leq t-3\\right )   \\right ) \\\\ -b_{t}\\left (   \\widehat{\\mathbb{u}}_{t}\\left (   \\left (   1\\right )   _ { k_{1}\\left ( 0,0\\right )   } ^{k_{1}\\left (   0,1\\right )   } , \\left (   l+1\\right )   _ { k_{1}\\left ( l,0\\right )   } ^{k_{1}\\left (   l,1\\right )   } , \\left (   t-1\\right )   _ { k_{1}\\left ( t-2,0\\right )   } ^{k_{1}\\left (   t-2,0\\right )   } , 1\\leq l\\leq t-3\\right )   \\right ) \\end{array } \\right ) \\end{array } \\right\\}\\end{gathered}\\ ] ]      this lemma explains how to use higher order u - statistics to estimate the @xmath781th order contribution of @xmath3075 with a residual bias not exceeding @xmath3076",
    "our estimator uses this idea to reduce fourth and higher order estimation bias to the optimal rate .",
    "( lemma [ cp])@xmath3077{c}\\widehat{e}\\left (   \\delta b\\delta g\\overline{z}_{k_{1}\\left (   1,0\\right ) } ^{k_{1}\\left (   1,1\\right )   t}\\right ) { \\textstyle\\prod\\limits_{u=2}^{t-3 } } \\widehat{e}\\left (   \\delta g\\widehat{q}^{2}\\overline{z}_{k_{1}\\left ( u-1,0\\right )   } ^{k_{1}\\left (   u-1,1\\right )   } \\overline{z}_{k_{1}\\left ( u,0\\right )   } ^{k_{1}\\left (   u,1\\right )   t}\\right )   \\times\\\\ \\widehat{e}\\left (   \\delta g\\widehat{q}\\overline{z}_{k_{1}\\left (   t-3,0\\right ) } ^{k_{1}\\left (   t-3,1\\right )   } \\left (   \\widehat{q}^{-1}\\delta p-\\widehat{\\pi } \\left (   \\widehat{q}^{-1}\\delta p|\\widehat{q}\\overline{z}_{k_{1}\\left ( t-2,0\\right )   } ^{k_{1}\\left (   t-2,1\\right )   } \\right )   \\right )   \\right ) \\end{array } \\right\\ } \\\\ &   -\\left\\ { \\begin{array } [ c]{c}\\widehat{e}\\left (   \\widehat{q}\\delta g\\overline{z}_{k_{1}\\left (   1,0\\right ) } ^{k_{1}\\left (   1,1\\right )   t}\\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{k_{1}\\left (   0,0\\right )   } ^{k_{1}\\left ( 0,1\\right )   } \\right )   \\right )   \\right ) { \\textstyle\\prod\\limits_{u=2}^{t-3 } } \\widehat{e}\\left (   \\delta g\\widehat{q}^{2}\\overline{z}_{k_{1}\\left ( u-1,0\\right )   } ^{k_{1}\\left (   u-1,1\\right )   } \\overline{z}_{k_{1}\\left ( u,0\\right )   } ^{k_{1}\\left (   u,1\\right )   t}\\right ) \\\\ \\times\\widehat{e}\\left (   \\delta g\\widehat{q}\\overline{z}_{k_{1}\\left ( t-3,0\\right )   } ^{k_{1}\\left (   t-3,1\\right )   } \\left (   \\widehat{q}^{-1}\\delta p-\\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline{z}_{k_{1}\\left (   t-2,0\\right )   } ^{k_{1}\\left (   t-2,1\\right ) } \\right )   \\right )   \\right )   \\right ) \\end{array } \\right\\ } \\\\ &   = \\left\\ { \\begin{array } [ c]{c}\\widehat{e}\\left (   \\widehat{q}\\delta g\\overline{z}_{k_{1}\\left (   1,0\\right ) } ^{k_{1}\\left (   1,1\\right )   t}\\widehat{\\pi}^{\\bot}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{k_{1}\\left (   0,0\\right ) } ^{k_{1}\\left (   0,1\\right )   } \\right )   \\right )   \\right ) { \\textstyle\\prod\\limits_{u=2}^{t-3 } } \\widehat{e}\\left (   \\delta g\\widehat{q}^{2}\\overline{z}_{k_{1}\\left ( u-1,0\\right )   } ^{k_{1}\\left (   u-1,1\\right )   } \\overline{z}_{k_{1}\\left ( u,0\\right )   } ^{k_{1}\\left (   u,1\\right )   t}\\right ) \\\\ \\times\\widehat{e}\\left (   \\delta g\\widehat{q}\\overline{z}_{k_{1}\\left ( t-3,0\\right )   } ^{k_{1}\\left (   t-3,1\\right )   } \\widehat{\\pi}^{\\bot}\\left ( \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline{z}_{k_{1}\\left ( t-2,0\\right )   } ^{k_{1}\\left (   t-2,1\\right )   } \\right )   \\right )   \\right ) \\end{array } \\right\\ } \\\\ &   = \\left\\ {   \\widehat{e}\\left (   \\delta g\\left [   \\left ( { \\textstyle\\prod\\limits_{u=1}^{t-3 } } r_{u}\\right )   \\left (   \\widehat{\\pi}^{\\bot}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{k_{1}\\left (   0,0\\right )   } ^{k_{1}\\left ( 0,1\\right )   } \\right )   \\right )   \\right )   \\right ]   \\widehat{\\pi}^{\\bot}\\left ( \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline{z}_{k_{1}\\left ( t-2,0\\right )   } ^{k_{1}\\left (   t-2,1\\right )   } \\right )   \\right )   \\right ) \\right\\}\\end{aligned}\\ ] ] where@xmath3078 @xmath3079   , $ ]",
    "@xmath3080 @xmath3081    since projection operator has operator norm of 1 , we have @xmath3082{c}\\left\\vert \\left\\vert \\delta g\\right\\vert \\right\\vert _ { \\infty}^{t-2}\\left\\ { \\widehat{e}\\left (   \\widehat{\\pi}^{\\bot}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{1}\\left (   0,1\\right )   } \\right )",
    "\\right )   \\right )   ^{2}\\right\\ }   ^{1/2}\\\\ \\times\\left\\ {   \\widehat{e}\\left (   \\widehat{\\pi}^{\\bot}\\left (   \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{1}\\left (   t-2,1\\right ) } \\right )   \\right )   \\right )   ^{2}\\right\\ }   ^{1/2}\\end{array } \\right ] \\\\ &   = o_{p}\\left (   \\left (   \\frac{\\log n}{n}\\right )   ^{-\\frac{\\left (   t-2\\right ) \\beta_{g}}{d+2\\beta_{g}}}\\left (   k_{1}\\left (   0,1\\right )   \\right ) ^{-\\frac{\\beta_{b}}{d}}\\left (   k_{1}\\left (   t-2,1\\right )   \\right ) ^{-\\frac{\\beta p}{d}}\\right)\\end{aligned}\\ ] ]          moreover@xmath3089 \\\\ &   = \\widehat{e}\\left (   \\delta g\\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{k\\left (   1,0\\right )   } ^{k\\left ( 1,1\\right )   } \\right )   \\right )   \\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline{z}_{k\\left (   2,0\\right )   } ^{k\\left ( 2,1\\right )   } \\right )   \\right )   \\right)\\end{aligned}\\ ] ]          ( theorem [ beyond4th ] ) let @xmath3098 for @xmath3099 and @xmath3100 . for @xmath1939",
    "@xmath3101 define @xmath3102{c}b_{t}\\left (   \\mathbb{g}\\left (   s , t-2\\right )   \\right )   -b_{t}^{bg}\\left ( \\mathbb{g}\\left (   s , t-1\\right )   \\right ) \\\\",
    "-b_{t}^{pg}\\left (   \\mathbb{g}\\left (   s , t-1\\right )   \\right )   + b_{t}\\left ( \\mathbb{g}\\left (   s , t\\right )   \\right ) \\end{array } \\right ) \\\\ b_{t}^{\\left (   q\\right )   }   &   = \\left (   -1\\right )   ^{t-1}\\left (   b_{t}\\left ( \\mathbb{q}_{t-2}\\right )   -b_{t}^{bg}\\left (   \\mathbb{q}_{t-1}\\right ) -b_{t}^{pg}\\left (   \\mathbb{q}_{t-1}\\right )   + b_{t}\\left (   \\mathbb{q}_{t}\\right )   \\right)\\end{aligned}\\ ] ] then @xmath3103 _ { -\\left (   l4.1\\right )   } \\\\ &   -b_{2}\\left (   \\mathbb{h}_{2}^{\\ast}\\right )   + b_{3}\\left (   \\mathbb{h}_{3}^{\\ast}-\\mathbb{h}_{2}^{\\ast\\ } \\right )   + { \\displaystyle\\sum\\limits_{s=1}^{j } } b_{3}\\left (   \\mathbb{g}\\left (   s,3\\right )   \\right )   + b_{3}\\left ( \\mathbb{q}_{3}\\right ) \\\\ &   + \\sum_{t=4}^{m\\left (   \\beta_{g},\\beta_{b},\\beta_{p}\\right )   + 2}\\left ( b_{t}^{\\left (   h^{\\ast}\\right )   } + b_{t}^{\\left (   g\\right )   } + b_{t}^{\\left ( q\\right )   } \\right)\\end{aligned}\\]]@xmath3104 \\\\ + \\widehat{e}\\left (   \\widehat{e}\\left (   h_{1}|x\\right )   \\left (   b-\\widehat{b}\\right )   \\left (   p-\\widehat{p}\\right )   \\right ) \\\\ = \\widehat{e}\\left (   \\delta g\\widehat{q}^{-2}\\delta b\\delta p\\right ) _ { -\\left (   l4.2\\right )   } + \\widehat{e}\\left (   \\widehat{q}^{-2}\\delta b\\delta p\\right )   _ { -\\left (   l4.3\\right )   } \\ ] ] @xmath3105 \\\\ &   = \\widehat{e}\\left [   \\widehat{\\pi}^{\\bot}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{-1}}\\right )   \\right )   \\widehat{\\pi } ^{\\bot}\\left (   \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{-1}}\\right )   \\right )   \\right]\\end{aligned}\\ ] ] then @xmath3106@xmath3107{c}\\widehat{e}\\left (   \\widehat{q}^{-2}\\delta b\\delta p\\delta",
    "g\\right ) -\\widehat{e}\\left (   \\delta b\\delta g\\overline{z}_{0}^{k_{-1}t}\\right ) \\widehat{e}\\left (   \\delta p\\overline{z}_{0}^{k_{-1}}\\right ) \\\\ -\\widehat{e}\\left (   \\delta b\\overline{z}_{0}^{k_{-1}t}\\right )   \\widehat{e}\\left (   \\delta p\\delta g\\overline{z}_{0}^{k_{-1}}\\right ) \\end{array } \\right\\ }   _ { -\\left (   l5.1\\right )   } \\\\ &   + \\widehat{e}\\left (   \\delta b\\overline{z}_{0}^{k_{0}t}\\right ) \\widehat{e}\\left (   \\delta g\\widehat{q}^{2}\\overline{z}_{0}^{k_{0}}\\overline { z}_{0}^{k_{0}t}\\right )   \\widehat{e}\\left [   \\delta p\\overline{z}_{0}^{k_{0}}\\right ] \\\\ &   + \\widehat{e}\\left (   \\delta b\\overline{z}_{k_{0}}^{k_{-1}t}\\right ) \\widehat{e}\\left (   \\delta g\\widehat{q}^{2}\\overline{z}_{k_{0}}^{k_{-1}}\\overline{z}_{0}^{k_{0}t}\\right )   \\widehat{e}\\left [   \\delta p\\overline{z}_{0}^{k_{0}}\\right ] \\\\ &   + \\widehat{e}\\left (   \\delta b\\overline{z}_{0}^{k_{0}t}\\right ) \\widehat{e}\\left (   \\delta g\\widehat{q}^{2}\\overline{z}_{0}^{k_{0}}\\overline { z}_{k_{0}}^{k_{-1}t}\\right )   \\widehat{e}\\left [   \\delta p\\overline{z}_{k_{0}}^{k_{-1}}\\right ] \\\\ &   + \\sum_{s=1}^{j}\\left\\ { \\begin{array } [ c]{c}\\widehat{e}\\left (   \\delta b\\overline{z}_{k_{2s-2}}^{k_{2s-1}t}\\right ) \\widehat{e}\\left (   \\delta g\\widehat{q}^{2}\\overline{z}_{k_{2s-2}}^{k_{2s-1}}\\overline{z}_{k_{2s-2}}^{k_{2s}t}\\right )   \\widehat{e}\\left [   \\delta p\\overline{z}_{k_{2s-2}}^{k_{2s}}\\right ] \\\\ + \\widehat{e}\\left (   \\delta b\\overline{z}_{k_{2s-2}}^{k_{2s}t}\\right ) \\widehat{e}\\left (   \\delta g\\widehat{q}^{2}\\overline{z}_{k_{2s-2}}^{k_{2s}}\\overline{z}_{k_{2s}}^{k_{2s-1}t}\\right )   \\widehat{e}\\left [   \\delta p\\overline{z}_{k_{2s}}^{k_{2s-1}}\\right ] \\end{array } \\right\\ } \\\\ &   + \\widehat{e}\\left (   \\delta b\\overline{z}_{k_{2j}}^{k_{2j+1}t}\\right ) \\widehat{e}\\left (   \\delta g\\widehat{q}^{2}\\overline{z}_{k_{2j}}^{k_{2j+1}}\\overline{z}_{k_{2j}}^{k_{2j+1}t}\\right )   \\widehat{e}\\left [   \\delta p\\overline{z}_{k_{2j}}^{k_{2j+1}}\\right]\\end{aligned}\\ ] ] because @xmath3108{c}\\delta g\\left\\ {   \\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta b|\\left ( \\widehat{q}\\overline{z}_{0}^{k_{-1}}\\right )   \\right )   + \\widehat{\\pi}^{\\bot } \\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{-1}}\\right )   \\right )   \\right\\ } \\\\",
    "\\times\\left\\ {   \\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta p|\\left ( \\widehat{q}\\overline{z}_{0}^{k_{-1}}\\right )   \\right )   + \\widehat{\\pi}^{\\bot } \\left (   \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{-1}}\\right )   \\right )   \\right\\ } \\end{array } \\right ) \\\\ &   -\\widehat{e}\\left (   \\delta g\\delta p\\widehat{q}^{-1}\\widehat{\\pi}\\left ( \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{-1}}\\right ) \\right )   \\right ) \\\\ &   -\\widehat{e}\\left (   \\delta g\\delta b\\widehat{q}^{-1}\\widehat{\\pi}\\left ( \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{-1}}\\right ) \\right )   \\right ) \\\\ &   = \\widehat{e}\\left (   \\delta g\\widehat{\\pi}^{\\bot}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{-1}}\\right )   \\right ) \\widehat{\\pi}^{\\bot}\\left (   \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{-1}}\\right )   \\right )   \\right ) \\\\ &   -\\widehat{e}\\left (   \\delta g\\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{-1}}\\right )   \\right )   \\widehat{\\pi } \\left (   \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{-1}}\\right )   \\right )   \\right)\\end{aligned}\\ ] ] and by lemma [ on2 ] , we now have @xmath3109{c}\\widehat{e}\\left [   \\delta g\\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{k_{2s}}^{k_{2s-1}}\\right )   \\right ) \\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline { z}_{k_{2s+1}}^{k_{2s-1}}\\right )   \\right )   \\right ]   _ { -\\left (   l5.3\\right ) } \\\\ + \\widehat{e}\\left [   \\delta g\\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{k_{2s+1}}^{k_{2s-1}}\\right )   \\right ) \\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline { z}_{k_{2s}}^{k_{2s+1}}\\right )   \\right )   \\right ]   _ { -\\left (   l5.4\\right )   } \\end{array}\\end{aligned}\\ ] ] therefore it is easy to show from theorem [ tbrate ] that @xmath3110    @xmath1939 @xmath3111@xmath3112{c}b_{t}\\left (   \\widehat{\\mathbb{u}}_{t-2}\\left (   _ { 0}^{k_{0}}\\right )   \\right ) -b_{t}^{bg}\\left (   \\widehat{\\mathbb{u}}_{t-1}\\left (   _ { 0}^{k_{0}}\\right ) \\right ) \\\\ -\\left [   b_{t}^{pg}\\left (   \\widehat{\\mathbb{u}}_{t-1}\\left (   _ { 0}^{k_{0}}\\right )   \\right )   -b_{t}\\left (   \\widehat{\\mathbb{u}}_{t}\\left (   _ { 0}^{k_{0}}\\right )   \\right )   \\right ] \\end{array } \\right\\ }   _ { -\\left (   l6.1\\right )   } \\\\ &   + { \\displaystyle\\sum\\limits_{u=1}^{t-3 } } \\left\\ { \\begin{array } [ c]{c}\\left [   b_{t}\\left (   \\widehat{\\mathbb{u}}_{t-2}^{\\left (   u\\right )   } \\left ( _ { k_{0}}^{k_{-1}},_{0}^{k_{0}}\\right )   \\right )   -b_{t}^{bg}\\left ( \\widehat{\\mathbb{u}}_{t-1}^{\\left (   u\\right )   } \\left (   _ { k_{0}}^{k_{-1}},_{0}^{k_{0}}\\right )   \\right )   \\right ] \\\\ -\\left [   b_{t}^{pg}\\left (   \\widehat{\\mathbb{u}}_{t-1}^{\\left (   u+1\\right ) } \\left (   _ { k_{0}}^{k_{-1}},_{0}^{k_{0}}\\right )   \\right )   -b_{t}\\left ( \\widehat{\\mathbb{u}}_{t}^{\\left (   u+1\\right )   } \\left (   _ { k_{0}}^{k_{-1}},_{0}^{k_{0}}\\right )   \\right )   \\right ] \\end{array } \\right\\ }   _ { -\\left (   l6.2.u\\right )   } \\\\ &   -b_{t}^{pg}\\left (   \\widehat{\\mathbb{u}}_{t-1}^{\\left (   1\\right )   } \\left ( _ { k_{0}}^{k_{-1}},_{0}^{k_{0}}\\right )   \\right )   -b_{t}\\left ( \\widehat{\\mathbb{u}}_{t}^{\\left (   1\\right )   } \\left (   _ { k_{0}}^{k_{-1}},_{0}^{k_{0}}\\right )   \\right )   _ { -\\left (   l6.3\\right )   } \\\\ &   -b_{t}^{bg}\\left (   \\widehat{\\mathbb{u}}_{t-1}^{\\left (   t-2\\right )   } \\left ( _ { k_{0}}^{k_{-1}},_{0}^{k_{0}}\\right )   \\right )   -b_{t}\\left ( \\widehat{\\mathbb{u}}_{t}^{\\left (   t-1\\right )   } \\left (   _ { k_{0}}^{k_{-1}},_{0}^{k_{0}}\\right )   \\right )   _ { -\\left (   l6.4\\right )   } \\ ] ] it can be shown that @xmath3113 with @xmath3114 @xmath3115 @xmath3116 @xmath3117 with @xmath3118 @xmath3119 for @xmath1828 and @xmath3120 @xmath3121 and @xmath3122{c}\\widehat{e}\\left (   \\delta g\\widehat{q}\\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{k_{0}}^{k_{-1}}\\right ) \\right )   \\overline{z}_{0}^{k_{0}t}\\right )   \\left (   \\widehat{e}\\left (   \\delta g\\widehat{q}^{2}\\overline{z}_{0}^{k_{0}}\\overline{z}_{0}^{k_{0}t}\\right ) \\right )   ^{t-4}\\\\ \\times e\\left (   \\delta g\\widehat{q}\\overline{z}_{0}^{k_{0}}\\widehat{\\pi}^{\\bot}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{0}}\\right )   \\right )   \\right ) \\end{array } \\right\\ }   \\right\\vert \\\\ &   = o_{p}\\left (   \\left (   \\frac{\\log n}{n}\\right )   ^{-\\frac{\\left (   t-2\\right ) \\beta_{g}}{d+2\\beta_{g}}}k_{0}^{-2\\beta / d}\\right)\\end{aligned}\\ ] ] similarly @xmath3123 from lemma [ cp ] , we now have@xmath3124 therefore @xmath3125 @xmath1939 @xmath3126 following the same argument as above , we know as well @xmath3127 @xmath1939 @xmath3128@xmath3129 from lemma [ on2 ] @xmath3130\\end{aligned}\\]]@xmath3131 \\\\ &   = \\widehat{e}\\left (   \\widehat{\\pi}^{\\bot}\\left (   \\widehat{q}^{-1}\\delta b|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{0}}\\right )   \\right )   \\delta g\\overline{z}_{k_{2s-2}}^{k_{2s-1}t}\\right )   \\widehat{e}\\left (   \\delta g\\widehat{q}\\overline{z}_{k_{2s-2}}^{k_{2s-1}}\\widehat{\\pi}\\left ( \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline{z}_{k_{2s-2}}^{k_{2s}}\\right )   \\right )   \\right ) \\\\ &   + \\widehat{e}\\left (   \\widehat{\\pi}\\left (   \\widehat{q}^{-1}\\delta b|\\left ( \\widehat{q}\\overline{z}_{k_{2s-2}}^{k_{2s-1}}\\right )   \\right )   \\delta g\\overline{z}_{k_{2s-2}}^{k_{2s}t}\\right )   \\widehat{e}\\left (   \\delta g\\widehat{q}\\overline{z}_{k_{2s-2}}^{k_{2s}}\\widehat{\\pi}^{\\bot}\\left ( \\widehat{q}^{-1}\\delta p|\\left (   \\widehat{q}\\overline{z}_{0}^{k_{0}}\\right ) \\right )   \\right)\\end{aligned}\\ ] ] similarly,@xmath3132 therefore@xmath3133 applying the above argument to @xmath3134 we can show that : @xmath3135 in addition , @xmath1939 @xmath3136 @xmath3137 @xmath3138 which completes the proof of bias",
    ". the order of variance follows directly from theorem [ var_multi ] .",
    "( theorem [ tt ] ( iii ) ) as proved in ( ii ) , @xmath70@xmath3139 by part 5c ) of theorem [ eift ] , consider any suitably smooth one dimensional parametric submodel @xmath243 with range containing @xmath179 and contained in @xmath3140 , and differentiate both sides of eq .",
    "( [ if1_tau ] ) wrt .",
    "@xmath2752 then , @xmath3141 \\\\ &   -e_{\\theta}\\left [   if_{1,if_{1,\\psi}\\left (   o;\\cdot\\right )   } \\left ( o_{2};\\theta\\right )   s_{\\zeta}\\left (   \\theta\\right )   \\right ]   .\\end{aligned}\\ ] ]    further , @xmath3142   $ ] and @xmath3143   } { \\partial\\tau}|_{\\tau=\\tau^{\\ast}}\\\\ &   = e_{\\theta}\\left [   \\frac{\\partial if_{1,\\psi\\left (   \\tau^{\\dagger},\\cdot\\right )   } \\left (   o_{2};\\theta\\right )   } { \\partial\\tau}s_{\\zeta}\\left ( \\theta\\right )   \\right]\\end{aligned}\\ ] ]    thus , @xmath3144 \\\\ &   -e_{\\theta}\\left [   if_{1,if_{1,\\psi}\\left (   o;\\cdot\\right )   } \\left ( o_{2},\\theta\\right )   s_{\\zeta}\\left (   \\theta\\right )   \\right ] \\\\ &   -if_{1,\\tau\\left (   \\cdot\\right )   } \\left (   o;\\theta\\right )   \\left ( \\begin{array } [ c]{c}\\psi_{\\tau\\tau}e_{\\theta}\\left [   if_{1,\\tau\\left (   \\cdot\\right )   } \\left ( o_{2},\\theta\\right )   s_{\\zeta}\\left (   \\theta\\right )   \\right ]",
    "\\\\ + e_{\\theta}\\left [   \\frac{\\partial if_{1,\\psi\\left (   \\tau^{\\dagger},\\cdot\\right )   } \\left (   o_{2};\\theta\\right )   } { \\partial\\tau}s_{\\zeta}\\left ( \\theta\\right )   \\right ] \\end{array } \\right)\\end{aligned}\\ ] ] for any @xmath3145  wp",
    "that is , there exists a first order influence function for @xmath3146 , and @xmath3147   \\right\\ } \\\\ &   = -\\psi_{\\tau}^{-1}\\left\\ { \\begin{array } [ c]{c}\\mathbb{if}_{2,2,\\tau\\left (   \\cdot\\right )   } \\left (   \\theta\\right )   + \\frac { 1}{2}\\psi_{\\tau\\tau}\\mathbb{v}\\left (   if_{1,\\tau\\left (   \\cdot\\right ) } \\left (   o_{1};\\theta\\right )   if_{1,\\tau\\left (   \\cdot\\right )   } \\left ( o_{2},\\theta\\right )   \\right ) \\\\",
    "+ \\frac{1}{2}\\mathbb{v}\\left ( \\begin{array } [ c]{c}if_{1,\\tau\\left (   \\cdot\\right )   } \\left (   o_{1};\\theta\\right )   d_{1,\\theta } \\left (   \\frac{\\partial if_{1,\\psi\\left (   \\tau^{\\dagger},\\cdot\\right ) } \\left (   o_{2};\\theta\\right )   } { \\partial\\tau}\\right ) \\\\ + d_{1,\\theta}\\left (   \\frac{\\partial if_{1,\\psi\\left (   \\tau^{\\dagger},\\cdot\\right )   } \\left (   o_{1};\\theta\\right )   } { \\partial\\tau}\\right ) if_{1,\\tau\\left (   \\cdot\\right )   } \\left (   o_{2};\\theta\\right ) \\end{array } \\right ) \\end{array } \\right\\}\\end{aligned}\\ ] ] which completes the proof .",
    "note that @xmath3148 is defined in eq .",
    "( [ deg ] ) .",
    "( ii ) : to obtain eq ( [ joel ] ) , note by theorem [ gg ] , we have @xmath3155   ^{-1}\\times\\\\ &   \\pi_{\\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   } \\left [   \\mathbb{if}_{2,\\widetilde{\\psi}_{k}\\left (   \\tau^{\\dagger},\\cdot\\right )   } \\left ( \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   \\right )   |\\gamma_{2}^{test}\\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   , \\tau^{\\dagger } \\right )   \\right]\\end{aligned}\\ ] ] but by theorem [ tt ] and the definition of @xmath3156 we have @xmath3157 ^{-1}\\\\ \\times\\left\\ {   y^{\\ast}\\left (   \\tau^{\\dagger}\\right )   -\\widehat{b}\\left ( x,\\tau^{\\dagger}\\right )   \\right\\ }   \\left\\ {   a-\\widehat{p}\\left (   x\\right ) \\right\\}\\end{gathered}\\ ] ] thus , we obtain @xmath3158 ^{-1}=v\\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   \\right )   ^{-1}$ ]    now @xmath3159 \\\\ &   = \\mathbb{if}_{2,\\widetilde{\\psi}_{k}\\left (   \\tau^{\\dagger},\\cdot\\right ) } \\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   \\right ) -\\pi_{\\widehat{\\theta}}\\left [   \\mathbb{if}_{2,\\widetilde{\\psi}_{k}\\left ( \\tau^{\\dagger},\\cdot\\right )   } \\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger } \\right )   \\right )   |\\left\\ {   \\mathbb{u}_{2,2,\\widetilde{\\tau}_{k}\\left ( \\cdot\\right )   } ^{test,\\perp}\\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger } \\right )   , \\tau^{\\dagger}\\right )   \\right\\ }   \\right]\\end{aligned}\\ ] ] let @xmath2293 denote @xmath2294 and @xmath2295 denote @xmath2296  next , we show that @xmath3160   = \\mathbb{u}_{2,2,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right )   } ^{\\ast , test,\\perp}\\left ( \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   , \\tau^{\\dagger}\\right)\\ ] ] where@xmath2298   \\right )   ^{-1}\\times\\widehat{\\epsilon}_{i}\\widehat{\\delta}_{i}\\\\ &   \\left\\ { \\begin{array } [ c]{c}-\\left\\ { \\begin{array } [ c]{c}\\left (   e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta } _ { i}^{2}\\right ]   \\right )   ^{-1}e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon } \\widehat{\\delta}^{2}\\overline{z}_{k}^{t}\\right ] \\\\",
    "\\times e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}^{2}\\widehat{\\delta } \\overline{z}_{k}^{t}\\right ]   \\widehat{\\epsilon}_{j}\\widehat{\\delta}_{j}\\end{array } \\right\\ } \\\\ + e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta}_{i}\\overline{z}_{k , i}^{t}\\right ]   \\overline{z}_{k , j}\\widehat{\\delta}_{j}\\\\ + e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}\\widehat{\\delta}_{i}^{2}\\overline{z}_{k , i}^{t}\\right ]   \\overline{z}_{k , j}\\widehat{\\epsilon}_{j}\\end{array } \\right\\}\\end{aligned}\\ ] ] as proved in theorem [ ff],@xmath3161   = 0\\right\\}\\ ] ]    we assume that @xmath3162 \\\\ &   = \\mathbb{v}\\left\\ {   if_{1,\\tau\\left (   \\cdot\\right )   , i}^{eff}\\left ( \\theta\\right )   h^{\\ast}\\left (   o_{j};\\theta\\right )   \\right\\ }   , \\end{aligned}\\ ] ] then by the definition of the projection , for any @xmath3163 such that @xmath3164   = 0,$ ] we have @xmath3165 \\\\ &   = e_{\\widehat{\\theta}}\\left [   \\mathbb{v}\\left\\ {   if_{1,\\tau\\left ( \\cdot\\right )   , i}^{eff}\\left (   \\theta\\right )   h^{\\ast}\\left (   o_{j};\\theta\\right )   \\right\\ }   \\mathbb{v}\\left\\ {   if_{1,\\tau\\left (   \\cdot\\right ) , i}^{eff}\\left (   \\theta\\right )   h\\left (   o_{j};\\theta\\right )   \\right\\ } \\right ]   , \\end{aligned}\\ ] ] which is equivalent to @xmath3166 + e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}\\widehat{\\delta}_{i}^{2}\\overline{z}_{k , i}^{t}\\overline{z}_{k , j}\\widehat{\\epsilon}_{j}h\\left ( o_{j}\\right )   \\right ]   \\right\\ } \\\\ &   = v\\left (   \\widehat{\\theta}\\right )   ^{-2}\\left\\ {   e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta}_{i}^{2}h^{\\ast}\\left ( o_{j}\\right )   h\\left (   o_{j}\\right )   \\right ]   + e_{\\widehat{\\theta}}\\left [ \\widehat{\\epsilon}_{i}\\widehat{\\delta}_{i}h\\left (   o_{i}\\right ) \\widehat{\\epsilon}_{j}\\widehat{\\delta}_{j}h^{\\ast}\\left (   o_{j}\\right ) \\right ]   \\right\\ }   .\\end{aligned}\\ ] ]    as the equation above holds for any mean zero function @xmath3167 therefore@xmath3168   h^{\\ast}\\left (   o\\right )   + \\widehat{\\epsilon } \\widehat{\\delta}e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{j}\\widehat{\\delta}_{j}h^{\\ast}\\left (   o_{j}\\right )   \\right ]   \\right\\ } \\\\ &   = v\\left (   \\widehat{\\theta}\\right )   \\left\\ {   e_{\\widehat{\\theta}}\\left [ \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta}_{i}\\overline{z}_{k , i}^{t}\\right ] \\overline{z}_{k}\\widehat{\\delta}+e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon } _ { i}\\widehat{\\delta}_{i}^{2}\\overline{z}_{k , i}^{t}\\right ]   \\overline{z}_{k}\\widehat{\\epsilon}\\right\\}\\end{aligned}\\]]@xmath3169@xmath3170   \\right )   ^{-1}\\left\\ { \\begin{array } [ c]{c}c_{h}\\left (   \\widehat{\\theta}\\right )   \\widehat{\\epsilon}\\widehat{\\delta}\\\\ + v\\left (   \\widehat{\\theta}\\right )   e_{\\widehat{\\theta}}\\left [ \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta}_{i}\\overline{z}_{k , i}^{t}\\right ] \\overline{z}_{k}\\widehat{\\delta}\\\\ + v\\left (   \\widehat{\\theta}\\right )   e_{\\widehat{\\theta}}\\left [ \\widehat{\\epsilon}_{i}\\widehat{\\delta}_{i}^{2}\\overline{z}_{k , i}^{t}\\right ] \\overline{z}_{k}\\widehat{\\epsilon}\\end{array } \\right\\}\\end{aligned}\\ ] ] and @xmath3171 is determined by the following equation @xmath3172 \\overline{z}_{k}\\widehat{\\delta}\\\\ &   + v\\left (   \\widehat{\\theta}\\right )   e_{\\widehat{\\theta}}\\left [ \\widehat{\\epsilon}_{i}\\widehat{\\delta}_{i}^{2}\\overline{z}_{k , i}^{t}\\right ] \\overline{z}_{k}\\widehat{\\epsilon}\\\\ &   + \\widehat{\\epsilon}\\widehat{\\delta}e_{\\widehat{\\theta}}\\left [ \\widehat{\\epsilon}\\widehat{\\delta}\\left (   e_{\\widehat{\\theta}}\\left [ \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta}_{i}^{2}\\right ]   \\right ) ^{-1}\\left\\ { \\begin{array } [ c]{c}c_{h}\\left (   \\widehat{\\theta}\\right )   \\widehat{\\epsilon}\\widehat{\\delta}\\\\ + v\\left (   \\widehat{\\theta}\\right )   e_{\\widehat{\\theta}}\\left [ \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta}_{i}\\overline{z}_{k , i}^{t}\\right ] \\overline{z}_{k}\\widehat{\\delta}\\\\ + v\\left (   \\widehat{\\theta}\\right )   e_{\\widehat{\\theta}}\\left [ \\widehat{\\epsilon}_{i}\\widehat{\\delta}_{i}^{2}\\overline{z}_{k , i}^{t}\\right ] \\overline{z}_{k}\\widehat{\\epsilon}\\end{array } \\right\\ }   \\right ] \\\\ &   = v\\left (   \\widehat{\\theta}\\right )   \\left\\ {   e_{\\widehat{\\theta}}\\left [ \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta}_{i}\\overline{z}_{k , i}^{t}\\right ] \\overline{z}_{k}\\widehat{\\delta}+e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon } _ { i}\\widehat{\\delta}_{i}^{2}\\overline{z}_{k , i}^{t}\\right ]   \\overline{z}_{k}\\widehat{\\epsilon}\\right\\}\\end{aligned}\\]]@xmath3169@xmath3173{c}c_{h}\\left (   \\widehat{\\theta}\\right )   + \\left (   e_{\\widehat{\\theta}}\\left [ \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta}_{i}^{2}\\right ]   \\right ) ^{-1}\\times\\\\ \\left\\ { \\begin{array } [ c]{c}e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}^{2}\\widehat{\\delta}^{2}\\right ] c_{h}\\left (   \\widehat{\\theta}\\right ) \\\\ + 2v\\left (   \\widehat{\\theta}\\right )",
    "e_{\\widehat{\\theta}}\\left [ \\widehat{\\epsilon}\\widehat{\\delta}^{2}\\overline{z}_{k}^{t}\\right ] e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}^{2}\\widehat{\\delta}\\overline { z}_{k}^{t}\\right ] \\end{array } \\right\\ } \\end{array } \\right ]   = 0\\]]@xmath3169@xmath3174   \\right ) ^{-1}e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}\\widehat{\\delta}^{2}\\overline{z}_{k}^{t}\\right ]   e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon } ^{2}\\widehat{\\delta}\\overline{z}_{k}^{t}\\right]\\end{aligned}\\ ] ]    in summary,@xmath3175   \\right )   ^{-1}\\times\\\\ &   \\mathbb{v}\\left\\ { \\begin{array } [ c]{c}\\widehat{\\epsilon}_{i}\\widehat{\\delta}_{i}\\times\\\\ \\left\\ { \\begin{array } [ c]{c}-\\left\\ { \\begin{array } [ c]{c}\\left (   e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta } _ { i}^{2}\\right ]   \\right )   ^{-1}e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon } \\widehat{\\delta}^{2}\\overline{z}_{k}^{t}\\right ] \\\\",
    "\\times e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}^{2}\\widehat{\\delta } \\overline{z}_{k}^{t}\\right ]   \\widehat{\\epsilon}_{j}\\widehat{\\delta}_{j}\\end{array } \\right\\ } \\\\ + e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta}_{i}\\overline{z}_{k , i}^{t}\\right ]   \\overline{z}_{k , j}\\widehat{\\delta}_{j}\\\\ + e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}\\widehat{\\delta}_{i}^{2}\\overline{z}_{k , i}^{t}\\right ]   \\overline{z}_{k , j}\\widehat{\\epsilon}_{j}\\end{array } \\right\\ } \\end{array } \\right\\}\\end{aligned}\\ ] ]    to obtain @xmath3176 we divide eq . (",
    "[ joel ] ) by @xmath3177 . to obtain @xmath3178",
    ", we take the variance of both sides of eq .",
    "( [ joel ] ) under law @xmath2271 giving @xmath3179   \\right ] \\\\ &   = v\\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   \\right ) ^{-2}\\left\\ {   var_{\\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   } \\left [ \\mathbb{if}_{2,\\widetilde{\\psi}_{k}\\left (   \\tau^{\\dagger},\\cdot\\right ) } \\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right )   \\right )   \\right ] -var\\left [   \\mathbb{u}_{2,2,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right )   } ^{\\ast , test,\\perp}\\left (   \\widehat{\\theta}\\left (   \\tau^{\\dagger}\\right ) , \\tau^{\\dagger}\\right )   \\right ]   \\right\\}\\end{aligned}\\ ] ]      parts ( i ) and ( ii ) :  that @xmath3180   = o_{p}\\left ( 1/n\\right )   \\ \\ $ ] and @xmath2320 = o_{p}\\left (   1/n\\right )   \\ $ ] is a straightforward calculation .",
    "the remainder of @xmath1880 and @xmath1883 follows from the fact that @xmath3181   \\asymp\\max\\left (   \\frac{1}{n},\\frac{k}{n^{2}}\\right )   .$ ]    part ( iv ) : by part ( ii ) of theorem [ aa ] , it is sufficient to show that @xmath3182 \\\\ &   = o_{p}\\left\\ {   \\left (   p-\\widehat{p}\\right )   \\left (   b\\left (   \\tau ^{\\dagger}\\right )   -\\widehat{b}\\left (   \\tau^{\\dagger}\\right )   \\right )   \\left [ \\left (   p-\\widehat{p}\\right )   + \\left (   b\\left (   \\tau^{\\dagger}\\right ) -\\widehat{b}\\left (   \\tau^{\\dagger}\\right )",
    "\\right )   \\right ]   \\right\\}\\end{aligned}\\ ] ] below we show @xmath3183 \\\\ &   = \\left (   e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta}_{i}^{2}\\right ]   \\right )   ^{-1}\\times\\\\ &   \\left\\ { \\begin{array } [ c]{c}e_{\\theta}\\left [   \\left (   b\\left (   \\tau^{\\dagger}\\right )   -\\widehat{b}\\left ( \\tau^{\\dagger}\\right )   \\right )   \\left (   p-\\widehat{p}\\right )   \\right ] \\times\\\\ \\left\\ { \\begin{array } [ c]{c}e_{\\theta}\\left [   \\left (   p-\\widehat{p}\\right )   \\overline{z}_{k}^{t}\\right ] e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta}_{i}\\overline{z}_{k , i}^{t}\\right ] \\\\ + e_{\\theta}\\left [   \\left (   b\\left (   \\tau^{\\dagger}\\right )   -\\widehat{b}\\left ( \\tau^{\\dagger}\\right )   \\right )   \\overline{z}_{k}^{t}\\right ] e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}\\widehat{\\delta}_{i}^{2}\\overline{z}_{k , i}^{t}\\right ]",
    "\\\\ -\\left\\ { \\begin{array } [ c]{c}\\left (   e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}_{i}^{2}\\widehat{\\delta } _ { i}^{2}\\right ]   \\right )   ^{-1}e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon } \\widehat{\\delta}^{2}\\overline{z}_{k}^{t}\\right ] \\\\",
    "\\times e_{\\widehat{\\theta}}\\left [   \\widehat{\\epsilon}^{2}\\widehat{\\delta } \\overline{z}_{k}^{t}\\right ]   \\times\\\\ e_{\\theta}\\left [   \\left (   b\\left (   \\tau^{\\dagger}\\right )   -\\widehat{b}\\left ( \\tau^{\\dagger}\\right )   \\right )   \\left (   p-\\widehat{p}\\right )   \\right ] \\end{array } \\right\\ } \\end{array } \\right\\ } \\end{array } \\right\\}\\end{aligned}\\ ] ] which is @xmath3184   \\right\\}\\ ] ]",
    "when , as is the case under our assumptions @xmath3185   $ ] and @xmath3185   $ ] are both order @xmath3186 but would be @xmath3187   \\right\\}\\ ] ] in the ( unlikely ) special case in which the semiparametric regression model was precisely true , since then @xmath3188 /\\left\\ {   e_{\\widehat{\\theta}}\\left [   y\\left (   \\tau^{\\dagger}\\right )    1\\right)\\ ] ] so @xmath3189   = o_{p}\\left ( p-\\widehat{p}\\right)\\ ] ] and @xmath3190   = o_{p}\\left ( b\\left (   \\tau^{\\dagger}\\right )   -\\widehat{b}\\right )   .\\ ] ] the expression for @xmath3191   $ ] is obtained from the formula for @xmath3192 in theorem [ aa ] by noting that , because @xmath3193 = \\psi_{1,k}\\left (   \\tau^{\\dagger},\\widehat{\\theta}\\left (   \\tau^{\\dagger } \\right )   \\right )   $ ] and @xmath3194 \\\\ = e_{\\theta}\\left [   \\psi_{1,k}\\left (   \\tau^{\\dagger},\\widehat{\\theta}\\left ( \\tau^{\\dagger}\\right )   \\right )   -\\widetilde{\\psi}_{k}\\left (   \\tau^{\\dagger } , \\theta\\right )   \\right ] \\\\ = e_{\\theta}\\left [   \\left (   p-\\widehat{p}\\right )   \\overline{z}_{k}^{t}\\right ] e_{\\theta}\\left [   \\overline{z}_{k}\\overline{z}_{k}^{t}\\right ]   e_{\\theta } \\left [   \\left (   b\\left (   \\tau^{\\dagger}\\right )   -\\widehat{b}\\left ( \\tau^{\\dagger}\\right )   \\right )   \\overline{z}_{k}\\right]\\end{gathered}\\ ] ] by theorem [ ebrate ] .    part ( v ) : we first note that by theorem [ eiet ] , @xmath3195   } { e_{\\theta}\\left [   \\mathbb{if}_{3,3,\\widetilde{\\tau } _ { k}\\left (   \\cdot\\right )   } \\left (   \\widehat{\\theta}\\right )   \\right ] } = -\\left (   1+o_{p}\\left (   1\\right )   \\right )   .$ ] it can be shown that @xmath3196{c}if_{3,3,\\psi\\left (   \\tau^{\\dag},\\cdot\\right )   , i_{1}i_{2}i_{3}}\\left ( \\theta\\right )   + \\frac{1}{6}\\psi_{\\backslash\\tau^{3}}if_{1,\\tau\\left ( \\cdot\\right )   , i_{1}}\\left (   \\theta\\right )   if_{1,\\tau\\left (   \\cdot\\right ) , i_{2}}\\left (   \\theta\\right )   if_{1,\\tau\\left (   \\cdot\\right )   , i_{3}}\\left ( \\theta\\right ) \\\\ + \\frac{1}{3}\\psi_{\\backslash\\tau^{2}}\\left ( \\begin{array } [ c]{c}if_{1,\\tau\\left (   \\cdot\\right )   , i_{1}}\\left (   \\theta\\right )   if_{2,2,\\tau \\left (   \\cdot\\right )   , i_{2}i_{3}}\\left (   \\theta\\right ) \\\\ + if_{1,\\tau\\left (   \\cdot\\right )   , i_{2}}\\left (   \\theta\\right )   if_{2,2,\\tau \\left (   \\cdot\\right )   , i_{1}i_{3}}\\left (   \\theta\\right ) \\\\ + if_{1,\\tau\\left (   \\cdot\\right )   , i_{3}}\\left (   \\theta\\right )   if_{2,2,\\tau \\left (   \\cdot\\right )   , i_{1}i_{2}}\\left (   \\theta\\right ) \\end{array } \\right ) \\\\ + \\frac{1}{3}\\left ( \\begin{array } [ c]{c}d_{1,\\theta}\\left (   \\frac{\\partial if_{1,\\psi\\left (   \\tau^{\\dag},\\cdot\\right ) , i_{1}}\\left (   \\theta\\right )   } { \\partial\\tau}\\right )   if_{2,2,\\tau\\left ( \\cdot\\right )   , i_{2}i_{3}}\\left (   \\theta\\right ) \\\\ + d_{1,\\theta}\\left (   \\frac{\\partial if_{1,\\psi\\left (   \\tau^{\\dag},\\cdot\\right )   , i_{2}}\\left (   \\theta\\right )   } { \\partial\\tau}\\right ) if_{2,2,\\tau\\left (   \\cdot\\right )   , i_{1}i_{3}}\\left (   \\theta\\right ) \\\\ + d_{1,\\theta}\\left (   \\frac{\\partial if_{1,\\psi\\left (   \\tau^{\\dag},\\cdot\\right )   , i_{3}}\\left (   \\theta\\right )   } { \\partial\\tau}\\right ) if_{2,2,\\tau\\left (   \\cdot\\right )   , i_{1}i_{2}}\\left (   \\theta\\right ) \\end{array } \\right ) \\\\ + \\frac{1}{3}\\left ( \\begin{array } [ c]{c}if_{1,\\tau\\left (   \\cdot\\right )   , i_{1}}\\left (   \\theta\\right )   d_{2,\\theta } \\left (   \\frac{\\partial if_{2,2,\\psi\\left (   \\tau^{\\dag},\\cdot\\right ) i_{2}i_{3}}\\left (   \\theta\\right )   } { \\partial\\tau}\\right ) \\\\ + if_{1,\\tau\\left (   \\cdot\\right )   , i_{2}}\\left (   \\theta\\right )   d_{2,\\theta } \\left (   \\frac{\\partial if_{2,2,\\psi\\left (   \\tau^{\\dag},\\cdot\\right ) i_{1}i_{3}}\\left (   \\theta\\right )   } { \\partial\\tau}\\right ) \\\\ + if_{1,\\tau\\left (   \\cdot\\right )   , i_{3}}\\left (   \\theta\\right )   d_{2,\\theta } \\left (   \\frac{\\partial if_{2,2,\\psi\\left (   \\tau^{\\dag},\\cdot\\right ) i_{1}i_{2}}\\left (   \\theta\\right )   } { \\partial\\tau}\\right ) \\end{array } \\right ) \\\\ + \\frac{1}{6}\\left ( \\begin{array } [ c]{c}d_{1,\\theta}\\left (   \\frac{\\partial^{2}if_{1,\\psi\\left (   \\tau^{\\dag},\\cdot\\right )   , i_{1}}\\left (   \\theta\\right )   } { \\partial\\tau^{2}}\\right ) if_{1,\\tau\\left (   \\cdot\\right )   , i_{2}}\\left (   \\theta\\right )   if_{1,\\tau \\left (   \\cdot\\right )   , i_{3}}\\left (   \\theta\\right ) \\\\ + d_{1,\\theta}\\left (   \\frac{\\partial^{2}if_{1,\\psi\\left (   \\tau^{\\dag},\\cdot\\right )   , i_{2}}\\left (   \\theta\\right )   } { \\partial\\tau^{2}}\\right ) if_{1,\\tau\\left (   \\cdot\\right )   , i_{1}}\\left (   \\theta\\right )   if_{1,\\tau \\left (   \\cdot\\right )   , i_{3}}\\left (   \\theta\\right ) \\\\ + d_{1,\\theta}\\left (   \\frac{\\partial^{2}if_{1,\\psi\\left (   \\tau^{\\dag},\\cdot\\right )   , i_{3}}\\left (   \\theta\\right )   } { \\partial\\tau^{2}}\\right ) if_{1,\\tau\\left (   \\cdot\\right )   , i_{1}}\\left (   \\theta\\right )   if_{1,\\tau \\left (   \\cdot\\right )   , i_{2}}\\left (   \\theta\\right ) \\end{array } \\right ) \\end{array } \\right\\}\\end{gathered}\\ ] ]    from the fact that @xmath3197 we conclude that the order of @xmath3198   $ ] is equal to the order of@xmath3199   + e_{\\theta}\\left [   d_{1,\\theta } \\left (   \\partial if_{1,\\widetilde{\\psi}_{k}\\left (   \\tau,\\cdot\\right ) } \\left (   \\widehat{\\theta}\\right )   /\\partial\\tau\\right )   \\right ]   e_{\\theta } \\left [   if_{2,2,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right )   } \\left ( \\widehat{\\theta}\\right )   \\right ] \\\\ &   + e_{\\theta}\\left [   d_{2,\\theta}\\left (   \\partial if_{2,2,\\widetilde{\\psi } _ { k}\\left (   \\tau,\\cdot\\right )   } \\left (   \\widehat{\\theta}\\right ) /\\partial\\tau\\right )   \\right ]   e_{\\theta}\\left [   if_{1,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right )   } \\left (   \\widehat{\\theta}\\right )   \\right]\\end{aligned}\\ ] ] now @xmath3200   = o_{p}\\left [ \\left (   p-\\widehat{p}\\right )   \\left (   b\\ -\\widehat{b}\\ \\right )   \\left ( \\frac{g\\left (   x\\right )   } { \\widehat{g}\\left (   x\\right )   } -1\\right )   \\right ] , \\\\ e_{\\theta}\\left [   d_{1,\\theta}\\left (   \\partial if_{1,\\widetilde{\\psi}_{k}\\left (   \\tau,\\cdot\\right )   } \\left (   \\widehat{\\theta}\\right )   /\\partial \\tau\\right )   \\right ]   = e_{\\theta}\\left [   \\left (   a-\\widehat{p}\\right ) ^{2}\\right ]   -e_{\\widehat{\\theta}}\\left [   \\left (   a-\\widehat{p}\\right ) ^{2}\\right ]",
    "\\\\ = e_{\\widehat{\\theta}}\\left [   \\left (   \\frac{f\\left (   a|x\\right )   } { \\widehat{f}\\left (   a|x\\right )   } \\frac{g\\left (   x\\right )   } { \\widehat{g}\\left ( x\\right )   } -1\\right )   \\left (   a-\\widehat{p}\\right )   ^{2}\\right ]",
    "\\\\ = e_{\\widehat{\\theta}}\\left [   \\left (   \\left [   \\left (   \\frac{f\\left ( a|x\\right )   } { \\widehat{f}\\left (   a|x\\right )   } -1\\right )   \\frac{g\\left ( x\\right )   } { \\widehat{g}\\left (   x\\right )   } \\right ]   \\right )   \\left ( a-\\widehat{p}\\right )   ^{2}\\right ]",
    "\\\\ + e_{\\widehat{\\theta}}\\left [   \\left (   \\frac{g\\left (   x\\right )   } { \\widehat{g}\\left (   x\\right )   } -1\\right )   \\left (   a-\\widehat{p}\\right )   ^{2}\\right ] = o_{p}\\left [   \\left (   p-\\widehat{p}\\right )   + \\left (   \\frac{g\\left (   x\\right ) } { \\widehat{g}\\left (   x\\right )   } -1\\right )   \\right]\\end{gathered}\\ ] ] by @xmath12 binary , @xmath3201 = -\\left\\ {   e_{\\theta}\\left [   \\left (   a-\\widehat{p}\\right )   \\right ]   \\right\\ } ^{2}=o_{p}\\left [   \\left (   p-\\widehat{p}\\right )   ^{2}\\right ] \\\\ e_{\\theta}\\left [   if_{1,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right )   } \\left ( \\widehat{\\theta}\\right )   \\right ]   = e_{\\theta}\\left [   \\left (   a-\\widehat{p}\\right )   \\left (   y\\ -\\widehat{b}\\ \\right )   \\right ]",
    "\\\\ = e_{\\widehat{\\theta}}\\left [   \\left (   p-\\widehat{p}\\right )   \\left ( b\\ -\\widehat{b}\\ \\right )   \\right]\\end{gathered}\\ ] ] and using @xmath3202 and the explicit expression for @xmath3203 in eq ( [ q22 ] ) @xmath3204 \\\\ &   = o_{p}\\left [   e_{\\theta}\\left [   if_{2,2,\\widetilde{\\psi}_{k}\\left ( \\tau^{\\dagger},\\cdot\\right )   } \\left (   \\widehat{\\theta}\\right )   \\right ] \\right ]   + e_{\\theta}\\left [   \\partial if_{1,\\widetilde{\\psi}_{k}\\left ( \\tau,\\cdot\\right )   } \\left (   \\widehat{\\theta}\\right )   /\\partial\\tau\\right ] \\\\ &   \\times e_{\\theta}\\left [   if_{1,\\widetilde{\\tau}_{k}\\left (   \\cdot\\right ) } \\left (   \\widehat{\\theta}\\right )   \\right ] \\\\ &   = o_{p}\\left [   \\left (   p-\\widehat{p}\\right )   \\left (   b\\ -\\widehat{b}\\ \\right )   \\right ] \\\\ &   + o_{p}\\left [   \\left (   p-\\widehat{p}\\right )   + \\left (   \\frac{g\\left ( x\\right )   } { \\widehat{g}\\left (   x\\right )   } -1\\right )   \\right ]   \\times\\\\ &   o_{p}\\left [   \\left (   p-\\widehat{p}\\right )   + \\left (   \\frac{g\\left ( x\\right )   } { \\widehat{g}\\left (   x\\right )   } -1\\right )   + \\left (   b\\ -\\widehat{b}\\ \\right )   \\right]\\end{aligned}\\ ] ] combining terms completes the proof .",
    "next , we motivate and derive the formula of @xmath3205 for an assumed unique functional @xmath643 implicitly defined by @xmath3206 @xmath357  to motivate the general formula of @xmath3207 for arbitrary @xmath425 we first consider the following formula for @xmath3208 which was derived from @xmath2232 following part 5c ) of theorem [ eift ] .",
    "@xmath3209{c}\\psi_{\\backslash\\tau^{4}}if_{1,\\tau}\\left (   i\\right )   if_{1,\\tau}\\left ( j\\right )   if_{1,\\tau}\\left (   s\\right )   if_{1,\\tau}\\left (   t\\right ) \\\\ + d_{1,\\theta}\\left (   \\frac{\\partial if_{1,\\tau}\\left (   i\\right )   } { \\partial\\tau^{3}}\\right )   if_{1,\\tau}\\left (   j\\right )   if_{1,\\tau}\\left ( s\\right )   if_{1,\\psi\\left (   \\tau,\\cdot\\right )   } \\left (   t\\right ) \\\\ + if_{1,\\tau}\\left (   i\\right )   d_{1,\\theta}\\left (   \\frac{\\partial if_{1,\\tau } \\left (   j\\right )   } { \\partial\\tau^{3}}\\right )   if_{1,\\tau}\\left (   s\\right ) if_{1,\\psi\\left (   \\tau,\\cdot\\right )   } \\left (   t\\right ) \\\\ + if_{1,\\tau}\\left (   i\\right )   if_{1,\\tau}\\left (   j\\right )   d_{1,\\theta}\\left ( \\frac{\\partial if_{1,\\tau}\\left (   s\\right )   } { \\partial\\tau^{3}}\\right ) if_{1,\\psi\\left (   \\tau,\\cdot\\right )   } \\left (   t\\right ) \\\\ + if_{1,\\tau}\\left (   i\\right )   if_{1,\\tau}\\left (   j\\right )   if_{1,\\tau}\\left ( s\\right )   d_{1,\\theta}\\left (   \\frac{\\partial if_{1,\\psi\\left (   \\tau , \\cdot\\right )   } \\left (   t\\right )   } { \\partial\\tau^{3}}\\right ) \\end{array } \\right\\ } \\\\ &   + \\frac{1}{2}\\mathbb{v}\\left\\ { \\begin{array } [ c]{c}\\psi_{\\backslash\\tau^{3}}if_{22,\\tau}\\left (   ij\\right )   if_{1,\\tau}\\left ( s\\right )   if_{1,\\tau}\\left (   t\\right ) \\\\ + d_{1,\\theta}\\left (   \\frac{\\partial if_{1,\\psi}\\left (   t\\right )   } { \\partial\\tau^{2}}\\right )   if_{22,\\tau}\\left (   ij\\right )   if_{1,\\tau}\\left ( s\\right ) \\\\ + d_{1,\\theta}\\left (   \\frac{\\partial if_{1,\\psi}\\left (   s\\right )   } { \\partial\\tau^{2}}\\right )   if_{22,\\tau}\\left (   ij\\right )   if_{1,\\tau}\\left ( t\\right ) \\\\ + d_{1,\\theta}\\left (   \\frac{\\partial if_{22,\\psi}\\left (   ij\\right )   } { \\partial\\tau^{2}}\\right )   if_{1,\\tau}\\left (   t\\right )   if_{1,\\tau}\\left ( s\\right ) \\end{array } \\right\\ } \\\\ &   + \\frac{1}{2}\\mathbb{v}\\left\\ { \\begin{array } [ c]{c}\\psi_{\\backslash\\tau^{2}}if_{22,\\tau}\\left (   ij\\right )   if_{22,\\tau}\\left ( st\\right )   + \\\\ d_{2,\\theta}\\left (   \\frac{\\partial if_{22,\\psi}\\left (   ij\\right )   } { \\partial\\tau}\\right )   if_{22,\\tau}\\left (   st\\right ) \\\\ + if_{22,\\tau}\\left (   ij\\right )   d_{2,\\theta}\\left (   \\frac{\\partial if_{22,\\psi}\\left (   st\\right )   } { \\partial\\tau}\\right ) \\end{array } \\right\\ } \\\\ &   + \\mathbb{v}\\left\\ { \\begin{array } [ c]{c}\\psi_{\\backslash\\tau^{2}}if_{1,\\tau}\\left (   i\\right )   if_{33,\\tau}\\left ( jst\\right )   + \\\\ d_{1,\\theta}\\left (   \\frac{\\partial if_{1,\\psi}\\left (   i\\right )   } { \\partial \\tau}\\right )   if_{33,\\tau}\\left (   jst\\right )   + d_{3,\\theta}\\left ( \\frac{\\partial if_{33,\\psi}\\left (   jst\\right )   } { \\partial\\tau}\\right ) if_{1,\\tau}\\left (   i\\right ) \\end{array } \\right\\}\\end{aligned}\\ ] ]    this formula for @xmath3210 reveals a very nice pattern .",
    "note that in addition to @xmath3211 the rhs consists of four pieces with leading terms @xmath3212 @xmath3213 @xmath3214 and @xmath3215 respectively .  within each piece , the remaining terms can be constructed simply by applying the algorithm * te * described below .    * te * i )",
    "remove the partial derivative of @xmath271 wrt .",
    "@xmath3216 ii ) for each factor of the leading term , replace the @xmath3217 in the subscript with @xmath3218 and iii ) partially differentiate the newly replaced factor wrt .",
    "@xmath2041 , and make the partial derivative degenerate . here",
    "the order of the partial derivative equals the total number of the factors in the leading term minus 1 .    moreover , each piece corresponds to one of the following ways of representing the number 4 as a sum : @xmath3219 @xmath3220 , @xmath3221 , and @xmath3222 furthermore , assume @xmath135 can be written as @xmath3223 @xmath2820with @xmath3224 e.g. , @xmath3225 then the number in front of the piece corresponding to the sum representation @xmath3226 equals @xmath3227 note that @xmath3228      [ testing ] let @xmath643 be the assumed unique functional defined by @xmath3229 then , for @xmath3230 , whenever @xmath3231 and @xmath3232 exist , @xmath3233{c}\\psi_{\\backslash\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } } { \\displaystyle\\prod\\limits_{r=1}^{m-1 } } \\left ( { \\displaystyle\\prod\\limits_{s=1}^{\\varkappa_{m , r } } } if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left (   s\\right ) } \\left (   \\theta\\right )   \\right ) \\\\ + { \\displaystyle\\sum\\limits_{r , s}^{m-1,\\varkappa_{m , r } } } \\left ( \\begin{array } [ c]{c}d_{r,\\theta}\\left (   \\frac{\\partial^{\\left [   sum\\left (   \\overline{\\varkappa } _ { m}\\right )   -1\\right ]   } \\left (   if_{r , r,\\psi\\left (   \\tau,\\cdot\\right ) , \\overline{i}_{r , s}}^{\\left (   s\\right )   } \\right )   } { \\partial\\tau^{\\left [ sum\\left (   \\overline{\\varkappa}_{m}\\right )   -1\\right ]   } } \\right )   \\times\\\\{\\displaystyle\\prod\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left ( r , s\\right )   } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right ) \\end{array } \\right\\ } \\label{tsm}\\ ] ]      note that @xmath3235{c}d_{r,\\theta}\\left (   \\frac{\\partial^{\\left [   sum\\left (   \\overline{\\varkappa } _ { m}\\right )   -1\\right ]   } \\left (   if_{r , r,\\psi\\left (   \\tau,\\cdot\\right ) , \\overline{i}_{r , s}}^{\\left (   s\\right )   } \\right )   } { \\partial\\tau^{\\left [ sum\\left (   \\overline{\\varkappa}_{m}\\right )   -1\\right ]   } } \\right )   \\times\\\\{\\displaystyle\\prod\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left ( r , s\\right )   } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right )   \\right\\}\\ ] ] can be constructed by applying algorithm * te * to the leading term @xmath3236    eq . ( [ ts1 ] )",
    "has been proved in theorem [ tt ] .",
    "next , we shall prove eq .",
    "( [ tsm ] ) by induction .",
    "the case where @xmath2238 was proved in theorem [ tt ] as well .",
    "now , we assume eq .",
    "( [ tsm ] ) holds for @xmath3237and prove it is also true for @xmath286 by part 5c ) of theorem [ eift ] . specifically , by induction assumption , @xmath3238{c}\\psi_{\\backslash\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } } { \\displaystyle\\prod\\limits_{r=1}^{m-1 } } \\left ( { \\displaystyle\\prod\\limits_{s=1}^{\\varkappa_{m , r } } } if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left (   s\\right ) } \\left (   \\theta\\right )   \\right ) \\\\ + { \\displaystyle\\sum\\limits_{r , s}^{m-1,\\varkappa_{m , r } } } \\left ( \\begin{array } [ c]{c}d_{r,\\theta}\\left (   \\frac{\\partial^{\\left [   sum\\left (   \\overline{\\varkappa } _ { m}\\right )   -1\\right ]   } \\left (   if_{r , r,\\psi\\left (   \\tau,\\cdot\\right ) , \\overline{i}_{r , s}}^{\\left (   s\\right )   } \\right )   } { \\partial\\tau^{\\left [ sum\\left (   \\overline{\\varkappa}_{m}\\right )   -1\\right ]   } } \\right )   \\times\\\\{\\displaystyle\\prod\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left ( r , s\\right )   } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right ) \\end{array } \\right\\}\\end{aligned}\\ ] ]    consider any sufficiently smooth @xmath3239dimensional parametric submodel @xmath3240 mapping @xmath3241 to @xmath245 .  for any @xmath179 in the range of @xmath3242 differentiate both sides of the above equation w.r.t @xmath781 and evaluate at @xmath3243 @xmath3244 then @xmath3245{c}\\left [ \\begin{array } [ c]{c}\\psi_{\\backslash\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   + 1}}\\tau_{\\backslash t}\\left (   \\theta\\right ) \\\\ + \\frac{\\partial\\psi_{\\backslash\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } } \\left (   \\tau,\\theta_{t}\\right )   } { \\partial t}|_{t = t^{\\ast}}\\end{array } \\right ] { \\displaystyle\\prod\\limits_{r=1}^{m-1 } } \\left ( { \\displaystyle\\prod\\limits_{s=1}^{\\varkappa_{m , r } } } if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left (   s\\right ) } \\left (   \\theta\\right )   \\right ) \\\\ + \\psi_{\\backslash\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } } { \\displaystyle\\sum\\limits_{\\left (   r , s\\right )   } ^{sum\\left (   \\overline { \\varkappa}_{m}\\right )   } } if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s},\\backslash t}^{\\left ( s\\right )   } \\left (   \\theta\\right ) { \\displaystyle\\prod\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left ( r , s\\right )   } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\\\ + { \\displaystyle\\sum\\limits_{\\left (   r , s\\right )   } ^{sum\\left (   \\overline { \\varkappa}_{m}\\right )   } } \\left\\ { \\begin{array } [ c]{c}\\left [ \\begin{array } [ c]{c}\\frac{\\partial^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } \\left ( if_{r , r,\\psi\\left (   \\tau,\\cdot\\right )   , \\overline{i}_{r , s}}^{\\left (   s\\right ) } \\right )   } { \\partial\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } } \\tau_{\\backslash t}\\left (   \\theta\\right ) \\\\ + \\frac{\\partial^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } \\left ( if_{r , r,\\psi\\left (   \\tau,\\cdot\\right )   , \\overline{i}_{r , s}}^{\\left (   s\\right ) } \\left (   \\tau,\\theta_{t}\\right )   \\right )   } { \\partial\\tau^{\\left [   sum\\left ( \\overline{\\varkappa}_{m}\\right )   -1\\right ]   } \\partial t}|_{t = t^{\\ast}}\\end{array } \\right ] \\\\ \\times{\\displaystyle\\prod\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left ( r , s\\right )   } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\\\ + \\frac{\\partial^{\\left [   sum\\left (   \\overline{\\varkappa}_{m}\\right ) -1\\right ]   } \\left (   if_{r , r,\\psi\\left (   \\tau,\\cdot\\right )   , \\overline{i}_{r , s}}^{\\left (   s\\right )   } \\right )   } { \\partial\\tau^{\\left [   sum\\left ( \\overline{\\varkappa}_{m}\\right )   -1\\right ]   } } \\\\ \\times{\\displaystyle\\sum\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left (   r , s\\right ) } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}},\\backslash t}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\\\ \\times{\\displaystyle\\prod\\limits_{\\left (   r_{2},s_{2}\\right )   \\neq\\left ( r_{1},s_{1}\\right )   \\neq\\left (   r , s\\right )   } } if_{r_{2},r_{2},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{2},s_{2}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right\\ } \\end{array } \\right\\ } \\label{m2m+1}\\ ] ]    note that @xmath3246 is the derivative of @xmath3247 w.r.t .",
    "@xmath781 while fixing @xmath2041 at @xmath2044 therefore , @xmath3248 \\\\ &   = e_{\\theta}\\left [   \\frac{\\partial^{p-1}if_{1,if_{r , r,\\psi\\left (   \\tau , \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left (   s\\right )   } \\left (   \\cdot\\right ) } \\left (   o_{m+1};\\theta\\right )   } { \\partial\\tau^{p-1}}s_{1,t}\\left ( o_{m+1}\\right )   \\right ]   , \\end{aligned}\\ ] ]    and@xmath3249{c}\\ \\psi_{\\backslash\\tau^{2}}if_{1,\\tau\\left (   \\cdot\\right )   , i_{m+1}}\\left ( \\theta\\right )   if_{m , m,\\tau\\left (   \\cdot\\right )   } ^{\\left (   s\\right )   } \\left ( \\mathbf{o}_{i_{m}};\\theta\\right )",
    "\\\\ + \\frac{\\partial}{\\partial\\tau}if_{1,\\psi\\left (   \\tau,\\cdot\\right )   , i_{m+1}}if_{m , m,\\tau\\left (   \\cdot\\right )   } ^{\\left (   s\\right )   } \\left ( \\mathbf{o}_{i_{m}};\\theta\\right ) \\\\ + if_{m , m,\\psi\\left (   \\tau,\\cdot\\right )   , \\backslash\\tau}^{\\left (   s\\right ) } if_{1,\\tau\\left (   \\cdot\\right )   , i_{m+1}}\\end{array } \\right\\ } \\\\ &   + \\sum_{\\left (   \\varkappa_{m,1},\\varkappa_{m,2}, ... ,\\varkappa_{m , m-1}\\right )   } \\left ( { \\displaystyle\\prod\\limits_{r=1}^{m-1 } } \\varkappa_{m , r}!\\right )   ^{-1}\\times\\\\ &   \\left\\ { \\begin{array } [ c]{c}\\left\\ { \\begin{array } [ c]{c}\\left\\ { \\begin{array } [ c]{c}\\left [ \\begin{array } [ c]{c}\\psi_{\\backslash\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   + 1}}if_{1,\\tau\\left (   \\cdot\\right )   , i_{m+1}}\\left (   \\theta\\right ) \\\\ + \\frac{\\partial^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } } { \\partial \\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } } \\left (   if_{1,\\psi\\left ( \\tau,\\cdot\\right )   , i_{m+1}}\\right ) \\end{array } \\right ] \\\\ \\times{\\displaystyle\\prod\\limits_{r=1}^{m-1 } } \\left ( { \\displaystyle\\prod\\limits_{s=1}^{\\varkappa_{m , r } } } if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left (   s\\right ) } \\left (   \\theta\\right )   \\right ) \\end{array } \\right\\ } \\\\ + { \\displaystyle\\sum\\limits_{\\left (   r , s\\right )   } ^{sum\\left (   \\overline { \\varkappa}_{m}\\right )   } } \\left [ \\begin{array } [ c]{c}\\frac{\\partial^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } \\left ( if_{r , r,\\psi\\left (   \\tau,\\cdot\\right )   , \\overline{i}_{r , s}}^{\\left (   s\\right ) } \\right )   } { \\partial\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } } if_{1,\\tau\\left (   \\cdot\\right )   , i_{m+1}}\\left (   \\theta\\right ) \\\\ \\times{\\displaystyle\\prod\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left ( r , s\\right )   } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right ] \\end{array } \\right\\ } \\\\ + { \\displaystyle\\sum\\limits_{\\left (   r , s\\right )   } ^{sum\\left (   \\overline { \\varkappa}_{m}\\right )   } } \\left ( \\begin{array } [ c]{c}\\left [ \\begin{array } [ c]{c}\\psi_{\\backslash\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } } if_{1,if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left ( s\\right )   } \\left (   \\cdot\\right )   } \\left (   o_{i_{m+1}};\\theta\\right ) \\\\ \\times{\\displaystyle\\prod\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left ( r , s\\right )   } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right ] \\\\ + \\left [ \\begin{array } [ c]{c}\\frac{\\partial^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } \\left ( if_{1,if_{r , r,\\psi\\left (   \\tau,\\cdot\\right )   , \\overline{i}_{r , s}}^{\\left ( s\\right )   } \\left (   \\cdot\\right )   } \\left (   o_{i_{m+1}};\\theta\\right )   \\right ) } { \\partial\\tau^{\\left [   sum\\left (   \\overline{\\varkappa}_{m}\\right )   -1\\right ] } } \\\\ \\times{\\displaystyle\\prod\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left ( r , s\\right )   } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right ] \\end{array } \\right ) \\\\ + { \\displaystyle\\sum\\limits_{\\left (   r , s\\right )   } ^{sum\\left (   \\overline { \\varkappa}_{m}\\right )   } } \\left\\ { \\begin{array } [ c]{c}\\frac{\\partial^{\\left [   sum\\left (   \\overline{\\varkappa}_{m}\\right )   -1\\right ] } \\left (   if_{r , r,\\psi\\left (   \\tau,\\cdot\\right )   , \\overline{i}_{r , s}}^{\\left ( s\\right )   } \\right )   } { \\partial\\tau^{\\left [   sum\\left (   \\overline{\\varkappa } _ { m}\\right )   -1\\right ]   } } \\times\\\\{\\displaystyle\\sum\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left (   r , s\\right ) } } \\left ( \\begin{array } [ c]{c}if_{1,if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\cdot\\right )   } \\left (   o_{i_{m+1}};\\theta\\right )   \\times\\\\{\\displaystyle\\prod\\limits_{\\left (   r_{2},s_{2}\\right )   \\neq\\left ( r_{1},s_{1}\\right )   \\neq\\left (   r , s\\right )   } } if_{r_{2},r_{2},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{2},s_{2}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right ) \\end{array } \\right\\ } \\end{array } \\right\\}\\end{aligned}\\ ] ]    consider the last term @xmath3250{c}\\frac{\\partial^{\\left [   sum\\left (   \\overline{\\varkappa}_{m}\\right )   -1\\right ] } \\left (   if_{r , r,\\psi\\left (   \\tau,\\cdot\\right )   , \\overline{i}_{r , s}}^{\\left ( s\\right )   } \\right )   } { \\partial\\tau^{\\left [   sum\\left (   \\overline{\\varkappa } _ { m}\\right )   -1\\right ]   } } { \\displaystyle\\sum\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left (   r , s\\right ) } } if_{1,if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\cdot\\right )   } \\left (   o_{i_{m+1}};\\theta\\right ) \\\\ \\times{\\displaystyle\\prod\\limits_{\\left (   r_{2},s_{2}\\right )   \\neq\\left ( r_{1},s_{1}\\right )   \\neq\\left (   r , s\\right )   } } if_{r_{2},r_{2},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{2},s_{2}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right\\}\\ ] ]    wlog , we exchange @xmath3251 with @xmath3252 then we have @xmath3253{c}\\frac{\\partial^{\\left [   sum\\left (   \\overline{\\varkappa}_{m}\\right )   -1\\right ] } \\left (   if_{r_{1},r_{1},\\psi\\left (   \\tau,\\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\right )   } { \\partial\\tau^{\\left [ sum\\left (   \\overline{\\varkappa}_{m}\\right )   -1\\right ]   } }",
    "\\times\\\\ if_{1,if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left ( s\\right )   } \\left (   \\cdot\\right )   } \\left (   o_{i_{m+1}};\\theta\\right ) { \\displaystyle\\prod\\limits_{\\left (   r_{2},s_{2}\\right )   \\neq\\left ( r_{1},s_{1}\\right )   \\neq\\left (   r , s\\right )   } } if_{r_{2},r_{2},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{2},s_{2}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right]\\ ] ] and the sum of the last two terms equals @xmath3254{c}\\left [ \\begin{array } [ c]{c}\\psi_{\\backslash\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } } if_{1,if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left ( s\\right )   } \\left (   \\cdot\\right )   } \\left (   o_{i_{m+1}};\\theta\\right ) \\\\",
    "\\times{\\displaystyle\\prod\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left ( r , s\\right )   } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right ]",
    "\\\\ + \\left [ \\begin{array } [ c]{c}\\frac{\\partial^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } \\left ( if_{1,if_{r , r,\\psi\\left (   \\tau,\\cdot\\right )   , \\overline{i}_{r , s}}^{\\left ( s\\right )   } \\left (   \\cdot\\right )   } \\left (   o_{i_{m+1}};\\theta\\right )   \\right ) } { \\partial\\tau^{\\left [   sum\\left (   \\overline{\\varkappa}_{m}\\right )   -1\\right ] } } \\\\ \\times{\\displaystyle\\prod\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left ( r , s\\right )   } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right ] \\\\ + \\left ( \\begin{array } [ c]{c}if_{1,if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left ( s\\right )   } \\left (   \\cdot\\right )   } \\left (   o_{i_{m+1}};\\theta\\right )   \\times\\\\{\\displaystyle\\sum\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left (   r , s\\right ) } } \\left [ \\begin{array } [ c]{c}\\frac{\\partial^{\\left [   sum\\left (   \\overline{\\varkappa}_{m}\\right )   -1\\right ] } \\left (   if_{r_{1},r_{1},\\psi\\left (   \\tau,\\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\right )   } { \\partial\\tau^{\\left [ sum\\left (   \\overline{\\varkappa}_{m}\\right )   -1\\right ]   } } \\times\\\\{\\displaystyle\\prod\\limits_{\\left (   r_{2},s_{2}\\right )   \\neq\\left ( r_{1},s_{1}\\right )   \\neq\\left (   r , s\\right )   } } if_{r_{2},r_{2},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{2},s_{2}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right ] \\end{array } \\right ) \\end{array } \\right)\\ ] ]    now , we have shown that , in addition to @xmath3255 the rhs of eq.([m2m+1 ] ) can be written as the sum of three pieces with the following leading terms@xmath3256{c}\\left [   \\psi_{\\backslash\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   + 1}}if_{1,\\tau\\left (   \\cdot\\right )   , i_{m+1}}\\left (   \\theta\\right )   \\right ] { \\displaystyle\\prod\\limits_{r=1}^{m-1 } } \\left ( { \\displaystyle\\prod\\limits_{s=1}^{\\varkappa_{m , r } } } if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left (   s\\right ) } \\left (   \\theta\\right )   \\right ) \\\\",
    "+ { \\displaystyle\\sum\\limits_{\\left (   r , s\\right )   } ^{sum\\left (   \\overline { \\varkappa}_{m}\\right )   } } \\left ( \\begin{array } [ c]{c}\\psi_{\\backslash\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } } if_{1,if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left ( s\\right )   } \\left (   \\cdot\\right )   } \\left (   o_{i_{m+1}};\\theta\\right ) \\\\ \\times{\\displaystyle\\prod\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left ( r , s\\right )   } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right ) \\end{array } \\right\\ }   , \\label{lead_m+1}\\ ] ] while the remaining terms can be constructed by applying the algorithm * te * to the above leading terms .",
    "this can be proved following a simple but important fact that , for any sum representation of@xmath3260 either i ) @xmath3261 and @xmath3262 for @xmath1939 @xmath3263 or ii ) @xmath3264 and there exists a sum representation of @xmath3265 @xmath3266 such that ,        define@xmath3275 \\\\ &   \\times{\\displaystyle\\prod\\limits_{r=1}^{m-1 } } \\left ( { \\displaystyle\\prod\\limits_{s=1}^{\\varkappa_{m , r } } } if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left (   s\\right ) } \\left (   \\theta\\right )   \\right ) \\\\ &   lt2\\left (   \\varkappa_{m,1}, ... ,\\varkappa_{m , m-1};r\\right ) \\\\ &   = \\left ( { \\displaystyle\\prod\\limits_{r=1}^{m-1 } } \\varkappa_{m , r}!\\right )   ^{-1}\\sum_{s=1}^{\\varkappa_{m , r}}\\left ( \\begin{array } [ c]{c}\\psi_{\\backslash\\tau^{sum\\left (   \\overline{\\varkappa}_{m}\\right )   } } if_{1,if_{r , r,\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r , s}}^{\\left ( s\\right )   } \\left (   \\cdot\\right )   } \\left (   o_{i_{m+1}};\\theta\\right ) \\\\ \\times{\\displaystyle\\prod\\limits_{\\left (   r_{1},s_{1}\\right )   \\neq\\left ( r , s\\right )   } } if_{r_{1},r_{1},\\tau\\left (   \\cdot\\right )   , \\overline{i}_{r_{1},s_{1}}}^{\\left (   s\\right )   } \\left (   \\theta\\right ) \\end{array } \\right )   .\\end{aligned}\\ ] ]    note that for any @xmath3276 if @xmath3277 then @xmath3278 and for any @xmath3279 such that @xmath3280 @xmath3281 as @xmath3282  now , it is obvious that the term with @xmath3283 in eq .",
    "( [ lead2 ] ) comes from the following terms in eq .",
    "( [ lead_m+1 ] ) @xmath3284 while the  term with @xmath3285 in eq .",
    "( [ lead2 ] ) comes from the following terms in eq .",
    "( [ lead_m+1 ] ) @xmath3286    ( theorem [ hoiprod ] ) we proceed by induction .  for @xmath2581 @xmath3287 \\right\\ } \\\\ &   = \\sum_{r=1}^{\\zeta}\\left\\ {   e_{\\theta}\\left [   \\mathbb{if}_{\\psi_{r},1}\\left (   \\theta\\right )   \\mathbb{\\zeta}_{1;\\backslash l_{1}}\\left ( \\theta\\right )   \\right ]   \\times\\left [   \\prod\\limits_{s\\leq\\zeta , s\\neq r}\\psi_{s}\\left (   \\theta\\right )   \\right ]   \\right\\}\\end{aligned}\\ ] ] therefore @xmath3288   \\right\\ } \\\\ &   = \\mathbb{v}\\left [ { \\displaystyle\\sum\\limits_{\\left\\ {   t_{1}, ... t_{\\zeta}\\right\\ }   \\in \\upsilon_{\\zeta;1 } } } { \\displaystyle\\prod\\limits_{s=1}^{\\zeta } } if_{\\psi_{s}\\left (   \\theta\\right )   ; t_{s},t_{s};\\overline{i}_{s",
    ", t_{s}}}\\left ( \\theta\\right )   \\right]\\end{aligned}\\ ] ] assume that the lemma holds for @xmath972i.e . :",
    "@xmath3289\\ ] ] we now show that it holds for @xmath3290 now , @xmath3291 \\\\ &   -\\pi_{\\theta , m}\\left [   \\mathbb{v}\\left [   if_{1,if_{\\psi\\left (   \\theta ; \\zeta\\right )   ; j , j}\\left (   o_{i_{1}}, ... ,o_{i_{j}};\\cdot\\right )   \\ } \\left ( o_{i_{j+1}};\\theta\\right )   \\right ]   |\\mathcal{u}_{j}\\left (   \\theta\\right ) \\right]\\end{aligned}\\ ] ] so that @xmath3292 @xmath3293{c}\\left\\ { \\begin{array } [ c]{c}if_{if_{\\psi_{r}\\left (   \\theta\\right )   ; t_{r},t_{r};\\overline{i}_{r , t_{r}}}\\left (   \\theta\\right )   ; 1,1\\ } \\left (   o_{i_{r , t_{r}+1}};\\theta\\right ) \\\\ -\\pi_{\\theta , t_{r}}\\left [   \\mathbb{v}\\left [   if_{if_{\\psi_{r}\\left ( \\theta\\right )   ; t_{r},t_{r};\\overline{i}_{r , t_{r}}}\\left (   \\theta\\right ) ; 1,1\\ } \\left (   o_{i_{r , t_{r}+1}};\\theta\\right )   \\right ]   |\\mathcal{u}_{t_{r}}\\left (   \\theta\\right )   \\right ] \\end{array } \\right\\ } \\\\",
    "\\times\\left ( { \\displaystyle\\prod\\limits_{s\\leq\\zeta , s\\neq r } } if_{\\psi_{s}\\left (   \\theta\\right )   ; t_{s},t_{s};\\overline{i}_{s , t_{s}}}\\left ( \\theta\\right )   \\right ) \\end{array } \\right\\ }   \\right ] \\\\ &   = \\mathbb{v}\\left [ { \\displaystyle\\sum\\limits_{\\left\\ {   t_{1}, ... t_{\\zeta}\\right\\ }   \\in \\upsilon_{\\zeta;j } } } { \\displaystyle\\sum\\limits_{r=1}^{\\zeta } } \\left\\ { \\begin{array } [ c]{c}\\left (   t_{r}+1\\right )   if_{\\psi_{r}\\left (   \\theta\\right )   ; t_{r}+1,t_{r}+1;\\overline{i}_{r , t_{r}+1}}\\left (   \\theta\\right ) \\\\",
    "\\times\\left ( { \\displaystyle\\prod\\limits_{s\\leq\\zeta , s\\neq r } } if_{\\psi_{s}\\left (   \\theta\\right )   ; t_{s},t_{s};\\overline{i}_{s , t_{s}}}\\left ( \\theta\\right )   \\right ) \\end{array } \\right\\ }   \\right]\\end{aligned}\\ ] ] now , consider an arbitrary term in the double sum corresponding to the index vector @xmath3294 where the star indicates the index of the second summation .",
    "then this term and terms corresponding to @xmath3295 all share the common factor @xmath3296 but with respective multiplicative constants @xmath3297 .  so that the total contribution of terms in the double summation that share this common factor is given by:@xmath3298{c}if_{\\psi_{r^{\\ast}}\\left (   \\theta\\right )   ; t_{r^{\\ast}}^{\\prime}+1,t_{r^{\\ast } } ^{\\prime}+1;\\overline{i}_{r^{\\ast},t_{r^{\\ast}}^{\\prime}+1}}\\left ( \\theta\\right ) \\\\",
    "\\times\\left ( { \\displaystyle\\prod\\limits_{s\\leq\\zeta , s\\neq r^{\\ast } } } if_{\\psi_{s}\\left (   \\theta\\right )   ; t_{s}^{\\prime},t_{s}^{\\prime};\\overline { i}_{s , t_{s}^{^{\\prime}}}}\\left (   \\theta\\right )   \\right ) \\end{array } \\right ) \\\\ &   = ( j+1)\\left ( \\begin{array } [ c]{c}if_{\\psi_{r^{\\ast}}\\left (   \\theta\\right )   ; t_{r^{\\ast}}^{\\prime}+1,t_{r^{\\ast } } ^{\\prime}+1;\\overline{i}_{r^{\\ast},t_{r^{\\ast}}^{\\prime}+1}}\\left ( \\theta\\right ) \\\\ \\times\\left ( { \\displaystyle\\prod\\limits_{s\\leq\\zeta , s\\neq r^{\\ast } } } if_{\\psi_{s}\\left (   \\theta\\right )   ; t_{s}^{\\prime},t_{s}^{\\prime};\\overline { i}_{s , t_{s}^{^{\\prime}}}}\\left (   \\theta\\right )   \\right ) \\end{array } \\right)\\end{aligned}\\ ] ] repeating this argument over all common factors in the set @xmath3299{c}if_{\\psi_{r^{\\ast}}\\left (   \\theta\\right )   ; t_{r^{\\ast}}+1,t_{r^{\\ast}}+1;\\overline{i}_{r^{\\ast},t_{r^{\\ast}}+1}}\\left (   \\theta\\right ) \\\\",
    "\\times\\left ( { \\displaystyle\\prod\\limits_{s\\leq\\zeta , s\\neq r^{\\ast } } } if_{\\psi_{s}\\left (   \\theta\\right )   ; t_{s}^{\\prime},t_{s}^{\\prime};\\overline { i}_{s , t_{s}^{^{\\prime}}}}\\left (   \\theta\\right )   \\right ) \\end{array } \\right )   : \\left\\ {   t_{1}, ... t_{\\zeta}\\right\\ }   \\in\\upsilon_{\\zeta;j}\\right\\}\\ ] ] that appear in the double sum , we recover the desired sum @xmath3300\\ ] ] this is because the set of all common factors in @xmath1384 is precisely : @xmath3301 this concludes the proof .",
    "( theorem [ btomm ] ) for @xmath2113@xmath3302   + e_{\\theta } \\left [   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   \\widehat{b}_{1}-\\widehat{b}_{0}\\right )   \\right ]   + e_{\\theta}\\left (   \\widehat{b}_{0}-b_{0}\\right ) \\\\ &   = e_{\\theta}\\left [   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\delta p_{1}\\delta b_{1}+\\delta p_{0}\\delta b_{0}\\right]\\end{aligned}\\ ] ] next we proceed to prove eq.([ebm ] ) and eq.([tbm ] ) by induction ,    first of all , @xmath3303 \\\\ &   = e_{\\theta}\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   b_{1}-\\widehat{b}_{1}\\right )   \\left (   \\frac{r_{1}}{\\widehat{\\pi}_{1}}-1\\right ) + \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\left (   b_{1}-\\widehat{b}_{1}+\\widehat{b}_{1}-\\widehat{b}_{0}\\right )   |l_{0}\\right ) \\\\ &   = e_{\\theta}\\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\delta b_{1}\\delta p_{1}+\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\delta b_{0}|l_{0}\\right)\\end{aligned}\\ ] ] and @xmath3304 \\\\ = e_{\\theta}\\left (   \\pi_{\\theta}\\left (   q_{01}^{1/2}\\delta b_{1}|\\left ( q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right )   \\pi_{\\theta}\\left ( q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right )   \\right )   + \\\\ e_{\\theta}\\left (   \\pi_{\\theta}^{\\bot}\\left (   q_{01}^{1/2}\\delta b_{1}|\\left ( q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right )   \\pi_{\\theta}^{\\bot}\\left ( q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right )   \\right)\\end{gathered}\\ ] ] with @xmath3305 for @xmath2211@xmath3306   + e_{\\theta}\\left (   \\widehat{\\psi}_{2,2}\\right ) \\\\ &   = e_{\\theta}\\left [   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\delta p_{1}\\delta b_{1}+\\delta p_{0}\\delta b_{0}\\right ]   -e_{\\theta}\\left [   \\frac{r_{0}\\pi_{1}}{\\widehat{\\pi}_{0}\\widehat{\\pi}_{1}}\\delta b_{1}\\overline{w}_{k_{1}}^{t}\\right ]   e_{\\theta}\\left [   \\overline{w}_{k_{1}}\\frac{r_{0}}{\\widehat{\\pi } _ { 0}}\\delta p_{1}\\right ] \\\\ &   -e_{\\theta}\\left\\ {   \\left (   \\frac{r_{0}}{\\widehat{\\pi}_{0}}\\delta p_{1}\\delta b_{1}+\\frac{r_{0}}{\\widehat{\\pi}_{0}}\\delta b_{0}\\right ) \\overline{z}_{k_{0}}^{t}\\right\\ }   e_{\\theta}\\left [   \\overline{z}_{k_{0}}\\delta p_{0}\\right ] \\\\ &   = -e_{\\theta}\\left (   q_{0}\\delta b_{0}\\overline{z}_{k_{0}}^{t}\\right ) e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right ) ^{-1}\\left [   e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right )   -i\\right ]   e_{\\theta}\\left (   \\overline{z}_{k_{0}}\\delta p_{0}\\right ) \\\\ &   -e_{\\theta}\\left (   q_{01}\\delta b_{1}\\overline{w}_{k_{1}}^{t}\\right ) e_{\\theta}\\left (   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right ) ^{-1}\\left (   e_{\\theta}\\left (   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right )   -i\\right )   e_{\\theta}\\left (   \\overline{w}_{k_{1}}q_{0}\\delta p_{1}\\right ) \\\\ &   -e_{\\theta}\\left (   q_{0}\\delta p_{1}\\delta b_{1}\\overline{z}_{k_{0}}^{t}\\right )   e_{\\theta}\\left [   \\overline{z}_{k_{0}}\\delta p_{0}\\right ] \\\\ &   + e_{\\theta}\\left (   \\pi_{\\theta}^{\\bot}\\left (   q_{0}^{1/2}\\delta b_{0}|\\left (   q_{0}^{1/2}\\overline{z}_{k_{0}}\\right )   \\right )   \\pi_{\\theta } ^{\\bot}\\left (   q_{0}^{-1/2}\\delta p_{0}|\\left (   q_{0}^{1/2}\\overline { z}_{k_{0}}\\right )   \\right )   \\right ) \\\\ &   + e_{\\theta}\\left (   \\pi_{\\theta}^{\\bot}\\left (   q_{01}^{1/2}\\delta b_{1}|\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right )   \\pi_{\\theta } ^{\\bot}\\left (   q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right )   \\right)\\end{aligned}\\ ] ] if e.q([ebm ] ) holds for @xmath3307 we next show it also holds for @xmath425@xmath3308{c}e_{\\theta}\\left [   q_{0}\\delta p_{1}\\delta b_{1}\\overline{z}_{k_{0}}^{t}\\right ]   \\left [   e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline { z}_{k_{0}}^{t}-i\\right )   \\right ]   ^{m-2}e_{\\theta}\\left (   \\overline{z}_{k_{0}}\\delta p_{0}\\right ) \\\\ + e_{\\theta}\\left [   q_{0}\\delta b_{0}\\overline{z}_{k_{0}}^{t}\\right ]   \\left [ e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right ) \\right ]   ^{m-2}e_{\\theta}\\left (   \\overline{z}_{k_{0}}\\delta p_{0}\\right ) _ { \\_\\left (   l1.1\\right )   }",
    "\\\\ + e_{\\theta}\\left (   q_{01}\\delta b_{1}\\overline{w}_{k_{1}}^{t}\\right )   \\left [ e_{\\theta}\\left (   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right ) \\right ]   ^{m-2}e_{\\theta}\\left (   \\overline{w}_{k_{1}}q_{0}\\delta p_{1}\\right ) \\__{\\left (   l1.2\\right )   } \\\\ + { \\textstyle\\sum\\limits_{j=2}^{m-1 } } \\begin{array } [ c]{c}e_{\\theta}\\left [   \\overline{z}_{k_{0}}\\delta p_{0}^{t}\\right ]   \\left [ e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right ) \\right ]   ^{j-2}e_{\\theta}\\left [   q_{01}\\delta b_{1}\\overline{z}_{k_{0}}\\overline{w}_{k_{1}}^{t}\\right ] \\\\",
    "\\times e_{\\theta}\\left (   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right )   ^{m-1-j}e_{\\theta}\\left [   \\overline{w}_{k_{1}}q_{0}\\delta p_{1}\\right ]   \\__{\\left (   l1.3.j\\right )   } \\end{array } \\end{array } \\right\\}\\end{aligned}\\]]@xmath3309@xmath3310{c}\\left\\ { \\begin{array } [ c]{c}e_{\\theta}\\left\\ {   \\left [   q_{0}\\delta b_{0}\\right ]   \\overline{z}_{k_{0}}^{t}\\right\\ }   e_{\\theta}\\left [   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right ]   ^{-1}\\times\\\\ \\left [   e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right )   -i\\right ]   ^{m-2}e_{\\theta}\\left [   \\overline{z}_{k_{0}}\\delta p_{0}\\right ] \\end{array } \\right\\ }   _ { -\\left (   l2.1\\right )   }",
    "\\\\ + \\left\\ { \\begin{array } [ c]{c}e_{\\theta}\\left\\ {   \\left [   q_{01}\\delta b_{1}\\right ]   \\overline{w}_{k_{1}}^{t}\\right\\ }   e_{\\theta}\\left [   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right ]   ^{-1}\\times\\\\ \\left [   e_{\\theta}\\left (   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right )   -i\\right ]   ^{m-2}e_{\\theta}\\left [   \\overline{w}_{k_{1}}q_{0}\\delta p_{1}\\right ] \\end{array } \\right\\ }   _ { -\\left (   l2.2\\right )   } \\\\",
    "+ { \\displaystyle\\sum\\limits_{j=2}^{m-2 } } \\left\\ { \\begin{array } [ c]{c}e_{\\theta}\\left [   \\overline{z}_{k_{0}}\\delta p_{0}^{t}\\right ]   \\left [ e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right ) \\right ]   ^{j-2}\\times\\\\ e_{\\theta}\\left [   q_{01}\\delta b_{1}\\overline{z}_{k_{0}}\\overline{w}_{k_{1}}^{t}\\right ]   e_{\\theta}\\left [   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right ]   ^{-1}\\times\\\\ e_{\\theta}\\left (   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right ) ^{m-1-j}e_{\\theta}\\left [   \\overline{w}_{k_{1}}q_{0}\\delta p_{1}\\right ] \\end{array } \\right\\ }   _ { \\_\\left (   l2.3.j\\right )   } \\\\ + e_{\\theta}\\left\\ {   \\left [   q_{0}\\delta p_{1}\\delta b_{1}\\right ]   \\overline { z}_{k_{0}}^{t}\\right\\ }   \\left [   e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right )   -i\\right ]   ^{m-3}e_{\\theta}\\left [ \\overline{z}_{k_{0}}\\delta p_{0}\\right ]   _ { \\_\\left (   l2.4\\right )   } \\end{array } \\right\\}\\ ] ] it can be shown @xmath3311{c}e_{\\theta}\\left\\ {   \\left [   q_{0}\\delta b_{0}\\right ]   \\overline{z}_{k_{0}}^{t}\\right\\ }   e_{\\theta}\\left [   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right ]   ^{-1}\\times\\\\ \\left [   e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right )   -i\\right ]   ^{m-1}e_{\\theta}\\left [   \\overline{z}_{k_{0}}\\delta p_{0}\\right ] \\end{array } \\right\\}\\end{aligned}\\]]@xmath3312{c}e_{\\theta}\\left\\ {   \\left [   q_{01}\\delta b_{1}\\right ]   \\overline{w}_{k_{1}}^{t}\\right\\ }   e_{\\theta}\\left [   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right ]   ^{-1}\\times\\\\ \\left [   e_{\\theta}\\left (   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right )   -i\\right ]   ^{m-1}e_{\\theta}\\left [   \\overline{w}_{k_{1}}q_{0}\\delta p_{1}\\right ] \\end{array } \\right\\}\\end{aligned}\\ ] ] @xmath1939 @xmath3313@xmath3314{c}e_{\\theta}\\left [   \\overline{z}_{k_{0}}\\delta p_{0}^{t}\\right ]   \\left [ e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right ) \\right ]   ^{j-2}\\times\\\\ e_{\\theta}\\left [   q_{01}\\delta b_{1}\\overline{z}_{k_{0}}\\overline{w}_{k_{1}}^{t}\\right ]   e_{\\theta}\\left [   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right ]   ^{-1}\\times\\\\ e_{\\theta}\\left (   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right ) ^{m - j}e_{\\theta}\\left [   \\overline{w}_{k_{1}}q_{0}\\delta p_{1}\\right ] \\end{array } \\right\\}\\end{aligned}\\ ] ] if @xmath3315   \\left [   e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right )   \\right ]   ^{m-3}\\overline{z}_{k_{0 } } , $ ] then @xmath3316   e_{\\theta}\\left [   \\overline{w}_{k_{1}}q_{0}\\delta p_{1}\\right ]   -e_{\\theta}\\left\\ {   \\zeta_{m}\\left (   l_{0},\\theta\\right )   q_{0}\\delta p_{1}\\delta b_{1}\\right\\ } \\\\ &   = \\left (   -1\\right )   ^{m-1}\\left\\ { \\begin{array } [ c]{c}e_{\\theta}\\left (   q_{01}\\delta b_{1}\\zeta_{m}\\left (   l_{0},\\theta\\right ) \\overline{w}_{k_{1}}^{t}\\right )   e_{\\theta}\\left (   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}\\right )   ^{-1}\\times\\\\ \\left (   e_{\\theta}\\left (   q_{01}\\overline{w}_{k_{1}}\\overline{w}_{k_{1}}^{t}-i\\right )   \\right )   e_{\\theta}\\left (   \\overline{w}_{k_{1}}q_{0}\\delta p_{1}\\right ) \\end{array } \\right\\ } \\\\ &   + \\left (   -1\\right )   ^{m}e_{\\theta}\\left ( \\begin{array } [ c]{c}\\pi_{\\theta}^{\\bot}\\left [   \\left ( \\begin{array } [ c]{c}q_{01}^{1/2}\\delta b_{1}\\zeta_{m}\\left (   l_{0},\\theta\\right ) \\end{array } \\right )   |\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\\\ \\times\\pi_{\\theta}^{\\bot}\\left [   q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left ( q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\end{array }",
    "\\right )   _ { -\\left (   tb_{m-1,m-1}^{\\left (   2\\right )   } \\right )   } \\ ] ] in summary@xmath3317 next we show @xmath3318@xmath3319   \\left [ e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right ) \\right ]   ^{m-3}\\overline{z}_{k_{0}}\\\\ &   = \\left\\ { \\begin{array } [ c]{c}e_{\\theta}\\left [   \\delta p_{0}\\overline{z}_{k_{0}}^{t}\\right ]   \\left (   \\left ( \\begin{array } [ c]{c}e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right ) ^{-1}+\\\\ i - e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right ) ^{-1}\\end{array } \\right )   \\right ) \\\\ \\times\\left [   e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right )   \\right ]   ^{m-3}\\overline{z}_{k_{0}}\\end{array } \\right\\ } \\\\ &   = \\left\\ {   e_{\\theta}\\left [   \\delta p_{0}\\overline{z}_{k_{0}}^{t}\\right ] e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right ) ^{-1}\\left [   e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right )   \\right ]   ^{m-3}\\overline{z}_{k_{0}}\\right\\ } \\\\ &   + \\left\\ { \\begin{array } [ c]{c}e_{\\theta}\\left [   \\delta p_{0}\\overline{z}_{k_{0}}^{t}\\right ]   e_{\\theta } \\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right ) ^{-1}e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right ) \\\\ \\times\\left [   e_{\\theta}\\left (   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right )   \\right ]   ^{m-3}\\overline{z}_{k_{0}}\\end{array } \\right\\}\\end{aligned}\\ ] ] applying the above expression of @xmath3320 to e.q@xmath3321 we find that @xmath3322      we want to mention that @xmath3323 @xmath3324 equals @xmath3325 , @xmath3326 @xmath3327 @xmath3328 @xmath3329 @xmath3330 and @xmath3331 therefore the results from this theorem is consistent with eq.([bsp ] ) .",
    "technical details are not presented here .",
    "eq.([ebm2 ] ) follows from eq.([ebm ] ) similarly as in the proof of theorem [ btomm ] and eq.([ebm3 ] ) can be derived from eq.([ebm2 ] ) by our assumption of rate optimality of the initial estimator .",
    "next we prove eq.([tbm2]),@xmath3332   \\right)\\end{aligned}\\ ] ] ( cauchy - shwartz inequality . the last equality can easily be shown as in the proof of theorem [ on])@xmath3333{c}\\pi_{\\theta}^{\\bot}\\left [   \\left ( \\begin{array } [ c]{c}\\frac{\\pi_{1}^{1/2}}{\\widehat{\\pi}_{1}^{1/2}}\\delta b_{1}\\pi_{\\theta}\\left ( q_{0}^{1/2}\\delta p_{0}|\\left (   q_{0}^{1/2}\\overline{z}_{k_{0}}\\right ) \\right ) \\end{array } \\right )   |\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\\\ \\times\\pi_{\\theta}^{\\bot}\\left [   q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left ( q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\end{array } \\right ]   \\right\\vert \\\\ &   \\leq\\left\\vert e\\left [ \\begin{array } [ c]{c}\\pi_{\\theta}^{\\bot}\\left [   \\left ( \\begin{array } [ c]{c}\\delta b_{1}q_{01}^{1/2}\\delta p_{0}\\end{array } \\right )   |\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\\\ \\times\\pi_{\\theta}^{\\bot}\\left [   q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left ( q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\end{array } \\right ]   \\right\\vert \\\\ &   + \\left\\vert e\\left [ \\begin{array } [ c]{c}\\pi_{\\theta}^{\\bot}\\left [   \\left ( \\begin{array } [ c]{c}\\frac{\\pi_{1}^{1/2}}{\\widehat{\\pi}_{1}^{1/2}}\\delta b_{1}\\pi_{\\theta}^{\\bot } \\left (   q_{0}^{1/2}\\delta p_{0}|\\left (   q_{0}^{1/2}\\overline{z}_{k_{0}}\\right )   \\right ) \\end{array } \\right )   |\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\\\ \\times\\pi_{\\theta}^{\\bot}\\left [   q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left ( q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\end{array } \\right ]   \\right\\vert \\\\ &   \\leq o_{p}\\left (   \\max\\left (   k_{1}^{-\\left (   \\min\\left (   \\beta_{\\pi_{0}},\\beta_{b_{1}}\\right )   + \\beta_{\\pi_{1}}\\right )   /d_{1}},k_{1}^{-\\beta _ { \\pi_{1}}/d_{1}}k_{0}^{-\\beta_{\\pi_{0}}/d_{0}}\\left (   \\frac{\\log n}{n}\\right )   ^{\\frac{\\beta_{b_{1}}}{d_{1}+\\beta_{b_{1}}}}\\right )   \\right)\\end{aligned}\\ ] ] the last inequality holds because @xmath3334{c}\\frac{\\pi_{1}^{1/2}}{\\widehat{\\pi}_{1}^{1/2}}\\delta b_{1}\\pi_{\\theta}^{\\bot } \\left (   q_{0}^{1/2}\\delta p_{0}|\\left (   q_{0}^{1/2}\\overline{z}_{k_{0}}\\right )   \\right ) \\end{array } \\right )   |\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ]   \\right ) ^{2}\\\\ &   \\leq e\\left (   \\frac{\\pi_{1}^{1/2}}{\\widehat{\\pi}_{1}^{1/2}}\\delta b_{1}\\pi_{\\theta}^{\\bot}\\left (   q_{0}^{1/2}\\delta p_{0}|\\left (   q_{0}^{1/2}\\overline{z}_{k_{0}}\\right )   \\right )   \\right )   ^{2}\\\\ &   \\leq\\left\\vert \\left\\vert \\frac{\\pi_{1}^{1/2}}{\\widehat{\\pi}_{1}^{1/2}}\\delta b_{1}\\right\\vert \\right\\vert _ { \\infty}^{2}e\\left (   \\pi_{\\theta}^{\\bot } \\left (   q_{0}^{1/2}\\delta p_{0}|\\left (   q_{0}^{1/2}\\overline{z}_{k_{0}}\\right )   \\right )   \\right )   ^{2}\\ ] ] ( the first inequality holds because the orthogonal projection operator has a norm no greater than 1)@xmath3335{c}\\pi_{\\theta}^{\\bot}\\left [   \\left ( \\begin{array } [ c]{c}q_{01}^{1/2}\\delta b_{1}e\\left [   \\delta p_{0}\\overline{z}_{k_{0}}^{t}\\right ] e\\left [   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right ]   ^{-1}\\\\ \\times\\left (   e\\left [   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right ]   \\right )   ^{m-2}\\overline{z}_{k_{0}}\\end{array } \\right )   |\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\\\ \\times\\pi_{\\theta}^{\\bot}\\left [   q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left ( q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ] \\end{array } \\right ]   \\right\\vert \\\\ &   \\leq\\left\\ { \\begin{array } [ c]{c}\\left\\ {   e\\left ( \\begin{array } [ c]{c}q_{01}^{1/2}\\delta b_{1}e\\left [   \\delta p_{0}\\overline{z}_{k_{0}}^{t}\\right ] e\\left [   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}\\right ]   ^{-1}\\\\ \\times\\left (   e\\left [   q_{0}\\overline{z}_{k_{0}}\\overline{z}_{k_{0}}^{t}-i\\right ]   \\right )   ^{m-2}\\overline{z}_{k_{0}}\\end{array } \\right )   ^{2}\\right\\ }   ^{1/2}\\\\ \\times\\left\\ {   e\\left (   \\pi_{\\theta}^{\\bot}\\left [   q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ]   \\right ) ^{2}\\right\\ }   ^{1/2}\\end{array } \\right\\}\\end{aligned}\\ ] ] ( cauchy - shwartz inequality and projection operator has operator norm of 1)@xmath3336{c}\\left\\vert \\left\\vert q_{01}^{1/2}\\frac{f_{1}}{\\widehat{f}_{1}}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\delta b_{1}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert q_{0}^{-1/2}\\frac{\\widehat{f}_{0}}{f_{0}}\\right\\vert \\right\\vert _ { \\infty}\\left\\vert \\left\\vert \\frac{f_{0}}{\\pi _ { 0}\\widehat{\\pi}_{0}}\\right\\vert \\right\\vert _ { \\infty}\\\\ \\times\\left\\vert \\left\\vert \\delta g_{0}\\right\\vert \\right\\vert _ { \\infty } ^{m-2}\\left\\ {   \\int\\left (   \\pi_{0}-\\widehat{\\pi}_{0}\\right )   ^{2}dl_{0}\\right\\ }   ^{1/2}\\times\\\\ \\left\\ {   e\\left (   \\pi_{\\theta}^{\\bot}\\left [   q_{0}q_{01}^{-1/2}\\delta p_{1}|\\left (   q_{01}^{1/2}\\overline{w}_{k_{1}}\\right )   \\right ]   \\right ) ^{2}\\right\\ }   ^{1/2}\\end{array } \\right\\ } \\\\ &   = o_{p}\\left (   \\left (   \\frac{\\log n}{n}\\right )   ^{-\\frac{\\beta_{b_{1}}}{d_{1}+\\beta_{b_{1}}}-\\frac{\\left (   m-2\\right )   \\beta_{g_{0}}}{d_{0}+\\beta_{g_{0}}}}n^{-\\frac{\\beta_{\\pi_{0}}}{d_{0}+\\beta_{\\pi_{0}}}}k_{1}^{-\\beta_{\\pi_{1}}/d_{1}}\\right)\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> robins et al , 2008 , published a theory of higher order influence functions for inference in semi- and non - parametric models .  </S>",
    "<S> this paper is a comprehensive manuscript from which robins et al , was drawn .  </S>",
    "<S> the current paper includes many results and proofs that were not included in robins et al due to space limitation . </S>",
    "<S> particular results contained in the present paper that were not reported in robins et al include the following . given a set of functionals and their corresponding higher order influence functions , </S>",
    "<S> we show how to derive the higher order influence function of their product . </S>",
    "<S> we apply this result to obtain higher order influence functions and associated estimators for the mean of a response @xmath0 subject to monotone missingness under missing at random . </S>",
    "<S> these results also apply to estimating the causal effect of a time dependent treatment on an outcome @xmath0 in the presence of time - varying confounding .  </S>",
    "<S> finally , we include an appendix that contains proofs for all theorems that were stated without proof in robins et al , 2008 . the initial part of the paper is closely related to robins et al , , the latter parts differ .    </S>",
    "<S> specifically , we present a theory of point and interval estimation for nonlinear functionals in parametric , semi- , and non - parametric models based on higher order influence functions ( robins @xcite , sec . 9 </S>",
    "<S> ; li et al . </S>",
    "<S> @xcite , tchetgen et al , @xcite , robins et al , @xcite ) . </S>",
    "<S> higher order influence functions are higher order u - statistics . </S>",
    "<S> our theory extends the first order semiparametric theory of bickel et al . @xcite and van der vaart @xcite by incorporating the theory of higher order scores considered by pfanzagl @xcite , small and mcleish @xcite and lindsay and waterman @xcite . </S>",
    "<S> the theory reproduces many previous results , produces new non-@xmath1 results , and opens up the ability to perform optimal non-@xmath1 inference in complex high dimensional models . </S>",
    "<S> we present novel rate - optimal point and interval estimators for various functionals of central importance to biostatistics in settings in which estimation at the expected @xmath1 rate is not possible , owing to the curse of dimensionality . </S>",
    "<S> we also show that our higher order influence functions have a multi - robustness property that extends the double robustness property of first order influence functions described by robins and rotnitzky @xcite and van der laan and robins @xcite . </S>"
  ]
}