{
  "article_text": [
    "consider the following generative model for independent component analysis ( ica ) @xmath3 where the elements of the non - gaussian source vector @xmath4 are mutually independent with zero mean , @xmath5 is an unknown nonsingular mixing matrix , @xmath6 is an observable random vector ( signal ) , and @xmath7 is a shift parameter . let @xmath8 be the whitened data of @xmath9 , where @xmath10 .",
    "an equivalent expression of model  ( [ ica ] ) in @xmath11-scale is @xmath12 where @xmath13 is the mixing matrix in @xmath11-scale .",
    "it is reported in literature that prewhitening the data can make the ica inference procedure more stable . in the rest of the discussion",
    ", we will work with model  ( [ ica.z ] ) in estimating the mixing matrix @xmath14 based on the prewhitened @xmath11 .",
    "it is easy to transform back to the original @xmath9-scale via @xmath15 .",
    "note that both @xmath14 and @xmath16 are unknown , and there exists the identifiability problem .",
    "this can be seen from the fact that @xmath17 for any nonsingular diagonal matrix @xmath18 . to make @xmath19 identifiable , we assume the following conventional conditions for @xmath16 : @xmath20 where @xmath21 is the identity matrix .",
    "it then implies that @xmath22 and @xmath23 which means that the mixing matrix @xmath14 in @xmath11-scale is orthogonal .",
    "we will use notation @xmath24 to denote the space of orthogonal matrices in @xmath25 . note that",
    ", if @xmath26 is a parameter of model  ( [ ica.z ] ) , so is @xmath27 .",
    "thus , to fix one direction , we consider @xmath28 , where @xmath29 consists of orthogonal matrices with determinant one .",
    "this set @xmath30 is called the special orthogonal group .",
    "the main purpose of ica is to estimate the orthogonal @xmath31 based on the whitened data @xmath32 , or equivalently , to look for a recovering matrix @xmath33 so that components in @xmath34 have the maximum degree of independence . in the latter case",
    ", @xmath35 provides an estimate of @xmath19 .",
    "we first briefly review some existing methods for ica .",
    "one idea is to estimate @xmath35 via _ minimizing the mutual information_. let @xmath36 be the joint probability density function of @xmath37 , and @xmath38 be the marginal probability density function of @xmath39 . the mutual information , denoted by @xmath40 , among random variables @xmath41 , is defined to be @xmath42 where @xmath43 and @xmath44 are the shannon entropy . ideally , if @xmath35 is properly chosen so that @xmath45 has independent components , then @xmath46 and , hence , @xmath47 .",
    "thus , via minimizing @xmath48 with respect to @xmath35 , it leads to an estimate of @xmath35 .",
    "another method is to estimate @xmath35 via _ maximizing the negentropy _ , which is equivalent to minimizing mutual information as described below .",
    "the negentropy of @xmath45 is defined to be @xmath49 where @xmath50 is a gaussian random vector having the same covariance matrix as @xmath45 ( hyvrinen and oja , 2000 ) .",
    "it can be deduced that @xmath51 where the second equality holds since , by @xmath52 , @xmath53 . moreover , as @xmath54 with @xmath33 , we have @xmath55 , which does not depend on @xmath35 .",
    "that is , the negentropy is invariant under orthogonal transformation .",
    "thus , minimizing the mutual information @xmath48 is equivalent to maximizing the negentropy @xmath56 .",
    "the negentropy @xmath57 , however , involves the unknown density @xmath38 . to avoid nonparametric estimation of @xmath38",
    ", one can use the following approximation ( hyvrinen , 1998 ) via a non - quadratic contrast function @xmath58 , @xmath59 ^ 2,\\label{ne_approx}\\end{aligned}\\ ] ] where @xmath60 is a random variable having the standard normal distribution . here",
    "@xmath61 can be treated as a measure of non - gaussianity , and minimizing the sample analogue of @xmath62 to search @xmath35 corresponds to the fast - ica ( hyvrinen , 1999 ) .    another widely used estimation criterion for @xmath35 is via _ maximizing the likelihood_. under model  ( [ ica.z ] ) and by modeling @xmath63 with some known probability density function @xmath64 , the density function of @xmath11 takes the form @xmath65 since @xmath33 and hence @xmath66 .",
    "the mle - ica then searches the optimum @xmath35 via @xmath67 where @xmath68 is the kullback - leibler divergence ( kl - divergence ) , and @xmath69 is the empirical distribution of @xmath32 .",
    "possible choices of @xmath64 include @xmath70 for sub - gaussian models , and @xmath71 for super - gaussian models , where @xmath72 and @xmath73 are constants so that @xmath64 is a probability density function .",
    "it can be seen from ( [ likelihood.z ] ) that , for any row permutation matrix @xmath74 , we have @xmath75 .",
    "that is , we can estimate and identify @xmath14 only up to its row - permutation .    as will become clear later that the above mentioned methods are all related to _ minimizing the kl - divergence _ , which is not robust in the presence of outliers .",
    "outliers , however , frequently appear in real data analysis , and a robust ica inference procedure becomes necessary .",
    "for the purpose of robustness , instead of the kl - divergence , mihoko and eguchi ( 2002 ) considers the _",
    "minimum @xmath2-divergence _ estimation for @xmath35 ( @xmath2-ica ) .",
    "the issues of consistency and robustness of @xmath2-ica are discussed therein . on the other hand , the @xmath1-divergence , which can be induced from @xmath2-divergence ,",
    "is shown to be super robust ( fujisawa and eguchi , 2008 ) against data contamination .",
    "it is our aim in this paper to propose a unified ica inference procedure by minimum divergence estimation .",
    "moreover , due to the property of super robustness , we will focus on the case of @xmath1-divergence and propose a robust ica procedure , called @xmath1-ica .",
    "hyvrinen , karhnen and oja ( 2001 ) have provided a sufficient condition to ensure the validity of mle - ica under the orthogonality constraint of @xmath35 , in the sense of being able to recover all independent components .",
    "amari , chen , and cichocki ( 1997 ) studied necessary and sufficient conditions for consistency under a different constraint of @xmath35 , and this consistency result is further extended by mihoko and eguchi ( 2002 ) to the case of @xmath2-ica . in this work , we also derive necessary and sufficient conditions for the consistency of @xmath1-ica . in the limiting case @xmath76 ,",
    "our necessary and sufficient condition for the consistency of mle - ica is weaker than the condition stated in hyvrinen , karhnen and oja ( 2001 ) . to the best of our knowledge",
    ", this result is not explored in existing literature .",
    "some notation is defined here for the convenience of reference . for any @xmath77 ,",
    "let @xmath78 be the commutation matrix such that @xmath79 ; @xmath80 ( resp .",
    "@xmath81 ) means @xmath18 is strictly positive ( resp .",
    "negative ) definite ; and @xmath82 is the matrix exponential . note that @xmath83 for any nonsingular square matrix @xmath18 . for a lower triangular matrix @xmath18 with 0 diagonals",
    ", @xmath84 stacks the nonzero elements of the columns of @xmath18 into a vector with length @xmath85 .",
    "there exist matrices @xmath86 and @xmath87 such that @xmath88 and @xmath89 .",
    "each column vector of @xmath90 is of the form @xmath91 , @xmath92 , where @xmath93 is a vector with a one in the @xmath94-th position and 0 elsewhere , and @xmath95 is the kronecker product . @xmath21 is the identity matrix and @xmath96 is the @xmath97-vector of ones .",
    "the rest of this paper is organized as follows .",
    "a unified framework for ica estimation by minimum divergence is introduced in section  2 .",
    "a robust @xmath1-ica procedure is developed in section  3 , wherein the related statistical properties are studied . a geometrical implementation algorithm for @xmath1-ica",
    "is further illustrated in section  4 . in section  5 ,",
    "the issue of selecting @xmath1 value is discussed .",
    "numerical studies are conducted in section  6 to demonstrate the robustness of @xmath1-ica .",
    "the paper is ended with a conclusion in section  7 .",
    "all the proofs are placed in appendix .",
    "in this section we introduce a general framework for ica by means of a minimum @xmath0-divergence , which covers the existing methods reviewed in section  1 .",
    "the aim of ica is to search a matrix @xmath33 so that the joint probability density function @xmath36 for @xmath54 is as close to marginal product @xmath99 as possible .",
    "this aim then motivates estimating @xmath35 by minimizing a distance metric between @xmath36 and @xmath99 .",
    "a general estimation scheme for @xmath35 can be formulated through the following minimization problem @xmath100 where @xmath101 is a divergence function .",
    "different choices of @xmath102 will lead to different estimation criteria for ica . here",
    "we will consider a general class of divergence functions , the @xmath0-divergence ( murata et al . , 2004 ; eguchi , 2009 ) , as described below .",
    "the @xmath0-divergence is a very general class of divergence functions .",
    "consider a strictly convex function @xmath103 defined on @xmath104 , or on some interval of @xmath104 where @xmath103 is well - defined .",
    "let @xmath105 be the inverse function of @xmath106 .",
    "consider @xmath107 which defines a mapping from @xmath108 to @xmath109 , where @xmath110 .",
    "define the @xmath0-cross entropy by @xmath111 and the @xmath0-entropy by @xmath112 .",
    "then the @xmath0-divergence can be written as @xmath113 in the subsequent subsections , we will introduce some special cases of @xmath0-divergence , which will lead to specific methods of ica .      by taking the @xmath114 pair @xmath115",
    "the corresponding @xmath0-divergence is equivalent to the kl - divergence @xmath116 . in this case",
    ", it can be deduced that @xmath117 where @xmath48 is the mutual information defined in ( [ mi ] ) .",
    "as described in section  1 that @xmath118 we conclude that the following criteria , minimum mutual information , maximum negentropy , and fast - ica , are all special cases of ( [ div_ica ] ) . on the other hand , observe that @xmath119 where @xmath120 is the joint probability density function of @xmath11 .",
    "if we consider the model @xmath121 , and if we estimate @xmath120 by its empirical probability mass function @xmath69 , minimizing ( [ mutual_mle ] ) is equivalent to mle - ica in ( [ mle_ica_z ] ) . in summary , choosing the kl - divergence @xmath116 covers minimum mutual information , maximum negentropy , fast - ica , and mle - ica .",
    "consider the convex set @xmath123 .",
    "take the @xmath114 pair @xmath124 the resulting @xmath0-divergence defined on @xmath125 is calculated to be @xmath126 which is called @xmath2-divergence ( mihoko and eguchi , 2002 ) , or density power divergence ( basu et al . , 1998 ) .",
    "note that @xmath127 if and only if @xmath128 for some @xmath129 . in the limiting case @xmath130 ,",
    "it gives the kl - divergence .",
    "if we replace @xmath131 in ( [ mle_ica_z ] ) by @xmath132 , it gives the @xmath2-ica of mihoko and eguchi ( 2002 ) .",
    "the @xmath1-divergence can be obtained from @xmath2-divergence through a @xmath0-volume normalization , @xmath134 where @xmath135 is defined the same way as ( [ beta_div ] ) with the plug - in @xmath136 , and where @xmath137 is some normalizing constant .",
    "here we adopt the following normalization , called the volume - mass - one normalization , @xmath138 it leads to @xmath139 .",
    "then , @xmath140 it can be seen that @xmath1-divergence is scale invariant .",
    "moreover , @xmath141 if and only if @xmath128 for some @xmath129 .",
    "the @xmath1-divergence , indexed by a power parameter @xmath1 , is a generalization of kl - divergence . in the limiting case @xmath142",
    ", it gives the kl - divergence .",
    "it is well known that mle ( based on minimum kl - divergence ) is not robust to outliers . on the other hand ,",
    "the minimum @xmath1-divergence estimation is shown to be super robust ( fujisawa and eguchi , 2008 ) against data contamination .",
    "hence , we will adopt @xmath1-divergence to propose our robust @xmath1-ica procedure .",
    "in particular , the main idea of @xmath1-ica is to replace @xmath131 in ( [ mle_ica_z ] ) by @xmath143 .",
    "though the idea is straightforward , there are many issues need to be studied .",
    "detailed inference procedure and statistical properties of @xmath1-ica are discussed in section  3 .",
    "the ica is actually a two - stage process .",
    "first , we need to whiten the data .",
    "the whitened data are then used for the recovery of independent sources . since the main purpose of this study is to develop a robust ica inference procedure ,",
    "the robustness for both data prewhitening and independent source recovery should be guaranteed . here",
    "we will utilize the @xmath1-divergence to introduce a robust prewhitening method called @xmath1-prewhitening , followed by illustrating @xmath1-ica based on the prewhitened data . in practice , the @xmath1 value for @xmath1-divergence should also be determined . in the rest of discussion",
    ", we will assume @xmath1 is given , and leave the discussion of its selection to section  [ sec.select_gamma ] .",
    "although prewhitening is always possible by a straightforward standardization of @xmath9 , there exists the issue of robustness of such a whitening procedure .",
    "it is well known that empirical moment estimates of @xmath145 are very sensitive to outliers . in mollah ,",
    "eguchi and minami ( 2007 ) , the authors proposed a robust @xmath2-prewhitening procedure .",
    "in particular , let @xmath146 be the probability density function of @xmath97-variate normal distribution with mean @xmath147 and covariance @xmath148 , and let @xmath149 be the empirical distribution based on data @xmath150 . with a given @xmath2 , mollah et al .",
    "( 2007 ) proposed the following minimum @xmath2-divergence estimators @xmath151 and then suggested to use @xmath152 for whitening the data .",
    "interestingly , @xmath152 can also be derived from the minimum @xmath1-divergence as @xmath153 at the stationarity of ( [ gamma.prewiten.obj ] ) , the solutions @xmath154 will satisfy @xmath155 where @xmath156 the robustness property of @xmath152 can be found in mollah et al .",
    "we call the prewhitening procedure @xmath157 the @xmath1-prewhitening .",
    "the whitened data @xmath32 then enter the @xmath1-ica estimation procedure .",
    "we are now in the position to develop our @xmath1-ica based on the @xmath1-prewhitened data @xmath32 . as discussed in section  [ sec.gamma.div ] , the @xmath35 estimator is derived from @xmath158 where @xmath159 and @xmath64 is the working model for @xmath38 . since @xmath33 , @xmath160 which does not involve @xmath35 .",
    "thus , @xmath161 can be equivalently obtained via @xmath162 finally , the mixing matrix @xmath163 is estimated by @xmath164 .",
    "let @xmath165 and @xmath166^\\top,\\;\\ ; { \\rm where}\\;\\;\\phi_j(y)=\\frac{d}{dy}\\ln f_j(y).\\ ] ] we have the following proposition .",
    "[ prop.stationarity ] at the stationarity , the maximizer @xmath167 defined in ( [ gamma_ica_z0 ] ) will satisfy @xmath168^\\top-\\phi(\\widehat w^\\top z_i)\\left[\\widehat w^\\top z_i\\right]^\\top\\right\\}=0.\\end{aligned}\\ ] ]    from proposition  [ prop.stationarity ] , it can be easily seen the robustness nature of @xmath1-ica : the stationary equation is a weighted sum with the weight function @xmath169 .",
    "when @xmath170 , an outlier with extreme value will contribute less to the stationary equation . in the limiting case of @xmath171 , which corresponds to mle - ica , the weight @xmath169 becomes uniform and , hence , is not robust .",
    "a critical point to the likelihood - based ica method is to specify a working model @xmath64 for @xmath38 .",
    "a sufficient condition to ensure the consistency of mle - ica can be found in hyvrinen , karhnen and oja ( 2001 ) .",
    "here the ica consistency means recovery consistency .",
    "that is , an ica procedure is said to be recovery consistent if it is able to recover all the independent components .",
    "note that the consistency of mle - ica does not rely on the correct specification of working model @xmath64 , but only on the positivity of @xmath172 $ ] , @xmath173 .",
    "this subsection aims to investigate the consistency of @xmath1-ica for @xmath174 $ ] , where @xmath175 is some constant .",
    "we will deduce necessary and sufficient conditions such that @xmath1-ica is recovery consistent .",
    "the main result is summarized below .",
    "[ thm.consistency ] assume the ica model  ( [ ica.z ] ) .",
    "assume the existence of @xmath176 for @xmath177 $ ] such that    1 .",
    "@xmath178=0\\,$ ] for all @xmath179 and all @xmath173 .",
    "then , for @xmath179 , the associated @xmath1-ica is recovery consistent if and only if @xmath180 where @xmath181 , @xmath182 , @xmath183 $ ] , @xmath184 , @xmath185 $ ] , and @xmath186 $ ] .    condition  ( a ) of theorem  [ thm.consistency ] can be treated as a weighted version of @xmath187 .",
    "it is satisfied when @xmath188 is symmetrically distributed about zero , and when the model probability density function @xmath64 is an even function .",
    "we believe condition  ( a ) is not restrictive and should be approximately valid in practice .",
    "notice that @xmath189 .",
    "thus , for the validity of ( [ hessian_tangent ] ) , we must require that @xmath190 , and the effect of @xmath191 can be exceeded by @xmath190 .",
    "fortunately , due to the coefficient @xmath192 , when @xmath1 is small , the effect of @xmath193 will eventually outnumber the effect of @xmath194 , so that @xmath195 can be ensured .",
    "in this situation , the negative definiteness of @xmath196 mainly relies on the structure of @xmath197 .",
    "moreover , a direct calculation gives @xmath198 to be a diagonal matrix with diagonal elements @xmath199 .",
    "we thus have the following corollary .    [ cor.consistency ]",
    "assume the ica model  ( [ ica.z ] ) . assume the existence of a small enough @xmath176 for @xmath177 $ ] such that    1 .",
    "@xmath178=0 $ ] for @xmath179 , @xmath173 .",
    "@xmath200 + e[f^\\gamma(s)\\{\\phi_k(s_k)s_k-\\phi_k'(s_k)s_j^2\\}]>0 $ ] for @xmath179 , for all pairs @xmath201 , @xmath202 .",
    "then , for every @xmath179 , the associated @xmath1-ica can recover all independent components .    to understand the meaning of condition  ( b )",
    ", we first consider an implication of corollary  [ cor.consistency ] in the limiting case of @xmath171 , which corresponds to the mle - ica . in this case ,",
    "condition  ( a ) becomes @xmath187 , which is automatically true by the model assumption of @xmath16 .",
    "moreover , since @xmath203 , condition  ( b ) becomes @xmath204 + e[\\phi_k(s_k)s_k-\\phi_k'(s_k)]>0,\\quad \\mbox{for all pairs $ ( j , k)$ , $ j\\neq k$}.\\label{consistency.mle}\\end{aligned}\\ ] ] a sufficient condition to ensure the validity of ( [ consistency.mle ] ) is @xmath205>0,\\quad\\forall j , \\label{consistency.mle2}\\ ] ] which is the same condition given in theorem  9.1 of hyvrinen , karhnen and oja ( 2001 ) for the consistency of mle - ica .",
    "we should note that ( [ consistency.mle ] ) is a weaker condition than ( [ consistency.mle2 ] ) .",
    "in fact , from the proof of theorem  [ thm.consistency ] , ( [ consistency.mle ] ) is also a necessary condition .",
    "one implication of ( [ consistency.mle ] ) is that , we can have at most one @xmath64 to be wrongly specified or at most one gaussian component involved , and mle - ica is still able to recover all independent components .",
    "this can also be intuitively understood that once we have determined @xmath206 directions in @xmath207 , the last direction is automatically determined . however",
    ", this fact can not be observed from ( [ consistency.mle2 ] ) which requires all @xmath64 to be correctly specified .",
    "we summarize the result for mle - ica below .",
    "assume the ica model  ( [ ica.z ] ) .",
    "then , mle - ica is recovery consistent if and only if @xmath208 + e[\\phi_k(s_k)s_k-\\phi_k'(s_k)]>0 $ ] for all pairs @xmath201 , @xmath202 .    turning to the case of @xmath1-ica , condition  ( b ) of corollary  [ cor.consistency",
    "] can be treated as a weighted version of ( [ consistency.mle ] ) with the weight function @xmath169 .",
    "however , one should notice that the validity of @xmath1-ica has nothing to do with that of mle - ica , since there is no direct relationship between condition  ( b ) and its limiting case ( [ consistency.mle ] ) .",
    "in particular , even if ( [ consistency.mle ] ) is violated ( i.e. , mle - ica fails ) , with a proper choice of @xmath1 , it is still possible that condition  ( b ) holds and , hence , the recovery consistency of @xmath1-ica can be guaranteed .    by theorem  [ thm.consistency ] , a valid @xmath1-ica procedure must correspond to @xmath195 , or equivalently , the maximum eigenvalue of @xmath196 , denoted by @xmath209 , must be negative .",
    "how should one pick a @xmath210-interval so that @xmath179 is legitimate in the sense that @xmath211 ? our suggestion for a rule of thumb is as follows .",
    "let @xmath212 be the empirical estimator of @xmath196 based on the estimated source @xmath213 , where @xmath214 .",
    "the plot of @xmath215 then provides a guidance in determining @xmath210 , over which @xmath216 should be far away below zero . with the @xmath210-interval , a further selection procedure , introduced in section  [ sec.select_gamma ] ,",
    "can be applied to select an optimal @xmath1 value from @xmath210 .",
    "it is confirmed in our numerical study in section  6 that , the interval @xmath210 , where @xmath217 , is quite wide , and the suggested rule does provide adequate choice of @xmath210 .",
    "it also implies that the choice of @xmath176 in corollary  [ cor.consistency ] is not critical , as @xmath176 is allowed to vary in a wide range and not limited to very small number .",
    "it is the condition  ( b ) that plays the most important role to ensure the recovery consistency of @xmath1-ica .      by using @xmath2-divergence ,",
    "mihoko and eguchi ( 2002 ) proposed @xmath2-ica to recover independent components .",
    "the objective function of @xmath2-ica being maximized is of the form @xmath218 where @xmath219 is a known constant .",
    "if we restrict @xmath33 , then @xmath220 and maximizing ( [ div.beta ] ) is equivalent to maximizing @xmath221 , which has the same form with the population objective function of @xmath1-ica in ( [ gamma_ica_z0 ] ) .",
    "we should emphasize that mihoko and eguchi ( 2002 ) considered the ica problem under the original @xmath9-scale , while the constraint @xmath33 is a consequence of prewhitening . without considering the constraint @xmath33 ,",
    "the objective function of @xmath1-ica is deduced to be @xmath222 which is different from ( [ div.beta ] ) .",
    "however , ( [ div.beta ] ) is similar to ( [ div.gamma ] ) when @xmath219 is small .",
    "this fact also confirms the observation of mihoko and eguchi ( 2002 ) that setting @xmath223 does not affect the performance of @xmath2-ica . in summary ,",
    "@xmath1-ica and @xmath2-ica based on the whitened data @xmath11 are equivalent . for data @xmath9 in original scale ,",
    "however , @xmath1-ica maximizing ( [ div.gamma ] ) is different from @xmath2-ica maximizing ( [ div.beta ] ) , but they will have similar performance for small @xmath2 .",
    "in this section , we introduce an algorithm for estimating @xmath35 constrained to the special orthogonal group @xmath30 , which is a lie group and is endowed with a manifold structure .",
    "is a lie group if the group operations @xmath225 defined by @xmath226 and @xmath227 defined by @xmath228 are both @xmath229 mappings ( boothby , 1986 ) . ]",
    "the lie group @xmath30 , which is a path - connected subgroup of @xmath230 , consists of all orthogonal matrices in @xmath25 with determinant one .",
    "is that @xmath230 itself is not connected . in the case that the desired orthogonal matrix @xmath35 has determinant @xmath231 , our algorithm in fact searches for @xmath232 for some permutation matrix @xmath74 with @xmath233 . ]",
    "recall @xmath234 being the objective function of @xmath1-ica maximization problem defined in ( [ gamma_ica_z0 ] ) .",
    "a desirable algorithm is to generate an increasing sequence @xmath235 with @xmath236 , such that @xmath237 converges to a local maximizer @xmath238 of @xmath234 .",
    "various approaches can be used to generate such a sequence @xmath237 in @xmath30 , for instance , geodesic flows and quasi - geodesic flows ( nishimori and akaho , 2005 ) . here",
    "we focus on geodesic flows on @xmath30 . in particular , starting with the current @xmath239 , the update @xmath240 is selected from one geodesic path of @xmath239 along the steepest ascent direction such that @xmath241 . in fact , this approach has been applied to the general stiefel manifold ( nishimori and akaho , 2005 ) .",
    "below we briefly review the idea and then introduce our implementation algorithm for @xmath1-ica .",
    "we note that the proposed algorithm is also applicable to mle - ica by changing the corresponding objective function .",
    "let @xmath242 denote the tangent space of @xmath30 at @xmath35 .",
    "consider a smooth path @xmath243 on @xmath30 with @xmath244 .",
    "differentiating @xmath245 yields the tangent space at @xmath35 @xmath246 clearly , @xmath247 is the set of all skew - symmetric matrices .",
    "each geodesic path starting from @xmath248 has an intimate relation with the matrix exponential function .",
    "in fact , @xmath249 if and only if @xmath250 is skew - symmetric ( see page 148 in boothby , 1986 ; proposition  9.2.5 . in marsden and ratiu , 1998 ) . moreover , for any @xmath251 , there exists ( not unique ) a skew - symmetric @xmath250 such that @xmath252 .",
    "if the killing metric ( nishimori and akaho , 2005 ) @xmath253 is used , the geodesic path starting from @xmath248 in the direction @xmath250 is given by @xmath254    since the lie group is homogeneous , we can compute the gradient and geodesic at @xmath255 by pulling them back to the identity @xmath248 and then transform back to @xmath239 . in the implementation algorithm , to ensure all the iterations lying on the manifold @xmath30 , we update @xmath240 through @xmath256 where the skew - symmetric matrix @xmath257 and the step size @xmath258 are chosen properly to meet the ascending condition @xmath241 . since , from ( [ geodesic.ip ] ) , @xmath259 lies on the geodesic path of @xmath248 , then @xmath260 must lie on the geodesic path of @xmath239",
    ". moreover , since @xmath261 by @xmath262 , the sequence in ( [ w.update ] ) satisfies @xmath263 for all @xmath264 .",
    "the determination of the gradient direction @xmath257 and the step size @xmath258 is discussed below .    to compute the gradient and geodesic at @xmath239 by pulling them back to @xmath248 , define @xmath265",
    "we then determine @xmath260 from one geodesic at @xmath248 in the direction of the projected gradient of @xmath266 .",
    "specifically , to ensure the ascending condition , we choose each skew - symmetric @xmath257 to be @xmath267 , the projected gradient of @xmath266 at @xmath248 , defined to be @xmath268^\\top-\\phi(w_k^\\top z_i)\\left[w_k^\\top z_i\\right]^\\top\\right\\}.\\end{aligned}\\ ] ] this particular choice of @xmath257 ensures the existence of the step size @xmath258 for the ascending condition .",
    "note that in the case of @xmath30 imposed with the killing metric , the projected gradient coincides with the natural gradient introduced by amari ( 1998 ) .",
    "see also fact  5 in nishimori and akaho ( 2005 ) for further details . as to the selection of the step size @xmath258 at each iteration",
    "@xmath264 with @xmath239 and @xmath269 , we propose to select @xmath258 such that @xmath270 is the  first improved rotation \" . in particular , we consider @xmath271 for some @xmath272 and @xmath273 , where @xmath274 is the nonnegative integer .",
    "to proceed , we search @xmath274 such that @xmath275 and then update @xmath276 . in our implementation , @xmath277 and @xmath278 are used . for the convergence issue",
    ", one can instead consider the armijo rule for @xmath258 ( given in equation  ( [ armijo ] ) ) .",
    "our experiments show that the above  first improved rotation \" rule works quite well .",
    "lastly , in the implementation , to save the storage for @xmath239 , we  rotate @xmath279 directly \" instead of manipulating @xmath35 , where @xmath279 is the @xmath280 data matrix whose columns are @xmath281 , @xmath282 .",
    "that is , we use the update @xmath283 . to retrieve the matrix @xmath35",
    ", we simply do a matrix right division of the final @xmath279 and the initial @xmath279 .",
    "the algorithm for @xmath1-ica based on gradient ascend on @xmath30 is summarized below .",
    "+    ' '' ''    1 .",
    "initialization : @xmath277 , @xmath278 , prewhitened data @xmath284 ( @xmath285 matrix ) .",
    "2 .   for each iteration @xmath286 ,",
    "* compute the skew - symmetric matrix @xmath257 in ( [ v ] ) . * for @xmath287 , if @xmath288 , then break the loop .",
    "* update @xmath289 by @xmath290 .",
    "check the convergence criterion .",
    "if the criterion is not met , go back to ( i ) .",
    "3 .   output @xmath291 .    ' '' ''     + finally , we would like to mention the convergence issue .",
    "the statement is similar to proposition  1.2.1 of bertsekas ( 2003 ) .",
    "[ thm.armijo ] let @xmath234 be continuously differentiable on @xmath30 , and @xmath292 be defined in ( [ f ] ) .",
    "let @xmath293 be a sequence generated by @xmath294 , where @xmath257 is a projected gradient related ( see ( [ projected ] ) below ) and @xmath258 is a properly chosen step size by the armijo rule : reduce the step size @xmath295 , @xmath296 until the inequality holds for the first nonnegative @xmath274 , @xmath297 where @xmath298 is a fixed constant .",
    "then , every limit point @xmath299 of @xmath293 is a stationary point , i.e. , @xmath300 for all @xmath301 , or equivalently , @xmath302 .",
    "the statement that @xmath257 is a projected gradient related corresponds to the condition @xmath303 this condition is true when @xmath257 is the projected gradient @xmath304 itself or some natural gradient @xmath305 ( theorem 1 , amari , 1998 ) , where @xmath18 is a riemannian metric tensor , which is positive definite .",
    "the estimation process of @xmath1-ica consists of two steps : @xmath1-prewhitening and the geometry - based estimation for @xmath35 , in which the values of @xmath1 are essential to have robust estimators .",
    "hence , we carefully select the value of @xmath1 based on the adaptive selection procedures proposed by minami and eguchi ( 2003 ) and mollah et al .",
    "we first introduce a general idea and then apply the idea to the selection of @xmath1 in both @xmath1-prewhitening and @xmath1-ica . define the measurement of generalization performance as @xmath306,\\ ] ] where @xmath307 is the underlying true joint probability density function of the data , @xmath308 is the considered model for fitting , @xmath309 is the minimum @xmath1-divergence estimator of @xmath310 , and @xmath311 is the empirical estimate of @xmath307 .",
    "the @xmath312 is called the anchor parameter and is fixed at @xmath313 throughout this paper .",
    "this value is empirically shown to be insensitive to the resultant estimators ( minami and eguchi , 2003 ) .",
    "let @xmath314 be the sample analogue of @xmath315 .",
    "we propose to select the value of @xmath1 over a predefined set @xmath210 through @xmath316 for @xmath1-prewhitening , @xmath317 and @xmath318 with @xmath319 . for @xmath1-ica , @xmath320 and @xmath321 with @xmath322 .",
    "the above selection criterion requires the estimation of @xmath315 . to avoid the problem of overfitting",
    ", we apply a @xmath323-fold cross - validation .",
    "let @xmath324 be the whole data , and let @xmath323 partitions of @xmath324 be @xmath325 , that is , @xmath326 if @xmath327 and @xmath328 . the whole selection procedure is summarized below",
    ". +    ' '' ''    1 .   for @xmath329 , 1 .",
    "for every @xmath179 , obtain @xmath330 , where @xmath331 is the empirical estimate of @xmath307 based on @xmath332 .",
    "2 .   compute the cross validation estimate @xmath333 , where @xmath334 is the empirical estimate of @xmath307 based on @xmath335 .",
    "estimate @xmath315 by @xmath336 and obtain @xmath337 .    ' '' ''",
    "+ eventually , we have two optimal values of @xmath1 : @xmath338 for @xmath1-prewhitening and @xmath339 for estimation of the recovering matrix @xmath35 .",
    "we conduct two numerical studies to demonstrate the robustness of the @xmath1-ica procedure . in the first study ,",
    "the data is generated from independent sources with some known distributions . in the second study ,",
    "we use transformations of lena images to form mixed image .",
    "we independently generate the two sources @xmath188 , @xmath340 , from a non - gaussian distribution with sample size @xmath341 .",
    "the observable @xmath9 is then given by @xmath342 , where @xmath343.\\end{aligned}\\ ] ] among the @xmath344 observations , we add to each of the last @xmath345 observations a random noise @xmath346 .",
    "the data thus contains @xmath347 uncontaminated i.i.d .",
    "observations from the ica model , @xmath342 , and @xmath345 contaminated i.i.d . observations from @xmath348 , where @xmath349 with @xmath350 and @xmath351 .",
    "we consider two situations for the independent source @xmath352 :    1 .",
    "uniform source : each @xmath188 , @xmath340 , is generated from uniform@xmath353 .",
    "student-@xmath354 source : each @xmath188 , @xmath340 , is generated from @xmath354-distribution with 3 degrees of freedom .",
    "for the case of uniform source , we use the sub - gaussian model @xmath355 with @xmath356 , which ensures the variance under @xmath64 is close to unity . as to the case of @xmath354 source , the super - gaussian model @xmath357 is considered , and we follow the suggested range of hyvrinen and oja ( 2000 ) and set @xmath358 .",
    "we also implement mle - ica ( using the geometrical algorithm introduced in section  4 ) and fast - ica ( using the code available at ` http://www.cis.hut.fi/projects/ica/fastica/ ` ) based on the @xmath1-prewhitened data for fair comparisons . to evaluate the performance of each method ,",
    "we modify from the performance index of parmar and unhelkar ( 2009 ) by a rescaling and by replacing the 2-norm with 1-norm and define the following performance index @xmath359 where @xmath360 is the @xmath361-th element of @xmath362 .",
    "we will expect @xmath74 to be a permutation matrix , when the method performs well .",
    "in that situation , the value of @xmath363 should be very close to 0 , and attains @xmath364 if @xmath74 is indeed a permutation matrix .",
    "simulation results with 100 replications are reported in figure  [ fig.sim ] .",
    "for the case of no outliers ( @xmath365 ) , all three methods perform well except the performance index @xmath363 of @xmath1-ica increases as @xmath1 increases .",
    "this is reasonable since , according to theorem  [ thm.consistency ] , @xmath1-ica may fail to apply when @xmath1 is too large .",
    "however , this influence is not severe as the the performance index @xmath363 is slightly increased only . as to the case of involving outliers ( @xmath366 ) ,",
    "it can be seen that the proposed @xmath1-prewhitening followed by @xmath1-ica does possess the advantage of robustness for a wide range of @xmath1 values , while the other two methods are not able to recover the latent sources .",
    "the performance of @xmath1-ica becomes worse when @xmath1 is small , since in the limiting case @xmath171 , @xmath1-ica reduces to the non - robust mle - ica .",
    "we note that both @xmath1-prewhitening and @xmath1-ica are critical .",
    "this can be seen from the poor performance of mle - ica and fast - ica , even they use the @xmath1-prewhitened data as the input .",
    "indeed , @xmath1-prewhitening only ensures that we shift and rotate the data in a robust manner , while the outliers will still enter into the subsequent estimation process and , hence , a non - robust result is expected . in figure  [ fig.sim.scatter ]",
    "we report the scatter plots of the recovered sources @xmath367 from each method , of @xmath9 , and of @xmath368 for one simulation run ( @xmath366 ) .",
    "these plots still convey the same message that @xmath1-ica is the winner among three methods , where the pattern of the reconstructed sources from @xmath1-ica is the most close to that of @xmath368 .",
    "we use the lena picture to evaluate the performance of @xmath1-ica . in our experiment",
    ", we use the lena image with @xmath369 pixels .",
    "we construct four types of lena as the latent independent sources @xmath16 as shown in figure  [ fig.lena ] .",
    "we randomly generate the mixing matrix to be @xmath370 , where the elements of @xmath371 are independently generated from uniform@xmath372 .",
    "the observed mixed pictures are also placed in figure  [ fig.lena ] , wherein about @xmath373 of the pixels are added with random noise generated from @xmath374 for contamination .",
    "the aim of this data analysis is to recover the original lena pictures based on the observed contaminated mixed pictures . in this analysis ,",
    "the pixels are treated as the random sample , each with dimension 4 .",
    "we randomly select @xmath375 pixels to estimate the demixing matrix , and then apply it to reconstruct the whole source pictures .",
    "we conduct two scenarios to evaluate the robustness of each method :    1 .   using the original image @xmath9 as the input ( see the second row of figure  [ fig.lena ] ) .",
    "2 .   using the filtered image @xmath376 from @xmath9 as the input ( see the third row of figure  [ fig.lena ] ) .",
    "the filtering process in the second scenario can be treated as a pre - processing to alleviate the influence of additive gaussian noise . in both scenarios ,",
    "the estimated demixing matrix is applied to the original images @xmath9 to recover @xmath16 .",
    "note that with gaussian noise contamination , conventional prewhitening by empirical moment estimators is not robust and , hence , both fast - ica and mle - ica may fail to apply .",
    "therefore , we prewhiten the data by @xmath1-prewhitening first and then apply @xmath1-ica , mle - ica , and fast - ica to the same whitened data for fair comparison .",
    "the plot @xmath377 introduced in the end of section  3.3 is placed in figure  [ fig.eig ] , which suggests that @xmath378 $ ] is a good candidate for possible @xmath1 values .",
    "we then apply the cross - validation method developed in section  5 to determine the optimal @xmath379 .",
    "the estimated values of @xmath314 are plotted in figure  [ fig.cv ] , from which we select @xmath380 for @xmath1-prewhitening and @xmath381 for @xmath1-ica .",
    "the recovered pictures are placed in figures  [ fig.lena_gamma_recover]-[fig.lena_fast_recover ] , where for each figure the first row is for scenario-1 and the second row is for scenario-2 .    it can be seen that @xmath1-ica is the best performer under both scenarios , while mle - ica and fast - ica can not recover the source images well when data is contaminated .",
    "it also demonstrates the applicability of the proposed @xmath1-selection procedure .",
    "we detect that mle - ica and fast - ica perform better when using filtered images @xmath376 , but can still not reconstruct images as good as @xmath1-ica does .",
    "interestingly , @xmath1-ica has a reverse performance , where the best reconstructed images are from the original images instead of the filtered ones .",
    "the filtering process , which aims to achieve robustness , replaces the original pixel value by the median of the pixel values over its neighborhood .",
    "therefore , while filtering process will alleviate the influence of outlier , it is also possible to lose useful information at the same time .",
    "for instance , a pixel without being contaminated will still be replaced by certain median value during the filtering process .",
    "@xmath1-ica , however , works on the original data @xmath9 that possesses all the information available , and then weights each pixel according to its observed value to achieve robustness .",
    "hence , a better performance for @xmath1-ica based on the original images is reasonably expected .",
    "in this paper , we introduce a unified estimation framework by means of minimum @xmath0-divergence . for the reason of robustness consideration , we further focus on the specific choice of @xmath1-divergence , which gives the proposed @xmath1-ica inference procedure .",
    "statistical properties are rigorously investigated . a geometrical algorithm based on gradient flows on orthogonal group",
    "is introduced to implement our @xmath1-ica .",
    "the performance of @xmath1-ica is evaluated through synthetic and real data examples .",
    "there are still many important issues that are not covered by this work .",
    "for example , we only consider full ica problem , i.e. , simultaneous extraction of all @xmath97 independent components , which is unpractical in the case of large @xmath97 .",
    "it is of interest to extend our current @xmath1-ica to partial @xmath1-ica .",
    "another issue of interest is also related to the large-@xmath97-small-@xmath344 scenario . in this work , data have to be prewhitened before entering the @xmath1-ica procedure .",
    "prewhitening can be very unstable especially when @xmath97 is large . how to avoid such a difficulty is an interesting and challenging issue .",
    "tensor data analysis is now becoming popular and attracts the attention of many researchers .",
    "many statistical methods include ica have been extended to deal with such a data structure by means of multilinear algebra techniques .",
    "an extension of @xmath1-ica to a multilinear setting to cover tensor data analysis is also of great interest for future study .",
    "eguchi , s. ( 2009 ) .",
    "information divergence geometry and the application to statistical machine learning . in _ information theory and statistical learning _ , f. emmert - streib and m. dehmer ( eds . ) , 309 - 332 .",
    "springer , berlin .",
    "parmar , s. d. and unhelkar , b. ( 2009 ) . performance analysis of ica algorithms against multiple - sources interference in biomedical systems .",
    "_ international journal of recent trends in engineering _ , 2 , 19 - 21 .          ,",
    "and for the true sources @xmath16 .",
    "in each plot , the red dots are observations without contamination , and the blue pluses are contaminated ones .",
    "( a)-(e ) : uniform source ( scenario-1 ) , ( f)-(j ) : @xmath354 source ( scenario-2 ) .",
    ", title=\"fig : \" ] , and for the true sources @xmath16 . in each plot , the red dots are observations without contamination , and the blue pluses are contaminated ones .",
    "( a)-(e ) : uniform source ( scenario-1 ) , ( f)-(j ) : @xmath354 source ( scenario-2 ) .",
    ", title=\"fig : \" ]       with @xmath313 for ( a ) @xmath1-prewhitening and ( b ) @xmath1-ica for the lena data analysis .",
    "the red dot indicates the place where the minimum value is attained.,title=\"fig:\",height=226 ]   with @xmath313 for ( a ) @xmath1-prewhitening and ( b ) @xmath1-ica for the lena data analysis . the red dot indicates the place where the minimum value is attained.,title=\"fig:\",height=226 ]"
  ],
  "abstract_text": [
    "<S> independent component analysis ( ica ) has been shown to be useful in many applications . however , most ica methods are sensitive to data contamination and outliers . in this article </S>",
    "<S> we introduce a general minimum @xmath0-divergence framework for ica , which covers some standard ica methods as special cases . within the @xmath0-family </S>",
    "<S> we further focus on the @xmath1-divergence due to its desirable property of super robustness , which gives the proposed method @xmath1-ica . </S>",
    "<S> statistical properties and technical conditions for the consistency of @xmath1-ica are rigorously studied . in the limiting case </S>",
    "<S> , it leads to a necessary and sufficient condition for the consistency of mle - ica . </S>",
    "<S> this necessary and sufficient condition is weaker than the condition known in the literature . </S>",
    "<S> since the parameter of interest in ica is an orthogonal matrix , a geometrical algorithm based on gradient flows on special orthogonal group is introduced to implement @xmath1-ica . </S>",
    "<S> furthermore , a data - driven selection for the @xmath1 value , which is critical to the achievement of @xmath1-ica , is developed . </S>",
    "<S> the performance , especially the robustness , of @xmath1-ica in comparison with standard ica methods is demonstrated through experimental studies using simulated data and image data . + * key words and phrases * : @xmath2-divergence ; @xmath1-divergence ; geodesic ; independent component analysis ; minimum divergence ; robust statistics ; special orthogonal group . </S>"
  ]
}