{
  "article_text": [
    "owing to the long - term efforts on distilling the explosive information on the web , several large scale knowledge bases ( kbs ) , such as wordnet @xcite , opencyc @xcite , yago @xcite and freebase @xcite , have already been built . despite the different domains these kbs serve for , nearly all of them concentrate on enriching _ entities _ and _ relations_. to facilitate storing , displaying and even retrieving knowledge",
    ", we use the highly structured form , i.e. ( _ head_entity , relation , tail_entity _ ) , for knowledge representation .",
    "each triplet is called a _",
    "fact_. so far , some general domain kbs , such as freebase and yago , have contained millions of entities and billions of facts , and they should have led a huge leap forward for some canonical ai - related tasks , e.g. _ question answering systems_. however , it is realised that this symbolic and atomic framework of representing knowledge makes it difficult to be utilized by most of ai - machines , especially of those which are dominated by statistical approaches .     +    recently , many ai - related fields , such as _ image classification _",
    "@xcite , _ natural language understanding _ @xcite and _ speech recognition _",
    "@xcite , have made significant progress by means of learning _ distributed representations_. taking an example of distributed representations of words applied to _ statistical language modeling _",
    "@xcite , it has achieved considerable success by grouping similar words which are close to each other in low - dimensional vector spaces",
    ". moreover , mikolov et al .",
    "@xcite discovered somewhat surprising patterns that the learnt word embeddings , to some extent , implicitly capture syntactic and semantic regularities in language .",
    "for example , the result of vector calculation @xmath3 is closer to @xmath4 than to any other words @xcite .    inspired by the idea of word vector calculation",
    ", we look forward to making knowledge computable . if we ideally consider the example mentioned above , the most probable reason @xmath5 ,",
    "capital_city_of _ is the relation between _ madrid _ and _",
    "spain _ , and so is _ paris _ and _ france_. in other words , we can conclude that :    * there are two facts / triplets , i.e. ( _ madrid _ , _ capital_city_of _ , _ spain _ ) and ( _ paris _ , _ capital_city_of _ , _ france _ ) . * we also derive the approximate equation that @xmath6 from @xmath5 .    the shared relation _ capital_city_of _ may help establish the approximate equation due to certain implicit connection . if we assume that the relation _",
    "capital_city_of _ can also be explicitly represented by a vector , i.e. @xmath7 , the connection will be @xmath8 . therefore , the fact ( _ madrid _ , _ capital_city_of _ , _ spain _ ) can be modeled in another way , i.e. @xmath9 .    generally speaking , this paper explores a better approach on knowledge representation by means of learning a unique low - dimensional embedding for each entity and relation in kbs , so that each fact ( _ head_entity , relation , tail_entity _ ) can be represented by a simple vector calculation @xmath0 . to achieve this goal",
    ", we contribute a generic model named _ large margin nearest neighbor embedding _",
    "( * lmnne * ) . as intuitively shown by figure 1 , * lmnne * follows the principle of _ pulling _ the positive tail entities @xmath10 close to @xmath11 , and simultaneously _ pushing _ the negative tail entities ( @xmath12 ) a large margin @xmath13 away from the positives @xmath10 .",
    "the details about model formulation are described in section 3 .",
    "section 4 presents the algorithm that solves * lmnne * efficiently based on _",
    "stochastic gradient descent _",
    "( * sgd * ) in iterative fashion . to prove both effectiveness and efficiency of the proposed model , we conduct quantitative experiments in section 5 , i.e. evaluating the performance of _ link prediction _ and _ triplet classification _ on several benchmark datasets .",
    "we also perform qualitative analysis via comparing model complexity among all related approaches in section 6 .",
    "results demonstrate that * lmnne * can achieve the state - of - the - art performance and demand for fewer computational resources compared with many prior arts .",
    "all the related studies work on studying better way of representing a fact / triplet .",
    "usually , they design various scoring functions @xmath14 to measure the plausibility of a triplet ( @xmath15 ) .",
    "the lower dissimilarity of the scoring function @xmath14 is , the higher compatibility of the triplet ( @xmath15 ) will be .",
    "* unstructured * @xcite is a naive model which just exploits the occurrence information of the head and the tail entities without considering the relation between them .",
    "it defines a scoring function @xmath16 , and this model obviously can not discriminate entity - pairs with different relations .",
    "therefore , * unstructured * is commonly regarded as the baseline approach .",
    "* distance model ( se ) * @xcite uses a pair of matrices @xmath17 , to characterize a relation @xmath18 .",
    "the dissimilarity of a triplet ( @xmath15 ) is calculate by the @xmath19 _ norm _ of @xmath20 . as pointed out by socher",
    "@xcite , the separating matrices @xmath21 and @xmath22 weaken the capability of capturing correlations between entities and corresponding relations , even though the model takes the relations into consideration .    *",
    "single layer model * proposed by socher et al .",
    "@xcite aims at alleviating the shortcomings of * distance model * by means of the nonlinearity of a single layer neural network @xmath23 , in which @xmath24 .",
    "then the linear output layer gives the scoring function : @xmath25 .",
    "* bilinear model * @xcite is another model that tries to fix the issue of weak interaction between the head and tail entities caused by * distance model * with a relation - specific bilinear form : @xmath26 .    *",
    "neural tensor network ( ntn ) * @xcite proposes a general scoring function : @xmath27 , which combines the * single layer model * and the * bilinear model*. this model is more expressive as the second - order correlations are also considered into the nonlinear transformation function , but the computational complexity is rather high .",
    "* transe * @xcite is the state - of - the - art method so far .",
    "differing from all the other prior arts , this approach embeds relations into the same vector space of entities by regarding the relation @xmath18 as a translation from @xmath28 to @xmath29 , i.e. @xmath30 .",
    "this model works well on the facts with one - to - one mapping property , as minimizing the global loss function will impose @xmath31 equaling to @xmath32 .",
    "however , the facts with multi - mapping properties , i.e many - to - many , many - to - one and one - to - many , impact the performance of the model . given a series of facts associated with a one - to - many relation @xmath18 , e.g. @xmath33 , * transe * tends to represent the embeddings of entities on many - side extremely the same with each other and hardly to be discriminated .",
    "moreover , the learning algorithm of * transe * only considers the randomly built negative triplets , which may bring in bias for embedding entities and relations .",
    "therefore , we propose a generic model ( * lmnne * ) in the subsequent section to tackle the margin - based knowledge embedding problem .",
    "this model can fully take advantage of both positive and negative triplets , and in addition , be flexible enough when dealing with the multi - mapping properties .",
    "given a triplet ( @xmath15 ) , we use the following formula as the scoring function @xmath14 to measure its plausibility , i.e. @xmath34 where @xmath35 stands for the generic distance metrics ( @xmath36 _ norm _ or @xmath37 _ norm _ ) depending on the model selection .    ideally , we look forward to learning a unique low - dimensional vector representation for each entity and relation , so that all the triplets in the kb will satisfy the equation @xmath38 .",
    "however , this can not be done perfectly because of the subsequent multi - mapping reasons ,    * not all entity pairs involve in only one relation .",
    "for example , ( _ barack obama , president_of , u.s.a _ ) and ( _ barack obama , born_in , u.s.a _ ) are both correct facts in the kb .",
    "however , we do not expect that the embedding of the relation _",
    "president_of _ is the same as the relation _",
    "born_in _ , since those relations are not semantically related .",
    "* besides the multi - relation property above , we may also face the problem of multiple head entities or tail entities when the other two elements are given .",
    "for example , there are at least five correct tail entities , i.e. _ comedy film , adventure film , science fiction , fantasy _ and _ satire _ , given ( _ wall - e , has_the_genre , ? _ ) .",
    "likewise , we do not desire that those tail entities share the same embedding .",
    "therefore , we suggest a ` soft ' way of modeling triplets in kbs .",
    "suppose that @xmath39 is the set of facts in the kb .",
    "for each triplet ( @xmath15 ) in @xmath39 , we build a set of reconstructed triplets @xmath40 by means of replacing the head or the tail with all the other entities in turn .",
    "@xmath40 can be divided into two sets , i.e. @xmath41 and @xmath42 .",
    "@xmath41 is the positive set of triplets reconstructed from ( @xmath15 ) as @xmath43 , and @xmath42 is the negative because @xmath44 . the intuitive goal of our model is to learn the embeddings of positive tail entities @xmath1 closer to @xmath11 than any other negative embeddings @xmath2 .",
    "therefore , the goal contains two objects , i.e. pulling the positive neighbors near ( * nn * ) each other while pushing the negatives a large margin ( * lm * ) away .",
    "specifically , for a pair of positive triplets , ( @xmath15 ) and ( @xmath45 ) , we _",
    "pull _ the head or the tail entities together ( left panel , figure 1 ) by minimizing the loss of variances in distance , i.e. @xmath46 simultaneously , we _",
    "push _ the negative head or tail entities away ( right panel , figure 1 ) via keeping all the negative distances @xmath47 at least one margin @xmath48 farther than @xmath49 .",
    "therefore , the objective function is , @xmath50_{+}.\\ ] ] @xmath51_{+}$ ] is the hinge loss function equivalent to @xmath52 , which can spot and update the negative triplets that violate the margin constrains .",
    "finally , we propose the _ large margin nearest neighbor embedding _ model which uses @xmath53 to control the trade - off between the pulling @xmath54 and pushing @xmath55 operations by setting the total loss @xmath56 as follows , @xmath57",
    "to efficiently search the optimal solution of * lmnne * , we use _ stochastic gradient descent _ ( sgd ) to update the embeddings of entities and relations in iterative fashion .",
    "as shown in algorithm 1 , we firstly initial all the entities and relations following a uniform distribution .",
    "each time we pick a triplet ( @xmath15 ) from @xmath39 , an accompanied triplet ( @xmath58 ) is sampled at the same time by replacing the head or the tail with another entity from the entity set @xmath59 , i.e. @xmath60",
    ". then we choose one of the pair - wise sgd - based updating options depending on which camp ( @xmath41 or @xmath61 ) that ( @xmath58 ) belongs to .",
    "+ training set @xmath62 , entity set @xmath59 , relation set @xmath63 ; dimension of embeddings @xmath64 , margin @xmath48 , learning rate @xmath65 and @xmath66 for @xmath54 and @xmath55 respectively , convergence threshold @xmath67 , maximum epoches @xmath68 and the trade - off @xmath53 .",
    "+ @xmath69 @xmath70    @xmath71 @xmath72 @xmath73    @xmath74 @xmath75 @xmath76     +   + all the embeddings of _ e _ and _ r _ , where @xmath77 and @xmath78 .",
    "embedding the knowledge into low - dimensional vector spaces makes it much easier for ai - related computing tasks , such as _ link prediction _ ( predicting @xmath29 given @xmath28 and @xmath18 or @xmath28 given @xmath18 and @xmath29 ) and _ triplet classification _ ( to discriminate whether a triplet @xmath79 is correct or wrong ) .",
    "two latest studies @xcite used subsets of wordnet ( * wn * ) and freebase ( * fb * ) data to evaluate their models and reported the performance on the two tasks respectively .    in order to conduct solid experiments , we compare our model ( * lmnne * ) with many related studies including state - of - the - art and baseline approaches involving in the two tasks , i.e. _ link prediction _ and _ triplet classification_. all the datasets , the source codes and the learnt embeddings for entities and relations can be downloaded from http://1drv.ms/1pupwzp .",
    "one of the benefits on knowledge embedding is that we can apply simple vector calculations to many reasoning tasks .",
    "for example , _ link prediction _ is a valuable task that contributes to completing the knowledge graph .",
    "specifically , it aims at predicting the missing entity or the relation given the other two elements in a mutilated triplet .    with the help of knowledge embeddings ,",
    "if we would like to tell whether the entity @xmath28 has the relation @xmath18 with the entity @xmath29 , we just need to calculate the distance between @xmath11 and @xmath32 .",
    "the closer they are , the more possibility the triplet ( @xmath15 ) exists .",
    "bordes et al .",
    "@xcite released two benchmark datasets which were extracted from wordnet ( * wn18 * ) and freebase ( * fb15k * ) .",
    "table 1 shows the statistics of these two datasets .",
    "the scale of * fb15k * dataset is larger than * wn18 * with much more relations but fewer entities .",
    ".statistics of the datasets used for link prediction task . [ cols=\"^,^,^,^\",options=\"header \" , ]     & @xmath80 +",
    "in addition to the quantitative experiments of evaluating the performance on _ link prediction _ and _ triplet classification _ with several benchmark datasets , we analytically compare the parameter complexity among the approaches that we have mentioned as well .",
    "table 6 lists the theoretical costs on representing triplets @xmath79 based on the scoring functions of nearly all the classical models . except for *",
    "unstructured * , * transe * and * lmnne * , the other approaches regard the relation @xmath18 as a transportation matrix serving for the entities @xmath28 and @xmath29 .",
    "these models need more resources on storing and computing embeddings . *",
    "unstructured * costs least , but does not contain any information on relations . * lmnne * and * transe * embed both entities and relations into the low - dimensional vector spaces from different aspects of observations : * lmnne * regards a relation as the inner connections of word embedding calculations , and * transe * considers it as a kind of translation from one entity to another . despite the varies angles of modeling , both of them are relatively efficient models for knowledge representation .",
    "knowledge embedding is an alternative way of representing knowledge besides displaying in triplets , i.e. ( @xmath15 ) .",
    "its essence is to learn a distributed representation for each entity and relation , to make the knowledge computable , e.g. @xmath81 .    to achieve higher quality of embeddings",
    ", we propose * lmnne * , a both effective and efficient model on learning a low - dimensional vector representation for each entity and relation in knowledge bases .",
    "some canonical tasks , such as _ link prediction _ and _ triplet classification _ , which were ever based on hand - made logic rules , can be truly facilitated by means of the simple vector calculation .",
    "the results of extensive experiments on several benchmark datasets and complexity analysis show that our model can achieve higher performance without sacrificing efficiency .    in the future",
    ", we look forward to paralleling the algorithm which can encode a whole kb with billion of facts , such as freebase and yago .",
    "another direction is that we can apply this new way of _ knowledge representation _ on reinforcing some other related studies , such as _ relation extraction _ from free texts and _ open question answering_.",
    "this work is supported by national program on key basic research project ( 973 program ) under grant 2013cb329304 , national science foundation of china ( nsfc ) under grant no.61373075 .",
    "the first author conducted this research while he was a joint - supervision ph.d .",
    "student in new york university .",
    "this paper is dedicated to all the members of the proteus project , and thanks so much to your help .",
    "f.  m. suchanek , g.  kasneci , and g.  weikum , `` yago : a core of semantic knowledge , '' in _",
    "16th international world wide web conference ( www 2007)_.1em plus 0.5em minus 0.4emnew york , ny , usa : acm press , 2007 .",
    "k.  bollacker , r.  cook , and p.  tufts , `` freebase : a shared database of structured general human knowledge , '' in _ aaai _ , vol .  7 , 2007 , pp . 19621963 .",
    "[ online ] .",
    "available : http://www.aaai.org/papers/aaai/2007/aaai07-355.pdf    d.  c. ciresan , u.  meier , and j.  schmidhuber , `` multi - column deep neural networks for image classification . '' in _",
    "cvpr_.1em plus 0.5em minus 0.4emieee , 2012 , pp . 36423649 .",
    "[ online ] .",
    "available : http://dblp.uni-trier.de/db/conf/cvpr/cvpr2012.html#ciresanms12",
    "h.  schwenk and j .- l .",
    "gauvain , `` connectionist language modeling for large vocabulary continuous speech recognition . '' in _",
    "icassp_.1em plus 0.5em minus 0.4emieee , 2002 , pp .",
    "[ online ] .",
    "available : http://dblp.uni-trier.de/db/conf/icassp/icassp2002.html#schwenkg02    y.  bengio , r.  ducharme , p.  vincent , and c.  janvin , `` a neural probabilistic language model . ''",
    "_ journal of machine learning research _ , vol .  3 , pp .",
    "11371155 , 2003 .",
    "[ online ] .",
    "available : http://dblp.uni-trier.de/db/journals/jmlr/jmlr3.html#bengiodvj03    t.  mikolov , k.  chen , g.  corrado , and j.  dean , `` efficient estimation of word representations in vector space , '' _ arxiv preprint arxiv:1301.3781 _ , 2013 .",
    "[ online ] .",
    "available : http://arxiv.org/abs/1301.3781    t.  mikolov , w.  tau yih , and g.  zweig , `` linguistic regularities in continuous space word representations . '' in _ hlt - naacl_.1em plus 0.5em minus 0.4emthe association for computational linguistics , 2013 , pp .",
    "[ online ] .",
    "available : http://dblp.uni-trier.de/db/conf/naacl/naacl2013.html#mikolovyz13    t.  mikolov , i.  sutskever , k.  chen , g.  s. corrado , and j.  dean , `` distributed representations of words and phrases and their compositionality , '' in _ advances in neural information processing systems 26 _ , c.  burges , l.  bottou , m.  welling , z.  ghahramani , and k.  weinberger , eds . , 2013 , pp .",
    "31113119 .",
    "a.  bordes , n.  usunier , a.  garcia - duran , j.  weston , and o.  yakhnenko , `` translating embeddings for modeling multi - relational data , '' in _ advances in neural information processing systems _ , 2013 , pp .",
    "27872795 .",
    "a.  bordes , j.  weston , r.  collobert , y.  bengio _ et  al .",
    "_ , `` learning structured embeddings of knowledge bases . '' in _ aaai _ , 2011 .",
    "[ online ] .",
    "available : http://www.aaai.org/ocs/index.php/aaai/aaai11/paper/viewpdfinterstitial/3659/3898          a.  bordes , n.  usunier , a.  garcia - duran , j.  weston , and o.  yakhnenko , `` irreflexive and hierarchical relations as translations , '' _ arxiv preprint arxiv:1304.7158 _ , 2013 .",
    "[ online ] .",
    "available : http://arxiv.org/abs/1304.7158    a.  bordes , x.  glorot , j.  weston , and y.  bengio , `` a semantic matching energy function for learning with multi - relational data , '' _ machine learning _ ,",
    "94 , no .  2 , pp . 233259 , 2014 .",
    "[ online ] .",
    "available : http://link.springer.com/article/10.1007/s10994-013-5363-6    m.  nickel , v.  tresp , and h .- p .",
    "kriegel , `` a three - way model for collective learning on multi - relational data , '' in _ proceedings of the 28th international conference on machine learning ( icml-11 ) _ , 2011 , pp .",
    "809816 .",
    "a.  bordes , x.  glorot , j.  weston , and y.  bengio , `` joint learning of words and meaning representations for open - text semantic parsing , '' in _ international conference on artificial intelligence and statistics _ , 2012 , pp ."
  ],
  "abstract_text": [
    "<S> traditional way of storing facts in triplets ( _ head_entity , relation , tail_entity _ ) , abbreviated as ( _ h , r , t _ ) , makes the knowledge intuitively displayed and easily acquired by mankind , but hardly computed or even reasoned by ai machines . </S>",
    "<S> inspired by the success in applying _ distributed representations _ to ai - related fields , recent studies expect to represent each entity and relation with a unique low - dimensional embedding , which is different from the symbolic and atomic framework of displaying knowledge in triplets . in this way , </S>",
    "<S> the knowledge computing and reasoning can be essentially facilitated by means of a simple _ vector calculation _ </S>",
    "<S> , i.e. @xmath0 . </S>",
    "<S> we thus contribute an effective model to learn better embeddings satisfying the formula by pulling the positive tail entities @xmath1 to get together and close to * h * + * r * ( _ nearest neighbor _ ) , and simultaneously pushing the negatives @xmath2 away from the positives @xmath1 via keeping a _ </S>",
    "<S> large margin_. we also design a corresponding learning algorithm to efficiently find the optimal solution based on _ </S>",
    "<S> stochastic gradient descent _ in iterative fashion . </S>",
    "<S> quantitative experiments illustrate that our approach can achieve the state - of - the - art performance , compared with several latest methods on some benchmark datasets for two classical applications , i.e. _ link prediction _ and _ triplet classification_. moreover , we analyze the parameter complexities among all the evaluated models , and analytical results indicate that our model needs fewer computational resources on outperforming the other methods . </S>"
  ]
}