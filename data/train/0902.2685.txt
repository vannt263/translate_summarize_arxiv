{
  "article_text": [
    "scientific communities are using a growing number of distributed systems , from local batch systems and community - specific services to generic , global grid infrastructures .",
    "users may debug applications using a desktop computer , then perform small - scale application testing using local resources and finally run at full - scale using globally distributed grids .",
    "sometimes new resources are made available to the users through systems previously unknown to them , and signficant effort may be required to gain familiarity with these systems interfaces and idiosyncracies .",
    "the time cost of mastering application configuration , tracking of computational tasks , archival and access to the results is prohibitive for the end - users if they are not supported by appropriate tools .",
    "is an easy - to - use frontend for the configuration , execution , and management of computational tasks .",
    "the implementation uses an object - oriented design in  @xcite .",
    "it started as a project to serve as a user interface for data analysis within the  @xcite and  @xcite experiments in high energy physics where large communities of physicists need access to resources for data mining and simulation tasks .",
    "a list of projects which supported the development of may be found in section [ sec : acknowledgements ] .",
    "provides a simple but flexible programming interface that can be used either interactively at the prompt , through a graphical user interface  ( gui ) or programmatically in scripts .",
    "the concept of a _ job _ component is essential as it contains the full description of a computational task , including : the code to execute ; input data for processing ; data produced by the application ; the specification of the required processing environment ; post - processing tasks ; and metadata for bookkeeping .",
    "the purpose of can then be seen as making it easy for a user to create , submit and monitor the progress of jobs",
    ". keeps track of all jobs and their status through a repository that archives all information between independent sessions .",
    "it is possible to switch between executing a job on a local computer and executing on the by changing a single parameter of a job object .",
    "this simplifies the progression from rapid prototyping on a local computer , to small - scale tests on a local batch system , to the analysis of a large dataset using resources .    in",
    ", the user has programmatic access through an application programming interface ( api ) , and has access to applications locally for quick turnaround during development .",
    "is a user- and application - oriented layer above existing job submission and management technologies , such as globus @xcite , condor @xcite , unicore @xcite or glite @xcite . rather than replacing the existing technologies , allows them to be used interchangeably , using a common interface as the interoperability layer .",
    "it is possible to make available to a user community with a high level of customisation .",
    "for example , an expert within a field can implement a custom application class describing the specific computational task .",
    "the class will encapsulate all low - level setup of the application , which is always the same , and only expose a few parameters for configuration of a particular task . the plugin system provided in means that this expert customisation will be integrated seamlessly with the core of at runtime , and can be used by an end - user to process tasks in a way that requires little knowledge about the interfaces of or batch systems .",
    "issues such as differences in data access between jobs executing locally and on the are similarly hidden .",
    "may be used as a job management system integrated into a larger system . in this case",
    "acts as a library for job submission and control . in particular , may be used as a building block for the implementation of portals which allow users access to functionality through their web browsers in a simplified way .",
    "these portals are normally domain specific and allow users of a distributed application to run it on the without needing to know much about tools .    is licensed under the gnu general public license and is available for download from the project website : http://www.cern.ch/ganga .",
    "the installation of is trivial and does not require privileged access or any server configuration .",
    "the installer script provides a self - contained package and most of the external dependencies are resolved automatically .",
    "however , generally does not attempt to install or batch submission tools or the application software . typically such software is installed and managed separately by system administrators .",
    "simple configuration files allow customisation and configuration of at the level of site , workgroup and user",
    ".    between january 2007 and december 2008 was used at 150 sites around the world , with 2000 unique users running about 250k sessions .    in this paper , we describe in section  [ sec : functionality ] the overall functionality , in section  [ sec : implementation ] details of the implementation , and in section  [ sec : mon ] how the progress of jobs is monitored .",
    "section  [ sec : gui ] gives an overview of the graphical user interface . in sections",
    "[ sec : usehep ] and  [ sec : other ] we discuss how is customised for specific user communities . interfacing and embedding in other frameworks is presented in section  [ sec : gangainotherframeworks ] . in appendix",
    "[ sec : examples ] we provide some examples of how the api in can be used .",
    "is a user - centric tool that allows easy interaction with heterogeneous computational environments , configuration of the applications and coherent organisation of jobs .",
    "functionality may be accessed by a user through any of several interfaces : a text - based command line in , a file - based scripting interface and a graphical user interface  ( gui ) .",
    "this reflects the different working styles in different user communities , and addresses various usage scenarios such as using the gui for training new users , the command line to exploit advanced use - cases , and scripting for automation of repetitive tasks . for sessions the current usage fractions are 55% , 40% and 5% respectively for interactive prompt , scripts and gui . as shown in fig .",
    "[ fig : gpi_architecture ] , the three user interfaces are built on top of the public interface  ( ) which in turn provides access to the core implementation .",
    "a job in is constructed from a set of components .",
    "all jobs are required to have an application component and a backend component , which define respectively the software to be run and the processing system to be used .",
    "many jobs also have input and output dataset components , specifying data to be read and produced .",
    "finally , computationally intensive jobs may have a splitter component , which provides a mechanism for dividing into independent subjobs , and a merger component , which allows for the aggregation of subjob outputs .",
    "the overall component structure of a job is illustrated in fig .",
    "[ fig : jobcomponents ] .        by default ,",
    "the exposes a simplified , top - level view suitable for most users in their everyday work , but at the same time allows for the details of underlying systems to be exposed if needed .",
    "an example interactive session using the gpi is given in appendix  [ sec : examples ] .",
    "prevents modification by the user of a submitted job .",
    "however , a copy of the job may easily be created and the copy can be modified . monitors the evolution of submitted jobs and categorises them into the simplified states _ submitted _ , _ running _ , _ completed _ , _ failed _ or _",
    "killed_.    all job objects are stored in a job repository database , and the input and output files associated with the jobs are stored in a file workspace . both the repository and the workspace may be in a local filesystem or on a remote server",
    ".    a large computational task may be split into a number of subjobs automatically according to user - defined criteria and the output merged at a later stage .",
    "each subjob will execute on its own and the merging of the output will take place when all have finalised .",
    "the submission of subjobs is automatically optimised if the backend component supports bulk job submission .",
    "for example , when submitting to the glite workload management system  @xcite the job collection mechanism is used transparently to the user .",
    "job splitting functionality provides a flat list of subjobs suitable for parallel processing of fully independent workloads .",
    "however , certain backends allow users to make use of more - sophisticated parallelisation schemes , for example the message passing interface ( mpi ) [ 8 ] . in this case",
    ", ganga may be used to manage collections of subjobs corresponding to mpi processes .",
    "the allows frequently used job configurations to be stored as ` templates ` , so that they may easily be reused , and allows jobs to be labelled and organised in a hierarchical ` jobtree ` .",
    "has built - in support for handling user credentials , including classic proxies , proxies with extensions for a virtual organisation management service ( voms )  @xcite , and kerberos  @xcite tokens for access to an andrew filesystem ( afs )  @xcite .",
    "a user may renew and destroy the credentials directly using the gpi .",
    "gives an early warning to a user if the credentials are about to expire .",
    "the minimum credential validity and other aspects of the credential management are fully configurable",
    ".     supports multiple security models . for local and batch backends ,",
    "the authentication and authorisation of the users is based on the local security infrastructure including user name and network authentication protocols such as kerberos .",
    "security infrastructure ( gsi ) @xcite provides for security across organizational boundaries for the backends .",
    "different security models are encapsulated in pluggable components , which may be simultaneously used in the same session .",
    "a ` robot ` has been implemented for repetitive use - cases .",
    "it is a script that periodically executes a series of actions in the context of a session .",
    "these actions are defined by implementations of an action interface . without programming",
    ", the driver can be configured using existing action implementations to submit saved jobs , wait for the jobs to complete , extract data about the jobs to an xml file , generate plain text or html summary reports , and email the reports to interested parties .",
    "custom actions can easily be added by either extending or aggregating the existing implementations or implementing the action interface directly , allowing for a diverse variety of repetitive use - cases .",
    "an example is given in section  [ sec : lhcb ] .",
    "details of the different kinds of component are given below , along with generic examples .",
    "more specialised components , designed for a particular problem domain , are considered in sections [ sec : usehep ] and [ sec : other ] .",
    "the application component describes the type of computational task to be performed .",
    "it allows the characteristics and settings of some piece of software to be defined , and provides methods specifying actions to be taken before and after a job is processed .",
    "the pre - processing ( configuration ) step typically involves examination of the application properties , and may derive secondary information .",
    "for example , intermediate configuration files for the application may be created automatically .",
    "the post - processing step can be useful for validation tasks such as determining the validity of the application output .",
    "the simplest application component ( ` executable ` ) has three properties :    ` exe : ` : :    the path to an executable binary or script ; ` args : ` : :    a list of arguments to be passed to the executable ; ` env : ` : :    a dictionary of environment variables and the values they should be    assigned before the executable is run .    the configuration method carries out integrity checks  for example ensuring that a value has been assigned to the ` exe ` property .",
    "a backend component contains parameters describing the behaviour of a processing system .",
    "the list of parameters can vary significantly from one system to another , but can include , for example , a queue name , a list of requested sites , the minimum memory needed and the processing time required .",
    "in addition , some parameters hold information that the system reports back to the user , for example the system - specific job identifier and status , and the machine where a job executed .",
    "a backend component provides methods for submitting jobs , and for cancelling jobs after submission , when this is needed .",
    "it also provides methods for updating information on job status , for retrieving output of completed jobs and for examining files produced while a job is running .",
    "backend components have been implemented for a range of widely used processing systems , including : local host , batch systems ( portable batch system ( pbs )  @xcite , load sharing facility ( lsf )  @xcite , sun grid engine ( sge )  @xcite , and condor  @xcite ) , and systems , for example based on glite  @xcite , arc  @xcite and osg  @xcite .",
    "remote backend component allows jobs to be launched directly on remote machines using ssh .",
    "as an example , the batch backend component defines a single property that may be set by the user :    ` queue  : ` : :    name of queue to which job should be submitted , the system default    queue being used if this left unspecified ,    and defines three properties for storing system information :    ` i d  : ` : :    job identifier ; ` status  : ` : :    status as reported by batch system ; ` actualqueue : ` : :    name of queue to which job has been submitted .",
    "in addition , a remote - backend component allows a job defined in a session running on one machine to be submitted to a processing system known to a remote machine to which the user has access .",
    "for example , a user who has accounts on two clusters may submit jobs to the batch system of each from a single machine .",
    "dataset components generally define properties that uniquely identify a particular collection of data , and provide methods for obtaining information about it , for example its location and size .",
    "the details of how data collections are described can vary significantly from one problem domain to another , and the only generic dataset component in represents a null ( empty ) dataset .",
    "other dataset components are specialised for use with a particular application , and so are discussed later .",
    "a strict distinction is made between the datasets and the sandbox ( job ) files .",
    "the former are the files or databases which are stored externally .",
    "the sandbox consists of files which are transferred from the user s filesystem together with the job .",
    "the sandbox mechanism is designed to handle small files ( typically up to 10 mb ) while the datasets may be arbitrarily large .",
    "splitter components allow the user to specify the number of subjobs to be created , and the way in which subjobs differ from one another . as an example , one splitter component ( ` argsplitter ` ) deals with executing the same task many times over , but changing the arguments of the application executable each time .",
    "it defines a single property :    args : : :    list of sets of arguments to be passed to an application .    specialised splitters deal with creating subjobs that process different parts of a dataset .",
    "merger components deal with combining the output of subjobs .",
    "typical output includes files containing data in a particular format , for example text strings or data representing histograms . as examples , one merger component ( ` textmerger ` ) concatenates the files of standard output and error returned by a set of subjobs , and another ( ` rootmerger ` ) sums histograms produced in root format  @xcite .",
    "merging may be automatically performed in the background when retrieves the job output or it may be controlled manually by the user .",
    "in this section we provide details of the actual implementation of some of the most important parts of .",
    "job components are implemented as plugin classes , imported by at start - up if enabled in a user configuration file .",
    "this means that users only see the components relevant to their specific area of work .",
    "plugins developed and maintained by the team are included in the main distribution and are upgraded automatically when a user installs a newer version .",
    "currently , the list includes around 15 generic plugins and around 20 plugins specific to and .",
    "plugins specific to other user communities need to be installed separately but could easily be integrated into the main distribution .",
    "plugin development is simplified by having a set of internal interfaces and a mechanism for generating proxy classes  @xcite .",
    "component classes inherit from an interface class , as seen in fig .",
    "[ fig : components ] .",
    "each plugin class defines a schema , which describes the plugin attributes , specifying type ( read - only , read - write , internal ) , visibility , associated user - convenience filters and syntax shortcuts .",
    "the user does not interact with the plugin class directly but rather with an automatically generated proxy class , visible in the .",
    "the proxy class only includes attributes defined as visible in the schema and methods selected for export in the plugin class .",
    "this separation of the plugin and proxy levels is very flexible . at the level ,",
    "the plugin implementation details are not visible ; all proxy classes follow the same design logic ( for example , copy - by - value ) ; persistence is automatic , session - level locking is transparent . in this way the low - level , internal api is separated from the user - level .",
    "the framework does not force developers to support all combinations of applications and backends , but only the ones that are meaningful or interesting . to manage this , the concept of a _ submission handler _ is introduced .",
    "the submission handler is a connector between the application and backend components . at submission time",
    ", it translates the internal representation of the application into a representation accepted by a specific backend .",
    "this strategy allows integration of inherently different backends and applications without forcing a lowest - common - denominator interface .",
    "most of the plugins interact with the underlying backends using shell commands .",
    "this down - to - earth approach is particularly useful for encapsulating the environments of different subsystems and avoiding environment clashes . in verbose mode , prints each command executed so that a user may reproduce the commands externally if needed .",
    "higher - level abstractions such as jsdl @xcite , ogsa - bes @xcite or saga api @xcite are not currently used , but specific backends that support these standards could readily be added .",
    "the _ job repository _",
    "provides job persistence in a simple database , so that any subsequent session has access to all previously defined jobs .",
    "once a job is defined in a session it is automatically saved in the database .",
    "the repository provides a bookkeeping system that can be used to select particular jobs according to job metadata .",
    "the metadata includes such parameters as job name , type of application , type of submission backend , and job status .",
    "it can readily be extended as required",
    ".    supports both a local and a remote repository . in the case of the former ,",
    "the database is stored in the local file system , providing a standalone solution . in the case of the latter ,",
    "the client accesses an amga  @xcite metadata server .",
    "the remote server supports secure connections with user authentication and authorisation based on certificates .",
    "performance tests of both the local and remote repositories show good scalability for up to 10 thousand jobs per user , with the average time of individual job creation being about 0.2 seconds .",
    "there is scope for further optimisation in this area by taking advantage of bulk operations and job loading on demand .",
    "the job repository also includes a mechanism to support schema migration , allowing for evolution in the schema of plugin components .",
    "stores job input and output files in a _ job workspace_. the current implementation uses the local file system , and has a simple interface that allows transparent access to job files within the framework . these files are stored for each job in a separate directory , with sub - directories for input and output and for each subjob .",
    "users may access the job files directly in the file - system or using commands such as ` job.peek ( ) ` .",
    "internally , handles the input and output files using a simple abstraction layer which allows for trivial integration of additional workspace implementations .",
    "tests with a prototype using a webdav  @xcite server have shown that all workspace data related to a job can be accessed from different locations . in this case ,",
    "a workspace cache remains available on the local file system .",
    "the combination of a remote workspace and a remote job repository effectively creates a roaming profile , where the same session can be accessed at multiple locations , similar to the situation for accessing e - mail messages on an imap  @xcite server .",
    "provides two types of monitoring : the internal monitoring updates the user with information on the progress of jobs , and the external monitoring deals with information from third - party services",
    ".      automatically keeps track of changes in job status , using a monitoring procedure designed to cope with varying backend response times and load capabilities . as seen in fig .",
    "[ fig : job_status_monitoring_mechanism ] , each backend is polled in a different thread taken from a pool , and there is an efficient mechanism to avoid deadlocks from backends that respond slowly .",
    "the poll rate may be set separately for each backend .",
    "the monitoring sub - system also keeps track of the remaining validity of authentication credentials , such as proxies and kerberos tokens .",
    "the user is notified that renewal is required , and if no action is taken then is placed in a state where operations requiring valid credentials are disabled .",
    "s external monitoring provides a mechanism for dynamically adding third - party monitoring sensors , to allow reporting of different metrics for running jobs .",
    "the monitoring sensors can be inserted both on the client side - where runs - and on the remote environment ( worker node ) where the application runs , allowing the user to follow the entire execution flow .",
    "monitoring events are generated at job submission time , at startup , periodically during execution , and at completion .",
    "individual application and backend components in can be configured to use different monitoring sensors , allowing collection of both generic execution information and application - specific data .",
    "use is currently made of two implementations of external monitoring sensors .",
    "one is the dashboard application monitoring  @xcite .",
    "another is a custom service that allows the user to examine job output in real - time on the .",
    "this streaming service is not enabled by default , but must be set up for each user community separately , and may then be requested by a user for specific jobs .",
    "the graphical user interface ( gui ) , shown in fig .",
    "[ fig : gui ] and built using pyqt3  @xcite , makes available all of the job - management functionality provided at the level of the public interface .",
    "the gui incorporates various convenience features , and its multi - threaded nature results in a degree of parallelism not possible at the command line : job monitoring and most job - management actions run concurrently , ensuring a good response time for the user .",
    "the job monitoring window takes centre stage , with job status and other monitored attributes displayed in table format .",
    "other features include subjob monitoring , subjob folding / hiding , a job - details display drawer , a logical - collections drawer , and a text - based job - search facility .",
    "many characteristics of the monitoring window can be customised , allowing , for example , selection of the job attributes to be monitored , and of the colours used to denote different job states .",
    "the construction of a job , entailing selection of the required plugins and the entry of attribute values , is achieved from a job - builder window .",
    "this displays a foldable tree of job attributes , and associated data - entry widgets .",
    "the tree and widgets are generated dynamically based on plugin schemas , ensuring that the gui automatically supports user - defined plugins without any change being needed to the gui code . to assist with data entry ,",
    "drop - down menus list allowed values , wherever these are defined ; and tool tips provide explanations of individual job attributes .",
    "the job - builder window also features tool buttons for performing a wide range of job - related actions , including creation , saving , copying , submission , termination and removal . finally , a multifunction ` extras ` tool button provides access to arbitrary additional functionality implemented in the plugins .",
    "the gui also has a scriptor window , providing a favourite - scripts collection , a job - script editor and an embedded session .",
    "the favourite - scripts collection allows frequently used scripts to be created , imported , exported and cloned ; the job - script editor facilitates quick modification and execution of scripts ; and the embedded session allows interactive use of commands .    finally , a scrollable log window collects and displays all messages generated by .",
    "the and experiments aim to make discoveries about the fundamental nature of the universe by detecting new particles at high energies , and by performing high - precision measurements of particle decays .",
    "the experiments are located at the large hadron collider  ( ) at the european laboratory for particle physics  ( cern ) , geneva , with first particle collisions ( events ) expected in 2009 .",
    "both experiments require processing of data volumes of the order of petabytes per year , rely on computing resources distributed across multiple locations , and exploit several implementations .",
    "the data - processing applications , including simulation , reconstruction and final analysis for the experiments , are based on the ` c++ ` /  @xcite framework .",
    "this provides core services , such as message logging , data access , histogramming , and a run - time configuration system .",
    "the data from the experiments will be distributed at computing facilities around the world .",
    "users performing data analysis need an on - demand access mechanism to allow rapid pre - filtering of data based on certain selection criteria so as to identify data of specific interest .",
    "the role of within and is to act as the interface for data analysis by a large number of individual physicists .",
    "also allows for the easy exchange of jobs between users , something that can otherwise be difficult because of the complex configuration of analysis jobs .",
    "the experiment is dedicated to studying the properties of _ b _ mesons ( particles containing the _ b _ quark ) and in this section we describe the way in which interacts with the application and backend plugins specific to .    in a typical analysis , users supply their own shared libraries , containing user - written classes , and these are loaded at run - time .",
    "the applications are driven by a configuration file , which includes definitions of the libraries to load , non - default values for object parameters , the input data to be read , and the output to be created .",
    "includes an application component for -based applications to simplify the task of performing an analysis . during the configuration stage , and before job submission",
    ", the application component undertakes the following tasks :    * it locally sets up the environment for the chosen application ; * it determines the user - owned shared libraries required to run the job ; * it parses the configuration file supplied , including all its dependencies ; * it uses information obtained from the configuration file to determine the input data required and the outputs expected ; * it registers the inputs and outputs with the submission backend .",
    "the user , then , only needs to specify the name and version of the application to run , and the configuration file to be used .",
    "code under development by a user may contain bugs that cause runtime errors during job execution .",
    "the transparent switching between processing systems when using means that debugging can be performed locally , with quick response time , before launching a large - scale analysis on the , where response times tend to be longer .    some studies in , rather than being based on ,",
    "are performed using the  @xcite framework , most notably studies that make use of simplified event simulations .",
    "jobs for these studies require large amounts of processing power , but do not require input data and produce only small amounts of output .",
    "this makes them very easy to deploy on the , with support in provided by a generic  @xcite application component .     in the computing model  @xcite",
    ", jobs are routed through the  @xcite workload management system  ( wms ) .",
    "is a pilot - based system where user jobs are queued in the wms server and the server submits generic pilot scripts to the grid .",
    "each pilot queries the wms for a job with resource requirements satisfied by the machine where the pilot script is running .",
    "if a compatible job is available , it is pulled from the wms and started .",
    "otherwise , the pilot terminates and the wms sends a new pilot to the .",
    "this system improves the reliability of the system as seen by the user . provides a backend component that supports submission of jobs to the wms , making use internally of s api  @xcite .",
    "a _ splitter _",
    "component implemented specifically for is able to divide the analysis of a large dataset into many smaller subjobs . during the splitting ,",
    "a file catalogue is queried to ensure that all data associated with an individual subjob is available in its entirety at a minimum of one location on the .",
    "this gives significant optimisation , as it avoids subjobs having to copy data across the network before an analysis can start .",
    "in total , above 300k user jobs finished successfully in 2008 with a total cpu consumption of 87 cpu years .",
    "the jobs ran at a total of 140 grid sites across the globe .",
    "the system was responsive to a highly irregular usage pattern and spikes of several thousand simultaneous jobs were observed during the year .",
    "this usage is expected to rise dramatically after the start of the data taking .",
    "the ` robot ` in is used within for _ end - to - end _ testing of the distributed analysis model .",
    "it submits a representative set of analysis jobs on a daily basis , monitors their progress , and checks the results produced .",
    "the overall success rate and the time to obtain the results is recorded and published on the web .",
    "the ` robot ` monitors this information , producing statistics on the long - term system performance .",
    "is a general - purpose experiment , designed to allow observation of new phenomena in high - energy proton - proton collisions .",
    "the distributed analysis model is part of the computing model  @xcite which requires that data are distributed at various computing sites , and user jobs are sent to the data .",
    "an analysis job typically consists of a or shell script that configures and runs user algorithms in the framework  @xcite , reads and writes event files , and fills histograms / n - tuples .",
    "more - interactive analysis may be performed on large datasets stored as n - tuples .",
    "there are several scenarios relevant for a user analysis .",
    "some analyses require a fast response time and a high level of user interaction , for which the parallel facility proof  @xcite is well suited .",
    "other analyses require a low level of user interaction , with long response times acceptable , and in these cases and processing are ideal .",
    "analysis jobs can produce large amounts of data , which may initially be stored at a single site , and may subsequently need to be transferred to other machines .",
    "this is supported in by the distributed data management system dq2  @xcite .",
    "this provides a set of services for moving data between -enabled computing facilities , and maintains a series of databases that track the data movements .",
    "the vast amounts of data involved are grouped into datasets , based on various criteria , for example physics characteristics , to make queries and retrievals more efficient .",
    "the experiment employs three infrastructures for user analysis and for collaboration - wide event simulation and reconstruction .",
    "these are the developed in the context of enabling grids for e - science ( egee , mainly europe )  @xcite , accessed using glite middleware  @xcite , the open science grid ( osg , mainly north america )  @xcite , accessed using the panda system  @xcite , and nordugrid ( mainly nordic countries )  @xcite , accessed using the middleware  @xcite .",
    "seamlessly submits jobs to all three flavours .",
    "a typical user analysis consists of an event - selection algorithm developed in the athena framework .",
    "large amounts of data are filtered to identify events that meet certain selection criteria .",
    "the events of interest are stored in files grouped together as datasets in the dq2 system .",
    "the components for athena jobs include the following functionality :    * during job submission , dq2 is queried for the file content and location of the dataset to be analysed .",
    "the number of possible sites is then restricted to the dataset locations . *",
    "a job can be divided into several subjobs , each processing a given number of files from the full dataset . * in a job , after the application has completed , the user output is stored on the storage element of the site where the job was run , and is registered in dq2 .    in the second half of 2008 , more than",
    "@xmath0 jobs were submitted through by users . following a procedure similar to that of",
    ", the ` robot ` submits test jobs daily to sites .",
    "test results are used to guide users to sites that are performing well , avoiding job failures on temporarily misconfigured sites .",
    "in addition to data analysis , users sometimes need to simulate event samples of the order of a few tens of thousands of events .",
    "the _ athenamc _",
    "application component has been developed to integrate software used in the official system for event simulation .",
    "this component consists of a set of classes that together handle input parameters , input datasets and output datasets for the three production steps : event generation , detector simulation , and event reconstruction . as in the case of user analysis ,",
    "datasets are managed by the dq2 system .",
    "offers a flexible and extensible interface that make it useful beyond the original scope of particle - physics applications in the and experiments . here",
    "we provide details of just a few of the other contexts in which has been used .",
    "imense ltd , a cambridge - based startup company , has implemented a novel image retrieval - system ( fig .",
    "[ fig : camtologytech ] ) , featuring automated analysis and recognition of image content , and an ontological query language .",
    "the proprietary image analysis , developed from published research  @xcite , includes recognition of visual properties , such as colour , texture and shape ; recognition of materials , such as grass or sky ; detection of objects , such as human faces , and determination of their characteristics ; and classification of scenes by content , for example beach , forest or sunset .",
    "the system uses semantic and linguistic relationships between terms to interpret user queries and retrieve relevant images on the basis of the analysis results . moreover , the system is extensible , so that additional image classification modules or image context and metadata can easily be integrated into the index .    by using the framework for job submission and management , it has been possible to port and deploy a large part of imense s image - analysis technology to the and build a searchable index for more than twenty - million high - resolution photographic images .",
    "the processing stages for the image - search system  image analysis and indexing  are intrinsically sequential .",
    "analysis has been parallelised at the level of single images or small subsets of images",
    ". each image can therefore be processed in isolation on the , with this processing usually taking a few to ten seconds . in order to minimise overheads ,",
    "images are grouped in sets of a few hundred per job submitted through .",
    "results of the image processing and analysis are passed back to the submission server once a job has successfully completed .",
    "support for imense has been added to through the implementation of two specialised components : an application component that deals with running the image - processing software , and a dataset component for taking care of the output . as usual with",
    ", the jobs can run both locally and on the , giving maximum flexibility .    at runtime ,",
    "images are retrieved and segmented one at a time , all of the images are classified , and finally an archive is created of the output files ( several per input image ) .",
    "the archive is returned using the sandbox mechanism in when using the ` local ` backend , and is uploaded to a storage element when using the ` lcg ` backend .",
    "the specialised dataset component provides methods for downloading a results archive from a storage element , and for unpacking an archive to a destination directory .",
    "these methods are invoked automatically by when an image - processing job completes : the effect for the user is that a list of images is submitted for processing and results are placed in the requested output location independently of the backend used .",
    "large user communities , such as and , profit from encapsulating shared use cases as specialised applications in .",
    "in contrast , individual researchers or developers in the context of rapid prototyping activities may opt to use generic application components . in such cases , still provides the benefits of bookkeeping and a programmatic interface for job submission . as an example of this way of working , a small community of experts in the design of gaseous detectors use to run the  @xcite simulation program on the .",
    "a script has been written that generates a chain of simulation jobs using the generator of macro files and s ` executable ` application component .",
    "the executables , and a few small input files , are placed in the input sandbox of each job .",
    "histograms and text output are then returned in the output sandbox .",
    "this simple approach allowed integration of jobs in in just a few hours .",
    "the open - plugin architecture of allows easy integration of additional middleware , as has been achieved , for example , with the ( advanced resource connector ) middleware  @xcite .",
    "this is a product of the nordugrid project  @xcite , and is used by many academic institutions in the nordic countries and elsewhere .",
    "jobs are accepted and brokered by a manager , running at site level , and resource lookup is done through load balancing and runtime environments advertised by individual sites . file storage and access is cloudy , meaning that all files registered in wide catalogues are accessible to all worker nodes .",
    "file transfers are handled by the manager , between job acceptance and execution .",
    "connected resources are used e.g. by researchers in bioinformatics , genomics , meteorology , in addition to high  energy physics .",
    "has been interfaced to through a backend , which translates input into readable xrsl language .",
    "the user client is lightweight , and binaries are provided as an external library at install time .",
    "the main user of this integration is the experiment ( see sec .  [",
    "sec : atlas ] ) , where it is the main user access portal to one of the experiment s three main computing grids .",
    "further collaboration between and is envisaged , to employ as a fully featured frontend to .",
    "the public interface constitutes an api for generic job submission and management . as a result , may be programmatically interfaced to other frameworks , and used as a convenient abstraction layer for job management .",
    "has been used in combination with  @xcite , a lightweight agent - based scheduling layer on top of the , in a number of scientific activities .",
    "these have included : dosimetry - related simulation studies in medical physics  @xcite ; regression testing of the geant  4  @xcite detector - simulation toolkit ; in - silico molecular docking in searches for new drugs against potential variants of an influenza virus  @xcite ; telecommunication applications  @xcite ; and theoretical physics  @xcite .",
    "the worker agents are executed as jobs , so that resource usage may be controlled by the user from the interface .",
    "this approach allows the efficiency of the overlay scheduling system to be combined with the well - structured job management offered by , as well as combining and non - resources under a uniform interface .",
    "also , this allows the efficient implementation of low - latency access to resources and improvements to responsiveness when supporting on - demand computing and interactivity @xcite .",
    "may be embedded in web - based services such as the bio - informatics portal developed by asgc , taipei .",
    "the portal is fully customized for analysis of candidate drugs against avian flu .",
    "the portal engine delegates job management to the embedded /framework , as shown in fig .",
    "[ fig : webportal ] . following this approach",
    ", users can switch between different resources , or access heterogeneous computing environments through a single same web interface .",
    "has been presented as a tool for job management in an environment of heterogeneous resources and is particularly suited to the paradigm that has emerged in large - scale distributed computing .",
    "makes it easy to define a computational task that can be executed locally for debugging , and subsequently be run on the , for large scale data mining .",
    "we have shown how simplifies task specification , takes care of job submission , monitoring and output retrieval , and provides an intuitive bookkeeping system .",
    "we have demonstrated the advantages of having a well - defined api , which can be used interactively at the prompt , through a gui or programmatically in scripts . by virtue of its plugin system , is readily extended and customised to meet the requirements of new user communities .",
    "examples of usage have been provided in particle physics , medical physics and image processing .",
    "existing command - line submission interfaces , such as glite , tend to include only limited usability features . some higher level tools , for example gridway@xcite , present jobs as if they were unix processes and corresponding command line utilities . interfaces based on condor job - submission scripts",
    "have also been developed @xcite .",
    "a distinctive feature of is that it may easily be adapted to different styles of working , allowing simultaneous use of three different interfaces .",
    "also provides a higher level of abstraction than most job - management tools , and allows a user to focus on solving the domain - specific problems , rather than changing their way of working each time they switch to a new processing system .",
    "has a large user base and is in active development .",
    "is a tool which may easily be used to support new scientific or commercial projects on a wide range of distributed infrastructures .",
    "the development of has been supported by the gridpp project in the united kingdom  @xcite , with funding from the science and technology facilities council ( stfc ) and its predecessor , the particle physics and astronomy research council ( pparc ) ; by the d - grid project in germany , with funding from the bundesministerium fr bildung und forschung ( bmbf ) ; and by the project for enabling grids for e - sciencee ( egee ) , co - funded by the european commission ( contract number infso - ri-031688 ) through the sixth framework programme .",
    "has received contributions over the years from a number of individuals .",
    "particular thanks are due to david adams , marcello barisonzi , mike kenyon , wim lavrijsen , janusz martyniak , pere mato , caitriana nicholson , rebecca ronke , vladimir romanovski , david tuckett , ruth dixon del tufo , craig tull .",
    "the developers would also like to thank the large number of users , from both within and outside particle physics , for their valuable suggestions for improving , and for their help in debugging problems .",
    "below we give a set of examples of working with .",
    "for ease of reading , keywords are in bold .",
    "first we look at a complete session .",
    "this is free software ( gpl ) , and you are welcome to redistribute it under certain conditions ; type license ( ) for details .",
    "\\end{verbatim } ! [ 1 ] : j = job(name='myjob ' )",
    "# create a default job [ 2 ] : j.submit ( )                # submit the job      [ 3 ] : j.peek('stdout ' )          # look at the output [ 4 ] : j = j.copy(name='gridjob ' ) # make a copy of the job [ 5 ] : j.backend=lcg ( )           # change backend to the grid [ 6 ] : j.submit ( )                # submit the job [ 7 ] : jobs                      # list jobs ! \\begin{verbatim } ... job listing ... \\end{verbatim } ! [ 8 ] : exit                      # quit ganga . ....    in the next example ,",
    "we create a job for analysis of data .",
    "a splitter is used to divide the analysis between subjobs .",
    "data are assigned using logical identifiers , and the wms ensures that subjobs are sent to locations where the required data are available .    .... { } [ 1 ] : j = job(application = davinci(),backend = dirac ( ) ) [ 2 ] : j.inputdata = lhcbdataset(files= [   # data to read ...       ' lfn:/foo.dst ' , ...       ' lfn:/bar.dst ' , ...       many more data files ] ) [ 3 ] : j.splitter=diracsplitter ( )        # we want subjobs [ 4 ] : j.submit ( ) ! \\begin{verbatim } job submission output \\end{verbatim } ! ....      .... { } # status of jobs and where they ran [ 5 ] : for subjob in j.subjobs :   ...        print subjob.status , subjob.actualce ! \\begin{verbatim } 42 \\end{verbatim } ! # find backend identifier of all failed jobs [ 6 ] : for j in jobs.select(status='failed ' ) : ...        print j.backend.id ! \\begin{verbatim } 42 \\end{verbatim } !",
    "....                      a.  streit , _ unicore  from project results to production grids _",
    "computing : the new frontiers of high performance processing advances in parallel computing 14 , elsevier , 2005 , pp .",
    "357 - 376 ]        m.  li and m.  baker , _ a review of grid portal technology _ , pp .  126 - 156 of : http://www.springer.com/computer/programming/book/978-1-85233-998-2[j.c .",
    "cunha and o.f .",
    "rana ( eds . ) , _ grid computing : software environments and tools _ ] ( springer - verlag london ltd , 2006 ) .              r.l .",
    "henderson , _ job scheduling under the portable batch system _ , http://dx.doi.org/10.1007/3-540-60153-8[pp .",
    "279 - 294 of : d.g .",
    "feitelson and l.  rudolph ( eds . ) , _ job scheduling strategies for parallel processing _",
    "[ lecture notes in computer science * 949 * ] ] ( springer , berlin , 1995 ) .",
    "w.  gentzsch , _ sun grid engine : towards creating a compute power grid _ , http://dx.doi.org/10.1109/ccgrid.2001.923173[pp .",
    "35 - 36 of : r.  buyya , g.  mohay and p.  roe ( eds . ) , _ proc .",
    "first ieee / acm international symposium on cluster computing and the grid _ ] ( ieee computer society , los alamitos , ca , 2001 ) .",
    "g.  barrand , _ gaudi - a software architecture and framework for building hep data processing applications _ , http://dx.doi.org/10.1016/s0010-4655(01)00254-5[computer physics communications * 140 * ( 2001 ) 45 ] .",
    "w.  verkerke and d.  kirkby , _ the roofit toolkit for data modeling _ , http://www.slac.stanford.edu/econf/c0303241/proc/papers/molt007.pdf[contribution molt007 in : proc .",
    "2003 conference for computing in high energy and nuclear physics , la jolla , ca [ slac econf c0303241 ] ] .",
    "m.  branco _ managing atlas data on a petabyte - scale with dq2 _ , http://dx.doi.org/10.1088/1742-6596/119/6/062017[j .",
    "* 119 * ( 2008 ) 062017 ] .",
    "t.  maeno , _ panda : distributed production and distributed analysis system for atlas _ , http://dx.doi.org/10.1088/1742-6596/119/6/062036[j .",
    "* 119 * ( 2008 ) 062036 ] .",
    "r.  jones , _ an overview of the egee project _ , http://dx.doi.org/10.1007/11549819[pp .  1 - 8 of : c.  trker , m.  agosti and h .- j .",
    "schek ( eds . ) , _ peer - to - peer , grid , and service - orientation in digital library architectures _ [ lecture notes in computer science * 3664 * ] ] ( springer , berlin , 2005 ) .      c.  town and d.  sinclair , _ language - based querying of image collections on the basis of an extensible ontology _ , http://dx.doi.org/10.1016/j.imavis.2003.10.002[image and vision computing * 22 * ( 2004 ) 251 ] ."
  ],
  "abstract_text": [
    "<S> in this paper , we present the computational task - management tool , which allows for the specification , submission , bookkeeping and post - processing of computational tasks on a wide set of distributed resources . has been developed to solve a problem increasingly common in scientific projects , which is that researchers must regularly switch between different processing systems , each with its own command set , to complete their computational tasks . </S>",
    "<S> provides a homogeneous environment for processing data on heterogeneous resources . </S>",
    "<S> we give examples from high energy physics , demonstrating how an analysis can be developed on a local system and then transparently moved to a system for processing of all available data . </S>",
    "<S> has an api that can be used via an interactive interface , in scripts , or through a gui . </S>",
    "<S> specific knowledge about types of tasks or computational resources is provided at run - time through a plugin system , making new developments easy to integrate . </S>",
    "<S> we give an overview of the architecture , give examples of current use , and demonstrate how can be used in many different areas of science .    </S>",
    "<S> = 1    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    grid computing , data mining , task management , user interface , interoperability , system integration , application configuration    07.05.kf , 07.05.wr , 29.50.+v , 29.85.+c , 87.18.bb , 89.20.ff </S>"
  ]
}