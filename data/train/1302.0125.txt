{
  "article_text": [
    "the conjugate gradient method was first developed by hestenes and stiefel as a tool for solving the linear equation @xmath0 , where @xmath1 is an @xmath2 positive definite matrix @xcite .",
    "the strategy of the linear conjugate gradient method is to minimize the quadratic function @xmath3 of @xmath4 in the successive search directions which are generated in such a manner that those directions are mutually conjugate with respect to @xmath1 and eventually span the whole @xmath5 .",
    "as this method is generalized to be applicable to functions which are not restricted to those quadratic in @xmath4 , the conjugate gradient method in its original form is particularly called the linear conjugate gradient method .    according to a nonlinear conjugate gradient method for minimizing a smooth function @xmath6 which is not necessarily quadratic ,",
    "the search direction @xmath7 is determined by @xmath8 where @xmath9 is a parameter to be defined suitably .",
    "fletcher and reeves @xcite proposed to define @xmath9 by @xmath10 ( see @xcite for another way to determine @xmath9 ) .",
    "on the other hand , iterative optimization methods on @xmath5 have been developed so as to be applicable on riemannian manifolds @xcite .",
    "those generalized methods are called riemannian optimization methods , which provide procedures for minimizing objective functions defined on a riemannian manifold @xmath11 . in a riemannian optimization method , the usual line search",
    "should be replaced @xcite , as the concept of a line is generalized on a riemannian manifold .",
    "absil , mahony , and sepulchre proposed to use a retraction map to perform a search on a curve on @xmath11 in place of the line search . as for the conjugate gradient method , smith provided in @xcite a conjugate gradient method on @xmath11 along with other optimization algorithms on @xmath11 .",
    "the difficulty we encounter in generalizing the conjugate gradient method to that on a manifold is that eq .",
    "makes no longer sense .",
    "this is because @xmath12 and @xmath13 belong to tangent spaces at different points on @xmath11 in general , so that they can not be added .",
    "smith proposed to use the parallel translation along the geodesic at each iteration in order to make possible the addition of two tangent vectors and thereby to extend the iteration procedure . however , using the parallel translation on @xmath11 is not computationally effective in general .",
    "a way to perform the conjugate gradient method on @xmath11 in an efficient manner is to use a vector transport @xcite .",
    "the global convergence in the conjugate gradient method with a vector transport on @xmath11 has been recently discussed by ring and wirth @xcite .",
    "they proved the global convergence under the condition that the vector transport in use does not increase the norm of the search direction vector . on the contrary ,",
    "the present article provides numerical evidence to show that if the assumption is not satisfied , the conjugate gradient method with a general vector transport may fail to generate a globally converging series . in order to relax the assumption in @xcite , the notion of a  scaled \"",
    "vector transport is introduced in this article and a new conjugate gradient algorithm is proposed with only a mild computational overhead per iteration .",
    "the organization of this paper is as follows : the scaled vector transport is introduced in section [ cg ] after a brief review of some useful existing concepts . how to compute the step size",
    "is also discussed in this section . in section [ cgr ] ,",
    "a brief review is made of the conjugate gradient method on a riemannian manifold @xmath11 , and then a new algorithm is proposed , in which the scaled vector transport is applied only if the vector transport increases the norm of the previous search direction . in section",
    "[ analysis ] , the global convergence for the proposed algorithm is proved in a manner similar to the usual one performed on @xmath5 , where the scaled vector transport used on a fitting occasion makes a generated sequence into a globally convergent one .",
    "section [ experiments ] provides numerical experiments on simple problems which the existing algorithm can not solve efficiently but the proposed algorithm can do .",
    "the numerical experiments show why the present algorithm can generate convergent sequences .",
    "section [ conclusion ] includes concluding remarks .",
    "it is shown in appendix [ app ] that the lipschitzian condition referred to in subsection [ zousec ] is satisfied for some practical riemannian optimization problems .",
    "an unconstrained optimization problem on a riemannian manifold @xmath11 is described as follows :    [ general_prob ] @xmath14    if @xmath11 is the euclidean space @xmath5 , the line search is performed with the updating formula @xmath15 where @xmath16 are a current point and an unknown next point , respectively , and where @xmath17 and @xmath18 are a search direction at @xmath19 and a step size , respectively .",
    "however , the line search does not make sense on a general manifold @xmath11 . in order to generalize the line search on @xmath5 to that on @xmath11",
    ", the search direction @xmath7 should be taken as a tangent vector in @xmath20 , and the addition in eq .",
    "should be replaced by another suitable operation . a natural alternative to the line search",
    "is a search along the geodesic emanating from @xmath19 in the direction of @xmath7 , but the geodesic will cause computational difficulty except for a few particular manifolds where the geodesics admit a tractable closed - form expression .",
    "a computationally efficient way is to use the following retraction map introduced in @xcite .",
    "[ retractiondef ] let @xmath11 and @xmath21 be a manifold and the tangent bundle of @xmath11 , respectively .",
    "let @xmath22 be a smooth map and @xmath23 the restriction of @xmath24 to @xmath25 .",
    "the @xmath24 is called a retraction on @xmath11 , if it has the following properties :    1 .",
    "@xmath26 , where @xmath27 denotes the zero element of @xmath25.[ret1 ] 2 .   with the canonical identification @xmath28",
    ", @xmath23 satisfies @xmath29 where @xmath30 denotes the derivative of @xmath23 at @xmath27 , and @xmath31 the identity map on @xmath25.[ret2 ]    as is easily seen , the exponential map on @xmath11 is a typical example of a retraction . if we can find a computationally preferable retraction , we can perform an optimization procedure as follows :    choose an initial point @xmath32 .",
    "compute the search direction @xmath33 and the step size @xmath18 .",
    "compute the next iterate by @xmath34 , where @xmath24 is a retraction on @xmath11 .",
    "the choice of a search direction and a step size characterizes the individual optimization method .",
    "we proceed to the vector transport in search for computationally efficient conjugate gradient methods .      in a ( nonlinear ) conjugate gradient method on the euclidean space @xmath5 , the search directions @xmath7 are chosen to be @xmath35 where @xmath36 , and where @xmath9 with @xmath37 are determined in several possible manners .",
    "for example , @xmath9 are determined by @xmath38 or @xmath39 where fr and pr are abbreviations of fletcher - reeves and polak - ribire , respectively @xcite .    however",
    ", if @xmath5 is replaced by a riemannian manifold @xmath11 , @xmath40 and @xmath41 belong to different tangent spaces , so that @xmath42 in eq .   does not make sense . the quantity @xmath43 in eq .",
    "makes no sense on @xmath11 either . in order to modify the vector addition in eqs .   and into a suitable operation on @xmath11 , smith proposed to use the parallel translation of tangent vectors along a geodesic @xcite .",
    "however , no computationally efficient formula is known for the parallel translation along a geodesic even for the stiefel manifold except when it reduces to the sphere or the orthogonal group .",
    "@xcite proposed the notion of a vector transport as an alternative to the parallel translation .",
    "the vector transport is a generalization of the parallel translation and can enhance computational efficiency of algorithms , if defined suitably .    in this paper , we focus on the differentiated retraction @xmath44 as a vector transport , which is defined to be @xmath45,\\qquad \\eta_x,\\xi_x\\in t_xm,\\label{dr}\\ ] ] where @xmath24 is a retraction on @xmath11 .",
    "we here note that @xmath44 satisfies the conditions in the definition of a vector transport , as is easily verified @xcite .    in what follows , we assume that @xmath11 is a riemannian manifold and denote the riemannian metric evaluated at @xmath46 by @xmath47 .",
    "the norm of a tangent vector @xmath48 evaluated at @xmath46 is defined to be @xmath49 .",
    "we here have to note that though the parallel translation is an isometry , a vector transport is not required to preserve the norm of vectors in general .",
    "the differentiated retraction @xmath44 is not always an isometry either . in analysing the convergence for the conjugate gradient method",
    "later , it will be crucial whether the vector transport @xmath44 increases the norm of vectors or not . in order to prevent the vector transport @xmath44 from increasing the norm of vectors ,",
    "we define the scaled vector transport @xmath50 associated with @xmath44 as follows :    [ t0def ] let @xmath24 be a retraction on a riemannian manifold @xmath11 .",
    "let @xmath44 be a vector transport defined by with respect to @xmath24 .",
    "the scaled vector transport @xmath51 associated with @xmath44 is defined as @xmath52    the scaled vector transport @xmath51 thus defined is no longer a vector transport since it is not linear .",
    "however , @xmath51 satisfies @xmath53 which is a key property for the global convergence of the algorithm we will propose .      in computing the step size @xmath54 in the conjugate gradient method on @xmath5 ,",
    "the strong wolfe conditions are often used @xcite , which require @xmath54 to satisfy @xmath55 with @xmath56 .",
    "in particular , @xmath57 and @xmath58 are often taken so as to satisfy @xmath59 in the conjugate gradient method .",
    "in order to extend the strong wolfe conditions on @xmath5 to those on @xmath11 , we start by reviewing the strong wolfe conditions and . for a current point @xmath19 and a search direction @xmath7 ,",
    "one performs a line search for the function defined by @xmath60 requiring @xmath54 to give a sufficient decrease in the value of @xmath6 , one imposes the condition @xmath61 which yields . in order to prevent @xmath54 from being excessively short ,",
    "the @xmath54 is required to satisfy @xmath62 which implies .    in order to generalize the strong wolfe conditions to those on @xmath11 , we define a function @xmath63 on @xmath11 , in an analogous manner to , to be @xmath64 where @xmath24 is a retraction on @xmath11 .",
    "the conditions and applied to give rise to @xmath65 @xmath66\\rangle_{r_{x_k}(\\alpha_k\\eta_k)}\\rvert}\\le c_2{\\lvert\\langle\\grad f(x_k),\\eta_k\\rangle_{x_k}\\rvert},\\label{wolfem2}\\ ] ] respectively , where @xmath56 .",
    "we call the conditions and the strong wolfe conditions . the existence of a step size satisfying and can be shown by an almost verbatim repetition of that for the strong wolfe conditions on @xmath5 ( see @xcite ) .",
    "[ wolfeprop ] let @xmath11 be a riemannian manifold with a retraction @xmath24 .",
    "if a smooth objective function @xmath6 on @xmath11 is bounded below on @xmath67 for @xmath68 and for a descent direction @xmath69 , and if constants @xmath57 and @xmath58 satisfy @xmath56 , then there exists a step size @xmath54 which satisfies the strong wolfe conditions and .",
    "we note that the strong wolfe conditions and together with the existence of a step size satisfying them are also discussed in @xcite .",
    "we now look into the second condition . if we introduce a vector transport @xmath44 as the differentiated retraction given by , then eq .",
    "can be expressed as @xmath70 an idea for further generalization of this condition to that in an algorithm with a general vector transport @xmath71 is to replace by @xmath72 however , if @xmath73 , the existence of a step size satisfying both and is unclear in general . in view of this , the differentiated retraction @xmath44 is considered to be a natural choice of a vector transport @xmath71 , for which a step size satisfying and is shown to exist . in",
    "what follows , we use the differentiated retraction @xmath44 and the scaled one @xmath51 .",
    "if a riemannian manifold @xmath11 is given a retraction @xmath24 and the corresponding vector transport @xmath44 , a standard fletcher - reeves type conjugate gradient method on @xmath11 is described as follows @xcite :    [ cgm ] choose an initial point @xmath32 . set @xmath74 .",
    "compute the step size @xmath18 satisfying the strong wolfe conditions and with @xmath59 .",
    "set @xmath75 where @xmath24 is a retraction on @xmath11 .",
    "set @xmath76 @xmath77 where @xmath44 is the differentiated retraction defined by .    in @xcite , the convergence property of algorithm [ cgm ]",
    "is verified under the assumption that the inequality @xmath78 holds for all @xmath79 .",
    "however , the assumption does not always hold in general .",
    "for example , the assumption does not hold on the sphere endowed with the orthographic retraction @xcite . in section [ experiments ] , we will numerically treat such a case .",
    "we wish to relax the assumption by using a scaled vector transport .",
    "an idea for improving algorithm [ cgm ] is to replace @xmath44 by the scaled vector transport @xmath51 defined by .",
    "however , this causes difficulty in computing effectively a step size @xmath54 satisfying with @xmath80 .",
    "a simple but effective idea for improving algorithm [ cgm ] is that each step size is always computed so as to satisfy the strong wolfe conditions and , but the scaled vector transport @xmath51 is adopted if it is necessary for the purpose of convergence .",
    "more specifically , we use the scaled vector transport @xmath51 only if the vector transport @xmath44 increases the norm of the previous search direction vector , that is , we introduce @xmath81 defined by @xmath82 as a substitute for @xmath44 in step 5 of algorithm [ cgm ] .",
    "this idea is realized in the following algorithm .",
    "[ cgmtk ] choose an initial point @xmath32 .",
    "set @xmath74 .",
    "compute the step size @xmath18 satisfying the strong wolfe conditions and with @xmath59 .",
    "set @xmath75 where @xmath24 is a retraction on @xmath11 .",
    "set @xmath83 @xmath84 where @xmath81 is defined by , and where @xmath44 and @xmath51 are the differentiated retraction and the associated scaled vector transport defined by and , respectively .",
    "we will prove in section [ analysis ] the global convergence property of the proposed algorithm , and give in section [ experiments ] numerical examples in which the inequality does not hold for all @xmath79 but our algorithm [ cgmtk ] indeed has an advantage in generating convergent sequences .",
    "in this section , we verify the convergence property of algorithm [ cgmtk ] .",
    "zoutendijk s theorem about a series associated with search directions on @xmath5 is not only valid for the conjugate gradient method but also valid for general descent algorithms @xcite .",
    "this theorem can be generalized so as to be applicable to a general descent algorithm ( algorithm [ algorithm1 ] ) on a riemannian manifold @xmath11 . in the same manner as in @xmath5 , we define on a riemannian manifold @xmath11 the angle @xmath85 between the steepest descent direction @xmath86 and the search direction @xmath7 through @xmath87 then , zoutendijk s theorem on @xmath11 is stated as follows :    [ zoutendijk ] suppose that in algorithm [ algorithm1 ] on a riemannian manifold @xmath11 , a descent direction @xmath7 and a step size @xmath54 satisfy the strong wolfe conditions and . if the objective function @xmath6 is bounded below and of @xmath88-class , and if there exists a lipschitzian constant @xmath89 such that @xmath90-\\d(f\\circ r_x)(0)[\\eta]\\rvert}\\le lt , \\qquad \\eta\\in t_xm\\ { \\rm with}\\ { \\lvert\\eta\\rvert}_x=1,\\ x\\in m,\\ t\\ge 0,\\label{lip}\\ ] ] then the following series converges ; @xmath91    the proof of this theorem can be performed in the same manner as that for zoutendijk s theorem on @xmath5 .",
    "see @xcite for more detail .",
    "we remark that the inequality is a weaker condition than the lipschitz continuous differentiability of @xmath92 .",
    "we will show in appendix [ app ] that eq",
    ".   holds for objective functions in practical riemannian optimization problems .",
    "a further discussion on the relation with the standard lipschitz continuous differentiability will be also made in the same appendix .",
    "we first extend a lemma in @xcite so as to be applicable to algorithm [ cgmtk ] as follows :    [ lemma ] the search direction @xmath7 determined in algorithm [ cgmtk ] is a descent direction satisfying @xmath93    the proof runs by induction . for @xmath94 ,",
    "the inequality clearly holds on account of @xmath95 we here note that @xmath59 .",
    "suppose that @xmath7 is a descent direction satisfying for some @xmath96 .",
    "note that on account of eq .   with eq .",
    ", @xmath44 and @xmath81 are related by @xmath97 in each case .",
    "since @xmath98 and @xmath99 are in the same direction with the inequality @xmath100 in norm , we have @xmath101 we also note that the vector transport @xmath44 is defined to be @xmath102 $ ] in the algorithm .",
    "it then follows from and that @xmath103 where it is to be noted that @xmath7 is in a descent direction .",
    "the middle term in with @xmath104 for @xmath96 is computed as @xmath105 where the definition of @xmath106 has been used .",
    "therefore , we obtain from and @xmath107 the inequality for @xmath104 immediately follows from the induction hypothesis .",
    "we proceed to the global convergence property of algorithm [ cgmtk ] .",
    "the convergence of the conjugate gradient method has been already proved on @xmath5 by al - baali @xcite .",
    "exploiting the idea of the proof used in @xcite , we show that algorithm [ cgmtk ] generates converging sequences on a riemannian manifold .",
    "[ thm_glocon ] consider algorithm [ cgmtk ] . if and hence hold , then @xmath108    if @xmath109 for some @xmath96 ,",
    "let @xmath110 be the smallest integer among such @xmath96 .",
    "then , we have @xmath111 and @xmath112 from and with @xmath113 , so that @xmath114 .",
    "it then follows that @xmath109 for all @xmath115 .",
    ".   clearly holds in such a case .",
    "we shall consider the case in which @xmath116 for all @xmath96 and prove by contradiction .",
    "assume that does not hold , that is , there exists a constant @xmath117 such that @xmath118 now from and , we obtain @xmath119 on account of thm .",
    "[ zoutendijk ] , eqs .   and",
    "are put together to provide @xmath120 on the other hand , eqs .  , , and the strong wolfe condition are put together to give @xmath121 using this inequality and the definition of @xmath9 , we obtain the recurrence inequality for @xmath122 as follows : @xmath123 where we have used the fact that @xmath124 and put + @xmath125 . the successive use of this inequality together with the definition of @xmath9 results in @xmath126 where use has been made of in the last inequality .",
    "the inequality gives rise to @xmath127 this contradicts and the proof is completed .",
    "in this section , we compare algorithm [ cgmtk ] with algorithm [ cgm ] by numerical experiments . as is shown in @xcite , if the vector transport @xmath44 as the differentiated retraction satisfies the inequality , the convergence property of algorithm [ cgm ] is proved .",
    "however , if does not hold , it is not always ensured that sequences generated by algorithm [ cgm ] converge .",
    "in contrast with this , algorithm [ cgmtk ] indeed works well even if fails to hold , as is verified in thm .",
    "[ thm_glocon ] . in the following ,",
    "we give two examples which show that algorithm [ cgmtk ] works better than algorithm [ cgm ] .",
    "one of the examples is somewhat artificial but well illustrates the situation in which a sequence generated by algorithm [ cgm ] is unlikely to converge .",
    "the other is a more natural example encountered in a practical problem .    in",
    "both of two examples , we consider the following rayleigh quotient minimization problem on the sphere @xmath128 @xcite :    [ rayleigh_prob ] @xmath129    where @xmath130 with @xmath131 .",
    "the optimal solutions of this problem are @xmath132 , which are the unit eigenvectors of @xmath1 associated with the smallest eigenvalue @xmath133",
    ".      consider problem [ rayleigh_prob ] with @xmath134 and @xmath135 .",
    "a riemannian metric @xmath136 on @xmath137 is here defined by @xmath138 where @xmath139 , and where @xmath140 denotes the first component of the column vector @xmath4 .",
    "it is to be noted that this metric is not the standard one on @xmath137 .",
    "the norm @xmath141 of @xmath142 is then defined to be @xmath143 .",
    "if @xmath4 is close to the optimal solutions @xmath144 , then @xmath145 is nearly @xmath146 . since the first diagonal element of @xmath147 is large because of the coefficient @xmath148 , the closer @xmath4 is to @xmath144 , the larger the norm @xmath141 tends to be .",
    "with respect to the metric , the gradient of @xmath6 is described as @xmath149 indeed , the right - hand side of belongs to @xmath150 and it holds that @xmath151\\ ] ] for any @xmath152 .",
    "let @xmath24 be the retraction on @xmath137 defined by @xmath153 which is the special case of the qr retraction on the stiefel manifold defined in appendix [ app ] . for this @xmath24 , the differentiated retraction @xmath44 defined by",
    "is written out as @xmath154    we note that though the metric endowed with is not the standard one , the lipschitzian condition holds , as is mentioned in rem .",
    "[ remark_app2 ] in appendix [ app ] .",
    "hence from thm .  [ thm_glocon ] , algorithm [ cgmtk ] works well in theory .     of the objective function @xmath6 evaluated on the sequence @xmath155 generated by algorithm [ cgm].,width=377 ]     from the sequence @xmath155 generated by algorithm [ cgm].,width=377 ]     evaluated on the sequences @xmath155 and @xmath156 generated by algorithm [ cgm].,width=377",
    "]    figs .",
    "[ fig : f ] , [ fig:1 ] , and [ fig:2 ] show numerical results from applying algorithm [ cgm ] to problem [ rayleigh_prob ] with the initial point @xmath157 with @xmath134 .",
    "the vertical axes of figs .",
    "[ fig : f ] , [ fig:1 ] , and [ fig:2 ] carry values of @xmath158 at @xmath19 , values of the first components @xmath159 of @xmath19 , and values of the ratios @xmath160 , respectively .",
    "note that for the optimal solution @xmath161 which the current generated sequence @xmath155 is expected to approach , the target value is @xmath162 in both figs .",
    "[ fig : f ] and [ fig:1 ] . though the @xmath155 seems to come close to @xmath163 bit by bit",
    ", the convergence is not observed even after @xmath164 iterations . at the iteration number @xmath164 , @xmath158 is far from @xmath165 , as is seen from fig .",
    "[ fig : f ] . fig .",
    "[ fig:1 ] shows that the sequence is intermittently repelled from the target point , when approaching it .",
    "if more iterations , say @xmath166 , are performed , the graph of @xmath167 has almost the same shape , that is , sharp peaks repeatedly appear in fig .",
    "[ fig:1 ] with extended iterations .",
    "if @xmath168 for all @xmath79 , the sequence @xmath155 would converge .",
    "however , as is shown in fig .",
    "[ fig:2 ] , the ratio @xmath160 intermittently exceeds the value @xmath146 .",
    "this fact seems to prevent the sequence from converging , as long as numerical experiments suggest . to gain insight into the non - convergence problem , we put figs .",
    "[ fig:1 ] and [ fig:2 ] together into fig .",
    "[ fig:3 ] , which shows that the peaks of two graphs synchronize .     and @xmath160 by algorithm [ cgm].,width=377 ]",
    "this suggests that the violation of the inequality makes the sequence fail to approach the optimal solution @xmath163 .",
    "this phenomenon is caused by the large first diagonal element of @xmath147 in the neighbourhood of @xmath163 .     from the sequence @xmath155 generated by algorithm [ cgmtk].,width=377 ]    in contrast with this , in algorithm [ cgmtk ] , the vector transport @xmath44 is scaled if necessary , and thereby generated sequences converge to solve problem [ rayleigh_prob ] . in comparison with fig .",
    "[ fig:1 ] , fig .",
    "[ fig:4 ] shows that the present algorithm generates a converging sequence , resolving the difficulty of being repelled from the optimal solution .",
    "we here note that the inequality @xmath169 is never violated in this algorithm .",
    "we now investigate the performance of algorithm [ cgmtk ] in more detail with interest in comparison with a restart strategy in the conjugate gradient method .     and @xmath163 with respect to the sequences @xmath155 generated by algorithm [ cgmtk ] with several restarting strategies.,width=377 ]    as is well known , in a nonlinear conjugate gradient method on the euclidean space , the iteration is often restarted at every @xmath170 steps by taking a steepest descent search direction , where @xmath170 is usually chosen to be the dimension of the search space in the problem . to gain a sight of the performance of the restart method on a riemannian manifold",
    ", we introduce a similar restart strategy into algorithms [ cgm ] and [ cgmtk ] , that is , we set @xmath171 in step 5 of each algorithm at every @xmath170 steps .",
    "a choice for @xmath170 is @xmath172 , which is the dimension of @xmath137 with @xmath134 .",
    "for comparison , the both algorithms with restarts are also performed for @xmath173 and @xmath174 .",
    "the results from algorithm [ cgmtk ] with and without restart are shown in fig .",
    "[ fig:6 ] .",
    "the vertical axis of fig .  [ fig:6 ] carries @xmath175 , which is an approximation of the distance between @xmath19 and @xmath163 on @xmath137 .",
    "we can observe from the graphs in fig .",
    "[ fig:6 ] that algorithm [ cgmtk ] with and without restart has a superlinear convergence property .",
    "[ fig:6 ] shows further that algorithm [ cgmtk ] without restart exhibits better performance than algorithm [ cgmtk ] with a few variants of restarts , which means that the restart strategy fails to improve the performance of algorithm [ cgmtk ] .     and @xmath163 with respect to the sequences @xmath155 generated by algorithm [ cgmtk ] and algorithm [ cgm ] with several restarting strategies.,width=377 ]    on the contrary , the restart strategy improves the performance of algorithm [ cgm ] , but the resultant performance is not comparable to algorithm [ cgmtk ] without restart yet . a numerical evidence is shown in fig .  [ fig:7 ] .",
    "we give a more natural example , in which the inequality is never satisfied .",
    "consider problem [ rayleigh_prob ] with @xmath176 and @xmath177 .",
    "the difference from the example in subsection [ sphere1 ] is the choice of a riemannian metric and a retraction .",
    "we in turn endow the sphere @xmath137 with the induced metric @xmath178 from the natural inner product on @xmath5 : @xmath179 the norm of @xmath142 is then defined to be @xmath180 as usual . with the natural metric @xmath178 ,",
    "the gradient of @xmath6 is written out as @xmath181 we consider the orthographic retraction @xmath24 on @xmath137 @xcite , which is defined to be @xmath182 associated with this @xmath24 , the vector transport @xmath44 is written out as @xmath183 for this @xmath44 , the norm @xmath184 is evaluated as @xmath185 where use has been made of @xmath186 and @xmath187 .",
    "thus , the inequality , which is the key condition for the proof of the global convergence property of algorithm [ cgm ] , is violated unless @xmath188 . in spite of this fact",
    ", we may try to perform algorithm [ cgm ] for this problem .",
    "if the generated sequence does not diverge , we can compare the result with that obtained by algorithm [ cgmtk ] .",
    "we performed algorithms [ cgm ] and [ cgmtk ] and obtained fig .",
    "[ fig:8 ] , whose vertical axis carries @xmath175 .",
    "the figure shows the superiority of the proposed algorithm .     and @xmath163 for the sequences @xmath155 generated by algorithms [ cgm ] and [ cgmtk ] with the orthographic retraction.,width=377 ]",
    "we have dealt with the global convergence of the conjugate gradient method with the fletcher - reeves @xmath189 .",
    "though the conjugate gradient method generates globally converging sequences in the euclidean space , the conjugate gradient method on a riemannian manifold @xmath11 has not been shown to have a convergence property in general , but under the assumption that the vector transport @xmath44 as the differentiated retraction does not increase the norm of the tangent vector , the convergence is proved in @xcite . if the parallel translation is adopted as a vector transport , the conjugate gradient method is shown to generate converging sequences , as is given in @xcite . however , the parallel translation is not convenient for computational effectiveness . for computational efficiency ,",
    "we have introduced a vector transport , in place of the parallel translation , with a modification that the vector transport @xmath44 is replaced by the scaled vector transport @xmath51 only when @xmath44 increases the norm of the search direction vector . the idea is simple but effective .",
    "we have achieved a balance between computational efficiency and the global convergence by proposing algorithm [ cgmtk ] .",
    "we have shown the convergence of the present algorithm both in the theoretical and the numerical viewpoints .",
    "in particular , we have performed numerical experiments to show that the present algorithm can solve problems for which the existing algorithm can not work well because of the violation of the assumption about the vector transport .",
    "in thm .  [ zoutendijk ] , we assume that the condition holds .",
    "we here compare with the condition that @xmath92 is lipschitz continuously differentiable uniformly for @xmath4 , that is , there exists a lipschitz constant @xmath89 such that @xmath190 where the @xmath191 of the left - hand side means the operator norm ( see @xcite for detail ) .",
    "the condition is equivalent to @xmath192\\rvert}\\le l{\\lvert\\xi-\\zeta\\rvert}_x,\\qquad \\xi,\\zeta\\in t_x m , x\\in m.\\label{appendixlip2}\\ ] ] in particular , setting @xmath193 and @xmath194 in yields . in this sense ,",
    "the condition is a weaker form of .",
    "the assumption is of practical use .",
    "for example , the problem of minimizing the brockett cost function on the stiefel manifold @xmath195 with the natural induced metric @xcite has this property , as is shown below .",
    "let @xmath196 be positive integers with @xmath197 .",
    "the stiefel manifold @xmath195 is defined to be @xmath198 .",
    "we consider @xmath195 as a riemannian submanifold of @xmath199 endowed with the natural induced metric @xmath200 let @xmath1 be an @xmath2 symmetric matrix and @xmath201 with @xmath202 .",
    "the brockett cost function @xmath6 is defined on @xmath195 to be @xmath203 further , the qr decomposition - based retraction ( which we call the qr retraction ) @xmath24 is defined to be @xmath204 where @xmath205 denotes the q - factor of the qr decomposition of a full rank matrix @xmath206 .",
    "that is , if @xmath207 is decomposed into @xmath208 , where @xmath209 and @xmath24 is an upper triangular @xmath210 matrix with positive diagonal elements , then @xmath211 .",
    "[ prop_app1 ] the inequality holds for the brockett cost function on @xmath212 , where @xmath195 is endowed with the natural induced metric , and where the qr retraction is adopted .",
    "since the function is smooth , we have only to show that @xmath213 in fact , eq .",
    "is a straightforward consequence of this inequality .",
    "let @xmath214 be a curve defined by @xmath215 , and @xmath216 denote the @xmath96-th column vectors of @xmath217 , respectively .",
    "then , through the gram - schmidt orthonormalization process , we obtain @xmath218 where @xmath219 and @xmath220 for @xmath221-dimensional vectors @xmath222 . by induction on @xmath96 , we can take vector - valued polynomials @xmath223 in @xmath224 satisfying @xmath225 indeed , for @xmath226 , holds with @xmath227 .",
    "suppose that holds for @xmath228",
    ". then we can write out @xmath229 as @xmath230 denoting by @xmath223 the numerator of the right - hand side of , which is a polynomial in @xmath224 , we obtain .",
    "let @xmath231 then , the @xmath232 is written out as @xmath233 since @xmath234 , and since the degree of the numerator polynomial in @xmath224 is not more than that of the denominator polynomial , the degree of the numerator polynomial from the right - hand side of is less than that of the denominator polynomial , so that one has , as @xmath235 , @xmath236 this implies that @xmath232 is bounded with respect to @xmath237 . moreover",
    ", the @xmath232 is continuous with respect to @xmath238 and @xmath239 on the compact set @xmath240 .",
    "it then turns out that @xmath232 is bounded on the whole domain , which implies that there exists @xmath89 such that holds .",
    "this completes the proof .",
    "[ remark_app1 ] reviewing the proof , we observe that since the qr retraction is irrespective of the metric with which the @xmath195 is endowed , and since the set @xmath240 is compact with respect to any metric on @xmath195 , the inequality with @xmath24 being the qr retraction holds for the brockett cost function independently of the choice of a metric .",
    "[ remark_app2 ] we also note that prop .  [ prop_app1 ] and rem .",
    "[ remark_app1 ] cover both the rayleigh quotient on the sphere @xmath137 as @xmath241 and the brockett cost function on the orthogonal group as @xmath242 .",
    "in particular , the inequality holds for the function , though the sphere @xmath137 is endowed with the non - standard metric .",
    "another example for comes from the problem of minimizing the function @xmath243 on @xmath244 , where @xmath1 is an @xmath245 matrix and @xmath246 with @xmath247 .",
    "an optimal solution to this problem gives the singular value decomposition of @xmath1 @xcite .",
    "let @xmath248 be positive integers with @xmath249 .",
    "we consider @xmath244 as a riemannian submanifold of @xmath250 endowed with the natural induced metric ; @xmath251 as in the previous example on @xmath195 , the qr retraction on @xmath244 is defined by @xmath252 for @xmath253 .",
    "the inequality holds for the objective function on @xmath254 , where @xmath11 is endowed with the natural induced metric and with the qr retraction .",
    "we shall show that @xmath255 for @xmath256 .",
    "put @xmath257 .",
    "let @xmath229 and @xmath258 denote the @xmath96-th column vectors of @xmath214 and @xmath259 , respectively . from prop .",
    "[ prop_app1 ] and its course of the proof , there exist vector - valued polynomials @xmath223 and @xmath260 such that @xmath261 let @xmath262 then we have @xmath263 since @xmath264 , by the same reasoning as that for @xmath265 in prop .",
    "[ prop_app1 ] , we have @xmath266 so that @xmath267 is bounded with respect to @xmath237 .",
    "further , @xmath267 is continuous with respect to @xmath268 on the compact set + @xmath269 .",
    "hence @xmath267 is bounded on the whole domain .",
    "this completes the proof .",
    "a remark similar to rem .",
    "[ remark_app1 ] can be made on the metric to be endowed with on @xmath244 .",
    "the validity of is independent of the choice of a metric .",
    "returning to the case of a general riemannian manifold @xmath11 , we make a further comment on .",
    "we are interested in the range of @xmath237 .",
    "assume that @xmath11 is compact and @xmath6 is smooth . a smooth function on a compact set is lipschitz continuously differentiable .",
    "however , the set @xmath270 is not compact even though @xmath11 is compact .",
    "therefore , it is not so clear that the inequality holds in general .",
    "we here note that the inequality is used in the form @xmath271-\\d ( f\\circ r_{x_k})(0)[\\eta_k]\\le \\alpha_k l{\\lvert\\eta_k\\rvert}_{x_k}^2\\label{appendixlip3}\\ ] ] for the proof of thm .",
    "[ zoutendijk ] .",
    "a question then arises as to under what condition the inequality holds .",
    "if it is ensured that there exists a constant @xmath272 such that @xmath273 for all @xmath96 , then we can prove .",
    "indeed , in order to prove in such a case , the range of @xmath224 in can be restricted to @xmath274 , and the inequality we need to prove as a counterpart to is written as @xmath90-\\d(f\\circ r_x)(0)[\\eta]\\rvert}\\le lt , \\quad",
    "\\eta\\in t_xm\\ { \\rm with}\\ { \\lvert\\eta\\rvert}_x=1,\\ x\\in m,\\ 0\\le t \\le m.\\label{appendixlip4}\\ ] ] in order that hold , it is sufficient that there exists a constant @xmath89 satisfying @xmath275 since the left - hand side of the inequality is continuous with respect to @xmath224 on a compact set @xmath276 , there exists @xmath277 for each @xmath278 such that with @xmath279 holds , where @xmath280 .",
    "the compactness of the set @xmath281 ensures the existence of @xmath282 and the @xmath283 thus defined satisfies .",
    "the authors would like to thank the anonymous referees for providing them with valuable comments that helped them to significantly brush up the paper .",
    "the first author appreciates the jsps research fellowship for young scientists ."
  ],
  "abstract_text": [
    "<S> this article deals with the conjugate gradient method on a riemannian manifold with interest in global convergence analysis . the existing conjugate gradient algorithms on a manifold endowed with a vector transport need the assumption that the vector transport does not increase the norm of tangent vectors , in order to confirm that generated sequences have a global convergence property . </S>",
    "<S> in this article , the notion of a scaled vector transport is introduced to improve the algorithm so that the generated sequences may have a global convergence property under a relaxed assumption . in the proposed algorithm , </S>",
    "<S> the transported vector is rescaled in case its norm has increased during the transport . </S>",
    "<S> the global convergence is theoretically proved and numerically observed with examples . </S>",
    "<S> in fact , numerical experiments show that there exist minimization problems for which the existing algorithm generates divergent sequences , but the proposed algorithm generates convergent sequences .    * keywords : * conjugate gradient method ; riemannian optimization ; global convergence ;  scaled \" vector transport ; wolfe conditions </S>"
  ]
}