{
  "article_text": [
    "what are the mechanisms that modulate learning on a neuronal level in animals or humans ? this question is up to now under debate , but the imagination one has for a biological learning rule is that the synaptic weights are changed according to a local rule . in the context of neural networks local",
    "means that only the adjacent neurons of a synapse contribute to changes of the synaptic weight .",
    "such a mechanism with respect to synaptic strengthening was proposed by donald hebb @xcite in 1949 and experimentally found by t. bliss and t. lomo @xcite . in a biological terminus hebbian learning",
    "is called _ long - term potentiation _ ( ltp ) .",
    "experimentally as well as theoretically there is a great body of investigations aiming to formulate precise conditions under which learning in neural networks takes place .",
    "e.g. the influence of the precise timing of pre- and postsynaptic neuron firing @xcite or the duration of a synaptic change ( for a review see @xcite ) termed _ short _ or _ long - term plasticity _ have been studied extensively .",
    "all of these analyses share the locality condition proposed by hebb @xcite .",
    "but there are also experimental findings which extend the traditional view of synaptic plasticity in three important points .",
    "first frey and morris @xcite found in the hippocampus of rats in vivo that there is a _",
    "synaptic tagging _ mechanism .",
    "this mechanism tagges synapses which were repeatly involved in information processing within a certain time window of up to 1.5h .",
    "if one of these synapses is restimulated within this time interval then ltp is induced .",
    "thus they concluded that there is a form of memory for past synaptic activity which leads to a kind of summing up past activities to induce ltp .",
    "the second result is from otmakhova and lisman @xcite who found an influence of a global dopamin signal on ltp and ltd in ca1 in the hippocampus .",
    "a third extention consists in results about heterosynaptic plasticity . in this form of learning not only the synapse between active pre- and postsynaptic neuron",
    "is changed but also further remote synapses of these neurons @xcite .",
    "heterosynaptic plasticity is observed for ltp as well as ltd .",
    "we emphasize that none of these additional findings exclude the classical locality condition by hebb but involve further contributions to specify learning more precisely .",
    "based on the exciting results of frey and morris @xcite and otmakhova and lisman @xcite there are theoretical investigations on learning dynamics in neural networks which interweave the locality condition of hebb with a _",
    "synaptic tagging _ mechanism and a global control signal .",
    "chialvo and bak @xcite suggested a learning rule which assigns each synapse a boolean scalar valued variable indicating if the synapse were involved in the last information processing or not .",
    "this mimics the tagging mechanism .",
    "additionally they assigned to the global control signal the role of an external reinforcement signal @xmath0 which forms a kind of feedback for the network performance .",
    "the reinforcement signal can also take only two different values whereas @xmath1 corresponds to a right and @xmath2 to a wrong output of the network .",
    "synaptic update was only allowed if the synapse was activated during the last signal processing and the reinforcement signal @xmath0 signaled a failure due to a wrong output of the network .",
    "an extention of the learning rule of bak and chialvo was presented by klemm , bornholdt and schuster @xcite .",
    "they allowed a synaptic memory @xmath3 for each synapse with @xmath4 discrete values .",
    "the dynamics of the synaptic counter @xmath3 is in each time step given by @xmath5 for active synapses which is restricted from below to @xmath6 . if @xmath7 occurs because the output of the network was wrong and hence a reinforcement signal @xmath8 was fed back into the network then the corresponding synapse will be depressed by a fixed amount @xmath9 and the corresponding synaptic counter is set to @xmath10 .    in this paper",
    "we present a novel hebb - like learning rule which has a memory for the past failures similar to @xcite .",
    "however , in contrast to these works we do not use synaptic but neuron counters . due to the use of neuron counters which can be seen as approximation for the synaptic counters we are lead to a stochastic update condition instead of a deterministic one for active synapses .",
    "the obtained stochastic learning rule whose character is still local can be interpreted biologically and corresponds in a qualitative way to heterosynaptic plasticity @xcite .",
    "the paper is organized as follows . in section [ model_xor ]",
    "we describe a model with which we investigate our stochastic learning rule .",
    "the learning rule itself is motivated and defined in section [ defslr ] .",
    "the result section [ results ] is subdivided in three parts . because a learning rule of a neural network is only one part of the entire system we investigate the interplay between our stochastic learning rule and three different network dynamics and hence their influence on the convergence behavior of the neural network .",
    "we compare a winner - take - all [ wta ] , a softmax [ softmax ] and a noisy winner - take - all mechanism [ nwta ] which are all different forms of lateral inhibition . in section [ nwta ] we investigate additionally the influence of a variable size of a synaptic change @xmath9 .",
    "the results are discussed and compared with @xcite .",
    "a biological interpretation of our stochastic hebb - like learning rule with respect to heterosynaptic plasticity is given in [ biointerpretation ] .",
    "the paper ends in section [ conclusions ] with a summary and conclusions .",
    "to investigate the learning dynamics of a neural network one has to define every item in table [ gen_sys ] .",
    ".[gen_sys]characterization of the entire system [ cols=\"^ , < \" , ]     the exponents used in figure [ v_b10_f1cp ] are again obtained by simulations for all values out of the interval @xmath11 in discrete steps of @xmath12 .",
    "the results analog to figure [ pw1 ] are shown in figure [ pw2 ] .",
    "one can see that the situation is quite similar to figure [ pw1 ] whereas the structure due to the influence of @xmath13 is now more succinct .",
    "a little surprising is the fact that the overall structure remained almost unchanged .",
    "hence the influence of @xmath14 seems to be almost linear at least up to @xmath13 .",
    "table [ emin_b ] gives a summary of all simulation results and shows the best exponents for which the mean ensemble error @xmath15 is minimal to the corresponding time steps @xmath16 .",
    "again @xmath17 is always greater than zero which excludes an equal distribution for @xmath18 .      in this subsection",
    "we use the noisy winner - take - all mechanism [ nwta_c4 ] , [ nwtas_c4 ] as network dynamics .",
    "in contrast to the preceding results we investigate now the convergence behavior over long time scales under the influence of additive noise @xmath19 over some orders of magnitude . further more we compare the influence of the size of the synaptic change @xmath9 .",
    "for this we use @xmath20 like in the preceding simulations and compare this with @xmath9 uniformly drawn out of @xmath21 $ ] .",
    "again we did the simulations for all values @xmath22 but show only the results for @xmath23 and @xmath24 which give the significant differences .",
    "the evaluation of the mean ensemble error @xmath15 is here a little different .",
    "we simulate as long as it takes the network to reach a stationary fix point and then average over the next @xmath25 time steps .",
    "in addition we average over a ( small ) ensemble @xmath26 .",
    "@xmath27 here @xmath28 , @xmath29 and @xmath30 .",
    "the first results are shown in figure [ pht1 ] . here",
    "the mean ensemble error @xmath15 is investigated in dependence of the noise @xmath19 and the exponent @xmath31 of the rank ordering distribution .",
    "the exponent of the distribution @xmath32 was fixed to @xmath33 under regard of the preceding results . in figure [ pht1 ] as well as in figure [ pht2 ] comparable @xmath34 values",
    "are arranged in rows with @xmath23 in the top and @xmath24 in the bottom row .",
    "the left columns give results for @xmath35 and the right for @xmath36\\sim o(10)$ ] . from the scales of the synaptic changes the steps occurring in figure [ pht1 ] and [ pht2 ] at noise values of order @xmath37",
    "respectively @xmath38 are crudely explained .    in the first row of figure [ pht1 ]",
    "one can clearly see that the additional degree of freedom in form of a variable size of synaptic alterations @xmath9 leads to a significant improvement for @xmath39 of one scale of order in @xmath19 .",
    "this effect can be gradually reduced by increasing @xmath34 @xcite from @xmath6 to @xmath40 .",
    "the lower part of figure [ pht1 ] gives the final results for this procedure for @xmath24 . here",
    "the influence of the additional degree of freedom is not only completely eliminated but one obtains for @xmath41 even better results in the range @xmath42 .",
    "this can be understood taking into consideration that learning is effective if the synaptic weights are changed as fast as possible and as often as necessary .",
    "on the first sting this looks like a contradiction but new paths in the network which connect input with output neurons correctly can be found only if synapses are changed . on the other hand by a synaptic change old paths in the neural network which",
    "are already learned correctly can be destroyed and hence unlearned . for this",
    "the three parameters constituting our learning rule @xmath34 , @xmath17 and @xmath31 have to be chosen so that the probability to fulfill the update condition @xmath43 is in accord with the motto given above .",
    "let us start at @xmath41 and @xmath23 with a qualitative explanation . here",
    "low @xmath31 values give either equal or slightly better results .",
    "this indicates that the probability to fulfill the update condition @xmath43 is more adequate for low @xmath31 values than for high . for low @xmath31 values",
    "the update probability is less than for high @xmath31 values because of equation [ p_rank ] under the natural assumption that the neural network did not learn the xor problem up to a certain time step which implies high values of the approximated synaptic counters @xmath44 .",
    "this holds for every @xmath34 value @xcite and hence also for @xmath24 shown in the bottom left figure in [ pht1 ] .",
    "when @xmath34 is gradually increased from @xmath23 to @xmath24 there is an additional effect on the performance .",
    "the update probability crosses the threshold from too high to too low values .",
    "this can be seen by increasing @xmath34 for fixed @xmath31 values which causes a decrease of the update probability because of equation [ p_rank_k ] .    in the bottom left figure [ pht1 ]",
    "one can see that for @xmath24 and @xmath39 this leads to an improvement of the performance of the network .",
    "this is in accordance with the explanation for the top left figure . for @xmath45 one would expect worse results than for @xmath39 which is true but better results for @xmath24 than for @xmath23",
    "this however does not hold because for these parameters the update probability crossed the critical threshold and hence is too small which prevents an efficient learning .",
    "if the update probability is greater or smaller than this critical threshold can also be seen from the steps occurring in figure [ pht1 ] .",
    "a step occurring at @xmath46 is for @xmath47 obviously .",
    "a shift to higher or lower noise values indicates a decreased ( increased ) update probability because averaging over the past network outcomes is prolonged ( reduced ) .",
    "this follows from the fact that the average time over a time series which is contaminated by noise to detect a signal has to be longer the higher the influence of the noise is .",
    "the influence of a variable synaptic change @xmath9 consists in an enhancement of the effects described above .",
    "an update probability which is too high is stronger punished due to higher mean synaptic changes and results in a higher probability to destroy already learned paths in the network .",
    "this can be seen in both of the right figures in [ pht1 ] .",
    "figure [ pht2 ] shows the same results for the mean ensemble error @xmath15 as figure [ pht1 ] but now @xmath17 is variable and @xmath48 is constant .",
    "the occurring effects are again explained by the influence of the three parameters @xmath34 , @xmath17 and @xmath31 on the update probability of the condition @xmath43 .",
    "the most interesting result here is the strong dependence of the mean ensemble error @xmath15 of @xmath34 for @xmath41 . for a value of @xmath23 the exclusive - or ( xor ) problem",
    "can not be learned completely for @xmath49 even for very low noise values but only for low @xmath31 values .",
    "the situation is almost completely changed for @xmath24 . now",
    "the performance for @xmath50 is worse than for higher @xmath31 values .",
    "this reflects too high ( low ) update probabilities for @xmath23 and @xmath49 ( @xmath24 and @xmath50 ) .",
    "in recent years there is an increasing number of experimental results which investigate heterosynaptic plasticity .",
    "in contrast to homosynaptic plasticity where only the synapse between active pre- and postsynaptic neuron is changed in form of either _ long - term depression _ ( ltd ) or _ long - term potentiation _ ( ltp ) heterosynaptic plasticity concerns also further remote synapses of the pre- and postsynaptic neuron . this scenario is schematically depicted in figure [ network_heterosyn_ncounter ] . there",
    "we suppose neuron 5 and 6 were active and induced ( homo-)synaptic plasticity on the synapse which is enclosed by these neurons .",
    "in addition to this form of plasticity fitzsimonds et al .",
    "@xcite found in cultured hippocampal neurons that the induction of ltd is also accompanied by back propagation of depression in the dendrite tree of the presynaptic neuron . further more , depression also propagates laterally in the pre- and postsynaptic neuron .",
    "similar results hold for the propagation of ltp , see @xcite for a review .",
    "the correspondence to our learning rule follows immediately from the working principle of our neuron counters . in figure [ network_heterosyn_ncounter ]",
    "the neuron counters are shown as @xmath51 , @xmath52 for each neuron in the schematic network .    according to our learning rule",
    "there is a communication between the neuron counters of adjacent neurons .",
    "this communication leads to the formation of the approximated synaptic counters @xmath53 . from this",
    ", one can see that an alteration of the neuron counters @xmath54 and @xmath55 leads not only to an alteration of @xmath56 but also of all approximated synaptic counters @xmath57 , @xmath58 , @xmath59 and @xmath60 with @xmath61 , @xmath62 , @xmath63 and @xmath64 . in biological terms @xmath57 corresponds to backpropagation , @xmath58 to presynaptic lateral and @xmath59 to postsynaptic lateral spread of ltd .",
    "interestingly the term @xmath60 which would correspond to forward propagated postsynaptic ltd was not experimentally found up to now @xcite .",
    "a biological explanation for the cellular mechanisms of these findings are currently under investigation .",
    "fitzsimonds et .",
    "@xcite suggest the existence of retrograde signaling from the post- to the presynaptic neuron which could produce a secondary cytoplasmic factor for back - propagation and presynaptic lateral spread of ltd . on the postsynaptic side lateral spread of ltd could be explained similarly under the assumption that there is a blocking mechanism for the cytoplasmic factor which prevents forward propagated ltd .",
    "they are of the opinion that extracellular diffusible factors are of minor importance . in an abstract sense",
    "the approximated synaptic counters of our learning rule could be interpreted as an intracellular mechanism and not as an extracellular one .",
    "this would be consistent with the suggestions of @xcite .",
    "the future will show if further experiments confirm or reject the non - existence of forward propagated ltd . from a theoretical point of view and based on the assumptions made in this paper such a symmetry breaking mechanism occurring during the propagation of heterosynaptic ltd would be more elaborated than our stochastic hebb - like learning rule .",
    "in this article we presented a novel stochastic hebb - like learning rule for neural networks and demonstrated its working mechanism exemplary in learning the exclusive - or ( xor ) problem in a three - layer network .",
    "we investigated the convergence behavior by extensive numerical simulations in dependence of three different network dynamics which correspond all to biological forms of lateral inhibition .",
    "we found in all cases , parameter configurations for @xmath34 , the length of the neuron memory , @xmath17 , the exponent of the coin distribution and @xmath31 , the exponent of the rank ordering distribution , which constitute the hebb - like learning rule , to obtain not only a solution to the exclusive - or ( xor ) problem but comparably well results to the learning rule recently proposed by klemm , bornholdt and schuster @xcite .",
    "comparably well means that for the exclusive - or ( xor ) problem @xmath65 was always better than any parameter configuration @xmath66 for our learning rule , but for @xmath67 there are a lot of parameter configurations @xmath66 which result in a faster convergence in dependence of the time scale . in this point",
    "we agree with @xcite where they take the opinion that natural systems try to solve problems satisficing and not optimally in a mathematical sense because of the lack of information biological systems are faced due to their inherent open character . in this",
    "respect our model consists of a large variety of parameters which work similar well without the need to find the very best parameter configuration .",
    "this parameter configuration can of course be found as shown in section [ wta ] and [ softmax ] .",
    "but that does not mean that other parameter configurations does not work at all .",
    "our aim was to establish a hebb - like learning rule which is very flexible with respect to special choices of the three parameters @xmath66 .",
    "moreover our learning rule works comparably well to @xcite if one keeps in mind that our learning rule uses much less parameters . because the number of neurons is always ( much ) less then the number of synapses the same holds for the respective numbers of synaptic and neuron counters which were used in the learning rules .",
    "an interesting implication of our learning rule and its inherent stochastic character is that it offers a very simple qualitatively explanation of heterosynaptic plasticity which is observed experimentally .",
    "in addition to the experimentally observed back - propagation , pre- and postsynaptic lateral spread of _ long - term depression _ ( ltd ) our learning rule predicts forward propagated postsynaptic ltd for reasons of a symmetric communication between adjacent neurons .",
    "as far as we know there is no theoretical explanation of that phenomenon so far .",
    "in further investigations we will demonstrate that our learning rule is not restricted to a multilayer network topology but works also in a class of recurrent networks constructed by an algorithm of watts and strogatz @xcite when learning the problem of timing @xcite .",
    "moreover , it would be of interest to enlighten the power law ansatz for the rank ordering [ p_rank ] and coin distribution [ p_coin ] which was motivated by @xcite in a more general context of stochastic optimization methods for rule - based systems ."
  ],
  "abstract_text": [
    "<S> we present a novel stochastic hebb - like learning rule for neural networks . </S>",
    "<S> this learning rule is stochastic with respect to the selection of the time points when a synaptic modification is induced by pre- and postsynaptic activation . </S>",
    "<S> moreover , the learning rule does not only affect the synapse between pre- and postsynaptic neuron which is called homosynaptic plasticity but also on further remote synapses of the pre- and postsynaptic neuron . </S>",
    "<S> this form of plasticity has recently come into the light of interest of experimental investigations and is called heterosynaptic plasticity . </S>",
    "<S> our learning rule gives a qualitative explanation of this kind of synaptic modification . </S>"
  ]
}