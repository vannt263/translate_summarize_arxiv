{
  "article_text": [
    "the problem of `` scaling up for high dimensional data and high speed data streams '' is among the `` ten challenging problems in data mining research''@xcite .",
    "this paper is devoted to estimating entropy of data streams using a recent algorithm called _ compressed counting ( cc ) _",
    "this work has four components : ( 1 ) the theoretical analysis of entropies , ( 2 ) a much improved estimator for cc , ( 3 ) the bias and variance in estimating entropy , and ( 4 ) an empirical study using web crawl data .",
    "while traditional data mining algorithms often assume static data , in reality , data are often constantly updated .",
    "mining data streams@xcite in ( e.g. , ) 100 tb scale databases has become an important area of research , e.g. , @xcite , as network data can easily reach that scale@xcite .",
    "search engines are a typical source of data streams@xcite .",
    "we consider the _ turnstile _ stream model@xcite .",
    "the input stream @xmath2 , @xmath3 $ ] arriving sequentially describes the underlying signal @xmath4 , meaning @xmath5 = a_{t-1}[i_t ] + i_t,\\end{aligned}\\ ] ] where the increment @xmath6 can be either positive ( insertion ) or negative ( deletion ) .",
    "for example , in an online store , @xmath7 $ ] may record the total number of items that user @xmath8 has ordered up to time @xmath9 and @xmath6 denotes the number of items that this user orders ( @xmath10 ) or cancels ( @xmath11 ) at @xmath12 . if each user is identified by the ip address , then potentially @xmath13 .",
    "it is often reasonable to assume @xmath14\\geq 0 $ ] , although @xmath6 may be either negative or positive . restricting @xmath14\\geq 0 $ ] results in the _ strict - turnstile",
    "_ model , which suffices for describing almost all natural phenomena .",
    "for example , in an online store , it is not possible to cancel orders that do not exist .    *",
    "compressed counting ( cc ) * assumes a _ relaxed strict - turnstile _",
    "model by only enforcing @xmath14\\geq0 $ ] at the @xmath12 one cares about . at other times",
    "@xmath15 , @xmath16 $ ] can be arbitrary .",
    "the @xmath0th frequency moment is a fundamental statistic : @xmath17^\\alpha .",
    "\\end{aligned}\\ ] ] when @xmath18 , @xmath19 is the sum of the stream .",
    "it is obvious that one can compute @xmath19 exactly and trivially using a simple counter , because @xmath20 = \\sum_{s=0}^t i_s$ ] .",
    "@xmath21 is basically a histogram and we can view @xmath22}{\\sum_{i=1}^d a_t[i ] } = \\frac{a_t[i]}{f_{(1)}}\\end{aligned}\\ ] ] as probabilities .",
    "a useful ( e.g. , in web and networks@xcite and neural comptutations@xcite ) summary statistic is * shannon entropy * @xmath23}{f_{(1)}}\\log \\frac{a_t[i]}{f_{(1 ) } } = -\\sum_{i=1}^d p_i\\log p_i.\\end{aligned}\\ ] ] various generalizations of the shannon entropy exist .",
    "the rnyi entropy@xcite , denoted by @xmath24 , is defined as @xmath25^\\alpha}{\\left(\\sum_{i=1}^d a_t[i]\\right)^\\alpha } = -\\frac{1}{\\alpha-1}\\log \\sum_{i=1}^dp_i^\\alpha\\end{aligned}\\ ] ] the tsallis entropy@xcite , denoted by @xmath26 , is defined as , @xmath27 as @xmath1 , both rnyi entropy and tsallis entropy converge to shannon entropy : @xmath28 thus , both rnyi entropy and tsallis entropy can be computed from the @xmath0th frequency moment ; and one can approximate shannon entropy from either @xmath24 or @xmath26 by using @xmath29 .",
    "several studies@xcite ) used this idea to approximate shannon entropy .",
    "network traffic is a typical example of high - rate data streams .",
    "an effective and reliable measurement of network traffic in real - time is crucial for anomaly detection and network diagnosis ; and one such measurement metric is shannon entropy@xcite .",
    "the _ turnstile _ data stream model ( [ eqn_turnstile ] ) is naturally suitable for describing network traffic , especially when the goal is to characterize the statistical distribution of the traffic . in its empirical form",
    ", a statistical distribution is described by histograms , @xmath14 $ ] , @xmath30 to @xmath31 .",
    "it is possible that @xmath32 ( ipv6 ) if one is interested in measuring the traffic streams of unique source or destination .",
    "the distributed denial of service ( * ddos * ) attack is a representative example of network anomalies .",
    "a ddos attack attempts to make computers unavailable to intended users , either by forcing users to reset the computers or by exhausting the resources of service - hosting sites . for example , hackers may maliciously saturate the victim machines by sending many external communication requests",
    ". ddos attacks typically target sites such as banks , credit card payment gateways , or military sites .    a ddos attack changes the statistical distribution of network traffic .",
    "therefore , a common practice to detect an attack is to monitor the network traffic using certain summary statics .",
    "since shannon entropy is a well - suited for characterizing a distribution , a popular detection method is to measure the time - history of entropy and alarm anomalies when the entropy becomes abnormal@xcite .",
    "entropy measurements do not have to be `` perfect '' for detecting attacks .",
    "it is however crucial that the algorithm should be computationally efficient at low memory cost , because the traffic data generated by large high - speed networks are enormous and transient ( e.g. , 1 gbits / second ) .",
    "algorithms should be real - time and one - pass , as the traffic data will not be stored@xcite .",
    "many algorithms have been proposed for `` sampling '' the traffic data and estimating entropy over data streams@xcite ,      in high - speed networks , anomaly events including network failures and ddos attacks may not always be detected by simply monitoring the traditional traffic matrix because the change of the total traffic volume is sometimes small .",
    "one strategy is to measure the entropies of all origin - destination ( od ) flows@xcite .",
    "an od flow is the traffic entering an ingress point ( origin ) and exiting at an egress point ( destination ) .",
    "@xcite showed that measuring entropies of od flows involves measuring the intersection of two data streams , whose moments can be decomposed into the moments of individual data streams ( to which cc is applicable ) and the moments of the absolute difference between two data streams .",
    "the recent work@xcite was devoted to estimating the shannon entropy of msn search logs , to help answer some basic problems in web search , such as , _ how big is the web ?",
    "_    the search logs can be viewed as data streams , and @xcite analyzed several `` snapshots '' of a sample of msn search logs .",
    "the sample used in @xcite contained 10 million @xmath33query , url , ip@xmath34 triples ; each triple corresponded to a click from a particular ip address on a particular url for a particular query .",
    "@xcite drew their important conclusions on this ( hopefully ) representative sample .",
    "alternatively , one could apply data stream algorithms such as cc on the whole history of msn ( or other search engines ) .",
    "a workshop in nips03 was denoted to entropy estimation , owing to the wide - spread use of shannon entropy in neural computations@xcite .",
    "( http://www.menem.com/~ilya/pages/nips03 ) for example , one application of entropy is to study the underlying structure of spike trains .      because the elements , @xmath14 $ ] , are time - varying",
    ", a nave counting mechanism requires a system of @xmath31 counters to compute @xmath35 exactly ( unless @xmath36 ) .",
    "this is not always realistic .",
    "estimating @xmath35 in data streams is heavily studied@xcite .",
    "we have mentioned that computing @xmath19 in _ strict - turnstile _",
    "model is trivial using a simple counter .",
    "one might naturally speculate that when @xmath29 , computing ( approximating ) @xmath35 should be also easy .",
    "however , before _ compressed counting ( cc ) _",
    ", none of the prior algorithms could capture this intuition .",
    "cc improves _",
    "symmetric stable random projections_@xcite uniformly for all @xmath37 as shown in figure [ fig_comp_var_factor ] in section [ sec_cc ] . however , one can still considerably improve cc around @xmath36 , by developing better estimators , as in this study .",
    "in addition , no empirical studies on cc were reported .",
    "@xcite applied _ symmetric stable random projections _ to approximate the moments and shannon entropy . the nice theoretical work @xcite provided the criterion to choose the @xmath0 so that shannon entropy can be approximated with a guaranteed accuracy , using the @xmath0th frequency moment .",
    "* * we prove that using rnyi entropy to estimate shannon entropy has ( much ) smaller bias than using tsallis entropy . *",
    "when data follow a common zipf distribution , the difference could be a magnitude . *",
    "* we provide a much improved estimator .",
    "* cc boils down to a statistical estimation problem .",
    "the new estimator based on _ optimal quantiles _ exhibits considerably smaller variance when @xmath38 , compared to @xcite .",
    "* * we demonstrate the bias - variance trade - off in estimating shannon entropy * , important for choosing the sample size and how small @xmath39 should be . *",
    "* we supply an empirical study .",
    "*      section [ sec_data ] illustrates what entropies are like in real data .",
    "section [ sec_limit ] includes some theoretical studies of entropy .",
    "the methodologies of cc and two estimators are reviewed in section [ sec_cc ] .",
    "the new estimator based on the _ optimal quantiles _ is presented in section [ sec_quantile ] .",
    "we analyze in section [ sec_entropy_est ] the biases and variances in estimating entropies .",
    "experiments on real web crawl data are presented in section [ sec_exp ] .",
    "since the estimation accuracy is what we are interested in , we can simply use static data instead of real data streams .",
    "this is because at the time @xmath12 , @xmath40^\\alpha$ ] is the same at the end of the stream , regardless whether it is collected at once ( i.e. , static ) or incrementally ( i.e. , dynamic ) .",
    "ten english words are selected from a chunk of web crawl data with @xmath41 pages .",
    "the words are selected fairly randomly , except that we make sure they cover a whole range of sparsity , from function words ( e.g. , a , the ) , to common words ( e.g. , friday ) to rare words ( e.g. , twist ) .",
    "the data are summarized in table [ tab_data ] .",
    "l l l l l l l + word & nonzero & @xmath42 & @xmath43 & @xmath44 & @xmath45 & @xmath46 +   + twist & 274 & 5.4873 & 5.4962 & 5.4781 & 6.3256 & 4.7919 + rice & 490 & 5.4474 & 5.4997 & 5.3937 & 6.3302 & 4.7276 + friday & 2237 & 7.0487 & 7.1039 & 6.9901 & 8.5292 & 5.8993 + fun & 3076 & 7.6519 & 7.6821 & 7.6196 & 9.3660 & 6.3361 + business & 8284 & 8.3995 & 8.4412 & 8.3566 & 10.502 & 6.8305 + name & 9423 & 8.5162 & 9.5677 & 8.4618 & 10.696 & 6.8996 + have & 17522 & 8.9782 & 9.0228 & 8.9335 & 11.402 & 7.2050 + this & 27695 & 9.3893 & 9.4370 & 9.3416 & 12.059 & 7.4634 + a & 39063 & 9.5463 & 9.5981 & 9.4950 & 12.318 & 7.5592 + the & 42754 & 9.4231 & 9.4828 & 9.3641 & 12.133 & 7.4775 +    [ tab_data ]     +    figure [ fig_entropy ] selects two words to compare their shannon entropies @xmath42 , rny entropies @xmath24 , and tsallis entropies @xmath26 .",
    "clearly , although both approach shannon entropy , rny entropy is much more accurate than tsallis entropy .",
    "this section presents two lemmas , proved in the appendix .",
    "lemma [ lem_bias ] says rnyi entropy has smaller bias than tsallis entropy for estimating shannon entropy .",
    "[ lem_bias ] @xmath47    lemma [ lem_bias ] does not say precisely how much better . note that when @xmath48 , the magnitudes of @xmath49 and @xmath50 are largely determined by the first derivatives ( slopes ) of @xmath24 and @xmath26 , respectively , evaluated at @xmath48 .",
    "lemma [ lem_limit ] directly compares their first and second derivatives , as @xmath48 .",
    "[ lem_limit ] as @xmath1 , @xmath51    lemma [ lem_limit ] shows that in the limit , @xmath52 , verifying that @xmath24 should have smaller bias than @xmath26 . also , @xmath53 .",
    "two special cases are interesting .      in this case ,",
    "@xmath54 for all @xmath8 .",
    "it is easy to show that @xmath55 regardless of @xmath0 .",
    "thus , when the data distribution is close to be uniform , rnyi entropy will provide nearly perfect estimates of shannon entropy .      in web and nlp applications@xcite ,",
    "the zipf distribution is common : @xmath56 .",
    ", @xmath57 .",
    "the curves largely overlap and hence we do not label the curves .",
    ", width=153 ]    figure [ fig_ratio ] plots the ratio , @xmath58 . at @xmath59 ( which is common@xcite ) ,",
    "the ratio is about @xmath60 , meaning that the bias of rnyi entropy could be a magnitude smaller than that of tsallis entropy , in common data sets .",
    "compressed counting ( cc ) assumes the _ relaxed strict - turnstile _ data stream model .",
    "its underlying technique is based on _ maximally - skewed stable random projections_.      a random variable @xmath61 follows a maximally - skewed @xmath0-stable distribution if the fourier transform of its density is@xcite @xmath62 where @xmath63 , @xmath64 , and @xmath65 .",
    "we denote @xmath66 . the skewness parameter @xmath67 for general stable distributions ranges in @xmath68 $ ] ; but cc uses @xmath65 , i.e. , * maximally - skewed*. previously , the method of _ symmetric stable random projections_@xcite used @xmath69 .",
    "consider two independent variables , @xmath70 .",
    "for any non - negative constants @xmath71 and @xmath72 , the `` @xmath0-stability '' follows from properties of fourier transforms : @xmath73 note that if @xmath69 , then the above stability holds for any constants @xmath71 and @xmath72 .",
    "we should mention that one can easily generate samples from a stable distribution@xcite .",
    "conceptually , one can generate a matrix @xmath74 and multiply it with the data stream @xmath21 , i.e. , @xmath75 .",
    "the resultant vector @xmath76 is only of length @xmath77 .",
    "the entries of @xmath78 , @xmath79 , are i.i.d .",
    "samples of a stable distribution @xmath80 .    by property of fourier transforms ,",
    "the entries of @xmath76 , @xmath81 @xmath82 to @xmath77 , are i.i.d .",
    "samples of a stable distribution @xmath83_j = \\sum_{i=1}^d r_{ij } a_t[i]%\\\\   \\sim s\\left(\\alpha,\\beta=1,f_{(\\alpha ) } = \\sum_{i=1}^d a_t[i]^\\alpha\\right),\\end{aligned}\\ ] ] whose scale parameter @xmath35 is exactly the @xmath0th moment . thus , cc boils down to a statistical estimation problem .    for real implementations , one should conduct @xmath84 incrementally .",
    "this is possible because the _ turnstile _ model ( [ eqn_turnstile ] ) is a linear updating model .",
    "that is , for every incoming @xmath2 , we update @xmath85 for @xmath82 to @xmath77 . entries of @xmath78 are generated on - demand as necessary .",
    "@xcite commented that , when @xmath77 is large , generating entries of @xmath78 on - demand and multiplications @xmath86 , @xmath82 to @xmath77 , can be prohibitive .",
    "an easy `` fix '' is to use @xmath77 as small as possible , which is possible with cc when @xmath29 .    at the same @xmath77 , all procedures of cc and _ symmetric stable random projections _",
    "are the same except the entries in @xmath78 follow different distributions .",
    "however , since cc is much more accurate especially when @xmath29 , it requires a much smaller @xmath77 at the same level of accuracy .",
    "cc boils down to estimating @xmath35 from @xmath77 i.i.d .",
    "samples @xmath87 .",
    "@xcite provided two estimators .",
    "the asymptotic ( i.e. , as @xmath88 ) variance is @xmath89 as @xmath1 , the asymptotic variance approaches zero .      which is asymptotically unbiased and has variance @xmath90    @xmath91 is defined only for @xmath92 and is considerably more accurate than the geometric mean estimator @xmath93 .",
    "the two estimators for cc in @xcite dramatically reduce the estimation variances compared to _",
    "symmetric stable random projections_. they are , however , are not quite adequate for estimating shannon entropy using small ( @xmath77 ) samples .",
    "we discover that an estimator based on the _ sample quantiles _ considerably improves @xcite when @xmath29 .",
    "given @xmath77 i.i.d samples @xmath94 , we define the @xmath95-quantile is to be the @xmath96th smallest of @xmath97 . for example , when @xmath98 , then @xmath99th quantile is the smallest among @xmath97 s .    to understand why the quantile works ,",
    "consider the normal @xmath100 , which is a special case of stable distribution with @xmath101 .",
    "we can view @xmath102 , where @xmath103 .",
    "therefore , we can use the ratio of the @xmath95th quantile of @xmath81 over the @xmath95-th quantile of @xmath104 to estimate @xmath105 . note that @xmath35 corresponds to @xmath106 , not @xmath105 .",
    "assume @xmath94 , @xmath82 to @xmath77 .",
    "one can sort @xmath97 and use the @xmath96th smallest @xmath97 as the estimate , i.e. , @xmath107 denote @xmath108 , where @xmath109 .",
    "denote the probability density function of @xmath61 by @xmath110 , the probability cumulative function by @xmath111 , and the inverse by @xmath112 .",
    "the asymptotic variance of @xmath113 is presented in lemma [ lem_q_var ] , which follows directly from known statistics results , e.g. , ( * ? ? ?",
    "* theorem 9.2 ) .",
    "[ lem_q_var ] @xmath114    we can then choose @xmath115 to minimize the asymptotic variance .",
    "we denote the optimal quantile estimator by @xmath116 .",
    "the optimal quantiles , denoted by @xmath117 , has to be determined by numerically and tabulated ( as in table [ tab_oq ] ) , because the density functions do not have an explicit closed - form .",
    "we used the * fbasics * package in * r*. we , however , found the implementation of those functions had numerical problems when @xmath118 and @xmath119 .",
    "table [ tab_oq ] provides the numerical values for @xmath120 , @xmath121 ( [ eqn_w ] ) , and the variance of @xmath122 ( without the @xmath123 term ) .    .in order to use the optimal quantile estimator , we tabulate the constants @xmath120 and @xmath121 . [ cols=\"<,<,<,<\",options=\"header \" , ]     [ tab_oq ]      figure [ fig_comp_var_factor ] ( left panel ) compares the variances of the three estimators for cc . to better illustrate the improvements , figure [ fig_comp_var_factor ] ( right panel ) plots the ratios of the variances .",
    "when @xmath124 , the _ optimal _ quantile reduces the variances by a factor of 70 ( compared to the _ geometric mean _",
    "estimator ) , or 20 ( compared to the _ harmonic mean _",
    "estimator ) .",
    "this section analyzes the biases and variances in estimating shannon entropy .",
    "also , we provide the criterion for choosing the sample size @xmath77 .",
    "we use @xmath125 , @xmath126 , and @xmath127 to denote generic estimators .",
    "since @xmath125 is ( asymptotically ) unbiased , @xmath129 and @xmath130 are also asymptotically unbiased .",
    "the asymptotic variances of @xmath129 and @xmath130 can be computed by taylor expansions : @xmath131    we use @xmath132 and @xmath133 to denote the estimators for shannon entropy using the estimated @xmath129 and @xmath130 , respectively .",
    "the variances remain unchanged , i.e. , @xmath134    however , @xmath132 and @xmath133 are no longer ( asymptotically ) unbiased , because @xmath135 the @xmath136 biases arise from the estimation biases and diminish quickly as @xmath77 increases .",
    "however , the `` intrinsic biases , '' @xmath137 and @xmath138 , can not be reduced by increasing @xmath77 ; they can only be reduced by letting @xmath0 close to 1 .",
    "the total error is usually measured by the mean square error : mse = bias@xmath139 + var .",
    "clearly , there is a variance - bias trade - off in estimating @xmath42 using @xmath24 or @xmath26 .",
    "the optimal @xmath0 is data - dependent and hence some prior knowledge of the data is needed in order to determine it .",
    "the prior knowledge may be accumulated during the data stream process .",
    "experiments on real data ( i.e. , table [ tab_data ] ) can further demonstrates the effectiveness of compressed counting ( cc ) and the new _ optimal quantile _ estimator .",
    "we could use static data to verify cc because we only care about the estimation accuracy , which is same regardless whether the data are collected at one time ( static ) or dynamically .",
    "we present the results for estimating frequency moments and shannon entropy , in terms of the normalized mses .",
    "we observe that the results are quite similar across different words ; and hence only one word is selected for the presentation .",
    "figure [ fig_rice_f ] provides the normalized mses ( by @xmath140 ) for estimating the @xmath0th frequency moments , @xmath35 , for word rice :    * the errors of the three estimators for cc decrease ( to zero , potentially ) as @xmath1 .",
    "the improvement of cc over _ symmetric stable random projections _ is enormous . *",
    "the optimal quantile estimator @xmath122 is in general more accurate than the geometric mean and harmonic mean estimators near @xmath141 .",
    "however , for small @xmath77 and @xmath142 , @xmath122 exhibits bad behaviors , which disappear when @xmath143 . *",
    "the theoretical asymptotic variances in ( [ eqn_f_gm_var ] ) , ( [ eqn_f_hm_var ] ) , and table [ tab_oq ] are accurate .",
    "+      figure [ fig_rice_hr ] provides the mses from estimating the shannon entropy using the rnyi entropy , for word rice :    * using _ symmetric stable random projections _ with @xmath0 close to 1 is not a good strategy and not practically feasible because the required sample size is enormous .",
    "* there is clearly a variance - bias trade - off , especially for the _ geometric mean _ and _ harmonic mean _ estimators .",
    "that is , for each @xmath77 , there is an `` optimal '' @xmath0 which achieves the smallest mse .",
    "* using the _ optimal quantile _ estimator does not show a strong variance - bias trade - off , because its has very small variance near @xmath36 and its mses are mainly dominated by the ( intrinsic ) biases , @xmath137 .",
    "+    figure [ fig_rice_ht ] presents the mses for estimating shannon entropy using tsallis entropy .",
    "the effect of the variance - bias trade - off for geometric mean and harmonic mean estimators , is even more significant , because the ( intrinsic ) bias @xmath138 is much larger .",
    "web search data and network data are naturally data streams .",
    "the entropy is a useful summary statistic and has numerous applications , e.g. , network anomaly detection . efficiently and accurately computing the entropy in large and frequently updating data streams , in one - pass , is an active topic of research .",
    "a recent trend is to use the @xmath0th frequency moments with @xmath29 to approximate shannon entropy .",
    "we conclude :    * we should use rnyi entropy to approximate shannon entropy .",
    "using tsallis entropy will result in about a magnitude larger bias in a common data distribution . *",
    "the _ optimal quantile _",
    "estimator for cc reduces the variances by a factor of 20 or 70 when @xmath144 , compared to the estimators in @xcite . * when _ symmetric stable random projections _ must be used",
    ", we should exploit the variance - bias trade - off , by not using @xmath0 very close 1 .",
    "@xmath145 and @xmath146 . note that @xmath147 , @xmath148 if @xmath92 and @xmath149 if @xmath142 .    for @xmath150 , @xmath151 always holds , with equality when @xmath152 .",
    "therefore , @xmath153 when @xmath92 and @xmath154 when @xmath142 .",
    "also , we know @xmath155 . therefore , to show @xmath156 , it suffices to show that both @xmath26 and @xmath24 are decreasing functions of @xmath157 .",
    "taking the first derivatives of @xmath26 and @xmath24 yields @xmath158 to show @xmath159 , it suffices to show that @xmath160 .",
    "taking derivative of @xmath161 yields , @xmath162 i.e. , @xmath163 if @xmath92 and @xmath164 if @xmath142 . because @xmath165 , we know @xmath160 .",
    "this proves @xmath166 .   to show @xmath167",
    ", it suffices to show that @xmath168 , where @xmath169 note that @xmath170 and hence we can view @xmath171 as probabilities . since @xmath172 is a concave function , we can use jensen s inequality : @xmath173 , to obtain @xmath174",
    "as @xmath1 , using lhopital s rule @xmath175^\\prime}{\\left[(\\alpha-1)^2\\right]^\\prime}\\\\\\notag = & \\lim_{\\alpha\\rightarrow1 }   \\frac{- ( \\alpha-1)\\sum_{i=1}^d p_i^\\alpha \\log^2p_i}{2(\\alpha-1)}=-\\frac{1}{2}\\sum_{i=1}^d p_i \\log^2 p_i.\\end{aligned}\\ ] ] note that , as @xmath48 , @xmath176 but @xmath177 .      again , applying lhopital s rule yields the expressions for @xmath180 and @xmath181 @xmath182^\\prime}{\\left[(\\alpha-1)^2\\sum_{i=1}^dp_i^\\alpha\\right]^\\prime}\\\\\\notag = & \\lim_{\\alpha\\rightarrow 1 } \\frac{\\left[\\sum_{i=1}^dp_i^\\alpha\\log p_i \\log \\sum_{i=1}^dp_i^\\alpha   - ( \\alpha-1)\\sum_{i=1}^d p_i^\\alpha \\log^2 p_i\\right]}{\\left[2(\\alpha-1)\\sum_{i=1}^dp_i^\\alpha + ( \\alpha-1)^2\\sum_{i=1}^dp_i^\\alpha\\log p_i \\right]}\\\\\\notag = & \\lim_{\\alpha\\rightarrow 1 } \\frac{\\left(\\sum_{i=1}^dp_i^\\alpha\\log p_i\\right)^2/\\sum_{i=1}^dp_i^\\alpha - \\sum_{i=1}^d p_i^\\alpha \\log^2 p_i + \\text{negligible terms}}{2\\sum_{i=1}^dp_i^\\alpha + \\text{negligible terms}}\\\\\\notag = & \\frac{1}{2}\\left(\\sum_{i=1}^dp_i\\log p_i\\right)^2 -\\frac{1}{2}\\sum_{i=1}^d p_i \\log^2 p_i\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> the shannon entropy is a widely used summary statistic , for example , network traffic measurement , anomaly detection , neural computations , spike trains , etc . </S>",
    "<S> this study focuses on estimating shannon entropy of data streams . </S>",
    "<S> it is known that shannon entropy can be approximated by rnyi entropy or tsallis entropy , which are both functions of the @xmath0th frequency moments and approach shannon entropy as @xmath1 . * * _ compressed counting ( cc)_**@xcite is a new method for approximating the @xmath0th frequency moments of data streams . </S>",
    "<S> our contributions include :    * we prove that rnyi entropy is ( much ) better than tsallis entropy for approximating shannon entropy . </S>",
    "<S> * we propose the _ optimal quantile _ estimator for cc , which considerably improves the estimators in @xcite . * </S>",
    "<S> our experiments demonstrate that cc is indeed highly effective in approximating the moments and entropies . </S>",
    "<S> we also demonstrate the crucial importance of utilizing the variance - bias trade - off . </S>"
  ]
}