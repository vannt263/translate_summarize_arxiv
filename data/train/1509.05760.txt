{
  "article_text": [
    "online convex optimization algorithms represent key tools in modern machine learning .",
    "these are flexible algorithms used for solving a variety of optimization problems in classification , regression , ranking and probabilistic inference .",
    "these algorithms typically process one sample at a time with an update per iteration that is often computationally cheap and easy to implement . as a result",
    ", they can be substantially more efficient both in time and space than standard batch learning algorithms , which often have optimization costs that are prohibitive for very large data sets .    in the standard scenario of online convex optimization @xcite , at each round @xmath0",
    ", the learner selects a point @xmath1 out of a compact convex set @xmath2 and incurs loss @xmath3 , where @xmath4 is a convex function defined over @xmath2 .",
    "the learner s objective is to find an algorithm @xmath5 that minimizes the regret with respect to a fixed point @xmath6 : @xmath7 that is the difference between the learner s cumulative loss and the loss in hindsight incurred by @xmath6 , or with respect to the loss of the best @xmath6 in @xmath2 , @xmath8",
    ". we will assume only that the learner has access to the gradient or an element of the sub - gradient of the loss functions @xmath4 , but that the loss functions @xmath4 can be arbitrarily singular and flat , e.g.  not necessarily strongly convex or strongly smooth .",
    "this is the most general setup of convex optimization in the full information setting .",
    "it can be applied to standard convex optimization and online learning tasks as well as to many optimization problems in machine learning such as those of svms , logistic regression , and ridge regression .",
    "favorable bounds in online convex optimization can also be translated into strong learning guarantees in the standard scenario of batch supervised learning using online - to - batch conversion guarantees @xcite .    in the scenario of online convex optimization just presented ,",
    "minimax optimal rates can be achieved by standard algorithms such as online gradient descent @xcite .",
    "however , general minimax optimal rates may be too conservative .",
    "recently , _ adaptive regularization _ methods have been introduced for standard descent methods to achieve tighter data - dependent regret bounds ( see @xcite , @xcite , @xcite , @xcite , @xcite ) .",
    "specifically , in the `` adagrad '' framework of @xcite , there exists a sequence of convex functions @xmath9 such that the update @xmath10 yields regret : @xmath11 where @xmath12 is an element of the subgradient of @xmath4 at @xmath1 , @xmath13 , and @xmath14 is the bregman divergence defined using the convex function @xmath9 .",
    "this upper bound on the regret has shown to be within a factor @xmath15 of the optimal a posteriori regret : @xmath16 note , however , that this upper bound on the regret can still be very large , even if the functions @xmath4 admit some favorable properties ( e.g.@xmath17 , linear ) .",
    "this is because the dependence is directly on the norm of @xmath18s .",
    "an alternative line of research has been investigated by a series of recent publications that have analyzed online learning in `` slowly - varying '' scenarios @xcite . in the framework of @xcite ,",
    "if @xmath19 is a self - concordant function , @xmath20 is the semi - norm induced by its hessian at the point @xmath1 , is defined for any @xmath21 by @xmath22 . ] and @xmath23 is a `` prediction '' of a time @xmath24 subgradient @xmath25 based on information up to time @xmath26 , then one can obtain regret bounds of the following form : @xmath27 here , @xmath28 denotes the dual norm of @xmath29 : for any @xmath21 , @xmath30 .",
    "this guarantee can be very favorable in the _ optimistic _ case where @xmath31 for all @xmath26 .",
    "nevertheless , it admits the drawback that much less control is available over the induced norm since it is difficult to predict , for a given self - concordant function @xmath19 , the behavior of its hessian at the points @xmath1 selected by an algorithm .",
    "moreover , there is no guarantee of `` near - optimality '' with respect to an optimal a posteriori regularization as there is with the adaptive algorithm .",
    "this paper presents a powerful general framework for designing online convex optimization algorithms combining adaptive regularization and optimistic gradient prediction which helps address several of the issues just pointed out .",
    "our framework builds upon and unifies recent techniques in adaptive regularization , optimistic gradient predictions , and problem - dependent randomization . in section  [ sec : ao_ftrl ] , we describe a series of _ adaptive and optimistic _ algorithms for which we prove strong regret guarantees , including a new _ adaptive and optimistic follow - the - regularized - leader _ ( ao - ftrl ) algorithm ( section  [ sec : ao_ftrl_algo ] ) and a more general version of this algorithm with composite terms ( section  [ sec : cao_ftrl_algo ] ) .",
    "these new regret guarantees hold at any time and under very minimal assumptions .",
    "we also show how different relaxations recover both basic existing algorithms as well as more recent sophisticated ones . in a specific application",
    ", we will also show how a certain choice of regularization functions will produce an optimistic regret bound that is also nearly a posteriori optimal , combining the two different desirable properties mentioned above .",
    "lastly , in section  [ sec : aos_ftrl ] , we further combine adaptivity and optimism with problem - dependent randomization to devise algorithms benefitting from more favorable guarantees than recent state - of - the - art methods .",
    "in view of the discussion in the previous section , we present an adaptive and optimistic version of the follow - the - regularized - leader ( ftrl ) family of algorithms . in each round of standard ftrl ,",
    "a point is chosen that is the minimizer of the average linearized loss incurred plus a regularization term . in our new version of ftrl",
    ", we will find a minimizer of not only the average loss incurred , but also a prediction of the next round s loss . in addition , we will define a dynamic time - varying sequence of regularization functions that can be used to optimize against this new loss term .",
    "algorithm  [ alg : ao_ftrl ] shows the pseudocode of our adaptive and optimistic follow - the - regularized - leader ( ao - ftrl ) algorithm .    *",
    "input : * regularization function @xmath32 .",
    "* initialize : * @xmath33 , @xmath34 .",
    "compute @xmath12 .",
    "construct regularizer @xmath35 .",
    "predict  gradient  @xmath36 .",
    "update @xmath37 .",
    "the following result provides a regret guarantee for the algorithm when one uses proximal regularizers , i.e.  functions @xmath38 such that @xmath39 .",
    "[ ao - ftrl - prox ] [ th : ao_ftrl - prox ] let @xmath40 be a sequence of proximal non - negative functions , and let @xmath41 be the learner s estimate of @xmath18 given the history of functions @xmath42 and points @xmath43 . assume further that the function @xmath44 is 1-strongly convex with respect to some norm @xmath45",
    "( i.e. @xmath46 is 1-strongly convex with respect to @xmath45 ) .",
    "then , the following regret bound holds for ao - ftrl ( algorithm  [ alg : ao_ftrl ] ) : @xmath47    recall that @xmath48 , and let @xmath49 then , by convexity , the following inequality holds : @xmath50 now , we first prove by induction on @xmath51 that for all @xmath52 the following inequality holds : @xmath53 for @xmath54 , since @xmath33 and @xmath55 , the inequality follows by the definition of @xmath56 . now",
    ", suppose the inequality holds at iteration @xmath51 .",
    "then , we can write @xmath57 \\\\ & \\quad \\quad",
    "\\quad + \\tilde{g}_{t+1 } \\cdot ( x_{t+1 } - y_{t+1 } ) +   g_{t+1 } \\cdot y_{t+1 } \\\\ & \\quad \\leq",
    "\\left [ \\sum_{t = 1}^{t } g_t \\cdot x_{t+1 } + r_{0:t}(x_{t+1 } ) \\right ] \\\\ & \\quad \\quad \\quad + \\tilde{g}_{t+1 } \\cdot ( x_{t+1 } - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\text { ( by the induction hypothesis for $ x = x_{t+1}$ ) } \\\\ & \\quad \\leq \\left [ \\left ( g_{1:t } + \\tilde{g}_{t+1 } \\right ) \\cdot x_{t+1 } + r_{0:t+1}(x_{t+1 } ) \\right ] \\\\ & \\quad \\quad \\quad + \\tilde{g}_{t+1 } \\cdot ( - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\text { ( since $ r_t \\geq 0 $ , $ \\forall t$ ) } \\\\ & \\quad \\leq \\left [ \\left ( g_{1:t } + \\tilde{g}_{t+1 } \\right ) \\cdot y_{t+1 } + r_{0:t+1}(y_{t+1 } ) \\right ] \\\\ & \\quad \\quad",
    "\\quad + \\tilde{g}_{t+1 } \\cdot ( - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 }   \\\\ & \\qquad \\text { ( by definition of $ x_{t+1}$ ) } \\\\ & \\quad \\leq g_{1:t+1 } \\cdot y + r_{0:t+1}(y ) , \\text { for any $ y$.}\\\\ & \\qquad \\text { ( by definition of $ y_{t+1}$)}\\end{aligned}\\ ] ] thus , we have that @xmath58 and it suffices to bound @xmath59 .",
    "notice that , by duality , one can immediately write @xmath60 . to bound @xmath61 in terms of the gradient , recall first that since @xmath38 is proximal and @xmath62 , @xmath63 the fact that @xmath64 is @xmath65-strongly convex with respect to the norm @xmath45 implies that @xmath66 is as well . in particular , it is @xmath65-strongly convex at the points @xmath1 and @xmath67 .",
    "but this then implies that the conjugate function is @xmath65-strongly smooth on the image of the gradient , including at @xmath68 and @xmath69 ( see lemma  [ lem : smooth_cvx_duality ] in the appendix or @xcite for a general reference ) , which means that @xmath70    since @xmath71 and @xmath72 , we have that @xmath73 .",
    "the regret bound just presented can be vastly superior to the adaptive methods of @xcite , @xcite , and others .",
    "for instance , one common choice of gradient prediction is @xmath74 , so that for slowly varying gradients ( e.g. nearly `` flat '' functions ) , @xmath75 , but @xmath76 .",
    "moreover , for reasonable gradient predictions , @xmath77 generally , so that in the worst case , algorithm  [ alg : ao_ftrl ] s regret will be at most a factor of two more than standard methods . at the same time , the use of non self - concordant regularization allows one to more explicitly control the induced norm in the regret bound as well as provide more efficient updates than those of @xcite .",
    "section  [ sec : aogd ] presents an upgraded version of online gradient descent as an example , where our choice of regularization allows our algorithm to _ accelerate _ as the gradient predictions become more accurate .",
    "note that the assumption of strong convexity of @xmath78 is not a significant constraint , as any quadratic or entropic regularizer from the standard mirror descent algorithms will satisfy this property .",
    "moreover , if the loss functions @xmath79 themselves are @xmath65-strongly convex , then one can set @xmath80 and still get a favorable induced norm @xmath81 .",
    "if the gradients and gradient predictions are uniformly bounded , this recovers the worst - case @xmath82 regret bounds . at the same time , algorithm  [ alg : ao_ftrl ] would also still retain the potentially highly favorable data - dependent and optimistic regret bound .",
    "liang and steinhardt ( 2014 ) @xcite also studied adaptivity and optimism in online learning in the context of mirror descent - type algorithms . if , in the proof above , we assume their condition : @xmath83 then we obtain the following regret bound : @xmath84 our algorithm , however , is generally easier to use since it holds for any sequence of regularization functions and does not require checking for that condition .    in some cases , it may be preferable to use non - proximal adaptive regularization . since non - adaptive non - proximal ftrl corresponds to dual averaging , this scenario arises , for instance , when one wishes to use regularizers such as the negative entropy to derive algorithms from the exponentiated gradient ( eg ) family ( see @xcite for background ) .",
    "we thus present the following theorem for this family of algorithms : adaptive optimistic follow - the - regularized - leader - general version ( ao - ftrl - gen ) .",
    "[ ao - ftrl - gen ] [ th : ao_ftrl - gen ] let @xmath40 be a sequence of non - negative functions , and let @xmath41 be the learner s estimate of @xmath18 given the history of functions @xmath42 and points @xmath43 . assume further that the function @xmath44 is 1-strongly convex with respect to some norm @xmath45",
    "( i.e. @xmath46 is 1-strongly convex wrt @xmath45 ) .",
    "then , the following regret bound holds for ao - ftrl ( algorithm  [ alg : ao_ftrl ] ) : @xmath85    due to spatial constraints , the proof of this theorem , as well as that of all further results in the remainder of section  [ sec : ao_ftrl ] , are presented in appendix  [ app : ao_ftrl ] .    as in the case of proximal regularization ,",
    "algorithm  [ alg : ao_ftrl ] applied to general regularizers still admits the same benefits over the standard adaptive algorithms .",
    "in particular , the above algorithm is an easy upgrade over any dual averaging algorithm .",
    "section  [ sec : aoeg ] illustrates one such example for the exponentiated gradient algorithm .    with the following suitable choices of the parameters in theorem  [ th : cao_ftrl - prox ] ,",
    "the following regret bounds can be recovered :    1 .",
    "adaptive ftrl - prox of @xcite ( up to a constant factor of 2 ) : @xmath86 .",
    "primal - dual adagrad of @xcite : @xmath87 , @xmath86 .",
    "optimistic ftrl of @xcite : @xmath88 where @xmath89 and @xmath19 a self - concordant function , @xmath90 .",
    "[ cor : aogd ] let @xmath91 $ ] be an @xmath92-dimensional rectangle , and denote @xmath93 . set @xmath94    then , if we use the martingale - type gradient prediction @xmath95 , the following regret bound holds : @xmath96 moreover , this regret bound is nearly equal to the optimal a posteriori regret bound : @xmath97    notice that the regularization function is minimized when the gradient predictions become more accurate .",
    "thus , if we interpret our regularization as an implicit learning rate , our algorithm uses a larger learning rate and _ accelerates _ as our gradient predictions become more accurate .",
    "this is in stark contrast to other adaptive regularization methods , such as adagrad , where learning rates are inversely proportional to simply the norm of the gradient .",
    "moreover , since the regularization function decomposes over the coordinates , this acceleration can occur on a per - coordinate basis .",
    "if our gradient predictions are more accurate in some coordinates than others , then our algorithm will be able to adapt accordingly . under the simple martingale prediction scheme ,",
    "this means that our algorithm will be able to adapt well when only certain coordinates of the gradient are slowly - varying , even if the entire gradient is not .    in terms of computation",
    ", the ao - gd update can be executed in time linear in the dimension ( the same as for standard gradient descent ) . moreover , since the gradient prediction is simply the last gradient received , the algorithm also does not require much more storage than the standard gradient descent algorithm .",
    "however , as we mentioned in the general case , the regret bound here can be significantly more favorable than the standard @xmath98 \\right)$ ] bound of online gradient descent , or even its adaptive variants .",
    "[ cor : aoeg ] let @xmath99 be the @xmath92-dimensional simplex and @xmath100 the negative entropy .",
    "assume that @xmath101 for all @xmath26 and set @xmath102 then , if we use the martingale - type gradient prediction @xmath95 , the following regret bound holds : @xmath103    the above algorithm admits the same advantages over predecessors as the ao - gd algorithm .",
    "moreover , observe that this bound holds at any time and does not require the tuning of any learning rate .",
    "steinhardt and liang @xcite also introduce a similar algorithm for eg , one that could actually be more favorable if the optimal a posteriori learning rate is known in advance .      in some cases , we may wish to impose some regularization on our original optimization problem to ensure properties such as generalization ( e.g.  @xmath104-norm in svm ) or sparsity ( e.g.  @xmath105-norm in lasso ) .",
    "this `` composite term '' can be treated directly by modifying the regularization in our ftrl update . however , if we wish for the regularization penalty to appear in the regret expression but do not wish to linearize it ( which could mitigate effects such as sparsity ) , then some extra care needs to be taken .",
    "we modify algorithm  [ alg : ao_ftrl ] to obtain algorithm  [ alg : cao_ftrl ] , and we provide accompanying regret bounds for both proximal and general regularization functions . in each theorem , we give a pair of regret bounds , depending on whether the learner considers the composite term as an additional part of the loss .",
    "all proofs are provided in appendix  [ app : ao_ftrl ] .",
    "* input : * regularization function @xmath32 , composite functions @xmath106 where @xmath107 . * initialize : * @xmath33 , @xmath34 .",
    "compute @xmath12 .",
    "construct regularizer @xmath35 .",
    "predict the next gradient @xmath108 .",
    "update @xmath109 .",
    "[ cao - ftrl - prox ] [ th : cao_ftrl - prox ] let @xmath40 be a sequence of proximal non - negative functions , such that @xmath39 , and let @xmath41 be the learner s estimate of @xmath18 given the history of functions @xmath42 and points @xmath43 .",
    "let @xmath106 be a sequence of non - negative convex functions , such that @xmath110 .",
    "assume further that the function @xmath111 is 1-strongly convex with respect to some norm @xmath45 .",
    "then the following regret bounds hold for cao - ftrl ( algorithm  [ alg : cao_ftrl ] ) : @xmath112 - \\left [ f_t(x ) + \\psi_t(x ) \\right ]   \\leq   r_{0:t}(x ) + \\sum_{t = 1}^t \\|g_t - \\tilde{g}_t\\|_{(t),*}^2 \\ , .\\end{aligned}\\ ] ]    notice that if we do nt consider the composite term as part of our loss , then our regret bound resembles the form of ao - ftrl - gen .",
    "this is in spite of the fact that we are using proximal adaptive regularization . on the other hand ,",
    "if the composite term is part of our loss , then our regret bound resembles the one using ao - ftrl - prox .",
    "[ cao - ftrl - gen ] [ th : cao_ftrl - gen ] let @xmath40 be a sequence of non - negative functions , and let @xmath41 be the learner s estimate of @xmath18 given the history of functions @xmath42 and points @xmath43 .",
    "let @xmath106 be a sequence of non - negative convex functions such that @xmath110 .",
    "assume further that the function @xmath111 is 1-strongly convex with respect to some norm @xmath45 .",
    "then , the following regret bound holds for cao - ftrl ( algorithm  [ alg : cao_ftrl ] ) : @xmath113   \\leq   r_{0:t-1}(x ) + \\sum_{t = 1}^t \\|g_t - \\tilde{g}_t\\|_{(t),*}^2 \\ , .\\end{aligned}\\ ] ]",
    "we now generalize the scenario to that of stochastic online convex optimization , where , instead of exact subgradient elements @xmath18 , we receive only estimates . specifically , we assume access to a sequence of vectors of the form @xmath114 , where @xmath115 = g_t$ ] .",
    "this extension is in fact well - documented in the literature ( see @xcite for a reference ) , and the extension of our adaptive and optimistic variant follows accordingly . for completeness",
    ", we provide the proofs of the following theorems in appendix  [ app : aos_ftrl ] .",
    "* input : * regularization function @xmath32 , composite functions @xmath106 where @xmath107 .",
    "* initialize : * @xmath33 , @xmath34 .",
    "query @xmath114 where @xmath116 = g_t \\in { \\partial}f_t(x_t)$ ] .",
    "construct regularizer @xmath35 .",
    "predict next gradient @xmath117 .",
    "update @xmath118 .",
    "[ th : caos_ftrl - prox ] let @xmath40 be a sequence of proximal non - negative functions , such that @xmath39 , and let @xmath41 be the learner s estimate of @xmath114 given the history of noisy gradients @xmath119 and points @xmath43 .",
    "let @xmath106 be a sequence of non - negative convex functions , such that @xmath110 .",
    "assume further that the function @xmath120 is 1-strongly convex with respect to some norm @xmath45 .",
    "then , the update @xmath121 of algorithm  [ alg : caos_ftrl ] yields the following regret bounds : @xmath122   \\leq { \\mathbb{e}}\\left [ \\psi_{1:t-1}(x ) + r_{0:t-1}(x ) + \\sum_{t = 1}^t \\|\\hat{g}_t - \\tilde{g}_t\\|_{(t-1),*}^2 \\right ] \\\\ & { \\mathbb{e}}\\left [ \\sum_{t = 1}^t f_t(x_t ) + \\psi_t(x_t ) - f_t(x ) - \\alpha_t \\psi_t(x ) \\right ]   \\leq { \\mathbb{e}}\\left [ r_{0:t}(x ) + \\sum_{t = 1}^t \\|\\hat{g}_t - \\tilde{g}_t\\|_{(t),*}^2 \\right].\\end{aligned}\\ ] ]    [ th : caos_ftrl - gen ] let @xmath40 be a sequence of non - negative functions , and let @xmath41 be the learner s estimate of @xmath114 given the history of noisy gradients @xmath119 and points @xmath43 . let @xmath106 be a sequence of non - negative convex functions , such that @xmath110 .",
    "assume furthermore that the function @xmath120 is 1-strongly convex with respect to some norm @xmath45 .",
    "then , the update @xmath121 of algorithm  [ alg : caos_ftrl ] yields the regret bounds : @xmath122 \\leq { \\mathbb{e}}\\left [ \\psi_{1:t-1}(x ) + r_{0:t-1}(x ) + \\sum_{t = 1}^t \\|\\hat{g}_t - \\tilde{g}_t\\|_{(t-1),*}^2 \\right ] \\\\ & { \\mathbb{e}}\\left [ \\sum_{t = 1}^t f_t(x_t ) + \\psi_t(x_t ) - f_t(x ) - \\psi_t(x ) \\right ]   \\leq { \\mathbb{e}}\\left [ r_{0:t-1}(x ) + \\sum_{t = 1}^t \\|\\hat{g}_t - \\tilde{g}_t\\|_{(t-1),*}^2 \\right]\\end{aligned}\\ ] ]    the algorithm above enjoys the same advantages over its non - adaptive or non - optimistic predecessors . moreover ,",
    "the choice of the adaptive regularizers @xmath123 and gradient predictions @xmath124 now also depend on the randomness of the gradients received .",
    "while masked in the above regret bounds , this interplay will come up explicitly in the following two examples , where we , as the learner , impose randomness into the problem .",
    "randomized coordinate descent is a method that is often used for very large - scale problems where it is impossible to compute and/or store entire gradients at each step .",
    "it is also effective for directly enforcing sparsity in a solution since the support of the final point @xmath1 can not be larger than the number of updates introduced .",
    "the standard randomized coordinate descent update is to choose a coordinate uniformly at random ( see e.g.@xcite ) .",
    "nesterov ( 2012 ) @xcite analyzed random coordinate descent in the context of loss functions with higher regularity and showed that one can attain better bounds by using non - uniform probabilities .    in the randomized coordinate descent framework , at each round @xmath26 we specify a distribution @xmath125 over the @xmath92 coordinates and pick a coordinate @xmath126 randomly according to this distribution .",
    "from here , we then construct an unbiased estimate of an element of the subgradient : @xmath127 .",
    "this technique is common in the online learning literature , particularly in the context of the multi - armed bandit problem ( see e.g.@xcite for more information ) .    the following theorem can be derived by applying theorem  [ th : caos_ftrl - prox ] to the gradient estimates just constructed .",
    "we provide a proof in appendix  [ app : rcd ] .",
    "[ th : cao_rcd ] assume @xmath128 $ ] . let @xmath129 be a random variable sampled according to the distribution @xmath125 , and let @xmath130 be the estimated gradient and estimated gradient prediction",
    ". denote @xmath131 , and let @xmath132 be the adaptive regularization .",
    "then , the regret of the algorithm can be bounded by : @xmath133 \\leq 4 \\sum_{i = 1}^n r_i \\sqrt { \\sum_{t = 1}^t { \\mathbb{e}}\\left [ \\frac { ( g_{t , i }        - \\tilde{g}_{t , i})^2}{p_{t , i } } \\right ] } \\end{aligned}\\ ] ]    in general , we do not have access to an element of the subgradient @xmath18 before we sample according to @xmath125 . however , if we assume that we have some per - coordinate upper bound on an element of the subgradient uniform in time , i.e. @xmath134 @xmath135 , then we can use the fact that @xmath136 to motivate setting @xmath137 and @xmath138 ( by computing the optimal distribution ) .",
    "this yields the following regret bound .",
    "[ cor : cao_rcd_lip ] assume that at any time @xmath26 the following per - coordinate lipschitz bounds hold on the loss function : @xmath139 .",
    "set @xmath140 as the probability distribution at time @xmath26 , and set @xmath141 .",
    "then , the regret of the algorithm can be bounded as follows : @xmath133 \\leq 2 \\sqrt{t } \\left ( \\sum_{i = 1}^n ( r_i l_i)^{2/3 } \\right)^{3/2 } .\\end{aligned}\\ ] ]    an application of hlder s inequality will reveal that this bound is strictly smaller than the @xmath142 bound one would obtain from randomized coordinate descent using the uniform distribution .",
    "moreover , the algorithm above still entertains the intermediate data - dependent bound of theorem  [ th : cao_rcd ] .",
    "notice the similarity between the sampling distribution generated here with the one suggested by @xcite .",
    "however , nesterov assumed higher regularity in his algorithm ( i.e. @xmath143 ) and generated his probabilities from there . in our setting , we only need @xmath144 .",
    "it should be noted that @xcite also proposed an importance - sampling based approach to random coordinate descent for the specific setting of multiple kernel learning . in their setting , they propose updating the sampling distribution at each point in time instead of using uniform - in - time lipschitz constants , which comes with a natural computational tradeoff .",
    "moreover , the introduction of adaptive per - coordinate learning rates in our algorithm allows for tighter regret bounds in terms of the lipschitz constants .",
    "we can also derive the analogous mini - batch update :    [ cor : cao_rcd_lip_batch ] assume @xmath128 $ ] .",
    "let @xmath145 be a partition of the coordinates , and let @xmath146 .",
    "assume we had the following lipschitz condition on the partition : @xmath147 @xmath148 .",
    "define @xmath149 .",
    "set @xmath150 as the probability distribution at time @xmath26 , and set @xmath141 .",
    "then the regret of the resulting algorithm is bounded by : @xmath133 \\leq 2 \\sqrt{t } \\left ( \\sum_{i = 1}^k ( s_i l_i)^{2/3 } \\right)^{3/2 }     \\end{aligned}\\ ] ]    while the expression is similar to the non - mini - batch version , the @xmath151 and @xmath152 terms now have different meaning .",
    "specifically , @xmath151 is a bound on the 2-norm of the components of the gradient in each batch , and @xmath152 is the 1-norm of the corresponding sides of the hypercube .",
    "many learning algorithms can be viewed as instances of regularized empirical risk minimization ( e.g.  svm , logistic regression , lasso ) , where the goal is to minimize an objective function of the following form : @xmath153 if we denote the first term by @xmath154 , then we can view this objective in our caos - ftrl framework , where @xmath155 and @xmath156 in the same spirit as for non - uniform random coordinate descent , we can estimate the gradient of @xmath157 at @xmath1 by sampling according to some distribution @xmath125 and use importance weighting to generate an unbiased estimate : if @xmath158 and @xmath159 , then @xmath160    this motivates the design of an algorithm similar to the one derived for randomized coordinate descent . here",
    "we elect to use as gradient prediction the last gradient of the current function being sampled @xmath161 .",
    "however , we may run into the problem of never seeing a function before .",
    "a logical modification would be to separate optimization into epochs and do a full batch update over all functions @xmath161 at the start of each epoch .",
    "this is similar to the technique used in the stochastic variance reduced gradient ( svrg ) algorithm of @xcite .",
    "however , we do not assume extra function regularity as they do in their paper , so the bounds are not comparable . the algorithm is presented in algorithm  [ alg : caos_reg - erm - epoch ] and comes with the following guarantee :    * input : * scaling constant @xmath162 , composite term @xmath163 , @xmath164 .",
    "* initialize : * initial point @xmath165 , distribution @xmath166 .",
    "sample @xmath167 according to @xmath168 , and set @xmath169 .",
    "compute @xmath170 @xmath171 . if @xmath172 , compute @xmath173 @xmath174 .",
    "set @xmath175 , and construct @xmath35 .",
    "sample @xmath176 and set @xmath177 .",
    "update @xmath178 and @xmath179 .",
    "[ cor : aos_erm - epoch ] assume @xmath128 $ ] .",
    "denote @xmath180 , and let @xmath181 be the adaptive regularization .",
    "then the regret of algorithm  [ alg : caos_reg - erm - epoch ] is bounded by : @xmath182 \\leq \\sum_{i=1}^n 4 r_i \\sqrt { \\sum_{s=1}^k \\sum_{t=(s-1)(t / k ) + 1}^{(s-1)(t / k)+t / k } \\sum_{j=1}^m \\frac{\\left|g_{t , i}^j - \\bar{g}_{s , i}^j \\right|^2}{p_{t , j } } }    \\ ] ]    moreover , if @xmath183 @xmath174 , then setting @xmath184 yields a worst - case bound of : @xmath185    we also include a mini - batch version of this algorithm in appendix  [ app : reg_erm ] , which can be useful due to the variance reduction of the gradient prediction .",
    "we presented a general framework for developing efficient adaptive and optimistic algorithms for online convex optimization .",
    "building upon recent advances in adaptive regularization and predictable online learning , we improved upon each method .",
    "we demonstrated the power of this approach by deriving algorithms with better guarantees than those commonly used in practice .",
    "in addition , we also extended adaptive and optimistic online learning to the randomized setting . here , we highlighted an additional source of problem - dependent adaptivity ( that of prescribing the sampling distribution ) , and we showed how one can perform better than traditional naive uniform sampling .",
    "[ lem : smooth_cvx_duality ] let @xmath2 be a convex set and @xmath186 be a convex function .",
    "suppose @xmath187 is @xmath65-strongly convex at @xmath188 .",
    "then @xmath189 , the legendre transform of @xmath187 , is @xmath65-strongly smooth at @xmath190 .",
    "notice first that for any pair of convex functions @xmath191 , the fact that @xmath192 for some @xmath193 implies that @xmath194 for @xmath190 .",
    "now , @xmath187 being @xmath65-strongly convex at @xmath188 means that @xmath195 .",
    "thus , it suffices to show that @xmath196 , since @xmath197 .    to see this , we can compute that @xmath198 \\\\ & = \\frac{1}{2 } \\|y - y_0\\|_2 ^ 2 + y \\cdot x_0 - f(x_0 ) \\\\ & = -f(x_0 ) + x_0 \\cdot y_0 + x_0 \\cdot ( y - y_0 ) + \\frac{1}{2 } \\|y - y_0\\|_2 ^ 2 \\\\ & = f^*(y_0 ) + x_0 \\cdot ( y - y_0 ) + \\frac{1}{2 } \\|y - y_0\\|_2 ^ 2 \\end{aligned}\\ ] ]    th : ao_ftrl - gen[ao - ftrl - gen ] let @xmath40 be a sequence of non - negative functions , and let @xmath41 be the learner s estimate of @xmath18 given the history of functions @xmath42 and points @xmath43 . assume further that the function @xmath44 is 1-strongly convex with respect to some norm @xmath45",
    "( i.e. @xmath46 is 1-strongly convex wrt @xmath45 ) .",
    "then , the following regret bound holds for ao - ftrl ( algorithm  [ alg : ao_ftrl ] ) : @xmath85    recall that @xmath199 , and let @xmath200 then by convexity , @xmath201    now , we first show via induction that @xmath202 , the following holds : @xmath203    for @xmath204 , the fact that @xmath35 , @xmath33 , and the definition of @xmath67 imply the result .",
    "+ now suppose the result is true for time @xmath51 . then @xmath205 \\\\ & \\quad \\quad",
    "\\quad + \\tilde{g}_{t+1 } \\cdot ( x_{t+1 } - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 } \\\\ & \\quad \\leq",
    "\\left [ \\sum_{t = 1}^{t } g_t \\cdot x_{t+1 } + r_{0:t-1}(x_{t+1 } ) \\right ] \\\\ & \\quad \\quad",
    "\\quad + \\tilde{g}_{t+1 } \\cdot ( x_{t+1 } - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\text { ( by the induction hypothesis for $ x = x_{t+1}$ ) } \\\\ & \\quad \\leq \\left [ \\left(g_{1:t } + \\tilde{g}_{t+1 } \\right ) \\cdot x_{t+1 } + r_{0:t}(x_{t+1 } ) \\right ] \\\\ & \\quad \\quad \\quad + \\tilde{g}_{t+1 } \\cdot ( - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\text { ( since $ r_t \\geq 0 $ , $ \\forall t$ ) } \\\\ & \\quad \\leq \\left [ \\left(g_{1:t } + \\tilde{g}_{t+1 } \\right ) \\cdot y_{t+1 } + r_{0:t}(y_{t+1 } ) \\right ] \\\\ & \\quad \\quad \\quad + \\tilde{g}_{t+1 } \\cdot ( - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\text { ( by definition of $ x_{t+1}$ ) } \\\\ & \\quad \\leq g_{1:t+1 } \\cdot y + r_{0:t}(y ) , \\text { for any $ y$.}\\\\ & \\qquad \\text { ( by definition of $ y_{t+1}$)}\\end{aligned}\\ ] ]    thus , we have that @xmath206 and it suffices to bound @xmath59 . by duality again , one can immediately get @xmath207 .",
    "to bound @xmath61 in terms of the gradient , recall first that @xmath208 the fact that @xmath209 is 1-strongly convex with respect to the norm @xmath210 implies that @xmath211 is as well . in particular , it is strongly convex at the points @xmath1 and @xmath67 .",
    "but , this then implies that the conjugate function is smooth at @xmath212 and @xmath213 , so that @xmath214 since @xmath215 and @xmath216 , we have that @xmath217    th : cao_ftrl - prox [ cao - ftrl - prox ] let @xmath40 be a sequence of proximal non - negative functions , such that @xmath39 , and let @xmath41 be the learner s estimate of @xmath18 given the history of functions @xmath42 and points @xmath43 .",
    "let @xmath106 be a sequence of non - negative convex functions , such that @xmath110 .",
    "assume further that the function @xmath111 is 1-strongly convex with respect to some norm @xmath45 .",
    "then the following regret bounds hold for cao - ftrl ( algorithm  [ alg : cao_ftrl ] ) : @xmath112 - \\left [ f_t(x ) + \\psi_t(x ) \\right ]   \\leq   r_{0:t}(x ) + \\sum_{t = 1}^t \\|g_t - \\tilde{g}_t\\|_{(t),*}^2 \\ , .\\end{aligned}\\ ] ]    for the first regret bound , define the auxiliary regularization functions @xmath218 , and apply theorem  [ th : ao_ftrl - gen ] to get @xmath219    notice that while @xmath38 is proximal , @xmath220 , in general , is not , and so we must apply the theorem with general regularizers instead of the one with proximal regularizers . + for the second regret bound , we can follow the prescription of theorem 1 while keeping track of the additional composite terms :    recall that @xmath221 , and let @xmath222 .",
    "+ we can compute that : @xmath223 & \\leq \\sum_{t = 1}^t g_t \\cdot ( x_t - x ) + \\psi_t(x_t ) - \\psi_t(x ) \\\\ & = \\sum_{t = 1}^t ( g_t - \\tilde{g}_t ) \\cdot ( x_t - y_{t } )   \\\\ & \\quad \\quad + \\tilde{g}_t \\cdot ( x_t - y_{t } ) + g_t \\cdot ( y_{t } - x ) + \\psi_t(x_t ) - \\psi_t(x)\\end{aligned}\\ ] ]    similar to before , we show via induction that @xmath202 , @xmath224    for @xmath204 , the fact that @xmath35 , @xmath225 , @xmath110 , and the definition of @xmath67 imply the result . +",
    "now suppose the result is true for time @xmath51 . then @xmath226 \\\\ &",
    "\\qquad \\qquad + \\tilde{g}_{t+1 } \\cdot ( x_{t+1 } - y_{t+1 } ) +   g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\qquad + \\psi_{t+1}(x_{t+1 } ) \\\\ &",
    "\\quad \\leq \\left [ \\sum_{t = 1}^{t } g_t \\cdot x_{t+1 } + r_{0:t}(x_{t+1 } ) + \\psi_t(x_{t+1 } ) \\right ] \\\\ & \\qquad \\qquad   + \\tilde{g}_{t+1 } \\cdot ( x_{t+1 } - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\qquad + \\psi_{t+1}(x_{t+1 } ) \\\\ & \\qquad \\qquad \\text { ( by the induction hypothesis for $ x = x_{t+1}$ ) } \\\\ & \\quad \\leq \\left(g_{1:t } + \\tilde{g}_{t+1 } \\right ) \\cdot x_{t+1 } + r_{0:t+1}(x_{t+1 } ) + \\psi_t(x_{t+1 } ) \\\\ & \\qquad \\qquad + \\tilde{g}_{t+1 } \\cdot ( - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\qquad + \\psi_{t+1}(x_{t+1 } ) \\\\ & \\qquad \\qquad \\text { ( since $ r_t \\geq 0 $ , $ \\forall t$ ) } \\\\ & \\quad \\leq \\left(g_{1:t } + \\tilde{g}_{t+1 } \\right ) \\cdot y_{t+1 } + r_{0:t+1}(y_{t+1 } )   + \\psi_t(y_{t+1 } ) \\\\ & \\qquad \\qquad + \\tilde{g}_{t+1 } \\cdot ( - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\qquad + \\psi_{t+1}(y_{t+1 } ) \\\\ & \\qquad \\qquad \\text { ( by definition of $ x_{t+1}$ ) } \\\\ & \\quad \\leq g_{1:t+1 } \\cdot y + r_{0:t+1}(y ) + \\psi_{1:t+1}(y ) , \\text { for any $ y$}\\\\ & \\qquad \\qquad \\text { ( by definition of $ y_{t+1}$)}\\end{aligned}\\ ] ]    thus , we have that @xmath227   & \\leq r_{0:t}(x ) + \\sum_{t = 1}^t ( g_t - \\tilde{g}_t)^t ( x_t - y_{t } ) , \\end{aligned}\\ ] ] and we can bound the sum in the same way as before , since the strong convexity properties of @xmath78 are retained due to the convexity of @xmath9 .    th : cao_ftrl - gen [ cao - ftrl - gen ] let @xmath40 be a sequence of non - negative functions , and let @xmath41 be the learner s estimate of @xmath18 given the history of functions @xmath42 and points @xmath43 .",
    "let @xmath106 be a sequence of non - negative convex functions such that @xmath110 .",
    "assume further that the function @xmath111 is 1-strongly convex with respect to some norm @xmath45 .",
    "then , the following regret bound holds for cao - ftrl ( algorithm  [ alg : cao_ftrl ] ) : @xmath113 \\leq   r_{0:t-1}(x ) + \\sum_{t = 1}^t \\|g_t - \\tilde{g}_t\\|_{(t),*}^2 \\ , .\\end{aligned}\\ ] ]    for the first regret bound , define the auxiliary regularization functions @xmath228 , and apply theorem  [ th : ao_ftrl - gen ] to get @xmath229    for the second bound , we can proceed as in the original proof , but now keep track of the additional composite terms .",
    "+ recall that @xmath230 , and let @xmath231 then @xmath232    now , we show via induction that @xmath202 , @xmath233    for @xmath204 , the fact that @xmath35 , @xmath225 , @xmath110 , and the definition of @xmath67 imply the result .",
    "+ now suppose the result is true for time @xmath51 .",
    "then @xmath234 \\\\ & \\qquad \\qquad + \\tilde{g}_{t+1 } \\cdot ( x_{t+1 } - y_{t+1 } ) +   g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\qquad + \\psi_{t+1}(x_{t+1 } ) \\\\ & \\quad \\leq \\left [ \\sum_{t = 1}^{t } g_t^t x_{t+1 } + r_{0:t-1}(x_{t+1 } ) + \\psi_t(x_{t+1 } ) \\right ] \\\\",
    "& \\qquad \\qquad + \\tilde{g}_{t+1 } \\cdot ( x_{t+1 } - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\qquad + \\psi_{t+1}(x_{t+1 } ) \\\\ & \\qquad \\qquad \\text { ( by the induction hypothesis for $ x = x_{t+1}$ ) } \\\\ & \\quad \\leq",
    "\\left [ \\left(g_{1:t } + \\tilde{g}_{t+1 } \\right ) \\cdot x_{t+1 } + r_{0:t}(x_{t+1 } ) + \\psi_t(x_{t+1 } ) \\right ] \\\\ & \\qquad \\qquad + \\tilde{g}_{t+1 } \\cdot ( - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\qquad + \\psi_{t+1}(x_{t+1 } ) \\\\ & \\qquad \\qquad \\text { ( since $ r_t \\geq 0 $ , $ \\forall t$ ) } \\\\ & \\quad \\leq g_{1:t+1 } \\cdot y_{t+1 } + \\tilde{g}_{t+1 } \\cdot y_{t+1 } + r_{0:t}(y_{t+1 } ) \\\\ & \\qquad \\qquad + \\psi_{1:t+1}(y_{t+1 } )   \\\\ & \\qquad \\qquad + \\tilde{g}_{t+1 } \\cdot ( - y_{t+1 } ) + g_{t+1 } \\cdot y_{t+1 } \\\\ & \\qquad \\qquad \\text { ( by definition of $ x_{t+1}$ ) } \\\\ & \\quad \\leq g_{1:t+1 } \\cdot y + r_{0:t}(y ) + \\psi_{1:t+1}(y ) , \\text { for any $ y$}\\\\ & \\qquad \\text { ( by definition of $ y_{t+1}$)}\\end{aligned}\\ ] ]    thus , we have that @xmath235 and the remainder follows as in the non - composite setting since the strong convexity properties are retained .",
    "the following lemma is central to the derivation of regret bounds for many algorithms employing adaptive regularization .",
    "its proof , via induction , can be found in auer et al ( 2002 ) .",
    "[ lem : adagrad ] let @xmath236 be a sequence of non - negative numbers .",
    "then @xmath237    cor : aogd[ao - gd ] let @xmath128 $ ] be an @xmath92-dimensional rectangle , and denote @xmath93 . set @xmath94 then , if we use the martingale - type gradient prediction @xmath95 , the following regret bound holds : @xmath238 moreover , this regret bound is nearly equal to the optimal a posteriori regret bound : @xmath239    @xmath46 is @xmath65-strongly convex with respect to the norm : @xmath240 which has corresponding dual norm : @xmath241 by the choice of this regularization , the prediction @xmath242 , and theorem  [ th : cao_ftrl - prox ] , the following holds : @xmath243 the last statement follows from the fact that @xmath244 since the infimum on the left hand side is attained when @xmath245",
    "cor : aoeg[ao - eg ] let @xmath99 be the @xmath92-dimensional simplex and @xmath100 the negative entropy .",
    "assume that @xmath101 for all @xmath26 and set @xmath246 then , if we use the martingale - type gradient prediction @xmath95 the following regret bound holds : @xmath247    since the negative entropy @xmath248 is @xmath65-strongly convex with respect to the @xmath249-norm , @xmath46 is @xmath250-strongly convex with respect to the same norm .",
    "+ applying theorem  [ th : ao_ftrl - gen ] and using the fact that the dual of @xmath249 is @xmath251 along with @xmath252 yields a regret bound of : @xmath253",
    "th : caos_ftrl - prox[caos - ftrl - prox ] let @xmath40 be a sequence of proximal non - negative functions , such that @xmath39 , and let @xmath41 be the learner s estimate of @xmath114 given the history of noisy gradients @xmath119 and points @xmath43 .",
    "let @xmath106 be a sequence of non - negative convex functions , such that @xmath110 .",
    "assume further that the function @xmath254 is 1-strongly convex with respect to some norm @xmath45 .",
    "then , the update @xmath121 of algorithm  [ alg : caos_ftrl ] yields the following regret bounds : @xmath122   \\leq { \\mathbb{e}}\\left [ \\psi_{1:t-1}(x ) + r_{0:t-1}(x ) + \\sum_{t = 1}^t \\|\\hat{g}_t - \\tilde{g}_t\\|_{(t-1),*}^2 \\right ] \\\\ & { \\mathbb{e}}\\left [ \\sum_{t = 1}^t",
    "f_t(x_t ) + \\psi_t(x_t ) - f_t(x ) - \\alpha_t \\psi_t(x ) \\right ]   \\leq { \\mathbb{e}}\\left [ r_{0:t}(x ) + \\sum_{t = 1}^t \\|\\hat{g}_t - \\tilde{g}_t\\|_{(t),*}^2 \\right ] .",
    "\\end{aligned}\\ ] ]    @xmath255   & \\quad \\leq \\sum_{t = 1}^t { \\mathbb{e}}\\left [ g_t \\cdot ( x_t - x ) \\right ] \\\\ & \\quad = \\sum_{t = 1}^t { \\mathbb{e}}\\left [ { \\mathbb{e } } [ \\hat{g}_t | \\hat{g}_1,\\ldots , \\hat{g}_{t-1 } , x_1 , \\ldots , x_t ] ^t ( x_t - x)\\right ] \\\\ & \\quad = \\sum_{t = 1}^t { \\mathbb{e}}\\left [ { \\mathbb{e } } [ \\hat{g}_t \\cdot ( x_t - x ) | \\hat{g}_1,\\ldots , \\hat{g}_{t-1 } , x_1 , \\ldots , x_t ] \\right ] \\\\ & \\quad = \\sum_{t = 1}^t { \\mathbb{e}}\\left [ \\hat{g}_t \\cdot ( x_t - x ) \\right ] \\\\ \\ ] ]    this implies that upon taking an expectation , we can freely upper bound the difference @xmath256 by the noisy linearized estimate @xmath257 . after that , we can apply algorithm  [ alg : cao_ftrl ] on the gradient estimates to get the bounds : @xmath258   \\leq { \\mathbb{e}}\\left [ \\psi_{1:t-1}(x ) + r_{0:t-1}(x ) + \\sum_{t = 1}^t \\|\\hat{g}_t - \\tilde{g}_t\\|_{(t-1),*}^2 \\right ] \\\\ & { \\mathbb{e}}\\left [ \\sum_{t = 1}^t \\hat{g}_t^t(x_t - x)+ \\psi_t(x_t ) - \\psi_t(x ) \\right ]   \\leq { \\mathbb{e}}\\left [ r_{0:t}(x ) + \\sum_{t = 1}^t \\|\\hat{g}_t - \\tilde{g}_t\\|_{(t),*}^2 \\right ] \\end{aligned}\\ ] ]    th : caos_ftrl - gen[caos - ftrl - gen ] let @xmath40 be a sequence of non - negative functions , and let @xmath41 be the learner s estimate of @xmath114 given the history of noisy gradients @xmath119 and points @xmath43 .",
    "let @xmath106 be a sequence of non - negative convex functions , such that @xmath110 .",
    "assume furthermore that the function @xmath254 is 1-strongly convex with respect to some norm @xmath45 .",
    "then , the update @xmath121 of algorithm  [ alg : caos_ftrl ] yields the regret bounds : @xmath122 \\leq { \\mathbb{e}}\\left [ \\psi_{1:t-1}(x ) + r_{0:t-1}(x ) + \\sum_{t = 1}^t \\|\\hat{g}_t - \\tilde{g}_t\\|_{(t-1),*}^2 \\right ] \\\\ & { \\mathbb{e}}\\left [ \\sum_{t = 1}^t f_t(x_t ) + \\psi_t(x_t ) - f_t(x ) - \\psi_t(x ) \\right ] \\leq { \\mathbb{e}}\\left [ r_{0:t-1}(x ) + \\sum_{t = 1}^t \\|\\hat{g}_t - \\tilde{g}_t\\|_{(t-1),*}^2 \\right ] \\end{aligned}\\ ] ]    the argument is the same as for theorem  [ th : caos_ftrl - prox ] , except that we now apply the bound of theorem  [ th : cao_ftrl - gen ] at the end .",
    "th : cao_rcd[cao - rcd ] assume @xmath128 $ ] . let @xmath129 be a random variable sampled according to the distribution @xmath125 , and let @xmath259 be the estimated gradient and estimated gradient prediction",
    ". denote @xmath131 , and let @xmath132 be the adaptive regularization .",
    "then the regret of the resulting algorithm is bounded by : @xmath260 & \\leq 4 \\sum_{i = 1}^n r_i \\sqrt { \\sum_{t = 1}^t { \\mathbb{e}}\\left [ \\frac { ( g_{t , i } - \\tilde{g}_{t , i})^2}{p_{t , i } } \\right ] } .",
    "\\end{aligned}\\ ] ]    we can first compute that @xmath261    = { \\mathbb{e}}\\left [ \\frac{(g_t \\cdot e_{i_t } ) e_{i_t}}{p_{t , i_t } } \\right ]   = \\sum_{i = 1}^n \\frac{(g_t \\cdot e_{i } ) e_{i}}{p_{t , i } } p_{t , i }   = g_t\\end{aligned}\\ ] ] and similarly for the gradient prediction @xmath41 .",
    "+ now , as in corollary  [ cor : aogd ] , the choice of regularization ensures us a regret bound of the form : @xmath260   & \\leq 4 \\sum_{i = 1}^n r_i { \\mathbb{e}}\\left [ \\sqrt{\\sum_{t = 1}^t ( \\hat{g}_{t , i } - \\tilde{g}_{t , i})^2 } \\right ] \\end{aligned}\\ ] ]    moreover , we can compute that : @xmath262   & \\leq \\sqrt { { \\mathbb{e}}\\left [ \\sum_{t = 1}^t { \\mathbb{e}}_{i_t } [ ( \\hat{g}_{t , i } - \\tilde{g}_{t , i})^2 ] \\right ] } \\\\ & = \\sqrt { \\sum_{t = 1}^t { \\mathbb{e}}\\left [ \\frac { ( g_{t , i } - \\tilde{g}_{t , i})^2}{p_{t , i } } \\right ] } \\\\ \\ ] ]",
    "we present here algorithm  [ alg : caos_reg - erm - epoch - batch ] , a mini - batch version of algorithm  [ alg : caos_reg - erm - epoch ] , with an accompanying guarantee .",
    "* input : * scaling constant @xmath162 , composite term @xmath163 , @xmath164 , partitions @xmath263 .",
    "* initialize : * initial point @xmath165 , distribution @xmath166 over @xmath264 .",
    "sample @xmath167 according to @xmath168 , and set @xmath169 .",
    "compute @xmath170 @xmath171 . if @xmath172 , compute @xmath173 @xmath174 .",
    "set @xmath265 , and construct @xmath35 .",
    "sample @xmath176 . set @xmath266 .",
    "update @xmath178 and @xmath179 .",
    "then the regret of algorithm  [ alg : caos_reg - erm - epoch - batch ] is bounded by : @xmath269 & \\leq \\sum_{i=1}^n 4 r_i \\sqrt { \\sum_{s=1}^k \\sum_{t=(s-1)(t / k ) + 1}^{(s-1)(t / k)+t / k }",
    "\\sum_{a=1}^l \\frac{\\left|\\sum_{j \\in \\pi_j } g_{t , i}^j - \\bar{g}_{s , i}^j \\right|^2}{p_{t , a } } }    \\ ] ]      a similar approach to regularized erm was developed independently by @xcite .",
    "however , the one here improves upon that algorithm through the incorporation of adaptive regularization , optimistic gradient predictions , and the fact that we do not assume higher regularity conditions such as strong convexity for our loss functions ."
  ],
  "abstract_text": [
    "<S> we present a powerful general framework for designing data - dependent optimization algorithms , building upon and unifying recent techniques in adaptive regularization , optimistic gradient predictions , and problem - dependent randomization . </S>",
    "<S> we first present a series of new regret guarantees that hold at any time and under very minimal assumptions , and then show how different relaxations recover existing algorithms , both basic as well as more recent sophisticated ones . </S>",
    "<S> finally , we show how combining adaptivity , optimism , and problem - dependent randomization can guide the design of algorithms that benefit from more favorable guarantees than recent state - of - the - art methods . </S>"
  ]
}