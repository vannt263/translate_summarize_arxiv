{
  "article_text": [
    "suppose a @xmath0-dimensional random vector @xmath5 is observed which is normally distributed , with mean vector @xmath4 and unknown positive definite covariance matrix @xmath9 , and we wish to estimate @xmath4 under the invariant quadratic loss @xmath10 since the covariance matrix @xmath9 is unknown , a random matrix @xmath6 is observed along with @xmath5 , which is assumed to be independent of @xmath5 , and has a wishart distribution with @xmath7 degrees of freedom , where @xmath11 .",
    "in high - dimensional estimation problems , where @xmath0 , the number of features , is nearly as large as or larger than @xmath7 , the number of observations , the ordinary least squares estimator does not typically provide a satisfactory estimate of @xmath4 .",
    "modern data sets are increasingly becoming characterized by a number of features that are much larger than the number of sample units ( large-@xmath0 , small-@xmath7 ) in contrast to classical data sets where the number of sample units is often much larger than the number of random variables ( small-@xmath0 , large-@xmath7 ) .",
    "modern applications in the @xmath8 setting include examples from microarrays , association mapping , proteomics , radiology , biomedical imaging , signal processing , climate modeling and finance .",
    "for instance , in the case of microarray data , the dimensionality is frequently in the thousands or beyond , while the sample size is typically in the order of tens .",
    "the large-@xmath0 , small-@xmath7 scenario poses challenges in most inferential settings .",
    "we are considering a canonical setting .",
    "for the usual multivariate location - scale estimation problem let @xmath12 denote an @xmath13 matrix of data ( @xmath14 is the number of observations and @xmath0 the number of features ) , where @xmath15 are taken from a @xmath0-dimensional normal distribution with mean vector @xmath4 and covariance matrix @xmath16 . in this article",
    "we let the @xmath5 and @xmath6 be the sample mean and covariance of the features , respectively . in the context of this notation , @xmath17 and @xmath18 .",
    "the usual estimator under invariant quadratic loss is @xmath19 .",
    "it is minimax and admissible when @xmath20 and @xmath21 .",
    "however , when @xmath22 and @xmath21 , @xmath23 remains minimax but is no longer admissible .",
    "explicit improvements are known in the multivariate normal case [ @xcite , gleser ( @xcite ) , @xcite ] and in the case of elliptically symmetric distribution [ @xcite ] .    in this article",
    "we primarily concentrate on the case @xmath24 and construct a class of estimators , depending on the sufficient statistics @xmath25 , of the form @xmath26 which dominate @xmath23 under invariant quadratic loss . note",
    "that , although the loss in ( [ eq2 ] ) is invariant , the estimate in ( [ eq3 ] ) may not be [ except for @xmath23 ] .",
    "this class generalizes several estimators studied previously for the multivariate normal distribution to the @xmath21 setting [ @xcite , gleser ( @xcite ) , @xcite ] .",
    "examples of estimators we study here in this setting extend the class of so - called baranchik estimators and includes a new high - dimensional james  stein estimator @xmath28 where @xmath29 and @xmath30 is the moore ",
    "penrose inverse of @xmath6 .",
    "the estimation of the inverse covariance matrix , namely , the precision matrix @xmath31 , of a multivariate normal distribution has been an important problem in practical situations as well as from a theoretical perspective .",
    "but , when @xmath8 , the wishart - distributed sample covariance matrix is singular ; in this case , one is tempted to construct estimators using the moore ",
    "penrose generalized inverse @xmath30 .",
    "recently there has been an increased interest in the problem of estimating the covariance matrix of large dimension given variables of dimension larger than the number of observations [",
    "our method of proof relies on an unbiased estimator of risk difference , say , @xmath32 .",
    "specifically , we show that , for @xmath33 of the form @xmath34 , the estimator @xmath35 dominates @xmath5 provided @xmath36 . in the next section we present the main results and their proofs are given in section  [ sectech ] .",
    "we need stein s integration - by - parts identity [ @xcite ] and the so - called stein ",
    "haff identity for the singular wishart distribution .",
    "the stein ",
    "haff identity was derived by @xcite and @xcite for the full rank wishart distribution .",
    "a  similar identity for the elliptically contoured model has been given by @xcite .",
    "we make some concluding comments in section  [ secnumstud ] .    for a matrix @xmath37 ,",
    "let @xmath38 denote its transpose , @xmath39 its moore ",
    "penrose pseudo - inverse and @xmath40 its componentwise derivative matrix , that is , the matrix such that @xmath41",
    ". moreover , let @xmath42 denote the kronecker delta.=-1",
    "let @xmath5 be a random vector distributed as @xmath43 with unknown @xmath4 and @xmath9 .",
    "suppose an estimator of @xmath9 is available , say , @xmath44 , with @xmath6 independent of @xmath45 . by definition of the wishart distribution ,",
    "we can write @xmath46 for some matrix normal @xmath47 . an elementary property of this distribution is that @xmath6 is ( almost surely ) invertible if @xmath48 , and ( almost surely ) singular if @xmath8 [ cf .",
    "@xcite ] .",
    "an usual estimator of @xmath4 is @xmath49 ; however , it turns out that this estimator is inadmissible under quadratic loss .",
    "if some estimator @xmath50 is available , with @xmath51 , @xmath52 is dominated by the so - called james  stein estimator @xmath53 the main contribution of this article is to extend this type of result to a more general class of estimators in the @xmath8 setting .    for some positive , bounded and differentiable function @xmath54 , define the baranchik - type estimator @xmath55\\\\[-9pt ] & = & x+g(x , s ) , \\nonumber\\end{aligned}\\ ] ] where @xmath56 is the identity matrix and @xmath30 denotes the moore ",
    "penrose inverse of @xmath6 .",
    "this estimator generalizes the usual @xcite estimator to the unknown covariance setting for @xmath57 .",
    "[ domination ] let @xmath58 .",
    "suppose that :    [ dom1 ] @xmath59 satisfies @xmath60 ;    @xmath59 is nondecreasing ; and    @xmath61 is bounded .    then under invariant quadratic loss , @xmath62 dominates @xmath52 .    throughout the article we will use the expression @xmath63 , which of course equals @xmath64 .",
    "this notation allows us to simultaneously handle both the @xmath57 and @xmath65 cases .",
    "the condition @xmath66 merely guarantees that condition ( i ) of theorem [ domination ] holds for some @xmath59 and is reminiscent of the dimension cutoff in classical stein estimation .",
    "proof of theorem  [ domination ] the hypotheses of the theorem imply that @xmath59 is differentiable almost everywhere . under invariant quadratic loss ,",
    "the difference in risk between @xmath62 and @xmath52 is given by @xmath67 \\nonumber \\\\ & & { } -e_\\theta \\bigl[(x-\\theta)'\\sigma^{-1}(x-\\theta ) \\bigr ] \\\\",
    "& = & 2e_\\theta \\bigl[g(x , s)'\\sigma^{-1}(x-\\theta ) \\bigr]+e_\\theta \\bigl[g(x , s)'\\sigma^{-1}g(x , s ) \\bigr].\\nonumber\\end{aligned}\\ ] ]    in order to show the domination result , we need to show that under the sufficient conditions on @xmath59 , ( [ thmdel ] ) is nonpositive for all @xmath4 .",
    "first , for the leftmost term of ( [ thmdel ] ) it can be shown that @xmath68 = 2e_\\theta \\bigl [ \\mathrm{div}_x g(x , s ) \\bigr].\\ ] ] @xcite give a more general form of this result in their lemma 1(i ) ; it is essentially an extension of stein s classical integration by parts identity . by using lemma  [ divx ] in section  [ sectech ]",
    ", we have that @xmath69 & = & -2e_\\theta \\biggl [ \\mathrm{div}_x \\frac{r(x's^+x)ss^+x}{x's^+x } \\biggr ] \\nonumber\\\\[-8pt]\\\\[-8pt ] & = & -2e_\\theta \\biggl [ 2r'\\bigl(x's^+x\\bigr ) + r\\bigl(x's^+x\\bigr)\\frac{\\operatorname { tr}(ss^+)-2}{x's^+x } \\biggr].\\nonumber\\end{aligned}\\ ] ]    for the right term of ( [ thmdel ] ) , we find , through lemma [ divkonno ] in section  [ sectech ] , @xmath70 \\\\ & & \\qquad= e_\\theta \\biggl[\\operatorname{tr } \\biggl(\\sigma^{-1}s r^2\\bigl(x's^+x\\bigr ) \\frac{s^+xx's^+s}{(x's^+x)^2 } \\biggr ) \\biggr ] \\\\ & & \\qquad= e_\\theta\\biggl [ n \\operatorname{tr } \\biggl(r^2 \\bigl(x's^+x\\bigr)\\frac { s^+xx's^+s}{(x's^+x)^2 } \\biggr ) \\\\ & & \\qquad\\quad{}+ \\operatorname{tr } \\biggl(y'\\nabla_y \\biggl\\ { r^2\\bigl(x's^{+}x\\bigr)\\frac{ss^{+}xx's^{+}}{(x's^{+}x)^2 } \\biggr\\ } \\biggr ) \\biggr].\\end{aligned}\\ ] ] the finiteness of the risk of @xmath62 is guaranteed to hold by theorem  [ momthm ] in section  [ sectech ] for all @xmath0 and @xmath7 .    now applying lemma  [ trdelta ] in section  [ sectech ] ,",
    "we find @xmath71 \\nonumber \\\\ & & \\hspace*{5.6pt}\\qquad= e_\\theta\\biggl [ n \\frac{r^2(x's^{+}x)}{x's^+x } -4r\\bigl(x's^{+}x \\bigr)r'\\bigl(x's^{+}x\\bigr ) \\\\ & & \\hspace*{47pt}\\hspace*{5.6pt}\\qquad\\quad { } + r^2\\bigl(x's^{+}x\\bigr)\\frac{p-2 \\operatorname { tr}(ss^{+})+3}{x's^{+}x } \\biggr ] \\nonumber \\\\ & & \\hspace*{5.6pt}\\qquad= e_\\theta \\biggl[r^2\\bigl(x's^{+}x \\bigr)\\frac{n+p-2 \\operatorname { tr}(ss^{+})+3}{x's^{+}x}-4r\\bigl(x's^{+}x \\bigr)r'\\bigl(x's^{+}x\\bigr ) \\biggr].\\nonumber\\end{aligned}\\ ] ] replacing ( [ thmmaina ] ) and ( [ thmmainb ] ) back into ( [ thmdel ] ) , we obtain @xmath72.\\end{aligned}\\ ] ] since @xmath59 is nonnegative and nondecreasing , it follows that @xmath73 . finally , for the @xmath5 and @xmath6 such that @xmath74 , @xmath75 therefore , under the three sufficient conditions on @xmath59 , it follows that @xmath76 for any  @xmath4 , that is , the domination result holds .",
    "in the @xmath57 setting , we obtain the following two corollaries .",
    "[ coro1 ] for @xmath77 , @xmath62 dominates @xmath52 under invariant quadratic loss for all @xmath59 nondecreasing , differentiable and satisfying @xmath78    for @xmath77 and @xmath79 , the james  stein - like estimator @xmath80 dominates @xmath52 under invariant quadratic loss for all @xmath81    note that if @xmath0 is only moderately larger than @xmath7 , corollary  [ coro1 ] implies that one can construct an estimator with substantial improvement over @xmath52",
    ". however , in the ultra - high - dimensional setting the denominator in ( [ cor1 ] ) could be quite large and , consequently , the amount of improvement over @xmath52 could be quite small .",
    "the estimator in ( [ cor2 ] ) generalizes the classical james  stein with unknown covariance matrix , @xmath82 which is , of course , restricted to the case @xmath21 , for @xmath83 . in this",
    "setting , this result is consistent with previous bounds in @xcite ( where @xmath84 is used instead of our @xmath7 ) .",
    "it remains to clarify several of the somewhat technical computations used in the proof of theorem  [ domination ] .",
    "we provide them in this section ; these computations are likely to be of independent interest and showcase several technical maneuvers that the reader could find useful in dealing with singular wishart matrices .",
    "[ compprop ] let @xmath85 be an @xmath86 matrix , @xmath46 , @xmath5 a @xmath0 vector and @xmath87 .",
    "it then follows that @xmath88    first , notice that from the usual chain - rule that @xmath89 this shows ( i ) .",
    "let @xmath90 be a symmetric matrix and @xmath91 , then @xmath92 this result was , it seems , first proved in @xcite , as their theorem 4.3 , but can be found in standard textbooks on elementary linear algebra . also , again for @xmath90 symmetric",
    ", we have @xmath93 and @xmath94 .",
    "this easily follows from elementary properties of the moore ",
    "penrose pseudoinverse .",
    "since @xmath46 , notice through a singular value decomposition argument that @xmath95 and , thus , @xmath96 . using ( i ) , we find that @xmath97 & & { } + \\sum_{k } \\bigl(x ' \\bigl(i - ss^{+}\\bigr)\\bigr)_k y_{\\alpha k } \\bigl(s^{+}s^{+}x\\bigr)_\\beta \\\\[-1pt ] & = & -2\\bigl(x's^{+}y'\\bigr)_\\alpha \\bigl(s^{+}x\\bigr)_\\beta+2\\bigl(x's^{+}s^{+}y ' \\bigr)_\\alpha \\bigl(\\bigl(i - ss^{+}\\bigr)x\\bigr)_\\beta,\\end{aligned}\\ ] ] which gives ( ii ) .    using ( i ) , we have that for any conformable matrices @xmath90 and @xmath98 @xmath99 & = & \\sum_{i , j } a_{ki } \\",
    "{ \\delta_{\\beta i } y_{\\alpha j } + \\delta_{\\beta j}y_{\\alpha i } \\ } b_{jl } \\\\[-1pt ] & = & \\sum_{j } a_{k\\beta } y_{\\alpha j } b_{jl } + \\sum_{i } a_{ki } y_{\\alpha i } b_{\\beta l } \\\\[-1pt ] & = & a_{k \\beta } ( yb)_{\\alpha l } + \\bigl(ay ' \\bigr)_{k \\alpha } b_{\\beta l}.\\end{aligned}\\ ] ] therefore , using again @xmath96 , @xmath100 & & \\qquad=\\biggl\\ { s^{+}s^{+}\\ , \\frac{\\partial s}{\\partial y_{\\alpha\\beta } } \\bigl(i - ss^{+}\\bigr)xx'ss^{+ } \\\\[-1pt ] & & \\hspace*{3.6pt}\\qquad\\quad{}-s^{+}\\,\\frac{\\partial s}{\\partial y_{\\alpha\\beta}}s^{+}xx'ss^{+ } + \\bigl(i - ss^{+}\\bigr)\\,\\frac{\\partial s}{\\partial y_{\\alpha\\beta } } s^{+}s^{+}xx'ss^{+ } \\\\[-1pt ] & & \\hspace*{3.6pt}\\qquad\\quad{}+ s^{+}xx'\\,\\frac{\\partial s}{\\partial y_{\\alpha\\beta}}s^{+ } + s^{+}xx'ss^{+}s^{+}\\,\\frac{\\partial s}{\\partial y_{\\alpha\\beta } } \\bigl(i - ss^{+}\\bigr ) \\\\[-1pt ] & & \\hspace*{3.6pt}\\qquad\\quad{}-s^{+}xx'ss^{+}\\,\\frac{\\partial s}{\\partial y_{\\alpha\\beta}}s^{+ } + s^{+}xx's\\bigl(i - ss^{+}\\bigr ) \\,\\frac{\\partial s}{\\partial y_{\\alpha\\beta } } s^{+}s^{+ } \\biggr\\}_{kl } \\\\[-1pt ] & & \\qquad= \\bigl(s^{+}s^{+}y ' \\bigr)_{k \\alpha } \\bigl(\\bigl(i - ss^{+}\\bigr)xx'ss^{+ } \\bigr)_{\\beta l } \\\\[-1pt ] & & \\qquad\\quad{}-s^{+}_{k\\beta}\\bigl(ys^{+}xx'ss^{+ } \\bigr)_{\\alpha l}-\\bigl(s^{+}y'\\bigr)_{k\\alpha } \\bigl(s^{+}xx'ss^{+}\\bigr)_{\\beta l } \\\\[-1pt ] & & \\qquad\\quad{}+\\bigl(i - ss^{+}\\bigr)_{k \\beta } \\bigl(ys^{+}s^{+}xx'ss^{+ } \\bigr)_{\\alpha l } \\\\[-1pt ] & & \\qquad\\quad{}+\\bigl(s^{+}xx'\\bigr)_{k \\beta } \\bigl(ys^{+}\\bigr)_{\\alpha l } + \\bigl(s^{+}xx'y ' \\bigr)_{k \\alpha } \\bigl(s^{+}\\bigr)_{\\beta l } \\\\[-1pt ] & & \\qquad\\quad{}+\\bigl(s^{+}xx's^{+}y ' \\bigr)_{k\\alpha}\\bigl(i - ss^{+}\\bigr)_{\\beta l } \\\\[-1pt ] & & \\qquad\\quad{}-\\bigl(s^{+}xx'ss^{+}\\bigr)_{k\\beta } \\bigl(ys^{+}\\bigr)_{\\alpha l } -\\bigl(s^{+}xx'ss^{+}y ' \\bigr)_{k\\alpha}\\bigl(s^{+}\\bigr)_{\\beta l},\\end{aligned}\\ ] ] which gives ( iii ) .",
    "[ trdelta ] under the hypotheses of theorem  [ domination ] , we have @xmath101 where @xmath102 is interpreted as the matrix with components @xmath103 .    to simplify computations , in what will follows , we let @xmath104 .",
    "we then have @xmath105_{ij } \\nonumber \\\\ & & \\qquad=\\sum_{\\alpha,\\beta}\\bigl(y ' \\bigr)_{i\\alpha}\\,\\frac{\\partial}{\\partial y_{\\alpha\\beta } } \\biggl\\ { r^2(f ) \\frac{(ss^{+}xx's^{+})_{\\beta j}}{f^2 } \\biggr\\ } \\nonumber \\\\ \\label{lemmaina } & & \\qquad=2 \\sum_{\\alpha,\\beta}\\bigl(y ' \\bigr)_{i\\alpha } r(f)r'(f ) \\,\\frac{\\partial f}{\\partial y_{\\alpha\\beta } } \\cdot \\frac{(ss^{+}xx's^{+})_{\\beta j}}{f^2 } \\\\",
    "\\label{lemmainb } & & \\qquad\\quad{}+\\sum_{\\alpha,\\beta}\\bigl(y ' \\bigr)_{i\\alpha } r^2(f)\\frac{({\\partial } /{\\partial y_{\\alpha\\beta}})\\{(ss^{+}xx's^{+})_{\\beta j}\\}}{f^2 } \\\\",
    "\\label{lemmainc } & & \\qquad\\quad{}+\\sum_{\\alpha,\\beta}\\bigl(y ' \\bigr)_{i\\alpha } r^2(f)\\frac{- 2\\,({\\partial f}/{\\partial y_{\\alpha\\beta } } ) ( ss^{+}xx's^{+})_{\\beta j}}{f^3}.\\end{aligned}\\ ] ] to simplify ( [ lemmaina ] ) and ( [ lemmainc ] ) , we apply proposition  [ compprop](ii ) to get @xmath106 using this , we get for ( [ lemmaina ] ) @xmath107\\\\[-8pt ] & & \\qquad=-4r(f)r'(f ) \\frac{(ss^{+}xx's^{+})_{ij}}{f}\\nonumber\\end{aligned}\\ ] ] and ( [ lemmainc ] ) becomes @xmath108\\\\[-8pt ] & & \\qquad= 4r^2(f ) \\frac{(ss^{+}xx's^{+})_{ij}}{f^2}.\\nonumber\\end{aligned}\\ ] ]    this leaves the term ( [ lemmainb ] ) to analyze . using proposition [ compprop](iii ) , @xmath109    next , applying this computation in ( [ lemmainb ] ) , we obtain @xmath110\\\\[-8pt ] & & \\qquad= \\bigl(p-\\operatorname{tr}\\bigl(ss^{+}\\bigr)-1\\bigr ) r^2(f)\\frac { ( ss^{+}xx's^{+})_{ij}}{f^2 }   -r^2(f ) \\frac{(ss^+)_{ij}}{f}.\\nonumber\\end{aligned}\\ ] ]    now we can combine ( [ lemsuba ] ) , ( [ lemsubb ] ) and ( [ lemsubc ] ) together to complete the proof .",
    "that is , we have @xmath111 & & \\qquad=\\sum_i \\biggl\\{-4r(f)r'(f ) \\frac{(ss^{+}xx's^{+})_{ii}}{f } \\\\[-2pt ] & & \\hspace*{18.1pt}\\qquad\\quad{}+ 4r^2(f ) \\frac{(ss^{+}xx's^{+})_{ii}}{f^2 } \\\\[-2pt ] & & \\hspace*{18.1pt}\\qquad\\quad{}+ \\bigl(p-\\operatorname{tr}\\bigl(ss^{+}\\bigr)-1\\bigr ) r^2(f)\\frac { ( ss^{+}xx's^{+})_{ii}}{f^2 } -r^2(f ) \\frac{(ss^+)_{ii}}{f } \\biggr\\ } \\\\[-2pt ] & & \\qquad=-4r(f)r'(f)+r^2(f)\\frac{p-2 \\operatorname{tr}(ss^{+})+3}{f}\\end{aligned}\\ ] ] as desired .",
    "[ divx ] under the hypotheses of theorem [ domination ] we have @xmath112    again , to simplify computations , let us denote @xmath113 by @xmath114 .",
    "we find @xmath115 & & \\qquad=\\sum_i \\frac{\\partial}{\\partial x_i } \\biggl\\ { r(f ) \\frac { ( ss^+x)_i}{f } \\biggr\\ } \\\\[-2pt ] & & \\qquad=\\sum_i r'(f ) \\,\\frac{\\partial f}{\\partial x_i } \\frac{(ss^+x)_i}{f } + r(f)\\frac{({\\partial}/{\\partial x_i } ) \\ { ( ss^+x)_i \\}}{f } \\\\[-2pt ] & & \\qquad\\quad{}- r(f)\\frac{({\\partial f}/{\\partial x_i } ) ( ss^+x)_i}{f^2 } \\\\[-2pt ] & & \\qquad=\\sum_i r'(f ) \\biggl\\ { \\frac{\\partial}{\\partial x_i } \\sum_{k , l } x_k x_l s^+_{kl } \\biggr\\}\\frac{(ss^+x)_i}{f } \\\\[-2pt ] & & \\qquad\\quad{}+ r(f)\\frac{({\\partial}/{\\partial x_i } ) \\sum_k ( ss^+)_{ik}x_k}{f } \\\\[-2pt ] & & \\qquad\\quad{}- r(f)\\frac { \\{({\\partial}/{\\partial x_i } ) \\sum_{k , l } x_k x_l s^+_{kl }   \\}(ss^+x)_i}{f^2 } \\\\[-2pt ] & & \\qquad=\\sum_i r'(f ) \\bigl\\ { \\bigl(x's^+\\bigr)_i+\\bigl(x's^+ \\bigr)_i \\bigr\\}\\frac { ( ss^+x)_i}{f } \\\\[-2pt ] & & \\qquad\\quad{}+ r(f)\\frac{(ss^+)_{ii}}{f } - r(f)\\frac { \\{(x's^+)_i+(x's^+)_i   \\}\\cdot ( ss^+x)_i}{f^2 } \\\\[-2pt ] & & \\qquad= 2r'(f ) + r(f)\\frac{\\operatorname{tr}(ss^+)-2}{f}\\end{aligned}\\ ] ] as desired .",
    "the following result is an extension of a result in @xcite .",
    "this type of result was first obtained by @xcite and then was extended by @xcite . in our generalization",
    "we make use of a divergence version of stein s lemma that comes with somewhat weaker moment conditions , rather than the element - by - element assumptions in @xcite .",
    "these weaker moment conditions allow us to cover the @xmath0 equals @xmath7 and @xmath116 cases .",
    "[ divkonno ] let @xmath117 , let @xmath46 which has , by definition , a  @xmath118 distribution , and let @xmath119 be a @xmath120 random matrix that depends on @xmath6 .",
    "let @xmath102 be interpreted as the matrix with components @xmath103 , and for @xmath90 the symmetric positive definite square root of @xmath9 , define @xmath121 and @xmath122",
    ". then @xmath123=e \\bigl[n \\operatorname{tr } ( g ) + \\operatorname{tr } \\bigl(y ' \\nabla_y g ' \\bigr ) \\bigr]\\ ] ] under the conditions @xmath124<\\infty,\\ ] ] where @xmath125 denotes the vectorization of a matrix @xmath37 .",
    "define @xmath126 .",
    "notice that , by construction , @xmath127this means , by definition of the matrix normal distribution , that @xmath128 .",
    "we can write @xmath129&=&e \\biggl[\\sum _ { \\alpha , i , j}\\tilde y_{\\alpha i } \\tilde y_{\\alpha j } h_{ji } \\biggr]\\\\ & = & e \\bigl[\\operatorname{vec}(\\tilde y ) \\cdot \\operatorname{vec } ( \\tilde y h ) \\bigr].\\end{aligned}\\ ] ] using the divergence form of stein s lemma , which can be found in lemma  a.1 in @xcite , we obtain , under the moment conditions outlined in ( [ divkonnocond ] ) , @xmath130 & = & e \\bigl[\\mathrm{div}_{\\operatorname{vec}(\\tilde y ) } \\operatorname { vec } ( \\tilde y h ) \\bigr ] \\\\ & = & e \\biggl[\\sum_{\\alpha , i , j}\\frac{\\partial}{\\partial\\tilde y_{\\alpha i } } \\tilde y_{\\alpha j } h_{ji } \\biggr ] \\\\ & = & e \\biggl[\\sum_{\\alpha , i , j } \\delta_{ij } h_{ji } + \\tilde y_{\\alpha j } \\,\\frac{\\partial h_{ji}}{\\partial\\tilde y_{\\alpha i } } \\biggr ] \\\\ & = & e \\biggl[n \\sum_{i } h_{ii } + \\sum _ { \\alpha , i , j } \\tilde",
    "y_{\\alpha j } \\,\\frac{\\partial}{\\partial\\tilde y_{\\alpha i } } h_{ji } \\biggr].\\end{aligned}\\ ] ] this last expression can be expressed in a compact matrix form as @xmath131=e \\bigl[n \\operatorname{tr}(h)+ \\operatorname{tr } \\bigl(\\bigl(\\tilde y'\\nabla_{\\tilde",
    "y } \\bigr)'h \\bigr ) \\bigr].\\ ] ] finally",
    ", we notice @xmath132&=&e \\bigl[\\operatorname{tr } \\bigl(aga^{-1 } \\bigr ) \\bigr ] , \\\\ e \\bigl[\\operatorname{tr } ( \\tilde sh ) \\bigr]&=&e \\bigl[\\operatorname{tr } \\bigl(a^{-1}sga^{-1 } \\bigr ) \\bigr ] , \\\\ e \\bigl[\\operatorname{tr } \\bigl(\\bigl(\\tilde y ' \\nabla_{\\tilde y}\\bigr)'h",
    "\\bigr ) \\bigr]&=&e \\bigl[\\operatorname{tr } \\bigl(a\\bigl(y'\\nabla_y\\bigr)'ga^{-1 } \\bigr ) \\bigr],\\end{aligned}\\ ] ] which concludes the proof .",
    "[ momthm ] let @xmath133 and for @xmath90 the symmetric positive definite square root of @xmath9 , let @xmath121 .",
    "let @xmath59 be any bounded differentiable nonnegative function @xmath134 $ ] with bounded derivative @xmath135 .",
    "define @xmath136 and @xmath122 .",
    "then for all @xmath0 and @xmath7 @xmath137<\\infty.\\ ] ]    we first compute @xmath138 . as always , to ease notation , we shall write @xmath139 . we have @xmath140 & & \\qquad=\\sum _ { \\alpha , i , j}\\,\\frac{\\partial}{\\partial\\tilde y_{\\alpha i } } \\{\\tilde y_{\\alpha j } h_{ji } \\ } \\nonumber \\\\[-2pt ] & & \\qquad = n\\sum_{i}h_{ii}+\\sum _ { \\alpha , j}\\tilde y_{\\alpha j } \\,\\frac{\\partial h_{ji}}{\\partial\\tilde y_{\\alpha i } } \\nonumber \\\\[-2pt ] & & \\qquad = n\\sum_{i}h_{ii } + \\sum _ { \\alpha,\\beta , i , j}\\tilde y_{\\alpha j } a_{\\beta i } \\,\\frac{\\partial}{\\partial y_{\\alpha\\beta } } \\biggl\\ { r^2(f)\\frac { \\ { as^{+}xx'ss^{+}a^{-1 } \\}_{ji}}{f^2 } \\biggr \\ } \\nonumber \\\\[-2pt ] & & \\qquad = n\\sum_{i}h_{ii}+\\sum _",
    "{ \\alpha,\\beta , i , j}\\tilde y_{\\alpha j } a_{\\beta i } \\nonumber \\\\[-2pt ] \\label{mom1a } & & \\hspace*{79pt}\\qquad\\quad{}\\times\\biggl\\{2r(f)r'(f ) \\,\\frac{\\partial f}{\\partial y_{\\alpha\\beta } } \\frac { \\ { as^{+}xx'ss^{+}a^{-1 } \\}_{ji}}{f^2 } \\\\[-2pt ] \\label{mom1b } & & \\hspace*{96.3pt}\\qquad\\quad{}+\\frac{r^2(f)}{f^2 } \\sum_{k , l } a_{jk } \\,\\frac{\\partial\\ { s^{+}xx'ss^{+}\\}_{kl}}{\\partial y_{\\alpha\\beta } } a^{-1}_{li } \\\\[-2pt ] \\label{mom1c } & & \\hspace*{96.3pt}\\qquad\\quad { } -r^2(f ) \\bigl\\ { as^{+}xx'ss^{+}a^{-1 } \\bigr\\}_{ji } \\frac{2\\,{\\partial f}/{\\partial y_{\\alpha\\beta}}}{f^3 } \\biggr\\}.\\end{aligned}\\ ] ] we simplify each part of the expression . for ( [ mom1a ] ) , using proposition  [ compprop](ii )",
    ", we find @xmath141 & & \\quad= 4\\frac{r(f)r'(f)}{f^2 } \\nonumber\\hspace*{-15pt } \\\\[-2pt ] & & \\qquad{}\\times\\sum_{\\alpha,\\beta , i , j } \\bigl\\{-\\bigl(x's^{+}y'\\bigr)_\\alpha \\tilde y_{\\alpha j } \\bigl\\ { as^{+}xx'ss^{+}a^{-1 } \\bigr\\}_{ji }",
    "a_{i \\beta}\\bigl(s^{+}x \\bigr)_\\beta \\nonumber\\hspace*{-15pt}\\\\[-2pt ] & & \\hspace*{41.1pt}\\qquad{}+ \\bigl(x's^{+}s^{+}y ' \\bigr)_\\alpha\\tilde y_{\\alpha j } \\bigl\\ { as^{+}xx'ss^{+}a^{-1 } \\bigr\\}_{ji } a_{i \\beta } \\bigl(\\bigl(i - ss^{+}\\bigr)x \\bigr)_\\beta \\bigr\\}\\hspace*{-15pt } \\\\[-2pt ] & & \\quad=-4\\frac{r(f)r'(f)}{f^2 } \\bigl(x's^{+}y'ya^{-1}as^{+}xx'ss^{+}a^{-1}as^{+}x \\bigr ) \\nonumber\\hspace*{-15pt } \\\\[-2pt ] & & \\qquad{}+4\\frac{r(f)r'(f)}{f^2 } \\bigl(x's^{+}s^{+}y'ya^{-1}as^{+}xx'ss^{+}a^{-1}a \\bigl(i - ss^{+}\\bigr)x \\bigr ) \\nonumber\\hspace*{-15pt } \\\\[-2pt ] & & \\quad=-4r(f)r'(f).\\nonumber\\hspace*{-15pt}\\end{aligned}\\ ] ] similarly , for ( [ mom1c ] ) @xmath142 & & \\qquad= 4\\frac{r^2(f)}{f^3 } \\sum_{\\alpha,\\beta , i , j } \\bigl(x's^{+}y'\\bigr)_\\alpha \\tilde y_{\\alpha j } \\bigl\\ { as^{+}xx'ss^{+}a^{-1 } \\bigr\\}_{ji}a_{i \\beta}\\bigl(s^{+}x \\bigr)_\\beta \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad= 4\\frac{r^2(f)}{f^3 } \\bigl(x's^{+}y'ya^{-1}as^{+}xx'ss^{+}a^{-1}as^{+}x \\bigr ) \\nonumber \\\\[-2pt ] & & \\qquad= 4\\frac{r^2(f)}{f}.\\nonumber\\end{aligned}\\ ] ] this leaves us with ( [ mom1b ] ) .",
    "using proposition [ compprop](iii ) , we obtain @xmath143 having re - expressed @xmath138 , we now need to bound it above . by virtue of ( [ mom2a ] ) ,",
    "( [ mom2c ] ) and ( [ mom2b ] ) , we have @xmath144 \\nonumber \\\\ & & \\qquad= e\\biggl[\\biggl| n \\operatorname{tr}(h)+4\\frac{r^2(f)}{f } \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\hspace*{15.1pt}\\qquad\\quad { } + \\bigl(p-\\operatorname{tr}\\bigl(ss^{+}\\bigr)-1 \\bigr ) \\frac { r^2(f)}{f } -4r(f)r'(f ) \\biggr|\\biggr ] \\nonumber \\\\ & & \\qquad\\leq c^2_1\\bigl{\\vert}3+p-\\operatorname{tr } \\bigl(ss^{+}\\bigr)+n\\bigr{\\vert}e \\biggl [ \\frac1{f } \\biggr ] + 4c_1c_2.\\nonumber\\end{aligned}\\ ] ] it only remains to show that @xmath145 $ ] is finite . by definition of the wishart matrix distribution , we can define a @xmath146 such that @xmath147 .",
    "let @xmath148 be the spectral decomposition of @xmath149 , with @xmath150 .",
    "write the eigenvalues of @xmath151 as @xmath152 , so that @xmath153 , and let @xmath154 be the smallest nonzero eigenvalue of @xmath151 .",
    "the following two identities follow from @xcite [ theorem 1.1 , equations ( 1.2 ) and ( 1.4 ) ] and symmetry of @xmath149 : @xmath155 using these identities , we have @xmath156 applying cauchy  schwarz provides us with the bound @xmath157 so that we then have @xmath158 to ease notation , let us write @xmath159 and @xmath160 .",
    "collecting the results together , we bound ( [ mom3 ] ) by @xmath161 + 4c_1c_2.\\ ] ] we now use some independence results .",
    "we can write the singular value decomposition of @xmath149 as @xmath148 , but we can also write it as @xmath162 , where @xmath163 is semi - orthogonal ( @xmath164 and @xmath165 is the matrix of the positive eigenvalues of  @xmath149 . if @xmath149 has full rank ( i.e. , @xmath65 ) , then this coincides with the singular value decomposition of @xmath149 . in the full rank case , @xcite [ section 3.4 , equation ( 3.4.3 ) ]",
    "provide the joint density of @xmath166 and @xmath167 in the standard wishart case ( which applies to @xmath149 ) as @xmath168\\\\[-8pt ] & & \\qquad = c(p , n)|d|^{(n - p-1)/2 } \\biggl[\\operatorname{etr } \\biggl(-\\frac12 d \\biggr ) \\biggr ] \\biggl[\\prod_{i < j}(d_i - d_j ) \\biggr]g_p(h)\\nonumber\\end{aligned}\\ ] ] for constants @xmath169 and functions @xmath170",
    ". therefore , @xmath166 and @xmath171 are independent . in the rank - deficient case ( @xmath57 ) , @xcite ( section 3 )",
    "provides an equivalent expression which , in the singular wishart case , gives @xmath172\\\\[-8pt ] & & \\qquad = k(p , n)|d_1|^{(p - n-1)/2 } \\biggl[\\operatorname{etr } \\biggl(-\\frac 12 d_1 \\biggr ) \\biggr ] \\biggl[\\prod_{i",
    "< j}(d_i - d_j ) \\biggr]g_{n , p}(h_1)\\nonumber\\end{aligned}\\ ] ] for constants @xmath173 and functions @xmath174 , so , again , we find @xmath163 and @xmath165 independent by factorization .",
    "now , @xmath154 is a function , in the full rank case ( resp . , rank - deficient case ) , of only @xmath175 ( resp . , @xmath176 ) , and we can write @xmath177 ( resp . , @xmath178 ) , so @xmath154 and @xmath179 are independent",
    ". being functions of @xmath6 , they are also both independent of @xmath5 . now , the nonzero eigenvalues of @xmath151 are the inverses of the nonzero eigenvalues of @xmath149 , a general fact about moore ",
    "penrose pseudo - inverses . therefore , denoting the largest eigenvalue of @xmath149 as @xmath180 , we can split up the expectations in ( [ mom4 ] ) and get the bound @xmath181 e \\biggl [ \\frac{x'qx}{x'rx } \\biggr ] + 4c_1c_2.\\ ] ]    now , it follows from positive semi - definiteness of @xmath149 that @xmath182\\leq e [ \\operatorname{tr}(t ) ] $ ] . if @xmath65 , @xmath183 [ cf .",
    "@xcite , theorem 3.2.20 ] and so @xmath184=pn<\\infty$ ] .",
    "if @xmath57 , recall we can write @xmath185 for @xmath186 by definition of the wishart distribution ; and @xmath187 so that @xmath188 ; so , again , @xmath184=pn<\\infty$ ] .",
    "therefore , in either case , @xmath189\\leq pn < \\infty$ ] .    we still have to check that the expectation involving @xmath5 , @xmath190 and @xmath191 in ( [ mom5 ] ) is finite .",
    "let @xmath192 and write the spectral decomposition of @xmath193 as @xmath194 , with @xmath195 where @xmath196 is the vector of the @xmath59 nonzero eigenvalues of @xmath193 .",
    "then @xmath197 ; let us define the @xmath198 matrix @xmath199'$ ] , that is , so that @xmath200 and @xmath201 has full column rank @xmath202 .",
    "notice that @xmath203 ' = au\\lambda u'u [ 0_{(p - r)\\times r } i_{(p - r)}]'=0 $ ] .",
    "since @xmath190 and @xmath191 are symmetric positive semidefinite , we can use results in @xcite [ theorem 1(i ) with @xmath204 and @xmath205 to conclude that @xmath206 < \\infty.\\ ] ] this concludes the proof of the theorem .",
    "this section provides some numerical results to showcase the improvement in risk of the minimax estimator over the usual estimator .",
    "more precisely , we compared the james  stein estimator in ( [ cor2 ] ) given by @xmath207 and the usual estimator @xmath208 under invariant loss .",
    "( in addition , we considered the positive james  stein estimator to be discussed in section  [ seccomments ] . ) the empirical approximations of the invariant risk of these estimators were plotted for @xmath209 and @xmath210 .",
    "three covariance matrix structures were considered :    _ spiked _ : a diagonal matrix with the first @xmath211 diagonal elements equal to  1 , and the last @xmath211 equal to 10 .",
    "_ autoregressive _ : autoregressive covariance matrices of the form @xmath212 for @xmath213 .",
    "_ block diagonal _ : block diagonal matrices with @xmath211 blocks of the form @xmath214 for @xmath215 .    in all cases ,",
    "the true mean was chosen as @xmath216 .",
    "we remind the reader that the risk of the trivial estimator is always @xmath0 , regardless of @xmath4 or @xmath9 . with this in mind , we see from figure  [ simplots ] that in all six scenarios the pattern of     and @xmath217 for @xmath218 are in the left and right columns , respectively .",
    "the lines , from thinnest to thickest , are for @xmath219 and @xmath220 .",
    "the solid and dashed lines are , respectively , for @xmath221 and @xmath222 . ]",
    "domination of the new estimator is similar to one of the usual james  stein estimators .",
    "also note that , as predicted by the theoretical results , the domination decreases as the smaller @xmath7 tends to @xmath0 .",
    "an interesting property of the moore ",
    "penrose inverse is that for any @xmath90 , @xmath223 is the matrix that projects onto the subspace spanned by @xmath90 ( its column space ) .",
    "it follows that the proposed generalized baranchik estimator can be expressed as @xmath224\\\\[-8pt ] & = & p_{s^\\perp}x+ \\biggl(1-\\frac{r(x's^+x)}{x's^+x } \\biggr)p_s x,\\nonumber\\end{aligned}\\ ] ] where @xmath225 and @xmath226 are the projection matrices onto the column space of @xmath6 and its orthogonal complement , respectively . in terms of the kernel and image of the symmetric matrix @xmath6 , @xmath227 and @xmath228 .",
    "when @xmath57 , this means we can interpret our estimator as applying shrinkage only on the component of @xmath5 in the subspace spanned by our covariance matrix estimator @xmath6 . in particular , note that the estimator @xmath229 dominates @xmath230 under invariant loss function ( [ eq2 ] ) , since @xmath231 if @xmath59 satisfies the conditions of theorem  [ domination ] .",
    "this suggests there might be an easier , more abstract proof of theorem  [ domination ] , one not relying on brute computations but on the already known full rank @xmath6 case , although we have not been able to obtain such a result .    a natural extension of the james  stein estimator , @xmath232 in ( [ cor2 ] ) , is a positive - part - type james  stein estimator .",
    "the form of the estimator in ( [ projform ] ) suggests @xmath233 where @xmath234 .",
    "simulation evidence from figure [ simplots ] suggests that for @xmath218 , @xmath235 dominates @xmath232 under invariant loss .",
    "one of the interesting differences between the @xmath236 and @xmath57 cases is the reversal of the roles of @xmath0 and @xmath7 .",
    "this is essentially due to the distribution of the singular values of @xmath6 .",
    "recall that for @xmath147 , @xmath237 .",
    "we can write the singular value decomposition of @xmath149 as @xmath148 , but we can also write it as @xmath162 , where @xmath163 is semi - orthogonal ( @xmath164 and @xmath165 is the matrix of the positive eigenvalues of @xmath149 .",
    "if @xmath149 has full rank ( i.e. , @xmath238 ) , this coincides with the singular value decomposition of @xmath149 . in the full rank case the joint density of @xmath166 and @xmath171 is given in ( [ singdist1 ] ) , whereas in the rank - deficient case ( @xmath57 ) joint density is given by ( [ singdist2 ] ) , from which stems the reversal of the roles of @xmath0 and @xmath7 .    in the heteroscedastic normal mean estimation problem",
    ", @xcite used the loss function that was weighted by the inverse of the variances and , consequently , the problem is essentially transformed to the homoscedastic case under ordinary squared error loss .",
    "similarly , in this article , we used the invariant loss function in ( [ eq2 ] ) , therefore skirting a somewhat subtle issue . in the heteroscedastic",
    "setting where there are differing coordinate variances , minimax estimation and bayes ( or empirical bayes ) estimates can be qualitatively different .",
    "it turns out that minimax estimators in general shrink most on the coordinates with smaller variances , while bayes estimators shrink most on large variance coordinates .",
    "@xcite shows that the james  stein shrinkage estimator does not dominate the @xmath5 when the largest variance is larger than the sum of the rest .",
    "moreover , @xcite points out that the james  stein shrinkage estimator may not be a desirable shrinkage estimator under heteroscedasticity even when it is minimax . @xcite and @xcite give an excellent perspective on minimaxity of the shrinkage estimator from bayes and empirical bayes points of view . consequently , it would be of interest to examine the shrinkage patterns of the proposed estimates in the case of a noninvariant loss function and assess how well the invariant loss works for @xmath57 applications .",
    "one can imagine an extension of the results of this article beyond the normal distribution setting .",
    "consider a model with the joint density for @xmath239 the form @xmath240 \\bigr),\\ ] ] where the @xmath241 location vector @xmath4 and the @xmath242 scale matrix @xmath9 are unknown . in the setting of @xmath243 , @xcite and @xcite give some results on improved location estimation for elliptically symmetric distributions . for more on elliptical symmetry and the various choices of @xmath244 in ( [ ess ] ) ,",
    "see @xcite ; the class in ( [ ess ] ) contains models such as the multivariate normal , @xmath245- and kotz - type distributions .",
    "finally , simulation study reveals that , when @xmath0 is much larger than @xmath7 , the estimate of @xmath9 and @xmath31 are quite poor .",
    "this observation agrees with @xcite , where @xcite - type improved estimates of @xmath9 are proposed .",
    "it would be of interest to use an improved estimator of @xmath9 in @xmath246 in ( [ deltar ] ) . as pointed out in the testing context by @xcite and @xcite ,",
    "a shortcoming of @xmath30 is that the associated estimator is only orthogonally invariant , while the sample mean vector is invariant .",
    "the authors are grateful to the associate editor and referees for helpful comments that strengthened the exposition and scope of this paper ."
  ],
  "abstract_text": [
    "<S> we consider the problem of estimating the mean vector of a @xmath0-variate normal @xmath1 distribution under invariant quadratic loss , @xmath2 , when the covariance is unknown . we propose a new class of estimators that dominate the usual estimator @xmath3 . </S>",
    "<S> the proposed estimators of @xmath4 depend upon @xmath5 and an independent wishart matrix @xmath6 with @xmath7 degrees of freedom , however , @xmath6 is singular almost surely when @xmath8 . </S>",
    "<S> the proof of domination involves the development of some new unbiased estimators of risk for the @xmath8 setting . </S>",
    "<S> we also find some relationships between the amount of domination and the magnitudes of @xmath7 and @xmath0 . </S>"
  ]
}