{
  "article_text": [
    "recently , the both schools of takemura and takayama have developed a quite interesting minimization method called holonomic gradient descent method(hgd ) .",
    "it utilizes grbner basis in the ring of differential operator with rational coefficients .",
    "grbner basis in the differential operators plays a central role in deriving some differential equations called a pfaffian system for optimization .",
    "hgd works by a mixed use of pfaffian system and an iterative optimization method .",
    "it has been successfully applied to several maximum likelihood estimation ( mle ) problems , which have been intractable in the past .",
    "for example , hgd solve numerically the mle problems for the von mises - fisher distribution and the fisher - bingham distribution on the sphere ( see , sei et al.(2013 ) and nakayama et al.(2011 ) ) .",
    "furthermore , the method has also been applied to the evaluation of the exact distribution function of the largest root of a wishart matrix , and it is still rapidly expanding the area of applications(see , hashiguchi et al.(2013 ) ) . on the other hand , in statistical models ,",
    "it is not rare that parameters are constrained and therefore the mle problem with constraints has been surely one of fundamental topics in statistics . in this paper",
    ", we develop hgd for mle problems with constraints , which we call the constrained holonomic gradient descent(chgd ) .",
    "the key of chgd is to separate the process into ( a ) updating of new parameter values by newton - raphson method with penalty function and ( b ) solving a pfaffian system .",
    "we consider the following the constrained optimization problem .",
    "@xmath2 where @xmath3 , @xmath4 and @xmath5 are all assumed to be continuously differentiable function .",
    "@xmath6 is an equality constraint function and @xmath7 is an inequality constraint function . in this paper ,",
    "the objective function @xmath8 is assumed to be holonomic .",
    "we call the interior region defined by the constraint functions _ the feasible region_.      a penalty function method replaces a constrained optimization problem by a series of unconstrained problems",
    ". it is performed by adding a term to the objective function that consists of a penalty parameter @xmath9 and a measure of violation of the constraints . in our simulation",
    ", we use _ the exact penalty function method_. the definition of the exact penalty function is given by ( see yabe ( 2006 ) ) . @xmath10",
    "assume that we seek the minimum of a holonomic function @xmath8 and the point @xmath11 which gives the minimum @xmath8 .",
    "in hgd , we use the iterative method together with a pfaffian system . in this paper",
    ", we use the the newton - raphson iterative minimization method in which the renewal rule of the search point is given by @xmath12 where @xmath13 and @xmath14 is the hessian of @xmath8 at @xmath15 .",
    "hgd is based on the theory of the grbner basis . in the following ,",
    "we refer to the relation of a numerical method and the grbner basis .",
    "let @xmath16 be the differential ring written as @xmath17 \\langle \\partial_1, ..",
    ",\\partial_n \\rangle \\nonumber\\end{aligned}\\ ] ] where @xmath18 $ ] are the rational coefficients of differential operators .",
    "suppose that @xmath19 is a left ideal of @xmath16 , @xmath20 $ ] is a field and @xmath21\\langle \\partial_1, .. ,\\partial_n \\rangle \\in i$ ] .",
    "if an arbitrary function @xmath22 satisfies @xmath23 for all @xmath24 , then @xmath22 is a solution of @xmath25 .",
    "that is @xmath26 when @xmath22 satisfies equation ( [ eq_h ] ) , @xmath22 is called _",
    "holonomic function_. let @xmath27 $ ] , with @xmath28 be a standard basis in the quotient vector space @xmath29 which is a finite dimensional vector spaces .",
    "let @xmath30 be the grbner basis of @xmath25 .",
    "the rank of arbitrary differential operations can be reduced by normalization by @xmath30 .",
    "assume that @xmath31 holds .",
    "for a solution @xmath22 of @xmath25 put @xmath32 .",
    "then , it holds that    ( see , e.g.,nakayama et al.(2011 ) ) @xmath33 where @xmath34 is a @xmath35 matrix with @xmath36 as a @xmath37 element    @xmath38_{j } , \\ \\",
    "i=1, ... ,n,\\ \\ j=1 ... ,t\\end{aligned}\\ ] ]    this proves the assertion .    the above differential equations are called _ pfaffian differential equations _ or _ pfaffian system _ of @xmath25 .",
    "so we can calculate the gradient of @xmath39 by using pfaffian differential equations .",
    "then , @xmath40 and @xmath41 are also given by pfaffian differential equations .",
    "( see hibi et al.(2012 ) )    let @xmath42 be the normal form of @xmath43 by @xmath30 and @xmath44 be the normal form of @xmath45 by @xmath30 . then we have , @xmath46 where @xmath47 denotes the first entry of a vector @xmath48 .      for hgd , we first give an ideal @xmath49 for holonomic function @xmath8 and calculate the grbner basis @xmath30 of @xmath25 and then the standard basis @xmath50 are given by @xmath30 .",
    "the coefficient matrix @xmath34 for pfaffian system is led by this standard basis , and @xmath41 and @xmath40 are calculated from @xmath50 by starting from a initial point @xmath51 through the pfaffian equations .",
    "after these , we can compute automatically the optimum solution by a mixed use of then newton - raphson method .",
    "the algorithm is given by below .",
    "* set @xmath52 and take an initial point @xmath53 and evaluate @xmath54 . *",
    "evaluate @xmath40 and @xmath55 from @xmath39 and calculate the newton direction , @xmath56 * update a search point by @xmath57 . *",
    "evaluate @xmath58 by solving pfaffian equations numerically .",
    "* set @xmath59 and calculate @xmath58 and goes to step.2 and repeat until convergence .",
    "the key step of the above algorithm is step 4 .",
    "we can not evaluate @xmath58 by inputting @xmath60 in the function @xmath8 since the hgd treats the case that @xmath8 is difficult to calculate numerically .",
    "instead , we only need calculate @xmath61 and @xmath62 numerically for a given initial value @xmath51 .",
    "now , we propose the method in which we add constraint conditions to hgd and call it the constrained holonomic gradient descent method(chgd ) .      for treating constraints we use the penalty function and add it to objective function and make a new objective function and can treat it as the unconstrained optimization problem .",
    "we use hgd for evaluation of gradients and hessian and use the exact penalty function method for constraints .",
    "the value of updating a search point can be obtained as the product of directional vector and step size .",
    "the step size @xmath63 is chosen so that the following armijo condition is satisfied .",
    "in fact we chose @xmath63 such that @xmath64 where @xmath65 and @xmath66 is the approximation of @xmath67 given by .",
    "@xmath68 the initial value of @xmath63 is set @xmath69 and then @xmath63 is made smaller iteratively until @xmath63 satisfies equation ( [ eq_s ] ) , or @xmath70 . in our algorithm ,",
    "holonomic gradient descent plays a role to calculate the gradient vectors and then the penalty function plays a role to control the step size iteratively .",
    "we apply chgd for mle for von mises distribution(vm ) .",
    "the process of applying for hgd is shown in nakayama et al.(2011 ) .",
    "the density function of vm is given by @xmath71 .",
    "the parameters of vm , @xmath72 and @xmath73 , show concentration and mean of angle data @xmath74 respectively .",
    "we set the parameters for mle @xmath75 and @xmath76 .",
    "now we solve the constrained optimization problem given by .",
    "@xmath77 let @xmath74 be sample data .",
    "let @xmath78 be sample size .",
    "then , @xmath79 and @xmath80 .      in our simulation",
    ", we set the vm s parameter @xmath81 of which the true value @xmath82 and the initial value @xmath83 . we tried the 2 patterns of constraints .",
    "both of the case worked under the same condition except constraints . in figure 1",
    ", the constraint is @xmath84 . in figure 2 ,",
    "the constraint is @xmath85 .",
    "figures 1,2 are the drawing of the trace of the search point .          the result of simulation , the convergence point of hgd is @xmath86 . in figure 1 ,",
    "the convergence point of chgd is @xmath87 . in figure 2 ,",
    "the convergence point of chgd is @xmath88 . in the chgd ,",
    "the search direction is almost same as the hgd , because the direction is decided by the hgd s algorithm . while , the constraints play the role to judge the search point is within the feasible region or not and decide the step size .",
    "chgd is the effective method for optimization with constraints .",
    "however , whenever chgd increases the cost of runtimes than hgd regardless of whether the solution is in the feasible region or not .",
    "the following table shows the runtimes when the optimization solution is within the feasible region .      in table",
    "[ tb1 ] , all numbers are the means of 500 times trials . the optimization problem is equation ( [ optvm ] ) .",
    "sample data is drawn from the vm with @xmath89 .",
    "the third column of table [ tb1 ] is the result with only newton - raphson method which optimize @xmath8 directly , not use pfaffian system .",
    "thus , we see that hgd and chgd is faster than newton - raphson method .",
    "we see that the runtimes of chgd is longer than hgd in general , where the both of solutions are almost the same value when the solution is inside the feasible region .",
    "sometimes the process finishes early by constraints , when the solution is outside the feasible region .",
    "although , we need consider the cost of calculation of chgd .    99 hashiguchi , h. , numata , y. , takayama , n. , takemura , a. ( 2013 ) .",
    "_ `` the holonomic gradient method for the distribution function of the largest root of a wishart matrix''_. journal of multiva , riate analysis 117 ( 2031 ) 296 - 312        nakayama , h. , nishiyama , k. , noro , m. , ohara , k. , sei , t. , takayama , n. , takemura , a. ( 2011 ) .",
    "_ `` holonomic gradient descent and its application to the fisher ",
    "bingham integral''_. advances in applied mathematics , 47(3 ) , 639 - 658 .",
    "yabe , h. ( 2006 ) .",
    "_ `` introduction and application of optimization problem(japanese)''_. surikougakusha publisher .",
    "cox , d. a. , little , j. , oshea , d. ( 2007 ) .",
    "_ `` ideals , varieties , and algorithms : an introduction to computational algebraic geometry and commutative algebra ( vol .",
    "10)''_. springer verlag ."
  ],
  "abstract_text": [
    "<S> recently , the school of takemura and takayama have developed a quite interesting minimization method called _ holonomic gradient descent method _ </S>",
    "<S> ( hgd ) . </S>",
    "<S> it works by a mixed use of pfaffian differential equation satisfied by an objective holonomic function and an iterative optimization method . </S>",
    "<S> they successfully applied the method to several maximum likelihood estimation ( mle ) problems , which have been intractable in the past . on the other hand , in statistical models , it is not rare that parameters are constrained and therefore the mle with constraints has been surely one of fundamental topics in statistics . in this paper we develop hgd with constraints for mle .    * holonomic decent minimization method for restricted maximum likelihood estimation * + rieko sakurai@xmath0 , and toshio sakata @xmath1 + @xmath0  _ graduate school of medicine , kurume university 67 asahimachi , kurume 830 - 0011 , japan _ </S>",
    "<S> + @xmath1  _ faculty of design human science , kyushu university , 4 - 9 - 1 shiobaru minami - ku , fukuoka 815 - 8540 , japan _ + email : a213gm009s@std.kurume-u.ac.jp    _ key words : holonomic gradinet descent method , newton - raphson method with penalty function , von mises - fisher distribution _ </S>"
  ]
}