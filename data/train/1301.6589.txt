{
  "article_text": [
    "traditionally , data transmission in a communication system is based on tight synchronization between the transmitter and the receiver .",
    "this tight synchronization is usually achieved through either of two strategies . in the first strategy",
    ", synchronization is achieved through periodic transmission of pilot signals , followed by transmission of information over the synchronized channel . in the second strategy",
    ", data bits are modulated differentially which implicitly achieves tight synchronization .",
    "the above strategies work well at high signal - to - noise ratios ( snrs ) as the energy overhead of achieving tight synchronization is negligible compared to that of data transmission .",
    "however , in many applications , such as wireless sensor networks , space communication , or in general any communication system requiring high energy efficiency , communication by necessity has to primarily take place in the low - snr regime ( due to the concavity of the power - rate function ) . in such scenarios , the energy overhead to achieve tight synchronization becomes significant and can render the aforementioned strategies highly suboptimal in terms of energy efficiency .",
    "in fact , it can be shown that requiring tight transmitter - receiver synchronization can have arbitrarily large loss in performance in terms of energy efficiency .    to mitigate this , in this paper ,",
    "we develop and analyze a framework to perform data transmission while only requiring loose synchronization between the transmitter and the receiver . to focus on the energy - efficiency aspect",
    ", we choose the rate per unit cost ( with energy being a prime example of the cost ) as our performance metric .",
    "we model synchronization errors through channel insertions / deletions  an approach introduced in  @xcite . to motivate this model , consider a transmitter - receiver pair with unsynchronized clocks , as illustrated in fig .  [ fig : motive ] . due to the absence of synchronization ,",
    "the value of the clock at the receiver exhibits drift and jitter with respect to the value of the reference clock at the transmitter .",
    "this leads to the receiver sampling the transmitted signal either faster than the transmitter , leading to channel insertions , or slower , leading to channel deletions .",
    "-axis ) as a function of the value of the reference clock at the transmitter ( @xmath0-axis ) .",
    "the drift and jitter of the receiver clock are visible . for a transmitted input sequence ,",
    "the lack of synchronization leads to insertions / deletions in the corresponding sampled output sequence at the receiver ( illustrated here for the case without receiver noise ) . ]",
    "before we describe the contributions of this work in more detail in section  [ sec : intro_summary ] , we provide a brief overview of related work on energy - efficient communication and on channels with synchronization errors .",
    "it is well known that the capacity per unit energy of a gaussian channel with noise variance @xmath1 is @xmath2 , and that this can be achieved with appropriately designed pulse - position modulation  @xcite . for a general discrete memoryless channel ( dmc ) , @xcite has analyzed the reliability function of the rate per unit cost .",
    "subsequently , @xcite has obtained a succinct single - letter characterization for the capacity per unit cost of a dmc with a general cost function .",
    "these results , however , strongly depend on the channel being memoryless . as discussed next ,",
    "synchronization errors introduce memory in the channel , and thus the aforementioned results do not apply .",
    "the insertion / deletion / substitution channel was introduced by dobrushin in 1967  @xcite . despite significant research effort since then ,",
    "the capacity of this channel is still not known  @xcite .",
    "indeed , even for one of the simplest versions of this problem , the noiseless binary deletion channel , only loose bounds on the capacity are known .",
    "for example , the recent paper  @xcite provides an approximation of the capacity of the binary deletion channel to within a factor @xmath3 .",
    "the main difficulty in analyzing these channels is due to the channel memory introduced by the insertions and deletions , which prevents direct application of standard information - theoretic tools .",
    "it is worth emphasizing that the synchronization errors considered here are those at the symbol level .",
    "there are other types of synchronization issues .",
    "one such issue is frame synchronization , where errors arise due to incorrect identification of the location of the `` sync word '' in the frame  @xcite .",
    "energy - efficient communication in the presence of such frame asynchronism has been investigated in  @xcite .       with a discrete memoryless channel @xmath4 . ]    in this paper , we consider communication channels which , in addition to synchronization errors , exhibit receiver noise .",
    "we model the end - to - end communication channel between the transmitter and the receiver as the concatenation of two sub - channels , as illustrated in fig .",
    "[ fig : fig_idc_dmc ] .",
    "the first sub - channel is an insertion / deletion channel ( idc ) , which models synchronization errors .",
    "the second sub - channel is a noisy memoryless channel , which models errors due to the receiver noise .",
    "the concatenation of the two channels is an insertion / deletion / substitution channel .",
    "the details of this model are discussed in section  [ sec : prob ] .",
    "we first study communication systems with synchronization errors operating over gaussian channels .",
    "we propose a new communication scheme that requires only loose synchronization between the transmitter and the receiver .",
    "specifically , the scheme deals with the lack of transmitter - receiver synchronization by developing new pulse - position - modulation waveforms , where the signal energy is spread over increasing intervals and guard spaces of increasing lengths are introduced .",
    "decoding at the receiver is based on a sequence of independent hypothesis tests . when the aforementioned durations are chosen appropriately",
    ", we show that the scheme asymptotically achieves the information - theoretically optimal performance in terms of the energy efficiency , i.e. , the capacity per unit energy . in the process , we also establish that the lack of transmitter - receiver synchronization causes negligible loss in terms of energy efficiency .",
    "we then analyze communication systems with synchronization errors operating over general dmcs and with a general cost function .",
    "we generalize the proposed achievable scheme for the gaussian case to dmcs , and we show that the scheme achieves a rate per unit cost within a factor two of the information theoretic optimum .",
    "thus , while only loose bounds are known for the _ capacity _ of the general insertion / deletion / substitution channel , we provide here a tight approximation for its _ capacity per unit cost_. to establish this , we obtain an upper bound on the capacity per unit cost of the channel in fig .",
    "[ fig : fig_idc_dmc ] by considering the effect of the idc as a specific way of encoding for the dmc with an appropriately modified cost function .",
    "the upper bound is then obtained by utilizing the characterization of the capacity per unit cost for memoryless channels in  @xcite .",
    "the remainder of the paper is organized as follows .",
    "section  [ sec : prob ] provides the detailed description of the channel model and the problem formulation .",
    "the main results of the paper are summarized in section  [ sec : main ] . section  [ sec : propose ] describes the proposed scheme for a general discrete memoryless channel with synchronization errors .",
    "section  [ sec : converse ] derives an upper bound on its capacity per unit cost .",
    "section  [ sec : propose_gauss ] discusses the proposed scheme for the gaussian channel with synchronization errors and establishes its asymptotic optimality .",
    "lastly , section  [ sec : propose_compound ] analyzes the performance of the scheme for the gaussian channels with synchronization errors where even the statistical properties of errors are not known precisely a priori ( i.e. , the compound setting ) .",
    "we consider an insertion / deletion / substitution channel with a cost constraint .",
    "the insertion / deletion / substitution channel consists of an idc connected to a dmc as shown in fig .",
    "[ fig : fig_idc_dmc ] in section  [ sec : intro ] . the insertion/ deletion part of the channel models synchronization errors ( see fig .  [",
    "fig : motive ] in section  [ sec : intro ] ) , the substitution part models noise .",
    "the idc maps the input sequence @xmath5 , \\dots , x[t])\\in\\mc{x}^t$ ] to the output sequence @xmath6 , \\dots , \\tilde{\\msf{x}}[\\msf{l}])\\in\\mc{x}^\\msf{l}$ ] for some random length @xmath7 , where here and in the following we use sans - serif font to denote random variables .",
    "the actions of the idc are governed by the sequence of states @xmath8 , \\dots , \\msf{s}[t])\\in\\{0,1,2,\\dots\\}^t$ ] .",
    "state @xmath9 $ ] describes how many times input symbol @xmath10 $ ] appears at the output of the idc .",
    "formally , the total number of output bits is given by @xmath11.\\ ] ] observe that @xmath7 is a random variable depending on the state sequence of the idc .",
    "define for each @xmath12 the random variable @xmath13 \\defeq \\min\\big\\ { t : \\sum_{j=1}^{t } \\msf{s}[j ]",
    "\\geq \\ell \\big\\}.\\ ] ] the relationship between the input and output of the idc is then given by @xmath14 \\defeq x[\\msf{t}[\\ell]].\\ ] ] we illustrate the operation of the idc with an example .",
    "@xmath15 & x[2 ] & x[3 ] & x[4 ] & x[5 ] & x[6 ] & \\\\              \\bm{\\msf{s } } & = & 1 & 1 & 2 & 1 & 0 & 2 & \\\\              \\bm{\\msf{t } } & = & 1 & 2 & 3 & 3 & 4 & 6 & 6   \\\\               \\bm{\\tilde{\\msf{x } } } & = & x[1 ] & x[2 ] & x[3 ] & x[3 ] & x[4 ] & x[6 ] & x[6 ]           \\end{array}\\ ] ]    in the example , @xmath16 and @xmath17 . here",
    "@xmath18 is the vector of inputs and @xmath19 is the vector of outputs of the idc .",
    "@xmath20 is the vector of states and the corresponding vector of sampling times is @xmath21 .",
    "for concreteness , assume @xmath22 see also fig .",
    "[ fig : motive ] in section  [ sec : intro ] .",
    "we denote by @xmath23 ) \\\\",
    "\\shortintertext{and}\\\\      \\sigma^2 & \\defeq \\var(\\msf{s}[1])\\end{aligned}\\ ] ] the mean and variance of the insertion / deletion process , respectively , and we refer to any idc with those parameters as @xmath24 . here",
    ", @xmath25 and @xmath26 can be interpreted as capturing the drift and jitter of the receiver clock , respectively . in most situations arising in practice ,",
    "the parameter @xmath25 is close to @xmath27 , e.g. , @xmath28 .",
    "our results will be presented for arbitrary insertion / deletion processes with finite mean and variance .",
    "for illustrative purposes , we present a commonly used special case of this setting .    [ eg : deletion ] one commonly used definition of the state process is @xmath29 =           \\begin{cases }              1 , & \\text{w.p . } 1-d , \\\\              0 , & \\text{w.p . }",
    "d ,          \\end{cases}\\ ] ] for some parameter @xmath30 .",
    "this results in the so - called _ deletion channel _ , which deletes each input symbol with probability @xmath30 .",
    "the output of the @xmath31 is then fed into a discrete memoryless channel @xmath4 described by the distribution @xmath32 of the channel output @xmath33 given the channel input @xmath34 .",
    "the insertion / deletion / substitution channel is the ( random ) mapping from @xmath0 to @xmath35 described by the concatenation of the @xmath31 and the @xmath4 .",
    "the goal is to maximize the number of bits reliably transmitted per unit cost over this insertion / deletion / substitution channel formed by the concatenation of the @xmath31 with the @xmath4 .",
    "we adopt the framework of  @xcite . the _ cost function _",
    "@xmath36 associates with each input symbol @xmath37 the cost @xmath38 incurred by transmitting @xmath0 over the channel .",
    "we make the assumption that @xmath39 contains a free input symbol , and without loss of generality we label this symbol as @xmath40 . in other words , @xmath41 and @xmath42 . for an input sequence @xmath5 , \\dots , x[t])\\in\\mc{x}^t$ ] ,",
    "the cost is given by @xmath43 , \\dots , x[t ] ) \\bigr )       \\defeq \\sum_{t=1}^t c(x[t]).\\ ] ] a _ @xmath44 code _ consists of @xmath45 codewords @xmath46 , \\dots , x_{m}[t ] ) , \\quad m\\in\\{1,2,\\ldots , m\\}\\ ] ] each of length @xmath47 and cost at most @xmath48 , with average ( assuming equiprobable messages ) probability of decoding error at most @xmath49 .    a rate @xmath50 per unit cost is _ achievable _ if for every @xmath51 and every large enough @xmath45 there exists a @xmath44 code satisfying and @xmath52 denote the logarithms to the base @xmath53 and @xmath54 , respectively . ]",
    "the _ capacity per unit cost _ @xmath56 is the supremum of achievable rates per unit cost .    throughout this paper",
    ", we are interested in @xmath57 , the capacity per unit cost of the insertion / deletion / substitution channel given by the @xmath31 concatenated with the @xmath4 .",
    "we also consider the compound capacity per unit cost @xmath58,[\\sigma_1 ^ 2,\\sigma_2 ^ 2],w)$ ] , for which the encoder and decoder have to be able to operate on any @xmath59 with @xmath60 $ ] and @xmath61 $ ] without knowledge of the actual values of @xmath25 and @xmath26 .",
    "this compound setting is of practical relevance , since usually the mean clock drift @xmath25 is only specified as an interval ( and might indeed be slowly time varying ) and is hence not known exactly at the transmitter or the receiver .",
    "we also treat the gaussian version of the problem , where the output of the @xmath31 is subject to additive gaussian noise of mean zero and variance @xmath1 .",
    "the cost function is in this case the signal energy , i.e. , @xmath62 . with slight abuse of notation",
    ", we refer to the capacity per unit energy in this case as @xmath63 .",
    "in this section , we summarize the main results ; their proofs are discussed in subsequent sections .",
    "we start with the results for gaussian channels with synchronization errors for the case where the statistics @xmath25 and @xmath26 of the insertion / deletion channel are known at the transmitter and the receiver .",
    "[ thm : chat_gauss ] the gaussian channel with synchronization errors having insertion / deletion process with mean @xmath25 and variance @xmath26 and having noise power @xmath1 has capacity per unit energy @xmath64    recall that the capacity per unit energy of the gaussian channel is @xmath65 . furthermore , as discussed in section  [ sec : prob ] , the mean @xmath25 of the insertion / deletion process is typically close to @xmath27 .",
    "hence , theorem  [ thm : chat_gauss ] implies that the lack of synchronization results in only negligible loss in the capacity per unit energy .    to establish achievability",
    ", we propose a new communication scheme that jointly performs data modulation and loose synchronization . to this end , we develop signaling waveforms where the signal energy is spread over increasing intervals and guard spaces of increasing lengths are introduced . decoding at the receiver is based on a sequence of independent hypothesis tests , which are carefully chosen to account for the uncertainty arising due to the lack of tight synchronization . in section  [ sec : propose_gauss ] , we show that , by appropriately choosing the aforementioned durations , the probability of error can be made arbitrarily small for any rate per unit energy up to @xmath56 .",
    "the details of the analysis of this scheme are reported in section  [ sec : propose_gauss ] .",
    "the upper bound in theorem  [ thm : chat_gauss ] follows as a special case of the upper bound derived for a general dmc with synchronization errors , discussed in theorem  [ thm : chat_dmc ] below .",
    "next , consider the case where the exact statistical properties of the insertion / deletion process are not known a priori .",
    "instead , we only know a range for each parameter , i.e. , the mean @xmath25 is in @xmath66 $ ] and the variance in @xmath67 $ ] .",
    "we are interested in a communication scheme that works simultaneously for every possible set of parameters in this range . as pointed out in section  [ sec : prob ] , this compound setting is of practical relevance , since the precision of the transmitter and receiver clocks are usually only known to lie within some range .",
    "[ thm : chat_compound ] the class of gaussian channels with synchronization errors having insertion / deletion process with mean @xmath60 $ ] and variance upper bounded by @xmath26 and having noise power @xmath1 has compound capacity per unit energy @xmath68 , [ 0 , \\sigma^2 ] , { \\cal n}(0,\\eta^2 ) )            = \\frac{\\mu_1}{2\\eta^2\\ln 2}.\\ ] ]    comparing theorems  [ thm : chat_gauss ] and  [ thm : chat_compound ] , we see that @xmath68 , [ 0 , \\sigma^2 ] , { \\cal n}(0,\\eta^2 ) )            = \\min_{\\mu\\in[\\mu_1,\\mu_2]}\\hat{c } ( \\mu , \\sigma^2 , { \\cal n}(0,\\eta^2)).\\ ] ] since a scheme for the compound setting must work for any possible value of @xmath25 and @xmath26 of the insertion / deletion process , it is clear that it must work for the worst one , so that @xmath68 , [ 0 , \\sigma^2 ] , { \\cal n}(0,\\eta^2 ) )            \\leq \\min_{\\mu\\in[\\mu_1,\\mu_2]}\\hat{c } ( \\mu , \\sigma^2 , { \\cal n}(0,\\eta^2 ) )          = \\frac{\\mu_1}{2\\eta^2\\ln 2}.\\ ] ] theorem  [ thm : chat_compound ] thus shows that there is no further loss beyond this resulting from the lack of precise knowledge of the insertion / deletion statistics at the transmitter and the receiver .",
    "the proof of theorem  [ thm : chat_compound ] is presented in section  [ sec : propose_compound ] .    finally , consider an insertion / deletion channel @xmath31 concatenated with a general discrete memoryless channel @xmath4 specified by its transition probability matrix @xmath69 .",
    "furthermore , consider an arbitrary cost function @xmath36 .",
    "as mentioned in section  [ sec : prob ] , we assume that @xmath41 and @xmath42 . then , the following bounds hold on the capacity per unit cost .",
    "[ thm : chat_dmc ] the insertion / deletion / substitution channel consisting of an @xmath31 concatenated with a @xmath4 has capacity per unit cost @xmath70 satisfying @xmath71 where @xmath72 is the kullback - leibler divergence between distributions @xmath48 and @xmath73 and where @xmath41 is an input symbol with zero cost .",
    "theorem  [ thm : chat_dmc ] approximates the capacity per unit cost of a general dmc with synchronization errors and general cost function to within a factor two .",
    "in contrast , recall from section  [ sec : intro ] that for the _ capacity _ , even of the noiseless deletion channel , only loose bounds are known despite over four decades since the introduction of the model in  @xcite .",
    "it was shown in  @xcite that the capacity per unit cost @xmath74 of a @xmath4 is @xmath75 thus , from the lower and upper bounds in theorem  [ thm : chat_dmc ] , we obtain the following corollary , showing that the loss due to synchronization errors is within a factor between @xmath76 and @xmath25 .",
    "[ cor : chat_dmc ] the insertion / deletion / substitution channel consisting of an @xmath31 concatenated with a @xmath4 has capacity per unit cost @xmath70 satisfying @xmath77    the achievability in theorem  [ thm : chat_dmc ] is established by generalizing the proposed scheme for the gaussian channel with synchronization errors to dmcs .",
    "the details are discussed in section  [ sec : propose ] . for the upper bound on the capacity per unit cost in theorem  [ thm : chat_dmc ]",
    ", we treat the effect of the idc as a specific way of encoding for the dmc with an appropriately modified cost function .",
    "the upper bound is then obtained by utilizing the characterization of the capacity per unit cost for _ memoryless _ channels in @xcite .",
    "section  [ sec : converse ] provides the details .",
    "we conclude this section by demonstrating through an example that the conventional schemes based on tight synchronization between the transmitter and the receiver can be highly suboptimal in terms of their rate per unit cost .",
    "let us consider the simplest synchronized communication setting : a channel with binary input and no noise , i.e. , @xmath78 for @xmath79 .",
    "further , let the cost function be the number of ones transmitted , i.e. , @xmath80 .",
    "this is a dmc , and by , its capacity per unit cost is @xmath81    let us now consider the scenario where the transmitter and the receiver are no longer perfectly synchronized .",
    "specifically , the input signals are first corrupted by a deletion channel with deletion probability @xmath82 ( see example  [ eg : deletion ] in section  [ sec : prob ] for a formal definition of this special case of an idc ) , before being sent over the aforementioned noiseless channel @xmath83 .",
    "consider first the operation of conventional schemes based on tight synchronization . in this example",
    ", we take this to mean any scheme that detects and corrects deletions without letting them accumulate .",
    "this definition applies to schemes using pilots as well as to schemes using differential modulation . to maintain tight synchronization",
    ", the channel inputs can not contain too many consecutive zeros ( since otherwise deletions would accumulate without any way of correcting for them ) .",
    "on average , we expect to see about one deleted bit every @xmath84 transmitted bits .",
    "thus , roughly every @xmath84 channel inputs needs to be a @xmath27 at a cost of @xmath85 .",
    "now , over a block of @xmath84 bits , we can reliably transmit at most @xmath84 bits .",
    "hence , the rate per unit cost achieved by any scheme based on tight synchronization is at most @xmath86    on the other hand , from corollary  [ cor : chat_dmc ] , the communication scheme proposed in this paper achieves a rate per unit cost that is within a factor of @xmath87 of the capacity per unit cost @xmath74 of the underlying @xmath4 .",
    "hence , the capacity per unit cost with synchronization errors is @xmath88 thus , even in the presence of synchronization errors , the rate per unit cost achieved by the proposed scheme is arbitrarily large .",
    "this illustrates that the improvement in the rate per unit cost achieved by the proposed scheme over schemes based on tight synchronization can be unbounded .",
    "in this section , we propose a coding scheme that achieves the lower bound on the rate per unit cost stated in theorem  [ thm : chat_dmc ] for a general dmc with synchronization errors .",
    "the scheme uses a type of pulse - position modulation . to send message @xmath89",
    ", the encoder sends a burst of symbols @xmath90 at a position corresponding to this message .",
    "the decoder searches for the location of the pulse using a sliding window .",
    "once the pulse is located , the decoder checks which decision region it is in and declares the corresponding message . in order to deal with insertions and deletions ,",
    "guard spaces need to be introduced around the pulses and the decision regions need to be chosen judiciously .",
    "we proceed with a detailed description of the scheme and its analysis .",
    "_ encoding : _ fix a target error probability @xmath91 and a number @xmath92 .",
    "let @xmath90 be a fixed nonzero channel input .",
    "the codeword for message @xmath93 is @xmath94 where @xmath95 thus , to communicate message @xmath89 , the transmitter sends a pulse of symbols @xmath96 at position @xmath97 and of duration @xmath98 .",
    "observe that between adjacent possible pulse positions is a guard space of @xmath99 zeroes .",
    "the block length of this code is @xmath100 and the cost of each codeword is @xmath101    _ decoding : _ recall that the output @xmath102 of the channel has length @xmath7 .",
    "the decoder forms the subsequences @xmath103 , \\msf{y}[\\ell+1 ] , \\ldots , \\msf{y}[\\ell+\\floor{b\\mu-\\beta}-1 ] \\bigr)\\ ] ] of length @xmath104 for @xmath105 with @xmath106 similarly , define the subsequences @xmath107 , \\tilde{\\msf{x}}[\\ell+1 ] ,       \\ldots , \\tilde{\\msf{x}}[\\ell+\\floor{b\\mu-\\beta}-1 ] \\bigr)\\ ] ] of the output of the idc / input to the dmc ( not observable at the receiver ) .",
    "finally , define the decision regions @xmath108 with @xmath109 in words , the decision region @xmath110 for message @xmath89 consists of all integer points within distance @xmath111 of @xmath112 .",
    "the receiver performs independent hypothesis tests for each @xmath113 for the two hypotheses @xmath114 let @xmath115 be the decision of the hypothesis test for @xmath113 .",
    "the receiver declares that message @xmath89 was sent if @xmath116 for _ some _ @xmath117 and @xmath118 for _ all _ @xmath119 with @xmath120 .",
    "if no such @xmath89 exists , an error is declared .    in order for the decoder to be well defined ,",
    "we need to ensure that the decision regions are disjoint , i.e. , that @xmath121 for @xmath122 .",
    "this is the case since , by the definitions of @xmath123 and @xmath111 , @xmath124 so that @xmath125 for all @xmath89 .    _",
    "error analysis : _ assume that message @xmath89 was sent . let @xmath126 be the event that @xmath118 , and @xmath127 be the event that @xmath116 . define the missed - detection event @xmath128 the probability of decoding error for message @xmath89 is equal to @xmath129 , where @xmath130 denotes probability conditioned on message @xmath89 being sent .",
    "we continue by upper bounding this probability .",
    "it will be convenient to define two auxiliary events isolating the behavior of the idc .",
    "let @xmath131 be the event that the total number of symbols in @xmath132 resulting from the first @xmath133 transmitted symbols is outside the interval @xmath134 , and let @xmath135 be the event that the number of symbols in @xmath132 resulting from symbols transmitted during time slots @xmath136 to @xmath137 is outside the interval @xmath138 .",
    "we have @xmath139 the first two probabilities correspond to the events that the @xmath31 is not well behaved and the last two correspond to the two possible detection errors caused by the @xmath4 conditioned on the behavior of the idc to be as expected .",
    "we continue by upper bounding each of the terms in in turn . by chebyshev s inequality , @xmath140 - ( m-1)n\\mu \\big\\rvert      \\geq \\nu \\bigr ) \\nonumber \\nonumber\\\\      & \\leq \\frac{(m-1)n\\sigma^2}{\\nu^2 } \\nonumber \\\\      & \\leq \\varepsilon/4 , \\\\      \\intertext{and }      \\label{eq : e2 }",
    "\\pp_m(\\mc{e}_4 )       & = \\pp_m\\bigl ( \\big\\lvert { \\textstyle\\sum_{t=(m-1)n+1}^{(m-1)n+b } } s[t ] - b\\mu \\big\\rvert      \\geq \\beta \\bigr ) \\nonumber \\\\      & \\leq \\frac{b\\sigma^2}{\\beta^2 } \\nonumber \\\\      & = \\varepsilon/4,\\end{aligned}\\ ] ] where we have used the definitions of @xmath111 and @xmath141 , respectively",
    ".    we proceed with the analysis of @xmath142 and @xmath143 .",
    "the following is the key observation for this analysis .",
    "conditioned on message @xmath89 being sent and on @xmath144 , the elements in the decision regions satisfy the following two properties for @xmath45 large enough ( not depending on @xmath89 ) :    1 .   for every @xmath119 with @xmath120 , we have @xmath145 .",
    "hence , the symbols in the subsequence @xmath113 are with distribution @xmath146 .",
    "2 .   there exists at least one @xmath117 such that @xmath147 .",
    "hence , the symbols in the subsequence @xmath113 are with distribution @xmath148 .",
    "we start by proving property  1 . by construction of the codewords , and since the idc part of the channel can only delete and duplicate symbols but never `` create '' them , we only need to argue that the burst of symbol @xmath96 sent in block @xmath89 by the transmitter can not be shifted into the decision region @xmath149 .",
    "assume first @xmath150 .",
    "the right - most element of @xmath149 is at position less than or equal to @xmath151 , and therefore the right - most element of @xmath152 with @xmath119 is at position at most @xmath153 now , conditioned on @xmath154 , there are at least @xmath155 symbols @xmath40 in @xmath132 before the first symbol @xmath96 .",
    "for there to be no overlap , it is sufficient to argue that @xmath156 or , equivalently , that @xmath157 this holds for @xmath45 large enough since we have @xmath158 by  , and since @xmath159 whereas @xmath160",
    ".    assume then that @xmath161 .",
    "the left - most element of any @xmath152 with @xmath119 is at position at least @xmath162 .",
    "conditioned on @xmath154 , there are at most @xmath163 symbols @xmath40 before the first symbol @xmath96 in @xmath132 . conditioned on @xmath164 ,",
    "the burst of symbol @xmath96 in @xmath132 is of length at most @xmath165 . for there to be no overlap ,",
    "it is sufficient to argue that @xmath166 or , equivalently , that @xmath167 this holds for @xmath45 large enough by the same argument as in the last paragraph since @xmath168 .",
    "together , this proves property  1 .    to prove property  2 , observe that , conditioned on @xmath164 , the burst of symbols @xmath96 in @xmath132 is of length at least @xmath169 .",
    "further , conditioned on @xmath154 , this burst must start at the receiver in the interval @xmath170 since the subsequences @xmath152 have length @xmath171 , these two statements show that there exists at least one @xmath117 such that @xmath172 .",
    "the two properties allow us to analyze the events @xmath126 and @xmath127 .",
    "recall that the hypothesis test on @xmath113 is performed under the assumption that either @xmath145 or @xmath172 .",
    "fix a threshold for the hypothesis test of @xmath113 such that the probability of missed detection satisfies @xmath173 by stein s lemma ( see , e.g. , ( * ? ? ?",
    "* theorem 12.8.1 ) ) , we then have that the probability of false alarm of the optimal test is upper bounded by @xmath174 as @xmath175 , and where we have used that @xmath176 and @xmath168 .",
    "consider then the value of @xmath117 guaranteed by property  2 .",
    "for this @xmath177 , we have by , @xmath178 by property  1 and , and using that @xmath179 , @xmath180 now , @xmath181 as @xmath175 .",
    "hence , using the definition of @xmath98 , @xmath182 for @xmath45 large enough .",
    "substituting , , , and into shows that for @xmath45 large enough the probability of decoding error is upper bounded by @xmath49 for every message @xmath89 . by ,",
    "the achievable rate per unit cost of this scheme is @xmath183 since @xmath184 can be made arbitrarily small , this shows that @xmath185 taking the supremum over all @xmath186 completes the proof of the lower bound in theorem  [ thm : chat_dmc ] .    in the presence of perfect synchronization ,",
    "the decoder knows exactly where the possible pulses are located , and thus needs to check only @xmath45 possible pulse positions .",
    "however , in the proposed scheme , we need to check @xmath187 possible pulse positions due to the lack of synchronization .",
    "it is this increase from @xmath45 to @xmath187 hypothesis tests that results in the reduction of rate per unit cost by a factor @xmath53 compared to the synchronized case .",
    "in this section , we provide an upper bound on the capacity per unit cost of the insertion / deletion / substitution channel . since the idc part of the channel is not memoryless , standard converse techniques are not applicable in this setting .",
    "instead , we use a simulation argument , namely that the insertion / deletion / substitution channel can be simulated with an encoder and decoder communicating over a discrete memoryless channel .",
    "this dmc , in turn , can be analyzed and yields the desired upper bound for capacity per unit cost of the insertion / deletion / substitution channel .",
    "we now provide the details of this argument .",
    "let @xmath188 and @xmath189 be an encoder - decoder pair achieving rate per unit cost @xmath190 and average probability of error at most @xmath49 for the insertion / deletion / substitution channel .",
    "note that , since the output of the channel is of random length @xmath7 , the decoder consists of several sub - decoders @xmath191 , one for each possible realization @xmath177 of @xmath7 .     by modifying the encoder and the cost function ( bottom figure ) . ]",
    "we want to argue that the statistical behavior between the input @xmath89 to the encoder @xmath188 and the output @xmath192 of the decoder @xmath193 can be simulated over the discrete memoryless channel @xmath83 alone ( see fig .",
    "[ fig : converse ] ) . consider a new encoder @xmath194 that consists of the concatenation of @xmath188 with an idc of same statistical behavior as the one in the original insertion / deletion / substitution channel .",
    "denote by @xmath195 $ ] the state random variables describing this idc .",
    "observe that the encoder @xmath194 is a randomized , variable - length encoder , mapping the message @xmath89 into a random sequence @xmath196 of random length @xmath197 .    the output @xmath196 of the encoder @xmath194 is transmitted over a dmc with the same transition probability matrix @xmath83 as in the original insertion / deletion / substitution channel .",
    "let @xmath198 be the output of this dmc .",
    "the decoder @xmath199 for the dmc is equal to @xmath200 .",
    "observe that this is a variable - length decoder , and denote by @xmath201 its output .    by construction , for the same message @xmath89 , the distributions of @xmath192 and @xmath201 are identical .",
    "in particular , the average probability of error of both systems is the same .",
    "hence , the average probability of error of @xmath194 and @xmath202 over @xmath83 is at most @xmath49 .",
    "define the new cost function @xmath203 for the simulating dmc .",
    "with respect to this cost function , and assuming a uniformly distributed message @xmath204 , the expected cost of using the variable - length encoder @xmath194 over @xmath83 is @xmath205 ) \\biggr )   \\\\      & = \\frac{1}{\\mu}\\e\\biggl ( \\sum_{t=1}^{t}\\msf{s}'[t]c(\\msf{x}'[t ] ) \\biggr)\\\\      & \\stackrel{(b)}{= } \\frac{1}{\\mu } \\e(\\msf{s}'[1])\\sum_{t=1}^{t}\\e c(\\msf{x}'[t ] ) \\\\      & \\stackrel{(c)}{=}\\frac{1}{\\mu } \\e(\\msf{s}[1 ] ) \\sum_{t=1}^{t } \\e c(\\msf{x}[t ] ) \\\\      & = \\e c(\\msf{x}^t).\\end{aligned}\\ ] ] here , ( a ) follows from the definition of @xmath206 ; ( b ) follows from the fact that the insertion / deletion process @xmath207\\}$ ] is identically distributed and independent of the channel inputs ; and ( c ) follows since @xmath208 $ ] and @xmath209 $ ] have the same distribution , and since @xmath210 $ ] and @xmath211 $ ] have the same distribution .",
    "hence , encoder @xmath194 used over the @xmath4 has the same expected cost with respect to the cost function @xmath206 as the encoder @xmath188 used over the insertion / deletion / substitution channel with respect to the cost function @xmath212 .",
    "observe that , while the two encoders have the same _ expected _ cost , the encoder @xmath188 satisfies the stronger _ per - codeword _ cost constraint , whereas the encoder @xmath194 does not .",
    "the arguments in the last two paragraphs show that there exists a randomized variable - length encoder @xmath194 and variable - length decoder @xmath202 achieving a rate per average unit cost of @xmath213 and average probability of error at most @xmath49 .",
    "since this is just one possible coding scheme , as @xmath175 , @xmath214 must be upper bounded by @xmath215 , the capacity per unit cost of the @xmath4 subject to expected cost constraint , and allowing randomized variable - length codes . thus",
    ", letting @xmath216 , @xmath217    it remains to analyze @xmath215 . by  (",
    "* ? ? ? * exercise  6.28 ) , we have @xmath218 furthermore , by ( * ? ?",
    "? * theorems  2 and 3 ) , @xmath219 so that @xmath220 combining and shows that @xmath221 completing the proof .",
    "the upper bound in theorem  [ thm : chat_gauss ] follows from the upper bound for dmcs . indeed , by corollary  [ cor : chat_dmc ] , @xmath222 yielding the desired upper bound .",
    "for the lower bound , we adapt the achievable scheme for the dmc described in section  [ sec : propose ] to the gaussian case . for simplicity",
    ", we consider the case of noise power @xmath223 and point out in the end how to extend the result for arbitrary values of @xmath1 .    _",
    "encoding : _ fix a target error probability @xmath91 and a number @xmath92 .",
    "let @xmath224 be a nonzero channel input .",
    "unlike the dmc case , we will choose @xmath225 as a function of @xmath45 here .",
    "the codeword for message @xmath93 is again given by @xmath226 with the same @xmath227 as before .",
    "however , here we choose the burst length @xmath98 as @xmath228 as opposed to @xmath229 in the dmc case . it can be verified that @xmath230 , and hence the codewords are well defined .",
    "the block length of this code is @xmath100 and the cost of each codeword is @xmath231    _ decoding : _ consider again the subsequences @xmath232 , \\msf{y}[\\ell+1 ] ,       \\ldots , \\msf{y}[\\ell+\\floor{b\\mu-\\beta}-1 ] \\bigr ) \\\\",
    "\\shortintertext{and}\\\\      \\tilde{\\bm{\\msf{x}}}_\\ell      & \\defeq \\bigl ( \\tilde{\\msf{x}}[\\ell ] , \\tilde{\\msf{x}}[\\ell+1 ] ,       \\ldots , \\tilde{\\msf{x}}[\\ell+\\floor{b\\mu-\\beta}-1 ] \\bigr)\\end{aligned}\\ ] ] of length @xmath104 for @xmath105 with @xmath233 define the decision regions @xmath234 with @xmath235 in words , the decision regions consist of @xmath236 ( for @xmath237 ) or of all multiples of @xmath238 between @xmath239 and @xmath240 ( for @xmath241 ) .",
    "this differs from the dmc case , where the boundaries of the decision regions are the same , but there the regions contain every _ integer _ between them . using the same arguments as in the dmc case",
    "shows that these decision regions are disjoint .",
    "the receiver independently performs the hypothesis test @xmath242 for each @xmath117 , @xmath243 , and where @xmath244 denotes the inner product .",
    "let @xmath115 be the decision of the hypothesis test for @xmath113 .",
    "as in the dmc case , the receiver declares that message @xmath89 was sent if @xmath116 for _ some _ @xmath117 and @xmath118 for _ all _ @xmath119 with @xmath120 .",
    "if no such @xmath89 exists , an error is declared .",
    "_ error analysis : _ assume that message @xmath89 was sent .",
    "we define the same events as in the dmc case .",
    "let @xmath126 be the event that @xmath118 , and @xmath127 be the event that @xmath116 .",
    "define the missed - detection event @xmath128 the probability of decoding error for message @xmath89 is then equal to @xmath129 , where again @xmath130 denotes probability conditioned on message @xmath89 being sent .",
    "we again define the two auxiliary events describing the behavior of the idc .",
    "let @xmath131 be the event that the total number of symbols in @xmath132 resulting from the first @xmath133 transmitted symbols is outside @xmath134 , and let @xmath135 be the event that the number of symbols in @xmath132 resulting from symbols transmitted during time slots @xmath136 to @xmath137 is outside @xmath138 . using the same argument as for the dmc case , we can upper bound @xmath245    using chebyshev s inequality as in the analysis of the dmc case , we obtain @xmath246    we proceed with the analysis of @xmath142 and @xmath143 . conditioned on message @xmath89 being sent and on @xmath144 , the elements in the decision regions satisfy the following two properties for @xmath45 large enough ( not depending on @xmath89 ) :    1 .   for every @xmath119 with @xmath120 , we have @xmath247 . hence , @xmath248 is gaussian with mean zero and variance one .",
    "2 .   there exists at least one @xmath117 such that @xmath249 hence , @xmath248 is gaussian with mean at least @xmath250 and variance one .",
    "the first property follows by the same arguments as for the dmc case , using that @xmath251 and @xmath252 as @xmath175 . for the second property ,",
    "note that by the arguments for the dmc case , there exists at least one @xmath253 for @xmath237 or @xmath254 for @xmath241 such that @xmath255 .",
    "however , this value of @xmath256 may not be a multiple of @xmath238 , and hence may not be an element of @xmath110 .",
    "let @xmath177 be the closest multiple of @xmath238 to @xmath256 that is in @xmath110 ; such a @xmath177 exists for @xmath45 large enough since @xmath159 .",
    "since @xmath257 , this implies that @xmath258 as required .",
    "the two properties allow us to analyze the events @xmath259 and @xmath260 . by property  1 , and using that @xmath179 , @xmath261 using the chernoff bound @xmath262 for the @xmath73-function , we have @xmath263 moreover , since @xmath159 , @xmath264 as @xmath175 .",
    "hence , @xmath265 for @xmath45 large enough .",
    "consider then the value of @xmath117 guaranteed by property  2 .",
    "for this @xmath177 , @xmath266 recall that @xmath267 and @xmath268 , so that @xmath269 as @xmath175 . by choosing @xmath270 we obtain @xmath271 for @xmath45 large enough",
    ".    substituting , , , and into shows that for @xmath45 large enough the probability of decoding error is upper bounded by @xmath49 for every message @xmath89 . by and ,",
    "the power required by this scheme is @xmath272 hence , the achievable rate per unit cost for this scheme is @xmath273 since @xmath184 can be made arbitrarily small , this shows that , for noise power @xmath223 , @xmath274    assume then that @xmath275 . by scaling the channel input at the transmitter by a factor @xmath276 and the channel output at the receiver by a factor @xmath277",
    ", we can transform the channel to one with unit noise power . since this increases the energy of the transmitted symbols by a factor @xmath1 , but does not change the probability of error , this shows that @xmath278 concluding the proof .",
    "this section adapts the coding scheme for the gaussian insertion / deletion / substitution channel with known value of @xmath25 described in section  [ sec : propose_gauss ] to the compound setting with @xmath25 only known to be in the range @xmath279 $ ] . as before",
    ", we will first assume that the noise power is @xmath223 and then generalize the result for arbitrary values of @xmath1 .     and @xmath280 .",
    "for simplicity , @xmath281 is set to @xmath40 .",
    "@xmath282 is chosen such that @xmath283 , ensuring that pulses corresponding to different messages ( indicated by the dotted lines ) are nonoverlapping at the receiver for all values of @xmath60 $ ] and under expected behavior of the idc . ]",
    "_ encoding : _ fix a target error probability @xmath91 and a number @xmath284 .",
    "let @xmath224 be a nonzero channel input .",
    "the codeword for message @xmath93 is @xmath285 observe that here , unlike the case with known @xmath25 , the value of the nonzero channel input is @xmath286 , which depends on the message @xmath89 .",
    "this construction is illustrated in fig .",
    "[ fig : compound ] .",
    "the block length of this code is @xmath287 , and the cost of codeword @xmath89 is @xmath288    _ decoding : _ define the decision regions @xmath289 note that , unlike the case with known value of @xmath25 , the decision regions here are increasing as a function of @xmath89 .",
    "however , each decision region contains approximately the same number @xmath290 of points .",
    "it is easy to verify that the decision regions are disjoint .    for @xmath117 ,",
    "define the subsequences @xmath232 , \\msf{y}[\\ell+1 ] , \\ldots ,       \\msf{y}[\\ell+\\floor{(\\mu_1-\\delta)b_m}-1 ] \\bigr ) \\\\      \\shortintertext{and}\\\\      \\tilde{\\bm{\\msf{x}}}_\\ell      & \\defeq \\bigl ( \\tilde{\\msf{x}}[\\ell ] , \\tilde{\\msf{x}}[\\ell+1 ] ,       \\ldots , \\tilde{\\msf{x}}[\\ell+\\floor{(\\mu_1-\\delta)b_m}-1 ] \\bigr)\\end{aligned}\\ ] ] of length @xmath291 .",
    "we point out that here , unlike the case with known value of @xmath25 , the subsequences in different regions @xmath292 and @xmath149 have different lengths .",
    "the receiver independently performs the hypothesis test @xmath293 for each @xmath117 , @xmath243 , and where @xmath244 denotes the inner product .",
    "let @xmath115 be the decision of the hypothesis test for @xmath113 .",
    "as in the case of known @xmath25 , the receiver declares that message @xmath89 was sent if @xmath116 for _ some _ @xmath117 and @xmath118 for _ all _ @xmath119 with @xmath120 .",
    "if no such @xmath89 exists , an error is declared .",
    "_ error analysis : _ assume that message @xmath89 was sent .",
    "we define the same error events as in the case of known @xmath25 .",
    "let @xmath126 be the event that @xmath118 , and @xmath127 be the event that @xmath116 .",
    "define the missed - detection event @xmath294 the probability of decoding error for message @xmath89 is then equal to @xmath129 , where as before @xmath130 denotes probability conditioned on message @xmath89 being sent .",
    "we again define two auxiliary events describing the behavior of the idc .",
    "let @xmath131 be the event that the total number of symbols in @xmath132 resulting from the first @xmath295 transmitted symbols is outside @xmath296 , and let @xmath135 be the event that the number of symbols in @xmath132 resulting from symbols transmitted during time slots @xmath297 to @xmath298 is outside @xmath299 . we can again upper bound the probability of error as @xmath300    we start with the analysis of @xmath301 and @xmath302 . using chebyshev s inequality together with the upper bound @xmath26 on the variance of the states",
    "@xmath209 $ ] , we obtain similarly to the case with known value of @xmath25 @xmath303 and @xmath304 for @xmath45 large enough and for any value of @xmath60 $ ] .",
    "we proceed with the analysis of @xmath142 and @xmath143 . conditioned on message",
    "@xmath89 being sent and on @xmath144 , the elements in the decision regions satisfy the following two properties for @xmath45 large enough ( not depending on @xmath89 ) :    1 .   for every @xmath119 with @xmath120",
    ", we have @xmath247 .",
    "hence , @xmath305 is gaussian with mean zero and variance one .",
    "2 .   there exists at least one @xmath117 such that @xmath306 hence , @xmath305 is gaussian with mean at least @xmath307 and variance one .",
    "property  2 can be proved using arguments analogous to the case with known value of @xmath25 . for property  1 , we need to argue that the burst of symbols @xmath96 can not be shifted into the incorrect decoding region .",
    "assume first @xmath150 .",
    "the right - most element of @xmath149 is at position less than or equal to @xmath308 , and thus the right - most element of @xmath152 with @xmath119 is at position at most @xmath309 now , conditioned on @xmath154 , there are at least @xmath310 symbols @xmath40 in @xmath132 before the first symbol @xmath96 . for there to be no overlap , it is sufficient to argue that @xmath311 or , equivalently , that @xmath312 this holds by the definition of @xmath295 .    assume then that @xmath161 .",
    "the left - most element of any @xmath152 with @xmath119 is at position at least @xmath313 .",
    "conditioned on @xmath154 , there are at most @xmath314 symbols @xmath40 before the first symbol @xmath96 in @xmath132 .",
    "conditioned on @xmath164 , the burst of symbol @xmath96 in @xmath132 is of length at most @xmath315 . for there to be no overlap , it is sufficient that @xmath316 or , equivalently , that @xmath317 this holds again by the definition of @xmath318 .",
    "the two properties allow us to analyze the events @xmath259 and @xmath260 . by property  1 , @xmath319 using the chernoff bound @xmath262 for the @xmath73-function , @xmath263 moreover , @xmath320 so that @xmath321 as @xmath175 .",
    "hence , @xmath322 for @xmath45 large enough .",
    "consider then the value of @xmath117 guaranteed by property  2 .",
    "for this @xmath177 , @xmath323 note that @xmath324 for @xmath237 , and @xmath325 as @xmath175 for @xmath241 .",
    "furthermore , @xmath326 as @xmath175 .",
    "substituting , , , and into shows that for @xmath45 large enough the probability of decoding error is upper bounded by @xmath49 for every message @xmath89 . by and , the power required by this scheme is @xmath329 hence , the achievable rate per unit cost for this scheme is @xmath330 since @xmath184 can be made arbitrarily small , this shows that , for noise power @xmath223 , @xmath331 by scaling the input and output as in the proof of theorem  [ thm : chat_gauss ] in section  [ sec : propose_gauss ] , this implies that @xmath332 for any value of noise power @xmath1 , concluding the proof ."
  ],
  "abstract_text": [
    "<S> communication systems are traditionally designed to have tight transmitter - receiver synchronization . </S>",
    "<S> this requirement has negligible overhead in the high - snr regime . </S>",
    "<S> however , in many applications , such as wireless sensor networks , communication needs to happen primarily in the energy - efficient regime of low snr , where requiring tight synchronization can be highly suboptimal .    in this paper </S>",
    "<S> , we model the noisy channel with synchronization errors as an insertion / deletion / substitution channel . for this channel </S>",
    "<S> , we propose a new communication scheme that requires only loose transmitter - receiver synchronization . </S>",
    "<S> we show that the proposed scheme is asymptotically optimal for the gaussian channel with synchronization errors in terms of energy efficiency as measured by the rate per unit energy . in the process </S>",
    "<S> , we also establish that the lack of synchronization causes negligible loss in energy efficiency . </S>",
    "<S> we further show that , for a general discrete memoryless channel with synchronization errors and a general cost function on the input , the rate per unit cost achieved by the proposed scheme is within a factor two of the information - theoretic optimum . </S>"
  ]
}