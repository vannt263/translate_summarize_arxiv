{
  "article_text": [
    "admm ( alternating direction method of multipliers ) is one of the most popular first order methods for non - smooth optimization and regularization problems widely used in signal processing and imaging @xcite .",
    "admm was first introduced in @xcite , and extensive studies can be found in  @xcite and so on .",
    "it aims at the following general constrained optimization problem , @xmath0 where @xmath1 , @xmath2 , @xmath3 and @xmath4 are real hilbert spaces with @xmath5 denoting the corresponding proper , convex and lower semi - continuous function spaces  @xcite . and @xmath6 , @xmath7 , where @xmath8 denotes the spaces of linear and continuous operators mapping from hilbert spaces @xmath9 to @xmath10 .",
    "with appropriate conditions @xcite , this problem is equivalent to the following saddle - point problem , and assume there exists a bounded saddle - point , @xmath11 where @xmath12 is the lagrangian function and @xmath13 is the lagrangian multiplier .",
    "also the associated fenchel - rockafellar dual problem of reads as follows @xcite , @xmath14 once writing the following augmented lagrangian , @xmath15 what follows is the admm method by solving the sub minimization problems @xmath16 and @xmath17 consecutively in , and updating the lagrangian multipliers at last , @xmath18,\\\\ p^{k+1 }   & = ( r b^*b + \\partial g)^{-1}[b^ * ( -rau^{k+1 } - \\bar{\\lambda}^{k } + rc ) ] ,        \\\\",
    "\\bar{\\lambda}^{k+1 } & = \\bar{\\lambda}^{k } + r(au^{k+1 } + bp^{k+1}-c ) . \\\\            \\end{aligned }      \\right.\\ ] ] here and following , we assume @xmath19 such that @xmath20 and @xmath21 exist and are lipschitz continuous",
    ".    it is known the convergence of method in finite dimensional spaces is clear , see @xcite and @xcite . however , the weak convergence is not quite clear in infinite dimensional hilbert space except @xcite .",
    "it is also well known that one can get admm through applying douglas - rachford splitting method to the dual problems while @xmath22 , @xmath23 @xcite , and there are some weak convergence results of douglas - rachford splitting method in infinite dimensional hilbert spaces , see @xcite and the recent paper @xcite . however , as our analysis in remark [ rem : weakdr : not : weakadmm ] of section [ sec : admm - equavi - dr ] , the weak convergence of douglas - rachford splitting method in @xcite could not lead to weak convergence of the corresponding admm directly .",
    "the reason is that the resolvent operators @xmath24 and @xmath25 of could not preserve the weak convergence , although the weak convergence of multipliers could be guaranteed .",
    "more analysis is needed to obtain the weak convergence of admm .",
    "the weak convergence of preconditioned admm is analysed in @xcite in infinite dimensional hilbert spaces , under the framework of general proximal point iterations with moderate conditions . here",
    "we extend the results to overrelaxed variants by a different approach .",
    "actually , admm could be seen as a special case of the following  fully \" relaxed and preconditioned version of , @xmath26,\\\\ p^{k+1 }   & = ( m + \\partial g)^{-1}[b^ * ( -r \\rho_{k } au^{k+1 } + r(1-\\rho_{k})bp^{k } - \\bar{\\lambda}^{k } + r\\rho_{k}c ) + ( m - rb^*b)p^k ] ,        \\\\",
    "\\bar{\\lambda}^{k+1 } & = \\bar{\\lambda}^{k } + r(\\rho_{k } au^{k+1 } + bp^{k+1}-(1-\\rho_{k})bp^{k}-\\rho_{k}c ) ,            \\end{aligned }      \\right.\\ ] ] with assumption on relaxation parameters @xmath27 throughout this paper , @xmath28 and @xmath29 , @xmath30 are both self - adjoint operators , and satisfy , @xmath31 actually , the updates of @xmath32 and @xmath33 in could be reformulated as follows , @xmath34,\\\\ p^{k+1 }   & = p^{k } + ( m + \\partial g)^{-1}[b^ * ( -r \\rho_{k } au^{k+1 } + r(1-\\rho_{k})bp^{k } - \\bar{\\lambda}^{k } + r\\rho_{k}c ) - ( \\partial g+ rb^*b)p^k ] ,      \\end{aligned }      \\right.\\ ] ] where @xmath35 and @xmath36 could be seen as generalized  preconditioners \" for the operator equations for calculating @xmath32 and @xmath33 in classical . it is shown in @xcite that one could benefit from efficient preconditioners for solving the implicit problems approximately with only one , two , or three cheap preconditioned iterations without controlling the errors , to guarantee the ( weak ) convergence of the iterations without relaxation .",
    "it is not surprising that while @xmath37 ( or @xmath38 ) is a quadratical form @xcite , the update of @xmath39 ( or @xmath40 ) above is just the classical preconditioned iteration in numerical linear algebra in finite dimensional spaces .",
    "and various preconditioning techniques are efficient and are widely used . even for nonlinear @xmath37 or @xmath38",
    ", there are still some preconditioning techniques available , see @xcite for diagonal preconditioning , and @xcite for nonlinear symmetric gauss - seidel preconditioning .    actually , is equivalent to the following iterations involving the augmented lagrangian , @xmath41 here @xmath42 and",
    "@xmath43 are weighted norm , i.e. , @xmath44 , and they are called as proximal terms as in @xcite .",
    "could also be recovered from by setting @xmath45 , @xmath46 and @xmath47 .    motivated by the  partial \" primal - dual gap analysis in @xcite and @xcite",
    ", we prove the weak convergence of the fully preconditioned and relaxed in infinite dimensional hilbert spaces , together with the corresponding ergodic convergence rate , under only conditions , and , by employing the detailed analysis of a kind of  partial \" primal - dual gap .",
    "similar but different partial gap function could be found in @xcite .",
    "the main contributions belong to the following parts .",
    "first , we prove weak convergence of original and its relaxed version as in @xcite in infinite dimensional hilbert space with conditions , and , through a different approach compared to @xcite . and",
    "no additional condition is needed as in @xcite where the proximal terms must both be positive definite , i.e. , @xmath48 and @xmath49 .",
    "this kind of relaxation is different from @xcite that only focused on relaxation for the updates of lagrangian multipliers .",
    "second , we proposed a relaxed admm with different relaxation parameters for separable components of the @xmath17 and @xmath13 , which could help us to get a kind of relaxed and preconditioned admm easily , by applying the weak convergence of relaxed to a specially designed modified constrained optimization problems , when only the update of @xmath32 is preconditioned , i.e. , @xmath50 .",
    "the case the added proximal term for update of @xmath32 in is positive definite , is discussed in @xcite , which is mainly focused on linearization strategy in finite dimensional spaces , i.e. , @xmath51 and @xmath50 .",
    "however the positive definiteness of @xmath52 or @xmath53 could prevent lots of interesting applications including the efficient symmetric ( block ) red - black gauss - seidel preconditioner for tv ( or tgv ) denosing problems , where @xmath52 is not positive definite , see @xcite .",
    "similar linearized admm with overrelaxation is also considered in @xcite while @xmath54 , where estimating the largest eigenvalue of @xmath55 or @xmath56 is necessary , which would be very expensive while @xmath55 is large and is not well - conditioned . besides , by applying douglas - rachford splitting method to the dual problems , while @xmath50 in , we found that the primal - dual gap stopping criterion in imaging @xcite could be used for admm , while applying to basic rof denoising model .",
    "third , we prove the weak convergence of  fully \" preconditioned admm with relaxation and give the corresponding ergodic convergence rate . and to the best knowledge of the author , the weak convergence and ergodic convergence rate of seem to be figured clear for the first time both in finite dimensional spaces and infinite dimensional hilbert spaces .",
    "we need to point out that the convergence of without relaxation , i.e. , @xmath45 , in finite dimensional spaces could be found in @xcite , and @xcite while @xmath22 and @xmath23 .",
    "the paper is organized as follows , we first consider the weak convergence and ergodic convergence rate of the fundamental in section [ sec : esti : admm ] by employing the  partial \" gap analysis .",
    "then we consider the weak convergence and the corresponding ergodic convergence rate for relaxation variant of , by similar but adjusted  partial \" gap analysis in section [ sec : relax : admm ] . in section [ sec : pre : admm : relax ] , by applying the results for relaxed admm in section [ sec : relax : admm ] to a modified constrained problem , we get a kind of preconditioned admm and its relaxation formally , with only preconditioning for the update of @xmath32 .",
    "furthermore , we present the relation to douglas - rachford splitting method , and show the limited equivalent weak convergence results for admm benefited from the existing weak convergence results of douglas - rachford method as in @xcite . in section [ sec : full : pre : admm ] , we discuss the weak convergence and the corresponding ergodic convergence rate of , with the help of adjusted  partial \" gap analysis and the analysis in the previous sections . in the last section ,",
    "some numerical tests are conducted to demonstrate efficiency of the proposed schemes in this paper .",
    "before the discussion of admm and its various variants . let s define the  partial \" primal - dual gap respecting to a point @xmath57 first , with notation @xmath58 and @xmath59 , @xmath60 .",
    "\\end{aligned}\\ ] ] here and following , we always assume that @xmath61 , such that @xmath62 and @xmath63 are finite , with @xmath64 ( or @xmath65 ) defined as @xmath66 ( or @xmath67 ) .",
    "[ thm : gap : estimate ] assuming @xmath68 , considering iteration , regarding to @xmath69 , we have the following partial primal dual gap estimate for @xmath70 , @xmath71    by the definition of subgradient , we have @xmath72 by the updates of @xmath32 and @xmath33 in , we have @xmath73 and what follows is @xmath74 and @xmath75 thus by the definition of , we have @xmath76 by the update of @xmath77 in , we see @xmath78 and with the help of @xmath79 , we have @xmath80 and what follows is , @xmath81 substituting and into , we get the estimate .",
    "[ lem : admm - fixed - points ] iteration   possesses the following properties :    1 .",
    "a point @xmath82 is a fixed point if and only if @xmath83 is a saddle point for  .",
    "if @xmath84 and @xmath85 , then @xmath86 converges weakly to a fixed point . here and following , denote @xmath87 as the weak convergence .    regarding to the first point",
    ", @xmath88 is a saddle point for   is equivalent to @xmath89 a point @xmath82 is a fixed point of iteration , if only if    @xmath90    and what follows is @xmath91 thus - are also equivalent to @xmath92 which means that @xmath83 is a saddle - point of .",
    "next , by the updates of @xmath32 and @xmath33 in , we see that @xmath93 and by the update of @xmath77 in and the second assumption , we have @xmath94    combine the weak convergence of @xmath95 as in assumption , and the maximally monotone of @xmath96 which is also weak - strong closed .",
    "consequently , @xmath97 , so @xmath88 is a saddle - point of  .",
    "[ thm : dr - weak - convergence ] with assumption , if   possesses a solution , then for  , the iteration sequences @xmath98 converges weakly to a saddle - point @xmath88 of and @xmath99 is a solution of  .",
    "define the solution set of @xmath100 as @xmath101 for any @xmath102 , by the definition of saddle - point of , @xmath103 so by   with the help of polarization identity @xmath104\\ ] ] together with , i.e. , @xmath105 for any @xmath102 , we have @xmath106 \\\\ & \\quad + \\frac{1}{2r}[\\|\\bar \\lambda^{k } - \\bar",
    "\\lambda'\\|^2 - \\|\\bar \\lambda^{k+1}-\\bar \\lambda'\\|^2 - \\|\\bar \\lambda^{k } - \\bar \\lambda^{k+1}\\|^2 ] .",
    "\\end{aligned}\\ ] ]    sum inequality from @xmath107 to @xmath108 , @xmath109 , we have @xmath110 \\\\      \\label{eq : dr_boundedness } & + \\sum_{k ' = k_0}^{k-1 }    \\langle p^{k ' } - p^{k'+1 } , -b^*(\\bar \\lambda^{k ' } -\\bar \\lambda^{k'+1})\\rangle    \\leq      \\frac{r{\\|{b(p^{k_0 } - p ' ) } \\|_{}}^2}{2 }      + \\frac{{\\|{\\bar \\lambda^{k_0 } - \\bar   \\lambda ' } \\|_{}}}{2r}.    \\end{aligned}\\ ] ] denoting @xmath111 , implies that @xmath112 is a non - increasing sequence with limit @xmath113 .",
    "furthermore , @xmath114 as well as @xmath115 as @xmath116 .",
    "now ,   says that @xmath117 are uniformly bounded whenever   has a bounded solution , hence @xmath39 is also uniformly bounded , by the update of @xmath32 of with the help of the lipschitz continuity of @xmath24 .",
    "what follows is the uniform boundedness of @xmath33 by the update of @xmath33 in with the help of the lipschitz continuity of @xmath118 .",
    "hence with the help of banach - alaoglu theorem , there exists a subsequence @xmath95 that weakly converges to a weak accumulation point @xmath119 , by the boundedness of @xmath120 . by virtue of lemma  [ lem : admm - fixed - points ]",
    ", @xmath83 is a fixed - point of the iteration .",
    "suppose that @xmath121 is another accumulation point of the same sequence .",
    "then , @xmath122 as both @xmath123 and @xmath124 are solutions to  , the right - hand side converges as a consequence of  . plugging in the subsequences weakly converging to @xmath125 and @xmath124 , on the left - hand side respectively",
    ", we see that both limits must coincide : @xmath126 hence @xmath127 and @xmath128 for any @xmath129 , @xmath130 .",
    "now we prove there is only one weak limit for all weakly convergent subsequences of @xmath131 . by lemma [ lem : admm - fixed - points ] , @xmath83 and @xmath124",
    "are both fixed points of the iteration .",
    "thus , they both satisfy @xmath132,\\\\ & p '   = ( r b^*b + \\partial g)^{-1}[b^ * ( -rau ' - \\bar{\\lambda}'+rc ) ] ,        \\\\        & au ' + bp'=c .            \\end{aligned }      \\right.\\ ] ] by the lipschitz continuity of @xmath24 and first equation of , we have @xmath133 what follows is @xmath134 .",
    "similarly , by the lipschitz continuity of @xmath25 and second equation of , we have @xmath135 , since @xmath136 which leads to the uniqueness of the weak limits of all weakly convergent subsequences of @xmath131 .",
    "thus we prove the iteration sequence @xmath131 of weakly converge to @xmath83 .",
    "if @xmath137 ( or @xmath138 ) is compact , we can get the weak convergence of @xmath139 to @xmath83 for iteration , and @xmath140 strongly converge to @xmath141 ( or @xmath142 strongly converge to @xmath143 ) , see corollary 3.2 of @xcite .",
    "if @xmath4 are both finite dimensional spaces , we recover the convergence results of proximal or generalized admm of @xcite , @xcite , or @xcite without any kind of relaxation .",
    "[ thm : douglas - rachford - ergodic - rate ] suppose @xmath144 is a saddle - point of , with assumption , then the following ergodic sequences with @xmath131 produced by iterations , @xmath145 converge weakly to @xmath141 , @xmath143 and @xmath146 , respectively , and the  partial \" primal - dual gap respecting to @xmath147 obeys @xmath148 with notation @xmath149 , and @xmath150      = { \\mathcal{o}}(\\frac{1}{k}).\\ ] ]    first of all , by theorem [ thm : dr - weak - convergence ] , @xmath151 converges weakly to a solution @xmath88 of   and @xmath152 converges weakly to a fixed point of . to obtain the weak convergence of @xmath153 , test with an @xmath16 and utilize the stolz  cesro theorem to obtain @xmath154 the property @xmath155 , @xmath156 can be proven analogously .    finally , in order to show the estimate on @xmath157 , observing that the function @xmath158 is convex , together with the definition of saddle - points of , the inequality and estimate , we have @xmath159    if choose @xmath160 that is equivalent to @xmath161 , together with @xmath162 by , we have @xmath163 thus , with initial value @xmath164 and @xmath160 , the estimate is more sharper .",
    "in this section , we will consider the following relaxed admm method , with the same assumptions on @xmath165 as , @xmath166,\\\\ p^{k+1 }   & = ( r b^*b + \\partial g)^{-1}[b^ * ( -r \\rho_{k } au^{k+1 } + r(1-\\rho_{k})bp^{k } - \\bar{\\lambda}^{k } + r\\rho_{k}c ) ] ,        \\\\        \\bar{\\lambda}^{k+1 } & = \\bar{\\lambda}^{k } + r(\\rho_{k } au^{k+1 } + bp^{k+1}-(1-\\rho_{k})bp^{k}-\\rho_{k}c ) .",
    "\\\\            \\end{aligned }      \\right.\\ ] ] we will employ the same idea as theorem [ thm : gap : estimate ] to estimate the  partial \" primal - dual gap , and give a derivation from the iteration directly .",
    "denote @xmath167 by direct calculation , notice that the last update of could be reformulated as follows , @xmath168.\\ ] ] also introduce the following adjacent variables for convenience throughout the paper , for @xmath169 , @xmath170    [ thm : gap : estimate : pre : u ] with @xmath171 , @xmath172 defined in , and assuming @xmath173 , for the primal - dual gap of the relaxed admm , we have , for @xmath70 , while @xmath174 , @xmath175 and while @xmath176 , we have @xmath177    similar to the proof of theorem [ thm : gap : estimate ] , by the update of @xmath32 in , we see @xmath178 and by the update of @xmath77 in , we see @xmath179 and what follows is @xmath180 thus we have @xmath181 and by the update of @xmath33 in , we see @xmath182 again by the update of @xmath77 of , we have @xmath183 and what follows is @xmath184 with the help of , and the assumption @xmath185 , the right hand side of could be rewritten as , @xmath186 and for @xmath187 and @xmath188 , we have @xmath189 @xmath190 by , we see , @xmath191 and together with the assumption @xmath192 , we have @xmath193 substitute these two equalities into i , we have @xmath194 by , we see @xmath195 substitute into ii , we see @xmath196 combine and , we have @xmath197 for last two mixed terms in , notice that @xmath198 thus if @xmath199 , we can replace the last two mixed terms in by linear combination of the first two terms of the right hand side of and the left hand side of , i.e. , @xmath200 , leading to the estimate . similarly , if @xmath201 , by and , get the estimate .    by theorem [ thm : gap",
    ": estimate : pre : u ] and , we have the following gap estimate .    [ cor : relax :",
    "partial ] for iteration , with @xmath171 , @xmath172 defined in , and assuming @xmath173 , for the primal - dual gap of , we have , for @xmath70 , while @xmath174 , @xmath202 while @xmath176 , we have @xmath203    we just need to show the following three inner products , since others are similar . by and",
    "polarization identity , @xmath204 and similarly , @xmath205 also we have @xmath206    and the cases for @xmath207 and @xmath208 are similar .",
    "substitute these formulas to theorem [ thm : gap : estimate : pre : u ] leading to this corollary .",
    "similar to lemma [ lem : admm - fixed - points ] , we also have the following lemma .",
    "[ lem : admm - fixed - points : relax ] iteration   possesses the following properties : if @xmath84 and @xmath85 , then @xmath86 converges weakly to a saddle point for  .",
    "[ thm : gap : estimate : pre : u : weak ] with the assumptions and on relaxation parameters @xmath165 , if possesses a solution , then the iteration sequences @xmath131 of converges weakly to a saddle - point @xmath83 of with @xmath99 being a solution of .",
    "the proof is a little different from the proof of theorem [ thm : dr - weak - convergence ] , because of the changing of @xmath165 . here",
    "we mainly prove the boundedness of the iteration sequence @xmath131 of , and the uniqueness of weak limit for all weakly convergent subsequences of @xmath131 .",
    "these lead to the weak convergence of @xmath131 , and the remaining proof is similar to theorem [ thm : dr - weak - convergence ] with the help of lemma [ lem : admm - fixed - points : relax ] .",
    "note that @xmath209 by , thus @xmath210 consider the case @xmath211 first .",
    "sum inequality from @xmath107 to @xmath108 , @xmath212 , with , non - increasing of @xmath213 by , and notation , @xmath214 we have @xmath215   \\leq       d_{k_0,\\rho_{k_0}}.\\ ] ]    implies that @xmath216 is a non - increasing sequence with limit @xmath113 .",
    "furthermore , @xmath114 as well as @xmath115 as @xmath116 , since @xmath217 , what follows that @xmath117 is bounded whenever   has a solution .",
    "similarly by the lipschitz continuity of @xmath24 , @xmath118 and the updates of @xmath32 , @xmath33 in , we can get the boundedness of @xmath139 of iteration . still by banach - alaoglu theorem",
    ", there exists a weakly convergent subsequence of @xmath131 and denote the weak limit of the subsequence as @xmath119 . by virtue of lemma  [ lem :",
    "admm - fixed - points : relax ] , @xmath83 is a fixed - point of the iteration .",
    "let s prove the uniqueness of the weak limit .",
    "suppose that @xmath124 is another weak limit of another subsequence .",
    "then , @xmath218 take the @xmath219 and @xmath220 , which have weak limits @xmath221 and @xmath121 correspondingly .",
    "we have , @xmath222 what follows @xmath223 and @xmath128 . by completely similar arguments as the proof of theorem [ thm : dr - weak - convergence ]",
    ", the uniqueness of weak limit @xmath83 for all weakly convergent subsequence could be guaranteed . to see this , one just need to replace the fixed point equation by fixed point equation of with @xmath165 replaced by @xmath224 .    for @xmath225 ,",
    "if @xmath226 being a constant , the arguments are completely similar to @xmath227 case with the help of . however , for varying @xmath165 , we need more analysis since @xmath228 and @xmath229 in have different monotonicity . by the definition of @xmath230 in , with @xmath231 being a solution of",
    ", we have @xmath232 .",
    "\\end{aligned}\\ ] ] substitute into , we have for @xmath176 , @xmath233 \\\\         & - \\frac{r(2-\\rho_{k})}{2\\rho_{k}^2}\\|b(p^{k}-p^{k+1})\\|^2-\\frac{(2-\\rho_{k})}{2r\\rho_{k}^2}\\|\\bar\\lambda^{k}-\\bar\\lambda^{k+1}\\|^2         - \\frac{\\rho_{k}-1}{r}\\frac{2-\\rho_{k}}{2\\rho_{k}^2}\\|w^{k}-w^{k+1}\\|^2 \\\\         & - \\frac{2-\\rho_{k}}{\\rho_{k}}\\langle -b(p^{k } - p^{k+1 } ) , \\bar \\lambda^{k } -\\bar \\lambda^{k+1}\\rangle .    \\end{aligned}\\ ] ] denote @xmath234 sum the inequality for @xmath107 to @xmath108 , @xmath235 , with the help of , we have @xmath236   \\\\ & \\leq d_{k_0}(x^{k_0},x ) .           \\end{aligned}\\ ] ] what follows is that @xmath237 is a decreasing sequence by .",
    "similarly , the uniform boundedness of @xmath238 could be obtained by and . and",
    "what follows is the uniform boundedness of the iteration sequence @xmath239 , by iteration with the help of lipschitz continuity of @xmath240 and @xmath241 .",
    "suppose there exist two subsequence @xmath242 and @xmath243 of @xmath244 that weakly converge to two weak limits @xmath245 and @xmath246 separately . by direct calculation , we have @xmath247 in order to avoiding the subtle issue of the convergence properties of the inner product of two weakly convergent sequences , we need to rewrite the inner product terms , @xmath248 substitute into , and take @xmath242 and @xmath243 instead of @xmath244 separately in the left hand side of , remembering the right hand side of only has a unique limit according to .",
    "thus , with notation @xmath249 , we have @xmath250 since @xmath251 while @xmath252 , what follows are @xmath223 and @xmath253 .",
    "the remaining proof is similar to the proof of theorem [ thm : dr - weak - convergence ] .",
    "thus we get the uniqueness of the weak limits of all weakly convergent subsequences , which leads to the weak convergence of the iteration sequence @xmath254 produced by while @xmath255 .",
    "similar to theorem [ thm : douglas - rachford - ergodic - rate ] , for @xmath227 , by and , and for @xmath225 , by , , and , we could get the ergodic convergence rate , and the proof is omitted . and here and following , denote @xmath256 .",
    "[ thm : douglas - rachford - ergodic - rate - relax ] suppose @xmath144 is a saddle - point of , with assumption , the following ergodic sequences with @xmath131 produced by iterations , @xmath257 converge weakly to @xmath141 , @xmath143 and @xmath146 , respectively . and",
    "the  partial \" primal - dual gap obeys @xmath258 where @xmath259 , and @xmath260 . in detail , for @xmath261 non - decreasing , @xmath262 , \\notag\\ ] ] and for @xmath263 non - decreasing , we have @xmath264= { \\mathcal{o}}(\\frac{1}{k } ) . \\notag    \\end{aligned}\\ ] ]    [ cor : overrelaxation :",
    "multiple : parameters ] from theorem [ thm : gap : estimate : pre : u ] and theorem [ thm : gap : estimate : pre : u : weak ] , noticing that if both @xmath137 and @xmath138 have multiple components and @xmath38 has multiple separable components , i.e. , @xmath265 where @xmath266 and @xmath267 as in , the relaxed admm becomes @xmath268 = ( ra^*a + \\partial f)^{-1}[\\sum_{i=1}^l a_{i}^*(-rbp_{i}^k + rc_{i } -\\bar \\lambda_{i}^k ) ] , \\notag\\\\ p_{1}^{k+1 }   & = ( r b_{1}^*b_{1 } + \\partial g_{1})^{-1}[b_{1}^ * ( -r \\rho_{k}^{1 } a_{1}u^{k+1 } + r(1-\\rho_{k}^{1})b_1p_{1}^{k } - \\bar{\\lambda}_{1}^{k } + r\\rho_{k}^{1}c_{1})],\\notag        \\\\        & \\cdots \\notag\\\\       p_{l}^{k+1 }   & = ( r b_{l}^*b_{l } + \\partial g_{l})^{-1}[b_{l}^ * ( -r \\rho_{k}^{l } a_{l}u^{k+1 } + r(1-\\rho_{k}^{l})b_lp_{l}^{k } - \\bar{\\lambda}_{l}^{k } + r\\rho_{k}^{l}c_{l } ) ] ,   \\\\",
    "\\bar{\\lambda}_{1}^{k+1 } & = \\bar{\\lambda}_{1}^{k } + r(\\rho_{k}^{1 } a_{1}u^{k+1 } + b_{1}p_{1}^{k+1}-(1-\\rho_{k}^{1})b_{1}p_{1}^{k}-\\rho_{k}^{1}c_{1 } ) , \\notag\\\\      &   \\cdots \\notag\\\\    \\bar{\\lambda}_{l}^{k+1 } & = \\bar{\\lambda}_{l}^{k } + r(\\rho_{k}^{l } a_{l}u^{k+1 } + b_{l}p_{l}^{k+1}-(1-\\rho_{k}^{l})b_{l}p_{l}^{k}-\\rho_{k}^{l}c_{l}),\\notag            \\end{aligned }      \\right.\\ ] ] by giving each component of @xmath269 or @xmath270 relaxation parameter @xmath271 satisfying .",
    "it could be checked that the results of theorem [ thm : gap : estimate : pre : u ] , theorem [ thm : gap : estimate : pre : u : weak ] and theorem [ thm : douglas - rachford - ergodic - rate - relax ] remain valid with adjustments by similar analysis focusing on each component @xmath272 .",
    "let @xmath273 , @xmath274 with @xmath275 , @xmath276 , @xmath277 , and @xmath278 with @xmath279 being the conjugate of the indicator function @xmath280 .",
    "then the original model could be reformulated as @xmath281 where @xmath16 is a block and @xmath17 is another block .",
    "we first write the augmented lagrangian , @xmath282 where @xmath283 . actually , by the definition of the fenchel conjugate @xcite , is equivalent to the following saddle - point problem , @xmath284 where @xmath285 .",
    "has a saddle - point solution @xmath286 with @xmath287 is a saddle - point solution of the original saddle - point problem .    in the case of @xmath50 , could be recovered by applying corollary [ cor : overrelaxation : multiple : parameters ] to the modified constrained problem .",
    "applying corollary to the modified constrained problem , we have @xmath288 ,              \\\\              p_{1}^{k+1 }   & = ( r b^*b + \\partial g)^{-1}[b^*(-\\bar{\\lambda}_{1}^{k }   -r \\rho_{k}^{1}a u^{k+1}+ r(1-\\rho_{k}^{1})bp_{1}^{k})+\\rho_{k}^{1}c ] ,        \\\\        p_{2}^{k+1 } & = ( r i + \\partial i_{\\{0\\}}^ * ) ^{-1}[\\bar{\\lambda}_{2}^{k } + r ( \\rho_{k}^{2}s u^{k+1}+(1-\\rho_{k}^{2})p_{2}^{k } ) ] , \\\\",
    "\\bar{\\lambda}_{1}^{k+1 } & = \\bar{\\lambda}_{1}^{k } + r[\\rho_{k}^{1}a u^{k+1}- ( 1-\\rho_{k}^{1})bp_{1}^{k } + b p_{1}^{k+1}-\\rho_{k}^{1}c ] , \\\\",
    "\\bar{\\lambda}_{2}^{k+1 } & = \\bar{\\lambda}_{2}^{k } + r[\\rho_{k}^{2}s u^{k+1}+(1-\\rho_{k}^{2})p_{2}^{k } - p_{2}^{k+1 } ] .            \\end{aligned }      \\right.\\ ] ] by moreau identity @xcite , for the update of @xmath289 in , we have @xmath290 combine with the update of @xmath291 , we have @xmath292 if setting @xmath293 , for the update of @xmath294 , we have @xmath295 and what follows is @xmath296 substitute into the update of @xmath32 in , get the relaxed preconditioned admm as follows , with notation @xmath297 : @xmath298 ,              \\\\",
    "p_{1}^{k+1 }   & = ( r b^*b + \\partial g)^{-1}[b^*(-\\bar{\\lambda}_{1}^{k }   -r \\rho_{k}^{1}a u^{k+1}+ r(1-\\rho_{k}^{1})bp_{1}^{k})+\\rho_{k}^{1}c ] ,        \\\\",
    "\\bar{\\lambda}_{1}^{k+1 } & = \\bar{\\lambda}_{1}^{k } + r[\\rho_{k}^{1}a u^{k+1}- ( 1-\\rho_{k}^{1})bp_{1}^{k } + b p_{1}^{k+1}-\\rho_{k}^{1}c ] .",
    "\\end{aligned }      \\right.\\ ] ] if choose @xmath299 , we get the preconditioned admm , @xmath300 ,        \\\\        \\bar{\\lambda}_{1}^{k+1 } & = \\bar{\\lambda}_{1}^{k } + r(au^{k+1 } + b p_{1}^{k+1}-c ) .",
    "\\\\            \\end{aligned }      \\right.\\ ] ]    thus we get the relaxed and preconditioned admm formally , by applying the original to the modified constrained optimization problem . the weak convergence and the corresponding ergodic convergence rate of",
    "could be got according to theorem [ thm : gap : estimate : pre : u : weak ] theorem [ thm : douglas - rachford - ergodic - rate - relax ] with and corollary [ cor : overrelaxation : multiple : parameters ] respectively . for compactness ,",
    "we do not state them here , and instead , they will be represented in theorem [ thm : dr - weak - convergence : full : pre : relax ] uniformly . here",
    ", we discuss the connections to some existing works .    in finite dimensional spaces , while @xmath301 and @xmath51 , the generalized admm ( gadmm ) in @xcite could be recovered by .",
    "it is not new that obtain preconditioned admm or other proximal admm with or without relaxation by applying the classical admm or relaxed admm in @xcite to the modified constrained problems including . for example , similar idea could be found in @xcite for designing preconditioned version of accelerated douglas - rachford splitting method . in finite dimensional spaces ,",
    "a similar modified constraint problem is also considered in @xcite for linearization of proximal admm , which is actually @xmath302 without the constraint @xmath303 in . however , @xcite direct applied relaxed admm in @xcite with only one relaxation parameter , i.e. , @xmath304 .",
    "thus the update of @xmath294 in could not be dropped and cooperated into the updates of @xmath32 in @xcite , and there are 4 updates for the linearized admm in @xcite instead of 3 updates as in .",
    "thus , to the best knowledge of the author , assigning different relaxations parameters to different components of @xmath17 or @xmath13 is new , e.g. , @xmath305 satisfying , @xmath306 .",
    "corollary [ cor : overrelaxation : multiple : parameters ] could help us to make the preconditioned and relaxed admm as compact as classical admm and probably more efficient , with benefit more from the classical admm or relaxed admm framework .",
    "an interesting relation between the douglas - rachford splitting method and admm was first pointed out by gabay @xcite , which was also studied in @xcite . here",
    "we extend it to relaxation variants with additional discussion .",
    "the optimality condition for the dual problem in is @xmath307 then we can apply douglas - rachford splitting method to the operator splitting by @xmath308 and @xmath309 . as suggested in @xcite ,",
    "we can write the douglas - rachford as @xmath310 where @xmath311 and @xmath312 write component wisely , we have @xmath313 where @xmath314 is the resolvent of the maximal monotone operator @xmath315 and @xmath316 , @xmath317 . denoting @xmath318 and using the moreau s identity",
    ", we have @xmath319 further more , adding relaxation to , we have @xmath320 , \\end{cases}\\ ] ] where @xmath321 , \\quad \\text{with } \\quad \\rho_{k}^{i } \\",
    "\\ \\text{satisfying \\eqref{eq : assum : relaxation } } , \\ \\",
    "i = 1,2.\\ ] ] originated from gabay @xcite , we can get the following theorem .",
    "[ thm : equivalent : dr : admm : relax ] with the assumption , preconditioned admm and its relaxation could be recovered from and separately .    for @xmath322 and @xmath323",
    ", it could be checked that by proposition 4.1 of @xcite ( or similar arguments as in proposition 23.23 of @xcite ) , while @xmath324 and @xmath118 exist and are lipschitz continuous which could be guaranteed by , they have the following forms @xmath325 where with moreau identity @xcite or direct calculation , @xmath326 together with @xmath327 and denoting @xmath328 , introduce some variables @xmath329 substitute the above relations to , we have @xmath330 by @xmath331 we have @xmath332 where @xmath333.\\ ] ] by the second equation of , we have @xmath334 if we set @xmath293 , by the update of @xmath335 in , we have @xmath336 and what follows is @xmath337 substitute it in to , we have the final form for updating @xmath32 , @xmath338.\\ ] ] if we set @xmath339 , becomes @xmath340.\\ ] ] this is exactly our preconditioned admm with only preconditioning for updating @xmath32 .",
    "we have figured clear the updates of @xmath32 , @xmath291 , @xmath294 , @xmath335 .",
    "let s consider the updates of @xmath341 , @xmath342 , @xmath343 .",
    "also starting from , by the update of @xmath342 , we have @xmath344 \\notag \\\\ & =   \\bar{\\lambda}_{1}^{k }   + r[(1 - \\rho_{k}^{1})(-bp_{1}^{k}+rc ) +   \\rho_{k}^{1}a u^{k+1 } + b p_{1}^{k+1}-c ] \\notag \\\\ & =   \\bar{\\lambda}_{1}^{k }   + r[\\rho_{k}^{1}a u^{k+1 } + b p_{1}^{k+1}-(1 - \\rho_{k}^{1})bp_{1}^{k }   - \\rho_{k}^{1}c ] .",
    "\\label{eq : lambda : update : fin}\\end{aligned}\\ ] ] and for the update of @xmath343 , by for @xmath345 , substituting the update of @xmath346 into the following equation @xmath347 with the help of , we have @xmath348\\ } \\in   \\partial g(p_{1}^{k+1 } ) + r b^*b p_{1}^{k+1}.\\ ] ] what follows is the update of @xmath343 @xmath349\\}),\\ ] ] and the update of @xmath342 @xmath350 collecting the updates of @xmath32 , @xmath343 and @xmath341 , we get our relaxed and preconditioned admm from relaxed douglas - rachford splitting , @xmath351\\})\\\\ \\bar{\\lambda}_{1}^{k+1 } =    \\bar{\\lambda}_{1}^{k }   + r[\\rho_{k}^{1}a u^{k+1 } + b p_{1}^{k+1}-(1 - \\rho_{k}^{1})bp_{1}^{k }   - \\rho_{k}^{1}c ] .",
    "\\end{cases}\\ ] ] thus we get relaxed and preconditioned admm from the corresponding douglas - rachford methods . and",
    "by setting @xmath299 , we also recover from .",
    "[ rem : gap : legal ] for or , if @xmath352 is the a constraint on the lagrangian multipliers @xmath13 , @xmath22 and @xmath23 , the lagrangian multipliers @xmath353 always satisfy the constraint , i.e. , @xmath354 is finite .",
    "this means the primal - dual gap function @xcite is bounded for or , if @xmath355 is also bounded .    actually , while @xmath22 , @xmath356 , the update of @xmath357 in or becomes @xmath358 what following @xmath359 always satisfy the constraint .",
    "by the definition of the primal - dual gap @xcite @xmath360 what follows @xmath361 is always bounded , and the primal dual gap is also bounded if @xmath362 is bounded .    [",
    "rem : weakdr : not : weakadmm ] while applying douglas - rachford splitting method to the dual problem of , i.e. , @xmath363 , @xmath364 , @xmath365 and @xmath366 in and , the weak convergence result of douglas - rachford splitting method without relaxation proved in @xcite is equivalent to @xmath367 as in , where @xmath368 in are actually @xmath369 in .",
    "for the dual inclusion problem , @xmath370 suppose @xmath371 then @xmath372 and @xmath244 as in @xcite are @xmath373 denoting @xmath374 , @xmath375 , then the weak convergence of @xcite without error tolerance could be stated as follows , @xmath376 and by , we see @xmath377 by , what follows is @xmath378 since by @xmath379 what follows is by theorem [ thm : equivalent : dr : admm : relax ] , @xmath380 thus we have @xmath381 and @xmath382 from .",
    "conversely , if we have @xmath367 . still by , we see @xmath383 then by the update of @xmath384 as in , we see @xmath385 hence @xmath386 also by and , we see @xmath387 and by , and",
    ", we see @xmath388 what follows is , @xmath389 take @xmath390 , is recovered .",
    "thus , we see the assumption @xmath367 could not lead to the weak convergence of admm method directly , since both @xmath24 and @xmath25 as in could not preserve the weak convergence properties .",
    "it is worth mentioning that while @xmath391 , iteration turns to the following  fully \" preconditioned admm without relaxations , @xmath392,\\\\ p^{k+1 }   & = ( m + \\partial g)^{-1}[b^ * ( -rau^{k+1 } - \\bar{\\lambda}^{k } + rc ) + ( m - rb^*b)p^{k } ] ,        \\\\",
    "\\bar{\\lambda}^{k+1 } & = \\bar{\\lambda}^{k } + r(au^{k+1 } + bp^{k+1}-c).\\\\            \\end{aligned }      \\right.\\ ] ] here , we would like to prove the weak convergence of directly , where the weak convergence and ergodic convergence rate of are recovered by setting @xmath45 .",
    "[ them : gap : estimate : full : pre : lao ] for iteration , with the same notation @xmath393 , @xmath394 as in and denoting @xmath395 , @xmath396 with assumption @xmath397 , for the  partial \" primal - dual gap for the fully relaxed admm , denoting @xmath398 with @xmath399 and @xmath400 , we have , for @xmath70 , while @xmath401 , @xmath402 and while @xmath403 , @xmath404    the proof is quite similar to theorem [ thm : gap : estimate : pre : u ] . by direct calculation",
    ", the updates of @xmath32 and @xmath33 in could be reformulated as follows @xmath405 with the help of the update of @xmath77 in , the second equation of above equations could be written as @xmath406 with the notation @xmath393 , @xmath394 as in , it could be seen the update of @xmath77 in becomes @xmath407 what follows is @xmath408 thus the update of @xmath32 could be written as @xmath409 with the help of and , by similar proof as in theorem [ thm : gap : estimate : pre : u ] with additional terms involving @xmath52 and @xmath53 , we see @xmath410 which is completely similar to , with the last two additional inner products involving @xmath411 and @xmath412 . thus , with the help of , and , one gets this theorem with similar arguments .    [",
    "cor : fully : relax : f ] for iteration , denoting @xmath395 , @xmath396 and assuming that @xmath185 , @xmath413 and @xmath414 , for the primal - dual gap for the relaxed admm , we have , for @xmath415 , while @xmath174 , @xmath416 while @xmath176 , we have @xmath417\\\\         & -\\frac{(2-\\rho_{k})^2}{2r\\rho_{k}^2}\\|\\bar\\lambda^{k}-\\bar\\lambda^{k+1}\\|^2         - \\frac{\\rho_{k}-1}{r}\\frac{2-\\rho_{k}}{2\\rho_{k}^2}\\|w^{k}-w^{k+1}\\|^2   -\\frac{r}{\\rho_k}\\|h(p^{k+1}-p^{k})\\|^2\\\\        &   + ( \\frac{r}{\\rho_k } -\\frac{r}{2})\\|h(p^k - p^{k-1})\\|^2 + \\frac{r}{2}(\\|h(p^{k}-p)\\|^2 - \\|h(p^{k+1}-p)\\|^2 ) \\\\         & + \\frac{r}{2}(\\|s(u^{k}-u)\\|^2 - \\|s(u^{k+1 } -u)\\|^2 - \\|s(u^{k+1}-u^k)\\|^2 ) - \\frac{r(2-\\rho_{k})^2}{2\\rho_{k}^2}\\|b(p^{k}-p^{k+1})\\|^2 .",
    "\\notag                \\end{aligned}\\ ] ]    the proof is similar to corollary [ cor : relax : partial ] .",
    "taking @xmath418 for example , after substituting @xmath419 and @xmath420 into the gap estimate in theorem [ them : gap : estimate : full : pre : lao ] by the expansion of @xmath419 and @xmath420 by as in , we have the detailed  partial \" gap estimate , @xmath421 since , @xmath422)\\rangle \\notag \\\\ & + \\langle p^{k+1}-p^{k } , ( m - rb^*b)(p^{k}-p^{k+1 } ) \\rangle - \\langle p^{k+1}-p^{k } , ( m - rb^*b)(p^{k-1}-p^{k } ) \\rangle \\notag \\\\ & + \\langle ( m - rb^*b)(p^k - p^{k+1}),p^{k+1}-p \\rangle . \\notag\\end{aligned}\\ ] ] by and monotone property of subgradient @xmath423 , we see for @xmath424 , @xmath425)\\rangle   \\leq 0,\\ ] ] and with the assumption @xmath426 , what follows is , @xmath427 since @xmath428 and @xmath429 are both self - adjoint and positive semi - definite operator , there exist square roots operator @xmath430 and @xmath431 , which are positive semi - definite , self - adjoint , bounded and linear ( see theorem vi.9 of @xcite ) , such that @xmath432 and @xmath433 .",
    "thus , by polarization identity , for the last term in , we have @xmath434 -   \\langle rh^*h(p^{k+1 } - p^{k } ) , p^{k } -p \\rangle \\notag\\\\ & \\leq   \\frac{r}{2}[\\|h(p^{k+1}-p^{k})\\|^2 + \\|h(p^k - p^{k-1})\\|^2]\\notag\\\\ & -\\frac{r}{2}[\\|h(p^{k+1}-p)\\|^2 - \\|h(p^k - p)\\|^2 - \\|h(p^{k+1}-p^k)\\| ] \\notag\\\\ & = r\\|h(p^{k+1}-p^{k})\\|^2   + \\frac{r}{2}\\|h(p^k - p^{k-1})\\|^2 -\\frac{r}{2}[\\|h(p^{k+1}-p)\\|^2 - \\|h(p^k - p)\\|^2].\\label{eq : full : gap : p : final}\\end{aligned}\\ ] ]    with the help of and , we have @xmath435\\\\ & + ( 2-\\frac{2}{\\rho_k})\\langle ( m - rb^*b)(p^k - p^{k+1 } ) , p^{k+1}-p \\rangle \\\\ & \\leq \\frac{2-\\rho_{k}}{\\rho_{k}}r\\{-\\|h(p^{k+1}-p^{k})\\|^2   + \\frac{1}{2}\\|h(p^k - p^{k-1})\\|^2 + \\frac{1}{2}[\\|h(p^{k}-p)\\|^2 - \\|h(p^{k+1}-p)\\|^2]\\ } , \\\\ & + ( 2-\\frac{2}{\\rho_k})\\frac{r}{2}(\\|h(p^k - p)\\|^2-\\|h(p^{k+1}-p)\\|^2-\\|h(p^{k}-p^{k+1})\\|^2 ) \\\\ & =   -\\frac{r}{\\rho_k}\\|h(p^{k+1}-p^{k})\\|^2          + ( \\frac{r}{\\rho_k } -\\frac{r}{2})\\|h(p^k - p^{k-1})\\|^2 + \\frac{r}{2}(\\|h(p^{k}-p)\\|^2 - \\|h(p^{k+1}-p)\\|^2 ) . \\end{aligned}\\ ] ] substitute into , we get corollary [ cor : fully : relax : f ] for @xmath436 case .    for @xmath225 , with the help of theorem [ them : gap :",
    "estimate : full : pre : lao ] , the expansion and , with similar arguments as in theorem [ thm : gap : estimate : pre : u : weak ] , one could get this corollary .",
    "still similar to lemma [ lem : admm - fixed - points ] , we also have the following lemma .",
    "[ lem : fully : padmm - fixed - points : relax ] iteration   possesses the following properties : if @xmath84 and @xmath437 , then @xmath86 converges weakly to a saddle point for  .    for the weak convergence of iteration and the ergodic convergence rate",
    ", we have similar theorem as theorem [ thm : gap : estimate : pre : u : weak ] and theorem [ thm : douglas - rachford - ergodic - rate - relax ] as follows , whose proof is omitted .",
    "[ thm : dr - weak - convergence : full : pre : relax ] with assumptions and the assumptions on @xmath165 for , if   possesses a solution , then for  , the iteration sequences @xmath98 converges weakly to a saddle - point @xmath438 of   and @xmath439 is a solution of .",
    "then the ergodic sequences @xmath440 converge weakly to @xmath141 , @xmath143 and @xmath146 , respectively .",
    "and we have @xmath441 with @xmath149 , and the ergodic convergence rate of the  partial \" primal - dual gap is as follows , for @xmath442 , @xmath443      = { \\mathcal{o}}(\\frac{1}{k } ) .",
    "\\end{aligned}\\ ] ] and for @xmath176 , we have @xmath444      = { \\mathcal{o}}(\\frac{1}{k } )",
    ".      \\end{aligned}\\ ] ]    here for simplicity and clarity , we use different ergodic sequence @xmath445 compared to ergodic sequences in previous sections . and @xmath446 is used in above theorem followed by the estimates involving @xmath447 with the help of corollary .",
    "let us apply the discrete framework for the total - variation regularized @xmath448- and @xmath449-type denoising problems ( the @xmath448-case is usually called the _ rof model _ ) @xmath450 where @xmath451 is a given noisy image and @xmath452 is a regularization parameter . for detailed discrete setting ,",
    "see @xcite .    * the case @xmath453 .",
    "* we see that for @xmath453 ,   could be reformulated as with introducing adjacent variable @xmath454 and the data @xmath455 note that a unique minimizer @xmath16 of   exists . furthermore , observe that @xmath456 for any @xmath457 .",
    "hence , a preconditioner for @xmath458 has to be chosen here . in order to implement the algorithm",
    ", we note that for @xmath459 , we have @xmath460 and since @xmath461 , the latter resolvent is given by the soft - shrinkage operator @xmath462 , see @xcite .",
    "this can in turn be expressed , for @xmath463 , by @xmath464 where @xmath465 .",
    "thus , preconditioned relaxed admm algorithms can be written as in table [ tab : precond - l2-tv ] .",
    ".the overrelaxed and preconditioned admm iteration for @xmath448-tv denoising . [ cols=\"<,>,<\",options=\"header \" , ]     -@xmath466 denoising : numerical convergence rates .",
    "the normalized primal - dual gap is compared in terms of iteration number and computation time for figure  [ peninsula : l2 ] with @xmath467 .",
    "the notations admm(@xmath468 ) , radmm(@xmath468 ) , padmm(@xmath469 ) , rpadmm(@xmath469 ) and pdrq(@xmath469 ) are used to indicate the step - size @xmath468 and @xmath470 inner iterations .",
    "note the double - logarithmic and semi - logarithmic scale , respectively , which is used in the plots.,title=\"fig:\",width=272 ] -@xmath466 denoising : numerical convergence rates .",
    "the normalized primal - dual gap is compared in terms of iteration number and computation time for figure  [ peninsula : l2 ] with @xmath467 .",
    "the notations admm(@xmath468 ) , radmm(@xmath468 ) , padmm(@xmath469 ) , rpadmm(@xmath469 ) and pdrq(@xmath469 ) are used to indicate the step - size @xmath468 and @xmath470 inner iterations .",
    "note the double - logarithmic and semi - logarithmic scale , respectively , which is used in the plots.,title=\"fig:\",width=272 ]    l@lr@lr@lr@l + & & & & + alg1 & & 341&(4.67s ) & 871&(11.55s ) & 3446&(34.13s ) + admm & & 143 & ( 9.82s ) & 351&(22.66s ) & 1371&(82.98s ) + radmm & & 104 & ( 7.26s ) & 199&(13.65s ) & 725&(53.20s ) + fadmm & & 115 & ( 8.18s ) & 265&(18.48s ) & 869&(62.43s ) + padmm & & 168 & ( 3.76s ) & 397&(8.74s ) & 1420&(27.80s ) + fpadmm & & 140 & ( 2.16s ) & 339&(5.12s ) & 938&(18.43s ) + rpadmm & & 107 & ( 1.97s ) & 241&(4.06s ) & 776&(15.66s ) +    -@xmath466 denoising : numerical convergence rates .",
    "the normalized error of primal energy is compared in terms of iteration number and computation time for figure  [ shooter : l1 ] with @xmath471 .",
    "the notations admm(@xmath468 ) , radmm(@xmath468 ) , padmm(@xmath469 ) , rpadmm(@xmath469 ) , fadmm(@xmath468 ) , and fpadmm(@xmath469 ) are used to indicate the step - size @xmath468 and @xmath470 inner iterations .",
    "note the double - logarithmic and semi - logarithmic scale , respectively , which is used in the plots.,title=\"fig:\",width=272 ] -@xmath466 denoising : numerical convergence rates .",
    "the normalized error of primal energy is compared in terms of iteration number and computation time for figure  [ shooter : l1 ] with @xmath471 .",
    "the notations admm(@xmath468 ) , radmm(@xmath468 ) , padmm(@xmath469 ) , rpadmm(@xmath469 ) , fadmm(@xmath468 ) , and fpadmm(@xmath469 ) are used to indicate the step - size @xmath468 and @xmath470 inner iterations .",
    "note the double - logarithmic and semi - logarithmic scale , respectively , which is used in the plots.,title=\"fig:\",width=272 ]    from table [ tab : peninsula:0.5 ] , table [ tab : shooter - l1denoise:0.6 ] , figure [ peninsula : denoise : l2rate ] and figure [ lena : denoise : l1rate ] , for both rof and @xmath472 model , it could be seen that overrelaxation with relaxation parameters setting @xmath473 could bring out certain acceleration , at least @xmath474 faster than @xmath475 . and",
    "efficient preconditioner could bring out more efficiency , which is also observed in @xcite through a proximal point approach .",
    "generally , the preconditioned admm with overrelaxation could bring out 3 - 5 times faster than the original admm without any preconditioning and relaxations . and",
    "for @xmath472 , it could be seen that , there is no big difference for between radmm and fadmm , or between rpadmm and fpadmm , which employ different relaxation strategies .",
    "besides , radmm or rpadmm could be slightly faster with fewer number of iterations in some cases . and for rof model , the primal - dual gap could be used for radmm and rpadmm , while the boundedness of the primal - dual gap for fadmm and fpadmm is unclear .",
    "the author is very grateful to kristian bredies , who suggested numerous improvements during preparing the draft of this article .",
    "k. bredies , h. sun , _ preconditioned douglas - rachford splitting methods for convex - concave saddle - point problems _ , siam",
    ". j. numer .",
    "anal . , 53(1 ) , pp . 421444 , 2015 .",
    "k. bredies , h. sun , _ a proximal - point analysis of preconditioned alternating direction method of multipliers _ , 2015 .",
    "a. chambolle , t. pock , _ a first - order primal - dual algorithm for convex problems with applications to imaging _ , j. math .",
    "imaging vision , 40(1 ) , pp . 120145 , 2011 .",
    "a. chambolle , t. pock , _ diagonal preconditioning for the first order primal - dual algorithms _ , iccv 2011 , ieee , pp . 17621769 .",
    "a. chambolle , t. pock , _ on the ergodic convergence rates of a first - order primal - dual algorithm _ ,",
    "math . program . , 159(1 ) ,",
    "pp . 253287 , 2016 .",
    "a. chambolle , t. pock , _ an introduction to continuous optimization for imaging _",
    ", acta numerica , 25:161319 , 5 , 2016 .",
    "l. chen , d. sun , k. c. toh , _ an efficient inexact symmetric gauss - seidel based majorized admm for high - dimensional convex composite conic programming _ , math . program .",
    "p. l. combettes , _ iterative construction of the resolvent of a sum of maximal monotone operators _ , j. convex anal .",
    ", 16(4 ) , pp . 727748 , 2009 .          , _ on decomposition - coordination methods using an augmented lagrangian _ , in : m. fortin and r. glowinski , eds .",
    ", _ augmented lagrangian methods : applications to the solution of boundary value problems _ , north - holland , amsterdam , 1983 .    ,",
    "_ applications of the method of multipliers to variational inequalities _ , in : m. fortin and r. glowinski , eds . , _ augmented lagrangian methods : applications to the solution of boundary value problems _ , north - holland , amsterdam , 1983 .      , _ sur lapproximation , par lments finis dordre un , et la rsolution , par pnalisation - dualit dune classe de problmes de dirichlet non linaires _ ,",
    "revue franaise dautomatique informatique recherche oprationnelle , analyse numrique , tome 9 , no . 2 ( 1975 ) ,",
    "41 - 76 .    , _ on the douglas ",
    "rachford splitting method and the proximal algorithm for maximal monotone operators _ , math .",
    ", 55 , ( 1992 ) , pp .",
    ", on the @xmath476 convergence rate of the douglas - rachford alternating direction method , siam .",
    ", 50 , pp .",
    "700709 , 2012 ."
  ],
  "abstract_text": [
    "<S> alternating direction method of multipliers ( admm ) is a powerful first order methods for various applications in signal processing and imaging . </S>",
    "<S> however , there is no clear result on the weak convergence of admm with relaxation studied by eckstein and bertsakas @xcite in infinite dimensional hilbert spaces . in this paper , by employing a kind of  partial \" gap analysis , we prove the weak convergence of general preconditioned and relaxed admm in infinite dimensional hilbert spaces , with preconditioning for solving all the involved implicit equations under mild conditions . </S>",
    "<S> we also give the corresponding ergodic convergence rates respecting to the  partial \" gap function . </S>",
    "<S> furthermore , the connections between certain preconditioned and relaxed admm and the corresponding douglas - rachford splitting methods are also discussed , following the idea of gabay in @xcite . </S>",
    "<S> numerical tests also show the efficiency of the proposed overrelaxation variants of preconditioned admm .    </S>",
    "<S> [ [ key - words . ] ] key words . </S>",
    "<S> + + + + + + + + + +    alternating direction method of multipliers , douglas - rachford splitting , relaxation , linear preconditioners technique , weak convergence analysis , partial primal - dual gap    [ [ ams - subject - classifications . ] ] ams subject classifications . </S>",
    "<S> + + + + + + + + + + + + + + + + + + + + + + + + + + + +    65k10 , 49k35 , 90c25 , 65f08 . </S>"
  ]
}