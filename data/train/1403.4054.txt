{
  "article_text": [
    "if a dataset is too large to naively process with a desired model , we can either a ) change the model , b ) discard some data , or c ) selectively order the calculation to yield useful partial results .",
    "this paper describes a framework for decision making to approximately evaluate complex calculations , with application to the specific problem of computing distance or more general similarity matrices .",
    "statistical reasoning is used to navigate the computational challenges involved .",
    "the framework has the character of a sequential decision problem .",
    "specifically , at any sequential step a set of objects are selected for evaluation .",
    "the determination of which objects to evaluate is guided by a loss function .",
    "this loss function can not be evaluated without the objects , and so we use an _ emulator _ to enable estimation of the loss",
    ". this emulator attempts to provide a computationally efficient prediction of unevaluated objects . to be of value",
    ", this computation must be less demanding than direct evaluation .",
    "the general framework is applied to the problem of computing similarity matrices .",
    "there are numerous uses of similarity matrices in data analysis , including clustering , near neighbour search , and anomaly detection . in principle , evaluating the entire similarity matrix provides a solution for these uses .",
    "the current focus on `` big data '' , fuelled by technology capable of automatically collecting and storing data at huge scale , provides new challenges for data analysis .",
    "since the number of elements in a similarity matrix is proportional to the square of the number of objects , evaluating the entire matrix soon becomes computationally intractable .",
    "additionally , we may not always require the complete evaluation of the similarity matrix .",
    "this paper develops a framework for a variety of data analysis activities based on similarity matrices that is intended to partially address the computational challenges of big data .",
    "data analytic uses of similarity matrices are used in numerous areas .",
    "bioinformatics is one , with applications including sequence alignment @xcite , and population genetics @xcite .",
    "a particularly topical driver is recommender systems ( eg .",
    "@xcite ) , which has motivated much work in the relatively new field of matrix completion ( e.g. @xcite ) .",
    "this field provides tools to impute the missing entries in a matrix . whilst this is a very useful tool for computation , it is possible to make progress with less computationally intensive approaches .",
    "the methodology we propose directly addresses scenarios involving data on @xmath0 objects , where @xmath0 is potentially very large .",
    "objects need not be of the same dimension , but a similarity matrix @xmath1 can be constructed via a similarity function @xmath2 .",
    "a critical feature of these scenarios is that the similarities are available , but their number and computational cost make brute force evaluation intractable .",
    "this feature is an important contrast from the standard matrix completion scenario .",
    "further , there is no feature space in which to place objects .",
    "hence we use the evaluated objects to create a dynamic space via a process we call ` data directional scaling ' .",
    "our framework uses statistical reasoning to carefully order a computation to obtain the maximum information benefit for the lowest possible computational cost . to do this ,",
    "we draw on a number of statistical and machine - learning topics , which are reviewed in section [ sec : lit ] .",
    "our methodology is introduced in section [ sec : method ] , including a simple ( non - similarity based ) example .",
    "we then define the similarity framework itself and introduce our computational framework , along with a simulation study to examine performance as we define the details .",
    "section [ sec : scaling ] examines how the approach scales with the size of the problem , with section [ sec : genetics ] providing a real - world application to a large genetics problem .",
    "we conclude in section [ sec : discussion ] with a discussion .",
    "this section reviews statistical , machine learning and computer science literature that is helpful or strongly related to our methodology .",
    "section [ sec : emulation ] discusses the use of emulators , section [ sec : completion ] describes matrix completion , section [ sec : index ] describes efficient indexing strategies , section [ sec : active ] describes active learning , and section [ sec : sequential ] describes sequential decision making .",
    "gaussian process emulation @xcite is a major tool in the design and analysis of computer experiments , with applications ranging from climate models @xcite , and carbon budgets @xcite to demography @xcite and testing cars in crashes @xcite . in typical use",
    ", a limited number of computer experiments can be run and the goal is to choose the parameter values to run the expensive simulation model at , in order to fit a costly simulator to data . if the prediction space can be treated as continuous , then treating the observations as varying smoothly according to a multivariate normal distribution proves surprisingly helpful in predicting good choices for where to evaluate the full simulator .",
    "there are naturally difficulties , one of which being that the cost of estimating a gaussian process grows rapidly with the size of the dataset .",
    "decision making to exclude some data for future predictions @xcite can limit this when the data are added sequentially ( e.g. @xcite ) . in this case",
    "diagnostics are of extra importance @xcite , to prevent the model from exploring the wrong area of the space .",
    "@xcite provide a computation - aware framework for gaussian processes .",
    "whilst gaussian process emulators are by far the most common , other emulators are possible ; for example , @xcite use an emulator over a binary space to speed computer vision .",
    "bespoke emulators are also commonplace in the analysis of computer networks ( e.g. @xcite ) .",
    "our work differs from the above in a ) providing a general class of non - gaussian process emulator for similarity matrices ; and b ) extending the use of emulators to the sort of tradeoff found in everyday computation in ` big data ' .",
    "this is in contrast to the standard emulator framework in which the emulator has negligible computational cost .      the very active area of matrix completion",
    "is broadly concerned with imputing missing entries in a matrix from a relatively small proportion of observed entries .",
    "@xcite provides a summary of recent work on matrix completion .",
    "the basic framework has two key features .",
    "first , the observed entries are assumed to be a random sample and are such that the observed matrix is full rank .",
    "second , it is assumed that the full matrix has a low rank representation . under these assumptions ,",
    "algorithms implementing nuclear norm minimisation provide the computational means to attack the matrix completion problem .",
    "notably proofs are available that a low rank solution can be recovered in few evaluations within a theoretical bound , in terms of the root - mean - square reconstruction error . a more recent review and a novel algorithm working under relaxed assumptions",
    "is provided by  @xcite .",
    "indeed , many variations and refinements have been proposed , including @xcite which provides an algorithm that claims to open the door for matrix completion with big data . scaling to big data , the target of our proposed framework , is a key challenge in matrix completion . to quote @xcite : ``  but the computational challenges of solving problems with millions if not billions of unknowns obviously still require much research '' .",
    "the methodology and soft impute algorithm developed in @xcite provides capability to handle much bigger problems than many proposed matrix completion algorithms .",
    "motivated by the soft impute algorithm and the fact the big data sometimes features sequential arrival , @xcite develop an online variant of matrix completion .",
    "they key aspect in this approach is a randomised technique for computing the singular value decomposition - a method that is at the core of all matrix completion algorithms .",
    "in addition to the application of matrix completion to recommender systems discussed in the introduction , other interesting applications include genotype imputation @xcite , functional genomics @xcite , computer network performance prediction @xcite and global positioning in sensor networks ( eg .",
    "this is an interesting example because there are often power constraints on sensors that means some distances are not observed , leading to a bias in the sampling mechanism .",
    "this is explored in @xcite .",
    "matrix completion would be a valid approach to some of the problems we face in this work .",
    "however , it is not the approach we take for several reasons .",
    "first , we want to make informed decisions with little information , when a matrix completion would be of insufficient rank .",
    "second , the requirement to obtain a full rank solution also provides a restriction on which elements can be evaluated , as all rows and all columns must be visited .",
    "third , little attention has been paid to how to update a matrix completion as data is observed sequentially .",
    "fourth , we can make use of limited information without performing a full matrix completion .",
    "finally , the literature does not provide an ` off the shelf ' solution tailored to the particular type of similarity matrices we face ; as we shall see below , the current methods perform badly because they make the wrong assumptions about the form of the matrix .",
    "there are numerous data structures useful for indexing multivariate spaces , including kd - trees @xcite , quadtrees @xcite r trees @xcite , and x - trees @xcite .",
    "our problem does not observe a multivariate space , but instead we view similarities as lying on some implicit manifold in an _ unknown _ space .",
    "these approaches therefore can not be directly applied . however",
    ", if we use some other algorithm to recover the space then indexing can be of value to allow rapid lookup of neighbours . to do this",
    "the tree algorithm would need to handle a growing space as the algorithm progresses , for which we are not aware of any easily applicable framework .",
    "some supervised classification problems have the following characteristics .",
    "first , there is a small amount of labelled data and a large amount of unlabelled data .",
    "second , the process of labelling is costly . in these scenarios ,",
    "active learning @xcite is used to select unlabelled data for presentation to an oracle for labelling .",
    "the objective is select those unlabelled data that will yield the greatest improvement to the classification model , thereby minimising the cost and maximising the utility of labelling .",
    "there are numerous variations on this basic theme .",
    "@xcite provides an excellent review , and details the many heuristics that have been explored .",
    "interestingly , it seems only recently that performance comparison against a random selection benchmark is important .",
    "our framework shares many characteristics with active learning : we seek to score unevaluated similarities @xmath1 with the intention that the scores are indicative of the value of the similarity to the task .",
    "@xcite discusses the role of an oracle in matrix completion problems . as with active learning ,",
    "random selection of similarities is an important benchmark in our framework . unlike most work in active learning , we obtain a continuous outcome",
    ", we do not have an explicit feature space in which to work , and we are interested in different loss functions",
    ".      sequential decision often involves selecting actions to maximise reward in noisy or uncertain environments . a good overview is given in  @xcite .",
    "the `` exploitation - exploration dilemma '' is always present in such problems .",
    "this dilemma concerns the competing objectives of selecting actions for maximal gain ( exploitation ) and for reducing uncertainty ( exploration ) .",
    "the core of our framework is sequential selection of objects @xmath3 to evaluate , and the exploitation - exploration dilemma naturally arises .    one simple approach to address the dilemma is to incorporate an element of randomised decision making . such methods ,",
    "called @xmath4-greedy methods , select the greedy action ( the action with highest predicted reward ) with probability @xmath5 , and a random non - greedy action with probability @xmath4 .",
    "such algorithms have solid theoretical justification for infinite horizon problems and have been demonstrated to behave well empirically .",
    "we start in section [ sec : general ] with the general decision framework for ordering computation .",
    "section [ sec : simple ] illustrates the advantage of reasoning about computation with the well known example of an autoregressive model .",
    "section [ sec : dds ] explains the data directional scaling method for constructing a similarity space .",
    "section [ sec : similarity ] defines the similarity problem precisely , whilst [ sec : doemulation ] fully defines the emulation framework .",
    "section [ sec : choice ] defines the choice framework , whilst the simpler issues of assessment ( section [ sec : assessment ] ) and termination ( section [ sec : termination ] ) .",
    "finally , this is wrapped up with a simulation study in section [ sec : simstudy ] .",
    "consider a discrete set of objects @xmath6 about which we can choose to take measurements ( hereafter called computations ) @xmath7 at cost @xmath8 , with mean cost @xmath9 .",
    "if @xmath10 is large relative to our computational budget , we can not afford to compute them all . despite this",
    ", we wish to compute some quantity , say @xmath11 , of the whole dataset .",
    "how well we estimate @xmath12 is defined by a loss @xmath13 .",
    "for example , if @xmath14 is the sample mean then we could define @xmath15 to specify a minimum variance solution .",
    "we will iteratively make choices about which object @xmath16 to evaluate at iteration @xmath17 .",
    "the number of iterations @xmath18 may not be fixed .",
    "let @xmath19 be the set of objects selected up to now , and @xmath20 be the associated observed quantities .",
    "our framework uses two key concepts :    * * a choice function * : @xmath21 takes the previously observed information @xmath22 and chooses an object @xmath23 from the unobserved objects @xmath24 . * * an emulator * : @xmath25 the emulator takes the previously observed information @xmath22 and returns a predictive distribution @xmath26 on any observed object @xmath23 .",
    "the choice function @xmath21 can use any of the available information . specifically",
    ", the choice function can use the emulator @xmath25 to make intelligent decisions via the following tools :    * * a loss estimator * : @xmath27 over possible values of @xmath26 .",
    "note that @xmath28 for @xmath29 . * * a heuristic * : @xmath30 is a decision rule avoiding emulation . in some special cases",
    ", it can be shown to minimise a loss @xmath31 . if it does , it is _ exact _ for that loss .",
    "although emulators that make a point prediction could be exploited , most interesting loss functions require as assessment of uncertainty",
    ". we will explore the loss function more fully in section [ sec : choice ] after introducing the similarity problem .",
    "the purpose of this framework is to reduce computation . as this restricts the number of quantities that the choice function can evaluate , it is helpful to describe it in more detail .",
    "@xmath21 proposes a number @xmath32 of objects @xmath33 for which the minimum predicted loss @xmath34 is estimated .",
    "a special case is where there is only one proposal , in which case no loss calculation is required .",
    "this can happen if an exact heuristic @xmath30 is available for the loss @xmath35 .",
    "even if there is no known exact heuristic , proposals from other heuristics are often helpful .",
    "finally , note that the choice function has available the current iteration @xmath36 as well as being able to assess the success of previous choices",
    ". this can be used choose to enable or disable proposals .",
    "for example , it is often useful to use more complex proposals initially , and then switch to simpler proposals as the marginal return of careful choice decreases .",
    "the algorithm proceeds as follows . for each iteration @xmath36 :    1 .",
    "_ choice _ : 1 .",
    "_ proposal _ : @xmath21 proposes objects @xmath37 to evaluate .",
    "emulation _ : if @xmath38 , estimate the loss @xmath39 .",
    "decision _ : choose the action @xmath40 that minimises the loss .",
    "evaluation and assessment _ : compute @xmath41 of the chosen objects @xmath42 .",
    "compare the predictive distribution @xmath43 to the observations @xmath44 .",
    "termination _ : stop if a stopping criterion is reached .",
    "these processes are precisely defined below .",
    "specifically , section [ sec : doemulation ] deals with emulation , which also uses the results of assessment .",
    "section [ sec : choice ] deals with the remaining aspects of the choice function .",
    "section [ sec : termination ] addresses stopping the algorithm .",
    "before we narrow our attention to similarity matrices , it is helpful to demonstrate the value of reasoning about computation more generally . as an example",
    ", let @xmath3 ( with @xmath6 ) describe an autoregressive ( ar1 ) model : @xmath45 with @xmath46 and @xmath47 .",
    "let us assume that the process has reached stationarity .",
    "we wish to estimate @xmath48 via an estimator @xmath49 .",
    "if either @xmath50 is large , or the cost per computation @xmath51 limits the number we can make , then we can not compute @xmath52 using @xmath53 .",
    "however , we can obtain a monte - carlo estimate by evaluating a subset @xmath54 . since they are correlated , which should we choose ?",
    "let @xmath55 , and the loss be @xmath56 . from the standard monte - carlo methodology",
    "( e.g. @xcite ) it is known that minimising this variance is the same as minimising the covariance between the samples .",
    "this can be performed explicitly to result in an optimal heuristic .",
    "if @xmath18 is known in advance then @xmath57 is known , i.e. we can choose @xmath58 before we compute any @xmath59 .",
    "this uses evenly spaced samples with separation @xmath60 ( called thinning in the markov - chain monte carlo literature ) .",
    "if however @xmath18 is not known in advance , @xmath30 can use an iterative greedy approach by picking the object @xmath61 furthest from all previous objects @xmath62 , i.e. select objects @xmath63 etc .",
    "if the autocorrelation length of @xmath59 is greater than @xmath18 then this approach massively reduces the variance over the naive approach of using the first @xmath18 objects .",
    "it also clearly improves on the selection of random objects , which is a heuristic that is frequently helpful when nothing else can be done .",
    "if our desired loss function is to instead estimate the parameters @xmath64 and @xmath12 as accurately as possible , then we must decide on an estimation framework ( least squares , yule - walker equations , maximum likelihood via numerical optimization , etc .",
    "then we face a tradeoff between obtaining high lags to estimate @xmath65 and low lags to estimate @xmath12 . in this case",
    ", the correct model can be used via a kalman filter @xcite .",
    "this can be seen as emulation , and is one way to handle missing data @xcite .",
    "developing the appropriate loss and choice functions is possible but outside the scope of this paper .",
    "before we go into the specifics of decision making for similarity matrices , we introduce ` data directional scaling ' ( dds ) which is a type of multidimensional scaling ( mds @xcite ) that has proven very helpful for our emulator . in mds ,",
    "the data is seen as having a position on a manifold or metric space described by a basis .",
    "there are a wide range of mds methods , but the most commonly used is principal component analysis ( pca or equivalently , singular value decomposition , svd ) . in pca ,",
    "basis vectors are chosen to be orthogonal and decreasing in their contribution to the variance observed in the dataset .",
    "for the similarity problem , consider a set of objects @xmath66 describing data @xmath67 with @xmath68 indexing information about those objects .",
    "there is not necessarily a consistent dimension to the data , but a non - negative similarity function @xmath69 can always be evaluated . if additionally @xmath59 forms a metric space then @xmath70 for all @xmath23 and we can write distances @xmath71 . @xmath72 are also non - negative with @xmath73 if and only if objects @xmath23 and @xmath74 are identical under @xmath59 .",
    "conversely , if @xmath59 is non - metric , we can still convert to a distance - like measure via @xmath75 but some self - distances are non - zero",
    ". we can interchangeably work with @xmath59 or @xmath76 as we do not exploit any properties of a metric .    without access to any space in which the objects can be embedded , most traditional inference frameworks",
    "are difficult to apply .",
    "the matrix completion approach seeks to find the underlying space by inferring a low - rank latent space ( typically an svd ) .",
    "however , this fails when insufficient matrix elements have been evaluated .",
    "our approach evaluates matrix elements in rows @xmath77 by calculating all similarities with object @xmath23 .",
    "this is called ` evaluating object @xmath23 ' .",
    "it leads directly to a vector space @xmath78 using only the information from observing objects @xmath62 .",
    "the basis of @xmath78 is the _ columns _ @xmath79 of the similarity of each evaluated object @xmath80 with each other object @xmath81 .",
    "therefore an additional observation at iteration @xmath82 simply creates a new direction in the space .",
    "figure [ fig : dds]a illustrates this procedure .",
    "this is useful because columns @xmath83 for @xmath84 are observed .",
    "the emulator @xmath85 provides a position @xmath86 for object @xmath23 in the space @xmath78 . for practical reasons ,",
    "we define @xmath86 to live in the @xmath87 dimensional simplex ; i.e. @xmath88 and @xmath89 for all @xmath23 .",
    "now our objects @xmath23 are standard observations in a vector space , and we can use standard tools to perform inference . specifically , we treat columns @xmath90 as observations , and rows @xmath91 as responses . we can therefore use standard linear methods to predict unobserved rows @xmath91 . this procedure is discussed in section [ sec : doemulation ] .",
    "figure [ fig : dds]b interprets this procedure when objects are truly embedded in @xmath92 , which is a metric space and therefore the similarities are symmetric .",
    "if we further assume that similarities are observed with noise , then triangulation is non - trivial . by observing objects at the ` corners ' of the space , the unobserved objects fall inside or near the convex hull of the observed objects . in this case",
    ", we will be able to accurately represent the whole space without extrapolation .",
    "[ ht ]     are shown at the top , with the vector space @xmath78 being defined by the set of columns of observed similarities @xmath93 for observed objects @xmath94 .",
    "object @xmath23 is associated with observations @xmath95 , which is mapped to a position @xmath96 in the vector space using the learning model for the emulator @xmath25 .",
    "the prediction space is the rows @xmath3 of the matrix , for which we observe @xmath97 with @xmath80 .",
    "right : example with an underlying euclidean space @xmath92 .",
    "if we have observed the three objects in red , then we can reconstruct any point in the @xmath98 simplex ( i.e. triangle ) shown in red .",
    "positions outside the simplex are mapped to the nearest point inside the simplex .",
    "[ fig : dds ] , title=\"fig:\",scaledwidth=45.0% ]   are shown at the top , with the vector space @xmath78 being defined by the set of columns of observed similarities @xmath93 for observed objects @xmath94 .",
    "object @xmath23 is associated with observations @xmath95 , which is mapped to a position @xmath96 in the vector space using the learning model for the emulator @xmath25 .",
    "the prediction space is the rows @xmath3 of the matrix , for which we observe @xmath97 with @xmath80 .",
    "right : example with an underlying euclidean space @xmath92 .",
    "if we have observed the three objects in red , then we can reconstruct any point in the @xmath98 simplex ( i.e. triangle ) shown in red .",
    "positions outside the simplex are mapped to the nearest point inside the simplex .",
    "[ fig : dds ] , title=\"fig:\",scaledwidth=45.0% ]      we will use dds and the choice framework to optimally select elements from the similarity matrix to evaluate .",
    "the expected cost of a similarity computation is defined to be @xmath51 , with @xmath99 if the computation is linear .",
    "the total cost of computing this matrix is therefore @xmath100 , which we assume is too large .",
    "our methodology will reduce this computation to @xmath101 for quite general problems .",
    "this can be reduced to @xmath102 if a ) the full matrix is not required and b ) either a less exact solution is satisfactory , or explicit solutions to certain sub - problems ( i.e. exact heuristics ) can be found .    as above",
    ", we use a choice function and an emulator .",
    "the choice function @xmath21 now can return either a single similarity element ( i.e. an @xmath103 pair ) , or a set of elements . to use dds",
    ", it is necessary to evaluate @xmath1 in entire rows .",
    "the emulator still returns the value of single elements , and takes the form @xmath104 which provides a distribution for any element @xmath103 . @xmath105 and",
    "@xmath106 both require further specification . whilst it is arguable whether the normal model is appropriate",
    ", we note that it is justifiable on computational grounds .",
    "further , if the similarities are sums over features then ( as in gaussian process emulation ) the central limit theorem justifies this choice .",
    "recall that in our framework , the utility of an emulator is to be able to predict how calculating a particular set of elements will improve our ability to optimise a loss .",
    "we therefore will need to be able to compute many emulated values , and recompute our emulator as new information arises .",
    "a slightly separate use is to ` complete ' the matrix at the end of the algorithm to provide a ` best estimate ' .",
    "compare this use with matrix completion , for which the computational budget is allowed to be larger .    for defining the emulator",
    ", we abuse our notation and set @xmath62 to be the index of the objects we have evaluated , i.e. _ full rows _ of the matrix @xmath59 evaluated up to time @xmath36 .",
    "the emulator takes the form : @xmath107 where we have dropped the obvious dependence on @xmath108 .",
    "the quantities @xmath109 and @xmath110 are both inferred using linear prediction frameworks , due to computational restrictions .",
    "using the data directional scaling from section [ sec : dds ] and figure [ fig : dds]a , @xmath111 is estimated as follows . for @xmath80 ,",
    "a linear regression is used to _ learn _ the observations in column @xmath23 using the observed columns @xmath112 : @xmath113 row @xmath23 is _ predicted _ using the observed rows : @xmath114 here , @xmath115 is the standard residual error on observed elements @xmath22 .",
    "@xmath116 is the out - of - sample prediction error for an unobserved element @xmath1 .",
    "the residual @xmath117 is useful , but we will are particularly interested in the understanding the ` emulator variance ' @xmath118 , the same quantity from equation [ eqn : emulation ] .",
    "as discussed in the dds section , @xmath96 is learnt under the constraint that @xmath119 and all @xmath120 using one of the following estimation procedures :    * nearest neighbour .",
    "choose @xmath121 . by weighting large similarities ` infinitely ' strongly",
    "this norm leads to selecting @xmath122 for @xmath123 , i.e. the nearest neighbour in the column @xmath124 with @xmath125 , and zero otherwise . *",
    "mixture model .",
    "choose @xmath126 , i.e. fit the mixture to minimise the square error .",
    "this is inferred using quadratic programming @xcite .    figure [ fig : dds]b captures the mixture model .",
    "we also considered an unconstrained linear regression , but this is unstable because the inference is overspecified when @xmath127 ; in practice two objects within one cluster are used with a very large positive and negative weight .",
    "because we are dealing with similarities , it is necessary to handle ` self ' specially .",
    "we swap @xmath128 and @xmath129 so that the estimator ` sees ' the similarity of @xmath112 with @xmath23 instead of the that of @xmath112 to @xmath112 ( which is only zero for metric spaces ) .      estimating @xmath109 must be rapid , and capture the distribution of the sampled objects and structure of the manifold on which they lie . in principle",
    "it is a function of the full set of similarities with the evaluated objects , and is not ` observable ' in the way @xmath110 was , because the uncertainty of an observed element is zero .",
    "we calculate the uncertainty of the prediction via a linear combination of most conceivable summary statistics : @xmath130 this includes coefficients for an intercept @xmath131 , the number of previously evaluated objects @xmath36 via ( @xmath36 and @xmath132 ) , and the regression residuals @xmath133",
    ". we also account for the full similarities distribution of object @xmath23 with evaluated objects @xmath80 using ` similarity distance ' in the @xmath134 norm ( using @xmath135 ) : @xmath136    _ fitting the emulator variance @xmath137 _ : the set of objects @xmath62 chosen for evaluation may be very far from random and @xmath138 ( for @xmath139 ) are typically biased to small values ( i.e. the distances between them are larger than average , as in figure [ fig : dds]b ) .",
    "extrapolation using linear regression can therefore be misleading and we enforce a sensible prediction by using non - negative least squares regression ( nnls,@xcite ) instead .",
    "this automatically sets some coefficients to zero , although explicit penalisation can also be used .    empirically , our nnls metho most commonly chooses the regression : @xmath140 this is computationally efficient to work with as only nearest neighbours have a changed @xmath141 coefficient and @xmath142 changes only by a single coordinate , leading to an efficient calculation for @xmath143 .",
    "therefore the results we report use equation [ eqn : deltaused ] , although we have implemented equation [ eqn : deltafull ] and checked that its performance is not significantly different .    _",
    "cross validation estimate of @xmath137 _ : we have a choice for how we fit @xmath137 to achieve the best predictive power for the lowest computational effort .",
    "we have available to us , for free , the observed values of @xmath144 for the previously chosen objects @xmath125 for previous times",
    "@xmath145 with a different set of observed objects @xmath146 .",
    "this acts as a ` poor mans cross validation ' . we can either :    1 .",
    "use only the observed history ; 2 .",
    "use only cross - validated estimates from the current observations @xmath36 , by treating each object @xmath147 as if it were the last object to be evaluated ; 3 .   combine the approaches by retaining any cross - validation performed at previous iterations and combining it appropriately with cross - validation at iteration @xmath36 .    using",
    "only the observed history will cause problems when there are few evaluated objects .",
    "additionally we might expect ` non - stationarity ' in that early objects are measured in a different space and hence follow a rather different distribution to later objects .",
    "conversely , using only cross - validation at the current iteration is computationally costly , does not account for any time - dependent learning , and exposes only a limited set of distances .",
    "combining the approaches would seem to be appropriate but requires choosing a way to ` age - off ' less accurate information from early in the process .",
    "additionally , we might wish to choose how much cross - validation to perform ; intuitively , less is needed later in the process .",
    "we therefore define :    * * a cross - validation operator * @xmath148 which decides which historical objects to retain",
    ".    by default we use @xmath149 which uses all the history up to time @xmath36 ( excluding the first 5 ) .",
    "we have an additional choice about how to handle the additional information about @xmath150 when @xmath151 if @xmath152 is known to be symmetric .",
    "first , we can set @xmath153 and use the above framework .",
    "second , we can use its inferred value to calibrate @xmath137 , estimating @xmath154 instead of the regression described above .",
    "we have not investigated these options in detail , instead treating all similarities as non - symmetric .",
    "we can exploit the emulator as defined to make intelligent decisions about which objects and/or matrix elements to evaluate using the choice function @xmath155 .",
    "the loss depends on the emulated matrix @xmath156 via the evaluated similarity elements @xmath22 .",
    "it also depends on the computation spent .",
    "let @xmath157 be the computational cost for computing element @xmath1 .",
    "then for simplicity we consider losses of the form @xmath158 where @xmath159 is the ` model fit ' loss , and @xmath160 is the ` computational cost ' loss .",
    "this simplification means that we can minimise each loss separately and easily combine them .",
    "if we further assume that @xmath161 for all pairs , then the computational cost of any proposal containing the same number of elements is the same . for notational simplicity",
    "we therefore omit the subscript @xmath159 below .    _",
    "loss for model fit _ :",
    "a fairly general loss is a ( weighted ) mean prediction error of the form : @xmath162 where the weights @xmath163 allow us to specify which elements are most important for a particular problem , and can allow for covariance .",
    "we consider the @xmath134 norm of the difference between the true and observed values of the matrix .",
    "we do not have access to the true value of @xmath1 at any iteration , but the emulator provides a calibrated way to estimate the expectation for decisions .",
    "therefore , although we do not have access to the absolute value of the loss , we can estimate the difference in loss between decisions with high accuracy , and hence make good decisions .",
    "it is helpful to distinguish between two classes of evaluation that we use :    * * global search : * to identify bulk cluster structure , propose to evaluate an entire row of @xmath59 , setting @xmath164 .",
    "evaluate all similarities @xmath1 of an object @xmath23 with all other objects @xmath74 . * * local search : * to identify local neighbourhood structure , propose to evaluate a single element @xmath103 with value @xmath1 .",
    "this is given the notation @xmath165 , and if chosen , @xmath62 now includes this element .",
    "i.e. @xmath36 is _ not _ incremented .      if we were to evaluate all similarities with object @xmath23 , then @xmath164",
    "this has two effects on the loss .",
    "first , it evaluates @xmath166 to be @xmath1 , and second , it can be used to improve predictions of other objects via the emulator .",
    "evaluating the improvement of the loss in principle requires integration over all assignments of @xmath166 .    however",
    ", we can obtain a good estimate of @xmath39 for significantly less compute by ` plugging - in ' the expected value @xmath110 .",
    "we therefore assume that a given row will be the mean predicted value , and predict the change in loss for the rest of the matrix . therefore we can write the loss and therefore the optimum choice @xmath167 as @xmath168 where @xmath137 for observed rows is defined to be zero .",
    "notice that this loss depends directly on the emulator variance only .",
    "it depends on the emulator predictions via their interaction with the variance ( and potentially via the weights ) .",
    "the expectation in equation [ eqn : lossprediction ] is easily evaluated by estimating @xmath169 via the same regression from equation [ eqn : deltaused ] , with the set of objects @xmath37 .",
    "we define one loss and two heuristics via this framework .    * _ rmse loss _ : the @xmath170 norm is a natural measure and can be used to directly minimise the root - mean - square error ( rmse ) in the prediction .",
    "this is a matrix completion approach .",
    "this loss can be computed by brute force in @xmath171 by evaluating the effect of each observation @xmath23 on the corresponding variance @xmath172 . * _ furthest distance heuristic _ : if we use uniform weights and the @xmath173 norm for the loss , then the largest value of @xmath109 dominates .",
    "the loss is minimized by evaluating the object that is furthest away from all evaluated objects .",
    "this heuristic is inspired by the nature of the space from section [ sec : dds ] .",
    "* _ random selection heuristic _ : whilst not strictly available under the loss framework , if all weights are zero the loss is uniform and we can select at random .",
    "this is an important benchmark .    in practice ,",
    "evaluating the full emulated rmse loss function is too costly .",
    "it is useful instead to use a monte - carlo estimate of it .",
    "we estimate the loss using a constant number @xmath174 of random unevaluated objects ( with the addition of one chosen by the furthest distance heuristic ) .",
    "we then estimate the loss using those samples only , reducing the computation from @xmath171 to @xmath175 .",
    "local search permits evaluation of elements without calculating an entire row .",
    "however , our emulation framework can not exploit this information without additional complication .",
    "we therefore allow local search to only influence the loss at specific values of @xmath103 and omit any learning that this point provides about the rest of the matrix . for some loss functions",
    ", however , it is extremely important to calculate certain matrix elements at the omission of others and therefore these choices might be applicable .",
    "we can choose local proposals using the same loss function as in the global case , and indeed decide between choices on the basis of this loss .",
    "in this case we need to consider @xmath160 in the loss . in practice",
    "we have used a linear function @xmath176 . in this case",
    "the cost of a local proposal is @xmath177 and the cost of a global proposal is @xmath178 .",
    "the factor @xmath179 is chosen to convert computational time units to ` loss per iteration ' units .",
    "evaluation and assessment are straightforward .",
    "the matrix elements are evaluated , and compared to the predictive distribution .",
    "the predicted mean of the emulator @xmath105 is compared to the observed values @xmath1 using its rmse : @xmath180 the predicted emulator variance @xmath118 of the emulator is also compared to the empirical residual variance @xmath181 using its error : @xmath182 this is much noisier since we only obtain one error per object observed .    in both cases ,",
    "it is of value to determine convergence which could be used in the termination criteria .",
    "there are standard test statistics that could be used .",
    "we have not tried to terminate the algorithm on this basis because the loss functions we have used do not converge , but instead slow down .",
    "other loss functions may converge , however .",
    "there are three fundamental ways of running the algorithm , which can be defined using an appropriate computational loss term @xmath160 .",
    "termination can be seen as an option that minimises the loss @xmath35 , when all possible actions result in a decrease in loss .    1 .",
    "obtain the minimal loss for a fixed computation .",
    "we have seen in section [ sec : simple ] that it is helpful to think about the total number of evaluations that will be available .",
    "however , most calculations are not this structured and we can run our algorithm until a computational budget is reached .",
    "the corresponding computational loss @xmath160 takes an arbitrary function up to the budget @xmath183 when it is set to @xmath184 .",
    "2 .   obtain the cheapest estimate with a fixed precision . if a desired precision of the loss is known _ apriori _ , then we can use standard results from sequential stopping rule theory @xcite under the assumption that the loss has normally distributed error .",
    "again , @xmath160 takes a step to @xmath184 but now when the precision is reached .",
    "3 .   use the full loss @xmath160 .",
    "we would then be able to explore the cost / benefit ratio of obtaining further similarities by making further computations .",
    "we have here only worked with criterion 1 , i.e. we compare the performance for a given amount of compute .",
    "further , in the simulated example , the cost of each computation @xmath51 is arbitrary and so we do not compute the real computation performed but estimate it to a factor of @xmath51 .",
    "we illustrate our methodology with application to a simulated dataset .",
    "@xmath185 gaussian features were simulated by assuming a tree correlation structure for @xmath186 samples in @xmath187 evenly sized clusters .",
    "this type of dataset is representative of many real world applications , including genetics .",
    "it is also challenging for standard matrix completion approaches because the eigenvalues correspond to the clusters @xcite , which might not be informative about all samples .",
    "this assumption is required for matrix completion to have performance guarantees @xcite .",
    "figure [ fig : simdata ] illustrates one such matrix .    to assess the robustness and performance characteristics of the algorithm , we vary the difficulty of the problem . for this",
    "we vary the correlation within and between clusters , and allow outliers of varying sizes .",
    "table [ tab : simpar ] describes the parameters used for the datasets we generated , which lie on a continuum from ` clustered ' in which inference is simple , to ` corrupted ' which contains many outliers and has weaker correlation structure .",
    ".simulation dataset parameters for the correlation between samples .",
    "there are 10 clusters , with close clusters being ( 1 - 2,3 - 4,5 - 6,7 - 8,9 - 10 ) , and distant clusters are ( 1 - 4,5 - 8 ) .",
    "outliers are generated by mixing rows with weight @xmath188 with a ` self ' direction ( 1 on the diagonal , 0 off it ) .",
    "the weights @xmath189 are biased to @xmath190 and are exactly @xmath190 when @xmath191 .",
    "11 datasets are generated , evenly space from the ` clustered ' to ` corrupted ' parameters ( referred to as a difficulty scale ) .",
    "[ tab : simpar ] [ cols=\"^,<,^,^\",options=\"header \" , ]     these arguments are summarised in table [ tab : scaling ] , with figure [ fig : scaling]a - b confirming the results . we show the computational cost @xmath192 of using various decision models as a function of either the total number of objects @xmath0 or the number to be evaluated @xmath18 .",
    "we have constructed the code to make evaluation of @xmath1 approximately negligible so that the scaling of the decision framework dominates .",
    "hence , random decisions take only the amount of time required to define the memory for the data , to copy it , and maintain state ( all models share this cost ) .",
    "the important criterion is the total compute @xmath193 , with the cost of evaluating the matrix elements being @xmath194 .",
    "hence is it vital to contrast the decision making cost to the calculation cost .",
    "figure [ fig : scaling]c - f show several scenarios of calculation cost whilst varying the problem difficulty .",
    "this leads to different optimal choices of decision strategy . assuming a problem size of @xmath195 , we consider two costs ( measured in seconds ) per similarity evaluation ; @xmath196 ( scenarios 1 - 2 ) and @xmath197 ( scenarios 3 - 4 ) .",
    "the first results in a final cost that is larger the most expensive ( rmse with a mixture model ) decision cost by a factor of 5 , whereas the second is within the range of computational budgets . by considering the rmse curves from figure [ fig : predictivepower ]",
    ", we can create curves comparing total computational cost to final rmse , regardless of @xmath18 . by construction , this leads to a varying optimal decision ; when @xmath51 is large , it pays to use the best model for most values of computational cost .",
    "when @xmath51 is small , it is optimal to either use a cheap heuristic ( furthest neighbour ) or random samples , depending on their performance in the problem .",
    "[ ht ]     ( @xmath198 ) for four configurations of our algorithm .",
    "these are the random choice heuristic , the furthest distance heuristic , and the rmse loss using either the mixture model or nearest neighbour emulators .",
    "b ) shows the same results for @xmath18 ( at @xmath195 ) .",
    "note the different scales between the top and bottom plots .",
    "c - f show rmse as a function of computational time ( note the logarithmic x and y axis ) .",
    "this is under different hypothetical scenarios about computational cost and model efficiency ( again assuming @xmath195 ) .",
    "scenarios 1 and 2 assume that @xmath199 seconds per matrix element computation , whereas scenarios 3 and 4 assume that @xmath197 .",
    "scenarios 1 and 3 use the @xmath200 curves from ` dataset  1 ' ( clustered ) as shown in figure  [ fig : predictivepower ] , whereas scenarios 2 and 4 use ` dataset  11 ' ( corrupted ) .",
    "low rmse for a given computational cost is desirable ; the scenarios have been constructed to illustrate different optimal choices .",
    "[ fig : scaling ] , scaledwidth=100.0% ]    figure [ fig : scaling]a - b shows that our approach does indeed scale well , with our implementation in r handling large matrices ( @xmath195 has @xmath201 million elements ) in completely acceptable times .",
    "the practical limitation is that we hope to make good decisions , which for large matrices would need heuristics that can be performed in @xmath202 time or less .",
    "performing careful calculations on an @xmath203 fraction of the matrix will not be effective on general similarity matrices ; we need a set of heuristics to pull out the most useful objects to consider .",
    "prior information , in the form of covariates or labels , could be very valuable for this .",
    "@xcite use such ` side information ' in a standard matrix completion framework .",
    "these computational scaling results do not include matrix completion of the final set of objects .",
    "this can be performed with any set of chosen objects using any available emulator .",
    "the cost of this is @xmath204 for nearest neighbour and @xmath205 for the mixture model .",
    "figure [ fig : scaling]c - f illustrate several scenarios for which the different computational costs @xmath51 are accounted for . if @xmath51 is moderate ( scenarios 1 - 2 , costing @xmath199 seconds per matrix element computation ) then the rmse loss is best .",
    "this is universally true for difficult problems whilst the furthest distance criterion is better for small budgets in the clustered dataset , when not much of the matrix is evaluated .",
    "conversely , if @xmath51 is very small ( scenarios 3 - 4 with @xmath197 seconds ) , heuristics are necessary since computing matrix elements is as cheap as emulating them . in scenario 3 with low @xmath51 and clustered dataset ,",
    "the furthest distance heuristic is unbeatable , whereas in scenario 4 the corrupted dataset means that the random choice heuristic is better for moderate computational budget .",
    "an important application in statistical genetics is the understanding of the relationship between individuals , and the overall structure of that relationship .",
    "the process generating genetics data is very well understood - a generative model called the ancestral recombination graph ( arg , @xcite ) operates within populations . between populations",
    ", migration occurs at varying rates over time @xcite . because of this",
    ", model - based inference is strongly preferred by this community . however , the true underlying genetic model is prohibitively computationally costly for all but completely unrealistically small and simple samples .",
    "therefore much effort goes into the development of approximation procedures that capture the key features of the arg for a manageable computational cost .",
    "we are interested in a population assignment problem @xcite , for which the likelihood for population assignment can be shown to approximately follow a similarity matrix .",
    "it is therefore of great importance to approximate this matrix .",
    "fundamentally , the difficulty is that computing the similarity matrix cost @xmath206 with @xmath51 taking values up to around 20 million for whole genome resequencing data .",
    "the number of individuals sampled @xmath0 is also growing increasingly large .",
    "@xmath207 is becoming standard ( the first being the 1000 genome project @xcite , now with 2500 comparable whole genomes ) , whilst @xmath195 lower density samples are in existence ( e.g. @xcite ) , and larger datasets are on their way . computing a row of similarities for this matrix takes days of computation time , and although parallelization is possible , most genetics departments do not have access to the 1000 + core compute farms needed to avoid serial computation .",
    "thus , there is a strong motivation to reduce the computation of these methods , whilst the fundamental computation can not be changed .",
    "since @xmath208 and @xmath51 are of the same order , we will use an @xmath209 implementation of the algorithm .",
    "we apply the method to the well understood human genome diversity panel dataset formed of @xmath210 individuals from across the globe and @xmath211 single nucleotide polymorphism ( snp ) data points per individual .",
    "this is large enough to be difficult to work with but well within the scope of institutional clusters , and was explored in the original @xcite paper . our goal here is to recreate the essential features of the matrix at a fraction of the computational cost .",
    "the true rank of this matrix is at least 226 as this is the number of populations identified using full model based inference .",
    "this problem contains all of the difficulties of the ` hard ' simulated data and several more : a ) it is not symmetric ; b ) some individuals are much less similar to any individuals than others ; c ) it is high rank ; d ) clusters are of all sizes .",
    "we first converted the genetic similarity to a distance by taking the negative log and adding the new minimum value .",
    "we then applied the rmse choice criterion with the mixture model emulator using cross - validation to estimate @xmath212 .",
    "up to 100 samples were used to recover the matrix .",
    "the performance is shown in figure [ fig : genetics ] , compared to both the random choice heuristic and a new ` prior heuristic ' .",
    "this exploits prior information about populations using self - declared ethnicities . to do this we sampled one individual from each population , and",
    "then a second individual from each population , etc , with both populations and individuals within those populations being defined at random .",
    "[ ht ]     , scaledwidth=100.0% ]",
    "massive data sets present new challenges for statistical reasoning .",
    "the sheer volume of data prevents the routine deployment of even straightforward statistical techniques . thus a different paradigm is required .",
    "our proposed framework attacks this problem using statistical reasoning to construct a sequential data selection paradigm . at core",
    ", this framework acknowledges that it is impossible to complete the statistical evaluation on all data , and instead seeks to find the most useful subset in a sequential manner .",
    "this is most suited to problems in which correlation prevents the use of simple sampling , such as the ar model example in section [ sec : simple ] , or the central example of similarity matrices .",
    "the framework combines elements of active learning , modelling , and sequential decision making . at a high - level ,",
    "the framework features a statistical emulator ( for cheaply estimating unknown data values ) , a loss , and a choice operator ( for selecting among candidates to determine the next computation ) .",
    "we have provided a number of choices for each , which have different capabilities and computational costs . in the case of similarity matrices , understanding the latent subspace is usually advantageous . however , with massive data this task is difficulty .",
    "hence we have introduced data directional scaling  a method of determining important directions in similarity space itself .",
    "this approach allows a rapid characterisation of the main directions without appeal to a costly latent variable model .    whilst we explored many advantages of data directional scaling",
    ", the key disadvantage is that the dimension of the space grows with the number of samples .",
    "it would be fairly simple to extend our decision framework to ` discard ' directions of the data that are not helpful for prediction .",
    "a more sophisticated approach is to perform online clustering to create pseudo - directions via k - means @xcite or the more rapid k - medians @xcite .",
    "this step brings with it a potential predictive performance drop , since we provide the emulator with less information , and would also need to solve the serious problem of estimating @xmath213 .",
    "further , we can not compute @xmath214 for these pseudo - objects although we can still use them in the emulator .",
    "hence we have omitted it from this work , although addressing these concerns is vital to replace the computational scaling factor @xmath18 by @xmath213 in our emulator steps .",
    "we have provided examples in which our approach is to be preferred over matrix completion approaches .",
    "this preference is not merely empirical .",
    "matrix completion may still require an intractable computation at large enough data scale , hence precluding its use .",
    "worse , the model assumptions of matrix completion may not match the characteristics of many similarity matrices , in which cluster structure is often expected . assessing empirical performance",
    "is complicated since we have to handle issues of computational cost , choice function etc . for the type of challenging problem with which we are concerned , where @xmath51 and @xmath0 are large , it is important to compare against the benchmark of random selection . in such cases ,",
    "empirical results show that the framework is far superior to random selection .",
    "further development of this framework will follow in two directions . on the abstract side , we seek some theoretical guarantees on the behaviour of the framework .",
    "moreover , refinements of the details of the framework , emulator and choice operator , should lead to performance enhancements . on",
    "the computational side , extension of the approach to parallel and cloud systems is of great interest .",
    "the big data we are interested in will usually reside on a distributed storing system , such as hadoop .",
    "bayarri , m. , j.  o. berger , m.  c. kennedy , a.  kottas , r.  paulo , j.  sacks , j.  a. cafeo , c .- h .",
    "lin , and j.  tu , 2009  predicting vehicle crashworthiness : validation of computer models for functional and hierarchical data .",
    "journal of the american statistical association  _",
    "104_(487 ) .            bijak , j. , j.  hilton , e.  silverman , and v.  d. cao , 2013  reforging the wedding ring : exploring a semi - artificial model of population for the united kingdom with gaussian process emulators .",
    "demographic research  * 29*.      busby , d. and m.  feraille , 2008  adaptive design of experiments for calibration of complex simulators  an application to uncertainty quantification of a mature oil field . in _ journal of physics : conference series _ ,",
    "volume 135 , pp .",
    "iop publishing .",
    "hao , n. , l.  horesh , and m.  kilmer , 2014  nuclear norm optimization and its application to observation model specification . in a.  y. carmi , l.  mihaylova , and s.  j. godsill ( eds . ) , _ compressed sensing & sparse filtering _ , signals and communication technology , pp .",
    "springer berlin heidelberg .",
    "monteiro , r.  d. , i.  adler , and m.  g. resende , 1990  a polynomial - time primal - dual affine scaling algorithm for linear and convex quadratic programming and its power series extension .",
    "mathematics of operations research  _",
    "15_(2 ) * : *  191214 .      ripke , s. , n.  r. wray , c.  m. lewis , s.  p. hamilton , m.  m. weissman , g.  breen , e.  m. byrne , d.  h. blackwood , d.  i. boomsma , s.  cichon , and others , 2013  a mega - analysis of genome - wide association studies for major depressive disorder .",
    "molecular psychiatry  _",
    "18_(4 ) * : *  497511 .",
    "rougier , j. , d.  m. sexton , j.  m. murphy , and d.  stainforth , 2009  analyzing the climate sensitivity of the hadsm3 climate model using ensembles from different but related experiments . journal of climate  _ 22_(13 ) .",
    "simmonds , r. , r.  bradford , and b.  unger , 2000   applying parallel discrete event simulation to network emulation . in _ proceedings of the fourteenth workshop on parallel and distributed simulation _",
    ", pp .   1522 .",
    "ieee computer society .",
    "thompson , j.  d. , d.  g. higgins , and t.  j. gibson , 1994  clustal w : improving the sensitivity of progressive multiple sequence alignment through sequence weighting , position - specific gap penalties and weight matrix choice .",
    "nucleic acids research  _",
    "22_(22 ) * : *  46734680 .",
    "xu , m. , r.  jin , and z .- h .",
    "zhou , 2013  speedup matrix completion with side information : application to multi - label learning .",
    "in c.  burges , l.  bottou , m.  welling , z.  ghahramani , and k.  weinberger ( eds . ) , _ advances in neural information processing systems 26 _ , pp ."
  ],
  "abstract_text": [
    "<S> as datasets grow it becomes infeasible to process them completely with a desired model . for giant datasets , </S>",
    "<S> we frame the order in which computation is performed as a decision problem . </S>",
    "<S> the order is designed so that partial computations are of value and early stopping yields useful results . </S>",
    "<S> our approach comprises two related tools : a decision framework to choose the order to perform computations , and an emulation framework to enable estimation of the unevaluated computations . </S>",
    "<S> the approach is applied to the problem of computing similarity matrices , for which the cost of computation grows quadratically with the number of objects . </S>",
    "<S> reasoning about similarities before they are observed introduces difficulties as there is no natural space and hence comparisons are difficult . </S>",
    "<S> we solve this by introducing a computationally convenient form of multidimensional scaling we call ` data directional scaling ' . </S>",
    "<S> high quality estimation is possible with massively reduced computation from the naive approach , and can be scaled to very large matrices . </S>",
    "<S> the approach is applied to the practical problem of assessing genetic similarity in population genetics . </S>",
    "<S> the use of statistical reasoning in decision making for large scale problems promises to be an important tool in applying statistical methodology to big data . </S>"
  ]
}