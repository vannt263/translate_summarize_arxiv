{
  "article_text": [
    "exploiting the fact that a signal @xmath0 has a sparse representation over some dictionary @xmath1 is the backbone of many successful signal reconstruction and data analysis algorithms . having a sparse representation means that @xmath2 is the linear combination of only a few columns of @xmath3 , referred to as _",
    "atoms_. formally , this reads as @xmath4 where the transform coefficient vector @xmath5 is _ sparse _ , most of its entries are zero or small in magnitude . for the performance of algorithms exploiting this model , it is crucial to find a dictionary that allows the signal of interest to be represented most accurately with a coefficient vector @xmath6 that is as sparse as possible .",
    "basically , dictionaries can be assigned to two classes : _ analytic dictionaries _ and _ learned dictionaries_. analytic dictionaries are built on mathematical models of a general type of signal they should represent . they can be used universally and allow a fast implementation .",
    "popular examples include wavelets @xcite , bandlets @xcite , and curvlets @xcite among several others .",
    "it is well known that learned dictionaries yield a sparser representation than analytic ones .",
    "given a set of representative training signals , dictionary learning algorithms aim at finding the dictionary over which the training set admits a maximally sparse representation .",
    "formally , let @xmath7 \\in { \\mathbb{r}^{n \\times m}}$ ] be the matrix containing the @xmath8 training samples arranged as its columns , and let @xmath9 \\in { \\mathbb{r}^{d \\times m}}$ ] contain the corresponding @xmath8 sparse transform coefficient vectors , then learning a dictionary can be stated as the minimization problem @xmath10 therein , @xmath11 is a function that promotes sparsity , @xmath12 reflects the noise power , and @xmath13 is some predefined admissible set of solutions .",
    "common dictionary learning approaches employing optimization problems related to include probabilistic ones like @xcite , and clustering based ones such as k - svd @xcite , see @xcite for a more comprehensive overview .",
    "the dictionaries produced by these techniques are unstructured matrices that allow highly sparse representations of the signals of interest . however , the dimension of the signals which are sparsely represented and , consequently , the possible dictionaries dimensions are inherently restricted by limited memory and limited computational resources . furthermore ,",
    "when used within signal reconstruction algorithms where many matrix vector multiplications have to be performed , those dictionaries are computationally expensive to apply .    in this paper",
    ", we present a method for learning dictionaries that are efficiently applicable in reconstruction tasks .",
    "the crucial idea is to allow the dictionary to have a separable structure , where separable means that the dictionary @xmath3 is given by the kronecker product of two smaller dictionaries @xmath14 and @xmath15 , @xmath16 the relation between a signal @xmath17 and its sparse representation @xmath18 as given in is accordingly @xmath19 where the vector space isomorphism @xmath20 is defined as the operation that stacks the columns on top of each other . employing this separable structure instead of a full , unstructured dictionary clearly reduces the computational costs of both the learning algorithm and the reconstruction tasks .",
    "more precisely , for a separation with @xmath21 , the computational burden reduces from @xmath22 to @xmath23",
    ". we will refer to this new learning approach as _ sedil _ ( separable dictionary learning ) .",
    "it is apparent that this approach applies in principle to any class of signals .",
    "however , we will focus on signals that have an inherently two dimensional structure such as images .",
    "however , it is worth mentioning that sedil can straightforwardly be extended to signals with higher dimensional structure , such as volumetric @xmath24-signals , by employing multiple kronecker products . to fix the notation for the rest of this work ,",
    "if @xmath25 and @xmath26 are as above , the two dimensional signal @xmath27 has the sparse representation @xmath28 , @xmath29 .",
    "the proposed dictionary learning scheme sedil is based on an adaption of problem to a product of unit spheres .",
    "furthermore , it incorporates a regularization term that allows to control the dictionary s mutual coherence .",
    "the arising optimization problem is solved by a riemannian conjugate gradient method combined with a nonmonotone line search . for the general separable case ,",
    "the method is able to learn dictionaries for large patch dimensions where conventional learning techniques fail while if we define @xmath30 sedil yields a new algorithm for learning standard unstructured dictionaries .",
    "a denoising experiment is given that shows the performance of both a separable and a non - separable dictionary learned by sedil on @xmath31-dimensional image patches . from this experiment",
    "it can be seen that the separable dictionary outperforms its analytic counterpart , the overcomplete discrete cosine transform , and the non - separable one achieves similar performance as state - of - the - art learning methods like k - svd . besides that , to show that a learned separable dictionary is able to extract and to recover the global information contained in the training data , a separable dictionary is learned on a face database with each face image having a resolution of @xmath32 pixels .",
    "this dictionary is then applied in a face inpainting experiment where large missing regions are recovered solely based on the information contained in the dictionary .",
    "instead of learning dense unstructured dictionaries , which are costly to apply in reconstruction tasks and are unable to deal with high dimensional signals , techniques exist that aim at learning dictionaries which bypass these limitations . in the following ,",
    "we shortly review some existing techniques that focus on learning efficiently applicable and high dimensional dictionaries , followed by introducing our approach .      in @xcite and @xcite ,",
    "two different algorithms have been proposed following the same idea of finding a dictionary such that the atoms themselves are sparse over some fixed analytic base dictionary .",
    "the algorithm proposed in @xcite enforces each atom to have a fixed number of non - zero coefficients , while the one suggested in @xcite imposes a less restrictive constraint by enforcing sparsity over the entire dictionary .",
    "however , both algorithms employ optimization problems that are not capable of finding a large dictionary for high dimensional signals . in @xcite",
    "an alternative structure for dictionaries has been proposed .",
    "the so called signature dictionary is a small image itself , where every patch at varying locations and size is a possible dictionary atom .",
    "the advantages of this structure include near - translation - invariance , reduced overfitting , and less memory and computational requirements , compared to unstructured dictionary approaches .",
    "however , the small number of parameters in this model also makes this dictionary more restrictive than other structures .",
    "this approach has been further extended in @xcite to learn real translational - invariant atoms .",
    "hierarchical frameworks for tackling high dimensional dictionary learning are presented in @xcite and @xcite .",
    "the latter work uses this framework in conjunction with a screening technique and random projections .",
    "we like to mention that our approach has the potential to be combined with hierarchical frameworks",
    ".      we aim at learning a separable dictionary @xmath33 from a given set of training samples @xmath34 by solving a problem related to .",
    "we denote the collection of the @xmath8 sparse representations by @xmath35 and measure its overall sparsity via @xmath36 where @xmath37 is the @xmath38-entry of @xmath39 and @xmath40 is a weighting factor .",
    "we impose the following regularization on the dictionary .",
    "a.   the _ columns _ of @xmath3 have unit euclidean norm .",
    "b.   the _ coherence _ of @xmath3 shall be moderate .    constraint ( i )",
    "is commonly employed in various dictionary learning procedures to avoid the scale ambiguity problem , the entries of @xmath3 tend to infinity , while the entries of @xmath41 tend to zero as this is the global minimizer of the unconstrained sparsity measure @xmath42 .",
    "matrices with normalized columns admit a manifold structure , known as the product of spheres , which we denote by @xmath43 here , @xmath44 forms a diagonal matrix with the diagonal entries of the square matrix @xmath45 , and @xmath46 is the @xmath47-identity matrix .",
    "consequently , we require that @xmath25 is an element of @xmath48 and that @xmath26 is an element of @xmath49 .",
    "the soft constraint ( ii ) of requiring a moderate mutual coherence of the dictionary is a well known regularization procedure in dictionary learning , and is motivated by the compressive sensing theory . roughly speaking",
    ", the mutual coherence of @xmath3 measures the similarity between the dictionary s atoms , or ,  _ a value that exposes the dictionary s vulnerability , as [ ... ] two closely related columns may confuse any pursuit technique . _  @xcite .",
    "the most common mutual coherence measure for a dictionary @xmath3 with normalized columns @xmath50 is @xmath51 for the rest of this paper we will follow this notation and denote the @xmath52 column of a matrix @xmath53 by the corresponding lower case character @xmath54 . in order to relax this worst case measure ,",
    "other measures have been introduced in the literature that are more suited for practical purpose , for example averaging the largest entries of @xmath55 as in @xcite , or by considering the sum of squares of all elements in @xmath55 , cf .",
    "@xcite . in this work",
    ", we introduce an alternative mutual coherence measure , which has been proven extremely useful in our experiments .",
    "explicitly , we measure the mutual coherence via @xmath56 since this measure is differentiable , it can be integrated into smooth optimization procedures . furthermore",
    ", when it is used within a dictionary learning scheme , the log - barrier function avoids the algorithm from producing dictionaries that contain repeated identical atoms .",
    "note that minimizing @xmath57 implicitly influences @xmath58 .",
    "concretely , the relation between and the classical mutual coherence is @xmath59 with @xmath60 denoting the number of summands of . to see the validity of the above equation , note that since the atoms @xmath61 are normalized to one , the equation @xmath62 holds due to the cauchy - schwarz inequality",
    "thus , all summands @xmath63 are non - negative .",
    "moreover , @xmath64 and therefore @xmath65 which implies equation . in order to exploit this relation for the separable case",
    "we first consider the following lemma .",
    "* lemma 1 . * _ the mutual coherence of the kronecker product of two matrices @xmath25 and @xmath26 with normalized columns is equal to the maximum of the individual mutual coherences , @xmath66 _    _ proof . _",
    "first , notice that since the columns of @xmath25 and @xmath26 all have unit norm , the diagonal entries of both @xmath67 and @xmath68 are equal to one and that the mutual coherence @xmath69 and @xmath70 is given by largest off - diagonal absolute value of @xmath67 and @xmath68 , respectively .",
    "analogously , @xmath71 is just the largest off - diagonal absolute value of the matrix @xmath72 . due to",
    "the definition of the kronecker product and the unit diagonal , each entry of @xmath73 and @xmath67 reappears in the off - diagonal entries of @xmath74 .",
    "this yields the two inequalities @xmath75 and @xmath76 , which can be combined to @xmath77    on the other hand , each entry of @xmath78 is a product of entries of @xmath73 and @xmath67 .",
    "this explicitly means that we can write @xmath79 with @xmath80 and @xmath81 being entries of @xmath73 and @xmath67 , respectively .",
    "since we have @xmath82 , this provides the two inequalities @xmath83 and @xmath84 , and hence @xmath85 combining and provides the desired result.@xmath86    substituting @xmath71 into equation and then applying lemma 1 yields @xmath87 due to the monotone behavior of the logarithm .",
    "therefore , if @xmath88 is small , @xmath89 is bounded as well",
    ". now , in order to keep the mutual coherence of @xmath90 moderate , we use the relation @xmath91 for some positive constants @xmath92 and minimize the sum @xmath93 instead of @xmath88 for computational convenience .",
    "finally , putting all the collected ingredients together , to learn a separable dictionary our goal is to minimize @xmath94 therein , @xmath95 weighs between the sparsity of @xmath96 and how accurately @xmath97 reproduces the training samples . using this parameter",
    ", sedil can handle both perfect noise free training data as well as noisy training data .",
    "the second weighting factor @xmath98 controls the mutual coherence of the learned dictionary .",
    "knowing that the feasible set of solutions to problem is restricted to a smooth manifold allows us to apply methods from the field of geometric optimization to learn the dictionary . to provide the necessary notation , we shortly recall the required concepts of optimization on matrix manifolds . for an in - depth introduction on optimization on matrix manifolds ,",
    "we refer the interested reader to @xcite .",
    "let @xmath99 be a smooth riemannian submanifold of some euclidean space , and let @xmath100 be a differentiable cost function .",
    "we consider the problem of finding @xmath101 to every point @xmath102 one can assign a tangent space @xmath103 , which is a real vector space containing all possible directions that tangentially pass through @xmath104 .",
    "an element @xmath105 is called a tangent vector at @xmath104 .",
    "each tangent space is associated with an inner product inherited from the surrounding euclidean space which we denote by @xmath106 and the corresponding norm by @xmath107 .",
    "the riemannian gradient of @xmath108 at @xmath104 is an element of the tangent space @xmath103 that points in the direction of steepest ascent of the cost function on the manifold . for the case where @xmath108 is globally defined on the entire surrounding euclidean space ,",
    "the riemannian gradient @xmath109 is simply the orthogonal projection of the ( standard ) gradient @xmath110 onto the tangent space @xmath103 , which reads as @xmath111 a _ geodesic _ is a smooth curve @xmath112 emanating from @xmath104 in the direction of @xmath105 , which locally describes the shortest path between two points on @xmath99 .",
    "intuitively , it can be interpreted as the generalization of a straight line to a manifold .",
    "the riemannian exponential mapping , which maps a point from the tangent space to the manifold , is defined as @xmath113 the geometric optimization method proposed in this work is based on iterating the following line search scheme .",
    "given the iterate @xmath114 , a search direction @xmath115 , and the step size @xmath116 at the @xmath117 iteration , the new iterate lying on @xmath99 is found via @xmath118 following the geodesic emanating from @xmath114 in the search direction @xmath119 for the length @xmath120 . in the following , we concretize the above concepts for the situation at hand and present all ingredients that are necessary to implement the proposed geometric dictionary learning method .",
    "the given formulas regarding the geometry of @xmath121 are derived e.g.  in @xcite .",
    "here we are considering the product manifold @xmath122 , which is a riemannian submanifold of @xmath123 , and an element of @xmath99 is denoted by @xmath124 .",
    "the tangent space at @xmath125 is given by @xmath126 and the orthogonal projection of some matrix @xmath127 onto the tangent space reads as @xmath128 due to the product structure of @xmath99 , the tangent space of @xmath99 at a point @xmath102 is simply the product of all individual tangent spaces , @xmath129 .",
    "consequently , in accordance with equation the orthogonal projection of some arbitrary point @xmath130 onto the tangent space @xmath103 is @xmath131 each tangent space of @xmath99 is endowed with the riemannian metric inherited from the surrounding euclidean space , which for two points @xmath132 and @xmath133 is given by @xmath134 the final required ingredient is a way to compute geodesics . while in general there is no closed form solution to the problem of finding a certain geodesic , the case at hand allows for an efficient implementation .",
    "let @xmath135 be a point on a sphere and @xmath136 be a tangent vector at @xmath137 , then the geodesic in the direction of @xmath138 is a great circle @xmath139 using this , the geodesic through @xmath140 in the direction of @xmath141 is simply the combination of the great circles emerging from each column of @xmath3 in the direction of the corresponding column of @xmath142 , @xmath143.\\ ] ] now , let @xmath144 be a given search direction .",
    "due to the product structure of @xmath99 a geodesic on @xmath99 is given by @xmath145 the shorthand notation @xmath146 will be used throughout the rest of this paper to denote the riemannian gradient at the @xmath117 iterate .",
    "to solve optimization problem , we employ a geometric conjugate gradient ( cg ) method , as it offers superlinear rate of convergence , while still being applicable to large scale optimization problems with acceptable computational complexity .",
    "therein , the initial search direction is equal to the negative riemannian gradient , @xmath147 . in the subsequent iterations ,",
    "@xmath148 is a linear combination of the gradient @xmath149 and the previous search direction @xmath119 .",
    "since addition of vectors from different tangent spaces is not defined , we need to map @xmath119 from @xmath150 to @xmath151 .",
    "this is done by the so - called parallel transport @xmath152 , which transports a tangent vector @xmath153 along the geodesic @xmath154 to the tangent space @xmath151 .",
    "similar to the way we derived a closed form solution for the geodesic , we consider the geometry of @xmath121 at first .",
    "the parallel transport of a tangent vector @xmath155 along the great circle @xmath156 is @xmath157 and the parallel transport of @xmath158 along the geodesic @xmath159 is given by @xmath160 .",
    "\\end{split}\\ ] ] thus , a tangent vector @xmath161 is transported in the direction of @xmath162 via @xmath163 now , using the shorthand notation @xmath164 , the new search direction is computed by @xmath165 we update @xmath166 following the hybrid optimization scheme which is proposed in @xcite and has shown excellent performance in practice .",
    "the authors combine the hestenes - stiefel ( hs ) and dai - yuan ( dy ) update formulas , which are given by @xmath167 with @xmath168 , to create the hybrid update formula @xmath169    in order to find an appropriate step size @xmath120 , we propose a riemannian adaption of the nonmonotone line search algorithm proposed in @xcite . like other nonmonotone line search schemes it has the potential to improve the likelihood of finding a global minimum as well as to increase the convergence speed , cf .",
    "in contrast to the standard armijo rule and standard nonmonotone schemes , which generally use the function value at the previous iterate or the maximum of the previous @xmath8 iterates , this particular method utilizes a convex combination of all function values at previous iterations .",
    "the pseudo code for a version of this line search scheme that is adapted to our geometric optimization problem can be found in algorithm [ al : nlsa ] .",
    "* input : * @xmath170 @xmath171 * set : * @xmath172 @xmath173 * set : * @xmath174 , + @xmath175 , + @xmath176 * output : * @xmath120 , @xmath177 , @xmath178    the line search is initialized with @xmath179 and @xmath180 .",
    "finally , our complete method of learning a dictionary with separable structure is summarized in algorithm [ al : learning1 ] .",
    "* input : * initial dictionaries @xmath181 , training data @xmath182 , parameters @xmath183 * set : * @xmath184 , @xmath185 , @xmath186 @xmath187 according to algorithm [ al : nlsa ] in conjunction with equation @xmath188 , cf .",
    "@xmath189 , cf .",
    "@xmath190 @xmath191 , cf .  ,",
    "@xmath192 * output : * @xmath193",
    "to show how dictionaries learned via sedil perform in real applications , we present the results achieved for denoising images corrupted by additive white gaussian noise of different standard deviation @xmath194 as a case study . the images and the noise levels chosen here are an excerpt of those commonly used in the literature .",
    "the peak signal - to - noise ratio ( @xmath195 ) between the ground - truth image @xmath196 and the recovered image @xmath197 computed by @xmath198 is used to quantify the reconstruction quality . as an additional quality measure , we use the mean structural similarity index ( @xmath199 ) computed with the same set of parameters as",
    "originally suggested in @xcite .",
    "@xmath199 ranges between zero and one , with one meaning perfect image reconstruction . compared to @xmath195 , the @xmath199 better",
    "reflects the subjective visual impression of quality .    here , we present the denoising performance of both a universal unstructured dictionary , @xmath200 , and a universal separable dictionary @xmath201 , both learned from the same training data using sedil . by universal , we mean that the dictionary is not specifically learned for a certain image class but universally applicable to any image content . without loss of generality",
    "we choose square image patches with @xmath202 , which is in accordance to the patch - sizes mostly used in the literature .",
    "for the unstructured dictionary we set @xmath203 , and for the separable one we choose @xmath204 , @xmath25 and @xmath26 are of equal size and @xmath205 is of the same dimension as its unstructured counterpart . for the training phase",
    ", we extracted @xmath206 image patches from four images at random positions and vectorize them .",
    "of course , these images are not considered further within the performance evaluations .",
    "the training patches were normalized to have zero mean and unit @xmath207-norm .",
    "we initialized @xmath25 and @xmath26 with random matrices with normalized columns .",
    "global convergence to a local minimum has always been observed , regardless of the initialization .",
    "the weighting parameters were empirically set to @xmath208 and @xmath209 .",
    "the resulting atoms of the unstructured dictionary @xmath210 and the separable dictionary @xmath205 are shown in figure [ fig : aop ] and [ fig : aop ] , respectively .",
    "to denoise the images , we first find the sparse representation @xmath211 of each noisy patch @xmath212 over @xmath213 by solving @xmath214 employing the fast iterative shrinkage - thresholding algorithm ( fista ) @xcite .",
    "the regularization parameter @xmath215 depends on the noise level and we set it to @xmath216 .",
    "after that , a clean image patch is computed from the sparse coefficients via @xmath217 .",
    "last , as all overlapping image patches are taken into account , several solutions for the same pixel exist , and the final clean image is built by averaging all overlapping image patches .",
    "all achieved results are given in table [ tb : psnrdenoise ] .    to compare and rank the learned dictionaries among existing state - of - the - art techniques , we present the denoising performance of a universal dictionary @xmath218 learned using k - svd from the same training set as used for sedil and of equal dimension as the unstructured dictionary @xmath210 . from table",
    "[ tb : psnrdenoise ] , it can be seen that employing @xmath210 always yields slightly better denoising results compared to employing @xmath218 . employing the separable dictionary",
    "@xmath201 leads to results that are slightly worse compared to employing the unstructured counterpart .",
    "this is the tribute that has to be paid for its predefined structure .",
    "however , the separability allows a fast implementation just as the popular and also separable overcomplete discrete cosine transform ( odct ) . here",
    ", it can be observed that the separable dictionary @xmath201 learned by sedil outperforms the odct for most images , while requiring exactly the same computational cost .",
    "[ tb : psnrdenoise ]    the second advantage besides computational efficiency that comes along with the capability of learning a separable dictionary is that sedil allows to learn sparse representations for image patches whose size lets other unstructured dictionary learning methods fail due to numerical reasons . in order to demonstrate the capability of sedil in this domain",
    ", a separable dictionary is learned from a training set consisting of @xmath219 images of dimension @xmath220 showing frontal face views of different persons .",
    "these training images were randomly extracted from the @xmath221 faces of the `` cropped labeled faces in the wild database '' @xcite .",
    "the remaining @xmath222 images were used for the following inpainting experiments .",
    "note that the face positions in the pictures are arbitrary , see figure [ fig : faceexample1 ] for five exemplary chosen training faces .",
    "the dimensions of the resulting matrices @xmath213 were set to @xmath223 and all other parameters required for the learning procedure were chosen as above .",
    "the ability of the separable dictionary to capture the global structure of the training samples is illustrated by an inpainting experiment for face images of size @xmath32 , where large regions are missing .",
    "these images have of course not been included in the training set .",
    "we assume that the image region that has to be filled up is given .",
    "the inpainting procedure is again conducted by applying fista on the inverse problem @xmath224 where the measurements @xmath225 are the available image data and @xmath226 is a projection onto the corresponding region with available image data .",
    "an excerpt of the achieved results is given in figure [ fig : faceexample2 ] .",
    "we like to mention that this experiment should not be seen as a highly sophisticated face inpainting method , but rather should supply evidence that sedil is able to properly extract the global information of the underlying training set .",
    "[ fig : faceexample1 ]    [ fig : faceexample2 ]",
    "we propose a new dictionary learning algorithms called sedil that is able to learn both unstructured dictionaries as well as dictionaries with a separable structure .",
    "employing a separable structure on dictionaries reduces the computational complexity from @xmath22 to @xmath23 compared to employing unstructured dictionaries , with @xmath227 being the considered signal dimension . due to this",
    ", separable dictionaries can be learned using far larger signal dimensions as compared to those used for learning unstructured dictionaries , and they can be applied very efficiently in image reconstruction tasks .",
    "another advantage of sedil is that it allows to control the mutual coherence of the resulting dictionary .",
    "therefore , we introduce a new mutual coherence measure and put it in relation to the classical mutual coherence . the sedil algorithm we propose is a geometric conjugate gradient algorithm that exploits the underlying manifold structure .",
    "numerical experiments for image denoising show the practicability of our approach , while the ability to learn sparse representations of large image - patches is demonstrated by a face inpainting experiment .",
    "g.  b. huang , m.  ramesh , t.  berg , and e.  learned - miller .",
    "labeled faces in the wild : a database for studying face recognition in unconstrained environments .",
    "technical report 07 - 49 , university of massachusetts , amherst , october 2007 .",
    "z.  j. xiang , h.  xu , and p.  j. ramadge .",
    "learning sparse representations of high dimensional data on large scale dictionaries .",
    "in _ advances in neural information processing systems _ , pages 900908 , 2011 .",
    "m.  zhou , h.  chen , j.  paisley , l.  ren , l.  li , z.  xing , d.  dunson , g.  sapiro , and l.  carin .",
    "nonparametric bayesian dictionary learning for analysis of noisy and incomplete images .",
    ", 21(1):130144 , 2012 ."
  ],
  "abstract_text": [
    "<S> many techniques in computer vision , machine learning , and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary . </S>",
    "<S> dictionaries are either available analytically , or can be learned from a suitable training set . while analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation , learned dictionaries often perform better in applications as they are more adapted to the considered class of signals . in imagery , unfortunately , the numerical burden for ( i ) learning a dictionary and for ( ii ) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information . </S>",
    "<S> the approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process . on the one hand , this permits larger patch - sizes for the learning phase , on the other hand , the dictionary is applied efficiently in reconstruction tasks . </S>",
    "<S> the learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole , thus enforces basic dictionary properties such as mutual coherence explicitly during the learning procedure . in the special case where no separable structure is enforced , our method competes with state - of - the - art dictionary learning methods like k - svd . </S>"
  ]
}