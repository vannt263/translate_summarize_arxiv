{
  "article_text": [
    "consider the classical regression quantile model : given independent observations @xmath4 , with @xmath5 fixed ( for fixed  @xmath6 ) , the conditional quantile of the response  @xmath7 given @xmath8 is @xmath9 let @xmath10 be the koenker ",
    "bassett regression quantile estimator of @xmath11 .",
    "@xcite provides definitions and basic properties , and describes the traditional approach to asymptotics for @xmath10 using a  bahadur representation : @xmath12 where @xmath13 is a  brownian bridge and  @xmath14 is an error term .",
    "unfortunately ,  @xmath14 is of order @xmath15 [ see , e.g. , @xcite and @xcite ] .",
    "this might suggest that asymptotic results are accurate only to this order . however , both simulations in regression cases and one - dimensional results [ @xcite ( @xcite ) ] justify a  belief that regression quantile methods should share ( nearly ) the @xmath16 accuracy of smooth statistical procedures ( uniformly in @xmath17 ) .",
    "in fact , as shown in @xcite , @xmath18 has a  limit with zero mean and that is independent of @xmath19 .",
    "thus , in any smooth inferential procedure ( say , confidence interval lengths or coverages ) , this error term should enter only through @xmath20 .",
    "nonetheless , this expansion would still leave an error of @xmath21 ( coming from the error beyond the  @xmath14 term in the bahadur representation ) , and so would still fail to reflect root-@xmath2 behavior .",
    "furthermore , previous results only provide such a second - order expansion for fixed  @xmath17.=1    it must be noted that the slower @xmath22 error rate arises from the discreteness introduced by indicator functions appearing in the gradient conditions . in fact , expansions can be carried out when the design is assumed to be random ; see @xcite and @xcite , where the focus is on analysis of the @xmath23 bootstrap .",
    "specifically , the assumption of a  smooth distribution for the design vectors together with a  separate treatment of the lattice contribution of the intercept does permit appropriate expansions .",
    "unfortunately , the randomness in  @xmath24 means that all inference must be in terms of the average asymptotic distribution ( averaged over  @xmath24 ) , and so fails to apply to the generally more desirable conditional forms of inference .",
    "specifically , unconditional methods may be quite poor in the heteroscedastic and nonsymmetric cases for which regression quantile analysis is especially appropriate .",
    "the main goal of this paper is to reclaim increased accuracy for conditional inference beyond that provided by the traditional bahadur representation .",
    "specifically , the aim is to provide a  theoretical justification for an error bound of nearly root-@xmath2 order uniformly in @xmath17 .",
    "define @xmath25    we first develop a  normal approximation for the density of @xmath26 with the following form : @xmath27 for @xmath28 , where @xmath29 .",
    "we then extend this result to the densities of a  pair of regression quantiles in order to obtain a  `` hungarian '' construction [ @xcite ( @xcite ) ] that approximates the process @xmath30 by a  gaussian process to order @xmath31 , where @xmath32 ( uniformly for @xmath33 ) .",
    "section  [ sec2 ] provides some applications of the results here to conditional inference methods in regression quantile models .",
    "specifically , an expansion is developed for coverage probabilities of confidence intervals based on the [ @xcite ] difference quotient estimator of the sparsity function .",
    "the coverage error rate is shown to achieve the rate @xmath34 for conditional inference , which is nearly the known `` optimal '' rate obtained for a  single sample and for unconditional inference .",
    "section  [ sec3 ] lists the conditions and main results , and offers some remarks .",
    "section  [ sec4 ] provides a  description of the basic ingredients of the proof ( since this proof is rather long and complicated ) .",
    "section  [ sec5 ] proves the density approximation for a  fixed @xmath17 ( with multiplicative error ) .",
    "section  [ sec6 ] extends the result to pairs of regression quantiles ( theorem [ den2d ] ) , and section  [ sec7 ] provides the `` hungarian '' construction ( theorem  [ hung ] ) with what appears to be a  somewhat innovative induction along dyadic rationals .",
    "as the impetus for this work was the need to provide some theoretical foundation for empirical results on the accuracy of regression quantile inference , some remarks on implications are in order .    [ remark1 ] clearly , whenever published work assesses the accuracy of an inferential method using the error term from the bahadur representation , the present results will immediately provide an improvement from @xmath35 to the nearly root-@xmath2 rate here .",
    "one area of such results is methods based directly on regression quantiles and not requiring estimation of the sparsity function [ @xmath36 .",
    "there are several papers giving such results , although at present it appears that their methods have theoretical justification only under location - scale forms of quantile regression models .",
    "specifically , @xcite introduced confidence intervals ( especially for fitted values ) based on using pairs of regression quantiles in a  way analogous to confidence intervals for one - sample quantiles .",
    "they showed that the method was consistent , but the accuracy depended on the bahadur error term .",
    "thus , results here now provide accuracy to the nearly root-@xmath2 rate of theorem  [ th2 ] .    a second approach directly using the dual quantile process",
    "is based on the regression ranks of @xcite .",
    "again , the error terms in the theoretical results there can be improved using theorem  [ th1 ] here , though the development is not so direct .    for a  third application",
    ", @xcite showed that the regression quantile process interpolated along a  grid of mesh strictly larger than @xmath1 is asymptotically equivalent to the full regression quantile process to first order , but ( because of additional smoothness ) will yield monotonic quantile functions with probability tending to 1 .",
    "however , their development used the bahadur representation , which indicated that a  mesh of order @xmath37 balanced the bias and accuracy and bounded the difference between @xmath38 and its linear interpolate by nearly @xmath39 . with some work , use of the results here would permit a  mesh slightly larger than the nearly root-@xmath2 rate here to obtain an approximation of nearly root-@xmath2 order .",
    "[ remark2 ] inference under completely general regression quantile models appears to require either estimation of the sparsity function or use of resampling methods .",
    "the most general methods in the ` quantreg ` package [ @xcite ] use the `` difference quotient '' method with the [ @xcite ] bandwidth of order @xmath37 , which is known to be optimal for coverage probabilities in the one - sample problem .",
    "as noted above , expansions using the randomness of the regressors can be developed to provide analogous results for unconditional inference .",
    "the results here ( with some elaboration ) can be used to show that the hall  sheather estimates provide ( nearly ) the same rates of accuracy for coverage probabilities under the conditional form of the regression quantile model .    to be specific ,",
    "consider the problem of confidence interval estimation for a  fixed linear combination of regression parameters : @xmath40 .",
    "the asymptotic variance is the well - known sandwich formula @xmath41 where @xmath42 is the sparsity , @xmath43 ( with @xmath44 being the gradient ) , and where  @xmath24 is the design matrix .",
    "following @xcite , the sparsity may be approximated by the difference quotient @xmath45 .",
    "standard approximation theory ( using the taylor series ) shows that @xmath46 the sparsity may be estimated by @xmath47 and the sparsity  ( [ sadef ] ) may be estimated by inserting @xmath48 in  @xmath49 .",
    "then , as shown in the , the confidence interval @xmath50 has coverage probability @xmath51 , which is within a  factor of @xmath3 of the optimal hall  sheather rate in a single sample .",
    "furthermore , this rate is achieved at the ( optimal ) @xmath52-value @xmath53 , which is the optimal hall  sheather bandwidth except for the @xmath54 term .    since the optimal bandwidth depends on @xmath55 , the optimal constant for the  @xmath56",
    "can not be determined , as it can when  @xmath24 is allowed to be random [ and for which the @xmath57 term is explicit ] .",
    "this appears to be an inherent shortcoming for using inference conditional on the design .",
    "note also that it is possible to obtain better error rates for the coverage probability by using higher order differences .",
    "specifically , using the notation of  ( [ deldef ] ) , @xmath58 as a  consequence , the optimal bandwidth for this estimator is of order @xmath59 , and the coverage probability is accurate to order @xmath60 ( except for logarithmic factors ) .",
    "[ remark3 ] a third approach to inference applies resampling methods .",
    "as noted in the , while the @xmath23 bootstrap is available for unconditional inference , the practicing statistician will generally prefer to use inference conditional on the design .",
    "there are some resampling approaches that can obtain such inference .",
    "one method is that of @xcite , which simulates the binomial variables appearing in the gradient condition .",
    "another is the `` markov chain marginal bootstrap '' of @xcite [ see also @xcite ] . however",
    ", this method also involves sampling from the gradient condition .",
    "the discreteness in the gradient condition would seem to require the error term from the bahadur representation , and thus leads to poorer inferential approximation : the error would be no better than order @xmath61 even if it were the square of the bahadur error term . while some evidence for decent performance of these methods comes from ( rather limited ) simulations , it is often noticed that these methods perform perhaps somewhat more poorly than the other methods in the ` quantreg ` package of @xcite . clearly , a  more complete analysis of inference for regression quantiles based on the more accurate stochastic expansions here would be useful",
    "under the regression quantile model of section  [ sec1 ] , the following conditions will be imposed :    let @xmath62 denote the coordinates of @xmath8 except for the intercept ( i.e. , the last @xmath63 coordinates , if there is an intercept ) .",
    "let @xmath64 denote the conditional characteristic function of the random variable @xmath65 , given @xmath8 .",
    "let @xmath66 and @xmath67 denote the conditional density and c.d.f",
    ". of  @xmath7 given @xmath8 .",
    "[ co1 ] for any @xmath68 , there is @xmath69 such that @xmath70 uniformly in @xmath71 .",
    "[ co2 ] @xmath72 are uniformly bounded , and there are positive definite @xmath73 matrices @xmath74 and @xmath75 such that for any @xmath68 ( as @xmath76 ) @xmath77 uniformly in @xmath71 .",
    "[ cof ] the derivative of @xmath78 is uniformly bounded on the interval @xmath79 .",
    "two fundamental results will be developed here .",
    "the first result provides a  density approximation with multiplicative error of nearly root-@xmath2 rate . a  result for fixed @xmath17",
    "is given in theorem  [ th5 ] , but the result needed here is a  bivariate approximation for the joint density of one regression quantile and the difference between this one and a  second regression quantile ( properly normalized for the difference in @xmath17-values ) .",
    "let @xmath80 for some @xmath68 , and let @xmath81 with @xmath82 for some @xmath83 .",
    "here , one may want to take @xmath84 near 1 [ see remark ( 1 ) below ] , though the basic result will often be useful for @xmath85 , or even smaller .",
    "define @xmath86.\\end{aligned}\\ ] ]    [ th1 ] [ den2d ] under conditions  [ co1 ] ,  [ co2 ] and  [ cof ] , there is a  constant  @xmath49 such that for @xmath87 and @xmath88 at @xmath89 and @xmath90 , respectively , satisfies @xmath91 where @xmath92 is a  normal density with covariance matrix @xmath93 having the form given in  ( [ diffcov ] ) .",
    "the second result provides the desired `` hungarian '' construction :    [ th2 ] [ hung ] assume conditions  [ co1 ] ,  [ co2 ] and  [ cof ] .",
    "fix @xmath94 with @xmath83 , and let @xmath95 be dyadic rationals with denominator less than @xmath96 . define @xmath97 to be the piecewise linear interpolant of @xmath98 [ as defined in  ( [ bndef ] ) ] .",
    "then for any @xmath68 , there is a  ( zero - mean ) gaussian process , @xmath99 , defined along the dyadic rationals @xmath95 and with the same covariance structure as @xmath100 ( along @xmath101 ) such that its piecewise linear interpolant @xmath102 satisfies @xmath103 almost surely .",
    "some remarks on the conditions and ramifications are in order :    \\(1 ) the usual construction approximates @xmath104 by a  `` brownian bridge '' process .",
    "theorem  [ hung ] really only provides an approximation for the discrete processes at a  sufficiently sparse grid of dyadic rationals . that the piecewise linear interpolants converge to the usual brownian bridge follows as in @xcite .",
    "the critical impediment to getting a  brownian bridge approximation to @xmath105 with the error in theorem  [ hung ] is the square root behavior of the modulus of continuity .",
    "this prevents approximating the piecewise linear interpolant within an interval of length greater than ( roughly ) order @xmath106 if a  root-@xmath2 error is desired . in order to approximate the density of the difference in @xmath105 over an interval between dyadic rationals ,",
    "the length of the interval must be at least of order @xmath107 ( for @xmath83 ) .",
    "clearly , it will be possible to approximate the piecewise linear interpolant by a  brownian bridge with error @xmath108 , and thus to get arbitrarily close to the value of @xmath109 for the exponent of @xmath2 . for most purposes",
    ", it might be better to state the final result as @xmath110 for any @xmath111 ( where @xmath112 is the appropriate brownian bridge ) ; but the stronger error bound of theorem  [ hung ] does provide a much closer analog of the result for the one - sample ( one - dimensional ) quantile process .",
    "\\(2 ) the one - sample result requires only the first power of @xmath113 , which is known to give the best rate for a  general result .",
    "the extra addition of @xmath114 in the exponent is clearly needed for the density approximation , but this may be only a  technical assumption .",
    "nonetheless , i conjecture that some extra amount is needed in the exponent .",
    "\\(3 ) conditions  [ co1 ] and  [ co2 ] can be shown to hold with probability tending to one under smoothness and boundedness assumptions of the distribution of @xmath115 .",
    "nonetheless , the condition that @xmath116 be bounded seems rather strong in the case of random  @xmath115 .",
    "it seems clear that this can be weakened , though probably at the cost of getting a  poorer approximation .",
    "for example , @xmath116 having exponentially small tails might increase the bound in theorem  [ hung ] by an additional factor of @xmath113 , and algebraic tails are likely worse .",
    "however , details of such results remain to be developed .",
    "\\(4 ) similarly , it should be possible to let @xmath117 , which defines the compact subinterval of @xmath17-values , tend to zero . clearly ,",
    "letting @xmath118 be of order @xmath106 would lead to extreme value theory and very different approximations . for slower rates of convergence of @xmath118 ,",
    "bahadur expansions have been developed [ e.g. , see @xcite ] and extension to the approximation result in theorem  [ hung ] should be possible .",
    "again , however , this would most likely be at the cost of a  larger error term .",
    "\\(5 ) the assumption that the conditional density of the response ( given @xmath115 ) be continuous is required even for the usual first order asymptotics .",
    "however , one might hope to avoid condition  [ cof ] , which requires a  bounded derivative at all points .",
    "for example , the double exponential distribution does not satisfy this condition .",
    "it is likely that the proofs here can be extended to the case where the derivative does not exist on a  finite set ( or even on a  set of measure zero ) , but dropping differentiability entirely would require a  rather different approach .",
    "furthermore , the apparent need for bounded derivatives in providing uniformity over @xmath17 in bahadur expansions suggests the possibility that some differentiability is required .",
    "\\(6 ) theorem  [ den2d ] provides a  bivariate normal density approximation with error rate ( nearly ) @xmath1 when @xmath119 and @xmath120 are fixed . when @xmath121 , of course , the error rate is larger .",
    "note , however , that the slower convergence rate when @xmath122 does not reduce the order of the error in the final construction since the difference @xmath123 is of order @xmath124 .",
    "the development of the fundamental results ( theorems  [ den2d ] and [ hung ] ) will be presented in three phases . the first phase provides the density approximation for a fixed @xmath17 , since some of the more complicated features are more transparent in this case .",
    "the second phase extends this result to the bivariate approximation of theorem [ den2d ] .",
    "the final phase provides the `` hungarian '' construction of theorem  [ th2 ] . to clarify the development ,",
    "the basic ingredients and some preliminary results will be presented first .",
    "[ ingredient1 ] begin with the finite sample density for a  regression quantile [ @xcite , @xcite ] : assume  @xmath7 has a  density , @xmath125 , and let @xmath17 be fixed . note that @xmath38 is defined by having  @xmath6 zero residuals ( if the design is in general position ) .",
    "specifically , there is a  subset , @xmath52 , of  @xmath6 integers such that @xmath126 , where  @xmath127 has rows @xmath128 for @xmath129 and  @xmath130 has coordinates  @xmath7 for @xmath131 .",
    "let @xmath132 denote the set of all such  @xmath6-element subsets .",
    "define @xmath133    as described in @xcite , the density of @xmath134 evaluated at the argument @xmath135 is given by @xmath136    here , the event in the probability above is the event that the gradient condition holds for a  fixed subset , @xmath137 , where @xmath138 , with @xmath139 the rectangle that is the product of intervals @xmath140 [ see theorem 2.1 of @xcite ] , and where @xmath141    [ ingredient2 ] since @xmath142 is approximately normal , and @xmath143 is bounded , the probability in  ( [ finite ] ) is approximately a  normal density evaluated at  @xmath89 . to get a  multiplicative bound",
    ", we may apply a  `` cramr '' expansion ( or a  saddlepoint approximation ) .",
    "if @xmath144 had a  smooth distribution ( i.e. , satisfied cramr s condition ) , then standard results would apply .",
    "unfortunately ,  @xmath144 is discrete .",
    "the first coordinate of @xmath144 is nearly binomial , and so a  multiplicative bound can be obtained by applying a  known saddlepoint formula for lattice variables [ see @xcite ] .",
    "equivalently , approximate by an exact binomial and ( more directly , but with some rather tedious computation ) expand the logarithm of the gamma function in stirling s formula . using either approach",
    ", one can show the following result :    [ th3 ] [ bin ] let @xmath145 , @xmath146 be any interval of length @xmath147 containing @xmath148 , and let @xmath149 .",
    "then @xmath150 where @xmath151 .    a proof based on multinomial expansions is given for the bivariate generalization in theorem  [ den2d ] .",
    "note that this result includes an extra factor of @xmath152 .",
    "this will allow the bounds to hold except with probability bounded by an arbitrarily large negative power of @xmath2 .",
    "this is clear for the limiting normal case ( by standard asymptotic expansions of the normal c.d.f . ) . to obtain such bounds for",
    "the distribution of @xmath144 will require some form of bernstein s inequality .",
    "such inequalities date to bernstein s original publication in 1924 [ see @xcite ] , but a  version due to @xcite may be easier to apply .",
    "[ ingredient3 ] using theorem  [ bin ] , it can be shown ( see section  [ sec4 ] ) that the probability in  ( [ finite ] ) may be approximated as @xmath153 where the first coordinate of @xmath154 is a  sum of @xmath2 i.i.d . @xmath155",
    "random variables , the last @xmath156 coordinates are those of @xmath144 , and @xmath157 . since we seek a  normal approximation for this probability with multiplicative error , at this point one might hope that a  known ( multidimensional ) `` cramr '' expansion or saddlepoint approximation would allow @xmath154 to be replaced by a normal vector ( thus providing the desired result ) .",
    "however , this will require that the summands be smooth , or ( at least ) satisfy a  form of cramr s condition .",
    "let @xmath158 denote the last @xmath159 coordinates of @xmath160 .",
    "one approach would be to assume @xmath158 has a  smooth distribution satisfying the classical form of cramr s condition . however , to maintain a  conditional form of the analysis , it suffices to impose a  condition on @xmath161 , which is designed to mimic the effect of a  smooth distribution and will hold with probability tending to one if @xmath158 has such a  smooth distribution .",
    "condition [ co1 ] specifies just such an assumption .",
    "note that the characteristic functions of the summands of @xmath162 , say , @xmath163 , will also satisfy condition  [ co1 ] [ equation  ( [ x - cond1 ] ) ] and so should allow application of known results on normal approximations .",
    "unfortunately , i have been unable to find a  published result providing this and so section  [ sec5 ] will present an independent proof .    clearly , some additional conditions will be required .",
    "specifically , we will need conditions that the empirical moments of @xmath164 converge appropriately , as specified in condition [ co2 ] .",
    "finally , the approach using characteristic functions is greatly simplified when the sums , @xmath154 , have densities .",
    "again , to avoid using smoothness of the distribution of @xmath165 ( and thus to maintain a  conditional approach ) , introduce a  random perturbation @xmath166 which is small and has a  bounded smooth density ( the bound may depend on @xmath2 ) .",
    "section  [ sec4 ] will then prove the following :    [ th4 ] [ sina ] assume conditions  [ co1 ] and  [ co2 ] and the regression quantile model of section  [ sec1 ] .",
    "let @xmath89 be the argument of the density of @xmath167 , and suppose @xmath168 for some constant @xmath169 .",
    "then a  constant @xmath170 can be chosen so that @xmath171 where @xmath172 has mean @xmath173 and covariance @xmath174 , @xmath170 can be arbitrarily large , and @xmath166 is a  small perturbation [ see ( [ vbound ] ) ] .    following the proof of this theorem , it will be shown that the effect of @xmath166 can be ignored , if @xmath166 is bounded by @xmath175 , where @xmath176 may depend on @xmath169 ( but not on @xmath170 ) .",
    "[ ingredient4 ] expanding the densities in  ( [ finite ] ) is trivial if the densities are sufficiently smooth .",
    "the assumption of a  bounded first derivative in condition  [ cof ] appears to be required to analyze second order terms ( beyond the first order normal approximation ) .",
    "[ ingredient5 ] finally , summing terms involving @xmath177 in ( [ finite ] ) over the @xmath178 summands will require vinograd s theorem and related results from matrix theory concerning adjoint matrices [ see @xcite ] .",
    "the remaining ingredients provide the desired `` hungarian '' construction .",
    "[ ingredient6 ] extend the density approximation to the joint density for @xmath179 and @xmath180 ( when standardized ) .",
    "a  major complication is that one needs @xmath181 , making the covariance matrix tend to singularity .",
    "thus , we focus on the joint density for standardized versions of @xmath179 and @xmath182 .",
    "clearly , this requires modification of the proof for the univariate case to treat the fact that @xmath183 converges at a  rate depending on  @xmath184 .",
    "the result is given in theorem  [ den2d ] .",
    "[ ingredient7 ] extend the density result to obtain an approximation for the quantile transform for the conditional distribution of differences @xmath183 ( between successive dyadic rationals ) .",
    "this will provide ( independent ) normal approximations to the differences whose sums will have the same covariance structure as the regression quantile process ( at least along a  sufficiently sparse grid of dyadic rationals ) .",
    "[ ingredient8 ] finally , the hungarian construction is applied inductively along the sparse grid of dyadic rationals .",
    "this inductive step requires some innovative development , mainly because the regression quantile process is not directly expressible in terms of sums of random variables ( as are the empiric one - sample distribution function and quantile function ) .",
    "let @xmath185 be the last @xmath186 coordinates of @xmath144 and @xmath187 be the interval @xmath188 . then , @xmath189 where @xmath190 is the set @xmath191 shifted as indicated above .",
    "note that by hoeffding s inequality [ @xcite ] , for any fixed @xmath169 , the shift satisfies @xmath192 except with probability bounded by @xmath193 .",
    "thus , we may apply theorem  [ bin ] [ equation  ( [ binbd ] ) ] with @xmath194 equal to the shift above to obtain the following bound ( to within an additional additive error of @xmath193 ) : @xmath195 where @xmath196 and @xmath184 is a  bound on @xmath197 , which may be taken to be of the form @xmath198 ( by hoeffding s inequality ) .",
    "finally , we obtain @xmath199 where the first coordinate of @xmath154 is a  sum of @xmath2 i.i.d .",
    "@xmath155 random variables and the last @xmath200 coordinates are those of @xmath144",
    ".    to treat the probability involving @xmath154 , standard approaches using characteristic functions can be employed . in theory , exponential tilting ( or saddlepoint methods ) should provide better approximations , but since we require only the order of the leading error term , we can proceed more directly . as in @xcite ,",
    "the first step is to add an independent perturbation so that the sum has an integrable density : specifically , for fixed @xmath201 let @xmath166 be a random variable ( independent of all observations ) with a  smooth bounded density and for which ( for each @xmath201 ) @xmath202 where @xmath176 will be chosen later .",
    "define @xmath203    we now allow @xmath143 to be any ( arbitrary ) set , say , @xmath204 .",
    "thus , @xmath205 has a  density and we can write [ with @xmath206 @xmath207 where @xmath208 denotes the characteristic function of the random variable @xmath209 .",
    "break domain of integration into 3 sets : @xmath210 , @xmath211 , and @xmath212 .    on @xmath213 ,",
    "expand @xmath214 .",
    "for this , compute @xmath215 \\\\ & = & x_i x_i ' \\tau ( 1 - \\tau ) + { { \\mathcal}{o } } \\bigl ( \\| x_i \\|^3 \\| \\delta\\|^2 / n \\bigr).\\end{aligned}\\ ] ]    hence , using the boundedness of @xmath216 , @xmath217 and @xmath218 ( on this first interval ) , @xmath219 where @xmath220 and @xmath221 are defined in condition  [ co2 ] [ see ( [ gdef ] ) and  ( [ hdef ] ) ] .    for the other two intervals on the @xmath222-axis , the integrands will be bounded by an additive error times @xmath223 since @xmath224 .    on @xmath225 ,",
    "the summands are bounded and so their characteristic functions satisfy @xmath226 for some constant @xmath227 .",
    "thus , on @xmath228 , @xmath229 for some constant @xmath230 . therefore , integrating times @xmath231 provides an additive bound of order @xmath232 , where @xmath233 and ( for any @xmath170 ) @xmath234 can be chosen sufficiently large so that @xmath235 .",
    "finally , on @xmath212 , condition  [ co1 ] [ see ( [ x - cond1 ] ) ] gives an additive bound of @xmath236 directly and , again ( as on the previous interval ) , an additive error bounded by @xmath237 can be obtained .",
    "therefore , it now follows that we can choose @xmath170 ( depending on @xmath169 , @xmath176 , @xmath234 and  @xmath238 ) so that @xmath239 from which theorem  [ sina ] follows .",
    "finally , we show that the contribution of @xmath166 can be ignored : @xmath240 where @xmath241 denotes the symmetric difference of the sets .",
    "since @xmath166 is bounded and @xmath242 , this symmetric difference is contained in a  set ,  @xmath49 , which is the union of @xmath243 ( boundary ) parallelepipeds each of the form @xmath244 , where  @xmath245 is a  rectangle one of whose coordinates has width @xmath246 and all other coordinates have length 1 .",
    "thus , applying theorem  [ sina ] ( as proved for the set  @xmath247 ) , @xmath248 where @xmath227 and @xmath249 are constants , and @xmath176 may be chosen arbitrarily large .",
    "[ th5 ] [ den1d ] assume conditions  [ co1 ] ,  [ co2 ] ,  [ cof ] and the regression quantile model of section  [ sec1 ] .",
    "let @xmath89 be the argument of the density of @xmath250 and suppose @xmath251 for some constant @xmath169 .",
    "then , uniformly in @xmath252 ( for @xmath253 ) , @xmath254 where @xmath255 denotes the normal density with covariance @xmath256 with @xmath220 and @xmath221 given by  ( [ gdef ] ) and  ( [ hdef ] ) .",
    "recall the basic formula for the density  ( [ finite ] ) : @xmath257 by theorem  [ sina ] , ignoring the multiplicative and additive error terms given in this result and setting @xmath258 , @xmath259 since @xmath260 is bounded by a  constant times @xmath61 on @xmath261 and the last integral equals @xmath262 .    by ingredient  [ ingredient4 ] ,",
    "the product is @xmath263    this gives the main term of the approximation as @xmath264    the penultimate step is to apply results from matrix theory on adjoint matrices [ specifically , the cauchy  binet theorem and the `` trace '' theorem ; see , e.g. , @xcite , pages 9 and 87 ] : the sum above is just the trace of the  @xmath6th adjoint of @xmath265 , which equals @xmath266 .",
    "the various determinants combine ( with the factor @xmath267 ) to give @xmath268 , which provides the asymptotic normal density we want .",
    "finally , we need to combine the multiplicative and additive errors into a  single multiplicative error .",
    "so consider @xmath269 ( for some constant  @xmath169 ) .",
    "then , the asymptotic normal density is bounded below by @xmath270 for some constant @xmath227 .",
    "thus , since the constant @xmath170 ( which depends on @xmath176 , @xmath234 , @xmath238 and @xmath271 ) can be chosen so that the additive errors are smaller than @xmath272 , the error is entirely subsumed in the multiplicative factor : @xmath273 .",
    "we first prove theorem  [ den2d ] , which provides the bivariate normal approximation .",
    "proof of theorem  [ den2d ] the proof follows the development in theorem  [ den1d ] .",
    "the first step treats the first ( intercept ) coordinate .",
    "since the binomial expansions were omitted in the proof of theorem  [ bin ] , details for the trinomial expansion needed for the bivariate case here will be presented .    the binomial sum in the first coordinate of  ( [ sndef ] )",
    "will be split into the sum of observations in the intervals @xmath274 , @xmath275 and @xmath276 . the expected number of observations in each interval is within  @xmath6 of @xmath2 times the length of the corresponding interval .",
    "thus , ignoring an error of order @xmath277 , we expand a  trinomial with @xmath2 observations and @xmath278 and .",
    "let @xmath279 be the ( trinomially distributed ) number of observation in the respective intervals and consider @xmath280 .",
    "we may take @xmath281\\\\[-8pt ] k_2 & = & { \\mathcal{o } } \\bigl ( { a_n ( \\log n)^{1/2 } } \\bigr),\\nonumber\\end{aligned}\\ ] ] since these bounds are exceeded with probability bounded by @xmath282 for any ( sufficiently large ) @xmath169 .",
    "so @xmath283 , where @xmath284    expanding ( using sterling s formula and some computation ) , @xmath285 & & \\hspace*{37.8pt } { } - \\biggl(n p_1 + k_1 + \\frac{1}{2}\\biggr ) \\log\\biggl ( np_1 + \\frac{k_1 + 1}{n p_1 } \\biggr ) \\\\[-1pt ] & & \\hspace*{37.8pt } { } - \\biggl(n p_2 + k_2 + \\frac{1}{2}\\biggr ) \\log \\biggl ( n p_2 + \\frac { k_2 + 1}{n p_2 } \\biggr)\\\\[-1pt ] & & \\hspace*{37.8pt } { } - \\biggl(n(1-p_1-p_2)-k_1-k_2 + \\frac{1}{2}\\biggr ) \\\\[-1pt ] & & \\hspace*{48.8pt } { }   \\times\\log\\biggl ( n(1-p_1-p_2 ) - \\frac{k_1+k_2 -1}{n(1-p_1-p_2 ) } \\biggr ) + { \\mathcal{o } } \\biggl ( { \\frac { 1}{np_2 } } \\biggr ) \\biggr\\ } \\\\[-1pt ] & = & \\frac{1}{2\\pi } \\exp\\biggl\\{\\frac{1}{2}\\log n - n p_1 \\log p_1 - \\biggl(k_1 + \\frac{1}{2}\\biggr ) \\log(n p_1)\\\\[-1pt ] & & \\hspace*{37.8pt } { } - n p_2 \\log p_2   - \\biggl(k_2 + \\frac{1}{2}\\biggr ) \\log(n p_2)\\\\[-1pt ] & & \\hspace*{37.8pt } { } - n ( 1 - p_1 - p_2 ) \\log(1 - p_1 - p_2 ) -\\biggl(k_1 + k_2 + \\frac{1}{2 } \\biggr ) \\\\[-1pt ] & & \\hspace*{48.8pt } { } \\times\\log\\bigl(n(1 - p_1 - p_2)\\bigr ) - \\frac{k_1 ^ 2}{np_1 } - \\frac{k_2 ^ 2}{np_2 } \\\\[-1pt ] & & \\hspace*{125.1pt}{}- \\frac { ( k_1+k_2)^2}{n(1-p_1-p_2 ) } + { \\mathcal{o } } \\biggl ( { \\frac{k_2 ^ 3}{(np_2)^2 } } \\biggr ) \\biggr\\ } \\\\[-1pt ] & = & \\frac{1}{2\\pi } \\exp\\biggl\\{- \\log n - \\biggl(np_1 + k_1 + \\frac{1}{2}\\biggr ) \\log p_1 - \\biggl(np_2 + k_2 + \\frac{1}{2}\\biggr ) \\log p_2 \\\\[-1pt ] & & \\hspace*{37pt } { } - \\biggl(n(1-p_1-p_2 ) -k_1 - k_2 + \\frac{1}{2}\\biggr ) \\log(1-p_1 - p_2 ) \\\\[-1pt ] & & \\hspace*{73.5pt } { } - \\frac{k_1 ^ 2}{np_1 } - \\frac{k_2 ^ 2}{np_2 } - \\frac { ( k_1+k_2)^2}{n(1-p_1-p_2 ) } + { \\mathcal{o } } \\biggl ( { \\frac { ( log n)^{3/2 } } { n a_n^2 } } \\biggr ) \\biggr \\ } , \\\\[-1pt ] b & = & \\exp\\bigl\\{(np_1+k_1 ) \\log p_1 + ( np_2 + k_2 ) \\log",
    "p_2 \\\\[-1pt ] & & \\hspace*{18.5pt } { } + \\bigl(n(1-p_1-p_2 ) -k_1 - k_2 \\bigr ) \\log(1-p_1-p_2 ) \\bigr\\}.\\end{aligned}\\ ] ]    therefore , @xmath286 & & \\hspace*{19.3pt } { } - \\frac{k_1 ^ 2}{np_1 } - \\frac{k_2 ^ 2}{np_2 } - \\frac { ( k_1+k_2)^2}{n(1-p_1-p_2 ) } + { \\mathcal{o } } \\biggl ( { \\frac { ( log n)^{3/2 } } { n a_n^2 } } \\biggr ) \\biggr \\}.\\end{aligned}\\ ] ]    some further simplification shows that @xmath287 gives the usual normal approximation to the trinomial with a  multiplicative error of @xmath288 [ when @xmath289 and @xmath290 satisfy ( [ ki ] ) ] .",
    "the next step of the proof follows that of theorem  [ sina ] ( see ingredient  [ ingredient3 ] ) .",
    "since the proof is based on expanding characteristic functions ( which do not involve the inverse of the covariance matrices ) , all uniform error bounds continue to hold .",
    "this extends the result of theorem  [ sina ] to the bivariate case : @xmath291 & & \\qquad= p \\bigl\\ { z_1 \\in a_{h_1 } / \\sqrt{n } , z_2 \\in a_{h_2 } / \\sqrt{n } \\bigr\\ } \\\\[-1pt ] & & \\qquad= p \\bigl\\ { z_1 \\in a_{h_1 } / \\sqrt{n } \\bigr\\ } \\times p \\bigl\\{(z_2 - z_1 ) / \\sqrt{n } \\in ( a_{h_2 } - z_2 ) / \\sqrt{n } | z_1 \\bigr\\}\\nonumber\\end{aligned}\\ ] ] for appropriate normally distributed @xmath292 ( depending on @xmath2 ) . this last equation is needed to extend the argument of theorem  [ den1d ] , which involves integrating normal densities .",
    "the joint covariance matrix for @xmath293 is nearly singular ( for @xmath294 small ) and complicates the bounds for the integral of the densities .",
    "the first factor above can be treated exactly as in the proof of theorem [ den1d ] , while the conditional densities involved in the second factor can be handled by simple rescaling .",
    "this provides the desired generalization of theorem  [ den1d ] .",
    "thus , the next step is to develop the parameters of the normal distribution for @xmath295 [ see  ( [ bndef ] ) ,  ( [ rndef ] ) ] in a  usable form .",
    "the covariance matrix for @xmath296 has blocks of the form @xmath297 where @xmath298 with @xmath220 and @xmath221 given in condition  [ co2 ] [ see  ( [ gdef ] ) and ( [ hdef ] ) ] .    expanding @xmath299 about @xmath300 ( using the differentiability of the densities from condition  [ cof ] ) , @xmath301 where @xmath302 are derivatives of @xmath220 at @xmath119 (",
    "note that @xmath303 ) .",
    "straightforward matrix computation now yields the joint covariance for @xmath295 : @xmath304 where @xmath305 are uniformly bounded matrices .",
    "thus , the conditional distribution of @xmath306 given @xmath307 has moments @xmath308 & = & ( \\tau_2 - \\tau_1 ) \\lambda_{11}^{-1 } \\delta_{12 } / \\bigl(\\tau_1 ( 1 - \\tau_1)\\bigr ) , \\\\[-2pt ] \\label{covcond } \\operatorname{cov } \\bigl [ r_n | b_n(\\tau_1 ) \\bigr ] & = & ( \\tau_2 - \\tau_1 ) \\biggl [ \\delta_{22}^ * - \\frac{\\tau_2 - \\tau_1}{\\tau_1 ( 1 - \\tau_1 ) } \\delta_{21}^ * \\lambda_{11}^{-1 } \\delta_{12}^ * \\biggr]\\end{aligned}\\ ] ] and analogous equations also hold for @xmath309 .",
    "finally , recalling that @xmath310 , the second term in ( [ zcond ] ) can be written @xmath311 thus , since the conditional covariance matrix is uniformly bounded except for the @xmath312 factor , the argument of theorem [ den1d ] also applies directly to this conditional probability .",
    "finally , the above results are used to apply the quantile transform for increments between dyadic rationals inductively in order to obtain the desired `` hungarian '' construction .",
    "the proof of theorem  [ hung ] is as follows :    proof of theorem  [ hung ] ( i ) following the approach in @xcite , the first step is to provide the result of theorem  [ den2d ] for conditional densities one coordinate at a  time . using the notation of theorem  [ den2d ] ,",
    "let @xmath313 and @xmath314 be successive dyadic rationals ( between @xmath117 and @xmath315 ) with denominator @xmath316 .",
    "so @xmath317 .",
    "let @xmath318 be the @xmath319th coordinate of @xmath320 [ see  ( [ rndef ] ) ] , let @xmath321 be the vector of coordinates before the @xmath319th one , and let @xmath322 .",
    "then the conditional density of @xmath323 satisfies @xmath324 for @xmath325 , @xmath326 , and @xmath327 , and where @xmath328 and  @xmath329 are easily derived from  ( [ econd ] ) and  ( [ covcond ] ) .",
    "note that @xmath328 has the form @xmath330 where @xmath331 can be bounded ( independent of @xmath2 ) and @xmath332 can be bounded away from zero and infinity ( independent of @xmath2 ) .",
    "this follows since the conditional densities are ratios of marginal densities of the form @xmath333 ( with @xmath334 satisfying theorem  [ den2d ] ) .",
    "the integral over @xmath335 has the multiplicative error bound directly .",
    "the remainder of the integral is bounded by @xmath336 , which is smaller than the normal integral over @xmath337 ( see the end of the proof of theorem  [ den1d ] ) .",
    "\\(ii ) the second step is to develop a  bound on the ( conditional ) quantile transform in order to approximate an asymptotic normal random variable by a  normal one .",
    "the basic idea appears in @xcite .",
    "clearly , from  ( [ condden ] ) , @xmath338 for @xmath339 , @xmath340 , and @xmath327 . by condition  [ cof ] , the conditional densities ( of the response given @xmath115 )",
    "are bounded above zero on @xmath341 .",
    "hence , the inverse of the above versions of the c.d.f.s also satisfy this multiplicative error bound , at least for the variables bounded by @xmath342 .",
    "thus , the quantile transform can be applied to show that there is a  normal random variable , @xmath343 , such that @xmath344 so long as @xmath345 and the quantile transform of @xmath345 are bounded by @xmath342 . using the conditional mean and variance",
    "[ see  ( [ mean1 ] ) ] , and the fact that the random variables exceed @xmath342 with probability bounded by @xmath336 ( where  @xmath169 can be made large by choosing  @xmath49 large enough ) , there is a  random variable  @xmath346 that can be chosen independently so that @xmath347 except with probability bounded by @xmath336 .",
    "\\(iii ) finally , the `` hungarian '' construction will be developed inductively .",
    "let @xmath348 and consider induction on @xmath349 .",
    "first consider the case where @xmath350 ; the argument for @xmath351 is entirely analogous .",
    "define @xmath352 , where @xmath227 bounds the big - o term in any equation of the form  ( [ couple ] ) .",
    "let @xmath204 be a  bound [ uniform over @xmath353 on @xmath354 in  ( [ couple ] ) .",
    "the induction hypothesis is as follows : there are normal random vectors @xmath355 such that @xmath356 except with probability @xmath357 , where for each @xmath358 , @xmath359 has the same covariance structure as @xmath360 , and where @xmath361    note : since the earlier bounds apply only for intervals whose lengths exceed @xmath362 ( for some positive @xmath363 ) , @xmath358 must be taken to be smaller than @xmath364 .",
    "thus , the bound in  ( [ epsdef ] ) becomes @xmath365 , as stated in theorem  [ den2d ] .    to prove the induction result , note first that theorem  [ den2d ] ( or theorem [ den1d ] ) provides the normal approximation for @xmath366 for @xmath367 .",
    "the induction step is proved as follows : following @xcite , take two consecutive dyadic rationals @xmath368 and @xmath369 with @xmath370 odd .",
    "so @xmath371/2^{\\ell-1 } = \\tau\\bigl([k/2 ] , \\ell-1\\bigr).\\ ] ] condition each coordinate of @xmath372 on previous coordinates and on @xmath373 , \\ell-1))$ ] .",
    "let @xmath374 be one such coordinate .",
    "now , as above , define @xmath375 by @xmath376 , \\ell-1\\bigr)\\bigr ) + r(k , \\ell).\\ ] ]    from  ( [ couple ] ) , there is a  normal random variable @xmath377 such that @xmath378 , \\ell-1\\bigr)\\bigr ) - z_n(k , \\ell ) \\bigr| \\leq \\varepsilon_n^*.\\ ] ]    by the induction hypothesis for @xmath379 , @xmath373 , \\ell-1 ) $ ] is approximable by normal random variables to within @xmath380 ( except with probability @xmath336 ) .",
    "thus , a  coordinate @xmath381 , \\ell-1 ) $ ] is also approximable with this error , and the error in approximating @xmath382 , \\ell- 1 ) $ ] is bounded by @xmath380 times @xmath383 .",
    "finally , since @xmath384 is independent of these normal variables , the errors can be added to obtain @xmath385 therefore , except with probability less than @xmath386 , the induction hypothesis  ( [ induct1 ] ) holds with error @xmath387 and the induction is proven .",
    "the theorem now follows since the piecewise linear interpolants satisfy the same error bound [ see @xcite ] .",
    "[ re1 ] under the conditions for the theorems here , the coverage probability for the confidence interval  ( [ confint ] ) is @xmath388 , which is achieved at @xmath389 ( where @xmath227 is a  constant ) .",
    "sketch of proof recall the notation of remark  [ remark2 ] in section  [ sec2 ] . using theorem  [ th1 ] and the quantile transform as described in the first steps of theorem  [ th2 ] ( and not needing the dyadic expansion argument ) ,",
    "it can be shown that there is a  bivariate normal pair @xmath390 such that @xmath391\\\\[-8pt ] { \\sqrt{n } } \\bigl ( \\hat\\delta(h_n ) - \\delta(h_n)\\bigr ) & = & z + r^*_n,\\qquad r^*_n = { { \\mathcal}{o}}_p \\bigl ( n^{-1/2 } ( \\log n)^{3/2 } \\bigr).\\nonumber\\end{aligned}\\ ] ] note that from the proofs of theorems  [ th1 ] and  [ th2 ] , the @xmath392 terms above are actually @xmath393 terms except with probability @xmath336 where @xmath169 is an arbitrary fixed constant .",
    "the `` almost sure '' results above take @xmath394 , but @xmath395 will suffice for the bounds on the coverage probability here .",
    "now consider expanding @xmath397 .",
    "first , note that under the design conditions here , @xmath398 will be of exact order @xmath61 ; specifically , if  @xmath24 is replaced by @xmath399 , all terms involving @xmath400 will remain bounded , and we may focus on @xmath401 .",
    "note also that for @xmath402 , the terms in the expansion of @xmath403 tend to zero [ specifically , @xmath404 .",
    "so the sparsity , @xmath397 , may be expanded in a  taylor series as follows : @xmath405 where @xmath406 is a  ( gradient ) vector that can be defined in terms of @xmath407 and @xmath11 ( and its derivatives ) , @xmath408 is a  quadratic function ( of its vector argument ) and @xmath409 is a  cubic function . note that under the design conditions , all the coefficients in @xmath406 , @xmath408 and @xmath409 are bounded , and so it is not hard to show that all the terms in @xmath410 tend to zero as long as @xmath411 .",
    "specifically , if @xmath412 is of order @xmath413 , then all the terms in @xmath410 tend to zero . also , @xmath55 is within a  @xmath3 factor of @xmath414 and @xmath415 is even smaller .",
    "finally , @xmath112 is a  difference of two quantiles separated by @xmath416 , and so @xmath417 has variance proportional to @xmath52 .",
    "thus , @xmath418 .",
    "thus , not only does @xmath419 , but powers of this term greater than 2 will also be @xmath420 .",
    "it follows that the coverage probability may be computed using only two terms of the taylor series expansion for the normal c.d.f . :",
    "@xmath421 note that the ( normal ) conditional distribution of @xmath422 given @xmath112 is straightforward to compute ( using the usual asymptotic covariance matrix for quantiles ) : the conditional mean is a  small constant ( of the order of @xmath412 ) times @xmath112 , and the conditional variance is bounded .",
    "expanding the lower probability in the same way and subtracting provides some cancelation",
    ". the contribution of  @xmath14 will cancel in the @xmath423 differences , and is negligible in subsequent terms since @xmath424 .",
    "similarly , the @xmath425 term will appear only in the @xmath423 difference where it contributes a  term that is @xmath426 times a  term of order @xmath427 , and will also be negligible in subsequent terms .",
    "also , the @xmath428 term will only appear in @xmath423 , as higher powers will be negligible .",
    "the only remaining terms involve @xmath429 .",
    "for the first power ( appearing in @xmath423 ) , @xmath430 .",
    "for the squared @xmath112-terms in @xmath431 , since var(@xmath417 ) is proportional to @xmath412 , @xmath432 , and all other terms involving @xmath112 have smaller order .    therefore , one can obtain the following error for the coverage probability : for some constants @xmath230 and @xmath433 , the error is @xmath434 ( plus terms of smaller order ) . since @xmath55 is of order nearly @xmath61",
    ", the first terms have nearly the same order . using @xmath435",
    ", it is straightforward to find the optimal @xmath412 to be a  constant times @xmath436 , which bounds the error in the coverage probability by @xmath437 ."
  ],
  "abstract_text": [
    "<S> traditionally , assessing the accuracy of inference based on regression quantiles has relied on the bahadur representation . </S>",
    "<S> this provides an error of order @xmath0 in normal approximations , and suggests that inference based on regression quantiles may not be as reliable as that based on other ( smoother ) approaches , whose errors are generally of order @xmath1 ( or better in special symmetric cases ) . </S>",
    "<S> fortunately , extensive simulations and empirical applications show that inference for regression quantiles shares the smaller error rates of other procedures . </S>",
    "<S> in fact , the `` hungarian '' construction of komls , major and tusndy [ _ z . wahrsch . </S>",
    "<S> verw . </S>",
    "<S> gebiete _ * 32 * ( 1975 ) 111131 , _ z.  wahrsch . verw . </S>",
    "<S> gebiete _ * 34 * ( 1976 ) 3358 ] provides an alternative expansion for the one - sample quantile process with nearly the root-@xmath2 error rate ( specifically , to within a  factor of @xmath3 ) . </S>",
    "<S> such an expansion is developed here to provide a  theoretical foundation for more accurate approximations for inference in regression quantile models . </S>",
    "<S> one specific application of independent interest is a  result establishing that for conditional inference , the error rate for coverage probabilities using the hall and sheather [ _ j . </S>",
    "<S> r. stat . </S>",
    "<S> soc . </S>",
    "<S> ser . </S>",
    "<S> b stat . </S>",
    "<S> methodol . _ </S>",
    "<S> * 50 * ( 1988 ) 381391 ] method of sparsity estimation matches their one - sample rate . </S>"
  ]
}