{
  "article_text": [
    "[ section1 ] unlike traditional imaging systems , hyperspectral imaging ( hsi ) sensors @xcite,@xcite acquire a scene with several millions of pixels in up to hundreds of contiguous wavelengths .",
    "such high resolution spatio - spectral hyperspectral data , i.e. , three - dimensional ( 3d ) datacube organized in the spatial and spectral domain , has an extremely large data size and enormous redundancy , which makes compressive sensing ( cs ) @xcite a promising solution for hyperspectral data acquisition .    to date , most existing designs for cs - based hyperspectral imagers can be grouped into frame - based acquisition in the spatial direction and pixel - based acquisition in the spectral direction .",
    "while a lot of reconstruction approaches for these two acquisition schemes have been proposed , most existing algorithms can only take advantage of the spatial and spectral information of hyperspectral data from the aspect of sparsity ( or joint - sparsity ) . because the foundation of these algorithms is built on conventional cs , which reconstructs the signals by solving a convex programming and proceeds without exploiting additional information ( aside from sparsity or compressibility ) @xcite .",
    "for hyperspectral data , the spatial and spectral correlations , which not only reflect in the correlation between the sparse structure of the data ( i.e. , structured sparsity ) , but also in the correlation between the amplitudes of the data , can be used to provide helpful prior information in the reconstruction processed and assist on increasing the compression ratios .    in this paper",
    ", the structured sparsity and the amplitude correlations are considered jointly by assuming that spatially and spectrally correlated data satisfies simultaneous low - rank and joint - sparse ( l&s ) structure . using a structured l&s factorization",
    ", we propose an iterative approximate message passing ( amp ) algorithm @xcite,@xcite , in order to enable joint reconstruction of the data with the practical compression ratio that is substantially higher than the state - of - the - art .",
    "specifically , in section [ section2 ] , we introduce the structured factorization representation of the l&s model . in section [ section3 ] ,",
    "we propose a novel amp - based approach , called l&s - approximate message passing ( l&s - amp ) , that decouples the global inference problem into two sub - problems .",
    "one sub - problem considers the linear inverse problem of recovering the signal matrix from its compressed measurements .",
    "another sub - problem exploits the l&s structure of the signal matrix . then a recently proposed ",
    "turbo amp \" framework @xcite is used to enable messages to pass between these two phases efficiently .",
    "section [ section4 ] presents simulation results with real hyperspectral data that support the potential of the approach to considerably reduce the reconstruction error . in section [ section5 ] , we conclude the paper .",
    "[ section2 ] in this section , we first present the problem for compressive hyperspectral imaging . then , we propose a structured l&s factorization model for the signal matrix , which will be later exploited to acquire any hsi with very few measurements , via a novel joint reconstruction approach .",
    "owing to the inherent 3d structure present in the hyperspectral datacube and the two - dimensional nature of optical sensing hardware , cs - based hyperspectral imagers generally capture a group of linear measurements across either the 2d spatial extent of the scene for a spectral band or the spectral extent for a spatial ( pixel ) location at a time , i.e. , @xmath0 $ ] .",
    "then , the compressed measurements @xmath1 are sent to a fusion station that will recover the original 3d datacube by utilizing a cs reconstruction algorithm .    by taking the highly correlated hyperspectral data vectors @xmath2 to admit a sparse representation in some orthonormal basis , e.g. , dct basis or wavelet basis , we have , @xmath3 where sparse signal vectors @xmath4 .",
    "@xmath5 is a certain orthonormal basis matrix .",
    "then we obtain the typical cs formulation as follows @xmath6where @xmath7\\in\\mathbb{r}^{m_t\\times n}$ ] , @xmath8 is the measurement output vector , and @xmath9 is an additive noise vector with unknown variance @xmath10 .",
    "[ section2.3 ] as mentioned in the introduction , while the original hyperspectral data can be reconstructed by using conventional cs recovery algorithms , it is possible to achieve a much better recovery performance by applying the l&s model to further exploit the structural dependencies between the values and locations of the coefficients of the sparse signal vectors @xmath11 .",
    "the main reason that we consider @xmath12 as a l&s matrix is two - fold .",
    "first , images from different spectral bands enjoy similar natural image statistics , and hence can be joint - sparse in a wavelet / dct basis ; second , a limited number of unique materials in a scenes implies that spectral signatures across pixels can be stacked to form a matrix that is often low - rank @xcite .    to precisely achieve the benefits of the l&s model and reconstruct the original hyperspectral data from a bayesian point of view , here we propose an accurate probabilistic model by performing a structured l&s factorization for @xmath12 as",
    "@xmath13where the diagonal matrix @xmath14 is the sparsity pattern matrix of the signals with the support indicates @xmath15 .",
    "we refer to @xmath16 as the sparsity level of @xmath12 .",
    "@xmath17\\in\\mathbb{r}^{n\\times r}$ ] and @xmath18\\in\\mathbb{r}^{r\\times t}$ ] are obtained from the low - rank matrix factorization of @xmath19 , which is the amplitude matrix of @xmath12 . for a joint - sparse matrix @xmath20 and an arbitrary matrix @xmath21",
    ", this factorization implies that @xmath12 is a simultaneous low - rank ( @xmath22 ) and joint - sparse matrix with rank @xmath23 , where all sparse signal vectors @xmath24 share a common support with sparsity level @xmath25 .",
    "assuming independent entries for @xmath26 , @xmath27 , and @xmath21 , the separable probability density functions ( pdfs ) of @xmath28 and @xmath21 become @xmath29 @xmath30 where both @xmath31 and @xmath32 are assumed to be i.i.d .",
    "gaussian with unknown mean @xmath33 and variance @xmath34 .",
    "in particular , we assume @xmath32 follow i.i.d .",
    "gaussian distribution with zero mean and unit variance , i.e. , @xmath35 , to avoid ambiguity and the unnecessary model parameters update . as @xmath36",
    "are treated as i.i.d .",
    "bernoulli random variables with @xmath37 , the sparse coefficients , @xmath38 , become i.i.d .",
    "bernoulli - gaussian ( bg ) , i.e. , the marginal pdf @xmath39 where @xmath40 is the dirac delta function .",
    "furthermore , due to the assumption of adaptive gaussian noise in ( [ cs ] ) , the likelihood function of @xmath41 is known and separable , i.e. , @xmath42where the measurement @xmath43 .    , @xmath44 , @xmath45 , and @xmath46.,title=\"fig:\",width=264 ] +",
    "[ section3 ] with the problem formulation in ( [ cs ] ) and ( [ signalmodel ] ) , our proposed method is to maximize the posterior joint distribution , i.e. , @xmath47 @xmath48 @xmath49 @xmath50 where @xmath51 denotes equality up to a normalizing constant scale factor .",
    "this posterior distribution can be represented with a factor graph shown in fig .",
    "[ fig_l&samp ] , where circles denote random variables and squares denote posterior factors based on belief propagation @xcite .",
    "each factor node represents the conditional probability distribution between all variable nodes it connected .",
    "@xmath52 vertical planes ( parallel to @xmath53 and @xmath54 axes ) exploit the linear measurement structure @xmath55 ( detailed in fig .",
    "[ fig_mgamp ] ) , while the remaining part of fig .",
    "[ fig_l&samp ] further exploits the l&s structure @xmath56 .    to bypass the intractable inference problem of marginalizing ( [ pjd ] )",
    ", we propose to solve an alternative problem that consists of two sub - problems that mainly require local information to complete their tasks @xcite .",
    "correspondingly , our proposed algorithm is divided into two phases : i ) the _ multiple generalized approximate message passing _",
    "( m - gamp ) phase ; ii ) the _ low - rankness and sparsity pattern decoding _ ( l&spd ) phase . owing to this ,",
    "an efficient  turbo amp \" iterative framework @xcite is used , that iteratively updates one of the phases beliefs , and passes the beliefs to another phase , and vice versa , repeating until both phases converge .    , @xmath44 , @xmath45 , and @xmath46.,title=\"fig:\",width=238 ] +      [ subsection3_1 ] in each frame @xmath57 of the m - gamp phase",
    ", we apply the _ generalized approximate message passing _ ( gamp ) approach @xcite in parallel for the linear inference problem : estimate the vector @xmath58 from the observation @xmath59 , as shown in fig .",
    "[ fig_mgamp ] . specifically , the gamp computes the approximated posteriors on @xmath60 as @xcite , @xcite @xmath61 @xmath62where @xmath63 denotes a message passed from node @xmath64 to the adjacent node @xmath65 in the factor graph .",
    "the parameters @xmath66 , and @xmath67 are obtained after the gamp iteration converges .",
    "for the prior distribution of @xmath68 , i.e. , @xmath69 used in ( [ xpost ] ) , we can assume bg prior pdf @xmath70 where @xmath71 and @xmath72 are the active - coefficient mean and the active - coefficient variance , respectively , of the variable @xmath68 .",
    "it is worth mentioning that the prior parameters @xmath73 and @xmath74 , are only initialized to agnostic values at the beginning of the l&s - amp algorithm ( e.g. , @xmath75 ) , then iteratively updated according to the message passed from the l&spd phase .",
    "this process will be detailed in next subsection .",
    "then the minimum - mean - squared error ( mmse ) estimation of @xmath76 is facilitated by the following prior - dependent integrals @xmath77      [ subsection3_2 ] in the l&spd phase , to exploit the l&s structure , we employ the recently proposed _",
    "bilinear generalized approximate message passing _",
    "( big - amp ) approach @xcite to a variant of the pca problem : estimate the matrices @xmath28 and @xmath12 from an observation @xmath78\\in\\mathbb{r}^{n\\times t}$ ] which is the posterior estimation of their product @xmath79 obtained form the m - gamp phase in ( [ xmarginal ] ) . in particular , the big - amp @xcite obtains the approximately gaussian posterior messages @xmath80 as @xmath81\\ ] ] @xmath82 where the parameters @xmath83 and @xmath84 are obtained after the big - amp iteration converges .",
    "the prior distribution of @xmath85 , i.e. , @xmath86 used in ( [ xpost1 ] ) , comes from the posterior message of @xmath87 given the observation @xmath59 in the m - gamp phase .",
    "the prior distribution of @xmath69 used in ( [ xpost ] ) comes from the posterior message of @xmath68 given the matrix factorization @xmath88 in the l&spd phase .    to enable effective implementation of  turbo amp \" iteration , given the construction of the factor graph in fig .",
    "[ fig_l&samp ] , the sum - product algorithm ( spa ) @xcite implies that , @xmath89 @xmath90 @xmath91 . \\vspace{-4pt}\\]]comparing ( [ xprior_spa ] ) and ( [ xprior_spa1 ] ) with ( [ xpost ] ) and ( [ xpost1 ] ) , respectively , we have @xmath92 @xmath93 thus , the parameters @xmath94 computed during the final iteration of the m - gamp phase , are treated as the prior parameters of @xmath12 in the l&spd phase .",
    "conversely , the parameters @xmath95 and @xmath96 computed during the final iteration of the l&spd phase are in turn used as the prior parameters of @xmath12 in the m - gamp phase in ( [ xmarginal ] ) .",
    "in addition ,  to  further  exploiting  the  joint - sparsity  of    @xmath97 , we use the local support estimate @xmath98 instead of the common sparsity rate @xmath99 in ( [ xmarginal ] ) .",
    "then , by applying the spa in the m - gamp phase , we get @xmath100where the posterior local support probability @xmath101      [ subsection3_3 ] beginning at the initial inter - phase iteration index , @xmath102 , the l&s - amp algorithm first performs the m - gamp phase with the initial prior parameters @xmath103 in ( [ xmarginal ] )",
    ". then the converged outgoing messages @xmath104 are treated as prior parameters in the l&spd phase . then the converged messages @xmath73 and @xmath105 obtained from the l&spd phase , along with the updated beliefs @xmath106 in ( [ lambdain ] ) , are used for the m - gamp phase at inter - phase iteration @xmath107 .",
    "this procedure continues until either a stopping condition or a maximum number of allowable iterations is reached .",
    "then we obtain the posterior mean estimates @xmath108 computed in ( [ xestimate ] ) .",
    "furthermore , we tune our prior and likelihood parameters @xmath109 using expectation - maximization @xcite,@xcite , and estimate the rank @xmath110 using a rank selection strategy based on the penalized log - likelihood maximization in @xcite .",
    "in addition , we recommend initializing l&s - amp using @xmath111 , @xmath112 and @xmath113 .",
    "[ section4 ] in this section , we present real data results to compare the performance of the proposed l&s - amp algorithm with prior state - of - art ppxa , ra - ormp @xcite , sa - music @xcite , and t - msbl @xcite algorithms .",
    "we evaluate the performance of the algorithms on two real hyperspectral datasets : 1 ) an urban dataset acquired over the university of houston , with 144 spectral bands , @xmath114 pixels , and a spatial resolution of @xmath115 .",
    "2 ) an agricultural dataset acquired over the salinas valley in california .",
    "the dataset has a spatial resolution of @xmath116 and consists of @xmath117 spectral bands with each band corresponding to an image with @xmath118 pixels .",
    "we assume that the l&s signal matrix @xmath119 is obtained using pixel - based acquisition , so that @xmath52 denotes the number of pixels and @xmath120 denotes the number of spectral band .",
    "the dct matrix is used as the sparsifying matrix @xmath121 , and gaussian noise is added to achieve snr @xmath122 db .",
    "it is worth noting that , for the sake of comparion , different random gaussian measurement matrices @xmath123 are used .",
    "also note that t - msbl , ra - ormp , and sa - music are derived only for the common measurement matrix case .     for the recovery of the urban dataset ( left ) and agriculture - oriented dataset ( right).,title=\"fig:\",width=302 ] +    fig .",
    "[ cnmse ] plots the column - averaged normalized mse ( cnmse ) versus the compressive ratio @xmath124 on the two real datasets .",
    "the cnmse is defined as @xmath125 , where @xmath126 is an estimate of @xmath12 . from the figure , we observe that the proposed algorithm outperforms all the other algorithms in terms of cnmse , e.g. , in fig . [ cnmse].(b )",
    ", we note that l&s - amp achieves nearly 3db reconstruction gain than the other algorithms at @xmath127 .",
    "in addition , a plus - minus sign ( @xmath128 ) is used ( i.e. , l&s - amp@xmath129 ) to denote the case of using random @xmath130 measurement matrices , which are easy to implement in dmd , and can significantly reduce the burden of storage .    some visual results of the recovered hyperspectral images by using different algorithms are presented in fig .",
    "[ visual ] . as expected ,",
    "our proposed algorithm preserves more fine details and much sharper edges , and shows much clearer and better visual results than the other competing methods .",
    "is fixed to 0.243 , and other simulation parameters remain unchanged . the whole scene is partitioned into a sequence of sub - scenes to enable parallel processing . ,",
    "title=\"fig:\",width=317 ] +",
    "[ section5 ] in this paper , we studied joint cs reconstruction of spatially and spectrally correlated hyperspectral data acquired , assuming that the hyperspectral signal matrix satisfies the joint - sparse model with a lower rank than the sparsity level , i.e. , the l&s model .",
    "we proposed an amp - based algorithm for recovering the signal matrix with the l&s model while exploiting the structured sparsity and the amplitude correlation of the data .",
    "the numerical results were presented to confirm the performance advantage of our algorithm .    1 a. plaza , j. a. benediktsson , j. boardman , j. brazile , l. bruzzone , g. camps - valls , j. chanussot , m. fauvel , p. gamba , j. gualtieri , m. marconcini , j. c. tilton , and g. trianni ,  recent advances in techniques for hyperspectral image processing , \" _ remote sens . environment , _ vol . 113 , no . 1 , pp .",
    "110122 , sep . 2009 .",
    "j. m. bioucas - dias , a. plaza , g. camps - valls , p. scheunders , n. m. nasrabadi , and j. chanussot ,  hyperspectral remote sensing data analysis and future challenges , \" _ ieee geoscience and remote sens . mag .",
    ", _ vol . 1 , no",
    ". 2 , pp . 636 , jun . 2013 .",
    "e. j. cands , j. romberg , and t. tao ,  robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , \" _ ieee trans .",
    "theory , _ vol .",
    "489509 , feb .",
    "m. f. duarte , m. a. davenport , d. takhar , j. n. laska , t. sun , k. e. kelly , and r. g. baraniuk ,  single - pixel imaging via compressive sampling , \" _ ieee signal process .",
    "2 , pp : 8391 , mar .",
    "2008 .",
    "m. golbabaee and p. vandergheynst ,  hyperspectral image compressed sensing via low - rank and joint - sparse matrix recovery , \" in _ proc .",
    "conf . on acoustics , speech , and",
    "signal proces .",
    "( icassp ) , _ pp : 27412744 , kyoto , japan , mar .",
    "2012 .",
    "r. m. willett , m. f. duarte , m. a. davenport , and r. g. baraniuk ,  sparsity and structure in hyperspectral imaging : sensing , reconstruction , and target detection , \" _ ieee signal process . mag .",
    "1 , pp : 116126 , jan . 2014 .                            z. zhang and b. d. rao ,  sparse signal recovery with temporally correlated source vectors using sparse bayesian learning , \" _ ieee j. select .",
    "topics signal process .",
    ", _ vol . 5 , no .",
    "5 , pp . 912926 , sep ."
  ],
  "abstract_text": [
    "<S> this paper considers a compressive sensing ( cs ) approach for hyperspectral data acquisition , which results in a practical compression ratio substantially higher than the state - of - the - art . applying simultaneous low - rank and joint - sparse ( l&s ) model to the hyperspectral data , we propose a novel algorithm to joint reconstruction of hyperspectral data based on loopy belief propagation that enables the exploitation of both structured sparsity and amplitude correlations in the data . </S>",
    "<S> experimental results with real hyperspectral datasets demonstrate that the proposed algorithm outperforms the state - of - the - art cs - based solutions with substantial reductions in reconstruction error .    compressive hyperspectral imaging , low - rank and joint - sparse , compressive sensing , approximate message passing </S>"
  ]
}