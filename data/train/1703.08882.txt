{
  "article_text": [
    "over the years , there has been an increased interest in the application of matrix variate approaches .",
    "one such application is multivariate longitudinal data , observations on multiple variables collected over time .",
    "although there are countless examples of clustering for multivariate distributions using finite mixture - models , there is very little work for matrix variate distributions .",
    "moreover , the examples in the literature deal exclusively with non - skew matrix variate distributions such as the matrix variate normal and the matrix variate @xmath0 distributions .",
    "we propose herein a mixture of matrix variate skew-@xmath0 distributions for use in clustering .",
    "the remainder of this paper is laid out as follows . in section  2 ,",
    "some background is presented . in section  3",
    ", we describe an expectation conditional maximization ( ecm ) algorithm for a mixture of matrix skew-@xmath0 distributions .",
    "section  4 looks at some simulations , and we conclude with a summary and some future work ( section  5 ) .",
    "clustering and classification look at finding and analyzing underlying group structures in data . there is no one agreed upon definition for a cluster and a detailed discussion of this can be found in @xcite .",
    "one common method used for clustering is model - based , and generally makes use of a @xmath1 component finite mixture model  @xcite and @xcite give reviews of work in model - based clustering .",
    "a random variate @xmath2 taken from a finite mixture model has density @xmath3 where @xmath4 , @xmath5 are the component densities and @xmath6 are called the mixing proportions with @xmath7 and @xmath8 .",
    "the earliest use of a finite mixture model for clustering can be found in @xcite , which uses a gaussian mixture model .",
    "other earlier work in this area can be found in @xcite and @xcite .",
    "although the gaussian mixture model is famous due to its mathematical tractability , other work has been done in the area of non - gaussian mixtures .",
    "examples include the @xmath0-distribution @xcite and the power exponential @xcite as well as many mixtures of asymmetric distributions , including the skew-@xmath0 distribution ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , the shifted asymmetric laplace distribution @xcite , and the generalized hyperbolic distribution @xcite .    even more recently , however , there has been an interest in the mixtures of matrix variate distributions , such as the work of @xcite who looked at multivariate longitudinal data with the matrix variate normal distribution .",
    "another example is @xcite , who considered a finite mixture of matrix variate @xmath0 distributions .",
    "three way data , such as multivariate longitudinal data , can be easily modelled using a matrix variate distribution .",
    "there are many examples of such distributions presented in the literature , the most notable being the matrix variate normal distribution . for notional clarity",
    ", we use @xmath2 to denote a realization of a random matrix @xmath9 .",
    "an @xmath10 random matrix @xmath9 follows an @xmath10 matrix variate normal distribution with location parameter @xmath11 and scale matrices @xmath12 and @xmath13 of dimensions @xmath14 and @xmath15 , respectively , denoted by @xmath16 if the density of @xmath9 can be written as @xmath17    one well known property of the matrix variate normal distribution @xcite is @xmath18 where @xmath19 is the multivariate normal density with dimension @xmath20 , @xmath21 is the vectorization of @xmath11 , and @xmath22 is the kronecker product .",
    "the matrix variate normal has many elegant mathematical properties that have made it so popular .",
    "however , there are non - normal examples such as the wishart distribution @xcite and the skew normal distribution , e.g. , @xcite , @xcite , and @xcite .",
    "more information on matrix variate distributions can be found in @xcite .",
    "@xcite , derived a @xmath10 matrix variate skew-@xmath0 distribution , denoted by @xmath23 as a special case of a matrix normal variance - mean mixture .",
    "specifically , a random matrix @xmath9 has a matrix variate skew-@xmath0 distribution with location @xmath11 , skewness @xmath24 , scale matrices @xmath12 and @xmath13 , and degrees of freedom @xmath25 if @xmath26 where @xmath11 and @xmath24 are @xmath10 matrices , @xmath27 , and @xmath28 , where @xmath29 represents the inverse gamma distribution with density @xmath30    it can then be shown that the density of @xmath9 is    @xmath31\\left[\\delta({\\mathbf{x}};{\\mathbf{m}},{\\mathbf\\sigma},{\\mathbf\\psi})+\\nu\\right]}\\right),\\end{aligned}\\ ] ]    where @xmath32 with some manipulation it can be shown using bayes theorem that @xmath33 where @xmath34 , where @xmath35 represents the generalized inverse gaussian distribution .",
    "the density of a @xmath36 random variable is @xmath37 where @xmath38 is the modified bessel function of the third kind with index @xmath39 .",
    "some useful expectations of functions of a @xmath40 random variable have a mathematical tractable form and are given by @xmath41 @xmath42 @xmath43 where @xmath44 , @xmath45 .",
    "in the mixture model context , we assume that a random matrix @xmath9 , following a matrix variate skew-@xmath0 distribution comes from a population with @xmath1 subgroups .",
    "the density of @xmath9 is then @xmath46    now suppose we observe @xmath47 matrices @xmath48 , then the observed - data likelihood is @xmath49    for the purposes of parameter estimation , we proceed as if the observed data is incomplete . in particular , we introduce the missing group membership indicators @xmath50 where , @xmath51 as well as the latent variables @xmath52 , where @xmath53    the complete - data log likelihood is then @xmath54 where @xmath55 { \\addtocounter{equation}{1}\\tag{\\theequation}}\\label{eq : complik2}\\\\\\end{aligned}\\ ] ] @xmath56{\\addtocounter{equation}{1}\\tag{\\theequation}}\\label{eq : complik3}\\end{aligned}\\ ] ]    where @xmath57 is constant with respect to the parameters .",
    "we proceed by using an expectation - conditional maximization ( ecm ) algorithm @xcite described overleaf .    * 1 ) initialization * : initialize the parameters @xmath58 . set @xmath59    *",
    "2 ) e step * : update @xmath60 , where @xmath61 where @xmath62[\\delta({\\mathbf{x}}_i;\\hat{{\\mathbf{m}}}_g^{(t)},\\hat{{\\mathbf\\sigma}}_g^{(t)},\\hat{{\\mathbf\\psi}}_g^{(t)})+\\hat{\\nu}_g^{(t)}]},\\ ] ] and @xmath63    * 3 ) first cm step * : update the parameters @xmath64 .",
    "@xmath65 where @xmath66 the update for the degrees of freedom can not be obtained in closed form . in the case where there is no restriction on @xmath67 between groups , the update for @xmath67 is given by solving for @xmath25 .",
    "@xmath68    where @xmath69 is the digamma function .",
    "* 4 ) second cm step * : update @xmath12 @xmath70 \\end{split}\\ ] ]    * 5 ) third cm step * : update @xmath13    @xmath71 \\end{split}\\ ] ]    * 6 ) check convergence * : if not converged , set @xmath72 and return to step 2 .",
    "there are several options for determining convergence of this ecm algorithm .",
    "the criterion used in the simulations in section  4 , is based on the aitken acceleration @xcite .",
    "the aitken acceleration at iteration @xmath0 is @xmath73 where @xmath74 is the ( observed ) log - likelihood at iteration @xmath0 .",
    "we then define @xmath75 ( cf . * ? ? ?",
    "* ; * ? ? ?",
    "the quantity @xmath76 is an asymptotic estimate ( i.e. , an estimate of the value after many iterations ) of the log - likelihood at iteration @xmath77 . as in @xcite",
    ", we stop our em algorithms when @xmath78 .",
    "the main benefit of this criterion when compared to lack of progress , is that the likelihood can sometimes  plateau \" before increasing again .",
    "if lack of progress was used , the algorithm would be terminated .",
    "however , this above criterion fixes the problem by considering the likelihood after very many iterations , i.e. @xmath76 .",
    "as discussed in @xcite and @xcite for parameter estimation in the matrix variate normal case , the estimates of @xmath79 and @xmath80 are unique only up to a multiplicative constant .",
    "indeed , we can take @xmath81 and @xmath82 , @xmath83 , and the likelihood would not change .",
    "however , we notice that , @xmath84 . therefore , the estimate of the kronecker product is unique .      in a general clustering scenario ,",
    "the number of groups , @xmath1 , are not known _ a priori_. it is therefore necessary to select an adequate number of groups .",
    "there are two methods that are very common in the literature .",
    "the first is the famous bayesian information criterion ( bic ; * ? ? ?",
    "* ) , which is defined as @xmath85 where @xmath86 is the estimated log likelihood , @xmath47 is the number of observations , and @xmath87 is the number of free parameters . the second criterion common in the literature is the integrated completed likelihood ( icl ; * ? ? ?",
    "* ) , which can be approximated as @xmath88 where @xmath89 the icl can be viewed as penalized version of the bic , penalized for uncertainty in the group membership .",
    "we performed two different simulations . in each simulation , we were interested in the both the number of selected groups , as well as the classification performance using the adjusted rand index ( ari ; * ? ? ?",
    "the ari is a method to compare two different classifications . when compared to the actual group memberships , an ari of 1 corresponds to perfect classification , while a value of 0 indicates that the classifier is no better than randomly choosing the group memberships .    in each simulation , we took 50 datasets , with 100 observations coming from each group .",
    "we then fit a mixture model of matrix variate skew-@xmath0 distributions with @xmath90 components .",
    "we used both the bic and icl for group selection .      in the first simulation , we considered two groups coming from a matrix variate skew-@xmath0 distribution with @xmath91 .",
    "the location matrices were    @xmath92 the skewness matrices were @xmath93 the @xmath12 matrices were    @xmath94 and finally , we took the @xmath13 matrices to be @xmath95    in figure [ fig : sim1 ] , we show the marginal distributions of the columns of a typical dataset labelled v1 , v2 , v3 , v4 .",
    "the dotted lines represent the true marginal location @xmath96 in orange and @xmath97 in yellow . for the first two columns ,",
    "the true marginal locations are the same for each component , and therefore , only the yellow is seen .",
    "we see that there is very little separation between the two groups .        in table",
    "[ tab : sim1 ] , we show the number of times each component was selected , as well as the average ari .",
    "we see that for the most part , the mixture model does well with selecting the correct number of components with a very high average ari ( @xmath98 ) .",
    "this is a little unexpected due to the closeness of the two components , especially for column 2 .",
    "we note that the results when using the icl were identical and therefore not shown in the table .",
    ".the number of groups selected by the bic and icl as well as the average ari for simulation 1 [ cols=\"<,>,>,>,>,>\",options=\"header \" , ]     [ tab : sim2 ]",
    "parameter estimation for a mixture of matrix variate skew-@xmath0 distributions was discussed .",
    "this was then applied to simulated data .",
    "we saw that even though the components were quite close in terms of location , we obtained good clustering results .",
    "moreover , the average ari in both simulations were quite high .",
    "in the multivariate setting , the multivariate skew-@xmath0 distribution is a special case of the generalized hyperbolic distribution .",
    "it is therefore reasonable to consider extending this work to a matrix variate version of the generalized hyperbolic distribution .",
    "moreover , it would be fairly simple to extend this to the general semi - supervised classification scenario , where some of the observations have known labels .",
    "this would just require an alteration of the observed and complete data likelihood functions , and the actual updates in the ecm algorithm would require minimal changes .",
    "andrews , j.  l.  mcnicholas , p.  d. 2011 _ b _ , ` mixtures of modified t - factor analyzers for model - based clustering , classification , and discriminant analysis ' , _ journal of statistical planning and inference _ * 141*(4 ) ,  14791486 .",
    "baum , l.  e. , petrie , t. , soules , g.  weiss , n. 1970 , ` a maximization technique occurring in the statistical analysis of probabilistic functions of markov chains ' , _ annals of mathematical statistics _ * 41 * ,  164171 .",
    "biernacki , c. , celeux , g.  govaert , g. 2000 , ` assessing a mixture model for clustering with the integrated completed likelihood ' , _ ieee transactions on pattern analysis and machine intelligence _ * 22*(7 ) ,  719725 .",
    "bhning , d. , dietz , e. , schaub , r. , schlattmann , p.  lindsay , b. 1994 , ` the distribution of the likelihood ratio for mixtures of densities from the one - parameter exponential family ' , _ annals of the institute of statistical mathematics _ * 46 * ,  373388 .",
    "domnguez - molina , j.  a. , gonzlez - faras , g. , ramos - quiroga , r.  gupta , a.  k. 2007 , ` a matrix variate closed skew - normal distribution with applications to stochastic frontier analysis ' , _ communications in statistics?theory and methods _ * 36*(9 ) ,  16911703 .",
    "lindsay , b.  g. 1995 , mixture models : theory , geometry and applications , _ in _ ` nsf - cbms regional conference series in probability and statistics ' , vol .  5 , hayward , california : institute of mathematical statistics .",
    "mcnicholas , p.  d. , murphy , t.  b. , mcdaid , a.  f.  frost , d. 2010 , ` serial and parallel implementations of model - based clustering via parsimonious gaussian mixture models ' , _ computational statistics and data analysis _ * 54*(3 ) ,  711723 ."
  ],
  "abstract_text": [
    "<S> clustering is the process of finding underlying group structures in data . </S>",
    "<S> although model - based clustering is firmly established in the multivariate case , there is relative paucity for matrix variate distributions , and there are even fewer examples using matrix variate skew distributions . in this paper , we look at parameter estimation for a finite mixture of matrix variate skew-@xmath0 distributions in the context of model - based clustering . </S>",
    "<S> simulated data is used for illustrative purposes . </S>"
  ]
}