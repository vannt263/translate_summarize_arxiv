{
  "article_text": [
    "finite mixture models presented as powerful and flexible tools for discovering heterogeneity in multivariate datasets . assuming no prior knowledge of class labels , the application of finite mixture models in this way is known as model - based clustering . as @xcite points out , the association between mixture models and clustering goes back at least as far as @xcite , who uses the former as a means of defining the latter .",
    "gaussian mixture models are historically the most popular tool for model - based clustering and dominated the literature for quite some time  ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "the multivariate @xmath0-distribution , being a heavy - tailed alternative to the multivariate gaussian distribution , made ( robust ) mixture modelling based on mixtures of multivariate @xmath0-distributions the most natural extension ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "in many practical situations , however , real world datasets exhibit clusters that are not just heavy tailed but also asymmetric ; furthermore , clusters can also be asymmetric yet not heavy tailed .",
    "over the few past years , much attention has been paid to non - gaussian approaches to model - based clustering and classification , including work on multivariate skew-@xmath0 distributions ( e.g. , * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ) , shifted asymmetric laplace distributions @xcite , multivariate power exponential distributions @xcite , multivariate normal inverse gaussian distributions @xcite , and generalized hyperbolic distributions @xcite .",
    "a comprehensive review of model - based clustering work , up to and including some recent work on non - gaussian mixtures , is given by @xcite .",
    "unobserved or missing observations are frequently a hindrance in multivariate datasets and so developing mixture models that can accommodate incomplete data is an important issue in model - based clustering .",
    "the maximum likelihood and bayesian approaches are two common imputation paradigms for analyzing data with incomplete observations .",
    "herein , the missing data mechanism is assumed to be missing at random ( mar ) , as per @xcite and @xcite , meaning that the probability that a variable is missing for a particular individual depends only on the observed data and not on the value of the missing variable .",
    "note that missing completely at random ( mcar ) is a special case of mar . under mar ,",
    "the missing data mechanisms are ignorable for methods using the maximum likelihood approach .",
    "the maximum likelihood approach to clustering incomplete data has been well studied and is often used , particularly for gaussian mixture models ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "@xcite present a framework maximum likelihood estimation using an expectation - maximization ( em ) algorithm @xcite to fit a mixture of multivariate @xmath0-distributions with arbitrary missing data patterns , which was generalized by @xcite to efficient supervised learning via the parameter expanded ( px - em ) algorithm @xcite through two auxiliary indicator matrices . @xcite further develops a family of multivariate-@xmath0 mixture models with 14 eigen - decomposed scale matrices in the presence of missing data through a computationally flexible em algorithm by incorporating two auxiliary indicator matrices .",
    "we consider fitting mixtures of generalized hyperbolic distributions ( mghd ) and mixtures of multivariate skew - t distributions ( mst ) with missing information . in each case",
    ", an em algorithm is used for model selection .",
    "the chosen formulation of the ( multivariate ) generalized hyperbolic distribution ( ghd ) is that used by @xcite and has formulations of several well - known distributions as special cases such as the multivariate skew-@xmath0 , normal inverse gaussian , variance - gamma , laplace , and gaussian distributions ( cf .",
    "in addition to considering missing data , we develop families of mghd and mst mixture models , each with 14 parsimonious eigen - decomposed scale matrices corresponding to the famous gaussian parsimonious clustering models ( gpcms ) of @xcite and @xcite .",
    "the random variable @xmath1 is said to have a generalized inverse gaussian ( gig ) distribution , introduced by @xcite , with parameters @xmath2 , @xmath3 , and @xmath4 if its probability density function is given by @xmath5 where @xmath6 , and @xmath7 is the modified bessel function of the third kind with index @xmath2 .",
    "herein , we write @xmath8 to indicate that a random variable @xmath9 has the gig density as parameterized in . the gig distribution has some attractive properties @xcite , including the tractability of the expectations : @xmath10 = \\left(\\frac{\\chi}{\\psi}\\right)^{\\alpha/2}\\frac{k_{\\lambda+\\alpha}(\\sqrt{\\psi\\chi})}{k_{\\lambda}(\\sqrt{\\psi\\chi})},\\ ] ] for @xmath11 , and @xmath12 = \\log\\left(\\sqrt{\\frac{\\chi}{\\psi}}\\right)+\\frac{\\partial}{\\partial\\lambda}\\log(k_\\lambda(\\sqrt{\\psi\\chi})).\\ ] ] specifically , for @xmath13 and @xmath14 , we have @xmath15 & = \\sqrt{\\frac{\\chi}{\\psi}}\\frac{k_{\\lambda+1}(\\sqrt{\\psi\\chi})}{k_{\\lambda}(\\sqrt{\\psi\\chi})},\\\\ \\nonumber \\operatorname{\\mathbb{e}}[{1}/{w } ] & = \\sqrt{\\frac{\\psi}{\\chi}}\\frac{k_{\\lambda-1}(\\sqrt{\\psi\\chi})}{k_{\\lambda}(\\sqrt{\\psi\\chi } ) } = \\sqrt{\\frac{\\psi}{\\chi}}\\frac{k_{\\lambda+1}(\\sqrt{\\psi\\chi})}{k_{\\lambda}(\\sqrt{\\psi\\chi})}-\\frac{2\\lambda}{\\chi}. \\nonumber\\end{aligned}\\ ] ]    @xcite introduce another parameterization of the gig distribution by setting @xmath16 and @xmath17 . write @xmath18 ; its density is given by @xmath19 for @xmath20 , where @xmath21 is a scale parameter and @xmath22 is a concentration parameter .",
    "these two parameterizations of the gig distribution are important ingredients for building the generalized hyperbolic distribution presented later .",
    "several alternative parameterizations of the ghd have appeared in the literature , e.g. , @xcite , @xcite , and @xcite .",
    "@xcite introduces the generalized hyperbolic distribution ( ghd ) to model the distribution of the sand grain sizes and subsequent reports described its statistical properties  ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) . however , under this standard parameterization , the parameters of the mixing distribution are not invariant by affine transformations .",
    "an important innovation was made by  @xcite , who gave a new parameterization of the ghd . under this new parameterization ,",
    "the linear transformation of ghd remains in the same sub - family characterized by the parameters of the mixing distribution .",
    "however , there is an identifiability issue arising under this parameterization . to solve this problem ,",
    "@xcite give an alternative parameterization .",
    "following  @xcite , a @xmath23 random vector @xmath24 is said to follow a generalized hyperbolic distribution with index parameter @xmath2 , concentration parameters @xmath3 and @xmath4 , location vector @xmath25 , dispersion matrix @xmath26 , and skewness vector @xmath27 , denoted by @xmath28 , if it can be represented by @xmath29 where @xmath8 , @xmath30 , the symbol @xmath31 indicates independence , and it follows that @xmath32 .",
    "so , the density of the generalized hyperbolic random vector @xmath24 is given by @xmath33^{\\frac{\\lambda - p/2}{2}}\\frac{(\\psi/\\chi)^{\\lambda/2}k_{\\lambda - p/2}\\left(\\sqrt{(\\chi + \\delta({\\mathbf{x}},{\\mbox{\\boldmath$\\mu$}}\\mid { \\mbox{\\boldmath$\\sigma$}}))(\\psi+{\\mbox{\\boldmath$\\alpha$}}\\intercal{\\mbox{\\boldmath$\\sigma$}}{^{\\raisebox{.2ex}{$\\scriptscriptstyle-1$}}}{\\mbox{\\boldmath$\\alpha$}})}\\right)}{(2\\pi)^{p/2 } |{\\mbox{\\boldmath$\\sigma$}}|^{1/2}k_\\lambda(\\sqrt{\\chi\\psi})\\exp\\{-({\\mathbf{x}}-{\\mbox{\\boldmath$\\mu$}})\\intercal{\\mbox{\\boldmath$\\sigma$}}{^{\\raisebox{.2ex}{$\\scriptscriptstyle-1$}}}{\\mbox{\\boldmath$\\alpha$}}\\}},\\ ] ] where @xmath34 is the squared mahalanobis distance between @xmath35 and @xmath25 , @xmath7 is the modified bessel function of the third kind with index @xmath2 , and @xmath36 denotes the model parameters .",
    "the mean and covariance matrix of @xmath24 are @xmath37 respectively , where @xmath38 and @xmath39 are the mean and variance of the random variable  @xmath9 , respectively .",
    "note that , in this parameterization , we need to hold @xmath40 to ensure identifiability . using @xmath40 solves the identifiability problem but would be prohibitively restrictive for model - based clustering and classification applications .",
    "hence ,  @xcite develop a new parameterization of the ghd with index parameter @xmath2 , concentration parameter @xmath41 , location vector @xmath25 , dispersion matrix @xmath26 , and skewness vector @xmath42 , denoted by @xmath43 .",
    "note that @xmath44 .",
    "this formulation is given by @xmath45 where @xmath46 and @xmath30 .",
    "under this parameterization , the density of the generalized hyperbolic random vector @xmath24 is @xmath47^{\\frac{\\lambda - p/2}{2}}\\frac{k_{\\lambda - p/2}\\left(\\sqrt{(\\omega + \\delta({\\mathbf{x}},{\\mbox{\\boldmath$\\mu$}}\\mid { \\mbox{\\boldmath$\\sigma$}}))(\\omega+{\\mbox{\\boldmath$\\beta$}}^{\\intercal}{\\mbox{\\boldmath$\\sigma$}}{^{\\raisebox{.2ex}{$\\scriptscriptstyle-1$}}}{\\mbox{\\boldmath$\\beta$}})}\\right)}{(2\\pi)^{p/2 } |{\\mbox{\\boldmath$\\sigma$}}|^{1/2}k_\\lambda(\\omega)\\text{exp}\\{-({\\mathbf{x}}-{\\mbox{\\boldmath$\\mu$}})^{\\intercal}{\\mbox{\\boldmath$\\sigma$}}{^{\\raisebox{.2ex}{$\\scriptscriptstyle-1$}}}{\\mbox{\\boldmath$\\beta$}}\\}},\\ ] ] where @xmath48 and @xmath49 are as described earlier .",
    "we use this parameterization when we describe parameter estimation ( cf .  section  [ sec : mghd ] ) .",
    "the following result shows an appealing closure property of the generalized hyperbolic distribution under affine transformation and conditioning as well as the formation of marginal distributions , which is useful for developing new methods presented later .",
    "suppose that @xmath24 is a @xmath50-dimensional random vector having a generalized hyperbolic distribution as in , i.e. , @xmath43 .",
    "assume that @xmath24 is partitioned as @xmath51 , where @xmath52 takes values in @xmath53 and @xmath54 in @xmath55 , with @xmath56 where @xmath24 , @xmath25 , and @xmath57 have similar partitions .",
    "furthermore , @xmath58 is @xmath59 and @xmath60 is @xmath61 .",
    "affine transformation of the generalized hyperbolic distribution .",
    "if @xmath43 and @xmath62 where @xmath63 and @xmath64 , then @xmath65    the result follows by substituting into @xmath62 .    the marginal distribution of @xmath52 is a generalized hyperbolic distribution as in with index parameter @xmath2 , concentration parameter @xmath41 , location vector @xmath66 , dispersion matrix @xmath58 , and skewness vector @xmath67 , i.e. , @xmath68 .",
    "the result follows by applying proposition 1 and choosing @xmath69 and @xmath70 .",
    "the parameters @xmath71 inherited from the mixing distribution @xmath72 remain the same under the affine transformation and marginal distribution .",
    "the conditional distribution of @xmath54 given @xmath73 is a generalized hyperbolic distribution as in , i.e. , @xmath74 , where @xmath75    the proof of proposition 3 is given in appendix  b.      there are several alternative formulations of multivariate skew - t distributions appearing in the literature ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "@xcite develop a mixture of multivariate skew - t distributions incomplete data using the formulation of @xcite . herein , the formulation of the multivariate skew-@xmath0 distribution arising from the generalized hyperbolic distribution is used .",
    "this formulation of the multivariate skew-@xmath0 distribution has been used by @xcite to develop a mixture of skew-@xmath0 factor analyzers model .",
    "following @xcite , a @xmath50 x @xmath76 random vector @xmath24 is said to follow a multivariate skew - t distribution with degree of freedom parameter @xmath77 , location vector @xmath25 , dispersion matrix @xmath26 , and skewness vector @xmath57 , denoted by @xmath78 , if it can be represented by @xmath79 where @xmath80 , @xmath30 , with @xmath81 denoting the inverse gamma distribution .",
    "it follows that @xmath82 and the pdf of the multivariate skew - t random vector @xmath24 is given by @xmath83^{\\frac{-v - p}{4}}\\frac{v^{v/2}k_{(-v - p)/2}\\left(\\sqrt{(v + \\delta({\\mathbf{x}},{\\mbox{\\boldmath$\\mu$}}\\mid { \\mbox{\\boldmath$\\sigma$}}))({\\mbox{\\boldmath$\\beta$}}^{\\intercal}{\\mbox{\\boldmath$\\sigma$}}{^{\\raisebox{.2ex}{$\\scriptscriptstyle-1$}}}{\\mbox{\\boldmath$\\beta$}})}\\right)}{(2\\pi)^{p/2 } |{\\mbox{\\boldmath$\\sigma$}}|^{1/2}\\gamma(v/2)2^{v/2 - 1}\\text{exp}\\{-({\\mathbf{x}}-{\\mbox{\\boldmath$\\mu$}})^{\\intercal}{\\mbox{\\boldmath$\\sigma$}}{^{\\raisebox{.2ex}{$\\scriptscriptstyle-1$}}}{\\mbox{\\boldmath$\\beta$}}\\}}.\\ ] ] this formulation of the multivariate skew - t distribution can be obtained as a special case of the generalized hyperbolic distribution by setting @xmath84 and @xmath85 , and letting @xmath86 .",
    "similarly , this formulation of the multivariate skew - t distribution has a closed form under affine transformation and conditioning , and the formation of marginal distributions , which is useful for developing new methods presented later .",
    "suppose that @xmath24 is a @xmath50-dimensional random vector having the multivariate skew - t distribution as in , i.e. , @xmath78 .",
    "assume that @xmath24 is partitioned as @xmath51 , where @xmath52 takes values in @xmath53 and @xmath54 in @xmath55 , with @xmath87 where @xmath24 , @xmath25 , and @xmath57 have similar partitions .",
    "furthermore , @xmath58 is @xmath59 and @xmath60 is @xmath61 .",
    "affine transformation of the multivariate skew - t distribution .",
    "if @xmath78 and @xmath62 , where @xmath63 and @xmath64 , then @xmath88    the proof follows easily by substituting into @xmath62 .    the marginal distribution of @xmath52 is a multivariate skew - t distribution as in with degree of freedom parameter @xmath77 , location vector @xmath66 , dispersion matrix @xmath58 , and skewness vector @xmath67 , i.e. , @xmath89 .",
    "the proof follows easily by applying proposition 4 and choosing @xmath69 and @xmath70 .",
    "the degree of freedom parameter @xmath77 inherited from the mixing distribution @xmath90 remains invariant under affine transformation and marginal distribution .",
    "the conditional distribution of @xmath54 given @xmath73 is a generalized hyperbolic distribution as in , i.e. , @xmath91 , where @xmath92    the proof of proposition 6 is similar to that for proposition 3 , hence is omitted .",
    "let @xmath93 be @xmath50-dimensional random variables arising from a heterogeneous population with @xmath94 disjoint mghd subpopulations .",
    "that is , each @xmath95 has the density @xmath96 where @xmath97 , such that @xmath98 are the mixing proportions , @xmath99 denotes the model parameters , and @xmath100 is the ghd density defined in .    to apply the mghd model in the clustering paradigm ,",
    "introduce @xmath101 , where @xmath102 if observation @xmath103 is in component @xmath104 and @xmath105 otherwise .",
    "we have @xmath106 , i.e. , @xmath107 follows a multinomial distribution with one trial and cell probabilities @xmath108 .",
    "a three - level hierarchical representation of the mghd model can be expressed by @xmath109 the complete - data consist of the observed @xmath110 together with the missing group membership @xmath111 and the latent @xmath112 , for @xmath113 and @xmath114 , and the complete - data log - likelihood is given by @xmath115.\\ ] ]    @xcite present an em algorithm for parameter estimation with the mghd when there is no missing data in @xmath116 .",
    "we are interested in parameter estimation for the mghd model when @xmath116 are partially observed with arbitrary missing patterns .",
    "the missing data mechanism is assumed to be mar .",
    "assume now that we split @xmath110 into two components , @xmath117 and @xmath118 that denote the observed and missing components of @xmath110 , respectively .",
    "in general , each data vector @xmath110 may have a different pattern of missing features , i.e. , @xmath119 , but can be simplified for the sake of clarity .",
    "for each @xmath120 , partition the vector mean @xmath121 , where @xmath122 and @xmath123 denote the sub - vectors of @xmath124 matching the observed and missing components of @xmath110 , respectively . similarly , the skewness vector is @xmath125 and the covariance matrix @xmath126 as @xmath127 correspond to @xmath120 . as a result , in addition to the observed @xmath117 , the missing group membership @xmath111 , and the latent variable @xmath112 , the complete - data also include the missing data @xmath118 . in the framework of the em algorithm ,",
    "the missing data @xmath118 are considered to be random variables that are updated in each iteration .",
    "hence , the complete - data log - likelihood is rewritten as @xmath128.\\end{aligned}\\ ] ] given ( [ eqn : mghdthreelevel ] ) , we establish the following :    * the marginal distribution of @xmath129 given is @xmath130 where @xmath131 is the dimension corresponding to the observed component @xmath117 , which should be exactly written as @xmath132 but here is simplified .",
    "* the conditional distribution of @xmath133 given @xmath117 and @xmath102 , according to proposition  3 , is @xmath134 where @xmath135 * the conditional distribution of @xmath133 given @xmath136 , and @xmath102 is @xmath137 * the conditional distribution of @xmath138 given @xmath117 and @xmath102 is @xmath139    after a little algebra , we get the complete data log - likelihood function is @xmath140\\\\ & -\\frac{1}{2}{\\sum_{i=1}^n}{\\sum_{g=1}^g}\\text{tr}\\left ( { \\mbox{\\boldmath$\\sigma$}}_g{^{\\raisebox{.2ex}{$\\scriptscriptstyle-1$}}}z_{ig } \\frac{1}{w_{ig}}\\begin{pmatrix } ( { \\mathbf{x}}_i^{\\text{o}}-{\\mbox{\\boldmath$\\mu$}}_{g , i}^{\\text{o}})({\\mathbf{x}}_i^{\\text{o}}-{\\mbox{\\boldmath$\\mu$}}_{g , i}^{\\text{o}})^{\\intercal } & ( { \\mathbf{x}}_i^{\\text{o}}-{\\mbox{\\boldmath$\\mu$}}_{g , i}^{\\text{o}})({\\mathbf{x}}_i^{\\text{m}}-{\\mbox{\\boldmath$\\mu$}}_{g , i}^{\\text{m}})^{\\intercal}\\\\({\\mathbf{x}}_i^{\\text{m}}-{\\mbox{\\boldmath$\\mu$}}_{g , i}^{\\text{m}})^{\\intercal}({\\mathbf{x}}_i^{\\text{o}}-{\\mbox{\\boldmath$\\mu$}}_{g , i}^{\\text{o}})&({\\mathbf{x}}_i^{\\text{m}}-{\\mbox{\\boldmath$\\mu$}}_{g , i}^{\\text{m}})({\\mathbf{x}}_i^{\\text{m}}-{\\mbox{\\boldmath$\\mu$}}_{g , i}^{\\text{m}})^{\\intercal}\\end{pmatrix } \\right)\\\\ & + \\frac{1}{2}{\\sum_{i=1}^n}{\\sum_{g=1}^g}\\text{tr}\\left (   { \\mbox{\\boldmath$\\sigma$}}_g{^{\\raisebox{.2ex}{$\\scriptscriptstyle-1$}}}z_{ig } \\begin{pmatrix}{\\mbox{\\boldmath$\\beta$}}_{g , i}^{\\text{o}}\\\\{\\mbox{\\boldmath$\\beta$}}_{g , i}^{\\text{m}}\\end{pmatrix}\\begin{pmatrix } ( { \\mathbf{x}}_i^{\\text{o}}-{\\mbox{\\boldmath$\\mu$}}_{g , i}^{\\text{o}})^{\\intercal}&({\\mathbf{x}}_i^{\\text{m}}-{\\mbox{\\boldmath$\\mu$}}_{g , i}^{\\text{m}})^{\\intercal}\\end{pmatrix}\\right)\\\\ & + \\frac{1}{2}{\\sum_{i=1}^n}{\\sum_{g=1}^g}\\text{tr}\\left (   { \\mbox{\\boldmath$\\sigma$}}_g{^{\\raisebox{.2ex}{$\\scriptscriptstyle-1$}}}z_{ig } \\begin{pmatrix } { \\mathbf{x}}_i^{\\text{o}}-{\\mbox{\\boldmath$\\mu$}}_{g , i}^{\\text{o}}\\\\{\\mathbf{x}}_i^{\\text{m}}-{\\mbox{\\boldmath$\\mu$}}_{g , i}^{\\text{m}}\\end{pmatrix}\\begin{pmatrix}{\\mbox{\\boldmath$\\beta$}}_{g , i}^{\\text{o}\\intercal}&{\\mbox{\\boldmath$\\beta$}}_{g , i}^{\\text{m}\\intercal}\\end{pmatrix}\\right)-\\frac{1}{2}{\\sum_{i=1}^n}{\\sum_{g=1}^g}z_{ig}w_{ig}{\\mbox{\\boldmath$\\beta$}}_{g , i}^{\\intercal}{\\mbox{\\boldmath$\\sigma$}}_g{^{\\raisebox{.2ex}{$\\scriptscriptstyle-1$}}}{\\mbox{\\boldmath$\\beta$}}_{g , i}\\\\ & + { \\sum_{i=1}^n}{\\sum_{g=1}^g}z_{ig}\\left[(\\lambda_g-1)\\log w_{ig}-\\log(2k_{\\lambda_g}(\\omega_g))-\\frac{\\omega_g}{2}\\left(w_{ig}+\\frac{1}{w_{ig}}\\right)\\right ] .",
    "\\end{split}\\ ] ]    on the @xmath141th iteration of the e - step , the expected value of the complete data log - likelihood is computed given the observed data @xmath142 and the current parameter updates @xmath143 .",
    "that is , we need to compute @xmath144 , @xmath145 , @xmath146 , @xmath147 , @xmath148 , and @xmath149 .",
    "first , let @xmath150 denote the _ a  posteriori _ probability that @xmath103-th observation belongs to the @xmath104-th component of the mixture , based on the observed data : @xmath151 given ( [ eqn : expgig ] ) , ( [ eqn : expgiglog ] ) , and ( [ eqn : wconpdf ] ) , we have the following expectations as to the latent variable @xmath9 : @xmath152 @xmath153 for convenience , we use the following notation analogous to @xcite : @xmath154 , @xmath155 , @xmath156 , and @xmath157 .",
    "for the actual missing data @xmath158 , we will also need the following expectations : @xmath159    on the @xmath141-th iteration of the m - step , the expected value of the complete data log - likelihood is maximized to get the updates for the parameter estimates as follows : @xmath160 where @xmath161 { { \\mbox{\\boldmath$\\sigma$}}}_{ig}^{(k+1 ) } & = \\begin{pmatrix}b_{ig}^{(k)}({\\mathbf{x}}_i^{\\text{o}}-{\\mbox{\\boldmath$\\mu$}}_g^{\\text{o}(k+1)})({\\mathbf{x}}_i^{\\text{o}}-{\\mbox{\\boldmath$\\mu$}}_g^{\\text{o}(k+1)})^{\\intercal}&({\\mathbf{x}}_i^{\\text{o}}-\\hat{{\\mbox{\\boldmath$\\mu$}}}_g^{\\text{o}(k+1)})(\\tilde{{\\mathbf{x}}}_{ig}^{\\text{m}(k)}-b_{ig}^{(k)}\\hat{{\\mbox{\\boldmath$\\mu$}}}_g^{\\text{m}(k+1)})^{\\intercal}\\\\(\\tilde{{\\mathbf{x}}}_{ig}^{\\text{m}(k)}-b_{ig}^{(k)}\\hat{{\\mbox{\\boldmath$\\mu$}}}_g^{\\text{m}(k+1)})({\\mathbf{x}}_i^{\\text{o}}-{\\mbox{\\boldmath$\\mu$}}_g^{\\text{o}(k+1)})^{\\intercal}&\\mathbf{k}_{ig}^{\\text{m}(k+1)}\\end{pmatrix},\\end{aligned}\\ ] ] where @xmath162    finally , the estimates of @xmath163 and @xmath164 are given as solutions to maximize the function @xmath165 and the associated updates are @xmath166{^{\\raisebox{.2ex}{$\\scriptscriptstyle-1$}}},\\\\   { \\omega}_g^{(k+1 ) } & = { \\omega}_g^{(k)}-\\left[\\frac{\\partial}{\\partial{\\omega}_g^{(k)}}q_g({\\lambda}_g^{(k+1)},{\\omega}_g^{(k)})\\right]\\left[\\frac{\\partial^2}{\\partial{\\omega}_g^{2(k)}}q_g({\\lambda}_g^{(k+1)},{\\omega}_g^{(k)})\\right]{^{\\raisebox{.2ex}{$\\scriptscriptstyle-1$}}}.   \\end{aligned}\\ ] ]    details on the mst with incomplete data are analogous to the mghd with incomplete data and are given in appendix  [ sec : mst ] .",
    "it is well known that the em algorithm can be heavily dependent on the initial values ; indeed , good initial values of parameter estimates may speed up convergence . in this study , the following procedure for automatically generating initial values is used , unless otherwise specified .    *",
    "fill in the missing values based on the mean imputation method .",
    "* perform @xmath141-means clustering and use the resulting clustering membership to initialize the _ a  posteriori _",
    "probability @xmath167 . accordingly ,",
    "the initial values for the model parameters are then given by : @xmath168 * set the skewness parameter @xmath169 to be close to zero for symmetric data .",
    "* when applicable , we set @xmath170 and @xmath171 for the index and concentration parameters and set @xmath172 for the near - normality assumption .",
    "to enhance the computational efficiency of the em algorithm , we update the parameters per missing pattern instead of per individual .",
    "we suggest rearranging @xmath24 according to unique patterns of the missing data . the procedure can be implemented as follows :",
    "* build a binary @xmath173 by @xmath50 indicator matrix @xmath174 $ ] , with each entry @xmath175 if @xmath176 is missing and @xmath177 otherwise ; * find all unique missing patterns ; and * update parameters per missing pattern instead of per individual .      in general , the number of mixture components @xmath94 is not known _ a priori _ , and needs to be estimated from the data .",
    "two widely used model selection techniques are the bayesian information criterion ( bic ; * ? ? ?",
    "* ) and the integrated completed likelihood ( icl ; * ? ? ?",
    "* ) , which are given respectively by @xmath178 where @xmath179 is the maximized log - likelihood evaluated at the maximum likelihood estimate @xmath180 , @xmath181 is the number of free parameters , @xmath173 is the number of observations , @xmath182 represents the estimated _ a  posteriori _ probability that @xmath110 arises from the @xmath104th component , and denotes the maximum _",
    "a  posteriori _ probability such that @xmath183 if @xmath184 occurs in the @xmath104th component and @xmath185 otherwise",
    ". the bigger the bic or icl value , the better the fitted model .",
    "the em algorithm can be stopped iterations after the maximum number of iterations , or when the aitken stopping criterion  @xcite is satisfied .",
    "the aitken acceleration at iteration @xmath141 is @xmath186 where @xmath187 is the log - likelihood at iterations @xmath141 .",
    "this yields an asymptotic estimate of the log - likelihood at iteration @xmath188 : @xmath189 @xcite , and the em algorithm is stopped when @xmath190 @xcite .",
    "studies based on both simulated and real datasets are used to compare the clustering performance of the proposed approach .",
    "the simulated datasets are each two - component mixtures : a mixture of gaussian distributions ( gmm ) with a general vee covariance structure , a mixture of skew - t distributions ( mst ) with a diagonal vei covariance structure , and a mixture of generalized hyperbolic distributions ( mghd ) with a general vee covariance structure .",
    "the gmm datasets are generated via the function from the mvtnorm package for , and the mst and mghd datasets are generated using r code based on the stochastic representations in and , respectively .",
    "for each mixture component , @xmath191 two - dimensional vectors @xmath110 are generated .",
    "the presumed parameters of @xmath126 ( @xmath192 ) for the vee and vei models are the same as those considered in @xcite and @xcite .",
    "each mixture component is centred on a different point giving well - separated and overlapping mixtures . where applicable , the skewness parameters are @xmath193 and @xmath194 , the degrees of freedoms for the mst is @xmath195 , and the values of other parameters for the mghd are @xmath196 and @xmath197 .",
    "for each scenario , we create artificially incomplete datasets by removing data through an mar mechanism from the simulated samples under missing rates @xmath198 ranging from 5% to 30% while maintaining the condition that each observation has at least one observed attribute . then our proposed model for incomplete data is compared to the mghd and mst for complete data once missing data have been ` filled - in ' with the sample mean of the associated attribute , via the mean imputation method .",
    "the misclassification rate @xmath199 and the adjusted rand index ( ari ; * ? ? ?",
    "* ) are used to compare predicted classifications with true classes .",
    "the datasets considered in the simulation studies are summarized in table  [ table : sim ] and plotted in figure  [ figure : sim ] .",
    "the datasets are overlapping , making this a relatively difficult clustering scenario even when the datasets are complete .",
    ".summary of simulated datasets [ cols=\"<,>,>,>\",options=\"header \" , ]",
    "approaches for clustering incomplete data where clusters may be heavy tailed and/or asymmetric is introduced , based on mghd and mst .",
    "there approaches were further extended to parsimonious families of mghd and mst models via eigen - decomposition of the component scale matrices .",
    "the bic and icl were used for model selection .",
    "it is well known that the bic can tend to overestimate the number of clusters in practice ; however , the results presented herein show that this overestimation can sometimes be mitigated via a more flexible component density such as the mghd .",
    "an em algorithm was developed to fit the mghd and mst models to incomplete data , and later implemented in r. it is worth mentioning that our approaches are also applicable in situations with no missing data ; and so we have mghd and mst analogues of the models of @xcite .",
    "our mghd and mst models were applied to real and simulated heterogeneous datasets for clustering in the presence of missing values , and the pmghd family performed favourably when compared to the pmst family as well as the mghd and mst approaches with mean imputation .",
    "going forward , the pmghd and pmst approaches for clustering with missing values can easily be extended to model - based classification , discriminant analysis , and density estimation .",
    "furthermore , bayesian analysis via a gibbs sampler is another popular approach to handle missing data in multivariate datasets ( e.g. , * ? ? ?",
    "* ) , so a fully bayesian treatment will be considered as an alternative to the em algorithm for parameter estimation .",
    "finally , it will also be interesting to generalize all existing approaches to developing mixture of generalized hyperbolic factor analyzer models and mixtures of multiple scaled generalized hyperbolic distributions for incomplete data @xcite .",
    "barndorff - nielsen , o. and p.  blsild ( 1981 ) .",
    "hyperbolic distributions and ramifications : contributions to theory and application . in c.",
    "taillie , g.  patil , and b.  baldessari ( eds . ) , _ statistical distributions in scientific work _ , volume  79 of _ nato advanced study institutes series _ , pp .",
    "springer netherlands .",
    "bhning , d. , e.  dietz , r.  schaub , p.  schlattmann , and b.  lindsay ( 1994 ) .",
    "the distribution of the likelihood ratio for mixtures of densities from the one - parameter exponential family .   _",
    "46_(2 ) , 373388 ."
  ],
  "abstract_text": [
    "<S> robust clustering from incomplete data is an important topic because , in many practical situations , real data sets are heavy - tailed , asymmetric , and/or have arbitrary patterns of missing observations . </S>",
    "<S> flexible methods and algorithms for model - based clustering are presented via mixture of the generalized hyperbolic distributions and its limiting case , the mixture of multivariate skew - t distributions . </S>",
    "<S> an analytically feasible em algorithm is formulated for parameter estimation and imputation of missing values for mixture models employing missing at random mechanisms . </S>",
    "<S> the proposed methodologies are investigated through a simulation study with varying proportions of synthetic missing values and illustrated using a real dataset . </S>",
    "<S> comparisons are made with those obtained from the traditional mixture of generalized hyperbolic distribution counterparts by filling in the missing data using the mean imputation method . </S>"
  ]
}