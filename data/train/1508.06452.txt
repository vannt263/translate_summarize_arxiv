{
  "article_text": [
    "an inverse problem converts noisy , incomplete , and possibly indirect observations to characterizations of some unknown parameters or states of a physical system .",
    "these unknowns are often functions defined on a spatial domain and linked to the observables via a forward model .",
    "high dimensionality due to the numerical discretization of the unknown functions is often viewed as one of the grand challenges in designing scalable inference methods .",
    "this challenge motivates the development of dimension reduction approaches , which exploit the possibly low - dimensional intrinsic structure of the inverse problem , to alleviate the effect of the `` curse of dimensionality . ''",
    "a typical inverse problem is ill - posed ; the unknowns are not uniquely identified by the observations .",
    "this is a joint effect of noisy , incomplete observations and the smoothing properties of the forward model . in the bayesian inference framework @xcite",
    ", ill - posedness is addressed by employing a suitable prior distribution and characterizing the posterior distribution of the unknowns conditioned on the observations . in this",
    "setting , the priors often encode structural information about the unknowns , such as spatial smoothness properties .",
    "this _ a priori _ structural information opens up the possibility of prior - based dimension reduction , especially in cases where the variation of the high - dimensional unknowns can be explained by a small number of basis functions .",
    "for instance , the truncated karhunen - love expansion @xcite of the prior distribution is employed in @xcite for identifying such a _ a priori _ low - dimensional basis in _",
    "static _ inverse problems . computational cost can be greatly reduced by projecting the original high - dimensional unknowns onto the subspace spanned by the resulting low - dimensional basis .    in this paper",
    ", we extend this concept of _ a priori _ dimension reduction to non - stationary inverse problems , in which the goal is to sequentially infer the state of a dynamical system .",
    "such problems can be solved efficiently using filtering methods , where the posterior prediction from the previous time step is used as the prior for the current state , and the new posterior is obtained by conditioning the current state on data observed at the current time .",
    "the computational difficulty of applying filtering methods to high - dimensional problems stems both from propagating the distribution of the high - dimensional state forward in time and from solving the high - dimensional inference problem when the new data is observed .    to reduce the computational complexity of filtering methods ,",
    "our proposed dimension reduction method is applied in an _ offline - online _ fashion . in the _ offline _ phase ,",
    "we identify a low - dimensional subspace of the state space before solving the inverse problem , using either the method of snapshots or regularized covariance estimation . in the _ online _ phase , the computational complexity of various ( gaussian ) filtering algorithms  including the kalman filter , extended kalman filter , and ensemble kalman filter  is reduced by constraining the update and the prediction steps within the resulting subspace , in a unified subspace - constrained bayesian framework .",
    "the success of the proposed approach naturally requires that the unknown states can be captured by a low - dimensional basis .",
    "this is the case , for instance , if either the model states are sufficiently smooth or the states can be captured by a low - dimensional attractor .",
    "we show numerical examples where the reduction provides significant computational savings in different ways  by reducing the dimension of the linear systems involved in the prediction and update steps , but also , in ensemble filtering approaches , by reducing the number of ensemble members required .",
    "we discuss the limitations of the proposed approach in cases where the state can not be represented efficiently in a fixed low - dimensional subspace .",
    "we also discuss several different ways to construct the subspace based on existing `` snapshots '' of the model states .",
    "the idea of reducing the dimension of filtering algorithms has been investigated before . the closest existing algorithm to our approach",
    "is the reduced - order kalman filter ( rokf ) @xcite , where dimension reduction is obtained by projecting the model dynamics onto a fixed low - dimensional subspace .",
    "while the rokf shares some similarities with the present approach , fundamental differences remain : our algorithms do not explicitly project the model dynamics onto the subspace , but rather constrain the inference ( update ) step using the subspace .",
    "we show that the latter strategy yields more appropriate prior distributions for each inference problem .",
    "moreover , we extend the discussion of dimension reduction to ensemble filtering methods . differences between the present approach and the rokf are analyzed in detail later in the paper .",
    "another related approach is the reduced - rank kalman filter ( rrkf ) of @xcite . in this approach ,",
    "one propagates prediction uncertainties only along directions in the state space where the variance of the states grows most quickly ( the so - called hessian singular vectors ) .",
    "the difference with our approach is that the subspace of rrkf is re - computed at each filtering step through the solution of an eigenvalue problem , whereas in our approach the basis is fixed and computed offline .",
    "thus , rrkf is a local - in - time approach , based on local linearization , whereas our approach tries to find a low - dimensional subspace for the state based on an analysis of global - in - time dynamics .",
    "our approach is less computationally intensive , but its applicability may be restricted to cases where a global low - dimensional representation for model states exists .",
    "our numerical examples , however , demonstrate that this simple strategy can yield significant computational savings in a range of filtering algorithms .    in @xcite ,",
    "dimension reduction is sought not in the gaussian filtering context , but for a sequential monte carlo ( particle filtering ) method .",
    "again , dimension reduction is performed locally in time .",
    "filtering is restricted to coordinates spanning the most unstable modes around the current nominal trajectory ; interestingly , for spatially distributed systems , these unstable modes are often the low - wavenumber components of the state .",
    "dimension reduction approaches for filtering problems can be beneficial in many ways .",
    "for instance , they reduce memory requirements , which are prohibitively large for high - dimensional problems when standard kalman filters are applied .",
    "indeed , memory constraints have motivated the development of various approximate filtering methods @xcite .",
    "the dimension reduction approach presented here reduces memory requirements , but also offers speedups that may be beneficial for ( even smaller scale ) real - time estimation and control problems , e.g. , chemical process tomography @xcite . as noted above",
    ", speedups also extend to ensemble filtering methods : constraining the inference step onto a subspace implicitly regularizes the problem , and thus reduces the number of ensemble members required to achieve a given accuracy . the paper is organized as follows . in section [ sec : redu ] , we review prior - based dimension reduction for static inverse problems and develop the linear - gaussian filtering equations for subspace coordinates . in section",
    "[ sec : nonlin ] , we discuss how the method applies to extended kalman filtering and ensemble methods .",
    "section [ sec : subspace ] discusses techniques for constructing the low - dimensional subspace .",
    "section [ sec : rokf ] analyzes the differences between our approach and the rokf . in sections  [ sec :",
    "numex][sec : numex2 ] , we study the behavior of the dimension reduction approach via linear and nonlinear examples .",
    "section [ sec : conc ] offers some concluding remarks .",
    "our starting point is the prior - based dimension reduction technique for static inverse problems , which we briefly review here .",
    "the unknown function @xmath0 , @xmath1 , is defined in some spatial domain @xmath2 .",
    "discretizing @xmath0 on a grid defined by a set of nodes @xmath3 and some basis functions yields a @xmath4-dimensional vector @xmath5^\\top \\in \\mathbb{r}^d$ ] .",
    "the discretized unknown vector @xmath6 is related to observations @xmath7 via the model @xmath8 where @xmath9 and @xmath10 is a ( possibly nonlinear ) mapping from the unknown @xmath6 to the observable output .",
    "moreover , let us assume that we have a gaussian prior @xmath11 .",
    "then , the posterior density for @xmath6 is @xmath12 where @xmath13 denotes the quadratic form @xmath14 .",
    "the idea in prior - based dimension reduction is to constrain the problem onto a subspace that contains most of the variability allowed by the prior ; see , for instance @xcite .",
    "this can be done by computing the singular value decomposition ( svd ) of the prior covariance matrix ",
    "@xmath15 , where @xmath16 contains the singular vectors @xmath17 ( as colums ) and @xmath18 has the singular vectors in the diagonal .",
    "dimension is reduced by representing the unknown as a linear combination of the @xmath19 leading ( scaled ) singular vectors : @xmath20.\\ ] ] inserting this parameterization into the problem leads to the following posterior density for subspace coordinates @xmath21 : @xmath22 it is easy to verify that the prior term simplifies to @xmath23 , where @xmath24 is the @xmath25 identity matrix .",
    "the posterior can be thus written simply as @xmath26 and the dimension of the inverse problem has been reduced from @xmath4 to @xmath19 .",
    "this can be helpful , for instance , when mcmc samplers , which are challenging to apply in high - dimensional problems , are used to quantify uncertainty in the parameters .",
    "if the model is linear , @xmath27 , the posterior is gaussian @xmath28 with mean and covariance matrix given by @xmath29 thus , one needs to apply the model to the @xmath19 columns of @xmath30 and solve @xmath19-dimensional linear system , which is computationally much easier than solving the full problem if @xmath31 .      here ,",
    "we discuss how dimension reduction can be implemented for dynamical state estimation problems .",
    "let us begin with the following linear gaussian state space model : @xmath32 in the above system , @xmath33 is the forward model that evolves the state in time and @xmath34 is the observation model that maps the state to the observations .",
    "the model and observation errors are assumed to be zero mean gaussians : @xmath35 and @xmath36 with known covariance matrices @xmath37 and @xmath38 .",
    "the linear gaussian problem can be solved with the kalman filter , which proceeds sequentially as follows .",
    "assume that , at time step @xmath39 , the marginal posterior is the following gaussian : @xmath40 the _ prediction step _ involves propagating this gaussian forward with the model @xmath41 , which yields the gaussian @xmath42 where @xmath43 and @xmath44 . throughout the paper ,",
    "we follow the notation commonly used in weather forecasting and data assimilation literature : we use the superscript @xmath45 to refer to the `` analysis '' ( posterior ) estimate updated with the most recent observations , and the superscript @xmath10 to refer to the `` forecast '' ( prior ) estimate .    in the _ update step _ ,",
    "the predicted gaussian is updated with the new observations that become available .",
    "the resulting posterior density is @xmath46 which is , again , gaussian with known mean and covariance matrix given by the standard kalman filter formulas , which we choose not to rewrite here ( see any standard textbook on the subject , e.g. , @xcite ) .",
    "the most straightforward application of the prior - based dimension reduction technique in dynamical problems would be to define @xmath30 separately for each filter step via the leading singular values and vectors of the prior covariance matrix @xmath47 .",
    "this approach has a few potential problems .",
    "first , computing the local leading singular vectors at each time step can be a computationally challenging task .",
    "second , by truncating the prior covariance , we might discard directions that seem less important ( i.e. , that have low prior variance ) locally in time , but that become relevant at a later time step . in our experiments , this approach led to inconsistent behavior of the filter ; good filtering accuracy was obtained for some cases , but in other cases the filter performed poorly or even diverged .    here",
    ", we examine an alternative , simpler strategy , where a global subspace is constructed _ a priori _ ( before the filtering is started ) and is then fixed for the filtering .",
    "this approach is motivated by the fact that the state of a dynamical system often lives in a subspace of much smaller dimension than the full state space ; the state vector often has some properties ( e.g. , smoothness ) that enable it to be effectively described in a low - dimensional subspace .",
    "if we can capture the subspace where the essential dynamics of the system happen , we can potentially reduce the whole filtering procedure onto this subspace .",
    "this idea is discussed here .",
    "now , we parameterize the unknown as @xmath48 , where @xmath49 is a fixed reduction operator that does not change in time and @xmath50 is the predicted ( prior ) mean . for",
    "now , we assume that such a representation exists ; discussion about how to construct @xmath30 is reserved for section [ sec : subspace ] . to derive the filtering equations for the subspace coordinates @xmath51 ,",
    "assume that the marginal posterior distribution for @xmath52 at time @xmath53 is @xmath54 transforming this gaussian distribution to the original coordinates using @xmath55 yields the following gaussian distribution in the full state space : @xmath56 by propagating this gaussian distribution forward with @xmath41 , we obtain the mean and the covariance matrix of the predictive distribution @xmath57 in the original coordinates : @xmath58 applying this as the prior , and inserting the parameterization @xmath59 into ( [ fullpost ] ) yields the following marginal posterior density for @xmath51 : @xmath60 this is equivalent to a linear problem with gaussian likelihood @xmath61 and zero mean gaussian prior @xmath62 .",
    "the resulting posterior is thus @xmath63 where @xmath64 and @xmath65 is the prediction residual .",
    "note that now @xmath30 does not whiten the prior , in contrast with the prior - based dimension reduction discussed in section [ sec : static ] for static problems , and thus the term @xmath66 is not equal to the identity .",
    "moreover , the matrix @xmath47 can not be formed explicitly in high - dimensional problems . to efficiently evaluate @xmath67 , we recall that here @xmath68 where @xmath69 is the matrix square root @xmath70 . to shorten the notation ,",
    "let us denote @xmath71 .",
    "now , applying the sherman - morrison - woodbury matrix inversion formula yields @xmath72 where @xmath24 is the @xmath25 identity matrix .",
    "now , the matrix @xmath73 that needs to be inverted is only @xmath25 .",
    "thus , a product @xmath74 can be efficiently computed as long as @xmath75 is easy to compute .",
    "this condition must hold in order for this technique to work . in practice",
    ", @xmath76 is usually a simple ( e.g. , diagonal ) matrix postulated by the user .    to sum up",
    ", a single step of the reduced kalman filter with a fixed basis is given as an algorithm below .",
    "assume that we have the previous estimate @xmath77 and its covariance matrix @xmath78 available .",
    "then the algorithm reads as follows : +   +   +   +    computationally , this version is much lighter than the standard kalman filter if @xmath31 . in the standard kalman filter ,",
    "the prediction covariance matrix is computed as @xmath79 ; that is , we need to compute products of @xmath80 matrices ( or to apply the forward model to the @xmath4 columns of @xmath81 ) . moreover ,",
    "when updating the prior covariance , one needs to operate with @xmath82 and solve a system of @xmath83 linear equations . in the approach described here , one needs to work with @xmath84 matrices @xmath85 , solve a system of @xmath19 linear equations , and do one inversion of a @xmath25 matrix .",
    "also , here the basis vectors are fixed , so we avoid solving local eigenvalue problems , which are needed , for instance , in the reduced rank kalman filter of @xcite .    here",
    ", the parameterization is centered at the predicted mean , @xmath86 .",
    "an alternative would be to use a fixed mean , @xmath87 , where @xmath88 is some fixed offset .",
    "the former looks for a correction to the predicted mean in the subspace , whereas the latter attempts to describe the state vector itself in a fixed subspace . in our experiments , the former yields much better filter accuracy , especially with small @xmath19 .",
    "here , we discuss two ways to extend the dimension reduction idea to problems where the evolution and/or observation models are nonlinear . we still assume additive gaussian model and observation errors , so our state space model now reads as @xmath89 where @xmath90 and @xmath91 are the nonlinear forward and observation models .",
    "we start by discussing the extended kalman filter , which requires linearizations of the forward and observation models .",
    "then , we discuss ensemble filtering techniques where linearizations are not needed .",
    "the extended kalman filter ( ekf ) replaces the model and observation matrices in the kf with their linearized versions .",
    "thus , the algorithm is the same as algorithm  1 in section [ sec : redu ] , but the mean is propagated with the nonlinear forward model and the prediction residual is calculated with the nonlinear observation model . elsewhere",
    "@xmath41 and @xmath92 are replaced with @xmath93 note that for large scale problems , computing the above matrices explicitly is not feasible . instead , one often derives the linearized model analytically ( as an operator ) , which makes it possible to propagate vectors forward with the linear model , which is equivalent to computing products @xmath94 where @xmath95 . here",
    ", these _ tangent linear codes _ need to be applied in steps  ( iii )  and  ( v ) of algorithm  1 . in step  ( iii ) , we compute @xmath96 , which can be done by applying the linearized forward model to the @xmath19 columns of @xmath97 . in step  ( v ) , we need @xmath98 , which can be computed by applying the linearized observation model to the @xmath19 columns of @xmath30 . in both cases ,",
    "we only need to propagate @xmath19 vectors through the linearized models , instead of @xmath4 vectors as in the standard ekf .",
    "ensemble filters have become popular for solving very high - dimensional dynamical state estimation problems arising in geophysical applications such as numerical weather prediction .",
    "the development started started from the ensemble kalman filter ( enkf , @xcite ) in the 1990s , and different variants are under active development .",
    "the idea of the enkf is to represent the state and its uncertainty with samples ( an `` ensemble '' of states ) , and , roughly speaking , to replace the covariances in the filtering formulas with their empirical estimates calculated from the samples .    for high - dimensional problems that involve complex physical models , the ensemble size is necessarily much smaller than the dimension of the problem . as a result , the obtained covariance estimates are rank - deficient and can suffer from `` spurious correlations '' ( unphysical correlations appearing randomly due to small sample size ) ; see , e.g. , @xcite for discussion . to overcome these issues , various _ localization _ techniques have been proposed , where the empirical covariance estimates are regularised by , for instance , explicitly removing unrealistic distant correlations from the covariance matrices ; see @xcite .",
    "recently , adaptive localization techniques have also been developed , where the localization mechanism is tuned on - line in the filter @xcite .",
    "localization is one of the key techniques to make enkfs work for small ensemble sizes .    in addition to rank deficiency ,",
    "the classical enkf suffers from sampling error , as the observation and model errors are accounted for by randomly perturbing the observations and predicted ensemble members during the estimation . to avoid this additional variance in the resulting estimates ,",
    "so - called square root enkfs have been developed , which are deterministic schemes where no such random perturbations are used for the observations @xcite .",
    "accounting for model error still remains a difficulty , although some techniques have been recently proposed ; see , e.g. , @xcite and the references therein .    in the method discussed here",
    ", the state is constrained onto a subspace , which heavily regularises the estimation problem ; for instance , if the state is smooth , the rough features are explicitly removed from the estimation problem , and all of the information in the samples can be used for inferring the smooth features . as a result ,",
    "the need for localization is diminished , as demonstrated in the numerical examples in section [ sec : numex ] .",
    "moreover , model error can be can be easily included in the approach , provided that we are able to apply the model error covariance matrix to a vector efficiently .    here , we present one way of extending the dimension reduction idea to ensemble filtering .",
    "let us now consider the state space model where the forward model is nonlinear but the observation model is linear ( the nonlinear observation model case is discussed later ) : @xmath99 in ensemble filtering , we represent the distribution of @xmath51 with samples .",
    "let us assume that at time step @xmath53 we have @xmath100 posterior samples @xmath101 available , sampled from the gaussian posterior @xmath102 .",
    "these obviously correspond to samples @xmath103 in the full state space . in the prediction step , we move the posterior samples forward with the dynamical model : @xmath104",
    ". then , we compute the empirical covariance matrix of the predicted samples and add the model error covariance to obtain the prediction error covariance : @xmath105 where @xmath106 / \\sqrt{n_{\\mathrm{ens } } } \\in \\mathbb{r}^{d \\times n_{\\mathrm{ens}}}. \\label{xk}\\ ] ] thus , @xmath107 is the empirical covariance estimate computed from the prediction ensemble .",
    "the mean @xmath108 is taken to be the posterior mean from the previous step propagated via the evolution model , @xmath109 , instead of the empirical mean computed from the prediction ensemble , which is why we divide by @xmath110 instead of @xmath111 , see the remarks below for more discussion about this choice .",
    "if the observation model is linear , the reduced dimension ensemble filtering algorithm stays almost the same as the reduced kf algorithm ( algorithm  1 in section  [ sec : redu ] ) .",
    "the only difference is the prior covariance matrix @xmath47 , which is now defined via the prediction ensemble .",
    "when @xmath112 , operating with @xmath113 , needed in step  ( v ) of algorithm  1 , can still be done efficiently via the sherman - morrison - woodbury inversion formula : @xmath114 note that now , when applying @xmath113 to a vector , we are left with the inversion of an @xmath115 matrix instead of an @xmath25 matrix .    to summarise",
    ", one step of the ensemble kalman filter with dimension reduction is given below .",
    "+   +   +   +    the computational cost of the ensemble algorithms is dictated by both the number of basis vectors @xmath19 and the number of ensemble members @xmath100 .",
    "the computational cost is similar to that of the standard enkf .",
    "what makes dimension reduction attractive from ensemble filtering point of view is that the number of samples needed to capture the distribution of the @xmath19-dimensional variable @xmath51 can be much smaller than the number of samples needed to get accurate filtering results for the @xmath4-dimensional variable @xmath116 in the full space .",
    "thus , similar performance can be obtained with fewer ensemble members , as can be observed in the numerical examples in sections [ sec : numex][sec : numex2 ] .",
    "the ensemble filter presented above differs from the classical ensemble kalman filter ( enkf ) developed in @xcite .",
    "the classical enkf is a non - gaussian filter ; it applies a linear update with perturbed observations to non - gaussian prediction samples to get the posterior ensemble .",
    "the version presented here is a gaussian filter in the sense that the prior is assumed to be a gaussian whose covariance matrix is estimated from the prediction ensemble .",
    "another difference between the proposed method and many other ensemble filters is that the prior mean here is @xmath117 instead of the empirical ensemble mean used , for instance , in the classical enkf .",
    "technically , one could easily choose the ensemble mean as @xmath108 as well .",
    "however , we have noticed that the choice @xmath117 for propagating the mean , analogous to ekf and variational ( 3d - var and 4d - var ) methods , works better for many problems .",
    "one reason might be that the @xmath108 obtained this way lies close to the attractor of the forward model , whereas the sample mean obtained from a small number of ensemble members might be further away from it and thus represent an `` unphysical '' state . a similar approach was taken in some recently developed filtering algorithms ; see , for instance , @xcite for some discussion .    if the observation model is nonlinear",
    ", the posterior distribution for @xmath51 is no longer gaussian .",
    "then , one way forward is to apply the linearized observation operator as in the ekf to obtain a gaussian approximation of the posterior , and to sample new members from the gaussian .",
    "another way is to sample new posterior samples for @xmath51 directly from the non - gaussian posterior @xmath118 using , for instance , markov chain monte carlo ( mcmc ) techniques , which should be feasible if @xmath19 is not too large and @xmath91 is relatively simple ( note that evaluating the posterior density does not require the forward model @xmath90 ) .",
    "for instance , novel optimization - based sampling techniques like @xcite ( which requires gaussian priors as above ) can potentially be used to generate posterior samples efficiently , as discussed in @xcite in connection with high - dimensional filtering problems .",
    "the subspace representation also opens up a way to implement other sample - based filtering techniques for high - dimensional problems , such as the popular unscented kalman filter ( see , e.g. , @xcite ) , which , in the subspace version , would require @xmath119 samples to propagate the covariance forward instead of @xmath120 as in the full state space version .",
    "even particle filtering in the subspace might be possible with a reasonable number of particles .",
    "these ideas are left for future study and not pursued further here .",
    "the reduced subspace basis @xmath30 for representing the unknown state @xmath116 is constructed in a similar way as for static problems discussed in section [ sec : static ] .",
    "consider the covariance matrix @xmath121 that represents the covariance structure of states , and in particular its eigendecomposition @xmath122 .",
    "we compute the basis @xmath30 from the @xmath19 leading eigenvectors @xmath123 and square roots of the corresponding eigenvalues @xmath124 : @xmath125 if the eigenvalues of the covariance matrix @xmath121 decay quickly , the variation of states can be captured by a low - dimensional subspace spanned by the leading eigenvectors .",
    "we note that scaling the basis by the eigenvalues in ( [ eq : repr ] ) is not necessary , but can unify the scales of the different state variables .",
    "of course , this method requires access to @xmath121 . in the rest of this section , we present several ways to estimate this covariance matrix .      principal component analysis ( pca )",
    "can be applied to `` snapshots''which are possible model states obtained from existing model simulations  for constructing low - dimensional subspaces of high - dimensional dynamical systems . depending on the field of application ,",
    "this procedure is also named empirical orthogonal functions @xcite in meteorology , or proper orthogonal decomposition @xcite in model reduction . the reduced basis obtained from pca",
    "can then be used in either model reduction @xcite or filtering ( e.g. , the rokf method @xcite or our approach as presented in section  [ sec : dyn ] ) . in non - stationary inverse problems ,",
    "our main interest is dynamical systems without steady states ( e.g. , chaotic models ) . in this setting",
    ", we use trajectories obtained from either a sufficiently long free model simulation or multiple model simulations with randomized initial conditions . given a sufficient number of snapshots @xmath126 , the subspace basis is computed using ( [ eq : repr ] ) via the eigendecomposition of empirical state covariance @xmath127 where @xmath128 is the empirical state mean . for high - dimensional dynamical systems , it is not feasible to form the empirical covariance directly and apply dense matrix eigendecomposition methods . in this case",
    ", either krylov subspace methods @xcite or randomized methods @xcite should be applied together with matrix - free operations  the matrix vector product with @xmath121to compute the eigendecomposition .",
    "the number of snapshots naturally can affect the quality of the basis @xmath30 , and a sufficiently large @xmath129 should be chosen to capture the essential behavior of the model . above",
    ", the basis is constructed from un - regularised empirical covariance estimates , without assuming any particular form for the covariance .",
    "if the number of snapshots available is limited , we can instead employ regularised covariance estimation techniques , where certain assumptions about the covariance structure are introduced to regularise the estimation problem .",
    "these techniques are discussed below .",
    "another viable route for state covariance estimation is to infer the state correlation structure from snapshots and _ a priori _ information such as smoothness assumptions . here ,",
    "we discuss how gaussian processes ( gps ) can be used for the task .",
    "we consider two types of gps : a stationary gp modeled by a kernel function @xcite , or a ( possibly ) non - stationary gp modeled by a a differential operator @xcite .",
    "we first discuss the kernel approach , where each element of the covariance matrix is given by a kernel function @xmath39 of the form @xmath130 where @xmath131 are spatial locations used to discretize the states , and all the information about smoothness , correlation length , and variability can be encoded in the parameter @xmath132 . for example , one commonly used kernel function is the squared exponential kernel @xmath133 where @xmath134 and @xmath135 control the variability and correlation length , respectively , and @xmath136 is a distance between points @xmath137 and @xmath138 . with this kernel function , the eigenvalues decay quickly ( exponentially ) , and it is easy to capture the kernel with a finite basis .    given the mean of the gp , @xmath88 , empirically estimated from the snapshots , we estimate the parameters @xmath132 in a bayesian framework .",
    "the likelihood function of @xmath132 , given a snapshot collection @xmath126 , takes the form @xmath139 combining the likelihood with a prior distribution @xmath140 , we obtain the _ maximum a posteriori _",
    "estimate @xmath141 of the kernel parameters .",
    "the reduced subspace basis can then be computed from the eigendecomposition of the state covariance @xmath142 .    here the smoothness assumption is used to `` fill the gap '' between the unknown high - dimensional correlations of the state and the information provided by a limited number of snapshots . for a gp defined by stationary kernels ,",
    "this assumption can be enforced by using a smooth kernel such as the squared exponential kernel .",
    "the stationary assumption we used in the above - mentioned kernel method may not be suitable for dynamical systems where the states have heterogeneous spatial correlations .",
    "furthermore , operations with the dense covariance matrix @xmath143 in ( [ eq : like ] ) can be computationally challenging for high - dimensional states , because the covariance matrix can be singular and computational costs of dense matrix operations  especially factorization and inversion  scale poorly with dimension .",
    "this motivates us to model the gp using the inverse of the covariance , i.e. , the precision matrix @xmath144 , so that gaussian markov random field ( gmrf ) models @xcite can be used to construct the precision as a sparse matrix .",
    "we particularly mention the work of @xcite , in which the sparse precision matrix is constructed from the finite element discretization of the following stochastic partial differential equation ( spde ) : @xmath145 where @xmath146 is a white noise process in space . for spatially constant @xmath147 and @xmath148",
    ", it can be shown that the solution @xmath149 of the spde ( [ eq : spde ] ) defines a gaussian process with the matrn family of correlation functions ; see @xcite and references therein .",
    "the functions @xmath150 and @xmath151 together control the correlation length and variability of the gp , and the scalar @xmath152 controls the smoothness , and therefore a nonstationary gp can be defined by prescribing spatially heterogeneous @xmath150 and @xmath151 .    for a positive integer @xmath152 , discretizing the spde ( [ eq : spde ] ) yields a sparse precision matrix @xmath153 .",
    "here , we discretize ( [ eq : spde ] ) using the finite element method with linear basis functions , @xmath154 where @xmath155 , is a set of nodal points and @xmath156 , is a set of linear basis functions associated with the nodal points . here",
    "the nodes are defined to coincide with the locations of the states in the filtering problem .",
    "the precision matrix @xmath153 can be constructed given the mass matrix and the stiffness matrix of the finite element discretization , which are defined as @xmath157 \\phi_j(s ) ds,\\ ] ] where @xmath158 .",
    "the functions @xmath150 and @xmath151 can also be discretized by linear basis functions , which are given as @xmath159 respectively .",
    "this yields the local mass and stiffness matrices @xmath160 \\phi_j(s ) ds,\\end{aligned}\\ ] ] such that the overall mass and stiffness matrices can be written as @xmath161 where @xmath162^\\top$ ] and @xmath163^\\top$ ] . here",
    ", the local mass and stiffness matrices , @xmath164 and @xmath165 , can be precomputed for a given set of finite element basis functions . following the recursive definition given in @xcite , the precision matrix @xmath166 parameterized by a scalar @xmath152 and vectors @xmath167 and",
    "@xmath168 is @xmath169 in the present work , we will pre - select the order of the differential operator by choosing an @xmath152 value , @xmath170 , where @xmath171 .",
    "a larger exponent , e.g. , @xmath172 , is not recommended as it will lead to a high computational cost in the following parameter estimation problem . for a fixed @xmath173 ,",
    "the parameters @xmath167 and @xmath168 can be estimated in a bayesian manner .",
    "the likelihood function for this estimation problem takes the form @xmath174 choosing spatially varying parameters @xmath167 and @xmath168 provides the flexibility needed to model a non - stationary covariance structure . however , estimating @xmath167 and @xmath168 from a limited number of snapshots is itself an ill - posed inverse problem , so priors must be assigned to these parameters to remove the ill - posedness .    to limit the degrees of freedom in the estimation , we prescribe the function @xmath150 to be a scalar , i.e. , @xmath147 , and use only a spatially varying @xmath151 to control the nonstationarity of the resulting gp .",
    "we use the mass lumping technique to approximate the mass matrix rather than dealing with the computationally prohibitive inversion @xmath175 . for the case @xmath147 ,",
    "the lumped mass matrix is given as @xmath176 we use an exponential prior to enforce the positivity of @xmath177 .",
    "furthermore , a smooth lognormal process is prescribed as the prior for @xmath151 to enforce the positive semi - definiteness of the stiffness matrix @xmath178 . by defining @xmath179 , the posterior distribution of the bayesian inference problem",
    "can be written as @xmath180 where @xmath181 define the mean and precision of the lognormal prior for @xmath182 .",
    "the precision matrix @xmath183 in ( [ eq : post ] ) is given as @xmath184 we can then obtain the _",
    "maximum a posteriori _ estimate of the gmrf parameters @xmath185 by maximizing the logarithm of the posterior density , which yields @xmath186 the optimization problem ( [ eq : mrf_post ] ) is continuous , and the gradient and hessian of the objective with respect to @xmath177 and @xmath187 can be analytically derived ( not reported here for brevity ) ; hence , gradient - based nonlinear programing tools can be used to obtain an optimum . in particular",
    ", we employ the subspace trust region method of @xcite with inexact newton steps .",
    "note that in the inexact newton solve , we use hessian - vector products rather than explicitly forming the full hessian , to ensure that computational costs and memory requirements scale favorably with parameter dimension . given the solution of ( [ eq : mrf_post ] ) , the low - dimensional subspace for our reduced filters can be computed from the eigendecomposition of the estimated state covariance @xmath188 using the matrix - free methods discussed in the pca case ( section [ sec : pca ] ) .",
    "a subtle issue in constructing the precision matrix is the choice of boundary condition for the spde ( [ eq : spde ] ) . for",
    "a gp specified with scalar @xmath177 and @xmath189 , prescribing a dirichlet boundary condition will enforce zero variability on the boundary , and prescribing a no flux boundary condition  which is a common choice in the literature  will roughly double the variability at the boundary compare to the variability in the interior .",
    "clearly both choices will result a gp that has nonstationary behavior near the boundary , even though the gp with scalar @xmath177 and @xmath189 defined on an infinite domain should be stationary in theory . in our numerical examples ,",
    "boundary conditions do not present any difficulties , since we use periodic boundary conditions that are inherited from the structure of the corresponding data assimilation problem . in a more general setting , we recommend to use a zero - flux boundary condition and to let the data determine the nonstationary correlation structure through the solution of ( [ eq : mrf_post ] ) . this way ,",
    "artifacts created by the boundary condition can be potentially compensated for via the inhomogeneous @xmath151 field .",
    "the reduced - order kalman filter ( rokf ) , developed in @xcite and discussed further in @xcite is , in principle , very similar to the dimension reduction approaches presented in this paper ; like our approach , rokf uses a fixed low - dimensional subspace to represent the state vector .",
    "therefore , it is useful to discuss the differences between the techniques in more detail .    to see the difference between the methods , it is instructive to compare what kind of priors ( predictive distributions ) the methods induce for the subspace coordinates .",
    "the prior mean is propagated in the same way , but the covariance is handled differently . in the rokf ,",
    "the prediction covariance in the reduced space , denoted here by @xmath190 , is computed by evolving @xmath191 as follows : @xmath192 where @xmath193 is the posterior covariance of the previous time step propagated forward with @xmath41 .",
    "this has two interpretations : ( _ a _ ) projecting the forward dynamics @xmath41 onto the subspace spanned by the columns of @xmath30 and using it to propagate the reduced covariance forward , and ( _ b _ ) propagating the posterior covariance of the previous time step with the full model and projecting the resulting prediction covariance onto the subspace .    on the other hand ,",
    "the prediction covariance in our approach ( see section [ sec : dyn ] ) is @xmath194 that is , while rokf projects the predicted _ covariance _ matrix @xmath47 , we project the predicted _ precision _ matrix @xmath113 .",
    "this difference has a nice geometric interpretation ; projecting the covariance matrix is equivalent to _ marginalizing _ the prior onto the subspace , whereas projecting the precision matrix amounts to taking the _ conditional _ of the prior in the subspace .    to visually see the difference ,",
    "consider the following simple example .",
    "assume that the full dimension of the state space is @xmath195 and that the subspace ( of dimension @xmath196 ) is the x - axis : @xmath197^\\top$ ] .",
    "the model @xmath41 is taken to be a @xmath198 matrix with random entries sampled from @xmath199 .",
    "we start with zero mean and covariance @xmath191 in the subspace ( x - axis ) , and propagate it forward with both the rokf and our approach .",
    "the results are shown in figure  [ fig : rokf_demo ] .",
    "one can clearly see that the prior induced by rokf can be significantly different than the prior induced by our approach .",
    "specifically , the prior induced by the rokf is always wider than the conditional prior of our approach .",
    "it is clear that marginalizing the prior can yield posterior estimates which are _ outside the essential support of the prior . _",
    "here , we demonstrate the proposed dimension reduction algorithms with two synthetic filtering problems : a 240-dimensional version of the lorenz model and a 1600-dimensional example using the quasi - geostrophic model .     as the reference methods , we use the standard extended kalman filter ( ekf ) and the standard ensemble kalman filter ( enkf ) ; see @xcite .",
    "the purpose of the experiments is to highlight some of the properties of the proposed approach , such as its behaviour with small sample sizes in ensemble filtering , rather than to draw conclusions about the performance of the approach relative to all the recent developments in the ensemble filtering literature .",
    "for this reason , and to keep the comparisons simple , we choose the well - known standard enkf as the reference method instead of one of the many variants developed recently .",
    "a thorough performance comparison with all the recent developments in filtering methods is a challenging task ( e.g. , handling all the tuning issues of the various filters ) and left for future research .    for the enkf",
    ", we implement a simple and widely used localization scheme , obtained by _ tapering _ ( setting the covariance between distant points to zero ) the prediction covariance matrix using the 5th order piecewise rational function @xcite .",
    "the correlation cut - off length was chosen experimentally so that roughly optimal filter performance was obtained .        as a small scale nonlinear example",
    ", we consider a generalized version of the lorenz 96 model , the model ii described in @xcite .",
    "the evolution model is given by an ode system of @xmath129 equations , each defined as @xmath200 where @xmath201 and @xmath202 is a chosen _ odd _ integer and @xmath203 .",
    "the variables are periodic : @xmath204 and @xmath205 for @xmath206 . with @xmath207 ,",
    "the system reduces to the standard lorenz 96 model introduced in @xcite .    in our experiments we use a range of values for @xmath202 , and choose the forcing @xmath208 so that the model attains chaotic behaviour ( verified experimentally ) . in the prediction model used in the estimation ,",
    "we use values @xmath209 and the corresponding forcing values @xmath210 for all @xmath211 . increasing @xmath202 introduces stronger spatial dependence between neighbouring variables , and yields spatially smoother solutions ; example solutions with @xmath212 and varying @xmath202 are given in figure [ fig : lorex ] .",
    "controlling the smoothness allows us to demonstrate how the dimension reduction works in different cases : the smoother the unknown , the fewer basis vectors we need to describe it accurately .",
    "this is demonstrated in figure  [ fig : energy ] , where we plot the fraction of the energy of the empirical covariance matrix of the model trajectories as a function of the number of basis vectors used in the representation .",
    "more precisely , we plot @xmath213 as a function of @xmath19 , where @xmath214 is the @xmath215th largest eigenvalue of the empirical covariance matrix computed from model simulation output .",
    "one can see that with large @xmath202 , most of the variability of the model state can be captured in a low - dimensional subspace , whereas with small @xmath202 more basis vectors are needed .      in the experiments , we use values @xmath209 and the corresponding forcing values @xmath210 for all @xmath211 .",
    "we generate data for the estimation by simulating the model and adding 1% normally distributed random perturbations to the forcing values to introduce error into the prediction model .",
    "the observation frequency is @xmath216 time units ( twice the time step used to solve the ode ) and every @xmath217th state variable is observed ( @xmath218 ) , which yields altogether 24 measurements per observation time step .",
    "data is generated for @xmath219 time units , which yields altogether 400 observation times .",
    "the ode was solved using the 4th order runge - kutta method .",
    "the model error covariance matrix used in the experiments is simply @xmath220 for all @xmath39 , where @xmath221 is chosen experimentally from the interval @xmath222 $ ] so that roughly optimal tracking performance is obtained ( rms error between the estimates and the truth is minimised ) for each filter .",
    "the candidate subspaces were constructed by the pca , gp , and gmrf techniques discussed in section  [ sec : subspace ] . for pca",
    ", we used the trajectory obtained by simulating the model for 1200 steps ( that is , the covariance was estimated using 1200 samples ) .",
    "gp and gmrf parameters were fitted using 32 snapshots of the model state . for the gp approach ,",
    "the squared exponential covariance function was used , see equation ( [ squared - exp ] ) , and the obtained estimates for the variance and correlation length parameters were @xmath223 and @xmath224 . for the gmrf approach , we use an exponent of @xmath225 .",
    "first , we compare the reduced ekf described in sections [ sec : redu ] and [ sec : ekf ] to the full ekf for the @xmath226 case , where the model state is spatially rather smooth and thus well described in a low - dimensional subspace ; see figures  [ fig : lorex ]  and  [ fig : energy ] .",
    "the first 16 basis vectors obtained via pca , gp , and gmrf are given in figure  [ fig : basis ] ; the first vectors represent large scale smooth features and the later ones describe finer scale features .",
    "all methods were started from an all - zero initial state , @xmath227 and identity covariance @xmath228 . in figure",
    "[ fig : lor_res1 ] , we compare the rms error of the ekf and reduced ekf with varying numbers of basis vectors @xmath19 , three ways for constructing the subspace ( pca , gp , and gmrf ) , and two ways to parameterize the unknown ( solid lines : centered at the prior mean @xmath86 , dashed lines : fixed offset @xmath87 ) . as the fixed offset @xmath88",
    ", we use the empirical mean of the simulated model trajectory for pca and a constant @xmath88 for gp and gmrf .",
    "we observe that centering the parameterization at the prior mean improves the results dramatically compared to fixed mean . with small @xmath19 ,",
    "the pca basis works slightly better than gp and gmrf . with the fixed mean parameterization ,",
    "pca and gmrf work roughly equally well , and gp a little worse . but",
    "all in all , the three different ways of constructing the subspace all yield similar results .    in this example ,",
    "we are able to obtain a reasonably accurate filter even with @xmath229 ( ! ) , whereas the fixed offset parameterization requires roughly @xmath230 for similar accuracy .",
    "thus , we are able to reduce the dimension and the computational complexity almost two orders of magnitude compared to the full state dimension @xmath212 .    a summary of the results for all cases @xmath209 using the pca basis is given in figure [ fig : lor_res_allk ] .",
    "we plot the relative difference between the rms error obtained with the full ekf and with the reduced ekf for all cases with varying @xmath19 , using only the parameterization @xmath86 .",
    "we observe that with high @xmath202 ( when the trajectories are smooth ) , a small @xmath19 is enough to capture the system ; for instance , with @xmath231 and @xmath232 , only roughly @xmath233 vectors are needed to get an accurate filter . with small @xmath202 , however , the system contains more fine scale features and larger @xmath19 is needed to obtain good filtering performance ; for example , @xmath234 requires roughly @xmath235 for similar accuracy .",
    "this example illustrates how the efficiency of the proposed approach depends on the smoothness properties of the system .     and @xmath19 .",
    "]      here , we compare the reduced enkf described in section [ sec : enkf ] to the standard enkf . the results for the @xmath226 case with varying @xmath19 are given in figure [ fig : lor_res3 ] for different ensemble sizes .",
    "we observe that the reduced enkf works much better than the standard enkfs with small ensemble sizes ; with the reduced enkf , we are able to obtain a convergent filter with an ensemble size as small as @xmath236 , whereas the standard enkf has problems converging with @xmath237 .",
    "for example , the reduced enkf with @xmath238 and @xmath236 yields similar performance as full enkf with @xmath239 . again",
    ", with sufficiently high @xmath100 ( here @xmath240 ) the enkf performance starts to catch up .",
    "figure  [ fig : lor_res3 ] also illustrates the interesting connection between @xmath19 ( number of basis vectors used ) and @xmath100 . with smaller @xmath19 , the performance that can be obtained is poorer , but , on the other hand , a smaller @xmath100 is needed to achieve that performance . with larger @xmath19 , better performance can be obtained , but only if @xmath100 is set high enough .",
    "that is , for each @xmath100 , there seems to be an @xmath19 that provides an optimal compromise between representation error ( due to small @xmath19 ) and sampling error ( due to small @xmath100 ) .",
    "we conclude that in ensemble filtering , the dimension reduction approach , when feasible , can offer a way to develop a reasonably accurate filter with fewer ensemble members . restricting the inference to a subspace",
    "can reduce the need for sample covariance matrix regularization via localization techniques , which otherwise are needed when high - dimensional filtering problems are solved with small ensemble sizes @xcite .",
    "next , we test the subspace filtering algorithms using the two - layer quasi - geostrophic model ( qg model ) @xcite , which is often used as a benchmark system for data assimilation studies for numerical weather prediction ( nwp ) .",
    "the model provides a reasonably good analogue of large - scale mid - latitude chaotic dynamics , while being relatively cheap computationally @xcite .",
    "next , we briefly describe the model equations and our estimation setup . for more details about the model as we use it , refer to @xcite .",
    "the two - layer quasi - geostrophic model simulates atmospheric flow for the geostrophic ( slow ) wind motions .",
    "the geometrical domain of the model is specified by a cylindrical surface vertically divided into two `` atmospheric '' layers .",
    "the model also accounts for an orographic component that defines the surface irregularities affecting the bottom layer of the model .",
    "the latitudinal boundary conditions are periodic , whereas the values on the top and the bottom of the cylindrical domain are user - supplied constant values .",
    "the geometrical layout of the two - layer qg model mapped onto a plane is illustrated in figure  [ qg_geometry ] . in the figure , parameters @xmath241 and @xmath242 denote mean zonal flows in the top and the bottom atmospheric layers , respectively .",
    "the model formulation we use is dimensionless , where the non - dimensionalization is defined by the length scale @xmath243 , velocity scale @xmath244 , and the layer depths @xmath245 and @xmath246 .",
    "the model operates with variables called _ potential vorticity _ and _ stream function _",
    ", where the latter is analogous to pressure .",
    "the model is formulated as a coupled system of pdes ( [ qg_conservation ] ) describing a conservation law for potential vorticity .",
    "the conservation law is given as @xmath247 where @xmath248 denotes the substantial derivatives for latitudinal wind @xmath249 and longitudinal wind @xmath250 , defined as @xmath251 ; @xmath252 denote the potential vorticity functions ; index @xmath215 specifies the top atmospheric layer ( @xmath253 ) and the bottom layer ( @xmath254 ) .",
    "interaction between the layers , as well as relation between the potential vorticity @xmath252 and the stream function @xmath255 , is modeled by the following system of pdes : @xmath256 @xmath257 here @xmath258 and @xmath221 denote dimensionless orography component and the northward gradient of the coriolis parameter , which we hereafter denote as @xmath259 .",
    "the relations between the physical attributes and dimensionless parameters that appear in ( [ pv_eq_1])([pv_eq_2 ] ) are as follows : @xmath260 where @xmath261 defines the potential temperature change across the layer interface , @xmath262 is the mean potential temperature , @xmath263 is acceleration of gravity , @xmath264 is the rossby number associated with the defined system , and @xmath265 and @xmath266 are dimensional representations of @xmath267 and @xmath221 , respectively .",
    "the system of ( [ qg_conservation])([pv_eq_2 ] ) defines the two - layer quasi - geostrophic model .",
    "the state of the model , and thus the target of estimation , is the stream function @xmath255 . for the numerical solution of the system ,",
    "we consider potential vorticity functions @xmath268 and @xmath269 to be known , and invert the spatial equations ( [ pv_eq_1 ] ) and ( [ pv_eq_2 ] ) for @xmath255 .",
    "more precisely , we apply @xmath270 to equation ( [ pv_eq_1 ] ) and subtract @xmath271 times ( [ pv_eq_2 ] ) and @xmath272 times ( [ pv_eq_1 ] ) from the result , which yields the following equation : @xmath273-\\left(f_1+f_2\\right)\\left[\\nabla^2\\psi_1\\right ] = \\nonumber\\\\ & \\nabla^2 q_1-f_2\\left(q_1-\\beta y\\right)-f_1\\left(q_2-\\beta y - r_s\\right ) .",
    "\\label{qg_helmholtz_eq}\\end{aligned}\\ ] ] equation ( [ qg_helmholtz_eq ] ) can be treated as a non - homogeneous helmholtz equation with negative parameter @xmath274 and unknown @xmath275 .",
    "once @xmath275 is solved , the stream function for the top atmospheric layer is determined by a poisson equation .",
    "the stream function for the bottom layer can be found by plugging the obtained value for @xmath276 into ( [ pv_eq_1 ] ) , ( [ pv_eq_2 ] ) and solving the equations for @xmath277 .",
    "the potential vorticity functions @xmath252 are evolved over the time by a numerical advection procedure which models the conservation equations ( [ qg_conservation ] ) .",
    "we run the qg model with @xmath278 grid in each layer , and the dimension of the state vector is thus 1600 .",
    "to generate data , we run the model with 1 hour time step using layer depths @xmath279 and @xmath280 .",
    "data is generated at every 6th step ( filter step is thus 6 hours ) by adding random noise for 100 randomly chosen grid points with standard deviation @xmath281 . for the estimation , bias is introduced to the forward model by using wrong layer depths , @xmath282 and @xmath283 .",
    "the model error covariance matrix was @xmath284 , where @xmath285 was chosen experimentally so that roughly optimal ekf performance was obtained ( here , the tracking performance was quite insensitive to @xmath221 ) .",
    "a snapshot of the model simulation at one time step and the measurement locations are illustrated in figure  [ fig : qg_setup ] .          in figure",
    "[ qg_res ] we compare the mean rms errors ( computed over 400 filter steps ) of the full ekf and the reduced ekf with two different parameterizations and different numbers of basis vectors @xmath19 used . only the pca basis is considered here .",
    "we observe , as in the other example , that the parameterization centered at the predicted mean works much better , especially with a small @xmath19 .",
    "we are able to obtain reasonably accurate filtering results using only @xmath230 basis vectors , which reduces the cpu time ( and memory requirements ) by almost two orders of magnitude compared to full ekf .",
    "interestingly , with a sufficiently high @xmath19 , the average rms error is actually _ lower _ than with the full ekf .",
    "this can be explained by the additional prior information brought into the problem by restricting the inference onto a subspace .",
    "when @xmath19 approaches the full dimension of the problem @xmath4 , all methods agree .",
    "next , we run the reduced enkf using the parameterization centered at the predicted mean , @xmath86 . in figure",
    "[ qg_res_enkf ] , we plot the mean rms error computed over 400 filter steps for varying ensemble sizes and varying number of basis vectors @xmath19 .",
    "the results qualitatively follow the same pattern as with the lorenz model in section [ sec : numex ] ; with large @xmath19 , a larger ensemble size is needed to get an accurate filter , and with small @xmath19 , a smaller ensemble size is sufficient to get close to the optimal performance that is achievable with that @xmath19 .",
    "for instance , with @xmath286 , ensemble size @xmath287 yields better tracking performance than @xmath288 with @xmath289 .",
    "the results are much better than what could be obtained with the standard enkf ; here , @xmath290 would be required even to get the standard enkf to converge ; see the results of @xcite .",
    "localization methods dramatically improve enkf performance ; for comparison , in figure [ qg_res_enkf ] we show the results for a simple localization , where we taper the prediction covariance matrix again using the 5th order piecewise rational function @xcite , experimentally tuning the cutoff length in the localization to achieve roughly optimal performance . however , the subspace algorithms still yield better results , as in the lorenz example .",
    "restricting the filtering onto a subspace regularizes the problem enough so that the need for localization is diminished .",
    "figure  [ qg_res_enkf ] contains some results with ensemble size @xmath291 . here",
    ", zero ensemble size means that no samples were used to propagate the uncertainty ( only the posterior mean was propagated ) , and the prediction covariance was taken to be the model error directly : @xmath292 . for small @xmath19 ,",
    "this simple 3d - var type of strategy with a fixed prior was enough to get a convergent filter . when the uncertainty propagation via samples was added and the sample size was increased , the filter accuracy was improved , as expected",
    ". the larger the value of @xmath19 , the more crucial the uncertainty propagation .",
    "this behavior can be explained by the fact that restricting the inference onto a subspace already heavily regularizes the problem , and thus propagating the covariance accurately is less important .",
    "for instance , using a small @xmath19 restricts the inference to a subspace spanned by spatially smooth basis vectors . in the full space algorithms",
    ", such smoothness information would be obtained by propagating the covariance forward in time . in the subspace method with small @xmath19 ,",
    "non - smooth directions are explicitly removed from the estimation problem , and even a simple , fixed prior can yield reasonably accurate results .",
    "note also that here the number of observations is 100 , which is , in many cases , larger than the dimension of the subspace , making the estimation problems numerically well posed and the role of the prior less important .     used .",
    "ensemble size 0 means that a fixed prior covariance was used , without error propagation : @xmath292 . ]",
    "in this paper , we presented an effective and simple - to - implement dimension reduction strategy for solving non - stationary inverse problems in a bayesian framework . by identifying a global reduced subspace that captures the essential features of the state vectors",
    ", we provided a new subspace - constrained bayesian estimation technique for reducing the computational cost of filtering algorithms .",
    "our approach is first applied to the kalman filter for linear gaussian models , and then generalized to nonlinear problems via the extended and ensemble kalman filters . in the kalman filter and extended kalman filter cases , the computational savings of our subspace - constrained technique is due to two sources : ( _ a _ ) the number of forward model simulations required in each prediction step is only equal to the reduced subspace dimensions , as the error is only propagated along the coordinates of subspace basis ; and ( _ b _ ) the update step can be formulated efficiently on the subspace coordinates .",
    "this way we also avoid handling matrices in the full dimension of the state space . in the ensemble version ,",
    "computational savings stem from the fact that when the inference is constrained into a low - dimensional subspace , fewer ensemble members are needed for covariance estimation compared to the full space approach . also , the need for covariance localization techniques to regularize the predicted covariance is diminished .    two approaches for constructing the reduced subspace",
    "are discussed . the first idea  widely used in model reduction community  is to obtain snapshots of typical model states ( e.g. , by performing a sufficiently long free model simulation ) and to compute the leading eigenvectors of the resulting empirical state covariance matrix .",
    "the second idea is to infer the state covariance matrix from a limited number of snapshots using a gaussian process hypothesis .",
    "this choice `` fills in '' the missing information about model states ( due to a limited number of snapshots ) using the correlation structure encoded in the particular choice of gp .",
    "we discussed gp constructions using either stationary kernels that directly specify the covariance matrix , or non - stationary differential operators that correspond to sparse precision matrices .",
    "the gp construction also opens the door to other possible state covariance reconstruction approaches ; for instance , one could infer the state covariance from previous data sets .",
    "we will investigate this extension in future work .",
    "we demonstrated the performance of our approach using two numerical examples .",
    "the first one is a 240-dimensional lorenz system , where the smoothness of the model states can be controlled with a tuning parameter .",
    "this rather low - dimensional example is used to demonstrate the performance of our dimension reduction approach in various regimes . for smooth settings ,",
    "the dimension can be reduced dramatically ( to less than 10 ) while still obtaining filtering accuracy  for both extended kalman filtering and ensemble filtering ",
    "comparable to the full space algorithms . on the other hand , for non - smooth settings with `` rough '' features , the level of dimension reduction that maintains filter accuracy becomes less dramatic .",
    "the second example is a 1600-dimensional two - layer quasi - geostrophic model , where the state dimension can be a reduced to about 30 without losing filtering accuracy for both the extended and ensemble filters , compared to their full space counterparts .",
    "a two order of magnitude reduction in computing time is achieved for extended kalman filtering in this case . in ensemble filtering ,",
    "our subspace approach yields accurate filtering results with smaller ensemble sizes than the standard enkf with localization .",
    "we thank alexander bibov for providing the quasi - geostrophic model implementation .",
    "a.  solonen and j.  hakkarainen acknowledge support from the academy of finland ( project numbers 284715 and 267442 ) .",
    "t.  cui and y.  marzouk also acknowledge support from the us department of energy , office of advanced scientific computing research ( ascr ) under grant number de - sc0003908 .",
    "10        auvinen h. , bardsley j.m .",
    ", haario h. and kauranne t. , 2009 . the variational kalman filter and an efficient implementation using limited memory bfgs , international journal on numerical methods in fluids , 64(3 ) , pp . 314335 .",
    "bardsley j. , solonen a. , haario h. and laine m. , 2014 .",
    "randomize - then - optimize : a method for sampling from posterior distributions in nonlinear inverse problems , siam j. sci . comput . , 36(4 ) ,",
    "a1895a1910 .          cane , m.a .",
    ", kaplan , a. , miller , r.n . ,",
    "tang , b. , hackert , e.c . ,",
    "busalacchi , a.j . , 1996 . mapping tropical pacific sea level : data assimilation via a reduced state kalman filter .",
    "j. geophys .",
    "101 , 599617 .",
    "liberty e. , woolfe f. , martinsson p. , rokhlin v. , and m. tygert , 2007 .",
    "randomized algorithms for the low - rank approximation of matrices .",
    "proceedings of the national academy of sciences , 104(51 ) , 2016720172 .",
    "lindgren f. , rue h. , and j. lindstrm , 2011 .",
    "an explicit link between gaussian fields and gaussian markov random fields : the stochastic partial differential equation approach , journal of the royal statistical society : series b , 73 , 423498 .",
    "e. ott , b. r. hunt , i. szunyogh , a. v. zimin , e. j. kostelich , m. corazza , e. kalnay , d. patil , and j. a. yorke , a local ensemble kalman filter for atmospheric data assimilation , tellus a , 56 ( 2004 ) , pp ."
  ],
  "abstract_text": [
    "<S> _ a priori _ dimension reduction is a widely adopted technique for reducing the computational complexity of stationary inverse problems . in this setting </S>",
    "<S> , the solution of an inverse problem is parameterized by a low - dimensional basis that is often obtained from the truncated karhunen - love expansion of the prior distribution . for high - dimensional inverse problems equipped with smoothing priors </S>",
    "<S> , this technique can lead to drastic reductions in parameter dimension and significant computational savings .    in this paper , we extend the concept of _ a priori _ dimension reduction to non - stationary inverse problems , in which the goal is to sequentially infer the state of a dynamical system . </S>",
    "<S> our approach proceeds in an _ offline - online _ fashion . </S>",
    "<S> we first identify a low - dimensional subspace in the state space before solving the inverse problem ( the offline phase ) , using either the method of `` snapshots '' or regularized covariance estimation . </S>",
    "<S> then this subspace is used to reduce the computational complexity of various filtering algorithms  including the kalman filter , extended kalman filter , and ensemble kalman filter  within a novel subspace - constrained bayesian prediction - and - update procedure ( the online phase ) . </S>",
    "<S> we demonstrate the performance of our new dimension reduction approach on various numerical examples . in some test cases , our approach reduces the dimensionality of the original problem by orders of magnitude and yields up to two orders of magnitude in computational savings .    </S>",
    "<S> _ keywords _ : state estimation , bayesian filtering , kalman filter , ensemble kalman filter , dimension reduction . </S>"
  ]
}