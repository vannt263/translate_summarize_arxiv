{
  "article_text": [
    "in a number of papers in the nineties ( cfr .  @xcite-@xcite and references therein ) the parallel dynamics of @xmath0-ising type neural networks has been discussed for several architectures extremely diluted , layered feedforward , recurrent using a probabilistic approach . for the asymmetric extremely diluted and layered architectures the dynamics can be solved exactly and it is known that the local field only contains gaussian noise . for networks with symmetric connections , however , things are quite different . even for extremely diluted versions of these systems feedback correlations",
    "become essential from the second time step onwards , complicating the dynamics in a nontrivial way .    a complete solution for the parallel dynamics of symmetric @xmath0-ising networks at zero - temperature taking into account all feedback correlations , has been obtained only recently using a probabilistic signal - to - noise ratio analysis @xcite-@xcite .",
    "thereby it is seen that both for the fully connected and the extremely diluted symmetric architectures , the local field contains a discrete and a normally distributed noise part .",
    "the difference between the two architectures is that for the diluted model the discrete part at a certain time @xmath1 does not involve the spins at all previous times @xmath2 up to @xmath3 but only the spins at time step @xmath4 .",
    "even so , this discrete part prevents a closed - form solution of the dynamics but a recursive scheme can be developed in order to calculate the complete time evolution of the order parameters , i.e. , the retrieval overlap and the activity .    in the work above",
    "the focus has been on the non - equilibrium behavior of the order parameters of the network .",
    "but , since the local field itself is a basic ingredient in the development of the relevant recursive scheme it is interesting to study also the non - equilibrium behavior of the local field distribution .",
    "the more so since this distribution does not convergence to a simple sum of gaussians as is frequently thought , but it develops a gap structure .",
    "this is precisely one of the points studied in detail in the present communication .",
    "moreover , the analogies and differences between the fully connected architecture and the symmetrically diluted one are highlighted .",
    "finally , numerical simulations are presented confirming the analytic study and giving additional insight in the structure of these local field distributions .",
    "consider a neural network @xmath5 consisting of @xmath6 neurons which can take values @xmath7 from a discrete set @xmath8 .",
    "the @xmath9 patterns to be stored in this network are supposed to be a collection of independent and identically distributed random variables ( i.i.d.r.v . ) , @xmath10 , @xmath11 and @xmath12 , with zero mean , @xmath13=0 $ ] , and variance @xmath14 $ ] .",
    "the latter is a measure for the activity of the patterns .",
    "given the configuration @xmath15 , the local field in neuron @xmath16 equals @xmath17 with @xmath18 the synaptic coupling from neuron @xmath19 to neuron @xmath16 . in the sequel",
    "we write the shorthand notation @xmath20 .    for the extremely diluted symmetric ( sed ) and the fully connected ( fc ) architectures",
    "the couplings are given by the hebb rule @xmath21 with the @xmath22 chosen to be i.i.d.r.v . with distribution @xmath23 and satisfying @xmath24 .",
    "for the diluted symmetric model the architecture is a local cayley - tree but , in contrast with the diluted asymmetric model , it is no longer directed such that it causes a feedback from @xmath25 onwards . in the limit",
    "@xmath26 the probability that the number of connections @xmath27 giving information to the site @xmath12 , is still a poisson distribution with mean @xmath28 $ ] .",
    "thereby it is assumed that @xmath29 and in order to get an infinite average connectivity allowing to store infinitely many patterns one also takes the limit @xmath30 @xcite .    at zero temperature",
    "all neurons are updated in parallel according to the rule @xmath31-",
    "\\theta\\left[b(s_k+s_{k-1})-x\\right ]                          \\right]\\end{aligned}\\ ] ] with @xmath32 and @xmath33 . here",
    "@xmath34 is the gain function and @xmath35 is the gain parameter of the system . for finite @xmath0 ,",
    "this gain function is a step function .",
    "the gain parameter @xmath36 controls the average slope of @xmath34 .",
    "in order to measure the retrieval quality of the system one can use the hamming distance between a stored pattern and the microscopic state of the network @xmath37 ^ 2          \\,.\\ ] ] this introduces the main overlap and the arithmetic mean of the neuron activities @xmath38 ^ 2     \\,.\\ ] ] the key question is then how these quantities evolve in time under the parallel dynamics specified before . for a general time step",
    "we find from eq .",
    "( [ eq : gain ] ) using the law of large numbers ( lln ) that in the thermodynamic limit @xmath39 where the convergence is in probability @xcite . in the above",
    "@xmath40 denotes the average both over the distribution of the embedded patterns @xmath41 and the initial configurations @xmath42 .",
    "the average over the latter is hidden in an average over the local field through the updating rule ( [ eq : gain ] ) .",
    "some remarks are in order . for the symmetric diluted model the sum over the sites @xmath16 is restricted to @xmath43 , the part of the tree connected to neuron @xmath19 . moreover",
    ", for that model the thermodynamic limit contains the limit @xmath44 besides the @xmath26 limit . in this thermodynamic limit @xmath45",
    "all averages have to be taken over the treelike structure , viz .",
    "@xmath46 , and the capacity defined by @xmath47 has to be replaced by @xmath48 .    in ( [ eq : a ] ) the local field is the main ingredient .",
    "suppose that the initial configuration of the network @xmath49 , is a collection of i.i.d.r.v .  with mean @xmath50=0 $ ] , variance @xmath51=a_0 $ ] , and correlated with",
    "only one stored pattern , say the first one @xmath52 : @xmath53=\\delta_{i , j}\\delta_{\\mu,1}m^1_0 a\\ ] ] with @xmath54 . by the lln one",
    "gets for the main overlap and the activity at @xmath55 @xmath56                  = m^1_0                  \\label{eq : mo }        \\\\",
    "a(0)&\\equiv&\\lim_{(c),n \\rightarrow \\infty } a_\\lambda ( 0 )                  \\ustr{pr}{= } \\e[\\sigma_i^2(0)]=a_0                  \\label{eq : a0}\\end{aligned}\\ ] ] where the notation should be clear . in order to obtain the configuration at @xmath57 we have to calculate the local field ( [ eq : h ] ) at @xmath55 . to do this",
    "we employ the probabilistic signal - to - noise ratio analysis ( @xcite-@xcite ) . recalling the learning rule ( [ eq : jfc ] ) we separate the part containing the signal from the part containing the noise . in the limit @xmath26",
    "we then arrive at @xmath58 where the convergence is in distribution @xcite and with @xmath59 representing a gaussian random variable with mean @xmath3 and variance @xmath60 .",
    "we note that this structure of the distribution of the local field at time zero  signal plus gaussian noise  is typical for all architectures treated in the literature .    for a general time step @xmath61",
    ", a tedious study reveals that the distribution of the local field is given by @xcite , @xcite @xmath62                \\label{eq : hrec}\\ ] ] where @xmath63 for the fully connected architecture and @xmath64 for the symmetrically diluted one .",
    "so , the local field at time @xmath1 consists out of a discrete part and a normally distributed part , viz .",
    "@xmath65 where @xmath66 and @xmath67 satisfy the recursion relations @xmath68",
    "+ \\xi_i^1m^1(t+1 )       \\label{eq : mrec } \\\\          \\label{eq : drec }         & & v(t+1)= \\alpha a(t+1)a+f\\chi^2(t)v(t)+                  2 f \\alpha a \\chi(t ) { cov}[\\tilde r^\\mu(t),r^\\mu(t ) ] \\ , .   \\end{aligned}\\ ] ]",
    "the quantity @xmath69 reads @xmath70 where @xmath71 is the probability density of @xmath72 in the thermodynamic limit .",
    "furthermore , @xmath73 is defined as @xmath74 and @xmath75 is given by a similar expression with @xmath76 replaced by @xmath77 . finally , as can be read off from eq .",
    "( [ eq : mrec ] ) the quantity @xmath66 consists out of a signal term and a discrete noise term , viz .",
    "@xmath78 \\ , \\sigma _ i(t ' )   \\ , .",
    "\\label{eq : mm}\\ ] ] since different architectures contain different correlations not all terms in these final equations are present , as is apparent through @xmath79 .",
    "we remark that for the asymmetric diluted and the layered feedforward architecture @xmath80 so that in these cases the local field consists out of a signal term plus gaussian noise for _ all _ time steps @xcite,@xcite .    for the architectures treated here we still have to determine the probability density @xmath81 in eq .",
    "( [ eq : chi ] ) . this can be done by looking at the form of @xmath66 given by eq .",
    "( [ eq : mm ] ) .",
    "the evolution equation tells us that @xmath82 can be replaced by @xmath83 such that the second and third terms of @xmath66 are the sums of stepfunctions of correlated variables .",
    "these are also correlated through the dynamics with the normally distributed part of @xmath84 .",
    "therefore , the local field can be considered as a transformation of a set of correlated normally distributed variables @xmath85 , which we choose to normalize .",
    "defining the correlation matrix @xmath86 \\right)$ ] we arrive at the following expression for @xmath81 for the fully connected model @xmath87 with @xmath88 .",
    "for the symmetric diluted case this expression simplifies to @xmath89 } dx_{t-2s } ~",
    "\\delta \\left(y -\\xi^1_i m^1(t)- \\alpha \\chi(t-1)\\sigma_i(t-1 )                -\\sqrt{\\alpha a(t)}\\,x_t\\right ) \\nonumber\\\\               & \\times & \\frac{1}{\\sqrt{\\mbox{det}(2\\pi w ) } }              ~\\mbox{exp}\\left(-\\frac{1}{2}{\\bf x } w^{-1 }              { \\bf x}^t \\right )              \\label{eq : fhdisd}\\end{aligned}\\ ] ] with @xmath90},\\ldots x_{t-2},x_t)$ ] .",
    "the brackets @xmath91 $ ] denote the integer part of @xmath92 .",
    "the equilibrium distribution of the local field can be obtained by eliminating the time dependence in the evolution equations ( [ eq : hrec ] ) @xmath93 with @xmath94 for the fully connected architecture and @xmath95 for the extremely diluted one . the corresponding updating rule ( [ eq : gain ] ) @xmath96 in general admits more than one solution . a maxwell construction ( see , e.g. , refs .",
    "@xcite,@xcite,@xcite ) can be made leading to a unique solution @xmath97 such that we have @xmath98 for @xmath99 .",
    "this unique solution can be used to obtain fixed - point equations for the main overlap and activity ( [ eq : a ] ) . those equations which we choose not to write down explicitly here ( see refs .",
    "@xcite,@xcite ) are equal to the equations derived from a thermodynamic replica - symmetric mean - field theory approach @xcite,@xcite .",
    "we remark that for analog networks ( @xmath100 ) such a maxwell construction is not necessary because eq .",
    "( [ eq : res1 ] ) has only one solution .",
    "next , we calculate the probability density of the local field by plugging this result ( [ eq : res1])-([eq : res3 ] ) into ( [ eq : hfix ] ) to obtain , forgetting about the site index @xmath16 and the pattern index @xmath101 @xmath102               -\\theta [ \\tilde{b}(s_k+s_{k-1})+\\alpha \\chi\\eta s_k -h ]         \\biggr )         \\label{eq : distri}\\end{aligned}\\ ] ] meaning that ( q-1 ) gaps occur respectively at @xmath103 with width @xmath104 . for analog networks",
    "no gaps occur .",
    "when @xmath105 the effective gain function ( [ eq : res2 ] ) becomes two - state ising - like as in the hopfield model such that case only one gap occurs .    for @xmath106",
    "this expression simplifies to @xmath107 and for q=3 we have @xmath108 similar formula can be written down for bigger values of @xmath0 .",
    "for @xmath106 this result seems to be consistent with the gap in the internal - field distribution for an infinite range spin glass found by a bethe - peierls - weiss approach @xcite ( see also  @xcite-@xcite ) .",
    "we have investigated this probability distribution numerically using the corresponding fixed - point equations mentioned before , for several values of @xmath0 and compared them with those obtained from numerical simulations of the dynamics for networks of @xmath109 neurons .",
    "some typical results are shown in figs .  1 - 6 .    in figs .  1 - 2 the local field distribution for the fully connected @xmath106 network",
    "is shown for a retrieval state ( @xmath110 ) just below the critical capacity and a non - retrieval spin - glass state ( @xmath111 ) just above it .",
    "both the first few time steps and the equilibrium result derived above are compared with numerical simulations .",
    "they are in agreement . for the retrieval state",
    "there is , typically , a small gap in the equilibrium distribution around h=0 . for small @xmath112",
    "the gap is very narrow .",
    "furthermore , in the simulations one sees that this gap shows up very quickly . for the non - retrieval state",
    "the gap is typically much bigger . again in the simulations one quickly sees the gap but it is extremely difficult numerically to find points touching the zero axis because of finite size effects .",
    "figure 3 shows the gap width at equilibrium , @xmath113 , for the non - retrieval state as a function of @xmath0 with @xmath114 .",
    "it scales as @xmath115 and , hence , decreases to zero for @xmath116 .",
    "this constant behaviour of @xmath117 attains already for values of @xmath118 and is also seen for the retrieval state .",
    "these results are insensitive to the structure of the symmetric architecture .    in figure 4 the gap boundaries in @xmath119 as a function of @xmath112",
    "are compared for retrieval and non - retrieval states in the symmetric diluted @xmath120 model .",
    "we remark that in this case the spin - glass states do not exist for @xmath121 @xcite so that there is no gap for these @xmath112-values . for @xmath112 large enough ( @xmath122 for retrieval states and @xmath123 for spin - glass states ) there exists one gap only since",
    "the effective gain function becomes ising - like @xcite .",
    "more gaps with smaller widths are formed when increasing @xmath0 for both the fully connected and diluted models . for @xmath116",
    "the gaps disappear .",
    "figure  5 compares the gaps for the spin - glass states in the fully connected and symmetric diluted @xmath124 models with @xmath114 . for @xmath125 there exist no spin - glass states in the diluted model @xcite and for @xmath126 there are none in the fully connected model @xcite . when both do exist the gap widths are almost equal .",
    "so the dilution has some influence on the existence of the gap but , again , not on its width .",
    "finally , fig .",
    "6 presents the local field distribution for the symmetric diluted @xmath127 model for a retrieval state ( @xmath128 ) just below the critical capacity . only the distribution with pattern values",
    "@xmath129 is shown .",
    "it is asymmetric and two gaps are found at equilibrium . for pattern values @xmath3 the distribution is symmetric and the gap locations and widths are the same ( see eq .",
    "( [ eq : distri ] ) ) but their height is different .    in conclusion ,",
    "we have studied the time evolution of the local field in symmetric @xmath0-ising neural networks both in the retrieval and spin - glass regime .",
    "we have found a gap structure in the local field distribution depending on the specific architecture and on the value of @xmath0 .",
    "the results agree with the numerical simulations we have performed .",
    "this work has been supported in part by the fund of scientific research , flanders - belgium and the korea science and engineering foundation through the src program .",
    "the authors are indebted to a.  coolen , g.  jongen and v.  zagrebnov for constructive discussions .    99 a.e .  patrick and v.a .",
    "zagrebnov , parallel dynamics for an extremely diluted neural network , : l1323 ( 1990 ) ; : 1009 ( 1992 ) .",
    "patrick and v.a .",
    "zagrebnov , on the parallel dynamics for the little - hopfield model , : 59 ( 1991 ) .",
    "watkin and d.  sherrington , the parallel dynamics of a dilute symmetric neural network , : 5427 ( 1991 ) .",
    "patrick and v.a .",
    "zagrebnov , a probabilistic approach to parallel dynamics for the little - hopfield model , : 3413 ( 1991 ) .",
    "d.  boll , b.  vinck , and v.a .",
    "zagrebnov , on the parallel dynamics of the @xmath0-state potts and @xmath0-ising neural networks , : 1099 ( 1993 ) .",
    "d.  boll , g.m .",
    "shim , b.  vinck , and v.a .",
    "zagrebnov , retrieval and chaos in extremely diluted q - ising neural networks , : 565 ( 1994 ) .",
    "d.  boll , g.m .",
    "shim , and b.  vinck , retrieval and chaos in layered q - ising neural networks , : 583 ( 1994 ) .",
    "d.  gandolfo , m.  sirugue - collin and v.a .",
    "zagrebnov , local instability and oscillations of trajectories in a diluted symmetric neural network , _ network : computation in neural systems _ * 9 * : 563 ( 1998 ) d.  boll , g.  jongen and g.m .",
    "shim , parallel dynamics of fully connected @xmath0-ising neural networks , : 125 ( 1998 ) .",
    "d.  boll , g.  jongen and g.m .",
    "shim , parallel dynamics of extremely diluted symmetric q - ising neural networks , : 861 ( 1999 ) .",
    "shiryayev , _ probability _ ( springer , new york , 1984 ) . m.  shiino and t.  fukai , self - consistent signal - to - noise analysis of the statistical behavior of analog neural networks and enhancement of the storage capacity , : 867 ( 1993 ) .",
    "d.  boll , h.  rieger and g.m .",
    "shim , thermodynamic properties of fully connected @xmath0-ising neural networks , : 3411 ( 1994 ) .",
    "d.  boll , d.  carlucci and g.m .",
    "shim , thermodynamic properties of extremely diluted @xmath0-ising neural networks , : 6481 ( 2000 ) .",
    "schowalter and m.w .",
    "klein , analytic treatment of the hole in the internal field distribution for an infinite - range spin glass , _ j.phys.c : solid state physics _",
    "* 12 * : l935 ( 1979 ) .",
    "zagrebnov and a.s .",
    "chvyrov , the little - hopfield model : recurrence relations for retrieval - pattern errors , _ sov.phys.jetp _ * 68 * : 153 ( 1989 ) a.c.c .",
    "coolen and d.  sherrington , order parameter flow in the fully connected hopfield model near saturation , : 1921 ( 1994 ) ."
  ],
  "abstract_text": [
    "<S> the time evolution of the local field in _ symmetric _ @xmath0-ising neural networks is studied for arbitrary @xmath0 . in particular , </S>",
    "<S> the structure of the noise and the appearance of gaps in the probability distribution are discussed . </S>",
    "<S> results are presented for several values of @xmath0 and compared with numerical simulations .    </S>",
    "<S> symmetric networks ; @xmath0-ising neurons ; parallel dynamics ; local field ; probabilistic approach </S>"
  ]
}