{
  "article_text": [
    "a lot of research in the field of information retrieval aims at improving the _ quality _ of search results .",
    "search quality might for instance be improved by new scoring functions , new indexing approaches , new query ( re-)formulation approaches , etc . to make a scientific judgment of the quality of a new search approach , it is good practice to use so - called benchmark test collections , such as those provided by trec @xcite .",
    "the following steps typically need to be taken :    1 .",
    "the researcher codes the new approach by adapting an experimental search system , such as lemur @xcite , pf / tijah @xcite , or terrier @xcite ; 2 .",
    "the researcher uses the system to create an inverted index on the documents from the test collection ; 3 .",
    "the researcher puts the queries to the experimental search engine and gathers the top @xmath0 search results ( a common value for trec experiments is @xmath1 ) ; 4 .",
    "the researcher compares the top @xmath0 to a golden standard by computing standard evaluation measures such as mean average precision .",
    "in our experience , step 1 , actually coding the new approach , takes by far the most effort and time when conducting an information retrieval experiment .",
    "coding new retrieval approaches into existing search engines like lemur , pf / tijah and terrier is a tedious job , even if the code is maintained by members of the same research team .",
    "it requires detailed knowledge of the existing code of the search engine , or at least , knowledge of the part of the code that needs to be adapted .",
    "radical new approaches to information retrieval , i.e. , approaches that need information that is not available from the search engines inverted index , require reimplementing part of the indexing functionality .",
    "such radical new approaches are therefore not often evaluated , and most research is done by small changes to the system .    in his wsdm keynote lecture , dean",
    "@xcite describes how mapreduce @xcite is used at google for experimental evaluations .",
    "new ranking ideas are tested off - line on human rated query sets similar to the queries from trec . running such off - line tests",
    "has to be easy for the researchers at google , possibly at the expense of the efficiency of the prototype .",
    "so , it is okay if it takes hours to run for instance 10,000 queries , as long as the experimental infrastructure allows for fast and easy coding of new approaches .",
    "a similar experimental setup was followed by microsoft at trec 2009 : craswell et al .",
    "@xcite use dryadlinq @xcite on a cluster of 240 machines to run web search experiments .",
    "their setup also sequentially scans all document representations , providing a flexible environment for a wide range of experiments .",
    "the researchers plan to do many more to discover its benefits and limitations .",
    "the work at google and microsoft shows that sequential scanning over large document collections is a viable approach to experimental information retrieval .",
    "some of the advantages are :    1 .",
    "researchers spend less time on coding and debugging new experimental retrieval approaches ; 2 .",
    "it is easy to include new information in the ranking algorithm , even if that information would not normally be included in the search engine s inverted index ; 3 .",
    "researchers are able to oversee all or most of the code used in the experiment ; 4 .",
    "large - scale experiments can be done in reasonable time .",
    "we show that indeed sequential scanning is a viable experimental tool , even if only a few machines are available . in section [ sec : approach ] we describe the mapreduce search system .",
    "sections [ sec : results ] and [ sec : conclusion ] contain experimental results and concluding remarks .",
    "mapreduce is a framework for batch processing of large data sets on clusters of commodity machines @xcite .",
    "users of the framework specify a _",
    "mapper _ function that processes a key / value pair to generate a set of intermediate key / value pairs , and a _ reducer _ function that processes intermediate values associated with the same intermediate key .",
    "the pseudo code in figure [ tab : pseudocode ] outlines our sequential search implementation .",
    "the implementation does a single scan of the documents , processing all queries in parallel .",
    "[ cols= \" < \" , ]      anchor text extraction on all english documents of clueweb09 takes about 11 hours on our cluster .",
    "the anchor text representation contains text for about 87  % of the documents , about 400 gb in total .",
    "a subsequent trec run using 50 queries on the anchor text representation takes less than 30 minutes .",
    "our linear search system implements a fairly simple language model with a length prior without stemming or stop words .",
    "it achieves expected precision at 5 , 10 and 20 documents retrieved of respectively 0.42 , 0.39 , and 0.35 ( mtc method ) , similar to the best runs at trec 2009  @xcite .",
    "figure [ fig : time ] shows how the system scales when processing up to 5,000 queries , using random sets of queries from the trec 2009 million query track .",
    "reported times are full hadoop job times including job setup and job cleanup averaged over three trials .",
    "processing time increases only slightly if more queries are processed .",
    "whereas the average processing time per query is about 35 seconds per query for 50 queries , it goes down to only 1.6 second per query for 5,000 queries . for comparison ,",
    "the graph shows the performance of `` lemur - one - node '' , i.e. , lemur version 4.11 running on _",
    "one fourteenth _ of the anchor text representation on a single machine . a distributed version of lemur searching the full full anchor text representation would not do faster",
    ": it would be as fast as the slowest node , it would need to send results from each node to the master , and to merge the results .",
    "lemur - one - node takes 3.3 seconds per query on average for 50 queries , and 0.44 seconds on average for 5,000 queries .",
    "the processing times for lemur were measured after flushing the file system cache .",
    "although lemur can not process queries in parallel , the system s performance benefits a lot from receiving a lot of queries .",
    "lemur s performance scales sublinearly because it caches intermediate results .",
    "still , at 5,000 queries lemur - one - node is only 3.6 times faster than the mapreduce system . for experiments at this scale , the benefits of the full , distributed lemur are probably negligible .",
    "the idea to use sequential scanning of documents to research new retrieval approaches is certainly not new : we know of at least one researcher who used sequential scanning over ten years ago for his thesis @xcite . without high - level programming paradigms like mapreduce , however , efficiently implementing sequential",
    "scanning is not a trivial task , and without a cluster of machines the approach does not scale to large collections .",
    "lin @xcite used hadoop mapreduce for computing pairwise document similarities .",
    "our implementation resembles lin s brute force algorithm that also scans document representations linearly .",
    "our approach is simpeler because our preprocessing step does not divide the collection into blocks , nor does it compute document vectors .",
    "a faster turnaround of the experimental cycle can be achieved by making coding of experimental systems easier .",
    "faster coding means one is able to do more experiments , and more experiments means more improvement of retrieval performance .",
    "we implemented a full experimental retrieval system with little effort using hadoop mapreduce . using 15 machines to search a web crawl of 0.5 billion pages ,",
    "the proposed mapreduce approach is less than 10 times slower than a single node of a distributed inverted index search system on a set of 50 queries .",
    "if more queries are processed per experiment , the processing times of the two systems get even more close .",
    "the code used in our experiment is open source and available to other researchers at : http://mirex.sourceforge.net",
    "many thanks to sietse ten hoeve , guido van der zanden , and michael meijer for early implementations of the system .",
    "the research was partly funded by the netherlands organization for scientific research , nwo , grant 639.022.809 .",
    "we are grateful to yahoo research , barcelona , for sponsoring our cluster .",
    "n.  craswell , d.  fetterly , m.  najork , s.  robertson , and e.  yilmaz . at trec 2009 : web and relevance feedback tracks . in _ proceedings of the 18th text retrieval conference ( trec)_. nist special publication 500 - 278 , 2009 .",
    "brute force and indexed approaches to pairwise document similarity comparisons with mapreduce . in _ proceedings of the 32nd international acm sigir conference on research and development in information retrieval _ , 2009 .",
    "y.  yu , m.  isard , d.  fetterly , m.  budiu , u.  erlingsson , p.  kumar , and j.  currey . : a system for general - purpose distributed data - parallel computing using a high - level language . in _ proceedings of the 8th symposium on operating system design and implemention ( osdi ) _ , 2008 ."
  ],
  "abstract_text": [
    "<S> we propose to use mapreduce to quickly test new retrieval approaches on a cluster of machines by sequentially scanning all documents . </S>",
    "<S> we present a small case study in which we use a cluster of 15 low cost machines to search a web crawl of 0.5 billion pages showing that sequential scanning is a viable approach to running large - scale information retrieval experiments with little effort . </S>",
    "<S> the code is available to other researchers at : http://mirex.sourceforge.net </S>"
  ]
}