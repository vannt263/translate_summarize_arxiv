{
  "article_text": [
    "in the past decade , european research institutions , scientific collaborations and resource providers have been involved in the development of software frameworks that eventually led to the set - up of unprecedented distributed e - infrastructures such as the european grid infrastructure ( egi ) @xcite ; their collaboration made it possible to produce , store and analyze petabytes of research data through hundreds of thousands of compute processors , in a way that has been instrumental for scientific research and discovery worldwide .",
    "this is particularly true in the area of high energy physics , where distributed computing has been instrumental as data analysis technology in the framework of the lhc computing grid ( wlcg ) @xcite .",
    "new technological advancements , such as virtualization and cloud computing , pose new important challenges when it comes to exploit scientific computing resources .",
    "services to researchers need to evolve in parallel to maximize effectiveness and efficiency , in order to satisfy different scenarios : from everyday necessities to new complex requirements coming from diverse scientific communities .    in order to expose to researchers the power of current technological capabilities",
    ", a key challenge continues to be accessibility .",
    "virtualization techniques have the potential to make available unprecedented amounts of resources to researchers .",
    "however , serving scientific computing is much more complex than providing access to a virtualized machine .",
    "it implies being able to support in a secure way complex simulations , data transfer & analysis scenarios , which are by definition in constant evolution .",
    "this is particularly the case of the hep areas , where we can distinguish three paradigmatic usage scenarios :    * massive data analysis at the lhc and experiments which in general must deal with analysis of large amounts of data in computing farms .",
    "the paradigmatic case is the data analysis of the lhc at experiments like cms and atlas . during lhc run 1 , the tools developed by wlcg oriented to grid computing worked very smoothly .",
    "however , both the amount of data involved in run 2 and beyond , and the evolution of the computing infrastructures implies that new scenarios for accessing resources need to be devised .",
    "the experiments cms and atlas have both already analyzed the technical feasibility of using resources in cloud mode ( see @xcite ) . * high performance computing in facilities with low latency interconnects dedicated to monte carlo simulations , for example for lattice quantum chromodynamics ( lattice qcd ) .",
    "resource requirements for development phases are medium size clusters with up to a few hundred cores ; in the production phase , lattice qcd is served by large hpc farms which can require from a few thousand , up to tens of thousands of cores for ground breaking projects .",
    "the storage requirements are in the order of a few terabytes for development and petabyte for production phases @xcite .",
    "* phenomenological simulation & prediction codes , often including legacy software components and complex library dependencies . the resources requirements can reach about a thousand cores , with storage in the range of the terabyte .",
    "here one current showstopper for researchers is the possibility of having installed the right environment ( in terms of legacy libraries for example ) @xcite .    making computing and storage resources",
    "really exploitable by scientists implies adapting access and usage procedures to the user needs .",
    "the challenge that remains is developing such procedures in a manner which is reliable , secure , sustainable , and with the guarantee that results are reproducible .    in order to settle the scenario where our platform is to be deployed",
    ", we must specify what we mean by the term resources . by this",
    "we always refer to a _ large pool of computing and storage resources _ , in which the users do not need to know ( because it is not relevant for the computation ) which machine it is actually being used . if the computing requirements would have a strong hardware dependency ( optimized for a particular processor ) , or if the application is so large scale that a significant fraction of the available resources in a given computing center needs to be used , the discussion would be completely different .",
    "this is the context of the work of the horizon 2020 project indigo - datacloud , referred to as indigo from now on @xcite , addressing the challenge of developing advanced software layers , deployable in the form of a data / computing platform , targeted at scientific communities .    in this paper",
    "we present an architectural design to satisfy the challenges mentioned above , and which we have developed in the framework of the indigo project .",
    "we highlight the interrelations among the different components involved and outline the reasoning behind the choices that were made .",
    "the remainder of this paper is structured as follows .",
    "we first summarize the proposed progress beyond the state of the art ( section [ sec : summary ] ) . in order to motivate our choices",
    "we next describe several generic user scenarios in section [ sec : motivation ] , with the main functionalities we want to satisfy , offering a global , high - level view of the indigo architecture highlighting the interrelations among the different layers .",
    "section [ sec : paas ] describes the architecture of the paas layer , together with the description of the user interfaces to be developed .",
    "section [ sec : infrastructure ] is devoted to the computer center layer of the architecture .",
    "in particular , it describes the middleware choices and developments needed to fully support the paas layer at the infrastructure level .",
    "section [ sec : portals ] describes how indigo interfaces with users .",
    "finally , section [ sec : data ] describes the solutions for unified data management .",
    "the paper is concluded by section [ sec : conclusions ] , drawing some conclusions and highlighting future work .",
    "in order to provide the reader with a global understanding of the practical implications of this work , we highlight here the technical progress we intend to achieve at the different layers that compose a scientific computing infrastructure .      at the resource provider level ,",
    "computing centers offer resources in an infrastructure as a service ( iaas ) mode .",
    "the main challenges to be addressed at this level are :    1 .   improved scheduling for allocation of resources by popular open source cloud platforms , i.e. openstack @xcite and opennebula @xcite .",
    "+ in particular , both better scheduling algorithms and support for spot - instances are currently much needed .",
    "the latter are in particular needed to support allocation mechanisms similar to those available on commercial clouds such as amazon web services and google cloud platform , while the former are useful to address the fact that typically all computing resources in scientific data centers are always in use .",
    "+ for lhc data analysis several attempts at using such spot - instances in the framework of commercial cloud providers have already taken place in the experiment cms @xcite and atlas @xcite .",
    "having an open source software framework supporting such operation mode at the iaas level will therefore be very important for the centers aiming to support such data analysis during lhc run 2 and beyond .",
    "2 .   improved quality of service ( qos ) capabilities of storage resources .",
    "the challenge here is to develop a better support for quality of services in storage , to enable high - level storage management systems ( such as fts ) and make it aware of information about the underlying storage qualities .",
    "+ the impact of such qos when applied to storage interfaces such as dcache will be obvious for lhc data analysis and for the long - term support , preservation and access of experiment data .",
    "+ for example can we save a lot of redundant copies , when the high level storage manager knows how many copies are already available at one storage location .",
    "due to a lack of this information",
    "the assumption made is that only one copy is available .",
    "3 .   improved capabilities for networking support .",
    "this is particularly the case when it comes to deploy tailored network configurations in the framework of opennebula and openstack .",
    "4 .   improved and transparent support for docker containers .",
    "+ containers provide an easy and efficient way to encapsulate and transport applications .",
    "indeed , they represent a higher level of abstraction than the crude concept of a `` virtual machine '' .",
    "the benefits of using containers , in terms of easiness in the deployment of specialized software , including contextualization features , eg . for phenomenology applications , are clear .",
    "+ they offer obvious advantages in terms of performance when compared with virtual machines , while also opening the door to exploit specialized hardware such as gpgpus and low - latency interconnection interfaces ( infiniband ) .",
    "+ the general idea here is to make containers `` first - class citizens '' in scientific computing infrastructures .      in the next layer",
    "we find the paas layer .",
    "this is a set of services whose objective is to leverage disparate hardware resources coming from the iaas level ( grid of distributed clusters , public and private clouds , hpc systems ) to enhance the user experience .    in this context",
    "a paas should provide advanced tools for computing and for processing large amounts of data , and to exploit current storage and preservation technologies , with the appropriate mechanisms to ensure security and privacy .",
    "the following points describe the most important missing capabilities which today require further developments :    1 .   improved capabilities in the geographical exploitation of cloud resources .",
    "end users do not need to know where resources are located , because the paas layer should be hiding the complexity of both scheduling and brokering .",
    "2 .   support for data requirements in cloud resource allocations .",
    "resources can be allocated where data is stored , therefore facilitating interactive processing of data .",
    "the benefits of such an enhancement are clear for software stacks for interactive data processing tools such as root @xcite and proof @xcite .",
    "3 .   support for application requirements in cloud resource allocations .",
    "for example , a given user can request to deploy an application on a cluster with infiniband interfaces , or with access to specialized hardware such as gpgpus .",
    "elasticity in the provisioning of such specialized small size clusters for development purposes would have a great impact in the everyday work of many researchers in the area of lattice qcd for example . 4 .",
    "transparent client - side import / export of distributed cloud data .",
    "deployment , monitoring and automatic scalability of existing applications , including batch systems on - demand .",
    "for example , existing applications such as web front - ends , proof clusters or even a complete batch system cluster ( with appropriate user interfaces ) can be automatically and dynamically deployed in highly - available and scalable configurations .",
    "integrated support for high - performance big data analytics and workflow engines such as taverna @xcite , ophidia @xcite or spark @xcite",
    "support for dynamic and elastic clusters of computational resources .      in the next layer",
    "we find the user interface , which is responsible to convey all the above mentioned developments to the user .",
    "this means in particular that it should provide ready - to - use tools for such capabilities to be exploited , with the smoothest possible learning curve .",
    "providing such an interface between the user and the infrastructure poses two fundamental challenges :    1 .   enabling infrastructure services to accept state of the art user authentication mechanisms ( e.g. openid connect , saml ) on top of the already existing x.509 technology .",
    "for example , distributed authorization policies are very much needed in scientific cloud computing environments , therefore a dedicated development effort is needed in this area .",
    "hence , the authentication and authorization infrastructure ( aai ) is a key ingredient to be fed into the architecture .",
    "2 .   making available the appropriate libraries , servlets and portlets , implementing the different functionalities of the platform ( aai , data access , job processing , etc . ) that are the basis to integrate such services with known user tools , portals and mobile applications .",
    "we have designed an architecture containing the elements needed to provide scientific users with the capability of using heterogeneous infrastructures , adressing the challenges described above . in the following",
    "we describe the rational and motivations of the technical choices we made .    as a first step we have performed a detailed user requirements analysis , whose main conclusions we show in the form of two generic scenarios : the first is computing oriented , while the second is data analysis oriented . for full details containing user communities description and detailed usage patterns",
    "we refer to our requirements document @xcite .",
    "our architecture is based on the analysis of a number of use cases originating from different research communities in the areas of high energy physics , environmental modelling , bioinformatics , astrophysics , social sciences and others . from this requirements analysis",
    "we have extracted two generic usage scenarios , which can support a wide range of applications in these areas .",
    "the first generic user scenario is a computing portal service . in such scenario ,",
    "computing applications are stored by the application developers in repositories as downloadable images ( in the form of vms or containers ) .",
    "such images can be accessed by users via a portal , and require a back - end for execution ; in the most common situation this is typically a batch queue .",
    "the number of nodes available for computing should increase ( scale out ) and decrease ( scale in ) , according to the workload .",
    "the system should also be able to do cloud - bursting to external infrastructures when the workload demands it .",
    "furthermore , users should be able to access and reference data , and also to provide their local data for the runs .",
    "a solution along these lines is shown in figure [ fig:1 ] .",
    "a second generic use case is described by scientific communities that have a coordinated set of data repositories and software services ( for example proof , or r - studio ) to access , process and inspect them .",
    "processing is typically interactive , requiring access to a console deployed on the data premises . in figure [ fig:2 ]",
    "we show a schematic view of such a use case .",
    "as pointed out in the introduction , the current technology based on lightweight containers and related virtualization developments make it possible to design software layers in the form of platforms that support such usage scenarios in a relatively straightforward way .",
    "we can see already many examples in the industrial sector , in which open source paas solutions such as openshift or cloud foundry are being deployed to support enterprise work in different sectors @xcite .    however , the case of supporting scientific users is more complex , first because of the heterogeneous nature of the infrastructures at the iaas level ( i.e. the resource centers ) , and secondly because of the inherent complexity of the scientific work requirements .",
    "the key point here is to find the right agreement to unify interfaces between the paas and iaas levels .",
    "for the architecture to go beyond just a theoretical implementation of tools and apis , we must include the practicalities of the computing centers in the discussion .",
    "the architecture should be capable of supporting the interaction with the resource centers via standard interfaces . here",
    "the word standard is meant in a very wide sense including _ de jure _ as well as _ de facto _ standards .",
    "virtualization of resources is the key word in order to properly address the interface with the resource centers . in other words ,",
    "the software stack should be able to virtualize local compute , storage and networking iaas resources , providing those resources in a standardized , reliable and performing way to remote customers or to higher level federated services .",
    "the iaas layer is normally provided to scientists by large resource centers , typically engaged in well - established european e - infrastructures .",
    "the e - infrastructure management bodies or the resource centers themselves will select the components they operate .",
    "therefore , the success of any software layer in this respect is being able to be flexible enough as to interact with the most popular choices of the computer centers , without interfering , or very minimally , in the operation of their facilities .    as a consequence , as a part of the development effort",
    ", we have analyzed a selection of the most prominent components to interface computing and storage in the resource centers , and develop the appropriate interfaces to high - level services based on standards .",
    "figure [ fig:3 ] shows a schematic view of the interrelation among those components .",
    "the paas core components will be deployed as a suite of small services using the concept of `` micro - service '' .",
    "this term refers to a software architecture style , in which complex applications are composed of small independent processes communicating with each other via lightweight mechanisms like http resource apis .",
    "the modularity of micro - services makes the approach highly desirable for architectural design of complex systems , where many developers are involved .",
    "kubernetes @xcite , an open source platform to orchestrate and manage docker @xcite containers , will be used to coordinate the micro - services in the paas .",
    "kubernetes is extremely useful for the monitoring and scaling of the services , and will ensure the reliability of all of them . in figure [ fig:4 ]",
    "we show the high - level view of the paas in which the interrelations among services are also indicated with arrows .        the following list briefly describes the key components of the indigo paas :    * the orchestrator : this is the core component of the paas layer .",
    "it receives high - level deployment requests from the user interface software layer , and coordinates the deployment process over the iaas platforms ; * the identity and access management ( iam ) service : it provides a layer where identities , enrolment , group membership , attributes and policies to access distributed resources and services can be managed in a homogeneous and interoperable way ; * the monitoring service : this component is in charge of collecting monitoring data from the targeted clouds , analysing and transforming them into information to be consumed by the orchestrator ; * the brokering / policy service : this is a rule - based engine that allows to manage the ranking among the resources that are available to fulfil the requested services .",
    "the orchestrator will provide the list of iaas instances and their properties to the rule engine .",
    "the rule engine will then be able to use these properties in order to choose the best site that could support the users requirements .",
    "the rule engine can be configured with different rules in order to customize the ranking ; * the qos / sla management service : it allows the handshake between a user and a site on a given sla ; moreover , it describes the qos that a specific user / group has , both over a given site or generally in the paas as a whole .",
    "this includes a priority for a given user , i.e. the capability to access different levels of qos at each site ( e.g. , gold , silver , bronze services ) ; * the qos / sla management service : it allows the handshake between a user and a site on a given sla ; moreover , it describes the qos that a specific user / group has , both over a given site or generally in the paas as a whole .",
    "this includes information about the actual service quality of storage spaces and stored files at endpoints plus the possibility to change these service qualities for stored data . * the managed service / application ( msa ) deployment service",
    ": it is in charge of scheduling , spawning , executing and monitoring applications and services on a distributed infrastructure ; it is implemented as a workflow programmatically created and executed by the orchestrator , as detailed in the next section . *",
    "the infrastructure manager ( i m ) @xcite : it deploys complex and customized virtual infrastructures on iaas cloud deployment providing an abstraction layer to define and provision resources in different clouds and virtualization platforms ; * the data management services : this is a collection of services that provide an abstraction layer for accessing data storage in a unified and federated way .",
    "these services will also provide the capabilities of importing data , schedule transfers of data , provide a unified view on qos and distributed data life cycle management .",
    "figure [ fig:4 ] shows also the interaction between the msa core service and the external components apache mesos , marathon and chronos @xcite .",
    "these are open - source components that have been selected after a deep analysis of the cutting edge technologies for application and container management .",
    "mesos is a smart resource manager originally conceived as research project at uc berkeley and currently used in production by the industrial sector as well .",
    "mesos abstracts cpu , memory , storage and other compute resources away from machines ( physical or virtual ) and allows sharing them across different distributed applications ( called frameworks ) .",
    "sophisticated two - level scheduling and efficient resource isolation are the key - features of this middleware that are exploited in the indigo paas , in order to run different workloads ( long - running services , batch jobs , etc ) on the same resources while preserving isolation and prioritizing their execution .",
    "the mesos cluster architecture @xcite is organized in two sets of nodes : masters , which coordinate the work , and slaves , which execute it .",
    "the master nodes are responsible for handling the resources available on the slaves and offer them to the frameworks according to specific policies ; then the frameworks are responsible for the application specific scheduling policy .",
    "this allows for more fine - tuned scheduling and dynamic partitioning of resources and for application aware scheduling .",
    "the msa service is implemented as a complex workflow managed by the orchestrator that delegates to two already available mesos frameworks for deploying containers on the iaas sites : marathon , which allows to deploy and manage long - running services , and chronos , which allows to execute jobs .",
    "the capabilities of mesos and its frameworks will be enhanced by adding crucial features like : the elasticity of the mesos cluster that will automatically shrink or expand depending on the tasks queue , as detailed in the next section , the automatic scaling of the user services that run on top of the mesos cluster , a stronger authentication mechanism based on openid connect .",
    "generally speaking , a platform as a service ( paas ) is a software suite , which is able to receive programmatic resource requests from end users , and execute these requests provisioning the resources on some e - infrastructures .    in the indigo approach",
    ", the paas will deal with the instantiation of services and with application execution upon user requests relying on the concept of micro - services . in turn",
    ", the micro - services will be managed using kubernetes , in order , for example , to select the right end - point for the deployment of applications or services .",
    "cross - site deployments will also be possible .",
    "the language in which the paas is going to receive end user requests is tosca @xcite ( topology and orchestration specification for cloud applications ) .",
    "it is an oasis specification for the interoperable description of application and infrastructure cloud services , the relationships between parts of these services , and their operational behaviour .",
    "in particular we will be using the tosca simple profile in yaml version 1.0 .",
    "tosca has been selected as the language for describing applications , due to the wide - ranging adoption of this standard , and since it can be used as the orchestration language for both opennebula ( through the i m @xcite ) and openstack ( through heat @xcite ) .",
    "the paas core provides an entry point to its functionality via the orchestrator service , which features a restful api that receives a tosca - compliant description of the application architecture to be deployed .",
    "providing such tosca - compliance enhances interoperability with existing and prospective software .",
    "users can choose between accessing the paas core directly or using a graphical user interface or simple apis .",
    "a user authenticated on the indigo platform will be able to access and customize a rich set of tosca - compliant templates through a gui - based portlet .",
    "the indigo repository will provide a catalogue of pre - configured tosca templates to be used for the deployment of a wide range of applications and services , customizable with different requirements of scalability , reliability and performance .",
    "in these templates a user can choose between two different examples of generic scenarios :    * scenario a. deploy a customized virtual infrastructure starting from a tosca template that has been imported , or built from scratch ( see figure [ fig:5 ] )",
    ". the user will be able to access the deployed customized virtual infrastructure and run / administer / manage applications running on it .",
    "* scenario b. deploy a service / application whose life - cycle will be directly managed by the paas platform ( see figure [ fig:6 ] ) .",
    "the user will be returned the list of endpoints to access the deployed services .    in both cases",
    "the selected template can be submitted to the paas orchestrator using its rest api endpoint .",
    "then , the orchestrator collects all the information needed to generate the deployment workflow :    * health status and capabilities of the underlying iaas platforms and their resource availability from the monitoring service ; * priority list of sites sorted by the brokering / policy service on the basis of rules defined per user / group / use - case ; * qos / sla constraints from the sla management system ; * the status of the data files and storage resources needed by the service / application and managed by the data management service .",
    "this information is used to perform the matchmaking process and to decide where to deploy each service .",
    "note that the orchestrator is able to trigger the data migration function provided by the data management service component if the data location does not meet the application deployment requirements .",
    "as pointed out before , the msa is implemented as a deployment plan managed by the workflow engine provided by the orchestrator , that , starting from the tosca template that describes the deployment request , creates the workflow programmatically .",
    "the msa workflow relies on the capabilities of mesos to manage the distributed set of iaas resources : its execution is accomplished through a set of calls to the apis endpoints of the mesos cluster , whose architecture consists of one or more master nodes , and of slave nodes that register with the master and offer resources from the iaas nodes .",
    "the master node is aware of the state of the whole iaas resources , and can share and assign them to the different applications ( called frameworks in the mesos terminology ) according to specific scheduling policies .",
    "mesos provides a default scheduling algorithm , the dominant resource fairness ( drf)@xcite , but it is possible to develop custom algorithms and easily configure mesos to use them thanks to its modular ( plugin ) architecture .    the automatic scaling service , based on ec3/clues @xcite , ensures the elasticity and scalability of the mesos cluster by monitoring its status . when additional computing resources ( worker nodes ) are needed , the orchestrator will be requested to deploy them on the underlying iaas matching the qos / sla , health and user / group / use - case policies selected by the broker .",
    "in the case of long - running services , the management service / application ( msa ) deployment service will use marathon @xcite ( a container orchestration platform available in the mesos framework ) to ensure that the services are always up and running .",
    "marathon is able to restart the services , migrate them if problems occur , handle their mutual dependencies and load - balancing , etc .",
    "the msa deployment service will also use chronos @xcite ( a fault tolerant scheduler available in the mesos framework ) to execute applications having input / output requirements or dependencies .",
    "it may also handle the rescheduling of failed applications , or simple workflows composed by different applications .    by leveraging the mesos plugin - based architecture ,",
    "new frameworks can be developed , such as the one able to deploy a batch cluster ( e.g. htcondor ) on demand , in order to meet specific use - cases .",
    "for example , batch execution of lhc data analysis is often using htcondor to manage the job scheduling @xcite    with respect to data management services ,",
    "some interfaces are provided to advanced users for specific data management tasks .",
    "first of all , the onedata @xcite component provides several features : a web - based interface for managing user spaces ( virtual folders ) and controlling access rights to files on a fine - grained level , a posix interface to a unified file system namespace addressing both local and remote data , caching capabilities and the possibility to map object stores such as s3 @xcite to posix filesystems .",
    "additionally , the fts-3 @xcite service will provide a web - based interface for monitoring and scheduling large data transfers .",
    "furthermore , all the standard interfaces exposed by the data management components will be accessible to users applications as well through standard protocols such as cdmi @xcite and webdav @xcite .",
    "the impact of the implementation of indigo software developments at the level of infrastructure resource providers is a key discussion to guarantee the adoption of the solutions being developed .",
    "a successful architecture should be able to provide the means to unify the interfaces between the paas layer and the core services .",
    "this is necessary , as resources sites have already their own administration software installed .",
    "a paas , like any software layer dealing with resource management , needs to be totally customizable to guarantee a good level of adoption by infrastructure providers .",
    "therefore our strategy is to focus on the most popular standards and provide well - documented common interfaces to these .",
    "examples are occi @xcite , tosca or a consistent container support for openstack and opennebula .",
    "an example in the data area is the support of cdmi for the various storage systems like dcache , storm , hpss and gpfs .    a closely related goal , often being the result of the topic discussed previously , is the functional unification between different software systems .",
    "support is being introduced in the i m to be able to deploy application architectures described in tosca on the different cloud back - ends supported by the i m , including opennebula , openstack and public cloud providers . in particular , for openstack , the heat - translator project will be employed to deploy tosca - compliant architectural descriptions on openstack sites    another area of development which is currently demanded by scientific communities is the introduction of `` quality of service '' and `` data life - cycle policies '' in the data area .",
    "this is the result of the various `` data management plans , dmp '' provided by data intensive communities and also required by the european commision ( ec ) when submitting proposals .",
    "one important aspect of dmps is the handling of quality of service and access control of precious and irreproducible data over time , resulting in support and manageability of those attributes at the site or storage system level .    although the different types of resources are closely interlinked , we distinguish between computing , storage and network resources for organizational reasons .    in the computing area",
    "the provision of standard apis is covered by supporting occi at the lowest resource management level and tosca at the infrastructure orchestration level .    within the storage area ,",
    "common access and control mechanisms are evaluated for negotiating data quality properties , e.g. access latency and retention policies , as well as the orchestration of data life cycles for archival .",
    "together with established standardization bodies , like rda@xcite and ogf@xcite , we envision to extend the snia cdmi protocol@xcite for our purposes .    similarly for networking , we need to evaluate commonalities in the use of software defined networks ( sdn ) between different vendors of network appliances .",
    "one notably attractive concept is that all features developed at the iaas level will not only be available through the indigo paas layer , but can be utilized by users accessing the iaas layer directly .",
    "similarly , tracking of user identities is available throughout the entire execution stack .",
    "consequently , users can be monitored down to the iaas layer with the original identities they provided to portals or workflow engines when logged via the paas indigo layer .",
    "based on the scientific use cases we have considered ( see @xcite ) , we identified a set of features that have the potential to impact in a positive way the usability and easy access to the infrastructure layers .    in the computing area , these features are enhanced support for containers , integration of batch systems , including access to hardware specific features like infiniband and general purpose gpus , support for trusted container repositories , introduction of spot instances and fair - share scheduling for selected cloud management frameworks ( cmf ) , as well as orchestration capabilities common to indigo selected cmfs using tosca",
    ". see figure [ fig:8 ] for a graphical representation",
    ".    in certain applications , the use of ` containers ' as a lightweight alternative to hypervisor - based virtualization is becoming extremely popular , due to their significantly lower overhead .",
    "however , support in major cloud management frameworks ( cmfs ) is still under development or does not exist at all .    for openstack and opennebula , the top two cmf s on the market , indigo , in collaboration with the corresponding open source communities , is spending significant efforts to make containers first - class citizens and , concerning apis and management , indistinguishable from traditional vms .",
    "while in openstack integration of nova - docker will introduce support for docker containers , for opennebula , additional developments are required .",
    "in particular , we have developed onedock @xcite , which introduces docker as an additional hypervisor for opennebula , maintaining full integration with the opennebula apis and web - based portal ( sunstone )    although cloud - like access to resources is becoming popular and cloud middleware is being widely deployed , traditional scientific data centers still provide their computational power by means of batch systems for htc and hpc . consequently , it is interesting to facilitate the integration of containers in batch systems , providing users with the ability to execute large workloads embedded inside a container .    with the pressure of optimizing computer center resources but at the same time providing fair , traceable and legally reproducible services to customers ,",
    "available cloud schedulers need to be improved .",
    "therefore , we are focusing on the support of spot - instances allowing brokering resources based on slas and prices .",
    "technically this feature requires the cmf to be able to preempt active instances based on priorities .    on the other hand , to guarantee an agreed usage of compute cycles integrated over a time interval , we need to invest in the evaluation and development of fair - share schedulers integrated in cmfs .",
    "this requires a precise recording of already used cycles and the corresponding readjustment of permitted current and future usage per individual or group .",
    "the combination of both features allows resource providers to partition their resources in a dynamic way , ensuring an optimized utilization of their infrastructures .",
    "the middleware also provides local site orchestration features by adopting the tosca standard in both openstack and opennebula , with similar and comparable functionalities .    finally , resource orchestration is also covered within this architecture .",
    "although this area can be managed at a higher level , we will provide compute , network and storage resource orchestration by means of the tosca language standard at the iaas level as well . as a result , both , the upper platform layer and the infrastructure user may deploy and manage complex configurations of resources more easily .        while in the cloud computing area , the specification of service qualities , e.g. number and power of cpus , the amount of ram and the performance of network interfaces , is already common sense , negotiating fine grained quality of service in the storage area , in a uniquely defined way , is not offered yet .    therefore , the high level objective of the storage area is to establish a standardized interface for the management of quality of services ( qos ) and data life cycle in storage ( dlc ) .",
    "users of e - infrastructures will be enabled to query and control properties of storage areas , like access latency , retention policy and migration policies with one standardized interface .",
    "a graphical representation of the components is shown in figure [ fig:9 ] .    to engage scientific communities into this endeavour as early as possible ,",
    "indigo initiated a working group within the framework of the data research alliance @xcite ( rda ) , and will incorporate ideas and suggestions of that group at any stage of the project into the development of the system .",
    "as with all infrastructure services , the interface is supposed to be used by either the paas storage federation layer or by user applications utilizing the infrastructure directly .    this will be pursued in a component - wise approach .",
    "development will focus on qos and interfaces for existing storage components and transfer protocols that are available at the computer centers . ideally , the storage qos component can be integrated just like another additional component into existing infrastructures .    besides providing the translation layer between the management interface and the underlying storage technologies",
    ", the software stack needs to be integrated into an existing local infrastructure .",
    "the high level objective of the network area is to provide mechanisms for orchestrating local and federated network topologies . to unify the orchestration management ,",
    "we will ensure that the network part of the occi open standard can be used in indigo supported cmfs : openstack and opennebula .",
    "the indigo architecture needs to address the challenge of guaranteeing a simple and effective final usage , both for software developers and application running .",
    "a key component with a big impact on the end - user experience is the authentication and authorization meachanism employed to access the e - infrastructures .    on the next layer ,",
    "the possibility of using user friendly end - points in the form of graphical interfaces , that user communities may tailor to their needs is a also big plus to enhance the end - user experience .",
    "we have designed a service providing user identity so that consistent authorization decisions can be enforced across distributed services .",
    "we can see an schema of the problem we intend to tackle in figure [ fig:15 ] .",
    "today users have different digital identities ( e.g. , institutional credentials , social logins , certificates ) and want the ability to leverage these identities for their scientific computing needs .",
    "so one problem that must be solved is how to map different identities to the same individual so that consistent authorization and accounting can be performed at various levels of the infrastructure .",
    "this identity harmonisation problem , which we describe in more detail in our aai architecture document @xcite , has many aspects that need to be tackled :    * ability to authenticate users coming with different credentials * ability to recognize which credentials are linked to which individuals , and provide a unique identifier linked to the individual ( orthogonal to the different credentials used ) * ability to link attributes to the identity that can be used to define and enforce authorisation policies * ability to provision identity information and authorisation policies to relying services    to address these points we have developed a service called _ identity access management _ ( iam ) , which provides a central solution for identity harmonisation , user authentication and authorisation .",
    "in particular , it provides a layer where identities , enrollment , group membership and other attributes management as well as authorization policies on distributed resources can be managed in a homogeneous way leveraging the supported federated authentication mechanisms ( see figure [ fig:16 ] ) .",
    "the iam service supports standard authentication mechanisms as saml , openid - connect ( oidc ) and x.509 .",
    "the user identity information collected in this way is then exposed to relying services through openid - connect interfaces . in a way",
    ", the iam acts as a credential translator for relying services , harmonizing the different identity of the users and exposing them using a single standardized protocol .",
    "this approach simplifies integration at services as it does nt force each service to understand and support each authentication mechanism used by users .",
    "openid - connect was chosen as the identity layer in indigo for the following reasons :    * easier integration in services .",
    "most services today are exposed via restful apis and openid - connect fits naturally to that use case and many products that are part of the indigo stack already support an oidc integration ( e.g. , openstack , kubernetes ) ; * support for a dynamic computing environment : oidc naturally supports dynamic client registration so that trust can be enstabilished across services without human intervention ( but according to well - defined policies ) ; * support for offline access : being based on oauth2 , oidc naturally supports offline access in a way which is independent of the authentication mechanism used ;    the iam service will represent an identity hub for indigo services , and provides standard interfaces ( scim @xcite ) for the provisioning / deprovisioning of user and group information at relying services .",
    "the iam integrates with the indigo token translation service ( tts ) to support non - http services ( ssh , amazon s3 ) and provide other needed credential translation functionality ( ie , x.509 certificate generation ) .",
    "the iam deployment model is flexible : it can be deployed centrally to accomodate a federated infrastructure and large user communities , or locally at a site to provide local identity harmonisation / attribute management services .",
    "we have developed the tools needed for the development of apis to access the indigo paas framework .",
    "it is via such apis that the paas features can be exploited via portals , desktop applications or mobile apps .",
    "therefore , the main goals of such endeavour from a high level perspective are to :    * provide user friendly front - ends demonstrating the usability of the paas services ; * manage the execution of complex workflows using paas services ; * develop toolkits ( libraries ) that will allow the exploitation of paas services at the level of scientific gateways , desktop and mobile applications ; * develop an open source mobile application toolkit that will be the base for development of mobile apps ( applied , for example , to a use case provided by the climate change community ) .",
    "the architectural elements of the user interface ( see figure [ fig:7 ] ) can be described as follows :    * futuregateway portal : it provides the main web front - end , enabling most of the operations on the e - infrastructure .",
    "a general - purpose instance of the portal will be available to all users .",
    "+ advanced features that can be included via portlets are : * * big data portlets for analytics workflows - aiming at supporting a key set of functionalities regarding big data and metadata management , as well as support for both interactive and batch analytics workflows .",
    "* * admin portlet - to provide the web portal administrator with a convenient set of tools to manage access of users to resources . * * workflow portlets - to show the status of the workflows being run on the infrastructure , and provide the basic operations . *",
    "futuregateway engine : it is a service intermediating the communication between e - infrastructures and the other user services developed .",
    "it incorporates many of the functionalities provided by the catania science gateway framework @xcite , extended by others specific to indigo .",
    "it exposes a simple restful api for developers building portals , mobile and desktop applications .",
    "full details can be found in @xcite . *",
    "scientific workflows systems : these are the scientific workflow management systems orchestrating data and job flow .",
    "we have selected ophidia , galaxy , loni and kepler as the ones more demanded by the user communities . * wfms plug - ins ",
    "these are plug - ins for the scientific workflow systems that will make use of the futuregateway engine rest api , and will provide the most common set of the functionalities .",
    "these plug - ins will be called differently depending on the scientific workflow system ( modules , plug - ins , actors , components ) .",
    "* open mobile toolkit : these are libraries that make use of the futuregateway engine rest api , providing the most common set of the functionalities that can be used by multiple domain - specific mobile applications running on different platforms . foreseen libraries include support for ios and android and , if required , for windowsphone implementations . * indigo token translation service client - the token translation service client enables clients that do not support the indigo - token to use the indigo aai architecture .",
    "the main goal in providing unified data access at the paas level is providing users with seamless access to data .",
    "the challenge resides in hiding the complexities and heterogeneities between the infrastructures where the data is actually being stored .",
    "this is particularly more important in the case of federated data infrastructures such as the deployment of lhc data over the wlcg @xcite .    as a matter of fact",
    "data access interoperability is currently the main challenge when it comes to federated data management . various grid and cloud infrastructures use different data management and access technologies , either open source or proprietary . although some solutions exist , such as cdmi",
    ", none is widely accepted and thus we need to define a unified api for users to access data from heterogeneous infrastructures in a coherent manner .    in the case of lhc data analysis in run-2 we expect a much more heterogenous infrastructure in place , besides the wlcg grid , resources based on cloud - like provision is also being explored .",
    "therefore it becomes critical to provide a certain level of integration .",
    "the indigo paas provides three data management services that allow accessing federated data in an unified way .",
    "depending on how data are stored / accessible , they will be made available through a different services in a way which is transparent to the user ( see figure [ fig:12 ] ) . in order to access and manage data",
    ", we will exploit the interfaces provided by the infrastructure layer :    * posix and webdav for data access .",
    "* gridftp for data transfer . * cdmi for the metadata management . *",
    "rest apis to expose qos features of the underline storage .",
    "onedata @xcite is a server - client type of service whose main purpose is to provide a unified federated and optimized data access based on various apis as well as legacy posix protocol .",
    "it allows transparent access to storage resources from multiple data centers simultaneously .",
    "onedata automatically detects whether data is available on local storage and can be accessed directly , or whether it has to be fetched from remote sites .",
    "support for federation is achieved by the possibility of establishing a distributed provider registry , where various infrastructures can setup their own provider registry and build trust relationship between these instances , allowing users from various platforms to share their data transparently    the main dependence of onedata is on the storage providers . for onedata to work the storage providers needs to expose cdmi or s3 interfaces , and supporting posix access to site storage .",
    "the architecture comprises two major components : oneprovider and oneclient .",
    "the former is installed in datacenters to provide a unified view of the site filesystems .",
    "the client side connects to the providers which the user registered in onedata portal , and his spaces are automatically provisioned from these providers .",
    "if data are stored on a posix - compliant filesystem and the site admin is willing to install the onedata gateway , data can be accessed with a more powerful graphical interface , including :    * acls and qos management . * posix access ( also remotely ) .",
    "* web or webdav access . * simple metadata management ( based on the concept of key - value pair ) .",
    "* data movement , replication and caching across the site of the federation . * transparent local caching of data .",
    "* transparent translation of object storage resources ( for example available through an s3 @xcite interface ) to a posix filesystem .",
    "the fts service will be exploited for the capability of managing third - party transfers among griftp servers .",
    "it will be used in order to import data from external gridftp servers , globus - online compatible services , etc .",
    "support to fts is needed in order to provide access to most wlcg storage elements .",
    "we describe here the main functionalities for completeness .    for authentication fts",
    "uses voms proxies .",
    "it optimizes transfers by monitoring the performance currently active transfers and adjusting the number of concurrent transfers on the fly .",
    "the users typically only provide file origin and destination storage elements . after submitting origin and destination to the client",
    "the user can monitor the progress , query the status , cancel and delete transfer requests and resubmit previously unsuccessful requests . a successful transfer from source to destination using the specified protocol ( srm / gsiftp / http / root ) can be verified by a checksum that was added during request creation .",
    "dynafed @xcite is a federated namespace of distributed storage namespaces .",
    "it provides a very fast loose coupling of storage endpoints as a single name - space exposed via http and webdav .",
    "this allows to have federation of existing storage endpoints without the need of maintaining a file catalog for global to local file name translation .",
    "the purpose of providing this service in the paas is to cover the case in which data are available only via a webdav gateway , they can be aggregated using dynafed .",
    "users will be able to read them via a federation layer regardless of where the data are really stored .    to wrap things up , in the indigo paas framework",
    ", the user will provide information about the data needed to execute the desired service / application , and how those data are to be accessed , at the level of the tosca template .    given the data requirements described in the template",
    ", the orchestrator will be able to understand if it has to request either fts or onedata to schedule a data import / movement , or if instead it is better to move the application `` close '' to the data .    at the end of this cycle",
    "the data will be available to the end user service exploiting onedata or dynafed .",
    "this will allow also legacy application / services to be supported with their native data access approach ( webdav or posix ) .    in summary",
    "the end user has the possibility to handle data in several ways :    * asking for an import action using a tosca template exploiting fts and gridftp . * uploading files or directories using a web interface . *",
    "importing data from his desktop via a dropbox - like tool .",
    "the indigo architecture , based on a three level approach ( user interface , paas layer and infrastructure layer ) , is able to fulfil the requirements described in the introduction .    at the infrastructure site level",
    "the architecture provides new scheduling algorithms for open source cloud frameworks .",
    "also very importantly , it provides dynamic partitioning of batch versus cloud resources at the site level . by implementing the cloud bursting tools available in the architecture , the site can also have access to external infrastructures .",
    "the infrastructure site is also enhanced with full support to containers , where the local containers repositories can as well be securely synchronized with dockerhub external repositories , facilitating enormously the automatic instantiation of applications .    from the point of view of data ,",
    "the architecture is able to integrate local and remote posix access for all types of resources : bare metal , virtual machines or containers .",
    "in particular it provides transparent mapping of object storage to posix , and a transparent gateway to existing filesystem ( like gpfs or lustre).data access is also enhanced with the support to webdav , gridftp and cdmi access .    using pre - defined available tosca templates we are also able to provide a high level of automatism for a number of scenarios .",
    "furthermore , the paas scenario has the advantage that the management of the application / services has advanced capabilities related with service resilience that make it very attractive for users .",
    "for example , in the case of failure of one the services or of an application , the platform itself will take care of restarting the service or re - executing the application .    throughout this paper we have summarized the work performed at the infrastructure , platform and user interface level , which also includes the description and implementation of some typical use case scenarios , to provide more clarity as to what we want to support with this architectural construction .",
    "the plan is to deliver a first release of the platform by july 2016 , implementing the most important features to let users deploy their services and applications across a number of testbed provided by the indigo partners , and provide developers with an initial feedback .    in the second indigo release , scheduled by march 2017",
    ", we plan to also integrate advanced support for features such as moving applications to cloud infrastructures , addressing cloud bursting , enhancing data services and providing additional sample templates to support use cases as they are presented by scientific communities .",
    "the release cycle , roadmap and key procedures of the indigo software are described here @xcite .",
    "`` dominant resource fairness : fair allocation of multiple resource types '' + a. ghodsi , m. zaharia , b. hindman , a. konwinski , s. shenker and i. stoica + proceedings of the 8th usenix conference on networked systems design and implementation ( nsdi 2011 ) , pp .",
    "323 - 336 usenix association , berkeley ( 2011 )              `` the dynamic federations : federate storage on the fly using http / webdav and dmlite '' + f. furano , r. rocha , o. keeble , a. alvarez and p. fuhrmann + isgc 2013 international symposium on grids and clouds ( isgc ) , 17 - 22 march 2013 .",
    "+ see pos at https://furano.web.cern.ch/furano/files/dynafedswiki/paper_dynafedsisgc.pdf        `` dynamic management of virtual infrastructures . ''",
    "+ miguel caballer , ignacio blanquer , germn molt , and carlos de alfonso journal of grid computing 13(1 ) : 5370 ( 2015 ) + http://link.springer.com/article/10.1007/s10723-014-9296-5 ."
  ],
  "abstract_text": [
    "<S> in this paper we describe the architecture of a platform as a service ( paas ) oriented to computing and data analysis . in order to clarify the choices we made , </S>",
    "<S> we explain the features using practical examples , applied to several known usage patterns in the area of hep computing . </S>",
    "<S> the proposed architecture is devised to provide researchers with a unified view of distributed computing infrastructures , focusing in facilitating seamless access . in this respect </S>",
    "<S> the platform is able to profit from the most recent developments for computing and processing large amounts of data , and to exploit current storage and preservation technologies , with the appropriate mechanisms to ensure security and privacy . </S>"
  ]
}