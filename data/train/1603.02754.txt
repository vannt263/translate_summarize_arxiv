{
  "article_text": [
    "-.01 in    machine learning and data - driven approaches are becoming very important in many areas .",
    "smart spam classifiers protect our email by learning from massive amounts of spam data and user feedback ; advertising systems learn to match the right ads with the right context ; fraud detection systems protect banks from malicious attackers ; anomaly event detection systems help experimental physicists to find events that lead to new physics .",
    "there are two important factors that drive these successful applications : usage of effective ( statistical )  models that capture the complex data dependencies and scalable learning systems that learn the model of interest from large datasets .    among the machine learning methods used in practice ,",
    "gradient tree boosting  @xcite is one technique that shines in many applications .",
    "tree boosting has been shown to give state - of - the - art results on many standard classification benchmarks  @xcite .",
    "lambdamart  @xcite , a variant of tree boosting for ranking , achieves state - of - the - art result for ranking problems . besides being used as a stand - alone predictor",
    ", it is also incorporated into real - world production pipelines for ad click through rate prediction  @xcite .",
    "finally , it is the de - facto choice of ensemble method and is used in challenges such as the netflix prize  @xcite .    in this paper , we describe xgboost , a scalable machine learning system for tree boosting .",
    "the system is available as an open source package .",
    "the impact of the system has been widely recognized in a number of machine learning and data mining challenges .",
    "take the challenges hosted by the machine learning competition site kaggle for example . among the 29 challenge winning solutions   published at kaggle s blog during 2015 , 17 solutions used xgboost . among these solutions , eight solely used xgboost to train the model , while most others combined xgboost with neural nets in ensembles . for comparison ,",
    "the second most popular method , deep neural nets , was used in 11 solutions .",
    "the success of the system was also witnessed in kddcup 2015 , where xgboost was used by every winning team in the top-10 . moreover , the winning teams reported that ensemble methods outperform a well - configured xgboost by only a small amount  @xcite .",
    "these results demonstrate that our system gives state - of - the - art results on a wide range of problems .",
    "examples of the problems in these winning solutions include : store sales prediction ; high energy physics event classification ; web text classification ; customer behavior prediction ; motion detection ; ad click through rate prediction ; malware classification ; product categorization ; hazard risk prediction ; massive online course dropout rate prediction .",
    "while domain dependent data analysis and feature engineering play an important role in these solutions , the fact that xgboost is the consensus choice of learner shows the impact and importance of our system and tree boosting .",
    "the most important factor behind the success of xgboost is its scalability in all scenarios .",
    "the system runs more than ten times faster than existing popular solutions on a single machine and scales to billions of examples in distributed or memory - limited settings .",
    "the scalability of xgboost is due to several important systems and algorithmic optimizations .",
    "these innovations include : a novel tree learning algorithm is for handling _ sparse data _ ; a theoretically justified weighted quantile sketch procedure enables handling instance weights in approximate tree learning .",
    "parallel and distributed computing makes learning faster which enables quicker model exploration .",
    "more importantly , xgboost exploits out - of - core computation and enables data scientists to process hundred millions of examples on a desktop . finally , it is even more exciting to combine these techniques to make an end - to - end system that scales to even larger data with the least amount of cluster resources .",
    "the major contributions of this paper is listed as follows :    * we design and build a highly scalable end - to - end tree boosting system . *",
    "we propose a theoretically justified weighted quantile sketch for efficient proposal calculation .",
    "* we introduce a novel sparsity - aware algorithm for parallel tree learning . *",
    "we propose an effective cache - aware block structure for out - of - core tree learning .    while there are some existing works on parallel tree boosting  @xcite , the directions such as out - of - core computation , cache - aware and sparsity - aware learning have not been explored .",
    "more importantly , an end - to - end system that combines all of these aspects gives a novel solution for real - world use - cases .",
    "this enables data scientists as well as researchers to build powerful variants of tree boosting algorithms  @xcite .",
    "besides these major contributions , we also make additional improvements in proposing a regularized learning objective , which we will include for completeness .",
    "the remainder of the paper is organized as follows .",
    "we will first review tree boosting and introduce a regularized objective in sec .",
    "[ sec : model ] .",
    "we then describe the split finding methods in sec .",
    "[ sec : exact ] as well as the system design in sec .",
    "[ sec : system ] , including experimental results when relevant to provide quantitative support for each optimization we describe .",
    "related work is discussed in sec .",
    "[ sec : rel ] .",
    "detailed end - to - end evaluations are included in sec .",
    "[ sec : exp ] .",
    "finally we conclude the paper in sec .",
    "[ sec : con ] .",
    "-.01 in    we review gradient tree boosting algorithms in this section .",
    "the derivation follows from the same idea in existing literatures in gradient boosting .",
    "specicially the second order method is originated from friedman et al .",
    "we make minor improvements in the reguralized objective , which were found helpful in practice .      for a given data set with @xmath0 examples and @xmath1 features @xmath2 ( @xmath3 ) , a tree ensemble model  ( shown in fig .",
    "[ fig : treemodel ] ) uses @xmath4 additive functions to predict the output . @xmath5 where @xmath6 is the space of regression trees ( also known as cart ) . here",
    "@xmath7 represents the structure of each tree that maps an example to the corresponding leaf index .",
    "@xmath8 is the number of leaves in the tree .",
    "each @xmath9 corresponds to an independent tree structure @xmath7 and leaf weights @xmath10 . unlike decision trees",
    ", each regression tree contains a continuous score on each of the leaf , we use @xmath11 to represent score on @xmath12-th leaf . for a given example",
    ", we will use the decision rules in the trees  ( given by @xmath7 ) to classify it into the leaves and calculate the final prediction by summing up the score in the corresponding leaves  ( given by @xmath10 ) . to learn the set of functions used in the model , we minimize the following _ regularized _ objective .",
    "@xmath13 here @xmath14 is a differentiable convex loss function that measures the difference between the prediction @xmath15 and the target @xmath16 .",
    "the second term @xmath17 penalizes the complexity of the model ( i.e. , the regression tree functions ) .",
    "the additional regularization term helps to smooth the final learnt weights to avoid over - fitting .",
    "intuitively , the regularized objective will tend to select a model employing simple and predictive functions .",
    "a similar regularization technique has been used in regularized greedy forest  ( rgf )  @xcite model .",
    "our objective and the corresponding learning algorithm is simpler than rgf and easier to parallelize .",
    "when the regularization parameter is set to zero , the objective falls back to the traditional gradient tree boosting .",
    "-.15 in    -.12 in      the tree ensemble model in eq .   includes functions as parameters and can not be optimized using traditional optimization methods in euclidean space .",
    "instead , the model is trained in an additive manner .",
    "formally , let @xmath18 be the prediction of the @xmath12-th instance at the @xmath19-th iteration , we will need to add @xmath20 to minimize the following objective .",
    "@xmath21 this means we greedily add the @xmath20 that most improves our model according to eq .  .",
    "second - order approximation can be used to quickly optimize the objective in the general setting  @xcite .",
    "@xmath22 + \\omega(f_t)\\ ] ] where @xmath23 and @xmath24 are first and second order gradient statistics on the loss function .",
    "we can remove the constant terms to obtain the following simplified objective at step @xmath19 .",
    "@xmath25 + \\omega(f_t)\\ ] ]    define @xmath26 as the instance set of leaf @xmath27 .",
    "we can rewrite eq   by expanding @xmath17 as follows @xmath28+\\gamma t + \\frac{1}{2}\\lambda\\sum^t_{j=1}w_j^2\\\\           & = \\sum^t_{j=1}[(\\sum_{i\\in i_j } g_i)w_j+\\frac{1}{2}(\\sum_{i\\in i_j } h_i+\\lambda)w_j^2]+\\gamma t \\end{split}\\ ] ]    for a fixed structure @xmath29 , we can compute the optimal weight @xmath30 of leaf @xmath27 by @xmath31 and calculate the corresponding optimal value by @xmath32     -.15 in    -.12 in    eq   can be used as a scoring function to measure the quality of a tree structure @xmath7 .",
    "this score is like the impurity score for evaluating decision trees , except that it is derived for a wider range of objective functions .",
    "[ fig : structscore ] illustrates how this score can be calculated .",
    "normally it is impossible to enumerate all the possible tree structures @xmath7 .",
    "a greedy algorithm that starts from a single leaf and iteratively adds branches to the tree is used instead .",
    "assume that @xmath33 and @xmath34 are the instance sets of left and right nodes after the split .",
    "lettting @xmath35 , then the loss reduction after the split is given by @xmath36 - \\gamma\\ ] ] this formula is usually used in practice for evaluating the split candidates .",
    "besides the regularized objective mentioned in sec .",
    "[ subsec : obj ] , two additional techniques are used to further prevent over - fitting . the first technique is shrinkage introduced by friedman  @xcite",
    ". shrinkage scales newly added weights by a factor @xmath37 after each step of tree boosting .",
    "similar to a learning rate in tochastic optimization , shrinkage reduces the influence of each individual tree and leaves space for future trees to improve the model .",
    "the second technique is column ( feature ) subsampling .",
    "this technique is used in randomforest  @xcite , it is implemented in a commercial software treenet   for gradient boosting , but is not implemented in existing opensource packages . according to user feedback , using column sub - sampling prevents over - fitting even more so than the traditional row sub - sampling ( which is also supported ) .",
    "the usage of column sub - samples also speeds up computations of the parallel algorithm described later .",
    "-.01 in      @xmath38 + @xmath39 , @xmath40 +    one of the key problems in tree learning is to find the best split as indicated by eq  . in order to do so , a split finding algorithm enumerates over all the possible splits on all the features .",
    "we call this the _ exact greedy algorithm_. most existing single machine tree boosting implementations , such as scikit - learn  @xcite , r s gbm  @xcite as well as the single machine version of xgboost support the exact greedy algorithm .",
    "the exact greedy algorithm is shown in alg .",
    "[ alg : exact - greedy ] .",
    "it is computationally demanding to enumerate all the possible splits for continuous features . in order to do so efficiently ,",
    "the algorithm must first sort the data according to feature values and visit the data in sorted order to accumulate the gradient statistics for the structure score in eq  .",
    "follow same step as in previous section to find max score only among proposed splits .",
    "+    the exact greedy algorithm is very powerful since it enumerates over all possible splitting points greedily .",
    "however , it is impossible to efficiently do so when the data does not fit entirely into memory .",
    "same problem also arises in the distributed setting . to support effective gradient tree boosting in these two settings ,",
    "an approximate algorithm is needed .",
    "we summarize an approximate framework , which resembles the ideas proposed in past literatures  @xcite , in alg .",
    "[ alg : approx - greedy ] . to summarize , the algorithm first proposes candidate splitting points according to percentiles of feature distribution  ( a specific criteria will be given in sec .  [",
    "sec : quantile ] ) .",
    "the algorithm then maps the continuous features into buckets split by these candidate points , aggregates the statistics and finds the best solution among proposals based on the aggregated statistics .",
    "-.15 in    there are two variants of the algorithm , depending on when the proposal is given .",
    "the global variant proposes all the candidate splits during the initial phase of tree construction , and uses the same proposals for split finding at all levels .",
    "the local variant re - proposes after each split .",
    "the global method requires less proposal steps than the local method .",
    "however , usually more candidate points are needed for the global proposal because candidates are not refined after each split .",
    "the local proposal refines the candidates after splits , and can potentially be more appropriate for deeper trees .",
    "a comparison of different algorithms on a higgs boson dataset is given by fig .",
    "[ fig : higgs - approx - cmp-10 m ] .",
    "we find that the local proposal indeed requires fewer candidates .",
    "the global proposal can be as accurate as the local one given enough candidates .",
    "most existing approximate algorithms for distributed tree learning also follow this framework .",
    "notably , it is also possible to directly construct approximate histograms of gradient statistics  @xcite .",
    "it is also possible to use other variants of binning strategies instead of quantile  @xcite .",
    "quantile strategy benefit from being distributable and recomputable , which we will detail in next subsection . from fig .",
    "[ fig : higgs - approx - cmp-10 m ] , we also find that the quantile strategy can get the same accuracy as exact greedy given reasonable approximation level .",
    "our system efficiently supports exact greedy for the single machine setting , as well as approximate algorithm with both local and global proposal methods for all settings .",
    "users can freely choose between the methods according to their needs .",
    "one important step in the approximate algorithm is to propose candidate split points .",
    "usually percentiles of a feature are used to make candidates distribute evenly on the data .",
    "formally , let multi - set @xmath41 represent the @xmath42-th feature values and second order gradient statistics of each training instances .",
    "we can define a rank functions @xmath43 as @xmath44 which represents the proportion of instances whose feature value @xmath42 is smaller than @xmath45 .",
    "the goal is to find candidate split points @xmath46 , such that @xmath47 [ eq : quantile ] here @xmath48 is an approximation factor .",
    "intuitively , this means that there is roughly @xmath49 candidate points . here each data point is weighted by @xmath50 . to see why @xmath50 represents the weight , we can rewrite eq   as @xmath51 which is exactly weighted squared loss with labels @xmath52 and weights @xmath50 . for large datasets",
    ", it is non - trivial to find candidate splits that satisfy the criteria . when every instance has equal weights , an existing algorithm called quantile sketch  @xcite solves the problem .",
    "however , there is no existing quantile sketch for the weighted datasets .",
    "therefore , most existing approximate algorithms either resorted to sorting on a random subset of data which have a chance of failure or heuristics that do not have theoretical guarantee .    to solve this problem , we introduced a novel distributed weighted quantile sketch algorithm that can handle weighted data with a _ provable theoretical guarantee_. the general idea is to propose a data structure that supports _ merge _ and _ prune _ operations , with each operation proven to maintain a certain accuracy level .",
    "a detailed description of the algorithm as well as proofs are given in the appendix .",
    "_ also applies to the approximate setting , only collect statistics of non - missing entries into buckets _",
    "+ @xmath38 + @xmath53,@xmath40 +         -.15 in    -.12 in     -.15 in    -.12 in    in many real - world problems , it is quite common for the input @xmath54 to be sparse .",
    "there are multiple possible causes for sparsity : 1 ) presence of missing values in the data ; 2 ) frequent zero entries in the statistics ; and , 3 ) artifacts of feature engineering such as one - hot encoding .",
    "it is important to make the algorithm aware of the sparsity pattern in the data . in order to do so ,",
    "we propose to add a default direction in each tree node , which is shown in fig .",
    "[ fig : default ] .",
    "when a value is missing in the sparse matrix @xmath54 , the instance is classified into the default direction .",
    "there are two choices of default direction in each branch .",
    "the optimal default directions are learnt from the data .",
    "the algorithm is shown in alg .",
    "[ alg : sparse - split ] .",
    "the key improvement is to only visit the non - missing entries @xmath55 .",
    "the presented algorithm treats the non - presence as a missing value and learns the best direction to handle missing values .",
    "the same algorithm can also be applied when the non - presence corresponds to a user specified value by limiting the enumeration only to consistent solutions .    to the best of our knowledge ,",
    "most existing tree learning algorithms are either only optimized for dense data , or need specific procedures to handle limited cases such as categorical encoding .",
    "xgboost handles all sparsity patterns in a unified way .",
    "more importantly , our method exploits the sparsity to make computation complexity linear to number of non - missing entries in the input .",
    "[ fig : sparse - allstate ] shows the comparison of sparsity aware and a naive implementation on an allstate-10k dataset  ( description of dataset given in sec .",
    "[ sec : exp ] ) .",
    "we find that the sparsity aware algorithm runs 50 times faster than the naive version .",
    "this confirms the importance of the sparsity aware algorithm .",
    "-.15 in    -.12 in    -.1 in",
    "-.01 in      the most time consuming part of tree learning is to get the data into sorted order . in order to reduce the cost of sorting , we propose to store the data in in - memory units , which we called _",
    "block_. data in each block is stored in the compressed column  ( csc ) format , with each column sorted by the corresponding feature value .",
    "this input data layout only needs to be computed once before training , and can be reused in later iterations .    in the exact greedy algorithm , we store the entire dataset in a single block and run the split search algorithm by linearly scanning over the pre - sorted entries .",
    "we do the split finding of all leaves collectively , so one scan over the block will collect the statistics of the split candidates in all leaf branches .",
    "[ fig : layout ] shows how we transform a dataset into the format and find the optimal split using the block structure .",
    "the block structure also helps when using the approximate algorithms .",
    "multiple blocks can be used in this case , with each block corresponding to subset of rows in the dataset .",
    "different blocks can be distributed across machines , or stored on disk in the out - of - core setting . using the sorted structure , the quantile finding",
    "step becomes a _ linear scan _ over the sorted columns .",
    "this is especially valuable for local proposal algorithms , where candidates are generated frequently at each branch .",
    "the binary search in histogram aggregation also becomes a linear time merge style algorithm .",
    "collecting statistics for each column can be _",
    "parallelized _ , giving us a parallel algorithm for split finding .",
    "importantly , the column block structure also supports column subsampling , as it is easy to select a subset of columns in a block .",
    "* time complexity analysis * let @xmath56 be the maximum depth of the tree and @xmath4 be total number of trees .",
    "for the exact greedy algorithm , the time complexity of original spase aware algorithm is @xmath57 .",
    "here we use @xmath58 to denote number of non - missing entries in the training data .",
    "on the other hand , tree boosting on the block structure only cost @xmath59 . here",
    "@xmath60 is the one time preprocessing cost that can be amortized .",
    "this analysis shows that the block structure helps to save an additional @xmath61 factor , which is significant when @xmath0 is large . for the approximate algorithm ,",
    "the time complexity of original algorithm with binary search is @xmath62 . here",
    "@xmath7 is the number of proposal candidates in the dataset .",
    "while @xmath7 is usually between 32 and 100 , the log factor still introduces overhead . using the block structure",
    ", we can reduce the time to @xmath63 , where @xmath64 is the maximum number of rows in each block .",
    "again we can save the additional @xmath65 factor in computation .",
    "-.15 in    -.12 in    -.15 in    -.12 in    while the proposed block structure helps optimize the computation complexity of split finding , the new algorithm requires indirect fetches of gradient statistics by row index , since these values are accessed in order of feature .",
    "this is a non - continuous memory access .",
    "a naive implementation of split enumeration introduces immediate read / write dependency between the accumulation and the non - continuous memory fetch operation  ( see fig .  [",
    "fig : cache - miss ] ) .",
    "this slows down split finding when the gradient statistics do not fit into cpu cache and cache miss occur .",
    "approximate global & approximate local & out - of - core & sparsity aware & parallel + * xgboost * & yes & yes & yes & yes & yes & yes + pgbrt & no & no & yes & no & no & yes + spark mllib & no & yes & no & no & partially & yes + h2o & no & yes & no & no & partially & yes + scikit - learn & yes & no & no & no & no & no + r gbm & yes & no & no & no & partially & no +    for the exact greedy algorithm , we can alleviate the problem by a cache - aware prefetching algorithm .",
    "specifically , we allocate an internal buffer in each thread , fetch the gradient statistics into it , and then perform accumulation in a mini - batch manner .",
    "this prefetching changes the direct read / write dependency to a longer dependency and helps to reduce the runtime overhead when number of rows in the is large .",
    "figure  [ fig : exact - cache ] gives the comparison of cache - aware vs. non cache - aware algorithm on the the higgs and the allstate dataset .",
    "we find that cache - aware implementation of the exact greedy algorithm runs twice as fast as the naive version when the dataset is large .    for approximate algorithms",
    ", we solve the problem by choosing a correct block size .",
    "we define the block size to be maximum number of examples in contained in a block , as this reflects the cache storage cost of gradient statistics .",
    "choosing an overly small block size results in small workload for each thread and leads to inefficient parallelization . on the other hand",
    ", overly large blocks result in cache misses , as the gradient statistics do not fit into the cpu cache .",
    "a good choice of block size balances these two factors .",
    "we compared various choices of block size on two data sets .",
    "the results are given in fig .",
    "[ fig : blocksize ] .",
    "this result validates our discussion and shows that choosing @xmath66 examples per block balances the cache property and parallelization .",
    "one goal of our system is to fully utilize a machine s resources to achieve scalable learning . besides processors and memory ,",
    "it is important to utilize disk space to handle data that does not fit into main memory . to enable out - of - core computation",
    ", we divide the data into multiple blocks and store each block on disk . during computation ,",
    "it is important to use an independent thread to pre - fetch the block into a main memory buffer , so computation can happen in concurrence with disk reading .",
    "however , this does not entirely solve the problem since the disk reading takes most of the computation time .",
    "it is important to reduce the overhead and increase the throughput of disk io .",
    "we mainly use two techniques to improve the out - of - core computation .",
    "* block compression * the first technique we use is block compression .",
    "the block is compressed by columns , and decompressed on the fly by an independent thread when loading into main memory .",
    "this helps to trade some of the computation in decompression with the disk reading cost .",
    "we use a general purpose compression algorithm for compressing the features values . for the row index ,",
    "we substract the row index by the begining index of the block and use a 16bit integer to store each offset .",
    "this requires @xmath66 examples per block , which is confirmed to be a good setting . in most of the dataset we tested ,",
    "we achieve roughly a 26% to 29% compression ratio .",
    "* block sharding * the second technique is to shard the data onto multiple disks in an alternative manner .",
    "a pre - fetcher thread is assigned to each disk and fetches the data into an in - memory buffer .",
    "the training thread then alternatively reads the data from each buffer .",
    "this helps to increase the throughput of disk reading when multiple disks are available .",
    "-.01 in    our system implements gradient boosting  @xcite , which performs additive optimization in functional space .",
    "gradient tree boosting has been successfully used in classification  @xcite , learning to rank  @xcite , structured prediction  @xcite as well as other fields .",
    "xgboost incorporates a regularized model to prevent overfitting .",
    "this this resembles previous work on regularized greedy forest  @xcite , but simplifies the objective and algorithm for parallelization .",
    "column sampling is a simple but effective technique borrowed from randomforest  @xcite . while sparsity - aware learning is essential in other types of models such as linear models  @xcite , few works on tree learning",
    "have considered this topic in a principled way .",
    "the algorithm proposed in this paper is the first unified approach to handle all kinds of sparsity patterns .",
    "there are several existing works on parallelizing tree learning  @xcite .",
    "most of these algorithms fall into the approximate framework described in this paper .",
    "notably , it is also possible to partition data by columns  @xcite and apply the exact greedy algorithm .",
    "this is also supported in our framework , and the techniques such as cache - aware pre - fecthing can be used to benefit this type of algorithm . while most existing works focus on the algorithmic aspect of parallelization , our work improves in two unexplored system directions : out - of - core computation and cache - aware learning .",
    "this gives us insights on how the system and the algorithm can be jointly optimized and provides an end - to - end system that can handle large scale problems with very limited computing resources .",
    "we also summarize the comparison between our system and existing opensource implementations in table  [ tbl : system - cmp ] .",
    "quantile summary  ( without weights ) is a classical problem in the database community  @xcite .",
    "however , the approximate tree boosting algorithm reveals a more general problem  finding quantiles on weighted data . to the best of our knowledge ,",
    "the weighted quantile sketch proposed in this paper is the first method to solve this problem .",
    "the weighted quantile summary is also not specific to the tree learning and can benefit other applications in data science and machine learning in the future .",
    "-.01 in      we implemented xgboost as an open source package .",
    "the package is portable and reusable .",
    "it supports various weighted classification and rank objective functions , as well as user defined objective function .",
    "it is available in popular languages such as python , r , julia and integrates naturally with language native data science pipelines such as scikit - learn .",
    "the distributed version is built on top of the rabit library for allreduce .",
    "the portability of xgboost makes it available in many ecosystems , instead of only being tied to a specific platform .",
    "the distributed xgboost runs natively on hadoop , mpi sun grid engine .",
    "recently , we also enable distributed xgboost on jvm bigdata stacks such as flink and spark .",
    "the distributed version has also been integrated into cloud platform tianchi of alibaba .",
    "we believe that there will be more integrations in the future .",
    ".dataset used in the experiments . [ cols=\"<,^,^,<\",options=\"header \" , ]        -.15 in    -.12 in    -.15 in    -.12 in     -.15 in    -.12 in    we next evaluate the performance of xgboost on the learning to rank problem .",
    "we compare against pgbrt  @xcite , the best previously pubished system on this task .",
    "xgboost runs exact greedy algorithm , while pgbrt only support an approximate algorithm .",
    "the results are shown in table  [ tbl : yahoo - cmp ] and fig .",
    "[ fig : sparse - yahoo ] .",
    "we find that xgboost runs faster .",
    "interestingly , subsampling columns not only reduces running time , and but also gives a bit higher performance for this problem .",
    "this could due to the fact that the subsampling helps prevent overfitting , which is observed by many of the users .",
    "we also evaluate our system in the out - of - core setting on the criteo data .",
    "we conducted the experiment on one aws c3.8xlarge machine  ( 32 vcores , two 320 gb ssd , 60 gb ram ) .",
    "the results are shown in figure  [ fig : disk ] .",
    "we can find that compression helps to speed up computation by factor of three , and sharding into two disks further gives 2x speedup .",
    "for this type of experiment , it is important to use a very large dataset to drain the system file cache for a real out - of - core setting .",
    "this is indeed our setup .",
    "we can observe a transition point when the system runs out of file cache .",
    "note that the transition in the final method is less dramatic .",
    "this is due to larger disk throughput and better utilization of computation resources .",
    "our final method is able to process 1.7 billion examples on a single machine .",
    "finally , we evaluate the system in the distributed setting .",
    "we set up a yarn cluster on ec2 with m3.2xlarge machines , which is a very common choice for clusters .",
    "each machine contains 8 virtual cores , 30 gb of ram and two 80 gb ssd local disks .",
    "the dataset is stored on aws s3 instead of hdfs to avoid purchasing persistent storage .",
    "we first compare our system against two production - level distributed systems : spark mllib  @xcite and h2o  .",
    "we use 32 m3.2xlarge machines and test the performance of the systems with various input size .",
    "both of the baseline systems are in - memory analytics frameworks that need to store the data in ram , while xgboost can switch to out - of - core setting when it runs out of memory .",
    "the results are shown in fig .",
    "[ fig : bigdata ] . we can find that xgboost runs faster than the baseline systems .",
    "more importantly , it is able to take advantage of out - of - core computing and smoothly scale to all 1.7 billion examples with the given limited computing resources .",
    "the baseline systems are only able to handle subset of the data with the given resources .",
    "this experiment shows the advantage to bring all the system improvement together and solve a real - world scale problem .",
    "we also evaluate the scaling property of xgboost by varying the number of machines .",
    "the results are shown in fig .",
    "[ fig : scale ] .",
    "we can find xgboost s performance scales linearly as we add more machines .",
    "importantly , xgboost is able to handle the entire 1.7 billion data with only four machines .",
    "this shows the system s potential to handle even larger data .",
    "-.01inin this paper , we described the lessons we learnt when building xgboost , a scalable tree boosting system that is widely used by data scientists and provides state - of - the - art results on many problems .",
    "we proposed a novel sparsity aware algorithm for handling sparse data and a theoretically justified weighted quantile sketch for approximate learning .",
    "our experience shows that cache access patterns , data compression and sharding are essential elements for building a scalable end - to - end system for tree boosting .",
    "these lessons can be applied to other machine learning systems as well . by combining these insights , xgboost is able to solve real - world scale problems using a minimal amount of resources .",
    "in this section , we introduce the weighted quantile sketch algorithm .",
    "approximate answer of quantile queries is for many real - world applications .",
    "one classical approach to this problem is gk algorithm  @xcite and extensions based on the gk framework  @xcite .",
    "the main component of these algorithms is a data structure called quantile summary , that is able to answer quantile queries with relative accuracy of @xmath48 .",
    "two operations are defined for a quantile summary :    * a merge operation that combines two summaries with approximation error @xmath67 and @xmath68 together and create a merged summary with approximation error @xmath69 . * a prune operation that reduces the number of elements in the summary to @xmath70 and changes approximation error from @xmath48 to @xmath71 .      in order to use quantile computation for approximate tree boosting",
    ", we need to find quantiles on weighted data . this more general problem is not supported by any of the existing algorithm . in this section ,",
    "we describe a non - trivial weighted quantile summary structure to solve this problem .",
    "importantly , the new algorithm contains merge and prune operations with _ the same guarantee _ as gk summary .",
    "this allows our summary to be plugged into all the frameworks used gk summary as building block and answer quantile queries over weighted data efficiently .",
    "given an input multi - set @xmath72 such that @xmath73 .",
    "each @xmath74 corresponds to a position of the point and @xmath11 is the weight of the point .",
    "assume we have a total order @xmath75 defined on @xmath76 .",
    "let us define two rank functions @xmath77",
    "@xmath78 @xmath79 we should note that since @xmath80 is defined to be a _",
    "multiset _ of the points .",
    "it can contain multiple record with exactly same position @xmath81 and weight @xmath10 .",
    "we also define another weight function @xmath82 as @xmath83 finally , we also define the weight of multi - set @xmath80 to be the sum of weights of all the points in the set @xmath84 our task is given a series of input @xmath80 , to estimate @xmath85 and @xmath86 for @xmath87 as well as finding points with specific rank .",
    "given these notations , we define quantile summary of weighted examples as follows :    [ def : sketch]quantile summary of weighted data + a quantile summary for @xmath80 is defined to be tuple @xmath88 , where @xmath89 is selected from the points in @xmath80  ( i.e. @xmath90 ) with the following properties :    \\1 ) @xmath91 , and @xmath92 and @xmath93 are minimum and maximum point in @xmath80 : @xmath94 2 ) @xmath95 , @xmath96 and @xmath97 are functions in @xmath98 , that satisfies @xmath99 the equality sign holds for maximum and minimum point  ( @xmath100 , @xmath101 and @xmath102 for @xmath103 ) . + finally , the function value must also satisfy the following constraints @xmath104    since these functions are only defined on @xmath105 , it is suffice to use @xmath106 record to store the summary .",
    "specifically , we need to remember each @xmath74 and the corresponding function values of each @xmath74 .",
    "[ def : extend]extension of function domains + given a quantile summary @xmath88 defined in definition  [ def : sketch ] , the domain of @xmath95 , @xmath96 and @xmath97 were defined only in @xmath105 .",
    "we extend the definition of these functions to @xmath107 as follows + when @xmath108 : @xmath109 when @xmath110 : @xmath111 when @xmath112 for some @xmath12 : @xmath113      the only non - trivial part is to prove the case when @xmath112 : @xmath116 @xmath117 this proves eq .  .",
    "furthermore , we can verify that @xmath118 @xmath119 @xmath120 using these facts and transitivity of @xmath75 relation , we can prove eq .",
    "we should note that the extension is based on the ground case defined in @xmath105 , and we do not require extra space to store the summary in order to use the extended definition .",
    "we are now ready to introduce the definition of @xmath48-approximate quantile summary .",
    "@xmath48-approximate quantile summary + given a quantile summary @xmath88 , we call it is @xmath48-approximate summary if for any @xmath121 @xmath122 we use this definition since we know that @xmath123 $ ] and @xmath124 $ ] .",
    "eq .   means the we can get estimation of @xmath85 and @xmath86 by error of at most @xmath125 .        *",
    "property of extended function * in this section , we have introduced the extension of function @xmath130 to @xmath107 .",
    "the key theme discussed in this section is the relation of _ constraints on the original function and constraints on the extended function_. lemma  [ lem : ext - constraint ] and  [ lemma : eps ] show that the constraints on the original function can lead to in more general constraints on the extended function .",
    "this is a very useful property which will be used in the proofs in later sections .",
    "given a small multi - set @xmath131 , we can construct initial summary @xmath132 , with @xmath105 to the set of all values in @xmath80  ( @xmath133 ) , and @xmath130 defined to be @xmath134 the constructed summary is @xmath135-approximate summary , since it can answer all the queries accurately .",
    "the constructed summary can be feed into future operations described in the latter sections .      in this section ,",
    "we define how we can merge the two summaries together .",
    "assume we have @xmath136 and @xmath137 quantile summary of two dataset @xmath138 and @xmath139 .",
    "let @xmath140 , and define the merged summary @xmath141 as follows . @xmath142",
    "the points in @xmath105 are combination of points in @xmath143 and @xmath144 . and the function @xmath145",
    "are defined to be @xmath146 @xmath147 @xmath148 here we use functions defined on @xmath98 on the left sides of equalities and use the extended function definitions on the right sides .    due to additive nature of @xmath149 , @xmath150 and @xmath151 , which can be formally written as @xmath152 and the extended constraint property in lemma  [ lem : ext - constraint ]",
    ", we can verify that @xmath153 satisfies all the constraints in definition  [ def : sketch ] .",
    "therefore it is a valid quantile summary .          for any @xmath87",
    ", we have @xmath159 - [ { \\tilde{r}_{{\\mathcal{d}}_{1}}}^-(y)+{\\tilde{r}_{{\\mathcal{d}}_{2}}}^-(y ) ] -[{\\tilde{\\omega}_{{\\mathcal{d}}_{1}}}(y ) + { \\tilde{\\omega}_{{\\mathcal{d}}_{2}}}(y)]\\\\ \\leq & { \\epsilon}_1 { \\omega}({\\mathcal{d}}_1)+{\\epsilon}_2{\\omega}({\\mathcal{d}}_2 ) \\leq \\max({\\epsilon}_1,{\\epsilon}_2 ) { \\omega}({\\mathcal{d}}_1\\cup { \\mathcal{d}}_2 ) \\end{split}\\ ] ] here the first inequality is due to lemma  [ lem : merge - extend ] .",
    "before we start discussing the prune operation , we first introduce a query function @xmath161 .",
    "the definition of function is shown in algorithm  [ alg : query ] .",
    "for a given rank @xmath56 , the function returns a @xmath81 whose rank is close to @xmath56 .",
    "this property is formally described in the following lemma .",
    "* @xmath164 $ ] and @xmath165 .",
    "note that the rank information for @xmath92 is accurate ( @xmath166 , @xmath167 ) , we have @xmath168\\\\   & \\leq { \\tilde{r}_{{\\mathcal{d}}}}^-(x_1 ) + { \\tilde{r}_{{\\mathcal{d}}}}^+(x_1)\\\\   & = { \\tilde{r}_{{\\mathcal{d}}}}^-(x_1 ) + { \\tilde{\\omega}_{{\\mathcal{d}}}}^+(x_1 ) \\end{split}\\ ] ] * @xmath169 $ ] and @xmath170 , then @xmath171\\\\   & = { \\tilde{r}_{{\\mathcal{d}}}}^+(x_k ) - \\frac{1}{2}[{\\tilde{r}_{{\\mathcal{d}}}}^+(x_k ) - { \\tilde{r}_{{\\mathcal{d}}}}^-(x_k ) ] \\\\     & = { \\tilde{r}_{{\\mathcal{d}}}}^+(x_k ) - \\frac{1}{2 } { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_k)\\\\   d & < { \\omega}({\\mathcal{d } } ) + \\frac{{\\epsilon}}{2 } { \\omega}({\\mathcal{d } } ) = { \\tilde{r}_{{\\mathcal{d}}}}^-(x_k ) + { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_k ) + \\frac{{\\epsilon}}{2 } { \\omega}({\\mathcal{d } } ) \\end{split}\\ ] ] * @xmath172 in the general case , then @xmath173 + [ { \\tilde{r}_{{\\mathcal{d}}}}^+(x_{i+1 } ) - { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_{i+1 } ) -",
    "{ \\tilde{r}_{{\\mathcal{d}}}}^-(x_i ) - { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_i)]\\\\ & \\leq 2[{\\tilde{r}_{{\\mathcal{d}}}}^-(x_i ) + { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_i ) ] + { \\epsilon}{\\omega}({\\mathcal{d}})\\\\   2 d & \\geq { \\tilde{r}_{{\\mathcal{d}}}}^-(x_i ) + { \\tilde{r}_{{\\mathcal{d}}}}^+(x_i)\\\\   & = 2 [ { \\tilde{r}_{{\\mathcal{d}}}}^+(x_i ) - { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_i ) ] - [ { \\tilde{r}_{{\\mathcal{d}}}}^+(x_i ) - { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_i ) - { \\tilde{r}_{{\\mathcal{d}}}}^-(x_i ) ] + { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_i)\\\\   & \\geq 2 [ { \\tilde{r}_{{\\mathcal{d}}}}^+(x_i ) - { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_i ) ] - { \\epsilon}{\\omega}({\\mathcal{d } } ) + 0 \\end{split}\\ ] ] * @xmath174 in the general case @xmath175 \\\\ & \\ \\ \\ \\ -   [ { \\tilde{r}_{{\\mathcal{d}}}}^+(x_{i+1 } ) - { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_{i+1 } ) - { \\tilde{r}_{{\\mathcal{d}}}}^-(x_i ) - { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_i)]\\\\ & \\geq 2[{\\tilde{r}_{{\\mathcal{d}}}}^+(x_{i+1 } ) + { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_{i+1 } ) ] - { \\epsilon}{\\omega}({\\mathcal{d}})\\\\   2 d & \\leq { \\tilde{r}_{{\\mathcal{d}}}}^-(x_{i+1 } ) + { \\tilde{r}_{{\\mathcal{d}}}}^+(x_{i+1})\\\\   & = 2 [ { \\tilde{r}_{{\\mathcal{d}}}}^-(x_{i+1 } ) + { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_{i+1 } ) ] \\\\   & \\ \\ \\ \\ + [ { \\tilde{r}_{{\\mathcal{d}}}}^+(x_{i+1 } ) - { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_{i+1 } ) - { \\tilde{r}_{{\\mathcal{d}}}}^-(x_{i+1 } ) ] - { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_{i+1})\\\\   & \\leq 2 [ { \\tilde{r}_{{\\mathcal{d}}}}^-(x_{i+1 } ) + { \\tilde{\\omega}_{{\\mathcal{d}}}}(x_{i+1 } ) ]   + { \\epsilon}{\\omega}({\\mathcal{d } } ) - 0 \\end{split}\\ ] ]    now we are ready to introduce the prune operation . given a quantile summary @xmath88 with @xmath176 elements , and a memory budget @xmath177 .",
    "the prune operation creates another summary @xmath178 with @xmath179 , where @xmath180 are selected by query the original summary such that @xmath181 the definition of @xmath130 in @xmath182 is copied from original summary @xmath183 , by restricting input domain from @xmath105 to @xmath184 .",
    "there could be duplicated entries in the @xmath184 .",
    "these duplicated entries can be safely removed to further reduce the memory cost .",
    "since all the elements in @xmath182 comes from @xmath183 , we can verify that @xmath182 satisfies all the constraints in definition  [ def : sketch ] and is a valid quantile summary .",
    "we only need to prove the property in eq .   for @xmath182 .",
    "using lemma  [ lem : query ] , we have @xmath187 combining these inequalities gives @xmath188 - [ \\frac{i-1}{b}{\\omega}({\\mathcal{d } } ) - \\frac{{\\epsilon}}{2 } { \\omega}({\\mathcal{d } } ) ] = ( \\frac{1}{b } + { \\epsilon}){\\omega}({\\mathcal{d } } ) \\end{split}\\ ] ]"
  ],
  "abstract_text": [
    "<S> tree boosting is a highly effective and widely used machine learning method . in this paper , we describe a scalable end - to - end tree boosting system called xgboost , which is used widely by data scientists to achieve state - of - the - art results on many machine learning challenges . we propose a novel sparsity - aware algorithm for sparse data and weighted quantile sketch for approximate tree learning . </S>",
    "<S> more importantly , we provide insights on cache access patterns , data compression and sharding to build a scalable tree boosting system . by combining these insights </S>",
    "<S> , xgboost scales beyond billions of examples using far fewer resources than existing systems .    </S>",
    "<S> < ccs2012 > < concept_id>10010147.10010257</concept_id > < concept_desc > computing methodologies  machine learning</concept_desc > < concept_significance>500</concept_significance > </S>",
    "<S> < /concept > < concept > < concept > < concept_id>10002951.10003227.10003351</concept_id > < concept_desc > information systems  data mining</concept_desc > < concept_significance>500</concept_significance > </S>",
    "<S> < /concept > < concept > </S>",
    "<S> < /ccs2012 >    -.1 in </S>"
  ]
}