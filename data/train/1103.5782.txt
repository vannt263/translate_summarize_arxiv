{
  "article_text": [
    "in the absence of electron - phonon scattering , the problem of computing density of states and transmission through the negf formalism reduces to a mathematical problem of finding select entries from the inverse of a typically large and sparse block tridiagonal matrix .",
    "although there has been research into numerically stable and computationally efficient serial computing algorithms  @xcite , when analyzing certain device geometries this type of method will result in prohibitive amounts of computation and memory consumption . in  @xcite a parallel divide - and - conquer algorithm ( pdiv )",
    "was shown to be effective for negf based simulations .",
    "two applications were presented , the atomistic level simulation of silicon nanowires and the two - dimensional simulation of nanotransistors .",
    "alternative , serial computing , negf based approaches such as  @xcite rely on specific problem structure ( currently not capable of addressing atomistic models ) , with computation limitations that again restrict the size of simulation .",
    "in addition , the ability to compute information needed to determine current characteristics for devices has not been demonstrated with methods such as  @xcite .",
    "the prohibitive computational properties associated with negf based simulation has prompted a transition to wave function based methods , such as those presented in  @xcite .",
    "given an assumed basis structure the problem translates from calculating select entries from the inverse of a matrix to solving large sparse systems of linear equations .",
    "this is an attractive alternative because solving large sparse systems of equations is one of the most well studied problems in applied mathematics and physics . in addition , popular algorithms such as umfpack  @xcite and superlu  @xcite have been constructed to algebraically ( based solely on the matrix ) exploit problem specific structure in an attempt to minimize the amount of computation .",
    "the performance of these algorithms for the wave function based analysis of several silicon nanowires has been examined in  @xcite",
    ". however , for many devices of interest wave function methods are currently unable to address more sophisticated analyses that involve electron - phonon scattering .",
    "thus , there remains a strong need to further develop negf based algorithms for the simulation of realistically sized devices considering these general modeling techniques .",
    "when incorporating the effects of scattering into negf based simulation , it becomes necessary to determine the entire inverse of the coefficient matrix associated with the device .",
    "this substantially increases both the computational and memory requirements .",
    "there are a number of theoretical results describing the structure of the inverses of block tridiagonal and block - banded matrices .",
    "representations for the inverses of tridiagonal , banded , and block tridiagonal matrices can be found in  @xcite .",
    "it has been shown that the inverse of a tridiagonal matrix can be compactly represented by two sequences @xmath4 and @xmath5  @xcite .",
    "this result was extended to the cases of block tridiagonal and banded matrices in  @xcite , where the @xmath4 and @xmath5 sequences generalized to matrices @xmath6 and @xmath7 .",
    "matrices that can be represented in this fashion are more generally known as semiseparable matrices  @xcite .",
    "typically , the computation of parameters @xmath4 and @xmath5 suffers from numerical instability even for modest - sized problems  @xcite .",
    "it is well understood that for matrices arising in many physical applications the @xmath4 and @xmath5 sequences grow exponentially  @xcite with the index @xmath8 .",
    "one approach that has been successful in ameliorating these problems , for the tridiagonal case , is the generator approach shown in  @xcite . here",
    ", ratios for sequential elements of the @xmath4 and @xmath5 sequences are used as the generators for the inverse of a tridiagonal matrix .",
    "such an approach is numerically stable for matrices of very large sizes .",
    "the extension of this generator approach to the general block - tridiagonal matrices was discussed by the same authors in  @xcite .",
    "the authors used the block factorization of the original block - tridiagonal matrix to construct a block cholesky decomposition of its inverse .",
    "a generator based approach for inversion , typically referred to as the recursive green s function ( rgf ) algorithm , was introduced in  @xcite for negf based simulation .",
    "it is important to note that the method of  @xcite requires @xmath9 less memory to compute only the density of states and transmission ( in the absence of scattering ) , when compared to the complete generator representation . in this work ,",
    "we extend the approach of  @xcite to consider the computation of a distributed generator representation for the inverse of the block tridiagonal coefficient matrix .",
    "we then demonstrate how the distributed generator representation allows for the efficient computation of the electron density and current characteristics of the device .",
    "our parallel algorithms facilitate the simulation of realistically sized devices by utilizing additional computing resources to efficiently divide both the computation time and memory requirements . as an illustration",
    ", we stably generate the mobility for @xmath3 nm cross - section nanowires assuming an atomistic model with scattering .",
    "a block - symmetric matrix @xmath10 is block tridiagonal if it has the form @xmath11 where each @xmath12 .",
    "thus @xmath13 , with @xmath14 diagonal blocks of size @xmath15 each .",
    "we will use the notation @xmath16 to represent such a block tridiagonal matrix .",
    "the negf based simulation of nanowires using the @xmath17 atomistic tight - binding model with electron - phonon scattering has been demonstrated in  @xcite .",
    "the block tridiagonal coefficient matrix for simulation is constructed in the following way : @xmath18 here , @xmath19 is the energy of interest , @xmath20 is the hamiltonian containing atomistic interactions , and @xmath21 , @xmath22 , and @xmath23 are the left and right boundary conditions and self energy scattering terms respectively . in order to calculate the current characteristics for the device we must first form the retarded greens function using the fact that @xmath24 .",
    "a standard numerically stable mathematical representation for the inverse of this block tridiagonal matrix is dependent on two sequences of generator matrices @xmath25,@xmath26 . here ,",
    "the terms @xmath27 and @xmath28 correspond to the forward and backward propagation through the device .",
    "specifically , we can use the diagonal blocks of the inverse @xmath29 and the generators to describe the inverse a block tridiagonal matrix @xmath10 in the following manner :    @xmath30    where the diagonal blocks of the inverse , @xmath31 , and the generator sequences satisfy the following relationships : @xmath32 the time complexity associated with determining the parametrization of @xmath33 by the above approach is @xmath34 , with a memory requirement of @xmath35 .",
    "it is important to note that if the block tridiagonal portion of @xmath33 is known , the generator sequences @xmath36 and @xmath37 can be extracted directly , i.e. without the use of entries from @xmath10 through the generator expressions  ( [ eqn : uv ] ) .",
    "examining closely the block tridiagonal portion of @xmath33 we find the following relations : @xmath38 where @xmath39 denotes the @xmath40 block entry of @xmath33 and @xmath41 denotes the @xmath42 block entry of @xmath33 .",
    "therefore , by being able to produce the block tridiagonal portion of @xmath33 we have all the information that is necessary to compute the compact representation .    as was alluded to in section  [ ss : intro ] , direct techniques for simulation of realistic devices often require prohibitive memory and computational requirements . to address these issues",
    "we offer a parallel divide - and - conquer approach in order to construct the compact representation for @xmath33 , i.e. the framework allows for the parallel inversion of the coefficient matrix .",
    "specifically , we introduce an efficient method for computing the block tridiagonal portion of @xmath33 in order to exploit the process demonstrated in  ( [ eqn : comp_ratio ] ) .",
    "the compact representation of @xmath33 can be computed in a distributed fashion by first creating several smaller sub - matrices @xmath43 .",
    "that is , the total number of blocks for the matrix @xmath10 are divided as evenly as possible amongst the sub - matrices .",
    "after each individual sub - matrix inverse has been computed they can be combined in a radix-2 fashion using the matrix inversion lemma from linear algebra .",
    "figure  [ fig : decomp_over ] shows both the decomposition and the two combining levels needed to form the block tridiagonal portion of @xmath33 , assuming @xmath10 has been divided into four sub - matrices . in general ,",
    "if @xmath10 is separated into @xmath44 sub - matrices there will be @xmath45 combining levels with a total of @xmath46 combining operations or `` steps '' .",
    "the notation @xmath47 is introduced to represent the result of any combining step , through the use of the matrix inversion lemma .",
    "for example , @xmath48 is the inverse of a matrix comprised of the blocks assigned to both @xmath49 and @xmath50 .",
    "it is important to note that using the matrix inversion lemma repeatedly to join sub - matrix inverses will result in a prohibitive amount of memory and computation for large simulation problems .",
    "this is due to the fact that at each combining step all entries would be computed and stored .",
    "thus , the question remains on the most efficient way to produce the block tridiagonal portion of @xmath33 , given this general decomposition scheme for the matrix @xmath10 .    in this work",
    ", we introduce a mapping scheme to transform compact representations of smaller matrix inverses into the compact representation of @xmath33 .",
    "the algorithm is organized as follows :    * decompose the block tridiagonal matrix @xmath10 into @xmath44 smaller block tridiagonal matrices .",
    "* assign each sub - matrix to an individual cpu .",
    "* independently determine the compact representations associated with each sub - matrix .",
    "* gather all information that is needed to map the sub - matrix compact representations into the compact representation for @xmath33 . *",
    "independently apply the mappings to produce a portion of the compact representation for @xmath33 on each cpu .    the procedure described above results in a `` distributed compact representation '' allowing for reduced memory and computational requirements . specifically",
    ", each cpu will eventually be responsible for the elements from both the generator sequences and diagonal blocks that correspond to the initial decomposition ( e.g. if @xmath49 is responsible for blocks @xmath51 from the matrix @xmath10 , the mappings will allow for the computation of @xmath52 ) .    in order to derive the mapping relationships needed to produce a distributed compact representation ,",
    "it is first necessary to analyze how sub - matrix inverses can be combined to form the complete inverse .",
    "consider the decomposition of the block tridiagonal matrix @xmath10 into two block tridiagonal sub - matrices and a correction term , demonstrated below :    @xmath53    thus , the original block tridiagonal matrix can be decomposed into the sum of a block diagonal matrix ( with its two diagonal blocks themselves being block tridiagonal ) and a correction term parametrized by the @xmath54 matrix @xmath55 , which we will refer to as the `` bridge matrix '' .",
    "using the matrix inversion lemma , we have @xmath56 where @xmath57 and @xmath58 and @xmath59 denote respectively the last and first block columns of @xmath60 and @xmath61 .",
    "this shows that the entries of @xmath62 are modified through the entries from the first rows and last columns of @xmath63 and @xmath64 , as well as the bridge matrix @xmath65 .",
    "specifically , since @xmath49 is before or `` above '' the bridge point we only need the last column of its inverse to reconstruct @xmath33 .",
    "similarly , since @xmath50 is after or `` below '' the bridge point we only need the first column of its inverse .",
    "these observations were noted in  @xcite , where the authors demonstrated a parallel divide - and - conquer approach to determine the diagonal entries for the inverse of block tridiagonal matrices .",
    "we begin by generalizing the method from  @xcite in order to compute all information necessary to determine the distributed compact representation of @xmath33  ( [ eqn : uv ] ) .",
    "that is , we would like to create a combining methodology for sub - matrix inverses with two major goals in mind .",
    "first , it must allow for the calculation of all information that would be required to repeatedly join sub - matrix inverses , in order to mimic the combining process shown in figure  [ fig : decomp_over ] .",
    "second , at the final stage of the combining process it must facilitate the computation of the block tridiagonal portion for the combined inverses .",
    "details pertaining to the parallel computation of @xmath33 are provided in appendix  [ ap : gr ] .",
    "the time complexity of the algorithm presented is @xmath66 , with memory consumption @xmath67 .",
    "the distribution of the compact representation is at the foundation of an efficient parallel method for calculating the less - than green s function @xmath2 and greater - than green s function @xmath68 .",
    "the parallel inversion algorithm described in section  [ s : pinv_blktri ] not only has advantages in computational and memory efficiency but also facilitates the formulation of a fast , and highly scalable , parallel matrix multiplication algorithm .",
    "this plays an important role during the simulation process due to the fact that computation of the less - than green s function requires matrix products with the retarded green s function matrix :    @xmath69    @xmath1 , which we will refer to as the less - than scattering matrix , is typically assumed to be a block diagonal matrix",
    ". we will demonstrate how the distributed compact representation of the semiseparable matrix @xmath0 presented in section  [ s : pinv_blktri ] can be used to calculate the necessary information from @xmath70 .",
    "specifically , the electron density for the device will be calculated through the diagonal entries of @xmath2 and the current characteristics through the first off - diagonal blocks of @xmath2 .",
    "recall that our initial state for this procedure would assume that portions of the block tridiaongal ( corresponding to the size and location of the divisions ) of @xmath0 have been calculated and stored .",
    "it is important to note that there are many generator representations for @xmath0 and we would like to select a representation that will facilitate efficient calculation of @xmath2 . for the mathematical operation shown in  ( [ eqn : gless_main ] ) , our starting point will be describing the @xmath71 block row of @xmath0 in terms of @xmath72 , the diagonal blocks and two generator sequences respectively .",
    "the following expressions are used to determine the generators from the block tridiagonal of the semiseparable matrix :    @xmath73    @xmath74 , are the diagonal , upper diagonal , and lower diagonal blocks respectively .",
    "thus , the generators @xmath75 and @xmath76 are used to describe the @xmath71 block row of @xmath0 semiseparable matrix in the following way :    @xmath77    this generator representation for @xmath0 along with the block diagonal structure of @xmath1 allows for us to express each diagonal block of @xmath2 in terms of recursive sequences .",
    "both the forward recursive sequence @xmath78 and backward recursive sequence @xmath79 are dependent on a common sequence of injections terms @xmath80 ( note : the arrow orientation for @xmath81 matches that of @xmath82 ) .",
    "the relationships between the sequences and the diagonal blocks of @xmath2 are shown below :",
    "@xmath83    similar serial recursions have been shown in  @xcite and  @xcite .",
    "our strategy in this work is to exploit the distributed compact representation of @xmath33 in order to produce these sequences efficiently .",
    "that is , we will divide the computation needed to calculate the three sequences @xmath80 , @xmath78 , and @xmath79 . into several sub - problems that can be efficiently separated across many processors .    as motivation , if we assume that there are @xmath84 blocks , we can define two sub - problems by separating @xmath85 , where @xmath86 contains blocks @xmath87 through @xmath88 and @xmath89 contains blocks @xmath90 through @xmath91",
    "then , we can write :    @xmath92    for this example , we have assumed an equal separation of the diagonal blocks for the scattering matrix @xmath1 , i.e. @xmath93 .",
    "thus , for the first sub - problem we have :    @xmath94    we then see the following relationships for the second sub - problem :    @xmath95    the recursions for the case of two sub - problems are illustrated in figure  [ fig : gless_2cpu ] with @xmath96 . here , the shaded terms in figure  [ fig : gless_2cpu_1 ] and figure  [ fig : gless_2cpu_2 ] represent the terms for each sub - problem that will be computed on cpu 1 and cpu 2 respectively . in summary ,",
    "the separation of the diagonal blocks , the generator sequences , and the scattering matrix evenly across two computers will result in the following order of operations :    @xmath97    if we consider multiplication to be of order @xmath98 , then on a single processor the @xmath2 calculation would require @xmath99 operations .",
    "in this case we have four stages , each with @xmath100 operations .",
    "therefore , using two processors we have reduced the number of multiplications to @xmath101 .",
    "we now generalize this process and demonstrate further speed - up as both the number of processors and the length of the device increase .      in order to simplify the presentation of the method",
    ", we will assume that the number of sub - problems @xmath44 evenly divides the total number of blocks @xmath102 .",
    "in general this assumption is not required .",
    "we will separate the @xmath2 operation evenly into @xmath44 sub - problems by dividing @xmath103 , where @xmath104 contains the @xmath71 portion of the less - than scattering matrix . given that @xmath105 , the @xmath71 sub - problem will be solved through the following recursions :    @xmath106    so , in a sense by splitting the scattering matrix we have divided the injection sequence @xmath107 evenly and created two new sub - problem propagating sequences @xmath108 and @xmath109 .",
    "however , the sub - problem propagating sequences have the special property that they do not start ( are zero ) until they reach the indices governed by the sub - problem .",
    "this is due to the fact that there are no injection terms for the given sub - problem until these points are reached .",
    "in addition , once outside the range of the sub - problem we do not have any additional injection terms in the recursions .",
    "thus , we have a standard two - sided auto - regressive expression where the terms are simply propagated by multiplication with generator matrices .",
    "the recursions for the case of four sub - problems are illustrated in figure  [ fig : gless_4cpu ] for an example with @xmath110 .",
    "here , the shaded terms in figure  [ fig : gless_4cpu_1 ] and figure  [ fig : gless_4cpu_4 ] represent the terms for each sub - problem that are computed on cpu 1 and cpu 4 respectively .",
    "for each sub - problem we introduce two new sequences that will be referred to as ",
    "skip matrices  .",
    "specifically , we define for each processor @xmath111 two matrices @xmath112_k$ ] and @xmath113_k^t$ ] that are accumulations of the generator terms stored on the processor .",
    "as can be clearly seen from figure  [ fig : gless_4cpu ] these skip matrices allow for several steps in the recursive process to be preformed through a single operation",
    ". therefore , if the skip matrices are made available to each processor several terms for each sub - problem can be found concurrently . in figure",
    "[ fig : gless_4cpu_4 ] , we can see that skip matrices will allow for @xmath114 , @xmath115 , and @xmath116 to be determined currently by cpus 3 , 2 , and 1 respectively .",
    "given that each sub - problem will produce both a forward and backward propagating sequence it should be clear that cpu @xmath111 will need to preform computation for @xmath117 sequences from sub - problems @xmath118 and @xmath119 sequences from sub - problems @xmath120 .",
    "however , each @xmath117 sequence and @xmath119 sequence from other sub - problems may be combined before the generators on that cpu are applied .",
    "this is due to the fact that the sequences from each sub - problem will eventually be added together to form @xmath121 .",
    "for the example shown in figure  [ fig : gless_4cpu ] we see that cpu @xmath87 will first need to form : @xmath122_2   \\left(\\left[g^{\\underleftarrow{r}}\\right]_3 g^{\\underleftarrow{<}}_{10;4 } \\left[g^{\\underleftarrow{r}}\\right]_3^ *   + g^{\\underleftarrow{<}}_{7;3 } \\right ) \\left[g^{\\underleftarrow{r}}\\right]_2^ * + g^{\\underleftarrow{<}}_{4;2},\\end{aligned}\\ ] ]    before the required terms for each sub - problem can be calculated .",
    "that is , after the lead term for the backward propagating sequence : @xmath123 has been computed , the generators governed by cpu 1 : @xmath124 , @xmath125 , and @xmath126 , can be applied to fulfill the sub - problem solutions .",
    "we now have all the tools necessary to construct a general procedure :    1 .",
    "use the parallel algorithm described in section  [ s : pinv_blktri ] in order to produce the block tridiagonal portion of @xmath127 .",
    "2 .   compute the generator sequences @xmath36 and @xmath37 based upon the procedure from  ( [ eqn : ratio_back ] ) .",
    "3 .   compute the skip products of each generator sequence @xmath36 and @xmath37 , i.e. compute @xmath128_k= \\prod\\limits_{i = ( k-1)n+1 } ^{kn } { g^{\\underleftarrow{r}}_{i}}$ ] and @xmath129_k^t = \\prod\\limits_{i = kn-1}^{(k-1)n } { \\left(g^{\\overrightarrow{r}}_{i}\\right)^{t}}$ ] , where we will define @xmath130 .",
    "4 .   compute the initial injection and propagation terms for each sub - problem : @xmath131 5 .",
    "transfer forward and backward skip products : @xmath129_k^t$ ] and @xmath128_k$ ] , as well as forward and backward lead propagating matrices : @xmath132 and @xmath133 , for each sub - problem @xmath111 to all cpus .",
    "this will be a total of @xmath134 entries .",
    "cpu @xmath111 will construct combined lead propagating term for @xmath119 sequences from sub - problems @xmath118 .",
    "@xmath135_l   \\right )   g^{\\underleftarrow{<}}_{(j-1)n+1;j }   \\left ( \\prod\\limits_{l = j-1}^{k+1 } \\left[g^{\\underleftarrow{r } } \\right]_l^ *   \\right)\\end{aligned}\\ ] ] cpu @xmath111 will construct combined lead propagating term for @xmath117 sequences from sub - problems @xmath120 .",
    "@xmath136_l^t   \\right )   g^{\\overrightarrow{<}}_{jn+1;j }   \\left ( \\prod\\limits_{l = j+1}^{k-1 } \\left[g^{\\overrightarrow{r } } \\right]_l^c   \\right)\\end{aligned}\\ ] ] 7 .",
    "successively multiply by the governed generator terms @xmath37 and @xmath36 in order fulfill sub - problem solutions .",
    "@xmath137 8 .",
    "each cpu will combine portions of all sub - problem solutions in order to produce corresponding portion of the diagonal blocks of @xmath2 based upon the relationships shown in  ( [ eqn : gless_rec ] ) .    in order to analyze the computational improvement for the approach we consider the number of multiplications required for each stage .",
    "the accumulation of the skip matrices described in step 1 will result in @xmath138 multiplications .",
    "the individual sub - problem recursions of step 2 results in @xmath139 multiplications .",
    "step 4 describes how the skip matrices may be applied in order to create the sum for each sub - problem propagating sequence , requiring @xmath140 multiplications .",
    "finally , these two sums must be propagated through each governed generator matrices , requiring @xmath141 multiplications .",
    "if one is interested in also computing the current characteristics for the device , they may be determined through the off - diagonal blocks of @xmath2 :    @xmath142    as portions of each generator sequence and propagating sequence have been evenly distributed amongst the processors , the resulting computation would be @xmath138 .",
    "therefore , the total number of multiplications for the approach is @xmath143 compared to @xmath144 for the serial algorithm .",
    "therefore , the speedup of our approach is @xmath145    we can clearly see that if @xmath146 we will approach a speedup of @xmath147 .     @xmath44&4&8&16&32&4&8&16&32&4&8&16&32 + & + @xmath0 & 35.8&21.5&16.2&n / a&411.1&241.4&163.6&n / a&1915.5&1099.7&726.1 & n / a + @xmath2 & 22.7&12.7&10.0&n / a&253.7&133.7&81.2&n / a&1159.2&598.5&349.7 & n / a + rgf@xmath148 & 1.5&2.7&3.4&n / a&1.4&2.6&4.1&n / a&1.4&2.6&4.2 & n / a + & + @xmath0 & 51.4&29.3&19.7&n / a&594.4&331.9&211.4&n / a&2756.1&1523.4&946.4 & n / a + @xmath2 & 33.5&18.5&13.8&n / a&380.9&198.5&113.8&n / a&1741.6&887.8&489.1 & n / a + rgf@xmath148 & 1.6&2.8&4.0&n / a&1.5&2.7&4.5&n / a&1.4&2.7&4.6 & n / a + & + @xmath0 & 67.1&38.8&23.8&17.7&779.0&422.6&255.0&181.3&3601.3&1953.7&1154.5&782.1 + @xmath2 & 44.6&24.4&15.8&12.9&507.6&261.5&145.5&103.4&2324.2&1182.2&640.2&408.9 + rgf@xmath148 & 1.6&2.8&4.5&5.7&1.5&2.8&4.8&6.8&1.4&2.7&4.9&7.4 +",
    "the parallel @xmath0 and @xmath2 algorithms have been implemented in c using mpi for inter - processor communication .",
    "all computational complexity analyses were performed on a cluster of intel e5410 processors with @xmath149 gb of shared memory for the @xmath150 core machines .",
    "the omen simulator  @xcite was used to perform simulations for square silicon nanowires employing the @xmath17 atomistic tight - binding model with electron - phonon scattering .",
    "we will begin by demonstrating the computational efficiency of our approach .",
    "we will then analyze device characteristics for large cross - section nanowires that have prohibitive memory requirements for the serial rgf approach .",
    "each negf nanowire simulation requires thousands of @xmath0 , @xmath2 , and @xmath68 computations . for our approach , as well as rgf , the time for each computation will remain constant given that the underlying structure of the hamiltonian and scattering matrices does not change . in order to analyze the computational benefits of our approach we have examined several different cross - section sizes and lengths of nanowires .",
    "specifically , silicon @xmath151 $ ] nanowires with lengths @xmath152 nm , @xmath153 nm , and @xmath154 nm are examined with cross - sections of @xmath155 nm , @xmath156 nm , and @xmath157 nm . table  [ tab : gless_scale ] shows the runtime for the @xmath0 and @xmath2 computations ( the time needed for @xmath68 is identical to that of @xmath2 ) . ",
    "rgf@xmath148  is the observed speed - up for the combined time of @xmath0 , @xmath2 , and @xmath68 calculations compared to rgf .",
    " n / a  is used to for devices that are not long enough to be divided based upon the number of cpus .",
    "we can clearly see that the efficiency of our algorithm improves as both the length and cross - section of the device increase .",
    "there are two reasons behind this trend .",
    "first , considering a fixed number of processors a longer device will devote less of the total time to sub - problem combining .",
    "in addition , as the cross - section size increases the inter - processor communication costs will be a smaller fraction of the total simulation time .",
    "we see these effects again when examining figure  [ fig : gless_scale ] . here",
    ", we verify the accuracy of the estimated @xmath2 scaling trend that was derived in  ( [ eqn : gless_speed ] ) .",
    "the speed - up over rgf for both @xmath158 nm and @xmath159 nm nanowires are compared against our theoretical estimate .",
    "it is important to note that although the speed - up estimate  ( [ eqn : gless_speed ] ) is independent of the cross - section size , effects such as data access time , vector scaling / addition , and inter - processor communication will play a role in determining the efficiency .",
    "thus , in both figures  [ fig : gless_scale_50 nm ] and  [ fig : gless_scale_100 nm ] we again see improved efficiency when considering the larger @xmath156 nm cross - section nanowires . in the case of the @xmath160nm@xmath161 nanowire we achieve @xmath162 speed - up when utilizing @xmath163 cpus .",
    "in addition to providing computational improvements , our algorithm facilitates the simulation of larger cross - section devices . if we consider the @xmath157 nm cross - section devices analyzed in table  [ tab : gless_scale ] , the rgf method would require between @xmath149 gb and @xmath163 gb of memory for lengths of @xmath152 nm to @xmath154 nm .",
    "these devices would not be able to be analyzed without the use of special purpose hardware .",
    "= 3.2 in    as an illustration of the capacity for our algorithm to simulate devices previously viewed to have prohibitive memory requirements , we stably generate the mobility for @xmath3 nm cross - section devices with electron - phonon scattering . in order to facilitate complete nanowire simulations",
    "we have implemented our algorithm on dual hex - core amd opteron 2435 ( istanbul ) processors running at 2.6ghz , 16 gb of ddr2 - 800 memory , and a seastar 2 + router . as an application",
    ", the low - field phonon - limited mobility of electrons @xmath164 is calculated in circular nanowires with diameters ranging from @xmath165 to @xmath3 nm and transport along the @xmath151 $ ] crystal axis .",
    "the `` dr / dl '' method  @xcite and the same procedure as in  @xcite are used to obtain @xmath164 . the channel resistance `` r '' is computed as function of the nanowire length `` l '' and then converted into a mobility . here ,",
    "`` l '' is set to @xmath152 nm , `` r '' is computed in the limit of ballistic transport and in the presence of electron - phonon scattering , and the difference between these two points is considered to evaluate dr / dl .",
    "the results are shown in figure  [ fig : mobility ] . from a numerical perspective , the computation of each green s function , at a given energy ,",
    "was parallelized on 16 cpus for all the device structures . as was alluded to above the simulation of these structures would not have been possible ( due to memory restrictions ) without the decomposition of the device through our parallel methods .",
    "in this work we have developed algorithms for parallel negf simulation with scattering .",
    "the computational benefits of our approach have been demonstrated on large cross - section silicon nanowires .",
    "we show improvements of over @xmath166 for the @xmath2 and @xmath68 computations .",
    "in addition , our approach enables simulations without the need for special purpose hardware .",
    "this can best be observed through our simulation results for @xmath3 nm cross - section silicon nanowires .",
    "the algorithms developed in this work are applicable for a wide range of device geometries considering both atomistic and effective - mass models .",
    "in addition to offering significant computational improvements over the serial recursive green s function algorithm , our approach facilitates simulation of realistically sized devices on typical distributed computing hardware",
    ".    10    a.  svizhenko , m.  p. anantram , t.  r. govindan , b.  biegel , and r.  venugopal .",
    "two - dimensional quantum mechanical modeling of nanotransistors .",
    ", 91(4):23432354 , 2002 .",
    "s.  cauley , j.  jain , c .- k .",
    "koh , and v.  balakrishnan .",
    "a scalable distributed method for quantum - scale device simulation . , 101(123715 ) , 2007 .",
    "s.  li , s.  ahmed , g.  klimeck , and e.  darve . computing entries of the inverse of a sparse matrix using the find algorithm .",
    ", 227(22):94089427 , 2008 .",
    "m.  stadele , r.  tuttle , and k.  hess .",
    "tunneling through ultrathin sio2 gate oxides from microscopic models .",
    ", 89(1):348363 , 2001 .",
    "timothy  a. davis .",
    "algorithm 832 : umfpack v4.3an unsymmetric - pattern multifrontal method .",
    "30(2):196199 , 2004 .    james  w. demmel , stanley  c. eisenstat , john  r. gilbert , xiaoye  s. li , and joseph w.  h. liu . a supernodal approach to sparse partial pivoting .",
    ", 20(3):720755 , 1999 .",
    "boykin , m.  luisier , and g.  klimeck .",
    "multiband transmission calculations for nanowires using an optimized renormalization method . ,",
    "b 77:165318 , 2008 .",
    "k.  bowden .",
    "a direct solution to the block tridiagonal matrix inversion problem .",
    ", 15:185198 , 1989 .",
    "b.  bukhberger and g.  a. emelyanenko .",
    "methods of inverting tridiagonal matrices . , 13:1020 , 1973",
    ".    e.  m. godfrin . a method to compute the inverse of an n - block tridiagonal quasi - hermitian matrix .",
    ", 3:78437848 , 1991 .",
    "t.  oohashi . some representation for inverses of band matrices .",
    ", 14:3947 , 1978 .",
    "t.  torii .",
    "inversion of tridiagonal matrices and the stability of tridiagonal systems of linear equations .",
    ", 16:403414 , 1966 .",
    "g.  meurant . a review on the inverse of symmetric block tridiagonal and block tridiagonal matrices .",
    ", 13(3):707728 , 1992 .    e.  asplund .",
    "inverses of matrices @xmath167 which satisfy @xmath168 for @xmath169 . , 7:5760 , 1959 .",
    "finite boundary value problems solved by green s matrix . , 7:4956 , 1959 .",
    "r.  bevilacqua , b.  codenotti , and f.  romani .",
    "parallel solution of block tridiagonal linear systems .",
    ", 104:3957 , 1988 .",
    "r.  nabben .",
    "ecay rates of the inverses of nonsymmetric tridiagonal and band matrices . , 20(3):820837 , 1999 .",
    "f.  romani . on the additive structure of inverses of banded matrices .",
    ", 80:131140 , 1986 .    p.  rozsa . on the inverse of band matrices . , 50:8295 , 1987 .",
    "p.  rozsa , r.  bevilacqua , p.  favati , and f.  romani . on the inverse of block tridiagonal matrices with applications to the inverses of band matrices and block band matrices .",
    ", 40:447469 , 1989 .",
    "p.  rozsa .",
    "band matrices and semi - separable matrices .",
    ", 50:229237 , 1986 .",
    "p.  concus and meurant g. on computing inv block preconditionings for the conjugate gradient method . , 26:493504 , 1986 .",
    "s.  demko , w.  f. moss , and smith  p. w. decay rates for inverses of band matrices .",
    ", 43:491499 , 1984 .",
    "p.  concus , g.  h. golub , and meurant g. block preconditionings for the conjugate gradient method .",
    ", 6:220252 , 1985 .",
    "m.  luisier and g.  klimeck .",
    "atomistic full - band simulations of si nanowire transistors with electron - phonon scattering . ,",
    "b 80:155430 , 2009 .",
    "j.  jain , s.  cauley , h.  li , c .- k .",
    "koh , and v.  balakrishnan .",
    "numerically stable algorithms for inversion of block tridiagonal and banded matrices , submitted for consideration .",
    ", 2007 .",
    "k.  rim , s.  narasimha , m.  longstreet , a.  mocuta , and j.  cai .",
    "low field mobility characteristics of sub-100 nm unstrained and strained si mosfets .",
    ", 43 - 46 ( 2002 ) .",
    "m.  luisier .",
    "phonon - limited and effective low - field mobility in n- and p - type [ 100]- , [ 110]- , and [ 111]-oriented si nanowire transistors .",
    ", 98 , 032111 ( 2011 ) .",
    "in appendix  [ ap : gr ] we build upon the approach of  @xcite to consider the computation of a distributed generator representation for @xmath33 . in section  [ ss : maps ] we introduce a mapping framework for combining sub - matrices corresponding to subsets of the device structure .",
    "this has similarities to the approach presented in  @xcite , where in that case only the diagonal entries of @xmath33 were needed to describe the density of states .",
    "section  [ ss : recur_comb ] illustrates how a recursive process can be formulated in order to reconstruct the compact representation of @xmath33 .",
    "this involves several key differences when compared to  @xcite as significantly more information is required from @xmath33 .",
    "finally , in section  [ ss : up_scheme ] the parallel @xmath33 algorithm is summarized including an analysis of the computational complexity .",
    "matrix mappings are constructed in order to eventually produce the block tridiagonal portion of @xmath33 while avoiding any unnecessary computation during the combining process .",
    "specifically , we will show that both the boundary block entries ( first block row and last block column ) and the block tridiagonal entries from any combined inverse @xmath47 must be attainable ( not necessarily computed ) for all combining steps .",
    "we begin by illustrating the initial stage of the combining process given four divisions , where for simplicity each will be assumed to have @xmath88 blocks of size @xmath170 .",
    "first , the two sub - matrices @xmath171 and @xmath172 are connected through the bridge matrix @xmath173 and together they form the larger block tridiagonal matrix @xmath174 . by examining figure  [ fig : decomp_over ]",
    "it can be seen that eventually @xmath48 and @xmath175 will be combined and we must therefore produce the boundaries for each combined inverse . from ( [ eqn : minv ] ) the first block row and last block column of @xmath176 can be calculated through the use of an `` adjustment '' matrix :        @xmath178^t\\\\ \\left [ -\\phi_{1}^{-1}(1,n)b_{n}j_{11}\\phi_{2}^{-1}(1 , : ) \\right]^t\\\\   \\end{pmatrix}^t , \\nonumber   \\\\ \\label{eqn : rc_map } \\\\",
    "\\nonumber \\phi_{1\\sim2}^{-1}(:,2n ) & = \\begin{pmatrix } 0 & \\phi_{2}^{-1}(n , : ) \\end{pmatrix}^{t } -   \\begin{pmatrix } \\left [ -\\phi_{2}^{-1}(n,1)b_{n}^{t}j_{22}\\phi_{1}^{-1}(:,n)^{t } \\right]^t \\\\ \\left [ -\\phi_{2}^{-1}(n,1)b_{n}^{t}j_{21}\\phi_{2}^{-1}(1 , : ) \\right]^t \\\\   \\end{pmatrix}. \\nonumber \\end{aligned}\\ ] ]            the combination of @xmath183 and @xmath184 through the bridge matrix @xmath185 results in similar relationships to those seen above .",
    "thus , in order be able to produce both the boundary and block tridiagonal portions of each combined inverse we assign a total of twelve @xmath54 matrix maps for each sub - matrix @xmath111 .",
    "@xmath186 describe effects for the @xmath71 portion of the boundary , @xmath187 describe the effects for a majority of the tridiagonal blocks , while @xmath188 , which we will refer to as `` cross '' maps , can be used to produce the remainder of the tridiagonal blocks .",
    "initially , for each sub - matrix @xmath8 the mappings @xmath189 with all remaining mapping terms set to zero .",
    "this ensures that initially the boundary of @xmath190 matches the actual entries from the sub - matrix inverse , and the modifications to the tridiagonal portion due to combining are all set to zero . by examining the first block row , last block column , and the tridiagonal portion of the combined inverse @xmath191 we can see how the maps can be used to explicitly represent all of the needed information",
    "the governing responsibilities of the individual matrix maps are detailed below : @xmath192^t \\\\",
    "\\nonumber \\left [ m_{1;2}\\phi_{2}^{-1}(1 , : ) + m_{2;2}\\phi_{2}^{-1}(:,n)^{t } \\right]^t \\\\ \\nonumber   \\end{pmatrix}^t , \\nonumber \\\\ &",
    "\\phi_{1 \\sim 2}^{-1}(:,2n ) = \\begin{pmatrix }   \\left [ m_{3;1}\\phi_{1}^{-1}(1 , : ) + m_{4;1}\\phi_{1}^{-1}(:,n)^{t } \\right]^t",
    "\\\\ \\nonumber \\left [ m_{3;2}\\phi_{2}^{-1}(1 , : ) + m_{4;2}\\phi_{2}^{-1}(:,n)^{t } \\right]^t\\\\ \\nonumber   \\end{pmatrix } ,   \\label{eqn : mmaps_wm } \\nonumber \\\\ & \\phi_{1 \\sim 2}^{-1}(r , s ) = \\phi_{1}^{-1}(r , s ) - [ \\phi_{1}^{-1}(r,1)m_{5;1}\\phi_{1}^{-1}(1,s ) +     \\phi_{1}^{-1}(r,1)m_{6;1}\\phi_{1}^{-1}(s , n)^{t } + \\nonumber \\\\     & \\phi_{1}^{-1}(r , n)m_{7;1}\\phi_{1}^{-1}(1,s ) + \\phi_{1}^{-1}(r , n)m_{8;1}\\phi_{1}^{-1}(s , n)^{t } ] , \\\\   \\nonumber \\nonumber \\\\ & \\phi_{1 \\sim 2}^{-1}(r , s+n ) = - [ \\phi_{1}^{-1}(r,1)c_{1;1}\\phi_{2}^{-1}(1,s ) +    \\phi_{1}^{-1}(r,1)c_{2;1}\\phi_{2}^{-1}(s , n)^{t } + \\nonumber \\\\    & \\phi_{1}^{-1}(r , n)c_{3;1}\\phi_{2}^{-1}(1,s ) + \\phi_{1}^{-1}(r , n)c_{4;1}\\phi_{2}^{-1}(s , n)^{t } ] ,   \\nonumber   \\\\",
    "\\nonumber \\nonumber \\\\ \\nonumber & \\phi_{1 \\sim 2}^{-1}(r+n , s+n ) = \\phi_{2}^{-1}(r , s ) - [ \\phi_{2}^{-1}(r,1)m_{5;2}\\phi_{2}^{-1}(1,s ) +    \\phi_{2}^{-1}(r,1)m_{6;2}\\phi_{2}^{-1}(s , n)^{t } + \\nonumber \\\\    & \\phi_{2}^{-1}(r , n)m_{7;2}\\phi_{2}^{-1}(1,s ) + \\phi_{2}^{-1}(r , n)m_{8;2}\\phi_{2}^{-1}(s , n)^{t } ] ,   \\nonumber \\\\",
    "\\nonumber & r , s \\leq n ,   \\nonumber\\end{aligned}\\ ] ]      it is important to note that all of the expressions  ( [ eqn : rc_map])-([eqn : odiag_map ] ) can be written into the matrix map framework of  ( [ eqn : mmaps_wm ] ) .",
    "figure  [ fig : comb_maps ] shows the mapping dependencies for the first block row and last block row ( or column since @xmath10 is symmetric ) . from  ( [ eqn : rc_map ] ) we see that both of the block rows are distributed based upon the location of each sub - matrix with respect to the bridge point , i.e. the mapping terms associated with @xmath60 can be used to produce the first portion of the rows while those associated with @xmath61 can be used for the remainder .",
    "in fact , this implicit division for the mapping dependencies holds for the block tridiagonal portion of the combined inverses as well , enabling an efficient parallel implementation .",
    "thus , from this point we can deduce that the matrix maps for the first block row  ( [ eqn : mmaps_wm ] ) must be updated in the following manner :      in order to understand these relationships it is important to first recall that the updates to the maps associated with sub - matrix @xmath171 are dependent on the last block column @xmath194 .",
    "thus , we see a dependence on the previous state for the last block column @xmath194 , i.e. the new state of the mapping terms @xmath195 and @xmath196 are dependent on the previous state of the mapping terms @xmath197 and @xmath198 respectively .",
    "similarly , a dependence on @xmath199 results in the new state of the mapping terms @xmath200 and @xmath201 being functions of the previous state of the mapping terms @xmath200 and @xmath200 respectively . finally ,",
    "although some of the mapping terms remain zero after this initial combining step ( @xmath201 for example ) , the expressions described in  ( [ eqn : mmaps_wm ] ) need to be general enough for the methodology .",
    "that is , the mapping expressions must be able to capture combining effects for multiple combing stages , regardless of the position of the sub - matrix with respect to a bridge point . for example ,",
    "if we consider sub - matrix @xmath50 for the case seen in figure  [ fig : decomp_over ] , during the initial combining step it would be considered a lower problem and for the final combining step it would be considered a upper problem . alternatively , sub - matrix @xmath183 would be associated with exactly the opposite modifications .",
    "it is important to note that every possible modification process , for the individual mapping terms , is encompassed within this general matrix map framework .      in order to formalize the notion of a recursive update scheme we will continue the example from section  [ ss : maps ] . by examining the final combing stage for the case of four divisions ,",
    "we notice that the approach described in  ( [ eqn : rc_map])-([eqn : odiag_map ] ) can again be used to combine sub - matrix inverses @xmath191 and @xmath202 , through the bridge matrix @xmath203 .",
    "the first block row and last block column of @xmath204 can be calculated as follows : @xmath205^t\\\\ \\left [ -\\phi_{1\\sim2}^{-1}(1,2n)b_{2n}j_{11}\\phi_{3\\sim4}^{-1}(1 , : ) \\right]^t\\\\   \\end{pmatrix}^t , \\nonumber   \\\\ \\label{eqn : rc_map_2lvl } \\\\",
    "\\nonumber \\phi_{1\\sim4}^{-1}(:,4n ) & = \\begin{pmatrix } 0 & \\phi_{3\\sim4}^{-1}(2n , : ) \\end{pmatrix}^{t }   -   \\begin{pmatrix } \\left [ -\\phi_{3\\sim4}^{-1}(2n,1)b_{2n}^{t}j_{22}\\phi_{1\\sim2}^{-1}(:,2n)^{t } \\right]^t \\\\ \\left [ -\\phi_{3\\sim4}^{-1}(2n,1)b_{2n}^{t}j_{21}\\phi_{3\\sim4}^{-1}(1 , : ) \\right]^t \\\\",
    "\\end{pmatrix } , \\nonumber \\end{aligned}\\ ] ] given the adjustment matrix :          again , it is important to note that each of the expressions  ( [ eqn : rc_map_2lvl])-([eqn : odiag_map_2lvl ] ) are implicitly divided based upon topology .",
    "for example , the first @xmath91 diagonal blocks of @xmath209 can be separated into two groups based upon the size of the sub - matrices @xmath49 and @xmath50 .",
    "that is , @xmath210 can be separated for @xmath211 as : @xmath212^t \\\\",
    "\\end{pmatrix } \\cdot b_{2n}j_{12 } \\cdot \\\\ & \\begin{pmatrix } \\left [ \\phi_{2}^{-1}(n,1)b_{n}^{t}j_{22}\\phi_{1}^{-1}(r , n)^{t } \\right ] \\\\",
    "\\end{pmatrix } , \\\\ \\\\ & \\phi_{1\\sim4}^{-1}(r+n , r+n ) = \\phi_{2}^{-1}(r , r ) -   \\begin{pmatrix } \\left [ \\phi_{2}^{-1}(n , r)+\\phi_{2}^{-1}(n,1)b_{n}^{t}j_{21}\\phi_{2}^{-1}(1,r ) \\right]^t \\\\",
    "\\end{pmatrix } \\cdot b_{2n}j_{12 } \\cdot \\\\ & \\begin{pmatrix } \\left [ \\phi_{2}^{-1}(n , r)+\\phi_{2}^{-1}(n,1)b_{n}^{t}j_{21}\\phi_{2}^{-1}(1,r ) \\right ] \\\\",
    "\\end{pmatrix}.\\end{aligned}\\ ] ] thus , the modifications to the diagonal entries can be written as just a function of the first block row and last block column from the individual sub - matrices , using the matrix map framework introduced in  ( [ eqn : mmaps_wm ] ) for @xmath211 : @xmath213^t \\\\",
    "\\end{pmatrix } \\cdot b_{2n}j_{12 } \\cdot \\\\ & \\begin{pmatrix } \\left [ m_{1;1}\\phi_{1}^{-1}(1,r ) + m_{2;1}\\phi_{1}^{-1}(r , n)^{t } \\right ] \\\\",
    "\\end{pmatrix } , \\\\ \\\\ & \\phi_{1\\sim4}^{-1}(r+n , r+n ) = \\phi_{2}^{-1}(r , r ) -   \\begin{pmatrix } \\left [ m_{1;2}\\phi_{2}^{-1}(1,r ) + m_{2;2}\\phi_{2}^{-1}(r , n)\\right]^t",
    "\\\\   \\end{pmatrix } \\cdot b_{2n}j_{12 } \\cdot \\\\ & \\begin{pmatrix } \\left [ m_{1;2}\\phi_{2}^{-1}(1,r ) + m_{2;2}\\phi_{2}^{-1}(r , n ) \\right ] \\\\",
    "\\end{pmatrix}. \\end{aligned}\\ ] ] here , the matrix maps are assumed to have been updated based upon the formation of the combined inverses @xmath176 and @xmath214 .",
    "therefore , we can begin to formulate the recursive framework for updating the matrix maps to represent the effect of each combining step .          after the compact representation for each inverse has been found independently ,",
    "the combining process begins .",
    "three reference positions are defined for the formation of a combined inverse @xmath216 : the `` start '' position @xmath217 = i$ ] , the `` stop '' position @xmath218 = j$ ] , and the bridge position @xmath219 = \\lceil \\frac{j - i}{2 }   \\rceil$ ] .",
    "due to the fact that a cpu @xmath220 will only be involved in the formulation of a combined inverse when @xmath217 \\leq t \\leq [ \\mathtt{sp}]$ ] all combining stages on the same level ( see figure  [ fig : decomp_over ] ) can be performed concurrently . when forming a combined inverse @xmath216 , each cpu @xmath217 \\leq t \\leq [ \\mathtt{sp}]$ ] will first need to form the adjustment matrix for the combining step . assuming a bridge matrix @xmath55 , we begin by constructing four `` corner blocks '' . if the upper combined inverse is assumed to have @xmath221 blocks and the lower to have @xmath222 , the two matrices need from the upper combined inverse are : @xmath223 = \\phi_{[\\mathtt{st}]\\sim[\\mathtt{bp}]}^{-1}(1,n_u ) ~\\mathrm{and}~[\\mathtt{lr } ] = \\phi_{[\\mathtt{st}]\\sim[\\mathtt{bp}]}^{-1}(n_u , n_u)$ ] , with the two matrices from the lower being : @xmath224 = \\phi_{[\\mathtt{bp+1}]\\sim[\\mathtt{sp}]}^{-1}(1,1 ) ~\\mathrm{and}~[\\mathtt{ll } ] = \\phi_{[\\mathtt{bp+1}]\\sim[\\mathtt{sp}]}^{-1}(n_l,1)$ ] .",
    "these matrices can be generated by the appropriate cpu through their respective matrix maps ( recall the example shown in figure  [ fig : comb_maps ] ) . specifically , the cpus corresponding to the @xmath217,~[\\mathtt{bp}],~[\\mathtt{bp+1}]~\\mathrm{and}~[\\mathtt{sp}]$ ] divisions govern the required information .",
    "the adjustment matrix for the combining step can then be formed :        @xmath226)~~\\mathrm{then } \\nonumber \\\\ & ~~c_{1}\\leftarrow c_{1}-m_{3;t}^{t}(b_{k } j_{12})m_{3;t+1 } ; \\nonumber \\\\ & ~~c_{2}\\leftarrow c_{2}-m_{3;t}^{t}(b_{k } j_{12})m_{4;t+1 } ; \\nonumber \\\\ & ~~c_{3}\\leftarrow c_{3}-m_{4;t}^{t}(b_{k } j_{12})m_{3;t+1 } ; \\nonumber \\\\ & ~~c_{4}\\leftarrow c_{4}-m_{4;t}^{t}(b_{k } j_{12})m_{4;t+1 } ; \\nonumber \\\\ & \\mathrm{else if}~~(t = = [ \\mathtt{bp}])~~\\mathrm{then }   \\label{eqn : maps_c }   \\\\ & ~~c_{1}\\leftarrow c_{1}-m_{3;t}^{t}(b_{k } j_{11})m_{1;t+1 } ; \\nonumber \\\\ & ~~c_{2}\\leftarrow c_{2}-m_{3;t}^{t}(b_{k } j_{11})m_{2;t+1 } ; \\nonumber \\\\ & ~~c_{3}\\leftarrow   c_{3}-m_{4;t}^{t}(b_{k } j_{11})m_{1;t+1 } ; \\nonumber \\\\ & ~~c_{4}\\leftarrow c_{4}-m_{4;t}^{t}(b_{k } j_{11})m_{2;t+1 } ; \\nonumber \\\\ & \\mathrm{else if}~~(t < [ \\mathtt{sp}])~~\\mathrm{then } \\nonumber \\\\ & ~~c_{1}\\leftarrow c_{1}-m_{1;t}^{t}(b_{k } ^ { t } j_{21})m_{1;t+1 } ; \\nonumber \\\\ & ~~c_{2}\\leftarrow c_{2}-m_{1;t}^{t}(b_{k } ^ { t } j_{21})m_{2;t+1 } ; \\nonumber \\\\ & ~~c_{3}\\leftarrow c_{3}-m_{2;t}^{t}(b_{k } ^ { t } j_{21})m_{1;t+1 } ; \\nonumber \\\\ & ~~c_{4}\\leftarrow c_{4}-m_{2;t}^{t}(b_{k } ^",
    "{ t } j_{21})m_{2;t+1 } ; \\nonumber\\end{aligned}\\ ] ]    notice that the cross maps for cpu @xmath220 are dependent on information from its neighboring cpu @xmath227 .",
    "this information must be transmitted and made available before the cross updates can be performed .",
    "next , updates to the remaining eight matrix maps can be separated into two categories .",
    "the updates to the matrix maps for the upper sub - matrices @xmath228)$ ] , are summarized below : @xmath229 b_{k } j_{12 } ) m_{3;t } ; \\nonumber \\\\     & m_{2;t}\\leftarrow m_{2;t}+([\\mathtt{ur } ] b_{k } j_{12 } ) m_{4;t } ; \\nonumber \\\\     & m_{3;t}\\leftarrow ( [ \\mathtt{ll}]^{t } b_{k}^{t } j_{22})m_{3;t } ; \\nonumber \\\\     & m_{4;t}\\leftarrow ( [ \\mathtt{ll}]^{t } b_{k}^{t } j_{22})m_{4;t } ; \\nonumber \\end{aligned}\\ ] ]    the updates to the matrix maps for the lower sub - matrices @xmath230)$ ] , will be : @xmath231^{t } b_{k}^{t } j_{21})m_{1;t } ; \\nonumber \\\\ & m_{4;t}\\leftarrow m_{4;t}+([\\mathtt{ll}]^{t } b_{k}^{t } j_{21})m_{2;t } ; \\nonumber \\\\ & m_{1;t}\\leftarrow ( [ \\mathtt{ur } ] b_{k } j_{11})m_{1;t } ; \\nonumber \\\\ & m_{2;t}\\leftarrow ( [ \\mathtt{ur } ] b_{k } j_{11})m_{2;t } ; \\nonumber\\end{aligned}\\ ] ]    the above procedure , shown in  ( [ eqn : maps_c])-([eqn : maps_l ] ) , for modifying the matrix maps can be recursively repeated for each of the combining stages beginning with the lowest level of combining the individual sub - matrix inverses .    on completion the maps",
    "can then be used to generate the block tridiagonal entries of @xmath33 .",
    "this subsequently allows for the computation of the generator sequences for @xmath33 , via the relationships shown in  ( [ eqn : comp_ratio ] ) , in a purely distributed fashion .",
    "the time complexity of the algorithm presented is @xmath66 , with memory consumption @xmath67 .",
    "the first term @xmath232 in the computational complexity arises from the embarrassingly parallel nature of both determining the generator sequences and applying the matrix maps to update the block tridiagonal portion of the inverse .",
    "the second term @xmath233 is dependent on the number of levels needed to gather combining information for @xmath44 sub - matrix inverses .",
    "similarly , the first term in the memory complexity is due to the generator sequences and diagonal blocks , and the second represents the memory required for the matrix maps of each sub - problem governed ."
  ],
  "abstract_text": [
    "<S> through the non - equilibrium green s function ( negf ) formalism , quantum - scale device simulation can be performed with the inclusion of electron - phonon scattering . </S>",
    "<S> however , the simulation of realistically sized devices under the negf formalism typically requires prohibitive amounts of memory and computation time . </S>",
    "<S> two of the most demanding computational problems for negf simulation involve mathematical operations with structured matrices called semiseparable matrices . in this work , </S>",
    "<S> we present parallel approaches for these computational problems which allow for efficient distribution of both memory and computation based upon the underlying device structure . </S>",
    "<S> this is critical when simulating realistically sized devices due to the aforementioned computational burdens . </S>",
    "<S> first , we consider determining a distributed compact representation for the retarded green s function matrix @xmath0 . </S>",
    "<S> this compact representation is exact and allows for any entry in the matrix to be generated through the inherent semiseparable structure . </S>",
    "<S> the second parallel operation allows for the computation of electron density and current characteristics for the device . </S>",
    "<S> specifically , matrix products between the distributed representation for the semiseparable matrix @xmath0 and the self - energy scattering terms in @xmath1 produce the less - than green s function @xmath2 . as an illustration of the computational efficiency of our approach </S>",
    "<S> , we stably generate the mobility for nanowires with cross - sectional sizes of up to @xmath3 nm , assuming an atomistic model with scattering . </S>"
  ]
}