{
  "article_text": [
    "max - margin learning has been effective on learning discriminative models , with many examples such as univariate - output support vector machines ( svms )  @xcite and multivariate - output max - margin markov networks ( or structured svms )  @xcite .",
    "however , the ever - increasing size of complex data makes it hard to construct such a fully discriminative model , which has only single layer of adjustable weights , due to the facts that : ( 1 ) the manually constructed features may not well capture the underlying high - order statistics ; and ( 2 ) a fully discriminative approach can not reconstruct the input data when noise or missing values are present .    to address the first challenge ,",
    "previous work has considered incorporating latent variables into a max - margin model , including partially observed maximum entropy discrimination markov networks  @xcite , structured latent svms  @xcite and max - margin min - entropy models  @xcite .",
    "all this work has primarily focused on a shallow structure of latent variables . to improve the flexibility , learning",
    "svms with a deep latent structure has been presented in  @xcite .",
    "however , these methods do not address the second challenge , which requires a generative model to describe the inputs .",
    "the recent work on learning max - margin generative models includes max - margin harmoniums  @xcite , max - margin topic models  @xcite , and nonparametric bayesian latent svms  @xcite which can infer the dimension of latent features from data .",
    "however , these methods only consider the shallow structure of latent variables , which may not be flexible enough to describe complex data .",
    "much work has been done on learning generative models with a deep structure of nonlinear hidden variables , including deep belief networks  @xcite , autoregressive models  @xcite , and stochastic variations of neural networks  @xcite . for such models , inference is a challenging problem , but fortunately there exists much recent progress on stochastic variational inference algorithms  @xcite .",
    "however , the primary focus of deep generative models ( dgms ) has been on unsupervised learning , with the goals of learning latent representations and generating input samples .",
    "though the latent representations can be used with a downstream classifier to make predictions , it is often beneficial to learn a joint model that considers both input and response variables .",
    "one recent attempt is the conditional generative models  @xcite , which treat labels as conditions of a dgm to describe input data .",
    "this conditional dgm is learned in a semi - supervised setting , which is not exclusive to ours . in this paper",
    ", we revisit the max - margin principle and present a max - margin deep generative model ( mmdgm ) , which learns multi - layer representations that are good for both classification and input inference .",
    "our mmdgm conjoins the flexibility of dgms on describing input data and the strong discriminative ability of max - margin learning on making accurate predictions .",
    "we formulate mmdgm as solving a variational inference problem of a dgm regularized by a set of max - margin posterior constraints , which bias the model to learn representations that are good for prediction .",
    "we define the max - margin posterior constraints as a linear functional of the target variational distribution of the latent presentations .",
    "then , we develop a doubly stochastic subgradient descent algorithm , which generalizes the pagesos algorithm  @xcite to consider nontrivial latent variables . for the variational distribution ,",
    "we build a recognition model to capture the nonlinearity , similar as in  @xcite .",
    "we consider two types of networks used as our recognition and generative models : multiple layer perceptrons ( mlps ) as in  @xcite and convolutional neural networks ( cnns )  @xcite .",
    "though cnns have shown promising results in various domains , especially for image classification , little work has been done to take advantage of cnn to generate images . the recent work  @xcite presents a type of cnn to map manual features including class labels to rbg chair images by applying unpooling , convolution and rectification sequentially ; but it is a deterministic mapping and there is no random generation .",
    "generative adversarial nets  @xcite employs a single such layer together with mlps in a minimax two - player game framework with primary goal of generating images .",
    "we propose to stack this structure to form a highly non - trivial deep generative network to generate images from latent variables learned automatically by a recognition model using standard cnn .",
    "we present the detailed network structures in experiments part .",
    "empirical results on mnist  @xcite and svhn  @xcite datasets demonstrate that mmdgm can significantly improve the prediction performance , which is competitive to the state - of - the - art methods  @xcite , while retaining the capability of generating input samples and completing their missing values .",
    ": the recent work  @xcite presents a convolutional neural networks conditioned on class labels with the primary goal of generating 3d images instead of predicting class labels .",
    "regularized bayesian inference ( regbayes )  @xcite presents a generic framework for constrained bayesian inference , from which our work draws inspirations ; but no existing work has attempted to learn a multi - layer representations with highly nonlinear transformations .",
    "inspired by the power of dgms , it s natural to use these representations , or features , to do some supervised tasks , such as classification , regression and so on .",
    "recently , dgms are used in semi - supervised learning @xcite .",
    "two kinds of models were proposed : the latent - feature discriminative model and the conditional generative model .",
    "latent - feature discriminative model trains the generative model and prediction model independently .",
    "conditional generative model treats labels as latent class variables that can be missing .",
    "however , we want to deal with supervised learning problems with full labelled data and learn prediction - oriented deep features through dgms .",
    "we propose to apply the regbayes @xcite framework to dgms to do maximum margin learning with features learned by dgms .",
    "often , regbayes introduces linear operator on the posterior distribution as the regularization term to capture the structures of data .",
    "regbayes has successful applications in several domains , such as topic models , i.e. medlda @xcite .",
    "medlda is a concrete example to learn predictive features via a generative model .",
    "medlda employs maximum entropy discrimination ( med ) principle @xcite to combine max - margin prediction models with hierarchical bayesian models .",
    "we marry the large margin idea , which leads to maximum margin supervised learning with dgms .",
    "our model will be presented in section 2 .",
    "we design global coordinate decent inference algorithm to learn for both latent structures and global parameters . to deal with expectations in the objective function , we use the stochastic variational methods raised recently @xcite . to handle large scale dataset , we uses pagesos @xcite , a stochastic subgradient svm solver instead of batch svm to obtain a complete stochastic inference algorithm .",
    "the details are shown in section 3 .",
    "our experiments show two benefits of our model : firstly it can explain the data well and secondly it leads to more accurate prediction in supervised tasks .",
    "the rest of the paper is organized as follows .",
    "2 reviews the basics of deep generative models .",
    "3 presents the max - margin deep generative models , with a doubly stochastic subgradient algorithm .",
    "sec . 4 presents experimental results .",
    "finally , sec .",
    "5 concludes .",
    "we start from a general setting , where we have @xmath0 i.i.d .",
    "data @xmath1 . a deep generative model ( dgm )",
    "assumes that each @xmath2 is generated from a vector of latent variables @xmath3 , which itself follows some distribution .",
    "the joint probability of a dgm is as follows : @xmath4 where @xmath5 is the prior of the latent variables and @xmath6 is the likelihood model for generating observations . for notation simplicity ,",
    "we define @xmath7 . depending on the structure of @xmath8 ,",
    "various dgms have been developed , such as the deep belief networks  @xcite , deep sigmoid networks  @xcite , deep latent gaussian models  @xcite , and deep autoregressive models  @xcite . in this paper , we focus on the directed dgms , which can be easily sampled from via an ancestral sampler",
    ".    however , in most cases learning dgms is challenging due to the intractability of posterior inference .",
    "the state - of - the - art methods resort to stochastic variational methods under the maximum likelihood estimation ( mle ) framework , @xmath9 .",
    "specifically , let @xmath10 be the variational distribution that approximates the true posterior @xmath11 .",
    "firstly , we estimate the partial marginal likelihood of the model , i.e. @xmath12 . since the posterior distribution @xmath13 is intractable in general , we can introduce a variational distribution @xmath14 to approximate the true posterior distribution . a variational upper bound of the per data point negative log - likelihood ( nll ) @xmath15 is : @xmath16 , \\end{aligned}\\ ] ] which is the same as auto - encoding variational bayes @xcite .",
    "then , a lower bound of the whole log - likelihood , @xmath17 , can be obtained as : @xmath18 a variational upper bound of the per sample negative log - likelihood ( nll ) @xmath19 is : @xmath20 , \\end{aligned}\\ ] ] where @xmath21 is the kullback - leibler ( kl ) divergence between distributions @xmath22 and @xmath23 . then , @xmath24 upper bounds the full negative log - likelihood @xmath25 .",
    "it is important to notice that if we do not make restricting assumption on the variational distribution @xmath22 , the lower bound is tight by simply setting @xmath26 .",
    "that is , the mle is equivalent to solving the variational problem : @xmath27 .",
    "however , since the true posterior is intractable except a handful of special cases , we must resort to approximation methods .",
    "one common assumption is that the variational distribution is of some parametric form , @xmath28 , and then we optimize the variational bound w.r.t the variational parameters @xmath29 . for dgms",
    ", another challenge arises that the variational bound is often intractable to compute analytically . to address this challenge",
    ", the early work further bounds the intractable parts with tractable ones by introducing more variational parameters  @xcite .",
    "however , this technique increases the gap between the bound being optimized and the log - likelihood , potentially resulting in poorer estimates .",
    "much recent progress  @xcite has been made on hybrid monte carlo and variational methods , which approximates the intractable expectations and their gradients over the parameters @xmath30 via some unbiased monte carlo estimates .",
    "furthermore , to handle large - scale datasets , stochastic optimization of the variational objective can be used with a suitable learning rate annealing scheme .",
    "it is important to notice that variance reduction is a key part of these methods in order to have fast and stable convergence .",
    "it is important to notice that if we do not make restricting assumption on the variational distribution @xmath22 , the lower bound is tight simply by setting @xmath31 .",
    "that is , the maximum likelihood estimation problem is equivalent to solving the variational problem @xmath32 however , since the true posterior is intractable , we must resort to approximation methods . for instance , we can assume a parametric form of the variational distribution @xmath33 , and then maximize the variational bound under this assumption .",
    "most work on directed dgms has been focusing on the generative capability on inferring the observations , such as filling in missing values  @xcite , while little work has been done on investigating the predictive power , except the semi - supervised dgms  @xcite which builds a dgm conditioned on the class labels and learns the parameters via mle .",
    "below , we present max - margin deep generative models , which explore the discriminative max - margin principle to improve the predictive ability of the latent representations , while retaining the generative capability .",
    "we consider supervised learning , where the training data is a pair @xmath34 with input features @xmath35 and the ground truth label @xmath36 . without loss of generality , we consider the multi - class classification , where @xmath37 .",
    "a max - margin deep generative model ( mmdgm ) consists of two components : ( 1 ) a deep generative model to describe input features ; and ( 2 ) a max - margin classifier to consider supervision . for the generative model , we can in theory adopt any dgm that defines a joint distribution over @xmath38 as in  eq .",
    "( [ eq : dgm - joint - dist ] ) . for the max - margin classifier , instead of fitting the input features into a conventional svm",
    ", we define the linear classifier on the latent representations , whose learning will be regularized by the supervision signal as we shall see .",
    "specifically , if the latent representation @xmath8 is given , we define the latent discriminant function @xmath39 where @xmath40 is an @xmath41-dimensional vector that concatenates @xmath42 subvectors , with the @xmath36th being @xmath8 and all others being zero , and @xmath43 is the corresponding weight vector .",
    "we consider the case that @xmath43 is a random vector , following some prior distribution @xmath44 .",
    "then our goal is to infer the posterior distribution @xmath45 , which is typically approximated by a variational distribution @xmath46 for computational tractability .",
    "notice that this posterior is different from the one in the vanilla dgm .",
    "we expect that the supervision information will bias the learned representations to be more powerful on predicting the labels at testing . to account for the uncertainty of @xmath47 , we take the expectation and define the discriminant function @xmath48 , $ ] and the final prediction rule that maps inputs to outputs is : @xmath49 note that different from the conditional dgm  @xcite , which puts the class labels upstream , the above classifier is a downstream model , in the sense that the supervision signal is determined by conditioning on the latent representations .",
    "we use a linear predictor on a test data @xmath50 : @xmath51 since @xmath52 and @xmath43 are latent variables , the predictor on each label should be integrated over @xmath53 and @xmath54 : @xmath55 \\cdot \\mathbb{e}_{q({\\mathbf{z}}'^{(n)})}[{\\mathbf{f}}(y , { \\mathbf{z}}'^{(n)})]\\ ] ] the final prediction is as following : @xmath56      since we have fully labelled data when training the model , we can estimate the marginal likelihood i.e. @xmath57 directly.formally , we incorporate the labels into the variational inference , for each datapoint , we have : @xmath58 here we assume that @xmath59 is derived from @xmath60 by optimization , which will be formulated in later section .",
    "we want to jointly learn the parameters @xmath61 and infer the posterior distribution @xmath46 .",
    "based on the equivalent variational formulation of mle , we define the joint learning problem as solving : @xmath62 \\ge \\delta l_n(y ) -",
    "\\xi_n\\\\ \\xi_n \\ge 0 , \\end{array } \\right . \\nonumber\\end{aligned}\\ ] ] where @xmath63 is the difference of the feature vectors ; @xmath64 is the loss function that measures the cost to predict @xmath36 if the true label is @xmath65 ; and @xmath66 is a nonnegative regularization parameter balancing the two components . in the objective , the variational bound is defined as @xmath67 $ ] , and the margin constraints are from the classifier  ( [ eq : mm - classifier ] ) .",
    "if we ignore the constraints ( e.g. , setting @xmath66 at 0 ) , the solution of @xmath46 will be exactly the bayesian posterior , and the problem is equivalent to do mle for @xmath61 .      by applying the maximum entropy discrimination ( med ) @xcite principle , we combine the deep generative models and max - margin classification together as an optimization problem : @xmath68 @xmath69 \\ge \\delta l_n(y ) -",
    "\\xi_n\\\\ \\xi_n \\ge 0 , \\end{array } \\right.\\ ] ] by absorbing the slack variables , we can rewrite the problem in an unconstrained form : @xmath70 where the hinge loss is : @xmath71 ) .",
    "$ ] due to the convexity of @xmath72 function , it is easy to verify that the hinge loss is an upper bound of the training error of classifier  ( [ eq : mm - classifier ] ) , that is , @xmath73 . furthermore , the hinge loss is a convex functional over the variational distribution because of the linearity of the expectation operator .",
    "these properties render the hinge loss as a good surrogate to optimize over .",
    "previous work has explored this idea to learn discriminative topic models  @xcite , but with a restriction on the shallow structure of hidden variables .",
    "our work presents a significant extension to learn deep generative models , which pose new challenges on the learning and inference .",
    "consequently , we can use subgradient method , i.e. pagesos @xcite , to optimize the primal problem directly in a stochastic manner .",
    "overall , our goal is to optimize parameters @xmath74 and the posterior distribution of @xmath43 .",
    "namely , we can do three things in a whole model : generating data or filling missing values , learning more useful latent representation in supervised learning and predicting when new unlabelled data is observed .      the variational formulation of problem  ( [ eq : joint - problem ] ) naturally suggests that we can develop a variational algorithm to address the intractability of the true posterior .",
    "we now present a new algorithm to solve problem  ( [ eq : joint - problem ] ) .",
    "our method is a doubly stochastic generalization of the pegasos ( i.e. , primal estimated sub - gradient solver for svm ) algorithm  @xcite for the classic svms with fully observed input features , with the new extension of dealing with a highly nontrivial structure of latent variables .",
    "first , we make the structured mean - field ( smf ) assumption that @xmath75 . under the assumption",
    ", we have the discriminant function as @xmath76 = { \\mathbb{e}}_{q({\\boldsymbol{\\eta}})}[{\\boldsymbol{\\eta}}^{\\top } ]   { \\mathbb{e}}_{q_{{\\boldsymbol{\\phi}}}({\\mathbf{z}}^{(n)})}[\\delta { \\mathbf{f}}_n(y)].$ ] moreover , we can solve for the optimal solution of @xmath53 in some analytical form .",
    "in fact , by the calculus of variations , we can show that given the other parts the solution is @xmath77 \\big ) , $ ] where @xmath78 are the lagrange multipliers ( see  @xcite for details ) .",
    "if the prior is normal , @xmath79 , we have the normal posterior : @xmath80 .",
    "$ ] therefore , even though we did not make a parametric form assumption of @xmath53 , the above results show that the optimal posterior distribution of @xmath43 is gaussian .",
    "since we only use the expectation in the optimization problem and in prediction , we can directly solve for the mean parameter @xmath81 instead of @xmath53 .",
    "further , in this case we can verify that @xmath82 and then the equivalent objective function in terms of @xmath81 can be written as : @xmath83 where @xmath84 is the total hinge loss , and the per - sample hinge - loss is @xmath85)$ ] .",
    "below , we present a doubly stochastic subgradient descent algorithm to solve this problem .",
    "the _ first stochasticity _ arises from a stochastic estimate of the objective by random mini - batches . specifically , the batch learning needs to scan the full dataset to compute subgradients , which is often too expensive to deal with large - scale datasets .",
    "one effective technique is to do stochastic subgradient descent  @xcite , where at each iteration we randomly draw a mini - batch of the training data and then do the variational updates over the small mini - batch .",
    "formally , given a mini batch of size @xmath86 , we get an unbiased estimate of the objective : @xmath87 or equivalently : @xmath88 where we ignore the coefficient @xmath89 because we use rmsprop to do optimization , which is scale - invariant .",
    "the _ second stochasticity _ arises from a stochastic estimate of the per - sample variational bound and its subgradient , whose intractability calls for another monte carlo estimator .",
    "formally , let @xmath90 be a set of samples from the variational distribution , where we explicitly put the conditions . then , an estimate of the per - sample variational bound and the per - sample hinge - loss is @xmath91 where @xmath92 . note that @xmath93 is an unbiased estimate of @xmath94 , while @xmath95 is a biased estimate of @xmath96 .",
    "nevertheless , we can still show that @xmath95 is an upper bound estimate of @xmath96 under expectation .",
    "furthermore , this biasedness does not affect our estimate of the gradient .",
    "in fact , by using the equality @xmath97 , we can construct an unbiased monte carlo estimate of @xmath98 as : @xmath99 where the last term roots from the hinge loss with the loss - augmented prediction @xmath100 . for @xmath61 and @xmath81 , the estimates of the gradient @xmath101 and the subgradient @xmath102 are easier , which are : @xmath103 notice that the sampling and the gradient @xmath104 only depend on the variational distribution , not the underlying model .",
    "r0.58    initialize @xmath61 , @xmath81 , and @xmath29 draw a random mini - batch of @xmath86 data points draw random samples from noise distribution @xmath105 compute subgradient @xmath106 update parameters @xmath107 using subgradient @xmath108 .",
    "@xmath61 , @xmath81 , and @xmath29    the above estimates consider the general case where the variational bound is intractable . in some cases",
    ", we can compute the kl - divergence term analytically , e.g. , when the prior and the variational distribution are both gaussian . in such cases ,",
    "we only need to estimate the rest intractable part by sampling , which often reduces the variance  @xcite .",
    "similarly , we could use the expectation of the features directly , if it can be computed analytically , in the computation of subgradients ( e.g. , @xmath109 and @xmath110 ) instead of sampling , which again can lead to variance reduction .",
    "initialize @xmath61 , @xmath81 , and @xmath29 draw a random mini - batch of @xmath86 data points draw random samples from noise distribution @xmath105 compute subgradient @xmath106 update parameters @xmath107 using subgradient @xmath108 .",
    "@xmath61 , @xmath81 , and @xmath29    with the above estimates of subgradients , we can use stochastic optimization methods such as sgd  @xcite and adam  @xcite to update the parameters , as outlined in alg .",
    "[ alg : double - stochastic ] .",
    "overall , our algorithm is a doubly stochastic generalization of pegasos to deal with the highly nontrivial latent variables .",
    "now , the remaining question is how to define an appropriate variational distribution @xmath111 to obtain a robust estimate of the subgradients as well as the objective .",
    "two types of methods have been developed for unsupervised dgms , namely , variance reduction  @xcite and auto - encoding variational bayes ( avb )  @xcite .",
    "though both methods can be used for our models , we focus on the avb approach . for continuous variables @xmath112 , under certain mild conditions we can reparameterize the variational distribution @xmath111 using some simple variables @xmath113 .",
    "specifically , we can draw samples @xmath113 from some simple distribution @xmath105 and do the transformation @xmath114 to get the sample of the distribution @xmath115 .",
    "we refer the readers to  @xcite for more details . in our experiments",
    ", we consider the special gaussian case , where we assume that the variational distribution is a multivariate gaussian with a diagonal covariance matrix : @xmath116 whose mean and variance are functions of the input data .",
    "this defines our recognition model .",
    "then , the reparameterization trick is as follows : we first draw standard normal variables @xmath117 and then do the transformation @xmath118 to get a sample . for simplicity",
    ", we assume that both the mean and variance are function of @xmath119 only .",
    "however , it is worth to emphasize that although the recognition model is unsupervised , the parameters @xmath29 are learned in a supervised manner because the subgradient  ( [ eq : var - grad ] ) depends on the hinge loss .",
    "further details of the experimental settings are presented in sec .",
    "there are different ways to parameterize the variational distribution :    1 .",
    "* supervised mlp * : we assume that @xmath120 , where the unsupervised part @xmath121 can be defined as in the mlp case and @xmath122 is some function that takes the supervision into account .",
    "for example , section 3.4.3 provides an example .",
    "* class - specific mlp * : a simple way to consider the supervision is to define a class - specific mlp , one for each category .",
    "that is , we assume that @xmath123 , where the class - specific mean and variance can be defined as a mlp as in the first case .",
    "* semi - parametric * : the above three cases all assume an analytical form of the mean and variance , which may restrict the choices . one possibly flexible way is semi - parametric .",
    "specifically , we assume that @xmath124 , where @xmath125 is the representation given by an arbitrary model that best fits the training data @xmath126 , and @xmath127 is the residual part that can be flexibly defined as a mlp .",
    "since @xmath53 is independent from the variational bound , when we optimize @xmath53 given other parameters and latent variables , our objective function is equivalent to : @xmath128 \\mathbb{e } [ \\delta { \\mathbf{f}}_n(y ) ] ) \\end{split}\\ ] ] by calculus of variations , we can find that the optimal solution is : @xmath129)\\ ] ] where @xmath130 \\mathbb{e } [ { \\mathbf{f}}(y , { \\mathbf{z}}_n)])$ ] is the loss - augmented prediction . if the prior is normal , @xmath79 , we have the normal posterior : @xmath131\\ ] ] therefore",
    ", we know that the optimal posterior distribution of @xmath43 is gaussian . since we only use the expectation in optimization of all parameters and in prediction , we can optimize @xmath81 instead of @xmath53 actually .",
    "further , in this case we can verify that @xmath82 and then the equivalent objective function in terms of @xmath81 can be written as : @xmath83 where @xmath84 and @xmath132)$ ] .    by fixing the other parts and ignoring the irrelevant terms",
    ", this substep involves solving the problem : @xmath133 which indicates that the optimization algorithm is the same as the underlying generative models .",
    "we can also remove the irrelevant parts in the objective function and get : @xmath134 the subgradient should be : @xmath135)\\ ] ] where @xmath136 =   { \\mathbb{e } } [ { \\mathbf{f}}(y_n , { \\mathbf{z}}_n ) - { \\mathbf{f}}(\\hat{y}_n , { \\mathbf{z}}_n)]$ ] and @xmath137)$ ] .",
    "the subgradient of parameters @xmath29 is non - trivial to get directly , because @xmath29 always appear as the parameters of certain distribution in expectation form , i.e. @xmath138 $ ] .    instead of using traditional mean filed method , an alternative way is to take advantage of the stochastic variational methods raised recently @xcite to do the inference . under ceratin mild conditions mentioned in these papers ,",
    "the value of the objective function as well as the gradient can be easily computed by sampling from simple distribution .",
    "specifically , introduce an auxiliary random variable @xmath113 and a differentiable function @xmath139 such that : @xmath140    then the expectation can be approximated by : @xmath141 = \\mathbb{e}_{p({\\boldsymbol{\\epsilon}})}[f(g_{{\\boldsymbol{\\phi}}}({\\boldsymbol{\\epsilon } } , { \\mathbf{x } } ) ) ] \\\\ \\simeq & \\frac{1}{l } \\sum_{l = 1}^l f(g_{{\\boldsymbol{\\phi}}}({\\boldsymbol{\\epsilon } } , { \\mathbf{x } } ) ) \\textrm { where } { \\boldsymbol{\\epsilon}}\\sim p ( { \\boldsymbol{\\epsilon } } ) \\end{split}\\ ] ] consequently , the gradient of the variational bound part and the subgradient of the empirical loss part can be estimated efficiently by sampling .      given @xmath60 , we solve another regbayes problem to inference @xmath59 as following : @xmath142 ) \\end{split}\\ ] ]    the close form solution is : @xmath143 where @xmath144 and @xmath59 can be solved alternatively .    firstly fix @xmath59 and compute @xmath145||^2}\\ ] ] where @xmath146)$ ] .",
    "secondly fix @xmath144 , and update @xmath59 .",
    "if @xmath147 , where @xmath148 is a diagonal matrix .",
    "we have : @xmath149 where @xmath150 is the vector of diagonal element of @xmath148 and @xmath151 means element - wise product .",
    "the relevant part of @xmath81 in the objective function is : @xmath152)\\ ] ]    we uses pegasos @xcite , where an adaptive learning rate is predefined , to optimize @xmath81 .",
    "however , we use adagrad instead of the pre - fixed learning rate .",
    "the objective function is equivalent to @xmath153)\\ ] ]    the subgradient of @xmath81 is : @xmath154\\ ] ] where @xmath155 =   \\mathbb{e } [ { \\mathbf{f}}(y_n , { \\mathbf{z}}_n ) - { \\mathbf{f}}({\\hat{y}}_n , { \\mathbf{z}}_n)]$ ] and @xmath156)$ ] .",
    "@xmath157)$ ] can be estimated by sampling method mentioned in section 3.2.2 .",
    "our model is flexible because of the untied weight and any successful discriminative model structures could be used as our encoders .",
    "unconvnets in @xcite is used to map hand - designed features to rbg images .",
    "similar structures is used to generate images in gan , begio in a different framework while it concentrates on generative performance .",
    "understanding cnn used this structure to visualize features , no training procedure .",
    "we now present experimental results on the widely adopted mnist @xcite and svhn @xcite datasets . though mmdgms are applicable to any dgms that define a joint distribution of @xmath158 and @xmath112 , we concentrate on the variational auto - encoder ( va )  @xcite , which is unsupervised .",
    "we denote our mmdgm with va by mmva . in our experiments , we consider two types of recognition models : multiple layer perceptrons ( mlps ) and convolutional neural networks ( cnns ) .",
    "we implement all experiments based on theano  @xcite .",
    "in the mlp case , we follow the settings in  @xcite to compare both generative and discriminative capacity of va and mmva . in the cnn case",
    ", we use standard convolutional nets  @xcite with convolution and max - pooling operation as the recognition model to obtain more competitive classification results . for the generative model ,",
    "we use unconvnets  @xcite with a `` symmetric '' structure as the recognition model , to reconstruct the input images approximately .",
    "more specifically , the top - down generative model has the same structure as the bottom - up recognition model but replacing max - pooling with unpooling operation  @xcite and applies unpooling , convolution and rectification in order .",
    "the total number of parameters in the convolutional network is comparable with previous work  @xcite . for simplicity",
    ", we do not involve mlpconv layers  @xcite and contrast normalization layers in our recognition model , but they are not exclusive to our model .",
    "we illustrate details of the network architectures in appendix a.    in both settings , the mean and variance of the latent @xmath8 are transformed from the last layer of the recognition model through a linear operation . it should be noticed that we could use not only the expectation of @xmath8 but also the activation of any layer in the recognition model as features .",
    "the only theoretical difference is from where we add a hinge loss regularization to the gradient and back - propagate it to previous layers . in all of the experiments , the mean of @xmath8",
    "has the same nonlinearity but typically much lower dimension than the activation of the last layer in the recognition model , and hence often leads to a worse performance . in the mlp case ,",
    "we concatenate the activations of 2 layers as the features used in the supervised tasks . in the cnn case",
    ", we use the activations of the last layer as the features .",
    "we use adam  @xcite to optimize parameters in all of the models .",
    "although it is an adaptive gradient - based optimization method , we decay the global learning rate by factor three periodically after sufficient number of epochs to ensure a stable convergence .",
    "we denote our mmdgm with mlps by * mmva*. to perform classification using va , we first learn the feature representations by va , and then build a linear svm classifier on these features using the pegasos stochastic subgradient algorithm  @xcite .",
    "this baseline will be denoted by * va+pegasos*. the corresponding models with cnns are denoted by * cmmva * and * cva+pegasos * respectively .",
    "we now present our experimental results on the widely adopted mnist and its variant datasets for handwritten digits recognition . since the goal of our model is to learn latent features that both explain the data well and are more discriminative in supervised tasks",
    ", we examine the results with two types of criteria , i.e. , the variational lower bound and prediction error rates . though mmdgms are applicable to any deep generative models that define a joint distribution of @xmath158 and @xmath112 , we concentrate on the variational auto - encoder ( va )  @xcite , which is unsupervised .",
    "we denote our mmdgm with va by * mmva*. to perform classification using va , we first learn the feature representations by va , and then build a linear svm classifier on these features using the pegasos stochastic subgradient algorithm  @xcite .",
    "this baseline will be denoted by * va+pegasos*. to have a fair competition , all of these models are under same corresponding parameters .",
    "there are three sets of parameters in our model : parameters in generative model and recognition model , parameters in prediction model and parameters that control the relative weight of generative part and discriminative part .",
    "typically , we choose the first two sets of parameters following related works .",
    "if we fine tune the parameters according to the validation result , we will state it explicitly .    for the recognition model in defining the variational distribution  ( [ eq : recognition - model ] ) , we follow the settings in @xcite and employ a two - layer mlp in both va and mmva .",
    "the mean and variance of the latent @xmath8 are transformed from the last layer of the recognition model through a linear operation .",
    "it should be noticed that we could use not only the expectation of @xmath8 but also the activation of any layer in the recognition model as features .",
    "the only theoretical difference is from where we add a hinge loss regularization to the gradient and back - propagate it to previous layers . in the experiments of va and mmva ,",
    "the mean of @xmath8 has the same nonlinearity but typically much lower dimension than the activation of the last layer in the recognition model , and hence often leads to a worse performance . to obtain the best empirical results , we add label information explicitly in both layers of the recognition model , i.e. , we concatenate the activations of 2 layers as the features used in the supervised tasks .",
    "we present both the prediction performance and the results on generating samples of mmva and va+pegasos with both kinds of recognition models on the mnist  @xcite dataset , which consists of images of 10 different classes ( 0 to 9 ) of size @xmath159 with 50,000 training samples , 10,000 validating samples and 10,000 testing samples .      to take advantages of bayesian approach , instead of using the zero - mean standard gaussian prior",
    ", we introduce an informative gaussian prior in the training procedure to refine the posterior distribution . in this",
    "setting , the mean vectors , which could be pre - extracted from the training data by any other discriminative models , are different for individual data points to explain different digits and writing styles .",
    "notice that we do not use the informative prior of the testing data when we do classification since it is irrelevant to the computation of the posterior distribution of testing data . in our experiments , we use features learned by a well - tuned deep convolutional network from an unpublished work as the mean of the prior for both va and mmva .",
    "the network has 5 convolutional layers and 2 max - pooling layers , and the features could achieve test error rate 0.3% on the standard mnist data .",
    "we still use the identity matrix as the covariance to remain the generative capability .",
    "in the zero mean case , we set the dimension of the latent variables to 50 following  @xcite to compare with va and the conditional generative va . in the case with an informative prior",
    ", we change the dimension of latent variables to 96 both for va and mmva to match the dimension of the prior .",
    "all of the mlps have 500 hidden units at each layer .",
    "we set the weight decay term of pegasos to @xmath160 in all of the models and train pegasos with 200,000 mini - batches of size 100 for all va models to ensure the convergence of the prediction weights .",
    "our model is not too sensitive to the choice of @xmath66 , which controls the relative weight of generative part and discriminative part .",
    "we set @xmath161 for all mmva models on standard mnist dataset .",
    "typically , our model has better performance with an unsupervised pre - training procedure because the hinge loss part makes sense only when we have reasonable features .",
    "the number of total epochs and corresponding global learning rate are the same for va and mmva ."
  ],
  "abstract_text": [
    "<S> deep generative models ( dgms ) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability . </S>",
    "<S> however , little work has been done on examining or empowering the discriminative ability of dgms on making accurate predictions . </S>",
    "<S> this paper presents max - margin deep generative models ( mmdgms ) , which explore the strongly discriminative principle of max - margin learning to improve the discriminative power of dgms , while retaining the generative capability . </S>",
    "<S> we develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective . </S>",
    "<S> empirical results on mnist and svhn datasets demonstrate that ( 1 ) max - margin learning can significantly improve the prediction performance of dgms and meanwhile retain the generative ability ; and ( 2 ) mmdgms are competitive to the state - of - the - art fully discriminative networks by employing deep convolutional neural networks ( cnns ) as both recognition and generative models . </S>"
  ]
}