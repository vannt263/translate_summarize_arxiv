{
  "article_text": [
    "the rapid advance of sequencing technology and its falling cost are driving the use of next - generation sequencing ( ngs ) in a great variety of domains .",
    "the volume of data generated by ngs was projected to double every five months @xcite .",
    "processing raw data from a sequencer and translating it into usable insights in the timely manner is a formidable task .",
    "typical 50x coverage of whole genome sequencing ( wgs ) can easily generate up to  500 gb fastq files .",
    "the data processes on modern computers is still very time - consuming for such huge data sets . for typical 50x coverage of wgs data , the processing time for the broad s genome analysis toolkit ( gatk ) best practices pipeline @xcite from reference alignment to variant calling can take up to a day or days to finish @xcite .",
    "genome data analysis pipelines involve the data pre - processing steps before variant calling , which are necessary to achieve accurate variant calling .",
    "they scan the input data , analyze reads , and filter out reads that can affect accuracy .",
    "the pre - processing steps take fastq - format @xcite files as the input and produces a compressed binary file in the bam format @xcite , which are widely accepted as common file formats to represent aligned sequenced data .",
    "the calibrated bam file is then used in the variant discovery to identify the sites where the data displays variation relative to the reference genome .",
    "the pre - processing steps for sam parsing , sorting , duplicate marking , and bam file compression can take tens of hours for a wgs sam file .",
    "their total runtime dominates the whole pre - processing workflow ( explained in the next sub - section ) and is a clear performance bottleneck .",
    "the purpose of sam2bam is to improve the efficiency of this pre - processing through fully utilizing available cpus and memory .    the overall architecture of the tools",
    "should be redesigned so that the computer resources are fully utilized to significantly reduce the runtime of such data pre - processing steps ( e.g. , by 100x ) .",
    "current major software tools are single- or partially multi - threaded .",
    "partially multi - threaded tools usually have to wait for data generated from single - threaded components because every component ( single- or multi - threaded ) is executed one by one .",
    "therefore , they can not fully utilize multiple cpus that are available at all times .",
    "single - threaded components become bottlenecks in performance on multi - cpu systems . for example , suppose that 80% of the runtime is executed by multi - threads and the remaining time is executed by a single thread .",
    "speed - up for such tools by using multiple cpus is limited to 5x even if hundreds of cpus are available on the system .",
    "the sam2bam simultaneously executes functional components ( e.g. , file i / o , sam parsing , and data compression ) to achieve further speed - up on such a many - cpu system , instead of executing the components one by one in a big loop .",
    "the components are combined as a pipeline .",
    "in addition , most of the steps are multi - threaded .",
    "an appropriate number of cpus are allocated to each component so that no components become a bottleneck in the pipeline .",
    "the sam2bam can achieve more than 100x speed - up on a single node system with these redesigned framework for ngs data pre - processing .      to maximize the accuracy of variant discovery , pre - processing steps are necessary to prepare the data for analysis .",
    "pre - procesing is also recommanded in gatk best practices @xcite .",
    "pre - processing starts with fastq - format files and ends in a calibrated bam file . for the dna data ,",
    "pre - processing usually involves the following steps .    1 .",
    "* mapping the sequence reads to the reference genome * usually this step is done by bwa mem @xcite or other reference alighment tools .",
    "sam files are generated .",
    "2 .   * sorting the sequence reads based on coordinates * this step is usually done by picard sortsam or samtools sort .",
    "some tools that are used in the following steps such as picard markduplicates require the sorted input files .",
    "3 .   * marking duplicate alignments * this step is commonly done by picard markduplicates tool to remove the alignments of duplicate reads .",
    "performing local realignment around indels * this is usually done by gatk realignertargetcreator and indelrealigner tools to reduce artifacts produced in the regions around the indels .",
    "* recalibrating the base quality score * this step is usually done by gatk baserecalibrator and printreads tools to improve the accuracy of base quality scores which the variant calling step relies on .",
    "preprocessing is very time cosuming usually tens of hours for a wgs dataset and hours for a wex dataset . among the five steps of preprocessing , the sorting and duplicate marking steps take most of the runtime , usually ranging from 60 - 70% of the total pre - processing time , depending on the tools used and the test case size .",
    "therefore the sorting and duplicate marking steps are identified as a bottleneck of overall pre - processing steps , and sam2bam is focused on improving the performance of these two steps by redesigning the framework , paralleling most of the process and taking advantage of hardware compression when available .",
    "the design goal of sam2bam was to provide a high - throughput framework to process genome files at a rate of gigabytes per second ( gb / s ) .",
    "the framework consists of a data pipeline that converts the data format from sam to bam as outlined in figs [ arch - without - baminfo ] and [ arch - with - baminfo ] .",
    "data format conversion is divided into multiple steps .",
    "many of these steps are multi - threaded , while a few steps that order the data stream are single - threaded .",
    "more cpus are allocated for more complex steps so that such steps are not bottlenecks in performance .",
    "each step continuously processes data by using cpus as long as the data drives from the previous step . while sam2bam provides a data processing pipeline , it uses samtools / high - throughput sequencing library ( htslib ) @xcite for the data structures and utility functions for handling the sam and bam data formats .",
    "[ arch - without - baminfo ]    [ arch - with - baminfo ]      _ plug - in codes _ that analyze , filter , and modify data can be attached to sam2bam at runtime .",
    "we can develop the plug - in codes and run them on the high - throughput framework .",
    "there are three types of plug - in codes .",
    "* _ filter _ the filter plug - in code can be inserted as an additional step of the pipeline .",
    "it analyzes input data and determines if the data meets the criteria that the plug - in has .",
    "for example , with a filter plug - in that only includes read alignments that overlap a given region of the reference genome , the produced bam file only includes alignments that overlap the specified region . * _ accelerator _ the accelerator plug - in code improves the target pipeline step by using hardware accelerators , such as field - programmable gate arrays ( fpgas ) .",
    "compression acceleration is currently supported in sam2bam .",
    "it produces a compressed bam file using the standard compression library ( or zlib ) @xcite .",
    "if sam2bam detects an accelerator , it automatically offloads compression to hardware while carrying out compression with software .",
    "an accelerator that provides the same application programming interface ( api ) as zlib can be enabled by using the accelerator plug - in . * _ analyzer _ the analyzer plug - in code analyzes a set of read alignments and modifies them .",
    "if the analyzer plug - in code is attached to sam2bam , the latter runs the first half of the pipeline that parses data in the sam format and pools the alignment information in the system . how the alignment information is pooled will be explained in the next subsection (:",
    "items [ virtualizing ] and [ build_alignment_db ] ) .",
    "when all alignment information is pooled in the system , the analyzer plug - in scans the pooled information , analyzes it , and generates output on the basis of the analysis .",
    "when the analyzer plug - in has completed analysis , the pooled data are transferred to the second half of the pipeline to produce a compressed bam file .",
    "for example , with an analyzer plug - in for marking duplicate alignments , the alignments that were mapped to the same reference region are identified and are marked as duplicates except for the one that has the highest level of quality .",
    "the entire process of sam2bam conversion is split into functional stages such as file reading , sam line detection , and sam parsing .",
    "the stages run in parallel .",
    "also each stage can process multiple data blocks in parallel .    1 .",
    "[ sam_reading]*reading a sam file * data blocks ( e.g. , 64 kb each ) are read from a sam file , which is a sequence of text lines ( called sam lines ) .",
    "data blocks do not always end at the boundaries between sam lines .",
    "this step adjusts the boundaries between the blocks so that the new blocks end at the sam line boundaries by scanning each block from the end to the beginning until the first new - line character is found to enable multi - threads to process the blocks independently on the next step .",
    "[ line_splitting]*splitting a block into sam lines * sam lines are extracted from the data blocks transferred from the previous stage .",
    "the data blocks are scanned from the beginning to the end to find new - line characters , which separate sam lines . a major effort in this stage",
    "is to find these characters by using a function of the standard c - language library ( memchr ) .",
    "the performance of the function is optimized by using vector instructions , if they are available @xcite .",
    "a scanned data block is transferred to the next pipeline stage , which parses sam lines with the positions where sam lines start .",
    "[ sam_parsing]*parsing sam lines * binary alignment records are created from the sam lines .",
    "the sam2bam locates a sam line in the data block at each line position calculated by the previous stage , parses the sam line by using a library function of samtools ( sam_parse1 ) , and creates a binary alignment record in the data format used in samtools ( bam1_t ) .",
    "the binary alignment record contains the same contents as the corresponding sam / bam alignment .",
    "each binary record has a global sequence number , which is created on the basis of the block number and a local number of the record within the block .",
    "this global sequence number is used when analysis tools need to know which record has appeared first in the input file for a given set of records .",
    "[ virtualizing]*virtualizing binary alignment records * the sam2bam supports two modes : all the binary alignment records are placed in the main memory ( _ memory mode _ ) or in external storage ( _ storage mode _ ) .",
    "if there is sufficient memory , the memory mode is recommended to be run to obtain the best performance ; otherwise , the storage mode can be used .",
    "the binary alignment records are moved to either the main memory or the external storage in this step .",
    "the binary alignment records are managed in the later stages in the virtual space of sam2bam .",
    "a virtual address is assigned to each binary alignment record .",
    "a virtual address can be translated into either a main memory address or an offset of the file in external storage .",
    "+ if sam2bam is only invoked with plug - ins that access streaming input data but not pooled information , the input data are transferred from this step to the last step [ write_compressed_bam_file ] . in that case , only the memory mode is enabled .",
    "also , steps [ build_alignment_db ] and [ analyze_baminfo ] are skipped .",
    "[ build_alignment_db]*building alignment database * some plug - ins can start analysis after all the input data are read .",
    "an alignment database is built for such plug - ins to provide pooled information on the alignments in this step .",
    "each database entry is represented by a data record called _ baminfo _ , which is unique to sam2bam .",
    "baminfo is created from each binary alignment record but it only contains information that can be used by the analyzer plug - ins in step [ analyze_baminfo ] .",
    "it does not include long string data , including reference sequence names , concise idiosyncratic gapped alignment report ( cigar ) , sequences , base qualities , or optional fields . if any analyzer plug - in needs any of the omitted data in step [ analyze_baminfo ] , the plug - in summarizes the data and saves them into baminfo when creating baminfo .",
    "+ the baminfo records are arranged in the reference genome position space to construct the alignment database .",
    "the baminfo records can be looked up from the alignment database by using the mapping positions of clipped sequences .",
    "the baminfo records that have the same mapping position are gathered and are found in the database by a single lookup .",
    "if there is an analysis that needs the functionality of looking up the baminfo records by using an unclipped mapping position , the baminfo records are arranged by using the unclipped position as well as the clipped position .",
    "[ analyze_baminfo]*analyzing baminfo records * the plug - in code performs any analysis by using information in the alignment database .",
    "duplicate marking is an example of such code .",
    "multi - threading can accelerate this step by decomposing the input space into sub spaces .",
    "for example , we can split the data set of the alignment database into @xmath0 blocks and allocate @xmath1 blocks to each of the @xmath2 threads . 7 .",
    "[ write_compressed_bam_file]*writing a compressed bam file * this step groups the binary alignment records into  64-kb blocks ( bam blocks ) , compresses them , and writes a sequence of the compressed bam blocks to a bam file .",
    "this step produces a sorted bam file if the alignment database is available .",
    "the sam2bam does not require a separate step for sorting . if some plug - ins set up the alignment database , this step obtains input by scanning the alignment database in the order of sort keys and writes the alignments to the bam file in the sort key order .",
    "therefore , all the output from sam2bam is automatically sorted .",
    "in contrast , if no analyzer plug - ins are used , the output is not sorted .",
    "+ there is a dispatcher that splits a sequence of the binary alignment records into 64-kb sub - sequences by only using their virtual addresses and their sizes .",
    "the dispatcher obtains input data by traversing the alignment database or receiving them from step [ virtualizing ] .",
    "the dispatcher is single - threaded to ensure the order of the binary alignment records in each bam block and the order of bam blocks in the output .",
    "it does not construct actual bam blocks by copying the data to avoid performance bottlenecks in the dispatcher .",
    "it instead transfers each set of virtual addresses that construct a single bam block to a thread pool for compression .",
    "+ if the dispatcher receives the input data from step [ virtualizing ] , the binary alignment records in the physical memory provide their record sizes .",
    "otherwise , the dispatcher traverses baminfo records in the alignment database to obtain the virtual addresses and record sizes for the binary alignment records .",
    "+ multi - threads are used for compressing bam blocks to accelerate this step .",
    "bam blocks are constructed and compressed in parallel .",
    "the compression method is gzip , which is widely used in the real world @xcite .",
    "if hardware compression accelerators are available in the system , sam2bam dynamically loads their library codes that can be called via the standard zlib api and it can offload compression to the accelerators .",
    "to demonstrate that sam2bam can significantly reduce the runtime of marking duplicate alignments , we compared the runtime of sam2bam versus picard @xcite , which is a widely - used tool set that is also recommended in gatk best practices @xcite .",
    "two data sets in the sam format were used to evaluate performance .",
    "the first was a150x coverage of whole exome ( wex ) data , and second was a 50x coverage of whole genome sequencing ( wgs ) data .",
    "the wex data were part of the 1000 genome project data @xcite .",
    "the input sam file size was 52 gb . the wgs data were part of the cancer genome atlas ( tcga ) benchmark 4 dataset , g15512.hcc1954.1 @xcite .",
    "the input sam file size was 546 gb . the sam files that we used were created by running burrows - wheeler aligner ( bwa ) @xcite for the fastq - format data converted from the original bam files .",
    "the sam2bam handled the sam and bam data formats by calling the modified code of samtools .",
    "the original code for samtools was obtained from its development repository as of august 2015 .",
    "two plug - ins were created for the performance evaluation : an analyzer plug - in for marking duplicate alignments and a compression accelerator plug - in .",
    "the source code of sam2bam and the instructions on how to build sam2bam are available from a github repository ( https://github.com/t-ogasawara/sam-to-bam ) .",
    "the analyzer plug - in was enabled to mark duplicate alignments .",
    "the sam2bam created the alignment database , as was explained earlier in section : item [ build_alignment_db ] .",
    "the analyzer plug - in traverses the alignment database by using the unclipped position to find the candidates of duplicate alignments and finds alignments that have the same beginning and end positions .",
    "it also finds their pairs by using mate information that is available in the alignment database .",
    "the plug - in further uses the same criteria as picard markduplicates @xcite for the candidates to select one alignment among duplicates .",
    "the duplicates are analyzed in parallel by assigning the segmented unclipped position regions to threads .",
    "this analyzer plug - in is provided as a pre - built library for power8 systems and is installed when sam2bam is built .",
    "the accelerator plug - in enables the use of a hardware compression card .",
    "part of the multi - threads for compression offload compression tasks to the hardware card instead of performing compression with software .",
    "this accelerator plug - in is also built when sam2bam is built .",
    "two picard tools , sortsam and markduplicates , have been suggested to mark duplicate alignments on gatk best practices @xcite .",
    "sortsam first takes a sam file as input , sorts the alignments , and writes the result to a bam file .",
    "markduplicates then takes the produced bam file as the input , marks duplicate alignments , and writes the result to another bam file .",
    "the bam files are compressed by default .",
    "we used picard tools ( version 2.1.1 ) @xcite in the biobuilds package ( version 2016 - 04 ) @xcite .",
    "openjdk 1.8.0_72-internal was used to run the picard tools with 21 gb of java heap memory .",
    "the runtime of the target programs and the maximum size of the memory that was used by the programs during program execution were measured by using a command , /usr / bin / time .",
    "the programs were run on a single node of ibm power systems s822lc @xcite that had 16 power8-based cpu cores @xcite , where 128 logical processors were available ( eight logical processors per core ) with 1 tb of memory .",
    "the machine was attached to high performance storage , i.e. , ibm elastic storage server ( ess ) gl4 via mellanox fdr switch @xcite . a hardware card that provided fpga - based zlib acceleration @xcite",
    "was attached to the machine and this could speed up compression of the bam data .",
    "the operating system was ubuntu 14.04.1 .",
    "theoretical maximum performance for the storage mode of sam2bam explained in was measured by using the file system in the main memory ( /dev / shm ) , which simulated an ideal high - performance device ( e.g. , a solid state drive ( ssd ) ) .",
    "such a device is mandatory to achieve high levels of performance in the storage mode since sam2bam in the storage mode performs a huge number of i / o operations that are not always sequential accesses .",
    "the sam2bam demonstrated more than 100 times better performance than picard and finished in about one minute in both memory and storage modes while picard needed more than two hours ( table [ wex_result ] ) .",
    "-2.25in0 in    l|p3.9cm|p3.3cm||r & * sam parsing , sorting * & * duplicate marking * & * total runtime * + picard & 59.4 min ( 17.5 gb ) & 86.6 min ( 22.2 gb ) & * 146.0 min * + sam2bam ( memory mode ) & & * 1.0 min * + sam2bam ( storage mode ) & & * 1.0 min * + sam2bam ( memory mode , no hw compression ) & & * 1.4 min * + sam2bam ( storage mode , no hw compression ) & & * 1.3 min * +    although sam2bam was more than 186 times faster than the standard tools , it required more memory than picard in the memory mode .",
    "the sam2bam reduced the maximum memory size by placing binary alignments in external storage instead of in the main memory in the storage mode .",
    "the performance of sam2bam with data compression by both software and hardware was 43% better than that of sam2bam with software - only compression .",
    "the java heap size for picard was sufficient since the time spent in garbage collection of the java heap was negligible ( about 0.63% of the total runtime ) .",
    "[ wex_result ]    the sam2bam benefited from multi - threading and pipelining , which was explained in .",
    "the sam2bam read a sam file for sam parsing and parsed it at rates of 1.7 - 2.0 gb / s .",
    "this high level of performance was due to multi - threading and pipelining ( we will discuss performance without them in ) .",
    "the runtime of duplicate marking was  5% of the total runtime .",
    "bam blocks were compressed at rates of 1.5 - 1.7 gb / s , including a rate of additional 0.9 gb / s with hardware compression .",
    "such a high throughput was achieved by pipelining as well as multi - threading ( we will discuss performance without pipelining in ) .",
    "the performance of the framework on which alignments were analyzed and processed is critical for high performance tools .",
    "the runtime of a picard tool that converts the file format from sam to bam is 92% that of sortsam and 70% that of markduplicates for picard ( we will discuss the details in ) .",
    "the sam2bam demonstrated 156 times better performance than picard .",
    "the sam2bam finished in about  9 minutes in the memory mode while picard needed more than 20 hours ( table [ wgs_result ] ) .",
    "the storage mode was 81% slower than the memory mode for wgs data , while the memory and storage modes demonstrated similar performance for wex data .",
    "this slowdown was mainly due to slowdown in bam block compression in the storage mode ( 37% of throughput in the memory mode ) .",
    "we collected the system - level profiles to analyze the slowdown in bam block compression .",
    "the profiles indicated that the computation time in the operating system was significantly increased by 33 times in the storage mode using the wgs data , but it was only increased by 151% when using the wex data .",
    "we need to further investigate additional activities undertaken by the operating system to address the slowdown with the wgs data in the storage mode .",
    "-2.25in0 in    l|p3.9cm|p3.3cm||r & * sam parsing , sorting * & * duplicate marking * & * total runtime * + picard & 631.9 min ( 18.6 gb ) & 707.5 min ( 22.4 gb ) & * 1339.4 min * + sam2bam ( memory mode ) & & * 8.6 min * + sam2bam ( storage mode ) & & * 15.6 min * + sam2bam ( memory mode , no hw compression ) & & * 16.2 min * + sam2bam ( storage mode , no hw compression ) & & * 21.7 min * +    [ wgs_result ]      the accuracy of duplicate marking for sam2bam could be evaluated by measuring the number of alignments that picard markduplicates marked but sam2bam did not and also by measuring the number of the alignments that sam2bam marked but picardmarkduplicates did not .",
    "outputs were compared between picard markduplicates and sam2bam ( we will discuss how we compared the outputs in ) to evaluate the accuracy of duplicate marking . if sam2bam and picard markduplicates marked the same sets of alignments , sam2bam could be considered to be accurate and could be used as a fast alternative to picard markduplicates .",
    "we tested and verified that duplicate marking by sam2bam was _ accurate _ , based on the experimental results obtained from wex and wgs data sets .",
    "there were  16 million duplicate alignments for the wex data set and  188 million for the wgs data set .",
    "these alignments were the same between sam2bam and picard markduplicates when making the comparison explained in .",
    "10 baker m. next - generation sequencing : adjusting to data overload .",
    "nat methods .",
    "doi : 10.1038/nmeth0710 - 495    van der auwera ga , carneiro mo , hartl c , poplin r , del angel g , levy - moonshine a et al . from fastq data to high confidence variant calls : the genome analysis toolkit best practices pipeline .",
    "curr protoc bioinformatics .",
    "2013 oct 15;11(1110):11 .",
    "doi : 10.1002/0471250953.bi1110s43 pmid : 25431634    prabhakaran a , shifaw b , naik m , narvaez p , van der auwera g , powley g , osokin s , srinivasa g. infrastructure for gatk best practices pipeline deployment .",
    "available : http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/deploying-gatk-best-practices-paper.pdf    cock pja , fields cj , goto n , heuer ml , rice pm . the sanger fastq file format for sequences with quality scores , and the solexa / illumina fastq variants .",
    "nucleic acids res .",
    "2010 apr ; 38(6):1767 - 1771 .",
    "doi : 10.1093/nar / gkp1137 pmid : 20015970    li h , hansaker b , wysoker a , fennell t , ruan j , homer n , et al .",
    "the sequence alignment / map format and samtools .",
    "2009 aug 15 ; 25(16):20789 .",
    "doi : 10.1093/bioinformatics / btp352 pmid : 19505943    gatk best practices  pre - processing .",
    "available : https://www.broadinstitute.org/gatk/guide/bp_step.php?p=1 .",
    "li h , durbin r. fast and accurate short read alignment with burrows - wheeler transform .",
    "2009 jul 15 ; 25(14):1754.60 .",
    "doi : 10.1093/bioinformatics / btp324 pmid : 19451168    the sam / bam format specification working group .",
    "sequence alignment / map format specification .",
    "2015 march 3 .",
    "available : http://github.com/samtools/sam-spec .",
    "optimization of glibc s memcpy , memmove and memchr functions .",
    "ibm developerworks .",
    "available : https://www.ibm.com/developerworks/community/wikis/home/wiki/ w51a7ffcf4dfd_4b40_9d82_446ebc23c550/page / porting%20story%20%231",
    ".    picard .",
    "available : http://broadinstitute.github.io / picard/.    ( howto ) map and mark duplicates .",
    "available : http://gatkforums.broadinstitute.org/gatk/discussion/2799/howto-map-and-mark-duplicates .",
    "na12878 exome alignment .",
    "available : ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/na12878/exome_alignment/ na12878.mapped.illumina.bwa.ceu.exome.20121211.bam .",
    "the cancer genome atlas ( tcga ) benchmark 4 dataset .",
    "available : https://cghub.ucsc.edu/datasets/benchmark_download.html .",
    "overview of the dedup function of bamutil .",
    "available : http://genome.sph.umich.edu/wiki/bamutil:_dedup .",
    "available : https://biobuilds.org / downloads/.    ibm power systems s822lc technical overview and introduction .",
    "available : http://www.redbooks.ibm.com/redpapers/pdfs/redp5283.pdf .",
    "sinharoy b , et al . ,",
    "ibm power8 processor core microarchitecture , ibm journal of research and development .",
    "2015 jan - feb ; 59(1 ) : 2:1 - 21 .",
    "doi : 10.1147/jrd.2014.2376112    introducing the elastic storage server for power .",
    "available : http://www.ibm.com/support/knowledgecenter/ssysp8_4.0.0/ com.ibm.ess.v4r0.deploy.doc/bl8dep_intro.htm .    a hardware accelerated version of zlib based compression / decompression rfc1950/rfc1951/rfc1952 with help of an fpga based pcie card .",
    "available : https://github.com/ibm-genwqe/genwqe-user ."
  ],
  "abstract_text": [
    "<S> this paper introduces a high - throughput software tool framework called _ sam2bam _ that enables users to significantly speedup pre - processing for next - generation sequencing data . </S>",
    "<S> the sam2bam is especially efficient on single - node multi - core large - memory systems . </S>",
    "<S> it can reduce the runtime of data pre - processing in marking duplicate reads on a single node system by 156 - 186x compared with de facto standard tools . </S>",
    "<S> the sam2bam consists of parallel software components that can fully utilize the multiple processors , available memory , high - bandwidth of storage , and hardware compression accelerators if available .    </S>",
    "<S> the sam2bam provides file format conversion between well - known genome file formats , from sam to bam , as a basic feature . </S>",
    "<S> additional features such as analyzing , filtering , and converting the input data are provided by _ </S>",
    "<S> plug - in _ tools , e.g. , duplicate marking , which can be attached to sam2bam at runtime .    </S>",
    "<S> we demonstrated that sam2bam could significantly reduce the runtime of ngs data pre - processing from about two hours to about one minute for a whole - exome data set on a 16-core single - node system using up to 130 gb of memory . </S>",
    "<S> the sam2bam could reduce the runtime for whole - genome sequencing data from about 20 hours to about nine minutes on the same system using up to 711 gb of memory . </S>"
  ]
}