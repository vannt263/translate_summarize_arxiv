{
  "article_text": [
    "the assessment of catastrophic death risk is important for life insurers , and for non - life and health insurers writing accidental death covers . a good view of this risk is needed to explicitly assess the need for reinsurance , and to establish the amount of capital required to cover the risks left at the company s own account .",
    "conversely , the reinsurer needs to be able to assess the catastrophe risk quantitatively in order to set an adequate  but competitive  price for the reinsurance cover",
    ". a model for catastrophe risk is also needed in the design of a reinsurance programme , to assess the effect of different proposed features , such as aggregate limits or drop - down features .",
    "the measurement of catastrophe risks as well as the suitability and likely outcomes of different plans of action  such as acquiring reinsurance , reserving capital , or using insurance linked securities to trasfer risk to capital markets  require appropriate quantitative tools . in this paper",
    "we propose one tool for addressing the catastrophe risk in life and ( health / non - life ) accident insurance .",
    "our approach is based on a population level model for occurrence of catastrophe events , combined with policy level simulation of the effects the catastrophes have on a specific insurance portfolio : the times and sizes of accidents are modeled using poisson point process specification , followed by first modeling the propotion of total lives lost that had a ( certain type of ) insurance policy ( based on national insurance coverage statistics ) , and finally the proportion of insured lives lost that were customers of a specific insurance company ( based on market share statistics ) . based on the accidental death model , we propose a novel application of a policy - by - policy simulation framework to life catastrophe risk measurement and reinsurance pricing ; a concrete implementation of the simulation framework is made possible by the catastrophe model .",
    "catastrophe risk models are now commonly used in property insurance , where there are widely acknowledged commercial vendor models in use ( e.g. , air , rms , eqecat ) .",
    "these models generally simulate occurrence times , intensity , geographical location , and possible other relevant characteristics of events causing catastrophes , and then estimate the effect these events have on the insurance company through the policies it has written .",
    "the approach works particularly well when we examine losses caused by a specific , well - defined phenomenon .",
    "examples of these are natural phenomena , such as earthquakes , hurricanes , windstorms and floods .",
    "these events are caused by underlying physical processes , and can often be modeled with good or moderate success based on the scientific knowledge about the characteristics of the processes generating them . indeed , the commercial catastrophe risk models are mainly intended for modeling losses caused by natural disasters .    with life policies the situation is more complicated in",
    "that the death covers written by life insurers generally cover all causes of death .",
    "this means that there is no well - defined process responsible for causing the insured losses of a company , and hence the setting does not lend itself to the kind of modeling adopted with natural perils .",
    "an alternative is to assess the number of deaths using purely statistical methods , without explicitly modeling the causes of deaths .",
    "this is the approach taken in this paper .",
    "the statistical approach to catastrophes has of course its own problems , the biggest being the scarcity of relevant data about extreme events .",
    "furthermore , our ultimate aim is not only to describe the observed accidental deaths by means of a statistical model , but to use the model for extrapolating outside the range of data : i.e. , to assess the probabilities of events more extreme than those observed so far .",
    "this requires well - founded probabilistic and statistical methods , backed by a sound mathematical theory : we use extreme value theory ( evt ) ( @xcite )  in particular , the point process view of extremes ( @xcite )  in order to develop a plausible population level model for accidental death events .    to go from population level risk to insurance company specific risk ,",
    "we leverage the special structure of the insurance products considered . with products covering the death of an insured",
    ", the insured event is binary : the customer either dies or does not die  there are no partial losses , in contrary to non - life insurance .",
    "in addition , the risk sum payable given the time of death is usually either known or directly calculatable based on contract terms ( in risk and non - participating traditional savings policies ) ; or it can be reasonably estimated e.g. as part of a policy level simulation model ( for with - profit and unit - linked savings policies ) .",
    "it is our view that these features of life and personal accident products call for an explicit simulation of catastrophic events ( numbers of deaths ) , combined with simulating the effect the event has on the insurance company under consideration , given the data regarding all its policies in force .",
    "the standard intensity - based approaches to modeling mortality in life insurance are not suitable for modeling catastrophes , and hence we need to use techniques more in line with those applied in non - life insurance ( cf .",
    "although the use of traditional reinsurance pricing methods  experience rating and exposure rating , see @xcite and the references therein  is still prevalent in practice , and these methods have their place , it has long been recognized that ( proper ) pricing of some features of reinsurance contracts requires a simulation approach .",
    "unlike in ( non - life ) loss reserving , where aggregate methods have been the norm and approaches based on simulating ( the development of ) individual claims have only recently been applied to real data ( @xcite ) , simulation of individual claims has long been used in the more straightforward pricing domain ; see @xcite for a good example .",
    "this so - called frequency - severity method is based on simulating both the number and sizes of individual claims ( see also @xcite ) .",
    "what is different in our approach compared with the traditional distribution - based simulation methods for pricing , is that we do not simply generate claim amounts from an estimated claim size distribution .",
    "instead , we exploit the characteristics of the life / accident insurance setting by explicitly modeling the deaths occurring to policyholders and the subsequent insurance payments laid out in the contract terms ; this makes it possible to accurately capture the structure of the insurance portfolio under consideration , as well as the ( exact , individual ) contract terms of the underlying policies , giving us the loss distribution of the primary insurer . by recording all the simulated losses individually at contract level",
    ", it is straightforward to overlay or embed the terms and conditions of the proposed reinsurance programme to arrive at the full loss distribution to the reinsurer .",
    "the contract level simulation approach is called microsimulation in this paper , in contrast to the traditional individual claims simulation methods , which make no reference to the actual contracts .",
    "when finalizing this paper , we learned of a similar work already published . in @xcite ,",
    "ekheden and hssjer are the first to propose a marked poisson process type model for life catastrophes ; the mechanism by which total catastrophe deaths are converted into those covered by an insurance company is also somewhat similar to the one proposed in this paper .",
    "the article @xcite provides an interesting analysis of a dataset larger than ours , and for different regions : our results in the first part of this paper mostly agree with their findings .",
    "however , while the authors in @xcite _ assume _ that the occurrence of catastrophic events follows a homogenous poisson process , we start by considering more general poisson point processes , and base our choice of the specific marked poisson process  implying a homogenous poisson process for the occurrence times  on statistical tests and model comparisons .",
    "this paper therefore provides justification for the model a priori assumed and used in @xcite .",
    "we further extend the catastrophe model by modeling small and big events with separate mark distributions , making it possible to model the whole range of accident sizes and leading to a better description of the relatively smaller accidents .",
    "our paper further differs from @xcite in that we suggest a microsimulation approach to pricing life ( re)insurance at individual contract level to capture the exact features of the underlying insurance portfolio : we do nt only simulate the number of lives lost , but the actual customers that die , and the resulting contractual payments ; whereas the claim distribution model used in @xcite is rather simplistic ( gamma distribution ) , without reference to actual contractual amounts or contract features .",
    "the authors in @xcite concentrate on pricing a specific type of catastrophe excess - of - loss reinsurance contract on a term insurance portfolio , whereas our approach is fully general , covering all types of reinsurance contracts  and , in principle , extending to all types of underlying life insurance products .",
    "the outline of the paper is as follows . using extreme value theory and statistical techniques based on it , introduced in section  [ bg ]",
    ", we build a model for the occurrence and sizes of catastrophic accidental deaths in finland , using data on finnish and swedish populations as the basis of our analysis in section  [ accdeaths ] .",
    "we also discuss catastrophe risk measurement by means of the model . a microsimulation framework for reinsurance pricing",
    "is then introduced in section  [ reins_pricing ] , and the developed accidental death model is extended for pricing excess reinsurance . finally , section  [ conclusions ] ends with summary and conclusions .",
    "in this section we will introduce the notation and concepts from extreme value theory needed for our purposes .",
    "let @xmath0 be a sequence of independent and identically distributed ( iid ) random variables with common distribution function ( df ) @xmath1 , defined on a probability space @xmath2 .",
    "the random variables ( rvs for short ) @xmath3 take their values in a measurable space @xmath4 ; in what follows , we generally take @xmath5 , or a subset thereof , equipped with the corresponding borel sigma - algebra @xmath6 , as usual .    in practical applications we do nt typically know the underlying distribution @xmath1 of the rvs @xmath7 whose observed realization forms our data . in using extreme value theory to model extremal events ,",
    "we therefore seek an asymptotic distribution directly for the maxima , via mathematical limit arguments .",
    "these limiting distributions are then used in applications as approximations for distributions of maxima or excesses of high thresholds .    in classical extreme value theory ,",
    "one is interested in the behaviour of the sample maxima , @xmath8 , @xmath9 .",
    "that is , one looks for normalizing sequences @xmath10 , @xmath11 such that @xmath12 converges in distribution ( weakly ) , meaning that @xmath13 as @xmath14 , @xmath15 , where @xmath16 is a non - degenerate df .",
    "an alternative equivalent condition for the convergence is , the limit is taken to be equal to @xmath17 . ]",
    "@xmath18 if the above hold for some non - degenerate df @xmath16 , then the underlying distribution @xmath1 is said to be in the _ maximum domain of attraction _ of @xmath16 , denoted by @xmath19 .",
    "one of the key results of classical extreme value theory , due to fisher and tippett @xcite and gnedenko @xcite , is that if the limiting distribution @xmath16 exists , it must be one of three types of distributions , _ regardless _ of the underlying distribution @xmath1 .",
    "these three are called extreme value distributions and can be parameterized into one distribution , the _ generalized extreme value _ ( gev ) _ distribution _ @xmath20 with shape parameter @xmath21 , scale @xmath22 and location @xmath23 . the case @xmath24 is interpreted as a limit when @xmath25 .",
    "the shape parameter @xmath26 governs the tail behaviour of the distribution . for fixed @xmath27",
    ", it holds that @xmath28 from either side , meaning that the gev parametrization is continuous in @xmath26 .      the gev distribution discussed above is the limiting distribution for normalized sample maxima . an alternative approach to modeling extremes is to consider all observations exceeding a certain ( high ) threshold . the approach is then based on the distribution of exceedances over a threshold @xmath29 , say , which is @xmath30 where @xmath31 is a rv with df @xmath1 , and @xmath32 is the right endpoint of @xmath1 .",
    "under the same conditions that lead to , the excess distribution @xmath33 in can be approximated , for large @xmath29 , by the _ generalized pareto distribution _",
    "( gpd ) family , originally due to pickands ( see @xcite ) : @xmath34 where the scale parameter @xmath35 . if the shape parameter @xmath36 , the variance of gp distribution does not exist ; and if @xmath37 , the mean does nt exist either . as in the case of gev distribution",
    ", we again have that for fixed @xmath27 , the gpd parametrization is continuous in @xmath26 , so that @xmath38 .",
    "this is especially useful in statistical modeling .",
    "the gpd turns out to be a natural limiting distribution for many underlying distributions @xmath1 .",
    "in fact , it can be shown that ( for some positive function @xmath39 ) @xmath40 _ if and only if _ @xmath41 , @xmath42 ; this is the so - called pickands - balkema - de haan theorem ( @xcite ) .",
    "the result means that the distributions for which normalized maxima converge to a gev distribution are those distributions for which the excess distribution converges to gp distribution as the threshold @xmath29 grows .",
    "in addition , the shape parameter @xmath26 is the same for both the limiting gev and gp distributions .",
    "based on the result above , the generalized pareto distribution can be viewed as a fundamental limiting model for the distribution of observations exceeding a high threshold . with statistical applications in mind ,",
    "we get the following approximation for the excess distribution @xmath33 , for large @xmath29 : @xmath43 @xmath44 , where @xmath45 and @xmath26 are to be estimated from data .",
    "pareto distribution has long been used as a claim severity distribution in ( non - life ) insurance due to the good empirical fit to many insurance datasets it provides ; see , e.g. , @xcite .",
    "the use of generalized pareto in the extreme value context , though a relatively standard technique by now , is a more recent development in the actuarial field . for early applications to insurance with good discussion , see @xcite .",
    "the discussion of the previous section only considered the magnitudes of the threshold exceedances .",
    "however , often it is natural to extend the view by explicitly including a time dimension , and to consider both the magnitudes of exceedances and the timing of them .",
    "this leads naturally to a point process view of high - level exceedances ( originally introduced by pickands @xcite and smith @xcite ) as events in space and time .",
    "let @xmath46 be a sequence of iid rvs with df @xmath1 taking values in state space @xmath47 , and assume holds for some normalizing sequences @xmath48 , @xmath11 .",
    "consider a sequence of thresholds @xmath49 for some fixed @xmath27 and let @xmath50 as in @xcite .",
    "the variable @xmath51 returns either the normalized time @xmath52 of an exceedance , or zero if there s no exceedance . the _ point process of exceedances _",
    "@xmath53 , @xmath54 , with state space @xmath55 $ ] , counts the exceedances of threshold @xmath56 with time of occurrence in the set @xmath57 .",
    "alternatively , the point process of exceedances can be written as @xmath58 with two - dimensional state space @xmath59\\times(u_n,\\infty)$ ] .    in the context of extremes , we are again interested in the asymptotic behaviour of the point process of exceedances as @xmath60 .",
    "it can be shown that @xmath61 converges in distribution to a poisson ( point ) processpoint process @xmath62 is a poisson point process on @xmath47 with intensity measure @xmath63 , if the following conditions hold : + ( i ) for @xmath57 and @xmath64 , @xmath65 if @xmath66 , and @xmath67 if @xmath68 .",
    "+ ( ii ) for any @xmath69 , if @xmath70 are mutually disjoint subsets of @xmath47 ( @xmath71 ) , then the random variables @xmath72 are independent . ] @xmath73 ; the weak convergence is established under a topology that essentially excludes sets bordering on the lower boundary ( see @xcite and ( * ? ? ? * ch .  5 ) ) .",
    "the intensity measure of the limiting point process can be derived from and , and is given by @xmath74 for sets of the form @xmath75 .",
    "the above means , among other things , that the numbers of exceedances in separate rectangles @xmath76 are independent poisson - distributed rvs with expected values @xmath77 .",
    "the approaches of previous subsections can be seen as special cases of the point process approach , as all the results mentioned so far can be derived from the point process representation .",
    "the tail of the excess distribution associated with a given threshold @xmath29 is obtained as the conditional probability that @xmath78 given @xmath79 : @xmath80 where @xmath81 .",
    "this is just the generalized pareto distribution model of subsection [ bg_gpd ] .",
    "similarly , the point process model can be seen to imply the gev distribution model for maxima .",
    "consider the event @xmath82 for some @xmath83 : this is just the event that @xmath61 has no points in @xmath84 .",
    "the limiting probability of this event is then given by @xmath85 .",
    "futhermore , we note that for any @xmath83 , the one - dimensional process of exceedances of the level @xmath27 implied by is a homogenous poisson process with rate @xmath86 .",
    "the limiting model obtained suggests that the threshold exceedances in iid data can be approximated by a poisson point process for a high threshold @xmath29 . to summarize , the limiting point process model has the following properties : ( i ) exceedances occur as a homogenous poisson process in time ;",
    "( ii ) magnitudes of excesses are iid and independent of exceedance times ; and ( iii ) the distribution of excesses is generalized pareto .",
    "this model is often called the peaks - over - threshold or pot model ( e.g. , @xcite ) .",
    "so far we have discussed the point process approach in the context of iid random variables .",
    "the results for iid sequences continue to hold for stationary sequences with small modifications , under certain conditions ( see @xcite , the review article @xcite , or @xcite for a textbook treatment ) . in the case of non - stationary sequences ,",
    "the general theory is not helpful in practice ; as noted by smith in @xcite , the class of limiting distributions is much too broad to be of use in identifying parametric statistical models . in practice",
    "non - stationarity in data can be taken care of by appropriate statistical modeling : the most straightforward way to generalize the point process model discussed above is to allow the model parameters @xmath87 to depend on time , or other explanatory variables .",
    "in the following we will consider catastrophe risk relating to the life type of insurance business .",
    "more precisely , we will concentrate on the risk arising from catastrophic events causing large numbers of deaths .",
    "our aim is to build a probabilistic model for the occurrence and sizes of these events using the theory of previous section .",
    "the data analysis , estimation and simulations in this paper are performed using the matlab computing language by mathworks .",
    "our data consists of accidents that have occurred in finnish and swedish populations during the period 19102009 , and have claimed more than three lives .",
    "although our aim is to model the accidental deaths in finland , when analyzing the finnish data alone and fitting extreme value models to it , we found that the models did nt capture the right tail of the implied severity ( death count ) distribution well .",
    "we therefore aim to improve the tail modeling by basing the statistical model estimation on an extended dataset , including swedish accidental deaths as well .",
    "we regard combining swedish experience with the finnish one justifiable on the basis that sweden is the country ( area , population ) that most closely resembles finland , and the geographical and economic conditions in both countries are similar .    the limit of three lives was set based on an examination of the numbers of accidental deaths , and to get as complete a dataset as possible , as the bigger accidents are much better documented .",
    "this censoring of the smallest observations from the data does not have an impact on the modeling of extreme death counts in practice .    for each accident , the date of occurrence and the number of deaths was picked .",
    "accidental deaths clearly related to wars were removed from the data to avoid distorting the results .",
    "as the accident death counts between the countries are generally independent , we can use the combined dataset directly for estimation of the loss severity distribution . however , there are two accidents common to both original datasets : the sinking of cruise ship estonia in 1994 ( 10 finns and 552 swedes died ) , and the indian ocean tsunami in 2004 ( 179 finns and 543 swedes died ) .",
    "we take the value from swedish data for both accidents , but scale the tsunami deaths by the ratio of the populations ( in year end 2011 ) , to arrive at a death counth of 316 : it is plausible that both ( types and sizes of ) accidents could happen in finnish population , but the number of tsunami deaths is scaled down to reflect the different population sizes , and thus number of tourists .",
    "figure [ fig : catplot_tot ] shows the resulting combined accidental death data , consisting of 139 accidents that claimed four lives or more , with maximum death count 552 . with long series of historical observations , we have to consider whether the observations corresponding to different periods are on an equal basis .",
    "for example , when considering monetary quantities over a long period , it is essential to allow for the effects of inflation . in the context of accidental deaths ,",
    "the data consists of absolute death counts , and our view is that adjustment to data is not appropriate ",
    "possible ( trend - like ) features in the data should be taken care of by statistical modeling .",
    "one of the main challenges in extreme value modeling ( using either threshold exceedance or point process methods ) is the selection of an appropriate threshold for fitting the model .",
    "there is inevitably a trade - off between bias and variance , as setting the threshold too low will lead to invalidity of the ( limiting ) gpd approximation of the excess distribution  and hence bias in estimated parameters  whereas setting the threshold too high will leave only few observations for the statistical model estimation , leading to high variance in parameter estimates , or even failure of the numerical estimates to converge . in practical applications",
    ", we usually try to set the threshold as low as possible , subject to the gpd providing an acceptable fit .",
    "there are two main tools used in practice for testing whether the generalized pareto distribution ( as implied by both the exceedance and point process methods ) is a valid model for the excess distribution of observations , starting from certain threshold level @xmath29 : the mean excess plot , and the stability of parameter estimates . based on our analysis of the data ,",
    "a threshold value of 20 is chosen ; for more details , see appendix [ app : threshold ] .",
    "there are a total of 23 observations in the combined data exceeding the threshold @xmath88 . because of the small number of observations left for statistical estimation , we compared the results from using the threshold of 20 to the results obtained by using the whole dataset , corresponding to a threshold of 3 : however , the value @xmath89  used in @xcite for a dataset concerning sweden  was found to be clearly too low for the gpd approximation to work , and is not shown here .",
    "figure [ fig : gpd_cdf_tot_u20 ] displays the gp distribution fitted to exceedances of @xmath88 .",
    "we see that the fit for the chosen threshold is quite good .",
    "based on our analysis , the accidental death data can be regarded iid and is well modeled by a time - homogenous poisson process : for more details see the tests of appendix [ app : homogtest ] and the comparison of point process models in appendix [ app : pp_models ] . in this case",
    "it is convenient to interpret the two - dimensional poisson process as a marked poisson point process , with exceedance times the points and exceedance sizes the marks .",
    "that is , the number of points follows a ( one - dimensional ) poisson process with intensity @xmath90 , and the marks are iid generalized pareto distributed .",
    "this is the model first proposed in @xcite .",
    "given a treshold @xmath29 , a random number @xmath91 from the sample @xmath92 will exceed it ; re - label these observations as @xmath93 .",
    "the corresponding excesses are denoted by @xmath94 , where @xmath95 .",
    "since the number and mark sizes of the exceedances are assumed to be independent , the process log - likelihood can be written @xmath96 where @xmath97 , @xmath98 is the gpd density , and @xmath99 .",
    "we see that the log - likelihood can be partitioned into a sum of two terms , @xmath100 , where the terms involve different parameters .",
    "this means that we can separately estimate the frequency of the exceedances ( i.e. , the poisson parameter @xmath90 ) and the sizes of the excesses ( i.e. , the gp parameters @xmath101 ) .",
    "this marked point process representation is convenient for our purposes in section [ reins_pricing ] , as it allows the separate simulation of event times and event sizes .",
    "the maximum likelihood estimate for the poisson process intensity @xmath90 is just @xmath102 , the number of observed exceedances of threshold @xmath29 in the sample , with variance equal to the mean @xmath90 .",
    "the number of exceedances of the level @xmath88 in the combined data is 23 ; however , our aim is to model accidental deaths in finnish population , not the combined experience of finland and sweden . to this end",
    ", we estimate the intensity from the finnish data alone , with 15 exceedances of the threshold . to get an annualized rate",
    ", we divide the estimated intensity with the number of years of observation , @xmath103 , to arrive at an annual intensity of @xmath104 .. ] the 95 % confidence interval is given by @xmath105 $ ] .",
    "the parameters of the mark size distribution are obtained by numerically maximizing the gp log - likelihood @xmath106 with the constraints @xmath35 and @xmath107 for all @xmath108 .",
    "the maximization is done using matlab s fminsearch function .",
    "the resulting parameter estimates , with approximate 95 % confidence intervals ( based on the asymptotic normality of mles ) , are given in table [ tbl : gpd_tot_mles ] .",
    "the confidence intervals obtained from the asymptotic covariance matrix of the maximum likelihood parameter estimates are , in practice , often not very good due to the small sample size typically encountered in extreme value problems .",
    "usually more accurate confidence intervals can be obtained by using the so - called _ profile likelihood_. , the profile likelihood is the log - likelihood maximized w.r.t .",
    "all other parameters . for more details , see e.g. @xcite . ]",
    "these are also shown in table [ tbl : gpd_tot_mles ] .",
    "the value of the shape parameter estimate @xmath109 is over 1/2 , so that the variance does not exist , and very close to 1 , in which case the mean would nt exist either .",
    "this points to a very heavy - tailed distribution .    by plotting the empirical probabilities from the finnish data against the ones given by the model",
    ", we get the probability plots on the left side of figure [ fig : prob_qq_plot_totfi_u20 ] .",
    "the right side shows the corresponding quantile - quantile ( qq ) plots , i.e. , empirical quantiles against model quantiles .",
    "we see that the fit of the model for threshold @xmath88 is good , and without the problems encountered when the mark distribution was based on the finnish data alone ( not shown ) .",
    "the fitted model for accidental deaths can be used to analyze the frequency and sizes of accidents .",
    "this is conveniently done using using the concepts of return level and return period . given the frequency of a loss event ( say , a 1-in-100 year event ) , the magnitude of this event",
    "is called the _ return level_. conversely , given the size of an event , the frequency of this kind of event is called the _ return period_. more precisely , let @xmath7 be a sequence of iid or stationary rvs with common df @xmath1 .",
    "the return period of an event @xmath110 is @xmath111 .",
    "similarly , the return level corresponding to ( return ) period @xmath112 is given by @xmath113 , where @xmath114 is the generalized inverse of @xmath1 .    in our marked poisson process model , the distribution of threshold excesses , conditional on an exceedance occurring , is given by the generalized pareto distribution : @xmath115 , @xmath116 .",
    "to calculate return levels and return periods , we also need the exceedance probability , given by @xmath117 , where @xmath118 is the intensity of the poisson process of exceedances related to threshold @xmath29 .",
    "figure [ fig : gpd_retlev_totfi_u20 ] shows the return level plot for the model , with approximate 95 % confidence intervals .",
    "the model fit is quite good , with all the observations inside the confidence intervals .",
    "table [ tbl : gpd_retlevels_totfi ] shows the model - based return levels for 10 , 100 , 200 and @xmath119 year accidental death events for both the delta - method based on asymptotic normality and for the profile likelihood method .",
    "the lower endpoints go negative when using the delta - method , which of course is not physically possible ; there s no such problem with the profile likelihood . from the table",
    "we see that , for example , a 100-year event corresponds to 170 deaths , with 95 % confidence interval for this point estimate ranging from 80 up to @xmath120 deaths .",
    "the return levels are closely connected to the popular risk measure value - at - risk ( var ) , both being quantiles of the underlying ( loss ) distribution .",
    "recall that the @xmath112-year return level is given by @xmath121 ; this is equal to value - at - risk at confidence level @xmath122 , @xmath123 , where @xmath124 .",
    "this means that table [ tbl : gpd_retlevels_totfi ] implicitly gives 1-year @xmath125 , for @xmath126 , with associated confidence intervals .",
    "for example , the 1-year 99.5 % var for accidental deaths is estimated to be 320 deaths , with upper 95 % confidence interval for the risk measure value being @xmath127 deaths .",
    "we can also calculate value - at - risk explicitly by considering tail probabilities in our model : as above , with @xmath83 , @xmath128 here the exceedance probability @xmath129 is estimated as before . by inverting the above equation , we obtain @xmath130 we can also obtain explicit formula for the other popular risk measure , the expected shortfall ( es ) .",
    "essentially , expected shortfall gives the mean loss in case the loss is greater than a given high level ( var ) , and is given by @xmath131 provided that @xmath132 ( otherwise the mean of the distribution does not exist , and the integral is infinite ) .",
    "in the case of our estimated model the shape parameter point estimate is very close to one , @xmath133 , but expected shortfalls can nevertheless be calculated .",
    "the estimated values are tabulated in table [ tbl : gpd_vares_totfi ] .",
    "we end this section with a brief remark concerning solvency regulations . in the solvency ii framework directive of the european union ,",
    "1-year 99.5 % value - at - risk is chosen as the risk measure , whereas in the swiss solvency test , the risk measure used is 1-year 99 % es . in the context of our model , these risk measure values are estimated to be 320 and @xmath134 deaths , respectively , giving the _ population _",
    "level catastophe death counts .",
    "the difference between @xmath135 and @xmath136 , even though not significant for normal distribution , is remarkable here as the distribution of accidental deaths is very heavy - tailed . in this case , the capital requirement based on es would be dramatically higher than that based on var , and this is true more generally for very heavy - tailed distributions .",
    "one could argue that using es as a risk measure correctly captures the tail risk ( the risk that matters most ) , but ultimately the risk measure ( and confidence level ) used in statutory solvency calculations is a policy decision , and depends on how big losses the insurers are required to be able to absorb .",
    "although es has theoretically better properties than var , the present example also shows one of the practical problems related to its use : if the mean of the distribution does nt exist ( here , if @xmath137 ) , the expected shortfall does nt exist either . in the case of our model",
    ", @xmath26 is very close to one with wide confidence intervals , making the use of es somewhat suspect .",
    "having chosen a model for the occurrence of _ catastrophic _ accidental deaths , we further extend it , in order to apply the model to reinsurance pricing in this section .",
    "the accidental death model enables explicit simulation of the times and sizes of accidents , which is key to the pricing approach presented here : combined with individual contract level data , and via a couple of intermediate steps , this allows us to obtain an accurate and consistent distribution of losses .",
    "this is due to the fact that all the features of primary insurance contracts in a reinsured portfolio , as well as the reinsurance contract terms , can be captured as they are without introducing approximations .      the marked poisson point process model of section [ accdeaths ] and subsection [ mppp ] describes the occurrence and sizes of `` big '' events ( defined as those that exceed the threshold @xmath29 ) .",
    "this is enough when we are interested in the extremes , i.e. , the tail probabilities .",
    "however , for pricing purposes we gererally need the full distribution and therefore have to consider also the small events , for which the gpd is not a suitable model . by taking advantage of the independence assumption between event numbers and their associated mark sizes ( subsection [ mppp ] )",
    ", we can conveniently consider the frequency and severity components in turn .",
    "let @xmath138 denote the conditional excess distribution above threshold @xmath139 , @xmath140 , and @xmath141 the conditional distribution below the threshold , @xmath142 , for @xmath116 . denoting the exceedance probability of level @xmath29 by @xmath143",
    ", we can combine the conditional distributions into one unconditional distribution with @xmath144 . as indicated above",
    ", the excess distribution @xmath138 is gpd , while the distributional form of @xmath141 can be taken to be any count distribution , for example negative binomial , providing a good fit to data below the threshold @xmath29 .",
    "the `` small event '' distribution needs to be truncated from above to obtain the conditional distribution @xmath145 with support on @xmath146 $ ] .",
    "let @xmath147 be the full , untruncated distribution underlying @xmath141 , with density ( or probability function ) @xmath148 .",
    "in general , the density of the truncated version of @xmath147 over @xmath149 $ ] , @xmath150 , is @xmath151 . integrating ( or summing ) over this , we get the truncated df @xmath152    the small and big events occur according to ( homogenous ) poisson processes , @xmath153 and @xmath154 , with intensities @xmath155 and @xmath156 , respectively . these are mutually independent , and the poisson intensities can be estimated based on the observed numbers of occurrences , that is , the number of events with death count within @xmath157 $ ] for @xmath155 and @xmath158 for @xmath156 . using the well known property that the sum of independent poisson processes is again a poisson process with rate equal to the sum of the rates of the individual processes , we get that the combined process @xmath159 , with @xmath160 , is poisson with rate @xmath161 .",
    "conversely , let @xmath162 be a poisson process with rate @xmath90 .",
    "suppose that the process is subdivided into two processes , @xmath163 and @xmath164 : that is , arrivals of @xmath165 are independently sent to @xmath166 with probability @xmath167 , and to @xmath168 with probability @xmath169 .",
    "the resulting processes are each poisson , with rates @xmath170 and @xmath171 , and independent of each other .",
    "the above means that we can either simulate the times of occurrence of small and big events separately ; or we can simulate the occurrence of all events at once , and independently simulate the `` type '' of each event , i.e. , the draw the conditional distribution from which to draw the size of the event . the results continue to hold for @xmath172 independent poisson processes @xmath173 .      to account for the fact that our accident size data is effectively left - censored , we proceed by dividing the conditional distribution concerning the smaller death counts ( denoted by @xmath141 in previous section ) in two : the small death numbers on which we have data , in the interval @xmath174:=(3,20]$ ] , are modeled by a ( truncated ) negative binomial distribution ; while the smallest death numbers on which we do not have data , in the interval @xmath175:=(0,3]$ ] , are modeled by selecting discrete probabilities for the accident sizes @xmath176 .",
    "the accidents exceeding threshold @xmath177 are modeled by the marked poisson process of subsection [ mppp ] , with occurrence intensity @xmath178 and mark size distribution @xmath179 .",
    "the intensity of accidents with death count falling within interval @xmath174 $ ] is denoted by @xmath156 and estimated from data as @xmath180 , with the conditional size distribution being truncated negative binomial with parameters @xmath181 and denoted by @xmath138 .",
    "are missing .",
    "the resulting distribution is then truncated into the interval @xmath174=[u_1 + 1,u_2]$ ] . ]    finally , we specify the intensity @xmath155 for accidents with sizes in @xmath175 $ ] subjectively , by looking at the ratios of the frequencies of different sizes of accidents ( and the individual `` intensities '' of such accidents ) in the combined data , and extrapolating based on these .",
    "we end up with a combined intensity estimate of @xmath182 for the smallest accidents .",
    "the conditional probabilities @xmath183 , @xmath184 , making up the distribution @xmath141 are obtained as the ratios of individual intensities to the combined intensity @xmath155 .",
    "is to subjectively specify the probability of no events occurring , which is @xmath185 ; from this we get @xmath186 once @xmath187 is specified . ]    we can now write the combined size distribution of accidents , given the occurrence of an accident , as @xmath188 . here",
    "@xmath189 , with @xmath190 , is the probability that an event ( conditional on occurring ) is of type @xmath191 , meaning that the death count associated with the event is drawn from the conditional distribution @xmath192 .",
    "table [ tbl : example_f_params ] lists the component distributions together with their parameters .",
    "furthermore , the unconditional distribution of event sizes can in this case be expressed as @xmath193 , with @xmath194 the probability of no event occurring .",
    "given the combined point process model of the previous subsection , the simulation procedure for obtaining the loss distribution for the primary insurer is outlined below .",
    "we assume a simulation period of length @xmath195 , typically one year , and an initial number of insureds of @xmath196 in the insurance portfolio . for each simulation @xmath197 ( suppressing the dependence on @xmath198 to lighten notation )    1 .",
    "simulate the occurrence of events within @xmath199 $ ] from a poisson process with rate @xmath90 .",
    "denote the total number of events by @xmath200)$ ] ; if @xmath201 , set total loss to zero and continue to next simulation ; otherwise denote the times of events by @xmath202 , @xmath203 .",
    "2 .   simulate the number of deaths @xmath204 for each event @xmath203 .",
    "for this , first simulate the type of event @xmath108 : with probability @xmath189 , the number of deaths is drawn from distribution @xmath192 , where @xmath205 .",
    "3 .   for each event @xmath108 , simulate the proportion of lives lost in the event that were insured ( with a policy of the type under consideration ) . call this the insured percentage and denote it by @xmath206 $ ] .",
    "the number of insured deaths in event @xmath108 is given by @xmath207 .",
    "4 .   for each event @xmath108 , simulate the proportion of insured lives lost that are covered by the insurance company .",
    "call this the covered percentage and denote it by @xmath208 $ ] .",
    "the number of covered deaths in event @xmath108 is now @xmath209 .",
    "if @xmath210 and @xmath211 ( i.e. , not all insureds have died ) , randomly draw @xmath212 insureds from the ones still alive .",
    "set the state of the chosen insureds to dead , reduce @xmath196 , terminate all the contracts still in force of each insured , and calculate the claim amounts @xmath213 , @xmath214 based on contract terms for each individual contract .",
    "the total claims amount from event @xmath108 is @xmath215 .",
    "sum all the claims from each event @xmath203 to get the total claims amount @xmath216 for the current simulation path .",
    "repeating the procedure for @xmath217 times , we obtain @xmath217 independent realizations of the primary insurer s total claims amount @xmath218 , that is , a distribution for losses .",
    "to arrive at the loss distribution for the reinsurer , we overlay or embed the rules of the reinsurance cover into the simulation steps .",
    "for instance , per occurrence aggregate limits are naturally applied to aggregate losses @xmath219 from each event , whereas per risk limits are applied to losses from each individual affected policy , @xmath213 .",
    "the realizations of the poisson process , or the event times , in the first step can be simulated by any of the standard techniques , such as summing exponential waiting times .",
    "a relatively efficient way is to first simulate the total number of events falling in the interval @xmath199 $ ] , @xmath220 , and then use the well - known order statistic property of poisson process : given the number of events @xmath221 in @xmath199 $ ] , the event times are distributed as uniform order statistics over @xmath199 $ ] .",
    "that is , we may simulate iid uniform rvs @xmath222 $ ] , @xmath223 , and order them to get a realization of the times of the poisson process @xmath224 .    in step two , the accident severities",
    "are simulated from the generalized pareto distribution , or from a truncated count distribution chosen for the smaller accidents .",
    "this can be done using the standard inverse transform method based on uniform random numbers .",
    "be a distribution and @xmath1 a truncated version of it , truncated to interval @xmath149 $ ] .",
    "the inverse of @xmath1 is given by @xmath225 for a quantile @xmath167 .",
    "random variables can be generated based on this . ]    moving to steps three and four , the distribution of the proportion of people dying in a particular event that are insured is assumed to be product type specific , within a given market / country .",
    "similarly , the distribution of the proportion of insured accident victims that are insured in a given company is both product type and company specific .",
    "we propose to use the insurance coverage percentage , that is , the proportion of people in a given country / market having a specific type of insurance policy , as a proxy for the mean of the distribution of insured percentage @xmath226 .",
    "this , or similar , figure is usually available from statistics published by government institutions or national insurance associations . in the same vein",
    ", we propose to use the market share of the insurance company ( within the specific product type under consideration ) as a proxy for the mean of the distribution of covered percentage @xmath227 .",
    "the actual distributions for these proportions can be taken to be any distributions on the interval @xmath228 $ ] ( or truncated to it ) which the user regards reasonable .",
    "see also @xcite for another approach , which goes directly from total lives lost to customers lost without accounting for the insurance coverage within a country / market .",
    "the main advantage of the presented pricing approach , relying on individual contract level simulation , is the explicit simulation of accidents  and ultimately the customers of an insurance company dying in these accidents  combined with contract level data , which generally allows for more accurate distribution of losses : when simulation is done at policy level , we do nt need to assume some specific distributional form for the claims , but can calculate the exact contractual amounts . bringing the simulation to contract level also makes it possible to model features of contracts ( both underlying primary , and reinsurance ) which otherwise can not be readily accounted for .",
    "below we list some additional advantages that the microsimulation approach offers , in addition to the general benefits ( such as flexibility and the possibility to explicitly simulate order and times of events ) shared with traditional frequency - severity simulation methods , and with the approach presented in @xcite :    * consistent handling of random events : if an insured dies , all the policies where ( s)he is the insured terminate at that point ; and the contracts where ( s)he is the policyholder but not the insured can either terminate or not , depending on the terms of the specific contract .",
    "explicitly accounting for the time dimension also means that the same insured can not die more than once ( i.e. , the same insured , or policy , can not be drawn many times during a simulation path ) .",
    "* accounting for contract maturity : the termination of policies during the reinsurance period can be taken into account based on the maturity date of each policy . *",
    "explicit modeling and inclusion of new business : this can be very important in treaty reinsurance if the new policies incepted during the reinsurance coverage period are automatically covered by the reinsurance agreement .",
    "new policies can be included by picking policies ( randomly or non - randomly ) from an external data file as they become available ( according to their inception date ) , or by simulating the number and characteristics of policies to be added . *",
    "possibility to explicitly model the sizes of total and insured populations during each simulation path in the catastrophe context : see appendix [ accfor_inspop ] . *",
    "modeling of additional characteristics of the accident events : for example , we can attach a certain _ type _ to each event , and this event type can again influence other variables within the simulation , such as ( the existence of ) reporting delays ( see subsection [ ibnr ] ) individually for each affected contract . * contract - level attribution : it is possible to directly attribute losses and reinsurance premiums down to the level of individual policy .",
    "all the features mentioned above , except the last , contribute to producing a more accurate distribution for claims .",
    "depending on the intended use and the features of both the reinsurance and the underlying primary insurance contracts , the specific advantages listed above can have importance .",
    "they are certainly nice from a technical point of view .",
    "naturally , there may be cases where policy level simulation does not bring much benefits compared to a simpler individual claim , or even aggregate claim , simulation model in practice  as always , a right tool should be chosen for each task .",
    "the link that appears the weakest in our approach is , admittedly , the conversion of death counts from market level events into company - specific death numbers .",
    "however , we regard the adopted approach of using the insurance coverage and market share as proxies to determine the means of the affected proportion distributions as a pragmatic choice . with this approach ,",
    "all the different possible outcomes are covered in principle ; and if the distributions are chosen reasonably , the weights ( probabilities ) assigned to each outcome provide a realistic result , given a large enough number of simulations .",
    "in contrast to p&c ( re)insurance  where it can take a long time before a claim is reported to the insurer , and a long time thereafter before the claim is fully developed and thus the final amount known  in insurance products offering cover for death the reporting delays are usually negligible , and do nt have practical importance from modeling point of view .",
    "that is , for all practical purposes the deaths caused by accidents can usually be assumed to be reported and known without delay .",
    "there s generally no development period either , and thus no need to model the development of ( new and open ) claims with respect to time , as oppposed to p&c insurance  in fact , with these assumptions there is no such a thing as open claims",
    ".    however , there can still be cases where modeling the reporting delays and thus the ibnr ( incurred but not reported ) claims is relevant in life / accident insurance also .",
    "the reporting delay could depend on the size of an accident , as well as on its type : for instance , only deaths related to big enough accidents might be taken to have a reporting delay , and the distribution of the delay might depend on the type of an accident . to give just one example ,",
    "some of the casualties of the 2004 tsunami took some time to be identified ( and some were never found ) .",
    "if desired , the reporting delays can be implemented within the current simulation framework by simulating a reporting delay for each death of an insured within each accident . in the pricing context",
    "the inclusion of reporting delays does not affect the primary insurer s total claims amount for some product types ( such as term insurance with specified fixed death benefits ) , but might affect them for others ( such as savings contracts , unless the death benefit payment is tied to the savings amount at the time of death ) ; depending on reinsurance terms , it might also affect the reinsurance recoveries ; and if discounting is used or inflation is included , it has an effect , though probably not very significant in most cases . if the simulation model is used for solvency assessment of the primary insurer , instead of pricing , reporting delays mean that a claims reserve ( inbr ) might form at the end of the simulation period .",
    "the accidental death model used in this section can be easily extended to include the non - catastrophe related ( `` normal '' ) small accidents , by specifying a mortality model for these , and then simulating the `` normal '' accidental deaths for each interval @xmath229 $ ] , @xmath230 , of @xmath199 $ ] .",
    "this is done separately for each insured , and the accidental death intensity can depend on the gender , age , and other characteristics of the insured .",
    "furthermore , the model can be extended to include also normal  non - accident related  mortality in exactly the same way .",
    "this makes it possible to account for all mortality risk in a unified way within one model .",
    "we apply the simulation approach to rating of different reinsurance contracts , building on our case study of the finnish accidental deaths in section [ accdeaths ] .",
    "we consider a portfolio consisting of @xmath231 risk life / term insurance policies , with each one belonging to different insured for illustration purposes . in the event of death ,",
    "the policies pay out the risk sum ( death benefit ) agreed in the contract to the beneficiary .",
    "the insurance portfolio , although hypothetical , represents a realistic example of a portfolio of a finnish life insurer .",
    "the mean risk sum in this example is around 63 keur , while the minimum and maximum are 5 keur and 10 meur , respectively .",
    "the total risk sum of the portfolio is approximately 25 geur .",
    "figure [ fig : amts_hist_300k ] displays the distribution of risk sum sizes , separately for smaller and bigger sums to provide better impression . we observe the spikes in the number of contracts corresponding to round numbers , such as 20 , 50 , and 100 keur , among others .",
    "this is typical of real risk life portfolios , and shows that modeling the risk sums ( i.e. , the claim size distribution ) by using a continuous distribution is not likely to yield very realistic results ; at least , a number of discrete point masses would have to be mixed with a continuous distribution , but even this is likely not enough to capture the typically relatively long and sparse tail of the actual risk sum distribution .",
    "we use beta distribution for both the insured percentage ( @xmath232 ) distribution and the covered percentage ( @xmath233 ) distribution , and initially assume that these distributions remain the same for each accident @xmath108 during the simulation period .",
    "the mean @xmath234 of the @xmath226 distribution is taken to be the proportion of people in finland who have a ( private ) risk life insurance policy , which is approximately 40 % according to statistics .",
    "the market share of our example company is assumed to be 20 % , and this is used as the mean @xmath235 of the @xmath227 distribution .",
    "the mean of the beta distribution with parameters @xmath236 is given by @xmath237 ; in this example we choose the parameter values @xmath238 and @xmath239 , with the resulting values @xmath240 and @xmath241 as the means are fixed .",
    "the parameters for the insured distribution are chosen so that all sizes of outcomes are possible , but very low and very high values are little less likely .",
    "the covered distribution parameters , on the other hand , are chosen so that the probability of a very high value ( i.e. , all the insured persons covered in the company ) is not too negligible , and at the same time there is a real chance that none of the insured victims are insured in the specific company .      to illustrate the simulation approach to pricing",
    ", we consider the following types of non - proportional reinsurance contracts written on the example portfolio :    * * per risk excess - of - loss ( xl ) * , with cedant s retention and the limit of coverage applying on a per risk ( policy ) basis .",
    "the reinsured layer is taken to be `` 10 m xs 100k '' , i.e. , losses in excess of the retention 100 keur are covered up to the limit 10 meur .",
    "in addition , we assume an annual aggregate limit ( aal ) of 80 meur , that is , the total losses covered by the reinsurer are limited to this amount per annum , and an annual aggregate deductible ( aad ) of 1 meur , meaning that the first 1 meur of reinsurance recoveries are retained by the ceding company . * * per occurrence excess - of - loss * , with layer 50 m xs 0.5 m , providing a limit of coverage in excess of the retention on a per occurrence ( i.e. , event , accident ) basis .",
    "that is , the per occurrence treaty applies to all risks affected by a single event ( as defined in the reinsurance agreement ) , providing catastrophe protection .",
    "we additionally assume that the contract includes one free reinstatement , and an annual aggregate limit of 150 meur . * * stop - loss * , where the reinsurer covers all the ceding insurer s losses in excess of a given amount ( or a given loss ratio percentage ) .",
    "this is technically similar to excess - of - loss contract ( aggregate xl ) , but the risk is now the total claims amount from the insurance portfolio / business .",
    "we consider a stop - loss contract with 20 meur retention and 500 meur limit .        without loss of generality ,",
    "we consider the typical 1-year contract period ( @xmath242 ) , and run @xmath243 simulations for each reinsurance contract .",
    "figure [ fig : sim_realization_perocc_wlegend ] illustrates the simulation output by showing one example realization for the per occurrence reinsurance cover .",
    "table [ tbl : sim_results ] shows the results from one set of simulations .",
    "the means of the loss distributions are of interest in pricing , as they give expected loss costs from the corresponding contracts . very high",
    "quantiles are also displayed to show the working of the reinsurance contracts at the extreme tail of the loss distribution .",
    "as shown by the second column of the table , the loss cost is estimated to be approximately @xmath244 eur for the per risk ( policy ) contract , @xmath245 eur for the per occurrence contract , and @xmath246 eur for the stop - loss contract , with the specific features described above ; put another way , the loss cost compared to the layer limit ( sometimes the rate on line ) is 0.060 % ( of the 10 meur per policy limit ) , 0.090 % ( of the 50 meur per occurrence limit ) , and 0.0050 % ( of the 500 meur stop - loss limit ) , respectively , for these contracts . of course , these are not the reinsurance contract prices yet , as the very large uncertainty has to be taken into account in the form of a suitable risk load , based on the standard deviation perhaps , or , more appropriately , on the higher quantiles of the loss distribution ; see the end of this subsection for a very brief note on the actual pricing .    the statistics for events and event sizes , including the resulting ground - up losses on the primary insurance portfolio before reinsurance ( all of which are independent of the modeled reinsurance contracts ) can be used to gauge the reasonability and accuracy of individual simulation runs . in this respect , the used amount of @xmath243 simulations appears to be enough to provide consistent results for pricing purposes .",
    "however , due to the extreme heavy - tailedness of the gpd severity component , the losses in the extreme right tail can differ from simulation to simulation , and thus the results are not exactly the same each time .",
    "note , in particular , that the lower quantiles are similar between the three simulations , but there are differences in the highest quantiles . looking at the maximum claim amounts , we observe that in the `` per occurrence '' simulation an event happened that wiped the whole insurance portfolio , whereas the other two do not include an event of this magnitude  however , based on other simulations not shown here , this only materially affects the utmost right tail of the loss distribution . for reinsurance pricing purposes ,",
    "the maximum possible loss is usually limited , effectively mitigating the problems with estimating the very highest quantiles of the underlying ( primary insurer s ) loss distribution .    for a comparison",
    ", we also ran another set of simulations where the insured and covered proportions ( modeled with beta distributions above ) were fixed at their expected values instead , to assess the effect this random component has on the results .",
    "it turned out that the variation in the outcomes reduced , as expected , in that the values of lower quantiles of the loss distributions increased while those of higher quantiles decreased . in sum",
    ", the the loss costs for the per policy and per occurrence reinsurance contracts decreased , to around @xmath247 eur and @xmath248 eur , respectively , while loss cost of the stop - loss contract ( with higher variation anyway ) essentially remained the same .    finally , note that we concentrated here on the estimation of the pure or net premium , corresponding to the loss cost to the reinsurer without additional loadings or margins .",
    "the gross reinsurance premium usually contains several possible elements such as expense loading , brokerage fee , ceding commission , and risk load or targer return rate .",
    "ultimately , the actual premium rate quoted may be partly determined by other considerations , such as strategic or market conditions , or overall customer relationship . in spite of this , knowing the `` correct '' technical premium rate remains essential .",
    "in the first part of this paper , we examined poisson point process models for the occurrence and sizes of extreme accidental , or catastrophic , deaths .",
    "we provided a case study concerning accidents in finnish population ; due to the scarcity of data , we based the estimation of the ( conditional ) loss size distribution on combined finnish and swedish accident data .",
    "this approach turned out to yield good results in practice .",
    "based on statistical tests and model comparisons , a marked poisson process model with independent generalized pareto distributed marks was chosen as the final model .",
    "this is the model originally proposed in @xcite ; our paper provides justification for the choice .",
    "the model describes the observed big accident events well , while being simple in structure .",
    "it also allows extrapolation outside the range of data and the assessment of the risk of events more extreme than those observed so far , a key requirement for a useful catastrophe risk model . as the implied gp distribution",
    "does not describe the smaller death counts well , we adopt a pragmatic approach and model smaller accidents separately with a suitable count distribution ( or several of them ) .",
    "the resulting combined mpp model is able to capture the whole range of accidental death counts flexibly .    with the great uncertainty related to extremes , the limitations of any statistical model have to be acknowledged .",
    "still the fact remains that tail risk needs to be assessed , and models are needed for that purpose ; the models that are ultimately used should have a sound basis .",
    "we see the catastrophe risk modeling approach presented in this paper as a tool to support decision making and to facilitate an honest assessment of the risk and uncertainty related to extreme events .    in the second part of the paper we presented a framework for reinsurance pricing , based on individual contract level simulation and using our extended accidental death model .",
    "the microsimulation approach enables the use of all the information a ( re)insurer has , with the potential to provide more accurate results than traditional pricing techniques : by explicitly simulating the occurrence of accidents and the resulting deaths of insureds , the terms of each affected contract can be individually accounted for , leading to an accurate and consistent estimate of ultimate claims and outstanding liabilities . as all the claims are recorded at contract level per each accident , it is straightforward to overlay or embed the features of reinsurance contracts into the simulation .",
    "the approach makes it possible to model even complex reinsurance features without approximations . as an application",
    ", the simulation model was used to price non - propotional excess reinsurance contracts with additional aggregate features .",
    "the method presented here is tailored for life and accident catastrophe reinsurance , but could be useful in non - life contexts also .",
    "some of the features we discussed might be seen as unnecessary complications from a purely practical point of view , taking into account the fundamental uncertainty that is anyway related to modeling and pricing catastrophes . however , it is our view that we should try to minimize the uncertainty as much as possible where this can be plausibly done by proper ( i.e. , more realistic ) modeling .",
    "the contract level simulation approach may also reveal risks that remain hidden when using aggregate methods , or even when simulating individual claims without direct reference to underlying contracts .",
    "our approach does not constrain the modeler or force the use of approximations  instead , ( s)he is free to include the level of detail fit for purpose .",
    "in addition to pricing and reinsurance design , the presented microsimulation approach is also suitable for assessing the risks of an insurance portfolio from the viewpoint of a primary insurer",
    ". this can include assessment of the need for reinsurance , as well as examining the ( life ) catastrophe risk from a solvency perspective .",
    "the author would like to thank dr .",
    "lasse koskinen from model it ltd and aalto university for helpful comments on the paper ; and mr .",
    "tapani tuominen , formerly with kaleva mutual insurance company , for providing what formed the basis of the accidental death data used in this study .    99    k.  antonio and r.  plat , _ micro - level stochastic loss reserving for general insurance _ ,",
    "scandinavian actuarial journal ( 2013 ) , published online 17 may 2013 .",
    "balkema and l.  de  haan , _ residual life time at great age _ , annals of probability * 2 * ( 1974 ) , 792804 .",
    "coles , _ an introduction to statistical modeling of extreme values _ , springer , new york , 2001 .",
    "d.j . daley and d.  vere - jones ,",
    "_ an introduction to the theory of point processes : volume i : elementary theory and methods _ , 2nd ed . , springer , 2003 .",
    "daykin , t.  pentikinen , and m.  pesonen , _ practical risk theory for actuaries _ , chapman and hall , london , 1994 .",
    "e.  ekheden and o.  hssjer , _ pricing catastrophe risk in life ( re)insurance _ , scandinavian actuarial journal ( 2012 ) , published online 20 aug 2012 .",
    "embrechts , c.  klppelberg , and t.  mikosch , _ modelling extremal events for insurance and finance _ , springer , berlin , 1997 .",
    "fisher and l.h.c .",
    "tippett , _ limiting forms of the frequency distribution of the largest or smallest member of a sample _ , proceedings of the cambridge philosophical society * 24 * ( 1928 ) , 180190 , available online at : http://hdl.handle.net/2440/15198 .",
    "m.  flower , _ a review of papers relevant to non - life reinsurance ( with a liability focus ) _ , general insurance convention , institute and faculty of actuaries , 2006 .",
    "gnedenko , _ sur la distribution limit du terme dune srie alatoire _ , annals of mathematics * 44 * ( 1943 ) , no .  3 , 423453 .    m.r .",
    "leadbetter , g.  lindgren , and h.  rootzn , _ extremes and related properties of random sequences and processes _ , springer , berlin , 1983 .",
    "m.r . leadbetter and h.  rootzn ,",
    "_ extremal theory for stochastic processes _ , annals of probability * 16 * ( 1988 ) , 431478 .    a.j .",
    "mcneil , _ estimating the tails of loss severity distributions using extreme value theory _ , astin bulletin * 27 * ( 1997 ) , 117137 .    a.j .",
    "mcneil , f.  rdiger , and p.  embrechts , _ quantitative risk management : concepts , techniques and tools _ , princeton university press , princeton and oxford , 2005 .",
    "y.  ogata , _ statistical models for earthquake occurrences and residual analysis for point processes _ , journal of the american statistical association * 83 * ( 1988 ) , 927 .",
    "papush , _ a simulation approach in excess reinsurance pricing _ , casualty actuarial society forum , casualty actuarial society , spring 1997 , pp .",
    "philbrick , _ a practical guide to the single parameter pareto distribution _ , proceedings of the casualty actuarial society , no .",
    "lxxii , casualty actuarial society , 1985 , pp .",
    "j.  pickands , _ the two - dimensional poisson process and extremal processes _ , journal of applied probability * 8 * ( 1971 ) , 745756 .",
    "j.  pickands , _ statistical inference using extreme order statistics _ , annals of statistics * 3 * ( 1975 ) , 119131 .",
    "m.  pigeon , k.  antonio , and m.  denuit , _ individual loss reserving with the multivariate skew normal framework _ , astin bulletin * 43 * ( 2013 ) , 399428 .",
    "swiss re , _ the essential guide to reinsurance _ ,",
    "swiss reinsurance company , available online at : http://media.swissre.com/documents/the_essential_guide_to_reinsurance_updated_2013.pdf .",
    "resnick , _ extreme values , regular variation , and point processes _ , springer , new york , 1987 .",
    "h.  rootzn and n.  tajvidi , _ extreme value statistics and wind storm losses : a case study _ , scandinavian actuarial journal * 1 * ( 1997 ) , 7094 .",
    "smith , _ extreme value analysis of environmental time series : an application to trend detection in ground - level ozone _ , statistical science * 4 * ( 1989 ) , 367393 .",
    "in some cases we might want to explicitly take into account the population ( market ) size and the size of insured population . the following example makes this clear : in the setting of our example , suppose that the total population in the insurance market considered is initially @xmath249 million people , and suppose further that @xmath250 % of the population have an insurance policy of the kind we are interested in , meaning that a total of @xmath251 million people are insured for our purposes .",
    "now if an accident happens where the death count @xmath204 is greater than @xmath252 million ( say it s 4 million ) , we know that the number of insured people dying in the accident is at least @xmath253 ( say , 1 million ) . here @xmath254 and @xmath255 are the total and insured population sizes , respectively , just before accident @xmath108 , at time @xmath256 ( with @xmath257 ) .",
    "let @xmath258 be the ( simulated ) insured proportion in accident @xmath108 in one simulation , as before",
    ". in the context of the simulation framework , accounting for the limited insured population size means that we replace the number of insureds affected , @xmath259 , with @xmath260 after each accident , the total and insured population sizes are reduced by the numbers of casualties , @xmath204 and @xmath261 , respectively , subject to a floor of zero . as the population sizes reduce due to accidents , also the insurance coverage in the population changes ( depending on who dies and who does nt ) , and is given by the new proportion of insureds to total population as @xmath262 applying between @xmath263 $ ] ( i.e. , between accidents @xmath108 and @xmath264 ) ; here the notation means @xmath265 , that is , the number of insured population immediately after accident @xmath108 ( at time @xmath266 )  in general , this does not need to be same as @xmath267 ( at time @xmath268 ) if , for instance , other deaths than those caused by ( bigger ) accidents are modeled , or if new business is included in the simulation model ; see section [ pricing_discussion ] .    a procedure similar to the one described above applies also to the number of insured people , dying in a given accident , that are covered by a specific insurance company .",
    "say that the market share of the particular company is @xmath269 % , in which case the number of people ( who have the insurance protection in question ) covered by the company is taken to be @xmath270 million at the beginning . again ,",
    "if the number of insured people dying in an accident is greater than @xmath271 ( 1.6 million initially in our example ) , we know that at least a certain amount of them inevitable are insured in the company in question . denoting by @xmath272 the random proportion of insured accident victims who are covered by the company ,",
    "we thus replace the number of covered deaths , @xmath273 , with @xmath274 again , the remaining number of people covered by the company after accident @xmath108 is denoted by @xmath275 , and the new market share ( used as proxy for the mean of the covered percentage distribution for the next accident ) is given by @xmath276 if @xmath277 , and @xmath278 otherwise .",
    "the simulations of subsection [ pricing_example ] were repeated with the described explicit accounting for population sizes : in our case , the obtained results did not materially differ from the previous ones without it ( shown in table [ tbl : sim_results ] ) .",
    "however , for populations that are smaller still , the explicit accounting for population sizes could be important . in these cases",
    "smaller accidents can be enough to affect the outcomes .",
    "* mean excess plot . * the mean excess function @xmath279 of a rv @xmath31 ( if exists ) gives the mean of the excess amount over @xmath29 , conditional on @xmath31 exceeding @xmath29 . for gp distribution ,",
    "this is given by @xmath280 , and is thus a linear function of @xmath29 ; the linearity remains when considering higher thresholds @xmath281 .",
    "the choice of threshold can be based on this property . in practice",
    "we use the estimate of @xmath282 based on the sample @xmath283 , the sample mean excess function @xmath284 and look visually for a value @xmath285 from which the _ mean excess plot _ @xmath286 becomes roughly linear ( after allowing for sample variance ) ; for more details , see @xcite .",
    "figure [ fig : meplot_tot ] shows the sample mean excess plot for the combined finnish and swedish data .",
    "we see that the plot curves up until around the value 30 and seems to straighten after that , with some kind of slight `` kink '' already at the value 20 . based on the mean excess plot alone",
    ", we might choose a threshold value of 30 .",
    "similar conclusions are drawn by considering the finnish data alone , although in that case there s more evidence for choosing the lower threshold 20 ( analysis not shown ) .",
    "* stability of parameter estimates . *",
    "generalized pareto distribution has a well - known `` stability property '' , in the sense that the excess distribution of a gp distribution is always also gp distribution .",
    "this means , in practice , that if generalized pareto is an appropriate model for excesses over a given threshold @xmath29 , it is that also for all higher thresholds @xmath281 .",
    "the excess df has the same shape parameter @xmath26 but a scaling that grows linearly with threshold , @xmath287 , where @xmath288 is the scale parameter corresponding the initial threshold @xmath289 .",
    "the estimate of @xmath26 based on the observed sample should therefore stay relatively constant ( after allowing for sample variance , in practice stable ) after threshold @xmath289 , if gpd is a reasonable distributional model for excesses over @xmath289 .",
    "similarly , the scale parameter @xmath290 should change in an approximately linear fashion , _ except _ if @xmath24 ; the latter inconvenience can be avoided by considering an alternative parametrization @xmath291 , which is constant with respect to @xmath29 .",
    "the parameter estimates for different threshold values are shown in figure [ fig : varthr_tot ] . in this case",
    "the threshold value 30 looks still possible , but likely not the best choice . in particular , for the pricing and risk assessment purposes we have in mind , relative stability of the model output with respect to small variations in the parameters is essential . in this light",
    "the value 30 seems too high .",
    "instead , we take the threshold to be @xmath88 .",
    "this choice is also supported if we look at the finnish data alone .",
    "underlying both the threshold exceedance method ( implicitly ) and the point process approach , in its basic form , is the assumption that the exceedances of a high threshold occur according to a homogenous poisson process , i.e. , the numbers of exceedances in separate intervals are iid poisson - distributed random variables .",
    "if this assumption is not fulfilled , the basic methods of evt formulated for iid data are not automatically applicable , and it might be necessary to consider more general models for adequately describing the data - generating process .",
    "let @xmath292 denote the exceedance times . from the basic properties of poisson distribution",
    "we know that if the exceedance times follow a poisson process with intensity @xmath90 , the inter - arrival ( or waiting ) times @xmath293 are iid exponential rvs with parameter @xmath90 .",
    "further , the quantities @xmath294 are then iid and uniformly distributed on @xmath295 .",
    "we use kolmogorov - smirnov ( k - s ) test to test the uniformity of the sample @xmath296 : the null hypothesis that the data is uniformly distributed is not rejected .",
    "left panel of figure [ fig : iatimes_tot_cdf_uk1 ] shows the comparison of the empirical distribution function with the reference distribution @xmath297 , together with 95 % confidence intervals . in the present case",
    "the k - s test does not have much power to reject the null hypothesis due to the small number of observations  this is demonstrated by the wide confidence intervals .",
    "however , the figure shows no particular evidence against the uniformity of @xmath296 and hence the poisson - distributedness of the exceedance times .    as in @xcite , the independence of inter - arrival times",
    "can also be tested by plotting the transformed waiting times @xmath298 against preceding waiting times @xmath299 .",
    "if the ( adjacent ) inter - arrival times are independent , the points @xmath300 should be uniformly distributed on the unit rectangle @xmath301 .",
    "right panel of figure [ fig : iatimes_tot_cdf_uk1 ] contains such a plot .",
    "the scarcity of points makes the interpretation somewhat difficult , but there does nt seem to be any structure",
    ". already seem to follow a homogenous poisson process ( analysis not shown ) . ]          based on the tests of the previous appendix , the accidental death data can be taken to be generated by a time - homogenous poisson process .",
    "however , we will further investigate the existence of possible trends  and whether the model can be improved by including trend components  by fitting nonhomogenous point processes to the data .",
    "we start by considering the ( time - homogenous ) pot model .",
    "this establishes the base case to which other models are compared .",
    "the points of the process @xmath73 consists of points @xmath302\\times(u,\\infty)$ ] from the underlying sequence of rvs @xmath303 that exceed the chosen threshold @xmath29 .",
    "the realization of the point process @xmath73 is thus a set of points @xmath304 , where @xmath91 is the number of exceedances and @xmath202 and @xmath305 represent the time and size of an exceedance , respectively .    with the intensity of the poisson point process @xmath73 at a point @xmath306 being @xmath307 whenever @xmath308 , and @xmath309",
    "otherwise ( see ) , the likelihood based on the observed exceedance times @xmath310 and sizes of exceedances @xmath311 ( with @xmath312 ) is @xmath313\\times(u,\\infty)\\right)\\right\\}\\prod_{i=1}^{k}\\lambda(\\tilde{x}_i ) \\\\",
    "= \\exp\\left\\{-n\\tau(u)\\right\\}\\prod_{i=1}^{k}\\lambda(\\tilde{x}_i ) , \\ ] ] where @xmath314 and @xmath315 ( see subsection [ bg_pp ] ) .",
    "the log - likelihood is therefore given by @xmath316 when @xmath317 for all @xmath318 .",
    "the parameter estimates are obtained by maximizing the ( log-)likelihood numerically . for more details on point process intensities and likelihood functions ,",
    "see e.g. ( * ? ? ?",
    "* ch .  7 ) .",
    "fitting the pot model to the accidental death data using treshold @xmath88 , we get the mles @xmath319 with the maximum log - likelihood being @xmath320 .",
    "the above implies a very heavy - tailed distribution with infinite variance . we can check that the parameters correspond to the estimates @xmath109 and @xmath321 of the marked point process formulation in section [ mppp ] ( using the relationship @xmath322 between the models ) .",
    "these are compared to the homogenous model of previous section , denoted by @xmath330 , to examine whether trend components can improve the model performance .",
    "fitting the models by maximum likelihood , we get the following parameter estimates : @xmath331 the log - likelihood at the maximums are in both cases approximately @xmath332 . as model @xmath330 is a special case of the other models ( with @xmath333 ) , we can use the standard likelihood ratio ( lr ) test to compare the models . according to the test , including a trend to the location or scale parameter",
    "does not improve the fit of the model in a statistically significant way  in fact , it barely improves it at all .",
    "goodness - of - fit tests confirm this ."
  ],
  "abstract_text": [
    "<S> recently , a marked poisson process ( mpp ) model for life catastrophe risk was proposed in @xcite . </S>",
    "<S> we provide a justification and further support for the model by considering more general poisson point processes in the context of extreme value theory ( evt ) , and basing the choice of model on statistical tests and model comparisons . a case study examining accidental deaths in the finnish population is provided .    </S>",
    "<S> we further extend the applicability of the catastrophe risk model by considering small and big accidents separately ; the resulting combined mpp model can flexibly capture the whole range of accidental death counts . using the proposed model </S>",
    "<S> , we present a simulation framework for pricing ( life ) catastrophe reinsurance , based on modeling the underlying policies at individual contract level . </S>",
    "<S> the accidents are first simulated at population level , and their effect on a specific insurance company is then determined by explicitly simulating the resulting insured deaths . </S>",
    "<S> the proposed microsimulation approach can potentially lead to more accurate results than the traditional methods , and to a better view of risk , as it can make use of all the information available to the re / insurer and can explicitly accommodate even complex re / insurance terms and product features . as an example we price several excess reinsurance contracts . </S>",
    "<S> the proposed simulation model is also suitable for solvency assessment .    : </S>",
    "<S> accidental deaths , catastrophe risk , extreme value theory , generalized pareto distribution , life and accident insurance , poisson point processes , pricing , reinsurance , solvency . </S>"
  ]
}