{
  "article_text": [
    "the goal of reinforcement learning is to come up with a way to optimally interact with the environment , modeled as a sequential decision making process . in case of fully observable domains ,",
    "the environment is modelled as a markov decision process . off the shelf ,",
    "algorithms like q - learning are designed to find the optimal policy in such domains . in the case of partially observable domains ,",
    "the environment is modeled as a partially observable markov decision process ( pomdps ) .",
    "solving a pomdp is still a field of research due to the non - scalability of value iteration like approaches .",
    "one of the big disadvantages of pomdps is the inherent need to specify the underlying states beforehand .",
    "this is because the state representation in pomdps consist of beliefs over latent set of states .",
    "this causes problems since in some sense `` states '' are an imaginary entities unobservable to the agent .    to alleviate the above problem a new way of modeling partially observable domains known as predictive state representation was introduced . in this model",
    ", there is no concept of states .",
    "the agent using this model learns and plans entirely using only observations .",
    "this has a huge advantage over pomdps in that , not only we do not need to mention the number of states beforehand , but also some systems which can not be modeled as a pomdp can be modeled as a psr [ 8 ] [ 10 ] .",
    "this is the main reason we choose predictive state representation as our basic framework .",
    "recent trends in reinforcement learning indicate the development of algorithms that can achieve transfer of knowledge across domains .",
    "transfer learning is an active area of research in reinforcement learning .",
    "the goal behind transfer learning is to learn how to act in some ( possibly smaller ) source tasks and transfer that knowledge to perform better in a ( possibly larger ) target domain .",
    "transfer learning in partially observable domains has not been attempted before .",
    "such an algorithm would find applications in many real - world problems since many of them are partially observable .",
    "consider a robot trained to grasp a series of regular objects like a sphere , a pyramid and a cube with noisy sensors .",
    "our aim is to model each of these tasks as a psr . with the model ,",
    "the robot now has learned a policy for each of the tasks . given an object with a very complicated shape , the robot has to learn to grasp the new object from scratch which would take more time and be very inefficient .",
    "the motivation behind this work is to develop a framework that uses the already learned regular object grasping policies as input and transfer that knowledge to come up with a policy to grasp the complex object .",
    "the advantage of using psr is their ability to handle uncertainty robustly , so even in the presence of noisy perception system our framework would be able to transfer that knowledge to a larger task .    in this paper",
    "we give a basic formulation for the transferring policy between a set of source tasks and a target task .",
    "we model them as predictive state representation so that our algorithm applies to both fully and partially observable domains . from the standpoint of the basic formulation we develop two algorithms for transfer , test them in a simple domain and discuss the results and shortcomings in detail in this work .    in the next section",
    "we give a high level overview of related approaches , followed by a mathematical overview of psrs",
    ". then we formulate the problem mathematically .",
    "we then explain our procedure in detail , i.e. how we learn the model , how we plan using the learned model and then explain our approach for transfer .",
    "we then show the results of our experiments followed by inferences and directions for future work .",
    "there are a variety of different approaches available in the transfer learning literature for different environment settings . but all of them address only fully observable domains . [ 8 ] presents a good survey of the available transfer learning literature . on the other hand , many real - world problems are partially observable . to our knowledge ,",
    "none of the previous works address transfer learning between partially observable domains .",
    "one of the approaches that is most relevant to our goal is transfer learning using bisimulation metrics [ 3 ] .",
    "the bisimulation metric specifies the extent of similarity between two states in a markov decision process .",
    "this metric bounds the difference of value functions between the two states . but",
    "no such metrics have been developed for partially observable domains so far .",
    "the closest approach available in literature is the development of bisimulation equivalence , which states whether two belief states in a partially observable markov decision process are similar [ 3 ] .",
    "it does not give information about the extent of similarity .",
    "various algorithms have been proposed for learning the psr model . [ 4 ] formulated the first discovery algorithm for linear psrs with reset .",
    "spectral learning provides a framework for both learning and planning simultaneously .",
    "this method was exploited by [ 6 ] [ 2 ] to integrate learning and planning in predictive representations . [ 4 ] have also made progress in the problem of planning using a learned model in which they have suggested value iteration and approximate q - learning methods . in our approach",
    "we use approximate q - learning using cerebellar model articulation controller ( cmac ) [ 1 ] [ 5 ] .",
    "a controlled dynamical partially observable discrete system with a finite set of actions @xmath0 can emit a finite number of observations @xmath1 .",
    "an agent in the system executes an action @xmath2 and perceives an observation @xmath3 .",
    "a history @xmath4 at any point of time in the system is defined as an action - observation sequence that has occurred till that point of time @xmath5 .",
    "a test is defined as an action - observation sequence that may occur in the future @xmath6 .",
    "the prediction for test @xmath7 given by @xmath8 is defined as the probability that the test @xmath7 will occur in the future given the agent s current history is @xmath4 .",
    "a predictive state representation is a tuple @xmath9 where , @xmath10 is the set of observations , @xmath11 is the set of possible actions , @xmath12 is the set of core tests , @xmath13 $ ] is the matrix whose columns are vectors associated with one - step extended core test , @xmath14 is the matrix whose columns are vectors associated with one - step tests , @xmath15 is the initial prediction vector .",
    "we use the term `` state vector '' , `` belief '' and `` prediction vector '' interchangeably to refer @xmath16 .",
    "when a new action - observation pair is observed we update the state vector as follows .",
    "@xmath17    any @xmath18 for any @xmath19-length test @xmath6 , can be calculated from the psr model using the expression[10 ] ,    @xmath20    to do planning in psr , we incorporate a discrete set of rewards @xmath21 along with the observation [ 11 ] .",
    "we have a set of source tasks @xmath22 and a target task @xmath23 .",
    "these tasks are modeled as a predictive state representation , defined by the tuples @xmath24 and @xmath25 respectively .",
    "we assume both the source and target tasks has the same discrete observation and action set , and we have access to the full model of either tasks .",
    "action selection is done in the source task @xmath26 by using @xmath27 function as follows @xmath28 our goal in transfer learning is to transfer the policy from source to the target . more formally , for any state in target @xmath29 we have to find an appropriate state in source such that following the optimal policy w.r.t the source would give optimal action for the current state in the target .",
    "mathematically , we want to find @xmath30 , such that , @xmath31 we denote the extent of similarity of a source task @xmath32 to the target task as a function of @xmath29 as @xmath33 .",
    "we sometimes overload the similarity operator with two arguments , @xmath34 which represents the extent of similarity of target task in the current state @xmath35 with the source task when its state is @xmath36 .",
    "since we have a set of source tasks , we have to find the most similar source @xmath37 where @xmath38 our goal is to devise an algorithm to find @xmath30 and @xmath39 .",
    "the @xmath40 in @xmath30 denotes the optimal belief mapping . throughout the paper",
    "we denote the task to which a symbol belongs using superscripts .",
    "for example we denote core test @xmath41 of task @xmath26 as @xmath42 .",
    "we first learn the model of the source tasks and target task .",
    "then we find the optimal @xmath41 function for each of the source tasks using approximate q - learning .",
    "we then develop and test two of our approaches for transfer on the target task .",
    "we use analytical discovery learning ( adl ) algorithm [ 4 ] to build the model of the psr . since we have access to the pomdp model of the environment",
    ", we can calculate any entry of the @xmath43 matrix .",
    "initially , we enumerate all entries corresponding to one step tests and histories .",
    "we find the one step core tests and histories , by taking the linearly independent columns and rows .",
    "this is followed by calculating the entries associated with one step extensions to core tests and histories .",
    "this procedure is repeated till the rank of core matrix on two successive iterations remains constant .",
    "then the linearly independent rows and columns are taken as the final core test / histories .",
    "coretest , @xmath41 @xmath44 \\ { }    p(@xmath45 ) @xmath44 * belupdate*(@xmath46(@xmath47),@xmath41 ) p(@xmath48 ) @xmath44 * belupdate*(b(@xmath49),(@xmath50 ) ) p(@xmath51 ) @xmath44 * belupdate*(@xmath46 ( @xmath52),(@xmath50 ) ) @xmath49 @xmath44 * independentrows*(@xmath53 ) @xmath41 @xmath44 * independentcolums*(@xmath53 ) * return * @xmath53,@xmath41,@xmath49    the @xmath18 associated with any test @xmath7 can be found using the formula , @xmath54    where @xmath55 is the core matrix obtained at the end of the adl algorithm and @xmath56 is the probability of seeing test @xmath7 from each of the @xmath49 core histories . using this formula ,",
    "the model parameters @xmath57 of the psr can be obtained .      as the psr state containing the prediction vectors is continuous and high - dimensional planning",
    "has to be done in a continuous space .",
    "james _ et al . _",
    "[ 5 ] used cerebellar model articulation controller ( cmac ) [ 1 ] as a function approximator to implement @xmath27-learning [ 9 ] .",
    "this is the planning stage in which the @xmath27 values of the state and action space are approximated in an online fashion .",
    "cmac is a class of sparse coded memory that has @xmath58 overlapped and offset tilings with each of them having number of edges equal to the length of query variables .",
    "each edge of the tiling spans the entire space of corresponding state variable and is quantized into various levels based on its sparsity and length of the prediction vector .",
    "any combination of state vector components and the action value activates exactly one portion of each tiling known as a tile .",
    "every such tile of the entire network is initialized with a random @xmath27 value that is reinforced over time to learn the optimal value .",
    "the @xmath27 value of state @xmath26 containing the prediction vector and action @xmath59 is given by ,    @xmath60    where @xmath61 returns the indices of the tiles that gets activated upon querying and @xmath62 returns the corresponding value which are summed up to give the state action value .",
    "the online @xmath27-learning works by updating the value of appropriate tiles by @xmath63 , where @xmath64 is the per grid learning rate and @xmath65 is given as ,    @xmath66    the generalization and resolution of cmac depends on number of tilings and number of tiles within each tiling . + * remark : * the sparsity of cmac grows exponentially with the length of the prediction vector . a domain with @xmath67 quantization levels for each state and @xmath68 tilings will have @xmath69 tiles to pick @xmath70 features for every iteration .",
    "moreover there is a trade - off between weighting the quantization levels of each dimension and computational efficiency in selecting unequally sized tiles within each tiling .      _",
    "definition : _ we define the projection of a test @xmath7 onto a task @xmath71 after history @xmath4 as @xmath72 , the probability of occurrence of @xmath7 on task @xmath71 after history @xmath4 @xmath73 if we have a set of tests @xmath74 , with a slight abuse of notation , we represent the projection vector as @xmath75 we interchangeably use the terms `` projection of test onto a task '' and `` projection of a task onto a test '' .",
    "they both mean the same .",
    "we define @xmath76 as the projection of source core test on target .",
    "it is the probability of occurrence of source core test on the target at any point of time .",
    "mathematically , @xmath77 we can calculate this by the following expression @xmath78 where @xmath79 is the matrix that relates the probability of occurrence of source core tests @xmath80 in target task @xmath23 , whose columns can be calculated using eq.[mo ] .",
    "we use this projection matrix @xmath79 for every iteration to select an action .",
    "we propose that the magnitude of similarity is proportional to the probability of occurrence of core - test .",
    "so we calculate similarity as @xmath81 .",
    "this results in the core - test projection algorithm .",
    "+ source set @xmath82 + target @xmath83 @xmath84 @xmath85 @xmath86 @xmath87 @xmath88 @xmath89 @xmath90 @xmath91 @xmath92    the core - test projection algorithm does not work well . the way we choose @xmath93 as @xmath94 is flawed .",
    "choosing a source task that has the core tests with maximum probability of occurrence in the target does not work well because core tests are not necessarily the most important tests of a task . by",
    "important tests we mean that higher probability of occurrence of these tests in the two tasks implies maximum similarity .    as a solution",
    "we introduce a set of tests called * validating tests*. these tests signify , that equal probability of occurrence of these tests , in source and target is geared towards optimal transfer .      instead of projecting the target onto the core - test of source , we now project both the source and the target onto a common set of validating tests .",
    "we then find the distance between their projections and draw a conclusion on their similarity .",
    "remember that from the psr model that the probability of any test occurring given the current belief is @xmath95 .",
    "let the validating test be @xmath96 .",
    "hence the projection of a task @xmath23 onto these tests is given by @xmath97 .",
    "value of @xmath98 for any test @xmath7 and any task @xmath19 is calculated using eq.[mo ] .",
    "let @xmath99 be the projection of task @xmath26 onto validating tests @xmath100 after observing history @xmath4 .",
    "we define the distance between source(@xmath26 ) and target(@xmath23 ) as the dot product between their projections on validation tests given by , @xmath101    even for two similar tasks , their projections depend on their histories .",
    "we assume that there exists a history in every source task which when applied results in a similar configuration as in target task . applying a history to a source",
    "means updating the initial belief of the source using the history as observed trajectory .",
    "we call the history that maximizes the similarity between a source @xmath32 and the target while at the same time has a high probability of occurrence as * history offset * @xmath102 of that source . mathematically ,    @xmath103    @xmath104 @xmath105 @xmath106 @xmath107 @xmath108 @xmath109 @xmath110 @xmath111 * return * action    the numerator of eq.[s1 ] is the probability of history occurring given the initial state , @xmath112 .",
    "the denominator is the distance between the source and target as we discussed in the previous paragraph .",
    "a sub - problem of finding similarity would be to find the history offset of every source task with respect to the target . solving eq.[s1 ]",
    "is formulated as a search problem . in this problem",
    "the search space is huge .",
    "for a history of length @xmath113 the search space is exponential in action observation space @xmath114 .",
    "we use genetic algorithm to search through this huge space as the solution is confined to a very small part the space and there are potentially many such solutions i.e. more than one history can result in same belief state .    for a given task ,",
    "the algorithm generates random trajectories of random lengths .",
    "the length of each trajectory is limited to the rank of system dynamics matrix of that task .",
    "it generates @xmath115 such trajectories and initialize them as initial population .",
    "the genetic algorithm uses the function under maximization , as the fitness function eq.[s2 ] .",
    "@xmath116    since we find the history offset at every time step in target , we use current belief for the target in [ s2 ] that is obtained after observing the current history @xmath4 in target .",
    "the genetic algorithm crosses - over between two selected individuals from the population based on their fitness .",
    "since the fitness of any individual lies between 0 and 1 , @xmath117 , we can not get high variance in the weights ( for prominent selection of individuals with high fitness when weighted sampling is used ) .",
    "hence the fitness of every individual is reassigned to the rank of every individual ( ranked based on their current fitness ) .",
    "we select ( weighted selection using our updated fitness ) @xmath115 pairs or @xmath118 individuals for crossover .",
    "the cut - point @xmath119 is chosen between 1 through maximum length of two parents under crossover using weighted - random sampling .",
    "encouraging histories of shorter lengths , the algorithm gives more weights to 1 through minimum length of the parents .",
    "crossing over at the cut point gives @xmath118 children . with a fixed probability our method tweaks the actions and observations in the children .",
    "also with a small probability it adds selected @xmath120 ( one time step action - observation trajectory ) at the end of an individual .",
    "this one - step @xmath120 is either randomly selected or copied from the last action - observation of an individual .",
    "for rejection , the algorithm rejects @xmath115 of @xmath118 such individuals based on their fitness .",
    "next , it reassigns the current population to the selected children .",
    "we repeat this algorithm for a fixed number of generations . the individual with maximum fitness",
    "is taken as the history offset for the source under search .    at every time - step",
    "our algorithm finds the history offset @xmath121 of every source @xmath37 .",
    "we apply this history offset to the initial belief of the corresponding sources .",
    "mathematically , @xmath122 .",
    "the source to transfer policy from is selected using eq.[s2 ] as @xmath123 finally , the policy from @xmath124 after applying its history offset @xmath125 is transferred at this time - step .",
    "@xmath126    thus , we approximate @xmath30 as @xmath127",
    "[ [ section ] ]    we evaluated our proposed transfer algorithm on two different sets ( set-1 and set-2 ) of partially observable mazes depicted in fig.[set1 ] & [ set2 ] .",
    "the target task of set-1 and set-2 have 6 and 8 free cells respectively .",
    "we use partially observable pacman ( pocman ) with one pellet and no ghosts ( we consider only navigation through a maze ) .",
    "the agent can not sense its exact position but can sense the presence of wall in specified directions .",
    "we simplified the environment and dimensions in observation space for the adl to model the world in reasonable time .",
    "in set-1 we sense in west and east directions while in set-2 we sense in north and west directions .",
    "hence the size of observation space is 4 .",
    "every sensor ( in each direction ) reads the correct value with probability 0.95 . at each time step , the agent takes an action in one of the four directions .",
    "the action gets executed with probability 0.9 otherwise the agent moves in one of the perpendicular directions .",
    "if the agent bumps into a wall it remains in place .",
    "the agent receives a -1 reward at every time - step except at goal state where it receives + 10 .            for a given set of tasks",
    ", our algorithm learns the psr model of each task from their pomdp model using adl .",
    "we generated 100 trajectories each of length 50 in every source task .",
    "the trajectories are used to train cmac for approximate q - learning .",
    "the @xmath27-learning algorithm is run using a completely random policy .",
    "the action edge of the cmac is discretized into @xmath128 equal segments , 4 here .",
    "every component of the prediction vector is the probability of core test occurring given history @xmath129 and ranges between 0 and 1 . in our case ( set-2 ) the source task and target task have 4 and 8 core tests respectively .",
    "for example , the edge representing the prediction vector component of cmac for the target task is quantized in steps of 0.125 .",
    "also there are 8 overlapping hypercube tilings offset by 0.015625 units .",
    "the tile values initialized randomly and the update is applied online .    in genetic algorithm ,",
    "the maximum size of every individual in the population limited is limited to 10 .",
    "the best individuals for crossover was selected with a probability 0.8 .",
    "the probability of mutation was fixed to 0.15 . during mutation we either randomly tweak the action , observations in individuals or insert or delete elements from individuals with probability 0.5 .",
    "finally , individuals with low fitness are rejected with a probability of 0.9 . at every iteration",
    "our algorithm deleted randomly selected individuals and initialized them to random histories to keep the diversity up in population .    in set-2 , we used @xmath130 , [ ( n2 - 1)(n0 - 1)(e1 - 1)(e1 - 1 ) ] , [ ( e1 - 1)^2 ] , [ ( n2 - 1)(n0 - 1)(e2 - 1)(e2 + 10)]\\}$ ] as the validating set of tests",
    ". there are four tests in the set .",
    "the first element in each test is an action followed by observation and reward .",
    "for example @xmath131 denotes taking the action south and observing 2 ( binary equivalent : 10 meaning we observe no wall in north and a wall in west ) and a reward -1 .",
    "given the initial states as shown in fig.[set2 ] , the genetic algorithm returns history offsets of source-1 and source-2 as @xmath132 and @xmath133 respectively .    after this update on initial beliefs of both the sources with their corresponding history offsets and finding their respective projections on the set of validating tests , the projection of @xmath134 was close to the projection of target .",
    "in fact , the correlation between the projections of @xmath134 and target was 0.902 while that of @xmath135 and target was 0.001 . given the updated ( after applying history offset ) belief of @xmath134 , the q - function returned the action south , which is the expected optimal action given the initial state in target fig.[set2 ] ( set-2 ) .",
    "when the position of pocman was at the red dot ( fig.[set2 ] ) , the history offsets of source-1 and source-2 were @xmath136 and @xmath137 .",
    "after applying these offsets to their respective sources ( the pocman will be at the blue dots . fig.[set2 ] ) , we see that the optimal action in both these environment is east. which implies we must observe almost equal similarity between both the sources and target .",
    "the similarity of @xmath134 and @xmath135 were 0.73 and 0.42 respectively .",
    "the results of similarity vs different states is plotted in fig.@xmath138 .",
    "table [ y=@xmath139 , x = s]simi.dat ; table [ y=@xmath140 , x = s]simi.dat ;    to illustrate the validity of our approach we compare the cumulative reward received by following policy returned from planning in target environment ( regular approach ) and that returned from our transfer algorithm .",
    "the benchmark for our transfer is to converge to the cumulative reward received from planning .",
    "the cumulative reward ( averaged over 10 trials ) against the no . of generations and",
    "population is given in fig [ g2 ] .",
    "[ g2 ] was plotted with constant population , @xmath141 and fig .",
    "[ g3 ] was plotted with constant generations , @xmath142 .",
    "we ran the algorithm till 100 time - steps or till the pocman emerges victorious , whichever happened first .",
    "hence the maximum possible return is + 2 while the minimum possible return is -100 . considering the limited time available for genetic algorithm at every time - step , we used generations = 30 , population = 50 throughout our experiments .",
    "table [ y=@xmath139 , x = i]popcons.dat ; coordinates ( 0,1.9 ) ( 100,1.9 ) ;    table [ y=@xmath139 , x = p]itercons.dat ; coordinates ( 0,1.9 ) ( 80,1.9 ) ;",
    "the results we obtained were promising",
    ". however more empirical results from other domains is needed . using the adl algorithm we were able to model only simple pocman environments with small observation spaces .",
    "the same transfer algorithm should be tested using models of complex environment .",
    "tpsr [ 7 ] provides a good framework for modeling complex environments .",
    "also we have assumed that the cost of learning the model to be zero or that the model is given to us .",
    "our next step would be to improve our algorithm for online learning where the model of the target task is not known beforehand .",
    "interleaved learning and planning [ 6 ] provides a method to simultaneously learn and plan using psrs .",
    "one important direction of research is to incorporate the transfer algorithm into this framework .",
    "we used hand - coded validation tests for set-1 and set-2 .",
    "we are currently investigating ways to automatically find a good set of validating tests given the source tasks .",
    "one simple example to show the limitations of our approach would be a source pocman where the optimal action consist of only `` east '' , and a target pocman where the optimal action consists of only `` north '' .",
    "our current approach does not address the problem of finding the appropriate action mappings .",
    "we are working an approach to map a given action - observation in target to an action - observation in source . at every time - step we can find the change in history offset and relate this change to action - observation in target .",
    "this shall provide the required mapping . with this mapping finding the history offset at",
    "every time step is unnecessary .",
    "we also plan on searching in beliefs space instead of histories , since this would lead to a smaller search spaces or we could even find a closed form solution for the optimal belief .",
    "one future work could be to extend the bisimulation metrics to psrs and use it to transfer the policy .",
    "this is the first work for transfer in partially observable environments .",
    "we develop a basic framework for the problem .",
    "we show in simple experiments the successful transfer of policy to solve a partially observable maze .",
    "our method was able to find similarity ( quantitatively ) between tasks and was able to transfer policy from the most similar source task .",
    "our method is suitable when there is a reachable configuration in one of the source tasks that is similar to the target task and has the same optimal action ( as in target task ) . in case",
    "if all sources have no such configuration or no common optimal action , the algorithm needs an action map between the sources and the target .",
    "we thank prof .",
    "dmitry berenson for his constant motivation and constructive feedback throughout the project .",
    "we also extend our gratitude to prof .",
    "balaraman ravindran , mr .",
    "prasanna parthasarathi and mr .",
    "janarthanan rajendran from iit madras",
    ".      1 .   j.  s.  albus . a theory of cerebellar function .",
    "_ mathematical biosciences _ , 10(1):25 - 61 , 1971 .",
    "b.  boots , s.  m.  siddiqi , and g.  j.  gordon . closing the learning - planning loop with predictive state representations . _ the international journal of robotics research _ , 30(7):954 - 966 , 2011 .",
    "p.  s.  castro .",
    "on planning , prediction and knowledge transfer in fully and partially observable markov decision processes .",
    "_ mcgill university , phd thesis , _ 2011 .",
    "m.  r.  james and s.  singh .",
    "learning and discovery of predictive state representations in dynamical systems with reset . in _ proceedings of the twenty - first international conference on machine learning _",
    ", page 53 .",
    "acm , 2004 .",
    "m.  r.  james and s.  singh .",
    "planning with predictive state representations .",
    "_ ieee proceedings .",
    "2004 international conference on machine learning and applications _ , pages 304 - 311 , 2004",
    "s. ong , y. grinberg , and j. pineau .",
    "goal - directed online learning of predictive models . in _ recent advances in reinforcement learning ,",
    "volume 7188 of lecture notes in computer science _ ,",
    "pages 18 - 29 .",
    "springer berlin heidelberg , 2012 .",
    "isbn 978 - 3 - 642 - 29945 - 2 .",
    "m.  rosencrantz , g. gordon , and s. thrun .",
    "learning low dimensional predictive representations . in _ proceedings of the twenty - first international conference on machine learning _",
    ", page 88 .",
    "acm , 2004 . 8 .",
    "m.  e.  taylor and p.  stone .",
    "an introduction to inter - task transfer for reinforcement learning . _ ai magazine _ , 32(1):15 - 34 , 2011 . 9 .",
    "c.  j.  c.  h.  watkins .",
    "learning from delayed rewards .",
    "_ phd thesis , university of cambridge , england _ , 1989 . 10 .",
    "b.  d.  wolfe .",
    "modeling dynamical systems with structured predictive state representations .",
    "_ phd thesis , university of michigan _ , 2009 .",
    "m.  l.  littman , r.  s.  sutton , s.  singh .",
    "predictive representations of state .",
    "advances in neural information processing systems 14 ( nips ) , pages 1555 - 1561,2002"
  ],
  "abstract_text": [
    "<S> * in this paper we tackle the problem of transferring policy from multiple partially observable source environments to a partially observable target environment modeled as predictive state representation . </S>",
    "<S> this is an entirely new approach with no previous work , other than the case of transfer in fully observable domains . </S>",
    "<S> we develop algorithms to successfully achieve policy transfer when we have the model of both the source and target tasks and discuss in detail their performance and shortcomings . </S>",
    "<S> these algorithms could be a starting point for the field of transfer learning in partial observability . * </S>"
  ]
}