{
  "article_text": [
    "shannon s source - channel separation theorem asserts that there is no essential loss in point - to - point communication systems , when the source coding component and channel coding component are designed and operated separately @xcite .",
    "this separation architecture simplifies the overall communication system tremendously , because the decoupled subsystems are much easier to design and implement , with the codeword index as the only interface between the source coding component and the channel coding component .",
    "unfortunately , it has been shown that the separation approach is not optimal in very simple multiuser scenarios ( e.g. , @xcite ) , which suggests that the optimality of source - channel separation may not hold beyond the conventional point - to - point case .    because of the clear benefits of the source - channel separation architecture , it is important to understand the optimality issue better . in this work ,",
    "we seek to answer the following sequence of questions : is there a general class of multiuser communication systems for which :    * the separation approach is optimal ? * if the answer to the first question is negative , then is the separation approach at least approximately optimal ?",
    "the difficulty in answering these questions lies in the fact that in most multiuser communication scenarios , we do not have explicit characterizations of the rate - distortion regions , the channel capacity regions , or the joint coding achievable distortion regions ; however , in order to determine whether the separation approach is optimal , it is natural to first couple the rate - distortion region and the channel capacity region , then compare it with the joint coding achievable distortion region . with at least one region unknown in most cases , it seems utterly impossible to answer the above questions even in some of the simplest settings ( e.g. , communicating sources on an interference channel ) , let alone in more complex networks . in this work , we show that this difficulty in determining the optimality of source - channel separation can in fact be circumvented completely in several important settings , and the answers to the sequence of questions posed earlier are indeed positive .",
    "more precisely , we show that for lossy coding of memoryless sources in a network , the source - channel separation approach is optimal for the following two general scenarios : the first scenario , referred to as _ distributed network joint source - channel coding _ ( dnjscc ) , is when the sources are arbitrarily correlated , each of which is to be reconstructed at possibly multiple destinations within certain distortions , but the channels between any pair of nodes in this network are synchronized , orthogonal , and memoryless ; the second scenario , referred to as _ joint source - channel multiple unicast with distortion _ ( jscmud ) , is when the sources are mutually independent , each of which is to be reconstructed only at one destination within a certain distortion , but the channels can be general , including multi - user channels such as multiple access , broadcast , interference and relay channels , possibly with feedback .    the third scenario is a natural extension of the second one by allowing a source to be reconstructed at multiple destinations ; this case is referred to as _ joint source - channel multiple multicast with distortion _ ( jscmmd ) . for this scenario , the classical example of sending a gaussian source over a gaussian broadcast channel",
    "@xcite reveals that the source - channel separation approach is not optimal in general .",
    "thus we turn our attention to whether the separation approach is approximately optimal , and show that under the  difference \" distortion measure , it is indeed so in the sense that the loss from the optimum can be upper - bounded . in the important special case of quadratic distortion measure , the upper bound is at most 0.5 bit per ( additional ) user which reconstructs the same source .",
    "though our results demonstrate the effectiveness of source - channel separation , a large portion of the difficulty in designing efficient codes remains in the extracted pure source coding problem or the extracted pure channel coding problem . particularly for the dnjscc problem",
    ", the network induces a pure source - coding problem with rather complex multiple - session user interactions where each session has the same rate , which suggests a distinct and perhaps under - studied line of research in network source coding .",
    "some simple interactive settings have indeed been considered in the literature : kaspi considered the lossy two - way source coding problem @xcite ( see more recent results in @xcite ) , and in a series of papers , orlitsky considered the lossless two - way problem from the perspective of worst - case vs. average - case communication complexity @xcite .",
    "our work suggests that it is important to consider the interactive coding problems for the case of a large ( or infinite ) number of sessions , and the interactive communication rates do not need to change from session to session .",
    "this class of source - coding problems naturally occur in practice , and their solutions in fact solve a large class of joint source channel coding problems .    when discussing source - channel separation , it is natural to assume that the sources are independent of the channels , which is often the default assumption in the information theory literature , and it is also assumed in this work .",
    "more precisely , the assumption is that the channel output is conditionally independent of the sources given the channel input .",
    "this is because otherwise , even if the encoding and decoding functions are designed according to the separation architecture , the inherent dependence between source and channel will render such a separation rather meaningless even in a point - to - point setting , and joint source - channel coding is expected to be more effective in these scenarios .    in the rest of this section",
    ", we give a brief overview of our proof approach , and discuss the relation between this work and existing work in the literature .",
    "as mentioned earlier , the direct proof approach of first characterizing all the relevant regions in their single letter forms , and then making comparison is rather difficult , thus instead , our approach is an indirect one .    for the distributed network joint source - channel coding problem ,",
    "the key observation is in fact extremely simple , which is to abstract the block channel input into a super source which can be compressed with certain rate .",
    "roughly speaking , for a fixed joint source - channel code , the block channel input of this code can be viewed as a super source , and thus we can simulate the channel output by compressing this super source with some lossy source code , whose compression rate is upper - bounded by the channel capacity of the original channel . thus any joint source - channel code can be converted into a separation - based code with asymptotically the same performance , which implies the optimality of source - channel separation .",
    "the first example in section [ sec : examples ] illustrates this idea for a simple dnjscc problem . for more complex networks with relay and feedback",
    ", there is an additional difficulty introduced by the interaction of the users , and this can be resolved by properly interleaving many copies of the original joint source - channel code ( see also the discussion in the next subsection regarding the relation with @xcite ) .",
    "the technique we rely on to tackle the second scenario , _",
    "i.e. _ , joint source - channel multiple unicast with distortion , is complementary to the previous case . here",
    "the overall communication system is abstracted into an interference channel with certain individual mutual information guarantees that are implied by the achievable end - to - end distortions .",
    "roughly speaking , for a fixed joint source - channel code , the deterministic encoding - decoding operations together with the probabilistic channel operations can be viewed as an induced ( memoryless ) super block channel , where these mutual information guarantees can be utilized to provide a lower bound to the capacity of this super block channel .",
    "thus the sources can be sent over this new super block channels , and it follows again that any joint source - channel code can be converted into a separation - based scheme with asymptotically the same performance .",
    "the second example in section [ sec : examples ] illustrates this case with a small interference network .    for the third scenario of joint source - channel multiple multicast with distortion , a similar abstraction yields a more complex interference channel .",
    "however , for this case , the mutual information guarantees are less straightforward , and they have to be derived in conjunction with the separation - based coding scheme .",
    "the separation architecture we use here is based on successive refinement codes for each source @xcite , concatenated with delivery of messages within degraded message sets @xcite . by strategically comparing the rate expressions of this separation scheme and utilizing the rate loss technique @xcite",
    ", we are able to upper - bound the performance gap between the joint source - channel scheme and the separation - based scheme ( see also discussion in the next subsection ) . as in earlier scenarios ,",
    "this result does not rely on an explicit solution of either the degraded message set problem or the joint source - channel coding problem .",
    "closely related to source - channel separation in the dnjscc problem is the problem of separation between network coding and channel coding , which has received considerable attention in recent years @xcite . in general",
    ", the approach based on separating network coding and channel coding is also not optimal @xcite .",
    "however , a surprising result by koetter , effros and medard @xcite essentially states that for general multicast on networks with orthogonal , synchronized and memoryless point - to - point channels ( _ i.e. , _ `` noisy '' graphs ) , there is no loss of optimality by employing such a separation",
    ". the dnjscc problem we consider can be thought of as a generalization of the problem studied in @xcite to correlated sources with distortions , and in fact the interleaving technique in our proof is directly borrowed from @xcite . the super source view and the channel simulation idea are also inherent in the proof in @xcite , however , they are encapsulated in the `` stacked network '' technique , and thus less transparent .",
    "in contrast , we explicitly apply the super source view and the channel simulation idea on the original network instead of the `` stacked network '' , which results in a conceptually more straightforward proof , despite the more general setting of lossy source coding and correlated sources .",
    "it is worth noting that @xcite does not directly focus on the issue of source - channel separation , and the result is given from the perspective of an equivalence model .",
    "for this reason , the induced pure block source coding problem is not defined formally in their work .",
    "in contrast , we directly consider the separation issue on the original network , and the induced pure source coding problem shall be defined explicitly in the traditional block coding framework .",
    "another approach of treating source - channel separation for various special cases in dnjscc is to utilize the infinite letter expressions , instead of the single letter characterizations , of the source coding rate regions and the joint coding achievable distortion regions .",
    "this is also an indirect approach , since the infinite letter expressions are not computable , and thus do not qualify as characterizations ; see @xcite for discussions on the computability issue . in @xcite , yeung applied this indirect approach to several classical multiuser source coding problems on orthogonal communication channels , including the ( two - user ) distributed source coding problem @xcite , the multiple description problem @xcite and the cascade communication problem @xcite .",
    "xiao and luo @xcite applied this approach to the problem of distributed source coding ( with any number of users ) on orthogonal multiple access channels .",
    "though this approach is successful in these simple settings , it becomes rather unwieldy in more complex networks .",
    "in contrast , our proof approach can be naturally applied to more general networks , and consequently the result for the dnjscc problem in this work subsumes those in @xcite and @xcite .",
    "a more recent work related to the dnjscc problem is by han @xcite , where the problem of lossless coding of correlated sources on acyclic networks ( _ i.e. _ , no feedback ) with orthogonal channels is considered ; see also @xcite for earlier results on such networks with a single sink node .",
    "han provided a necessary and sufficient condition for transmissibility in terms of source entropies and channel cut - set capacity .",
    "the approach took in @xcite is to first give a necessary condition for transmissibility using conventional information inequalities , and then show that a separation - based approach can transmit the sources as long as this condition is satisfied .",
    "this proof approach appears difficult to apply on networks with feedback and lossy reconstructions .",
    "interestingly , the result in @xcite establishes a source - channel separation different from the one we consider in this work , and this intriguing point will be revisited in the last section .",
    "similar to the approach we take in the jscmud problem , the super - channel view was also used in @xcite where non - ergodic point - to - point communications were considered .",
    "the focus of @xcite is mainly on non - ergodic point - to - point channels , whereas our focus is on ergodic channels but in a more general network setting .",
    "the network scenario is quite different from the point - to - point case since it induces a coupling of the transmitted signals through the abstracted interference channel .",
    "the difficulty encountered in the jscmmd problem is similar to that in @xcite , where a simpler version of the problem , _ i.e. _ , broadcasting a single gaussian source with bandwidth mismatch , was considered .",
    "the proof approach in @xcite is to introduce many additional auxiliary random variables not in the original problem ( originally applied on the multiple description problem ; see @xcite ) , then derive explicit outer bound by leveraging the markov relation among them .",
    "our approach for the jscmmd problem is a generalization of this technique , however , the introduced auxiliary random variables serve another role in addition to providing an outer bound : their probability distribution is also used to generate codewords in the successive refinement source codes . by strategically utilizing the rate loss technique @xcite together with these auxiliary random variables ,",
    "we are able to upper - bound the performance loss by the separation - based scheme .",
    "the dnjscc problem and jscmud problem , for which source - channel separation is optimal , include many cases previously considered in the literature .",
    "for example , the joint source - channel coding problem of successive refinement coding with degraded decoder side information in @xcite , and the two - way successively refined joint source - channel coding problem considered in @xcite are special cases of the dnjscc problem ; when the sources are independent , the problem of sending a pair of gaussian sources over gaussian broadcast channels @xcite and sending a pair of gaussian sources over interference channels @xcite , are special cases of the jscmud problem .",
    "shannon s source - channel separation in the classical sense has several immediate implications , some of which may be taken individually as weaker notions of source - channel separation .",
    "for example , tuncel @xcite discovered an `` operational separation '' where the source codebook and the channel codebook are largely designed independently , between which only certain codeword indices serve as their connection .",
    "this notion of operational separation is weaker than the classical source - channel separation , which was referred to as `` information separation '' in @xcite . to see this ,",
    "observe that in the classical source - channel separation architecture , the digital codeword indices are the only information interface between the source coding component and the channel coding component , and particularly the source decoder takes the decoded digital codeword indices from the channel decoder as the sole information provider originating from the channel output , and ignores any other information from the channel .",
    "the decoder in the coding strategy in @xcite is in fact a joint decoder which is impossible to be separated into two components . in our work",
    ", we shall use the notion of source - channel separation in the classical ( informational separation ) sense ; this point regarding the meaning of source - channel separation will be revisited in later sections .",
    "the rest of this paper is organized as follows . in section [ sec : examples ] we discuss a few examples to provide some intuitions for the solutions .",
    "necessary notation and definitions are given in section [ sec : def ] .",
    "the main results and the proofs on dnjscc , jscmud and jscmmd are given in sections [ sec : dnsc ] , [ sec : proofunicast ] and [ sec : proofmulticast ] , respectively .",
    "section [ sec : conclusion ] concludes the paper .",
    "in this section we discuss three examples in the context of sending sources on interference channels to provide some intuitions for the optimality ( or approximate optimality ) of source - channel separation in dnjscc , jscmud and jscmmd .",
    "the main results of this work are built on these intuitions , and sections [ sec : dnsc ] , [ sec : proofunicast ] and [ sec : proofmulticast ] essentially make them more precise and rigorous . for simplicity , the channel bandwidth and source bandwidth are assumed to match .      , width=453 ]    consider the example depicted in fig .",
    "[ fig : example0 ] , where the discrete memoryless sources @xmath0 are correlated .",
    "each discrete memoryless channel between a transmitter and a receiver is orthogonal to the other channels .",
    "more precisely , the channel from node @xmath1 to node @xmath2 has transition probability @xmath3 , and the overall transition probability is given by @xmath4 . both node @xmath5 and node @xmath6 require a lossy reconstruction of source @xmath7 , denoted as @xmath8 and @xmath9 , respectively .",
    "node @xmath6 also requires a lossy reconstruction of source @xmath10 , denoted as @xmath11 .    though the capacity region of this special case of the interference channel is not difficult to establish , the rate - distortion region of the source coding problem is not known , and this problem is at least as difficult as the well known distributed source coding problem @xcite .",
    "thus the conventional proof approach of characterizing separately the rate - distortion region , channel capacity region and the joint source - channel coding achievable distortion region , and then making comparison , does not yield the desired separation result .",
    "next , we illustrate through this example the methodology that enables us to prove the optimality of source - channel separation for the dnjscc problem .",
    ", width=604 ]    suppose there exists a length-@xmath12 joint source - channel code that achieves the distortion triple @xmath13 .",
    "the key observation is the following simple fact .",
    "if we fix this joint source - channel code , then the channel input for any given channel , for example @xmath14 , can be viewed as a super ( block ) source , independent and identically distributed across blocks ; see fig .",
    "[ fig : dnstrans ] .",
    "therefore , we can encode a length-@xmath15 sequence of such blocks using a `` rate - distortion '' code of rate per block slightly greater than @xmath16 , the codewords of which are generated using the distribution @xmath17 .",
    "it follows that with probability approaching one ( as @xmath15 goes to infinity ) we can find a @xmath18 codeword in the codebook that is jointly typical with a channel input sequence @xmath19 , ( _ i.e. _ , a length-@xmath15 vector of the super source samples ) , for sufficiently large @xmath15 .",
    "this lossy source code essentially simulates the channel output over @xmath15 length-@xmath12 blocks , and only the codeword index needs to be known at node @xmath5 to reconstruct @xmath18 . as such , this digital codeword index can be simply sent across this channel using any good channel code , and the original joint source - channel decoding function can be performed on this simulated channel output , which eventually ( asymptotically ) achieves the same distortion as the original code .",
    "it is easy to see that since @xmath20 , where @xmath21 is the channel capacity of the channel between node @xmath22 and node @xmath5 , this rate - distortion codeword index is expected to be reliably transmitted on this channel .",
    "replacing all the channel outputs with such simulated outputs in this problem results in a new scheme . in this ",
    "new \" coding scheme , the codeword indices of these  rate - distortion \" codes are the only informational interface between the source coding component and the channel coding component , and this is a separation - based scheme which asymptotically achieves the same distortions @xmath13 originally achieved by the joint coding scheme . in other words ,",
    "any distortions that are achievable by joint coding scheme can be achieved by a separation - based scheme .",
    "the above observation largely reflects the intuition behind the optimality proof of source - channel separation for the general dnjscc problem , however , some technical details ( besides the asymptotically diminishing quantities omitted in the above discussion ) need to be addressed : the main difficulty is that when the network has relays or cycles , the super source argument given above does not apply since channel usage constraints prevent coding over long super - channel blocks directly .",
    "the proof given section [ sec : dnsc ] will resolve this difficulty through an intricate arrangement of channel simulation .",
    "consider the problem depicted in fig .",
    "[ fig : example1 ] , where the sources @xmath7 , @xmath10 and @xmath23 are mutually independent ; here the interference channel is more generally given by the transition probability @xmath24 , where @xmath25 are the channel inputs by node @xmath22 and node @xmath26 , respectively , and @xmath27 are the channel outputs at node @xmath5 and node @xmath6 , respectively .     on an interference channel.[fig : example1],width=453 ]    since the capacity region of interference channel is unknown , it is impossible to explicitly characterize ( in a single - letter manner ) the achievable distortion region of the separation approach .",
    "however , suppose for the problem in fig .",
    "[ fig : example1 ] , a distortion triple @xmath28 is achievable using some joint source - channel code of length-@xmath12 .",
    "the conventional rate - distortion theorem @xcite dictates that we must have @xmath29 , where @xmath30 is the rate - distortion function of source @xmath31 .",
    "the key is now the following simple fact , which is different from that discussed for the dnjscc problem : if we fix this particular source - channel code , then the transition probability of @xmath32 can be viewed as that of an alternative super interference channel with three users . on this channel ,",
    "the individual mutual information guarantee @xmath29 holds for @xmath33 , due to the conventional rate - distortion theorem .",
    "thus intuitively , this super interference channel should be able to support a rate triple of @xmath34 per super - block channel use ( or per @xmath12 original channel uses ) by a digital code .",
    "now a good digital channel code on this super interference channel can be used to transmit a digital rate - distortion source code for the source @xmath7 , @xmath10 and @xmath23 , respectively .",
    "this is indeed a separation - based scheme , and it can achieve the distortion pair @xmath28 asymptotically .",
    "in other words , any achievable distortion triple @xmath28 can be achieved by the separation approach .    in order to show that the super - interference channel can indeed asymptotically support the rate triple @xmath34",
    ", we essentially need to construct ( random ) codes over large super - channel blocks , and prove that the error probability can be made small , just as in conventional channels .",
    "the proof in section [ sec : proofunicast ] follows this approach and makes the above intuitive argument more rigorous .       on an interference channel to multiple destinations , _",
    "i.e. _ , source @xmath7 is required at both destination node @xmath5 and node @xmath6.[fig : example2],width=453 ]    consider the problem depicted in fig .",
    "[ fig : example2 ] , which is only slightly different from that in fig .",
    "[ fig : example1 ] in that source @xmath7 is to be reconstructed at both node @xmath5 and node @xmath6 , denoted as @xmath8 and @xmath9 , respectively ; the reconstruction of source @xmath23 at node @xmath5 is denoted as @xmath35 and the reconstruction of source @xmath10 at node @xmath6 is denoted as @xmath36 . taking a similar view as in the previous example , the abstracted channel now has transition probability @xmath37 . however",
    ", the mutual information bounds by the conventional rate - distortion theorem can not be directly used as in the previous case .",
    "a moment of thought should convince the readers that the broadcast nature of the marginal transition probability @xmath38 is the culprit , and some additional coding component is needed .",
    "a natural separation architecture is to use a successive refinement @xcite source code to produce descriptions satisfying the distortion requirements for each source and couple it to a superposition broadcast code @xcite to deliver reliably these messages in the degraded message set @xcite .",
    "more precisely , in the example of fig .",
    "[ fig : example2 ] , assume without loss of generality that the distortion for source @xmath7 at node @xmath5 is greater than that at node @xmath6 . we shall use a successive refinement code for @xmath7 to produce messages @xmath39 such that @xmath40 is to be delivered to node @xmath5 and both @xmath39 are to be delivered to node @xmath6 .",
    "node @xmath22 also produces a message @xmath41 to encode source @xmath23 , and node @xmath26 produces a message @xmath42 to encode source @xmath10 . the messages @xmath43 need to be reliably transmitted to node @xmath5 , and the messages @xmath44 to node @xmath6 .",
    "let us for the moment isolate source @xmath7 and focus on the super block broadcast channel @xmath38 with the messages @xmath39 , since it is the main difficulty in generalizing the proof approach for jscmud .",
    "we will introduce an auxiliary random variable ( in general more than one auxiliary random variable is needed ) , and show that this broadcast channel can support a certain rate pair for the degraded message set broadcast @xcite , parametrized by this newly - introduced random variable .",
    "the same probability distribution of this auxiliary random variable is also used to construct successive refinement source code for @xmath7 .",
    "however , the afore - mentioned channel coding rates for degraded message set broadcast are in fact insufficient to support this successive refinement source code ; nevertheless , the shortfall in the rates can be upper - bounded by comparing the broadcast channel coding rates and the successive refinement source coding rates , both of which are parametrized by this afore - introduced auxiliary random variable .",
    "this upper bound implies the approximate optimality of source - channel separation in jscmmd .",
    "one may argue that the schemes described above for the given examples are not really based on source - channel separation , because in the example for dnjscc , the source codes embed in themselves the original joint source - channel codes , and thus these source codes are designed with the knowledge of the channel statistics .",
    "similarly , in the examples for jscmud and jscmmd , the channel codes also embed in themselves the original joint source - channel codes , and thus they are designed with the knowledge of the source statistics . following this argument ,",
    "it can not be claimed that in these scenarios source - channel separation is optimal ( or approximately optimal ) .",
    "this is , however , a rather subtle misconception .",
    "indeed in a scheme based on source - channel separation , the source code should not rely on the channel statistics , and the channel code should not rely on the source statistics .",
    "in fact , the separated source coding problem ( _ i.e. _ , charactering the rate - distortion region ) is defined to be the original joint coding problem without the channel statistics , and the separated channel coding problem ( _ i.e. _ , characterizing the capacity region ) is defined to be the original joint source channel coding problem without the source statistics ; moreover , the optimality of source - channel separation can be defined ( in a manner similar to shannon s original work @xcite ) as when the source coding rate - distortion region coupled with the channel coding capacity region is the same as the joint coding achievable distortion region , and this is precisely the kind of results we shall establish in this work .    without more precise mathematical definitions , it may be difficult to convince the readers that the arguments given in the previous sub - sections indeed show the optimality of source - channel separation .",
    "nevertheless , here we shall provide an informal explanation in the context of the example in fig .",
    "[ fig : example1 ] . note that when each source in fig .",
    "[ fig : example1 ] is replaced by a message , we are left with an interference channel ( with a common message between node @xmath22 and node @xmath26 ) .",
    "the capacity region of this channel is unknown , and the question whether the capacity - achieving codes for this interference channel depends on any source statistics is not even relevant , because there exists no source in this new channel coding problem . in the previous discussion",
    "we have essentially shown that if for a set of sources @xmath45 the distortion triple @xmath28 is achievable in the joint coding setting , then the rate triple @xmath46 is also achievable on this interference channel in the pure channel coding setting .",
    "though the channel code constructed in the previous sub - section relies on the source statistics , the statement that `` the rate triple @xmath47 is achievable on this interference channel '' does not depend on any source , and the newly constructed channel code can be understood as merely a tool to show this statement is true .",
    "if the rate triple @xmath47 is achievable on this interference channel , then in the joint coding setting , we can also use other capacity achieving channel codes ( not related to the original source statistics ) to send the individual rate - distortion codeword index for each source @xmath31 , which results in a separation scheme .",
    "conversely , the channel code constructed in the previous section can also be used to send rate - distortion codes for any other sources , not necessarily the original sources , and this is also an apparent separation scheme . at this point , it is clear that we can indeed conclude source - channel separation is optimal in this setting . in sections [ sec :",
    "dnsc ] , [ sec : proofunicast ] and [ sec : proofmulticast ] , the formal proofs follow a similar line of argument in a more rigorous manner .",
    "in this section , we provide the notation and define the codes in consideration .",
    "the notation would become rather unwieldy if a unified framework were used for all the problems treated in this work .",
    "we therefore forgo this ambitious goal and define the problems separately ; however , when convenient , the notation will be kept consistent among them .",
    "we focus on the problems with finite - alphabet discrete sources , finite - alphabet discrete channels and bounded distortion measures , unless noted otherwise explicitly .      for this case",
    ", the network with a total of @xmath48 nodes can be conveniently written as a directed graph @xmath49 , where @xmath50 is the set of nodes , and @xmath51 is the set of edges between any two nodes ; from here on , for any integer @xmath52 , we use @xmath53 to denote the set @xmath54 . each edge @xmath55",
    "is associated with a discrete memoryless channel , whose transition probability is given as @xmath3 with input alphabet @xmath56 and output alphabet @xmath57 ; these channels are assumed to be synchronized .",
    "each node @xmath1 has a discrete memoryless source @xmath31 , distributed in the alphabet @xmath58 , and the collection of the sources are distributed according to the joint distribution @xmath59 at each time instance . for simplicity , we are inherently assuming these sources are synchronized , and thus the notation @xmath59 is meaningful .",
    "a length-@xmath12 vector of a source @xmath31 is written as @xmath60 , and the @xmath61-th symbol in this vector is written as @xmath62 ; _ i.e. _ , @xmath63 .",
    "similar notation is also used for other random variables .",
    "we use upper case for random variables , and lower case for their realizations . for any set @xmath64",
    ", we write the @xmath65-th order product set as @xmath66 .    for each source",
    ", a distortion measure is defined in a general manner as @xmath67 where @xmath68 is the reconstruction alphabet .",
    "a node @xmath2 may be interested in only a subset of the sources @xmath69 ; notationally , we may write the set of sources that node @xmath2 is interested in as @xmath70 .",
    "next we define the class of codes being considered for the distributed network source coding problem , which are conventional block codes .",
    "[ def : dnsj1 ] an @xmath71 distributed network joint source - channel code on a joint source - channel network @xmath72 consists of the following components :    * at each transmitter node @xmath1 , for each @xmath2 such that @xmath73 , an encoding function for time instance @xmath61 @xmath74 * at each receiver node @xmath2 , for each source @xmath75 , a decoding function @xmath76    the encoding and decoding functions induce the distortions @xmath77 where @xmath78 is the reconstruction of source @xmath79 at node @xmath2 .",
    "here @xmath80 is the source block length and @xmath12 is the channel block length , which imply that there is a source - channel bandwidth mismatch factor of @xmath81 ( channel uses per source sample ) .",
    "note that if a node is not interested in a certain source , the distortion of the reconstruction at this node can simply be assumed to be large .",
    "thus we can write a distortion matrix without loss of generality , whose element @xmath82 is the distortion associated with the reconstruction of source @xmath79 at node @xmath2 .",
    "clearly , without loss of generality , we can let the element @xmath83 and simply define @xmath84 for @xmath85 , where @xmath86 is the distortion achievable at rate zero for source @xmath31 . with this in mind , the region of achievable distortion matrix can be defined as follows .",
    "[ def : dnsj2 ] a distortion matrix @xmath87 is achievable for distributed network joint source - channel coding with bandwidth mismatch factor @xmath88 on a joint source - channel network @xmath72 , if for any @xmath89 and sufficiently large @xmath80 , there exist an integer @xmath90 and an @xmath71 distributed network joint source - channel code , such that @xmath91 , @xmath92 .",
    "the collection of all such distortion matrices is the distributed network joint source - channel coding achievable distortion region , denoted as @xmath93 .    to discuss source - channel separation ,",
    "it is important to clarify the individual source code and channel code used in the separation - based approach . to this end",
    ", we essentially need to define the pure source coding problem and the pure channel coding problem .",
    "the channel coding problem in the dnjscc problem is simply the point - to - point channel capacity problem , and the codes used are naturally block channel codes .",
    "the source coding problem is more complex : intuitively speaking , it is the original problem when the noisy channels are replaced by noise - free bit - pipes . however , this statement needs to be made more precise because although the  bit - pipe \" channels lead to a more intuitive understanding , in a network setting , we also need to specify the timing , and the causality relation of all the channel inputs and outputs ; moreover , when the rates of the channels are not integers , the concept of  bit - pipe \" channel is not easily defined , and this may cause further confusion .",
    "our definition of the pure source coding problem given below naturally eliminates such confusions .",
    "it is important to note that the definition of the block source codes on this network needs to incorporate the interactive communication aspect carefully .",
    "[ def : ddnsc ] an @xmath94 distributed network source code with a total of @xmath95 sessions on a source communication network @xmath96 consists of the following components :    * at each ( transmitter ) node @xmath1 , for each @xmath2 such that @xmath97 , an encoding function for transmission session @xmath98 , @xmath99 where @xmath100 and @xmath101 s are positive integers .",
    "* at each receiver node @xmath2 , for each source @xmath75 , a decoding function @xmath102    the encoding and decoding functions induce the distortions @xmath103 where again @xmath78 is the reconstruction of source @xmath79 at node @xmath2 .",
    "[ def : sourcecodingrd ] a rate - distortion - matrix tuple @xmath104 is achievable on a source communication network @xmath96 , if for any @xmath89 , there exists an integer @xmath95 , such that for any sufficiently large @xmath80 , there exists an @xmath94 distributed network source code such that @xmath105 the collection of distortion matrix @xmath87 for which the rate - distortion - matrix tuple @xmath104 is achievable for a given rate vector @xmath106 is denoted to denote the achievable distortion region for the joint coding problem , and here we slightly abuse the notation by using @xmath107 to denote the distortion - rate function in this pure source coding problem .",
    "this does not cause any confusion since the concept of rates does not naturally exist in the joint coding problem . ] as @xmath107 .    roughly speaking",
    ", @xmath108 is the rate of  noise - free bit - pipe channel \" on edge @xmath109 per source symbol in each session .",
    "each session has the same rate in this source coding problem , and the separation result we present will show that this additional requirement does not cause any loss of optimality .",
    "the above definition is given in the traditional block source coding framework , and there is no need to introduce the concept of  noise - free bit - pipe channels \" , which eliminates any possible confusions accompanied with it . in the above source code , there are a total of @xmath95 sessions of coding which generate code indices at node @xmath110 ; at the end of each session , the index @xmath111 in this session becomes available at destination node @xmath112 , and thus can be used by node @xmath112 in the next coding session .",
    "in other words , the encoding functions need to observe the causality constraints on the session level on this network , by allowing the coding operation at a node to utilize only the source at this node and all previous session information from its incoming edges .    with the above definitions ,",
    "it is clear that we can combine the source codes together with the capacity - achieving channel codes for each channel on the original communication network .",
    "more precisely , we can define the achievable distortion region using such a separation approach as @xmath113 where @xmath114 is the channel capacity between node @xmath1 and node @xmath2 , and we sometimes also write it as @xmath115 with any @xmath55 .",
    "it is straightforward to see that @xmath116    the separation approach we take is indeed in the classical sense , since not only the source codes and the channel codes are defined separately , the digital codeword indices are the only information interface between them during encoding and decoding .      for this case , there are a total of @xmath52 mutually independent discrete memoryless sources , denoted as @xmath31 , distributed in the alphabet @xmath58 according to some distribution @xmath117 , @xmath118 ; note that the index @xmath1 here is not related to the index of the node , which is the case for the distributed network source coding problem given in the last section . for notational simplicity , we shall assume all the sources are synchronized .",
    "the distortion measures are defined similarly as in the last subsection ; here we do not allow the existence of multiple distortion measures for the same source , or distortion measures defined on the functions of more than one source .    let the number of nodes be @xmath48",
    ". for the jscmud and jscmmd problem , the graph theoretical notation is not suitable , since multiuser communication channels with broadcast and multiple access signal interactions are involved . for notational simplicity",
    ", we treat the overall communication network as a single channel , with inputs @xmath119 over the alphabets @xmath120 and outputs @xmath121 over the alphabets @xmath122 , and transition probability given by @xmath123 ; here @xmath124 and @xmath125 are the channel input and output at node @xmath1 , respectively .",
    "this model naturally includes feedback , and we have inherently assumed the channels are synchronized by using this notation . note that this general channel model includes the case that the overall network consists of many independent individual multiuser channels , and moreover , some of inputs and outputs can simply be set as constant if the particular node does not send or receive over the channel . for simplicity we shall also assume the channel is memoryless .",
    "each source @xmath31 can be present at several nodes , and for each node @xmath110 , we denote the sources present at node @xmath2 as @xmath126 .",
    "the receiver demands are defined as follows :    * * joint source - channel multiple unicast with distortion : * each source is to be reconstructed ( with or without distortion ) at a single destination . again denote for receiver node @xmath2 the set of the sources it is interested in as @xmath70 , then we have @xmath127 for any @xmath128 . *",
    "* joint source - channel multiple multicast with distortion : * each source is to be reconstructed ( with or without distortions ) at multiple destinations , _",
    "i.e. _ , it is possible that @xmath129 .",
    "next we define the class of codes being considered in this work , which are again block codes .",
    "[ def : jscodeunicast ] an @xmath130 jscmud code on a joint source - channel communication network @xmath131 consists of the following components :    * at each transmitter node @xmath2 , an encoding function for ( time ) index @xmath61 @xmath132 * at each receiver node @xmath2 , for each source @xmath75 , a decoding function @xmath133    the encoding and decoding function induces the distortion @xmath134 where @xmath135 is the reconstruction of source @xmath79 at a node @xmath2 such that @xmath136 .",
    "[ def : jscodeunicastregion ] a distortion vector @xmath137 is achievable for jscmud on a joint source - channel communication network @xmath131 with a bandwidth mismatch factor @xmath88 , if for any @xmath89 and sufficiently large @xmath80 , there exist an integer @xmath138 and an @xmath130 jscmud code , such that @xmath139 , @xmath118 .",
    "the collection of all such distortion vectors is the achievable jscmud distortion region , denoted as @xmath140 .",
    "next we need to define the source codes and the channel codes extracted for the separation - based approach .",
    "for the jscmud problem , the source codes are conventional lossy source codes . in the channel coding problem",
    "we consider , each source @xmath31 is replaced with a message @xmath141 of cardinality @xmath142 with a uniform distribution ; moreover , these messages are mutually independent .",
    "the precise channel code definition is as follows .",
    "[ def : digchannelcodeunicast ] an @xmath143 multiple unicast channel code on a communication network @xmath144 consists of the following components :    * at each transmitter node @xmath2 , an encoding function for ( time ) index @xmath61 @xmath145 * at each receiver node @xmath2 , for each message @xmath146 where @xmath75 , a decoding function @xmath147    denote the decoded message as @xmath148 at node @xmath2 where @xmath149 , the encoding and decoding functions induce the average decoding error probability @xmath150    a rate vector @xmath151 is achievable for multiple unicast channel coding on a communication network @xmath144 , if for any @xmath89 and sufficiently large @xmath12 , there exists an @xmath152 multiple unicast channel code , such that @xmath153 , @xmath118 . the collection of such achievable rate vectors is the achievable capacity region of the network , denoted as @xmath154 .",
    "[ def : diguniregion ]    using conventional rate - distortion codes on each source and then combining it with the above defined multiple unicast channel codes as the coding scheme , an achievable distortion region is immediate , which will be denoted as @xmath155 .",
    "more precisely , we can write @xmath156 where @xmath157 is the distortion - rate function of the source @xmath31 .    in the case of jscmmd ,",
    "a source is to be reconstructed with possibly different distortions at multiple destinations .",
    "the jscmmd codes are defined in the same manner as in the case of jscmud , and thus the detailed definitions are omitted here .",
    "the achievable distortion matrix and the achievable distortion region @xmath158 can also be defined accordingly .",
    "the source - channel separation scheme for jscmmd is slightly more involved . consider first source @xmath31 , and assume it is to be reconstructed in a lossy manner at nodes in the set @xmath159 .",
    "the source codes we shall consider are successive refinement codes @xcite , and source @xmath31 is encoded in @xmath160 stages , where the operator @xmath161 denotes the cardinality of a set .",
    "for the channel codes in the separation approach , we shall consider a generalized version of the two - user degraded message set problem @xcite .",
    "more precisely , in the given source communication network , let us fix an order @xmath162 for the elements in the set @xmath163 for each @xmath118 .",
    "the source @xmath31 is replaced with a total of @xmath160 messages , denoted as @xmath164 , whose rate is @xmath165 , @xmath166 , where @xmath167 is the @xmath2-th element in the order @xmath162 .",
    "the @xmath112-th node in this given order @xmath162 is required to reconstruct the first @xmath112 messages , @xmath164 , @xmath168 .",
    "we can now define the achievable capacity region @xmath169 for this generalized degraded message set problem , which depends on the set of orders @xmath170 ; see the jscmmd example in section [ subsec : mummd ] , where @xmath171 and the specific order discussed is @xmath172 .    clearly , the degraded message set scenario naturally sets the stage for the successive refinement source codes , and by combining these two components , we arrive at an achievable distortion region using the separation appraoch for a given set of orders @xmath173 .",
    "we shall denote this achievable region as @xmath174 .",
    "our first main result is the following theorem , which formally states that the joint coding achievable distortion region is the same as separation coding achievable distortion region in the distributed network joint source - channel coding problem .",
    "[ theorem : dnsc ] for any discrete and memoryless distributed network joint source - channel coding problem ( with orthogonal communication channels ) , we have @xmath175 .",
    "the following uniform markov lemma is needed in the proof of this theorem , which can be found in @xcite .",
    "this lemma is alternative version of the well known markov lemma @xcite , and it has been used more recently in @xcite .",
    "we rewrite it below using notation more convenient to us .",
    "[ lemma : markov ] let @xmath176 be a markov string in finite alphabets . for any fixed strongly jointly typical sequence pair @xmath177 , let @xmath178 be chosen uniformly at random from the set which consists of all sequences that are strongly typical with @xmath179",
    "let @xmath180 be the probability measure induced by this random choice .",
    "then @xmath181 , and the convergence is uniform over the set of strongly jointly typical @xmath177 sequence pairs .    we first show that @xmath182 .",
    "consider an @xmath95-session length-@xmath80 distributed network source code with rates @xmath183 .",
    "with the bandwidth mismatch factor @xmath88 , we have a total of @xmath184 channel uses , and we shall partition them into @xmath95 channel sessions , each with @xmath185 channel uses ; for simplicity , we shall assume @xmath186 is an integer .",
    "thus the channel on edge @xmath187 in each session can support a message of cardinality up to @xmath188 , with vanishing error probability , where we assume natural logarithm for mutual information definition .",
    "each session of the source code has a message output of cardinality @xmath189 . thus omitting the rounding issue , as long as @xmath190 , _",
    "i.e. _ , @xmath191 , we can use the digital channel codes to transmit the source codes reliably ( with vanishing error probability ) .",
    "since the sources are discrete and thus finite , it follows that indeed , @xmath192 because the achievable distortion region @xmath93 is a closed set and the distortion - matrix - rate function @xmath193 is continuous , the condition @xmath191 can be replaced by @xmath194 and the rounding issues can be safely ignored , resulting in the achievable region @xmath195 .",
    "thus we have @xmath182 .",
    "next we focus on the direction @xmath196 . for an achievable distortion matrix @xmath87",
    ", there exists a sequence of distributed network joint source - channel codes approaching it .",
    "in other words , for any @xmath89 , there exists an @xmath197 distributed network joint source - channel code ( see definition [ def : dnsj1 ] and definition [ def : dnsj2 ] ) , where @xmath198 .",
    "it is instructive to first examine the joint probability distribution induced by this particular code .",
    "let @xmath199 and @xmath200 denote the collection of channel inputs and outputs at time @xmath61 ; similarly we use @xmath201 to denote the collection of all the source vectors in the network . at @xmath202",
    ", @xmath203 is a function of @xmath201 , _",
    "i.e. _ , @xmath204 in the notation of ; @xmath203 generates @xmath205 via @xmath206 orthogonal channels ; note that we have @xmath207 form a markov chain and @xmath208 since the channels are orthogonal . at @xmath209 ,",
    "@xmath210 is a function of @xmath201 and @xmath205 , _",
    "i.e. _ , @xmath211 ; @xmath210 further generates @xmath212 via @xmath206 orthogonal channels .",
    "successively , at time @xmath61 , we have    * condition one : @xmath213 ; * condition two : @xmath214 form a markov chain , and @xmath215 .",
    "we next show that if a distortion matrix @xmath87 is achievable in the joint coding problem given by definition [ def : dnsj1 ] and definition [ def : dnsj2 ] , which is denoted as @xmath216 , then the rate distortion matrix pair @xmath217 is also achievable in the pure source coding problem given by definition [ def : ddnsc ] and definition [ def : sourcecodingrd ] , which is denoted as @xmath218 . for this purpose , we shall construct an @xmath12-session distributed network source code for @xmath218 that operates on a source sequence of length @xmath219 . definition [ def : ddnsc ] and definition [ def : sourcecodingrd ] are rather crucial in the discussion below , and the readers are encouraged to familiarize with them before proceeding .",
    "we first partition the source sequence @xmath220 , @xmath221 , into @xmath15 disjoint block components , each of length @xmath80 .",
    "the @xmath222-th block component of @xmath220 is written as @xmath223 , _",
    "i.e. _ , @xmath224 to make this partition explicit , @xmath220 is written in the sequel as @xmath225 .",
    "for each @xmath226 and each _ session _",
    "@xmath227 , a source coding codebook @xmath228 of size @xmath229 is generated by choosing from the strongly typical set of the random variable @xmath230 uniformly at random with replacement .",
    "this codebook is revealed to both the encoder and decoder on edge @xmath187 in the problem @xmath231 .",
    "now consider encoding for session @xmath202 at any given edge @xmath226 .",
    "we first apply the original joint source channel encoding function @xmath232 on each block component @xmath233 , @xmath234 ; _ i.e. _ , @xmath235 .",
    "the outputs are then concatenated to produce a length-@xmath15 vector @xmath236 , @xmath237 for each @xmath226 , if @xmath238 is strongly typical , we then find a codeword @xmath239 in @xmath240 such that @xmath238 and @xmath239 are strongly jointly typical with respect to @xmath241 ; if there does not exist such a codeword , an error is declared .",
    "denote the index of this chosen @xmath239 codeword in @xmath240 as @xmath242 ; the @xmath222-th location in the vector @xmath239 is written as @xmath243 . for notational simplicity ,",
    "we shall also write @xmath244 the new encoding functions @xmath245 for the distributed network source coding problem ( _ i.e. _ , @xmath231 ) are given by @xmath246    we continue to construct the new code for the source coding problem @xmath231 . in the @xmath61-th session , for any given edge @xmath226 ,",
    "the original joint source - channel encoding function @xmath247 is applied , and the outputs are concatenated ( see fig .",
    "[ fig : evolution ] ) , _",
    "i.e. , _ @xmath248",
    "then for any @xmath226 , if @xmath249 is strongly typical , find a codeword @xmath250 in @xmath251 such that @xmath249 and @xmath250 are strongly jointly typical with respect to @xmath252 ; if there does not exist such a codeword , an error is declared .",
    "the index of the chosen codeword @xmath250 in @xmath251 is denoted as @xmath253 , and thus the new encoding functions @xmath254 for the distributed network source coding problem ( _ i.e. _ , @xmath231 ) are given by @xmath255    after @xmath12 sessions of encoding , at node @xmath256 , we apply the originally joint source - channel decoding function @xmath257 to reconstruct the @xmath222-th block component of source @xmath258 , _ i.e. _ , @xmath259 which are then concatenated to form @xmath260 , _",
    "i.e. _ the length-@xmath219 reconstruction of source @xmath112 at node @xmath2 . thus the new decoding functions @xmath261 for the distributed network source coding problem ( _ i.e. _ , @xmath231 ) are given by @xmath262    there are three kinds of error events in session-@xmath61 :    * @xmath263 are not strongly jointly typical with respect to @xmath264 ; this event is denoted as @xmath265 . note that if @xmath265 does not occur , then for all @xmath226 , the sequence @xmath249 is strongly typical . * for an edge @xmath226 , given @xmath249 is strongly typical , there does not exist any codeword in @xmath251 such that it is strongly jointly typical with @xmath249 , with respect to @xmath252 ; this event is denoted as @xmath266 . * @xmath263 and @xmath267 are not strongly jointly typical with respect to @xmath268 ; this event is denoted as @xmath269 .",
    "the overall error event is given as @xmath270 where we used @xmath271 to denote the complement of set @xmath272 , and @xmath273 is the event that the sequences @xmath274 are not strongly jointly typical .    by",
    "the union bound , we have @xmath275 next we show that @xmath276 as @xmath277 .",
    "it is clear that @xmath278 by the basic properties of the strongly jointly typical sequences ( @xcite , pp .",
    "358 - 362 ) .",
    "since @xmath279 is a deterministic function of @xmath274 , it is clear that @xmath280 , and similarly @xmath281 for @xmath282 .",
    "for the second summation in ( [ eqn : errorevents ] ) , again by the union bound @xmath283 since @xmath284 implies that @xmath249 is strongly typical , it is clear that @xmath285 for any @xmath61 and @xmath97 , by the properties of the strongly typical sequences ( @xcite , lemma 13.6.2 ) , and the fact that the number of codewords in @xmath251 is @xmath229 .    to bound the third summation in ( [ eqn : errorevents ] ) , let us fix an arbitrary order for the edges in the set @xmath51 , and write it as @xmath286 .",
    "define @xmath287 as the event that @xmath263 and @xmath288 are not strongly jointly typical",
    ". we can then rewrite @xmath289 where @xmath290 . to bound @xmath291 ,",
    "observe that @xmath292 is a markov string .",
    "invoking lemma [ lemma : markov ] gives that @xmath293 , for any @xmath61 and @xmath112 , as @xmath277 .",
    "there are a total of @xmath12 terms in the first summation of ( [ eqn : errorevents ] ) , a total of @xmath294 terms in the second summation , and a total of @xmath294 terms in the third summation .",
    "since @xmath12 and @xmath206 are fixed in the above construction , and each term can be made arbitrarily small by making @xmath15 sufficiently large , it is seen that @xmath276 as @xmath277 .",
    "this implies that the sequences @xmath295 are strongly jointly typical with respect to the original distribution @xmath296 with probability approaching one as @xmath277 .",
    "this further implies that @xmath297 and @xmath260 are strongly jointly typical with respect to @xmath298 , and the new code induces a distortion @xmath299 , where @xmath300 as @xmath277 .     in session @xmath301 for node @xmath1 with an incoming link @xmath302 and an outgoing link @xmath187.[fig : evolution ]",
    "each narrow horizontal box represents a vector ; the vectors @xmath303 s and @xmath304 s are shaded partially because at this point , the later parts have not been generated .",
    "each component of the lossy encoder output , _",
    "i.e. _ , @xmath305 , is appended to the existing @xmath306 to form @xmath304.,width=680 ]    it remains to analyze the rates of this digital scheme , _",
    "i.e. _ , the cardinalities of indices delivered during each session for the problem @xmath231 .",
    "it is clear that for each link @xmath307 , for each session @xmath61 , we have @xmath308 where @xmath115 is the capacity of the channel on edge @xmath309 , due to the conventional channel coding theorem . thus it is seen that the cardinality of above digital codes for each session associated with any given link @xmath309 is bounded as @xmath310 it follows that the following rate is achievable in the problem @xmath231 @xmath311 according to definition [ def : sourcecodingrd ] .",
    "thus we have shown that this newly constructed code can operate at the rate - distortion - matrix tuple @xmath312 for the problem @xmath231 , where @xmath313 and @xmath314 can be made arbitrarily small by letting @xmath277 .",
    "since the achievable rate - distortion - matrix region for @xmath231 is a closed set , the asymptotically small terms @xmath313 and @xmath314 can be safely ignored , and it follows that the rate - distortion - matrix tuple @xmath315 is achievable in the problem @xmath231 .",
    "now since the distortion matrix @xmath87 is achievable in the joint coding problem @xmath216 , for any @xmath89 , there exists an @xmath197 joint source - channel code , where @xmath198 .",
    "it follows that @xmath315 is an achievable rate - distortion - matrix tuple in the source coding problem @xmath231 for any @xmath89 , and again since the achievable rate - distortion - matrix region for @xmath231 is a closed set , it is clear that @xmath316 is indeed achievable for @xmath231 .",
    "the proof can be completed for @xmath196 by applying ( [ eqn : disregiondef ] ) .",
    "_ remark : _ in the proof , the original joint source - channel code length @xmath317 and the number of super blocks @xmath15 need to be controlled carefully .",
    "to arrive at the conclusion , we essentially need to drive both of them to infinity , but at different rates .",
    "this consideration also applies in the proof for the jscmud problem in the next section .",
    "the main result of this section is the following theorem , which formally states that the joint coding achievable distortion region is the same as separation coding achievable distortion region in the jscmud problem .",
    "[ theorem : main ] for discrete and memoryless joint source - channel multiple unicast with distortion , we have @xmath318 .    the direction @xmath319 is rather obvious , and thus we focus on the other direction @xmath320 . for any achievable distortion vector @xmath137 , there exists a sequence of jscmud codes to approach it .",
    "in other words , for any @xmath89 , there exists an @xmath321 jscmud code , where @xmath138 ( see definition [ def : jscodeunicast ] and [ def : jscodeunicastregion ] ) .",
    "the sources and the above given block code induce a joint distribution @xmath322 and we view the second term as the transition probability of a block - level interference channel , which has input alphabets @xmath323 , and output alphabets @xmath324 .",
    "moreover , by the conventional rate - distortion theorem @xcite , it is clear that @xmath325 where @xmath326 is the rate - distortion function for source @xmath31 .",
    "this super interference channel operates in the same manner as a memoryless interference channel , however it operates on a block level @xmath327 , instead of on a single time instance level on the original channel @xmath328 .",
    "next we show that if a distortion vector @xmath137 is achievable on the joint coding problem defined in definition [ def : jscodeunicast ] and definition [ def : jscodeunicastregion ] , which we denoted again as @xmath216 , then the rate vector @xmath329 is achievable on the pure channel coding problem defined in definition [ def : digchannelcodeunicast ] and definition [ def : diguniregion ] , which we denoted as @xmath330 . for this purpose",
    ", we shall construct a multiple unicast channel code for @xmath330 using the afore - mentioned @xmath321 jscmud code for @xmath216 .    the coding scheme for @xmath330",
    "can be formally described as follows . for each source @xmath31 ,",
    "@xmath331 codewords of length-@xmath332 are generated independently , accordingly to the @xmath219-th product distribution of @xmath117 ; denote this codebook as @xmath333 .",
    "the codebooks are revealed to all the nodes .    to encode for @xmath330 , given a message @xmath334 , we shall choose the @xmath334-th codeword @xmath335 in the @xmath333 codebook generated above .",
    "each codeword is partitioned into @xmath15 blocks of equal length , and let us denote the @xmath222-th block as @xmath336 ; to emphasize this partition , we also write @xmath335 as @xmath337 . for a fixed @xmath222",
    ", the blocks @xmath338 from the chosen codewords at all the nodes can be viewed as the length-@xmath80 source vectors in the original jscmud problem , and thus the original @xmath321 jscmud encoding and decoding functions can be used on them .",
    "this results in a set of reconstruction sequences @xmath339 . at the end of @xmath15 blocks , we concatenate the reconstruction for each source block as @xmath340 .",
    "mathematically , let the original joint source - channel encoding and decoding function at node @xmath2 be @xmath341 , and @xmath257 , respectively .",
    "similarly as the notation of @xmath336 , the @xmath222-th length-@xmath12 block of @xmath342 is written as @xmath343 , and the first @xmath61 symbols of the block @xmath343 is written as @xmath344 .",
    "then the new channel code encoding function @xmath345 is given by @xmath346 the reconstructions are simply @xmath347    we proceed to construct the multiple unicast channel code decoder ( for problem @xmath330 ) . at node @xmath2 , for which @xmath348 , find a unique codeword in the codebook @xmath349 such that it is ( weakly ) jointly typical @xcite with @xmath350 according to the distribution @xmath351 , _",
    "i.e. _ , the marginal from ( [ eqn : superdistribution ] ) .",
    "if there is a unique codeword , then the corresponding message @xmath352 is declared ; otherwise an error is declared .",
    "there are three kinds of errors in this new scheme :    * the sequences @xmath353 are not jointly typical with respect to ( [ eqn : superdistribution ] ) ; denote this event as @xmath354 . *",
    "the sequences @xmath355 are not jointly typical with respect to ( [ eqn : superdistribution ] ) ; denote this event as @xmath356 .",
    "* for a given message @xmath334 , there is more than one codeword in @xmath333 that is jointly typical with @xmath357 , with respect to the marginal of ( [ eqn : superdistribution ] ) ; denote this event as @xmath358 .",
    "the overall error probability can be bounded as @xmath359 where the inequality is by the union bound .    since all the codewords are generated according to @xmath117 s independently , by the basic properties of the jointly typical sequences ( @xcite , theorem 14.2.1 ) , @xmath360 as @xmath361 .",
    "this implies that the reconstructions @xmath362 are jointly typical with @xmath363 with probability approaching one , _",
    "i.e. _ , @xmath364 as @xmath361 .",
    "it follows that @xmath365 as @xmath361 , by ( [ eqn : mutualinformation ] ) and the basic property of the jointly typical sequences ( @xcite , theorem 14.2.1 and theorem 14.2.2 ) , and the fact that the number of codewords in @xmath333 is @xmath366 . since there are a total of @xmath367 terms in ( [ eqn : jscmuderror ] ) , it is clear that @xmath368 as @xmath361 .    by making @xmath15 large and thus @xmath313 small",
    ", it is clear that the rate tuple @xmath369 for any fixed @xmath89 .",
    "since the rate - distortion functions @xmath326 s are continuous , and the capacity region @xmath154 is a closed set , it follows that @xmath370 .",
    "therefore we can conclude that @xmath371 , by the definition of @xmath155 given in ( [ eqn : duni ] ) , and thus @xmath320 .",
    "this completes the proof .    _",
    "remark : _ when a source @xmath31 is present at multiple nodes , these nodes in fact cooperate to communicate to the destination .",
    "a source @xmath31 present at multiple nodes in the joint coding problem implies a common message @xmath141 available also at these nodes in the pure channel coding problem , and thus these nodes can cooperate to transmit this common message .",
    "this cooperation is implicit in the original jscmud code , but in the induced channel code it does not appear explicitly . note that since our proof given above only relies on weak typicality , it can be extended straightforwardly to problems with more general ( continuous ) alphabets .",
    "in this section we examine the third scenario considered in this paper , _",
    "i.e. , _ the case where there could be multiple receivers interested in the same source , but at different distortion levels .",
    "we limit ourselves to a set of distortion measures often referred to as the  difference \" distortion measures , since its properties play an important role in the proof .",
    "more precisely , the distortion measure @xmath372 is given in the form of @xmath373 , where @xmath374 is an abelian group with a proper addition operation ; furthermore , @xmath375 _ i.e. , _ the distortion mapping only depends on the difference between the two variables .    in order to present our results ,",
    "some additional definitions are quoted directly from @xcite . for a random variable @xmath48 in the alphabet @xmath374 ,",
    "the capacity of the additive noise channel @xmath376 where @xmath377 is also in the alphabet @xmath374 , under a @xmath378 constraint is defined as @xmath379 the addition @xmath380 is defined on the abelian group of @xmath374 , and it can be real addition , modulo addition or finite field addition where appropriate ; @xmath381 here stands for independence .",
    "the minimax ( or worst noise ) capacity is defined as @xmath382 @xmath383 can be interpreted as the capacity at equilibrium in a mutual information jammer game , played over an additive - noise channel , in which both the expected noise and expected input are limited to within @xmath384 in terms of @xmath378 .",
    "though for general difference distortion measures , the quantity @xmath383 is a function of @xmath384 , for some specific cases , considerable simplification is possible .",
    "for example , when the distortion is mean squared error , @xmath383 is always @xmath385 bit @xcite .",
    "our approximation result is in a genie - aided form , where additional communication links with bounded capacities are provided by a genie .",
    "we will show that a separation - based approach using the original communication network together with the additional genie - provided communication network can achieve any distortion matrix @xmath87 that is achievable in the original communication network with arbitrary joint coding schemes . to quantify this genie - provided communication network , recall that the @xmath1-th row of an achievable distortion matrix @xmath386 corresponds to the distortions achieved for source @xmath31 at the destinations in the set @xmath163 .",
    "it will become clear in the proof that if the reconstructions of a source @xmath31 at multiple destinations in the set @xmath163 are required to be at the same distortion level a priori , then these destinations can be viewed as a single super - destination , and the problem can be reduced to a simpler one .",
    "therefore , without loss of generality we shall assume the reconstructions of a source at multiple destinations are at different distortion levels .",
    "the decreasing sequence of distortions for the elements on the @xmath1-th row thus specifies an order @xmath162 of the set @xmath163 ; let @xmath167 be the @xmath2-th element in the set of @xmath163 according to the order @xmath162 .",
    "we require this genie - provided network to support degraded message set broadcast from source @xmath31 to the nodes in the set @xmath163 for each @xmath1 where @xmath387 .",
    "in other words , for each source @xmath31 such that @xmath387 , for each @xmath388 , there is a common link of capacity @xmath165 per source sample is present at more than one node , _",
    "i.e. _ , @xmath389 , then @xmath165 should be the sum rate per source sample of such common links from each of the node in @xmath390 to all the nodes @xmath391 . ] from @xmath31 to all the nodes @xmath391 .",
    "these rate entries are collected and written together as the rate matrix @xmath392 .",
    "consider adding this genie - provided communication network on top of the original source communication network , and denote the achievable distortion using a separation approach of successive refinement coupled with superposition channel code on this joint communication network as @xmath393 .",
    "* example : * consider the example given in fig .",
    "[ fig : example2 ] .",
    "the sets @xmath163 s are @xmath394 the orders when the distortion of @xmath8 is larger than @xmath9 are @xmath395 the rate matrix of the genie - provided communication network has the form @xmath396\\end{aligned}\\ ] ] where @xmath397 at row-@xmath1 and column-@xmath2 means that the genie does not need to provide an additional communication capability from source @xmath31 to node @xmath2 , thus @xmath398 is not defined . the new network consisting of the additional genie communication network on top of the original source communication network is given in fig .",
    "[ fig : genienew ] .     the example in fig .",
    "[ fig : example2 ] with the additional genie links , which are in drawn in dashed lines .",
    "the region @xmath393 is the achievable distortion region using separation - based scheme on this joint network.,width=453 ]    the following theorem is our first result on general network multicast .",
    "[ theorem : multicast ] let @xmath87 be an achievable distortion matrix , for which @xmath173 is the corresponding orders induced by @xmath87 . for any random variable @xmath399 in the alphabet @xmath400 , @xmath166 , such that @xmath401 where @xmath167 is the @xmath2-th node index in the set of @xmath163 according to the order @xmath162 , and @xmath402 s are mutually independent such that @xmath403 , let the genie communication network support the rate matrix @xmath404 whose elements are @xmath405 then we have @xmath406 .    _",
    "remark : _ it is clear that @xmath407 in the above theorem .",
    "this theorem in fact provides more than one approximation , one for each set of @xmath402 random variables , resulting in a rather powerful bounding tool . to find the tightest bound",
    ", we can optimize over these random variables under certain constraints depending on the achievable distortion matrix @xmath87 .    under certain distortion measures",
    ", significant simplifications can be made .",
    "the quadratic distortion measure is an important special case of difference distortion measures since it is used in many practical systems , for all letters @xmath408 , @xmath409 .",
    "this condition assures that the asymptotically small decoding error probability does not cause significant change in the distortion behavior .",
    "this condition is not uncommon in source coding , see e.g. , @xcite ; we shall refer to it as  bounded expected distortion \" condition . ] .",
    "the main result of this section is given for this case , which shows that a separation - based scheme is approximately optimal , universally across all distortion values , for the quadratic distortion measure .",
    "note that the sources need not be gaussian .",
    "[ corollary : gaussian ] let @xmath87 be an achievable distortion matrix , for which @xmath173 is the corresponding orders induced by @xmath87 .",
    "let the sources @xmath31 s satisfy the condition that for all letters @xmath408 , @xmath410 .",
    "let the genie communication network support the rate matrix @xmath404 whose elements are @xmath411 we have @xmath406 under the mean squared error distortion measure .    in the simplest case where a single node broadcasts a gaussian source to a set of receivers , this result essentially reduces to corollary 1 given in @xcite for the gaussian source .",
    "the intuitive translation of the above result is that when a genie helps the separation - based scheme by providing half a bit information for each receiver , and at the same time , all the receivers with better quality reconstructions receive this information for free , then the genie - aided separation - based scheme is as good as the optimal ones .",
    "theorem [ corollary : gaussian ] implies that the total aggregate throughput nodes as @xmath112 separate links ; _",
    "e.g. _ , in fig .",
    "[ fig : genienew ] the common link of rate @xmath412 is considered as two separate links of capacity @xmath412 each . ]",
    "provided by the genie is @xmath413 , where @xmath414 .",
    "this depends on the demand structure and grows large in networks with many sources and when most sources are required to be reconstructed at many destinations . however , for any fixed network , the approximation in theorem [ corollary : gaussian ] holds regardless of the quality of the channel . as such",
    ", this result is more useful in the high resolution regime for large networks , when the genie - network becomes negligible compared to the original communication network . in networks with only a few sources , or when",
    "most sources are to be reconstructed at only a few destinations , the genie network aggregated throughput is not large , and the approximation is usually sufficiently accurate .",
    "next , we focus on the proof of theorem [ theorem : multicast ] , since theorem [ corollary : gaussian ] can be directly obtained by using gaussian auxiliary random variables @xmath415 s in theorem [ theorem : multicast ] .    to simplify the notation ,",
    "let us first consider a single source @xmath31 ; we can assume for the time - being that the joint source - channel encoding procedure is still performed on other sources , and we shall return to this point at the end of the proof . without loss of generality , we can assume the destination nodes of source @xmath31 are @xmath416 , and moreover the distortions , which are achieved by this given source - channel joint code , are ordered as @xmath417 . to simplify notation , we shall omit the index @xmath1 in @xmath31 for the time - being .",
    "this will also imply that instead of the cumbersome notation for the distortions as @xmath418 , we will simplify it to @xmath419 .",
    "next we choose a set of auxiliary random variables as specified in the theorem .",
    "more precisely , we define the auxiliary random variables in the alphabet @xmath64 such that , @xmath420 where @xmath421 s are random variables in the alphabet @xmath64 , which are _ independent of everything else_. furthermore the auxiliary random variables are chosen such that @xmath422 . for the moment this choice seems arbitrary , but we will link these auxiliary variables to the achievable distortions .    consider a jscmmd code which induces the distortion vector @xmath423 for source @xmath272 , whose reconstructions are denoted as @xmath424 .",
    "we shall view the transition probability @xmath425 , as a broadcast channel and denote it as @xmath426 .",
    "we need the following lemma to proceed , whose proof will be given shortly . for simplicity",
    ", we shall ignore the asymptotically small quantities such as @xmath427 in the previous section , which are inconsequential .",
    "[ lem : mullemma2 ] on the broadcast channel @xmath426 , the following degraded message set broadcast rates can be ( asymptotically ) supported @xmath428 moreover , these rates can be achieved by a random superposition code @xcite based on the joint distribution @xmath429 .",
    "though this lemma is regarding the broadcast channel @xmath426 , in a manner similar to the proof for general network unicast , we can in fact conclude that on the original network , when all the other encoders still perform the original joint source - channel encoding , the communication channel from source @xmath272 to its destinations can support degraded message set broadcast rates @xmath430 per @xmath80 source samples .",
    "this is because the broadcast channel @xmath426 is simply the communication channel in the original network with certain additional operations on the block level .",
    "using this lemma , it is clear that together with the genie - provided communication network , we can send messages within the degraded message set from source @xmath31 to its destinations at rates per @xmath80-samples @xmath431 in other words , these degraded message set broadcast channel rates can be supported for pure channel coding purpose .",
    "note that , however , if we use the distribution @xmath432 to construct a successive refinement code @xcite , the rates @xmath433 are exactly the ( asymptotic ) source coding rates of this code per @xmath80-samples .",
    "thus we can safely conclude that the distortion @xmath434 is achievable using the separation approach in this genie - aided network , because we can simply use the codewords generated by distribution @xmath435 as the construction to achieve distortion @xmath436 .",
    "it remains to argue that if all the users simultaneously replace the original joint source - channel codes with the newly constructed channel codes , the rates that can be supported are still the same as above .",
    "this is indeed true , because in the superposition code in lemma [ lem : mullemma2 ] , we rely on the joint typicality on the block level when the channel input is of distribution @xmath437 .",
    "this however does not change if the other users replace the original joint source - channel codes also with their newly constructed channel codes , since the superposition channel codes constructed this way for all the broadcast channels indeed preserve joint typicality according to the distribution @xmath438 .",
    "this completes the proof with exception of lemma [ lem : mullemma2 ] .    to prove lemma [ lem : mullemma2 ]",
    ", we first give an auxiliary lemma .",
    "[ lem : mullemma1 ] let @xmath439 , @xmath440 and @xmath441 be specified as in the proof of theorem [ theorem : multicast ] , then we have for @xmath442 @xmath443 where @xmath444 is defined in .",
    "we can write @xmath445 in two ways @xmath446 and @xmath447 where @xmath448 , because of the construction of the auxiliary random variable @xmath449 ensures that @xmath450 is independent of @xmath451 , as seen in .    thus we have @xmath452 where @xmath453 follows again since @xmath454 is independent of @xmath451 , and the last step follows the concavity of @xmath455 as a function of the marginal distribution .",
    "this proves ( [ eq : mullemma11 ] ) in lemma [ lem : mullemma1 ] .",
    "note further that for @xmath456 , we have @xmath457 as well as @xmath458 it follows that @xmath459 note that @xmath460 where @xmath461 follows because of the markov string @xmath462 .",
    "this proves ( [ eq : mullemma12 ] ) in lemma [ lem : mullemma1 ] .",
    "furthermore , because of the markov string @xmath463 , we have @xmath464 and it follows that @xmath465 this proves ( [ eq : mullemma13 ] ) in lemma [ lem : mullemma1 ] , and thus the proof is complete .",
    "now we are ready to prove lemma [ lem : mullemma2 ] .",
    "we shall use the distribution @xmath429 to construct superposition broadcast channel code on the broadcast channel @xmath426 for the degraded message set .",
    "the rates ( per length-@xmath80 block ) for these messages within the degraded message set are ( asymptotically ) @xmath466 we need to show that the chosen rates can indeed be supported on this broadcast channel with a degraded message set .",
    "since this channel itself is not degraded , we have to show that the superposition coding scheme succeeds for all the receivers . to see this ,",
    "observe that for the @xmath1-th receiver , we have @xmath467 by lemma [ lem : mullemma1 ] .",
    "it follows that @xmath468 where the last inequality is straightforward by noticing @xmath469 when @xmath470 .",
    "thus the @xmath1-th receiver can indeed decode the first message for any @xmath471 .",
    "similarly , we have for @xmath472 @xmath473 and thus we conclude the @xmath1-th receiver can decode the messages @xmath474",
    ". the @xmath475-th receiver does not pose any additional difficulty .",
    "thus indeed the rates specified in ( [ eqn : r1])-([eqn : rk ] ) can be supported on this super - broadcast channel , and the proof is complete .",
    "we considered the optimality of source - channel separation architecture in networks , and showed that the separation approach is optimal for the problems of distributed network joint source - channel coding and joint source - channel multiple unicast with distortion .",
    "moreover , the separation approach is also approximately optimal for the problem of joint source - channel multiple multicast with distortion under certain distortion measures .",
    "the results in this work are obtained without explicit characterizations of the underlying regions .",
    "such an approach of identifying properties without explicit individual component solutions is a valuable tool which may lead to further insights into network information theory problems .",
    "the source coding problem extracted from the distributed network source coding scenario implies that the interactive coding aspect needs to be carefully incorporated into this source coding problem , which suggests a distinct line of research direction into network source coding .",
    "the requirement on the sources being independent in the general network unicast and multicast problem may seem rather stringent .",
    "however , in many practical situations the sources are indeed either independent or identical .",
    "furthermore , the dependence structure among the sources can sometimes be approximated reasonably well by a model only allowing multiple independent or identical sources ; one such example is in @xcite , where an approximate characterization for the rate - distortion region of the three source gaussian distributed source coding problem was given .",
    "thus our results on general network unicast and multicast may have implications in even broader settings .    for notational and conceptual simplicity",
    ", we made many assumptions which are not strictly necessary .",
    "the results can be straightforwardly extended to more general cases with some minimal efforts , the proofs of which are left to interested readers .    *",
    "* distributed network joint source - channel coding : * the synchronization requirement among sources can be removed , _",
    "i.e. _ , the source bandwidths do not have to be the same throughout the whole network .",
    "the multiple reconstructions of a source @xmath31 can be under different distortion measures ; in fact the distortion measures defined can be defined on multiple sources , such as to reconstruct @xmath476 .",
    "the optimality result can be also extended to lossless reconstruction under vanishing block error probability requirement , instead of zero distortion requirement and reconstructions @xmath477 are strongly jointly typical , and if the original joint source - channel code guarantees vanishing length-@xmath80 block error probability , then we only need to add an error correction code ( treating each length-@xmath80 block as a symbol ) to boost the new code to have vanishing length-@xmath219 block error probability .",
    "the rate loss of this error correction code is negligible . ] . * * joint source - channel multiple unicast with distortion : * the synchronization requirement among sources and channels can be removed and the memoryless requirement on the channel can be relaxed to channels with finite memory ( see @xcite for an outline ) . as mentioned , the restriction on the discrete alphabets can be relaxed to sources and channels with continuous alphabets , where the sources satisfy the",
    " bounded expected distortion \" condition .",
    "the condition that each source is to be reconstructed at one destination can be relaxed to some extent .",
    "more precisely , when each source is to be reconstructed at multiple destinations but at the exact same distortion under the same distortion measure , then the source - channel separation architecture is still optimal . * * joint source - channel multiple multicast with distortion : * similar to the jscmud case , the synchronization , the memoryless channel , and the discrete alphabet requirement can be relaxed .",
    "the condition that each source is to be reconstructed under the same distortion measure can be relaxed to different distortion measures . in this case , the orders @xmath173 among the reconstructions are not straightforward , however as we take all the possible order @xmath173 , there is always one order that yield a valid approximation .",
    "if some of the reconstructions of a source @xmath31 are specified to have the same distortion a priori , then the approximation upper bound can be improved ; particularly , when the reconstructions of each source are specified to all have the same distortion , then source - channel separation is optimal , which is essentially the generalization discussed above for the jscmud problem .    for dnjscc in a communication network without any relay or feedback , the orthogonal channels do not need to be synchronized with each other , and the source - channel separation architecture is still optimal .",
    "we believe the optimality result holds in more general networks without the synchronization requirement , however the interleaving argument becomes exceedingly complex .    our proof for dnjscc relies on the ( uniform )",
    "markov lemma and thus applies only to discrete alphabets , however given the recent effort on extending the markov lemma to abstract alphabets @xcite ( see also the generalized markov lemma to gaussian sources in @xcite ) , we believe it should be possible to extend our result on dnjscc to the sources and channels in more general alphabets .    the result on jscmmd is given in a genie - aided - communication - network form , and this may be less pleasing . in some cases , such a bound can be translated to multiplicative bounds on the difference between the separation scheme distortion region and joint coding scheme distortion region . in @xcite , we provide such multiplicative bounds for the gaussian source broadcast problem ; in fact , two kinds of multiplicative bounds were provided in @xcite , one is a direct translation of theorem [ corollary : gaussian ] , and the other is a single constant bound which holds uniformly for all the components of the distortion tuples . the result in this work",
    "is presented in the current genie - aided form partly due to the abstract setting . for a more specific model in which bandwidths , power , and other resources are given",
    ", the genie - aided form can often be converted to other more explicit forms , such as the multiplicative distortion bound or a genie - aided - resource form ; this is an interesting future research direction .",
    "one fascinating aspect of our results is the difference between the separation in the dnjscc problem and that in the jscmud problem . from a philosophical point of view , in the point - to - point setting , the source and channel are specified by their statistical behaviors alone ; however in the network setting , the new network components of the connectivity structure among nodes and the source - demand coding requirements are introduced .",
    "our result in dnjscc treats the source statistics and these network components as a whole , and treats the channel statistics as the other , resulting in the separation between a complex network source coding problem and multiple conventional point - to - point channel coding problems .",
    "in contrast , the result in jscmud treats the channel statistics and the network components as a whole , and the source statistics as the other , resulting in the separation between a complex network channel coding problem and multiple conventional point - to - point source coding problems .",
    "these separations are not the only possibility , and one can choose to separate in a different manner . indeed , the work of han @xcite suggests for lossless coding over network with orthogonal links without feedback , the joint coding problem can be separated without loss of optimality into a moderately complex network source coding problem ( slepian - wolf coding ) and a complex network channel coding problem .",
    "thus the problem of source - channel separation is by no means solved , and it calls for further investigation .",
    "r. koetter , m. effros , and m. medard , `` on a theory of network equivalence , '' in _ proc .",
    "ieee information theory workshop on networking and information theory , _ jun .",
    "2009 , pp .",
    "326330 . extended version : http://arxiv.org/abs/1007.1033[arxiv:1007.1033 ] .",
    "m. agarwal , a. sahai , and s. mitter , `` coding into a source : a direct inverse rate - distortion theorem , '' in _ proc .",
    "forty - fourth annual allerton conference on communication , control , and computing _ , uiuc , il .",
    ", usa , sep . 2006 , pp . 569578 .      c. tian , s. n. diggavi and s. shamai , `` approximate characterizations for the gaussian broadcasting distortion region , '' in _ proc .",
    "information theory _ , seoul , korea , jul .",
    "2009 , pp .",
    "2477 - 2482 .                      c. tian , j. chen , s. diggavi and s. shamai , `` optimality and approximate optimality of source - channel separation in networks , '' in _ proc .",
    "information theory _",
    ", austin , tx , jul .",
    "2010 , pp . 495499 .",
    "m. a. maddah - ali and d. n. c. tse , `` approximating the rate - distortion region of the distributed source coding for three jointly gaussian tree - structured sources , '' in _ proc .",
    "information theory _ , seoul , korea , jul .",
    "2009 , pp ."
  ],
  "abstract_text": [
    "<S> we consider the source - channel separation architecture for lossy source coding in general communication networks . </S>",
    "<S> it is shown that the separation approach is optimal in two general scenarios , and is approximately optimal in a third scenario . </S>",
    "<S> the two general scenarios for which separation is optimal complement each other : the first scenario is when the memoryless sources at source nodes are arbitrarily correlated , each of which is to be reconstructed at possibly multiple destinations within certain distortions , but the channels in this network are synchronized , orthogonal and memoryless point - to - point channels ; the second scenario is when the memoryless sources are mutually independent , each of which is to be reconstructed only at one destination within a certain distortion , but the channels are general , including multi - user channels such as multiple access , broadcast , interference and relay channels , possibly with feedback . </S>",
    "<S> the third general scenario , for which we demonstrate approximate optimality of source - channel separation , relaxes the second scenario by allowing each source to be reconstructed at multiple destinations . for this case </S>",
    "<S> , the loss from optimality by using the separation approach can be upper - bounded when the  difference \" distortion measure is taken , and in the special case of quadratic distortion measure , this leads to universal constant bounds .    </S>",
    "<S> these results are shown without explicitly characterizing the achievable joint source - channel coding distortion region or the achievable separation - based coding distortion region . </S>",
    "<S> such an approach of identifying properties without explicit individual component solutions may lead to further insights into network information theory problems . </S>",
    "<S> furthermore , for the first general scenario , the extracted pure network source - coding problem has to incorporate a large number of rounds of user interactions and the corresponding causality constraints , which suggests a distinct research direction into interactive network source coding that has not received much attention in the literature . </S>"
  ]
}