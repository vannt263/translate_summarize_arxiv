{
  "article_text": [
    "in view of the rapid development of non - extensive statistics based on tsallis information entropy the problem of choosing the form of information entropy becomes pressing .",
    "the most justified one appears to be one - parameter family of renyi s entropies ( or simply renyi entropy ) .",
    "when renyi parameter @xmath0 is equal to unity , renyi entropy goes to well - known boltzmann  shannon entropy .",
    "the renyi entropy is extensive but its linearization in the neighbourhood of a point @xmath1 results in the non - extensive tsallis entropy .    when the principle of maximum of an information entropy ( mep ) is applied to the renyi entropy of an isolated system , a homogeneous ( microcanonical ) distribution @xmath7 over @xmath8 micro - states",
    "is derived . in this case",
    ", the renyi entropy becomes the boltzmann entropy @xmath3 thus afforcing _ boltzmann principle _ from which all thermodynamic properties of extensive and non - extensive hamiltonian systems can be deduced .    for a system in contact with a heat bath ,",
    "the levy distribution ( or @xmath0-distribution ) @xmath9 over microscopic states of a system is derived with the use of mep applied to the renyi entropy . applying this principle to the tsallis entropy",
    "one would be forced to go from the original distribution @xmath9 to an escort distribution @xmath10 , and use it further for evaluations of mean values of dynamical variables . such going contradicts to main principles of probability description .",
    "the levy distribution can be obtained also for small subsystems experiencing large fluctuations of temperature by way of averaging of gibbs distribution over the temperature fluctuations . besides , this procedure permits to associate the renyi parameter @xmath0 with physical properties of the subsystem .",
    "a long  range `` tail '' ( for @xmath11 ) of the levy distribution can be approximated by a power  law distribution @xmath12 , where the exponent @xmath4 is expressed in terms of @xmath0 .",
    "then at least for power  law distributions , the mep can be expanded to determine the renyi parameter @xmath0 and the exponent @xmath4 , respectively .",
    "the information entropy , or simply entropy , is the measure of uncertainty in information in the case of statistical ( incomplete ) description of a system using the distribution of probabilities @xmath13 the best known is the representation of entropy in the boltzmann ",
    "shannon form @xmath14 in the case when the subscripts @xmath15 indicate dynamic microstates in the gibbs phase space and the distribution @xmath16 corresponds to the macroscopic equilibrium state of the system , the entropy @xmath17 coincides with the thermodynamic entropy .",
    "it is just this type of entropy was justified by khinchin [ 1 ] and shannon [ 2 ] in a theorem form based on a system of axioms .",
    "their axioms were analyzed in [ 3,4 ] , where it was shown that a unique determination of entropy in the boltzmann ",
    "shannon form is provided by a quite artificial axiom related to a form of conditional entropy ( that is , the entropy of subsystem of a system being in a prescribed state ) .",
    "a variety of papers on this subject were analyzed by uffink [ 4 ] who found that the most convincing appears to be the system of axioms of shore and johnson [ 5 ] leading to renyi s one - parameter family of entropies [ 6 ] ; for the distribution @xmath9 normalized to unity , this family is written in the form @xmath18 where @xmath0 is an arbitrary positive number ( it can not be less than zero , because @xmath9 may include zero values ) .",
    "various properties of renyi entropy are discussed , in particular , in the monographs [ 6 - 8 ] . among its basic properties we may mention : positivity ( @xmath19 ) , concavity for @xmath20 and , in addition , @xmath21 .    in the case of @xmath22 ( which , in view of normalization of the distribution @xmath9 , corresponds to the condition @xmath23 )",
    ", one can restrict oneself to the linear term of logarithm expansion in the expression for @xmath24 over this difference , and @xmath24 changes to @xmath25 such a linearization of renyi entropy was first suggested by havdra and charvat [ 9 ] and daroczy [ 10 ] ; at present , this expression for entropy came to be known as tsallis entropy [ 11 ] .",
    "the logarithm linearization results in the entropy becoming nonextensive .",
    "this property is widely used by tsallis and by the international scientific school that has developed around him for the investigation of diverse nonextensive systems [ 11 - 16 ] . in so doing",
    ", the above - identified restriction @xmath23 is disregarded , as a rule . in our opinion ,",
    "the attempt by abe [ 16 ] at independent validation of this form of entropy appears unconvincing , because it is based on the axiomatic introduction of such a form for conditional entropy which uniquely provides for obtaining tsallis entropy .",
    "according to mep , in the case of statistical ( incomplete , from the dynamic standpoint ) description of the system , its distribution function must provide for correct values of those few average quantities which appear in the statistical description ; otherwise , it must be as undetermined as possible .",
    "such an approach in application to equilibrium thermodynamic systems ( isolated or weakly interacting with the thermostat ) has long been used to construct equilibrium statistical thermodynamics .",
    "however , it was only after studies by jeynes [ 17 ] that it came to be firmly established as a principle validating ( at least , on the physical level of rigor ) the use of gibbs ensembles in statistical description of thermodynamic systems .",
    "the information entropy is traditionally taken to mean the boltzmann ",
    "shannon entropy . here",
    ", mep will be applied to the renyi entropy .",
    "we are interested firstly in the distribution of probabilities @xmath9 , providing for the extremality of information entropy with an additional condition of normalization of @xmath26 .    then , the distribution @xmath9 must be determined from the extremum of the functional @xmath27 where @xmath28 is the lagrange multiplier dependent on @xmath0 .    we equate its functional derivative to zero to derive @xmath29 multiplying this equation by @xmath26 and summing up over @xmath15 , with account of normalization condition @xmath30 we get @xmath31",
    ". then , it follows from equation ( 4 ) that @xmath32 using once more the condition @xmath30 we get @xmath33 and , finally , @xmath34 thus , we have obtained equally probable distribution @xmath16 corresponding to the microcanonical gibbs ensemble of statistical mechanics .",
    "the renyi entropy for this case takes a form @xmath35 this expression does not depend on the renyi parameter and at any @xmath0 coincides exactly with the boltzmann entropy definition @xmath36 called by einstein as _",
    "boltzmann s principle _",
    "from which all equilibrium statistical thermodynamics both extensive and non - extensive hamiltonian systems can be deduced ( see , e.g. [ 18 ] ) . it was proposed [ 18 ] to take boltzmann principle as the axiomatic assumption .",
    "here we obtained it as a consequence of the axiomatics of shore and johnson [ 5 ] led us to renyi entropy and then to boltzmann principle .",
    "thus , the renyi entropy gives new physical insight into the boltzmann principle .",
    "the tsallis entropy exhibits this mandatory property only when @xmath2 .",
    "next we will look for the distribution of probabilities @xmath9 , providing for the extremality of information entropy with an additional conditions which consist in preassigning the average value @xmath37 of the random quantity @xmath38 and the requirement of normalization of @xmath26 .",
    "then , the distribution @xmath9 must be determined from the extremum of the functional @xmath39 where @xmath40 and @xmath28 are lagrange multipliers dependent on @xmath0 .",
    "note that , in the @xmath2 limit it changes to well - known functional @xmath41 its extremum is ensured by the gibbs canonical distribution , in which @xmath42 and @xmath43 is the free energy .",
    "we equate a functional derivative of @xmath44 to zero to derive @xmath45 multiplying this equation by @xmath26 and summing up over @xmath15 , with account of normalization condition @xmath30 we get @xmath46 .",
    "then , it follows from equation ( 10 ) that @xmath47 using once more the condition @xmath30 we get @xmath48 and , finally , @xmath49 such distribution is known now as levy distribution or @xmath0distribution . at @xmath2",
    "the distribution @xmath9 becomes the gibbs canonical distribution in which the constant @xmath50 is the reciprocal of the temperature .",
    "if the tsallis entropy was used instead of the renyi entropy the levy distribution was derived [ 11 ] in the form @xmath51 but the starting functional was forced to be taken as @xmath52 here , the question arises about forms of lagrange multipliers @xmath53 and @xmath54 , but the main problem is that the functional @xmath55 does not pass to the functional ( 9 ) when @xmath2 as the second term in ( 13 ) vanishes .",
    "it seems reasonable to suppose that just this difficulty causes to use the escort distribution @xmath56 in nonextensive thermodynamics [ 12,13,15 ] .",
    "the consistency of the transition to escort distribution is partly justified by the condition conservation of a preassigned average value of the energy @xmath57 , however other average values are to be calculated with the use of the same escort distribution also , that contradicts to the main principles of probability description .",
    "it should be noted also that escort distributions were introduced [ 7 ] as a tool to scan the structure of an original distributions @xmath9 .",
    "indeed , at @xmath58 , the importance of @xmath26 with the maximal values increases , and at @xmath59 , of @xmath26 with minimal values . in view of this",
    ", it is evident that use of escort distributions in statistical thermodynamics does not lead to true average values of dynamical variables .",
    "to clear up a physical sense of the levy distribution ( 11 ) and the renyi parameter @xmath0 we use here an approach proposed by wilk and wlodarchuk [ 19 ] .",
    "we will treat a subsystem which is a minor part of a large equilibrium system and experiences thermal fluctuations of both energy and temperature .",
    "this is a radical difference of the suggested approach from the gibbs approach traditionally employed in statistical physics , in which temperature is preassigned by a constant characterizing the thermostat .    in equilibrium theory of thermodynamic fluctuations",
    "the mean square temperature fluctuation is estimated as @xmath60 where @xmath61 is the heat capacity of the subsystem .",
    "more detailed analysis [ 20 ] , based on the landau ",
    "lifshits theory of hydrodynamic fluctuations , gives the next nonlinear stochastic langevin equation for fluctuating temperature @xmath62 where @xmath63 , @xmath64 is the surface area of the subsystem , @xmath65 is the heat transfer coefficient , and @xmath66 is the average temperature of the subsystem .",
    "a random function of time @xmath67 satisfies the relation @xmath68    corresponding to the derived stochastic langevin equation with @xmath69-correlated noise is the fokker ",
    "planck equation for the temperature distribution function @xmath70 @xmath71 the coefficients @xmath72 and @xmath73 of this equation are expressed in terms of the first @xmath74 and second @xmath75 conditional moments of stochastic equation langevin .",
    "a steady - state solution [ 20,21 ] to equation ( 16 ) is @xmath76 where the dimensionless constant @xmath77 is introduced , and @xmath78 .",
    "therefore , the thus derived distribution function of the inverse temperature of the subsystem in the dimensionless form is the gamma distribution .",
    "if the mean energy of the singled - out volume @xmath79 is introduced , this expression may be rewritten as @xmath80 by its form , this distribution is close to the gibbs distribution ; however , unlike the latter , it accounts for the temperature fluctuation of the subsystem with the preassigned mean energy @xmath81 .    in order to describe a subsystem in contact with a large thermally equilibrium system ( thermostat )",
    ", the gibbs canonical distribution is used in statistical physics ( here and below , the factor @xmath82 allowing for number of states of energy @xmath38 is omitted for brevity ) : @xmath83 where @xmath38 is the energy of the subsystem ( the subscript @xmath15 may indicate the number of discrete energy level or totality of the values of coordinates and momenta of molecules of the subsystem ) , and @xmath84 is the partition function . in so doing , the inverse temperature @xmath85 is taken to be known preassigned quantity .    as was demonstrated above",
    ", the temperature may fluctuate . in view of this",
    ", the question arises as to how the gibbs distribution is modified under the effect of temperature fluctuations .",
    "the answer to this question may be obtained by the way of averaging the gibbs distribution ( 19 ) with the gamma - distribution for temperature @xmath86 ( or @xmath87 ) .    for further treatment , @xmath88",
    "may be conveniently represented in an equivalent form , @xmath89 where the symbol @xmath90 may indicate both the summation and integration over a totality of the values of coordinates and momenta .    using the mean value theorem we represent the gibbs distribution averaged over @xmath91 in the form @xmath92 where @xmath93 lies in the range of possible variation of @xmath94 from @xmath95 to @xmath96 . from the conditions of normalization to unity of the distributions @xmath97 and @xmath98 , we have @xmath99 whence we find @xmath100 therefore ,",
    "it is sufficient to calculate only the average value of the exponent , @xmath101 finally , the averaged gibbs distribution takes the form @xmath102 in the @xmath103 limit corresponding to a high heat capacity of the singled - out subsystem , @xmath104 goes to @xmath88",
    ".    resulted equation for the modified gibbs distribution is similar to equation ( 11 ) for @xmath26 in its structure .",
    "to identify @xmath104 with @xmath26 it is enough to present @xmath105 as @xmath106 , then equation ( 25 ) takes the form @xmath107 the full identity of this expression with the probability @xmath26 ( 11 ) ensuring the extremality of renyi entropy enables one to take a new view of the physical meaning of the renyi entropy and parameter @xmath108 so , the renyi parameter differs significantly from unity only in the case where the heat capacity of the singled out system is of the same order of magnitude as the boltzmann constant @xmath109 .",
    "the thermodynamics of such systems must be constructed on the basis of renyi entropy and boltzmann principle or the levy distribution function @xmath110 or @xmath9 for systems in contact with a heat bath .",
    "we shall emphasize once again that here this function was obtained without invoking any additional considerations as to the nonextensiveness of the systems being treated .",
    "when @xmath15-th state of the system is determined by a value @xmath38 ( indexed in increasing order of magnitude , so that @xmath111 ) , and @xmath112 then for @xmath113 and sufficiently significant fluctuations of @xmath114 exceeding the minimal value @xmath115 expression ( 6 ) transforms to the power distribution @xmath116 so , we derived the zipf  pareto power  law distribution ensuring the extremality of renyi entropy at @xmath113 . in this aspect",
    "it can be said that the power ",
    "law distribution is inherent to the renyi entropy as much as boltzmann or gibbs distributions are inherent to the boltzmann ",
    "shannon entropy . as the exponent @xmath4 is expressed ( 29 ) in terms of the renyi parameter @xmath0 , the evident requirement @xmath117 produces a new constraint @xmath118 that coincides with the condition of concavity of the renyi entropy that produces a constraint @xmath58 . for such @xmath0",
    "the renyi entropy is nor pure concave . ] .",
    "otherwise the renyi parameter remains arbitrary , so while it is varying from @xmath119 to @xmath1 the exponent @xmath4 varies from @xmath120 to @xmath121 . hence it is evident that an additional physical concept should be invoked to specify @xmath0 uniquely .",
    "as a rule , the exponent @xmath4 of power  law distributions for stochastic systems of different nature lies in the narrow range of values between 1 and 2 .",
    "it follows from it that a corresponding value of the renyi parameter @xmath0 connected with @xmath4 by equation ( 29 ) lies too in a narrow range .    in this connection , it is reasonable to assume an existence of some variational principle providing realization of value of the renyi parameter just in this range .",
    "inasmuch as the levy distribution possessing the power  law `` tail '' is inherent in the renyi entropy , it seems to be reasonable to look for extremum on @xmath0 of the difference @xmath122 where @xmath9 is the levy distribution ( 11 ) .",
    "when the subscript @xmath123 for all @xmath15 , we can pass to a continuous picture .",
    "then the probability @xmath26 is replaced by @xmath124 , where @xmath125 is the probability density and @xmath126 . for simplicity , we assume that all @xmath127 are of the same value ( @xmath128 for all @xmath15 ) , then we get @xmath129 @xmath130=-\\ln\\delta x + s_{bd}(p(x))\\ ] ] where @xmath131 it is suggested that the probability density and differential @xmath132 satisfy all requirements which are necessary when passing from darboux sums to integrals in equations ( 31 ) and ( 32 ) .",
    "it is conventional that @xmath133 is referred to as the differential boltzmann entropy , so we call @xmath134 differential renyi entropy .",
    "the original boltzmann and renyi entropies are positive defined values , but differential ones are not positive defined due to smallness of @xmath132 .",
    "nevertheless , their difference is the same as the difference of original entropies , so that @xmath135    an investigation of this function of @xmath0 for the presence of its extremum is only possible for a concrete system when all parameters , @xmath136 of the levy distribution are known .",
    "however , if a main contributions into the integrals of differential entropies ( 34 ) are provided by the range of @xmath114 values from @xmath137 to @xmath138 , we can use the power  law distribution density instead of levy distribution density @xmath139 then @xmath140+\\frac 1{(1-q)}\\ln\\left(\\frac { b^q(1 - x_{min}^{1 - q s}}{1 - q s}\\right).\\end{aligned}\\ ] ] this function of @xmath0 depends on the parameter @xmath137 only .",
    "its three - dimensional plot is illustrated in fig .",
    "well defined maximum with respect to @xmath0 is seen at it and its position @xmath141 depends slightly on @xmath137 . substituting @xmath142 into equation ( 29 ) , we get @xmath143 , which is illustrated in fig .",
    "it is seen that values of @xmath144 are in the range from 1.1 to 2 depending on @xmath137 .",
    "distributions of this kind were found for an extremely wide class of natural and social phenomena . as examples",
    ", we can mention the distributions of magnitudes of earthcrackings ( gutenberg  richter law ) , vortices over their energies in turbulent flows , and also in the science of sciences [ 22 ] , economics ( zipf  pareto distribution law for citizens over their incoming or enterprizes over number of collaborators , or banks over their capitals ) , geography ( distribution law for countries or cities over their citizens ) etc .    in these examples",
    ", we deal with parameters being analogs of energy . in opposite cases , when we are interested in the power ",
    "law distribution over other parameters , say @xmath145 , we should recalculate the exponent . really , if @xmath146 then @xmath147 , that is , the modified exponent is @xmath148 . as an example , for the distribution of fragments over their masses in impact fragmentation of solids , we have [ 23 ] @xmath149 and @xmath150 , then , from the fig .",
    "2 , we find @xmath151 whence @xmath152 , that is in agreement with the experimental results [ 24 ] and theoretical estimations [ 23 ] .",
    "dover publ .",
    ", n .- y . , 1957 + [ 2 ]",
    "shannon , bell syst .",
    "* 27 * 379 ( 1948 ) + [ 3 ] a.g .",
    "bashkirov , a.v .",
    "vityazev , physica a * 277 * , 136 ( 2000 ) . +",
    "[ 4 ] j. uffink , studies in hist .",
    "and philos . of mod .",
    "26b * , 223 ( 1995 ) .",
    "+ [ 5 ] j.e .",
    "shore , r.w .",
    "johnson , ieee trans .",
    "theory * it-26 * , 26        academic publishers .",
    "dordrecht , 1994 + [ 9 ] j.s .",
    "havrdra , f. charvat , kybernatica * 3 * , 30 ( 1967 ) + [ 0pt ] [ 10 ] z. daroczy , information and control * 16 * , 36 ( 1970 ) + [ 11 ] c. tsallis , j.stat.phys .",
    "* 52 * , 479 ( 1988 ) + [ 12 ] e.m.f .",
    "curado , c. tsallis , j.phys . a : math.gen .",
    "* 24 * , l69 ( 1991 ) + [ 0pt ] [ 13 ] m. casas , s. martinez , f. pennini , a. plastino , physica a * 305 * , 41 ( 2002 ) + [ 14 ] a.r .",
    "plastino , a.plastino , physica a * 222 * , 347 ( 1995 ) + [ 15 ] c. tsallis , r.s .",
    "mendes , a.r .",
    "plastino , physica a * 261 * , 534 ( 1998 ) . +",
    "[ 0pt ] [ 16 ] s. abe , phys.lett .",
    "a * 271 * , 74 ( 2000 ) .",
    "+ [ 0pt ] [ 17 ] e.t .",
    "jaynes , phys.rev . * 106 * , 620 ; * 107 * , 171 ( 1957 ) + [ 18 ] d.h.e .",
    "gross , physica a * 305 * , 89 ( 2002 ) + [ 19 ] g. wilk , z. wlodarczyk , phys.rev.lett . *",
    "84 * , 2770 ( 2000 ) . +",
    "[ 20 ] a.g .",
    "bashkirov , a.d .",
    "sukhanov , j. exp .",
    "* 95 * , 440 ( 2002 ) + [ 0pt ] [ 21 ] s. akhmanov , yu.e .",
    "djakov , a.s .",
    "chirkin , introduction to    statistical radiophysics and optics .",
    "nauka , moscow , 1981 + [ 0pt ] [ 22 ] d. price , little science , big science .",
    "columbia univ .",
    "1963 + [ 0pt ] [ 23 ] a.g .",
    "bashkirov , a.v .",
    "vityazev , planet .",
    "space sci .",
    "v.44 , 909915 ( 1996 ) + [ 24 ] a. fujiwara , g. kamimoto and a. tsukamoto , icarus , * 31 , * 277 ( 1977 ) + [ 0pt ]"
  ],
  "abstract_text": [
    "<S> the renyi entropy with a free renyi parameter @xmath0 is the most justified form of information entropy , and the tsallis entropy may be regarded as a linear approximation to the renyi entropy when @xmath1 . </S>",
    "<S> when @xmath2 , both entropies go to the boltzmann  shannon entropy . </S>",
    "<S> the application of the principle of maximum of information entropy ( mep ) to the renyi entropy gives rise to the microcanonical ( homogeneous ) distribution for an isolated system . </S>",
    "<S> whatever the value of the renyi parameter @xmath0 is , in this case the renyi entropy becomes the boltzmann entropy @xmath3 , that provides support for universality of the boltzmann s principle of statistical mechanics . for a system being in contact with a heat bath , the application of mep to the renyi entropy gives rise to levy distribution ( or , @xmath0-distribution ) accepted as one of the main results of the so - called nonextensive statistics . </S>",
    "<S> the same distribution is derived here for a small physical system experiencing temperature fluctuations . </S>",
    "<S> the long  range `` tail '' of the levy distribution is the power  </S>",
    "<S> law ( zipf - pareto ) distribution with the exponent @xmath4 expressed via @xmath0 . </S>",
    "<S> the exponent and free renyi parameter @xmath0 can be uniquely determined with the use of a further extension of mep </S>",
    "<S> . then typical values of @xmath4 are found within the range @xmath5 and of @xmath0 within the range @xmath6 , in dependence on parameters of stochastic systems .     </S>",
    "<S> +     + institute dynamics of geospheres , ras , + leninskii prosp . </S>",
    "<S> 38 ( bldg.1 ) , 117334 , moscow , russia +    pacs : 05.10.gg , 05.20.gg , 05.40.-a + renyi entropy , tsallis entropy , escort distribution , temperature fluctuations . </S>"
  ]
}