{
  "article_text": [
    "this paper concerns the nonparametric estimation of a regression function @xmath0 that regresses @xmath1 on @xmath2 , where the nonnegative variable @xmath1 is subject to various filtering schemes and where @xmath3 is an observed vector of regressors .",
    "we consider both the mean and the median regression case .",
    "a common particular case is the standard censored regression model @xmath4 , where @xmath3 is an observed @xmath5-dimensional vector of regressors , @xmath1 is subject to random right censoring and @xmath6 is an unobserved error satisfying @xmath7 .",
    "we make two contributions .",
    "first , we present a completely nonparametric estimation methodology . this",
    "is done under more general censoring patterns than in previous papers .",
    "second , we assume that the error is independent of the covariate and we show how to construct a more efficient estimator that takes account of the common shape",
    ".    parametric and semiparametric estimators of censored regression models include heckman @xcite , buckley and james @xcite , koul , susarla and van ryzin @xcite , powell @xcite , duncan @xcite , fernandez @xcite , horowitz @xcite , ritov @xcite , honor and powell @xcite , buchinsky and hahn @xcite and heuchenne and van keilegom @xcite .",
    "many of these authors either assume @xmath8 or some other parametric form , provide estimates of average derivatives only up to an unknown scale or assume that the error distribution is parametric .",
    "the fully nonparametric @xmath0 model we consider is important because of the sensitivity of the parametric and semiparametric estimators to misspecification of functional form .",
    "a small number of estimators exist for nonparametric censored regression models , in most cases focusing on the standard random censoring model .",
    "dabrowska @xcite and van keilegom and veraverbeke @xcite proposed nonparametric censored regression estimators based on quantile methods .",
    "lewbel and linton @xcite considered the above standard censoring model , except that the censoring time @xmath9 is taken to be a degenerate random variable ( i.e. , it is constant ) , while heuchenne and van keilegom @xcite considered the standard model when it is supposed that @xmath6 is independent of @xmath3 .    in this paper",
    ", we propose a unified approach to the estimation of the regression function from filtered data .",
    "filtering , for example , left truncation or right censoring , means that even though some information is available about @xmath1 , @xmath1 itself is sometimes not observed , even though @xmath3 is observed .",
    "it is imperative for us that our estimation principles are natural and well known in the simple case of independent identically distributed errors with no filtering .",
    "our approach makes use of tools from the field of counting process theory ; see @xcite and @xcite .",
    "first , we recognize that the generic regression model can be reformulated through the counting process @xmath10 such that @xmath11 the advantage of the counting process approach is that it readily lends itself to quite general filtering mechanisms , allowing for complicated left truncation and right censoring patterns .",
    "we reformulate the regression model in terms of a counting process @xmath12 having stochastic intensity function @xmath13 with respect to the increasing , right - continuous and complete filtration @xmath14 . here , @xmath15 and @xmath16 is the conditional hazard function of @xmath1 given that @xmath2 . with these definitions , we have that the conditional mean is given by @xmath17 and the conditional median is given by @xmath18 where the relation between the conditional survival function @xmath19 and the conditional hazard function @xmath20 is given by @xmath21 this connection between the hazard function and the regression function is the basis of our estimation .    for the first contribution of this paper ,",
    "we consider @xmath22 as estimated from a local constant least - squares principle or a local linear least - squares principle . plugging these estimators into the expressions ( eq :",
    "mean ) and ( [ eq : median ] ) results in , respectively , a local constant @xmath23 and a local linear estimator @xmath24 of the conditional mean or median .",
    "it is important to note that in the absence of filtering , the traditional local constant and local linear kernel regression estimators are special cases of the estimators @xmath23 and @xmath24 .",
    "the second contribution of this paper is concerned with the estimation of the functions @xmath25 and @xmath26 when some structure is imposed on the model .",
    "if there is a substantial level of filtering , then one can envision areas where truncation or censoring imply that we do not have local information on the entire shape of the error distribution around every  @xmath27 .",
    "one can alleviate this by imposing assumptions on the shape of these local error distributions .",
    "the simplest model assumption in this connection is the multiplicative regression model @xmath28 where the error term @xmath29 is independent of @xmath3 and has mean or median equal to one , and where @xmath30 is either @xmath31 or @xmath32 . under this model ,",
    "@xmath33 for some function @xmath34 where @xmath35 is the conditional hazard function of @xmath36 given that @xmath2 .",
    "if model ( [ eq : genmodel2 ] ) is true , then it can be used to improve estimation , even in the case without filtering ; see @xcite . our estimation strategy in this case is sequential .",
    "we first obtain the unrestricted estimator @xmath37 or @xmath38 described above .",
    "we then use the relation @xmath39 or , equivalently , @xmath40 to obtain an estimate for @xmath41 we use a minimum chi - squared approach to do this optimally , which involves replacing @xmath0 by @xmath42 and @xmath22 by the completely nonparametric estimator @xmath43 . given an estimator of @xmath44",
    ", we then obtain a new estimator of @xmath0 using the minimum chi - squared approach , again based on the relation @xmath45 , but now replacing @xmath46 by @xmath47 and @xmath22 by @xmath48 .",
    "we will argue that our estimator fulfills a local efficiency criterion .",
    "van keilegom and akritas @xcite and heuchenne and van keilegom @xcite discuss estimation of @xmath49 and @xmath50 , respectively , in the additive error model when @xmath51 is independent of @xmath3 . in the first two papers , @xmath49 or @xmath52",
    ", respectively , is written as a functional of the error distribution and of the distribution of the covariates .",
    "the estimator is based on plugging in estimates of these distributions . in the last paper ,",
    "censored observations are replaced by synthetic data points . in all three of these papers ,",
    "efficiency issues are not discussed and the analysis is restricted to the case of random right censoring .    the outline of the paper is as follows . in section  [ sec2 ] , we describe the theoretical background in terms of the counting process formulation , including the important special case of filtered data . in section  [ sec3 ] , we introduce our approach to regression based on filtered data in the general situation , where we do not restrict the functional form of the error distribution .",
    "we present the local constant case in detail ; the local linear case is given in the .",
    "the more efficient estimator ( at least when the assumption is correct ) based on the assumption on the functional form ( assumption ( [ eq : alfanul ] ) ) is introduced in section  [ sec4 ] , where we also give its asymptotic distribution . in section  [ sec5 ] ,",
    "we present a small simulation study . in the",
    ", we give the proofs of the main distribution results contained in the text .",
    "let @xmath53 , @xmath54 , be @xmath55 i.i.d .",
    "replications of the random vector @xmath56 , where the response @xmath57 is subject to filtering and therefore possibly unobserved , and the covariate @xmath58 is completely observed .",
    "define @xmath59 for all @xmath60 in the support of @xmath61 .",
    "then @xmath62 is an @xmath55-dimensional counting process with respect to possibly different , increasing , right - continuous , complete filtrations @xmath63 ; see @xcite , page 60 .",
    "we assume that with respect to the filtration , @xmath64 has stochastic intensity @xmath65 where @xmath66 is a predictable process taking values in @xmath67 .",
    "we have not restricted the conditional distribution of @xmath68 and the functional form of the conditional hazard function @xmath69 is likewise unrestricted . with these definitions",
    ", @xmath70 is predictable , and the processes @xmath71 @xmath72 and compensators @xmath73 are square - integrable local martingales on the support of @xmath57",
    ".    we can allow this extremely general model description since the martingale central limit theorem dating back to rebolledo @xcite can be applied in this context ; see @xcite , pages 8285 .",
    "our framework is sufficiently general to include a number of interdependencies , including a variety of time series analyses .      in this section ,",
    "we follow andersen @xcite , page 50 .",
    "let @xmath74 be a predictable process taking values in @xmath67 , indicating ( by the value @xmath75 ) when the @xmath76th individual is at risk .",
    "note that the predictability condition of @xmath74 allows it to depend on @xmath77 in every possible way .",
    "let @xmath78 be the filtered counting process and introduce the filtered filtration @xmath79 the random intensity process @xmath80 is then @xmath81 and the integrated random intensity process is @xmath82 with these definitions , @xmath83 is a square - integrable martingale with respect to the filtration @xmath84 note that , in the filtered case , @xmath85 is not always observed , but the product @xmath86 is always observable .",
    "in this section , local constant and local linear estimators under the general nonparametric model are given .",
    "these estimators take the local constant and the local linear marker - dependent kernel hazard estimators of nielsen and linton @xcite and nielsen @xcite as their starting point . in the special case of no filtering ,",
    "this results in the convenient property that the regression estimator based on the local constant hazard estimator is the well - known local constant regression estimator , the nadaraya ",
    "watson estimator , and the local linear hazard estimator results in the local linear regression estimator ; see , for example , @xcite .",
    "let @xmath87 be a @xmath5-dimensional kernel , @xmath88 be a one - dimensional kernel , @xmath89 be a @xmath5-dimensional bandwidth vector and @xmath90 be a one - dimensional bandwidth .",
    "for any real @xmath91 and any @xmath5-dimensional vector @xmath92 , define @xmath93 and @xmath94 where @xmath95 and @xmath96 .",
    "the estimator suggested by nielsen and linton ( 1995 ) is @xmath97 where @xmath98 this estimator was identified as a local constant least - squares estimator in @xcite . the super / subscript @xmath9 stands for local constant smoothing .",
    "below , we will also introduce estimators based on local linear smoothing .",
    "this will be indicated by a super / subscript @xmath99 in the notation .",
    "we wish to estimate the conditional integrated hazard @xmath100 we could just integrate @xmath101 with respect to @xmath102 but a better strategy is to first let the bandwidth @xmath103 which eliminates redundant smoothing@xmath104 the resulting estimator is @xmath105 note that @xmath106 equals the estimator of @xmath107 proposed by beran @xcite and dabrowska @xcite in the case of random censoring .",
    "we then estimate the conditional survivor function @xmath49 by the product limit estimator of johansen and gill @xcite ; see @xcite , that is , @xmath108 for @xmath109 , where @xmath110 satisfies assumption ( a ) below .",
    "the local constant estimator of @xmath111 is@xmath112 a local constant estimator of @xmath113 is given by @xmath114 where for any @xmath115 , @xmath116 .",
    "another option would have been to define @xmath117 in the above formula .",
    "the advantage of the weighted product limit estimator is that we arrive at exactly the extension of the kaplan ",
    "meier estimator to filtered data in the absence of covariates and at the weighted empirical distribution function @xcite in the absence of filtering . as a consequence , ( [ gt ] )",
    "reduces to the well - known nadaraya ",
    "watson estimator when @xmath118 and when all data are completely observed .    in a similar way",
    ", the local linear estimators of @xmath119 , @xmath120 and @xmath121 , denoted @xmath122 , @xmath123 and @xmath124 , respectively , can be defined .",
    "we refer to the for their precise definitions .    for the asymptotic properties of the unrestricted estimators @xmath125 and @xmath123 of @xmath120",
    ", we need to assume the following for @xmath126 , where @xmath127 is a bounded interval in the interior of the support of @xmath3 .",
    "all of our results are stated for the special case of a one - dimensional covariate  @xmath3 , @xmath128 .",
    "the results can be easily generalized to a multivariate setting .    1 .   the derivatives @xmath129 and @xmath130 exist and are uniformly continuous in @xmath131 $ ] .",
    "the kernel @xmath87 is symmetric , continuous and has bounded support .",
    "the bandwidth @xmath132 satisfies @xmath133 , @xmath134 and @xmath135 .",
    "the truncation variable @xmath110 is such that @xmath136}\\varphi_{x}(u)>0 $ ] .",
    "4 .   there exists a continuous function @xmath137 such that @xmath138 } \\biggl\\vert\\frac{1}{n}\\sum_{i=1}^{n}k_{b } ( x - x_{i } ) c_{i}(y)z_{i}(y)-\\varphi _ { x}(y)\\biggr \\vert&\\stackrel{p}{\\rightarrow}&0 , \\\\ \\sup_{y\\in\\lbrack0,t ] } \\biggl\\vert\\frac{1}{n}\\sum_{i=1}^{n}\\frac { ( x - x_{i } ) ^2 } { b^{2 } } k_{b } ( x - x_{i } ) c_{i}(y)z_{i}(y)-\\frac{1 } { 2}\\mu_{2}(k ) \\varphi_{x}(y ) \\biggr\\vert & \\stackrel{p}{\\rightarrow}&0,\\end{aligned}\\ ] ] where @xmath139 .",
    "5 .   the derivative @xmath140 exists and is continuous .",
    "it holds that @xmath141 } \\biggl\\vert\\frac{1}{n}\\sum_{i=1}^{n } ( x_{i}-x ) b^{-2 } k_{b } ( x - x_{i } ) c_{i}(y)z_{i}(y)-\\mu _ { 2}(k ) \\frac{\\partial\\varphi_{x}(y)}{\\partial x } \\biggr\\vert\\stackrel { p}{\\rightarrow}0.\\ ] ] 6 .   for @xmath142",
    ", it holds that @xmath138 } \\vert\\widehat{s}_{x , a}(y)-s_{x}(y ) \\vert&\\stackrel{p}{\\rightarrow}&0 , \\\\   { \\sup_{y\\in\\lbrack0,t ] } } \\vert{s}^ * _ { x , a}(y)-s_{x}(y ) \\vert&\\stackrel{p}{\\rightarrow}&0.\\end{aligned}\\ ] ] here , @xmath143 is defined as @xmath144 in ( [ def1s ] ) , ( [ def2s ] ) , ( [ def1sl ] ) and ( [ def2sl ] ) , but with @xmath145 replaced by @xmath146 .",
    "( an explicit definition of @xmath147 is also given in the proof of theorem 3.1 . )",
    "these assumptions are rather standard smoothing assumptions .",
    "assumptions ( d4)(d6 ) are low - level assumptions .",
    "we chose them instead of high - level assumptions to avoid more specific assumptions on the censoring .",
    "for the unfiltered case , these assumptions are classical smoothing results . for the filtered case , consider first the case of random right censoring .",
    "then @xmath148 where @xmath149 is the minimum of the survival time @xmath57 and the censoring time @xmath150 , which are supposed to be independent of each other given @xmath151 .",
    "it is easily seen that the latter quantity converges to @xmath152 uniformly in @xmath153 and @xmath154 $ ] .",
    "other examples of filtering ( including , e.g. , left and/or right truncation and/or censoring ) can be handled in a similar way .",
    "assumption ( d5 ) is only needed for the asymptotic result based on local constant smoothing and not for local linear smoothing .",
    "[ ga ] suppose that assumptions hold .",
    "there then exist bounded continuous functions @xmath155 and @xmath156 @xmath157 , such that for all @xmath126 , @xmath158 where@xmath159    to be consistent with the theory for kernel regression estimators , it must be that in the absence of filtering,@xmath160 where @xmath161 $ ] and @xmath162 is the covariate density@xmath104 note that @xmath163=2\\int us_{x}(u)\\,\\mathrm{d}u- \\biggl ( \\int s_{x}(u)\\,\\mathrm{d}u \\biggr ) ^{2}.\\ ] ] in the absence of filtering , @xmath164 therefore , it should be the case that@xmath165 this follows by integration by parts . for @xmath121 , it has been shown in @xcite that @xmath166 is asymptotically normal when the data are subject to random right censoring .",
    "it can be shown that this result continues to hold true for general filtering patterns .",
    "under some circumstances , it may be plausible to assume that the error distribution , when adjusted for the mean or the median , is generated by the same underlying shape . if there is a substantial level of filtering , then one can envision areas where truncation or censoring imply that we do not have local information on the entire shape of the error distribution around every  @xmath167 one can alleviate this by imposing assumptions on the shape of these local error distributions . the simplest assumption in this connection",
    "is simply that @xmath168 does not depend on @xmath27 , where @xmath29 is the error term in model ( [ eq : genmodel2 ] ) .",
    "this is @xmath169 for some @xmath170 and all @xmath171 if this assumption is true , then it can be used to improve estimation , even in the case without filtering , as we now discuss .",
    "the notion of efficiency is here tied to asymptotic variance , which yields mean - squared error holding bias constant , and comes from the classical parametric theory of likelihood .",
    "the local likelihood method was introduced in @xcite and has been applied in many other contexts .",
    "tibshirani @xcite , chapter 5 , presents the justification for the local likelihood method ( in the context of an exponential family ) : the author shows that its asymptotic variance is the same as the asymptotic variance of the maximum likelihood estimator ( mle ) of a correctly specified parametric model at the point of interest using the same number of observations as the local likelihood method .",
    "this type of result has been shown in other settings , for example , linton and xiao @xcite establish efficiency of a local likelihood estimator in the context of nonparametric regression with additive errors . in generalized additive models ,",
    "linton @xcite shows the improvement according to variance obtainable by the local likelihood method .",
    "in what follows , @xmath0 is either @xmath172 or @xmath121 and similarly for the estimators of @xmath0 .",
    "first , we note that both the local constant and the local linear kernel estimator of the full marker - dependent hazard model have the form @xmath173 where @xmath174 equals @xmath9 for the local constant case and @xmath174 equals @xmath99 for the local linear case .",
    "let us suppose that an oracle told us what @xmath170 is .",
    "we define the local constant estimator and the local linear estimators of @xmath175 based on the assumption ( [ eq : alfanul ] ) to be any minimizer @xmath176 of the criterion function @xmath177 ^{2 } \\ { \\hat{\\alpha}_{x , a}(y ) \\ } ^{-1}e_{x , y}^{a}w(x , y)\\,\\mathrm{d}x\\,\\mathrm{d}y,\\ ] ] where @xmath178 is an appropriate weight function .",
    "this is motivated by the theory of minimum chi - squared estimation @xcite , in which efficiency is achieved by weighting a least - squares criterion with the inverse of the asymptotic variance of the unrestricted estimator ( in this case , @xmath179 which has asymptotic variance @xmath180 , where @xmath137 is the probability limit of the exposure @xmath181 for a fixed @xmath27 , this expression is minimized by minimizing the pointwise criterion@xmath182 ^{2 } \\ { \\hat{\\alpha}_{x , a}(y ) \\ } ^{-1}e_{x , y}^{a}w(x , y)\\,\\mathrm{d}y \\label{t2}\\ ] ] with respect to @xmath183 and setting @xmath184 for some compact set @xmath185 not containing 0 .",
    "this is a nonlinear estimator , not obtainable in closed form .",
    "define@xmath186 ^{2 } \\",
    "{ \\alpha_x ( y ) \\ }",
    "^{-1}\\varphi_{x}(y ) w(x , y ) \\,\\mathrm{d}y\\ ] ] and let @xmath187 for the asymptotic result below , we need to assume the following :    1 .",
    "\\(i ) the weight function @xmath178 is continuous and satisfies @xmath188 for @xmath189 and @xmath190 for all @xmath191 , where @xmath192 and @xmath193 , where @xmath194 and @xmath195 are continuous functions and where , as in ( d1)(d5 ) , @xmath127 is a bounded interval in the interior of the support of @xmath3 .",
    "+ \\(ii ) there exists a continuous function @xmath196 with @xmath197 such that the convergence statements in ( d4 ) and ( d5 ) hold with the supremum running over @xmath191 instead of @xmath198 $ ] .",
    "the function @xmath137 is twice continuously differentiable in @xmath60 for @xmath191 .",
    "the function @xmath199 $ ] is twice continuously differentiable in @xmath191 and @xmath200 .",
    "the probability density functions @xmath87 and @xmath88 are symmetric around 0 and have support @xmath201 $ ] , @xmath202 , @xmath203 , @xmath204 , and @xmath87 and @xmath88 are twice continuously differentiable .",
    "4 .   for all @xmath205 , @xmath206",
    ", @xmath207 is twice differentiable with respect to @xmath183 in a neighborhood of @xmath208 and @xmath209 .",
    "the bandwidths @xmath90 and @xmath132 satisfy @xmath210 , @xmath133 , @xmath211 , @xmath212 and @xmath135 .",
    "conditions ( a2 ) , ( a3 ) and ( a5 ) are standard smoothing assumptions . assumption ( a1 ) is stated uniformly in @xmath27 because such a uniform version is required in the later theorems  [ alpha0 ] and  [ g2step ] .",
    "[ goracle ] suppose that assumptions hold .",
    "there then exist bounded continuous functions @xmath213 and @xmath214 , @xmath215 , such that for all @xmath216 , @xmath217 where , with @xmath218@xmath219 ^{-1 } , \\\\",
    "v_{l}^{o}(x ) & = & v_{c}^{o}(x ) .\\end{aligned}\\ ] ]    in the absence of filtering , the optimal estimator of @xmath0 , given the knowledge of @xmath44 or , equivalently , of the density @xmath220 of @xmath29 , is the local likelihood estimator that maximizes@xmath221 which has score function@xmath222 where @xmath223 the object @xmath224 is known as the _ fisher scale score _ and @xmath225 is the corresponding information .",
    "one can show that the asymptotic variance of this oracle local likelihood estimator is@xmath226 supposing that we had @xmath227 observations from the model @xmath228 the mle of @xmath229 would have asymptotic variance @xmath230 in this sense , the local likelihood method has the efficiency of the mle from a sample of size @xmath231    by efron and johnstone @xcite , we have@xmath232 which explains the form of the asymptotic variance above .",
    "suppose that we take @xmath233 , make a change of variables @xmath234 in @xmath235 and make use of the fact that , under no filtering , @xmath236 and so @xmath237 then @xmath238([jj ] ) .",
    "this shows that @xmath239 is asymptotically equivalent to the oracle local likelihood method , that is , efficient in this sense .      for a given @xmath241 , an estimator of @xmath170",
    "can be based on the minimization principle @xmath242 ^{2 } \\ { \\hat{\\alpha}_{x , a}(y ) \\ }",
    "^{-1}e_{x , y}^{a } w(x , y ) \\,\\mathrm{d}x\\,\\mathrm{d}y,\\ ] ] where the choice of weighting function is again motivated by efficiency considerations .",
    "changing variables @xmath243 the objective function becomes@xmath244 ^{2}g(x ) \\ { \\hat{\\alpha}_{x , a}(ug(x ) ) \\ }",
    "^{-1}e_{x , ug(x)}^{a } w(x , ug(x ) ) \\,\\mathrm{d}x\\,\\mathrm{d}u \\\\ & & \\quad = \\int\\!\\!\\int [ g(x)\\hat{\\alpha}_{x , a}(ug(x))-\\alpha(u ) ] ^{2}\\frac{e_{x , ug(x)}^{a}}{g(x)\\hat{\\alpha}_{x , a}(ug(x ) ) } w(x , ug(x ) ) \\,\\mathrm{d}x\\,\\mathrm{d}u , \\end{aligned}\\ ] ] ignoring support considerations . then , because @xmath245 does not depend on @xmath27 , we can replace it by the pointwise criteria @xmath246 ^{2}\\frac{e_{x , ug(x)}^{a}}{g(x)\\hat{\\alpha } _ { x , a}(ug(x ) ) } w(x , ug(x ) ) \\,\\mathrm{d}x\\ ] ] for each @xmath247 whence we obtain the closed form solution@xmath248 in practice , one computes @xmath249 as ( [ eq : alfaet ] ) with @xmath0 replaced by a preliminary completely nonparametric estimator @xmath250 , that is , @xmath251    let @xmath60 be a fixed value , that is , such that @xmath252 , where @xmath253 and @xmath254 ( and where we assume that @xmath255 ) .",
    "we require the following assumptions :    1 .",
    "the preliminary estimator @xmath256 satisfies @xmath257 .",
    "the function @xmath0 is twice continuously differentiable in @xmath258 and @xmath259 .",
    "3 .   the bandwidths @xmath90 and @xmath132 satisfy @xmath260 , @xmath261 , @xmath262 , @xmath263 , @xmath264 and @xmath265 .",
    "[ alpha0 ] suppose that assumptions and hold .",
    "there then exist bounded continuous functions @xmath266 and @xmath267 , @xmath268 , such that for all @xmath252 , @xmath269 where @xmath270 f_x(yg(x ) ) w^2(x , yg(x ) ) \\ } , \\\\",
    "s_{l}(y ) & = & s_{c}(y ) , \\\\",
    "b^o(y ) & = & e[(cz)\\{yg(x)\\ } w\\{x , yg(x)\\ } ] / \\alpha_0(y ) .\\end{aligned}\\ ] ]    finally , we compute a new estimate of @xmath175 using the estimate of @xmath271 specifically , define the weighted least - squares objective function @xmath272 ^{2 } \\ { \\hat{\\alpha}_{x , a}(y ) \\ } ^{-1}e_{x , y}^{a } w(x , y ) \\,\\mathrm{d}y\\ ] ] with @xmath273 or @xmath274 , and with @xmath275 equal to @xmath276 , @xmath277 or another estimator of @xmath278",
    ". then let @xmath279 where the argmin runs over a shrinking neighborhood @xmath280 of a consistent estimator of @xmath0 .",
    "in the next theorem , we state that under some conditions on the estimator @xmath275 , we obtain the same variance and bias as in the oracle case .",
    "one possibility is to use the estimator of @xmath175 given in section  [ sec3 ] as preliminary estimator and to base the final estimation of @xmath175 on the method of the above section  [ sec4.1 ] , but replacing the oracle @xmath170 by @xmath281 .",
    "we make use of the following additional assumptions :    1 .   for a neighborhood @xmath282 of the closed interval @xmath283 $ ]",
    ", it holds uniformly for @xmath284 that @xmath285 for sequences @xmath286 , @xmath287 and @xmath288 with @xmath289 , @xmath290 , @xmath291 , @xmath292 , @xmath293 and @xmath294 .",
    "2 .   with a bounded function @xmath295",
    ", it holds that @xmath296 \\rho_{0}\\biggl \\ { \\frac{y}{g(x ) } \\biggr\\ } \\frac{1}{g(x)^{2}}\\frac{e_{x , y}^{c}}{\\alpha _ { x}(y)}w(x , y)\\,\\mathrm{d}y - h^{2}\\gamma(x)=\\mathrm{o}_{p}\\bigl((nb)^{-1/2}\\bigr),\\ ] ] where @xmath297 .",
    "3 .   the bandwidths @xmath90 and @xmath132 satisfy @xmath298 , @xmath299 , @xmath135 and @xmath300 .",
    "these assumptions are rather weak .",
    "assumption ( c1 ) is fulfilled for a standard one - dimensional kernel smoother which fulfills the conditions with @xmath301 , @xmath302 and @xmath303 .",
    "the assumption is fulfilled under much slower rates of convergence .",
    "the assumption could be replaced by another type of condition using the general approach of mammen and nielsen @xcite based on cross - validation arguments .",
    "assumption ( c2 ) is a standard property of kernel smoothers : kernel smoothers are local weighted averages .",
    "integration of the estimator leads to a global weighted average with stochastic part of parametric rate @xmath304 .",
    "typically , the rate of the bias part does not change .",
    "[ g2step ] suppose that assumptions and hold .",
    "there then exist bounded continuous functions @xmath305 and @xmath306 such that for all @xmath126 , @xmath307 where @xmath308    this shows that the two - step estimator achieves the desired oracle property .",
    "in this section , we look at the small - sample performance of our estimators .",
    "the design involves a combination of commonly occurring features in the literature : we take the true underlying regression function to be identical to that of fan and gijbels @xcite , but our disturbance term has a different distribution and we also consider a different censoring mechanism .",
    "thus , @xmath309 where @xmath310 , $ ] @xmath311 , $ ] while @xmath312 and @xmath313 are independent and @xmath314 .",
    "the censoring time mechanism is independent of the covariate and constructed as follows:@xmath315 where @xmath316 @xmath317 and we observe @xmath318 , that is , an example of right censoring .",
    "we employ two methods of estimation of @xmath319 : the simple local constant estimation of section  [ sec3 ] and the feasible oracle estimation , as discussed in section  [ sec4.2 ] . for the purposes of illustration",
    ", we use silverman s rule of thumb bandwidth and the built - in minimization routine based on the golden section search and parabolic interpolation . for the more efficient estimator",
    ", we note that using the one - dimensional grid search gives a very similar estimate.=1            we use a sample of size @xmath320 and @xmath321 replications over @xmath322 evenly spaced grids on @xmath323 $ ] . in this example , approximately @xmath324 of the @xmath322 observations are censored .",
    "figure  [ fig1 ] displays the average ( over replications ) of the two estimates .",
    "the true regression function chosen possesses a high degree of curvature , with the function increasing less steep to the right of @xmath325 than to the left of @xmath325 .",
    "both estimates are capable of capturing the basic structure of the true curve .",
    "the efficient estimate appears to adapt better at both peaks and troughs , and the quality of fit declines with the steepness of the true curve .",
    "although it is not shown here , the relative performance of the simple local constant estimator improves toward the feasible oracle estimates when the true regression function has lower degree variation . figures  [ fig2 ] and  [ fig3 ] are the qq - plots for the efficient and inefficient estimates , respectively ( i.e. , @xmath326 .",
    "the linear trends in the qq - plots are distinct with the efficient estimates performing a little better away from the sample means .",
    "figure  [ fig4 ] plots the interquartile range ( divided by 1.3 ) and the standard deviation ( across replications ) for the efficient estimate against grid points .",
    "performance clearly worsens in the boundary region .",
    "since it is widely perceived that the silverman s rule of thumb bandwidth tends to oversmooth , we also performed some experiments with smaller bandwidths .",
    "smaller bandwidth leads to much larger simulation time during optimization , due to higher variance . in terms of goodness of fit , it does not make a big difference with the feasible oracle estimation .",
    "however , the improvement of fit for the simple local constant estimation is more pronounced . in that case , the feasible oracle estimation still performs better than the simple estimator , as expected .",
    "in this section , we first define the local linear marker - dependent estimator , @xmath327 as defined in @xcite , page 118 , @xmath328 where , with @xmath329 and @xmath330 ( to simplify the notation , we consider the same kernel and bandwidth for @xmath27 and @xmath60 ) , @xmath331 k_{w , b}(v)&= & \\ { k_{b}(v)-k_{b}(v)v^{t}d^{-1}c_{1 } \\}\\qquad ( v \\in r^{d+1 } ) , \\nonumber \\\\[-0.5pt ] c_{0}&=&n^{-1}\\sum_{i=1}^{n}\\int k_{b } \\ { w - w_{i}(u ) \\ }",
    "c_{i}(u ) z_i(u ) \\,\\mathrm{d}u , \\\\[-0.5pt ] c_{1j}&=&n^{-1}\\sum_{i=1}^{n}\\int k_{b } \\ { w - w_{i}(u ) \\ } \\ { w_{j}-w_{ij}(u ) \\ }",
    "c_{i}(u ) z_i(u ) \\,\\mathrm{d}u , \\nonumber\\\\[-0.5pt ] d_{jk}&=&n^{-1}\\sum_{i=1}^{n}\\int k_{b } \\ { w - w_{i}(u ) \\ } \\ { w_{j}-w_{ij}(u ) \\ } \\ { w_{k}-w_{ik}(u ) \\ }",
    "c_{i}(u ) z_i(u ) \\,\\mathrm{d}u , \\nonumber\\end{aligned}\\ ] ] and @xmath332 and @xmath333 .",
    "we then consider the local linear estimator of the integrated conditional hazard function , obtained when we undersmooth in the @xmath60-direction .",
    "first , we define the necessary kernel constants : @xmath334 we then get that the local linear estimator of the integrated hazard is @xmath335 we estimate correspondingly the conditional survival function @xmath119 and the regression functions @xmath120 and @xmath121 by @xmath336      we restrict attention in the proofs to the case of local constant smoothing ( i.e. , when @xmath273 ) . the case of local linear smoothing ( @xmath274 ) can be considered in a very similar way and is therefore omitted . throughout this section , we use the notation @xmath337 to indicate that @xmath338 .",
    "first , we state a useful lemma .",
    "its simple proof is omitted .",
    "let @xmath339 be the limit from the left at @xmath60 for any cadlag function @xmath90 .",
    "[ lem ] suppose @xmath340 and @xmath341 are cadlag functions .",
    "let @xmath342 , @xmath343 and @xmath344 then @xmath345    proof of theorem [ ga ] define @xmath346 then @xmath347    let @xmath348 we then divide our analysis into an analysis of the variable part @xmath349 and of the stable part @xmath350    note that @xmath351 where @xmath352 @xmath353 and @xmath354 using integration by parts , we obtain@xmath355=\\int_0^t [ \\widehat { s}_{x , c}(y)-s_{x}(y)]\\,\\mathrm{d}y={\\mathcaligr}{v}(x)+{\\mathcaligr}{b}(x),\\ ] ] where @xmath356 and @xmath357 by lemma [ lem ] , we have@xmath358 & = & \\int_{0}^{t}s_{x}^{\\ast}(y)\\int_{0}^{y}\\frac{\\widehat{s}_{x , c}(u- ) } { s_{x}^{\\ast}(u-)}\\,\\mathrm{d } \\ { \\widehat{a } _ { x , c}(u)-a _ { x , c}^{\\ast}(u ) \\ } \\,\\mathrm{d}y \\\\[-1pt ] & = & \\int_{0}^{t}s_{x}^{\\ast}(y)\\int_{0}^{y}\\frac{\\widehat{s}_{x , c}(u- ) } { s_{x}^{\\ast}(u-)}\\sum_{i=1}^{n}k_{b } ( x - x_{i } ) \\frac{\\mathrm{d}\\overline{m}_{i}(u)}{\\sum_{j=1}^{n}k_{b}(x - x_{j})c_{j}(u)z_{j}(u)}\\,\\mathrm{d}y \\\\[-1pt ] & = & \\sum_{i=1}^{n}\\int_{0}^{t}\\hat{h}_{x}^{i}(u)\\,\\mathrm{d}\\overline{m}_{i}(u),\\end{aligned}\\ ] ] where @xmath359    let @xmath360 then @xmath361 and by nielsen and linton @xcite , proposition 1 , @xmath362 where@xmath363 the results given in theorem [ ga ] on the variable part follow from standard martingale theory ; see , among many others , @xcite .",
    "we now turn to the bias . using ( d4)(d6 ) , we have@xmath364    the derivation of the asymptotic theory of the local linear case parallels the local constant case . while the variable part has the same asymptotic distribution , the stable part changes due to the bias properties of the local linear hazard estimator . by checking the derivation of the stable part of the local linear kernel hazard estimation of nielsen @xcite , page 119 ,",
    "it is easy to see that the stable part of the local linear estimator can be written as @xmath365    proof of theorem [ goracle ] consistency of @xmath366 follows from condition ( a1 ) and the fact that@xmath367 ( see , e.g. , @xcite , theorem 5.7 , page  45 ) .",
    "the result ( [ q1 ] ) follows from assumption ( a2 ) and the uniform consistency of @xmath368 this is established in @xcite , theorem 2 . actually , @xmath369 ^{2 } \\ { \\hat{\\alpha}_{x , c}(y ) \\ }",
    "^{-1}e_{x , y}^{c } w(x , y ) \\,\\mathrm{d}y \\\\ & & \\qquad{}-\\int\\biggl [ \\alpha_{x}(y)-\\frac{1}{\\theta}\\alpha_{0 } \\biggl\\ { \\frac{y}{\\theta } \\biggr\\ } \\biggr ] ^{2 } \\ { \\alpha_{x}(y ) \\ } ^{-1}\\varphi _ { x}(y ) w(x , y ) \\,\\mathrm{d}y   \\\\ & & \\quad = \\int [ \\hat{\\alpha}_{x , c}(y)-\\alpha_{x}(y ) ] ^{2 } \\ { \\alpha_{x}(y ) \\ } ^{-1}\\varphi_{x}(y ) w(x , y ) \\,\\mathrm{d}y \\\\ & & \\qquad{}+2\\int [ \\hat{\\alpha}_{x , c}(y)-\\alpha_{x}(y ) ] \\biggl [ \\alpha_{x}(y)-\\frac{1}{\\theta}\\alpha_{0 } \\biggl\\ { \\frac{y}{\\theta } \\biggr\\ } \\biggr ] \\ { \\alpha_{x}(y ) \\",
    "} ^{-1}\\varphi_{x}(y ) w(x , y ) \\,\\mathrm{d}y \\\\ & & \\qquad { } + \\int\\biggl [ \\hat{\\alpha}_{x , c}(y)-\\frac{1}{\\theta}\\alpha _ { 0 } \\biggl\\ { \\frac{y}{\\theta } \\biggr\\ } \\biggr ] ^{2 } \\\\ & & \\qquad\\hphantom{+\\int[}{}\\times [ \\ { \\hat{\\alpha}_{x , c}(y ) \\ } ^{-1}e_{x , y}^{c}- \\ { \\alpha_{x}(y ) \\ }",
    "^{-1}\\varphi_{x}(y ) ] w(x , y ) \\,\\mathrm{d}y\\end{aligned}\\ ] ] and this converges to zero in probability , uniformly in @xmath370 .",
    "we next establish asymptotic normality .",
    "first , we consider the taylor expansion@xmath371 where @xmath372 lies between @xmath366 and @xmath373 .",
    "we have @xmath374 \\rho_{0 } \\biggl\\ { \\frac{y}{\\theta } \\biggr\\ } \\frac{1}{\\theta^{2}}\\frac{e_{x , y}^{c}}{\\hat{\\alpha}_{x , c}(y ) } w(x , y ) \\,\\mathrm{d}y,\\ ] ] where @xmath297 and @xmath375 \\rho_{0}^{\\prime } \\biggl\\ { \\frac{y}{\\theta } \\biggr\\ } \\frac{y}{\\theta^{4}}\\frac { e_{x , y}^{c}}{\\hat{\\alpha}_{x , c}(y ) } w(x , y ) \\,\\mathrm{d}y \\\\ & & { } -4\\int\\biggl [ \\hat{\\alpha}_{x , c}(y)-\\frac{1}{\\theta}\\alpha _ { 0 } \\biggl\\ { \\frac{y}{\\theta } \\biggr\\ } \\biggr ] \\rho_{0 } \\biggl\\ { \\frac{y}{\\theta } \\biggr\\ } \\frac{1}{\\theta^{3}}\\frac{e_{xy}^{c}}{\\hat{\\alpha}_{x , c}(y ) } w(x , y ) \\,\\mathrm{d}y.\\end{aligned}\\ ] ]    we first establish the properties of @xmath376 recall from @xcite that @xmath377 where @xmath378 c_{i}(y^{\\prime } ) z_{i}(y^{\\prime})\\,\\mathrm{d}y^{\\prime } .\\end{aligned}\\ ] ] therefore , @xmath379 \\rho_{0 } \\biggl\\ { \\frac{y}{g(x)}\\biggr\\ } \\frac{1}{g(x)^{2}}\\frac{e_{x , y}^{c}}{\\hat{\\alpha}_{x , c}(y ) } w(x , y ) \\,\\mathrm{d}y \\\\ & = & 2\\int\\rho_{0 } \\biggl\\ { \\frac{y}{g(x ) } \\biggr\\ } \\frac{1}{g(x)^{2}}\\frac{{\\mathcaligr}{v}_{x , y}}{\\hat{\\alpha}_{x , c}(y ) } w(x , y ) \\,\\mathrm{d}y\\\\ & & { } + 2\\int\\rho _ { 0 } \\biggl\\ { \\frac{y}{g(x ) } \\biggr\\ } \\frac{1}{g(x)^{2}}\\frac{{\\mathcaligr}{b}_{x , y}}{\\hat{\\alpha}_{x , c}(y ) } w(x , y ) \\,\\mathrm{d}y \\\\ & \\simeq & 2\\int\\rho_{0 } \\biggl\\ { \\frac{y}{g(x ) } \\biggr\\ } \\frac{1}{g(x)^{2}}\\frac{{\\mathcaligr}{v}_{x , y}}{\\alpha_{x}(y ) } w(x , y ) \\,\\mathrm{d}y\\\\ & & { } + 2\\int\\rho_{0 } \\biggl\\ { \\frac{y}{g(x ) } \\biggr\\ } \\frac{1}{g(x)^{2}}\\frac{{\\mathcaligr}{b}_{x , y}}{\\alpha _ { x}(y ) } w(x , y ) \\,\\mathrm{d}y , \\end{aligned}\\ ] ] where the last line follows from @xcite , lemma 3 . consider@xmath380 \\,\\mathrm{d}m_{i}(y^{\\prime } ) \\\\ & & \\quad \\simeq\\sum_{i=1}^{n}\\int h_{ni}(x , u)\\,\\mathrm{d}m_{i}(u),\\end{aligned}\\ ] ] where @xmath381 by the central limit theorem for martingales , one gets ( see , e.g. , @xcite , proposition 1 ) @xmath382    furthermore,@xmath383 c_{i}(y^{\\prime } ) z_{i}(y^{\\prime})\\,\\mathrm{d}y^{\\prime}\\,\\mathrm{d}y\\end{aligned}\\ ] ] and it is easily seen that this can be written as a bias term of order @xmath384 , plus a remainder term of order @xmath385 .",
    "finally , note that for any sequence @xmath386 , we have @xmath387 where@xmath388 and @xmath389    from ( [ te ] ) , we then obtain @xmath390 and the asymptotic distribution follows .",
    "proof of theorem [ alpha0 ] we first consider the infeasible estimator @xmath391 . consider the following decomposition : @xmath392 \\bigl(1+\\mathrm{o}_p(1)\\bigr),\\end{aligned}\\ ] ] where @xmath393 $ ] , @xmath394 / \\alpha_0(y)$],@xmath395 @xmath396 $ ] and @xmath397 are the limits of the corresponding quantities with hats .",
    "straightforward calculations show that @xmath398 and @xmath399 next , we consider the term @xmath400 .",
    "decomposing @xmath401 in @xmath402 in a similar way as above , we obtain , after some calculations , that @xmath403 & & \\quad   = b^o(y ) + \\frac{1}{\\alpha_0(y ) } n^{-1 } \\sum_{i=1}^n k_h\\ { yg(x_i)-y_i\\ } c_i(y_i ) w(x_i , yg(x_i ) ) \\\\[-1pt ] & & \\qquad { } - \\frac{1}{\\alpha_0(y ) } n^{-1 } \\sum_{i=1}^n ( c_iz_i)\\{yg(x_i)\\ } w\\{x_i , yg(x_i)\\ } + \\mathrm{o}_p((nh)^{-1/2 } ) + \\mathrm{o}(h^2 ) + \\mathrm{o}(b^2).\\end{aligned}\\ ] ] putting the three terms together , we get that@xmath404 we now consider the feasible estimator @xmath405 .",
    "write @xmath406 where @xmath407 and @xmath408 $ ] . therefore",
    ", @xmath409 - [ \\hat{\\alpha}_{0,a}^{o}(y)-e\\hat{\\alpha}_{0,a}^{o}(y ) ] \\nonumber \\\\[-8pt ] \\\\[-8pt ] & & \\quad   = \\biggl [ \\frac{1}{b(y)}-\\frac{1}{b^{o}(y ) } \\biggr ] n(y)+\\frac{1}{b^{o}(y ) } [ n(y)-n^{o}(y ) ] , \\nonumber\\end{aligned}\\ ] ] where @xmath410 and @xmath411 are defined by replacing @xmath241 in the formulas of @xmath412 and @xmath413 by @xmath414 , and where @xmath415 is the expected value of @xmath249 with @xmath416 considered as fixed .",
    "write @xmath417 \\bigl ( u , yg(u)-hz \\bigr ) \\bigr).\\ ] ] hence , @xmath418 provided @xmath419 .",
    "next , note that @xmath420 , @xmath421 and hence ( [ alphan ] ) is @xmath422 .",
    "since it can be easily seen that @xmath423 , it follows that @xmath405 and @xmath424 are asymptotically equivalent .",
    "finally , we consider the calculation of the asymptotic variance of @xmath425 : @xmath426 \\\\ & = & \\frac{n^{-1}}{b^o(y)^2 } \\int\\!\\!\\int k_h^2\\{yg(x)-t\\ } e[c(t)|y = t , x = x ] \\,\\mathrm{d}f_x(t)\\\\ & & \\hphantom{\\frac{n^{-1}}{b^o(y)^2 } \\int\\!\\!\\int}{}\\times w^2(x , yg(x ) ) \\,\\mathrm{d}f(x)\\bigl ( 1+\\mathrm{o}(1)\\bigr ) \\\\ & = & \\frac{(nh)^{-1}}{b^o(y)^2 } \\int k^2(u ) \\,\\mathrm{d}u e \\ { e[c(yg(x))|y = yg(x),x]\\\\ & & \\hphantom{\\frac{(nh)^{-1}}{b^o(y)^2 } \\int}{}\\times f_x(yg(x ) ) w^2(x , yg(x ) ) \\ } \\bigl(1+\\mathrm{o}(1)\\bigr).\\end{aligned}\\ ] ]    proof of theorem [ g2step ] consistency of @xmath427 follows similarly as in the proof of theorem [ goracle ] from condition ( a1 ) , ( [ q1 ] ) and the fact that@xmath428 equation ( [ q1add ] ) follows from assumption ( c1 ) and the uniform consistency of @xmath429 see the proof of theorem goracle . for the proof of theorem [ g2step ] , it remains to show for @xmath430 that for some @xmath431 , @xmath432 uniformly for @xmath183 in a neighborhood of @xmath208 .",
    "claim ( [ claim2 ] ) follows immediately from assumption ( c1 ) . for the proof of ( [ claim1 ] ) , note first that @xmath433 ( \\widehat\\rho_{0}- \\rho _ { 0})\\biggl ( \\frac{y}{\\theta_0 } \\biggr ) \\frac{1}{\\theta_0^{2}}\\frac { e_{x , y}^{c}}{\\hat{\\alpha}_{x , c}(y ) } w(x , y ) \\,\\mathrm{d}y \\\\ & & \\qquad{}+ 2\\int\\biggl [ { \\frac{1 } { \\theta_0}}(\\hat\\alpha_0 -\\alpha_{0 } ) \\biggl({\\frac{y } { \\theta_0 } } \\biggr ) \\biggr ] ( \\widehat\\rho_{0}- \\rho_{0 } ) \\biggl ( \\frac{y}{\\theta_0 }",
    "\\biggr ) \\frac{1}{\\theta_0^{2}}\\frac { e_{x , y}^{c}}{\\hat{\\alpha}_{x , c}(y ) } w(x , y ) \\,\\mathrm{d}y \\\\ & & \\qquad{}+ 2\\int\\biggl [ { \\frac{1 } { \\theta_0}}(\\hat\\alpha_0 -\\alpha_{0 } ) \\biggl({\\frac{y } { \\theta_0 } } \\biggr ) \\biggr ] \\rho_{0 } \\biggl ( \\frac{y}{\\theta_0 } \\biggr ) \\frac{1}{\\theta_0^{2}}\\frac{e_{x , y}^{c}}{\\hat{\\alpha } _ { x , c}(y ) } w(x , y ) \\,\\mathrm{d}y , \\end{aligned}\\ ] ] where @xmath434 .",
    "it follows from ( c1 ) that the second term of the right - hand side is of order @xmath435 . from ( c1 ) and ( c2 ) , we get that up to a deterministic term of order @xmath436 , the third term is also of order @xmath435 .",
    "the first term is equal to @xmath437 , where @xmath438 ( \\widehat\\rho_{0}- \\rho _ { 0 } ) \\biggl ( \\frac{y}{\\theta_0 } \\biggr ) \\frac{1}{\\theta_0^{2}}\\frac { e_{x , y}^{c}}{{\\alpha}_{x}(y ) } w(x , y ) \\,\\mathrm{d}y.\\ ] ] for the proof of theorem [ g2step ] , it remains to show that @xmath439 by application of ( [ nl ] ) , we can write @xmath440 , where @xmath441    it can be easily checked that @xmath442 ( cf .",
    "the proof of theorem [ goracle ] ) .",
    "the term @xmath443 can be decomposed into @xmath444 , where @xmath445 with @xmath446 , \\\\",
    "g_{ni}(x , u ) & = & \\frac{2}{n}k_{b}(x - x_{i})\\int\\biggl [ \\int\\biggl\\ { \\frac{y}{\\theta_{0 } } \\biggr\\ } ( \\hat{\\alpha}_{0}^{\\prime}-\\alpha_{0}^{\\prime } ) \\biggl\\ { \\frac{y}{\\theta_{0 } } \\biggr\\ } \\frac{1}{\\theta_{0}^{2}}\\frac{1}{\\alpha_{x}(y)}k_{h}(y - u)w(x , y)\\,\\mathrm{d}y \\biggr ] .\\end{aligned}\\ ] ] we now show that @xmath447 . the claim @xmath448 can be shown by similar methods . for the proof , we apply @xcite , lemma 5.14 .",
    "this lemma gives a bound on the increments of the empirical process applied to function classes that depend on the sample size .",
    "we apply the lemma with a fixed value of @xmath27 , conditional on the event that the number of values of @xmath312 in the support of @xmath449 is equal to @xmath450 , where @xmath450 is of the same order as @xmath451 .",
    "we consider the class of functions @xmath452 such that , with a sufficiently large constant @xmath9 , for all @xmath284 , @xmath453 and @xmath454 .",
    "we apply the lemma with @xmath455 and @xmath456 .",
    "we get that @xmath457 \\,\\mathrm{d}m_{i}(u ) \\biggr\\vert\\ ] ] is of order @xmath458 .",
    "this shows that @xmath447 and thus concludes the proof of theorem [ g2step ] .",
    "research of o. linton was supported by the esrc .",
    "research of e. mammen was supported by the deutsche forschungsgemeinschaft , project ma1026/11 - 1 .",
    "research of i. van keilegom was supported by iap research network grant no .",
    "p6/03 of the belgian government ( belgian science policy ) and by the european research council under the european community s seventh framework programme ( fp7/2007 - 2013)/erc grant agreement no . 203650 . the authors would like to thank sorawoot srisuma for research assistance .",
    "heckman , j.j .",
    "the common structure of statistical models of truncation , sample selection , and limited dependent variables and a simple estimator for such models .",
    "econ . social measurement _ * 15 * 475492 ."
  ],
  "abstract_text": [
    "<S> we present a general principle for estimating a regression function nonparametrically , allowing for a wide variety of data filtering , for example , repeated left truncation and right censoring . both the mean and the median regression cases are considered . </S>",
    "<S> the method works by first estimating the conditional hazard function or conditional survivor function and then integrating . </S>",
    "<S> we also investigate improved methods that take account of model structure such as independent errors and show that such methods can improve performance when the model structure is true . </S>",
    "<S> we establish the pointwise asymptotic normality of our estimators .    ,    ,     + </S>"
  ]
}