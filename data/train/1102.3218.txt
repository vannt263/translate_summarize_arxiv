{
  "article_text": [
    "the problem of pricing american and bermudan style options is of fundamental importance in option pricing theory . in the continuous time settings",
    "mckean ( 1965 ) proposed an algorithm for pricing an american put option with an infinite maturity via ordinary differential equations and partial differential equations .",
    "further developments of this technique and applications to other american style securities are considered in peskir and shiryaev ( 2006 ) .",
    "another approach is pricing via monte - carlo simulations that is described by glasserman ( 2004 ) .",
    "one of the most difficult tasks in the theory of pricing of american and bermudan options is the determination of an optimal stopping rule and the valuing of the option under such a rule .",
    "longstaff and schwartz ( 2001 ) proposed an algorithm for pricing american and bermudan style options via monte carlo simulations , least squares monte carlo or lsm .",
    "this technique is especially useful when we deal with multi - factor processes . in this case",
    "the methods based on binomial , trinomial trees , or partial differential equations become slow and thus inefficient due to the high dimensionality of the problem .    as in the majority of the numerical algorithms the starting point of lsm for american options is a substitution of the continuous time interval with a discrete set of exercise dates .",
    "practically , by doing this we substitute the american option with a bermudan one .",
    "then for each exercise time ( except the first and the last one ) we project the value of continuation onto a set of basis functions via linear regression .",
    "clement , lamberton and protter ( 2002 ) investigated the convergence of the algorithm with the growth of the number of the basis functions and the monte carlo simulations .",
    "under fairly general conditions they proved the almost sure convergence of the complete algorithm . also , they obtained the rate of convergence when the number of monte carlo simulations increases and showed that the normalized error of the algorithm is asymptotically gaussian .",
    "however , they considered a fixed partition of the time interval and thus , essentially , they discussed the properties of the bermudan , not american option .",
    "glasserman and yu ( 2004 ) investigated the behavior of lsm with the simultaneous grows of the number of the basis functions and the number of the monte - carlo simulations and estimated the rate of convergence in some more specific settings .",
    "moreno and navas ( 2001 ) considered the lsm for different basis functions , namely , power series , laguerre , legendre , chebyshev polynomials , and deduced that the algorithm converges at least for american put options when the underlying problem has a small number of factors .",
    "stentoft ( 2004 ) obtained the rate of convergence of the algorithm in the two period multidimensional case .    in the present work we consider the stability of lsm algorithm , when the number of exercise dates increases in such a way that there are exercise dates close to an initial time , which we assume to be equal to zero without loss of generality .",
    "we prove that the algorithm is unstable when the time parameter is close to zero , because the underlying regression problem is ill - conditioned .",
    "the remainder of this work is organized as follows . in section [ description ] , we describe the algorithm . in section [ ill - conditioning ] , we prove the main result , which is formulated in proposition [ mainprop ] , instability of the algorithm for the small values of the time parameter due to the ill - conditioning of the corresponding matrix in the regression problem .",
    "in addition we present the results of the numerical simulations that illustrate the assertions of proposition [ mainprop ] .",
    "in section [ conclusion ] , we give the concluding remarks .",
    "assume that the stock price process @xmath0}$ ] is given by the strong soluton to the following stochastic differential equation : @xmath1 here @xmath2 is a constant , @xmath3}$ ] is a brownian motion ( possibly multidimensional ) on a filtered probability space @xmath4 } ,    \\mathbb{p}\\right),$ ] where @xmath5 is the risk - neutral probability measure , @xmath6 and @xmath7 are the progressively measurable functionals , such that the strong solution to equation ( [ sde ] ) exists and unique on the time interval @xmath8 $ ] , see karatzas and shreve @xcite , chapter 5 , for the discussion of this topic .",
    "the time horizon is @xmath9 which we assume to be a finite constant .",
    "usually equations of such a form are used to describe the evolution of the stock prices in practice .",
    "let the payoff of an american option at the time of exercise @xmath10 is given by @xmath11 where @xmath12 is the corresponding payoff function .",
    "then the value of the option is determined by the formula : @xmath13.\\ ] ] here @xmath14}$ ] is an interest rate process , which we assume to be deterministic for simplicity ; @xmath15 is the set of the stopping times with respect to the filtration @xmath16}.$ ]    to approximate @xmath17 numerically let us conduct @xmath18 monte - carlo simulations of the process @xmath19 .",
    "first , we need to divide the time interval @xmath20 $ ] into @xmath21 subintervals @xmath22 $ ] of the length @xmath23 , where @xmath24 , @xmath25 .",
    "thus at every moment @xmath26 we obtain @xmath18 realizations of the process @xmath27 , @xmath28 .",
    "second , for each simulation we compute the value of the option at time @xmath29 ( under the assumption that the option was not exercised before @xmath30 ) : @xmath31 discounting these values we get a cash flow vector @xmath32 where @xmath33 is the discount factor .    to obtain the value of the option at @xmath34 @xmath35 ( under the assumption the option",
    "was not exercised before @xmath36 ) , we chose a hypothesis of linear regression and project the cash flow vector @xmath37 for example , on a constant , @xmath38 and @xmath39 . according to @xcite",
    "this is one of the simplest yet successful regression models . according to @xcite a good alternative choice of basis functions can be hermite , laguerre , legendre , or chebyshev polynomials .",
    "if we use @xmath40 and @xmath41 as the basis , the estimate of the conditional expectation becomes @xmath42= \\alpha + \\beta x_{t_{m-1 } }   + \\gamma x^2_{t_{m-1}},\\ ] ] where @xmath43 , @xmath44 , @xmath45 are some constants .",
    "then along every path we compare values of immediate exercise , @xmath46 , with values of continuation that are obtained by substitution of @xmath47 into equation ( [ condexpreg ] ) .",
    "the bigger of two gives @xmath48 , @xmath49 . if value of immediate exercise is bigger we set @xmath50 .",
    "similarly we obtain @xmath51 $ ] for each @xmath52 via solving linear regression problems @xmath53 where components of the matrix @xmath54 depend on the regression hypothesis and the outcomes of the simulations , @xmath55 is an unknown vector of coefficients , and the vector @xmath56 is given by equation @xmath57 where @xmath58 is the discount factor .",
    "finally we discount the cash flow up to the moment of time @xmath59 and compare it with the value of immediate exercise at time @xmath60 , @xmath61 .",
    "the bigger is the value of the option .",
    "let @xmath63 be a partition of the interval @xmath64.$ ] in order to compute the estimates of the value of the option at each @xmath65 ( under the assumption that it was not exercised before ) we solve the linear regression problem @xmath66 where @xmath67 is an unknown vector of coefficients , vector @xmath68 is determined by equation ( [ cashflowvector2 ] ) , and the matrix @xmath69 depends on the regression hypothesis and the outcome of the monte carlo simulations .",
    "assume that we have chosen @xmath70 continuous functions @xmath71 as the hypothesis . examples of such functions are power series , laguerre , legendre , hermite polynomials , etc .",
    "in this case @xmath69 has the following form @xmath72 where @xmath18 is the number of the simulations .",
    "we show that , if the underlying process @xmath19 is almost surely continuous , then for small @xmath62 the problem ( [ regression ] ) is ill - conditioned . for a matrix @xmath73 in the @xmath74 norm",
    "the * condition number * is defined as @xmath75 where @xmath76 and @xmath77 are maximal and minimal singular values of the matrix @xmath73 respectively .",
    "a problem with a low condition number is called * well - conditioned * , while a problem with a high condition number is called * ill - conditioned*.    usually , problem ( [ regression ] ) is solved via one of the following methods : householder triangularization , gram - schmidt orthogonalization , singular value decomposition , or normal equations .",
    "let @xmath78 be the condition number of matrix @xmath73 .",
    "if @xmath79 exists then the exact solution to the least - squares problem is given by the vector @xmath80 i.e. it is a product of the left - inverse of the matrix @xmath73 and the vector @xmath81 one can see that the solution to the problem ( [ regression ] ) , obtained via normal equations , is governed by @xmath82 , whereas the solution obtained via svd , householder or gram - schmidt is governed by @xmath78 .",
    "consequently , normal equations are the least stable with respect to the grows of the condition number .",
    "nevertheless , the analytical solution to the least - squares problem is defined in terms of the normal equations .",
    "let @xmath83 denote the condition number of the matrix @xmath69 for @xmath84.$ ] we show below that @xmath85 @xmath5-almost surely .",
    "therefore , no matter what algorithm one uses ( householder , gram - schmidt , svd or normal equations ) , for small values of time the underlying regression problem is ill - conditioned , and thus the algorithm is unstable .",
    "we prove ill - conditioning for an arbitrary number of basis functions in the following proposition .",
    "in addition , we illustrate the phenomenon with the results of the numerical simulations in the case when regression is done on three basis functions @xmath86 , @xmath87 , and @xmath88     as a function of @xmath89 @xmath90,$ ] @xmath19 - lognormal process , @xmath91 @xmath92 @xmath93 paths , @xmath94 time steps , milstein discretization scheme.,scaledwidth=90.0% ]     as a function of @xmath89 @xmath95,$ ] @xmath19 - normal process ,",
    "@xmath91 @xmath96 @xmath93 paths , @xmath94 time steps , euler discretization scheme.,scaledwidth=90.0% ]    [ mainprop ] assume that the process @xmath19 is given by equation ( [ sde ] ) , @xmath97 , @xmath98 , are continuous functions , such that @xmath99 let for each @xmath84 $ ] the matrix @xmath69 is defined by equation ( [ matrixa ] ) and @xmath83 is the condition number of @xmath100 then @xmath101 = 1.\\ ] ]    * proof.*consider equation ( [ matrixa ] ) .",
    "it follows from equation ( [ sde ] ) that all rows of @xmath102 are identical .",
    "consequently the rank of @xmath102 equals to @xmath86 .",
    "let us look at the following matrix : @xmath103 the matrix @xmath104 has two eigenvalues : @xmath105 is the first one ( with multiplicity @xmath86 ) , @xmath106 is the second one ( with multiplicity @xmath107 ) .",
    "thus @xmath102 has singular values @xmath108 and @xmath109 consequently @xmath110 .    note that the matrix @xmath111 is real and symmetric , thus the eigenvalues of @xmath111 are real for all @xmath112s . since eigenvalues are continuous functions of the components of the matrix and the components in turn are a.s .",
    "continuous processes , we deduce that for small @xmath112s the first eigenvalue is in the neighborhood of @xmath113 which is greater then zero by the assumption of proposition , whereas all other eigenvalues are is in the neighborhood of @xmath106 .",
    "the conclusion , @xmath114 = 1 $ ] follows from continuity of the underlying process @xmath19 and the basis functions @xmath115 s , @xmath116    ( 6,6)[br ] +    intuitively proposition [ mainprop ] shows that for small values of @xmath62 the condition number is large .",
    "we proved that for a continuous underlying stock price process , lsm algorithm for pricing american options is unstable when time parameter is small .",
    "an interesting question is to obtain an exact bound on applicability of this algorithm .",
    "a possible criterion of applicability is the condition number of matrix @xmath69 , @xmath117 .",
    "for example , if @xmath118 exceeds a certain value , one can treat ( [ regression ] ) as a rank deficient least squares problem ( see @xcite for details ) , or switch to another method : backward induction or the method introduced by mckean @xcite of option pricing via ordinary differential equations or partial differential equations considered on a smaller domain . for certain problems",
    "it is possible to obtain desired accuracy using relatively small number of time intervals , then one does not have to solve the regression problem for small @xmath112s , and consequently the algorithm can be stable . also , if the underlying process @xmath19 is discontinuous with high probability the algorithm can be stable even for small values of the time parameter ."
  ],
  "abstract_text": [
    "<S> consider least squares monte carlo ( lsm ) algorithm , which is proposed by longstaff and schwartz ( 2001 ) for pricing american style securities . </S>",
    "<S> this algorithm is based on the projection of the value of continuation onto a certain set of basis functions via the least squares problem . </S>",
    "<S> we analyze the stability of the algorithm when the number of exercise dates increases and prove that , if the underlying process for the stock price is continuous , then the regression problem is ill - conditioned for small values of the time parameter .    </S>",
    "<S> _ * keywords : * _ option pricing , optimal stopping , american option , least squares monte carlo , monte carlo methods , stability , ill - conditioning . </S>"
  ]
}