{
  "article_text": [
    "consider the problem of _ online learning and optimization _ of affine memoryless models with unknown parameters that follow a markov jump process . by online learning and optimization we mean",
    "that the control input of the unknown model is chosen sequentially to minimize the expected total cost or to maximize the expected cumulative reward procured over a time horizon @xmath0 . in this context , the online learning problem is one of exploration and exploitation ; the need of exploring the space of unknown parameters must be balanced by the need of exploiting the knowledge acquired through learning .    for online learning problems with deterministic unknown parameters ,",
    "a commonly used performance measure is the so - called _ regret _ , defined by the difference between the cumulative cost / reward of an online learning policy and that of a decision maker who knows the model completely and sets the input optimally .",
    "the regret grows monotonically with the time horizon @xmath0 , and the rate of growth measures the efficiency of online learning policies .",
    "the online learning problem considered in this paper is particularly relevant in dynamic pricing problems when the consumers demand is unknown and possibly varying stochastically .",
    "the goal of dynamic pricing is to set the price sequentially , using the observations from the previous sales , to match a certain contracted demand . besides applications in dynamic pricing ,",
    "results are also relevant to the learning and control problem of markov jump linear systems with unknown parameters .    in this paper , we study the online learning and optimization problem of markov jump affine models under two different objectives : ( i ) target matching with a quadratic cost and ( ii ) revenue ( profit ) maximization .",
    "our goal is to establish fundamental limits on the rate of regret growth for markov jump affine models and develop an online learning policy that achieves the lowest possible regret growth .      without markov jump as part of the model , when there is a single state , the problem considered here is the classical problem of control in experiment design studied by anderson and taylor .",
    "anderson and taylor proposed a certainty equivalence rule where the input is determined by using the maximum likelihood estimates of system parameters as if they were the true parameters . despite its intuitive appeal ,",
    "the anderson - taylor rule was shown to be suboptimal for the quadratic regulation problem by lai and robbins in and also for the revenue maximization problem by den boer and zwart in .",
    "in fact , there is a non - zero probability that the anderson - taylor rule produces an input which converges to a suboptimal value for both cases ; therefore , this rule results in _ incomplete learning _ and a linear growth of regret .    for the scalar model in which the quadratic cost of the regulation problem is to be minimized , lai and robbins showed that a robbins - monro stochastic approximation approach achieves the optimal regret order of @xmath1 .",
    "later , lai and wei showed that this regret order is also achievable for a more general linear dynamic system by an adaptive regulator that uses least square estimates of a reparametrized model and ensures convergence via occasional uses of white - noise probing inputs .",
    "the result was further generalized by lai @xcite to multivariate linear dynamic systems with a square invertible system matrix .",
    "the result presented in this paper can be viewed as a generalization of this line of work to allow both time - varying linear models and time - invariant models with a non - invertible system matrix .",
    "the problem considered in this paper also falls into the category of continuum - armed bandit problem where the control input is chosen from a subset of @xmath2 with the goal of minimizing expected cost ( or maximizing expected reward ) that is an unknown continuous function of the input .",
    "this problem was introduced by agrawal @xcite who studied the scalar problem and proposed a policy that combines certainty equivalence control with kernel estimator - based learning .",
    "agrawal showed that this policy has a regret growth rate of @xmath3 for a uniformly lipschitz expected cost function .",
    "later , kleinberg @xcite proved that the optimal growth rate of regret for this problem can not be smaller than @xmath4 and proposed a policy that achieves @xmath5 .",
    "kleinberg @xcite also considered the multivariate problem , @xmath6 , and showed that an adaptation of zinkevich s greedy projection algorithm achieves the regret growth rate of @xmath3 if the cost function is smooth and convex on a closed bounded convex input set .    within the continuum - armed bandit formulation ,",
    "the work of cope @xcite is particularly relevant because of its use of stochastic approximation to achieve the order - optimal regret growth of @xmath7 for a different class of cost functions .",
    "cope s results ( both the regret lower bound and the kiefer - wolfowitz technique ) , unfortunately , can not be applied here because of the time - varying markov jump affine models treated here . also relevant",
    "is the work of rusmevichientong and tsitsiklis on the so - called linearly parameterized bandit problem where the objective is to minimize a linear cost with input selected from the unit sphere .",
    "a learning policy developed in is shown to achieve the lower bound of @xmath7 using decoupled exploration and exploitation phases . even though the model considered in this paper is similar to the one in in terms of the observed output being a linear function of the input , in our paper ,",
    "the unknown model parameters follow a markov jump process and the specific cost functions studied are quadratic ; thus the problem objective is different .",
    "there is a considerable amount of work on dynamic pricing problem with the objective of revenue maximization under a demand model uncertainty in different areas such as operations research , statistics , mathematics , and computer science . in ,",
    "a multi - armed bandit approach with a regret growth rate of @xmath8 was proposed for a nonparametric formulation of the problem .",
    "see also where the same problem under a general parametric demand model is considered and a modified version of myopic maximum likelihood based policy is shown to achieve the regret order of @xmath9 , and where a similar result is obtained for a class of parametric demand models . in both and , authors proved that the lower bound for regret growth rate is @xmath7 .    besides more general classes of demand models , affine model similar to the one in this paper has been also studied extensively ; . in both and",
    ", it is shown that approximate dynamic programming solutions may outperform greedy method numerically .",
    "a special case of our formulation of revenue maximization problem without any markov jump characteristics ( with time - invariant model parameters ) is previously investigated by keskin and zeevi .",
    "keskin and zeevi proposed a semi - myopic policy that uses orthogonal pricing idea to explore and learn the system .",
    "they showed that the lowest possible regret order is @xmath7 for any policy , and their semi - myopic policy achieves this bound up to a logarithmic factor ; @xmath10 .",
    "even though the system model is assumed to be time - invariant in most of the literature , there is a considerable amount of work especially in dynamic pricing that deals with time - varying demand models due to unpredictable environmental factors affecting demand ; see for a demand model that evolves according to a discrete state space markov chain in a revenue management with finite inventory problem , and for a dynamic programming formulation of a profit maximization problem with an unknown demand parameter following an autoregressive process .",
    "see also for a revenue maximization problem with an affine demand model where the model parameters are time - varying , yet the cumulative change in the model parameters over the time horizon @xmath0 is bounded . since keskin and zeevi measure the regret of a policy by the difference between the cumulative cost of the policy and that of a clairvoyant who knows all the future temporal changes exactly and chooses the optimal action , their characterization of regret is too pessimistic for the markov jump model considered here",
    ".    some other examples of related work on online learning with time - varying models apart from dynamic pricing are and . in , besbes , gur , and zeevi studied the online learning problem of more general time - varying cost functions where",
    "the cumulative temporal changes is restricted to a budget similar to . however , their characterization of regret is also similar to and thus , incomparable with the one in this paper .",
    "yin , ion , and krishnamurthy also considered the problem of estimating a randomly evolving optimum of a cost function which follows a markov jump process .",
    "their analysis deals with the convergence of the estimate obtained via stochastic approximation to the limit ( stationary ) solution , whereas in this paper , we are concerned about estimating the optimum of the cost function at each time instant given the previous state of the markov chain .",
    "moreover , different than our paper , their analysis relies on the availability of the noisy observations of the cost function gradient and they do not characterize regret .      the main contribution of this paper is the generalization of online learning of time - invariant affine models to that of markov jump affine models . extending spall s stochastic approximation method @xcite to the optimization problem of an objective function that evolves according to a markov jump process , we propose an online learning policy , referred to as markovian simultaneous perturbations stochastic approximation ( mspsa ) .",
    "we show that mspsa achieves the optimal regret order of @xmath11 for two different objective functions studied for the affine model : ( i ) quadratic regulation and ( ii ) the revenue maximization .",
    "furthermore , we also show that the control input of mspsa policy converges to the optimal solution both with probability one and in mean square as @xmath12 .",
    "therefore , the proposed policy eventually learns the optimal solution .",
    "a key implication of our results is that , in comparing with lai s result on the learning problem of a time - invariant affine model with the quadratic regulation objective @xcite , modulating a linear model by a markov jump process introduces substantial learning complexity ; hence , the regret order increases from @xmath1 to @xmath11 . as a special case",
    ", we also show that , even in the absence of markovian jump , when the system matrix is full column rank but not invertible , the best regret order is also @xmath11 .",
    "it worths noting that adding just one row to a square and invertible matrix can change the worst case regret from @xmath1 to @xmath11 .    in the second part of this paper , we study the profit maximization problem with markov jump demand . to this end",
    ", we generalize the lower bound obtained by keskin and zeevi for time - invariant demand to markov jump demand , and we show that this bound is achievable by the mspsa policy for a more general case with markov jump characteristics .    the results presented here are obtained using several techniques developed in different contexts . the mspsa policy is a generalization of spall s stochastic approximation method to the optimization problem of an objective function following a markov jump process . to show the optimality of mspsa , we use the van trees inequality to lower bound the estimation error , which is the technique used in .",
    "lastly , a result on the convergence of non - negative almost supermartingales @xcite is used to obtain the convergence result for mspsa policy .",
    "the model considered here is an affine model , modulated by an exogenous finite state time - homogeneous markov chain @xmath13 where @xmath14 is the state space and @xmath15 $ ] the transition probability matrix .",
    "we assume that the state space @xmath16 and the transition matrix @xmath17 are unknown .",
    "each state @xmath18 of the markov chain is associated with an affine model whose parameters are denoted by @xmath19 where @xmath20 has full column rank and @xmath21 .",
    "all system parameters @xmath22 are assumed deterministic and unknown . at time @xmath23 , the input - output relation of the system is given by    rcl [ eq : linear ] y_t & = & a_s_tx_t + b_s_t + w_t ,    where @xmath24 is the control input , @xmath25 the observable output , @xmath26 the state of the system , and @xmath27 is a random vector that captures the system noise .",
    "it is assumed that the random noise @xmath28 is drawn from a possibly state dependent distribution @xmath29 with zero mean ( without loss of generality ) and unknown finite variance @xmath30 .",
    "furthermore , for any @xmath31 , @xmath28 and @xmath32 are conditionally independent given the states @xmath33 and @xmath34 .",
    "the objective of _ online learning and optimization _ is to find a control input sequence @xmath35 that minimizes the expected cumulative cost incurred at each stage .",
    "because @xmath36 is determined before @xmath33 is realized , the stage cost @xmath37 at @xmath23 is a function of @xmath36 and @xmath38 , and the expected cumulative cost is @xmath39 where @xmath0 is the learning and optimization horizon . note that the above quantity is a function of the deterministic parameters @xmath40 and the distributions @xmath17 and @xmath41 .    for a decision maker who wants to minimize its expected cumulative cost ,",
    "the difficulty in finding the optimal control input sequence is that the system parameter @xmath40 and the transition matrix @xmath17 are unknown . if the system parameters @xmath42 were known , then the decision maker would have used this information along with its observation history to determine the optimal decision rule that minimizes the expected cumulative cost . in that case",
    ", the problem could be formulated as a dynamic program and solved via backward induction .",
    "we refer to this optimal solution under known model as the optimal input and denote it by @xmath43 ( which is made precise in the following sections ) .",
    "we assume that a convex compact set @xmath44 containing the optimal input is known to the decision maker .    before choosing the control input of period @xmath23 ,",
    "the only information the decision maker has is a vector @xmath45 containing its decision and observation history up to time @xmath46 , which includes input vector @xmath47 , output vector @xmath48 , state vector @xmath49 , and the set @xmath50 .",
    "consequently , a policy @xmath51 of a decision maker is defined as a sequence of decision rules , @xmath52 , such that , at time @xmath46 , @xmath53 maps the information history vector @xmath45 to the system input @xmath54 at time @xmath23 .",
    "we denote the input determined by policy @xmath51 as @xmath55 .    to measure the performance of an online learning policy , we use the regret measure as a proxy .",
    "in particular , the ( cumulative ) regret @xmath56 of a learning policy @xmath51 is measured by the difference between the expected cumulative cost of the decision maker , who follows the policy @xmath51 , and that of a decision maker who knows the system parameters @xmath42 and sets the system input optimally ,    rcl [ eq : defregret ] _",
    "t^(,p ) & = & ( _ t=1^t ( s_t-1,x_t^ ) - _ t=1^t ( s_t-1,x_t^ * ) ) .",
    "since the regret defined above is a function of system parameters , we characterize the performance of @xmath51 by the worst case regret @xmath57 in terms of the worst - case analysis , it is assumed that the worst - case system parameter @xmath40 is chosen from a compact set @xmath58 for any fixed values of the state space size @xmath59 and system dimensions @xmath60 and @xmath61 . since @xmath62 is compact , for any @xmath63 and for all @xmath18 , the largest singular value of @xmath64 is bounded by a positive constant @xmath65 , and the parameter @xmath66 is bounded by a positive constant @xmath67 , @xmath68 .",
    "it is also assumed that the variance of @xmath28 is bounded , @xmath69 for some positive constant @xmath70 where @xmath71 denotes the @xmath72th entry of @xmath28 . the optimal input for the worst - case parameters @xmath42 certainly has to be contained in the set @xmath50 , and @xmath64 has to be full column rank for every @xmath18 for the worst - case parameter @xmath40 .",
    "note that @xmath73 grows monotonically with @xmath0 .",
    "we are interested in the learning rule that has the slowest regret growth .    in the following sections , we focus on two different stage costs , hence two different objective functions .",
    "the first is a quadratic cost that arises naturally from the regulation problem . in particular , the stage cost at time @xmath23 is given by    rcl [ eq : cost ] ^(s_t-1,x_t ) & & ( ||y^ * - y_t ||_2 ^ 2 | s_t-1 , x_t ) ,    where @xmath74 is a constant target value for output .",
    "for the quadratic regulation problem , we assume that the forth order moment of @xmath28 is bounded , @xmath75 , in addition to the previous boundedness assumptions .",
    "the second stage cost we consider is the minus revenue that arises from revenue maximization problem ( or profit maximization problem since profit can be expressed as revenue minus total cost ) .",
    "specifically , the stage cost of period @xmath23 is given by    rcl [ eq : cost_revenue ] ^(s_t-1,x_t ) & & ( - x_t y_t | s_t-1 , x_t ) .    here",
    ", the revenue is calculated as the inner product of the input and the output vector where the entries of the input and the output vector corresponds to the price and the demand of each product , respectively .",
    "therefore , the input and the output dimensions match , @xmath76 @xmath77 , for the revenue maximization problem . for this objective , it is assumed that the matrix @xmath78 is negative definite for all @xmath79 which is a reasonable assumption in dynamic pricing problems , see .",
    "in this section , we study the online learning problem with the quadratic cost .",
    "we first derive an expression for regret using the optimal solution under known model referred to as the optimal input .",
    "then , we introduce an online learning approach and establish its order optimality via the analysis of its regret growth rate and the analysis of the minimum regret growth rate achievable by any policy .",
    "we also show that the input of the online learning policy converges to the optimal input both almost surely and in mean square .      in order to calculate the regret for any policy , we begin by deriving the optimal solution of a decision maker who knows the system @xmath42 in addition to @xmath45 and aims to minimize the expected cumulative cost ,    rcl [ eq : cum_risk ] _",
    "\\{x_t}_t=1^t ( _ t=1^t ^(s_t-1,x_t ) ) .",
    "the problem under known model becomes a dynamic program due to the known @xmath42 .",
    "since the markov process is exogenous , independent of the decision policy , the optimization problem decouples to choosing the system input @xmath36 separately for each decision stage with stage cost given in ( [ eq : cost ] ) which is equivalent to    rcl [ eq : risk ] ^(s_t-1,x_t ) & = & _ j p_s_t-1,j ( y^*-a_jx_t - b_j_2 ^ 2 + ( _ w^(j ) ) ) +    by ( [ eq : linear ] ) .",
    "the optimal input @xmath80 minimizing the stage cost is then given by    rcl [ eq : optprice ] x_s_t-1^ * & = & ( _ j p_s_t-1,ja_j^ a_j)^-1(_j p_s_t-1,ja_j^(y^*-b_j ) ) .",
    "thus , the optimal input @xmath81 at any time @xmath23 depends only on the system parameter @xmath40 , the transition matrix @xmath17 , and the previous state @xmath38 . in the sequel ,",
    "we use @xmath82 to represent @xmath80 , dropping the explicit parameter dependency on @xmath42 in the notation .",
    "hence , the stage regret at @xmath23 , which is the expected difference of the stage cost obtained by policy @xmath51 and the stage cost of the optimal input @xmath83 , can be written as    rcl r_t^(,p ) & = & ( ^(s_t-1,x_t^ ) - ^(s_t-1,x_s_t-1^ * ) ) + & = & ( a_s_t(x_t^-x_s_t-1^*)_2 ^ 2 ) ,    which is obtained using the first order optimality condition ( foc ) for @xmath82 .",
    "the t - period regret given in ( [ eq : defregret ] ) can then be expressed as    rcl [ eq : regret ] _",
    "t^(,p ) & = & ( _ t=1^t a_s_t(x_t^-x_s_t-1^*)_2 ^ 2 ) .      here , we present an online learning policy to the quadratic regulation problem that achieves the slowest regret growth rate possible . referred to as mspsa , the policy is an extension of the simultaneous perturbation stochastic approximation ( spsa ) algorithm proposed by spall @xcite to markov jump models considered here .",
    "spall s spsa is a stochastic approximation algorithm that updates the estimate of the optimal input by a stochastic approximation of the objective gradient .",
    "the key step is to generate two consecutive observations corresponding to two inputs , that are set to be the current optimal - input estimate perturbed by some random vector in opposite directions , and use them to construct the gradient estimate . in applying this idea to the optimization problem of a markov jump system",
    ", a complication arises due to the uncertainty associated with the system state at the time when the system input is determined ; consecutive observations that are used to determine the gradient estimate may correspond to different system states .",
    "let @xmath84 be an arbitrary vector @xmath85 @xmath86 @xmath87 @xmath88    where @xmath89 with some positive constant @xmath90 and a non - negative integer @xmath91 , and @xmath92^{\\tra}$ ] with @xmath93 s drawn from an independent and identical distribution that is symmetrical around zero , and satisfies @xmath94 and @xmath95 for some positive constants @xmath96 and @xmath97 .",
    "@xmath98 @xmath99 @xmath100 @xmath101 @xmath86 update :    rcl [ eq : mspsaupdate_regulation ] _ i , t_i+1 & & ( _ i , t_i - a_t_i()|_t_i ) _    where @xmath102 denotes the euclidean projection operator onto @xmath50 , @xmath103^{\\tra}$ ] , and @xmath104 with some positive constant @xmath105 and a non - negative integer @xmath106    the key idea of mspsa is to keep track of each state @xmath107 and the estimate of the optimal input associated with each state @xmath107 .",
    "when state @xmath72 is realized , the estimate of the optimal input associated with state @xmath72 is perturbed by some random vector and this randomly perturbed estimate is used as input for the next stage .",
    "the estimate of the optimal input associated with state @xmath72 is updated only when we obtain two observations of the system output corresponding to two inputs that are generated by perturbing the current estimate in opposite directions by the same amount right after observing state @xmath72 .    details of this implementation",
    "is given in  [ mspsa_for_qr ] .",
    "whenever a new state @xmath107 is observed that has not been observed before , mspsa policy assigns an arbitrary predetermined vector @xmath84 as the initial estimate of the optimal input @xmath108 ( line 3 - 7 of  [ mspsa_for_qr ] ) . at the beginning of each stage @xmath23 ,",
    "mspsa checks the previous state @xmath38 ( line 2 ) , and whether any observation is taken using the most recent optimal - input estimate @xmath109 ( line 8) where @xmath110 is the number of times the optimal - input estimate @xmath109 is updated up to time t ( since two observations , that are taken right after observing state @xmath38 , are used for each update of @xmath109 , @xmath110 is approximately half of the number of times state @xmath38 is observed up to @xmath23 . ) .",
    "if an observation has not taken using the most recent estimate yet , the input for that stage is set to be a randomly perturbed @xmath109 ( line 10 ) .",
    "otherwise mspsa sets the input by perturbing the estimate @xmath109 in the opposite direction by the same amount as the previous one ( line 14 ) .",
    "then , it updates the optimal - input estimate by a stochastic approximation ( line 17 ) obtained using the stage costs calculated from both observations ( line 11 and 15 ) and projects it onto @xmath50 .",
    "the constant @xmath90 of the perturbation gain sequence @xmath111 should be chosen larger in the high noise setting for an accurate gradient estimate .",
    "the choice of the sequence @xmath112 used for the update step determines the step size .",
    "the non - negative integers @xmath106 of @xmath112 and @xmath91 of @xmath111 can be set to zero as default , but if the update of the optimal - input estimate fluctuates between the borders of @xmath50 at the beginning of the mspsa policy , setting @xmath106 greater than zero can prevent this fluctuation .",
    "we now analyze the regret performance of mspsa .",
    "let @xmath113 denote the minimum eigenvalue operator , and @xmath114 be the mean squared error ( mse ) between the optimal input @xmath115 and its estimate @xmath116 given state @xmath72 and @xmath117 , where @xmath117 , as defined in previous section , is the number of times the estimate @xmath116 has been updated up to time @xmath23 by mspsa .",
    "the following lemma provides a bound for the decreasing rate of @xmath118 , and hence the convergence rate of the estimate to its true value in terms of the number of times the estimate is updated and thus in terms of the number of times the state @xmath72 has occurred up to @xmath23 ( which is equal either to @xmath119 or to @xmath120 ) .",
    "it shows that the mse converges to zero with a rate equal or faster than the inverse of the square root of the number of times state @xmath72 has occurred .",
    "[ lemma : regret ] for any @xmath107 , if @xmath121 then there exists a constant @xmath122 satisfying @xmath123 for any @xmath124 .",
    "see appendix .    to satisfy the condition of lemma [ lemma :",
    "regret ] , the decision maker , who follows mspsa , needs to have some information about a lower bound on the minimum eigenvalue of @xmath125 , knowing a non - trivial lower bound @xmath126 for the singular values of the system matrices .",
    "this assumption may not be restrictive in practice since the decision maker can set @xmath105 sufficiently large .",
    "let the worst - case cumulative input - mse @xmath127 be the worst - case cumulative mse between the input @xmath55 of policy @xmath51 and the optimal input @xmath83 , @xmath128 using the result of lemma @xmath129 , we provide a bound for the growth rate of the worst - case cumulative input - mse and the worst - case regret of mspsa .",
    "theorem  [ thm : upper ] shows that the mspsa policy achieves the regret growth rate of @xmath9 .",
    "[ thm : upper ] if @xmath121 for every @xmath107 , then there exist some positive constants @xmath130 and @xmath131 such that @xmath132 and @xmath133    the input of mspsa @xmath134 is equal to either @xmath135 or @xmath136 given @xmath137 and @xmath117 .",
    "by lemma  [ lemma : regret ] , observe that    rcl [ eq : r_i_t_i ] r_i , t_i & = & ( _ i , t_i c_t_i _ t_i -x_i^*_2 ^ 2|i , t_i ) + & = & e_i , t_i + c_t_i^2 ( _ t_i^_t_i ) c_i/    where @xmath138 .",
    "let @xmath139 be the number of times the estimate of the optimal input associated with state @xmath72 has been updated until period @xmath0 . because mspsa uses two observations per update , we can express the worst - case cumulative input - mse for mspsa as @xmath140 by ( [ eq : r_i_t_i ] ) , we bound @xmath141 where @xmath142 . since @xmath139 is smaller than the number of times state @xmath72",
    "is observed , which is a fraction of @xmath0 for any @xmath17 , @xmath143 where @xmath144 .",
    "consequently , by ( [ eq : regret ] ) and using the upper bound @xmath65 for the singular values of @xmath145 , @xmath146 where @xmath147 .    according to theorem  [ thm :",
    "upper ] , the average regret converges to zero with a rate equal or faster than @xmath148 .",
    "hence , it proves that the average performance of mspsa policy approaches to that of the optimal solution under known model as @xmath12 .",
    "however , this does not imply the convergence of the mspsa policy to the optimal input .",
    "the following theorem provides both almost sure and mean square convergence of mspsa policy to the optimal input .",
    "[ thm : convergence ] for the quadratic regulation problem ,    rcl [ eq : asconv ] ( _ t x_t^-x^*_s_t-1 ^ 2_2 = 0 ) & = & 1 ,    and    rcl [ eq : inputconv ] _ t ( x_t^-x^*_s_t-1_2 ^ 2 ) & = & 0 .",
    "see appendix .",
    "theorem  [ thm : convergence ] shows that , the input generated by mspsa converges to its optimal value as @xmath12 for any choice of @xmath149 .",
    "hence , the condition given in theorem  [ thm : upper ] is not necessary for the convergence of mspsa policy .",
    "the intuition is as follows ; for any observed recurrent state @xmath107 , the input of mspsa converges to its optimal value at time periods when the previous state is @xmath72 as @xmath12 , and any observed transient state @xmath107 will occcur only a finite number of times .",
    "therefore , the input of mspsa converges to its optimal value as @xmath12 .",
    "we now show that mspsa in fact provides the slowest possible regret growth . to this end , we provide a lower bound of regret growth for all decision policies .    for any policy @xmath51 , the estimate of the optimal input and the actual input of the policy may not be the same ; for example , the input of mspsa is a randomly perturbed estimate of the optimal input and not the estimate itself .",
    "hence , let s denote an optimal - input estimate obtained using the past observations corresponding to inputs of policy @xmath51 at time @xmath23 by @xmath150 and the input at time @xmath23 by @xmath55 .",
    "we define the worst - case cumulative estimation - mse as @xmath151    in particular , the following theorem states that the product of the growth rate of the worst - case cumulative input - mse @xmath127 and the worst - case cumulative estimation - mse @xmath152 of any sequence @xmath153 can not be lower than @xmath0 for any policy @xmath51 .",
    "[ thm : lower ] for any value of @xmath154 , there exists a constant @xmath155 such that , for any policy @xmath51 ,    rcl [ eq : tradeoffbound ] _",
    "t^ |_t^ & & c t.    see appendix .",
    "theorem  [ thm : lower ] shows the trade - off between exploration ( minimizing the estimation error ) and exploitation ( minimizing the input error ) . if the goal is to minimize the cumulative estimation - mse rather than the regret , than it is possible to find a policy for which @xmath152 grows slower than @xmath156 in which case the cumulative input - mse @xmath127 has to grow faster than @xmath156 .",
    "in fact , if @xmath111 is set to be constant rather than a decreasing sequence of @xmath117 , by following the proof of theorem  [ thm : upper ] , it is easy to show that mspsa s cumulative estimation - mse grows no faster than @xmath157 whereas its regret would grow linearly with @xmath0 .",
    "however , the slowest growth rate of @xmath127 can not be slower than that of @xmath152 for the optimal choice of the estimate sequence @xmath153 ( in other words , one can always take the estimate equal to the input , @xmath158 , in which case @xmath159 ) .",
    "therefore , the growth rate of the worst - case cumulative input - mse @xmath127 , and , consequently , the growth rate of the worst - case regret can not be lower than @xmath156 for any policy @xmath51 . therefore , the regret growth rate of mspsa is the optimal one and achieves @xmath7 as stated in theorem  [ thm : lower_regret ] .",
    "[ thm : lower_regret ] for any value of @xmath154 , there exist some constants @xmath160 such that , for any policy @xmath51 ,    rcl [ eq : inputmsebound ] |_t^ & & c ,    and    rcl [ eq : regretbound ] |_t^ & & c  .",
    "we choose the estimate @xmath161 equal to the input @xmath162 .",
    "then , by theorem  [ thm : lower ] , we have @xmath163 . as a result , @xmath164 .",
    "let @xmath165 be the parameter satisfying @xmath166 . in theorem  [ thm : lower",
    "] , we fixed @xmath167 for any @xmath168 .",
    "hence , by ( [ eq : regret ] ) , @xmath169 where @xmath170 by the extreme value theorem .",
    "then , the worst case regret @xmath171 where @xmath172 .    to prove theorem  [ thm : lower ] , we consider a hypothetical case in which the decision maker receives additional observations at each period @xmath23 . it is assumed that the additional observations provided to the decision maker are the observation values corresponding to input @xmath173 from the states that did nt occur at @xmath23 . since such observations ca nt increase the growth rate of regret of the optimal policy , we establish a lower bound for this case by showing that it becomes equivalent to a single state case with @xmath174 and using the multivariate van trees inequality in a similar way as in . if @xmath175 and @xmath174 , the proofs of theorem  [ thm : lower ] and theorem  [ thm : lower_regret ] lead to the result regarding the single state case given in corollary  [ corollary : lower ] .",
    "[ corollary : lower ] for @xmath175 and for any value of @xmath60 and @xmath61 satisfying @xmath176 , there exist some constants @xmath177 such that , for any policy @xmath51 , inequalities ( [ eq : tradeoffbound ] ) , ( [ eq : inputmsebound ] ) , and ( [ eq : regretbound ] ) hold .",
    "as mentioned in related work , it has been shown that for @xmath175 and @xmath77 case , the regret growth rate is @xmath178 @xcite . we show that the characteristics of regret growth changes from @xmath178 to @xmath11 for markov jump system . additionally , corollary  [ corollary : lower ] states that , even in the absence of markov jump , when system matrix @xmath179 is not invertible , the best regret growth rate also jumps from @xmath178 to @xmath11 .",
    "these results can be interpreted as a consequence of the fact that the minimum of the cost function for markov jump system or for single state system with @xmath174 given in ( [ eq : risk ] ) is not a root of the cost function as in the case of @xmath175 with @xmath77 , and decision maker ca nt understand how close it is to the minimum just by looking at its observations .",
    "the single state setting of the revenue maximization problem has been previously studied by keskin and zeevi . in this paper , we consider the more general setting where the affine demand parameters can change depending on the state of nature , more precisely the setting where demand parameters follows a markov jump process .    as for the quadratic regulation objective , to obtain a regret expression for revenue maximization objective , we first determine the optimal solution of a decision maker who knows the system @xmath42 .",
    "then , we present mspsa policy for revenue maximization problem and establish its optimality in regret performance and its convergence to the optimal solution .      by following the same argument as before , under known model , the optimal solution of a decision maker aimed at minimizing the expected cumulative cost given in ( [ eq : cum_risk ] ) , which is equal to minus expected t - period revenue , is to choose the system input minimizing the respective stage cost given in ( [ eq : cost_revenue ] ) . by using ( [ eq : linear ] ) ,",
    "this stage cost can be written as    rcl [ eq : risk_revenue ] ^(s_t-1,x_t ) & = & -_j p_s_t-1,j x_t^ ( a_j x_t+b_j ) .    the optimal input @xmath80 , which depends only on @xmath42 and the previous state @xmath38 , is then given by @xmath180 by dropping the explicit dependency of @xmath80 on @xmath42 in the notation .    using the foc for the optimal input @xmath82",
    ", we obtain the stage regret of a policy @xmath51 ,    rcl r_t^(,p ) & = & ( ^(s_t-1,x_t^)-^(s_t-1,x_s_t-1^ * ) ) + & = & - ( ( x_t^-x_s_t-1^ * ) ^ a_s_t ( x_t^-x_s_t-1^ * ) ) .    since @xmath145 is negative definite",
    ", the stage regret is always non - negative .",
    "consequently , the t - period regret is given by    rcl [ eq : cumulative_revenue ] _",
    "t^(,p ) & = & - ( _ t=1^t ( x_t^-x_s_t-1^ * ) ^ a_s_t ( x_t^-x_s_t-1^ * ) ) .",
    "+      here , we present the mspsa policy for revenue maximization objective .",
    "the only difference between the two problems considered is their respective stage costs ( objectives ) .",
    "therefore , the only change in mspsa policy is how the stage costs are calculated to approximate the objective gradient which corresponds to line 11 and 15 of  [ mspsa_for_qr ] . in the corresponding steps of mspsa policy for revenue maximization , that is given in  [ mspsa_for_rm ] in details ,",
    "the stage costs are calculated as minus the observed revenue at that stage .",
    "let @xmath84 be an arbitrary vector @xmath85 @xmath86 @xmath87 @xmath88    where @xmath89 with some positive constant @xmath90 and a non - negative integer @xmath91 , and @xmath92^{\\tra}$ ] with @xmath93 s drawn from an independent and identical distribution that is symmetrical around zero , and satisfies @xmath94 and @xmath95 for some positive constants @xmath96 and @xmath97 .",
    "@xmath181 @xmath99 @xmath100 @xmath182 @xmath86 update :    rcl _",
    "i , t_i+1 & & ( _ i , t_i - a_t_i()|_t_i ) _    where @xmath102 denotes the euclidean projection operator onto @xmath50 , @xmath103^{\\tra}$ ] , and @xmath104 with some positive constant @xmath105 and a non - negative integer @xmath106 .      to obtain the regret growth rate for mspsa policy for revenue maximization",
    ", we first derive an upper bound on how fast the estimate @xmath116 of the optimal input @xmath108 converges to its true value as we did for the regulation problem .",
    "lemma  [ lemma : regret_revenue ] shows that the conditional mse @xmath118 between the optimal input and its estimate converges to zero with a rate no smaller than the inverse of the square root of the number of times the estimate @xmath116 is updated by mspsa .",
    "[ lemma : regret_revenue ] for any @xmath107 , if @xmath183 then there exists a constant @xmath122 satisfying @xmath123 for any @xmath124",
    ".    see appendix .",
    "the condition of lemma  [ lemma : regret_revenue ] is slightly different than that of lemma  [ lemma : regret ] .",
    "the bound on the step size constant @xmath105 depends on the minimum eigenvalue of @xmath184 .",
    "this difference is due to the choice of a different stage cost .",
    "however , the information of a non - trivial lower bound @xmath126 on the singular values of the system matrices is still sufficient to satisfy this condition .    using the result of lemma  [ lemma : regret_revenue ]",
    ", we prove that mspsa achieves the regret growth rate of @xmath9 for revenue maximization objective as given in theorem  [ thm : upper_revenue ] , and the input of mspsa converges to the optimal input both almost surely and in mean square as given in theorem  [ thm : convergence_revenue ] .",
    "[ thm : upper_revenue ] if @xmath185 for every @xmath107 , then there exist some positive constants @xmath130 and @xmath131 such that @xmath132 and @xmath186    same as the proof of theorem  [ thm : upper ] up to the step that @xmath143 is obtained .",
    "then , by the regret given in ( [ eq : cumulative_revenue ] ) for revenue maximization objective and the fact that @xmath187 is positive definite with eigenvalues upper bounded by @xmath65 , @xmath188 where @xmath189 .    [ thm : convergence_revenue ] for revenue maximization problem , @xmath190 and @xmath191    in the proof of lemma  [ lemma : regret_revenue ]",
    ", we showed that ( [ eq : boundforasconv ] ) holds for any state @xmath107 .",
    "therefore , the proof follows the proof of theorem  [ thm : convergence ] .",
    "previously , for single state setting of this problem , keskin and zeevi have shown that for any policy @xmath51 the worst case regret growth for this problem can not be smaller than @xmath7 and they have shown that @xmath10 is achievable by a semi - myopic policy that they referred to as multivariate constrained iterated least squares ( mcils ) policy . here , we showed that it is possible to achieve the lower bound @xmath7 given in by mspsa policy for more general problem with markov jumped demand .",
    "next , we generalize keskin and zeevi s lower bound result to markov jump case by showing that for any policy @xmath51 and for any state space size @xmath192 , the growth rate of worst - case regret is bounded by @xmath7 , and hence mspsa achieves the optimal rate of @xmath11 .",
    "[ thm : lower_revenue ] for any value of @xmath192 , there exist some constants @xmath193 such that , for any policy @xmath51 ,    rcl [ eq : tradeoffbound_revenue ] _",
    "t^ |_t^ & & c^2 t ,    rcl [ eq : inputmsebound_revenue ] |_t^ & & c ,    and    rcl [ eq : regretbound_revenue ] |_t^ & & c.    see appendix .",
    "we present simulation results to illustrate the growth rate of regret and the optimal - input estimate convergence of mspsa policy both for quadratic regulation and revenue maximization problems .",
    "note that , by these simulation examples , we can only exhibit the performance of `` typical '' parameters and not the worst - case performance as studied in the theoretical characterization of regret .    for a benchmark comparison , we consider the greedy least square estimate ( lse ) method proposed by anderson and taylor .",
    "at each period , the greedy lse determines the input by using the least square estimates of system parameters as if they were the true parameters and projects it onto the set @xmath50 . in order to calculate the initial lses of the system parameters ,",
    "the first samples corresponding to the inputs generated by perturbing the initial input @xmath194 in each direction are taken until the lse of @xmath40 is computationally tractable .",
    "although , in general , greedy lse performs well numerically , it was shown that it can lead to incomplete learning and may not converge with positive probability which causes linear growth in regret .      to illustrate the performance of mspsa policy for quadratic regulation problem",
    ", we consider the problem studied in , the problem of an electricity retailer who wants to set hourly electricity prices for the next day to meet its predetermined quantity @xmath195 in demand for each hour .",
    "therefore , the system dimension was set to be @xmath196 where each dimension corresponds to an hour of the day .",
    "different than where the demand is time - invariant , it is assumed that the demand of its customers changes depending on the state of the day , weather conditions , which follows a markov jump process .",
    "for this example , we considered two states and set the transition probability from any state to the same state to be 0.6 and to the other state to be 0.4 . to calculate the average performance ,",
    "@xmath197 monte carlo runs were used .",
    "[ multivariate_target_matching ] shows the average performance of mspsa and greedy lse for this quadratic regulation example . in  [ sqrtregret_2state ] , we plot the regret of both policies with respect to square root of the time horizon .",
    "we observe that the t - period regret of mspsa grows linearly with @xmath156 , which is consistent with the theoretical upper bound . on the other hand ,",
    "the regret of greedy lse seems to grow faster than linear .",
    "therefore , we observe that mspsa outperforms greedy lse and the difference between the performance of two policies is getting bigger as @xmath0 increases .    [ priceconv_2state ] illustrates how averaged normed squared error between the optimal input and its estimate changes with time in a log - log plot . from theorem  [ thm :",
    "convergence ] , we know that the optimal - input estimate and thus the input itself converges in mean square for mspsa . in  [ priceconv_2state ] , we observe that the convergence of mspsa is consistent with this result . furthermore",
    ", the logarithm of the estimation error seems to decrease almost linearly with the logarithm of the time horizon . in other words , mse seems to converge with a rate equal to @xmath198 .",
    "this is reasonable because in lemma  [ lemma : regret ] , we show that , for each @xmath107 , the estimation error decreases with a rate equal or faster than the inverse of the square root of the number of times state @xmath72 is observed . on the other hand , convergence trend for greedy lse seems to be much slower and it performs",
    "poorly compared with mspsa s performance .      here , we present an example for revenue maximization problem with system size @xmath199 and 3 different states where the transition probability from any state to the same state was set to be @xmath200 and to any other state to be @xmath201 .",
    "we used @xmath197 monte carlo runs to calculate the average performance of both policies .",
    "the average performance of mspsa and greedy lse for this example with revenue maximization objective is given in  [ multivariate_rev_max ] .",
    "the regret growth and the convergence of averaged normed squared error between the optimal input and its estimate are illustrated in  [ sqrtregret_3state ] and  [ priceconv_3state ] , respectively . in both plots ,",
    "we observe a trend similar to the previous example even though the regret characterizations and the optimal inputs are different due to different objectives .",
    "[ sqrtregret_3state ] shows that mspsa s regret grows linearly with @xmath156 whereas greedy lse s regret grows almost exponentially with @xmath156 .",
    "therefore , we observe that mspsa eventually outperforms greedy lse as @xmath0 increases even though greedy lse performs better at the beginning of the time horizon .",
    "[ priceconv_3state ] shows that , after a slight increase at the beginning of the time horizon , the logarithm of the estimation error of mspsa decreases linearly with the logarithm of the time horizon and becomes sufficiently small ; whereas the estimation error of greedy lse stays almost constant except the spike that is probably due to the poor initial lses .",
    "overall , we can say that mspsa outperforms greedy lse in both numerical examples .",
    "we present in this paper an online learning and optimization approach for jump markov affine models with unknown parameters for two different objectives : ( i ) quadratic regulation and ( ii ) revenue maximization . for both objectives",
    ", we establish that mspsa achieves the optimal rate of regret growth @xmath11 .",
    "our results highlight a change of the minimum order of regret growth from @xmath1 of the classical time - invariant affine models to @xmath11 of the jump markov affine models for the quadratic regulation objective . on the other hand , we show that introducing a markov jump process to the system does not change the optimal rate of regret growth for the revenue maximization objective",
    ". we also establish the convergence of mspsa policy to the optimal solution .",
    "our simulation results verify that proposed method mspsa can outperform the greedy lse method .",
    "let @xmath202 . by mspsa update step given in ( [ eq : mspsaupdate_regulation ] ) and the fact that projection onto @xmath50 maps a point closer to @xmath108",
    ", we have ,    rcl [ eq : boundt ] _",
    "i , t_i+1 & & _ i , t_i - a_t_i()|_t_i - x_i^*_2 ^ 2 + & = & _ i , t_i -2 ( d_i , t_i^+-d_i , t_i^-)(_i , t_i - x_i^*)^|_t_i + & & + ( d_i , t_i^+-d_i , t_i^-)^2|_t_i^|_t_i",
    ".    our goal is to bound @xmath203 by simplifying ( [ eq : boundt ] ) . by ( [ eq : risk ] )",
    ", we obtain ,    rcl + & = & 4c_t_i _ t_i^ _ j p_i , j a_j^(a_j_i , t_i+b_j - y^ * ) + & = & 4c_t_i _ t_i^ ( _ j p_i , j a_j^",
    "a_j)(_i , t_i - x_i^ * )    where last equality is obtained using the foc for @xmath108 . let @xmath204 . using the independence of @xmath93 s , we get ,    rcl + & = & -8_j p_i , j a_j(_i , t_i - x_i^*)_2 ^ 2 + & & -8_min , i_i , t_i . [ eq : secondterm ]    since @xmath50 is compact , @xmath205 where constant @xmath206 .",
    "for any @xmath207 , because @xmath208 and singular values of @xmath78 are bounded , @xmath209 where constant @xmath210 . by holder",
    "s inequality , we have @xmath211 , @xmath212 , and @xmath213 . then ,",
    "after simplification , we obtain , @xmath214 where @xmath215 , and    rcl + & & 8c_t_i^2 ( _ t_i^ _ j p_i , j a_j^(a_j_i , t_i+b_j - y^*))^2 + & = & 8c_t_i^2 ( _ t_i^ ( _ j p_i , j a_j^ a_j)(_i , t_i - x_i^*))^2 ,    where last equality is obtained using the foc of @xmath108 .",
    "consequently ,    rcl [ eq : lastterm ] ( ( ) ^2|_t_i^|_t_i|i , t_i,_i , t_i ) & & c_2_i , t_i + , +    where @xmath216 and @xmath217 .    thus , by expressions ( [ eq : boundt ] ) , ( [ eq : secondterm ] ) , and ( [ eq : lastterm ] ) ;    rcl + & & ( 1-a_t_i8_min , i + a_t_i^2c_2)_i , t_i + a_t_i^2 .",
    "[ eq : boundforasconv ]    consequently , @xmath218 using this result recursively and since @xmath219 for all @xmath220 , we have ,    rcl e_i , t_i+1 & & ( _ j=1^t_i(1 - 8a_j_min , i+a_j^2c_2 ) ) e_i,1 + & & + _ j=1^t_i(_l = j+1^t_i(1 - 8a_l_min , i+a_l^2c_2 ) ) a_j^2 + & & e^_j=1^t_i ( -8a_j_min , i+a_j^2c_2 ) e_i,1 + & & + _",
    "j=1^t_i e^_l = j+1^t_i(-8a_l_min , i+a_l^2c_2 ) a_j^2 .",
    "since @xmath221 and @xmath222 ,    rcl e_i , t_i+1 & & e^-(t_i+1+n_i)+(1+n_i)+2_i^2 c_2e_i,1 + & & + _",
    "j=1^t_i e^(1+n_i)^-1 + 2_i^2 c_2 a_j^2 + & & + _",
    "j=1^t_i + & & ,    where @xmath223 , @xmath224 , and @xmath225 .",
    "in the proof of lemma  [ lemma : regret ] , for any state @xmath107 , we showed that the inequality ( [ eq : boundforasconv ] ) holds where @xmath202 . by theorem  1  of  robbins  and  siegmund  @xcite",
    ", we know that @xmath226 exists and @xmath227 almost surely ( a.s . ) . since @xmath228 , we obtain that @xmath229    let @xmath230 be the indicator function .",
    "given @xmath137 and @xmath117 , @xmath134 is equal to either @xmath135 or @xmath136 .",
    "hence , @xmath231 .",
    "if state @xmath72 is recurrent , @xmath232 because @xmath117 is greater or equal to half of the number of times state @xmath72 is occurred up to @xmath23 .",
    "therefore , for a recurrent state @xmath107 ,    rcl + & = & ( _ t 1_i(s_t-1)x_t^-x^*_i^2_2 = 0 | _",
    "t t_i = ) + & & ( _ t ( 2_i , t_i+2(_i)^2n_1 ^ 2 t_i^-1/2 ) = 0 | _",
    "t t_i = ) + & = & ( _ t_i _ i , t_i=0)=1 .",
    "so , for a recurrent state @xmath72 and for any @xmath233 , we have ,    rcl [ eq : limrecurrent ] _ t ( 1_i(s_t-1)x_t^-x^*_i^2_2 > t t ) & = & 0",
    ". +    if a state @xmath107 is transient , then for any @xmath233 , we have ,    rcl + & & _ t ( s_t-1=i t t ) = 0 , [ eq : limtransient ]    where last equality is due to borel - cantelli lemma and the fact that @xmath234 for a trasient state @xmath72 .    by definition , expression ( [ eq : asconv ] )",
    "holds , if and only if , for every @xmath235 , @xmath236 .",
    "any state @xmath107 is either recurrent or transient .",
    "hence , by ( [ eq : limrecurrent ] ) and ( [ eq : limtransient ] ) , we obtain that , for any @xmath235 ,    rcl + & & _ t _ i=1^k ( 1_i(s_t-1)x_t^-x^*_i^2_2 > t t ) + & = & 0 .    since ( [ eq : asconv ] ) holds and @xmath237 where @xmath238 , by lebesgue s dominated convergence theorem , @xmath239",
    "let the transition probability from any state to any other state be @xmath240 .",
    "without loss of generality , take @xmath241 .",
    "let @xmath28 be i.i.d . with distribution @xmath242 which is independent of the state .    because additional observations ca nt increase the growth rate of regret for an optimal policy , we assume that the decision maker receives the observation values corresponding to the input @xmath55 from all other states that did nt occur at time @xmath23 as additional observations at time @xmath23 . hence , at each @xmath23 , the decision maker gets observations from the affine functions of all states for input @xmath55 .",
    "let s define @xmath179 , @xmath243 , and @xmath28 as @xmath244 where @xmath245 denotes the system noise of observation from state @xmath72 .",
    "now , for any policy @xmath51 , we can express the observation vector at @xmath23 as @xmath246    observe that foc for the optimal input @xmath82 at time @xmath23 obtained from minimizing ( [ eq : risk ] ) is the same for any state @xmath247 for our fixed choice of @xmath17 .",
    "hence , we drop the dependence on the previous state @xmath38 along with @xmath17 and denote it as @xmath248 , @xmath249 , to express the dependence on @xmath40 . with the new notation",
    ", foc can be expressed as @xmath250 consequently , the optimal price given in ( [ eq : optprice ] ) becomes @xmath251    let s express @xmath252 as @xmath253^{\\tra}$ ] where @xmath254 is the @xmath72th entry of @xmath255 and @xmath256 is the @xmath72th row vector of @xmath64 .",
    "we fix a compact rectangle @xmath58 such that , for any @xmath63 , @xmath248 is contained in @xmath50 and @xmath64 is full column rank for all @xmath18 .",
    "can be shown by the continuity of @xmath248 on a compact rectangle @xmath257 which satisfies @xmath64 to be full column rank for all @xmath18 and for any @xmath258 , and contains a fixed point @xmath259 in its interior for which @xmath260 is in the interior of @xmath50 .",
    "the existance of @xmath257 can be shown by using the continuity of the determinant of @xmath261 for each @xmath18 at the fixed point @xmath259 .",
    "] since @xmath17 and @xmath41 are already fixed , our goal is to obtain the performance of the worst - case system parameter @xmath40 that is chosen from the set @xmath62 .",
    "applying implicit function theorem on @xmath262 gives    rcl [ eq : diffopt ] = -()^-1 = - ( a^a)^-1 ,    and by calculus , we have ,    rcl [ eq : difffoc ] ( ) ^ & = & ( ax^*()+b )    _ n^ + _",
    "n    + a    1 + x^ * ( )    .    let @xmath263 .",
    "density of the output vector up to time @xmath23 given the parameter vector @xmath40 and input vector @xmath264 can be written as @xmath265 by writing the joint distribution as a product of conditionals and by the conditional independence of the input for any policy @xmath51 from the parameter @xmath40 given the information history vector @xmath45 , we get , @xmath266 by using the mixed product property @xmath267 and the independence of @xmath268 , we obtain the fisher information for @xmath269 as    rcl i_t^ ( ) & = & ( ^ | x^t , ) + & = & i_m ( _ i=1^t    1 + x_i^    1 , ( x_i^)^    ) .",
    "[ eq : fisher ]    now , we choose a prior distribution @xmath270 as an absolutely continuous density on @xmath62 taking positive values in the interior of @xmath62 and zero on its boundary .",
    "we choose @xmath179 and @xmath243 to be independently distributed with distributions @xmath271 and @xmath272 , respectively , so that @xmath273 .",
    "take @xmath274 .",
    "now , we use the multivariate van trees inequality in a similar way in .",
    "this inequality can be expressed as    rcl [ eq : vantreesineq ] ( ^_t - x^*()_2 ^ 2 ) & & +    where the expectation operators are also taken over the prior distribution @xmath270 and @xmath275 is some constant given @xmath270 , which can be seen as the fisher information for the distribution @xmath270 .    by ( [ eq : fisher ] ) , we have ,    rcl ( c()i^_t-1()c()^ ) & = & _ i=1^t-1x_i^-x^*()_2 ^ 2 + & & c_0 _ i=1^t-1x_i^-x^*()_2 ^ 2 [ eq : denom ]    where @xmath276 .    let s define @xmath277 .",
    "since @xmath278 is symmetric positive definite , by ( [ eq : diffopt ] ) and ( [ eq : difffoc ] ) , we obtain    rcl ( c()^ ) & = & ( -c()^ ( a^a)^-1 ) + & = & - b^ ( ax^*()+b ) ( ( a^ a)^-1 ) + & = & - b^ p b ( ( a^a)^-1 ) . +    by singular value decomposition ( svd ) of @xmath179 , observe that @xmath279 where @xmath280 is an orthogonal matrix , and @xmath281 denotes a diagonal matrix with diagonal entries @xmath282 .",
    "hence , @xmath17 is symmetric positive semidefinite .",
    "also observe that @xmath283 .",
    "then , we can bound the numerator term ,    rcl [ eq : num ] ( ( ( ( ) ^)))^2 & & ( ( b^p b))^2 .    observe that @xmath284 is symmetric positive semidefinite and nonzero for @xmath154 ( or @xmath175 and @xmath174 ) since @xmath285 .",
    "hence , there exists some direction @xmath286 such that @xmath287 , and , consequently , there exists some distribution @xmath272 such that @xmath288 .",
    "more specifically , if @xmath289 for some choice of @xmath272 , we can change that choice of @xmath272 to shift the mean of @xmath243 slightly in the direction of @xmath290 , and have @xmath288 . by independence of @xmath243 and @xmath17 ,    rcl ( b^ p b ) & = & ( b)^ |p ( b ) + ( ( b-(b))^ p ( b-(b ) ) ) + & & ( b)^ |p ( b ) .",
    "[ eq : numineq ]    hence , by expressions  ( [ eq : vantreesineq ] ) , ( [ eq : denom ] ) , ( [ eq : num ] ) , and ( [ eq : numineq ] ) ;    rcl _",
    "t=2^t ( ^_t - x^*()_2 ^ 2 ) & & _",
    "t=2^t + & & _ t=2^t , + [ eq : vantrees ]    where @xmath291 and @xmath292 .    since @xmath293 , by ( [ eq : vantrees ] ) , we have , @xmath294 let @xmath295 denote the @xmath296th entry of @xmath248 , and , by extreme value theorem , @xmath297 and @xmath298 are attained .",
    "since @xmath248 is not a constant over @xmath62 ( otherwise @xmath299 would be zero for all @xmath63 , and left hand side of ( [ eq : num ] ) would be zero for any @xmath270 which is a contradiction ) , @xmath300 . for any policy @xmath51 , @xmath301 .",
    "hence , we have , @xmath302 where @xmath303 .",
    "we will follow the steps in lemma  [ lemma : regret ] and simplify inequality  ( [ eq : boundt ] ) . by ( [ eq : risk_revenue ] )",
    ", we obtain ,    rcl + & = & -2c_t_i _ t_i^ _",
    "j p_i , j ( ( a_j+a_j^ ) _ i , t_i+b_j ) + & = & - 2 c_t_i _ t_i^ ( _ j p_i , j(a_j + a_j^ ) ) ( _ i , t_i - x_i^ * ) ,    where last equality is obtained using the foc for @xmath108 .",
    "let @xmath304 . using the independence of @xmath93 s",
    ", we obtain ,    rcl + & = & 4(_i , t_i - x_i^*)^ ( _ j p_i , j ( a_j + a_j^ ) ) ( _ i , t_i - x_i^ * ) + & & -8_min , i_i , t_i .    as in lemma",
    "[ lemma : regret ] , @xmath305 .",
    "hence , for any @xmath79 , @xmath306 . since @xmath307 , @xmath308 where constant @xmath309",
    "consequently ,    rcl + & & 2c_t_i^2 ( _ t_i^ ( _ j p_i , j(a_j+a_j^))(_i , t_i - x_i^*))^2 + 2c_1 ,    and thus , we obtain ( [ eq : lastterm ] ) where @xmath310 and @xmath217 . therefore , ( [ eq : boundforasconv ] ) holds and the rest of the proof is the same as in lemma  [ lemma : regret ] .",
    "the inequality given in ( [ eq : tradeoffbound_revenue ] ) is used to obtain ( [ eq : inputmsebound_revenue ] ) and ( [ eq : regretbound_revenue ] ) as in theorem  [ thm : lower_regret ] .",
    "the proof of inequality ( [ eq : tradeoffbound_revenue ] ) follows the proof of theorem  [ thm : lower ] with some slight modifications to bound the numerator term of the van trees inequality due to revenue maximization objective .",
    "the foc for @xmath248 becomes @xmath311 and the optimal price @xmath312 .",
    "for this problem , the compact rectangle @xmath62 is such that , for any @xmath63 , @xmath248 is contained in @xmath50 and @xmath64 is negative definite for all @xmath18 . by using the continuity of the maximum eigenvalue of @xmath64 rather than the determinant . ]",
    "the implicit function theorem on @xmath262 gives @xmath313 where @xmath314 we take @xmath272 such that @xmath315 .",
    "consequently , the numerator term of the van trees inequality can be bounded as"
  ],
  "abstract_text": [
    "<S> the problem of online learning and optimization of unknown markov jump affine models is considered . </S>",
    "<S> an online learning policy , referred to as markovian simultaneous perturbations stochastic approximation ( mspsa ) , is proposed for two different optimization objectives : ( i ) the quadratic cost minimization of the regulation problem and ( ii ) the revenue ( profit ) maximization problem . </S>",
    "<S> it is shown that the regret of mspsa grows at the order of the square root of the learning horizon . </S>",
    "<S> furthermore , by the use of van trees inequality , it is shown that the regret of any policy grows no slower than that of mspsa , making mspsa an order optimal learning policy . </S>",
    "<S> in addition , it is also shown that the mspsa policy converges to the optimal control input almost surely as well as in the mean square sense . </S>",
    "<S> simulation results are presented to illustrate the regret growth rate of mspsa and to show that mspsa can offer significant gain over the greedy certainty equivalent approach .    </S>",
    "<S> online learning , stochastic approximation , stochastic cramer - rao bounds , continuum - armed bandit , sequential decision making . </S>"
  ]
}