{
  "article_text": [
    "the zero - norm on @xmath1 , denoted @xmath2-norm or @xmath3 , is defined by @xmath4where @xmath5 is the cardinality of the set @xmath6 the @xmath2-norm is an important concept for modelling the sparsity of data and plays a crucial role in optimization problems where one has to select representative variables .",
    "sparse optimization , which refers to an optimization problem involving the @xmath2-norm in objective or constraints , has many applications in various domains ( in particular in machine learning , image processing and finance ) , and draws increased attention from many researchers in recent years .",
    "the function @xmath2 , apparently very simple , is lower - semicontinuous on @xmath7 but its discontinuity at the origin makes nonconvex programs involving @xmath3 challenging .",
    "note that although one uses the term norm  to design @xmath8 , @xmath8 is not a norm in the mathematical sense .",
    "indeed , for all @xmath9 and @xmath10 , one has @xmath11 which is not true for a norm .",
    "formally , a sparse optimization problem takes the form @xmath12@xcite@xcitewhere the function @xmath13 corresponds to a given criterion and @xmath14 is a positive number , called the regularization parameter , that makes the trade - off between the criterion @xmath13 and the sparsity of @xmath15 . in some applications ,",
    "one wants to control the sparsity of solutions , the @xmath2-term is thus put in constraint**s * * , and the corresponding optimization problem is @xmath16    let us mention some important applications of sparse optimization corresponding to these models .    _ feature selection in classification learning : _ feature selection is one of fundamental problems in machine learning . in many applications such as text classification , web mining , gene expression , micro - array analysis , combinatorial chemistry , image analysis , etc ,",
    "data sets contain a large number of features , many of which are irrelevant or redundant . feature selection is often applied to high - dimensional data prior to classification learning .",
    "the main goal is to select a subset of features of a given data set while preserving or improving the discriminative ability of the classifier . given a training data @xmath17 where each @xmath18 is labeled by its class @xmath19 , the discrete set of labels .",
    "the aim of classification  learning is to construct a classifier function that discriminates the data points @xmath20 with respect to their classes@xmath21 .",
    "the embedded feature selection in classification consists of determining the classifier which uses as few features as possible , that leads to a sparse optimization problem like ( mainpbl0 ) .    _",
    "sparse regression : _ given a training data set @xmath22 of @xmath23 independent and identically distributed samples composed of explanatory variables @xmath18 ( inputs ) and response variables @xmath24 ( ouputs ) .",
    "let @xmath25 denote the vector of outputs and @xmath26 denote the matrix of inputs .",
    "the problem of the regression consists in looking for a relation which can possibly exist between @xmath27 and @xmath28 , in other words , relating @xmath28 to a function of @xmath27 and a model parameter @xmath15 .",
    "such a model parameter @xmath15 can be obtained by solving the optimization problem @xmath29where @xmath30 is called loss function .",
    "sparse regression _ problem aims to find a sparse solution of the above regression model , it takes the form of ( [ mainpbl0 ] ) : @xmath31    _ sparse fisher linear discriminant analysis : _ discriminant analysis captures the relationship between multiple independent variables and a categorical dependent variable in the usual multivariate way , by forming a composite of the independent variables . given a set of @xmath23 independent and identically distributed samples composed of explanatory variables @xmath18 and binary response variables @xmath32",
    "the idea of fisher linear discriminant analysis is to determine a projection of variables onto a straight line that best separables the two classes .",
    "the line is so determined to maximize the ratio of the variances of between and within classes in this projection , i.e. maximize the function @xmath33 where @xmath34 and @xmath35 are , respectively , the between and within classes scatter matrix ( they are symmetric positive semidefinite ) given by @xmath36@xmath37here , for @xmath38 , @xmath39 is the mean vector of class @xmath40 , @xmath41 is the number of labeled samples in class @xmath40 .",
    "if @xmath42 is an optimal solution of the problem , then the classifier is given by @xmath43 , @xmath44.the sparse fisher discriminant model is defined by ( @xmath45 ) @xmath46    _ compressed sensing : _ compressed sensing refers to techniques for efficiently acquiring and reconstructing signals via the resolution of underdetermined linear systems .",
    "compressed * *  * * sensing concerns sparse signal representation , sparse signal recovery and sparse dictionary learning which can be formulated as sparse optimization problems of the form ( [ mainpbl0 ] ) .",
    "_ portfolio selection problem with cardinality constraint : _ in portfolio selection problem , given a set of available securities or assets , we want to find the optimum way of investing a particular amount of money in these assets .",
    "each of the different ways to diversify this money among the several assets is called a portfolio . in portfolio management one",
    "wants to limit the number of assets to be investigated in the portfolio , that leads to a problem of the form ( [ l0-constraint ] ) .",
    "_ other applications : _ other applications of sparse optimization include sensor networks ( @xcite ) , error correction ( @xcite ) , digital photography ( @xcite ) , etc .    * existing works*. during the last two decades , research is very active in models and methods optimization involving the zero - norm .",
    "works can be divided into three categories according to the way to treat the zero - norm : convex approximation , nonconvex approximation , and nonconvex exact reformulation .    in the machine learning community ,",
    "one of the best known approaches , belonging to the group `` convex approximation '' , is the @xmath0 regularization approach proposed in @xcite in the context of linear regression , called lasso ( least absolute shrinkage and selection operator ) , which consists in replacing the @xmath2 term @xmath47 by @xmath48 , the @xmath0 -norm of the vector @xmath15 . in @xcite ,",
    "the authors have proved that , under suitable assumptions , a solution of the @xmath2- regularizer problem over a polyhedral set can be obtained by solving the @xmath0- regularizer problem . however , these assumptions are quite restrictive .",
    "since its introduction , several works have been developed to study the @xmath0-regularization technique , from the theoretical point of view to efficient computational methods ( see @xcite , chapter 18 for more discussions on @xmath0-regularized methods ) .",
    "the lasso penalty has been shown to be , in certain cases , inconsistent for variable selection and biased @xcite .",
    "hence , the adaptive lasso is introduced in @xcite in which adaptive weights are used for penalizing different coefficients in the @xmath0-penalty . at the same time , nonconvex continuous approaches , belonging to the second group `` nonconvex approximation '' ( the @xmath2 term @xmath49 is approximated by a nonconvex continuous function ) were extensively developed . a variety of sparsity - inducing penalty functions",
    "have been proposed to approximate the @xmath2 term : exponential concave function @xcite , @xmath50-norm with @xmath51 fu98 and @xmath52 @xcite , smoothly clipped absolute deviation ( scad ) @xcite , logarithmic function @xcite , capped-@xmath0 peleg08 ( see ( [ log - lp ] ) , ( [ scad ] ) and table  [ tab : app - form ] in section  [ approxim ] for the definition of these functions ) . using these approximations ,",
    "several algorithms have been developed for resulting optimization problems , most of them are in the context of feature selection in classification , sparse regressions or more especially for sparse signal recovery : successive linear approximation ( sla ) algorithm bradley - mangasarian , dca ( difference of convex functions algorithm ) based algorithms xia10,colo06,gas09,guan - gray13,leetal2013,lethietal2008a , lethietal2009,ltnl13,ltn13,neumann05,ong12 , local linear approximation ( lla ) @xcite , two - stage @xmath0 zhang09 , adaptive lasso @xcite , reweighted-@xmath0 algorithms @xcite ) , reweighted- @xmath53 algorithms such as focal underdetermined system solver ( focuss ) ( @xcite ) , iteratively reweighted least squares ( irls ) and local quadratic approximation ( lqa ) algorithm @xcite .    in the third category named nonconvex exact reformulation approaches , the @xmath2-regularized problem is reformulated as a continuous nonconvex program . there are a few works in this category . in @xcite ,",
    "the author reformulated the problem ( [ mainpbl0 ] ) in the context of feature selection in svm as a linear program with equilibrium constraints ( lpec ) .",
    "however , this reformulation is generally intractable for large - scale datasets . in @xcite an exact penalty technique in dc programming",
    "is used to reformulate ( [ mainpbl0 ] ) and ( [ l0-constraint ] ) as dc programs . in @xcite this technique",
    "is used for sparse eigenvalue problem with @xmath2-norm in constraint functions @xmath54where @xmath55 @xmath56 is symmetric and @xmath57 an integer , and a dca based algorithm was investigated for the resulting problem .    beside the three above categories ,",
    "heuristic methods are developed to tackle directly the original problem ( [ mainpbl0 ] ) by greedy based algorithms , e.g. matching pursuit , orthogonal matching pursuit , mallat - zang93,pati93 , etc .",
    "convex regularization approaches involve convex optimization problems which are so far `` easy '' to solve , but they do nt attain the solution of the @xmath58-regularizer problem .",
    "nonconvex approximations are , in general , deeper than convex relaxations , and then can produce good sparsity , but the resulting optimization problems are still difficult since they are nonconvex and there are many local minima which are not global .",
    "many issues have not yet been studied or proved in the existing approximation approaches .",
    "first , the consistency between the approximate problems and the original problem is a very important question but still is open .",
    "only a weak result has been proved for two special cases in @xcite ( resp .",
    "@xcite ) when @xmath13 is concave , bounded below on a polyhedral convex set @xmath59 and the approximation term is an exponential concave function ( resp .",
    "a logarithm function and/or @xmath50-norm ( @xmath60 ) ) .",
    "it has been shown in these works that the intersection of the solution sets of the approximate problem and the original problem is nonempty .",
    "moreover no result on the consistency between local minimum of approximate and original problems has been available , while most of the proposed algorithms furnish local minima .",
    "second , several existing algorithms lack a rigorous mathematical proof of convergence .",
    "hence the choice of a good  approximation remains relevant .",
    "two crucial questions should be studied for solving large scale problems , that are , _ how to suitably approximate the zero - norm _ and _ which computational method to use _ for solving the resulting optimization problem .",
    "the development of new models and algorithms for sparse optimization problems is always a challenge for researchers in optimization and machine learning .    * our contributions .",
    "* we consider in this paper the problem ( mainpbl0 ) where @xmath59 is a polyhedral convex set in @xmath61 and @xmath13 is a finite dc function on @xmath61 .",
    "we address all issues cited above for approximation approaches and develop an unifying approach based on dc programming and dca , a robust , fast and scalable approach for nonconvex and nonsmooth continuous optimization ( @xcite ) .",
    "the contributions of this paper are multiple , from both a theoretical and a computational point of view .",
    "firstly , considering a common dc approximate function , we prove the consistency between the approximate problem and the original problem by showing the link between their global minimizers as well as their local minimizers .",
    "we demonstrate that any optimal solution of the approximate problem is in a @xmath62neighbourhood of an optimal solution to the original problem ( [ mainpbl0 ] ) .",
    "more strongly , if @xmath13 is concave and the objective function of the approximate problem is bounded below on @xmath59 , then some optimal solutions of the approximate problem are exactly solutions of the original problem .",
    "these new results are important and very useful for justifying the performance of approximation approaches .",
    "secondly , we provide an in - depth analysis of usual sparsity - inducing functions and compare them according to suitable parameter values .",
    "this study suggests the choice of good approximations of the zero - norm as well as that of good parameters for each approximation .",
    "a reasonable comparison via suitable parameters identifies capped -@xmath0 and scad as the best approximations .",
    "thirdly , we prove , via an exact reformulation approach by exact penalty techniques that , with suitable parameters ( @xmath63 ) , nonconvex approximate problems resulting from capped -@xmath0 or scad functions are equivalent to the original problem . moreover ,",
    "when the set @xmath59 is a box , we can show directly ( without using exact penalty techniques ) the equivalence between the original problem and the approximate capped -@xmath64 problem and give the value of @xmath65 such that this equivalence holds for all @xmath63 .",
    "these interesting and significant results justify our analysis on usual sparsity - inducing functions and the pertinence of these approximation approaches .",
    "it opens the door to study other approximation approaches which are consistent with the original problem .",
    "fourthly , we develop solution methods for all dc approximation approaches .",
    "our algorithms are based on dc programming and dca , because our main motivation is to exploit the efficiency of dca to solve this hard problem .",
    "we propose three dca schemes for three different formulations of a common model to all concave approximation functions .",
    "we show that these dca schemes include all standard algorithms as special versions .",
    "the fourth dca scheme is concerned with the resulting dc program given by the dc approximation ( nonconcave piecewise linear ) function in * * ( * * @xcite ) .",
    "using dc programming framework , we unify all solution methods into dca * * , * * and then convergence properties of our algorithms are guaranteed , thanks to general convergence results of the generic dca scheme .",
    "it permits to exploit , in an elegant way , the nice effect of dc decompositions of the objective functions to design various versions of dca .",
    "it is worth mentioning here the flexibility / versatility of dc  programming and dca : the four algorithms can be viewed as an @xmath0-perturbed algorithm / a reweighted-@xmath0 algorithm ( intimately related to the @xmath0-penalized lasso approach ) / a reweighted-@xmath53 algorithm in case of convex objective functions .",
    "finally , as an application , we consider the problem of feature selection in svm and perform a careful empirical comparison of all approaches .",
    "the rest of the paper is organized as follows .",
    "since dc programming and dca is the core of our approaches , we give in section  [ dca ] a brief introduction of these theoretical and algorithmic tools . the consistency between approximate problems and the original one , the link between their global minimizer as well as their local minimizer are studied in section  approxim , while a comparative analysis on usual approximations is discussed in section  [ sect.compare ] .",
    "a deeper study on capped-@xmath0 approximation and the relation between some approximate problems and exact penalty approaches is presented in section  [ exact ] .",
    "solution methods based on dca are developed in section [ dca - prox ] , while the application of the proposed algorithms for feature selection in svm and numerical experiments are described in section  [ num ] . at last , section *  * [ conclu ] concludes the paper .",
    "let @xmath66 * *  * * be the euclidean space * *  * * @xmath67 * *  * * equipped with the canonical inner product * *  * * @xmath68 * *  * * and its euclidean norm * *  * * @xmath69 the dual space of @xmath66 , denoted by * *  * * @xmath70 * * , * * can be identified with * *  * * @xmath66 * *  * * itself**. * *    dc(difference of convex functions ) programming and dca ( dc algorithms ) , which constitute the backbone of nonconvex programming and global optimization , are introduced in 1985 by pham dinh tao in the preliminary state , and extensively developed by le thi hoai an and pham dinh tao since 1994 ( @xcite and references quoted therein ) .",
    "their original key idea relies on the structure dc of objective function and constraint functions in nonconvex programs which are explored and exploited in a deep and suitable way .",
    "the resulting dca introduces the nice and elegant concept of approximating a nonconvex ( dc ) program by a sequence of convex ones : each iteration of dca requires solution of a convex program .",
    "their popularity resides in their rich , deep and rigorous mathematical foundations , and the versatility / flexibility , robustness , and efficiency of dca s compared to existing methods , their adaptation to specific structures of addressed problems and their ability to solve real - world large - scale nonconvex programs .",
    "recent developments in convex programming are mainly devoted to reformulation techniques and scalable algorithms in order to handle large - scale problems .",
    "obviously , they allow for enhancement of dc programming and dca in high dimensional nonconvex programming .",
    "standard dc  programs are of the form : @xmath71where @xmath72(@xmath73 , the convex cone of all lower semicontinuous proper ( i.e. , not identically equal to @xmath74 convex functions defined on @xmath67 and taking values in @xmath75 such a function @xmath13 is called a dc function , and @xmath76 a dc  decomposition of @xmath13 while @xmath77 and @xmath78 are the dc components of @xmath79 the convex constraint @xmath80 can be incorporated in the objective function of @xmath81 by using the indicator function of @xmath82 denoted by @xmath83 which is defined by @xmath84 if @xmath80 , and @xmath85 otherwise  : @xmath86    the vector space of dc functions , @xmath87 , forms a wide class encompassing most real - life objective functions and is closed with respect to usual operations in optimization .",
    "dc programming constitutes so an extension of convex programming , sufficiently large to cover most nonconvex programs ( @xcite and references quoted therein ) , but not too in order to leverage the powerful arsenal of the latter .",
    "the conjugate of @xmath88 , denoted by @xmath89is given by@xmath90    dc duality associates the primal dc program @xmath81 with its dual @xmath91 , which is also a dc program with the same optimal value and defined by @xmath92and studies the relation between primal and dual solution sets denoted by @xmath93 and @xmath94 respectively . in dc programming we adopt the explainable convention @xmath95 for avoiding ambiguity .",
    "note that the finiteness of @xmath42  implies that dom @xmath96 dom @xmath78 and dom @xmath97  dom @xmath98 , where the effective domain of @xmath99(@xmath100 is dom @xmath101 the function @xmath99(@xmath100 is polyhedral convex if it is the sum of the indicator function of a nonempty polyhedral convex set and the pointwise supremum of a finite collection of affine functions .",
    "polyhedral dc program is a dc program in which at least one of the functions @xmath77 and @xmath78 is polyhedral convex .",
    "polyhedral dc programming , which plays a key role in nonconvex programming and global optimization , has interesting properties ( from both a theoretical and an algorithmic point of view ) on local optimality conditions and the finiteness of dca s convergence .    for @xmath99(@xmath73",
    ", the subdifferential of @xmath88 at @xmath102 dom @xmath103 denoted by @xmath104 is defined by @xmath105the subdifferential @xmath106 is a closed convex set , which generalizes the derivative of @xmath88 in the sense that @xmath88 is differentiable at @xmath107 if and only if @xmath106 is reduced to a singleton , that is nothing but @xmath108    dc programming investigates the structure of @xmath109 , dc duality and local and global optimality conditions for dc programs .",
    "the complexity of dc programs clearly lies in the distinction between local and global solution and , consequently ; the lack of verifiable global optimality conditions .",
    "we have developed necessary local optimality conditions for the primal dc program @xmath81 , by symmetry those relating to dual dc program _ _  _ _ @xmath91 _ _  _ _ are trivially deduced:@xmath110(such a point @xmath111 is called critical point of @xmath76 or ( critical point ) a generalized karusk - kuhn - tucker ( kkt ) condition for @xmath81 ) , and @xmath112the condition ( [ subdifferential_inclusion ] ) is also sufficient ( for local optimality ) in many important classes of dc programs . in particular",
    "it is sufficient for the next cases quite often encountered in practice :    * in polyhedral dc programs with @xmath78 being a polyhedral convex function . in this case , if @xmath78 is differentiable at a critical point @xmath111 , then @xmath111 is actually a local minimizer for @xmath81 . since",
    "a convex function is differentiable everywhere except for a set of measure zero , one can say that a critical point @xmath111 is almost always a local minimizer for @xmath81 . * in case the function @xmath13 is locally convex at @xmath111 . note that , if @xmath78 is polyhedral convex , then @xmath113 is locally convex everywhere @xmath78 is differentiable .",
    "the transportation of global solutions between @xmath81 and @xmath91 is expressed by : @xmath114\\subset \\mathcal{p}\\text { } , \\text { } [ \\bigcup\\limits_{x^{\\ast } \\in \\mathcal{p}}\\,\\partial h(x^{\\ast } ) ] \\subset \\mathcal{d } \\label{global transportation}\\]]the first ( second ) inclusion becomes equality if the function @xmath78 ( resp .",
    "@xmath98 ) is subdifferentiable on @xmath115 ( resp .",
    "@xmath116 ) .",
    "they show that solving a dc program implies solving its dual .",
    "note also that , under technical conditions , this transportation also holds for local solutions of @xmath81 _",
    "_  _ _ and @xmath91 .",
    "( lethi - website , lethithesis , plt97,coconut , siopt2,ltp05,acta , plt98 and references quoted therein ) .",
    "* philosophy of dca : * dca is based on local optimality conditions and duality in dc programming .",
    "the main original idea of dca is simple , it consists in approximating a dc  program by a sequence of convex programs : each iteration @xmath57 of dca approximates the concave part @xmath117 by its affine majorization ( that corresponds to taking @xmath118 and minimizes the resulting convex function .",
    "the generic dca scheme can be described as follows :    * dca scheme *    * initialization : * let @xmath119 be a guess , set @xmath120    * repeat *    * calculate some@xmath121 * calculate @xmath122:x\\in \\mathrm{i\\!r}^{n}\\}\\quad ( p_{k})$ ] * increasing @xmath57 by @xmath123    * until * convergence of @xmath124    note that @xmath125 is a convex optimization problem and is so far `` easy '' to solve .    convergence properties of dca and its theoretical basis can be found in @xcite .",
    "for instance it is important to mention that ( for the sake of simplicity we omit here the dual part of dca ) .",
    "* dca is a descent method without linesearch ( the sequence  @xmath126 is  decreasing ) but with global convergence ( dca converges from any starting point ) . *",
    "if @xmath127 , then @xmath128 is a critical point of @xmath76 . in such a case , dca terminates at @xmath57-th iteration . * if the optimal value @xmath129of problem @xmath81 is finite and the infinite sequence @xmath130is bounded , then every limit point @xmath111 of the sequence @xmath131  is a critical point of @xmath77 @xmath132 @xmath78 .",
    "* dca has a _ linear convergence _ for dc programs .",
    "* dca has a _",
    "finite convergence _ for polyhedral dc programs . moreover , if @xmath78 is polyhedral and @xmath78 is differentiable at @xmath111 then @xmath111 is a local optimizer of @xmath81 .",
    "\\vi ) in dc programming with subanalytic data , the whole sequence * *  * * @xmath130generated by dca converges and dca s rate convergence is stated .",
    "it is worth mentioning that the construction of dca involves dc components @xmath133 and @xmath78 but not the function @xmath13 itself .",
    "hence , for a dc program , each dc decomposition corresponds to a different version of dca . since a dc function @xmath13  has infinitely many dc decompositions which have crucial implications on the qualities ( speed of convergence , robustness , efficiency , globality of computed solutions ,  ) of dca , the search of a good  dc decomposition is important from an algorithmic point of view . for a given dc program ,",
    "the choice of optimal dc  decompositions is still open .",
    "of course , this depends strongly on the very specific structure of the problem being considered . in order to tackle the large - scale setting ,",
    "one tries in practice to choose * *  * * @xmath77 * *  * * and * *  * * @xmath78 * *  * * such that sequences * *  * * @xmath131 * *  * * and * *  * * @xmath134 * *  * * can be easily calculated * * , * * i.e. , either they are in an explicit form or their * *  * * computations are inexpensive . * *  * * very often in practice , the sequence * *  * * @xmath134 * *  * * is explicitly computed because the calculation of a subgradient of * *  * * @xmath78 * *  * * can be * *  * * explicitly obtained by using the usual rules for calculating subdifferential of convex functions**. * * but the solution of the convex program * *  * * @xmath125 * * , * * if not * *  * * explicit , should be achieved by efficient algorithms well - adapted to its special structure , in order to handle the large - scale setting**. * *    how to develop an efficient algorithm based on the generic dca scheme for a practical problem is thus a sensible question to be studied . generally , the answer depends on the specific structure of the problem being considered .",
    "the solution of a nonconvex program @xmath81 by dca must be composed of two stages : the search of an _ appropriate _ dc decomposition of @xmath13 and that of a _ good _ initial point .",
    "dc programming and dca have been successfully applied for modeling and solving many and various  nonconvex programs from different fields of applied sciences , especially in machine learning ( see also the more complete list of references in @xcite ) .",
    "note that with appropriate dc decompositions and suitably equivalent dc reformulations , dca permits to recover most of standard methods in convex and nonconvex programming as special cases .",
    "in particular , dca is a global algorithm ( i.e. providing global solutions ) when applied to convex programs recast as dc programs and therefore dc programming and dca can be used to build efficiently customized algorithms for solving convex programs generated by dca itself .    for a complete study of dc programming and dca",
    "the reader is referred to ( @xcite and the references quoted therein ) .",
    "we focus on the sparse optimization problem with @xmath2-norm in the objective function , called the @xmath2-problem , that takes the form @xmath135where @xmath14 is a positive parameter , @xmath59 is a convex set in @xmath136 and @xmath13 is a finite dc function on @xmath136 .",
    "suppose that @xmath13 has a dc decomposition @xmath137where @xmath138 are finite convex functions on @xmath136 . through the paper , for a dc function @xmath139 , @xmath140 stands for the set @xmath141 .",
    "more precisely , the notation @xmath142 means that @xmath143 for some @xmath144 , @xmath145 .    define the step function @xmath146 by @xmath147 for @xmath148 and @xmath149 otherwise . then @xmath150 .",
    "the idea of approximation methods is to replace the discontinuous step function by a continuous approximation @xmath151 , where @xmath152 is a parameter controling the tightness of approximation .",
    "this leads to the approximate problem of the form @xmath153    [ assump ] @xmath154 is a family of functions @xmath155 satisfying the following properties :    * @xmath156 , @xmath157 . *",
    "for any @xmath152 , @xmath151 is even , i.e. @xmath158 and @xmath151 is increasing on @xmath159 . * for any @xmath152 , @xmath151 is a dc function which can be represented as @xmath160where @xmath161 are finite convex functions on @xmath162 . *",
    "@xmath163 @xmath164.where @xmath165 . *",
    "for any @xmath166 and @xmath167 $ ] : @xmath168\\right\\ } = 0.$ ]    first of all , we observe that by assumption ii ) above , we get another equivalent form of @xmath169where @xmath170indeed , and are equivalent in the following sense .",
    "[ prop : eqv ] a point @xmath171 is a global ( resp .",
    "local ) solution of the problem if and only if @xmath172 is a global ( resp .",
    "local ) solution of the problem .",
    "moreover , if @xmath173 is a global solution of then @xmath174 is a global solution of .    since @xmath175 is an increasing function on @xmath176",
    ", we have @xmath177 then the conclusion concerning global solutions is trivial .",
    "the result on local solutions also follows by remarking that if @xmath178   stands for the set of vectors @xmath179 such that @xmath180 then @xmath181 , and if @xmath182 then @xmath183 .    in standard nonconvex approximation approaches to @xmath2-problem , all the proposed approximation functions @xmath151",
    "are even and concave increasing on @xmath159 ( see table [ tab : app - form ] below ) and the approximate problems were often considered in the form ( [ prob : app - z ] ) . here",
    "we study the general case where @xmath151 is a dc function and consider both problems ( [ prob : app ] ) and ( [ prob : app - z ] ) in order to exploit the nice effect of dc decompositions of a dc program .",
    "now we show the link between the original problem ( [ prob : l0 ] ) and the approximate problem .",
    "this result gives _ a mathematical _ _ foundation _ of approximation methods .",
    "[ thm : glo ] let @xmath184 be the solution sets of the problem and respectively .    1 .",
    "let @xmath185 be a sequence of nonnegative numbers such that @xmath186 and @xmath187 be a sequence such that @xmath188 for any @xmath57 .",
    "if @xmath189 , then @xmath190 .",
    "2 .   if @xmath59 is compact , then for any @xmath191 there is @xmath192 such that @xmath193 3 .   if there is a finite set @xmath194 such that @xmath195 , then there exists @xmath196 such that @xmath197    \\i ) let @xmath198 be arbitrary in @xmath59 . for any @xmath57 , since @xmath199 , we have @xmath200 by assumption [ assump ] ii ) , if @xmath201 , we have @xmath202 if @xmath203 , there exist @xmath204 and @xmath205 such that @xmath206 $ ] and @xmath207 $ ] for all @xmath208 .",
    "then we have @xmath209 since @xmath210 and @xmath211 , we have @xmath212 .",
    "note that @xmath13 is continuous , taking @xmath213 of both sides of , we get @xmath214 thus , @xmath215 for any @xmath216 , or @xmath217 .",
    "\\ii ) we assume by contradiction that there exists @xmath218 and a sequence @xmath219 such that @xmath220 , and for any @xmath57 there is @xmath221 . since @xmath222 and @xmath59 is compact , there exists a subsequence @xmath223 of @xmath224 converges to a point @xmath171 . by i ) , we have @xmath217 . however , @xmath225 that is a closed set , so @xmath226 .",
    "this contradicts the fact that @xmath217 .",
    "iii ) assume by contradiction that there is a sequence @xmath219 such that @xmath220 , and for any @xmath57 there is @xmath227 . since @xmath194 is finite , we can extract a subsequence such that @xmath228 .",
    "then we have @xmath229 .",
    "this contradicts the fact that @xmath230 following i ) .",
    "the assumption that @xmath151 is an even function is not needed for proving this theorem .",
    "more precisely , the theorem still holds when the assumption ii ) is replaced by for any @xmath152 , @xmath151 is decreasing on @xmath231 $ ] and is increasing on @xmath232 for the zero - norm , since the step function is even , it is natural to consider its approximation @xmath151 as an even function .",
    "theorem [ thm : glo ] shows that any optimal solution of the approximate problem ( [ prob : app ] ) is in a @xmath62neighboohord of an optimal solution to the original problem ( [ prob : l0 ] ) , and the tighter approximation of @xmath2-norm is , the better approximate solutions are .",
    "moreover , if there is a finite set @xmath194 such that @xmath233 then any optimal solution of the approximate problem ( [ prob : app ] ) contained in @xmath194 solves also the problem ( [ prob : l0 ] ) . by considering the equivalent problem ( [ prob : app - z ] ) ,",
    "we show in the following corollary that such a set @xmath194 exists in several contexts of applications ( for instance , in feature selection in svm ) .",
    "[ crl:1 ] suppose that @xmath234 is concave on @xmath159 , @xmath59 is a polyhedral convex set having at least a vertex and @xmath13 is concave , bounded below on @xmath59 .",
    "then @xmath235 defined in is also a polyhedral convex set having at least a vertex**. * * let @xmath236 be the vertex set of @xmath235 and @xmath237then @xmath238 and there exists @xmath239 such that @xmath240,@xmath241",
    ".    by the assumptions , we have @xmath242 is concave , bounded below on @xmath243 , so @xmath244 .",
    "let @xmath245 . by proposition [ prop : eqv ] , we have @xmath246 .",
    "since @xmath236 is finite , so is @xmath194 .",
    "the property iii ) of theorem [ thm : glo ] implies the existence of @xmath247 such that @xmath248    note that the consistency between the solution of the approximate problem and the original problem have been carried out in @xcite ( resp .",
    "@xcite ) for the case where @xmath13 is concave , bounded below on the polyhedral convex set @xmath59 and @xmath234 is the exponential approximation defined in table [ tab : app - form ] below ( resp .",
    "@xmath234 is the logarithm function and/or @xmath50-norm ( @xmath60 ) ) . here",
    ", besides general results carried out in theorem [ thm : glo ] , our corollary [ crl:1 ] gives a much stronger result than those in @xcite where they only ensure that @xmath249 .    observing that",
    "the approximate problem is still nonconvex for which , in general , only local algorithms are available , we are motivated by the study of the consistency between local minimizers of the original and approximate problems . for this purpose ,",
    "first , we need to describe characteristics of local solutions of these problems .",
    "\\i ) a point @xmath250 is a local optimum of the problem if and only if @xmath251 is a local optimum of the problem @xmath252where @xmath253",
    ".    \\ii ) if @xmath250 is a local optimum of the problem then @xmath254for some @xmath255 .",
    "\\i ) the forward implication is obvious , we only need to prove the backward one .",
    "assume that @xmath256 is a local solution of the problem .",
    "there exists a neighbourhood @xmath236 of @xmath256 such that @xmath257 and @xmath258 for any @xmath259 , two cases occur :    - if @xmath260 , then @xmath261 and @xmath262 .    - if @xmath263 , then @xmath264 and @xmath265 .",
    "+ in both cases , we have @xmath266 .",
    "thus , @xmath256 is a local solution of the problem .",
    "+ ii ) since @xmath267 is a dc function , is a dc program .",
    "therefore , the necessary local condition of the problem can be stated by @xmath268 or equivalently , there exists @xmath269 such that @xmath270    as for the characteristics of local solutions of the problem ( [ prob : app ]    ) , we follow the condition ( [ critical point ] ) above for a dc program . writing the problem in form of a dc program @xmath271with @xmath272 then , for a point @xmath273 the necessary local optimality condition ( [ critical point ] ) can be expressed as @xmath274which is equivalent to @xmath275for some @xmath255 and @xmath276 .",
    "now we are able to state consistency results of local optimality .",
    "[ thm : local ] let @xmath277 and @xmath278 be the sets of @xmath216 satisfying the conditions and respectively .    1 .",
    "let @xmath185 be a sequence of nonnegative numbers such that @xmath186 and @xmath187 be a sequence such that @xmath279 .",
    "if @xmath189 , we have @xmath280 .",
    "2 .   if @xmath59 is compact then , for any @xmath218 , there is @xmath281 such that @xmath282 3 .",
    "if there is a finite set @xmath194 such that @xmath283 , then there exists @xmath284 such that @xmath285    \\i ) by definition , there is a sequence @xmath286 such that for all @xmath287 @xmath288 @xmath289 for @xmath287 , we have @xmath290 where @xmath291 , and @xmath292 .",
    "since @xmath224 converges to @xmath256 , there is @xmath293 and a compact set @xmath294 such that @xmath295 .",
    "it follows by theorem 24.7 ( @xcite ) that @xmath296 and @xmath297 are compact sets .",
    "thus , there is an infinite set @xmath298 such that the sequence @xmath299 converges to a point @xmath300 and the sequence @xmath301 converges to a point @xmath302 . by theorem 24.4 ( @xcite ) , we have @xmath303 and @xmath304 .",
    "therefore , the sequence @xmath305 converges to @xmath306 .    by assumption [ assump ]",
    "iv ) , we have @xmath307 . moreover ,",
    "for any @xmath308 , there exist @xmath204 and @xmath205 such that @xmath309 $ ] and @xmath207 $ ] for all @xmath208 . by assumption [ assump ] v )",
    ", we deduce that @xmath310 as @xmath311 .    for arbitrary @xmath260 , implies that @xmath312 taking @xmath313 , we get @xmath314 thus , @xmath315 .",
    "\\ii ) and iii ) are proved similarly as in theorem [ thm : glo ] .",
    "first , let us mention , in chronological order , the approximation functions proposed in the literature in different contexts , but we do nt indicate the related works concerning algorithms using these approximations ) .",
    "the first was concave exponential approximation proposed in @xcite in the context of feature selection in svm , and @xmath50-norm with @xmath51 for sparse regression ( @xcite ) .",
    "later , the @xmath50-norm with @xmath52 was studied in @xcite for sparse signal recovery , and then the smoothly clipped absolute deviation ( scad ) @xcite in the context of regression , the logarithmic approximation @xcite for feature selection in svm , and the capped-@xmath0 ( @xcite ) applied on sparse regression .",
    "a common property of these approximations is they are all even , concave increasing functions on @xmath232 it is easy to verify that these function satisfy the conditions in assumption 1 and so they are particular cases of our dc approximation @xmath234 .",
    "more general dc approximation functions are also investigated , e.g. , pil ( @xcite ) that is a ( nonconcave ) piecewise linear function defined intable [ tab : app - form ] .",
    "note that , some of these approximation functions , namely logarithm ( log ) , scad and @xmath50-norm defined by @xmath316@xmath317do not directly approximate @xmath2-norm .",
    "but they become approximations of @xmath2-norm if we multiply them by an appropriate factor ( which can be incorporated into the parameter @xmath14 ) , and add an appropriate term ( such a procedure does nt affect the original problem ) .",
    "the resulting approximation forms of these functions are given in table [ tab : app - form ] .",
    "we see that @xmath318 is obtained by multiplying the scad function by @xmath319 and setting @xmath320 .",
    "similarly , by taking @xmath321 , we have @xmath322for using @xmath50-norm approximation with @xmath51 , we take @xmath323 . note that @xmath324 . to avoid singularity at @xmath325 , we add a small @xmath191 . in this case , we require @xmath326 satisfying @xmath327 to ensure that @xmath328 .",
    ".@xmath329-approximation functions @xmath234 and the first dc decomposition @xmath330 .",
    "the second dc decomposition is @xmath331 . [",
    "cols=\"<,<,<\",options=\"header \" , ]      all algorithms were implemented in the visual c++ 2008 , and performed on a pc intel i5 cpu650 , 3.2 ghz of 4 gb ram .",
    "cplex 12.2 was used for solving linear / quadratic programs .",
    "we stop all algorithms with the tolerance @xmath332 .",
    "the non - zero elements of @xmath15 are determined according to whether @xmath333 exceeds a small threshold ( @xmath334 ) .    for the comparison of algorithms , we are interested in the accuracy ( pwco - percentage of well classified objects ) and the sparsity of obtained solution as well as the rapidity of the algorithms .",
    "@xmath335 ( resp .",
    "@xmath336 ) denotes the powc on training set ( resp .",
    "test set ) .",
    "the sparsity of solution is determined by the number ( and percentage ) of selected features ( @xmath337 ) while the rapidity of algorithms is measured by the cpu time in seconds .      in this experiment",
    ", we study the effectiveness of the three proposed dca schemes dca1 , dca2 and dca3 for a same approximation .",
    "capped-@xmath64 approximation is chosen for this experiment .",
    "for each dataset , the same value of @xmath14 is used for all algorithms .",
    "we set @xmath338 for first three datasets ( _ ionosphere , wpbc(24 ) , wpbc(60 ) _ ) while @xmath339 is used for five large datasets ( _ adv , arcene , breast , gisette , leukemia _ ) .",
    "to chose a suitable value of @xmath340 for each algorithm dca1 , dca2 and dca3 , we perform them by @xmath341 folds cross - validation procedure on the set @xmath342 and then take the value corresponding to the best results . once @xmath343 is chosen ( its value is given in table [ tab : comparedca1 - 2 - 3 ] ) , we perform these algorithms @xmath341 times from @xmath341 random starting solutions and report , in the columns 3 - 5 of table [ tab : comparedca1 - 2 - 3 ] , the mean and standard deviation of the accuracy , the sparsity of obtained solutions and cpu time of the algorithm .",
    "we are also interested on the efficiency of updating @xmath340 procedure .",
    "for this purpose , we compare two versions of dca1 - with and without updating @xmath340 procedure ( in case of capped-@xmath64 approximation ) . for a fair comparison , we first run dca1 with updating @xmath340 procedure and then perform dca1 with the fixed value @xmath344 which is the last value of @xmath340 when the updating @xmath340 procedure stops .",
    "computational results are reported in the columns 6 ( dca1 with fixed @xmath340 ) and @xmath345 ( dca1 with updating @xmath340 procedure ) of table [ tab : comparedca1 - 2 - 3 ] .    to evaluate the globality of the dca based algorithms we use cplex 12.2 for globally solving the exact formulation problem ( [ l0 obj penal ] ) via exact penalty techniques ( mixed 0 - 1 linear programming problem ) and report the results in the last column of table [ tab : comparedca1 - 2 - 3 ] .",
    "bold values in the result tables correspond to best results for each data instance .",
    "* comments on numerical results *    * comparison between dca1 , dca2 and dca3 ( columns @xmath346 - @xmath347 ) * * concerning the correctness , dca1 furnishes the best solution out of the three algorithms for all datasets ( with an important gain of @xmath348 on dataset wpbc(24 ) ) .",
    "dca2 and dca3 are comparable in terms of correctness .",
    "* * as for the sparsity of solution , all the three dca schemes reduce considerably the number of selected features ( up to @xmath349 on large datasets such as _ arcene _ , _ breast _ , _ leukemia _ ,  ) .",
    "moreover , dca1 gives better results than dca2/dca3 on @xmath350 out of @xmath351 datasets .",
    "* * in terms of cpu time , dca1 and dca2 are faster than dca3 .",
    "this is natural , since at each iteration , the first two algorithms only require solving one linear program while dca3 has to solve one convex quadratic program .",
    "dca1 is somehow a bit faster than dca2 on @xmath347 out @xmath345 datasets . * * overall , we see that dca1 is better than dca2 and dca3 on all the three evaluation criteria .",
    "hence , it seems to be that the first dca scheme is more appropriate than the other two for capped-@xmath352 approximation .",
    "* dca1 with and without updating @xmath343 procedure ( columns @xmath346 , @xmath350 and @xmath345 ) : * * for all datasets , updating @xmath340 procedure gives a better solution ( on both accuracy and sparsity ) than dca1 with @xmath353 . * * except for dataset _",
    "wpbc(24 ) _ , updating @xmath340 procedure is better than dca1 with @xmath340 chosen by @xmath341 folds cross - validation in terms of sparsity of solution . as for accuracy ,",
    "the two algorithms are comparable . * * the choice of the value of @xmath343 defining the approximation function is very important .",
    "indeed , the results given in columns @xmath346 and @xmath350 are far different , due to the fact that , the value of @xmath340 chosen by @xmath341 folds cross - validation is much more smaller than @xmath344 .",
    "these results confirm our analysis in subsection  [ sect : update_theta ] above : while the approximate function would be better with larger values of @xmath343 , the approximate problems become more difficult and it can be happened that the obtained solutions are worse when @xmath343 is quite large . to overcome this",
    " contradiction  between theoretical and computational aspects , the proposed updating @xmath343 procedure seems to be efficient . *",
    "comparison between dca based algorithms and cplex for solving the original problem ( [ l0 obj penal ] ) * * for _ ionosphere _ and _ wpbc(60 ) _ , updating @xmath340 procedure for capped-@xmath0 gives exactly the same accuracy and the same number of selected features as cplex .",
    "it means that updating @xmath340 procedure reaches the global solution for those two datasets . for _",
    "wpbc(24 ) _ , the two obtained solutions are slightly different ( same accuracy on training set and @xmath345 selected features for cplex instead of @xmath354 for updating @xmath340 procedure ) . * * for large datasets , cplex ca nt furnish a solution with a cpu time limited to @xmath355 seconds while dca based algorithms give a good solution in a short time .      in the second experiment , we study the effectiveness of different approximations of @xmath329 .",
    "we use dca1 for all approximations except pil for which dca4 is applied ( cf",
    ". section [ new_approximation ] ) .    in this experiment , for the trade - off parameter @xmath14 , we used the following set of candidate values @xmath356 .",
    "the value of parameter @xmath340 is chosen in the set @xmath342 .",
    "the second parameter @xmath357 of scad approximation is taken from @xmath358 .",
    "for each algorithm , we firstly perform a @xmath341-folds cross - validation to determine the best set of parameter values . in the second step ,",
    "we run each algorithm , with the chosen set of parameter values in step 1 , @xmath341 times from @xmath341 starting random points and report the mean and standard deviation of each evaluation criterion .",
    "the comparative results are reported in table tab : compareapproximation .",
    "we observe that :    * in terms of sparsity of solution , the quality of all approximations are comparable . all the algorithms reduce considerably the number of selected features , especially for @xmath347 large datasets ( _ adv , arcene , breast , gisette , leukemia _ ) . for _ breast _ dataset ,",
    "our algorithms select only about thirty features out of @xmath359 while preserving very good accuracy ( up to @xmath360 correctness on train set ) .",
    "* capped-@xmath0 is the best in terms of accuracy : it gives best accuracy on all train sets and @xmath361 out of @xmath345 test sets .",
    "the quality of other approximations are comparable . *",
    "the cpu time of all the algorithms is quite small : less than @xmath362 seconds ( except for _ gisette _ , cpu time of dcas varies from @xmath363 to @xmath364 seconds ) .",
    "we have intensively studied dc programming and dca for sparse optimization problem including the zero - norm in the objective function .",
    "dc approximation approaches have been investigated from both a theoretical and an algorithmic point of view . considering a class of dc approximation functions of the zero - norm including all usual sparse inducing approximation functions , we have proved several novel and interesting results : the consistency between global ( resp .",
    "local ) minimizers of the approximate problem and the original problem , the equivalence between these two problems ( in the sense that , for a sufficiently large related parameter , any optimal solution to the approximate problem solves the original problem ) when the feasible set is a bounded polyhedral convex set and the approximation function is concave , the equivalence between capped-@xmath0 ( and/or scad ) approximate problems and the original problem with sufficiently large parameter @xmath340 ( in the sense that they have the same set of optimal solutions ) , the way to compute such parameters @xmath340 in some special cases , and a comparative analysis between usual sparse inducing approximation functions . considering the three dc formulations for a common model to all concave approximation functions",
    "we have developed three dca schemes and showed the link between our algorithms with standard approaches .",
    "it turns out that all standard nonconvex approximation algorithms are special versions of our dca based algorithms .",
    "a new dca scheme has been also investigated for the dc approximation ( piecewise linear ) which is not concave as usual sparse inducing functions . concerning the application to feature selection in svm , among the four dca schemes , three ( resp .",
    "one ) require solving one linear ( resp .",
    "convex quadratic ) program at each iteration and enjoy interesting convergence properties ( except algorithm 3 ) : they converge after finitely many iterations to a local solution in almost all cases .",
    "numerical experiments confirm the theoretical results : the capped-@xmath0 has been identified as the winner  among sparse inducing approximation functions .",
    "our unified dc programming framework shed a new light on sparse nonconvex programming .",
    "it permits to establish the crucial relations among existing sparsity - inducing methods and therefore to exploit , in an elegant way , the nice effect of dc decompositions of objective functions .",
    "the four algorithms can be viewed as an @xmath0-perturbed algorithm / reweighted-@xmath0 algorithm ( intimately related to the @xmath0-penalized lasso approach / reweighted-@xmath53 algorithm in case of convex objective functions .",
    "it specifies the flexibility / versatility of these theoretical and algorithmic tools .",
    "these results should enhance deeper developments of dc programming and dca , in order to efficiently model and solve real - world nonconvex sparse optimization problems , especially in the large - scale setting .",
    "baron , d. , wakin , m.b . ,",
    "duarte , m.f . ,",
    "sarvotham , s. & baraniuk , r.g ( 2009 ) . distributed compressed sensing .",
    "technical report ece06 - 12 , electrical and computer engineering department , rice university , november 2006 .",
    "fawzi , a. , davies , m. , & frossard , p. ( 2014 ) .",
    "dictionary learning for fast classification based on soft - thresholding . submitted to _ international journal of computer vision _ , http://arxiv.org/abs/1402.1973 .",
    ", le thi h.a . & nguyen , m.c .",
    "dca based algorithms for feature selection in semi - supervised support vector machines .",
    "_ machine learning and data mining in pattern recognition _ , petra perner ( ed ) , lnai 7988 , 528542            le thi , h.a .",
    "& pham dinh , t. ( 2002 ) .",
    "dc programming : theory , algorithms and applications .",
    "the state of the art ( 28 pages ) .",
    "proceedings of the first international workshop on global constrained optimization and constraint satisfaction ( cocos 02 ) , valbonne - sophia antipolis , france , october 2 - 4 .",
    "le thi , h.a .",
    ", pham dinh , t. & and nguyen van , t. ( 2002 ) .",
    "combination between local and global methods for solving an optimization problem over the efficient set .",
    "_ european journal of operational research _ , 142 , 258 - 270 .",
    "le thi , h.a . & pham dinh , t. ( 2005 ) .",
    "the dc ( difference of convex functions ) programming and dca revisited with dc models of real world nonconvex optimization problems . _",
    "annals of operations research _ , 133 , 2346 .",
    "le thi , h.a . , nguyen , t.p . & pham dinh , t. ( 2007 ) . a continuous approach for solving the concave cost supply problem by combining dca and b&b techniques .",
    "_ european journal of operational research _ , 183 , 10011012 .",
    "le thi , h.a .",
    ", le h.m . , nguyen , v.v & pham dinh , t. ( 2008 ) . a dc programming approach for feature selection in support vector machines learning .",
    "_ journal of advances in data analysis and classification _ , 2 , 259278 .",
    "le thi , h.a .",
    ", huynh , v.n . , & pham dinh , t. ( 2009 ) .",
    "convergence analysis of dc algorithms for dc programming with subanalytic data .",
    "research report , national institute for applied sciences , rouen 2009 http://www.optimization-online.org/db_html/2013/08/3996.html .",
    "le thi h.a . ,",
    "tran , d.q . & pham dinh , t. ( 2012 ) . a dc programming approach for a class of bilevel programming problems and its application in portfolio selection .",
    "_ numerical algebra , control and optimization _ ( naco ) , 1 , 167185 .",
    "le thi h.a . , moeini , m. , pham dinh , t. & joaquim , j. ( 2012 ) . a dc programming approach for solving the symmetric eigenvalue complementarity problem . _ computational optimization and applications , _ 51:3 , 10971117    le thi , h.a .",
    "& moeini , m. ( 2012 ) .",
    "long - short portfolio optimization under cardinality constraints by difference of convex functions algorithm .",
    "_ journal of optimization theory & applications _ , doi 10.1007/s10957 - 012 - 0197 - 0 october 2012 , 27 pages .",
    "le thi , h.a . , pham dinh , t. ( 2013 ) .",
    "dc programming approaches for distance geometry problems . in  distance geometry : theory , methods and applications  ,",
    "mucherino , a ; lavor , c ; liberti , l. ; maculan , n. ( eds ) , springer , 225290 .",
    "le thi , h.a . & nguyen m.c .",
    "efficient algorithms for feature selection in multi - class support vector machine . _ advanced computational methods for knowledge engineering _ , studies in computational intelligence 479 , springer .",
    "le thi , h.a . ,",
    "huynh , v.n . & pham dinh , t. ( 2014 ) .",
    "dc programming and dca for solving general dc programs .",
    "proceedings of 2nd international conference on computer science , applied mathematics and applications(iccsama 2014 ) , in press , ( 35 pages ) .",
    "mangasarian , o.l .",
    "machine learning via polyhedral concave minimization , in  applied mathematics and parallel computing ",
    "festschrift for klaus ritter  , h. fischer , b. riedmueller , s. schaeffler , editors , physica - verlag , germany , 175188 .",
    "mohri , m. , & medina , a.m. ( 2014 ) .",
    "learning theory and algorithms for revenue optimization in second - price auctions with reserve .",
    "_ proceeding icml14 proceedings of the 31th international conference on machine learning _ , http://arxiv.org/abs/1310.5665 .",
    "niu , y.s . ,",
    "pham dinh , t. , le thi h.a .",
    "& judice , j. ( 2012 ) .",
    "efficient dc programming approaches for asymmetric eigenvalue complementarity problem , optimization methods and software , doi:10.1080/10556788.2011.645543 , online first feabruary 2012 .",
    "pati , y.c . ,",
    "rezaifar , r. & krishnaprasa , p.s .",
    "orthogonal matching pursuit : recursive function approximation with applications to wavelet decomposition .",
    "_ 27th asilomar conf . on signals , systems and comput .",
    "_ , nov . 1993 .",
    "pham dinh , t. , nguyen canh , n. & le thi , h.a .",
    ". an efficient combination of dca and b&b using dc / sdp relaxation for globally solving binary quadratic programs .",
    "_ journal of global optimization _",
    ", 48:4 , 595632 .",
    "schmidt , m. , fung , g. & rosales , g. ( 2007 ) .",
    "fast optimization methods for l1 regularization : a comparative study and two new approaches .",
    "_ proceedings of machine learning : ecml 2007 _ , lecture notes in computer science , 4701 , 286297 .",
    "sriperumbudur , b.k . ,",
    "torres , d.a .",
    "& lanckriet , r.g .",
    "sparse eigen methods by d.c . programming .",
    "_ proceeding icml 07 , proceedings of the 24th international conference on machine learning _",
    ", 831 - 838 .",
    "takhar , d. , laska , j.n . ,",
    "wakin , m.b . ,",
    "duarte , m.f . ,",
    "baron , d. , sarvotham , s. , kelly , k.f .",
    "& baraniuk , r.g ( 2006 ) . a new compressive imaging camera architecture using optical - domain compression . _ computational imaging iv at is&t / spie electronic imaging _ , san jose , california , january 2006 .",
    "thiao , m. , pham dinh , t. & le thi , h.a .",
    "dc programming approach for solving a class of nonconvex programs dealing with zero - norm .",
    "_ modelling , computation and optimization in information systems ans management science , _ ccis 14 , 348357 , springer - verlag ."
  ],
  "abstract_text": [
    "<S> sparse optimization refers to an optimization problem involving the zero - norm in objective or constraints . in this paper , </S>",
    "<S> nonconvex approximation approaches for sparse optimization have been studied with a unifying point of view in dc ( difference of convex functions ) programming framework . considering a common dc approximation of the zero - norm including all standard sparse inducing penalty functions , we studied the consistency between global minimums ( resp . </S>",
    "<S> local minimums ) of approximate and original problems . </S>",
    "<S> we showed that , in several cases , some global minimizers ( resp . local minimizers ) of the approximate problem are also those of the original problem . using exact penalty techniques in dc programming , we proved stronger results for some particular approximations , namely , the approximate problem , with suitable parameters , is equivalent to the original problem . the efficiency of several sparse inducing penalty functions have been fully analyzed . </S>",
    "<S> four dca ( dc algorithm ) schemes were developed that cover all standard algorithms in nonconvex sparse approximation approaches as special versions . </S>",
    "<S> they can be viewed as , an @xmath0-perturbed algorithm / reweighted-@xmath0 algorithm / reweighted-@xmath0 algorithm . </S>",
    "<S> we offer a unifying nonconvex approximation approach , with solid theoretical tools as well as efficient algorithms based on dc programming and dca , to tackle the zero - norm and sparse optimization . as an application </S>",
    "<S> , we implemented our methods for the feature selection in svm ( support vector machine ) problem and performed empirical comparative numerical experiments on the proposed algorithms with various approximation functions .    </S>",
    "<S> global optimization , sparse optimization , dc approximation function , dc programming , dca , feature selection in svm </S>"
  ]
}