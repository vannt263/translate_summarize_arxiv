{
  "article_text": [
    "the real world can be viewed as a gigantic distributed system that evolves over time .",
    "an intelligent agent in this system can learn from two sources : examples from the environment , as well as information from other agents .",
    "one way to state the question addressed by the _ data - distributed online learning _ ( ddol ) schemes we introduce can be informally described as follows : within an interconnected network of learning agents , although an agent only receives @xmath1 samples of input data , can it be made to perform as if it has received @xmath2 samples ? here",
    "the performance is measured by generalization abilities ( prediction error or regret for the online setting ) .",
    "in other words , to what extent can an agent make fewer generalization errors by utilizing information from other online - learning agents ?",
    "this question can also be phrased another way . in recent years , the increasing ubiquity of massive datasets as well as the opportunities for distributed computing ( cloud computing , multi - core , etc . ) , have conspired to spark much interest in developing distributed algorithms for machine learning ( ml ) methods .",
    "while it is easy to see how parallelism can be obtained for most of the computational problems in ml , the question arises whether online learning , which appears at first glance to be inherently serial , can be fruitfully parallelized to any significant degree .",
    "while several recent papers have proposed distributed schemes , the question of whether significant speedups over the default serial scheme can be achieved has remained fairly open .",
    "theory establishing or disallowing such a possibility is particularly to be desired . to the best of our knowledge",
    ", this paper is the first work that answers these questions for the general online learning setting .    in this paper",
    "we show both theoretically and experimentally that significant speedups are possible in online learning by utilizing parallelism .",
    "we introduce a general framework for data - distributed online learning which encapsulates schemes such as weighted majority , online subgradient descent , and online exponentiated gradient descent .      in an empirical study @xcite , the authors proposed to make a trade - off between batch and stochastic gradient descent by using averaged mini - batches of size @xmath3 .",
    "a parameter averaging scheme was proposed in @xcite to solve a batch regularized conditional max entropy optimization problem , where the distributed parameters from each agent is averaged in the final stage .",
    "a distributed subgradient descent method was proposed in @xcite and an incremental subgradient method using a markov chain was proposed in @xcite . in @xcite ,",
    "a distributed dual averaging algorithm was proposed for minimizing convex empirical risks via decentralized networks .",
    "convergence rates were reported for various network topologies .",
    "the same idea of averaged subgradients was extended to centralized online learning settings in @xcite .",
    "the problem of multi - agent learning has been an active research topic in reinforcement learning . in this paper , we will focus on the online supervised learning setting .",
    "in this paper , we assume that each agent only has access to a portion of the data locally and communications with other agents .",
    "suppose we have @xmath0 learning agents . at round @xmath4",
    ", the @xmath5 learning agent @xmath6 receives an example @xmath7 from the environment and makes a prediction @xmath8 .",
    "the environment then reveals the correct answer @xmath9 corresponding to @xmath10 and the agent suffer some loss @xmath11 .",
    "the parameter set of an agent @xmath12 at time @xmath4 is @xmath13 .",
    "each agent is a vertex of a connected graph @xmath14 .",
    "there will be a bidirectional communication between @xmath12 and @xmath15 if they are neighbors ( connected by an edge @xmath16 ) .",
    "@xmath12 has @xmath17 neighbors .",
    "the generic meta - algorithm for data - distributed online learning ( ) is very simple : each agent @xmath12 works according to the following procedure :    @xmath12 makes local prediction(s ) on example(s ) @xmath10 ; @xmath12 @xmath18 using local correct answer(s ) @xmath9 ; @xmath12 @xmath18 with its neighbors and do over @xmath19 ;    to derive a distributed online learning algorithm , one need to specify the , and schemes . in the following sections",
    ", we will discuss how to use two classic online learning methods as the basic scheme , and how the combination with different schemes leads to different performance guarantees .",
    "we firstly propose two expert - advise - based online classification algorithms which can be regarded as distributed versions of the classic _ weighted majority algorithm _ ( wma ) @xcite . for simplicity",
    ", we assume that in both algorithms , all the experts are shared by all the agents , and each agent is adjacent to _ all _ the other agents ( @xmath20 is a complete graph ) .",
    "[ alg : dist_wm_imit ] is named _ distributed weighted majority by imitation _ ( ) . in the communication step , each @xmath12 * mimics * other agent s operations by penalizing each expert @xmath21 in the same way as any other agents do , and then makes a geometric averaging .",
    "initialize all weights @xmath22 of @xmath23 shared experts for agent @xmath12 to @xmath24 . given experts predictions @xmath25 over @xmath10 , @xmath12 predicts @xmath26 environment reveals @xmath27 for @xmath12 .",
    "@xmath28 @xmath29 .",
    "the following result gives the upper bound of the average number of mistakes by each agent , assuming that each agent is receiving information from all the other agents .",
    "[ thm : dist_wm_imit ] for algorithm [ alg : dist_wm_imit ] with @xmath0 agents and @xmath23 shared experts , @xmath30 , where @xmath31 is the number of mistakes by agent @xmath12 and @xmath32 is the minimum number of mistakes by the best expert over all agents so far .",
    "the proof essentially follows that of wma .",
    "the best expert @xmath33 makes @xmath34 mistakes over all agents so far .",
    "so for any @xmath12 , its weight of @xmath33 is @xmath35 . upon each mistake made by any agent , the total weights @xmath36 of @xmath12 decreases by a factor of at least @xmath37 .",
    "so the total weights for @xmath12 is at most @xmath38^{m_i}$ ] .",
    "therefore for any @xmath39 , @xmath40 .",
    "it follows that @xmath41 taking @xmath42 , @xmath43    comparing ( [ eq : dist_wm_imit_2 ] ) with the result of wma : @xmath44 , in the most optimistic case , if @xmath34 is of the same order as the number of mistakes made by the best expert @xmath33 in the _ single _ agent scheme , in other words , if @xmath33 makes @xmath45 error over all agents other than @xmath12 , then the upper bound is decreased by a factor of @xmath46 . in the most pessimistic case ,",
    "if @xmath33 makes exactly the same number of mistakes over _ all _ @xmath0 agents , then the upper bound is the same as with a single agent .",
    "this happens when all agents are receiving the same inputs and answers from the environment , hence there is no new information being communicated among the network and no communications are needed . in reality",
    ", @xmath34 falls between these two extremes .",
    "theorem [ thm : dist_wm_imit ] is stated from an individual agent point of view . from the social point of view",
    ", the total number of mistakes made by all agents @xmath47 is upper bounded by @xmath48 which is not larger than that in a single agent scheme ( @xmath49 can be ignored in comparing with the first term @xmath34 which could be very large in practice ) .",
    "imagine that @xmath50 samples are processed in the single agent scheme , while in the @xmath0 agents scheme , each @xmath12 process @xmath51 samples .",
    "in the most pessimistic case , upper bound ( [ eq : dwm_total_bound ] ) is the same for these two schemes .",
    "this is a very good property for parallel computing , since the proposed online dwm can achieve the same generalization capacity , while being @xmath0 times faster than a serial algorithm .",
    "this property is verified by the experiments in section [ sec : exp ] .    as in the randomized weighted majority algorithm ( rwm ) @xcite",
    ", we can introduce some randomness to our choices of experts by giving each expert a probability of being chosen depending on the performance of this expert in the past . specifically , in each round we choose an expert with probability @xmath52 .",
    "we can have a distributed randomized weighted majority and obtain a similar upper bound as that of rwm with a constant of @xmath46 as in theorem [ thm : dist_wm_imit ] .",
    "the upper bound ( [ eq : dist_wm_imit_1 ] ) can be further improved by an alternative algorithm ( alg . [ alg : dist_wm_avg ] ) , which differs from alg .",
    "[ alg : dist_wm_imit ] only in the way that each agent utilizes information received from others . instead of mimicking other agents operations ,",
    "an agent now updates its weights by arithmetically * averaging * together with all the weights it received from its neighbors .",
    "@xmath53 @xmath54 .",
    "@xmath53    [ thm : dist_wm_avg ] for algorithm [ alg : dist_wm_avg ] with @xmath0 agents and @xmath23 shared experts , @xmath55 , where @xmath31 is the number of mistakes by agent @xmath12 so far and @xmath56 is the minimum number of mistakes by the best expert at round @xmath4 over all agents .",
    "denote the weight of expert @xmath21 for agent @xmath12 at round @xmath4 as @xmath57 .",
    "indeed , @xmath58 \\bigg [ 1-\\frac{m_p^{t-1}(1-\\alpha)}{n}\\bigg]\\\\ & = \\cdots = n \\bigg [ 1-\\frac{m_p^t(1-\\alpha)}{n}\\bigg ] \\cdots \\bigg [ 1-\\frac{m_p^{1}(1-\\alpha)}{n}\\bigg ] .",
    "\\end{split}\\ ] ] using @xmath59 and the fact that @xmath60 , we have for any agent @xmath12 , @xmath61 \\geq \\exp\\bigg(\\sum_{t } \\frac{-m_p^t(1-\\alpha)}{n - m_p^t(1-\\alpha ) } \\bigg ) . \\end{split}\\ ] ] on the other hand , for any @xmath12 , @xmath62^{m_i}.\\ ] ] since @xmath63 , combining ( [ eq : dist_wm_avg_1 ] ) and ( [ eq : dist_wm_avg_2 ] ) , @xmath64^{m_i } \\geq \\exp\\bigg ( -\\sum_t \\frac{m_p^t(1-\\alpha)}{n - m_p^t(1-\\alpha ) } \\bigg).\\ ] ] it follows that @xmath65 @xmath66    now we are ready to compare the refined bound ( [ eq : dist_wm_avg_3 ] ) with ( [ eq : dist_wm_imit_1 ] ) using @xmath67 . without considering the @xmath68 part of the bounds which is much smaller than the @xmath34 part , it is easy to verify that if @xmath69 , then @xmath70 without any assumption on @xmath56 ; if @xmath71 , then the above inequality holds when @xmath72 the rhs of ( [ eq : dist_wm_avg_5 ] ) is lower bounded by @xmath73 . specifically , when @xmath74 and by taking @xmath42 , the difference in ( [ eq : dist_wm_avg_4 ] ) is @xmath75 hence the error bound in theorem @xmath76 is much lower",
    ". experimental evidence will be provided in section [ sec : exp ] .",
    "in this section we extend the idea of distributed online learning to online convex optimization ( oco ) problems .",
    "oco is an online variant of the convex optimization , which is ubiquitous in many machine learning problems such as support vector machines , logistic regression and sparse signal reconstruction tasks .",
    "each of these learning algorithms has a convex loss function to be minimized .",
    "one can consider oco as a repeated game between an algorithm @xmath77 and the environment . at each round @xmath4",
    ", @xmath77 chooses a strategy @xmath78 and the environment reveals a convex function @xmath79 . here",
    "we assume that all convex functions share the same feasible set @xmath80 .",
    "the goal of @xmath77 is to minimize the difference between the cumulation @xmath81 and that of the best strategy @xmath82 it can play in hindsight .",
    "this difference is commonly known as _ external regret _ , defined as below .",
    "the regret of convex functions @xmath83 for @xmath84 is defined as @xmath85 .    in",
    "the distributed setting , this game is played by every agent @xmath86 .",
    "the goal of @xmath12 is to minimize its own regret @xmath87 , named the _",
    "individual regret_. we call the sum of individual regrets @xmath88 _ social regret_.    we will present the online mirror descent ( omd ) framework which generalize many oco algorithms such as online subgradient descent @xcite , winnow @xcite , online exponentiated gradient @xcite , online newton s method @xcite .",
    "we firstly introduce some notations used in this section .",
    "a distance generating function @xmath89 is a continuously differentiable function that is @xmath90-strongly convex w.r.t .",
    "some norm @xmath91 associated with an inner product @xmath92 . using bregman divergence @xmath93 as a proximity function ,",
    "the update rule of omd can be expressed as @xmath94 , where @xmath95 is a subgradient of @xmath79 at @xmath96 and @xmath97 is a learning rate which plays an important role in the regret bound . denote the dual norm of @xmath91 as @xmath98 .",
    "suppose agent @xmath12 has @xmath17 neighbors .",
    "we propose distributed online mirror descent algorithm in alg .",
    "[ alg : dist_domd ] . in this algorithm",
    ", the update rule has explicit expressions for some special proximity functions @xmath99 .",
    "next we derive distributed update rules for two well - known omd examples : online gradient descent ( ogd ) and online exponentiated gradient ( oeg ) .",
    "initialize @xmath100 local prediction using @xmath18 . @xmath101 $ ] .",
    "taking @xmath102 ( i.e. the proximity is measured by squared euclidean distance ) , an agent @xmath12 needs to solve the minimization @xmath103 $ ] , which leads to a simple updating rule @xmath104      taking the unnormalized relative entropy as the proximity function @xmath105 , we can solve the minimization @xmath106 $ ] , and obtain the update rule for : @xmath107 if the feasible set @xmath80 is a simplex ball @xmath108 instead of @xmath109 , one only needs to do an extra normalization : @xmath110 if @xmath111 .",
    "updating rules ( [ eq : dogd ] ) and ( [ eq : doeg ] ) share the same spirit as stated in the meta - algorithm [ alg : dol ] : each agent updates its parameters @xmath112 individually , then it averages with its neighbors parameters , either arithmetically , or geometrically .",
    "the following results shows how this simple averaging scheme works , in terms of average _ individual regrets _",
    "as in theorem [ thm : dist_wm_imit ] and theorem [ thm : dist_wm_avg ] , for simplicity , we assume that the graph @xmath20 is complete , i.e. each agent has @xmath114 neighbors .    [",
    "lem : dol]@xcite let @xmath115 , for any @xmath116 and @xmath117 one has @xmath118    [ thm : dol ] if @xmath0 agents in algorithm [ alg : dist_domd ] are connected via a complete graph , @xmath79 are convex , distances between two parameter vectors are upper bounded @xmath119 , let @xmath120 , then the average individual regret @xmath121    since @xmath20 is complete , at a fixed @xmath4 , @xmath18 is the same for any @xmath39 . hence @xmath122",
    "= \\arg\\min_{\\mathbf{z}\\in\\mathcal{w } } \\big\\langle \\frac{\\eta_t}{n}\\sum_{j=1}^n \\mathbf{g}_j^t,\\ \\mathbf{z}-\\mathbf{w}_i^t \\big\\rangle + \\psi(\\mathbf{z } , \\mathbf{w}_i^t ) = p_{\\mathbf{w}_i^t}(\\frac{\\eta_t}{n}\\sum_{j=1}^n \\mathbf{g}_j^t)$ ] .",
    "let @xmath123 in ( [ eq : lem_dol ] ) , we have @xmath124 + \\frac{\\eta_t}{2a}\\big\\|\\frac{1}{n}\\sum_{j=1}^n \\mathbf{g}_j^t\\big\\|_*^2 . \\end{split}\\ ] ] using the convexity of @xmath79 and summing the above inequality over @xmath4 we have @xmath125   \\\\",
    "& \\leq \\sum_{t=1}^t \\big\\langle \\frac{1}{n}\\sum_{j=1}^n \\mathbf{g}_j^t,\\ \\mathbf{w}_i^t -\\mathbf{w}^ * \\big\\rangle \\leq \\frac{1}{\\eta_1}\\psi(\\mathbf{w}_i^1 , \\mathbf{w}^ * ) - \\\\ & \\frac{1}{\\eta_t}\\psi(\\mathbf{w}_i^{t+1 } , \\mathbf{w}^ * ) + \\sum_{2\\leq t\\leq t}\\left(\\frac{1}{\\eta_t } - \\frac{1}{\\eta_{t-1 } } \\right ) \\psi(\\mathbf{w}_i^t , \\mathbf{w}^ * ) + \\\\ & \\sum_{t=1}^t\\bigg ( \\frac{\\eta_t}{2an^2 } \\big\\| \\sum_{j=1}^n \\mathbf{g}_j^t \\big\\|_*^2\\bigg ) .",
    "\\end{split}\\ ] ] setting @xmath126 and using the assumption on the upper bound of @xmath99 we reach the result .    to appreciate the above theorem , we further assume that the subgradient is upper bounded : @xmath127 . in the most optimistic case , at a given round @xmath4 , if the subgradients @xmath128 are mutually orthogonal , then the second term of the upper bound ( [ eq : domd_bound ] ) can be bounded by @xmath129 , which is @xmath46 times smaller than using a single agent . in the most pessimistic case ,",
    "if all the subgradients @xmath128 are exactly the same , then the second term is bounded by @xmath130 , which is the same as in a single agent scheme .    according to the regret bound ( [ eq : domd_bound ] ) , the social regret @xmath131 . in the most optimistic case , the bound is @xmath132 . in the most pessimistic case",
    ", the bound becomes @xmath133 .",
    "imagine that @xmath50 samples need to be processed . in the single agent scheme ,",
    "they are accessed by only @xmath24 agent , while in the @xmath0 agents scheme , these @xmath50 samples are evenly distributed with each @xmath12 processing @xmath51 samples . in the most optimistic case ,",
    "the bound for the @xmath0 agent scheme is @xmath134 , while in the most pessimistic case , it is @xmath135 . in comparison ,",
    "the bound for the single agent scheme is @xmath136 .",
    "we can not draw an immediate conclusion of which one is better , since it depends on the correlations of examples , as well as @xmath137 and @xmath20 .",
    "but it is clear that the @xmath0 agent scheme is at most @xmath138 times larger in its social regret bound , while being @xmath0 times faster .",
    "in this section , several sets of online classification experiments will be used to evaluate the theories and the proposed distributed online learning algorithms .",
    "three real - world binary - class datasets from various application domains are adopted .",
    "table [ tab : datasets ] summarizes these datasets and the parameters used in section [ sec : exp_domd ] .",
    "-0.1 in    to simulate the behavior of multi - agents , we use pthreads ( posix threads ) for multi - threaded programming , where each thread is an agent , and they communicate with each other via the shared memory .",
    "barriers are used for synchronizations .",
    "all experiments are carried out on a workstation with a 4-core 2.7ghz intel core 2 quad cpu .      to evaluate the proposed dwm algorithms , the simplest decision stumps",
    "are chosen as experts , and all the experts are trained off - line .",
    "we randomly choose @xmath139 dimensions .",
    "within each dimension , @xmath140 probes are evenly placed between the min and max values of this dimension .",
    "the probe with the minimum training error over the whole dataset is selected as the decision threshold . in all the following weighted majority experiments ,",
    "we choose the penalty factor @xmath141 .",
    "the first set of experiments report the behaviors of and from the individual agent point of view .",
    "each agent share the same @xmath142 experts , and communicates with all the others .",
    "[ fig : dwm_i_svmguide1_1234_threads ] and [ fig : dwm_a_svmguide1_1234_threads ] depict the cumulative number of mispredictions made by each thread as a function of the number of samples accessed by a single agent , where @xmath24 , @xmath143 , @xmath144 and @xmath145 agents are compared .",
    "each plot in a subfigure represents an agent .",
    "it is clear that an agent @xmath12 makes fewer mistakes @xmath31 as it receives more information from its neighbors .",
    "with @xmath145 agents , @xmath31 is reduced by half comparing with the single agent case .",
    "this provides some evidence for the @xmath46 error reduction as stated in theorem [ thm : dist_wm_imit ] .",
    "-0.2 in        -0.2 in    as discussed in section [ sec : dwm ] , from the social point of view , with the same number of samples accessed , the bound ( [ eq : dwm_total_bound ] ) of the total number of mistakes made by all agents ( @xmath146 ) is almost as small as that in a single agent case .",
    "the comparisons for both and are illustrated in fig.[fig : dwm_ia_svmguide1_total_misp ] .",
    "this result is not surprising , since no more information is provided for multiple agents , and one should not hope that @xmath146 is much lower than @xmath147 .",
    "but on the other hand , the dwm algorithms achieve the same level of mistakes , while they are @xmath0 times faster .",
    "it can also be observed from fig .",
    "[ fig : dwm_i_svmguide1_1234_threads ] , [ fig : dwm_a_svmguide1_1234_threads ] and [ fig : dwm_ia_svmguide1_total_misp ] that makes slightly fewer mistakes than .     of mistakes over all agents . ]",
    "-0.2 in    to verify the tightness of bound ( [ eq : dist_wm_imit_1 ] ) and the refined ( [ eq : dist_wm_avg_3 ] ) , we compare the number of mistakes @xmath34 make by the best expert @xmath33 over all agents with that of a single agent @xmath31 .",
    "[ fig : dwm_ia_svmguide1_best_expert ] shows that with @xmath148 or @xmath149 agents , @xmath34 is around @xmath143 or @xmath149 times larger than @xmath31 , which means @xmath150 .",
    "however , choosing @xmath151 in bound ( [ eq : dist_wm_imit_1 ] ) leads to @xmath152 .",
    "this shows that the bound in theorem [ thm : dist_wm_avg ] is indeed tighter than theorem [ thm : dist_wm_imit ] .     of mistakes :",
    "best expert v.s .",
    "multi - agents . ]",
    "-0.2 in      in this section , several online classification experiments will be carried out using the proposed and algorithms . for",
    ", we choose the l2-regularized instance hinge loss function as our convex objective function : @xmath153 for , we take @xmath154 and @xmath155 . since the update rule ( [ eq : doeg ] ) can not change the signs of @xmath96 , we use a similar trick like @xmath156 proposed in @xcite , i.e. letting @xmath157 , where @xmath158 .",
    "since we will not compare the generalization capacities between these two algorithms , in all the following experiments , the parameters of @xmath159 and @xmath160 are chosen according to table [ tab : datasets ] without any further tuning .",
    "the subgradient of the non - smooth hinge loss is take as @xmath161 if @xmath162 and @xmath45 otherwise .",
    "we firstly illustrate the generalization capacities of and . since we do not know @xmath163 , it is not easy to calculate the individual regret or social regret",
    ". hence we only compare the number of mispredictions and the average accumulated objective function values as functions of the number of samples accessed by a single agent .",
    "the results are shown in fig .",
    "[ fig : dogd_misp_cod_14816_threads ] @xmath164 [ fig : doeg_obj_cod_12481632_threads ] .",
    "-0.2 in        -0.2 in    it is clear that for both and , the number of mispredictions decreases when more agents communicate with each other .",
    "the average objective values @xmath165 also decrease with the increasing number of agents @xmath0 .",
    "however , as shown in fig .",
    "[ fig : doeg_obj_cod_12481632_threads ] , when @xmath166 , the averaged @xmath165 is larger than @xmath167 .",
    "this might be due to the insufficient number of samples of the dataset cod - rna .",
    "this conjecture is experimentally verified in fig .",
    "[ fig : doeg_obj_covtype_12481632_threads ] , where the size of covtype is @xmath168 .",
    "-0.2 in        -0.2 in        -0.2 in    as discussed at the end of section [ sec : domd ] , the social regret bound of @xmath0 agents is at most @xmath138 times larger than that of a single agent scheme .",
    "the next set of experiments will be used to verify this claim .",
    "[ fig : doeg_obj_covtype_total ] depicts the result .",
    "-0.2 in    we can see that the total loss @xmath169 for @xmath170 is even lower than using a single agent .",
    "@xmath171 is slightly higher , but the difference is still much lower than the theoretical @xmath172 .",
    "this suggests that there might exist a bound tighter than ( [ eq : domd_bound ] ) .",
    "we proposed a generic data - distributed online learning meta - algorithm .",
    "as concrete examples , two sets of distributed algorithms were derived .",
    "one is for distributed weighted majority , and the other is for distributed online convex optimization .",
    "their effectiveness is supported by both analysis and experiments .",
    "the analysis shows that with @xmath0 agents , dwm can have an upper error bound that is @xmath46 lower than using a single agent . from the social point of view , the bound of total number of errors made by all @xmath0 agents is the same as using @xmath24 agent , while processing the same amount of examples .",
    "this indicates that dwm attains the same level of generalization error as wm , but is @xmath0 times faster .",
    "the average individual regret for domd algorithms is also much lower than omd , although it is not @xmath46 lower as in dwm . in the worst case , the bound of social regret",
    "is at most @xmath138 higher than using a single agent .    in follow - on work , two assumptions made in this paper",
    "will be removed to make the proposed algorithms more robust in practical applications .",
    "firstly , as discussed in @xcite , the connected graph @xmath20 does not need to be complete .",
    "we are working on distributed active learning and active teaching , which might lead to a data - dependent communication topology .",
    "secondly , the learning process should be fully asynchronous .",
    "this brings up the problem of ` delays ' in label feedbacks @xcite .",
    "moreover , for oco , with more structural information on @xmath79 rather than the black - box model , we might be able to find better distributed algorithms and achieve tighter bounds ."
  ],
  "abstract_text": [
    "<S> in this paper , we focus on the question of the extent to which online learning can benefit from distributed computing . </S>",
    "<S> we focus on the setting in which @xmath0 agents online - learn cooperatively , where each agent only has access to its own data . </S>",
    "<S> we propose a generic data - distributed online learning meta - algorithm . </S>",
    "<S> we then introduce the distributed weighted majority and distributed online mirror descent algorithms , as special cases . </S>",
    "<S> we show , using both theoretical analysis and experiments , that compared to a single agent : given the same computation time , these distributed algorithms achieve smaller generalization errors ; and given the same generalization errors , they can be @xmath0 times faster . </S>"
  ]
}