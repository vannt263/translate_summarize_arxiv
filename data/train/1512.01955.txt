{
  "article_text": [
    "in many geophysical applications , in particular in the petroleum industry and in hydrology , distributed parameter estimation problems are often solved by means of iterative ensemble kalman filters @xcite .",
    "the basic methodology is to introduce an artificial dynamical system , to supplement this with observations , and to apply the ensemble kalman filter .",
    "the methodology is described in a basic , abstract form , applicable to a general , possibly nonlinear , inverse problem in @xcite . in this basic form of the algorithm regularization",
    "is present due to dynamical preservation of a subspace spanned by the ensemble during the iteration .",
    "the paper @xcite gives further insight into the development of regularization for these ensemble kalman inversion methods , drawing on links with the levenberg - marquardt scheme @xcite . in this paper",
    "our aim is to further the study of filters for the solution of inverse problems , going beyond the ensemble kalman filter to encompass the study of other filters such as 3dvar and the kalman filter itself ",
    "see @xcite for an overview of these filtering methods",
    ". a key issue will be the implementation of regularization with the aim of deriving optimal error estimates .",
    "we focus on the linear inverse problem @xmath0 where @xmath1 is a compact operator acting between hilbert spaces @xmath2 and @xmath3 .",
    "the exact solution is denoted by @xmath4 and @xmath5 is a noise polluting the observations .",
    "we will consider two situations : * data model @xmath6 * where multiple observations are made in the form [ eq : base ] ; and * data model @xmath7 * where a single observation is made . for modelling purposes we will assume that the noise @xmath5 is generated by the gaussian @xmath8 , independently in the case of multiple observations .",
    "in each case we create a sequence @xmath9 ; for data model @xmath6 the elements of this sequence are i.i.d .",
    "@xmath10 whilst for data model @xmath7 they are @xmath11 , with @xmath12 a single draw from @xmath10 .",
    "the case where multiple independent observations are made is not uncommon in applications ( for example in electrical impedance tomography ( eit , @xcite ) and , although we do not pursue it here , our methodology also opens up the possibility of considering multiple instances with correlated observational noise , by means of similar filtering - based techniques .",
    "the artificial , partially observed linear dynamical system that underlies our methodology is as follows : @xmath13 in _ deriving _ the filters we apply to this dynamical system , it is assumed that the @xmath14 are i.i.d . from @xmath8 .",
    "note , however , that whilst the data sequence @xmath9 we use in data model @xmath6 is of this form , the assumption is not compatible with data model @xmath7 ; thus for data model @xmath7 we have a form of _ model error _ or _ model mis - specification _ @xcite .    by studying the application of filtering methods to the solution of the linear inverse problem",
    "our aim is to open up the possibility of employing the filtering methodology to ( static ) inverse problems of the form [ eq : base ] , and nonlinear generalizations .",
    "we confine our analysis to the linear setting as experience has shown that a deep understanding of this case is helpful both because there are many linear inverse problems which arise in applications , and because knowledge of the linear case guide methodologies for the more general nonlinear problem @xcite .",
    "the last few decades have seen a comprehensive development of the theory of linear inverse problems , both classically and statistically ",
    "see @xcite and the references therein . consider the tikhonov - phillips regularization method @xmath15",
    "this can be reformulated from a probabilistic perspective as the map estimator for bayesian inversion given a gaussian smoothness prior , with mean @xmath16 and cameron - martin space @xmath17 compactly embedded into @xmath2 , and a gaussian noise model as defined above ; this connection is eludicated in @xcite .",
    "we note that from the point of view of tikhonov - phillips regularization only the parameter @xmath18 is relevant , but that each of @xmath19 and @xmath20 have separate interpretations in the overarching bayesian picture , the first as a scaling of the prior precision and the second as observational noise variance . in this paper",
    "we deepen the connection between the bayesian methodology and classical methods .",
    "the recent paper @xcite opens up the prospect for a statistical explanation of iterative regularization methods in the form of @xmath21 with a general kalman gain operator @xmath22 . in this paper",
    ", we establish the connection between iterative regularization methods ( c.f .",
    "@xcite ) and filter based methods @xcite with respect to an artificial dynamic system .",
    "more precisely , for a linear inverse problem , we verify that the iterative tikhonov regularization method @xmath23 is closely related to filtering methods such as 3dvar and the kalman filter when applied to the partially observed linear dynamical system [ eq : dynamic ] .",
    "the similarity between both schemes provides a probabilistic interpretation of iterative regularization methods , and allows the possibility of quantifying uncertainty via the variance . on the other hand",
    ", we will employ techniques from the convergence analysis arising in regularization theories @xcite to shed light on the convergence of filter based methods , especially when the linear observation operator is ill - posed .",
    "the paper is organized as follows .",
    "we first introduce filter based methods for the artificial dynamics [ eq : dynamic ] in section [ se_artidyna ] .",
    "section [ se_assumptions ] describes some general useful formulae which are relevant to all the filters we study , and lists our main assumptions on the inverse problem of interest . in sections [ se_kalman ] and [ se_3dvar ] respectively , detailed asymptotic analyses are given for the kalman filter method and 3dvar , for both data models .",
    "the final section [ se_num ] presents numerical illustrations confirming the theoretical predictions .",
    "recall the artificial dynamics , where the observation operator @xmath1 also defines the inverse problem , and @xmath14 is an i.i.d .",
    "sequence with @xmath24 .",
    "the aim of filters is to estimate @xmath25 given the data @xmath26 in particular , probabilistic filtering aims to estimate the probability distribution of the conditional random variable @xmath27    if we assume that @xmath28 then the desired conditional random variable is gaussian , because of the linearity inherent in , together with the assumed gaussian structure of the noise sequence @xmath14 .",
    "furthermore the independence of the elements of the noise sequence means that the gaussian can be updated sequentially in a markovian fashion .",
    "if we denote by @xmath29 the mean , and by @xmath30 the covariance , then we obtain the kalman filter updates for these two quantities :    @xmath31    the operator @xmath32 is known as the _ kalman gain _ and the inverse of the covariance , the _ precision operator _",
    "@xmath33 , may be shown to satisfy @xmath34 all of these facts concerning the kalman filter may be found in chapter 4 of @xcite .",
    "expression requires careful justification in infinite dimensions , and this is provided in @xcite in certain settings",
    ". however we will only use as a quick method for deriving useful formulae , not expressed in terms of precision operators , which can be justified directly under the assumptions we make .",
    "a simplification of the kalman filter method is the 3dvar algorithm @xcite which is not , strictly speaking , a probabilistic filter because it does not attempt to accurately track covariance information . instead",
    "the covariance is fixed in time at @xmath35 for some fixed positive and self - adjoint operator @xmath36 the parameter @xmath19 is a scaling constant the inverse of which measures the relative size of the fixed covariance of the filter relative to that of the data .",
    "imposing this simplification on equations , gives    [ eq:3dvarn ] @xmath37    it is also helpful to define , from , @xmath38 notice @xcite that the iteration looks like a stationary iterative tikhonov method with @xmath1 replaced by @xmath39    throughout the paper @xmath40 stands for kalman gain , updated mean and updated covariance for the kalman filter method and @xmath41 is the related sequence of quantities for 3dvar .",
    "we will view the filters as methods for reconstructing the truth @xmath42 ; in particular we will study the proximity of @xmath29 ( for the kalman filter ) and @xmath43 ( for 3dvar ) to @xmath42 for various large @xmath44 asymptotics .",
    "although the assumption in the _ derivation _ of the filters is that @xmath45 is an i.i.d .",
    "sequence of the form @xmath46 , we will not always assume that the data available is of this form ; to be precise data model @xmath6 is compatible with this assumption whilst data model @xmath7 is not .",
    "recall that data model @xmath6 refers to the situation where the data used in the kalman and 3dvar filters has the form @xmath47 , where the @xmath48 are i.i.d .",
    "@xmath49 given such a data sequence we can generate an auxiliary element @xmath50 with @xmath51 and @xmath52 .",
    "the law of large numbers and central limit theorem thus allows us to consider an inverse problem of the form with noise level reduced by a factor of @xmath53 we will study , in the sequel , whether the kalman or 3dvar filters are able to automatically exploit the decreased uncertainty inherent in an i.i.d .",
    "data set of this form .    for data model @xmath7",
    "we simply assume that the data used in the filters is of the form @xmath11 where @xmath12 is given by with @xmath54 from the discussion in the preceding paragraph , we clearly expect less accurate reconstruction in this case . for this data model",
    "we may view 3dvar as a stationary iterative tikhonov regularization , whilst the kalman filter is an alternative iterative non - stationary regularization scheme , since @xmath32 is updated in each step .",
    "in addition , the statistical perspective not only allows us to obtain an estimator ( the mean ( [ eq : mean ] ) or ( [ eq:3dvarmean ] ) ) , but also in the case of the kalman filter method , to quantify the uncertainty ( via the covariance ( [ eq : covariance ] ) ) .",
    "this uncertainty quantification perspective provides additional motivation for the filtering approaches considered herein .    in this paper",
    "our primary focus is the asymptotic large @xmath44 behavior of the kalman filter method and 3dvar .",
    "more precisely , we are interested in the accurate recovery of the true state @xmath55 when the noise variance vanishes , i.e. @xmath56 for data models @xmath6 and @xmath7 , or as @xmath57 for data model @xmath6 ( by the law of large numbers / central limit theorem discussion above ) .",
    "to highlight the difficulties inherent in ill - posed inverse problems in this regard , we note the following which is a straightforward consequence of theorem 4.10 in @xcite when specialized to linear problems . here , and in what follows , @xmath58 denotes both the norm on @xmath2 and the operator norm from @xmath2 into itself .",
    "consider the 3dvar algorithm with @xmath59 .",
    "assume that there exists a constant @xmath60 such that @xmath61 and that @xmath62 then , for data model @xmath7 , it yields @xmath63    note however , that if the observation operator @xmath1 is compact or the inversion is ill - posed , the assumption @xmath64 in the preceding proposition can not hold .",
    "more precisely , the operator @xmath65 is no longer contractive since the spectrum of the operator @xmath66 clusters at the origin .",
    "our focus in the remainder of the paper will be on such ill - posed inverse problems .",
    "recall that @xmath58 denotes both the norm on @xmath2 and the operator norm from @xmath2 into itself . throughout @xmath67",
    "will denote a generic constant , independent of the key limiting quantities @xmath20 and @xmath44 .",
    "our main assumption is :    [ assp_main ] for both the kalman filter and the 3dvar filter , we assume    ( i ) : :    the variance @xmath68 and    @xmath69 ,    where @xmath19 is a positive constant and    @xmath70 is positive self - adjoint , and    @xmath71 is a densely defined unbounded    self - adjoint strictly positive operator ; ( ii ) : :    the forward operator @xmath1 satisfies    @xmath72    on @xmath2 for some constants @xmath73 and    @xmath74 ; ( iii ) : :    the initial mean satisfies    @xmath75    ( or    @xmath76 )    with @xmath77 ; ( iv ) : :    the operator @xmath70 in item ( i ) is trace class on    @xmath2",
    ".    we briefly comment on these items .",
    "item @xmath78 allows a well defined operator @xmath79 which is essential in carrying out our analysis .",
    "item @xmath80 is often called the _ link condition _ and it connects both operators @xmath1 and @xmath70 ( or @xmath81 ) .",
    "the third item @xmath82 is the _ source condition _ ( regularity ) of the true solution @xcite . the final item @xmath83 makes @xmath81 a well - defined covariance operator on @xmath2 @xcite .",
    "item @xmath80 in the preceding assumption is automatically satisfied if @xmath84 and @xmath70 have the same eigenfunctions and certain decaying singular values .",
    "item @xmath82 can then be expressed in this eigenbasis . when studying the kalman filter we will , in some instances , employ the following specific form of items @xmath80 , @xmath82 .",
    "comparison of assumptions [ assp_main ] and [ assp_main2 ] we see that they are identical if @xmath85 and @xmath86    [ assp_main2 ] let the variance @xmath87 .",
    "the operators @xmath70 and @xmath84 have the same eigenfunctions @xmath88 with their eigenvalues @xmath89 and @xmath90 satisfying @xmath91 for some @xmath92 , @xmath93 and @xmath94 . furthermore , by choosing the initial mean @xmath95 , the true solution @xmath42 with its coordinates @xmath96 in the basis @xmath88 obeys @xmath97 .",
    "we start by deriving properties of the kalman filter method under data model @xmath6 .",
    "other cases can be simply derived from minor variants of this setting .",
    "recall from ( [ eq : mean ] ) @xmath98 and note that @xmath99 under data model @xmath6 we have @xmath100 and hence the total error @xmath101 satisfies @xmath102 here @xmath103    to establish a rigorous convergence analysis , the mean squared error ( mse ) @xmath104 is of particular interest . since @xmath55 is deterministic and",
    "each @xmath48 is i.i.d gaussian we obtain a bias - variance decomposition of the mse : @xmath105 to proceed further , both terms @xmath106 and @xmath107 need to be calibrated more carefully .",
    "we consider the operator @xmath108 which appears in both terms . by ( [ eq : covariance ] )",
    ", we obtain @xmath109 which is equivalent to @xmath110 notice that ( [ eq : preci ] ) yields @xmath111 by ( [ eq : prod ] ) and ( [ eq : recur_preci ] ) we obtain @xmath112 we will use this expression ( which is well - defined in view of assumption [ assp_main ] @xmath78 ) and the labelled equations preceding it in this subsection , frequently in what follows .",
    "in this section we investigate the asymptotic behaviour of the kalman filter ( [ eq : kgain])-([eq : covariance ] ) , under assumption [ assp_main ] . in particular , we are interested in whether we can reproduce the minimax convergence rate .",
    "this minimax rate is achieved by adopting assumption [ assp_main ] in the diagonal form of assumption [ assp_main2 ] .",
    "we present the main results in current subsection .",
    "[ thm_kfdm1 ] let assumption [ assp_main ] hold . then the kalman filter method ( [ eq : kgain])-([eq : covariance ] ) yields a bias - variance decomposition of the mse @xmath113 for the data model @xmath6 . setting @xmath114 and stopping the iteration when @xmath115 then gives @xmath116    [ thm_kfdm1ass2 ]",
    "let assumption [ assp_main2 ] hold .",
    "then the kalman filter method ( [ eq : kgain])-([eq : covariance ] ) yields a bias - variance decomposition of the mse @xmath117 for the data model @xmath6 . setting @xmath118 and stopping the iteration when @xmath115 then gives the following minimax convergence rate : @xmath119    * \\(i ) under the assumption [ assp_main2 ] we prove unconditional convergence of the kalman filter method for any fixed @xmath120 and @xmath121 , noticing that both the bias and variance vanish when @xmath44 goes to infinity .",
    "the key ingredient which leads to this unconditional convergence , in comparison with assumption [ assp_main ] , is that the rate of decay of the eigenvalues of the variance operator @xmath70 is made explicit under assumption [ assp_main2 ] ; this is to be contrasted with the weaker assumption @xmath122 made in assumption [ assp_main ] @xmath83 . *",
    "\\(ii ) by choosing @xmath19 depending on the update step @xmath123 , again with fixed @xmath20 , both theorems [ thm_kfdm1 ] and [ thm_kfdm1ass2 ] yield convergence rates in the mse sense .",
    "indeed in the second case , where we use assumption [ assp_main2 ] , the minimax rate of @xmath124 is achieved .",
    "this minimax rate may also be achieved from the bayesian posterior distribution with appropriate tuning of the prior in terms of the ( effective ) noise size @xmath125 @xcite ; the tuning of the prior is identical to the tuning of the initial condition for the covariance @xmath81 , via choice of @xmath126 @xmath127    proof of theorems [ thm_kfdm1 ] and [ thm_kfdm1ass2 ] is straightforward by means of a bias - variance decomposition .",
    "let assumption [ assp_main ] @xmath78 hold , noting that then @xmath128 is well - defined .",
    "we thus obtain , by ( [ eq : kgexplicit ] ) , @xmath129 where @xmath130 the operator - valued function @xmath131 ( [ eq_rn ] ) has been verified to be powerful in the convergence analysis of deterministic regularization schemes  see ( * ? ? ?",
    "* ch.2 ) . in that context",
    "the following inequality is useful : @xmath132 , \\quad 0\\leq t \\leq 1.\\end{aligned}\\ ] ]    following these ideas we obtain the next two lemmas describing the bias and variance error bounds .",
    "we leave the proofs of both lemmas to the appendix .",
    "theorems [ thm_kfdm1 ] and [ thm_kfdm1ass2 ] are consequences , by choosing the parameter @xmath19 appropriately .",
    "[ lemma_kalmanbias ] let assumption [ assp_main ] @xmath78-@xmath82 hold . then the kalman filter method ( [ eq : kgain])-([eq : covariance ] ) yields @xmath133 furthermore , if assumption [ assp_main2 ] is valid , the bias obeys @xmath134    [ lemma_kalmanvariance ] let assumption [ assp_main ] @xmath78 , @xmath83 hold and @xmath135 in ( [ eq : dynamic ] ) be an i.i.d sequence with @xmath136 .",
    "then the kalman filter method ( [ eq : kgain])-([eq : covariance ] ) yields @xmath137 furthermore ,",
    "if assumption [ assp_main2 ] is valid , the variance obeys @xmath138      the key difference between data model @xmath7 and data model @xmath6 is that , in the case @xmath7 , the noises @xmath48 appearing in the expression for the term @xmath107 are identical , rather than i.i.d .",
    "mean zero as in case @xmath6 .",
    "this results in a reduced rate of convergence in case @xmath7 over case @xmath6 , as seen in the following two theorems :    [ thm_kfdm2 ] let assumption [ assp_main ] hold . then the kalman filter method ( [ eq : kgain])-([eq : covariance ] )",
    "yields a bias - variance decomposition of the mse @xmath139 for the data model @xmath7 .",
    "fix @xmath140 and assume that the noise variance @xmath141 .",
    "if the iteration is stopped at @xmath115 then the following convergence rate is valid : @xmath142    [ thm_kfdm2ass2 ] let assumption [ assp_main2 ] hold .",
    "then the kalman filter method ( [ eq : kgain])-([eq : covariance ] ) yields a bias - variance decomposition of the mse @xmath143 for the data model @xmath7 .",
    "fix @xmath140 and assume that the noise variance @xmath144 if the iteration is stopped at @xmath115 then the following convergence rate is valid : @xmath145    both convergence rates in theorems [ thm_kfdm2 ] and [ thm_kfdm2ass2 ] are of the same order since the variance has been tuned to scale in the same way as the bias by choosing to stop the iteration at @xmath123 , depending on @xmath146 , appropriately . in comparison with the convergence rates in theorems [ thm_kfdm1 ] and [ thm_kfdm1ass2 ] , the ones in this section under data model @xmath7 require small noise @xmath20",
    "; those in the preceding subsection do not because multiple observations , with additive independent noise , are made of @xmath147 proof of the two preceding theorems is straightforward : the @xmath106 terms is analyzed as in the preceding subsection and the @xmath107 term must be carefully analyzed under the assumptions of data model @xmath7 .",
    "the key new result is stated in the following lemma , whose proof may be found in the appendix .",
    "[ lemma_kalmanvariancedm2 ] let assumption [ assp_main ] hold and each observation @xmath148 be fixed",
    ". then the kalman filter method ( [ eq : kgain])-([eq : covariance ] ) yields @xmath149 furthermore , if assumption [ assp_main2 ] is valid , the variance obeys @xmath150",
    "the mean of the 3dvar algorithm is given by ( [ eq:3dvarmean ] ) and has the form @xmath151 furthermore @xmath152 if we define @xmath153 then we obtain , since @xmath100 , @xmath154 with @xmath155 .",
    "we further derive @xmath156 by inserting the definition of the kalman gain ( [ eq:3dvarkgain ] ) , and by assuming assumption [ assp_main ] @xmath78 .",
    "the operator - valued function @xmath157 is defined @xmath158 similarly to the analysis of the kalman filter , we derive @xmath159 thus , the mse takes the bias - variance decomposition form @xmath160 this leads to the following two theorems :    [ thm_3dvardm1 ] let assumption [ assp_main ] hold . then 3dvar filter ( [ eq:3dvarkgain])-([eq:3dvarmean ] ) yields a bias - variance decomposition of the mse @xmath161 for the data model @xmath6 . setting @xmath114 and stopping the iteration when @xmath115 then gives @xmath162    [ thm_3dvardm1ass2 ]",
    "let assumption [ assp_main2 ] hold .",
    "then 3dvar filter ( [ eq:3dvarkgain])-([eq:3dvarmean ] ) yields a bias - variance decomposition of the mse @xmath163 for the data model @xmath6 . setting @xmath164 and stopping the iteration when @xmath115 then gives @xmath165",
    "the decay rate at the end of the preceding theorem is the same as that in theorem [ thm_3dvardm1 ] if @xmath85 and @xmath166 .",
    "this is the setting in which assumptions [ assp_main ] and [ assp_main2 ] are identical .",
    "the preceding two theorems show that , under data model @xmath6 and for any fixed @xmath19 , the ( bound on the ) mse of the 3dvar filter blows up logarithmically as @xmath57 under assumption [ assp_main ] , and is asymptotically bounded for assumption [ assp_main2 ] .",
    "in contrast , for the kalman filter method the mse is asymptotically bounded or unconditionally converges in @xmath44 under the same assumptions ",
    "see theorems [ thm_kfdm1 ] and [ thm_kfdm1ass2 ] .    with optimal choice of @xmath19 in terms of the stopping time of the iteration at @xmath115 , comparison of the convergence rates in theorems [ thm_kfdm1 ] and",
    "[ thm_3dvardm1 ] ( or theorems [ thm_kfdm1ass2 ] and [ thm_3dvardm1ass2 ] ) shows that the kalman filter outperforms 3dvar , but only by a logarithmic factor ( or a hlder factor ) . for simplicity",
    "we only analyze and discuss data model @xmath6 for 3dvar filter under the additional assumption [ assp_main2 ] ; as for data model @xmath7 one can derive consequences similar to those in the preceding section in an analogous manner .",
    "@xmath127    we now study data model @xmath7 and 3dvar .",
    "we consider only assumption [ assp_main ] ; however the reader may readily extend the analysis to include assumption [ assp_main2 ] . in the case of data model @xmath7 ,",
    "both the kalman and 3dvar filters have the same error bounds :    [ thm_3dvardm2 ] let assumption [ assp_main ] hold . then the 3dvar algorithm ( [ eq:3dvarkgain])-([eq:3dvarmean ] ) yields a bias - variance decomposition of the mse @xmath167 for the data model @xmath7 .",
    "fix @xmath140 and assume that the noise variance @xmath141 .",
    "if the iteration is stopped at @xmath115 then the following convergence rate is valid : @xmath168    the preceding three theorems can be proved by the bias - variance decomposition and application of the following three lemmas , whose proofs are left to the appendix .",
    "however the proof of theorem [ thm_3dvardm1ass2 ] is not as straightforward as the others and we present details in the appendix .",
    "[ lemma_3dvarbias ] let assumption [ assp_main ] @xmath78-@xmath82 hold .",
    "then 3dvar ( [ eq:3dvarn ] ) yields @xmath169 furthermore , if assumption [ assp_main2 ] is valid , the bias obeys @xmath170    [ lemma_3dvarvariancedm1 ] let assumption [ assp_main ] @xmath78 , @xmath83 hold and each noise @xmath48 in ( [ eq : dynamic ] ) i.i.d . generated by @xmath171 .",
    "then 3dvar ( [ eq:3dvarn ] ) yields @xmath172 for data model @xmath6 . furthermore ,",
    "if assumption [ assp_main2 ] is valid , the variance obeys @xmath173 and simultaneously @xmath174    [ lemma_3dvarvariancedm2 ] let assumption [ assp_main ] hold and each observation @xmath148 be fixed . then",
    "3dvar ( [ eq:3dvarn ] ) yields @xmath175 for data model @xmath7 .",
    "recall that the 3dvar iteration looks like a stationary iterative tikhonov method @xcite , with @xmath1 replaced by @xmath176 and with a fixed parameter @xmath19 .",
    "the non - stationary iterative tikhonov regularization , with varying @xmath19 , has been proven to be powerful in deterministic inverse problems @xcite .",
    "we generalize this method to the 3dvar setting .",
    "furthermore we demonstrate that , for data model @xmath6 , it is possible to see blow - up phenomena with this algorithm .",
    "starting from the classical form of the 3dvar filter as given in , , we propose a variant method in which @xmath19 varies with @xmath44 .",
    "we obtain    @xmath177    if we define @xmath178 then we obtain , analogously to the derivation of ( [ eq : generaldecomposition ] ) and ( [ eq:3dvarerror ] ) , the bias - variance decomposition as follows : @xmath179 with @xmath180    by calibrating both terms @xmath181 , @xmath182 carefully , we obtain the following blow - up result , proved in the appendix . although the result only provides an upper bound , numerical evidence does indeed show blow - up in this regime .",
    "[ thm_3dvarvariant ] let assumption [ assp_main ] hold and let @xmath183 be a sequence satisfying @xmath184 , with constant @xmath185 , for @xmath186 then the variant enkf method ( [ eq : variantkgain])-([eq : variantrcovariance ] ) yields a bias - variance decomposition of the mse @xmath187 for data model @xmath188 in particular , the geometric sequence @xmath189 with @xmath120 and @xmath190 yields @xmath191",
    "in this section we provide numerical results which display the capabilities of 3dvar and kalman filter for solving linear inverse problems of the type described in section [ se_intro ] .",
    "in addition , we verify numerically some of the theoretical results from section [ se_kalman ] and section [ se_3dvar ] .",
    "we consider the two - dimensional domain @xmath192 ^ 2 $ ] and define the operators @xmath193 with @xmath194 with this domain @xmath195 is positive , self - adjoint and invertible.note that ( [ eq : ca ] ) from assumption [ assp_main ] is satisfied with @xmath196 and @xmath197 . in the following subsections",
    "we produce synthetic data from a true function @xmath198 that we generate as a two - dimensional random field drawn from the gaussian measure @xmath199 with covariance @xmath200 with domain of definition @xmath201 and where @xmath202 is selected as described below .",
    "the shift of @xmath195 by a constant introduces a correlation length into the true function .",
    "furthermore , for simplicity we consider @xmath203 and note @xcite that the given selection of @xmath198 yields @xmath204 for all @xmath205 .",
    "therefore , for discussion of the present experiments we simply assume that @xmath206 . consequently ,",
    "in order to satisfy assumption [ assp_main ] ( iii ) we need @xmath207 such that @xmath208 .",
    "note that operator @xmath70 in ( [ num1 ] ) satisfies assumption [ assp_main ] ( iv ) .",
    "the numerical generation of @xmath198 is carried out by means of the karhunen - loeve decomposition of @xmath198 in terms of the eigenfunctions of @xmath209 which , from the definition of @xmath210 , are cosine functions .",
    "we recall that for the application of the kalman filter and 3dvar with data model 1 we need to generate @xmath123 instances of synthetic data @xmath211 where @xmath123 is the maximum number of iterations of the scheme .",
    "below we discuss the selection of such @xmath123 .",
    "the aforementioned synthetic data are generated by means of @xmath212 with @xmath213 and @xmath20 specified below .",
    "for data model 2 we produce synthetic data simply by setting @xmath214 with @xmath215 . in order to avoid inverse crimes @xcite ,",
    "all synthetic data used in our experiments are generated on a finer grid ( @xmath216 cells ) than the one ( of @xmath217 cells ) used for the inversion .",
    "we use splines to interpolate synthetic data on the coarser grid that we use for the application of the filters .    for both 3dvar and kalman filter with data model 1",
    ", we fix the number of iterations @xmath123 for the scheme and consider the selection @xmath218 stated in theorem [ thm_kfdm1 ] and theorem [ thm_3dvardm1 ] .",
    "provided that the filters are stopped according to @xmath115 , these theorems ensure the convergence rates in ( [ rate1_kf ] ) and ( [ rate1_3dvar ] ) that we verify numerically in the following subsection . for data model 2 , theorem [ thm_kfdm2 ] and theorem [ thm_3dvardm2 ] suggest that convergence rates ( [ rate2_kf ] ) and ( [ rate2_3dvar ] ) are satisfied for @xmath140 and provided that @xmath141 .",
    "the latter equality provides an expression for the number of iterations @xmath219 that we may use as stopping criteria for these filtering algorithms applied to data model 2 . in algorithm",
    "[ al1 ] we summarize the kalman filter and 3dvar schemes applied to both data models .",
    "kalman filter/3dvar ( data model @xmath220data model @xmath7)[al1 ]   + let @xmath221 and @xmath222 for @xmath223 , update @xmath224 and @xmath225 as follows @xmath226 @xmath227 where @xmath228 and where we recall that @xmath229 note that for data model @xmath6 we need at least @xmath123 independent instances of data .      in this subsection",
    "we demonstrate how the filters under consideration in the iterative framework described in algorithm [ al1 ] can be used , with data model 1 and data model 2 , to solve the linear inverse problem presented in section [ se_intro ] .",
    "let us consider the truth @xmath198 displayed in figure [ fig1 ] ( top ) generated , as described in subsection [ se_num : setup ] , from a gaussian measure with covariance ( [ num3 ] ) and @xmath230 .",
    "synthetic data are generated as described above with three different choices of @xmath20 that yield noise levels of approximately @xmath231 , @xmath232 , and @xmath233 of the norm of the noise free data ( i.e. @xmath234 ) .",
    "we apply algorithm [ al1 ] to this synthetic data generated both for the application of data model @xmath6 and data model @xmath7 . for data model @xmath6",
    "we consider a selection of @xmath235 .",
    "algorithm [ al1 ] states that the these schemes should be stopped at iteration level @xmath115 .",
    "however , in order to observe the performance of these schemes , in these experiments we allowed for a few more iterations . in the left column of figure [ fig2 ]",
    "we display the plots of the error w.r.t the truth of the estimator @xmath236 as a function of the iterations , i.e. @xmath237 where @xmath238 denotes the interpolation of @xmath198 on the coarse grid used for the inversion .",
    "note that the error w.r.t .",
    "the truth of the estimates produced by both schemes decreases monotonically .",
    "interestingly , the value at the final iteration displayed in these figures is approximately the same for all these experiments independently of the noise level .",
    "moreover , the stability of the scheme does not seem to depend on the early termination of the scheme .",
    "in addition , we note that both kalman filter and 3dvar exhibit very similar performance in terms of reducing the error w.r.t the truth .",
    "however , for larger noise levels we observe small fluctuations in the error obtained with 3dvar .",
    "in figure [ fig1 ] we display the estimates @xmath29 obtained with 3dvar with data model @xmath6 at iterations @xmath239 for noise levels ( determined by the choice of @xmath20 ) of @xmath231 ( top - middle ) , @xmath232 ( middle - bottom ) and @xmath233 ( bottom ) .",
    "we can visually appreciate from figure [ fig1 ] that the estimates obtained at the final iteration @xmath240 are indeed similar one to another even though the one in the bottom row was computed by inverting data five times noisier than the one in the first row .",
    "similar estimates ( not shown ) were obtained with the kalman filter for data model @xmath6 .    for data model @xmath7 , the selection of @xmath20 corresponding to noise levels of @xmath231 , @xmath232 , and",
    "@xmath233 yields a maximum number of iterations @xmath241 respectively .",
    "clearly , for data model @xmath7 , smaller observational noise results in schemes that can be iterated longer , presumably achieving more accurate estimates .",
    "similarly to data model @xmath6 , we are required to stop the algorithm at the iteration @xmath115 . however , for the purpose of this study we iterate until @xmath240 . in the right column of figure [ fig2 ]",
    "we display the plots of the error w.r.t the truth of @xmath29 .",
    "in contrast to data model @xmath6 , we observe that the error w.r.t the truth increases for @xmath242 . in other words , the kalman filter and 3dvar , when applied to data model @xmath7 , need to be stopped at @xmath115 in order to stabilize the scheme and obtain accurate estimates of the truth . moreover , stopping the scheme at @xmath115 results in estimates whose accuracy increases with smaller noise levels . clearly , in the small noise limit",
    ", both data models tend to exhibit similar behaviour . in figure [ fig3 ]",
    "we display @xmath29 obtained from 3dvar applied to data model @xmath7 at iterations @xmath239 for data with noise levels of @xmath231 ( top - middle ) , @xmath232 ( middle - bottom ) and @xmath233 ( bottom ) .",
    "similar estimates ( not shown ) were obtained with the kalman filter for data model @xmath7 .",
    "it is clear indeed , that for data model @xmath6 , the application of kalman filter and 3dvar results in more accurate and stable estimates when the noise level in the data is not sufficiently small . the results from this subsection show that the reduction in the variance of the noise due to the law of large numbers and central limit theorem effect in data model @xmath6 produces more accurate algorithms .",
    "although data model @xmath6 requires multiple instances of the data , in some applications such as in eit @xcite , the data collection can be repeated multiple times in order to obtain these data .    .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) .",
    ", title=\"fig : \" ] + .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) .",
    ", title=\"fig : \" ] . top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) .",
    ", title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) .",
    ", title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) . , title=\"fig : \" ] + .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) .",
    ", title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) .",
    ", title=\"fig : \" ] . top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) .",
    ", title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) .",
    ", title=\"fig : \" ] + .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) .",
    ", title=\"fig : \" ] . top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) .",
    ", title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) .",
    ", title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 1 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) .",
    ", title=\"fig : \" ]      +     +        .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ] + .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ] + .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ] + .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ] . top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ] .",
    "top - middle , bottom - middle and bottom : estimates obtained with 3dvar and data model 2 at iterations ( from left to right ) 1 , 10 , 20 , 30 ) for noise levels of @xmath231 ( top - middle ) , @xmath232 ( bottom - middle ) and @xmath233 ( bottom ) , title=\"fig : \" ]    .",
    "[ fig3 ]      in this subsection we test the convergence rates from theorems [ thm_kfdm1 ] , [ thm_3dvardm1 ] , [ thm_kfdm2 ] , and [ thm_3dvardm2 ] . for the verification of each of these rates we let @xmath243 denote the covariance from , and we consider @xmath244 experiments corresponding to different truths @xmath198 generated from @xmath245 with the four selections of @xmath246 . note that assumption [ assp_main ] ( iii ) is satisfied only for @xmath247 .",
    "again , for data model 1 we generate synthetic data for each of the truths and for as many iterations used for the application of both schemes .",
    "inverse crimes are avoided as described in subsection [ se_num : setup ] .",
    "the verification of theorem [ thm_kfdm1 ] and theorem [ thm_3dvardm1 ] by means of algorithm [ al1 ] in the case of data model 1 is straightforward . for each of the set of synthetic data associated to each of the 20 truths @xmath198",
    "previously mentioned , we fix @xmath248 . for each @xmath123 ( with @xmath249 ) we run algorithm [ al1 ] , stop the schemes at @xmath115 and record the value of @xmath250 . in the right ( resp",
    ". left ) figure [ fig4 ] we display a plot of @xmath250 vs @xmath251 for the kalman filter ( resp .",
    "3dvar ) for each of the set of 20 experiments associated to different truths ( red solid lines ) generated as described above with ( from top to bottom ) @xmath246 . from theorem [ thm_kfdm1 ]",
    "we note that the corresponding slopes of the convergence rates should be approximately given by @xmath252 . for theorem [ thm_3dvardm1 ]",
    "there is an additional term of @xmath253 , but this is of course negligible compared to the algebraic decay and we ignore it for the purposes of this discussion . for comparison , a line ( black dotted ) with slope @xmath252 is displayed in figure [ fig4 ] .",
    "we now verify the convergence rates of theorem [ thm_kfdm2 ] and theorem [ thm_3dvardm2 ] .",
    "note first that in algorithm [ al1 ] for data model @xmath7 we define @xmath123 in terms of the given small noise @xmath20 , in order to obtain convergence .",
    "however , for the purpose of the verification of the aforementioned convergence rates we define @xmath20 in terms of @xmath123 by means of the same expressions . in other words",
    ", for each @xmath123 ( @xmath249 ) we produce synthetic data ( or each of the 20 truths ) with @xmath254 and @xmath255 .",
    "we then run algorithm [ al1 ] and stop the schemes at @xmath115 . in the right ( resp .",
    "left ) figure [ fig5 ] we display a plot of @xmath250 vs @xmath251 for the kalman filter ( resp .",
    "3dvar ) for each of the set of 20 experiments associated to different truths ( red solid lines ) generated as before with ( from top to bottom ) @xmath246 .",
    "we again include a line ( black dotted ) with slope of @xmath256 which is the asymptotic behavior predicted by theorems [ thm_kfdm2 ] and [ thm_3dvardm2 ] .",
    "we can clearly appreciate that , for @xmath207 satisfying assumption [ assp_main ] ( iii ) ( i.e. @xmath257 ) , the numerical convergence rates fit very well the ones predicted by the theory . note that the higher the regularity of the truth ( i.e. the larger the @xmath207 ) , the smaller the error .",
    "w.r.t the truth in the estimates .",
    "we note that for @xmath258 , the aforementioned assumption is violated and , in the case of data model 1 , the slopes of the numerical convergence rates are slightly smaller than the theoretical ones . in this case",
    "( @xmath258 ) there are also fluctuations of the error w.r.t .",
    "the truth obtained with 3dvar .",
    "these fluctuations may be associated with the fact that since for the error w.r.t .",
    "the truth is very small for sufficiently large iterations and for data model @xmath6 the noise level is fixed a priori ( recall @xmath248 ) .",
    "however , for the kalman filter these fluctuations are not so evident ; presumably updating the covariance has a stabilizing effect . for data model @xmath7 , as @xmath123 increases , the corresponding @xmath20 decreases and",
    "so these fluctuations in the error are non existent .",
    "* we have presented filter based algorithms for the linear inverse problem , based on introduction of an artificial dynamic .",
    "this results in methods which are closely related to iterative tikhonov - type regularization . two data scenarios are considered , one ( data model @xmath6 ) involving multiple realizations of the data , with independent noise ; the other ( data model @xmath6 ) involving a single realization of the data ; both are relevant in applications .",
    "* we present theoretical results demonstrating convergence of the algorithms in the two data scenarios . for multiple realizations of the noisy",
    "the convergence is induced by the inherent averaging present in the iterative method , and the link to the law of large numbers and central limit theorem .",
    "for the single instance of data the small observational noise limit must be considered . * for both data model @xmath6 and data model @xmath7 the kalman filter and 3dvar produced very similar results for relatively small @xmath123 ( @xmath259 ) . in practice",
    "it is clear that 3dvar is preferable as the kalman filter requires covariance updates which may be impractical for large scale models .",
    "however , updating the covariance in the kalman filter seems to have an stabilizing effect in the error w.r.t the truth . * for data model @xmath6 the level of accuracy of the estimator is independent of the noise level .",
    "moreover , the stability of the scheme is not conditioned to the early termination of the scheme . in contrast , for data model @xmath7 we need to stop at @xmath115 to avoid an increase in the error w.r.t the truth . again",
    ", this illustrates that , whenever multiple instances of the data are available , data model @xmath6 offers a more stable and accurate framework for solving the inverse problems under consideration . *",
    "the theoretical results from this work are verified numerically whenever the assumptions of the theory are satisfied .     with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ]   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ] +   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ]   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ] +   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ]   with @xmath207 ( from top to bottom ) 1,2,3 and 4 . ,",
    "title=\"fig : \" ] +   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ]   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ]     with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ]   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ] +   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ]   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ] +   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ]   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ] +   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ]   with @xmath207 ( from top to bottom ) 1,2,3 and 4 .",
    ", title=\"fig : \" ]",
    "andrew m. stuart is funded by epsrc ( under the programme grant equip ) , darpa ( under equips ) and onr .",
    "shuai lu is supported by special funds for major state basic research projects of china ( 2015cb856003 ) , nsfc ( 91130004 , 11522108 ) , shanghai science and technology commission grant ( 14qa1400400 ) and the programme of introducing talents of discipline to universities ( number b08018 ) , china .    1    agapiou s. ; larsson s. and stuart a.  m. : _ posterior contraction rates for the bayesian approach to linear ill - posed inverse problems _ , stochastic process .",
    "( 2013 ) , no .",
    "10 , 38283860 .",
    "bissantz n. ; hohage t. ; munk a. and ruymgaart f. : _ convergence rates of general regularization methods for statistical inverse problems and applications _",
    ", siam j. numer .",
    ", 45 ( 2007 ) , 2610,c2636 .",
    "bogahcev v.  i. : _ gaussian measures _ , vol .",
    "62 of _ mathematical surveys and monographs_. american mathematical society , ( 1998 ) .",
    "borcea l , _ electrical impedance tomography .",
    "_ , inverse problems series 18(6 ) , 2002    dashti m ; and stuart a. m. : _ the bayesian approach to inverse problems_. to appear in the handbook of uncertainty quantification , editors r. ghanem , d. higdon and h. owhadi , springer , 2016 .",
    "arxiv:1302.6989    engl h.  w. ; hanke m. and a. neubauer : _ regularization of inverse problems _ , mathematics and its applications , vol .",
    "375 , kluwer academic publishers group , dordrecht , 1996 .",
    "hanke m. : _ a regularizing levenberg - marquardt scheme , with applications to inverse groundwater filtration problems . _ inverse problems , 13 ( 1997 ) , 7995 .",
    "hanke m. and groetsch c.  w. : _ nonstationary iterated tikhonov regularization _ ,",
    "journal of optimization theory and applications 98 ( 1998 ) , no .  1 , 3753 .    iglesias m.  a. ; law k.  j. and stuart a.  m. : _ ensemble kalman methods for inverse problems _ , inverse problems 29 ( 2013 ) , no .  4 , 045001 .",
    "iglesias m.  a. : . to apper in inverse problems .",
    "arxiv:1505.03876 .",
    "kaipio j. ; somersalo e. : _ statistical and computational inverse problems_. applied mathematical sciences , 160 .",
    "springer - verlag , new york , 2005 .",
    "knapik b. t. ; van der vaart a. w. and van zanten j. h. : _ bayesian inverse problems with gaussian priors _ , the annals of statistics 39 ( 2011 ) , no . 5 , 26262657 .",
    "law k.  j. ; stuart a.  m. and zygalakis k.c . : _ data assimilation : a mathematical introduction _ , springer , 2015 .",
    "lu s. and pereverzev s.  v. , _ regularization theory for ill - posed problems . selected topics .",
    "_ , inverse and ill - posed problems series , vol .  58 , walter de gruyter , berlin , 2013 .",
    "oliver d. ; reynolds a. , and liu n. : .",
    "cambridge univ pr , 2008 .",
    "_ proof of lemma [ lemma_kalmanbias ] .",
    "_ the proof follows the classic arguments on tikhonov regularization with hilbert scales , c.f .",
    "* ch8.4 ) .",
    "we recall ( [ eq : ca ] ) in assumption [ assp_main ] @xmath80 and rewrite it in the form @xmath260 notice that the definition of @xmath261 in gives , since @xmath2 is a hilbert space , and using assumption [ assp_main ] @xmath78 to ensure that @xmath261 and @xmath262 are well - defined bounded linear operators ,    @xmath263 combining the two preceding displays we obtain @xmath264 and a duality argument yield @xmath265 for any @xmath266 .",
    "let @xmath267 $ ] .",
    "then the inequality of heinz ( * ? ? ?",
    "* ch.8.4 , pp .",
    "213 ) and an additional duality argument gives @xmath268 which yields @xmath269 .",
    "let assumption [ assp_main ] @xmath82 be valid with @xmath270 , and define @xmath271 . since @xmath272",
    "$ ] , and consequently @xmath273,$ ] we obtain from ( [ eq : cb ] ) that @xmath274 furthermore there exists a @xmath275 such that @xmath276 noting that @xmath277 , and employing ( [ eq : cb ] ) with @xmath278 , together with ( [ eq : rapproxi_km2 ] ) and ( [ eq : pos_rr ] ) , we have @xmath279 in case of assumption [ assp_main2 ] we insert @xmath280 and @xmath281 .",
    "_ proof of lemma [ lemma_kalmanvariance ] . _",
    "notice that @xmath283 and @xmath284 thus we obtain @xmath285 by virtue of ( [ eq : prod ] ) and ( [ eq : cnkn ] ) we derive @xmath286 we denote @xmath287 and obtain @xmath288 by the definition of @xmath289 and assumption [ assp_main ] @xmath78 , @xmath83 we obtain      and consequently derive @xmath291 with the operator - valued function @xmath292 .",
    "such an observation then yields @xmath293 concerning assumption [ assp_main2 ] , we further estimate , by exploiting ( * ? ? ?",
    "* lemma 8.2 ) , @xmath294 and @xmath295          _ proof of theorem [ thm_3dvardm1ass2 ] .",
    "_ as for the other theorems , the proof rests , of course , on the bias variance decomposition , and then use of lemmas [ lemma_3dvarbias ] and [ lemma_3dvarvariancedm1 ] .",
    "this yields @xmath163 and simultaneously @xmath298 choosing @xmath299 for the former inequality and @xmath300 for the latter inequality , we conclude that , by stopping the iteration when @xmath115 , @xmath301 @xmath282    _ proof of lemma [ lemma_3dvarbias ] . _",
    "analogously to the proof of lemma [ lemma_kalmanbias ] , it may be shown that @xmath302 the final inequality follows from the asymptotic behavior of @xmath303 , established , for example . in (",
    "* ch.2 , pp .",
    "in the case of assumption [ assp_main2 ] we insert @xmath280 and @xmath281 .",
    "_ proof of lemma [ lemma_3dvarvariancedm1 ] .",
    "_ we denote @xmath304 and obtain @xmath305 furthermore , we derive @xmath306 and @xmath307 thus , @xmath308 for assumption [ assp_main2 ] we need to estimate @xmath309 carefully .",
    "we substitute the given decay rate of the different eigenvalues , to obtain @xmath310 by arguments similar to those used in the proof of lemma [ lemma_kalmanvariance ] ( or ( * ? ? ?",
    "* lemma 8.2 ) ) , we further estimate @xmath311 and @xmath312 where the summation term in the right - hand side is bounded",
    ".        _ proof of lemma [ lemma_3dvarvariancedm2 ] . _ since @xmath315 in this case , we derive @xmath316 and by operator - valued calculation we obtain @xmath317 where @xmath318 .",
    "thus we obtain , by the asymptotic behavior of @xmath319 derived in ( * ? ? ?",
    "* ch.2 , pp .",
    "64 ) , @xmath320 @xmath282    _ proof of theorem [ thm_3dvarvariant ] .",
    "_ similar to the kalman filter method and 3dvar , by assumption [ assp_main ] @xmath78-@xmath82 , we obtain the bias error estimate @xmath321 now we need upper bounds of the following operator - valued function @xmath322 define @xmath323 and assume the sequence @xmath324 satisfying @xmath325 with a constant @xmath185 .",
    "then the results in @xcite yield @xmath326      notice that for any @xmath330 @xmath331 we derive , @xmath332 and @xmath333 a rough variance estimate for the variant method is @xmath334 the first term vanishes but the second term blows up when @xmath57 and @xmath335 .",
    "to further investigate the blow up , we consider a special geometric sequence @xmath336 with @xmath190 .",
    "thus , we have @xmath337 and ( [ eq : sequence ] ) is satisfied with @xmath338 .",
    "actually , we derive @xmath339 thus the results in @xcite refine , by the asymptotic behavior of ( [ eq : f_nv ] ) , @xmath340 and we derive @xmath341 summing up , for the geometric sequence , we obtain @xmath342 the second term blows up exponentially when @xmath44 goes to infinity ."
  ],
  "abstract_text": [
    "<S> ill - posed inverse problems are ubiquitous in applications . understanding of algorithms for their solution has been greatly enhanced by a deep understanding of the linear inverse problem . in the applied communities </S>",
    "<S> ensemble - based filtering methods have recently been used to solve inverse problems by introducing an artificial dynamical system . </S>",
    "<S> this opens up the possibility of using a range of other filtering methods , such as 3dvar and kalman based methods , to solve inverse problems , again by introducing an artificial dynamical system . </S>",
    "<S> the aim of this paper is to analyze such methods in the context of the ill - posed linear inverse problem .    </S>",
    "<S> statistical linear inverse problems are studied in the sense that the observational noise is assumed to be derived via realization of a gaussian random variable . </S>",
    "<S> we investigate the asymptotic behavior of filter based methods for these inverse problems . </S>",
    "<S> rigorous convergence rates are established for 3dvar and for the kalman filters , including minimax rates in some instances . </S>",
    "<S> blowup of 3dvar and a variant of its basic form is also presented , and optimality of the kalman filter is discussed . </S>",
    "<S> these analyses reveal a close connection between ( iterative ) regularization schemes in deterministic inverse problems and filter based methods in data assimilation . </S>",
    "<S> numerical experiments are presented to illustrate the theory . </S>"
  ]
}