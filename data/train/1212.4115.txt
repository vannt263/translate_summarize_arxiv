{
  "article_text": [
    "the fermi spacecraft supports two gamma - ray instruments ; the large area telescope ( lat ) @xcite and the gamma - ray burst monitor ( gbm ) @xcite .",
    "the lat is a wide - field gamma - ray telescope ( 20 mev - 300 gev ) that continuously scans the sky , providing all - sky coverage every two orbits .",
    "the gbm is an all - sky monitor ( 10 kev - 25 mev ) that detects transient events such as occultations and gamma - ray bursts ( grb ) .",
    "gbm detections of strong grbs can result in an autonomous re - point of the observatory to allow the lat to obtain afterglow observations .",
    "the satellite sends data to the ground every 3 hours .",
    "data is transferred via relay satellites at 40 mb / s to the white sands ground station .",
    "it then follows a leased line to the mission operations center ( moc ) at goddard space flight center where data is split into two parts and sent for science processing to both the gbm and the lat teams , the latter being located at stanford national accelerator laboaratory ( slac ) .",
    "the processing of downlinked satellite data is a time - critical operation .",
    "it is , therefore , necessary to automatically trigger the pipeline for each newly arrived block of data , and to exploit parallel processing in a batch farm to achieve the required latency for the production of the various data products .",
    "this processing is complex and is abstracted in a process graph .",
    "an xml representation of this process graph is interpreted by the pipeline to become a _ task_. once defined , a task is exercised by the creation of _ streams _ , each of which is one instance of the process graph and which consists of an arbitrary number of interconnected batch jobs ( or scripts ) known as _ process instances _ @xcite . to that end",
    ", the software is designed to be able to handle and monitor thousands of streams being processed at separate sites with a daily average throughput of about 1/2 cpu - year of processing . to date peak usage has been 45,000 streams in a single day and 167 cpu - years of processing in a single month @xcite .",
    "the pipeline was designed with the l1 data processing task ( `` level 1 processing '' , the core data processing of raw data that comes from the satellite ) , automatic science processing ( asp ) and monte carlo simulations as principle task types in mind and to ensure the tight connection to the fermi data catalog .",
    "it is literally impossible to depict the whole l1 task scheme in one single figure as it contains many dozens of sub - stream creations , dependencies and automatic re - run mechanisms .",
    "thus we refrain from including them in this paper . for details",
    "the reader is referred to @xcite .",
    "instead we show the simple layout of a monte carlo task ( as our efforts initially are geared towards porting them to grid sites ) in fig .",
    "[ [ fig : mctask ] ] that does not rely on external dependencies such as local databases .",
    "this task simply consists of 3 steps , the generation of monte carlo data on a computing node , the transfer of the mc products to slac and their registration in the data catalog .",
    "the first 2 steps are batch operations where the registration step takes only split seconds and is achieved through running a dedicated jython @xcite scriptlet .",
    "we use a three tier architecture as shown in fig .  [ [ fig : keytech ] ] comprising of back - end components , middle - ware and front - end user interfaces .",
    "we describe them more in detail below .",
    "core of the back - end components is the oracle database that stores all processing states .",
    "in addition we make extensive use of oracle technologies such as the scheduler that is used to run periodic jobs for system monitoring including resource monitoring of the oracle server itself .",
    "most quantities are made available to the user through trending plots that allow quick judgment about the state of the system along with its resource usage .",
    "another back - end component is the pipeline job control service . using remote method invocation ( rmi @xcite )",
    "they publish the uniform interface and communicate with the pipeline server .",
    "we discuss some more details on this component in the next section .",
    "to provide asynchronous persistent messaging from batch jobs to the pipeline server we use email messaging . at the beginning of a job an email",
    "is sent detailing the host name and other worker node specific information .",
    "another email is sent to indicate that a job has finished .",
    "this email may also contain additional commands that invoke new pipeline commands , such as creation of sub - streams as the next step . in order to avoid overloading the slac email server , by relaying tens of thousands emails per day",
    ", we use a dedicated email server running the free apache james @xcite software .",
    "the pipeline server is the core of the system and contains two pools for threads , a worker and an admin pool . when a batch process is ready to run on one of the farms , a thread is allocated on the worker pool to perform the submission using the appropriate job control service .",
    "extensions to the pipeline ( called _ plugins _ ) can be used to add additional functionality , for example to provide access to experiment specific databases or to communicate with other middleware services without compromising the experiment independent design of the core pipeline software .",
    "plugins are written in java and loaded dynamically when the pipeline starts .",
    "the user can also provide jython scriptlets to run within the pipeline threads to perform simple calculations and to communicate with plugins .",
    "the fermi data catalog is implemented as a plugin .",
    "the pipeline server api allows queries for processes , stream management as well as means to get or set environment variables .",
    "the admin thread pool is used to identify work to be delegated to the worker pool .",
    "this includes gathering processes which are ready to run as well as various database queries .",
    "we provide a subset of the pipeline api as java management extension ( jmx ) , that provides a call interface to various user - interface applications .",
    "these come both as web interface and line command applications .",
    "the web interface provides password protected world - wide access to the pipeline and its control interfaces and allow simultaneous monitoring of tens of thousands of jobs in various states . for detailed technical information on",
    "the pipeline components the reader is referred to @xcite .      each job control service ( refer to fig .  [",
    "[ fig : jcd ] ] ) implements job control and status methods that are specific to the batch or grid system . to that",
    "end each job control service needs to provide the following commands : _",
    "submitjob , getjobstatus , killjob_. the code for the job control service is written in java and runs as a daemon on a dedicated service machine at each computing site .",
    "to date the pipeline supports lsf @xcite , bqs and more recently also condor as well as sun grid engine ( sge @xcite ) . at",
    "present  uses lsf at slac and sge at the lyon computing center .",
    "the code can easily be adapted to any other desired batch system that follows the same job logic as the currently supported ones . to that end a java class _ batchjobcontrolservice _ and _ batchstatus _ need to be implemented for the desired new batch system , where _ batch _ denotes the system to be implemented .    for the use with dirac ( distributed infrastructure with remote agent control ) the job control code wraps python scripts that provide the bridge in both format and language by using the dedicated dirac api .",
    "we describe further aspects of the dirac system and motivate our choice to use it in section [ dirac ] .    in the past",
    "a task was defined on top level to be handled by a specific job control service .",
    "recently the pipeline has been enabled to support multi - site tasks that allow the pipeline server to delegate the running of commands to our dirac job control service while we leave the transfer step to the lyon - based sge job control service .",
    "as of this writing the lat has been granted resources both at slac ( dynamic allocation scheme ) and lyon ( guaranteed 1200 cores allocation ) . at lyon",
    "all resources are used for monte carlo productions while at slac the total allocation is shared between l1 , monte carlo production and individual user jobs from the collaboration . as a recent challenge we have started reprocessing all of our data that was taken from the beginning of the mission up until now and are reprocessing it with our current state of knowledge about the experiment .",
    "the reprocessing requires a significant amount of our computing resources .",
    "as allocations are dynamic , users running science analysis may be directly impacted through less available slots on the batch farm .",
    "another recent challenge was a massive monte carlo production of proton runs that occupied our resources both at slac and lyon for several months .",
    "while not being particularly storage intensive , we do require significant amounts of cpu time .",
    "since this production run was setup with the previous interation of the instrument response , dubbed `` pass 6 '' , it is likely that simulation requests of this kind may be repeated as our knowledge of the experiment grows .",
    "it is thus important to investigate possibilities to extend our resources that can be utilized within the current pipeline framework .",
    "although we perform standard computing tasks at our two sites on local batch farms , there exists the virtual organization ( vo ) glast.org .",
    "this organization was founded in 2009 to provide access to glite @xcite resources granted by participating institutions in italy and france . at present",
    "the vo includes 13 sites that are partially enabled for use by .",
    "its use has however been limited to non - pipeline operations .",
    "most notably , our existing grid resources have been used for stand - alone pulsar blind searches and some large monte carlo simulations .",
    "these stand - alone tasks were unable to take advantage of the pipline s integration with the batch system and data catalog which made them more man - power intensive than their pipeline counterparts .      in order to optimize the resource usage provided by the egi @xcite sites supporting the glast.org vo",
    ", we are exploring the use of the dirac system as potential connection between the pipeline and the grid resources @xcite .",
    "the dirac system , originally developed to support production activities of the lhcb experiment , is today a general solution to manage the distributed computing activities of several communities .",
    "one of its main components is the workload management system ( wms ) .",
    "the key feature of the dirac wms is the implementation of the ` pilot job ' mechanism , which is widely exploited by all lhc communities as a way to guarantee high success rate for user jobs ( workloads ) .",
    "more in detail , workloads are submitted to the dirac wms and inserted in the dirac _ central task queue_. the presence of workloads in the dirac _ central task queue _ triggers pilot jobs submission .",
    "pilot jobs are regular grid jobs that are submitted through the standard glite wms .",
    "once the pilot job gets on the worker node , it performs some environment checks and only in case these tests succeed , the workload is pulled from the dirac _ central task queue _ and executed . in case the environment checks fail or the pilot job gets aborted ( for example because of a mis - configured site ) , only the pilot job is affected .",
    "the net result of this mechanism is a significant improvement on the workload success rate . also , since the resources are pre - reserved by pilot jobs , the waiting time to get the workload execution started is reduced .",
    "while dirac has specifically been designed to tackle intrinsic grid inefficiencies through its pilot job concept , it can not solve some of the initial issues when being connected to the pipeline system .",
    "in particular :    * the grid uses personalized certificates . when submitting jobs through the pipeline web interface , we submit them with a generic user i d , which is most feasible as the number of authorized pipeline users is small .",
    "* some of our sites , in particular those from infn , can not directly send emails to the pipeline server , thus rendering the standard communication of job status and task logic de - facto unusable . * in general large data on the grid",
    "would be stored on grid storage elements ( grid se ) and require user intervention to download it once finished .",
    "this makes the automatic copy to our central slac xrootd space difficult to be used , in particular using the tight integration with the data catalog .",
    "we decided to implement an interface to the dirac system for several reasons :    * independence of grid middleware : currently the vo comprises only glite sites but dirac is able to communicate with all common grid middleware platforms , thus making it less difficult to connect to other grid initiatives , such as the open science grid in the us @xcite . *",
    "additional monitoring functionality : dirac introduces a more detailed job monitor that in addition reports minor and major application statuses together with the overall job status , thus allowing easier debugging of tasks without the definite need to inspect log files manually .",
    "this task has always been among the most time consuming .",
    "we hope to achieve an improvement by using the new monitor , albeit as read - only system .",
    "the reason for that is that the pipeline i d and the grid i d are generally not the same and a re - submitted job on the pipeline keeps its i d while on the grid it is relaunched and assigned a new unique grid i d . * while  is entering its 4th mission year , the number of developers for software has begun to dwindle .",
    "thus it is important to keep the required manpower as low as possible while maintaining full performance and possibly improving the capabilities of the pipeline . by using dirac",
    "we can build our interface on a system in active development with many possibilities to influence the development process to meet our needs .    implementing the dirac interface comes in two steps , first the job control daemon as described in the previous section that wraps the basic pipeline commands through the dirac api and secondly the configuration of a dedicated dirac server .",
    "the implementation is shown in fig .",
    "[ [ fig : p2dirac ] ] .",
    "we make use of two of diracs core technologies : the pilot factory mechanism is used to renew proxies and authenticate the job control daemon to the grid allowing us to use one certificate retaining our previous user scheme .",
    "the dirac notification service provides means for the dirac server to communicate with grid worker nodes .",
    "this service is both safe to inception and can be modified to relay the content of our status emails to the dirac server that itself implements the email communication with the pipeline server .",
    "one typical by - product of running our code are more or less extensive log files .",
    "usually we have several hundreds of small log - files that each do not exceed a few mb . in the past",
    "other experiments had to artificially enlarge their log files to ensure the stability of the grid ses due to journalling of small files .",
    "we can effectively circumvent this by declaring a local storage element at the computing center in lyon as a dedicated dirac se .",
    "these storage elements are addressed like normal grid elements but they do not need journaling to function . since this is a local se , we can view its content e.g. through a web server providing access to log - files that can directly be linked from the pipeline server .    in conclusion",
    "we believe that this solution provides an easy to implement and maintain interface to the pipeline system used for -lat .",
    "the existing vo resources of glast.org suggest the possibility to establish a new connection to grid services with our existing pipeline architecture .",
    "we mitigate issues such as the transfer and subsequent registration of data products at the fermi data catalog by using existing pipeline technologies .",
    "grid inefficiencies are handled by the dirac system that acts as a broker providing asynchronous communication with grid worker nodes and a closed mechanism to automatically renew proxies to use for grid operations .",
    "we leave handling of meta data to the existing pipeline technologies .    as such",
    ", the pipeline software itself was designed in a manner to not contain any fermi specific functionality . through the plug - in feature of the middleware it is successfully used for data handling for the enriched xenon observatory or mc simulations for the cryogenic dark matter search experiment ( cdms @xcite ) , the upcoming cherenkov telescope array ( cta @xcite ) and the large synoptic survey telescope ( lsst @xcite ) , and",
    "is also being considered for use by other nasa missions .",
    "we acknowledge the ongoing generous support of slac ( us ) and in2p3 ( france ) as well as we thank them for increasing allocations at both sites .",
    "furthermore we are thankful for grid resources provided by infn and in2p3 .",
    "sz likes to thank the organizers for a stimulating meeting and particularly helpful discussions with stphane guillaume poss .",
    "the -lat collaboration acknowledges generous ongoing support from a number of agencies and institutes that have supported both the development and the operation of the lat as well as scientific data analysis .",
    "these include the national aeronautics and space administration and the department of energy in the united states , the commissariat  lenergie atomique and the centre national de la recherche scientifique / institut national de physique nuclaire et de physique des particules in france , the agenzia spaziale italiana and the istituto nazionale di fisica nucleare in italy , the ministry of education , culture , sports , science and technology ( mext ) , high energy accelerator research organization ( kek ) and japan aerospace exploration agency ( jaxa ) in japan , and the k.  a.  wallenberg foundation , the swedish research council and the swedish national space board in sweden .",
    "additional support for science analysis during the operations phase is gratefully acknowledged from the istituto nazionale di astrofisica in italy and the centre national dtudes spatiales in france .",
    "9 atwood et al",
    ". _ apj _ * 697 * ( 2009 ) http://cdms.berkeley.edu/ http://www.cta-observatory.org/ dubois , _ asp conf .",
    "ser . _ * 411 * , eds .",
    "d bohlender , d durand , & p dowler ( 2009 ) http://www.egi.eu/ http://www-project.slac.stanford.edu/exo/ flath et al . , _ asp conf .",
    "ser . _ * 411 * , eds .",
    "d bohlender , d durand , & p dowler , slac - pub-13549 ( 2009 ) see http://gammaray.msfc.nasa.gov/gbm/ for details on the gbm http://glite.cern.ch/ focke , `` implementation and performance of the fermi lat level 1 pipeline '' , ii .",
    "fermi symposium ( 2009 ) http://www.platform.com/workload-management/high-performance-computing http://james.apache.org http://www.lsst.org/lsst/ http://www.jython.org/ https://www.opensciencegrid.org http://java.sun.com/javase/technologies/core/basic/rmi/index.jsp http://www.oracle.com/us/sun/index.htm tsaregorodtsev at al . , _ j. phys .",
    "_ * 119 * ( 2008 )"
  ],
  "abstract_text": [
    "<S> the data handling pipeline ( `` pipeline '' ) has been developed for the  gamma - ray space telescope ( )  large area telescope ( lat ) which launched in june 2008 . </S>",
    "<S> since then it has been in use to completely automate the production of data quality monitoring quantities , reconstruction and routine analysis of all data received from the satellite and to deliver science products to the collaboration and the fermi science support center . aside from the reconstruction of raw data from the satellite ( _ level 1 _ ) , data reprocessing and various event - level analyses are also reasonably heavy loads on the pipeline and computing resources . </S>",
    "<S> these other loads , unlike level 1 , can run continuously for weeks or months at a time . </S>",
    "<S> in addition it receives heavy use in performing production monte carlo tasks .    in daily use </S>",
    "<S> it receives a new data download every 3 hours and launches about 2000 jobs to process each download , typically completing the processing of the data before the next download arrives . </S>",
    "<S> the need for manual intervention has been reduced to less than 0.01% of submitted jobs .    </S>",
    "<S> the pipeline software is written almost entirely in java and comprises several modules . </S>",
    "<S> the software comprises web - services that allow online monitoring and provides charts summarizing work flow aspects and performance information . </S>",
    "<S> the server supports communication with several batch systems such as lsf and bqs and recently also sun grid engine and condor . </S>",
    "<S> this is accomplished through dedicated job control services that for  are running at slac and the other computing site involved in this large scale framework , the lyon computing center of in2p3 . while being different in the logic of a task , we evaluate a separate interface to the dirac system in order to communicate with egi sites to utilize grid resources , using dedicated grid optimized systems rather than developing our own . </S>",
    "<S> more recently the pipeline and its associated data catalog have been generalized for use by other experiments , and are currently being used by the enriched xenon observatory ( exo ) , cryogenic dark matter search ( cdms ) experiments as well as for monte carlo simulations for the future cherenkov telescope array ( cta ) . </S>"
  ]
}