{
  "article_text": [
    "bagging and various variants of it have been widely popular and studied extensively in the last two decades @xcite",
    ". there has been notable work in understanding the theoretical underpinning of bootstrap aggregating and as to what makes it such a powerful method @xcite . in traditional",
    "bagging , each training example is sampled with replacement and with probability @xmath0 .",
    "adaptive resampling and combining ( arcing ) techniques which modify the probability of each training example being sampled based on heuristics have also been developed and widely used @xcite .",
    "random subspace methods also known as attribute bagging refer to creating ensembles of predictors trained on randomly selected subsets of total features , that is , predictors constructed on randomly chosen sub - spaces .    both methods , sub - sampling and sub - spacing",
    "reduce the variance of the final ensemble and hence increase the accuracy .",
    "arcing methods are known to reduce the bias of the model as well .",
    "error based resampling algorithms which try to set the train - set error to zero @xcite , designed bagged ensembles with minimal intersection @xcite , diversity and uncorrelated errors @xcite , importance sampling @xcite etc .",
    "are some of the areas being studied to improve bagged ensembles .",
    "either there are multiple answers to the question , or the answer changes with each dataset .    instead of figuring out precisely as to _ what _ sampling and combination of training sets make a bagged ensemble better",
    ", we try to fix the _ definition _ of better , and allow the bootstrapped training sets to _ evolve _ themselves in order to align with the definition .",
    "we generate multiple sampled candidate training sets for the final ensemble and let them compete , mutate and mate their way to the optimal sampling and combination .",
    "evolutionary computation has been used for selection of different predictors to be part of an ensemble @xcite and also for the selection of the most suitable machine learning pipeline for a classification problem @xcite .    to our best knowledge ,",
    "genetic algorithms havent been directly used to evolve bootstrapped samples of the training data .",
    "evolutionary computation techniques evolve a population of solution variables ( bootstrapped training sets in our case ) to optimize towards a given criteria .",
    "the fittest offspring across all the generations is considered as the most optimal solution . in evolutionary sampling ( es )",
    ", we followed a standard genetic algorithm .",
    "initially , a population of multiple ensembles is generated by randomly sampling from the training data multiple times . each ensemble ( henceforth referred to as an individual ) in a generation",
    "is evaluated based on a fitness function .",
    "fit individuals are selected for the next generation . after this",
    ", crossover is applied on a fixed percentage of individuals wherein two individuals swap their predictors .",
    "post this , a fixed percentage of individuals unaffected by crossover undergo a random mutation .",
    "randomly selected member datasets from the selected individual have some of their rows / features deleted , replaced or inserted with equal probability . in feature sub - spacing",
    "the features are subject to perturbation whereas in sub - sampling rows are perturbed .",
    "we ve used the python package deap @xcite to implement es .",
    "ga parameters are shown in table [ table : ga_parameter ] . as suggested before , instead of understanding as to what makes a bagged ensemble better , we try to rely on the definition of better and try to evolve our ensemble into the same .",
    "the fitness function is what guides the sampling and combination of the different sampled datasets .",
    "we propose three fitness functions and then try to analyse their performance .",
    "_ fempo _ : fitness each model private out of bag .",
    "it takes each predictor part of the candidate ensemble and measures their performance on the samples that were left out of it s training bag @xcite .",
    "final fitness of the ensemble is the mean of each model s rmse .",
    "_ fempt _ : fitness each model private test .",
    "it is the average of the performance of member predictors on a private test - set which is held out for each sampled dataset during its instantiation .",
    "_ fegt _ : fitness ensemble global test . during the start of the algorithm ,",
    "20% of the training data is set aside .",
    "each ensemble s prediction is based on the average prediction of it s member predictors .",
    "rmse is calculated against the set aside global test .",
    "we conduct experiments on two variants of sampling : sub - sampling and sub - spacing .",
    "sub - sampling works on sampling training examples , whereas sub - spacing works on generating multiple feature sets .",
    "we conduct our experiments on 4 benchmark datasets [ see table [ table : results ] ] .",
    "we compare the mean squared error of the first individual ( fi ) of the first generation with the hall of fame ( best individual ) after 30 generations .",
    "we assume that the first individual in the first generation is representative of an ensemble which randomly samples its rows or features like in traditional bagging .",
    "we try to analyse whether es is able to evolve better ensembles starting from random specimens .",
    "we uniformly use an unpruned decision tree regressor with max depth arbitrarily set as 5 .",
    "a 50% win - ratio would suggest that the performance of the ensemble after undergoing es is better than its random counterpart only half the times .",
    "mean mse and standard deviation of the same is also a good metric to compare es with random instantiation .",
    "the null hypothesis in the paired t - test suggests that the average mean squared error between the two methods is the same .",
    "if the p - value is smaller than a threshold , then we reject the null hypothesis of equal averages .    in sub - sampling ,",
    "fempo and fegt performs equally or better than their random counterparts . though the win percentages are almost half in many cases , it could be that the algorithm was initialized with an optimal combination and sampling .",
    "fegt shows the most improvement in abalone ( 72% win - ratio ) .    in sub - spacing ,",
    "both fempo and fegt do significantly better than random sub - spacing .",
    "ga has definitely helped in improving accuracy of the model .",
    "one should note , ga has a narrow exploration space in case of feature sub - spacing as compared to sub - sampling and features play a more significant role in deciding the model s behaviour than a few rows of data .",
    "results suggest that es is possibly useful in cases where maximum accuracy needs to be juiced out and computation is not an issue .",
    "it s evident that better and more robust fitness functions need to be explored , even multi - objective fitness functions , which better represent generalizability and error of the ensemble .",
    "it needs to be explored how these methods can be used to generate different models for smaller segments or patches of the dataset @xcite . can the segments , suggested by es along with fitness functions that take into account each model s fitness ( fept or fempo ) , be used to find different cohorts in the dataset ?",
    "it will be interesting to see what happens if the algorithm is allowed to run for generations till the fitness test error reduces approximately to zero .",
    "we plan to experiment with different base estimators for each sampled dataset and also explore how sub - spacing and sub - sampling can be combined into one algorithm .",
    "going with the theme of reproduction , we ve released the basic framework for es on github ( http://github.com/evoml/evoml ) .",
    "we encourage researchers to contribute to the project and test out different fitness functions themselves .",
    "fortin , flix - antoine , de rainville , franois - michel , gardner , marc - andr , parizeau , marc , and gagn , christian .",
    ": evolutionary algorithms made easy . _ journal of machine learning research _ , 13:0 21712175 , jul 2012 .",
    "gagn , christian , sebag , michele , schoenauer , marc , and tomassini , marco .",
    "ensemble learning for free with evolutionary algorithms ?",
    "in _ proceedings of the 9th annual conference on genetic and evolutionary computation _ , pp .   17821789 .",
    "acm , 2007 .",
    "olson , randal  s. , urbanowicz , ryan  j. , andrews , peter  c. , lavender , nicole  a. , kidd , la  creis , and moore , jason  h. automating biomedical data science through tree - based pipeline optimization . in squillero , g and burelli , p ( eds . ) , _",
    "proceedings of the 18th european conference on the applications of evolutionary and bio - inspired computation _ , lecture notes in computer science , berlin , germany , 2016 .",
    "springer - verlag ."
  ],
  "abstract_text": [
    "<S> perturb and combine ( p&c ) group of methods generate multiple versions of the predictor by perturbing the training set or construction and then combining them into a single predictor @xcite . </S>",
    "<S> the motive is to improve the accuracy in unstable classification and regression methods . </S>",
    "<S> one of the most well known method in this group is bagging . arcing or adaptive resampling and combining methods like adaboost are smarter variants of p&c methods . in this extended abstract </S>",
    "<S> , we lay the groundwork for a new family of methods under the p&c umbrella , known as evolutionary sampling ( es ) . </S>",
    "<S> we employ evolutionary algorithms to suggest smarter sampling in both the feature space ( sub - spaces ) as well as training samples . </S>",
    "<S> we discuss multiple fitness functions to assess ensembles and empirically compare our performance against randomized sampling of training data and feature subspaces . </S>"
  ]
}