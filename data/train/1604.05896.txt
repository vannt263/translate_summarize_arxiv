{
  "article_text": [
    "factor models are extensively used to model asset returns ( see , e.g. , campbell _ et al . _ , 1997 ) .",
    "the two main types of factor models are fundamental factor models and statistical factor models . in a fundamental factor model ,",
    "the aim is to find observable asset characteristics , e.g. , financial ratios and macro - economic variables , capable of explaining the behavior of the market stock prices .",
    "the explanatory fundamental and economic variables are often highly correlated with each other , which may cause multi - collinearity and other issues in a fundamental factor model .",
    "returns predicted by a fundamental factor model are often more correlated than the observed returns , which is the main reason for the inclusion of specific risk components in a factor model .",
    "statistical factor models are a commonly - used alternative for fundamental factor models . in a statistical factor model ,",
    "factors are extracted from asset returns but they often are not directly observable . the principal component analysis ( pca ;",
    "see , e.g. , alexander , 2001 ) is an example of a statistical technique for developing multi - factor models used in investment risk management .",
    "pca works well when the analyzed time - series are highly correlated , which may indicate the presence of a common driver .",
    "applications of pca include models of interest rate term structure , credit spreads , futures , and volatility forwards . in pca",
    ", several principal components often have an intuitive financial interpretation . in ordered highly - correlated systems , the first principal component captures an almost parallel shift in all variables and is generally labeled the common trend component .",
    "the second principal component captures an almost linear tilt in the variables , while the higher order principal components capture changes that are quadratic , cubic and so forth ( see , e.g. , alexander , 2009 ) . in the equity markets , the higher order principal components may often , but certainly not always , be interpreted as market movements caused by different investment style tilts .",
    "classical factor models include only a handful factors .",
    "the best - known factor model in the literature is likely the capital asset pricing model ( capm ) , which assumes that a single risk factor , the market , drives returns in a portfolio of assets ( sharpe , 1964 ) .",
    "a number of factor models have extended this view ( see , e.g. , ross , 1976 , fama and french , 1993 ) .",
    "recent increase in computational power has enabled development of models with a large set of factors . today , factor models are popular in market risk modeling , e.g. , the barra models ( grinold and kahn , 2000 ) depend on hand - picked market factors to explain behavior of the analyzed financial instruments .",
    "the choice of factors clearly influences the ability of a factor model to explain investment risk of a portfolio , in particular when the factor model consists of only a few carefully - chosen factors . when the number of factors is large compared to the number of time - series analyzed , it may not much matter which factors are chosen as long as the factors span a sufficiently large sample space .",
    "however , it is not clear how well an arbitrary set of factors would enable analysis - or at least description - of the risk .",
    "this is the issue that we analyze in this study : take a random set of factors and see whether it enables reproduction of the data and its interdependencies .    to understand whether a large set of long time - series can be described using an arbitrary set of factors , we analyze data using a factor model based on a random set of factors . a good starting point for developing",
    "a random factor model is the random projection method ( see , e.g , bingham and mannila , 2001 ; vempala , 2005 ) .",
    "random projection method consists of a projection of data to a lower - dimensional space by a random matrix . from the econometric point of view , a random projection can be viewed as a projection of time - series to a collection of almost non - autocorrelated factors that are also expected to be non - crosscorrelated , as will be seen in section 2 .",
    "it turns out that this kind of factor set can be used to describe the time - series accurately when a sufficiently large number of factors are used .",
    "the random projection method has been used , e.g. , to reduce the complexity of the data for classification purposes ( e.g. , kohonen _ et al . _ , 2000 ) , for structure - preserving perturbation of confidential data in scientific applications ( liu _ et al .",
    "_ , 2006 ) , for data compression ( bingham and mannila , 2001 ) , for compression of images ( amador , 2006 ) , and in the design of approximation algorithms ( e.g. , blum , 2006 ) .",
    "it allows one to reduce dimensionality of the investigated problem , often substantially , while preserving the structure of the problem .",
    "analysis of the risk in investment portfolio requires that risks of individual instruments are combined into the risk of the portfolio .",
    "this can be accomplished using dependence structures , e.g. , correlation matrix .",
    "it is no coincidence that dependence structures of financial time - series are central in modern investment theory ( e.g. , markowitz , 1952 )",
    ".    a factor model used in investment risk management should capture structure of the relevant data to describe , to decompose and to forecast portfolio risk .",
    "we will see that this is one of the fortes of a random factor model",
    ".    given two independent random vectors in a high - dimensional vector space , it is likely that the vectors are almost orthogonal with respect to each other .",
    "time - series corresponding to these two vectors are then almost uncorrelated .",
    "since correlation between two random vectors decreases with an increase in dimension of the vector space , high - dimensionality of the data may in some cases even simplify the problem .",
    "almost any set of random vectors yields an almost uncorrelated set of factor time - series that can be used as a basis for a random factor model .",
    "these considerations and the success of the random projection in data compression suggest that it is likely that a random factor model can represent a time - series even with a small set of factors .",
    "laloux _ et al .",
    "_  ( 1999 ) argue that most of the eigenvalue spectrum of correlation matrix for the standard & poor s 500 index ( s&p500 ) equities can be described as a product of noise , only about 20 eigenvalues of 500 total are informative . in more detail , laloux _",
    "et al . _  ( 1999 ) demonstrated that the bulk of the power spectrum of the s&p500  returns is indistinguishable from that produced by gaussian orthogonal ensemble ( goe ) .",
    "the informative eigenvalues may correspond to the market movements and sectors , but most of the eigenvalue spectrum does not correspond to linear factors .",
    "this suggests that noise has a significant role in the description of dependence structures in financial data .",
    "since goe consists of random matrices , the bulk of power spectrum can be explained as a product of noise .",
    "only the about 20 eigenvalues not part of the bulk are significant , which suggests that returns of the s&p500  index are largely driven by a small number of relevant factors and a large noise component .",
    "plerou _ et al .",
    "_  ( 2000 ) show that the most eigenvalues in the spectrum of the cross - correlation matrix of stock price changes agree surprisingly well with universal predictions of random matrix theory .",
    "malevergne and sornette ( 2002 ) construct a model with a spectrum that exhibits the features found in s&p500  spectrum : a few large eigenvalues and a bulk part .",
    "malevergne and sornette ( 2002 ) also point out the chicken - and - egg problem associated with factors : factors exist because stocks are correlated ; stocks are correlated because of a common factor impacting them .",
    "they argue that the apparent presence of factors is a consequence of the collective , bottom - up effect of the underlying time - series .      in this study",
    ", we develop a factor model based on randomly - chosen factor time - series , the random factor model .",
    "we show that randomness of factors results in certain desirable properties , such as almost orthogonality of factors and well - defined probabilistic limits on the accuracy of the factor representation .",
    "we also show how the random factor model converges toward the modeled data when the number of factors increases .",
    "random projection method preserves the structure of the data with high probability .",
    "we will show that a random factor model approximately preserves pair - wise correlations in the factor model , and make use of this fact in analyzing how random choice of factors influences the performance of a factor model .",
    "the article is structured as follows . in section 2",
    ", we develop the random factor model based on the random projection method and derive theoretical results describing the model ( more details can be found from appendix a ) . as an application of the random factor model , section 3 provides an analysis of the correlation matrix of russell 3,000 equity index using the factor models described in section 2 .",
    "the reproduction of data in a random factor model is compared with reproduction obtained using principal component analysis at both individual time - series level and at the dependence structure level .",
    "we analyze the ability of random factor models to reproduce equity log - return time - series and their correlations and covariances . in section 4 ,",
    "we compare different random factor models and show that the results , or rather their accuracy , are quite universal . in section 5",
    "we discuss the results and their possible implications .",
    "we also prove a version of the johnson - lindenstrauss - like theorem appropriate for the random factor model in appendix a. it gives probabilistic bounds on the accuracy of the correlation and covariance estimates .",
    "let @xmath0 observations of time - series @xmath1 be viewed as a vector in @xmath0-dimensional space @xmath2 , where each observation of the time - series corresponds to one coordinate of the vector .",
    "the set of @xmath3 such time - series can be packed into matrix @xmath4 , in which observations are in columns .",
    "we assume that the time - series data has been preprocessed , so that each time - series is averaged to zero , that is , @xmath5 for each @xmath6 .",
    "we employ population statistics in this study .",
    "definitions for mean @xmath7 , variance @xmath8 and covariance @xmath9 are @xmath10 where @xmath11 .",
    "the central parameters used in this study are summarized in table [ table:1 ] .",
    ".the central parameters used . [ cols=\"<,^\",options=\"header \" , ]      a linear factor model describes the target data set as a loading - weighted sum of factors ( e.g. , mcneil _ et al .",
    "_ , 2004 ) .",
    "let @xmath12\\in { { \\mathbb r}}^{d\\times k}$ ] contain @xmath0 observations of the factors @xmath13 , @xmath14 .",
    "then time - series @xmath15 , where @xmath16 is @xmath17:th column of matrix @xmath18 , @xmath6 , can be represented as a sum of products of factor loadings @xmath19 and factors @xmath20 , that is , @xmath21 where @xmath22 is an idiosyncratic risk component added for completeness . since we collect the observations into columns of @xmath18 and @xmath23 , the formula ( [ eq:1 ] ) is written in a matrix form as @xmath24 .",
    "random factors are here chosen using the random projection method .",
    "the key idea of random projection is based on the johnson - lindenstrauss lemma ( johnson and lindenstrauss , 1984 ; bingham and mannila , 2001 ) : if points in a high - dimensional space are projected onto a randomly selected subspace of suitably high dimension , then the distances between the points are approximately preserved .",
    "a suitably high - dimensional subspace has dimension proportional to @xmath25 , where @xmath3 is the number of time - series and @xmath26 the desired accuracy ( dasgupta and gupta , 2002 )",
    ".    random projection @xmath27 of matrix @xmath28 is a mapping defined by @xmath29 , where @xmath30 . here",
    "matrix @xmath31 is realization @xmath32 of a random variable - valued @xmath33 matrix .",
    "a large variety of probability distributions can be used to construct projection matrix @xmath31 ( more on this in section 3.3 ) .",
    "the most obvious choice is to assume that matrix @xmath31 is taken from the matrix - variate normal distribution with independent entries , that is , from @xmath34 ( gupta and nagar , 2000 ) .",
    "then each element is @xmath35-distributed and independent of other elements .",
    "one way to interpret random projection from @xmath2 to @xmath36 is to say that it consists of a random rotation in @xmath2 and a subsequent truncation to the first @xmath37 coordinates .",
    "we define the random factor model ( rfm ) for data set @xmath38 via a projection is not a projection matrix since it typically does not satisfy the equality @xmath39 .",
    "however , since its range is a lower dimensional subspace , we use the term `` projection '' also to describe @xmath40 , in analogy with the definition of the term `` random projection '' in section [ sec : randomproj ] . ]",
    "@xmath41 , @xmath42 where @xmath43 and @xmath44 is a normalization constant .",
    "mapping ( [ eq:2 ] ) can be interpreted as a linear factor model by setting @xmath45 where @xmath46 is a constant related to factor normalization , as discussed in section [ sec : orthogonality ]",
    ". then @xmath47 behaves as a matrix of @xmath37 @xmath0-dimensional factor time - series , with independent and identically distributed elements , and @xmath48 is a matrix of @xmath37 factor loadings for @xmath3 time - series .",
    "projection @xmath40 can be factored as @xmath49 defining @xmath50 yields an approximate factorization @xmath51 we will analyze this equation , and in particular @xmath52 , further in the following .    as an aside , let us mention that we could equally well have considered a random projection in the equity direction instead of the above time - series direction .",
    "this can be accomplished using a matrix @xmath53 , where @xmath44 and @xmath54 is a random matrix , and then considering @xmath55 as the projected matrix .",
    "this naturally leads to a factor model interpretation with a loading matrix @xmath56 and a factor matrix @xmath57 .",
    "in analogy to the earlier terminology , this model could be called _",
    "random loading model_. the properties proven later for the random projection @xmath40 then immediately carry over to the projection @xmath58 , one merely needs to replace `` @xmath0 '' with `` @xmath3 '' in all of the results .",
    "however , from the point of view of the time series , the two projection methods @xmath59 and @xmath55 could behave differently .",
    "for instance , if there are more pronounced correlations between different equities at a fixed time than between the same equity at two different times , then one would expect to need larger values of @xmath37 in the projection @xmath55 than in the projection @xmath59 to reach the same level of accuracy in the approximation .",
    "it is also possible to apply both random projections simultaneously and study @xmath60 instead of @xmath59 or @xmath55 .",
    "this double - sided projection would still have properties very similar to the one - sided projections , as long as the random matrices @xmath31 and @xmath61 are chosen independently of each other . since the three alternatives are on a technical level very similar , we focus only on the choice @xmath59 in the following .    as the next step , we need to find a suitable constant @xmath62 so that standard deviation , covariance and the expected value of the data are preserved , if possible . under these conditions , @xmath52 should be close to zero",
    ". different choices of @xmath62 yield slightly different properties for the rfm , but it turns out that we can not satisfy all these requirements at the same time if we base matrix @xmath31 on the normal distribution , representation ( [ eq:1 ] ) is exact on average , in the sense that @xmath63=x$ ] .",
    "however , this representation over - estimates the sample variance @xmath64 of a time - series @xmath65 , since then @xmath66=(1+d / k)\\sigma_x^2 $ ] .",
    "hence , the asset returns would fluctuate too much in this normalization in the typical regime where @xmath67 .",
    "in addition , although the projection would then produce the correct time - series on average , the actual values are dominated by fluctuations : the standard deviation of @xmath68 is at least @xmath69 and thus one given sample of the random factors is unlikely to be a useful representation of the data , unless @xmath37 is at least comparable to @xmath0 . ] .",
    "here we concentrate on preserving the covariance matrix @xmath70 in the projection .",
    "then normalization constant @xmath44 must be such that @xmath71=c_{x , y}\\ ] ] for any zero - mean vectors @xmath72 .",
    "theorem [ th : main ] in the appendix shows that this is possible but only if we choose @xmath73 .",
    "let @xmath62 have this value from this point on .",
    "the expected covariance between time - series @xmath74 and @xmath75 is then preserved in the rfm , regardless of the number of factors used .",
    "since @xmath76={\\mathbbm{e}}[c_{px , px}]=c_{x , x}=\\sigma^2_{x}$ ] , our choice of @xmath62 also preserves time - series variance .",
    "an application of the jensen s inequality then implies that @xmath77\\leq\\sigma_x$ ] , that is , volatility is not over - estimated .",
    "representation ( [ eq : reps ] ) always preserves the average of a zero - mean vector @xmath78 , that is , @xmath79=0 $ ] .",
    "in contrast , the @xmath80:th observation @xmath81 of time - series @xmath74 has an expectation @xmath82=\\sqrt{k/(k+d)}x_m$ ] and a variance @xmath83 . for a small number of factors , mapping to @xmath68 will on average underestimate the original value @xmath81 since @xmath84 . in the limit of large number of factors",
    "is not limited either by @xmath3 or by @xmath0 . ] , @xmath68 approaches @xmath81 , since @xmath85=\\lim_{k\\to\\infty}\\sqrt{k/(k+d)}x_m = x_m\\ , , \\ ] ] and the standard deviation of @xmath68 is @xmath86 and thus goes to zero when @xmath87 .",
    "hence , a rfm reproduces any vector @xmath88 component - by - component in the limit of large number of number of factors , for @xmath89 .",
    "thus @xmath90 of equation ( [ eq : reps ] ) approaches zero when the number of factors increases .",
    "the rfm is expected to reproduce mean , variance and covariance of time - series @xmath74 .",
    "component - wisely , the random factor model is expected to converge to the observed component values in the limit of large number of factors .",
    "johnson - lindenstrauss theorem ( johnson and lindenstrauss , 1984 ; dasgupta and gupta , 1999 ; matousek , 2008 ) gives probabilistic bounds for the accuracy of distance preservation in the random projection .",
    "a number of versions of johnson - lindestrauss theorem have been proven , however , in all versions known to us , it is assumed that random variables have zero expectation .",
    "matrix @xmath91 is a singular wishart matrix ( also known as an anti - wishart matrix ) , which has non - zero expectation , @xmath92 zero eigenvalues and @xmath37 non - zero eigenvalues . since matrix @xmath93 has non - zero expectation , it was not _ a priori _ clear if a johnson - lindenstrauss type theorem holds for it .",
    "theorem [ th : main ] proven in the appendix fills this gap for the present type of anti - wishart matrices , and it also contains a detailed derivation of the above expectation values for an arbitrary value of the scaling parameter @xmath62 .    we have collected in corollary [ th : coroll ] the corresponding results for the choice which preserves the sample covariance matrices in expectation , for @xmath73 .",
    "the precise control of fluctuations in the covariance estimates requires nontrivial combinatorial computations , given in the appendix .",
    "as proven in corollary [ th : coroll ] , for every @xmath94 and non - random vectors @xmath95 , with @xmath96 , we have @xmath97\\le \\frac{8}{k b^2 } \\sigma_u^2\\sigma_v^2 \\ , .\\ ] ] hence , if @xmath98 , the probability that covariance of vectors @xmath99 and @xmath100 is preserved more accurately than a bound @xmath17 is at least @xmath101 , where @xmath37 is the number of factors . in general , we can set @xmath102 , with @xmath103 , and also conclude that the accuracy , relative to the sample variance scale @xmath104 , is at least @xmath26 with a probability of at least @xmath105 . for the bound to be informative , it is necessary that @xmath106 .",
    "since our proof is based on the chebyshev inequality , there could still be room for improvement in the estimate .",
    "also , as noted in remark [ th : constantdisc ] after the proof , the prefactor @xmath107 above is not always optimal , and it could be reduced to @xmath108 in the regime @xmath109 .",
    "the error in the covariance estimate decreases at least inversely with the number of factors .",
    "given a sufficient number of factors , covariance of any two time - series @xmath2 can be approximated with arbitrarily high accuracy using the rfm .",
    "this and the fact that random factors are in no way fitted to the data suggest that the typical accuracy of the rfm depends mainly on the number of factors @xmath37 .",
    "corollary [ th : coroll ] also gives bounds on how accurately correlation between projected vectors is preserved .",
    "correlation @xmath110 coincides with covariance @xmath111 when @xmath112 .",
    "since then @xmath113 , inequality ( [ eq : t1 ] ) in fact gives an lower bound on how well @xmath114 approximates correlation between @xmath99 and @xmath100 .",
    "these results can be summarized as a statement about the projected matrix @xmath59 as follows : @xmath115\\ge 1-\\frac{8}{k { \\varepsilon}^2}\\ , , \\ ] ] valid for any @xmath116 and @xmath117 .",
    "orthogonality is a desirable property of a factor set .",
    "an orthogonalization procedure can be used to obtain an orthogonal factor set , but orthogonalization is computationally expensive .",
    "fortunately , orthogonalization is not a necessary step in the rfm .",
    "given any two random factors ( as defined above ) , their inner product is expected to be orthogonal , that is , @xmath118=(a')^2   \\sum_m{\\mathbbm{e}}\\left[b_{j'm}b_{jm}\\right]=(a')^2 d \\delta_{j',j}.\\ ] ] this shows that with the choice @xmath119 , the factors @xmath120 and @xmath20 are expected to be orthonormal as a consequence of the properties of normally distributed random variables .    using theorem a.1 ( first swap @xmath37 and @xmath0 , then set @xmath121 , and let @xmath99 vary over the unit vectors ) we can also compute the variance of the inner product .",
    "this yields @xmath122 = \\frac{1}{d } ( \\delta_{j',j } + 1 ) \\le   \\frac{2}{d}.\\ ] ] higher cumulants approach zero even more rapidly , as can be seen by analyzing the cumulant generating function @xmath123=\\begin{cases } -\\frac{d}{2}\\ln(1 - 2\\frac{\\lambda}{d } ) , \\quad \\text{when $ j = j'$,}\\\\ -\\frac{d}{2}\\ln(1-\\frac{\\lambda^2}{d^2 } ) , \\quad \\text{otherwise , } \\end{cases}\\ ] ] and its series expansion in @xmath124 .",
    "when @xmath125 , @xmath126th cumulant is of order @xmath127 . when @xmath128 , cumulants are of order @xmath127 for even @xmath126 and zero otherwise .",
    "convergence to the normal distribution in the limit of large @xmath0 then follows by the standard arguments ( e.g. , billingsley , 1995 ) .",
    "inner product matrix is approximately distributed as @xmath129 when @xmath0 is large ( @xmath130 ) , standard deviation is only a fraction of the expectation for diagonal elements .",
    "fluctuations around zero are small for non - diagonal elements .",
    "the cumulants show that the factors are almost orthonormal even at a relatively low dimension .",
    "in addition , the factors are on average orthogonal to the error term @xmath52 : since @xmath52 is an even polynomial of @xmath31:s and @xmath20 is linear in them , we have @xmath131=0 $ ] for all @xmath13 and @xmath132 .",
    "pca is a well - known technique which uses a linear transformation to form a simplified data set retaining the characteristics of the original data set ( see , e.g. , johnson and wichern , 2002 ) . in investment risk measurement",
    ", pca is used to explain the covariance structure of a set of market variables through a few linear combinations of these variables .",
    "the general objectives of using pca are to reduce the dimensions of covariance matrices and to find the main risk factors .",
    "the risk factors can then be used to analyze , e.g. , the investment risk sources of a portfolio , or to predict how the value of the portfolio will develop .",
    "projection to principal components is most directly obtained using the singular value decomposition ( golub and van loan , 1996 ) .",
    "given data matrix @xmath4 , svd decomposes it as @xmath133 , where @xmath134 is matrix of left singular vectors , @xmath135 is matrix of right singular vectors , and @xmath136 is the rectangular diagonal matrix of singular values .",
    "pca - based factor representation of @xmath18 is given by @xmath137 , where @xmath138 gives the factor loading matrix and @xmath139 defines the factors of the @xmath3 equities . when reducing the dimensions of the original dataset , the first @xmath37 principal components with the largest eigenvalues are chosen to represent the original dataset .",
    "this yields an approximation of the data matrix using a subset of factors . a @xmath37-factor approximation of matrix @xmath18 is given by @xmath140 , where @xmath141 contains the first @xmath37 factor loadings",
    ", @xmath142 contains the components of the first @xmath37 factors from @xmath143 .",
    "it can be shown that in the mean - error sense , pca gives the best linear @xmath37-factor approximation to matrix @xmath18 ( e.g. , reris and brooks , 2015 ; eckart and young , 1936 ) .",
    "principal components correspond to directions along which there is most variation in the data . however , there are no guarantees that pair - wise distances are preserved in pca .",
    "pca yields the relative importance of the most important risk sources ( defined in factor matrix @xmath23 ) in an investment portfolio .",
    "the relative importance of risk factors is shown by the size of eigenvalues in pca .",
    "the eigenvectors with highest eigenvalues correspond to the most important risk factors .",
    "loadings then tell how much investment instruments depend on these factors .          despite appearance , the rfm and pca share many features . in both models ,",
    "the data can represented as @xmath144 , where @xmath145 contains @xmath37 factor loadings and @xmath23 defines @xmath37 factor time - series with @xmath0 observations . in pca , the most important eigenvectors are found by choosing the largest eigenvalues .",
    "no such ordering is possible for random vectors .",
    "a random vector is as good as the next random vector .",
    "the rfm gives an _ almost _ orthonormal basis , while pca yields a strictly orthonormal basis .",
    "after finding the factors , both the rfm and pca project the data to these vectors . the ways in which the rfm and pca end up with representations of the data matrix are quite different : in pca",
    ", data is projected along principal components ( factors ) and only the desired set of these projections ( loadings ) are kept . in the rfm",
    ", the data is projected along random factors .",
    "hence , the main difference is the way that factors are chosen .",
    "the russell 3,000 index ( ticker ray in bloomberg ) measures the performance of the 3,000 largest us companies .",
    "the index represents about 98 percent of the investable us equity market . here",
    ", we investigate how well a random factor model reproduces log - returns of the russell 3,000 equities and their cross - correlations , and compare the results to those obtained using pca .    for our analyses ,",
    "we employ daily log - returns of russell 3,000 equities from 2000 - 01 - 03 to 2013 - 02 - 20 ( in total 3,305 observations ) .",
    "this interval contains several phases of the business cycle and certain special events , e.g. , the crash of september 2008 . of the 3,000 constituent time - series in the index , we used a subset of 1,591 time - series with continuous daily data covering the entire period . to apply the analysis methods , the data is normalized by subtracting mean of each return time - series and by dividing by its standard deviation .          fig .",
    "1 provides three examples of time - series reproduction using the rfm and pca .",
    "the rfm ( grey solid curve in fig .",
    "1 ) provides a reasonably good reproduction of the single time - series even with a low number of factors .",
    "the accuracy of the reproduction improves with the number of factors : the agreement of the rfm and the data is quite good with 500 factors .",
    "the number of factors is not limited to the number of time - series in the rfm , since the random factors do not necessarily span the entire space in which time - series may have values . only in the limit of large number of factors is the entire space covered .",
    "nevertheless , the performance of the random factor model is good .",
    "both pca and the rfm provide quite good reproductions of the data ( fig .  1 )",
    ", however , there are deviations from the data in each reproduction . in the root mean square error ( rmse )",
    "sense , pca gives a better reproduction of the time - series than the rfm ( fig .",
    "rmse in the reproduction of the entire data set is 0.79 in pca vs 1.37 in the rfm with 10 factors ( fig .",
    "1a ) .",
    "the rfm reproduces volatility of the time - series almost exactly even with a small number of factors , while in pca volatility estimates improve pronouncedly with more factors ( fig .",
    "since volatility of each time - series is normalized to 1 separately , accuracy of volatility reproduction is relative to volatilities of the underlying time - series in fig .",
    "in the rfm , error in volatility between correlation @xmath146 estimated from the modeled data and correlation @xmath147 computed from the original data . ] is about 3.1 percent of volatility with ten factors . in pca",
    ", error is about 41.7 percent of volatility with ten factors .",
    "accuracy increases until 1,000 factors is reached , after which essentially no error is observed in pca .",
    "while the rfm reproduces the overall volatility of the equity time - series faithfully , it does not capture time - dependence of volatility particularly well ( data not shown ) .",
    "3 shows the accuracy of reproduction of correlation coefficients in all analyzed pairs of stocks . in the rfm",
    ", the median error converges rapidly to zero with only a few factors .",
    "the 25th and 75th percentiles of error converge toward zero when the number of factors increases . together",
    "these three curves form a funnel ( fig .  3a ) that rapidly converges toward zero .",
    "this shows that the typical accuracy of the correlation coefficient reproduction increases rapidly with the number of factors .",
    "nevertheless , some noise persists in the rfm even with the `` full '' set of factors , for @xmath148 .    in pca ,",
    "median error approaches zero only with around 1,000 factors , which is largely a consequence of pca significantly underestimating volatilities of time - series .",
    "the 25th and 75th percentiles concentrate around the median away from zero in pca .",
    "3b shows results on absolute error in correlation coefficient . ] as a function of the number of factors . in the rfm ,",
    "correlation estimates converge toward the exact value when the number of factors is increased , however , convergence is less rapid than in analysis shown in fig .  3a .",
    "this is a result of the fact that error can be in either direction in the rfm .",
    "compared with pca , correlation estimates in the rfm converge significantly more rapidly toward the exact value .",
    "since error is always in the same direction in pca , there are no differences between absolute error and relative error in pca - based analyses .",
    "the rfm provides a more accurate description of correlation coefficients than pca , when the number of factors is less than about 500 .",
    "noise inherent in the random factor model has the consequence that the error in correlation estimates does not disappear in the rfm even with the full set of variables even though median estimate rapidly converges toward the observed correlation .",
    "the cross - over to regime where pca is more accurate occurs around 700 - 800 factors .",
    "when the number of factors is very high ( that is , over 1,000 ) , pca gives as good as or better correlation coefficients than the median estimate from the rfm .",
    "a factor model with this large number of factors is of little use in practical applications .",
    "the median error in covariance estimates converges rapidly toward zero in the rfm .",
    "the 25th and 75th percentiles form a funnel that converges toward zero when the number factors increases ( fig .",
    "despite the fact that pca is worse than the rfm in reproducing correlation coefficients , pca gives a better reproduction of covariance matrix ( fig .",
    "4b ) .",
    "the risk in the equity market is dominated by a single factor known as the market risk factor . to better analyze the other possible risk factors ,",
    "we subtract the first principal component , corresponding to the market risk factor , from the data and reanalyze the remaining data ( the `` reduced data '' ) .",
    "5a shows that pca becomes more accurate in reproducing the correlation coefficients when the impact of the market risk factor is removed from the data .",
    "perhaps more surprisingly , reproduction of data structure becomes equally accurate in the rfm and in pca with respect to both error measures in the correlation coefficient ( fig .",
    "5a and 5b ) .",
    "this suggests that the rfm and pca contain equal amounts of information about the correlations .    as a further check",
    ", we generated random data by sampling the normal probability distribution @xmath35 repeatedly .",
    "6 shows that the accuracy of both the rfm and pca is almost identical in this case .",
    "comparison with figures  5a and  5b shows that the accuracy of reproduction of the `` reduced '' russell correlations does not significantly differ from the accuracy of the random data .",
    "this indicates that the fluctuations around the market risk factor are largely a product of independent `` noise '' contributions .",
    "removal of the market risk factor from the data also influences the accuracy of covariance reproduction .",
    "pca is again more accurate than the rfm in covariance reproduction ( fig .",
    "5c and 5d ) . in this case , the median error of covariance matrix reproduction does not deviate from zero in the rfm , and the 25th and 75th percentiles are almost symmetrically around x - axis .",
    "a number of probability distributions have been found useful in the random projection method ( e.g. , achlioptas , 2003 ; kaski , 1998 ) .",
    "matousek ( 2008 ) found that almost any probability distribution with zero mean , unit variance , and subgaussian tail fulfills the requirements of the johnson - lindenstrauss theorem .",
    "these findings suggest that it may not matter much which probability distribution is used in the random projections . to find out whether this is the case in the rfm , we reanalyze the data using rfms based on six different probability distributions .",
    "we have also discussed some lowest order effects of varying the probability distribution , as well as reasons why deviation from a gaussian distribution leads only to small corrections , in remark [ th : nongauss ] after the proof in the appendix .",
    "the six probability distributions that we employ here are two sparse matrix models by achlioptas ( 2003 ) , a column - normalized gaussian model , a row - normalized gaussian model , the baseline gaussian model ( defined in sec .",
    "[ sec : randomfactormodel ] ) and a uniform model . in each case , the probability distribution is symmetric with respect to origin and such that the expectation is zero .",
    "each probability distribution also has a subgaussian tail .",
    "these rfms differ from the baseline gaussian rfm only by the construction of the random projection matrix @xmath31 , and by the normalization .",
    "the simplest specification for random projection is the `` random coin - flipping '' algorithm of achlioptas ( 2003 ) .",
    "it is defined by choosing each element @xmath149 of matrix @xmath31 independently according to rules : set @xmath150 with probability 0.5 and set @xmath151 with probability 0.5 .",
    "the second random projection that achlioptas ( 2003 ) proposes is based on a more sparse projection matrix defined by : set @xmath150 with probability 1/6 , set @xmath152 with probability 2/3 and set @xmath151 with probability 1/6 . again",
    "each element is chosen independently of the other elements .",
    "based on these random projections , we can define two rfms .",
    "in addition to the baseline gaussian rfm , we analyze two different rfms based on the normal distribution . in the first rfm",
    ", matrix @xmath31 is based on the spherical uniform distribution .",
    "the elements of matrix @xmath31 are defined by @xmath153 where @xmath154 are indendent and @xmath155 . in this rfm ,",
    "columns of matrix @xmath31 are normalized in such a way that their length is exactly one .    due to normalization of the columns of matrix @xmath31 , diagonal elements of matrix @xmath156",
    "behave as in an orthogonal matrix .",
    "then @xmath157 for all @xmath158 .",
    "non - diagonal elements of @xmath159 have expectation zero and variance proportional to @xmath160 ( kaski , 1998 ) .",
    "hence , non - diagonal elements of @xmath159 are approximately distributed according to zero - mean normal distribution at a relatively low dimension .",
    "therefore @xmath161 , where @xmath162 has non - zero elements only on off - diagonal , @xmath163=0 $ ] and @xmath164|<2/d$ ] .",
    "matrix @xmath31 is then almost orthonormal .",
    "the second rfm based on the gaussian probability distribution is a variation on the theme : instead of column - normalization in the first model , rows of projection matrix @xmath31 are normalized to unit length .",
    "this is the only difference between the two rfms , but it is sufficient to require a different normalization constant .      in the sixth considered rfm , projection matrix b is based on the continuous uniform probability distribution .",
    "each element in the projection matrix @xmath31 is chosen independently from the uniform distribution on interval @xmath165 $ ] , that is , @xmath166 for each @xmath167 .",
    "6 shows that all six rfms produce almost equally accurate results .",
    "to reduce noise , fig .",
    "6 shows results averaged over 50 sample runs . when the number of factors exceeds 10 , all rfms produce almost identical median accuracy .",
    "the only deviation is the column - normalized gaussian model , which deviates from the other rfms when the number of factors is less than 5 .",
    "all the other rfms produce identical results also in this regime .",
    "the accuracy of the 25th and 75th percentiles mainly depends on the number of factors , not much on the way factors are generated .",
    "the results suggest that the details of how the projection matrix is specified are not that important .",
    "almost any sufficiently random selection of random projection matrix ( when properly normalized ) produces a factor model , which preserves the approximate correlation structure .",
    "the main requirement here seems to be that matrix elements are chosen randomly and independently of other matrix elements .",
    "this supports the view that the rfm represents quite well how the bulk of factor models would describe the analyzed task .",
    "at the beginning of this study , we set out to analyze the impact of random factor selection on linear factor models .",
    "we were interested in whether and how randomness in the choice of factors impacts the reproduction of long equity time - series and , in particular , their interdependence .",
    "it may seem unlikely that a factor model with randomly chosen factors could be used for any kind of factor modeling . against this intuition",
    ", we demonstrated that random selection of factors yields a factor model that in some respects surpasses a pca based factor model ( sec .",
    "this result is also supported by ample theoretical work on the random projection method .",
    "the first reason for the ability of an rfm to capture the details of an equity time - series resides in the fact that random factors are , as a consequence of independence of elements , almost orthogonal to each other",
    ". there are more almost orthogonal factors than strictly orthogonal factors , which makes them significantly easier to find .",
    "the number of almost orthogonal vectors is higher in a higher - dimensional space , which reduces the impact of the `` curse of dimensionality '' ( bellman , 1957 ; indyk and motwani , 1998 ) and thereby makes data represention more feasible using the random factors in high dimensions .",
    "a suitably high number of random factors will then span a subspace sufficient to capture the return time - series at the desired accuracy .",
    "the influence of random selection of factors crucially depends on how universal accuracy of the data reproduction is in rfms .",
    "if accuracy is universal , it does not matter which rfm is chosen . on the other hand , if the accuracy is not universal at all , careful selection of factors should produce significantly better factor models .      in a classical fundamental factor model ,",
    "only a few factors are statistically significant .",
    "then , explanatory power of each factor should be large . in a statistical factor model ,",
    "a larger number of factors is often used , which also has the consequence that a larger ambiguity in the choice of factors is encountered .",
    "several different sets of factors may then provide almost equally good fit to the data . in a rfm",
    ", each factor has only a small explanatory power , which suggests that a large number of factor sets that provide essentially equally good descriptions of the data and its structure .",
    "this was observed in our computational experiments .",
    "the number of random factors is more important than fine - tuning of random factor time - series . the way a rfm is constructed is not important as long as elements of projection @xmath31 are independently drawn from a suitably regular probability distribution with zero expectation and sub - gaussian tails .",
    "regardless of the probability distribution used , we obtained almost identical results .",
    "these findings suggest that a kind of universality of rfms is present , at least with respect to correlation coefficients .",
    "the analysis of the proof of theorem a.1 ( see remark a.4 ) supports the view that universality is present with respect to probility distributions .",
    "the assumption that probability distribution are gaussian is not necessarily required in the theorem .",
    "the important assumption is the independence of the random matrix elements .    in our numerical experiments ,",
    "the results are largely dominated by a set of in a sense typical rfms that have rather similar accuracy of data reproduction .",
    "we have called this set of factor models the bulk .",
    "it is possible that an rfm would generate the same factors as pca , but it is highly unlikely , since pca produces sets of factors that are the best in the mean - error sense .",
    "rfms do not restrict the factors to optimize certain criterion .",
    "optimization produces sets of factors that are not typical , and hence not part of the bulk .      in our analyses",
    ", we compared factor modeling of daily russell 3,000 log - returns using both the random factor model and pca .",
    "pca reproduces individual data points of a time - series more accurately , but reproduction of cross - correlations of the time - series is not that good mainly due to underestimated volatility .",
    "the random projection method describes correlation structures and volatility well , but individual data points of time - series are reproduced less accurately . in other words , the rfm preserves the structure of the data but not necessarily the details of single time - series , while pca representation preserves the details but not necessarily the correlation structure .    the previous literature has compared the performance of the random projection method with pca , and found results similar to ours .",
    "bingham and mannila ( 2001 ) found that random projection method performed significantly better than pca in the compression of image data and in text clustering .",
    "goel et al .",
    "( 2005 ) found that random projection compares favorably with pca , although pca is more accurate with small number of dimensions .",
    "tang et al .",
    "( 2005 ) found that in text clustering a pca - based method provides better accuracy with small number of dimensions , while with high number of dimensions the random projection method dominates .",
    "degalla and bostrm ( 2006 ) found that in five image data sets and five micro array data sets , pca dominated with a small number of dimensions but its performance deteriorates with increasing the dimensions of the data ( cross - over occurs depending on the data set at 15150 dimensions ) , while random projection dominates at high number of dimensions",
    ". our findings are consistent with the results of these previous studies .",
    "pca requires @xmath168 operations , while the rfm requires @xmath169 operations , given the factor time - series .",
    "given that the number of factors is typically significantly smaller than dimension of data , the rfm is computationally much more efficient than pca .",
    "it took 9.9 seconds for the pca analysis to find 50 factors for a set of 1,591 time - series .",
    "for the rfm , a similar analysis took 1.21 seconds . in this case , the rfm was 8-fold faster than pca .",
    "the rfm is an alternative to pca with a lower computational cost and faithful reproduction of data structure , but less accuracy and sensitivity in individual time - series reproduction .",
    "the realm of rfms is the domain of huge data sets consisting of large number of long time - series .",
    "an rfm answers the question : how many factors will a generic linear factor model require to describe the data at a specific accuracy .",
    "the rfm becomes more accurate with high - dimensional data , and/or with high number of data series .",
    "this suggests that the choice of factors is most important when the number of both data points and time - series is low .",
    "this is the regime where pca - based factor model worked better than the rfm in our experiments",
    ". performance of pca deteriorates with the dimensionality of the data , while the performance in the random factors model depends less on the dimensionality .",
    "pca and the rfm are largely complementary : pca works best with relatively low - dimensional data , while the rfm works best with high or very high dimensional data .",
    "the authors thank dr .",
    "petri niininen for collecting the initial data set .",
    "the research of j.  lukkarinen has been supported by the academy of finland .",
    "jl is grateful to antti knowles for discussions and references about random matrix models .",
    ": :    achlioptas , d. , 2003 .",
    "database - friendly random projections :    johnson - lindenstrauss with binary coins . j. computer and system    sciences , 66 , 671687 .",
    ": :    alexander , c. , 2001 .",
    "market models : a guide to financial data    analysis .",
    "new york , ny : wiley .",
    ": :    alexander , c. , 2009 . market risk analysis iv : value - at - risk models .    new york , ny : wiley .",
    ": :    amador , j. , 2006 .",
    "random projection and orthonormality for lossy image    compression . image and vision computing 25 , 754766 : :    bellman , r. , 1957 .",
    "dynamic programming .",
    "princeton , nj : princeton    university press .",
    ": :    billingley , p. , 1995 . probability and measure , 3rd edition .",
    "new york ,    ny : wiley .",
    ": :    bingham , e. , mannila , h. , 2001 .",
    "random projection in dimensionality    reduction : applications to image and text data .",
    "7th acm sigkdd    int",
    ". conf . on knowledge discovery and data mining ( kdd-2001 ) , san",
    "francisco , ca , usa . 245250 .",
    ": :    blum , a. , 2006 . random projection , margins , kernels , and    feature - selection .",
    "lecture notes in computer science 3940 .",
    ": :    campbell , j. , lo , a. , mackinlay , a. , 1997 .",
    "the econometrics of    financial markets , new york , ny : cambridge university press .",
    ": :    dasgupta , s. gupta , a. , 2002 .",
    "an elementary proof of the    johnson - lindenstrauss lemma .",
    "random structures and algorithms , 22 ,    6065 . : :    degalla , s. , bostrm , h. , 2006 . reducing high - dimensional data by    principal component analysis vs. random projection for nearest    neighbor classification .",
    "proceedings of the 5th international    conference on machine learning and applications ( icmla06 ) .",
    ": :    eckart , c. , young , g. , 1936 .",
    "the approximation of one matrix by    another of lower rank , psychometrika 1 , 211218 .",
    ": :    fama , e. f. , french , k. r. , 1993 .",
    "common risk factors in the returns    on stocks and bonds , j. financial economics , 33 , 356 . : :    frankl , p. , maehara , h. , 1988 .",
    "the johnson - lindenstrauss lemma and the    sphericity of some graphs .",
    "j. combinatorial theory , 44 , 355362 .",
    ": :    golub , g. h. , van loan , c. f , 1996 .",
    "matrix computations , 3rd edition .",
    "johns hopkins university press , baltimore . : :    goel , n. , bebis , g. , nefian , a. , 2005 .",
    "face recognition experiments    with random projection .",
    "spie 5779 , 426 .",
    ": :    grinold , r. , kahn , r. , 2000",
    ". active portfolio management : a    quantitative approach for producing superior returns and controlling    risk , 2nd edition .",
    "mcgraw - hill .",
    ": :    gupta , a. , nagar , d. , 2000 .",
    "matrix variate distributions , chapman &    hall / crc .",
    ": :    hecht - nielsen , r. , 1994 .",
    "context vectors : general purpose approximate    meaning representations self - organized from raw data .",
    "computational    intelligence : imitating life ( 1994 ) : 4356 .",
    ": :    indyk , p. , and motwani , r. , 1998 .",
    "approximate nearst neighbors :    towards removing the curse of dimensionality .",
    "in stoc 98 proceedings    of the thirtieth annual acm symposium on theory of computing , 604613 . : :    janson , s. , 1997 .",
    "gaussian hilbert spaces .",
    "cambridge university press ,    1997 .",
    ": :    johnson , w. b. , lindenstrauss , j. , 1984 .",
    "extensions of lipshitz    mapping into hilbert space . in conference in modern analysis and    probability ,",
    "con . math .",
    "26 , 189206 .",
    ": :    johnson , r. a. , wichern , d. w. , 2002 .",
    "applied multivariate statistical    analysis , 5th edition .",
    "new jersey , prentice - hall .",
    ": :    kaski , s. , 1998 .",
    "dimensionality reduction by random mapping : fast    similarity computation for clustering .",
    "proc ijcnn98 , 1998 ieee intl    joint conf on neural networks .",
    ": :    kohonen , t. , kaski , s. , lagus , k. , salojarvi , j. , honkela , j. ,    paatero , v. , saarela , a. , 2000 .",
    "self organization of a massive    document collection .",
    "ieee transactions on neural networks , 11(3 ) ,    574585 .",
    ": :    laloux , l. , cizeau , p. , bouchaud , j .-",
    ". , potters , m. , 1999 .",
    "noise    dressing of financial correlation matrices , physics review letters 83 :    14671470 . : :    liu , k. , kargupta , h. , ryan , j. , 2006 .",
    "random projection - based    multiplicative data perturbation for privacy preserving distributed    data mining , ieee knowledge and data engineering .",
    ": :    lukkarinen , j. , marcozzi , m. , 2015 .",
    "wick polynomials and    time - evolution of cumulants , arxiv e - print , arxiv.org:1503.05851 : :    lukkarinen , j. , marcozzi , m. , nota , a. , 2016 .",
    "summability of joint    cumulants of nonindependent lattice fields , arxiv e - print ,    arxiv.org:1601.08163 : :    markowitz , h. , 1952 .",
    "portfolio selection , the journal of finance 7 :    7791 .",
    ": :    malevergne , y. , sornette , d. , 2004 . collective origin of the    coexistence of apparent random matrix theory noise and of factors in    large sample correlation matrices .",
    "physica a : statistical mechanics    and its applications , 331(3 ) , 660668 .",
    ": :    marchenko , v.a .",
    ", pastur , l. a. , 1967 .",
    "distribution of eigenvalues for    some sets of random matrices .",
    "( n.s . ) , 72 , 507536 .",
    ": :    matousek , j. , 2008 . on variants of the johnson",
    " lindenstrauss lemma .",
    "random structures and algorithms 33 , 142156 .",
    ": :    mcneil , a. j. , rdiger , f. , embrechts , p. , 2010 . quantitative risk    management : concepts , techniques , and tools .",
    "princeton university    press .",
    ": :    peccati , g. , taqqu , m. , 2011 .",
    "wiener chaos : moments , cumulants and    diagrams : a survey with computer implementation .",
    "bocconi & springer    series .",
    ": :    plerou , v. , gopikrishnan , p. , rosenow , b. , amaral , l. a. n. , stanley ,    h. e. , 1999 .",
    "universal and nonuniversal properties of cross    correlations in financial time series . physical review letters 83 :    1471 . : :    reris , r. , brooks , j. p. , 2015 . principal component analysis and    optimization : a tutorial .",
    "14th informs computing society conference    richmond , virginia , january 1113 , 212225 .",
    ": :    ross , s , 1976 . the arbitrage theory of capital asset pricing .",
    "j.    economic theory , 13 , 341360 .",
    ": :    sharpe , w. , 1964 , capital asset prices : a theory of market equilibrium    under conditions of risk , journal of finance 19 , 425 .",
    ": :    tang , b. , shepherd , m. , milios , e. , heywood , m. i. , 2005 . comparing    and combining dimension reduction techniques for efficient text    clustering .",
    "proceedings of the workshop on feature selection for data    mining : interfacing machine learning and statistics .",
    ": :    vempala , s. , 2005 .",
    "the random projection method .",
    "providence , ri :    american mathematical society .",
    "we prove here the following result about the mean and variance of the projection operators involved in the random factor models .        1 .",
    "@xmath174 = a k u_m$ ] for all @xmath80 , and @xmath175= a k \\mu_u$ ] .",
    "@xmath176 for all @xmath80 .",
    "@xmath177= a^2 k ( d+k ) c_{u , v } + a^2 k ( d-1 ) \\mu_u \\mu_v$ ] .",
    "4 .   if @xmath178 and @xmath179 , then @xmath180 .",
    "_ proof of the corollary : _   with the added assumptions @xmath178 , the definition of the sample variance yields the identities @xmath186 and @xmath187 .",
    "thus items 1 , 2 and 3 are immediate corollaries using @xmath188 . on the other hand ,",
    "if we denote the standard deviation of @xmath114 by @xmath189 , then by chebyshev s inequality we have @xmath190\\le c^{-2}$ ] for any @xmath191 . applying this with @xmath192",
    "thus implies @xmath193\\le s^2 b^{-2}$ ] .",
    "hence , item 4 of the theorem implies the bound in the last item of the corollary .",
    "_ proof of the theorem : _",
    "assume @xmath194 be given as in the theorem , and define @xmath195 , @xmath196 and @xmath197 .",
    "@xmath198 , @xmath199 , @xmath200 , and @xmath9 are then all real - valued random variables .    the following results could also be proven by straightforward but rather lengthy direct estimates relying on wick s product rule , i.e. , isserlis theorem , valid for the present gaussian random variables @xmath201 .",
    "a better control of the associated combinatorics is obtained by using wick polynomial expansions instead . for the present case of gaussian random variables",
    "wick polynomials reduce to hermite polynomials ; appendix a of ( lukkarinen _ et al . _ , 2016 ) provides a quick summary of the definition and main properties of general wick polynomials , and we refer to ( janson , 1997 ; peccati and taqqu , 2011 ; lukkarinen and marcozzi , 2015 ) for more detailed expositions .",
    "we rely on the following wick polynomial expansion of arbitrary centered expectations of monomials of random variables : for any product of random variables @xmath202 , with the corresponding index set @xmath203 , one has @xmath204",
    "= \\sum_{\\emptyset\\ne e \\subset i } { \\mathbbm{e}}\\biggl[\\prod_{i\\in i\\setminus e } x_i\\biggr ] \\w{\\prod_{i\\in e } x_i}\\ , .\\end{aligned}\\ ] ]    for any @xmath200 , the definitions yield @xmath205 since @xmath201 are i.i.d .",
    "centered , normalized gaussian , this implies @xmath206 = a \\sum_{n=1}^d \\sum_{j=1}^k { { \\mathbbm 1}_{n = m } } u_n = a k u_m\\ , , \\end{aligned}\\ ] ] where @xmath207 stands for an indicator function having value @xmath208 , when @xmath209 , and @xmath210 otherwise . thus @xmath175",
    "= \\frac{1}{d } \\sum_{n=1}^d { \\mathbbm{e}}[z_m ] = a k \\mu_u$ ] as @xmath211=0 $ ] , centering the variable @xmath198 yields the following simple wick polynomial expansion : @xmath212 = a \\sum_{n=1}^d   u_n \\sum_{j=1}^k \\w{b_{jm } b_{jn}}\\ , .\\end{aligned}\\ ] ]    the usefulness of wick polynomials lies in the property that their products satisfy the same moments - to - cumulants expansion as simple products , with the additional rule that _ any partition with a cluster of indices inside one of the wick polynomials will be missing _ from the expansion . for instance , for any random variables @xmath213which need not be independent nor gaussian ",
    "one has @xmath214 = \\kappa[x_1,x_2,x_3,x_4 ] + \\kappa[x_1,x_3]\\kappa[x_2,x_4 ] +   \\kappa[x_1,x_4]\\kappa[x_2,x_3 ] \\ , , \\end{aligned}\\ ] ] where @xmath215 denotes a cumulant .",
    "hence , for instance , @xmath216=\\operatorname*{cov}(x_1,x_3)$ ] . applying this to the @xmath31-variables yields @xmath217 = { { \\mathbbm 1}_{j'=j , m'=m}}{{\\mathbbm 1}_{j'=j , n'=n } } + { { \\mathbbm 1}_{j'=j , n'=m}}{{\\mathbbm 1}_{j'=j ,",
    "n = m ' } } \\ , , \\end{aligned}\\ ] ] since for gaussian random variables the fourth cumulant is equal to zero .",
    "therefore , by ( [ eq : centredz ] ) and ( [ eq : fourbs ] ) , we have @xmath218)(z'_{m ' } - { \\mathbbm{e}}[z'_{m ' } ] ) ]   \\nonumber\\\\ & \\quad = a^2 \\sum_{n',n=1}^d   u_n   v_{n ' } \\sum_{j',j=1}^k { \\mathbbm{e}}[\\w{b_{jm } b_{jn}}\\w{b_{j'm ' } b_{j'n ' } } ] \\nonumber\\\\ & \\quad",
    "= a^2 \\left(k { { \\mathbbm 1}_{m'=m}}\\sum_{n=1}^d   u_n v_n + k u_{m ' } v_{m}\\right ) = a^2 k \\left({{\\mathbbm 1}_{m'=m } } u\\cdot v + u_{m ' } v_{m}\\right)\\ , , \\end{aligned}\\ ] ] and thus , in particular , @xmath219)^2 ]   =   a^2 k \\left(|u|^2 +    u_m^2\\right)\\ , .\\end{aligned}\\ ] ] therefore",
    ", we have now proven the first two items of the theorem .",
    "the combinatorics gets progressively heavier in the remaining two items .",
    "let us begin with the scalar product @xmath220 where @xmath221=z_m - a k u_m$ ] , @xmath222=z'_m - a k v_m$ ] denote the centered variables .",
    "taking an expectation and using ( [ eq : zzpcov ] ) for @xmath223 thus yields @xmath224= \\sum_{m=1}^d",
    "a^2 k \\left(u\\cdot v + u_{m } v_{m}\\right ) + a^2 k^2   u\\cdot v = a^2 k ( d+ 1 + k ) u\\cdot v    \\ , .\\end{aligned}\\ ] ]    the definition of @xmath9 reads explicitly @xmath225 to compute its expectation , we still need to evaluate @xmath226 = \\operatorname*{cov}[\\mu_{z } , \\mu_{z ' } ] + { \\mathbbm{e } } [ \\mu_{z } ] { \\mathbbm{e } } [ \\mu_{z ' } ]   = \\frac{1}{d^2}\\sum_{m',m=1}^d \\operatorname*{cov}[z_m , z'_{m ' } ] + a^2 k^2 \\mu_u \\mu_v   \\nonumber\\\\ & \\quad = \\frac{a^2 k } { d^2}\\sum_{m',m=1}^d \\left({{\\mathbbm 1}_{m'=m } } u\\cdot v + u_{m ' } v_{m}\\right ) + a^2 k^2 \\mu_u \\mu_v = \\frac{a^2 k } { d } u\\cdot v + a^2 k ( k+1 ) \\mu_u \\mu_v\\ , .\\end{aligned}\\ ] ] therefore , @xmath227 = \\frac{a^2 k}{d } ( d+k ) u\\cdot v - a^2 k ( k+1 ) \\mu_u \\mu_v   = a^2 k ( d+k ) c_{u , v } + a^2 k ( d-1 ) \\mu_u",
    "\\mu_v   \\ , , \\end{aligned}\\ ] ] where in the last step we have used the identity @xmath228 .    for the final result , let us assume in addition that @xmath178 . to avoid iterated wick polynomials ,",
    "let us begin with @xmath229 and express the two terms separately in wick form .",
    "namely , now @xmath230 where the product of four @xmath31-factors can be expanded using wick polynomial expansion ( [ eq : wickpexp ] ) .",
    "since only expectations of products of even number of @xmath31:s can be non - zero , we obtain @xmath231   \\nonumber\\\\ & \\quad   = \\w{b_{jm } b_{jn } b_{j'm ' } b_{j'n ' } }    + { \\mathbbm{e}}[b_{jm } b_{jn}]\\w{b_{j'm ' } b_{j'n ' } }    + { \\mathbbm{e}}[b_{jm } b_{j'm'}]\\w { b_{jn } b_{j'n ' } }    \\nonumber\\\\ & \\qquad   + { \\mathbbm{e}}[b_{jm } b_{j'n ' } ] \\w{b_{jn } b_{j'm ' } }    + { \\mathbbm{e}}[b_{jn } b_{j'm'}]\\w { b_{jm }   b_{j'n ' } }    \\nonumber\\\\ & \\qquad   + { \\mathbbm{e}}[b_{jn } b_{j'n'}]\\w{b_{jm }   b_{j'm ' } }    + { \\mathbbm{e}}[b_{j'm ' } b_{j'n'}]\\w{b_{jm } b_{jn } }    \\nonumber\\\\ & \\quad   = \\w{b_{jm } b_{jn } b_{j'm ' } b_{j'n ' } }    + { { \\mathbbm 1}_{m = n}}\\w{b_{j'm ' } b_{j'n ' } }    + { { \\mathbbm 1}_{m'=n'}}\\w{b_{jm } b_{jn } }    \\nonumber\\\\ & \\qquad   + { { \\mathbbm 1}_{j'=j}}\\bigl [   { { \\mathbbm 1}_{m = m'}}\\w { b_{jn } b_{j n ' } }    + { { \\mathbbm 1}_{m = n ' } } \\w{b_{jn } b_{j m ' } }     + { { \\mathbbm 1}_{m'=n } } \\w { b_{jm }   b_{j n ' } }    + { { \\mathbbm 1}_{n = n ' } } \\w{b_{jm }   b_{jm ' } } \\bigr ] \\ ] ] therefore , @xmath232 = \\frac{a^2}{d^2 } \\sum_{n',n=1}^d v_{n ' } u_n \\sum_{m',m=1}^d \\sum_{j',j=1}^k \\w{b_{jm } b_{jn } b_{j'm ' } b_{j'n ' } }    \\nonumber\\\\ & \\qquad   + \\frac{a^2}{d } \\sum_{n',n=1}^d v_{n ' } u_n \\sum_{j=1}^k \\w { b_{jn } b_{j n ' } }    + \\frac{a^2}{d } c_{u , v } \\sum_{m',m=1}^d \\sum_{j=1}^k \\w{b_{jm }   b_{j m ' } } \\ , , \\end{aligned}\\ ] ] where we have applied the assumptions @xmath96 .    similarly , since @xmath233 and by ( [ eq : fourbwick ] ) @xmath234   \\nonumber\\\\ & \\quad   = \\w{b_{jm } b_{jn } b_{j'm } b_{j'n ' } }    + { { \\mathbbm 1}_{m = n}}\\w{b_{j'm } b_{j'n ' } }    + { { \\mathbbm 1}_{m = n'}}\\w{b_{jm } b_{jn } }    \\nonumber\\\\ & \\qquad   + { { \\mathbbm 1}_{j'=j}}\\bigl [   \\w { b_{jn } b_{j n ' } }    + { { \\mathbbm 1}_{m = n ' } } \\w{b_{jn } b_{j m } }    + { { \\mathbbm 1}_{m = n } } \\w { b_{jm }   b_{j n ' } }    + { { \\mathbbm 1}_{n = n ' } } \\w{b_{jm }   b_{j m } } \\bigr ] \\ , , \\ ] ] we obtain a wick polynomial expansion @xmath235",
    "= \\frac{a^2}{d } \\sum_{n',n=1}^d v_{n ' } u_n   \\sum_{m=1}^d \\sum_{j',j=1}^k \\w{b_{jm } b_{jn } b_{j'm } b_{j'n ' } }   \\nonumber\\\\ & \\qquad + \\frac{a^2}{d } ( 2 k + d + 2 )   \\sum_{n',n=1}^d v_{n ' } u_n \\sum_{j=1}^k \\w{b_{jn } b_{jn ' } }   + a^2 c_{u , v } \\sum_{m=1}^d \\sum_{j=1}^k \\w{b_{jm }   b_{j m } } \\ , .\\end{aligned}\\ ] ]    combining the above results together finally yields a wick polynomial expansion for the centered @xmath9 , @xmath236 =   \\frac{a^2}{d } \\sum_{n',n=1}^d v_{n ' } u_n",
    "\\sum_{m=1}^d \\sum_{j',j=1}^k \\w{b_{jm } b_{jn } b_{j'm } b_{j'n ' } }   \\nonumber\\\\ & \\qquad -\\frac{a^2}{d^2 } \\sum_{n',n=1}^d v_{n ' } u_n \\sum_{m',m=1}^d \\sum_{j',j=1}^k \\w{b_{jm } b_{jn } b_{j'm ' } b_{j'n ' } }    \\nonumber\\\\ & \\qquad + \\frac{a^2}{d } ( 2 k + d + 1 )   \\sum_{n',n=1}^d v_{n ' } u_n \\sum_{j=1}^k \\w{b_{jn } b_{jn ' } }    \\nonumber\\\\ & \\qquad + a^2 c_{u , v } \\sum_{m=1}^d",
    "\\sum_{j=1}^k \\w{b_{jm }   b_{j m } }   - \\frac{a^2}{d } c_{u , v } \\sum_{m',m=1}^d",
    "\\sum_{j=1}^k \\w{b_{jm }   b_{j m ' } } \\ , .\\end{aligned}\\ ] ] we use this formula to compute @xmath237)^2]$ ] . in the expanded formula terms containing a product of different degree wick polynomials yield zero since whatever three pairings is used for the six @xmath31-factors , one of these pairings connects two elements inside the degree four wick polynomial .",
    "hence , for instance , @xmath238=0 $ ] .",
    "the products of second order terms turn out to yield the dominant contribution .",
    "after first taking out a factor @xmath239 , it reads explicitly @xmath240    \\nonumber\\\\ & \\quad = d^2 ( 2 k + d + 1)^2 \\sum_{n'_1,n_1,n'_2,n_2=1}^d v_{n'_1 } u_{n_1 } v_{n'_2 } u_{n_2 }   \\sum_{j_1,j_2=1}^k { \\mathbbm{e}}[\\w{b_{j_1n_1 } b_{j_1n'_1 } } \\w{b_{j_2n_2 } b_{j_2n'_2 } } ]   \\nonumber\\\\ & \\qquad + d^2 ( u\\cdot v)^2 \\sum_{m_1,m_2=1}^d \\sum_{j_1,j_2=1}^k { \\mathbbm{e}}[\\w{b_{j_1 m_1 } b_{j_1 m_1 } } \\w{b_{j_2 m_2 } b_{j_2 m_2 } } ]   \\nonumber\\\\ & \\qquad + ( u\\cdot v)^2 \\sum_{m'_1,m_1,m'_2,m_2=1}^d \\sum_{j_1,j_2=1}^k { \\mathbbm{e}}[\\w{b_{j_1 m_1 } b_{j_1 m'_1 } } \\w{b_{j_2 m_2 } b_{j_2 m'_2 } } ]   \\nonumber\\\\ & \\qquad + 2 d^2 ( 2 k + d + 1 ) u\\cdot v \\sum_{n',n=1}^d v_{n ' } u_n \\sum_{j , j_2=1}^k \\sum_{m_2=1}^d { \\mathbbm{e}}[\\w{b_{jn } b_{jn ' } } \\w{b_{j_2m_2 }   b_{j_2 m_2 } } ]   \\nonumber\\\\ & \\qquad - 2 d ( 2 k + d + 1 ) u\\cdot v \\sum_{n',n=1}^d v_{n ' } u_n \\sum_{j , j_2=1}^k \\sum_{m_2.m'_2=1}^d { \\mathbbm{e}}[\\w{b_{jn } b_{jn ' } } \\w{b_{j_2m_2 }   b_{j_2 m'_2 } } ]   \\nonumber\\\\ & \\qquad - 2 d ( u\\cdot v)^2 \\sum_{j , j_2=1}^k \\sum_{m , m_2.m'_2=1}^d{\\mathbbm{e}}[\\w{b_{jm }   b_{j m } } \\w{b_{j_2m_2 }   b_{j_2 m'_2 } } ] ,   \\ ] ] which simplifies to @xmath241 ( u\\cdot v)^2 \\ , .\\end{aligned}\\ ] ]    in the remaining products , the allowed pairings are in one - to - one correspondence to permutations where each factor in the left product is paired with the factor in its `` permuted '' position in the right product .",
    "thus for order four terms we obtain a sum over @xmath242 terms , namely , @xmath243    \\nonumber\\\\ & \\quad   = { { \\mathbbm 1}_{j_1=j_2,j'_1=j'_2 } } \\bigl[{{\\mathbbm 1}_{m_1=m_2,n_1=n_2 } } + { { \\mathbbm 1}_{m_1=n_2,n_1=m_2 } } \\bigr ]   \\times   \\bigl[{{\\mathbbm 1}_{m'_1=m'_2,n'_1=n'_2 } } + { { \\mathbbm 1}_{m'_1=n'_2,n'_1=m'_2}}\\bigr ]   \\nonumber\\\\ & \\qquad   + { { \\mathbbm 1}_{j_1=j_2=j'_1=j'_2 } } { { \\mathbbm 1}_{m_1=m_2,n_1=m'_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=n_2,n'_1=n'_2 } } + { { \\mathbbm 1}_{m'_1=n'_2,n'_1=n_2}}\\bigr ]   \\nonumber\\\\ & \\qquad   + { { \\mathbbm 1}_{j_1=j_2=j'_1=j'_2 } } { { \\mathbbm 1}_{m_1=m_2,n_1=n'_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=n_2,n'_1=m'_2 } } + { { \\mathbbm 1}_{m'_1=m'_2,n'_1=n_2}}\\bigr ]   \\nonumber\\\\ & \\qquad   + { { \\mathbbm 1}_{j_1=j_2=j'_1=j'_2 } } { { \\mathbbm 1}_{m_1=n_2,n_1=m'_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=m_2,n'_1=n'_2 } } + { { \\mathbbm 1}_{m'_1=n'_2,n'_1=m_2}}\\bigr ]   \\nonumber\\\\ & \\qquad   + { { \\mathbbm 1}_{j_1=j_2=j'_1=j'_2 } } { { \\mathbbm 1}_{m_1=n_2,n_1=n'_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=m_2,n'_1=m'_2 } } + { { \\mathbbm 1}_{m'_1=m'_2,n'_1=m_2}}\\bigr ]   \\nonumber\\\\ & \\qquad   + { { \\mathbbm 1}_{j_1=j'_2,j'_1=j_2 } } \\bigl[{{\\mathbbm 1}_{m_1=m'_2,n_1=n'_2 } } + { { \\mathbbm 1}_{m_1=n'_2,n_1=m'_2 } } \\bigr ]   \\times   \\bigl[{{\\mathbbm 1}_{m'_1=m_2,n'_1=n_2 } } + { { \\mathbbm 1}_{m'_1=n_2,n'_1=m_2}}\\bigr ]   \\nonumber\\\\ & \\qquad   + { { \\mathbbm 1}_{j_1=j_2=j'_1=j'_2 } } { { \\mathbbm 1}_{m_1=m'_2,n_1=m_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=n_2,n'_1=n'_2 } } + { { \\mathbbm 1}_{m'_1=n'_2,n'_1=n_2}}\\bigr ]   \\nonumber\\\\ & \\qquad   + { { \\mathbbm 1}_{j_1=j_2=j'_1=j'_2 } } { { \\mathbbm 1}_{m_1=m'_2,n_1=n_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=n'_2,n'_1=m_2 } } + { { \\mathbbm 1}_{m'_1=m_2,n'_1=n'_2}}\\bigr ]   \\nonumber\\\\ & \\qquad   + { { \\mathbbm 1}_{j_1=j_2=j'_1=j'_2 } } { { \\mathbbm 1}_{m_1=n'_2,n_1=m_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=m'_2,n'_1=n_2 } } + { { \\mathbbm 1}_{m'_1=n_2,n'_1=m'_2}}\\bigr ]   \\nonumber\\\\ & \\qquad   + { { \\mathbbm 1}_{j_1=j_2=j'_1=j'_2 } } { { \\mathbbm 1}_{m_1=n'_2,n_1=n_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=m_2,n'_1=m'_2 } } + { { \\mathbbm 1}_{m'_1=m'_2,n'_1=m_2}}\\bigr ] \\ , .\\end{aligned}\\ ] ] therefore , @xmath244",
    "\\nonumber\\\\ & \\quad   = k^2 \\bigl\\ { \\bigl[{{\\mathbbm 1}_{m_1=m_2,n_1=n_2 } } + { { \\mathbbm 1}_{m_1=n_2,n_1=m_2 } } \\bigr ]   \\times   \\bigl[{{\\mathbbm 1}_{m'_1=m'_2,n'_1=n'_2 } } + { { \\mathbbm 1}_{m'_1=n'_2,n'_1=m'_2}}\\bigr ]   \\nonumber\\\\ & \\qquad\\quad   +   \\bigl[{{\\mathbbm 1}_{m_1=m'_2,n_1=n'_2 } } + { { \\mathbbm 1}_{m_1=n'_2,n_1=m'_2 } } \\bigr ]   \\times   \\bigl[{{\\mathbbm 1}_{m'_1=m_2,n'_1=n_2 } } + { { \\mathbbm 1}_{m'_1=n_2,n'_1=m_2}}\\bigr ] \\bigr\\ }   \\nonumber\\\\ & \\qquad    + k \\bigl\\ { { { \\mathbbm 1}_{m_1=m_2,n_1=m'_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=n_2,n'_1=n'_2 } } + { { \\mathbbm 1}_{m'_1=n'_2,n'_1=n_2}}\\bigr ]   \\nonumber\\\\ & \\qquad\\quad   +   { { \\mathbbm 1}_{m_1=m_2,n_1=n'_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=n_2,n'_1=m'_2 } } + { { \\mathbbm 1}_{m'_1=m'_2,n'_1=n_2}}\\bigr ]   \\nonumber\\\\ & \\qquad\\quad   +   { { \\mathbbm 1}_{m_1=n_2,n_1=m'_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=m_2,n'_1=n'_2 } } + { { \\mathbbm 1}_{m'_1=n'_2,n'_1=m_2}}\\bigr ]   \\nonumber\\\\ & \\qquad\\quad   +   { { \\mathbbm 1}_{m_1=n_2,n_1=n'_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=m_2,n'_1=m'_2 } } + { { \\mathbbm 1}_{m'_1=m'_2,n'_1=m_2}}\\bigr ]    \\nonumber\\\\ & \\qquad\\quad   +   { { \\mathbbm 1}_{m_1=m'_2,n_1=m_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=n_2,n'_1=n'_2 } } + { { \\mathbbm 1}_{m'_1=n'_2,n'_1=n_2}}\\bigr ]   \\nonumber\\\\ & \\qquad\\quad   +   { { \\mathbbm 1}_{m_1=m'_2,n_1=n_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=n'_2,n'_1=m_2 } } + { { \\mathbbm 1}_{m'_1=m_2,n'_1=n'_2}}\\bigr ]   \\nonumber\\\\ & \\qquad\\quad   +   { { \\mathbbm 1}_{m_1=n'_2,n_1=m_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=m'_2,n'_1=n_2 } } + { { \\mathbbm 1}_{m'_1=n_2,n'_1=m'_2}}\\bigr ]   \\nonumber\\\\ & \\qquad\\quad   +   { { \\mathbbm 1}_{m_1=n'_2,n_1=n_2 } }   \\bigl[{{\\mathbbm 1}_{m'_1=m_2,n'_1=m'_2 } } + { { \\mathbbm 1}_{m'_1=m'_2,n'_1=m_2}}\\bigr]\\bigr\\ }   \\ , .\\end{aligned}\\ ] ]    for the terms involving @xmath245 we also need restrictions of this result to cases where @xmath246 or @xmath247 : @xmath248   \\nonumber\\\\ & \\quad   = ( k^2+k ) \\bigl[{{\\mathbbm 1}_{m'_2=m_2,n_2=n_1,n'_2=n'_1 } } +    { { \\mathbbm 1}_{m'_2=n'_1,m_2=n'_2,n_2=n_1 } } +   { { \\mathbbm 1}_{m'_2=n_2,m_2=n_1,n'_2=n'_1 } } +   \\nonumber\\\\ & \\qquad\\quad   +    { { \\mathbbm 1}_{m'_2=n'_1,m_2=n_1,n'_2=n_2 } } +   { { \\mathbbm 1}_{m'_2=m_2,n_2=n'_1,n'_2=n_1 } } +   { { \\mathbbm 1}_{m'_2=n_2,m_2=n'_1,n'_2=n_1 } }    \\nonumber\\\\ & \\qquad\\quad   +   { { \\mathbbm 1}_{m'_2=n_1,m_2=n'_2,n_2=n'_1 } } +   { { \\mathbbm 1}_{m'_2=n_1,m_2=n'_1,n'_2=n_2 } }    \\bigr ]   \\nonumber\\\\ & \\qquad    + 2 k \\bigl [    { { \\mathbbm 1}_{m'_2=n_1,m_2=n_2,n'_2=n'_1 } } +    { { \\mathbbm 1}_{m'_2=n'_1,m_2=n_2,n'_2=n_1 } }    \\nonumber\\\\ & \\qquad\\quad    + { { \\mathbbm 1}_{m'_2=n'_2,m_2=n_1,n_2=n'_1 } } +    { { \\mathbbm 1}_{m'_2=n'_2,m_2=n'_1,n_2=n_1 } }    \\bigr ] \\ , .\\end{aligned}\\ ] ] then we can collect the three terms needed here which are @xmath249   \\nonumber\\\\ & \\quad   = k(k+1)\\bigl[d { { \\mathbbm 1}_{n_2=n_1,n'_2=n'_1 } } +    { { \\mathbbm 1}_{n'_1=n'_2,n_2=n_1 } }     +    { { \\mathbbm 1}_{n_2=n_1,n'_2=n'_1 } }    +   { { \\mathbbm 1}_{n'_1=n_1,n'_2=n_2 } }   \\nonumber\\\\ & \\qquad\\quad   +   d { { \\mathbbm 1}_{n_2=n'_1,n'_2=n_1 } } +   { { \\mathbbm 1}_{n_2=n'_1,n'_2=n_1 } }    +   { { \\mathbbm 1}_{n_1=n'_2,n_2=n'_1 } } +   { { \\mathbbm 1}_{n_1=n'_1,n'_2=n_2 } }    \\bigr ]   \\nonumber\\\\ & \\qquad    + 2 k \\bigl [    { { \\mathbbm 1}_{n_1=n_2,n'_2=n'_1 } } +    { { \\mathbbm 1}_{n'_1=n_2,n'_2=n_1 } }     + { { \\mathbbm 1}_{n'_2=n_1,n_2=n'_1 } } +    { { \\mathbbm 1}_{n'_2=n'_1,n_2=n_1 } }    \\bigr ]   \\nonumber\\\\ & \\quad    = [ ( d+2)k(k+1)+4 k ] { { \\mathbbm 1}_{n_2=n_1,n'_2=n'_1 } }    + 2k(k+1 ) { { \\mathbbm 1}_{n'_2=n_2,n'_1=n_1 } }    \\nonumber\\\\ & \\qquad   + [ ( d+2)k(k+1)+4 k ] { { \\mathbbm 1}_{n_2=n'_1,n'_2=n_1 } }   \\ , , \\end{aligned}\\ ] ] @xmath250   \\nonumber\\\\ & \\quad   = k(k+1 ) \\bigl[d { { \\mathbbm 1}_{n_2=n_1,n'_2=n'_1 } } +    { { \\mathbbm 1}_{n_2=n_1 } }    +    { { \\mathbbm 1}_{n'_2=n'_1 } } +   { { \\mathbbm 1}_{n'_2=n_2 } }    \\nonumber\\\\ & \\qquad\\quad   + d   { { \\mathbbm 1}_{n_2=n'_1,n'_2=n_1 } } +   { { \\mathbbm 1}_{n'_2=n_1 } }    +   { { \\mathbbm 1}_{n_2=n'_1 } } +   { { \\mathbbm 1}_{n'_2=n_2 } }    \\bigr ]   \\nonumber\\\\ & \\qquad    + 2 k \\bigl [    { { \\mathbbm 1}_{n'_2=n'_1 } } +    { { \\mathbbm 1}_{n'_2=n_1 } }     + { { \\mathbbm 1}_{n_2=n'_1 } } +    { { \\mathbbm 1}_{n_2=n_1 } }   \\bigr ] \\ , , \\end{aligned}\\ ] ] and @xmath251   \\nonumber\\\\ & \\quad   = k^2 \\bigl [ d^2 { { \\mathbbm 1}_{n_1=n_2,n'_1=n'_2 } } + d { { \\mathbbm 1}_{n_1=n_2 } } + d { { \\mathbbm 1}_{n'_1=n'_2 } } + 1    + d^2 { { \\mathbbm 1}_{n_1=n'_2,n'_1=n_2 } } + d { { \\mathbbm 1}_{n_1=n'_2 } } + d { { \\mathbbm 1}_{n'_1=n_2 } } + 1\\bigr ]   \\nonumber\\\\ & \\qquad    + k    \\bigl[3 d { { \\mathbbm 1}_{n'_1=n'_2 } } + 3 d { { \\mathbbm 1}_{n'_1=n_2 } } + 3 d { { \\mathbbm 1}_{n_1=n'_2 } } +   3 d { { \\mathbbm 1}_{n_1=n_2}}+   \\nonumber\\\\ & \\qquad    \\quad   + 2 +   d^2 { { \\mathbbm 1}_{n_1=n'_2,n'_1=n_2 } } +    d^2 { { \\mathbbm 1}_{n'_1=n'_2,n_1=n_2}}\\bigr ] \\ , .\\end{aligned}\\ ] ]    this yields @xmath252   \\nonumber\\\\ & \\quad = d^2 k \\left\\{[(d+2)(k+1)+4 ] |u|^2 |v|^2 + ( u\\cdot v)^2 [ ( d+2)(k+1)+4 + 2(k+1 ) ] \\right\\ }   \\nonumber\\\\ & \\qquad   + d^2 k ( k+1)(|u|^2 |v|^2 + ( u\\cdot v)^2 ) -2d^2 k ( k+1 ) ( |u|^2 |v|^2 + ( u\\cdot v)^2 )   \\nonumber\\\\ & \\quad = d^2 k [ ( dk + d+k+5)|u|^2 |v|^2 + ( dk+d+3k+7 ) ( u\\cdot v)^2 ] \\ , .\\end{aligned}\\ ] ] adding the term to ( [ eq : w2contrib2 ] ) , and multiplying the result by @xmath239 yields @xmath253)^2 ]   = \\frac{a^4}{d^4 } d^2 k \\bigl [   ( 2 k + d + 1)^2 |u|^2 |v|^2    + ( dk + d+k+5)|u|^2 |v|^2 \\nonumber\\\\ & \\qquad + [ ( 2 k + d + 1)^2 + 2 d + 2 + 4 ( 2k+d ) ] ( u\\cdot v)^2   + ( dk+d+3k+7 ) ( u\\cdot v)^2   \\bigr ]   \\nonumber\\\\ & \\quad   = \\frac{a^4}{d^2 } k \\bigl [   ( 4 k^2+d^2 + 5 dk+5k+3d+6 ) |u|^2 |v|^2    + ( 4 k^2+d^2 + 5 dk+15k+9d+10 ) ( u\\cdot v)^2   \\bigr ]   \\nonumber\\\\ & \\quad   \\le\\frac{a^4}{d^2 } 2 k   |u|^2 |v|^2 ( 4 k^2+d^2 + 5 dk+10 k+6 d+8 )   \\nonumber\\\\ & \\quad   = a^4 2 k \\sigma_u^2 \\sigma_v^2 ( 4 ( k+d)^2 - 3 d^2   -3 dk+10 k+6 d+8 ) \\ , .\\end{aligned}\\ ] ]    if @xmath179 , we have @xmath254 for all @xmath255 .",
    "hence , for @xmath179 , the above bound can be simplified to the form given in the theorem , namely then @xmath256 this concludes the proof of the theorem .",
    "[ th : constantdisc ] the exact bound in ( [ eq : varcresult ] ) can also be approximated in other ways . choosing the normalization as in the corollary , with @xmath257 , and assuming @xmath109 , we obtain an estimate @xmath258 with a reduction of the prefactor from @xmath107 to @xmath108 .",
    "the bound stated in the theorem becomes optimal in the opposite regime , when @xmath259 .",
    "[ th : nongauss ] the assumption about sufficiently fast decay of correlations , here taken to be i.i.d .",
    ", between the matrix elements is important for the above phenomena to occur .",
    "however , the precise statistics of the distribution of each matrix element plays much less a role .",
    "for instance , consider instead of gaussian @xmath35-distributed matrix elements taking them from some other distribution which has finite moments up to order four .",
    "for example , suppose that the distribution of each @xmath260 has mean zero , @xmath261=0 $ ] , a variance @xmath262 $ ] and a fourth cumulant @xmath263 $ ] .",
    "as shown below , the resulting changes to the first three items in the theorem are then an introduction of an overall scale @xmath264 and relatively weak dependence on @xmath265 , the excess kurtosis of the distribution of @xmath31 .    explicitly , instead of equation ( [ eq : fourbs ] )",
    "we then have @xmath266 \\nonumber\\\\ & \\quad = c_2 ^ 2\\bigl ( { { \\mathbbm 1}_{j'=j , m'=m}}{{\\mathbbm 1}_{j'=j , n'=n } } + { { \\mathbbm 1}_{j'=j , n'=m}}{{\\mathbbm 1}_{j'=j , n = m ' } } + b_4 { { \\mathbbm 1}_{j'=j , m'=m = n = n'}}\\bigr ) \\ , .\\end{aligned}\\ ] ] therefore , we obtain @xmath267 = c_2 a k   u_m\\ , , \\\\ & \\operatorname*{cov}((pu)_m,(pv)_{m ' } ) = c_2 ^ 2 a^2 k \\left({{\\mathbbm 1}_{m'=m } } u\\cdot v + u_{m ' } v_{m}+b_4 { { \\mathbbm 1}_{m'=m } } u_{m } v_{m } \\right)\\ , , \\end{aligned}\\ ] ] and , retracing the necessary steps of the above proof thus yields @xmath268 = a^2 k ( d+k ) c_{u , v } + a^2 k ( d-1 ) \\mu_u \\mu_v + a^2 k b_4   \\left(1-\\frac{1}{d}\\right ) ( c_{u , v } +   \\mu_u \\mu_v )   \\ , .\\end{aligned}\\ ] ]    therefore , the scaling preserving the mean covariance for centered time series with @xmath96 is then given by @xmath269}}\\ , .\\end{aligned}\\ ] ] thus the main effect of changing the distribution is a fairly obvious scaling which corresponds to normalization of the variance of @xmath31 to one .",
    "the effect of the fourth cumulant is insignificant , unless it is at least as large as @xmath37 and @xmath0 .",
    "the third cumulant plays no role in the above computation ; it will , however , affect the value of the variance of @xmath114",
    ". in fact , there are quite a few new terms introduced to the computation of @xmath270 .",
    "all of these , however , are still expected to be subdominant to the gaussian contribution , as long as the higher order cumulants are not comparable to @xmath37 and @xmath0 . as in the explicit example above , each higher order cumulant should merely introduce new restrictions reducing the combinatorial factors arising from the pairing partitions computed in the proof of the theorem ."
  ],
  "abstract_text": [
    "<S> factor models are commonly used in financial applications to analyze portfolio risk and to decompose it to loadings of risk factors . </S>",
    "<S> a linear factor model often depends on a small number of carefully - chosen factors and it has been assumed that an arbitrary selection of factors does not yield a feasible factor model . </S>",
    "<S> we develop a statistical factor model , the random factor model , in which factors are chosen at random based on the random projection method </S>",
    "<S> . random selection of factors has the important consequence that the factors are almost orthogonal with respect to each other . </S>",
    "<S> the developed random factor model is expected to preserve covariance between time - series . </S>",
    "<S> we derive probabilistic bounds for the accuracy of the random factor representation of time - series , their cross - correlations and covariances . as an application of the random factor model , </S>",
    "<S> we analyze reproduction of correlation coefficients in the well - diversified russell 3,000 equity index using the random factor model . </S>",
    "<S> comparison with the principal component analysis ( pca ) shows that the random factor model requires significantly fewer factors to provide an equally accurate reproduction of correlation coefficients . </S>",
    "<S> this occurs despite the finding that pca reproduces single equity return time - series more faithfully than the random factor model . </S>",
    "<S> accuracy of a random factor model is not very sensitive to which particular set of randomly - chosen factors is used . </S>",
    "<S> a more general kind of universality of random factor models is also present : it does not much matter which particular method is used to construct the random factor model , accuracy of the resulting factor model is almost identical .    </S>",
    "<S> factor modeling , equity markets , random variables + </S>"
  ]
}