{
  "article_text": [
    "there is a renewed interest in file - format choices for astronomy with discussions on the future of fits @xcite , projects adopting or considering hdf5 @xcite and investigations into general purpose image formats such as jpeg2000 @xcite .",
    "these discussions have provided an opportunity to consider existing astronomy file formats that are not as widely known in the community as fits . here",
    "we discuss the extensible _ n_-dimensional data format ( ndf ) developed by the starlink project @xcite in the late 1980s @xcite to bring order to the proliferation of data models that were being adopted by applications using the starlink hierarchical file format .    in sect .",
    "[ sec : hds ] we discuss the genesis and features of the starlink file format , with the ndf data model itself being discussed in sect .",
    "[ sec : ndf ] . in sect .",
    "[ sec : lessons ] we discuss the positive lessons learned from developing ndf ( expanding on some earlier work by @xcite ) and we follow that in sect .  [ sec : improve ] , by discussing the areas where ndf could be improved .",
    "following this , in sect .",
    "[ sec : social ] we address the social , political and economic considerations of data formats and models . in [ app : chaos ] we provide examples of data models devised in the mid-1980s , the development of which motivated the creation of ndf and also directly influenced the ndf design .",
    "a timeline describing the key developments relating to ndf is shown in table  [ tab : timeline ] .    in this paper",
    "the term `` data model '' refers to the organization , naming and semantics of components in a hierarchy .",
    "the term `` file format '' means how the bytes are arranged on disk and in this context refers to the use of the hierarchical data system ( hds ; see the next section ) .",
    "historically , ndf , as implied by the use of `` data format '' in the name itself , uses the term `` format '' to refer to the data model in the sense of how the hierarchical items are arranged or `` formatted '' ( cf .",
    "text formatting ) , and not specifically referring to the underlying file format . in this paper",
    "we use try to use consistent modern terminology although in some cases there can be ambiguity , and in sect .",
    "[ sec : social ] we use `` format '' to mean the all - encompassing concept of ndf as a whole .",
    "the starlink project , created in 1980 @xcite , was set up primarily to provide data reduction software and facilities to united kingdom astronomers .",
    "fits had recently been developed and was adopted as a tape interchange format but there was a requirement for a file format optimized for data reduction that all starlink applications could understand .",
    "files needed to be written to and read from efficiently with an emphasis on the ability to group and modify related information @xcite .",
    "this was many years before the ncsa developed the hierarchical data format @xcite and it was decided to develop a new file format .",
    "the resulting starlink data system was first proposed in 1981 with the first version being released in 1982 ( see e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) . in 1983",
    "the name was changed to the hierarchical data system ( hds ) to make the file format benefits more explicit .",
    "hds itself was in common usage within starlink by 1986 @xcite .",
    "it was originally written in the bliss programming language on a vax / vms system and later rewritten in c and ported to unix .",
    "it was , however , designed to be only callable from fortran at this point .",
    "some key features of the hds design are as follows .",
    "* it provides a hierarchical organization of arbitrary structures , including the ability to store arrays of structures . *",
    "the hierarchy is self - describing and can be queried . *",
    "it gives the data author the ability to associate structures with an arbitrary data type .",
    "* users can delete , copy or rename structures within a file .",
    "* it supports automatic byte swapping whilst using the native machine byte order for newly created output files . *",
    "vax and ieee floating - point formats are supported . *",
    "automatic type conversion allows a programmer to request that , say , a data array of 32-bit integers is accessed as 64-bit floating point numbers .",
    ".time line for developments relating to ndf .",
    "other developments in file format development are interspersed for reference .",
    "[ cols= \" < , < \" , ]",
    "the advantage of hds over flat file formats is that it allows many different kinds of data to be stored in a consistent and logical fashion .",
    "it is also very flexible , in that objects can be added or deleted whilst retaining the logical structure .",
    "hds also provides portability of data , so that the same data objects may be accessed from different types of computer despite platform - specific details of byte - order and floating - point format .",
    "hds files allowed people to arrange their data in the files however they pleased and placed no constraints on the organization of the structures or the semantics of the content .",
    "this resulted in serious interoperability issues when moving files between applications that nominally could read hds files . within the starlink ecosystem",
    "there were at least three prominent attempts at providing data models , and these are discussed in detail in [ app : chaos ] .",
    "the models were : wright - giddings _ image _ @xcite , figaro ( * ? ? ? * http://www.ascl.net/1203.013[ascl:1203.013 ] ) dst and asterix ( * ? ? ? * http://www.ascl.net/1403.023[ascl:1403.023 ] ) .",
    "the result was chaos .",
    "given the situation with competing schemes for using the hierarchy , it became clear that a unified data model was required .",
    "a working group was formed to analyze the competing models and come up with a standard ; this work was completed in the late 1980s @xcite .",
    "after much debate , it was decided to develop a data model that included the minimum structures to be useful for the general astronomer without attempting to be everything to everybody , but designing in an extension facility from the beginning .",
    "the resulting model was named the extensible _",
    "n_-dimensional data format ( ndf ) .",
    "ndf combined some features of the _ image _ scheme , such as the use of ` data_array ` , and some features adopted from asterix , from which the ` history ` structure was adopted complete . the decision to recognize ndf structures based solely on the presence of",
    "a ` data_array ` array was an important compromise as it allowed data using the _ image _ data model , which indirectly included data in the bulk data frame ( bdf ; * ? ? ? * ; * ? ? ? * ) format that had been converted previously , to be used immediately in applications that had been ported to use the ndf library .    in the following sections we describe the core components of the ndf data model to provide an overview of the ndf approach concerning what is covered by the model and what is deliberately left out of the model .",
    "an overview of the components of an ndf and how they relate to each other is shown in fig .  [",
    "fig : ndf - structure ] . for more details on ndf",
    "please see the detailed ndf design document ( sgp/38 ; * ? ? ?",
    "* ) and library documentation ( sun/33 ; * ? ? ?",
    "ndf supports the concept of a primary data array and an associated variance array and quality mask .",
    "all the hds numerical data types are available but there is also support for complex numbers for the data and variance components .",
    "variance was selected as the error component to reduce the computational overhead when propagating errors during processing .",
    "all three components share the same basic ` array ` structure which defines the dimensionality of the array and allows for the concept of a pixel origin .",
    "the pixel origin is used to specify where the ndf sits relative to a larger pixel coordinate system by specifying the coordinates of the bottom - left pixel .",
    "for example , when extracting a subset of data from a bigger image , the pixel origin will record where the subset came from . also , when images are registered for mosaicking they are resampled such that their origins share a common coordinate frame resulting in the final mosaic being a simple pixel - by - pixel combination .",
    "see sect .",
    "[ sec : wcs ] for a discussion on how the pixel origin relates to other coordinate systems . the ` bad_pixel ` flag is intended as a hint to application software to allow simpler and faster algorithms to be used if it is known that no bad values are present in the data array .",
    "additionally , the flag can be used to indicate that all values are to be used , thereby disabling bad value handling and allowing the full range of a data type .",
    "this was felt to be particularly important for the shorter integer data types .",
    "the quality mask uses the ` array ` type but includes an extra level in the structure to support a bit mask .",
    "the ` badbits ` mask can be used to enable or disable planes in the ` quality ` array .    for applications that do not wish to support explicit quality tracking",
    ", the ndf library supports automatic masking of the data and variance arrays ; this uses the quality mask in the input ndf , and sets the corresponding value in the variance array to a magic value when the data are mapped into memory .",
    "unlike the _ image _ scheme or fits , which allow the magic ( or blank ) value to be specified per data array , ndf specifies the magic value to be used for each data type covering both floating - point and integer representations , inheriting the definition from the underlying hds definition . indeed ,",
    "` nan ` ( defined by the ieee floating - point model @xcite ) is not explicitly part of the standard and it is usually best to convert ` nan ` elements to the corresponding floating - point magic value before further processing is applied .",
    "a single definition of the magic value for each data type simplifies applications programming and removes the need for the additional overhead of providing a value for every primitive data array .    `",
    "nan ` was excluded from the standard as it was not supported in vax floating point ( see e.g. * ? ? ?",
    "* ) and the starlink software was not ported to machines supporting ieee floating point until the 1990s ( e.g. , * ? ? ?",
    "unlike fits , which did not officially support floating point until 1990 @xcite when they were able to adopt ` nan ` as part of the standard , much software pre - existed in the starlink environment at this time and embodied direct tests for magic values in data .",
    "given the different semantics when comparing data values for equality with ` nan ` , it was decided to continue with the magic value concept rather than try to support two rather different representations of missing data simultaneously .",
    "the original ndf standard included the _ scaled _ data compression variant , which is commonly required when representing floating - point numbers in integer form ; this is equivalent to ` bscale ` and ` bzero ` in fits , although this variant was not implemented in the ndf library until 2006 @xcite . in 2010 a lossless delta compression system for integers was added to support raw scuba-2 data @xcite .",
    "this was a new implementation of the ` slim ` data compression algorithm developed for the atacama cosmology telescope ( * ? ? ?",
    "* http://www.ascl.net/1409.010[ascl:1409.010 ] ) .",
    "each ndf has three character attributes that can be specified : a title , a data label and a data unit .",
    "these values can be accessed through the library api without resorting to a fits - style extension .",
    "axis information was initially specified using an ` axis ` array structure with an element for each dimension of the primary data array .",
    "the axis information was specified in an ` array ` type structure as for data and variance , and allowed axis labels and units to be specified . for each axis , coordinates were specified for each pixel in the corresponding dimension of the data array .",
    "this allowed for non - linear axes to be specified and is similar to the ` -tab ` formalism later adopted in fits .",
    "the ` axis ` formalism worked well for spectral coordinates but it was not suitable for cases where the world coordinate axes are not parallel to the pixel axes , such as is often the case with right ascension and declination . nor did it provide the meta - data needed to allow the axis values to be transformed into other coordinate systems , for example when changing a spectral axis from frequency to velocity or sky axes from icrs to galactic .",
    "a more general solution was required and this prompted the development of the ast library ( * ? ? ?",
    "* http://www.ascl.net/1404.016[ascl:1404.016 ] ) with an object - oriented approach to generalized coordinate frames .",
    "a full description of the data model used by ast is outside the scope of this paper , but in outline ast uses three basic classes - `` frame '' , `` mapping '' and `` frameset '' :    ` frame ` : :    - describes a set of related axes that are used to specify positions    within some physical or notional domain , such as `` the sky '' , `` the    electro - magnetic spectrum '' , `` time '' , a `` pixel array '' , a `` focal    plane '' . in general , each such domain can be described using several    different coordinate systems .",
    "for instance , positions in the    electro - magnetic spectrum can be described using wavelength , frequency    or various types of velocity ; position on the sky can be described    using various types of equatorial , ecliptic or galactic coordinates .",
    "a    frame describes positions within its domain using a specified    coordinate system , but also encapsulates the information needed to    determine the transformation from that coordinate system to any of the    other coordinate systems supported by the domain .",
    "it should be noted    that a frame does not include any information that relates to a    different domain .",
    "for instance , a skyframe has no concept of `` pixel    size '' .",
    "+    several frames can be joined together to form a compound frame    describing a coordinate system of higher dimensionality . `",
    "mapping ` : :    - describes a numerical recipe for transforming an input vector into    an output vector .",
    "most importantly , a mapping makes no assumptions    about the coordinate system to which the input or output vector    relates .",
    "ast contains many different sub - classes of mapping that    implement different types of mathematical transformations , including    all the spherical projections included in the fits wcs standard .",
    "several mappings may be joined together , either in series or in    parallel , to form a more complex compound mapping . `",
    "frameset ` : :    - encapsulates a collection of frames , together with mappings that    transform positions from one frame to another .",
    "these are stored in the    form of a tree structure in which each node is a frame and each link    is a mapping . within the context of an ndf , the root node always    describes pixel positions in the form of ` grid ` coordinates ( see    below ) .",
    "each other node represents a coordinate system into which    pixels positions may be transformed , and will often include celestial    or spectral coordinate systems .",
    "facilities of the frameset class    include the ability to transform given positions between any two    nominated frames , and to adjust the mapping between two frames    automatically if either of the frames are changed to represent a    different coordinate system .",
    "ast support was added to the ndf standard in spring 1998 @xcite by adding a new top - level ` wcs ` structure to ndf .",
    "this holds a frameset that describes an arbitrary set of coordinate systems , together with the mappings that relate them to pixel coordinates .",
    "ast objects were stored simply as an array of strings using their native ascii representation , which is more general and flexible than adopting a fits - wcs serialization .",
    "the ndf library api was modified to support routines for reading and writing the wcs frameset without the library user knowing how the frameset is represented in the hierarchical model .",
    "the ndf library manages a number of wcs frames specifically intended to describe coordinate systems defined by the ndf library itself :    ` grid ` : :    - a coordinate system in which the first pixel in the ndf is centered    at coordinate ( 1,1 ) .",
    "this is the same as the `` pixel coordinate    system '' in fits . `",
    "pixel ` : :    - a coordinate system in which ( 0,0 ) corresponds to the pixel origin    of the ndf .",
    "the transformation between ` pixel ` and ` grid ` is a simple    shift of origin .",
    "` axis ` : :    - a coordinate system that corresponds to the axis structures ( if any )    stored in the ndf .",
    "` fraction ` : :    - a coordinate system in which the ndf spans a unit box on all pixel    axes .",
    "the ndf library always removes the above frames when storing the wcs frameset within an ndf , and re - creates them using the current state of the ndf when returning the wcs frameset for an ndf .",
    "this allows the user to modify the ` axis ` and pixel origin information in the ndf without having also to remember to update the wcs information .      the ` history ` structure is used to track processing history and includes the date , application name , arguments and a narrative description .",
    "the structure was first developed for the asterix  package and adopted directly into the ndf data model .",
    "the history was , by design , not expected to be parseable by application software ; applications such as surf ( * ? ?",
    "* http://www.ascl.net/1403.008[ascl:1403.008 ] ) did nonetheless use the history to determine whether a particular application had been run on the data , so that the user could be informed if a mandatory step in the processing had been missed .    comparing the history structure of fig .",
    "[ fig : ndf - structure ] with that shown in fig .",
    "[ fig : asterix ] shows that the components are identical apart from the addition of some additional fields to the ndf form ( user , host and dataset ) . the only other change involved the relatively recent addition to increase in resolution the time stamp to support milliseconds .",
    "this was required as computers have become faster over the years and many processing steps can occur within a single second .",
    "provenance handling , sect .",
    "[ sec : provenance ] , uses the history block to disambiguate provenance entries and relies on the timestamp field .",
    "the ndf standard included a special place , named ` more ` , for local extensions to the model .",
    "this allowed instruments and applications to track additional information without requiring standardization .",
    "the only rule was that each extension should be given a reserved name ( registered informally within the small contemporary community ) and that the data type would define the specific data model of an extension .",
    "some applications , for example , went so far as to include covariance information in extensions to overcome limitations in the default error propagation model for ndf ( for example specdre ; * ? ? ?",
    "* http://www.ascl.net/1407.003[ascl:1407.003 ] ) .",
    "three extensions proved so popular that they are now effectively part of the ndf standard , but for backwards compatibility with existing usage they can not be moved out of the extension component .",
    "these extensions covered fits headers , provenance tracking and data labels and are described in the following sections .",
    "fits headers , consisting of 80-character header cards , are extremely common . to simplify interoperability with fits files and to minimize structure overhead  as was found from experience with dst  it became commonplace to store the header _ as is _ as an array of 80-character strings matching the fits convention , rather than attempting to parse the contents and expand into structures .          in the late 1980s disk space and processing power were more highly constrained than they are now . at the time , therefore , it seemed prudent to limit history propagation to a single `` primary '' parent . as a consequence the ndf library ensures that each application copies history information from a single primary parent ndf to each output ndf , appending a new history record to describe the new application .",
    "this means that if many ndfs are combined together by a network of applications , then each resulting output ndf will , in general , contain only a subset of the history needed to determine all the ndfs and applications that were used to create the ndf .",
    "it would of course be possible to gather this information by back - tracking through all the intermediate ndfs , analyzing the history component of each one , but this depends on the intermediate ndfs still being available , which is often not the case .    in 2009",
    ", it was decided that the inconvenience of this `` single line of descent '' approach to history was no longer justified by the savings in disk space and processing time , and so an alternative system was provided that enables each ndf to retain full information about all ancestor ndfs , and the processing that was used to create them @xcite .",
    "thus each ndf may now contain a full `` family tree '' that goes back as far as any ndfs that have no recorded parents , or which have been marked explicitly as `` root '' ndfs .",
    "each node in the tree records the name of the ancestor ndf , the command that was used to create it , the date and time at which it was created , and its immediate parent nodes .",
    "each node also allows arbitrary extra information to be associated with the ancestor ndf .",
    "care is taken to merge nodes  possibly inherited from different input ndfs  that refer to the same ancestor .",
    "a simple provenance tree is shown in fig .",
    "[ fig : prov ] .    for reasons of backward compatibility",
    ", it was decided to retain the original ndf history mechanism , and add this new `` family tree '' feature as an extra facility named `` provenance '' .",
    "originally the family tree was stored as a fully hierarchical structure within an ndf extension , using raw hds .",
    "however , given the possibility for exponential growth in the number of ancestors , the cost of navigating such a complex structure quickly became prohibitive .",
    "therefore , storage as raw hds was replaced by an optimized bespoke binary format packed into an array of integers ( the provenance model is outlined in [ app : prov ] ) .",
    "individual bits in the quality mask can be addressed using the ` badbits ` attribute , but the ndf standard did not allow for these bits to be labeled .",
    "this confusion was solved first in the iras90  package ( * ? ? ?",
    "* http://www.ascl.net/1406.014[ascl:1406.014 ] ) which added a `",
    "quality_names ` extension associating names with bits .",
    "kappa  tasks ( * ? ? ?",
    "* http://www.ascl.net/1403.022[ascl:1403.022 ] )",
    "were then modified to understand this convention and allow users to enable and disable masks by name .      above and beyond the core data model",
    ", the ndf library provides some additional features that can simplify application development .",
    "the library uses an object - oriented interface , despite being developed in fortran , and the python interface , ` pyndf ` , provides a full object interface whereby an ndf object can have methods invoked upon it and provides direct access to object attributes such as dimensionality and data label .",
    "the ndf library allows output ndfs to be constructed from scratch , but it also recognizes that many applications build their output dataset(s ) using one of the input datasets , termed the `` primary '' input , as a template . in this case ,",
    "much of the primary input data may need to be propagated to the output unchanged , with the application attending only to those components affected by the processing it performs .",
    "for example , scaling an image would not affect coordinate data , nor many other aspects of an ndf .",
    "alternatively , an application might not support processing of certain ndf components and would then need to suppress their propagation to avoid creating invalid or inconsistent output .",
    "these requirements are accommodated through use of a component propagation list that specifies which of the input ndf components should be propagated directly to the output .",
    "a default propagation list is provided which applications can easily tailor to their specific needs .",
    "this helps to ensure uniformity in application behavior .",
    "the default behavior of the propagation list also encourages the convention that applications should copy any unrecognized extensions unchanged from primary input to output .",
    "applications are again free to modify this behavior , but in general do not do so .",
    "the ndf library allows a subset of the array to be selected using a slicing syntax that can work with a variety of coordinate systems .",
    "the user can specify the section in pixel coordinates ( where the pixel origin is taken into account ) or in world coordinates , in which case the supplied wcs values are transformed into pixel coordinates using the transformations defined either by the legacy ` axis ` scheme or the full ast - based wcs scheme , as described in sect .",
    "[ sec : wcs ] .",
    "the section can either be specified using bounds or a center coordinate and extent . if the section is specified using wcs values , the section of the pixel array actually used is the smallest box that encloses the specified wcs limits .",
    "example sections are    .... myimage(12h59m49s~4.5m,27.5d:28d29.8 m ) ....    to extract an area from an image where the limits along right ascension are specified by a center and a half width , and the extent along the declination axis is defined by bounds ;    .... myspectrum(400.:600 . ) ....    where the spectrum is truncated to a range of , say , 400 to 600km / s ;    .... mycube(12h34m56.7s,-41d52m09s,-100.0:250.0 ) ....    where a spectrum is extracted from a cube at a particular location and also truncated ; and    .... mycube(55:63,75~11,-100.0:250.0 ) ....    where a subset of a cube is extracted where the first two coordinates specify pixel bounds and the third coordinate is a world coordinate range . in all these examples",
    "a colon indicates a range , and a tilde indicates an extent , so in the final example ` 55:63 ` means pixels 55 to 63 inclusive , and ` 75~11 ` means 11 pixels centered on pixel 75 .      mapping large data arrays into memory can use considerable resources and astronomical data always seem to be growing at a rate slightly exceeding the capacity of computers available to the average astronomer .",
    "the ndf library provides an api to allow subsets of a data array to be mapped in chunks to allow data files to be processed in smaller pieces .",
    "the concept of blocking is also used to allow a subset of the image to be mapped in chunks of a specified dimension .",
    "the distinction between chunking and blocking is significant in that chunking returns the data in sections that are contiguous in memory for maximum efficiency whereas blocking allows spatially related pixel data to be returned , at the expense of a performance loss in reordering the data array .      in many cases an astronomer would like to use a particular ndf application on a fits",
    "file without realizing that the application does not natively support fits .",
    "if foreign - format conversion is enabled , the ndf library will run the appropriate conversion utility based on the file suffix and present the temporary , ndf , file to the application .",
    "if the user specifies an output file with a particular foreign - format suffix the ndf library will then create a temporary ndf for the output and convert it when the file is closed .",
    "the library api was designed to make history updating as easy as possible : by default , if the history structure exists the ndf library automatically adds a new entry when a file is written to , and if the history structure is missing no entry is recorded .",
    "callback routines can be registered to be called in response to files being read from , written to , opened or closed .",
    "this facility is used by the ndg library @xcite to enable provenance handling as a plugin without requiring direct changes to the ndf library itself .",
    "for a variety of reasons , the starlink software , and hence ndf , did not achieve much traction outside the uk and uk - affiliated observatories .",
    "in particular the us astronomical community that closely adhered to fits until the advent of hdf . this is regrettable as ndf proved to be a rich and flexible data model that has aged well against mounting requirements from data processing environments .",
    "perhaps the ultimate lesson learnt is that data formats and models are adopted and retained for sociological reasons as much as for technical reasons .",
    "ndf is a successful data model : it achieves its goal of supporting broad interoperability of astronomical data between applications in a pipeline and between pipelines , across multiple wavelengths , and across time .",
    "it did this without inhibiting applications from including any and all of the application- or instrument - specific features they needed to preserve .",
    "below , we try to tease apart some separately important strands of this design , but summarize the key points here .",
    "there are two incompatible approaches to designing a data model .",
    "one extreme aspires to think of everything that is needed for all of astronomy , and to design a model where all metadata are described and everything has its place .",
    "this is the approach taken by the ivoa ( see e.g. * ? ? ?",
    "this is an intelligible and worthy goal and it is clear that much can be gained if such data models can be utilized , especially if related models ( spectrum and image when combined into a data cube ) share common ground . in our experience , however , this approach leads to long and heated discussions that generate data models that are never quite perfect and continually need to be tweaked as new instrumentation and metadata are developed .    despite much detailed discussion and inventiveness at the time the model was devised",
    ", the ndf model remains modest : _ ndf does not try to do everything _ , but instead adds a bare minimum of useful structure .",
    "astronomy data are fundamentally rather simple , and the elements in the ndf data model represent both a large fraction of the information that is necessary for high - level understanding of a dataset , and a large fraction of what is readily interoperable between applications without relying on detailed documentation .    for ndf there was a need to generate a usable model quickly that took concepts that were generically useful , leaving instrumental details to extensions .",
    "these extensions were ignored by generic software packages although clear rules were made regarding how extensions should be propagated .",
    "this approach allowed for ndf to grow without requiring the model or the ndf library to understand the contents of extensions .",
    "this approach of being flexible and not attempting to solve everything at the first attempt has been very successful , not only in enabling new features to be added to ndf once the need became obvious , but also in so far that wavelength regimes not initially involved in the discussion can make use of ndf without requiring that the core data model be changed .",
    "initially it was very hard to convince people that hierarchical data structures were at all useful .",
    "this can be seen in the wright - giddings layout of a simple astronomical image file using hds . over the years",
    "the adoption of hierarchy has been an important part of the ndf experience , and application programmers quickly began to realize the power of being able to relocate or modify parts of a data file , to change the organization , or to copy related sections to a new file .",
    "ndf extensions may contain other ndf structures , allowing software packages to extract those ndfs , or simply focus on them , without regard to the enclosing data module ; this becomes very intuitive and obvious once it is learned .",
    "a flat layout would require additional grouping metadata to understand which components were related , but this is neatly handled by a simple hierarchy .",
    "the ndf model is strongly _",
    "both makes it easy to find and manipulate data elements , and makes it easy for applications to _ extend the model _ , by providing a space for private data , which does not interfere with the shareability or intelligibility of the dataset as a whole .",
    "the ndf model , by design , supports only a subset of the information associated with a realistic dataset ; it trivially follows that most data providers will want to include data that are not described within the ndf model , and if the model could not accommodate these extra data , it would be doomed to be no more than a ` quick - look ' format .    as described above ( sect .",
    "[ sec : more ] ) , the ndf model includes a ` more ` tree , where particular applications can store application - specific data using any of the available hds structures ( sect .  [",
    "sec : hds ] ) .",
    "some applications treated this as a black box ; others documented what they did here , making it an extended part of the application interface , so that their ` private ' data were effectively public , and were largely standardized in practice without the long drawn - out involvement of a formal standards process .",
    "thus , different elements of the data within an ndf file were painlessly standardized by different entities , on different timescales and in response to direct community demands .    when applications wished to take advantage of the possibilities here , they registered a label within the ` more ` tree , and were deemed to ` own ' everything below that part of the tree .",
    "the community of ndf application authors was small enough that this could be done informally , but if this were being designed now , then a simple namespacing mechanism ( using perhaps a reversed - domain - name system ) would be obvious good practice .",
    "once application writers understand that there is a standard place for error information , quality masks and other features of the standard data model , they begin to write application software that can use these features .",
    "not having to use a heuristic to determine whether a particular data array represents an image or an error can allow the application writer to focus more on the algorithms that matter .",
    "once users of the software understand that errors and masks are an option they begin to have an expectation that all the software packages will handle them .",
    "this then motivates the developer to support these features , creating a virtuous circle which tended to improve all of the software in the ndf community .",
    "this is especially true if the core concepts of the data model are simple enough that the learning curve is small .",
    "in addition to the key successes there were a number of other lessons learned while using the data model over the years .      for a new format to be used it must be possible to convert to and from existing formats in a safe and reliable manner without losing information ( see for example ,",
    "* for a discussion of this problem with fits and hdf files ) .",
    "the ndf model on hds format files has always been one format amongst many formats in astronomy and much work has been expended on providing facilities to convert ndf files into fits and iraf format , and _ vice versa _ @xcite . in particular , special code was used to recognize specific data models present in fits files to enable a more accurate conversion of scientific information into the ndf form .",
    "support was also added for the _ multispec _ data model in iraf @xcite to ensure that wavelength scales were not lost . a description of how ndf maps to fits is given in [ app : ndf2fits ] .",
    "given hierarchical structures the default assumption might be to use arrays of keyword structures where each structure would contain the value , the comment and the unit .",
    "alternatively , one might simply drop the unit and comment and just use keyword / value pairs .",
    "these approaches turned out to be extremely slow and space inefficient so the project took the pragmatic approach of standardizing on an array of characters formatted identically to a fits header .",
    "this allowed the fits to ndf conversion software to ignore new fits header conventions as they were developed , deferring this to the application writer who can decide whether a particular convention is important . without this pragmatic approach",
    "the user would have to re - convert the files whenever a new convention is supported and the ndf data model would have to be extended to support such features as hierarchical keywords and header card units . in practice the ast library @xcite is often used for processing the fits header from an ndf so this causes few problems and simplifies the ndf organization , allowing an application to understand new fits header conventions simply by upgrading the ast library .          in the early years of ndf",
    "there were many files in existence that did not quite conform to the standard and new files were being created by existing applications that did not use the main ndf library but nonetheless modified their structures to emulate an ndf ( see for example fig .",
    "[ fig : duck ] ) .",
    "the core ndf library thereby took an inclusive approach to finding ndf structures , with the only requirement being the existence of a ` data_array ` item .",
    "if other compliant structures were found , the ndf library would use them and ignore items that it did nt understand .",
    "this `` duck typing '' approach proved to be very useful and eased adoption of ndf .",
    "the ndf library uses this feature to scan for ndf structures within an ndf or an hds container .",
    "for example , the gaia  visualization tool ( * ? ?",
    "* http://www.ascl.net/1403.024[ascl:1403.024 ] ) will scan an hds file for all such ndf structures and make them available for display .",
    "the data type of the structure  which may be thought of as the class name  is not used by the library when determining whether a structure is an ndf .",
    "the initial ndf design document did not profess to know the future and was deliberately designed to allow new features to be added as they became necessary .",
    "the adoption of a character array matching an 80-character fits header was an early change but there have also been changes to support world coordinate objects @xcite , data compression algorithms @xcite and provenance tracking @xcite .",
    "the ndf standard therefore has a proven ability to continue to evolve to meet the needs of modern astronomy data processing .",
    "ndf combined with hds was a conscious decision to not develop an interchange or archive format .",
    "hds was designed solely to be accessed through a reference implementation library and api and many of the ndf features were directly integrated into the reference ndf library without being specified in the core data model .",
    "this yielded great flexibility regarding exactly how the data are stored and greatly simplified future enhancements ( and general future - proofing ) because changes to the data format and how data are accessed could all be hidden from applications by changes made in one place .",
    "however , care needs to be taken when rolling out updates : software linked with old library versions may not be able to read newer data .",
    "indeed , hds includes an internal version number and can read earlier versions of the data format , but if a newer version is encountered than supported by the library hds will know about it and report the problem .      some aspects involving the use of ndf caused confusion amongst users , particularly when extensions were involved .",
    "@xcite provided an algorithm for designing simple elemental building blocks that could be reused .",
    "however , many extensions created in the early years of ndf became more complex than was strictly necessary by ignoring the design decisions of ndf and using highly complex hds structures .",
    "the lack of a standard table structure was only partly to blame .",
    "in many cases it would have been beneficial to use ndf structures in the extensions , which would have allowed the ndf library to read the data ( by giving the full path to the structure ) without resorting to low - level hds api calls .",
    "the ndf structures would also have been visible to general - purpose tools for visualization .",
    "future designers should not expect their respective community to design any elemental structures or the api to support them .",
    "extension implementors often ignored or misunderstood another fundamental precept of standard structures , the difference between the name and type of a structure .",
    "they relied on the structure name , often omitting to define a type .",
    "the data type defines the content , semantics , and processing rules .",
    "the name is just an instance of the particular structure .",
    "this approach could lead to name clashes , or restrict to only one instance within a given structure .",
    "software other than the originator s encountering the structure would not know the meanings and how to handle the structure s contents .",
    "note that this happened at a time when object - oriented concepts were not widely known",
    ". again more oversight would have helped .",
    "the same confusion is evident in fits extensions where there is no standard header to record the extension s type , perhaps borne of fits s lack of semantics , although wells had added the extension keywords partly with starlink hierarchical data in mind .",
    "keyword ` extname ` is often used to attribute meaning . reserving ` extname ` for the ndf component path , and adding an ` exttype ` header permitted a round trip between ndf and",
    "fits ( see sect .  [",
    "sec : round_trip ] ) . `",
    "exttype ` , proposed by @xcite , was never adopted into the fits standard @xcite .",
    "as the ndf data model has been used we have come to realize that some parts of the standard could do with adjustment and other enhancements should be made .",
    "the following items could be implemented and the current standard does not preclude them .",
    "one caveat is that there are no longer any full - time developers tasked with improving ndf and any improvements would be driven by external priorities of the key stakeholders ( as was the case with the recent addition of provenance and data compression ) .",
    "the initial design for the quality mask used a single unsigned byte allowing eight different quality assignments in a single data file .",
    "the design did not allow for the possibility of supporting larger unsigned integer data types .",
    "this restriction should be raised to allow more assignments .",
    "the smurf map - maker ( * ? ? ? * http://www.ascl.net/1310.007[ascl:1310.007 ] ) already makes use of more than eight internally and uses an unsigned short .",
    "when the results are written out mask types have to be combined if more than eight were present in the output data file .",
    "tables were added to fits during the initial development of ndf and the need for tables was considered a lower priority in the drive for a standardized image model and table support was never added to ndf .",
    "the omission of a table model was belatedly addressed in the early 1990s from outside starlink .",
    "giaretta proposed and demonstrated that hds could be used to store tables , being especially suited to column - oriented operations .",
    "such a ` table ` data structure was later interfaced through the chi subroutine library @xcite for the catpac  package @xcite .",
    "this initiative failed , however , due to a lack of documentation and promotion .",
    "the effort was not completely wasted .",
    "the fitsndf  conversion tool @xcite creates an extended form of the ` table ` model ( see [ app : hdstable ] ) to encapsulate within an ndf the ancillary fits tables associated with image data .",
    "starlink software eventually took the pragmatic solution of using fits binary tables for output tables .",
    "this can not solve the problem of integrating data tables into image data files and the format would benefit from a native data type .",
    "the jcmt raw data format instead uses individual 1-dimensional data arrays to store time - series data but this is inefficient and adds programming overhead .      the adoption of variance as a standard part of ndf was an important motivator for application writers to add support for error propagation and almost all the starlink applications now support variance .",
    "the next step is to support different types of errors , including covariance ( see e.g. * ? ? ?",
    "this has been proposed many times ( see e.g. * ? ? ?",
    "* ) but is a very difficult problem to solve in the general case and may involve having to support pluggable algorithms for handling special types of error propagation .",
    "the fits ` datasum ` facility @xcite is very useful and ndf should support it .",
    "ideally it should be possible to generate a reference checksum for a structure .",
    "it may be that this has to be done in conjunction with hds .",
    "the ` _ char ` data type in hds uses an 8-bit character type , assumed to contain only ascii .",
    "it was designed long before unicode came to exist and has no support for accented characters or non - ascii character sets .",
    "multi - byte unicode should be supported in any modern format to allow metadata to be represented properly .",
    "hds can not support the storage of common astronomical unit symbols such as @xmath0 m or  .",
    "it may be possible to use the automatic type conversion concept already in use for numeric data types to allow unicode to be added to ndf without forcing every application to be rewritten .",
    "if an application uses the standard api for reading character components they will get ascii even if that involves replacing unicode characters with either normalized versions of characters outside the ascii character set ( for example , dropping accents ) or whitespace .",
    "a new api would be provided for reading and writing unicode strings .",
    "whilst the provenance handling works extremely well and is very useful for tracking what processing has gone into making a data product , the provenance information can become extremely large during long and complicated pipeline processing such as those found in products from orac - dr ( * ? ? ?",
    "* http://www.ascl.net/1310.001[ascl:1310.001 ] ) .",
    "there can be many thousands of provenance entries including some loops where products are fed back into earlier stages because of iterations .",
    "the provenance tracking eventually begins to take up a non - trivial amount of time to collate and copy from input files to output files .",
    "one solution to this may be to offload the provenance handling to a database during processing , only writing the information to the file when processing is complete and the file is to be exported .",
    "it would be fairly straightforward to modify the ndg library to use the core provenance library @xcite .",
    "whilst there are many advantages to having a single library providing the access layer to an ndf , there is also a related problem of limitations in this one library causing limitations to all users .",
    "in particular the current ndf library is single - threaded due to use of a single block of memory tracking access status .",
    "furthermore the hds library itself is also single - threaded with its own internal state .",
    "as more and more programs become multi - threaded to make use of increased numbers of cores in modern cpus this limitation becomes more and more frustrating .",
    "another issue associated with the library is the use of 32-bit integers as counters in data arrays .",
    "it is now easy to imagine data cubes that exceed this many pixels and a new api may be needed to ease the transition to 64-bit counters .    whilst the hds library is written entirely in ansi c ,",
    "the ndf and related ary @xcite and ndg @xcite libraries are written mainly in fortran .",
    "this puts off people outside the starlink community and adds complications when providing interfaces to higher - level languages such as python , perl and java . indeed a subset of the ndf model was written as a java layer on top of the c hds library , precisely to avoid the added dependency to the fortran runtime library .",
    "ideally ndf would be rewritten in c and be made thread - safe .",
    "we draw attention in this paper to many technical aspects of the ndf design that remain relevant today . however , the data format and associated model has its roots in the social , political , economic and , indeed , technological circumstances of the late 1980s .",
    "so it is relevant to examine what effect these had , how they have changed , and how they affect data format choices being made today .",
    "we also discuss briefly some of the political and economic issues that data format developers need to address when promoting their products .      in the period when the ndf was being designed ,",
    "the starlink project was funded to support the computing needs of astronomical research in the uk , across all wavebands . on the software side ,",
    "its remit was to centrally provide , curate and distribute software , to foster collaboration between uk software development projects , and to develop standards for their use .",
    "together with central purchasing of computer hardware , good networking and a centrally managed team of system administrators located around the uk , the project was an innovation that attracted much interest .",
    "some , however , saw an element of socialism in this arrangement and argued that , in a more free - market approach , individual research groups should develop software ( and by implication data formats and models ) independently and that the fittest should survive and be used by other groups .",
    "that , indeed , was the default situation in most countries . in the uk , however , two key objections trumped this argument .",
    "one was that the quality , reliability , maintainability and suitability for long - term re - use of software and standards produced in this way was inadequate , essentially because those embarking on such projects , while being talented scientists , at that time typically lacked software engineering skills .",
    "the second was that resources for astronomy are limited and that funding multiple similar projects in order to later discard most of the work in favor of just one is wasteful .",
    "consequently , uk - wide collaborative development of software and standards , including data formats , was the option that received funding .",
    "the broad range of astronomy to be supported by starlink and the consequent diverse data requirements was a severe challenge and exposed the project to many issues that other groups had to face only later on .",
    "as we have described , the problem was made more tractable by using a single very flexible and extensible data format , rather than a set of individual ad - hoc solutions for each branch of astronomy .",
    "so the motivation for developing ndf was , in essence , economic .",
    "it was cheaper to take that route .",
    "starlink s success was sometimes judged by international take - up of its software and standards , but its funding was primarily in support of uk astronomy , so software promotion and support activities outside the uk only took place on a best - efforts basis .",
    "while widespread adoption of ndf outside of uk projects would have been a bonus , it was not a funded goal and , indeed , the project took few steps even to monitor it .",
    "a key consequence of this `` funding boundary '' was that starlink staff were not involved in discussions about data handling for new projects unless there was direct uk involvement .",
    "such early discussions are typically crucial in securing `` buy - in '' from potential new users of software .",
    "they allow the software s capabilities and the costs and benefits of using it to be explored , and provide a timely opportunity to add features that may be of special interest to that project . by restricting this activity to uk projects , usage of starlink s",
    "ndf inevitably became much more widespread within the uk and its overseas observatories than elsewhere .",
    "nevertheless , widespread international adoption will be uppermost in the mind of anyone contemplating a new data format today .",
    "this paper s authors have been reflecting on the relevant issues over many years , but unfortunately we can not offer a simple winning formula .",
    "even the most seasoned of standards organisations will struggle to reconcile the vested interests of particular groups with the wider interests of their community and to inspire the confidence needed for widespread adoption of a standard .",
    "perhaps the best that can be said is that designing a standard requires a great deal of discussion and consultation .",
    "this can be lengthy and needs robust processes and good leadership , but it is something that has to be done and should not be rushed .",
    "time spent getting it right is repaid later by extending the lifetime and uptake of the standard when in service .    while we have no silver bullet , it is perhaps relevant to ask how we might start out again now , if embarking on a new data format project with widespread international adoption in today s environment as a goal .",
    "it turns out that many of the problems we faced in the 1980s have now been addressed by developments in other areas and that if we seek out current best practice in software development the number of difficult choices is fairly small .    despite its successes ,",
    "it is unlikely that we would take the fits route and define a file format without also implementing accompanying software .",
    "this is because it requires the software to be written independently , which means ( inevitably ) by different groups - and it all needs to inter - operate . specifying not just the bit - patterns but also the semantic interpretation of data in sufficient detail for widespread inter - operability is extremely difficult .",
    "if one examines , say , the html standards @xcite , they are very lengthy and require a huge effort to produce , yet still inter - operability often fails in practice .",
    "astronomical data formats have the potential to become much more complex than this .",
    "one must also avoid designing a data format that can not easily be implemented in software .",
    "this may be because the algorithmic details are not apparent but turn out to be complex or ambiguous , or because of the ubiquitous tendency to over - design and to be wildly over - optimistic about the resources needed for implementation .",
    "for these reasons we would probably consider developing any new data formats alongside a reference software implementation .",
    "this is a procedure used by most participants in international software standards discussions in order to guide and inform their negotiations and to ensure that they have a working product as soon as the standard is complete .",
    "it ensures that the standard and the implementation never diverge too far and that the data semantics and their implications are fully understood ( these details being difficult to capture other than in software ) . in an astronomy context",
    "it should be unnecessary for each participant to develop their own separate software .",
    "rather , a single collaboratively developed software project might provide a suitable nucleus for the whole enterprise .    when developing ndf , the discussion surrounding its design was open , inclusive and collaborative , but the development of software to implement it occurred later and ( despite eventually being released under an open - source licence ) followed what would now be recognized as `` closed source '' development practices .",
    "we did not then have the benefit of modern collaborative software development tools , but the success of the free open - source software ( foss ) movement has shown the power of the open - source development model and some of its features would certainly have been very attractive .",
    "there are , of course , many open - source software tools , environments and development processes in use in different foss projects . however",
    ", the relevant features found in most cases are : i ) frequent builds , so that a functioning development version of the software is always available , ii ) anyone can participate in discussions and iii ) anyone can contribute code ( possibly subject to review by others ) . such a system would seem to meet many of the requirements for developing a data format specification alongside a reference software implementation while simultaneously stimulating informed discussion , feedback and , of course , attracting coding contributions which are always the scarcest commodity in such an undertaking .",
    "one should perhaps add that good management would also be essential and that fundamental choices such as implementation language and software dependencies can be critically important .      a crucial issue for a project developing or maintaining an astronomical data format is to ensure that it can continually gain new users , something that tends to become easier as the project grows and becomes more capable and better - known .",
    "this principally means that it must work to be adopted by new instrumentation groups that will be producing important astronomical data in future .",
    "this , in turn , means winning the arguments for and against adopting a particular data format within these groups .",
    "although general - purpose application software suites are also big players in the data format game , new ones start up relatively infrequently and may take decades to mature .",
    "their capabilities also tend to lag behind the demands of new instrumentation .",
    "they therefore present less of an opportunity for promoting a new data format . having",
    "a major software suite on board is definitely worthwhile , but an existing well - entrenched data format in a major application suite presents a huge barrier to entry that can only really be addressed by implementing a transparent compatibility layer ( and/or automated conversion ) so that changes to existing applications are unnecessary .",
    "this is more of a technical challenge than a political one and we will asume that such a layer will always be put in place , so that access to existing applications is not an issue in deciding to adopt a data format .    the problems that then arise in discussions with instrumentation groups vary , but often economic factors enter .",
    "such a group is most likely to adopt a format that already closely matches their requirements , so that they minimise their work .",
    "but this only applies if the software is open , in the sense that they can make any changes themselves .",
    "if changes need to be implemented ( or integrated ) by a different data format project , it can be a serious disincentive unless the data format project commits to making the changes well in advance and enjoys a significant level of trust . with an open - source data format development project ,",
    "however , the openness is built - in and this barrier should be much reduced .",
    "nevertheless , any changes would still need to avoid regression in the software that already exists .",
    "this , in itself , can be a serious obstacle if the system is already complex , as most data format software is .",
    "if the data format is designed to be extensible , however , this burden is greatly reduced because extending the system consists largely of making additions and not changing what already exists .",
    "different problems can arise if a new instrumentation group is large and/or particularly well - funded .",
    "such groups may have little incentive to collaborate for economic reasons and might be tempted to start from scratch themselves producing , in effect , a rival project .",
    "this is arguably how most data formats arise in the first place .",
    "unfortunately , the result may have limited applicability outside the originating project .",
    "political arguments may sway the decision and these will be easier to make if the proposed data format already enjoys wide use . the argument becomes much easier to win , however , if the data format offering is intrinsically extensible , because this pretty much guarantees that the work involved in building on what exists will be substantially less than starting from scratch . moreover , it need not involve any compromise to the instrumentation group s goals because there is no constraint on what can be added .",
    "if major new instrumentation projects could be harnessed to contribute to a collaborative open - source data format project with worldwide adoption as a goal , then major benefits might accrue .",
    "perhaps the most significant would be that instead of new incompatible formats regularly arising they could instead be new compatible additions to a single format of increasing capability .",
    "there are many examples where well - funded initiatives build on an existing foss project for their own purposes and the new developments are then fed back into the main trunk for all to benefit .",
    "there seems to be little reason why astronomical data formats could nt develop in a similar , evolutionary way if openness and extensibility are fully embraced .",
    "the ndf data model did bring order to the chaos of arbitrary hierarchical structures and succeeded in the promise of providing a base specification that can be adopted by many applications processing data from disparate instrumentation .",
    "the shift from arbitrary use of hierarchical structures to a data model enforced by a library and api was extremely important and allowed application developers to know what to expect in data files .    from its beginnings in the mid-1980s",
    "the ndf data model has been used throughout the starlink software collection within diverse applications such as smurf  @xcite , ccdpack ( * ? ? ?",
    "* ; * ? ? ?",
    "* http://www.ascl.net/1403.021[ascl:1403.021 ] ) , gaia  and kappa .",
    "ndf was also adopted by uk - operated observatories .",
    "figaro  had a strong influence on the infrared spectroscopy community in the united kingdom and the united kingdom infrared telescope ( ukirt ) initially adopted dst ( sect .",
    "[ app : figaro ] ) for cgs3 and cgs4 @xcite .",
    "ndf was adopted at ukirt in 1995 although a unified ukirt ndf - based data model for all instruments , involving hds containers of ndf structures to handle multiple exposures , was not adopted until the release of the orac system @xcite .",
    "the james clerk maxwell telescope ( jcmt ) initially used a proposed submillimeter standard format known as the global section datafile ( gsd ; * ? ? ? * formerly general single dish data ) . in 1996",
    "scuba @xcite was delivered using ndf , and a unified ndf raw data model was adopted for acsis @xcite from 2006 and scuba-2 @xcite from 2009 .",
    "ndf data files are available from the ukirt and jcmt archives at the canadian astronomy data centre @xcite and both telescopes continue to write data using ndf .",
    "the anglo - australian observatory ( aao ) adopted hds and initially used dst for instruments such as ucles @xcite .",
    "iris @xcite could use both dst and ndf , whereas 2df @xcite used ndf .",
    "the ndf data model also supported a number of data - structuring experiments .",
    "the model allowed applications written in fortran to adopt object - oriented methodologies by adopting ndf as a backing store and using the self - describing features to represent objects @xcite .",
    "the hdx framework @xcite was developed around 2002 as a flexible way of layering high - level data structures , presented as a virtual xml dom , atop otherwise unstructured external data stores .",
    "this was in turn used to develop starlink s ndx framework,[sec : ndx ] which allowed fits files to be viewed and manipulated using the concepts from ndf .",
    "the ndx experiment was an attempt to directly apply the lessons of the long ndf / hds experience ",
    "namely that a small amount of structure , overlaid on conceptually separate bit buckets , can very promptly bring order out of chaos .",
    "the experiment successfully demonstrated the viability and power of the approach , and was used in some starlink java applications ( including treeview  @xcite and splat ( * ? ? ?",
    "* ; * ? ? ?",
    "* http://www.ascl.net/1402.007[ascl:1402.007 ] ) ) ; however it lost out to the more mainstream approach adopted by others within the vo , and was not more widely adopted .",
    "for the future we are considering the possibility of replacing the hds layer with a more widely used hierarchical data format such as hdf5 @xcite .",
    "this would have the advantage of making ndf available to a much larger community , albeit with ndf still being in fortran , and also remove the need to support hds in the longer term .",
    "ndf and all the existing starlink applications would continue to work so long as a conversion program is made available to convert hds structures to hdf5 structures .",
    "this would have the added advantage of making it straightforward to add support for tables natively to the ndf data model .",
    "many of the concepts in ndf map directly to hdf5 .",
    "one remaining issue is that hdf5 does not support the notion of arrays of groups so the ` history ` and ` axis ` structures in ndf would need to be remapped into a flatter layout , maybe with numbered components in an ` axis ` or ` history ` group .",
    "this research has made use of nasa s astrophysics data system .",
    "the starlink software is currently maintained by the joint astronomy centre , hawaii .",
    "we thank jim peden and trevor ponman for providing comments on the manuscript regarding the early days of hds and the development of asterix .",
    "we also thank to the two anonymous referees for their useful comments .",
    "the source code for the ndf library and the starlink software ( http://www.ascl.net/1110.012[ascl:1110.012 ] ) is open - source and is available on github at .",
    "this section provides an overview of the hds - based data models developed within the starlink ecosystem that influenced the development of ndf .      an early proposal ( * ? ? ?",
    "* but see also @xcite ) introduced the _ image _",
    "organizational scheme .",
    "this wright - giddings design specified that data should go into a ` data_array ` item and there should also be items for pre - computed data minimum and maximum , as well as a value for an array - specific blank value ( similar to the fits ` blank ` header keyword ) .",
    "errors were represented as standard deviations and stored in ` data_error ` and bad - pixel masks were stored in ` data_quality ` .",
    "an example layout is given in fig .  [",
    "fig : image ] .",
    "prior to hds becoming generally available , the starlink project adopted the bulk data frame ( bdf ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) as part of its _ interim _ software environment .",
    "bdf was heavily influenced by fits and used many of the same conventions .",
    "software was provided to convert bdf format files to hds using the _ image _ model @xcite , and the _ image _ model became reasonably popular , because of its simplicity , and because of the many bdf files that existed at the time .",
    "there were however a number of shortcomings with the _ image _ design , not the least of which was that it did not make use of hierarchical structures .",
    "the design was flat and heavily influenced by fits and bdf .",
    "the figaro  data reduction package ( * ? ? ?",
    "* ; * ? ? ?",
    "* http://www.ascl.net/1203.013[ascl:1203.013 ] ) independently adopted a hierarchical design based on hds .",
    "this dst data model made good use of structures and supported standard deviations for errors .",
    "axis information was stored in structures labeled x and y , and the main image / spectral data were stored in a structure labeled z. the main data array was ` z.data ` .",
    "fits - style keyword / value pairs were encoded explicitly in a structure called ` fits ` but using scalar components for each header item .",
    "any comments associated with the fits keywords were held in a similar structure labelled ` comments ` .",
    "this basic structure suggests a bias towards 1- or 2-dimensional data , but it could handle data of up to 6 dimensions ; the ` z.data ` array could have as many dimensions as hds would support , and the axis structures for the higher dimensions were labelled  awkwardly  from ` t ` through to ` w ` .",
    "an example layout can be found in fig .",
    "[ fig : dst ] .    around 1990 ,",
    "the code used by figaro  to access dst files was reworked to handle both dst and ndf files @xcite .",
    "support for ndf did not use the actual ndf library ; instead it used direct hds calls for both models , but would use different names for the hds items it accessed depending on the data model used in the file .",
    "this involved a significant reworking of the figaro  code , but maintained compatibility with existing dst files .",
    "however , it has failed to keep up with recent changes to ndf , such as support for 64-bit data .",
    "the asterix  x - ray data reduction package ( * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* http://www.ascl.net/1403.023[ascl:1403.023 ] ) used the hds format exclusively until the introduction of an abstract data access interface @xcite which allowed for the use of hds and fits format files .",
    "asterix  defined many data models designed for the specific uses of x - ray astronomy , with a particular focus on photon event lists .",
    "an example layout of an alignment file is shown in fig .",
    "[ fig : asterix ] . the asterix  data models were not competing directly with _ image _ or dst but this experience fed directly into the design of ndf .",
    "for example , the ` history ` structure was adopted without change .",
    "once ndf was available some data models were modified to use features from ndf such as adopting the ` data_array ` label ( see fig .  [",
    "fig : duck ] for an example ) .",
    "the fitsndf  conversion tool @xcite creates an ndf extension of type ` table ` for each fits table or bintable extension .",
    "the ` table ` structure is an extended version of the giaretta design .",
    "it comprises a scalar , ` nrows ` , to set the number of rows in the table , and an array structure ` columns ` also of type ` columns ` .",
    "the ` columns ` structure contains a series of ` column`-type structures , one for each field in the table .",
    "the name of each column comes from the corresponding ` ttypen ` keyword value .",
    "the conversion does rely on the column name not being longer than fifteen characters .",
    "a ` column ` structure has one mandatory component , ` data ` , an array of values for the field , and can be 2-dimensional if the field is an array .",
    "other fits keywords ( ` tformn ` , ` tscaln ` , and ` tformn ` ) prescribe the primitive type of ` data ` , including expansion of scaled form .",
    "components to store the original format , the ` ttypen ` comment , and units may also be present .",
    "an example of part of such a structure is shown in fig .",
    "[ fig : hdstable ] .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ....",
    "fits_ext_1    < table >        { structure }    nrows         < _ integer >     4    columns       < columns >      { structure }      sporder       < column >       { structure }        comment       < _ char*19 >     ' spectrum order '        data(4 )       < _ word >        1,2,3,5        format        < _ char*3 >      ' i11 '        nelem         < column >       { structure }        comment       < _ char*19 >     ' number of elements '        data(4 )       < _ word >        1024,1024,1024,1024        format        < _ char*3 >      ' i11 '        wavelength    < column >       { structure }        comment       < _ char*19 >     ' wavelengths of elements '        data(1024,4 ) < _ double >      2897.6015206853 ,                                   ... 5707.2796545742        units         < _ char*9 >      ' angstroms '        format        <",
    "_ char*6 >      ' g25.16 ' .... _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",
    "this appendix describes the data model used to record the `` family tree '' of ancestor ndfs that were used to create an ndf .",
    "each node in the tree describes a single ndf , with the root node being the ndf for which provenance is being recorded .",
    "thus the parents of the root node each describe one of the ndfs that were used to create the ndf described by the root node .",
    "a node stores the following items of information about the associated ndf :    * the path to the ndf within the local file system .",
    "this is a blank string for the root node , since the main ndf may be moved to a new location . *",
    "the utc date and time at which the ndf was created . * a boolean flag indicating if the ndf is `` hidden ''  meaning that the ndf will not be included as an ancestor when provenance is copied from one ndf to another . * a string identifying the command that created the ndf . * history information . for the root node ,",
    "this is just a 4-byte hash code that represents the contents of the main ndf s ` history ` component .",
    "this hash code is subsequently used to identify the same history information within other ndfs . for non - root nodes",
    ", the history information contains any history records read from the corresponding ancestor ndf that were not also present in any of the ancestors direct parents . *",
    "any extra arbitrary information associated with the ndf .",
    "there are no conventions on what this extra information represents . * a list of pointers to nodes representing the direct parents of the ndf .",
    "the full tree of nodes is stored on disk in an extension named ` provenance ` within the main ndf , and is encoded into an array of integers in order to avoid the overhead of reading and writing complicated hds structures .",
    "each application will normally first create a basic tree for each output ndf , holding a single root node describing the output ndf .",
    "it will then read the provenance tree from each input ndf and append each one to the provenance tree of the output ndf , making it a direct parent of the root node .",
    "when the application closes , the final output tree is stored in the output ndf .",
    "this whole process can be automated by registering handlers with the ndf library that are called whenever an ndf is opened . the only change that then needs to be made to an application to enable basic provenance tracking is for the application to add two calls to mark the start and end of a `` provenance recording context '' .",
    "alternatively , to achieve finer control of which input ndfs are recorded as parents of each output ndf , it is possible for an application to handle the reading and writing of provenance trees itself .",
    "it is possible that a single input ndf may be used many times in the creation of an output ndf .",
    "for instance , if ndf _ a _ is added to ndf _ b _ to create ndf _",
    "_ a _ is then added to _",
    "c _ to create _",
    ", then _ a _ ( and all its ancestors ) would appear twice in the provenance tree of _",
    "d_. to avoid this , whenever a new parent is added to the root node , each node within the tree of the new parent is compared with each node already in the tree .",
    "if they match , the tree of the new parent is `` snipped '' at that point to exclude the duplicated node ( and all its parent nodes ) .",
    "this comparison needs to be done carefully since it is possible for two nodes to include the same path , creator and date , and yet still refer to different ndfs . for this reason ,",
    "the comparison two nodes are considered equal if :    1 .",
    "they have the same path , date and creator , and 2 .",
    "they have the same number of parent nodes , and 3 .",
    "each pair of corresponding parent nodes are equal    the third requirement above means that two nodes will never be considered equal if any of the ancestors of the two nodes differ .",
    "the generalized extensions addition to the fits standard made provision for hierarchical data through three keywords : ` extname ` , ` extver ` , and ` extlevel `",
    ". indeed ` extlevel ` was added by don wells specifically with starlink hds in mind .",
    "however , there were two concepts missing from these to preserve an ndf structure .",
    "first and more important is the type of a structure .",
    "type defines the semantics and processing rules of an ndf structure . in object - oriented parlance",
    "it is the class .",
    "therefore we introduced an additional keyword ` exttype ` to preserve this information .",
    "note that the definition of ` extname ` was somewhat terse and vague in being just the _ name _ , and some fits writers have in effect used it as the type ( cf .",
    "[ sec : name_v_type ] ) .",
    "the second missing feature was a means to record the shape of a structure .",
    "to map from the hierarchical ndf structure to the flat fits serialization array components such as ` variance ` and ` quality ` are written to fits image extensions , whose headers retain information stored in other top - level components such as ` wcs ` and ` label ` .",
    "specially formatted headers may be written to record the ` history ` , which if not edited , can be read back into ndf ` history ` records .",
    "this is not as robust as we would like , and a recent formatting change to cfitsio  ( * ? ? ?",
    "* http://www.ascl.net/1010.001[ascl:1010.001 ] ) temporarily prevented recovery of some records .",
    "likewise provenance information , if present , is stored via five keywords , ` prv[cdimp]n ` for the @xmath1th ndf .",
    "this limits the maximum number of ndfs in the provenance tree to 9999 .",
    "existing fits - like headers within the ndf ( sect .",
    "[ sec : fitsheaders ] ) are merged , but with ndf information such as the array shape or wcs superseding the original keyword values .",
    "ndf extension structures become binary tables when in fits form .",
    "each primitive component within a structure becomes a column in the table with the appropriate data type and dimension .",
    "each element of an array of structures becomes a separate binary table .",
    "a more - compact storage would be to create a single table for the array structure , forming a superset of columns , then writing a row for each structure element , using null values where necessary .",
    "however , the adopted structure significantly simplified the recursive code . also in practice array structures are small and usually 1-dimensional .",
    "` extname ` : :    within a fits image extension this is the name of an array component ,    since these are in well - defined locations within the ndf . within a    binary table , which could record arbitrary ndf structures , ` extname `    stores the dot - separated path within the hierarchical structure",
    ". the    path may also include indices to the elements of an array of    structures , written as comma - separated list between parentheses .",
    "+    a common issue especially for ndf extensions is that the path name is    too long for the 68 characters in a fits header ` extname ` is set to a    special string ` @extnamef ` that can never be in the component path .",
    "the full path is written as a long string to keyword extnamef using    the heasarc long - string ` continue ` convention @xcite . `",
    "exttype ` : :    the non - primitive data type of the ndf extension or structure . `",
    "extshape ` : :    this records the shape of the ndf extension as a comma - separated list . `",
    "hduclas1 ` : :    set to ` ndf ` . `",
    "hduclas2 ` : :    the name of the ndf array component .    for the reverse operation , the fits reader decides whether or not it knows the semantics of the fits file to map fits extensions to ndf components .",
    "various products from known sources are recognised from keyword values . in the the case of a former ndf",
    ", the reader examines the ` hduclas1 ` keyword .",
    "if its value is ` ndf ` , the reader endeavors to recreate the ndf components and extensions from the keywords listed above , working through the fits extensions in order .",
    "for an arbitrary fits file there is no reason to expect that its data model maps well to the ndf model .",
    "the default behavior is for the primary hdu to map to the ndf ` data_array ` , in which blank and ` nan ` values become the ndf bad value ; the wcs headers are used to form an ndf ` wcs ` or ` axis ` component .",
    "for many cases having the primary array and metadata is adequate for the conversion .",
    "it s analogous to the early _",
    "image_-model files operating as ndfs",
    ". however , additional fits extensions are preserved by default too , being converted to a series of ndf extensions .",
    "the hds type of each ndf extension depends on the ` xtension ` keyword .",
    "it is ` ndf ` for an image , and ` table ` ( [ app : hdstable ] ) for bintable and table .",
    "the latter representation is often not optimal , as such extensions can be used to represent many different data models .",
    "an example is when a binary table holds a series of data arrays observed at different locations such as from multi - object spectroscopy .",
    "it is possible with generic hds tools to manipulate such data into ndf form . where the user knows the mappings between multi - extension fits and ndf array components , the fits reader has a mechanism for specifying these mappings .",
    "97 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , et  al . , . , in : , , ( eds . ) , , volume of _ _ .",
    ", . , in : ,",
    ", ( eds . ) , , volume   of _ _ . p. .",
    "starlink project .",
    ", d.  a. et  al . , . .",
    ", in : , , ( eds . ) , , volume   of _ _ . p. .",
    ", , , , . .",
    "starlink project .",
    ", , , , , . , in : , ( eds . ) , , volume of _ _ .",
    ", . , in : , , ( eds . ) , , volume of _ _ .",
    "starlink project .",
    ". starlink project .",
    ", , , , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    ", a. et  al . , . , in : ( ed . ) , , volume of _ _ .",
    ", j.  v. et  al . , . .",
    ", . , http://arxiv.org/abs/0907.3610[arxiv:0907.3610 ] .",
    ", , , , . . ,",
    ", , , , , , , , . . ,",
    "starlink project . , . .",
    ", in : ( ed . ) , ,",
    ". . , . , ,",
    "starlink project .",
    ", , , , , , .",
    ", in : , ( eds . ) , , volume of _ _ .",
    ", , , , , , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    ", , , , , . .",
    "starlink project .",
    "starlink project .",
    ", , , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    ", , , , . , in : ( ed . ) , ,",
    "volume of _ _ .",
    ", , , , , , , , . , in : , , ( eds . ) , , volume of _ _ .",
    ", , , , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    "starlink project .",
    ", , , , , , , , . .",
    ", , , , , , .",
    ", in : , , ( eds . ) , , volume of _ _ . p. .",
    ", , , , . , in : , ( eds . ) , , volume of _ _ .",
    ", , , , , . , in : , .",
    ", . , in : , , ( eds . ) , , volume of _ _",
    ", , , , , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    ", , , , , , , .",
    ", in : , , ( eds . ) , , volume of _ _ .",
    ", , , , . . ,",
    ", , , , . . ,",
    ", , , , . . ,",
    ", et  al . , . .",
    "/ tr / html5/. , et  al . , . . , . . , w.  s. et  al . , . . , .",
    ", http://arxiv.org/abs/astro-ph/9809122[arxiv:astro-ph/9809122 ] . , .",
    ", , , , , , .",
    ", in : , , ( eds . ) , , volume of _ _ . p. .",
    ", , . , in : gajadhar , s. et  al .",
    "( eds . ) , , p.  .",
    "http://arxiv.org/abs/1111.5855[arxiv:1111.5855 ] .",
    ", , . , in : , , ( eds . ) , , volume of _ _ .",
    "jenness , t. et  al . , . , in : , ( eds . ) , , volume of _ _ .",
    ", http://arxiv.org/abs/1406.1515[arxiv:1406.1515 ] .",
    ", , , , . .",
    "joint astronomy centre .",
    ", , , . , in : , , ( eds . ) , , volume   of _ _ . p. .",
    ", , , , . .",
    ". , http://arxiv.org/abs/1403.2801[arxiv:1403.2801 ] . , , .",
    ". national center for supercomputing applications , university of illinois at urbana - champaign . , . . , . , . .",
    "starlink project . , .",
    ". , . , i.  j. et  al . ,",
    ". , http://arxiv.org/abs/astro-ph/0202175[arxiv:astro-ph/0202175 ] .",
    ", , . , in : , , .",
    "tapp12 . pp . .",
    "http://dl.acm.org/citation.cfm?id=2342875.2342881 . ,",
    "j. et  al . , . .",
    "http://arxiv.org/abs/1204.3055[arxiv:1204.3055 ] .",
    ", in : , ( eds . )",
    ", , volume   of _ _ .",
    "starlink project .",
    ", , . , in : ( ed . ) , , volume of _ _ .",
    "ftp://legacy.gsfc.nasa.gov/fits_info/ofwg_minutes/ofwg_93jul21.txt . , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    ", , . , . , , , . , . , , , .",
    "http://arxiv.org/abs/1201.1345[arxiv:1201.1345 ] . ,",
    ". . , . , . , in : , , ( eds . ) , , volume   of _ _ .",
    ", et  al . , . , in : , ( eds . ) , , volume of _ _ . p. . ,",
    "et  al . , . .",
    ". , . , . , . , in : , , ( eds . ) , , volume   of _ _",
    ", , , , , . .",
    ", . , in : , ( eds . ) , , volume   of _ _ .",
    ", , . , in : , , ( eds . ) , , volume of _ _ .",
    "starlink project .",
    "starlink project .",
    ", , . , in : , , ( eds . ) , , p. . , , , . .",
    "starlink project . , . .",
    "starlink project .",
    ", , , , , , . , in : ( ed . ) , ,",
    "volume of _ _ ."
  ],
  "abstract_text": [
    "<S> the extensible _ n_-dimensional data format ( ndf ) was designed and developed in the late 1980s to provide a data model suitable for use in a variety of astronomy data processing applications supported by the uk starlink project . </S>",
    "<S> starlink applications were used extensively , primarily in the uk astronomical community , and form the basis of a number of advanced data reduction pipelines today . </S>",
    "<S> this paper provides an overview of the historical drivers for the development of ndf and the lessons learned from using a defined hierarchical data model for many years in data reduction software , data pipelines and in data acquisition systems .    </S>",
    "<S> data formats , data models , starlink , history of computing </S>"
  ]
}