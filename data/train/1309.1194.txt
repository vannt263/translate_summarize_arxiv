{
  "article_text": [
    "subspace signal processing theory and practice rely , conventionally , on the familiar @xmath1-norm based singular - value decomposition ( svd ) of the data matrix .",
    "the svd solution traces its origin to the fundamental problem of @xmath1-norm low - rank matrix approximation , which is equivalent to the problem of maximum @xmath1-norm orthonormal data projection with as many projection ( `` principal '' ) components as the desired low - rank value @xcite .",
    "practitioners have long observed , however , that @xmath1-norm principal component analysis ( pca ) is sensitive to the presence of outlier values in the data matrix , that is , values that are away from the nominal distribution data , appear only few times in the data matrix , and are not to appear again under normal system operation upon design .",
    "this paper makes a case for @xmath0-subspace signal processing .",
    "interestingly , in contrast to @xmath1 , subspace decomposition under the @xmath0 error minimization criterion and the @xmath0 projection maximization criterion are not the same .",
    "a line of recent research pursues calculation of @xmath0 principal components under error minimization [ ] or projection maximization [ ] , [ ] ./@xmath1-norm approach has been followed in [ ] . ]",
    "no algorithm has appeared so far with guaranteed convergence to the criterion - optimal subspace and no upper bounds are known on the expended computational effort .    in this present work , given any data matrix @xmath5 of @xmath2 signal samples of dimension @xmath3 , we show that the general problem of finding the maximum @xmath0-projection principal component of @xmath6 is formally np - hard for asymptotically large @xmath2 , @xmath3 .",
    "we prove , however , that the case of engineering interest of fixed given dimension @xmath3 is not np - hard . in particular , for the case where @xmath7 , we present in explicit form an algorithm to find the optimal component with computational cost @xmath8 . for the case where the sample support exceeds the data dimension",
    "( @xmath9 ) which is arguably of more interest in signal processing applications we present an algorithm that computes the @xmath0-optimal principal component with complexity @xmath10 , @xmath11 .",
    "we generalize the effort to the problem of calculating multiple @xmath0 components ( necessarily a joint computational problem ) and present an explicit optimal algorithm for multi - component subspace design in the form of matrix nuclear - norm maximization .",
    "consider @xmath2 real - valued measurements @xmath12 of dimension @xmath3 that form the @xmath13 data matrix @xmath14.\\ ] ] we are interested in describing ( approximating ) the data matrix @xmath15 by a rank-@xmath16 product @xmath17 where @xmath18 , @xmath19 , @xmath20 , in the form of problem @xmath21 defined below , @xmath22 where @xmath23 is the @xmath1 matrix norm ( frobenius ) of matrix @xmath24 with elements @xmath25 . by the projection theorem @xcite , @xmath26 for any fixed @xmath27 , @xmath28 .",
    "hence , we obtain the equivalent problem @xmath29 frequently referred to as left - side @xmath16-svd . since @xmath30 where @xmath31 denotes the trace of a matrix , @xmath32 is also equivalent to @xmath1 projection ( energy ) maximization , @xmath33 note that , if @xmath34 and we possess the solution @xmath35 for @xmath16 singular / eigen - vectors in ( [ eq : rs ] ) , ( [ eq : rr ] ) , ( [ eq : r ] ) , then the solution for rank @xmath36 is derived readily by @xmath37 $ ] with @xmath38 this is known as the pca scalability property .    by minimizing the sum of squared errors , @xmath1 principal component calculation becomes sensitive to extreme error value occurrences caused by the presence of outlier measurements in the data matrix .",
    "motivated by this observed drawback of @xmath1 subspace signal processing , in this work we study and pursue subspace - decomposition approaches that are based on the @xmath0 norm , @xmath39 .",
    "we may  translate \" the three equivalent @xmath1 optimization problems ( [ eq : rs ] ) , ( [ eq : rr ] ) , ( [ eq : r ] ) to new problems that utilize the @xmath0 norm as follows , @xmath40 a few comments appear useful at this point : ( i ) under the @xmath0 norm , the three optimization problems @xmath41 , @xmath42 , and @xmath43 are no longer equivalent .",
    "( ii ) under @xmath0 , the pca scalability property does not hold ( due to loss of the projection theorem ) .",
    "( iii ) even for reduction to a single dimension ( rank @xmath44 approximation ) , the three problems are difficult to solve .    in this present work ,",
    "we focus exclusively on @xmath43 .",
    "in this section , we concentrate on the calculation of the @xmath0-maximum - projection component of a data matrix @xmath45 ( problem @xmath46 in ( [ eq : rl1 ] ) , @xmath44 ) .",
    "first , we show that the problem is in general np - hard and review briefly suboptimal techniques from the literature .",
    "then , we prove that , if the data dimension @xmath3 is fixed , the principal @xmath0-norm component of @xmath6 is in fact computable in polynomial time and present a calculation algorithm with complexity @xmath47 , @xmath48 .      in proposition [ prop : quad ] below , we present a fundamental property of problem @xmath43 , @xmath44 , that will lead us to an efficient solution . the proof is omitted due to lack of space and can be found in @xcite .    for any data matrix @xmath45 , the solution to @xmath49 is given by @xmath50 where@xmath51 in addition , @xmath52 .",
    "@xmath53 [ prop : quad ]    the straightforward approach to solve ( [ eq : bopt ] ) is an exhaustive search among all @xmath8 binary vectors of length @xmath2 .",
    "proposition [ prop : nphard ] below declares that , indeed , in its general form @xmath46 , @xmath44 , is np - hard for jointly asymptotically large @xmath54 .",
    "the proof can be found in @xcite .",
    "computation of the @xmath0 principal component of @xmath55 by maximum @xmath0-norm projection ( problem @xmath56 , @xmath44 ) is np - hard in jointly asymptotic @xmath57 . @xmath53 [ prop : nphard ]      there has been a growing documented effort to calculate subspace components by @xmath0 projection maximization  [ ] , [ ] . for @xmath44 ,",
    "both algorithms in  @xcite , @xcite are identical and can be described by the simple single iteration @xmath58 for the computation of @xmath59 in  ( [ eq : bopt ] ) .",
    "equation , however , does not guarantee convergence to the @xmath0-optimal component solution ( convergence to one of the many local maxima may be observed ) . in the following section , we present for the first time in the literature an optimal algorithm to calculate the @xmath60 principal component of a data matrix with complexity polynomial in the sample support @xmath2 when the data dimension @xmath3 is fixed .      in the following ,",
    "we show that , if @xmath3 is fixed , then computation of @xmath61 is no longer np - hard ( in @xmath2 ) .",
    "we state our result in the form of proposition [ prop : polynomial ] below .    for any fixed data dimension @xmath3 ,",
    "computation of the @xmath0 principal component of @xmath55 has complexity @xmath62 , @xmath63 . @xmath53",
    "[ prop : polynomial ]    by proposition [ prop : nphard ] , computation of the @xmath60 principal component of @xmath6 is equivalent to computation of @xmath64 in ( [ eq : bopt ] ) . to prove proposition [ prop : polynomial",
    "] , we will then prove that @xmath59 can be computed with complexity @xmath47 .",
    "we begin our developments by defining @xmath65 then , @xmath66 has also rank @xmath67 and can be decomposed by @xmath68,\\;{\\bf q}_i^t{\\bf q}_j=0,\\;i\\neq j , \\label{eq : qq}\\end{aligned}\\ ] ] where @xmath69 , @xmath70 , @xmath71 , @xmath72 are the @xmath67 eigenvalue - weighted eigenvectors of @xmath66 with nonzero eigenvalue . by  ( [ eq : bopt ] ) , @xmath73    for the case @xmath7",
    ", the optimal binary vector @xmath59 can be obtained directly from  ( [ eq : qb ] ) by an exhaustive search among all @xmath8 binary vectors @xmath74 .",
    "therefore , we can design the @xmath0-optimal principal component @xmath61 with computational cost @xmath75 .",
    "for the case where the sample support exceeds the data dimension ( @xmath76 ) -which is arguably of higher interest in signal processing applications- we find it useful in terms of both theory and practice to present our developments separately for data rank @xmath77 , @xmath78 , and @xmath79 .",
    "+ _ 1 ) case @xmath77 : _ if the data matrix has rank @xmath77 , then @xmath80 and  ( [ eq : qb ] ) becomes @xmath81 by  ( [ eq : rl1 ] ) , the @xmath0-optimal principal component is @xmath82 designed with complexity @xmath83 .",
    "it is of notable practical importance to observe at this point that even when @xmath6 is not of true rank one , presents us with a quality , trivially calculated approximation of the @xmath0 principal component of @xmath6 : calculate the @xmath1 principal component @xmath84 of the @xmath85 matrix @xmath86 , quantize to @xmath87 , and project and normalize to obtain @xmath88 .",
    "+ _ 2 ) case @xmath78 : _ if @xmath78 , then @xmath89 $ ] and  ( [ eq : qb ] ) becomes @xmath90 the binary optimization problem was seen and solved for the first time in @xcite by the auxiliary - angle method @xcite with complexity @xmath91 . due to lack of space , we omit the specifics of the case @xmath78 and move directly to the general case @xmath92 .    _ 3 ) case @xmath93 : _ if @xmath94 , we design the @xmath0-optimal principal component of @xmath15 with complexity @xmath95 by considering the multiple - auxiliary - angle approach that was presented in  @xcite as a generalization of the work in  @xcite .",
    "consider a unit vector @xmath96 . by cauchy - schwartz , for any @xmath97 ,",
    "@xmath98 with equality if and only if @xmath99 is codirectional with @xmath100",
    ". then , @xmath101 by  ( [ eq : ca ] ) , the optimization problem in  ( [ eq : qb ] ) becomes @xmath102 for every @xmath96 , inner maximization in  ( [ eq : maxmax ] ) is solved by the binary vector @xmath103 which is obtained with complexity @xmath104 .",
    "then , by  ( [ eq : maxmax ] ) , the solution to the original problem in  ( [ eq : qb ] ) is met if we collect all binary vectors @xmath105 returned as @xmath99 scans the unit - radius @xmath67-dimensional hypersphere",
    ". that is , @xmath59 in  ( [ eq : qb ] ) is inth element of vector @xmath106 , @xmath107 , can be set nonnegative without loss of optimality , because , for any given @xmath106 , @xmath108 , the binary vectors @xmath109 and @xmath110 result to the same metric value in . ] @xmath111    two fundamental questions for the computational problem under consideration are what the size ( cardinality ) of set @xmath112 is and how much computational effort is expended to form @xmath112 .",
    "the candidate vector set @xmath112 has cardinality @xmath113 and it suffices to solve @xmath114 for every @xmath115 , @xmath116 ( i.e. , @xmath117 contains any @xmath118 rows of @xmath119 ) .",
    "the solution to  ( [ eq : qc ] ) is the unit vector in the null space of the @xmath120 matrix @xmath121 . is full - rank , then its null space has rank @xmath122 and @xmath123",
    "is uniquely determined ( within a sign ambiguity which is resolved by @xmath124 ) .",
    "if , instead , @xmath121 is rank - deficient , then the intersection of the @xmath118 hypersurfaces ( i.e. , the solution of  ( [ eq : qc ] ) ) is a @xmath125-manifold ( with @xmath126 ) in the @xmath127-dimensional space and does not generate new binary vectors of interest .",
    "hence , linearly dependent combinations of @xmath118 rows of @xmath128 are ignored . ] then , the binary vectors @xmath129 of interest are obtained by @xmath130 with complexity @xmath104 .",
    "note that  ( [ eq : sgnqc ] ) presents ambiguity regarding the sign of the intersecting @xmath118 hypersurfaces ( zero values ) . a straightforward way to resolve the ambiguity uses an alternative way of resolving the sign ambiguities at the intersections of hypersurfaces which was developed in  @xcite and led to the direct construction of a set @xmath112 of size @xmath131 with complexity @xmath132 . ]",
    "is to consider all @xmath133 sign combinations for the @xmath118 zero value positions . since complexity @xmath104",
    "is required to solve  ( [ eq : sgnqc ] ) for each subset of @xmath118 rows of @xmath128 , the overall complexity of the construction of @xmath112 is @xmath132 for any given matrix @xmath134 .",
    "our complete , new algorithm for the computation of the @xmath0-optimal principal component of a rank-@xmath67 matrix @xmath55 that has complexity @xmath95 is presented in detail in fig .",
    "[ fig : algo ] .    ' '' ''    ' '' ''    * the optimal @xmath0-principal - component algorithm *    ' '' ''     +    [ cols= \" < \" , ]      +     + else , @xmath135 + * output : * @xmath136 +    ' '' ''    ' '' ''    [ fig : algo ]",
    "in this section , we switch our interest to the joint design of @xmath137 principal @xmath0 components of a @xmath13 matrix @xmath15 .      for the case",
    "@xmath137 ,  @xcite proposed to design the first @xmath0 principal component @xmath61 by the coupled iteration  ( [ eq : kwak3 ] ) ( which does not guarantee optimality ) and then project the data onto the subspace that is orthogonal to @xmath61 , design the @xmath0 principal component of the projected data by the same coupled iteration , and continue similarly . to avoid the above suboptimal greedy approach ,  @xcite presented an iterative algorithm for the computation of @xmath138 altogether ( that is the joint computation of the @xmath16 principal @xmath0 components ) , which does not guarantee convergence to the @xmath0-optimal subspace .",
    "for any @xmath139 matrix @xmath140 , @xmath141 where @xmath142 denotes the nuclear norm ( i.e. , the sum of the singular values ) of @xmath140 .",
    "maximization in  ( [ eq : raa ] ) is achieved by @xmath143 where @xmath144 is the `` compact '' svd of @xmath140 , @xmath145 and @xmath146 are @xmath147 and @xmath148 , respectively , matrices with @xmath149 , @xmath150 is a nonsingular diagonal @xmath151 matrix , and @xmath67 is the rank of @xmath140 .",
    "this is due to the trace version of the cauchy - schwarz inequality  @xcite , according to which @xmath152 with equality if @xmath153 which is satisfied by @xmath143 .    to identify the optimal @xmath0 subspace for any number of components @xmath16",
    ", we begin by presenting a property of @xmath43 in the form of proposition  [ prop : nuclear ] below . the proof is omitted and can be found in @xcite .    for any data matrix @xmath45 , the solution to @xmath154 is given by @xmath155 where @xmath145 and @xmath146 are the @xmath139 and @xmath156 matrices that consist of the @xmath16 highest - singular - value left and right , respectively , singular vectors of @xmath157 with @xmath158 in addition , @xmath159 .",
    "@xmath53 [ prop : nuclear ]    by proposition  [ prop : nuclear ] , to find exactly the optimal @xmath0-norm projection operator @xmath138 we can perform the following steps :    1",
    ".   solve  ( [ eq : bopt ] ) to obtain @xmath160 .",
    "2 .   perform svd on @xmath161 .",
    "3 .   return @xmath162 .",
    "step @xmath122 can be executed by an exhaustive search among all @xmath163 binary matrices of size @xmath156 followed by evaluation in the metric of interest in  ( [ eq : bopt ] ) .",
    "that is , with computational cost @xmath164 we identify the @xmath0-optimal @xmath16 principal components of @xmath15 .",
    "an optimal algorithm for the computation of the @xmath0-optimal @xmath16 principal components of @xmath15 with complexity @xmath165 , @xmath48 , is presented in  @xcite .",
    "0.329     0.329     0.329     [ fig : dr ]",
    "we generate a data - set @xmath166 of @xmath167 two - dimensional ( @xmath168 ) observation points drawn from the gaussian distribution @xmath169 as seen in fig . [",
    "fig : dr1 ] .",
    "we calculate the @xmath1 ( by standard svd ) and @xmath0 ( by section iii.c , case @xmath78 , complexity about @xmath170 ) principal component of the data matrix @xmath6",
    ". principal component of @xmath171 would have required complexity proportional to @xmath172 ( by ) , which is of course infeasible . ] then , we assume that our data matrix is corrupted by three outlier measurements , @xmath173 , shown in the bottom right corner of fig .",
    "[ fig : dr2 ] .",
    "we recalculate the @xmath1 and @xmath0 principal component of the corrupted data matrix @xmath174 $ ] and notice ( fig .",
    "[ fig : dr1 ] versus fig .",
    "[ fig : dr2 ] ) how strongly the @xmath1 component responds to the outliers compared to @xmath0 . to quantify the impact of the outliers , in fig .",
    "[ fig : dr3 ] we generate @xmath175 new independent evaluation data points from @xmath169 and estimate the mean square - fit - error @xmath176 when @xmath177 or @xmath178 .",
    "we find @xmath179 versus @xmath180 .",
    "in contrast , when the principal component is calculated from the clean training set , @xmath181 or @xmath182 , we find mean square - fit - error @xmath183 and @xmath184 , correspondingly .",
    "we conclude that dimensionality reduction by @xmath0 principal components may loose only little in mean - square fit compared to @xmath1 when the designs are from clean training sets , but can protect significantly from outlier corrupted training .",
    "we consider a uniform linear antenna array of @xmath185 elements that takes @xmath186 snapshots of two incoming signals with angles of arrival @xmath187 and @xmath188 , @xmath189 where @xmath190 are the received - signal amplitudes with array response vectors @xmath191 and @xmath192 , correspondingly , and @xmath193 is additive white complex gaussian noise .",
    "we assume that the signal - to - noise ratio ( snr ) of the two signals is @xmath194 and @xmath195 .",
    "next , we assume that one arbitrarily selected measurement out of the ten observations @xmath196 \\in \\mathbb c^{5 \\times 10}$ ] is corrupted by a jammer operating at angle @xmath197 with amplitude @xmath198 .",
    "we call the resulting corrupted observation set @xmath199 and create the real - valued version @xmath200^t \\in \\mathbb r^{10 \\times 10}$ ] by @xmath201 part concatenation .",
    "we calculate the @xmath202 @xmath1-principal components of @xmath203 , @xmath204 \\in \\mathbb r^{10 \\times 2}$ ] , and the @xmath202 @xmath0-principal components of @xmath203 , @xmath205 \\in \\mathbb r^{10 \\times 2}$ ] . in fig .",
    "[ fig : doa ] , we plot the standard @xmath1 music spectrum @xcite @xmath206 where @xmath207^t$ ] , as well as what we may call `` @xmath0 music spectrum '' with @xmath208 in place of @xmath209 .",
    "it is interesting to observe how @xmath0 music ( in contrast to @xmath1 music ) does not respond to the one - out - of - ten outlying jammer value in the data set and shows only the directions of the two actual nominal signals .",
    "@xmath1 or @xmath0 calculated principal components ( data set of @xmath186 measurements with signals at @xmath210 and @xmath188 of which one measurement is additive - jammer corrupted with @xmath211 ; @xmath212db ; @xmath213db ) . ]",
    "[ fig : doa ]",
    "we presented for the first time in the literature optimal ( exact ) algorithms for the calculation of the maximum-@xmath0-projection component of data sets with complexity polynomial in the sample support size ( and exponent equal to the data dimension ) .",
    "we generalized to multiple @xmath0-max - projection components and presented an explicit optimal @xmath0 subspace calculation algorithm in the form of matrix nuclear - norm evaluations .",
    "when @xmath0 subspaces are calculated on nominal `` clean '' training data , they differ little arguably from their @xmath1-subspace counterparts in least - squares fit .",
    "however , subspaces for data sets with possibly erroneous , `` outlier '' entries , @xmath0 subspace calculation offers significant robustness / resistance to the presence of inappropriate data values .",
    "q. ke and t. kanade , `` robust @xmath0 norm factorization in the presence of outliers and missing data by alternative convex programming , '' in _ proc .",
    "ieee conf .",
    ". vision pattern recog .",
    "( cvpr ) _ , san diego , ca , june 2005 , pp .",
    "739 - 746 .",
    "f. nie , h. huang , c. ding , d. luo , and h. wang , `` robust principal component analysis with non - greedy @xmath214-norm maximization , '' in _ proc .",
    "joint conf .",
    "( ijcai ) _ , barcelona , spain , july 2011 , pp .",
    "1433 - 1438 .    c. ding , d. zhou , x. he , and h. zha , `` @xmath215-pca : rotational invariant @xmath0-norm principal component analysis for robust subspace factorization , '' in _ proc .",
    "_ , pittsburgh , pa , 2006 , pp .",
    "281 - 288 ."
  ],
  "abstract_text": [
    "<S> we describe ways to define and calculate @xmath0-norm signal subspaces which are less sensitive to outlying data than @xmath1-calculated subspaces . </S>",
    "<S> we focus on the computation of the @xmath0 maximum - projection principal component of a data matrix containing @xmath2 signal samples of dimension @xmath3 and conclude that the general problem is formally np - hard in asymptotically large @xmath2 , @xmath3 . </S>",
    "<S> we prove , however , that the case of engineering interest of fixed dimension @xmath3 and asymptotically large sample support @xmath2 is not and we present an optimal algorithm of complexity @xmath4 . </S>",
    "<S> we generalize to multiple @xmath0-max - projection components and present an explicit optimal @xmath0 subspace calculation algorithm in the form of matrix nuclear - norm evaluations . </S>",
    "<S> we conclude with illustrations of @xmath0-subspace signal processing in the fields of data dimensionality reduction and direction - of - arrival estimation . </S>"
  ]
}