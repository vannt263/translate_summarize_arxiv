{
  "article_text": [
    "in recent years , modern technology makes massive , large - scale data sets appear frequently .",
    "that is , the number of parameters ( @xmath0 ) is much larger than the sample size ( @xmath1 ) .",
    "financial problems for instance , investment portfolio involves hundreds of stocks but valid sample sizes are often only one hundred or less since the samples obtained before 6 months ago often loses their effectiveness .",
    "moreover , computational field , biological field , etc , data sets like this ( @xmath2 ) is becoming more and more important in diverse fields , and poses great challenges and opportunities for statistical analysis .",
    "consider the regression model @xmath3 where @xmath4 is the @xmath5 design matrix of predictor variables .",
    "@xmath6 is the true regression coefficients and @xmath7 is a vector of i.i.d .",
    "random variables with mean @xmath8 and variance @xmath9 .",
    "increasing statistic tools are developed to solve the high - dimensional data analysis , @xcite .",
    "penalized least squares like lasso , @xcite established the irrepresentable conditions for the variable selection @xcite ; elastic net @xcite , adaptive lasso @xcite , etc have been widely used .",
    "scad @xcite is also a very popular method due to its good computational and statistical properties .",
    "it enjoys the oracle property with probability tending to @xmath10 , and at the same time , estimate the nonzero components accurately @xcite . ] which means it can perform as well as the oracle .",
    "@xcite studied the penalized likelihood with the @xmath11-penalty .",
    "@xcite proposed the penalized composite likelihood method in ultra - high dimensions . discussed the @xmath11-penalized quantile regression in high - dimensional sparse models .",
    "@xcite proposed weighted robust lasso in the ultra - high dimensional setting that the number of parameters grow exponentially with the sample size .",
    "the adaptive elastic net estimator is defined as the minimizer of the weighted @xmath11-penalized and @xmath12-penalized least squares criterion function . @xmath13",
    "the @xmath11 part performs automatic variable selection , while the @xmath12 part stabilizes the solution paths and improves the prediction .",
    "@xmath14 are the adaptive data - driven weights , which used to reduce the bias problem induced by the @xmath11-penalty .",
    "hence the adaptive elastic net is an improved version of the lasso , elastic net and adaptive lasso .",
    "adaptive weights can be computed by different values : @xmath15 , @xmath16 where @xmath17 is a positive constant @xcite , @xmath18 and @xmath19 @xcite , @xmath20 @xcite .",
    "the adaptive elastic net method is shown that which enjoys the oracle property @xcite with a diverging number of predictors @xcite .",
    "although the oracle property of the adaptive elastic net estimators with a diverging number of predictors was already studied before , the asymptotic properties of the adaptive elastic net with the ultra - high dimensional setting remains unknown .",
    "furthermore , penalized least squares always need the particular condition to get variable selection consistency and few literatures discussed about the accuracy of this statistical inference on the nonzero regression parameters before .    in this paper , we first study the asymptotic properties of adaptive elastic net for the growing number of parameters where the dimensionality can grow exponentially with the sample size .",
    "we find a simple set @xmath21 where @xmath22 .",
    "@xmath23 is a positive constant .",
    "we compute adaptive parameter @xmath24 by the lasso estimator and the adaptive weighter @xmath25 is computed as @xmath26 , @xmath27 . according to the estimation consistency of adaptive parameter , the choice of @xmath17 and conditional on @xmath28 , we lead to variable selection consistency of the adaptive elastic net when the noise vector @xmath29 has i.i.d .",
    "entries in the ultra - high dimensional setting . in our proof ,",
    "the probability for adaptive elastic net to select true model is covered by the probability of @xmath30 .",
    "the first half part of the proof of theorem [ thm:1 ] states the probability of @xmath31 decays at an exponential rate under ultra - high dimensional setting .",
    "the latter part states the relationship between @xmath32 and @xmath33 without any other constrains .",
    "then , we introduce the mse and bias of adaptive elastic net and indicate that their decay rate depends on the probability of selecting wrong variables @xmath32 .",
    "consequently , the mse and bias can both decay to zero with suitable choice of tuning parameters @xmath34 and @xmath35 . however , one weakness of these rates is that they may lead to an inferior rate depending on the choice of the tuning parameters and the initial parameter @xmath36 .",
    "we also find that the traditional penalized least squares can not have an ideal prediction accuracy both in simulations and financial fields .",
    "for instance , we apply penalized least square method to track sp500 .",
    "it has @xmath37 to @xmath38 predicted ( annual ) tracking errors when select 50 constituent stocks .",
    "if we reduce the number of selected stocks like 20 , the tracking errors increase significantly .",
    "we want to improve the mentioned theoretical defect and prediction accuracy , oscillation simultaneous by applying other method .",
    "therefore , we propose a valid technique , called ssls , for separate selection from least squares .",
    "it selects variable first and sets others to @xmath8 , reducing high dimensional setting to low dimension setting by adaptive elastic net in the paper , then uses ordinary least squares ( ols ) to estimate coefficients . that is @xmath39 where @xmath40 obtained in the low dimensional linear regression models : @xmath41 .",
    "there are two reasons why the ordinary least squares ( ols ) estimates is unsuitable in high dimensional setting : prediction accuracy and interpretation @xcite .",
    "but if we do nt need shrink any coefficient to @xmath8 and consider the regression model in low dimension setting .",
    "ols estimates lead to a satisfying prediction accuracy .",
    "this method is similar as ols post - lasso estimator @xcite which is shown at least as well as lasso .",
    "we use adaptive elastic net to select the variable and study the properties of ssls , hence ssls has variable selection consistency .",
    "we show that the bias of ssls decays at an exponential rate and the decay rate of mse achieves the oracle convergence rate .",
    "also , the asymptotic normality of ssls is proved .    finally ,",
    "simulations and empirical part show that ssls produces large improvement compared with other methods . in the simulation part ,",
    "we implement five methods under different settings and use @xmath11 , @xmath12 loss to be measures .",
    "ssls has the best performance among others in all the settings based on @xmath42 replications .",
    "similarly in empirical part , ssls also outperforms lasso in the most months and significantly reduces the tracking error when select very few consistent stocks to track the index .",
    "the rest of the paper is organized as follows . in section 2 ,",
    "we state the regularity conditions and introduce the theoretical framework , then derive the accurate convergence rate of the adaptive elastic net s probability of variable selection , the bounds of bias , mse and the rate of convergence to the oracle distribution .",
    "section 3 proposes a new method called ssls and study the properties .",
    "computations are given in section 4 .",
    "section 5 and section 6 show simulation examples and applications , index tracking in financial field .",
    "we are interested in the sparse modeling problem where the true model has a sparse representation .",
    "that is , let @xmath43 with assumption of cardinality @xmath44 ( @xmath45 ) .",
    "the adaptive elastic net yields an estimator @xmath46 . without loss of generality , assume @xmath47 where @xmath48 for @xmath49 and @xmath50 for @xmath51 .",
    "then write @xmath52 and @xmath53 , @xmath54 and @xmath55 are the first @xmath56 and last @xmath57 columns of @xmath58 respectively .",
    "@xmath59 can be expressed in a block - wise : @xmath60 and @xmath61 .",
    "similarly , @xmath62 and @xmath63 indicate the first @xmath56 and last @xmath57 elements of @xmath64 .",
    "we want to use the ols estimator to be the initial estimator @xmath24 .",
    "however @xmath65 is always singular and the ols estimator of @xmath66 is no longer uniquely defined . in this case , we apply the lasso estimator @xmath67 where the lasso estimator is written as @xmath68 in this paper . ] to be the initial estimator .    according to the estimation consistency of @xmath68 ( related result",
    "is offered in the lemma 2 of appendix ) , we lead to variable selection consistency of the adaptive elastic net under follow constrains .",
    "let @xmath69 denotes the smallest eigenvalues of @xmath70 , we define the following regular conditions    1 .",
    "suppose @xmath71 for some @xmath72 and @xmath73 $ ] .",
    "furthermore , @xmath74 for @xmath75 .",
    "1 .   restricted eigenvalue ( re ) condition , i.e. there exists constant @xmath76 , such that @xmath77    ( c.1 ) gives the regularity conditions on the design matrix , which are typical assumptions in sparse linear regression literature , see for example @xcite .",
    "the first part of condition ( c.1 ) ensures a lower bound on the smallest eigenvalue of @xmath70 .",
    "the second part is needed for bernstein s inequality in theorem [ thm:1 ] .",
    "re condition , developed by @xcite , is a mild condition and has been studied in past work on lasso @xcite .",
    "we use @xmath68 to be the adaptive estimator of adaptive elastic net .",
    "this condition is applied to make sure the estimation consistency of lasso estimator .    as mentioned in the introduction part ,",
    "the choice of adaptive estimator @xmath24 is not unique .",
    "we know there must be other more optimal estimator than the lasso estimator . for instance",
    ", if @xmath78 , @xmath79 is a more appropriate choice .",
    "however , considering about the ultra - high dimensional setting and the existing choice in literature .",
    "we prefer the lasso estimator since the related results ( like the estimation consistency ) of lasso is mature enough .      in this section",
    ", we study the variable selection property of adaptive elastic net when the dimensionality can grow exponentially with the sample size .",
    "that is , @xmath80 as @xmath81 when @xmath82 .",
    "one defined sign consistency which stronger than the usual selection consistency , i.e. @xmath83@xcite .",
    "it can be satisfied if follow inequality holds .",
    "@xmath84    that is , by adding a simple restraint , @xmath85 , we can obtain the sign consistency when the adaptive elastic net achieves the variable selection consistency .",
    "we proof the probability of selecting wrong variables here mostly for simplicity of presentation .",
    "[ thm:1 ] assume @xmath86 are i.i.d .",
    "random variables with mean @xmath8 , and variance @xmath9 , let @xmath87 , where @xmath23 is a positive constant .",
    "if @xmath88 , where @xmath89 .",
    "then let @xmath23 bounded by @xmath90 where @xmath91 is a positive constant by setting in @xmath92 and @xmath93 , @xmath94 . under condition ( c.1 ) and",
    "( c.2 ) , we have @xmath95    if @xmath96 , we have the follow corollary    follow the same setting in the theorem 1 and consider the rest of @xmath34 that @xmath96 , then let @xmath23 bounded by @xmath97 we have @xmath98 .",
    "mention that @xmath23 and @xmath99 both are instruments help our proof but not a restraint for adaptive elastic net to select the true variables . for the choice of @xmath34 , we should mention that under the setting of the theorem [ thm:1 ] , @xmath34 is not decay to zero when @xmath1 tends to infinity . beyond that , there s no other special constraint on the parameters @xmath34 , @xmath35 , @xmath56 and @xmath0 .",
    "therefore , theorem  [ thm:1 ] shows that adaptive elastic net can select the true variables for most ultra - high dimensional data .    compared with other penalized least squares , @xcite proved that irrepresentable condition is almost necessary and sufficient for lasso to select the true variable both in the classical setting and high - dimensional setting . in this paper",
    ", we do nt need similar conditions .",
    "one of the other improvement of our technical is that , we do nt need control the size of @xmath34 and @xmath35 to obtain this property .",
    "similar , we also can obtain the variable selection consistency for adaptive lasso by using the similar technique in the proof for proving theorem [ thm:1 ] , which is also an improvement over literatures , e.g. @xcite proved the variable selection consistency with so many constrains like adaptive irrepresentable condition . we prefer adaptive elastic net to adaptive lasso since only @xmath11 penalization method may have poor performance where there are highly correlated variables in the predictor set .",
    "now we introduce the bounds of bias and mse of the adaptive elastic net :    [ thm:2 ] assume @xmath86 are i.i.d .",
    "random variables with mean @xmath8 and variance @xmath9 , under condition ( c.1 ) , the following bounds hold , @xmath100 and @xmath101 \\cdot ( kn^{1-a}+{\\lambda}_{2,n})^{-2 } \\cdot \\notag\\\\ & ( { \\lambda}_{2,n}^2 ||\\beta_n||^2_2 + { \\lambda}_{1,n}^2 e||\\hat w_{n}||^4_2+q\\cdot n ) \\cdot \\notag\\\\ & + 8 \\sqrt{p(\\hat s_n \\neq s_n ) } \\cdot ( ||\\beta_n||^2_2+n^2),\\end{aligned}\\ ] ]    for simplicity of presentation , let @xmath102 denotes the smallest eigenvalues of @xmath103 and suppose @xmath104 . then by choosing suitable parameter we have @xmath105 @xmath106    in the ultra - high dimensional setting , bias is not the only consideration of estimates .",
    "regularization has been a popular technique which results in a reduced mse .",
    "however , if two estimators have the same mse , we prefer the unbiased one . to the best of our knowledge , above",
    "bounds are the smallest one among literatures about penalized least squares .",
    "similar results can hardly obtain in other penalized least squares without the adaptive weights @xmath25 .",
    "hence theorem [ thm:2 ] makes adaptive elastic net very applicable .      in this part",
    ", we investigate the rate of convergence of adaptive elastic net estimator to the oracle distribution .",
    "let @xmath107 where @xmath108 is a @xmath109 matrix with @xmath110 .",
    "@xmath111 is an integer which can bigger than @xmath56 but not depending on @xmath1 .",
    "the main result of this part gives upper and lower bound on the accuracy of approximation by the limiting oracle distribution for the adaptive elastic net . to show this property of adaptive elastic net , we need more conditions :    1 .",
    "@xmath112 and @xmath113 , for some @xmath72 and @xmath114 , such that @xmath115 , where @xmath116 is set in ( c.1)(i ) .",
    "2 .   there exists @xmath117 and @xmath118 .",
    "+ \\(i ) @xmath119 .",
    ", + \\(ii ) @xmath120 ,    then @xmath121 and @xmath122    ( c.3 ) assumes that the nonzero coefficients are not masked by the estimation error , which makes it possible to separate out the signal from the noise by the adaptive elastic net .",
    "the first two bounds of ( c.4 ) require the maximum and the minimum eigenvalues of the @xmath123 matrix are bounded away from zero and infinity .",
    "other two inequalities are applied for the edgeworth expansion results for the adaptive elastic net estimator .",
    "then we have the following result :    [ thm:3 ] under conditions ( c.1 ) , ( c.3 ) and ( c.4 ) , choose suitable @xmath35 to make the smallest eign - values of @xmath124 greater than @xmath125 and assume that @xmath126 where @xmath127 is the @xmath128th element of @xmath129",
    ". then the rate of convergence to the oracle distribution can be given as follow @xmath130 where @xmath131 , @xmath132 is a @xmath133 vector with @xmath134th component @xmath135 , @xmath136 .",
    "theorem [ thm:3 ] indicates that the adaptive elastic net has a bias may lead to an inferior rate converging to the limiting normal distribution .",
    "the rate critically depends on the choices of the parameters .",
    "compared with the adaptive elastic net , this section proposes a valid inference procedure for both selection and estimation .",
    "we propose ssls ( separate selection from least squares ) to improve the accuracy of prediction and fitting result and show that : ( i ) ssls s biases decays at an exponential rate , which much faster than original penalized least squares .",
    "also , the mse of ssls can achieve at the oracle rate .",
    "( ii ) we already know that adaptive elastic net has a bias of rate of convergence to the oracle distribution . in this part , ssls estimator is proved have asymptotic normality .",
    "( iii ) furthermore , simulation and empirical part show that ssls have much smaller fitted and predicted error compared with other methods .",
    "similar setting as above , let @xmath137 where @xmath138 is the adaptive elastic net estimator",
    ". then we use ols to estimate the surplus low - dimension set as @xmath139    @xmath140 is obtained by the variable selection method ( adaptive elastic net in this paper ) .",
    "when the first part of ssls get variable selection consistency under conditions , ssls clearly achieve the variable selection consistency .",
    "we show the follow result for ssls using the adaptive elastic net as the first step .",
    "assume @xmath86 are i.i.d .",
    "random variables with mean @xmath8 and variance @xmath9 , under condition ( c.1 ) .... , the adaptive elastic net has variable selection consistency .",
    "that is @xmath141    follow the definition of ssls estimator @xmath142 , @xmath142 has the variable selection consistency too .    using the same notations as above , we show asymptotic normality of ssls as follow .",
    "[ thm:4 ] assume @xmath86 are i.i.d .",
    "random variables with mean @xmath8 , variance @xmath9 and @xmath143 < \\infty $ ] for some @xmath144 . under condition ( c.1 ) , the variable selection of adaptive elastic net holds .",
    "let @xmath145 and @xmath146 .",
    "then ssls are asymptotically normal , that is , @xmath147 where @xmath148 is a vector of norm @xmath10 .",
    "theorem [ thm:4 ] states that the variable selection consistency of adaptive elastic net implies the asymptotic normality of ssls estimator .",
    "finally , we provide the general bounds for bias and mse :    [ thm : final ] assume @xmath86 are i.i.d .",
    "random variables with mean @xmath8 and variance @xmath9 , under condition ( c.1 ) , then the bias and mse of ssls estimator satisfy @xmath149 @xmath150    theorem [ thm : final ] states that the bias of ssls estimator decays at an exponential rate .",
    "considering the mse of ssls estimator , @xmath32 decays at an exponential rate , hence it is completely determined by @xmath151 which corresponds to the oracle convergence rate and can not be improved any more .",
    "in this section we discuss the computational issues about ssls .",
    "we use adaptive elastic net to select the variables , hence the first half computation of ssls is solve the adaptive elastic net estimator by lars algorithm @xcite . the computation details are given as follow which omit the proof . + * algorithm 1 * ( the algorithm for the ssls )    1 .   given @xmath152 , @xmath58 and @xmath35 ,",
    "define the predictor matrix + @xmath153 \\in { \\mathds { r}}^{(n+p ) \\times p},\\ ] ] and @xmath154 2 .",
    "let @xmath155 3 .",
    "apply lars algorithm to choose the nonzero coefficient set @xmath140 by data @xmath156 and @xmath157 .",
    "4 .   assume the linear regression model @xmath158 where @xmath159 , and solve the ols estimator @xmath160 .",
    "after transform @xmath58 and @xmath152 into @xmath161 and @xmath157 , the lars algorithm is used to compute the solution path in step 3 .",
    "it is a popular and efficient algorithm hence we used in this paper .",
    "the final step is easy but important .",
    "the estimator @xmath162 obtained by ols estimation can get small error as much as possible , and the solution is also sparse since we get the sparse active set @xmath140 in the previous step .",
    "through simulations we investigate the performance of adaptive elastic net and ssls , starting with the comparison between the adaptive lasso and lasso with irrepresentable condition holds or not , and then considering the performance of ssls compared with others .",
    "we only give a simple high - dimensional setting example in the simulation part since after this part we also investigate the performance of the ssls which applying into the financial field compared with the traditional penalized least square method .",
    "the empirical analysis part can be seen as a more challenging scenarios .      to assess the performance of the adaptive elastic net estimator",
    ", we simulate data from the linear regression model @xmath163 where @xmath164 , @xmath165 and the true regression coefficients are set as follow @xmath166 where only the first two items are nonzero .",
    "we generate i.i.d .",
    "random variables @xmath167 , @xmath168 and @xmath169 from gaussian distribution with mean @xmath8 and variance @xmath9 for simplicity of presentation .",
    "@xmath170 is generated as @xmath171    according to the notations above , setting @xmath172 and @xmath173 .",
    "we get different solution paths from the lasso and adaptive elastic net ( as illustrated by figure [ fig:12 ] ) .",
    "one can easily obtain that this setting does nt satisfy the irrepresentable condition and hence lasso can not select variables correctly in figure [ fig:2 ] . as a contrast ,",
    "figure [ fig:1 ] shows the adaptive elastic net path correctly selects the true variables .      to assess the performance of the ssls estimator and compare it with other methods , we implement five methods under two different dimensional settings ( low dimensional setting vs high dimensional setting ) :    1 .",
    "lasso , the penalized least squares estimator with @xmath11 penalty proposed by @xcite .",
    "elastic net , the least squares estimator with both @xmath11 and @xmath12 penalty @xcite .",
    "adaptive lasso , penalized least squares method with an adaptive data - driven weights @xcite .",
    "adaptive elastic net , a combination of elastic net and the adaptive lasso @xcite .",
    "5 .   ssls , separate selection from least squares which defined in section 3 .",
    "we simulate data from linear regression model with fixed true regressions as @xmath174 no matter low dimension ( @xmath175 , @xmath165 ) or high dimension ( @xmath176 , @xmath165 ) .",
    "@xmath177 is generated from @xmath178 .",
    "correlation of the covariates matrices @xmath179 are chosen to be ( 1 ) identity ( @xmath180 ) and ( 2 ) generated with correlation @xmath181 , @xmath182 .",
    "we choose suitable tuning parameter for elastic net and adaptive elastic net to select variables for ssls .",
    "@xmath35 is selected in 20 different values and we find that relatively small values ( like 0.01 , 0.0001 and so on ) for @xmath35 lead to a better prediction result than larger one ( like 10 , 100 and so on ) .",
    "two measures are calculated : ( 1 ) @xmath11 loss : @xmath183 and ( 2 ) @xmath12 loss : @xmath184 . for each design",
    ", we run the simulation 100 times and present the average of the performance measure . for simplicity of presentation , we write aen for adaptive elastic net in the table . as depicted in table [ table:1 ] , one should compare the performance between each method .",
    "this comparison reflects the effectiveness of ssls deals with whether low dimensional or high dimensional setting .",
    "furthermore , comparing the behavior of each method in each design .",
    "it is seen that ssls has the best performance among others in all of four settings . beyond that , adaptive elastic net and adaptive lasso outperform lasso and elastic net in almost settings .",
    "furthermore , ssls has significantly lower @xmath11 and @xmath12 loss no matter smaller model size or bigger one .",
    "adaptive elastic net has good performance in the ideal setting like @xmath180 or low dimensional setting .",
    "but in the last model , both @xmath11 and @xmath12 loss have significantly increase .",
    "adaptive lasso has the similar behavior .",
    ".the @xmath11 loss and @xmath12 loss results based on 100 replications . [ cols=\"^,^,^,^,^\",options=\"header \" , ]     as described in table [ table:2 ] and table [ table:3 ] , using ssls and selecting 50 stocks to track sp500 , both fitted and predicted annual tracking errors are nearly between @xmath185 and @xmath37 .",
    "all these results show that ssls is very successful in assets selection .",
    "we thank peter hall for his helpful comments and suggestions on this paper . this work was supported in part by the national natural science foundation of china",
    "( grant no .",
    "first of all , we give follow results to illustrate the property of adaptive elastic net solution without detail proof .    for any @xmath152 , @xmath58 in , the adpative elastic net solution has at most @xmath186 nonzero components as follow @xmath187 and @xmath188 where @xmath140 is defined by @xmath189 and @xmath190 is the corresponding signs .    since the adaptive elastic net penalty function is strictly convex .",
    "the solution is always unique , regardless of @xmath58 .",
    "similar result for lasso can be seen in @xcite , the adaptive elastic net solution is given by a simple transformation hence omit proof here .",
    "consider the linear regression model with @xmath191 is a vector of i.i.d .",
    "random variables with mean @xmath8 and variance @xmath9 .",
    "@xmath58 satisfies ( c.1 ) and ( c.2 ) .",
    "given the lasso program with regularization parameter @xmath192 , then there exists constants @xmath193 such that , with probability at least @xmath194 , any solution @xmath195 satisfies the bounds      @xcite gave a similar property for lasso when the noise vector @xmath191 has i.i.d .",
    "@xmath197 entries . through their results , @xmath11-norm is decomposable when @xmath58 satisfies ( c.1 ) and ( c.2 ) conditions .",
    "also , the choice of @xmath198 is given in a similar way .",
    "the only difference is to compute the tail bound in the final step .",
    "this bound is also used in the proof of theorem 1 .            define @xmath205 and @xmath206 .",
    "let @xmath207 , @xmath208 and @xmath62 , @xmath63 as the first @xmath56 and last @xmath57 elements of @xmath138 and @xmath64 respectively .",
    "similar , @xmath209 and @xmath210 denote the first @xmath56 and last @xmath57 elements of @xmath211 .",
    "similar as in the proof of lemma 2 , we have @xmath212    = \\exp [ n^c - n^{\\frac{2}{3}\\eta } ]   = o(e^{-n^{c}}).\\end{aligned}\\ ] ]          since @xmath220 , the minimum of @xmath221 can not be attained at @xmath218 .",
    "then , assume @xmath222 , following inequalities hold uniformly : @xmath223 \\notag\\\\ & \\geqslant k \\sum\\limits_{j = q+1}^{p}|\\hat u_{j ,",
    "n}| \\left [    { \\lambda}_{1,n } \\cdot n^{-\\frac{1}{2 } } n ^{\\frac{\\gamma}{2}-\\eta \\gamma}- n^{\\eta } - q^{1/2}\\cdot m_n   \\right ] \\notag\\\\ & > 0.\\end{aligned}\\ ] ]              for every given @xmath242 , under condition ( c.1 ) , the first item of the right hand of can be calculated as follow @xmath243 where @xmath244 $ ] . by cauchy - schwarz inequality",
    ", the second item can be written as @xmath245            therefore , if @xmath1 is large enough , can be written as @xmath101 \\cdot ( kn^{1-a}+{\\lambda}_{2,n})^{-2 } \\cdot \\notag\\\\ & ( { \\lambda}_{2,n}^2 ||\\beta_n||^2_2 + { \\lambda}_{1,n}^2 e||\\hat w_{n}||^4_2+q \\cdot n ) \\cdot \\notag\\\\ & + 8 \\sqrt{p(\\hat s_n \\neq s_n ) } \\cdot ( ||\\beta_n||^2_2+n^2),\\end{aligned}\\ ] ] which completes the proof .",
    "we have @xmath263 where @xmath262 is the taylor s expansion of @xmath264 and @xmath265 is the remainder term obtained by taylor s expansion",
    ". therefore @xmath266 have bounds @xmath267 and hence the first item of is bounded by @xmath268 .",
    "the second inequality of holds after calculations and bounds by ( c.1 ) , ( c.3 ) and ( c.4 ) .",
    "more details can be seen in bhattacharya and ranga rao(1986 ) .",
    "j.  j. fuchs .",
    "recovery of exact sparse representations in the presence of noise . in _",
    "acoustics , speech , and signal processing , 2004 .",
    "proceedings.(icassp04 ) .",
    "ieee international conference on _ , volume  2 , pages ii533 .",
    "ieee , 2004 ."
  ],
  "abstract_text": [
    "<S> this paper studies the asymptotic properties of the adaptive elastic net in ultra - high dimensional sparse linear regression models and proposes a new method called ssls ( separate selection from least squares ) to improve prediction accuracy . besides </S>",
    "<S> , we prove that ssls has the superior performance both in the theoretical part and empirical part .    in this paper , we prove that the probability of adaptive elastic net selecting wrong variables can decays at an exponential rate with very few conditions . </S>",
    "<S> irrepresentable condition or similar constraint is nt necessary in our proof . </S>",
    "<S> we derive accurate bounds of bias and mean squared error ( mse ) which both depend on the choice of parameters , and also show that there exists a bias of asymptotic normality of the adaptive elastic net . </S>",
    "<S> furthermore , simulations and empirical part both show that the prediction accuracy of the penalized least squares requires more improvement .    therefore , we propose ssls to improve the prediction . </S>",
    "<S> it selects variable first , reducing high dimension to low dimension by using the adaptive elastic net in this paper . in the second step , </S>",
    "<S> the coefficients are constructed based on the ols estimation . </S>",
    "<S> we show that the bias of ssls can decays at an exponential rate . </S>",
    "<S> also , mse decays to zero . finally , we prove that the variable selection consistency of ssls implies the asymptotic normality of ssls . </S>",
    "<S> simulations given in this paper illustrate the performance of the ssls , adaptive elastic net and other penalized least squares . </S>",
    "<S> the index tracking problem in stock market is studied in the empirical part with other methods .    </S>",
    "<S> * keywords : * adaptive elastic net ; ssls ; variable selection ; oracle property . </S>"
  ]
}