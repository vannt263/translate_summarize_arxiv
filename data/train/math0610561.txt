{
  "article_text": [
    "clustering constitutes a fundamental problem in various areas of applications such as pattern recognition , image processing , data mining , data compression , or machine learning @xcite .",
    "the goal of clustering is grouping given training data into classes of similar objects such that data points with similar semantical meaning are linked together .",
    "clustering methods differ in various aspects including the assignment of data points to classes which might be crisp or fuzzy , the arrangement of clusters which might be flat or hierarchical , or the representation of clusters which might be represented by the collection of data points assigned to a given class or by few prototypical vectors . in this article , we are interested in neural clustering algorithms which deal with crisp assignments and representation of clusters by neurons or prototypes .",
    "popular neural algorithms representing data by a small number of typical prototypes include k - means , the self - organizing map ( som ) , neural gas ( ng ) , and alternatives @xcite .",
    "depending on the task and model at hand , these methods can be used for data compression , data mining and visualization , nonlinear projection and interpolation , or preprocessing for supervised learning .",
    "k - means clustering directly aims at a minimization of the quantization error @xcite .",
    "however , its update scheme is local , therefore it easily gets stuck in local optima .",
    "neighborhood cooperation as for som and ng offers one biologically plausible solution .",
    "apart from a reduction of the influence of initialization , additional semantical insight is gained : browsing within the map and , if a prior low dimensional lattice is chosen , data visualization become possible . however , a fixed prior lattice as chosen in som might be suboptimal for a given task depending on the data topology and topological mismatches can easily occur @xcite .",
    "som does not possess a cost function in the continuous case , and the mathematical analysis is quite difficult unless variations of the original learning rule are considered for which cost functions can be found @xcite .",
    "ng optimizes a cost function which , as a limit case , yields the quantization error @xcite .",
    "thereby , a data optimum ( irregular ) lattice can be determined automatically during training which perfectly mirrors the data topology and which allows to browse within the result @xcite .",
    "this yields very robust clustering behavior . due to the potentially irregular lattice",
    ", visualization requires additional projection methods .",
    "these neural algorithms ( or a variation thereof for som ) optimize some form of cost function connected to the quantization error of the data set",
    ". there exist mainly two different optimization schemes for these objectives : online variants , which adapt the prototypes after each pattern , and batch variants which adapt the prototypes according to all patterns at once .",
    "batch approaches are usually much faster in particular for high dimensional vectors , since only one adaptation is necessary in each cycle and convergence can usually be observed after few steps",
    ". however , the problem of local optima for k - means remains in the batch variant . for som , topological",
    "ordering might be very difficult to achieve since , at the beginning , ordering does usually not exist and , once settled in a topological mismatch , the topology can hardly be corrected .",
    "the problem of topological mismatches is much more pronounced in batch som than in online som as shown in @xcite such that a good ( and possibly costly ) initialization is essential for the success .",
    "however , due to their efficiency , batch variants are often chosen for som or k - means if data are available a priori , whereby the existence of local optima and topological mismatches might cause severe problems . for ng ,",
    "some variants of batch adaptation schemes occur at singular points in the literature @xcite , however , so far , no ng - batch scheme has been explicitely derived from the ng cost function together with a proof of the convergence of the algorithm . in this article",
    ", we put the cost functions of ng , ( modified ) som , and k - means into a uniform notation and derive batch versions thereof together with a proof for convergence .",
    "in addition , we relate batch ng to an optimization by means of the newton method , and we compare the methods on different representative clustering problems .    in a variety of tasks such as classification of protein structures , text documents , surveys , or biological signals ,",
    "an explicit metric vector space such as the standard euclidian vector space is not available , rather discrete transformations of data e.g.  the edit distance or pairwise proximities are available @xcite . in such cases , a clustering method which does not rely on",
    "a vector space has to be applied such as spectral clustering @xcite .",
    "several alternatives to som have been proposed which can deal with more general , mostly discrete data @xcite .",
    "the article @xcite proposes a particularly simple and intuitive possibility for clustering proximity data : the mean value of the batch som is substituted by the generalized median resulting in median som , a prototype - based neural network in which prototypes location are adapted within the data space by batch computations .",
    "naturally , the same idea can be transferred to batch ng and k - means as we will demonstrate in this contribution . as for the euclidian versions",
    ", it can be shown that the median variants of som , ng , and k - means converge after a finite number of adaptation steps .",
    "thus , the formulation of neural clustering schemes by means of batch adaptation opens the way towards the important field of clustering complex data structures for which pairwise proximities or a kernel matrix constitute the interface to the neural clustering method .",
    "assume data points @xmath0 are distributed according to an underlying distribution @xmath1 , the goal of ng as introduced in @xcite is to find prototype locations @xmath2 , @xmath3 , such that these prototypes represent the distribution @xmath1 as accurately as possible , minimizing the cost function @xmath4 where @xmath5 denotes the squared euclidian distance , @xmath6 is the rank of the prototypes sorted according to the distances , @xmath7 is a gaussian shaped curve with neighborhood range @xmath8 , and @xmath9 is the constant @xmath10 .",
    "the learning rule consists of a stochastic gradient descent , yielding @xmath11 for all prototypes @xmath12 given a data point @xmath13 .",
    "thereby , the neighborhood range @xmath14 is decreased during training to ensure independence of initialization at the beginning of training and optimization of the quantization error in the final stages . as pointed out in @xcite , the result",
    "can be associated with a data optimum lattice such that browsing within the data space constitutes an additional feature of the solution .    due to its simple adaptation rule ,",
    "the independence of a prior lattice , and the independence of initialization because of the integrated neighborhood cooperation , ng is a simple and highly effective algorithm for data clustering .",
    "popular alternative clustering algorithms are offered by the som as introduced by kohonen @xcite and k - means clustering @xcite .",
    "som uses the adaptation strength @xmath15 instead of @xmath16 , @xmath17 denoting the index of the closest prototype , the winner , for @xmath13 , and @xmath18 a priorly chosen , often two - dimensional neighborhood structure of the neurons .",
    "a low - dimensional lattice offers the possibility to easily visualize data .",
    "however , if the primary goal is clustering , a fixed topology puts restrictions on the map and topology preservation often can not be achieved @xcite .",
    "som does not possess a cost function in the continuous case and its mathematical investigation is difficult @xcite .",
    "however , if the winner is chosen as the neuron @xmath19 with minimum averaged distance @xmath20 it optimizes the cost @xmath21 as pointed out by heskes @xcite . here , @xmath22 denotes the winner index according to the averaged distance and @xmath23 is the characteristic function of @xmath24 .",
    "k - means clustering adapts only the winner in each step , thus it optimizes the standard quantization error @xmath25 where @xmath26 denotes the winner index for @xmath27 in the classical sense . unlike som and ng ,",
    "k - means is very sensitive to initialization of the prototypes since it adapts the prototypes only locally according to their nearest data points .",
    "an initialization of the prototypes within the data points is therefore mandatory .",
    "if training data @xmath28 ,  , @xmath29 are given priorly , fast alternative batch training schemes exist for both , k - means and som . starting from random positions of the prototypes , batch learning iteratively performs the following two steps until convergence    1 .",
    "determine the winner @xmath30 resp .",
    "@xmath31 for each data point @xmath32 , 2 .",
    "determine new prototypes as @xmath33 for k - means and @xmath34 for som .",
    "thereby , the neighborhood cooperation is annealed for som in the same way as in the online case .",
    "it has been shown in @xcite that batch k - means and batch som optimize the same cost functions as their online variants , whereby the modified winner notation as proposed by heskes is used for som .",
    "in addition , as pointed out in @xcite , this formulation allows to link the models to statistical formulations and it can be interpreted as a limit case of em optimization schemes for appropriate mixture models .    often , batch training converges after only few ( 10 - 100 ) cycles such that this training mode offers considerable speedup in comparison to the online variants : adaptation of the ( possibly high dimensional ) prototypes is only necessary after the presentation of all training patterns instead of each single one .    here",
    ", we introduce batch ng . as for som and k - means",
    ", it can be derived from the cost function of ng , which , for discrete data @xmath28 ,  ,",
    "@xmath29 , reads as @xmath35 @xmath36 being the standard euclidian metric . for the batch algorithm ,",
    "the quantities @xmath37 are treated as hidden variables with the constraint that the values @xmath38 ( @xmath3 ) constitute a permutation of @xmath39 for each point @xmath13 .",
    "@xmath40 is interpreted as a function depending on @xmath41 and @xmath38 which is optimized in turn with respect to the hidden variables @xmath38 and with respect to the prototypes @xmath12 , yielding the two adaptation steps of batch ng which are iterated until convergence :    1 .",
    "determine @xmath42 as the rank of prototype @xmath12 , 2 .",
    "based on the hidden variables @xmath38 , set @xmath43    as for batch som and k - means , adaptation takes place only after the presentation of all patterns with a step size which is optimized by means of the partial cost function .",
    "only few adaptation steps are usually necessary due to the fact that batch ng can be interpreted as newton optimization method which takes second order information into account whereas online ng is given by a simple stochastic gradient descent .    to show this claim , we formulate the batch ng update in the form @xmath44 newton s method for an optimization of @xmath40 yields the formula @xmath45 where @xmath46 denotes the jacobian of @xmath40 and @xmath47 the hessian matrix .",
    "since @xmath38 is locally constant , we get up to sets of measure zero @xmath48 and the hessian matrix equals a diagonal matrix with entries @xmath49 the inverse gives the scaling factor of the batch ng adaptation , i.e.  batch ng equals newton s method for the optimization of @xmath40 .      before turning to the problem of clustering proximity data",
    ", we formulate batch ng , som , and k - means within a common cost function . in the discrete",
    "setting , these three models optimize a cost function of the form @xmath50 where @xmath51 is the characteristic function of the winner , i.e.@xmath52 resp .",
    "@xmath53 , for k - means and som , and it is @xmath16 for neural gas . @xmath54 equals the distance @xmath55 for k - means and ng , and it is the averaged distance @xmath56 for som . the batch algorithms optimize @xmath57 with respect to @xmath38 in step * ( 1 ) * assuming fixed @xmath41 .",
    "thereby , for each @xmath24 , the vector @xmath38 ( @xmath3 ) is restricted to a vector with exactly one entry @xmath58 and @xmath59 , otherwise , for k - means and som .",
    "it is restricted to a permutation of @xmath39 for ng .",
    "thus , the elements @xmath38 come from a discrete set which we denote by @xmath60 . in step * ( 2 ) * , @xmath57 is optimized with respect to @xmath61 assuming fixed @xmath38 .",
    "the update formulas as introduced above can be derived by taking the derivative of @xmath62 with respect to @xmath41 .    for proximity data @xmath28 ,  ,",
    "@xmath29 , only the distance matrix @xmath63 is available but data are not embedded in a vector space and no continuous adaptation is possible , nor does the derivative of the distance function @xmath36 exist .",
    "a solution to tackle this setting with som - like learning algorithms proposed by kohonen is offered by the median som : it is based on the notion of the generalized median @xcite .",
    "prototypes are chosen from the _ discrete _ set given by the training points @xmath64 in an optimum way . in mathematical terms",
    ", @xmath57 is optimized within the set @xmath65 given by the training data instead of @xmath66 .",
    "this leads to the choice of @xmath12 as @xmath67 in step * ( 2)*. in @xcite , kohonen considers only the data points mapped to a neighborhood of neuron @xmath19 as potential candidates for @xmath12 and , in addition , reduces the above sum to points mapped into a neighborhood of @xmath19 . for small neighborhood range and",
    "approximately ordered maps , this does not change the result but considerably speeds up the computation .    the same principle can be applied to k - means and batch ng . in step * ( 2 ) * , instead of taking the vectors in @xmath66 which minimize @xmath57 , prototype @xmath19 is chosen as the data point in @xmath68 with @xmath69 assuming fixed @xmath70 for median k - means and @xmath71 assuming fixed @xmath72 for median ng . for roughly ordered maps ,",
    "a restriction of potential candidates @xmath73 to data points mapped to a neighborhood of @xmath19 can speed up training as for median som .",
    "obviously , a direct implementation of the new prototype locations requires time @xmath74 , @xmath75 being the number of patterns and @xmath76 being the number of neurons , since for every prototype and every possible prototype location in @xmath68 a sum of @xmath75 terms needs to be evaluated .",
    "hence , an implementation of median ng requires the complexity @xmath77 for each cycle , including the computation of @xmath38 for every @xmath19 and @xmath24 . for median som , a possibility to speed up training has recently been presented in @xcite which yields an exact computation with costs only @xmath78 instead of @xmath74 for the sum .",
    "unfortunately , the same technique does not improve the complexity of ng .",
    "however , further heuristic possibilities to speed - up median - training are discussed in @xcite which can be transferred to median ng .",
    "in particular , the fact that data and prototype assignments are in large parts identical for consecutive runs at late stages of training and a restriction to candidate median points in the neighborhood of the previous one allows a reuse of already computed values and a considerable speedup .",
    "all batch algorithms optimize @xmath79 by consecutive optimization of the hidden variables @xmath80 and @xmath41 .",
    "we can assume that , for given @xmath41 , the values @xmath38 determined by the above algorithms are unique , introducing some order in case of ties .",
    "note that the values @xmath38 come from a discrete set @xmath60 . if the values @xmath38 are fixed , the choice of the optimum @xmath41 is unique in the algorithms for the continuous case , as is obvious from the formulas given above , and we can assume uniqueness for the median variants by introducing an order .",
    "consider the function @xmath81 note that @xmath82 .",
    "assume prototypes @xmath41 are given , and new prototypes @xmath83 are computed based on @xmath80 using one of the above batch or median algorithms .",
    "it holds @xmath84 because @xmath85 are optimum assignments for @xmath38 in @xmath57 , given @xmath83 .",
    "in addition , @xmath86 because @xmath83 are optimum assignments of the prototypes given @xmath80 .",
    "thus , @xmath87 , i.e. , in each step of the algorithms , @xmath57 is decreased .",
    "since there exists only a finite number of different values @xmath38 and the assignments are unique , the algorithms converge in a finite number of steps toward a fixed point @xmath88 for which @xmath89 holds .    consider the case of continuous @xmath41 . since @xmath38 are discrete , @xmath80 is constant in a vicinity of a fixed point @xmath88 if no data points lie at the borders of two receptive fields .",
    "then @xmath90 and @xmath91 are identical in a neighborhood of @xmath88 and thus , a local optimum of @xmath92 is also a local optimum of @xmath57 .",
    "therefore , if @xmath41 can be varied in a real vector space , a local optimum of @xmath57 is found by the batch variant if no data points are directly located at the borders of receptive fields for the final solution .",
    "we demonstrate the behavior of the algorithms in different scenarios which cover a variety of characteristic situations .",
    "all algorithms have been implemented based on the som toolbox for matlab @xcite .",
    "we used k - means , som , batch som , and ng with default parameters as provided in the toolbox .",
    "batch ng and median versions of ng , som , and k - means have been implemented according to the above formulas .",
    "note that , for all batch versions , prototypes which lie at identical points of the data space do not separate in consecutive runs .",
    "thus , the situation of exactly identical prototypes must be avoided . for the euclidian versions ,",
    "this situation is a set of measure zero if prototypes are initialized at different positions . for median versions , however",
    ", it can easily happen that prototypes become identical due to a limited number of different positions in the data space , in particular for small data sets . due to this fact , we add a small amount of noise to the distances in each epoch in order to separate identical prototypes .",
    "vectorial training sets are normalized prior to training using z - transformation .",
    "initialization of prototypes takes place using small random values .",
    "the initial neighborhood rate for neural gas is @xmath93 , @xmath76 being the number of neurons , and it is multiplicatively decreased during training . for median som , we restrict to square lattices of @xmath94 neurons and a rectangular neighborhood structure , whereby @xmath95 is rounded to the next integer . here",
    "the initial neighborhood rate is @xmath96 .",
    "= 10.6 cm + = 10.6 cm    = 10.6 cm + = 10.6 cm      the first data set is the two - dimensional synthetic data set from @xcite consisting of @xmath97 data points and @xmath98 training points .",
    "clustering has been done using @xmath99 ,  , @xmath100 prototypes , resp .  the closest number of prototypes implemented by a rectangular lattice for som .",
    "training takes place for @xmath101 epochs .",
    "the mean quantization error @xmath102 on the test set and the location of prototypes within the training set are depicted in figs .",
    "[ synth ] and [ seg ] .",
    "obviously , the location of prototypes coincides for different versions of ng .",
    "this observation also holds for different numbers of prototypes , whereby the result is subject to random fluctuations for larger numbers . for k - means ,",
    "idle prototypes can be observed for large @xmath76 . for batch som and standard som , the quantization error is worse ( ranging from @xmath103 for @xmath104 neurons up to @xmath105 for @xmath106 neurons , not depicted in the diagram ) , which can be attributed to the fact that the map does not fully unfold upon the data set and edge effects remain , which could be addressed to a small but nonvanishing neighborhood in the convergent phase in standard implementations of som which is necessary to preserve topological order .",
    "median som ( which has been directly implemented in analogy to median ng ) yields a quantization error competitive to ng .",
    "thus , batch and median ng allow to achieve results competitive to ng in this case , however , using less effort .",
    "the segmentation data set from the uci repository consists of @xmath107 ( training set ) resp .",
    "@xmath108 ( test set ) @xmath109 dimensional data points which are obtained as pixels from outdoor images preprocessed by standard filters such as averaging , saturation , intensity , etc .",
    "the problem is interesting since it contains high dimensional and only sparsely covered data .",
    "the quantization error obtained for the test set is depicted in fig .",
    "[ synth ] . as",
    "beforehand , som suffers from the restriction of the topology .",
    "neural gas yields very robust behavior , whereas for k - means , idle prototypes can be observed .",
    "the median versions yield a larger quantization error compared to the vector - based algorithms .",
    "the reason lies in the fact that a high dimensional data set with only few training patterns is considered , such that the search space for median algorithms is small in these cases and random effects and restrictions account for the increased error .",
    "l|llllllll & ng & batch & median & som & batch & median & kmeans & median + & & ng & ng & & som & som & & kmeans +   + train&0.0043 & 0.0028 & 0.0043 & 0.0127 & 0.0126 & 0.0040 & 0.0043 & 0.0046 + test&0.0051 & * 0.0033 * & 0.0048 & 0.0125 & 0.0124 & 0.0043 & 0.0050 & 0.0052 +   + train&0.1032 & 0.0330 & 0.0338 & 0.2744 & 0.2770 & 0.0088&0.1136 & 0.0464 + test&0.1207 & 0.0426 & 0.0473 & 0.2944 & 0.2926 & * 0.0111 * & 0.1376 & 0.0606      this data set is taken from @xcite .",
    "two - dimensional data are arranged on a checkerboard , resulting in @xmath110 times @xmath110 clusters , each consisting of @xmath111 to @xmath112 points . for each algorithm , we train @xmath113 times @xmath114 epochs for @xmath114 prototypes . obviously , the problem is highly multimodal and usually the algorithms do not find all clusters . the number of missed clusters can easily be judged in the following way : the clusters are labeled consecutively using labels @xmath58 and @xmath104 according to the color black resp .",
    "white of the data on the corresponding field of the checkerboard .",
    "we can assign labels to prototypes a posteriori based on a majority vote on the training set .",
    "the number of errors which arise from this classification on an independent test set count the number of missed clusters , since @xmath115 error roughly corresponds to one missed cluster .",
    "the results are collected in tab .",
    "[ check ] .",
    "the smallest quantization error is obtained by batch ng , the smallest classification error can be found for median som . as beforehand , the implementations for som and batch som do not fully unfold the map among the data . in the same way",
    "online ng does not achieve a small error because of a restricted number of epochs and a large data set which prevents online ng from full unfolding .",
    "k - means also shows a quite high error ( it misses more than @xmath110 clusters ) which can be explained by the existence of multiple local optima in this setting , i.e.  the sensitivity of k - means with respect to initialization of prototypes . in contrast , batch ng and median ng find all but @xmath116 to @xmath117 clusters .",
    "median som even finds all but only @xmath58 or @xmath104 clusters since the topology of the checkerboard exactly matches the underlying data topology consisting of @xmath118 clusters .",
    "surprisingly , also median k - means shows quite good behavior , unlike k - means itself , which might be due to the fact that the generalized medians enforce the prototypes to settle within the clusters .",
    "thus , median versions and neighborhood cooperation seem beneficial in this task due to the multiple modes .",
    "batch versions show much better behavior than their online correspondents , due to a faster convergence of the algorithms . here",
    ", som suffers from border effects , whereas median som settles within the data clusters , whereby the topology mirrors precisely the data topology .",
    "both , batch ng and median ng , yield quite good classification results which are even competitive to supervised prototype - based classification results as reported in @xcite .",
    "= 9.0 cm      we used the protein data set described in @xcite and @xcite : the dissimilarity of @xmath119 globin proteins of different families is given in matrix form as depicted in fig .",
    "[ matrix ] .",
    "thereby , the matrix is determined based on sequence alignment using biochemical and structural information .",
    "in addition , prior information about the underlying protein families is available , i.e.  a prior clustering into semantically meaningful classes of the proteins is known : as depicted in fig  [ matrixcluster ] by vertical lines , the first 42 proteins belong to hemoglobin @xmath120 , the next clusters denote hemoglobin @xmath121 , @xmath122 , etc . thereby , several clusters are rather small , comprising only few proteins ( one or two ) . in addition , the cluster depicted on the right has a very large intercluster distance .",
    "since only a proximity matrix is available , we can not apply standard ng , k - means , or som , but we can rely on the median versions .",
    "we train all three median versions @xmath110 times using @xmath110 prototypes and @xmath123 epochs .",
    "the mean quantization errors ( and variances ) are @xmath124 ( @xmath125 ) for median ng @xmath126 ( @xmath127 ) for median som , and @xmath128 ( @xmath129 ) for median k - means , thus k - means yields worse results compared to ng and som and neighborhood integration clearly seems beneficial in this application scenario .",
    "we can check whether the decomposition into clusters by means of the prototypes is meaningful by comparing the receptive fields of the ten prototypes to the prior semantic clustering .",
    "typical results are depicted in fig .",
    "[ matrixcluster ] .",
    "the classification provided by experts is indicated by vertical lines in the images .",
    "the classification by the respective median method is indicated by assigning a value on the y - achses to each pattern corresponding to the number of its winner neuron ( black squares in the figure ) .",
    "thus , an assignment of all or nearly all patterns in one semantic cluster to one or few dedicated prototypes gives a hint for the fact that median clustering finds semantically meaningful entities .",
    "all methods detect the first cluster ( hemoglobin @xmath120 ) and neural gas and som also detect the eighth cluster ( myoglobin ) .",
    "in addition , som and ng group together elements of clusters two to seven in a reasonable way .",
    "thereby , according to the variance in the clusters , more than one prototype is used for large clusters and small clusters containing only one or two patterns are grouped together .",
    "the elements of the last two clusters have a large intercluster distance such that they are grouped together into some ( random ) cluster for all methods .",
    "note that the goal of ng and som is a minimization of their underlying cost function , such that the cluster border can lie between semantic clusters for these methods .",
    "thus , the results obtained by som and ng are reasonable and they detect several semantically meaningful clusters .",
    "the formation of relevant clusters is also supported when training with a different number of prototypes    = 9.0 cm + = 9.0 cm + = 9.0 cm      the data set as given in @xcite consists of silhouettes of @xmath130 chicken pieces of different classes including wings , backs , drumsticks , thighs , and breasts .",
    "the task is a classification of the images ( whereby the silhouettes are not oriented ) into the correct class .",
    "as described in @xcite , a preprocessing of the images resulting in a proximity matrix can cope with the relevant properties of the silhouette and rotation symmetry : the surrounding edges are detected and discretized into small consecutive line segments of @xmath112 pixels per segment .",
    "the images are then represented by the differences of the angles of consecutive line segments .",
    "distance computation takes place as described in @xcite by a rotation and mirror symmetric variant of the edit distance of two sequences of angles , whereby the costs for a substitution of two angles is given by their absolute distance , the costs for deletion and insertion are given by @xmath131 .",
    "we train median k - means , median ng , and median som with different numbers of neurons for @xmath123 epochs , thereby annealing the neighborhood as beforehand .",
    "the results on a training and test set of the same size , averaged over ten runs , are depicted in tab .",
    "[ chicken_pieces ] . obviously , a posterior labeling of prototypes obtained by median clustering allows to achieve a classification accuracy of more than @xmath132 .",
    "thereby , overfitting can be observed for all methods due to the large number of prototypes compared to the training set ( @xmath133 neurons constitute about @xmath134th of the training set ! )",
    ". however , median ng and median som are less prone to this effect due to their inherent regularization given by the neighborhood integration .",
    ".results for the median variants for different numbers of neurons on the chicken - piece - silhouettes data base .",
    "the best test classifications are depicted in bold .",
    "[ cols= \" < , < , < , < , < , < , < \" , ]",
    "we have proposed batch ng derived from the ng cost function which allows fast training for a priorly given data set .",
    "we have shown that the method converges and it optimizes the same cost function as ng by means of a newton method .",
    "in addition , the batch formulation opens the way towards general proximity data by means of the generalized median .",
    "these theoretical discussions were supported by experiments for different vectorial data where the results of batch ng and ng are very similar . in all settings ,",
    "the quality of batch ng was at least competitive to standard ng , whereby training takes place in a fraction of the time especially for high - dimensional input data due to the radically reduced number of updates of a prototype .",
    "unlike k - means , ng is not sensitive to initialization and , unlike som , it automatically determines a data optimum lattice , such that a small quantization error can be achieved and topological initialization is not crucial .",
    "median ng restricts the adaptation to locations within the data set such that it can be applied to non - vectorial data .",
    "we compared median ng to its alternatives for vectorial data observing that competitive results arise if enough data are available .",
    "we added several experiments including proximity data where we could obtain semantically meaningful grouping as demonstrated by a comparison to known clusters resp .",
    "a validation of the classification error when used in conjunction with posterior labeling .",
    "unlike som , ng solely aims at data clustering and not data visualization , such that it can use a data optimum lattice and it is not restricted by topological constraints .",
    "therefore better results can often be obtained in terms of the quantization error or classification . if a visualization of the output of ng is desired , a subsequent visualization of the prototype vectors is possible using fast standard methods for the reduced set of prototypes such as multidimensional scaling @xcite .",
    "thus , very promising results could be achieved which have been accompanied by mathematical guarantees for the convergence of the algorithms .",
    "nevertheless , several issues remain : for sparsely covered data sets , median versions might not have enough flexibility to position the prototypes since only few locations in the data space are available .",
    "we have already demonstrated this effect by a comparison of batch clustering to standard euclidian clustering in such a situation .",
    "it might be worth investigating metric - specific possibilities to extend the adaptation space for the prototypes in such situations , as possible e.g.  for the edit distance , as demonstrated in @xcite and @xcite .    a problem of median ng is given by the complexity of one cycle , which is quadratic in the number of patterns .",
    "since optimization of the exact computation as proposed in @xcite is not possible , heuristic variants which restrict the computation to regions close to the winner seem particularly promising because they have a minor effect on the outcome .",
    "a thorough investigation of the effects of such restriction will be investigated both theoretically and experimentally in future work .",
    "often , an appropriate metric or proximity matrix is not fully known a priori .",
    "the technique of learning metrics , which has been developed for both , supervised as well as unsupervised prototype - based methods @xcite allows a principled integration of secondary knowledge into the framework and adapts the metric accordingly , thus getting around the often problematic garbage - in - garbage - out  problem of metric - based approaches .",
    "it would be interesting to investigate the possibility to enhance median versions for proximity data by an automatic adaptation of the distance matrix during training driven by secondary information . a recent possibility to combine vector quantizers with prior ( potentially fuzzy )",
    "label information has been proposed in @xcite by means of a straightforward extension of the underlying cost function of ng .",
    "this approach can immediately be transferred to a median computation scheme since a well - defined cost function is available , thus opening the way towards supervised prototype - based median fuzzy classification for non - vectorial data .",
    "a visualization driven by secondary label information can be developed within the same framework substituting the irregular ng lattice by a som neighborhood and incorporating heskes cost function .",
    "an experimental evaluation of this framework is the subject of ongoing work ."
  ],
  "abstract_text": [
    "<S> neural gas ( ng ) constitutes a very robust clustering algorithm given euclidian data which does not suffer from the problem of local minima like simple vector quantization , or topological restrictions like the self - organizing map . </S>",
    "<S> based on the cost function of ng , we introduce a batch variant of ng which shows much faster convergence and which can be interpreted as an optimization of the cost function by the newton method . </S>",
    "<S> this formulation has the additional benefit that , based on the notion of the generalized median in analogy to median som , a variant for non - vectorial proximity data can be introduced . </S>",
    "<S> we prove convergence of batch and median versions of ng , som , and k - means in a unified formulation , and we investigate the behavior of the algorithms in several experiments .        neural gas , batch algorithm , proximity data , median - clustering , convergence </S>"
  ]
}