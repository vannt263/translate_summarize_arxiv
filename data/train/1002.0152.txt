{
  "article_text": [
    "in many concrete situations the statistician observes a finite path @xmath0 of a real temporal phenomena which can be modeled as realizations of a stationary process @xmath1 ( we refer , for example , to @xcite , @xcite and references therein ) .",
    "here we consider a second order weakly stationary process , which implies that its mean is constant and that @xmath2 only depends on the distance between @xmath3 and @xmath4 . in the sequel",
    ", we will assume that the process is gaussian , which implies that it is also strongly stationary , in the sense that , for any @xmath5 , @xmath6    our aim is to predict this series when only a finite number of past values are observed .",
    "moreover , we want a sharp control of the prediction error . for this , recall that , for gaussian processes , the best predictor of @xmath7 , when observing @xmath8 , is obtained by a suitable linear combination of the @xmath9",
    ". this predictor , which converges to the predictor onto the infinite past , depends on the unknown covariance of the time series .",
    "thus , this covariance has to be estimated . here",
    ", we are facing a blind filtering problem , which is a major difficulty with regards to the usual prediction framework .",
    "kriging methods often impose a parametric model for the covariance ( see @xcite , @xcite , @xcite ) .",
    "this kind of spatial prediction is close to our work .",
    "nonparametric estimation may be done in a functional way ( see @xcite , @xcite , @xcite ) .",
    "this approach is not efficient in the blind framework . here",
    ", the blind problem is bypassed using an idea of bickel @xcite for the estimation of the inverse of the covariance .",
    "he shows that the inverse of the empirical estimate of the covariance is a good choice when many samples are at hand .",
    "we propose in this paper a new methodology , when only a path of the process is observed . for this",
    ", following comte @xcite , we build an accurate estimate of the projection operator .",
    "finally this estimated projector is used to build a predictor for the future values of the process .",
    "asymptotic properties of these estimators are studied .",
    "the paper falls into the following parts . in section [ s : notations ] , definitions and technical properties of time series are given .",
    "section [ s : frame ] is devoted to the construction of the empirical projection operator whose asymptotic behavior is stated in section [ s : rate ] .",
    "finally , we build a prediction of the future values of the process in section [ section_schur ] .",
    "all the proofs are gathered in section [ s : append ] .",
    "in this section , we present our general frame , and recall some basic properties about time series , focusing on their predictions .",
    "let @xmath10 be a zero - mean gaussian stationary process . observing a finite past @xmath8 ( @xmath11 ) of the process , we aim at predicting the present value @xmath12 without any knowledge on the covariance operator .",
    "since @xmath13 is stationary , let @xmath14 be the covariance between @xmath15 and @xmath16 . here",
    "we will consider short range dependent processes , and thus we assume that @xmath17 so that there exists a measurable function @xmath18 defined by @xmath19    this function is the so - called spectral density of the time series .",
    "it is real , even and non negative .",
    "as @xmath20 is gaussian , the spectral density conveys all the information on the process distribution .",
    "+ define the covariance operator @xmath21 of the process @xmath20 , by setting @xmath22    note that @xmath21 is the toeplitz operator associated to @xmath23 .",
    "it is usually denoted by @xmath24 ( for a thorough overview on the subject , we refer to @xcite ) .",
    "this hilbertian operator acts on @xmath25 as follows @xmath26 for sake of simplicity , we shall from now denote hilbertian operators as infinite matrices .",
    "recall that for any bounded hilbertian operator @xmath27 , the spectrum @xmath28 is defined as the set of complex numbers @xmath29 such that @xmath30 is not invertible ( here @xmath31 stands for the identity on @xmath25 ) .",
    "the spectrum of any toeplitz operator , associated with a bounded function , satisfies the following property ( see , for instance @xcite ) : @xmath32 .\\ ] ]    now consider the main assumption of this paper :    [ a : fbornee ] @xmath33    this assumption ensures the invertibility of the covariance operator , since @xmath23 is bounded away from zero . as a positive definite operator",
    ", we can define its square - root @xmath34 .",
    "let @xmath35 be any linear operator acting on @xmath25 , consider the operator norm @xmath36 and define the warped operator norm as @xmath37 note that , under assumption @xmath38 , hence the warped norm @xmath39 is well defined and equivalent to the classical one @xmath40    finally , both the covariance operator and its inverse are continuous with respect to the previous norms .",
    "+ the warped norm is actually the natural inducted norm over the hilbert space @xmath41 where @xmath42    from now on , all the operators are defined on @xmath43",
    ". set @xmath44<+\\infty \\right\\}\\ ] ]    the following proposition ( see for instance @xcite ) shows the particular interest of @xmath43 :    the map @xmath45 defines a canonical isometry between @xmath43 and @xmath46 .",
    "the isometry will enable us to consider , in the proofs , alternatively sequences @xmath47 or the corresponding random variables @xmath48 .",
    ".1 in we will use the following notations : recall that @xmath21 is the covariance operator and denote , for any @xmath49 , the corresponding minor @xmath50 by @xmath51 note that , when @xmath27 and @xmath52 are finite , @xmath53 is the covariance matrix between @xmath54 and @xmath55 .",
    "diagonal minors will be simply written @xmath56 , for any @xmath57 . + in our prediction framework , let @xmath58 and assume that we observe the process @xmath20 at times @xmath59 .",
    "it is well known that the best linear prediction of a random variable @xmath60 by observed variables @xmath61 is also the best prediction , defined by @xmath62 $ ] . using the isometry",
    ", there exist unique @xmath47 and @xmath63 with @xmath64 and @xmath65 .",
    "hence , we can define a projection operator acting on @xmath43 , by setting @xmath66 .",
    "this corresponds to the natural projection in @xmath43 onto the set @xmath67 . note that this projection operator may be written by block @xmath68 u.\\ ] ] the operator @xmath69 is well defined since @xmath70",
    "finally , the best prediction observing @xmath71 is @xmath72=p_o(\\phi(u ) ) = \\phi(p_o u).\\ ] ]    this provides an expression of the projection when the covariance @xmath21 is known .",
    "actually , in many practical situations , @xmath21 is unknown and need to be estimated from the observations . recall that we observe @xmath73 . we will estimate the covariance with this sample and use a subset of these observations for the prediction .",
    "this last subset will be @xmath74 , with @xmath75 $ ] . here",
    "@xmath76 is a growing suitable sequence .",
    "hence , the predictor @xmath77 will be here @xmath78 where @xmath79 denotes some estimator of the projection operator onto @xmath80 , built with the full sample @xmath9 .    as usual",
    ", we estimate the accuracy of the prediction by the quadratic error @xmath81.\\ ] ]    the bias - variance decomposition gives @xmath82 =   \\mathbb{e}\\big [ \\left(\\hat{p}_{o_{k(n ) } } y - p_{o_{k(n ) } } y \\right)^2 \\big ]     +    \\mathbb{e}\\big [ \\left(p_{o_{k(n)}}y- p_{\\mathbb{z}^- } y   \\right)^2 \\big ] + \\mathbb{e}\\big [ \\left(p_{\\mathbb{z}^- } y - y \\right)^2\\big ]     , \\ ] ] where @xmath83 @xmath84,\\ ] ] and @xmath85.\\ ] ] this error can be divided into three terms    * the last term @xmath86 $ ] is the prediction with infinite past error .",
    "it is induced by the variance of the unknown future values , and may be easily computed using the covariance operator .",
    "this variance does not go to zero as @xmath87 tends to infinity .",
    "it can be seen as an additional term that does not depend on the estimation procedure and thus will be omitted in the error term . *",
    "the second term @xmath88 $ ] is a bias induced by the temporal threshold on the projector . *",
    "the first term @xmath89 $ ] is a variance , due to the fluctuations of the estimation , and decreases to zero as soon as the estimator is consistent .",
    "note that to compute this error , we have to handle the dependency between the prediction operator and the variable @xmath60 we aim to predict .    finally , the natural risk is obtained by removing the prediction with infinite past error : @xmath90     +    \\mathbb{e}\\big [ \\left(p_{o_{k(n)}}y- p_{\\mathbb{z}^- _ * } y   \\right)^2 \\big ] \\\\ & = \\mathbb{e}\\left [ \\left ( \\hat{y}-\\mathbb{e}\\left[y|(x_i)_{i < 0}\\right ] \\right)^2 \\right].\\end{aligned}\\ ] ] the global risk will be computed by taking the supremum of @xmath91 among of all random variables @xmath60 in a suitable set ( growing with @xmath87 ) .",
    "this set will be defined in the next section .",
    "recall that the expression of the empirical unbiased covariance estimator is given by ( see for example @xcite ) @xmath92    notice that , when @xmath93 is close to @xmath87 , the estimation is hampered since we only sum @xmath94 terms .",
    "hence , we will not use the complete available data but rather use a cut - off .",
    "recall that @xmath95 $ ] denotes the indices of the subset used for the prediction step .",
    "we define the empirical spectral density as @xmath96    we now build an estimator for @xmath97 ( see section [ s : notations ] for the definition of @xmath97 ) .",
    "first , we divide the index space @xmath98 into @xmath99 where :    * @xmath100 denotes the index of the past data that will not be used for the prediction ( missing data ) * @xmath101 the index of the data used for the prediction ( observed data ) * @xmath102 the index of the data we currently want to forecast ( blind data ) * @xmath103 the remaining index ( future data )    in the following , we omit the dependency on @xmath87 to alleviate the notations .    as discussed in section [ s : notations ] , the projection operator @xmath104 may be written by blocks as :    @xmath105.\\ ] ]    since , we will apply this operator only to sequences with support in @xmath106 , we may consider    @xmath107u.\\ ] ] the last expression is given using the following block decomposition , if @xmath108 denotes the complement of @xmath106 in @xmath98 : @xmath109.\\ ] ]    hence , the two quantities @xmath110 and @xmath111 have to be estimated . on the one hand , a natural estimator of the first matrix is given by @xmath112 defined as @xmath113    on the other hand , a natural way to estimate @xmath111 could be to use @xmath114 ( defined as @xmath115 ) and invert it .",
    "however , it is not sure that this matrix is invertible .",
    "so , we will consider an empirical regularized version by setting @xmath116 for a well chosen @xmath117 .",
    "set @xmath118 so that @xmath119 .",
    "remark that @xmath120 is the toeplitz matrix associated to the function @xmath121 , that has been tailored to ensure that @xmath122 is always greater than @xmath123 , yielding the desired control to compute @xmath124 .",
    "other regularization schemes could have been investigated .",
    "nevertheless , note that adding a translation factor makes computation easier than using , for instance , a threshold on @xmath125 .",
    "indeed , with our perturbation , we only modify the diagonal coefficients of the covariance matrix . .1 in",
    "finally , we will consider the following estimator , for any @xmath126 : @xmath127    where the estimator @xmath128 of @xmath129 , with window @xmath130 , is defined as follows @xmath131",
    "in this section , we give the rate of convergence of the estimator built previously ( see section [ s : frame ] ) .",
    "we will bound uniformly the bias of prediction error for random variables in the close future .",
    "first , let us give some conditions on the sequence @xmath132 :    [ a : k ] the sequence @xmath133 satisfies    * @xmath134 * @xmath135    recall that the pointwise risk in @xmath136 is defined by @xmath137 \\right)^2 \\right].\\ ] ]    the global risk for the window @xmath130 is defined by taking the supremum of the pointwise risk over all random variables @xmath138    @xmath139    notice that we could have chosen to evaluate the prediction quality only on @xmath12 .",
    "nevertheless the rate of convergence is not modified if we evaluate the prediction quality for all random variables from the close future .",
    "indeed , the major part of the observations will be used for the estimation , and the conditional expectation is taken only on the most @xmath130 recent observations .",
    "our result will be then quite stronger than if we had dealt only with prediction of @xmath12 . to get a control on the bias of the prediction , we need some regularity assumption .",
    "we consider sobolev s type regularity by setting @xmath140 and define @xmath141    [ a : sobol ] there exists @xmath142 such that @xmath143 .",
    "we can now state our results .",
    "the following lemmas may be used in other frameworks than the blind problem .",
    "more precisely , if the blind prediction problem is very specific , the control of the loss between prediction with finite and infinite past is more classical , and the following lemmas may be applied for that kind of questions .",
    "the case where independent samples are available may also be tackled with the last estimators , using rates of convergences given in operator norms .",
    "the bias is given by the following lemma    [ l : bias ] for @xmath87 large enough , the following upper bound holds,@xmath144 where @xmath145 .    in the last lemma , we assume regularity in terms of sobolev s classes .",
    "nevertheless , the proof may be written with some other kind of regularity . the proof is given in appendix , and is essentially based on proposition [ p : schur ] .",
    "this last proposition provides the schur block inversion of the projection operator .",
    "the control for the variance is given in the following lemma :    [ l : var ] @xmath146 where @xmath147    again , we choose this concentration formulation to deal with the dependency of the blind prediction problem , but this result gives immediately a control of the variance of the estimator whenever independent samples are observed ( one for the estimation , and another one for the prediction ) .",
    "the proof of this lemma is given in section [ s : proof_lemma ] .",
    "it is based on a concentration inequality of the estimators @xmath148 ( see comte @xcite ) .    integrating this rate of convergence over the blind data , we get our main theorem .    [",
    "t : main ] under assumptions [ a : fbornee ] , [ a : k ] and [ a : sobol ] , for @xmath87 large enough , the empirical estimator satisfies @xmath149 where @xmath150 and @xmath151 are given in appendix .",
    "again , the proof of this result is given in section [ s : proof_main_thm ] .",
    "it is quite technical .",
    "the main difficulty is induced by the blindness . indeed , in this step , we have to deal with the dependency between the data and the empirical projector . obviously , the best rate of convergence is obtained by balancing the variance and the bias and finding the best window @xmath130 .",
    "indeed , the variance increases with @xmath130 while the bias decreases .",
    "define @xmath152 as the projector @xmath153 associated to the sequence @xmath154 that minimizes the bound in the last theorem .",
    "we get :    [ tmain ] under assumptions @xmath155 and @xmath156 , for @xmath87 large enough and choosing @xmath157 , we get @xmath158    notice that , in real life issues , it would be more natural to balance the risk given in theorem [ t : main ] , with the macroscopic term of variance given by @xmath159 \\big].\\ ] ] this leads to a much greater @xmath130 .",
    "nevertheless , corollary [ tmain ] has a theoretical interest .",
    "indeed , it recovers the classical semi - parametric rate of convergence , and provides a way to get away from dependency .",
    "notice that , the estimation rate increases with the regularity @xmath4 of the spectral density @xmath23 .",
    "more precisely , if @xmath160 , we obtain @xmath161 .",
    "this is , up to the @xmath162-term , the optimal speed . as a matter of fact , in this case , estimating the first coefficients of the covariance matrix is enough .",
    "hence , the bias is very small . proving a",
    "lower bound on the mean error ( that could lead to a minimax result ) , is a difficult task , since the tools used to design the estimator are far from the usual estimation methods .",
    "we aim at providing an exact expression for the projection operator . for this",
    ", we generalize the expression given by bondon ( @xcite , @xcite ) for a projector onto infinite past .",
    "recall that , for any @xmath163 , and if @xmath164 denotes the complement of @xmath27 in @xmath98 , the projector @xmath165 may be written blockwise ( see for instance @xcite ) as : @xmath166 .\\ ] ] denote also @xmath167 the inverse of the covariance operator , the following proposition provides an alternative expression of any projection operators .",
    "[ p : schur ] one has @xmath168 \\end{aligned}\\ ] ]    furthermore , the prediction error verifies @xmath169=u^t\\lambda_{m}^{-1}u,\\ ] ] where @xmath170    the proof of this proposition is given in appendix .",
    "we point out that this proposition is helpful for the computation of the bias .",
    "indeed , it gives a way to calculate the norm of the difference between two inverses operators .",
    "first of all , @xmath173 is a toeplitz operator over @xmath43 with eigenvalues in @xmath174 $ ] .",
    "@xmath175 may be inverted as a principal minor of @xmath176 .",
    "let us define the schur complement of @xmath176 on sequences with support in @xmath177 : @xmath178 .",
    "the next lemma provides an expression of @xmath179 ( see for instance @xcite ) .",
    "one can check @xmath181 \\left[\\begin{matrix}s^{-1}&- s^{-1}\\lambda_{am}\\lambda_m^{-1 } \\\\ - \\lambda_m^{-1}\\lambda_{ma}s^{-1 } & \\lambda_m^{-1 } + \\lambda_m^{-1}\\lambda_{ma}s^{-1}\\lambda_{am}\\lambda_m^{-1 } \\end{matrix}\\right ] \\\\ & = & \\left[\\begin{matrix}\\lambda_as^{-1 } -\\lambda_{am}\\lambda_m^{-1}\\lambda_{ma}s^{-1 } & -\\lambda_as^{-1}\\lambda_{am}\\lambda_m^{-1 } + \\lambda_{am}(\\lambda_m^{-1 } + \\lambda_m^{-1}\\lambda_{ma}s^{-1}\\lambda_{am}\\lambda_m^{-1 } ) \\\\ \\lambda_{ma}s^{-1 } - \\lambda_m\\lambda_m^{-1}\\lambda_{ma}s^{-1 } & - \\lambda_{ma}s^{-1}\\lambda_{am}\\lambda_m^{-1 } + \\lambda_m ( \\lambda_m^{-1 } + \\lambda_m^{-1}\\lambda_{ma}s^{-1}\\lambda_{am}\\lambda_m^{-1 } )   \\end{matrix}\\right ] \\\\ & = & \\left[\\begin{matrix}ss^{-1 } & ( \\lambda_{am } \\lambda_m^{-1}\\lambda_{ma}s^{-1}+i_a-\\lambda_as^{-1})\\lambda_{am}\\lambda_m^{-1 } \\\\",
    "\\lambda_{ma}s^{-1}- \\lambda_{ma}s^{-1 } & - \\lambda_{ma }   s^{-1}\\lambda_{am}\\lambda_m^{-1 }   + i_m + \\lambda_{ma}s^{-1}\\lambda_{am}\\lambda_m^{-1 }    \\end{matrix}\\right ] \\\\ &",
    "= & \\left[\\begin{matrix}i_a & 0 \\\\ 0 & i_m    \\end{matrix}\\right].\\end{aligned}\\ ] ]      we now compute the projection operator : @xmath184 \\\\ &   = & \\left[\\begin{matrix } id_a & s \\gamma_{am }   \\\\0 & 0 \\end{matrix}\\right]\\\\ &   = & \\left[\\begin{matrix } id_a &   ( \\lambda_a - \\lambda_{am}\\lambda_{m}^{-1}\\lambda_{ma})\\gamma_{am }   \\\\0 & 0 \\end{matrix}\\right]\\\\ & = & \\left[\\begin{matrix } id_a &   \\lambda_a \\gamma_{am } - \\lambda_{am}\\lambda_{m}^{-1}(id_m - \\lambda_m \\gamma_m )   \\\\0 & 0 \\end{matrix}\\right]\\\\ & = & \\left[\\begin{matrix } id_a &   \\lambda_a \\gamma_{am } - \\lambda_{am}\\lambda_{m}^{-1 } + \\lambda_{am } \\gamma_m   \\\\0 & 0 \\end{matrix}\\right]\\\\ & = & \\left[\\begin{matrix } id_a &   - \\lambda_{am}\\lambda_{m}^{-1 }   \\\\0 & 0 \\end{matrix}\\right].\\end{aligned}\\ ] ] where we have used @xmath185 in the last two lines .    now consider @xmath35 the quadratic error operator .",
    "it is defined as @xmath186.\\ ] ] this operator @xmath35 can be obtained by a direct computation ( writing the product right above ) , but it is easier to use the expression of the variance of a projector in the gaussian case given for instance by @xcite .",
    "@xmath187 again , notice that @xmath35 is the schur complement of @xmath21 on sequences with support in @xmath27 , and thanks to lemma  [ l : schur ] applied to @xmath176 instead of @xmath21 , we get @xmath188 this ends the proof of proposition [ p : schur ] .",
    "denote @xmath196 .",
    "we can write , by applying twice cauchy - schwarz s inequality , @xmath197 &   = & \\int_\\omega \\big (   \\sum _ { i = -k(n)}^{-1 } \\sum_{j = 0}^{k(n)-1 } a_{ij}(\\omega)u_jx_i(\\omega)\\big)^2{\\mathrm{d}}\\mathbb{p}(\\omega ) \\\\ & \\leq & \\int_\\omega \\sum_{i = -k(n)}^{-1 } ( \\sum_{j = 0}^{k(n)-1 } a_{ij}(\\omega)u_j)^2 \\sum_{i = -k(n)}^{-1 } x_i^2(\\omega){\\mathrm{d}}\\mathbb{p}(\\omega ) \\\\ & \\leq & \\int_\\omega \\sum _ { i = -k(n)}^{-1 } \\sum_{j = 0}^{k(n)-1 }   a_{ij}^2(\\omega ) \\sum_{j = 0}^{k(n)-1 } u_j^2 \\sum_{i = -k(n)}^{-1 } x_i^2(\\omega){\\mathrm{d}}\\mathbb{p}(\\omega),\\end{aligned}\\ ] ] so that , @xmath198   & \\leq & \\int_\\omega    \\sum _ { i = -k(n)}^{-1}\\sum_{j = 0}^{k(n)-1 } a_{ij}^2(\\omega )   \\frac{1}{m } \\sum_{i = n_0 + 1}^{k(n)+n_0 } x_i^2{\\mathrm{d}}\\mathbb{p}(\\omega).\\end{aligned}\\ ] ] using the following equivalence between two norms for finite matrices with size @xmath199 ( see for instance @xcite ) ,          further , @xmath197    & \\leq & \\frac{k(n)}{m } \\int_\\omega   \\left\\| a(\\omega ) \\right\\|_{2,op}^2   \\sum_{j = n_0 + 1}^{k(n)+n_0 } x_j^2(\\omega){\\mathrm{d}}\\mathbb{p}(\\omega )   \\\\ & \\leq & \\frac{k(n)}{m } \\sqrt{\\int_\\omega   \\left\\| a ( \\omega)\\right\\|_{2,op}^4{\\mathrm{d}}\\mathbb{p}(\\omega ) } \\sqrt{\\int_\\omega\\left(\\sum_{j = n_0 + 1}^{k(n)+n_0 } x_j^2(\\omega)\\right)^2{\\mathrm{d}}\\mathbb{p}(\\omega ) } \\\\ & \\leq & \\frac{k(n)}{m } \\sqrt{\\int_{\\mathbb{r}^+ } \\mathbb{p}\\left ( \\left\\| a \\right\\|_{2,op}^4>t \\right){\\mathrm{d}}t } \\sqrt{k(n)^2\\int_\\omega\\left ( x_j^4\\right){\\mathrm{d}}\\mathbb{p}(\\omega)},\\end{aligned}\\ ] ] we have used here again cauchy - schwarz s inequality and the fact that , for all nonnegative random variable @xmath60 , @xmath201 = \\int_{\\mathbb{r}^+ } \\mathbb{p}\\left ( y > t\\right ) { \\mathrm{d}}t.\\ ] ]                recall that we aim to obtain a bound on @xmath206 . using proposition  [ p : schur ] ,",
    "we can write @xmath207 - \\left[\\begin{matrix } - \\lambda_{o_k\\mathbb{z}^+}(\\lambda_{\\mathbb{z}^+})^{-1 } \\\\   -\\lambda_{m_k^-\\mathbb{z}^+}(\\lambda_{\\mathbb{z}^+})^{-1 } \\end{matrix}\\right ] \\right\\|_{\\gamma}.\\end{aligned}\\ ] ] so that , using the norms equivalence , @xmath208 - \\left[\\begin{matrix } - \\lambda_{o_k\\mathbb{z}^+}(\\lambda_{\\mathbb{z}^+})^{-1 } \\\\   -\\lambda_{m_k^-\\mathbb{z}^+}(\\lambda_{\\mathbb{z}^+})^{-1 } \\end{matrix}\\right ] \\right\\|_{2,op } \\\\ & \\leq & \\frac{m'}{m } \\left\\| \\left [ \\begin{matrix } ( \\gamma_{o_k})^{-1}\\gamma_{o_k\\mathbb{z}^+ } + \\lambda_{o_k\\mathbb{z}^+}(\\lambda_{\\mathbb{z}^+})^{-1 } \\\\ \\lambda_{m_k^-\\mathbb{z}^+}(\\lambda_{\\mathbb{z}^+})^{-1 } \\end{matrix}\\right ]   \\right\\|_{2,op } \\\\   & \\leq &   \\frac{m'}{m}\\left\\| \\left [ \\begin{matrix } ( \\gamma_{o_k})^{-1}\\gamma_{o_k\\mathbb{z}^+}\\lambda_{\\mathbb{z}^+ } + \\lambda_{o_k\\mathbb{z}^+}\\\\ \\lambda_{m_k^-\\mathbb{z}^+}\\end{matrix}\\right ]   \\right\\|_{2,op } \\left\\|    ( \\lambda_{\\mathbb{z}^+})^{-1 } \\right\\|_{2,op}\\\\   & \\leq &   \\frac{m'}{m}\\left\\| ( \\lambda_{\\mathbb{z}^+})^{-1 } \\right\\|_{2,op } \\left ( \\left\\|(\\gamma_{o_k})^{-1}\\gamma_{o_k\\mathbb{z}^+}\\lambda_{\\mathbb{z}^+ }   + \\lambda_{o_k\\mathbb{z}^+ } \\right\\|_{2,op }   + \\left\\| \\lambda_{m_k^-\\mathbb{z}^+ } \\right\\|_{2,op } \\right).\\end{aligned}\\ ] ] the last step follows from the inequality : @xmath209 but , since @xmath210 , @xmath211 so , we obtain , but , we have ,                                                                    for ease of notations , we set @xmath241 and @xmath242 for the computation of the mean , the interval @xmath243 will be divided into three parts , where only the first contribution is significant , thanks to the exponential concentration .",
    "we will prove that the two other parts are negligible .",
    "now , we will prove that each term can be neglected .",
    "integrating by part , we obtain @xmath266_{t_1}^{\\infty } \\\\ & & + \\int_{t_1}^{\\infty}\\frac{c_0 ^ 2k(n)^2}{n\\sqrt{t } }   e^{-n\\sqrt{\\frac{t}{c_0 ^ 4k(n)^4}+\\log(k(n ) ) } } { \\mathrm{d}}t \\\\ & \\leq & \\frac{2\\log(k(n))c_0 ^ 4k(n)^4}{n^2 } + \\frac{2c_0 ^ 4k(n)^4}{n^2 } \\\\ & = & o\\left ( \\left(c_0k(n)\\sqrt{\\frac{\\log(k(n))}{n}}\\right)^4   \\right).\\end{aligned}\\ ] ]    then , @xmath267{\\frac{nt}{c_0 ^ 4k(n)^4}}}{\\mathrm{d}}t & \\leq & t_2   e^{-n\\sqrt[4]{\\frac{t_1}{c_0 ^ 4k(n)^4 } } } \\\\ & \\leq & t_2 e^{-\\sqrt{n\\log(k(n ) ) } } \\\\ & = & o\\left ( \\left(c_0k(n)\\sqrt{\\frac{\\log(k(n))}{n}}\\right)^4   \\right).\\end{aligned}\\ ] ]          so , integrating by part once more , we obtain @xmath273{t}-c_2}{c_0k(n)}\\right)^2+\\log(k(n))}{\\mathrm{d}}t & \\leq&\\int_{\\sqrt[4]{t_2}-c_3}^{+\\infty } 4(u+c_3)^3 e^{-n\\left(\\frac{u}{c_0k(n)}\\right)^2+\\log(k(n))}{\\mathrm{d}}u \\\\&\\leq & \\left[p_1(u , n , k(n ) )     e^{-n\\left(\\frac{u}{c_0k(n)}\\right)^2+\\log(k(n ) ) } \\right]_{\\sqrt[4]{t_2}-c_3}^{+\\infty } \\\\ & \\leq & p_1(u , n , k(n ) ) e^{-n } \\\\ & \\leq & o\\left ( \\left(c_0k(n)\\sqrt{\\frac{\\log(k(n))}{n}}\\right)^4   \\right).\\end{aligned}\\ ] ] here , @xmath274 is a polynomial of degree @xmath275 in @xmath276 and is rational function in @xmath87 and @xmath277 .",
    "furthermore , @xmath278{t}-c_3)}{c_0k(n ) } } { \\mathrm{d}}t & \\leq&\\int_{\\sqrt[4]{t_2}-c_3}^{+\\infty } 4(u+c_3)^3 e^{-n\\frac{u}{c_0k(n)}}{\\mathrm{d}}u \\\\&\\leq & \\left[p_2(u , n , k(n ) )     e^{-n\\frac{u}{c_0k(n ) } } \\right]_{\\sqrt[4]{t_2}-c_3}^{+\\infty } \\\\ & \\leq & p_2(u , n , k(n))e^{-\\sqrt{n(\\log(k(n))+n ) } } \\\\ & \\leq & o\\left ( \\left(c_0k(n)\\sqrt{\\frac{\\log(k(n))}{n}}\\right)^4   \\right),\\end{aligned}\\ ] ] where @xmath279 is a polynomial of degree @xmath275 in @xmath276 and is rational function in @xmath87 and @xmath277 .",
    "notice that @xmath281 with @xmath282 .",
    "we use the following proposition from comte @xcite .",
    "let @xmath283 be a centered gaussian stationary sequence and @xmath284 a bounded function such that @xmath285 is a symmetric non negative matrix .",
    "then the following concentration inequality holds for @xmath286 \\right)$ ] : @xmath287    by applying this result respectively with @xmath288 and @xmath289 and we obtain @xmath290 or , equivalently , @xmath291 with probability lower than @xmath292 . by taking an equivalent , we obtain that there exists @xmath236 such that , for all @xmath237 , for all @xmath293"
  ],
  "abstract_text": [
    "<S> we tackle the issue of the blind prediction of a gaussian time series . for this </S>",
    "<S> , we construct a projection operator build by plugging an empirical covariance estimation into a schur complement decomposition of the projector . </S>",
    "<S> this operator is then used to compute the predictor . </S>",
    "<S> rates of convergence of the estimates are given . </S>"
  ]
}