{
  "article_text": [
    "in many statistical applications , the asymptotic limit distribution can not be used to construct tests or confidence intervals , as the limit might be unknown or dependent on unknown parameters , which are hard to estimate .",
    "bootstrap versions of the statistical estimator provide a nonparametric alternative , so establishing the consistency of such bootstrap procedure is important . in the case of independent and identically distributed ( iid ) random variables , the main idea of the bootstrap",
    "consists of replacing the original sample @xmath1 of observations with unknown marginal distribution function @xmath2 by a new iid sample @xmath3 with the marginal distribution function @xmath4 which is the empirical distribution function constructed from the original sample ( see efron @xcite ) . as a bootstrap version of a statistic @xmath5 one can take the plug - in version @xmath6 .",
    "the validity of bootstrap for the sample mean of iid observations was first established by bickel and freedman @xcite and singh @xcite .",
    "it is well - known that in some cases the bootstrap method provides a better approximation to the distribution of the statistic than , for instance , normal approximation ( see hall @xcite ) .",
    "especially when sample size is relatively small , bootstrap has some preferences .    in the case of dependent observations",
    "this idea of getting a new sample does not work since a new iid sample does not capture the dependence structure , see singh @xcite .",
    "therefore several so - called blockwise bootstrap methods of getting a new sample were introduced ( for overview of blockwise bootstrap methods see lahiri @xcite ) . in all these methods ,",
    "we resample from blocks of @xmath7 consecutive observations , so that inside the blocks the dependence structure will be kept .",
    "we will consider two of them : the circular block bootstrap method introduced by politis and romano @xcite and nonoverlapping block bootstrap method by carlstein @xcite . in the case of circular block bootstrap method we extend the original sample to @xmath8 where @xmath9 is a block length . to get a new sample @xmath10 we choose randomly and independently @xmath7 consecutive observations of the sample",
    "@xmath11 $ ] times with @xmath12 for @xmath13 here and in what follows we denote by @xmath14 the conditional probability , conditional expectation and conditional variance respectively .",
    "for instance the bootstrap version of sample mean @xmath15 will be @xmath16 in the case of nonoverlapping block bootstrap method we construct a new sample @xmath17 by choosing randomly and independently blocks of @xmath7 consecutive observations of the sample @xmath18 as above @xmath11 $ ] times with @xmath19 for @xmath20 . in this case the bootstrap version of the sample mean is @xmath21 for weakly dependent observations strong consistency ( almost surely convergence ) of the @xmath22 and @xmath23 were proved in shao and yu @xcite and peligrad @xcite .",
    "gonalves and white @xcite and dehling , sharipov and wendler @xcite extended these results to functionals of mixing processes . in this paper",
    "we will concentrate on bootstrap for @xmath0-statistics of weakly dependent observations .",
    "bootstrap for @xmath0-statistics of independent observations were studied by bickel and freedman @xcite , arcones and gine @xcite , dehling and mikosch @xcite , leucht and neumann @xcite .",
    "recently , dehling and wendler @xcite , sharipov and wendler @xcite and leucht and neumann @xcite established the consistency of bootstrap estimators for @xmath0-statistics of weakly dependent observations . in the aforementioned papers the same idea of getting bootstrap versions of @xmath0-statistics have been used , which is based on the plug - in principle .",
    "we will now introduce @xmath0-statistics .",
    "let @xmath24 be a stationary sequence of random variables with a common distribution function @xmath2 . for simplicity reasons we consider @xmath0-statistics of degree two i.e. : @xmath25 where @xmath26 is the symmetric and measurable kernel .",
    "the kernel @xmath27 is called degenerate , if @xmath28 if this does not hold we call the kernel @xmath27 and the corresponding @xmath0-statistics nondegenerate . in what follows",
    "we will only consider @xmath0-statistics with a nondegenerate kernel . in the case of iid observations @xmath18 bickel and freedman @xcite used the bootstrap sample @xmath29 of conditional independent observations with common distribution function @xmath4 , which is the empirical distribution function constructed by original sample . in order to get bootstrap versions of @xmath0-statistics they plugged the bootstrapped observations in @xmath0-statistics i.e. @xmath30 in the case of dependent observations ,",
    "the same idea was explored by dehling , wendler @xcite using the circular block bootstrap method , the corresponding bootstrap version is @xmath31 while in the case of nonoverlapping block bootstrap method the corresponding bootstrap version of @xmath0-statistics is @xmath32 consistency of @xmath33 ( and @xmath34 ) can be proved using the hoeffding decomposition ( see @xcite ) : @xmath35 where @xmath36 in order to prove consistency of @xmath33 it is enough to show the convergence of bootstrapped distribution of the sample mean for the second summand of this decomposition and convergence to zero ( in probability or almost surely ) of the third summand .",
    "the goal of this paper is to suggest a new resampling method for @xmath0-statistics .",
    "the main idea is the following : we suggest to draw with replacement from the @xmath0-statistics calculated on subsamples instead of drawing with replacement from blocks of observations and then plug them in @xmath0-statistics .",
    "we introduce subsets @xmath37 by @xmath38\\end{aligned}\\ ] ] where @xmath39 us called block length .",
    "furthermore , let @xmath40 and @xmath41 be two sequences of iid random variables with distributions : @xmath42 a fixed realisations of the sample @xmath1 we denote by @xmath43 .",
    "we set @xmath44 where @xmath45 is the indicator function .",
    "so @xmath46 respectively @xmath47 are the @xmath0-statistics calculated from the @xmath48-th block and @xmath49 and @xmath50 are the results of the drawing with replacement from these @xmath0-statistics . in the case of @xmath46",
    "we assume that @xmath51 if @xmath52 . as the bootstrap versions of @xmath0-statistics @xmath53 we take the following : @xmath54 note that @xmath55 in the next sections we will state our main result : the strong consistency of the bootstrap based on @xmath56 , @xmath57 .",
    "the new approach reduces the computational burden of the monte carlo method usually used for the bootstrap .",
    "after calculating the values @xmath58 , for every run of the monte carlo evaluation one need only @xmath59 calculation steps , while for the plug - in method , in every run the @xmath0-statistic has to be calculated again in @xmath60 steps . as the linear parts of the plug - in bootstrap version and the new bootstrap version of a @xmath0-statistic are the same , we expect a similar behaviour",
    ". we will give an upper bound for the mean square error ( mse ) of the variances of @xmath56 , @xmath57 , which suggest a choice of the block length of order @xmath61 .",
    "in section 3 , we will present our simulation results .",
    "we consider the sample variance as a @xmath0-statistic and we will compare the new bootstrap approach with plug - in bootstrap and subsampling .",
    "the proofs of the main results will be given in section 4 .",
    "let @xmath62 be a stationary sequence of random variables with values in a separable linear space .",
    "we will assume that this sequence satisfies some form of short range dependence .",
    "namely we will consider strong mixing and absolute regularity conditions .",
    "recall that strong mixing coefficients @xmath63 and absolute regularity coefficients @xmath64 are defined as @xmath65 where @xmath66 is the @xmath67 - field generated by @xmath68 .",
    "now we can formulate our results .",
    "[ theo1 ] let @xmath62 be a stationary sequence of absolutely regular random variables .",
    "assume that the following conditions hold ( for some @xmath69 )    * @xmath70 for some @xmath71 , * @xmath72 for all @xmath73 , * @xmath74 for some @xmath75 , * @xmath76 for some @xmath77 .",
    "then as @xmath78 for @xmath79 @xmath80\\right)\\leq x\\right]-p\\left[\\sqrt{n}\\left(u_{n}\\left(h\\right)-\\theta\\right)\\leq x\\right]\\right|\\rightarrow0.\\ ] ] @xmath81-{{\\rm var}}\\left[\\sqrt{n}u_n\\left(h\\right)\\right]\\right|\\rightarrow0,\\ ] ] in probability .    in the next theorem we consider strongly mixing random variables .",
    "this is a weaker assumption on the dependence , but in this case we assume that the kernel satisfies the following condition : we say that a kernel is @xmath82-lipschitz - continuous , if there exists a constant @xmath83 such that @xmath84\\leq c\\epsilon\\ ] ] for every @xmath85 , every pair @xmath86 and @xmath87 with the common distribution @xmath88 for some @xmath89 or @xmath90 and @xmath91 and @xmath87 also with one of these common distributions .",
    "for the examples of kernels which satisfy above condition see @xcite , @xcite .",
    "[ theo2 ] let @xmath62 be a stationary sequence of strongly mixing random variables .",
    "assume that the following conditions hold    * @xmath70 , * @xmath72 for all @xmath73 , * @xmath76 for some @xmath77 , * @xmath92 is @xmath82-lipschitz - continuous , * @xmath93 for some @xmath94 , * @xmath95 for some @xmath96 .",
    "then the statements of theorem [ theo1 ] hold .    in the next theorem",
    "we give bounds for the mean squared error to give a deeper insight into the properties of bootstrap versions of @xmath0-statistics .",
    "[ theo3 ] let @xmath62 be a stationary sequence of absolutely regular random variables .",
    "assume that the following conditions hold :    * @xmath97 for some @xmath69 , * @xmath98 for all @xmath99 , * @xmath100 for some @xmath101 .",
    "then for @xmath57 @xmath102    choosing a block length of order @xmath103 , we can achieve that the mean squared error of @xmath104 is of order @xmath105 .",
    "we consider the estimator @xmath106 for the variance @xmath107 which is a @xmath0-statistic with the kernel @xmath108 : @xmath109 @xmath110 is a stationary , gaussian , autoregressive process with @xmath111 , where @xmath112 is a sequence of iid standard normal random variables and @xmath113 .",
    "we will compare three methods for constructing confidence intervals :    * the circular plug - in bootstrap ( already used in dehling , wendler @xcite ) . * the new bootstrap version @xmath114 , the drawing with replacement from @xmath0-statistics caculated on subsamples .",
    "* subsampling : the estimator of the unkown distribution is given by the empirical distribution function of the @xmath0-statistics caculated on subsamples , see politis and romano @xcite .    for each combination of construction method , ar - coefficient , sample size ( @xmath115 @xmath116 ) and block length , we have simulated 10.000 samples and evaluated the empirical probability of the 95% confidence interval to cover the true parameter .",
    "the two bootstrap version are evaluated by the monte carlo method with 1000 times drawing with replacement .",
    "the results are summarized in the table below .",
    ".simulated coverage propabilites for plug - in bootstrap/ new bootstrap/ subsampling , sample size @xmath117 , ar - coefficient @xmath118 , block length @xmath119 [ cols=\"<,^,^,^,^ \" , ]     in general , we draw the following conclusions from the simulation study : all three methods lead to confidence intervals with a coverage probabilty lower than the nominal confidence level of 95% .",
    "the performance of the subsampling is worse than the performance of two bootstrap methods in all situations .",
    "if the dependence in the ar - process is weak ( @xmath120 ) , both bootstrap methods lead to comparable results , while for stronger dependence ( @xmath121 ) , the plug - in bootstrap has a better performance .    for the plug - in bootstrap",
    ", the block length can be choosen smaller than for the new bootstrap method or for subsampling , especially in the case of stronger dependence ( @xmath121 ) . in figur 1",
    "below , we give a more detailed picture of the coverage probabilities for different block lengths in the case @xmath122 , @xmath123 .",
    ", sample size @xmath122 , ar - coefficient @xmath123,scaledwidth=80.0% ]",
    "[ lem1](dehling , wendler @xcite ) let @xmath62 be a stationary sequence and @xmath27 a degenerate kernel satisfying @xmath97 for some @xmath124 and @xmath98 for all @xmath99 .",
    "let be @xmath125 such that one of the following two conditions holds :        [ lem2](yoshihara @xcite ) let @xmath62 be a stationary sequence of absolutely regular random variables .",
    "assume that the the following conditions hold ( for some @xmath69 ) and @xmath130 @xmath131 @xmath132 then @xmath133    [ lem3 ] ( yokoyama @xcite ) let @xmath134 be a stationary strongly mixing sequence of random variables with @xmath135 and @xmath136 for some @xmath137 .",
    "suppose that @xmath138 and @xmath139 then there exists a constant @xmath140 depending only on @xmath141 and the mixing coefficients @xmath142 such that @xmath143      let us introduce blocks of indices @xmath144 note that the blocks @xmath145 and @xmath146 correspond to the circular and nonoverlapping blocking methods , respectively . in the circular blocking method instead of the sample @xmath18 we consider the completed sample @xmath147 a fixed realization of the sample @xmath18 we denote by @xmath148 and set @xmath149\\\\ x_{j}^{1*}=\\sum_{i=1}^{n}i(t_{1}(j)=i)\\overline{x}_{1,1}(i ) , \\\\",
    "x_{j}^{2*}=\\sum_{i=1}^{m}i(t_{2}(j)=i)\\overline{x}_{2,1}(i ) , \\displaybreak[0]\\\\ \\overline{x}_{n , m}^{1*}=\\frac{1}{m}\\sum_{i=1}^{m}x_{i}^{1 * } , \\\\",
    "\\overline{x}_{n , m}^{2*}=\\frac{1}{m}\\sum_{i=1}^{m}x_{i}^{2*}.\\end{aligned}\\ ] ]    first we will prove the statement of the theorem for @xmath150 ( circular bootstrap ) .",
    "using hoeffding decomposition we have @xmath151\\right)=\\frac{1}{m } \\sum^m_{j = 1}\\left(u^{1*}_{j}-e^{\\star}u^{1*}_{n}(h)\\right)\\\\ = \\frac{1}{m } & \\sum^m_{j=1}\\bigg[\\theta+\\frac{2}{l}\\sum_{k \\in b_{1}(t_{1}(j))}h_{1}(x_{k})+\\frac{2}{l(l-1)}\\sum_{(a , b ) \\in b^{'}_{1}(t_{1}(j))}h_{2}(x_{a},x_{b})\\\\ & -\\big(\\theta+\\frac{1}{n}\\sum^n_{i=1}\\frac{2}{l}\\sum_{k \\in b_{1}(i)}h_{1}(x_{k})+\\frac{1}{n } \\sum^n_{i=1}\\frac{2}{l(l-1)}\\sum_{(a , b ) \\in b^{'}_{1}(i)}h_{2}(x_{a},x_{b})\\big)\\bigg ] \\displaybreak[0]\\\\ = \\frac{1}{m } & \\sum^m_{j=1}\\bigg[\\frac{2}{l}\\big(\\sum_{k \\in b_{1}(t_{1}(j))}h_{1}(x_{k})-\\frac{1}{n}\\sum^n_{r = 1}h_{1}(x_{r})\\big)\\\\ & + \\frac{2}{l(l-1)}\\big(\\sum_{(a , b ) \\in b^{'}_{1}(t_{1}(j))}h_{2}(x_{a},x_{b})-\\frac{1}{n } \\sum^n_{r=1}\\frac{2}{l(l-1)}\\sum_{(a , b ) \\in b^{'}_{1}(r)}h_{2}(x_{a},x_{b})\\big)\\bigg]\\displaybreak[0]\\\\ = \\frac{2}{ml } & \\sum^m_{j = 1}\\sum_{k \\in b_{1}(t_{1}(j))}\\big ( h_{1}(x_{k})-\\frac{1}{n}\\sum^n_{r = 1}h_{1}(x_{r})\\big)\\\\ & + \\frac{2}{ml(l-1)}\\sum^m_{j = 1}\\sum_{(a , b ) \\in b^{'}_{1}(t_{1}(j))}\\big(h_{2}(x_{a},x_{b})-\\frac{1}{n } \\sum^n_{r=1}\\sum_{(a , b ) \\in b^{'}_{1}(r)}h_{2}(x_{a},x_{b})\\big)\\end{aligned}\\ ] ] using the latter @xmath152\\right)=\\frac{2}{\\sqrt{ml } } \\sum^m_{j = 1}\\big[\\sum_{k \\in b_{1}(t_{1}(j))}h_{1}(x_{k})-\\frac{1}{n}\\sum^n_{i = 1}h_{1}(x_{i})\\big]\\\\ + & \\frac{2}{\\sqrt{ml}(l-1)}\\sum^m_{j = 1}\\big[\\sum_{(a , b ) \\in b^{'}_{1}(t_{1}(j))}(h_{2}(x_{a},x_{b})-\\frac{1}{n } \\sum^n_{r=1}\\sum_{(a , b ) \\in b^{'}_{1}(r)}h_{2}(x_{a},x_{b}))\\big]\\\\ = & i_{n}+i\\ ! i_{n}.\\end{aligned}\\ ] ] in order to prove the first statement of theorem [ theo1 ] , we first show that @xmath153 as @xmath78 .",
    "note that @xmath154 by lemma [ lem1 ] we have as @xmath78 @xmath155 and consequently @xmath156 in probability as @xmath78 . with the chebyshev inequality",
    ", it follows that @xmath153 in conditional probability .",
    "it remains to show the convergence of @xmath157 . by theorem 3.2 of lahiri @xcite",
    "we have as @xmath78 @xmath158 in probability .",
    "furthermore , we have @xmath159 by lemma [ lem1 ] and can conclude that @xmath160 using slutzky s lemma and @xmath153 in probability , we arrive at at the first statement of the theorem : @xmath161\\right)\\leq x\\right]-p\\left[\\sqrt{n}\\left(u_{n}\\left(h\\right)-\\theta\\right)\\leq x\\right]\\right|\\rightarrow0.\\ ] ] for the second statement , we make use of theorem 3.2 of lahiri @xcite again , which also states that @xmath162 this together with @xmath156 in probability and @xmath159 ( see lemma [ lem1 ] ) leads to @xmath81-{{\\rm var}}\\left[\\sqrt{n}u_n\\left(h\\right)\\right]\\right|\\rightarrow0\\ ] ] in probability as @xmath78 .    in the case of @xmath163 ( nonoverlapping bootstrap ) analogously to previous one we have @xmath164\\right)\\\\ = \\frac{2}{\\sqrt{ml } } \\sum^m_{j = 1}\\sum^m_{i = 1}i(t_{2}(j)=i)\\sum_{k \\in b_{1}(i)}\\big(h_{1}(x_{k})-\\frac{1}{ml}\\sum^{ml}_{i = 1}h_{1}(x_{i})\\big)\\\\ + \\frac{2}{\\sqrt{ml}(l-1)}\\sum^m_{j = 1}\\sum_{(a , b ) \\in b^{'}_{1}(t_{2}(j))}\\big(h_{2}(x_{a},x_{b})-\\frac{1}{m } \\sum^m_{i = 1}\\sum_{(a , b ) \\in b^{'}_{1}(i)}h_{2}(x_{a},x_{b})\\big)=\\\\ = i^{'}_{n}+i\\ ! i^{'}_{n}.\\end{gathered}\\ ] ] theorem 3.2 of lahiri @xcite implies as @xmath78 @xmath165 @xmath166 in probability as @xmath78 .",
    "it remains to show that @xmath167 in probability , the rest of the proof can be done along the lines of the case @xmath150 ( circular bootstrap ) .",
    "we have @xmath168 by lemma [ lem1 ] we have as @xmath78 @xmath169 so @xmath170 .          to shorten the proof , we concentrate on the case @xmath171 ( circular bootstrap ) .",
    "we split the mean squared error into three parts @xmath172 for the first summand , we know by theorem 3.1 of politis and white @xcite that @xmath173 for the second summand , we make use of the equation @xmath174 and the hlder - inequality ( first for the bootstrap expectation and then for the unconditional expectation ) to obtain @xmath175\\\\ \\leq e\\bigg(e^{\\star}\\big(\\frac{1}{\\sqrt{ml}}\\sum_{i=1}^{ml}h_1(x_{1,i}^\\star)-e^\\star\\frac{1}{\\sqrt{ml}}\\sum_{i=1}^{ml}h_1(x_{1,i}^\\star)-\\sqrt{ml}u_{n}^{k*}(h)+e^\\star\\sqrt{ml}u_{n}^{k*}(h)\\big)^2\\\\ \\shoveright{e^{\\star}\\big(\\frac{1}{\\sqrt{ml}}\\sum_{i=1}^{ml}h_1(x_{1,i}^\\star)-e^\\star\\frac{1}{\\sqrt{ml}}\\sum_{i=1}^{ml}h_1(x_{1,i}^\\star)+\\sqrt{ml}u_{n}^{k*}(h)-e^\\star\\sqrt{ml}u_{n}^{k*}(h)\\big)^{2}\\bigg)}\\displaybreak[0]\\\\ \\shoveleft{= e\\bigg({{\\rm var}}^{\\star}\\big(\\frac{1}{\\sqrt{ml}(l-1)}\\sum_{j=1}^m\\sum_{i=1}^ni(t_1(j)=i)\\sum_{(a , b)\\in b_1'(i)}h_2(x_a , x_b)\\big)}\\\\ { { \\rm var}}^{\\star}\\big(2\\frac{1}{\\sqrt{ml}}\\sum_{i=1}^{ml}h_1(x_{1,i}^\\star)+\\frac{1}{\\sqrt{ml}(l-1)}\\sum_{j=1}^m\\sum_{i=1}^ni(t_1(j)=i)\\sum_{(a , b)\\in b_1'(i)}h_2(x_a , x_b)\\big)\\bigg)\\displaybreak[0]\\\\ \\leq \\bigg(e\\big({{\\rm var}}^{\\star}\\big(\\frac{1}{\\sqrt{ml}(l-1)}\\sum_{j=1}^m\\sum_{i=1}^ni(t_1(j)=i)\\sum_{(a , b)\\in b_1'(i)}h_2(x_a , x_b)\\big)\\big)^2\\bigg)^{\\frac{1}{2}}\\\\",
    "\\bigg(e\\big({{\\rm var}}^{\\star}\\big(2\\frac{1}{\\sqrt{ml}}\\sum_{i=1}^{ml}h_1(x_{1,i}^\\star)+\\frac{1}{\\sqrt{ml}(l-1)}\\sum_{j=1}^m\\sum_{(a , b)\\in b_1'(t_1(j))}h_2(x_a , x_b)\\big)\\big)^2\\bigg)^{\\frac{1}{2}}\\\\ = a_n\\times b_n.\\end{gathered}\\ ] ] we will treat these two factors separately , starting with @xmath176 . by the definition of the bootstrap procedure and the stationarity of the sequence , we get the following : @xmath177 as we know from lemma 3 of yoshihara @xcite that @xmath178 with the help of the inequality @xmath179 , we split the second summand into two parts : @xmath180 so it suffices to study @xmath181 .",
    "we use again stationarity and the definition of the bootstrap : @xmath182\\\\ \\leq \\bigg(ee^{\\star}\\big(\\frac{1}{\\sqrt{l}}\\sum_{i=1}^{l}h_1(x_{1,i}^\\star)\\big)^4\\bigg)^{\\frac{1}{2}}=\\bigg(e\\frac{1}{n}\\sum_{j=1}^n\\big(\\frac{1}{\\sqrt{l}}\\sum_{i = j}^{j+l-1}h_1(x_{i})\\big)^4\\bigg)^{\\frac{1}{2}}\\\\ = \\bigg(e\\big(\\frac{1}{\\sqrt{l}}\\sum_{i = j}^{j+l-1}h_1(x_{i})\\big)^4\\bigg)^{\\frac{1}{2}}\\leq c.\\end{gathered}\\ ] ] the last inequality follows from lemma [ lem3 ] .",
    "this shows that @xmath183 .",
    "in the same way one can show that @xmath184 is of the same order , which completes the proof .",
    "xxx , on the bootstrap for u and v statistics , _ ann .",
    "_ * 20 * ( 1992 ) 655 - 674 . , some asymptotic theory for the bootstrap , _ ann .",
    "stat . _ * 9 * ( 1981 ) 1196 - 1217 . , the use of subseries values for estimating the variance of a general statistic from stationary sequence , _ ann .",
    "* 14 * ( 1986 ) 1171 - 1179 . , random quadratic forms and the bootstrap for @xmath0-statistics , _ j. multivariate anal . _",
    "* 51 * ( 1994 ) 392 - 413 .",
    ", bootstrap for dependent hilbert space - valued random variables with application to von mises statistics , _",
    "j. multivariate anal . _",
    "* 133 * ( 2015 ) 200 - 215 .",
    ", central limit theorem and the bootstrap for @xmath0-statistics of strongly mixing data , _ j. multivariate anal . _ * 101 * ( 2010 ) 126 - 137 .",
    ", law of the iterated logarithm for @xmath0-statistics of weakly dependent observations , in : berkes , bradley , dehling , peligrad , tichy ( eds ) : _ dependence in probability , analysis and number theory _ , kendrick press , heber city ( 2010 ) . , bootstrap methods : another look at the jackknife , _ ann .",
    "_ * 7 * ( 1979 ) 1 - 26 . , the bootstrap of the mean for dependent hetereogeneous arrays , _ econometric theory _ * 18 * ( 2002 ) 1367 - 1384 .",
    ", _ the bootstrap and edgeworth expansions _ , springer , new york ( 1992 ) . , a class of statistics with asymptotically normal distribution , _ ann . math . stat . _",
    "* 19 * ( 1948 ) 293 - 325 . , _ resampling methods for depenent data _ , springer , new york ( 2003 ) . , consistency of general bootstrap methods for degenerate u- and v - type statistics , _ j. mult",
    ". anal . _ * 100 * ( 2009 ) 1622 - 1633 .",
    ", dependent wild bootstrap for degenerate u- and v - statistics .",
    "_ j. mult .",
    "_ , * 117 * ( 2013 ) 257 - 280 . , a circular block - resampling procedure for stationary data,_in : exploring the limits of the bootstrap _ ( r. lepage and l. billard , eds . ) ( 1992 ) 263-270.wiley , new york .",
    ", large sample confidence regions based on subsamples under minimal assumptions , _ ann .",
    "* 22 * ( 1994 ) 2031 - 2050 . ,",
    "automatic block - length selection for the dependent bootstrap , _ econometric reviews _ * 23 * ( 2004 ) 53 - 70 .",
    ", on the blockwise bootstrap for empirical processes for stationary sequences , _ ann .",
    "* 2 * ( 1998 ) 877 - 901 .",
    ", bootstrapping the sample means for stationary mixing sequences , _ stochastic process .",
    "appl . _ * 48 * ( 1993 ) 175 - 190 . , bootstrap for the sample mean and for @xmath0-statistics of mixing and near - epoch dependent processes , _ journal of nonparametric statistics _ * 24 * ( 2012 ) 317 - 342 .",
    ", on the asymptotic accuracy of efron s bootstrap , _ ann .",
    "_ * 9 * ( 1981 ) 1187 - 1195 .",
    ", moment bounds for stationary mixing sequences , _ z. wahrsch .",
    "gebiete _ * 52 * ( 1980 ) 45 - 57 . , limiting behavior of @xmath0-statistics for stationary , absolutely regular processes , _ z. wahrsch .",
    "gebiete _ * 35 * ( 1976 ) 237 - 252 ."
  ],
  "abstract_text": [
    "<S> bootstrap for nonlinear statistics like @xmath0-statistics of dependent data has been studied by several authors . </S>",
    "<S> this is typically done by producing a bootstrap version of the sample and plugging it into the statistic . </S>",
    "<S> we suggest an alternative approach of getting a bootstrap version of @xmath0-statistics , which can be described as a compromise between bootstrap and subsampling . </S>",
    "<S> we will show the consistency of the new method and compare its finite sample properties in a simulation study . </S>"
  ]
}