{
  "article_text": [
    "bayesian state estimation for nonlinear systems requires an efficient approximation for practical applications as closed - form solutions are not available in general .",
    "a common approximation technique is the discretization of the state space as done in grid filters or particle filters @xcite .",
    "theoretically , these techniques facilitate to approach the true statistics of the state with arbitrary accuracy .",
    "but they are only applicable to low - dimensional problems since their computational complexity increases exponentially with the dimension of the state space .    a famous exception that exhibits an analytic solution is the linear gaussian case .",
    "here , the famous kalman filter provides optimal results in an efficient manner @xcite .",
    "so - called gaussian filters try to adapted the kalman filter equations to nonlinear problems by assuming that the density function of the state can be represented by a gaussian density .",
    "the extended kalman filter @xcite applies first - order taylor series expansion for linearization .",
    "the unscented kalman filter @xcite or the gaussian estimator @xcite offer higher order accuracy by employing statistical linearization .",
    "but in general a single gaussian density is typically not a sufficient representation for the true density function , which may be skew or multimodal .",
    "thanks to their universal approximator property , gaussian mixtures @xcite are a much better approach for approximating complex density functions .",
    "examples for gaussian mixture filters applied to nonlinear estimation are in @xcite .",
    "the estimation accuracy of gaussian mixture filters significantly depend on the number of gaussian components used .",
    "this number is typically defined by the user . in this paper ,",
    "a novel gaussian mixture filter is proposed , which adapts the number of components dynamically and on - line .",
    "the nonlinear system and measurement models are linearized locally by means of statistical linearization at each component of the gaussian mixture . the induced linearization error is quantified by means of the linearization error covariance matrix .",
    "based on this error , a novel moment - preserving splitting procedure is proposed for introducing new mixture components .",
    "the component causing the highest linearization error is selected , while splitting is performed in direction of the strongest nonlinearity , i.e. , the strongest deviation between the nonlinear model and its linearized version .",
    "both linearization and splitting are independent of the used statistical linearization method , which makes the proposed filter versatilely applicable .",
    "the paper is structured as follows : the bayesian state estimation problem is formulated in the next section . in",
    ", a brief introduction in statistical linearization is given .",
    "the novel splitting scheme is derived in .",
    "based on this , describes the complete adaptive gaussian mixture filter with all major components .",
    "numerical evaluation by means of simulations is part of .",
    "the paper closes with concluding remarks .",
    "in this paper , discrete - time nonlinear dynamic systems @xmath0 are considered . here ,",
    "is the dynamics model with the known time - variant nonlinear system function @xmath1 , which propagates the system state @xmath2 at time step @xmath3 to time step @xmath4 , given the current system input @xmath5 and the process noise @xmath6 .",
    "the measurement model is given by , where @xmath7 is the known time - variant nonlinear measurement function , @xmath8 is the measurement vector , and @xmath9 is the measurement noise . note that an actual measurement value @xmath10 is a realization of the random vector @xmath11 in .    both noise processes @xmath12 and @xmath13 are assumed to be independent and white .",
    "the probability density functions of @xmath12 and @xmath13 are denoted by @xmath14 and @xmath15 , respectively .",
    "it is assumed that these density functions are described via _ gaussian mixtures _",
    "@xmath16 where @xmath17 , @xmath18 are the numbers of mixture components , @xmath19 , @xmath20 are non - negative weights that sum up to one , and @xmath21 is a gaussian density with mean vector @xmath22 and covariance matrix @xmath23 .",
    "the initial density function @xmath24 of the system state at time step @xmath25 is also assumed to be given as a gaussian mixture .    estimating the system state from noisy measurements is done according to the bayesian framework . here",
    ", two steps are performed alternately , namely the prediction step and the filtering step . in the prediction step ,",
    "the density @xmath26 of the previous filtering step is propagated to the next time step according to @xmath27 where @xmath28 denotes the measurements up to and including time step @xmath3 , @xmath29 is the transition density depending on the dynamics model , and @xmath30 is the .",
    "the filtering step determines the _ posterior density _",
    "@xmath31 of the system state @xmath32 based on all acquired measurement values according to _ bayes law _",
    "@xmath33 where @xmath34 is a normalization constant and @xmath35 is the likelihood function given by @xmath36 and the measurement model .    in general , for arbitrary nonlinear systems with arbitrarily distributed random vectors ,",
    "there exist no analytical solutions of the prediction step and filtering step .",
    "thus , for efficient estimation , it is inevitable to apply an approximate solution . in the following ,",
    "an adaptive approximation scheme is proposed , where the predicted and posterior state densities are represented by means of gaussian mixtures @xmath37 where the number @xmath38 of mixture components is variable and adapted on - line by the proposed gaussian mixture filter .",
    "substituting the gaussian mixtures representing the noise and the state density into the prediction step and the filtering step , it can be easily seen that estimation can be performed component - wise .",
    "for example in case of the prediction , using and with @xmath39 in gives rise to @xmath40 thus , it is sufficient to focus in the following on the simplified nonlinear transformation @xmath41 which maps the gaussian random vector @xmath42 with density @xmath43 to the random vector @xmath44 .",
    "this nonlinear transformation can be replaced by @xmath1 in the prediction step and by @xmath7 in the filtering step , while the gaussian random vector @xmath42 in represents the joint gaussian of the state and noise .      calculating the density or the statistics of @xmath44",
    "can not be carried out in closed form .",
    "hence , _ directly _ processing the density or the moments is computationally demanding and imprecise , or even impossible . an exception",
    "are linear transformations , where the kalman filter @xcite provides analytic expressions of the bayesian estimation problem .",
    "to apply the kalman filter equations to nonlinear transformations , a typical way is to linearize the nonlinear transformation , which results in the extended kalman filter @xcite . here",
    ", it is assumed that the nonlinear transformation can be approximated by a linear transformation through a first - order taylor series expansion around the mean @xmath45 . in case of mild nonlinearities",
    "the linearization error of this approximation is acceptable .",
    "however , for this type of linearization the spread of @xmath42 , i.e. , the covariance matrix @xmath46 is not taken into account and there is no measure which allows to quantify the linearization error .      to overcome these flaws ,",
    "deterministic sampling techniques are employed instead , which allow for propagating the mean and the covariance of @xmath42 through the nonlinear transformation . in doing so , linearizing",
    "the transformation by so - called statistical linear regression or _ statistical linearization _ is possible @xcite .",
    "more precisely , statistical linearization calculates a matrix @xmath47 and a vector @xmath48 such that @xmath49 where the error term @xmath50 describes the deviation of the nonlinear transformation and its linear approximation . to determine @xmath47 and @xmath48 ,",
    "the nonlinear transformation @xmath51 is evaluated at a set of weighted _ regression points _",
    "@xmath52 with non - negative weights @xmath53 with @xmath54 , which results in points @xmath55 for @xmath56 .",
    "this set of points is chosen in such a way that the mean @xmath45 and covariance @xmath46 of @xmath42 are captured exactly , that is @xmath57 then @xmath47 and @xmath48 are determined by minimizing the weighted sum of squared errors @xmath58 with @xmath59 .",
    "the solution of is given by @xmath60 where the set of propagated points @xmath61 is used to approximate the mean , covariance , and cross - covariance of @xmath44 according to @xmath62 the linearization error is characterized by the error term and has zero - mean and the covariance matrix @xmath63 thus , by means of the covariance @xmath64 it is possible to quantify the linearization error .",
    "if @xmath64 is a zero matrix , the density of the error @xmath65 corresponds to a @xcite and the transformation @xmath51 is affine with @xmath66 .",
    "many approaches for calculating the set of regression points have been proposed in the recent years .",
    "they differ in the number of regression points @xmath67 and the way these points are chosen . in the following example , both selection schemes used in this paper are briefly introduced .",
    "[ ex : regression ] in the simulations described in the famous _ unscented transform _",
    "@xcite and the _ gaussian estimator _",
    "@xcite are considered . for both , the calculation of the sigma points @xmath68 can be summarized as @xmath69 where @xmath70 , @xmath71 is the @xmath72th column of the matrix @xmath73 and @xmath74 , @xmath75 are scaling factors .",
    "this results in a number of @xmath76 regression points .",
    "the type of the matrix root , the scaling factors , and the weights @xmath53 of the regression points depend on the considered selection scheme .    in case of the unscented transform",
    ", the cholesky decomposition is chosen as matrix root .",
    "for the scaling factors holds @xmath77 and @xmath78 , where @xmath79 is a scaling parameter .",
    "the weights are @xmath80 and @xmath81 , for @xmath82 .",
    "the gaussian estimator utilizes the eigenvalue decomposition for calculating the matrix root .",
    "the number of scaling factors @xmath83 and thus the total number @xmath67 of regression points can be varied .",
    "since the scaling factors result from solving a optimization problem , there is no closed - form expression . for @xmath77 and @xmath84 ,",
    "the scaling factors can be calculated to @xmath85 the regression points are equally weighted with @xmath86 .",
    "it is important to note that the proposed adaptive gaussian mixture filter is not restricted to these two selection schemes .",
    "in fact , any selection scheme for statistical linearization including those described in @xcite can be used , depending on the considered application as well as the desired estimation performance and computational demand .",
    "given a random vector @xmath42 , whose density function @xmath87 is a gaussian mixture with @xmath88 components according to , it is possible to linearize the nonlinear transformation @xmath51 for each component of @xmath87 .",
    "this kind of component - wise or _ local linearization _ leads to an improved approximation of the true density function @xmath89 of @xmath44 compared to a single , global linearization . to further improve the approximation , especially in case of strong nonlinearities and/or large variances of some components ,",
    "the idea is to select a component of @xmath87 and split it into several components with reduced weights and covariances .",
    "it was demonstrated for example in @xcite that the filtering accuracy of local linearization approaches benefits from this decrease of the covariances and simultaneous increase of the number of gaussians .      a straightforward way to select a gaussian component for splitting is to consider the weights @xmath90 , @xmath91 .",
    "the component with the highest weight is then split .",
    "this however does not take the nonlinearity of @xmath51 in the support of the selected component into account . since linearization is performed component - wise and locally",
    ", a more reasonable selection would be to consider also the induced linearization error of each component .",
    "for this purpose , statistical linearization already provides an appropriate measure for the linearization error in form of the covariance matrix @xmath64 in .    in order to easily assess the linearization error in the multi - dimensional case , the trace operator is applied to @xmath64 , which gives the measure @xmath92 geometrically speaking ,",
    "the trace is proportional to circumference of the covariance ellipsoid corresponding to @xmath64 .",
    "the larger @xmath64 and thus the linearization error , the larger is @xmath93 .",
    "conversely , the trace is zero , if and only if @xmath64 is the zero matrix , i.e. , @xmath94 .",
    "hence , is only zero , when there is no linearization error , that is , the nonlinear transformation @xmath51 is affine in the support of the considered gaussian component .",
    "besides the linearization error , the contribution of a component to the nonlinear transformation is important as well .",
    "that is , the probability mass of the component , which is given by its weight @xmath95 , has also to be taken into account .",
    "this avoids splitting irrelevant components . putting all together the criterion for selecting a component @xmath96 for splitting",
    "is defined as @xmath97\\ ] ] for @xmath91 , where @xmath98 normalizes the linearization measure into the interval @xmath99 $ ] . for a geometric interpolation between weight and linearization error of component @xmath96 ,",
    "the parameter @xmath100 $ ] used .",
    "with @xmath101 , selecting a component for splitting only focuses on the linearization error , while @xmath102 considers the weight only .",
    "component selection criteria for splitting have also been proposed in @xcite .",
    "the criterion in @xcite is designed for the unscented transform only , while the criterion in @xcite can only be calculated analytically in some special cases .",
    "the proposed criterion instead is generally applicable .",
    "assume that according to the selection criterion , the gaussian component @xmath103 is chosen .",
    "splitting this gaussian into many can be formulated as replacing the gaussian by a gaussian mixture according to @xmath104 it can be easily verified that for @xmath105 , the number of free parameters , i.e. , weights , mean vectors and covariance matrices , is larger than the number of given parameters .",
    "more precisely , splitting a gaussian is an ill - posed problem . in order to reduce the degrees of freedom and to not introduce errors concerning the mean and covariance , splitting is performed in a moment - preserving fashion .",
    "thus , it must hold that @xmath106 to further simplify the problem , splitting is restricted in direction of the eigenvectors of @xmath46 , which is computationally cheap and numerically stable .",
    "furthermore , it reduces the problem to splitting a univariate standard gaussian .      a moment - preserving split of a univariate standard gaussian @xmath107 into a mixture with @xmath67 components requires to determine @xmath108 free parameters . by forcing symmetry , i.e. , the means",
    "@xmath109 are placed symmetrically around the mean @xmath110 with symmetrically chosen weights and for the variances holds @xmath111 , the number of free parameters is reduced to @xmath112 . in @xcite , a splitting library with symmetric components is proposed .",
    "unfortunately , preserving the moments is not guaranteed . instead ,",
    "the following split into two components is used throughout this paper .    following the approach proposed in @xcite , the univariate standard gaussian is split into the mixture @xmath113 .",
    "the moment - preserving constraints of splitting lead to the dependency @xmath114 between @xmath110 and @xmath115 , where @xmath110 is now the only free parameter .",
    "this equation is valid for @xmath116 $ ] and contains the trivial solution @xmath117 .",
    "generally , @xmath110 may be determined dynamically by minimizing the resulting linearization error .",
    "but throughout this paper , @xmath110 is set to @xmath118 for simplicity .    to determine the parameters of more than two components , additional constraints , e.g. , capturing higher order moments like the skewness or the kurtosis have to be considered additionally .",
    "since splitting is performed recursively in this paper ( see ) , the new introduced components can be split in the subsequent splitting step if the local linearization error may not be reduced sufficiently .",
    "splitting into two components is a good compromise between reducing the linearization error on the one hand and controlling the growth of the number of components and the computational load on the other  hand .",
    "applying univariate splitting to the multivariate case requires the eigenvalue decomposition of the covariance matrix @xmath119 , with @xmath120 being the matrix of eigenvectors and @xmath121 being the diagonal matrix of eigenvalues according to @xmath122 where @xmath123 are the ( orthonormal ) eigenvectors and @xmath124 are the eigenvalues .",
    "@xmath120 is a rotation matrix and the eigenvalue decomposition of @xmath46 corresponds to the transformation @xmath125 of a gaussian random vector @xmath126 with density @xmath127 to a gaussian random vector @xmath42 with density @xmath128 .",
    "since the gaussian @xmath129 has a diagonal covariance matrix , the eigenvectors are parallel to the axes of the coordinate system .",
    "thus , univariate splitting can be easily applied along the eigenvectors by replacing a univariate gaussian on the right - hand side of by a gaussian mixture .",
    "assuming that eigenvector @xmath130 is chosen for splitting and let @xmath131 be the gaussian mixture that approximates a univariate standard gaussian as described above . as this mixture approximates a standard gaussian , its components have to be shifted by adding @xmath132 and scaled by multiplying with @xmath133 in order to match the mean @xmath132 and the variance @xmath134 , respectively .",
    "these operations result in @xmath135 plugging into leads to @xmath136 transforming this mixture via gives the desired splitting result with the weights , means , and covariance matrices @xmath137 for @xmath138 .",
    "it is worth mentioning that the calculation of the parameters in is independent of the number of components @xmath67 and does not necessarily require a symmetric , moment - preserving splitting .",
    "thus , arbitrary splitting methods of univariate standard gaussians besides those described in this paper , can be used with these formulae .",
    "so far , no criterion for selecting an appropriate eigenvector for splitting is defined .",
    "a straightforward criterion may be the eigenvector with the largest eigenvalue as in @xcite .",
    "but since determines the gaussian component that causes the largest linearization error , merely splitting along the eigenvector with the largest eigenvalue does not take this error into account .",
    "the key idea of the proposed criterion is to evaluate the deviation between the nonlinear transformation and its linearized version along each eigenvector .",
    "the eigenvector with the largest deviation is then considered for splitting , i.e. , the gaussian is split in direction of the largest deviation in order to cover this direction with more gaussians , which will reduce the error in subsequent linearization steps .    by means of the error term , the desired criterion for the splitting direction is defined as @xmath139 with @xmath140 , @xmath71 , and @xmath130 being the @xmath72th eigenvector @xmath46 .",
    "the integral in cumulates the squared deviations along the @xmath72th eigenvector under the consideration of the probability at each point @xmath141 .",
    "the eigenvector that maximizes is then chosen for splitting .",
    "unfortunately , due to the nonlinear transformation @xmath51 in , this integral can not be solved in closed - form in general . for an efficient and approximate solution ,",
    "the regression point calculation schemes described in are employed to approximate the gaussian in in direction of @xmath130 by means of a dirac mixture .",
    "this automatically leads to a discretization of the integral at a few but carefully chosen points .    at ( -.1,0 ) ( linp ) linearization ; at ( 2,0 ) ( stopp ) stop ? ; at ( 2,1.5 ) ( splitp ) splitting ; at ( 4,0 ) ( pred ) prediction ; at ( 6,0 ) ( redp ) reduction ;    at ( -.3,-3 ) ( rede ) reduction ; at ( 1.7,-3 ) ( filt ) filtering ; at ( 3.7,-1.5 ) ( splite ) splitting ; at ( 3.7,-3 ) ( stope ) stop ? ; at ( 5.8,-3 ) ( line ) linearization ;    at ( 6,-1.55 ) ( delay ) @xmath142 ;    ( linp )  ( stopp ) ; ( stopp )  ( splitp ) node[right , near start ] ( ) ; ( stopp )  ( pred ) node[above , near start ] ( ) ; ( pred )  ( redp ) ; ( linp.128 ) |- ( splitp ) ; ( redp )  ( redp |- delay.north ) node[right , pos=.57 ] ( ) @xmath143 ;    ( line )  ( stope ) ; ( filt )  ( rede ) ; ( stope )  ( filt ) node[above , at start ] ( ) ; ( stope )  ( splite ) node[right , near start ] ( ) ; ( splite ) -| ( line.160 ) ; ( rede ) ",
    "( rede |- linp.south ) node[right , pos=.75 ] ( ) @xmath144 ; ( delay )  ( delay |- line.north ) node[right , pos=.4 ] ( ) @xmath145 ; ( linp.west )  + ( -.5,0 ) node[above , near end ] ( ) @xmath146 ; ( filt.north )  + ( 0,.4 )",
    "node[above , pos=.8 ] ( ) @xmath10 ;    at ( 5,1.5 ) ( ) _ prediction step _ ; at ( 1.5,-1.5 ) ( ) _ filtering step _ ; ( prediction ) ; ( prediction ) ;",
    "based on the statistical linearization described in and the splitting procedure proposed in , the complete adaptive gaussian mixture filter ( agmf ) is now derived .",
    "the key idea of agmf is to dynamically increase the number of gaussians of a given mixture at regions with large linearization errors .",
    "the number is reduced after each prediction and filtering in order to limit the computational and memory demand .",
    "the major operations to be performed in the prediction step are illustrated in the upper part of .",
    "the following paragraphs provide detailed descriptions of these operations .",
    "as shown in the prediction step can be performed component - wise .",
    "therefore , the nonlinear system function is linearized statistically at the weighted joint gaussian @xmath147 with @xmath148\\t$ ] , @xmath149 , @xmath150 , and @xmath151 with , the linearization results in @xmath152      due to the nonlinearity of the system function @xmath1 , some of the mixture components @xmath147 may locally cause severe linearization errors .",
    "these errors are quantified by means of the selection criterion .",
    "the component maximizing will be split in direction of the largest deviation between the nonlinear function @xmath1 and its linearized version as described in . after splitting this gaussian ,",
    "linearization is performed for the newly introduced mixture components .",
    "the linearization need not to be repeated for the remaining mixture components as they are not affected by the splitting .",
    "splitting gaussians and the subsequent linearization is repeated until a stopping criterion is satisfied .",
    "this stopping criterion combines three user - defined thresholds :    1 .   for _ each _ component the value of the selection criterion shall drop below the _ error threshold _",
    "@xmath153 $ ] .",
    "2 .   the number of gaussians shall not grow beyond the _ component threshold _ @xmath154 .",
    "3 .   the deviation between the original gaussian mixture @xmath155 and the mixture obtained via splitting @xmath156 shall remain below a _ deviation threshold _",
    "@xmath157 $ ] .    in the latter case ,",
    "the deviation is determined by means of the normalized integral squared distance measure  @xcite @xmath158~.\\ ] ] since splitting always introduces an approximation error to the original mixture @xmath155 , tracking the deviation during splitting and keeping the deviation below the threshold @xmath159 avoids that errors introduced by splitting neutralize the gain in linearization .",
    "splitting stops , if at least one threshold is reached .",
    "let @xmath160 be the gaussians resulting from the splitting step , with @xmath161 and @xmath162 . based on these gaussians and their corresponding locally linearized system models ,",
    "the parameters of each component of the predicted gaussian mixture @xmath163 can be calculated by means of the kalman predictor according to @xmath164 where @xmath165 is the linearization error covariance .",
    "the number of components @xmath166 in @xmath163 grows due to the multiplication of the gaussian mixtures @xmath31 and @xmath14 for prediction and due to splitting .",
    "it is necessary to bound this growth in order to reduce the computational and memory demand of subsequent prediction and filtering steps .",
    "for this purpose , one can exploit the redundancy and similarity of gaussian components .",
    "furthermore , many components will have weights close to zero , thus they can be removed without introducing significant errors . to reduce a gaussian mixture ,",
    "many algorithms have been proposed in the recent years ( see for example @xcite ) .",
    "most of these algorithms require a _ reduction threshold _ @xmath167typically much smaller than @xmath154to which the number of components of the given gaussian mixture has to be reduced . in the simulations ,",
    "runnalls reduction algorithm @xcite is employed as it provides a good trade - off between computational demand and reduction errors .    with the reduction to @xmath167 components",
    ", the calculation of the predicted gaussian mixture @xmath163 in is finished .",
    "the operations to be performed for the filtering step are almost identical to the prediction step ( see ) .",
    "thus , only linearization and filtering are described in the following .",
    "splitting and reduction coincide with the prediction step .",
    "linearization and filtering are also performed component - wise .",
    "let @xmath168 be the joint gaussian comprising the @xmath96th component of the predicted mixture @xmath169 and the @xmath170th component of the measurement noise mixture , where @xmath171 .",
    "the corresponding linearized measurement model is@xmath172 with joint state @xmath173 .",
    "let @xmath160 be the gaussians resulting from splitting , with @xmath174 and @xmath175 .",
    "given the current measurement value @xmath10 , the kalman filter update equations applied on these gaussians and their corresponding locally linearized measurement models give rise to the parameters of each component of the posterior gaussian mixture @xmath176 @xmath177 with predicted measurement @xmath178 , kalman gain @xmath179 , innovation covariance @xmath180 , and @xmath165 being the linearization error covariance .",
    "the calculation of the weight @xmath181 in is adapted from @xcite , where @xmath182 is a normalization constant .",
    "after the reduction to @xmath183 components , the posterior gaussian mixture @xmath176 in is completely determined .",
    ".approximation error ( kld @xmath184 @xmath185 ) for different splitting schemes and numbers of components . [ cols=\"^,^,^,^,^,^,^,^ \" , ]",
    "two numerical simulations are conducted in order to demonstrate the performance of the proposed agmf .      in the first simulation , the nonlinear growth process @xmath186 adapted from @xcite",
    "is considered , where @xmath187\\t \\sim f^x(\\vx ) = { { \\mathcal{n}}}(\\vx ; [ 1 , 0]\\t , \\i_2)$ ] with @xmath188 being the @xmath189 identity matrix . to approximate the density of @xmath190 , the gaussian @xmath87 is split recursively into a gaussian mixture , where the number of components is always doubled until a maximum of @xmath191 components is reached .",
    "no mixture reduction and no thresholds @xmath192 , @xmath159 are used .",
    "the gaussian estimator with @xmath193 scaling factors according to is employed for statistical linearization .",
    "the true density of @xmath190 is calculated via numerical integration .",
    "two different values for the parameter @xmath194 of the selection criterion are used : @xmath195 , which makes no preference between the component weight and the linearization error and @xmath102 , which considers the weight only .",
    "furthermore , a rather simple selection criterion is considered for comparison , where selecting a gaussian for splitting is based on the weights only ( as it is the case for @xmath102 ) , while the splitting is performed in direction of the eigenvector with the largest eigenvalue .",
    "shows the kullback - leibler divergence ( kld , @xcite ) between the true density of @xmath190 and the approximations obtained by splitting .",
    "the approximations of the proposed splitting scheme are significantly better than the approximations of the largest eigenvalue scheme .",
    "this follows from the fact that the proposed scheme not only considers the spread of a component .",
    "it also takes the linearization errors into account . in doing so , the gaussians are always split along the eigenvector that is closest to @xmath196 , since this variable is transformed nonlinearly , while @xmath197 is not .",
    "this is different for the largest eigenvalue scheme , which wastes nearly half of the splits on @xmath197 .",
    "the inferior approximation quality for @xmath102 compared to @xmath195 results from splitting components , which may have a high importance due to their weight but which do not cause severe linearization errors .",
    "thus , splitting these components will not improve the approximation quality much .",
    "( black , dashed ) and approximations with an increasing number of mixture components . ]    in , the approximate density of @xmath190 is depicted for different numbers of mixture components for @xmath195 . with an increasing number of components , the approximation approaches the true density very well .          for the second simulation example",
    ", a object tracking scenario is considered .",
    "the kinematics of the mobile object are modeled by means of the bicycle model @xmath198 where the system state @xmath199 comprises the position @xmath200\\t$ ] and the orientation @xmath201 of the bicycle . at time",
    "step @xmath25 , the initial estimate of the state @xmath202 is represented by a gaussian density with mean @xmath203\\t$ ] and covariance matrix @xmath204)$ ] .",
    "the system input @xmath205 with @xmath206 being the steering angle , is chosen randomly and uniformly distributed from the interval @xmath207 $ ] at each time step .",
    "the system noise @xmath12 is zero - mean gaussian with covariance matrix @xmath208 .    a radar sensor with measurement model @xmath209 is employed for observing the object , where the measurement noise @xmath13 is modeled as unimodal glint noise @xcite with density @xmath210 with covariances @xmath211 and @xmath212 .",
    "the parameter @xmath213 refers to the glint noise probability .",
    "six probability values @xmath214 are exploited for simulation . by increasing @xmath213",
    "it is possible to investigate the performance of the filters for stronger noise , which is also heavily tailed for @xmath215 and @xmath216 .    for this simulation setup",
    ", agmf is applied with parameter @xmath217 , error threshold @xmath218 , deviation threshold @xmath219 , and component threshold @xmath220 for both prediction and filtering .",
    "three values of reduction thresholds are used , @xmath221 . for comparison ,",
    "a gaussian mixture filter ( denoted as mwe ) employing the simple largest - weight - largest - eigenvalue - criterion as described in the previous section is considered .",
    "further , the adaptive level of detail ( ald ) gaussian mixture filter proposed in @xcite is employed as well .",
    "since ald is only designed for the unscented transform ( see example  [ ex : regression ] ) , this statistical linearization method is also used for agmf to allow a fair comparison .",
    "the scaling parameter @xmath79 of the unscented transform is set to @xmath118 , i.e. , all regression points are equally weighted .",
    "mwe and ald use the same parameters as agmf , except that mwe always splits until @xmath154 is reached since it exploits no linearization errors .",
    "besides these gaussian mixture filters , a particle filter ( pf ) with residual resampling and @xmath222 samples as well as the unscented kalman filter ( ukf ,  @xcite ) with @xmath223 are also applied .    for each glint probability and each reduction threshold , @xmath224 monte carlo simulation runs are performed , where the object is observed for @xmath225 time steps . in ( a ) , the average root means square error ( rmse ) of the position and the average runtime per simulation run are depicted .",
    "the agmfs with @xmath226 and @xmath227 components provide the best tracking performance .",
    "the pf is close to agmf , but with a significantly higher runtime .",
    "conversely , the ukf is by far the fastest algorithm , but leads to diverging estimates .",
    "the splitting criterion used for ald selects components that exhibit a high degree of nonlinearity .",
    "but splitting is performed merely in direction of the largest eigenvalue .",
    "this explains the relative poor tracking performance of ald .",
    "even if mwe is allowed to split until @xmath154 is reached , the performance of mwe is always inferior to agmf .",
    "this is due to wasting many splits , e.g. , in the prediction step only one quarter and less of the splits is used for @xmath201 , which is the only nonlinearly transformed variable . here",
    ", agmf is much more effective thanks to the novel splitting criterion . besides splitting mainly in direction of the nonlinearity",
    ", it does not require all available splits as shown in ( b ) .",
    "the maximum number of splits is @xmath228 in the prediction step and analogously in the filtering step .",
    "but at most @xmath229 splits are performed in case of the strongest noise and when the state mixture is reduced to two components . if more components are allowed to represent the state density , the number of splits decreases as the approximation before splitting is already of high quality .",
    "this also reduces the runtime as can be seen when comparing for example agmf  32 with agmf  2 . here",
    ", the time consuming splitting operation has to be performed less often and the reduction operation has to reduce a mixture with an already low number of components .",
    "in this paper , a novel adaptive gaussian mixture filter has been proposed .",
    "it is based on statistical linearization , which allows quantifying the induced linearization errors in terms of a linearization error covariance matrix . a criterion based on this covariance matrix",
    "is used for selecting gaussian components for splitting , while the direction of the split is performed in direction of the eigenvalue with the strongest linearization errors .",
    "compared to other splitting criteria , the proposed one reliably detects strong nonlinearities and keeps the number of splits on a low level .",
    "furthermore , arbitrary approaches for statistical linearization can be employed .",
    "m.  s. arulampalam , s.  maskell , n.  gordon , and t.  clapp , `` a tutorial on particle filters for online nonlinear / non - gaussian bayesian tracking , '' _ ieee transactions on signal processing _ , vol .  50 , no .  2 , pp . 174188 , feb .",
    "2002 .",
    "m.  f. huber and u.  d. hanebeck , `` gaussian filter based on deterministic sampling for high quality nonlinear estimation , '' in _ proceedings of the 17th ifac world congress _ ,",
    "seoul , republic of korea , jul .",
    "2008 .",
    "t.  vercauteren and x.  wang , `` decentralized sigma - point information filters for target tracking in collaborative sensor networks , '' _ ieee transactions on signal processing _ , vol .",
    "53 , no .  8 , pp . 29973009 , 2005 .",
    "f.  faubel and d.  klakow , `` an adaptive level of detail approach to nonlinear estimation , '' in _ proceedings of the 2010 ieee international conference on acoustics , speech and signal processing ( icassp ) _ , 2010 , pp .",
    "39583961 .",
    "a.  rauh , k.  briechle , and u.  d. hanebeck , `` nonlinear measurement update and prediction : prior density splitting mixture estimator , '' in _ proceedings of the 2009 ieee international conference on control applications ( cca ) _ , jul .",
    "m.  f. huber , t.  bailey , h.  durrant - whyte , and u.  d. hanebeck , `` on entropy approximation for gaussian mixture random vectors , '' in _ proceedings of the 2008 ieee international conference on multisensor fusion and integration for intelligent systems ( mfi ) _ , seoul , republic of korea , aug .",
    "2008 , pp . 181188 ."
  ],
  "abstract_text": [
    "<S> gaussian mixtures are a common density representation in nonlinear , non - gaussian bayesian state estimation . selecting an appropriate number of gaussian components , </S>",
    "<S> however , is difficult as one has to trade of computational complexity against estimation accuracy . in this paper , an adaptive gaussian mixture filter based on statistical linearization </S>",
    "<S> is proposed . </S>",
    "<S> depending on the nonlinearity of the considered estimation problem , this filter dynamically increases the number of components via splitting . for this purpose </S>",
    "<S> , a measure is introduced that allows for quantifying the locally induced linearization error at each gaussian mixture component . the deviation between the nonlinear and the linearized state space model </S>",
    "<S> is evaluated for determining the splitting direction . </S>",
    "<S> the proposed approach is not restricted to a specific statistical linearization method . </S>",
    "<S> simulations show the superior estimation performance compared to related approaches and common filtering algorithms .    </S>",
    "<S> = [ rectangle , rounded corners , draw = black , thick , text centered ] = [ blockdef , inner sep=4pt , text width=1.3 cm ] = [ shape = diamond , draw = black , thick , text centered , aspect=1.5 ] = [ blockdef , circle , inner sep=2pt , text width=2 mm ] = [ thick , -latex , black ] = [ ultra thick , latex - latex , black ] = [ draw = black , fill ] = [ blockdef , fill = none , dashed , inner sep = 1 mm ]    = [ anchor = base , rounded corners=3pt ] = [ anchor = base , rounded corners=3pt , thick ] = [ hlframenone , fill = blockblue , draw = hilightblue ] = [ hlframenone , fill = blockred , draw = hilightred ] = [ hlnone , fill = blockblue ] = [ hlnone , fill = blockgreen ] = [ hlnone , fill = blockred ]    * keywords : bayesian estimation , nonlinear filtering , statistical linearization , kalman filtering , gaussian mixtures . * </S>"
  ]
}