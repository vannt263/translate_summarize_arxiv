{
  "article_text": [
    "until now , the primary way that stars and galaxies have been classified in large sky surveys has been a morphological separation ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) of point sources ( presumably stars ) from resolved sources ( presumably galaxies ) . at bright apparent magnitudes",
    ", relatively few galaxies will contaminate a point source catalog and relatively few stars will contaminate a resolved source catalog , making morphology a sufficient metric for classification .",
    "however , resolved stellar science in the current and next generation of wide - field , ground - based surveys is being challenged by the vast number of unresolved galaxies at faint apparent magnitudes .    to demonstrate this challenge for studies of field stars in the milky way ( mw ) , we compare the number of stars to the number of unresolved galaxies at faint apparent magnitudes .",
    "figure  [ fig : stellarfraction ] shows the fraction of cosmos sources that are classified as stars as a function of @xmath7 magnitude and angular size .",
    "the cosmos catalog ( ( @xmath8 ) @xmath9 ( 237,43 ) degrees , * ? ? ?",
    "? * ; * ? ? ?",
    "* ) relies on 30-band photometry plus hst / acs morphology for source classification ( see section 4 for details ) . in figure",
    "[ fig : stellarfraction ] we plot separately relatively bluer ( @xmath10 ) and redder ( @xmath11 ) sources because bluer stars are representative of the old , metal - poor main sequence turnoff ( msto ) stars generally used to trace the mw s halo while redder stars are representative of the intrinsically fainter red dwarf stars generally used to trace the mw s disk .",
    "we will see that the effect of unresolved galaxies on these two populations is different , both because of galaxy demographics and because the number density of halo msto stars decreases at faint magnitudes while the number density of disk red dwarf stars increases at faint magnitudes .    in an optimistic scenario in which galaxies with fwhm @xmath12  arcsec can be morphologically resolved ( the blue line in figure  [ fig : stellarfraction ] , second from the top ) ,",
    "unresolved galaxies will still greatly outnumber field mw stars in a point source catalog . for studies of blue stars ,",
    "field star counts are dominated by unresolved galaxies by @xmath13 and are devastated by unresolved galaxies at fainter magnitudes .",
    "the problem is far less severe for studies of red stars , which may dominate point source counts for @xmath14 .",
    "although morphological identification of galaxies with fwhm as small as @xmath15  arcsec is better than possible for the sloan digital sky survey ( median seeing @xmath16  arcsec ) , future surveys with higher median image quality ( for example , @xmath17  arcsec predicted for lsst ) may approach this limit .    utilizing the fundamental differences between seds of stars and galaxies",
    "can mitigate the contamination of unresolved galaxies in point source catalogs .",
    "in general , stellar seds are more sharply peaked ( close to blackbody ) than galaxies , which exhibit fluxes more broadly distributed across wavelength .",
    "traditionally , color - color cuts have been used to eliminate galaxies from point source catalogs ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "advantages of the color - color approach include its simple implementation and its flexibility to be tailored to the goals of individual studies .",
    "disadvantages of this approach can include its simplistic treatment of measurement uncertainties and its limited use of information about both populations expected demographics .",
    "probabilistic algorithms offer a more general and informative approach to photometric classification .",
    "the goal of probabilistic photometric classification of an astronomical source is to use its observed fluxes @xmath18 to compute the probability that the object is of a given type .",
    "for example , a star ( @xmath19 ) galaxy ( @xmath20 ) classification algorithm produces the posterior probabilities @xmath21 and @xmath22 and decides classification by comparing the ratio of the probabilities @xmath23 a natural classification threshold is an odds ratio , @xmath24 , of 1 , which may be modified to obtain more pure or more complete samples .",
    "algorithmically there are a large number of approaches which produce probabilistic classifications .",
    "generally , these fall into i ) physically based methods  those which have theoretical or empirical models for what type of physical object a source is , or ii ) data driven methods  those which use real data with known classifications to construct a model for new data .",
    "physically based bayesian and @xmath25 template fitting methods have been extensively used to infer the properties of galaxies ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "however , in those studies relatively little attention has been paid to stars which contribute marginally to overall source counts ( although see @xcite ) .",
    "several groups have recently investigated data driven , support vector machine based star  galaxy separation algorithms ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) .    in this paper",
    ", we describe , test , and compare two physically based template fitting approaches to star  galaxy separation ( maximum - likelihood and hierarchical bayesian ) , and one data driven ( support vector machine ) approach . in section 2",
    ", we present the conceptual basis for each of the three methods . in section [ sec : data ] , we describe the cosmos data set with which we test the algorithms . in section [ sec : specifics ] , we discuss the specific details , choices , and assumptions made for each of our classification methods .",
    "finally , in section [ sec : results ] we show the performance of the algorithms , and discuss the advantages and limitations related to their use as classifiers .",
    "one common method for inferring a source s properties from observed fluxes is template fitting .",
    "this method requires a set of spectral templates ( empirical or theoretical ) that span the possible spectral energy distributions ( seds ) of observed sources .",
    "these template seds must each cover the full wavelength range spanned by the photometric filters used to measure the fluxes to be fit . the relative template flux in each filter ( for example @xmath4 ) for each sed",
    "is computed by convolving each sed with each filter response curve .",
    "once these relative flux values are computed for each sed template , the template model is fully specified except for a normalization constant @xmath26 .",
    "for a given observed source @xmath27 , the value of @xmath28 is proportional to the total luminosity of the source divided by the luminosity distance squared .",
    "this value of @xmath28 is unknown but can be ` fit ' to the data .",
    "the maximum likelihood ( ml ) value of @xmath28 for each template that best fits a source s observed fluxes , @xmath18 , is that which returns the lowest @xmath25 . after assessing the ml values of @xmath28 for all the templates ,",
    "classification is straightforward  one need only to compare the lowest star @xmath25 to the lowest galaxy @xmath25 . in other words",
    ", @xmath29 is the classification criteria ( see equation [ eqn : oddsratio ] ) .",
    "hierarchical bayesian ( hb ) algorithms provide another template fitting - based approach to photometric classification .",
    "unlike ml approaches , bayesian approaches offer the opportunity to utilize information about how likely a source is to be each kind of star or galaxy ; the different templates are not treated as equal _ a priori_. with a hierarchical bayesian algorithm , individual source prior probabilities",
    "do not need to be set in advance of the full - sample classification process ; the entire sample of sources can inform the prior probabilities for each individual source .",
    "consider the scenario where a @xmath20 model fits data @xmath30 only _ slightly _ better than the best @xmath19 model , while all other @xmath20 models give poor fits and all other @xmath19 models give nearly as likely fits . in this case , ignoring all other @xmath19 models besides the best is the wrong thing to do , since the data are stating that @xmath19 models are _ generally _ more favored . capturing this kind of information is one primary aim of most bayesian algorithms .    to capture this information , we _ marginalize _ over all possible star and galaxy templates to compute the total probability that a source belongs to a certain classification ( @xmath19 or @xmath20 ) .",
    "for a template fitting - based bayesian algorithm , this marginalization consists of summing up the likelihood of each @xmath19 template given @xmath30 , as well as the likelihood of each @xmath20 template ( across redshift ) .",
    "note that the likelihood of each template is itself calculated as a marginalized likelihood .",
    "for each template fit , we compute the total likelihood of the fit by marginalizing over the uncertainty in fitting coefficient @xmath28 .",
    "this marginalization is the total probability of a gaussian distribution with variance @xmath31a value which is returned using least squares fitting techniques ( e.g. , * ? ? ?",
    "* ) .    by bayes theorem",
    ", marginalization requires we specify the prior probability that any object might have a given sed template ( at a given redshift ) .",
    "the prior probability distributions might be chosen to be uninformative ( for example , flat ) , informed by knowledge from outside studies , or informed by the data on all the other objects .",
    "the latter approach , referred to as a hierarchical model , is widely used in statistical data analysis ( e.g. , * ? ? ? * ) and is beginning to be used in astronomy @xcite .",
    "the benefits of hierarchical approaches are many  because every inference is informed by every datum in the data set , they generally show improved probabilistic performance over simpler approaches , while requiring no additional knowledge outside the observed data and the template seds . functionally , hierarchical approaches consist of parameterizing the prior probability distributions ( for example , with the mean and variance of a normal distribution ) , and varying these parameters ( known as `` hyperparameters '' ) to determine the probability of _ all _ the data under _ all _ the models .    for our work ,",
    "we optimize the hyperparameters of the sed template prior distributions to return the maximum marginalized likelihood of all the data .",
    "this procedure will enable us to simultaneously infer the star ",
    "galaxy probability of each source while determining the hyperparameters that maximize the likelihood of the observed dataset . a brief description of the functional form of these priors is given below in section  [ ssec : hbspecifics ] .",
    "although we focus on the star ",
    "galaxy probabilities in this paper , the optimized hyperparameters themselves yield a measurement of the detailed demographics of a dataset .      a support vector machine ( svm ) is a type of machine learning algorithm particularly well suited to the problem of classification .",
    "svm algorithms are frequently used in non - astronomical problems , and are considered a gold standard against which to compare any new classification method .",
    "svm algorithms are `` supervised '' , meaning they train on a catalog of objects with known classifications to learn the high dimensional boundary that best separates two or more classes of objects . for classification problems which do not separate perfectly , svms account for misclassification errors by looking at the degree of misclassification , weighted by a user specified error penalty parameter . in",
    "general the optimal boundary need not be restricted to a linear hyperplane , but is allowed to be non - linear and so can require a very large number of parameters to specify the boundary . in order for non - linear svm classification to be computationally feasible , a kernel function is used to map the problem to a lower dimensional feature space @xcite .    for the case of star ",
    "galaxy separation based on broad band photometry , the svm algorithm learns the boundary which best separates the observed colors and apparent magnitudes magnitude here .",
    "] of stars and galaxies . for more details on the svm technique",
    ", please see @xcite .",
    "successful implementation of a svm algorithm requires a training dataset that is a sufficient analog to the dataset to be classified .",
    "a svm has recently been applied to source classification in the pan - starrs 1 photometric pipeline @xcite , with promising initial results .",
    "however , these results were obtained based on analysis of bright , high signal - to - noise data ( @xmath32 ) , using training data which is a subset of the data itself .    to investigate the impact of training set quality and demographics on the problem of star  galaxy separation , we will consider the utility and performance of svm algorithms in a new classification regime , where the data is of lower signal to noise ( described in section [ sec : data ] ) , and the number of unresolved galaxies is comparable to or larger than the number of stars .",
    "to investigate the advantages and disadvantages of star  galaxy classification techniques , we need a test catalog which has a large number of sources , is well understood and calibrated , and for which spectroscopy or multi - wavelength observations reveal the true source classifications .",
    "in addition , we want these data to be magnitude limited as faint as @xmath33 in order to understand the problem of classification in current and upcoming surveys like pan - starrs 1 , des , and lsst .",
    "the cosmos catalog satisfies these requirements .",
    "the http://cosmos.astro.caltech.edu/[cosmos ] survey @xcite covers @xmath9 2 square degrees on the sky using 30 band photometry , and is magnitude limited down to @xmath34 .",
    "broadband @xmath35 photometry exists down to limiting magnitudes which complement the @xmath7 limiting magnitude , and _ spitzer _",
    "irac coverage exist for sources as faint as @xmath36 @xcite .",
    "in addition , _ galex _ and _ xmm _ coverage are of sufficient depth to pick out relatively bright star - forming galaxies and agn @xcite . the spectral coverage beyond the optical , particularly the near - infrared , can be a powerful discriminator between star and galaxy classification .",
    "for instance , @xcite show the @xmath37 vs. @xmath38 colors cleanly separate star and galaxy loci , since stars have systematically lower @xmath39 colors .",
    "in addition to 30 band photometry , the cosmos field has _",
    "hst / acs _ @xmath40band coverage , down to a limiting magnitude of @xmath41 @xcite .",
    "diffraction limited _ hst _ imaging allows the morphological discrimination of point - like and extended sources , further strengthening the fidelity of the cosmos star  galaxy classification .",
    "we follow the cosmos team s star ",
    "galaxy classification criteria in order to determine the ` true ' classification for the purpose of testing our methods . these consist of a @xmath25 classification from fitting star and galaxy templates to the 30 band photometry , and a morphological classification using the acs_mu_class statistic derived by the analysis of the _ hst _ photometry by @xcite .",
    "we label cosmos sources as stars if acs_mu_class says the source is pointlike , and the ` star ' @xmath25 is lower than that for ` agn / qso ' . for galaxies",
    ", we require the source to have a non - pointlike acs_mu_class .",
    "this classification assumes that all galaxies in the _ hst _ images are resolved .",
    "we view this as an excellent approximation of the truth  cosmos _ acs _ images are very deep ( @xmath41 ) , and can thus detect the faint extended features of nearly unresolved galaxies . we have qualitatively confirmed this by examining the distribution of galaxy fwhm , and find the distribution to be smoothly decreasing down to the smallest fwhm in the data .",
    "we estimate the number of galaxies labeled as stars to be below the few - percent level . for the labeling ,",
    "we use an updated version of the publicly available photometric catalog , provided by p. capak ( private communication ) . while present in the catalog",
    ", we do not use any photometric redshift information in determining the classification of cosmos sources .    throughout this paper ,",
    "we restrict our analysis to sources likely to be unresolved in ground based data ( fwhm@xmath42 @xmath43  arcsec ) .",
    "we do so since commonly used morphological classification criteria will easily distinguish quite extended sources , accounting for a majority of galaxies to depths of @xmath44 .",
    "however , galaxies with angular sizes @xmath43  arcsec are unlikely to be resolved in surveys with seeing @xmath45  arcsec , and so are an appropriate test bed for the type of sources which will rely the most on photometric star  galaxy separation . in total , our sample consists of 7139 stars and 9167 galaxies with apparent magnitudes @xmath46 , and is plotted in _",
    "ugriz _ color  color space in figure [ fig : color - color - data ] . over this magnitude range , the median signal - to - noise in the @xmath7 band ranges from @xmath47 at @xmath48 to @xmath49 at @xmath50 , with lower corresponding ranges of 10 to 7 in the @xmath51 . of all 18606 sources with",
    "fwhm@xmath1  arcsec in the cosmos catalog , we identified 2300 agn , which we discard from our current analysis .",
    "in this section , we describe our implementation of ml template fitting , hb template fitting , and a svm on the @xmath4 photometry of cosmos sources for purposes of star  galaxy classification .",
    "template based star ",
    "galaxy classification relies on the use of spectral energy distribution templates which ( as well as possible ) span the space of colors for both stars and galaxies . for our stellar model library ,",
    "we first adopt the @xcite set of empirically derived seds , which span o to m type stars for both main sequence , giant , and supergiant stars .",
    "the vast majority of the seds in the pickles library have solar abundances , so we supplement the library with theoretical seds from castelli - kurucz ( ck ) @xcite .",
    "we use ck models with abundances ranging from @xmath52[fe / h]@xmath53 , surface gravities ranging from @xmath54 , and effective temperatures from @xmath55k .",
    "we include binary star templates by combining like - metallicity templates using flux calibrated ck models .",
    "finally , we include sdss m9 through l0 dwarf templates provided by j. j. bochanski ( private communication ) .",
    "these templates have been extended from the templates of @xcite into the near infrared , but lack data for wavelengths shorter than @xmath56 .",
    "we extend these templates down to the @xmath57 using a main sequence ck model with @xmath58k .",
    "details of this extension are likely to be unimportant , since the flux of such stars between @xmath59 is negligible .",
    "our final combined library of stellar templates includes 131 from the pickles library , 256 from the ck library , 11 from @xcite , and 1319 binary templates constructed from the ck library , for a total of 1717 stellar templates .",
    "we select for our galaxy templates those used by the cosmos team , described in @xcite , provided publicly through the http://www.cfht.hawaii.edu/%7earnouts/lephare/lephare.html[`le phare ` ] photometric redshift package @xcite .",
    "these templates consist of galaxy seds from @xcite , encompassing 7 elliptical and 12 spiral ( s0-sd ) seds .",
    "additionally , 12 representative starburst seds are included , which were added by @xcite to provide a more extensive range of blue colors .",
    "templates from @xcite include effects of dust extinction , since they were selected to fit spectral sources in the vimos vlt deep survey @xcite .",
    "we do not consider any additional dust extinction beyond these fiducial templates . in order to model our galaxies across cosmic time , we redshift these templates on a discrete linear grid of redshifts , ranging from 0 to 4 in steps of 0.08 .",
    "simple tests using the ml procedure indicate small changes to the step size of our grid are unimportant .    for all of the above templates ,",
    "model fluxes were constructed by integrating the sed flux density values with the throughput response curves for each filter .",
    "these consist of a @xmath60 response curve for the observations taken by the canada - france - hawaii telescope , and @xmath61 , @xmath62 , @xmath63 , @xmath64 response curves for data collected by the subaru telescope .",
    "we obtained the same response curves used by @xcite through http://www.cfht.hawaii.edu/%7earnouts/lephare/lephare.html[`le phare ` ] . to check for any mismatch between the data , calibrations , and/or response curves",
    ", we verified that model colors generated from the seds overlap well with the star and galaxy loci .      while the hb template fitting technique builds on the foundation described in section  [ ssec : mlspecifics ] , the details of star ",
    "galaxy inference require significantly more mathematical formalism to thoroughly describe .",
    "we present the details of this formalism and a detailed , step - by - step description of our hb inferential procedure in appendix a. open - source ` c ` code is available at ` http://github.com/rossfadely/star-galaxy-classification ` . in this section",
    ", we qualitatively describe features specific to our hb algorithm .",
    "we emphasize that hierarchical bayesian algorithms are unsupervised : we use no training set and do not set priors in advance of running the algorithms .",
    "as described in section [ ssec : hbmethod ] , the priors for the templates are inferred from the data itself .",
    "our hb template fitting method draws from the same set of sed templates described above in section [ ssec : mlspecifics ] .",
    "however , to speed up the algorithm , we used only 250 of the 1313 star templates , spanning a range of physical and color - color properties . in practice , we find the individual choice of these templates to be unimportant ( since many are very similar ) so long as the templates span the colors of stars , with a sampling close to or better than the typical color uncertainties of the data .",
    "we believe similar arguments to be true for galaxies , but have not explored such issues since we currently use only 31 galaxy templates .",
    "the primary choice we must make for our hb approach is the functional form(s ) of the prior probability distributions in the model .",
    "since our templates are discrete both in sed shape and physical properties , we parameterize the prior probability of each template to be a single valued weight , within the range 0 to 1 , such that the weights sum to 1 ( see , for example , [ eqn : tempmarg ] and [ eqn : tempconstraint ] ) . these weights themselves become hyperparameters in our optimization .",
    "we thus have 281 hyperparameters corresponding to template priors since we use 250 star and 31 galaxy templates . the overall prior probability that any given object is @xmath19 or @xmath20 is also parameterized as two weights that sum to one ( [ eqn : fullprob ] and [ eqn : sgpriorconstraint ] in appendix ) , which we optimize .    for the galaxy models",
    ", we must choose a form for our redshift priors .",
    "ideally , these should be parameterized as weights for each discrete redshift , repeated as a separate set for each galaxy template . unfortunately , this would not only add @xmath65 more hyperparameters to optimize , but also significantly slows down likelihood computations . instead , we adopt a flat prior distribution across redshifts . while not ideal , such a prior eases comparison with ml classification results , and eliminates the need to specify an informative prior which correctly describes the data .",
    "tests of flat versus fixed - form prior distributions indicate that the classification results presented in section [ sec : results ] do not vary substantially between the two choices .",
    "finally , for each template fit we marginalize over the ( gaussian ) uncertainty in the fit amplitude , for which we must specify a prior distribution ( [ eqn : fitmarg ] and [ eqn : fitconstraint ] in appendix ) .",
    "we adopt a log - normal prior for the fit amplitudes , which we set by taking the mean and variance of the log - amplitudes from fits of all the data for a given template .",
    "this approach makes the priors essentially uninformative , since the variance for all the data is large with respect to the variance for data which is well fit by the template . like redshift priors ,",
    "these too could be treated as hyperparameters but come at the cost of much slower likelihood computations .    in summary , we fix redshift and fit - amplitude priors and vary the prior weights of the template and @xmath66 probabilities . thus , we optimize 283 prior ( hyper)parameters to values which yield the maximum likelihood of the entire dataset .",
    "we use the http://www.csie.ntu.edu.tw/%7ecjlin / libsvm/[`libsvm`]]| set of routines , described in @xcite .",
    "the provided routines are quick and easy to implement , and only require the user to specify a training set of data , a set of data to be classified ( a.k.a . , test data ) , and the form and parameter values of the kernel function used .",
    "we employ a gaussian radial basis function for the svm kernel , for which we must specify a scaling factor @xmath67 .",
    "together with the error penalty parameter ( @xmath68 ) we have two nuisance parameters whose optimal values we need find .",
    "we do this by using a nelder - mead simplex optimization algorithm to find the parameter values which provide the highest number of correct classifications in the test data . in detail , the optimal values for @xmath69 will be different for each combination of training and test data .    to select the training data , we consider two scenarios . first is a ` best case ' situation ( svm@xmath2 ) , where a well - characterized training set exists which is a fair sampling of the test data , with both the same object demographics and same signal - to - noise ( @xmath70 ) as the data to be classified . to emulate this scenario , we select the training set as a random sample of the cosmos catalog .",
    "second , we consider a more realistic case where the available training set is only sampling the demographics of the high @xmath70 portion of the catalog to be classified ( svm@xmath3 ) . in this case , the demographics of objects in the training set may not match the demographics of the majority of objects in the set to be classified .",
    "we consider svm@xmath2 an optimistic scenario  obtaining a large spectroscopic or multi - wavelength sample of training data , down to the limiting magnitude of a given survey , is very costly in terms of telescope time .",
    "the other extreme , svm@xmath3 , is a bit more realistic  for a given survey , classifications are typically easily obtained only at the high @xmath70 end of the data . in both cases ,",
    "we consider a training sample size which is a fifth of the total catalog size .    finally , to implement the svm classification routine we need to scale both the training data and test data .",
    "that is , for the colors and apparent magnitude used , we must scale the range of each to lie between @xmath71 and @xmath72 .",
    "we map both training and test data to the interval @xmath73 $ ] using the full range of values in the test data .",
    "this is important in the case of svm@xmath3 , since the training data may not span the full range of values for the test data .",
    "we find that scaling can have a significant effect for the svm@xmath3 model .",
    "for example , poor classification performance is obtained if the svm@xmath3 training data is scaled to itself rather than to the test data .",
    "we report the classification performance of maximum likelihood ( ml ) and hierarchical bayesian ( hb ) template fitting , as well as a thoroughly tested support vector machine ( svm ) on our cosmos based test data .",
    "there are many different measures which can be used to assess the performance of each algorithm .",
    "first , we consider the completeness , correctly classified as @xmath74 . ] and purity , correctly classified as @xmath74 , divided by the total number of sources classified as @xmath74 .",
    "] of classified samples , evaluated at @xmath75 .",
    "figures [ fig : completeness ] and [ fig : purity ] , display the completeness and purity , respectively , as a function of magnitude . examining figure [ fig :",
    "completeness ] , all methods seem to be fairly competitive for galaxy classification , returning @xmath76 completeness across all magnitudes . svm@xmath2 and ml yield the most consistently robust completeness for galaxies . in the case of stars",
    ", however , it is clear only our hb template fitting and svm@xmath2 deliver acceptable completeness  at @xmath77 the completeness of ml template fitting falls to 50% or below , and the completeness for svm@xmath3 goes to zero .",
    "the mismatch in source demographics between the realistic training set and the faint cosmos sources severely undermines the efficacy of svm@xmath3 .    in terms of purity ( figure [ fig : purity ] )",
    ", svm@xmath2 outperforms all other approaches . for stars , hb yields similar performance to svm@xmath2 , but all approaches underperform svm@xmath2 in terms of galaxy purity .",
    "when taken in concert with the results of figure [ fig : completeness ] , we see that hb delivers similar or better performance than ml in all cases , even with the relatively simple hb approach presented here . for stars , ml and hb yield similar sample purity , but hb does so with a much higher completeness ( @xmath78 vs. @xmath79 ) . for galaxies ,",
    "hb yields a consistently higher sample purity by @xmath80 but a consistently lower sample completeness by @xmath81 .",
    "we infer below that the performance achieved by the svm@xmath2 algorithm may represent the best possible classification of stars and galaxies that could be done , based on single - epoch @xmath4 photometry alone .",
    "however , it is unlikely that an ideal training set will be available for object classification in future , deep datasets . identifying the regions of _ ugirz _ color  color space where classification fails can highlight possible ways to improve the unsupervised hb ( or ml ) classification methods implemented here .",
    "for example , we want to check for regions of color - color space in which templates used in ml and hb may be missing , or to check whether the implementation of simple , but stronger , priors could increase performance .    figures [ fig : color - color - hb - fraction ] and [ fig : color - color - svm - fraction ] show the fraction of sources correctly classified using hb and svm@xmath2 , distributed over colors . comparing with figure [ fig : color - color - data ]",
    "reveals that the places where classification is least successful are regions where stars and galaxies overlap the most in color . for example , both the svm@xmath2 and the hb algorithm struggle to correctly identify galaxies with 1 @xmath82 @xmath83 @xmath82 3 and 1 @xmath82 @xmath84 @xmath821.5 .",
    "the number density of galaxies in the failing region is low , making hb even more likely to call everything a star .",
    "similarly , both stars and galaxies populate @xmath85 and @xmath86 , presenting a challenge to both svm and hb algorithms . in this case , the number density of galaxies is higher than that of stars , making hb even more likely to call everything a galaxy and training svm on a color separation that favors galaxies over stars .    in the region of @xmath87 ,",
    "the stellar locus has essentially zero overlap with galaxies in the sample .",
    "the svm@xmath2 algorithm yields exquisite classification of these stars , while the hb algorithm returns only a mediocre performance ( although @xmath88 and @xmath89 is populated with few stars , so those poorly classified regions do not represent a significant fraction of all stars ) . in future work , the classification of @xmath90 stars could therefore be improved with the implementation of stronger priors on the permitted redshifts at which galaxies may live  for example , by forcing a zero probability of elliptical galaxies at high redshifts .",
    "locating regions of color space in which the classifiers struggle to correctly separate stars and galaxies not only helps to decipher weaknesses in classification algorithms , but can be used to identify the specific science cases which will be most highly impacted .",
    "to illustrate , we examine places where both svm@xmath2 and hb underperform and compare these regions to the object types in our templates . for stars",
    "we identify two such regions .",
    "the first lies within @xmath91 and @xmath92 , which has been suggested to be comprised of white dwarf , m dwarf binaries @xcite .",
    "the second region , with @xmath91 and @xmath93 , is consistent with metal - poor main - sequence turnoff stars .",
    "the relatively poorer performance in this region is particularly troubling , since these populations are some of the main tracers for low - surface brightness galactic halo structure .    for galaxies , association of underperforming regions to specific populations",
    "is less clear - cut .",
    "for instance , we find the poor performing region with @xmath94 consistent with s0/sa seds with redshifts less than 0.4 , but also with dusty starbursting galaxies across a wider redshift range .",
    "while far from comprehensive , these associations highlight the fact that classification performance can affect certain science cases more than others , and should be accounted for both during individual analyses and in future algorithm development .",
    "one of the great advantages of probabilistic classification is that one need not restrict the classification criterion to a fixed value . by moving away from @xmath95 , one can obtain more / less pure or complete samples of stars and galaxies , depending on the user s science case . in detail ,",
    "how the purity or completeness varies as a function of @xmath96 depends on the algorithm used . to illustrate",
    ", we show in figure [ fig : hb - logodds ] how purity and completeness vary for the log odds ratio output by our hb algorithm . in the figure ,",
    "as @xmath96 decreases , we are requiring that the relative likelihood that an object is a galaxy is much higher than that for a star .",
    "similarly , as @xmath96 increases we are requiring objects be more stringently classified as a star .",
    "thus , by moving away from @xmath95 we change the star / galaxy purity and completeness to the point where everything is called a star or galaxy , giving 100% complete samples with a purity set by the sample fraction .",
    "one caveat , however , is that modifying the threshold @xmath24 to achieve more pure samples may select objects which lie in particular regions in sed space . to illustrate ,",
    "we show in figure [ fig : color - color - hb - odds ] the distribution of @xmath96 in color space .    we have considered the completeness and purity of sets of data classified as stars or galaxies ( as a function of @xmath96 ) as one means of comparing different classification algorithms .",
    "a strength of this approach to quantifying the efficacy of classification algorithms is its transparent connection to different science requirements , in terms of purity and completeness .",
    "a weakness of this approach is the impossibility of selecting an overall  best \" algorithm that presents an average over competing scientific requirements .",
    "for example , figure [ fig : completeness ] shows that compared to svm@xmath2 , our hb method gives better completeness in stars but slightly worse completeness for galaxies  which performs better in general ?",
    "we assess the overall performance of the various classification algorithms using the receiver operating characteristic ( roc ) curve .",
    "a roc curve is a plot of the true positive rate versus the false positive rate of a binary classifier , as the classification threshold ( @xmath96 ) is varied . in figure",
    "[ fig : roc ] , we plot the roc curve for all four classification approaches considered here .",
    "an ideal classifier has a true positive rate equal to one for all values of @xmath96 .",
    "thus , the area under the curve ( auc ) statistic is an assessment of the overall performance of the classifier .",
    "there are several points worth noting in figure [ fig : roc ] .",
    "first , we find our hb approach to template fitting outperforms the ml approach . considering our simple hb implementation is not very computationally demanding ( tens of minutes on typical desktop computer ) , even a basic hb approach should always be preferred over the ml case .",
    "svm algorithms , when trained with data which accurately capture the sed and @xmath70 properties of the entire data , generally perform much better than our current template fitting methods .",
    "this is not surprising , since template driven algorithms are never likely to have as complete models as something data driven . in reality",
    ", available training data will likely only capture the high @xmath70 end of the survey in question .",
    "as shown in figure [ fig : roc ] , a svm@xmath3 scenario underperforms even ml template fitting , casting severe doubt onto the usefulness of svm with ill - suited training information .",
    "future surveys which intend to use supervised techniques , therefore , will have to carefully consider if alternate strategies for obtaining training data ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) can outperform template fitting methods .",
    "imminent and upcoming ground - based surveys are observing large portions of the sky in optical filters to depths ( @xmath97 ) , requiring significant amounts of money , resources , and person power . in order for such surveys to best achieve some of their science goals , accurate star ",
    "galaxy classification is required . at these new depths ,",
    "unresolved galaxy counts increasingly dominate the number of point sources classified through morphological means . to investigate the usefulness of photometric classification methods for unresolved sources ,",
    "we examine the performance of photometric classifiers using _ ugriz _ photometry of cosmos sources with intrinsic fwhm @xmath1  arcsec , as measured with _",
    "we have focused our analysis on the classification of full survey datasets with broad science goals , rather than on the classification of subsets of sources tailored to specific scientific investigations .",
    "our conclusions are as follows :    * maximum likelihood ( ml ) template fitting methods are simple , and return informative classifications . at @xmath95 , ml methods deliver high galaxy completeness ( @xmath98 ) but low stellar completeness ( @xmath99 ) .",
    "the purity of these samples range from @xmath100 , and are a strong function of the relative sample fraction .",
    "* we present a new , basic hierarchical bayesian ( hb ) approach to template fitting which outperforms ml techniques , as shown by the receiver operating characteristic ( roc ) .",
    "hb algorithms have no need for training , and have nuisance parameters that are tuned according to the likelihood of the data itself .",
    "further improvements to this basic algorithm are possible by hierarchically modeling the redshift distribution of galaxies , the seds of the input templates , and the distribution of apparent magnitudes .",
    "* support vector machine ( svm ) algorithms can deliver excellent classification , which outperforms template fitting methods .",
    "successful svm performance relies on having an adequate set of training data .",
    "for optimistic cases , where the training data is essentially a random sample of the data ( with known classifications ) , svm will outperform template fitting . in a more - realistic scenario , where the training data samples only the higher signal to noise sources in the data to be classified ,",
    "svm algorithms perform worse than the simplest template fitting methods . *",
    "it is unclear when , if ever , adequate training data will be available for svm - like classification , hb algorithms are likely the optimum choice for next - generation classifiers . *",
    "a downside of a paucity of sufficient training data is the inability to assess the performance of both supervised ( svm ) and unsupervised ( ml , hb ) classifiers .",
    "if knowing the completeness and purity in detail is critical to the survey science goals , it may be necessary to seek out expensive training / testing sets .",
    "otherwise , users will have to select the best unsupervised classifier ( hb here ) , and rely on performance assessments extrapolated from other studies .",
    "* ground based surveys should deliver probabilistic photometric classifications as a basic data product .",
    "ml likelihoods are useful and require very little computational overhead , and should be considered the minimal delivered quantities . basic or refined hb classifications require more overhead , but can be run on small subsets of data to learn the priors and then run quickly on the remaining data , making them a feasible option for large surveys . finally ,",
    "if excellent training data is available , svm likelihoods should either be computed or the data should be made available . in any scenario , we strongly recommend that likelihood values , not binary classifications , should be delivered so that they may be propagated into individual analyses .",
    "the future of astronomical studies of unresolved sources in ground based surveys is bright .",
    "surveys like panstarrs , des , and lsst will deliver data that , in conjunction with approaches discussed here , will expand our knowledge of stellar systems , the structure of the milky way , and the demographics of distant galaxies .",
    "we have identified troublesome spots for classification in single - epoch @xmath4 photometric data , which may hinder studies of m - giant and metal - poor main - sequence turnoff stars in the milky way s halo .",
    "future studies could improve upon our preliminary results by impleneting more - sophisticated prior distributions , by identifying crucial improvements needed in current template models or training data , or by pursuing complementary non - sed based classification metrics .",
    "we gratefully acknowledge p. capak and the cosmos team for providing an up - to - date version of their catalog , c .- c .",
    "chang and c .- c .",
    "lin for making their svm routines available , j. j. bochanski for providing his stellar templates , and the le phare photo - z team for making code and templates available .",
    "we wish to thank j. newman , p. thorman , s. j. schmidt , d. foreman - mackey , and m. juric for helpful and insightful conversations .",
    "a special thanks is owed to z. ivezi and p. yoachim for leading us to an improved understanding of the cosmos classifications .",
    "we also thank joe cammisa , mulin ding , and dustin lang for technical support .",
    "rf and bw also thank the nyu center for cosmology and particle physics , and drexel university s physics dept . for hosting them during the writing of this paper .",
    "rf and bw acknowledge support from nsf grant ast-0908193 .",
    "dh acknowledges support from the nsf grant iis-1124794 .",
    "44    , s. , cristiani , s. , moscardini , l. , matarrese , s. , lucchin , f. , fontana , a. , & giallongo , e. 1999 , , 310 , 540    , j.  j. , west , a.  a. , hawley , s.  l. , & covey , k.  r. 2007 , , 133 , 531    boser , b.  e. , guyon , i.  m. , & vapnik , v.  n. 1992 , in proceedings of the fifth annual workshop on computational learning theory , colt 92 ( new york , ny , usa : acm ) , 144152    , p. et  al .",
    "2007 , , 172 , 99    . 2007 , , 172 , 99    , f. , & kurucz , r.  l. 2004 , arxiv:0405087    chang , c .- c . , &",
    "lin , c .- j .",
    "2011 , acm transactions on intelligent systems and technology , 2 , 27:1 , software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm    , a.  l. , newman , j.  a. , kaiser , n. , davis , m. , ma , c .- p . , kocevski , d.  d. , & koo , d.  c. 2004 , , 617 , 765    , k. r. , et  al .",
    "2004 , , 134 , 2398    , e. , cimatti , a. , renzini , a. , fontana , a. , mignoli , m. , pozzetti , l. , tozzi , p. , & zamorani , g. 2004 , , 617 , 746    gelman , a. , carlin , j.  b. , stern , h.  s. , & rubin , d.  b. 2003 , bayesian data analysis , second edition ( chapman & hall / crc texts in statistical science ) , 2nd edn .",
    "( chapman & hall )    , a. , guhathakurta , p. , richstone , d. , & flynn , c. 1992 , , 388 , 345    , g. et  al .",
    "2007 , , 172 , 29    , m. , mortlock , d.  j. , hand , d.  j. , & gandy , a. 2011 , , 412 , 2286    , h. , et  al .",
    "2010 , arxiv:1008.0658    , d.  w. , bovy , j. , & lang , d. 2010 , arxiv:1008.4686    , d.  w. , myers , a.  d. , & bovy , j. 2010 , , 725 , 2166    , o. et  al .",
    "2006 , , 457 , 841    , o. , et  al .",
    "2009 , , 690 , 1236    , a.  m. et  al .",
    "2007 , , 172 , 196    , r.  g. 1980 , , 43 , 305    , o. et  al .",
    "2005 , , 439 , 877    , k.  s. , narayan , g. , & kirshner , r.  p. 2011",
    ", , 731 , 120    , k.  s. , wood - vasey , w.  m. , friedman , a.  s. , & kirshner , r.  p. 2009",
    ", , 704 , 629    mller , k .- r . , mika , s. , rtsch , g. , tsuda , k. , & schlkopf , b. 2001 , ieee neural networks , 12 , 181    , a.  j. 1998 , , 110 , 863    , m. et  al .",
    "2007 , , 663 , 81    , d.  b. , guhathakurta , p. , & gould , a. 1998 , , 116 , 707    , j.  w. , et  al .",
    ", , 744 , 192    , j.  w. , et  al .",
    ", , 419 , 1121    , a.  c. , et  al .",
    "2007 , , 172 , 545    , r.  p.",
    "2012 , , 746 , 128    , d.  b. et  al .",
    "2007 , , 172 , 86    , c. et  al .",
    "2007 , , 172 , 406    , n. et  al .",
    "2007 , , 172 , 38    . 2007 , , 172 , 1    , y. , bolton , a.  s. , schlegel , d.  j. , dawson , k.  s. , wake , d.  a. , brownstein , j.  r. , brinkmann , j. , & weaver , b.  a. 2012 , , 143 , 90    , n.  m. , et  al . 2006 , , 131 , 1674    , a. et  al .",
    "2012 , arxiv e - prints    , y. et  al .",
    "2007 , , 172 , 9    , p. et  al .",
    "2012 , , 537 , a42    , e.  c. , de carvalho , r.  r. , gal , r.  r. , labarbera , f.  l. , capelato , h.  v. , frago campos velho , h. , trevisan , m. , & ruiz , r.  s.  r. 2011 , , 141 , 189    , j. , groves , b. , budavri , t. , & dale , d. 2011 , , 331 , 1    , l. et  al .",
    "2009 , , 138 , 95    , h.  k.  c. 1991 , , 103 , 396    , m.  a. et  al .",
    "2007 , , 172 , 468",
    "in this appendix , we provide a detailed , step - by - step description of our hierarchical bayesian algorithm .",
    "first , let us define the data as the sets : @xmath101 where @xmath102 , @xmath103 is the observed magnitude and uncertainty in filter number @xmath104 for @xmath105 number of filters .",
    "one sequence for the filters @xmath104 corresponds to @xmath106 .",
    "the zeropoint , @xmath107 , is : @xmath108 where @xmath109 is the standard flux density spectrum ( vega or ab ) , and @xmath110 is the fraction of photons incident on the top of the atmosphere which are counted by the detector , as a function of wavelength .",
    "next , we generate a model for the data using the templates : @xmath111 where @xmath112 corresponds to the flux density of a given spectral template .",
    "finally , we define a goodness of fit statistic : @xmath113 where @xmath114 is a constant unitless amplitude applied to the model for the fit ( discussed more below as @xmath115 ) .",
    "the variance @xmath116 , where @xmath117 is a few percent and represents a nuisance parameter which ( in a global sense ) accounts for error in the models as well as underestimates in @xmath118 .",
    "the value of @xmath25 from our template fitting is the fundamental quantity on which our inference procedure is based , as follows below .",
    "we represent the hypothesis that an object @xmath27 is a star or a galaxy by `` @xmath19 '' or `` @xmath20 '' respectively . for a given object @xmath27",
    ", we fit a set of templates @xmath119 corresponding to @xmath19 using the procedure outlined above .",
    "the likelihood of template @xmath119 and amplitude @xmath115 under the stellar hypothesis @xmath19 given the single observed data point @xmath120 is : @xmath121 where @xmath120 is the full set of observations of object @xmath27 and the associated noise model , and @xmath25 is defined in equation [ eqn : chi ] .",
    "note that the @xmath25 is not necessarily the best - fit value for @xmath25 but rather the @xmath25 obtained with template @xmath119 when it is given amplitude @xmath115 .",
    "we could optimize this likelihood , but really we want to compare the whole @xmath19 model space to the whole @xmath20 model space .",
    "we must marginalize this likelihood over the amplitude and template . to demonstrate this ,",
    "let us step through each marginalization for the @xmath19 model space .",
    "marginalization over the amplitude @xmath115 looks like @xmath122 where the integral is over all permitted values for the amplitude @xmath115 , and the prior pdf @xmath123 depends on the template @xmath119 , the full hypothesis @xmath19 .",
    "note , the prior pdf obeys the normalization constraint @xmath124 here we have also introduced some `` hyperparameters '' @xmath125 , which are variables which parameterize prior distributions .",
    "the subset of hyperparameters @xmath125 which apply to @xmath123 might be , for example , the mean and variance of a log - normal distribution on @xmath115 .",
    "it is the simultaneous inference of the star ",
    "galaxy probabilities and the hyperparameters that make the approach hierarchical .",
    "any realistic prior pdf for the @xmath115 comes from noting that ( for stars ) , the @xmath115 are dimensionless squared distance ratios between the observed star and the template star ; in this case the prior involves parameters of the stellar distribution in the galaxy .",
    "when we look at galaxies ( below ) , this situation will be different .",
    "in the ( rare ) case that the prior pdf @xmath123 varies slowly around the best - fit amplitude , @xmath126 where @xmath127 is the best - fit chi - squared , @xmath128 is the best - fit amplitude , and @xmath129 is the standard uncertainty in @xmath128 found by least - square fitting .",
    "this approximation is that the prior does nt vary significantly within a neighborhood @xmath129 of the best - fit amplitude .",
    "marginalization over the template space looks like @xmath130 where @xmath131 is the prior probability ( a discrete probability , not a pdf ) of template @xmath119 given the hypothesis @xmath19 and the hyperparameters @xmath125 .",
    "it obeys the normalization constraint @xmath132 note @xmath131 is a discrete set of weights , whose value corresponds to the hyperparameter for template @xmath119 .",
    "the marginalized likelihood that source @xmath27 is a galaxy @xmath20 , is calculated following a very similar sequence . in calculating the likelihood",
    ", we allow a given galaxy template @xmath135 to be shifted in wavelength by a factor @xmath136 . this introduces another step in the calculation that marginalizes the likelihood across redshift for a template , giving      where now @xmath138 is the constant amplitude for galaxy template @xmath135 at a redshift @xmath139 .",
    "the marginalization across redshift also introduces a prior @xmath140 , which is also is parameterized by a subset of @xmath125 , under some assumed form for the prior .",
    "this model is fully generative ; it specifies for any observed flux @xmath120 the pdf for that observation given either the star hypothesis @xmath19 or the galaxy hypothesis @xmath20 .",
    "we can write down then the full probability for the entire data set of all objects @xmath27 : @xmath141 \\quad , \\end{aligned}\\ ] ] where even the overall prior probability @xmath142 that an object is a star ( or , conversely , a galaxy ) depends on the hyperparameters @xmath125 .",
    "these obey the normalization constraint @xmath143 the likelihood @xmath144 is the total , marginalized likelihood for the combined data set of all the observations @xmath120 for all objects @xmath27 . from here we can take a number of approaches .",
    "one option is to find the hyperparameters that maximize this total marginalized likelihood , or we can assign a prior pdf @xmath145 on the hyperparameters , and sample the posterior pdf in the hyperparameter space . for computational reasons ,",
    "we choose to optimize @xmath144 in this work .    with either a maximum - likelihood set of hyperparameters @xmath125 or else a sampling ,",
    "inferences can be made . for our purposes ,",
    "the most interesting inference is , for each object @xmath27 , the posterior probability ratio ( or odds ) @xmath146 @xmath147 where we have re - used most of the likelihood machinery generated ( above ) for the purposes of inferring the hyperparameters .",
    "that is , the star  galaxy inference and the hyperparameter inferences proceed simultaneously ."
  ],
  "abstract_text": [
    "<S> ground - based optical surveys such as panstarrs , des , and lsst , will produce large catalogs to limiting magnitudes of @xmath0 . </S>",
    "<S> star - galaxy separation poses a major challenge to such surveys because galaxies  even very compact galaxies  outnumber halo stars at these depths . </S>",
    "<S> we investigate photometric classification techniques on stars and galaxies with intrinsic fwhm @xmath1  arcsec . </S>",
    "<S> we consider unsupervised spectral energy distribution template fitting and supervised , data - driven support vector machines ( svm ) . for template fitting , </S>",
    "<S> we use a maximum likelihood ( ml ) method and a new hierarchical bayesian ( hb ) method , which learns the prior distribution of template probabilities from the data . </S>",
    "<S> svm requires training data to classify unknown sources ; ml and hb do nt . </S>",
    "<S> we consider i. ) a best - case scenario ( svm@xmath2 ) where the training data is ( unrealistically ) a random sampling of the data in both signal - to - noise and demographics , and ii . ) </S>",
    "<S> a more realistic scenario where training is done on higher signal - to - noise data ( svm@xmath3 ) at brighter apparent magnitudes . testing with cosmos @xmath4 data </S>",
    "<S> we find that hb outperforms ml , delivering @xmath5 completeness , with purity of @xmath6 for both stars and galaxies . </S>",
    "<S> we find no algorithm delivers perfect performance , and that studies of metal - poor main - sequence turnoff stars may be challenged by poor star - galaxy separation . </S>",
    "<S> using the receiver operating characteristic curve , we find a best - to - worst ranking of svm@xmath2 , hb , ml , and svm@xmath3 . </S>",
    "<S> we conclude , therefore , that a well trained svm will outperform template - fitting methods . </S>",
    "<S> however , a normally trained svm performs worse . </S>",
    "<S> thus , hierarchical bayesian template fitting may prove to be the optimal classification method in future surveys . </S>"
  ]
}