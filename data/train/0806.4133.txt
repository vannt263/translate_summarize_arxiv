{
  "article_text": [
    "consider the operations of a ` fast - fashion ' retailer such as zara or h&m .",
    "such retailers have developed and invested in merchandize procurement strategies that permit lead times for new fashions as short as two weeks . as a consequence of this flexibility ,",
    "such retailers are able to adjust the assortment of products offered on sale at their stores to quickly adapt to popular fashion trends .",
    "in particular , such retailers use weekly sales data to refine their estimates of an item s popularity , and based on such revised estimates weed out unpopular items , or else re - stock demonstrably popular ones on a week - by - week basis . in sharp contrast , traditional retailers such as j.c .",
    "penney or marks and spencer face lead times on the order of several months .",
    "as such these retailers need to predict popular fashions months in advance and are allowed virtually no changes to their product assortments over the course of a sales season which is typically several months in length .",
    "understandably , this approach is not nearly as successful at identifying high selling fashions and also results in substantial unsold inventories at the end of a sales season . in view of the great deal of a - priori uncertainty in the popularity of a new fashion and the speed at which fashion trends evolve ,",
    "the fast - fashion operations model is highly desirable and emerging as the de - facto operations model for large fashion retailers .    among other things",
    ", the fast - fashion model relies crucially on an effective technology to learn from purchase data , and adjust product assortments based on such data .",
    "such a technology must strike a balance between ` exploring ' potentially successful products and ` exploiting ' products that are demonstrably popular . a convenient mathematical model within which to design algorithms capable of accomplishing such a task is that of the multi - armed bandit . while we defer a precise mathematical discussion to a later section , a multi - armed bandit consists of multiple ( say @xmath2 ) ` arms ' , each corresponding to a _",
    "markov decision process_. as a special case , one may think of each arm as an independent binomial coin with an uncertain bias specified via some prior distribution . at each point in time , one may ` pull ' up to a certain number of arms ( say @xmath3 ) simultaneously , or equivalently , toss up to a certain number of coins .",
    "for each tossed coin , we earn a reward proportional to its realization and are able to refine our estimate of its bias based on this realization .",
    "we neither learn about , nor earn rewards from coins that are not tossed .",
    "the multi - armed bandit problem requires finding a policy that adaptively selects @xmath4 arms to pull at every point in time with a view to maximizing total expected reward earned over some finite time horizon or alternatively , discounted rewards earned over an infinite horizon or perhaps , even long term average rewards .    with multiple simultaneous pulls",
    "allowed , the multi - armed bandit problem we have described is computationally hard .",
    "a popular and empirically successful heuristic for this problem was proposed several decades ago by whittle .",
    "whittle s heuristic produces an index for every arm based on the state of that arm and simply calls for pulling the @xmath4 arms with the highest index at every point in time . while it has been empirically and computationally observed that whittle s heuristic provides excellent performance ,",
    "the heuristic typically calls for frequent changes to the set of arms pulled that might , in hindsight , have been unnecessary .",
    "for instance , in the retail context , such a heuristic may choose to discard from the assortment a product presently being offered for sale in favor of a new product whose popularity is not known precisely .",
    "later , the heuristic may well choose to re - introduce the discarded product . while such exploration may appear necessary if one is to discover profitable bandit arms ( or popular products ) , enabling such a heuristic in practice will typically call for a great number of adjustments to the product assortment  a requirement that is both expensive and undesirable .",
    "this begs the following question : is it possible to design a heuristic for the multi - armed bandit problem that comes close to being optimal with a minimal number of adjustments to the set of arms pulled over time ?",
    "this paper introduces a new ` irrevocable ' heuristic for the multi - armed bandit problem we call the packing heuristic .",
    "the packing heuristic establishes a static ranking of bandit arms based on a measure of their potential value relative to the time required to realize that value , and pulls arms in the order prescribed by this ranking . for an arm currently being pulled ,",
    "the heuristic may either choose to continue pulling that arm in the next time step or else discard the arm in favor of the next highest ranked arm not currently being pulled .",
    "once discarded , an arm will _ never _ be chosen again ; hence the term irrevocable .",
    "irrevocability is an attractive structural constraint to impose on arm selection policies in a number of practical applications of the bandit model such as the dynamic assortment problem we have discussed or sequential drug trials where recourse to drugs whose testing was discontinued in the past is socially unacceptable .",
    "it is clear that an irrevocable heuristic makes a minimal number of changes to the set of arms pulled .",
    "what is perhaps surprising , is that the restriction to an irrevocable policy is typically far less expensive than one might expect . in particular , we demonstrate via a theoretical analysis and computational experiments that the use of the packing heuristic incurs a small performance loss relative to an optimal bandit policy with _ no restriction _ on exploration , i.e. an optimal strategy that is _ allowed recourse _ to arms that were pulled but discarded in the past .",
    "more specifically , the present work makes the following contributions :    * we introduce a new ` irrevocable ' heuristic , the packing heuristic , for the multi - armed bandit problem with multiple simultaneous arm - pulls .",
    "the packing heuristic is irrevocable in that if an arm being pulled is at some point discarded from the set of arms being pulled , it is never pulled again . at the same time , the performance loss incurred relative to an optimal , potentially non - irrevocable , control policy is limited .",
    "in particular , computational experiments with the packing heuristic for a generative family of large scale bandit problems indicate performance losses of up to about a few percent relative to an upper bound on the performance of an optimal policy with no restrictions on exploration .",
    "this level of performance suggests that the packing heuristic is likely to serve as a viable heuristic for the multi - armed bandit with multiple plays even when irrevocability is not a concern .",
    "+ in addition to our computational study , we are able to demonstrate a _ uniform _ bound on the price of irrevocability for a broad , interesting class of bandits .",
    "this class includes most commonly used applications of the bandit model such as bandits whose arms are ` coins ' of unknown biases .",
    "we demonstrate that the packing heuristic earns expected rewards that are always within a factor of @xmath1 of an optimal , potentially non - irrevocable policy .",
    "such a uniform bound guarantees robust performance across all parameter regimes ; in particular , the packing heuristic will ` track ' the performance of an optimal , potentially non - irrevocable policy across all parameter regimes .",
    "in addition , our analysis sheds light on the structural properties that afford the surprising efficacy of the irrevocable policies considered here . * in the interest of practical applicability , we develop a fast combinatorial implementation of the packing heuristic . assuming that an individual arm has @xmath5 states , and given a time horizon of @xmath6 steps , optimal solution to the multi - armed bandit problem under consideration requires @xmath7 computations . the main computational step in the packing",
    "heuristic calls for the one time solution of a linear program with @xmath8 variables , whose solution via a generic lp solver requires @xmath9 computations .",
    "we develop a novel combinatorial algorithm that solves this linear program in @xmath10 steps by solving a sequence of dynamic programs for each bandit arm .",
    "the technique we develop here is potentially of independent interest for the solution of ` weakly coupled ' optimal control problems with coupling constraints that must be met in expectation . employing this solution technique ,",
    "our heuristic requires a total of @xmath11 computations per time step amortized over the time horizon . in comparison , the simplest theoretically sound heuristics in existence for this multi - armed bandit problem ( such as whittle s heuristic ) require @xmath12 computations per time step . as such , we establish that the packing heuristic is computationally attractive .",
    "the multi - armed bandit problem has a rich history , and a number of excellent references ( such as @xcite ) provide a thorough treatment of the subject .",
    "we review here literature especially relevant to the present work . in the case where @xmath13 , that is , allowing for a single arm to be pulled in a given time step ,",
    "@xcite developed an elegant index based policy that was shown to be optimal for the problem of maximizing discounted rewards over an infinite horizon .",
    "their index policy is known to be suboptimal if one is allowed to pull more than a single arm in a given time step .",
    "@xcite developed a simple index based heuristic for a more general bandit problem ( the ` restless ' bandit problem ) allowing for multiple arms to be pulled in a given time step . while his original paper was concerned with maximizing long - term average rewards , his heuristic is easily adapted to other objectives such as discounted infinite horizon rewards or expected rewards over a finite horizon ( see for instance @xcite ) .",
    "@xcite subsequently established that under suitable conditions , whittle s heuristic was asymptotically optimal ( in a regime where @xmath2 and @xmath4 go to infinity keeping @xmath14 constant ) .",
    "whittle s heuristic may be viewed as a modification to the optimal control policy one obtains upon relaxing the requirement that at most @xmath4 arms be pulled in a given time step to requiring that at most @xmath4 arms be pulled in expectation in any given time step .",
    "the packing heuristic we introduce is motivated by a similar relaxation .",
    "in particular , we restrict attention to policies that entail a total of at most @xmath15 arm pulls over the entire horizon in expectation while allowing for no more than @xmath6 pulls of any given arm . where we differ substantially from whittle s",
    "heuristic is the manner in which we construct a feasible policy ( one where at most @xmath4 arms are pulled in a given time step ) from the relaxed policy .",
    "in fact there are potentially many reasonable ways of transforming an optimal policy for the relaxed problem to a feasible policy for the multi - armed bandit ; for instance @xcite use a scheme distinct from both whittle s and ours , that employs optimal primal and dual solutions to a linear programming formulation of whittle s relaxation to construct an index heuristic for arm selection . nonetheless , none of these schemes are irrevocable and nor do they offer non - asymptotic performance guarantees , if any .",
    "the packing heuristic policy builds upon recent insights on the ` adaptivity ' gap for stochastic packing problems .",
    "in particular , @xcite recently established that a simple static rule ( smith s rule ) for packing a knapsack with items of fixed reward ( known a - priori ) , but whose sizes were stochastic and unknown a - priori was within a constant factor of the optimal adaptive packing policy .",
    "@xcite used this insight to establish a similar static rule for ` budgeted learning problems ' .",
    "in such a problem one is interested in finding a coin with highest bias from a set of coins of uncertain bias , assuming one is allowed to toss a _",
    "single _ coin in a given time step and that one has a finite budget on the number of such experimental tosses allowed .",
    "our work parallels that work in that we draw on the insights of the stochastic packing results of @xcite .",
    "in addition , we must address two significant hurdles - correlations between the total reward earned from pulls of a given arm and the total number of pulls of that arm ( these turn out not to matter in the budgeted learning setting , but are crucial to our setting ) , and secondly , the fact that multiple arms may be pulled simultaneously ( only a single arm may be pulled at any time in the budgeted learning setting ) . finally , a working paper ( @xcite ) , brought to our attention by the authors of that work considers a variant of the budgeted learning problem of @xcite wherein one is allowed to toss multiple coins simultaneously . while it is conceivable that their heuristic may be modified to apply to the multi - armed bandit problem we address , the heuristic they develop is also _ not _ irrevocable .",
    "restricted to coins , our work takes an inherently bayesian views of the multi - armed bandit problem .",
    "it is worth mentioning that there are a number of non - parametric formulations to such problems with a vast associated literature .",
    "most relevant to the present model are the papers by @xcite that develop simple ` regret - optimal ' strategies for multi - armed bandit problems with multiple simultaneous plays .",
    "our development of an irrevocable policy for the multi - armed bandit problem was originally motivated by applications of this framework to ` dynamic assortment ' problems of the type mentioned in the introduction .",
    "in particular , @xcite computationally explore the use of a number of simple index - type heuristics ( similar to whittle s heuristic ) for such problems , none of which are irrevocable ; nonetheless , they stress the importance of a minimal number of changes to the assortment if any such heuristic is to be practical .",
    "the remainder of this paper is organized as follows .",
    "section 2 presents the multi - armed bandit model we consider and develops an ( intractable ) lp whose solution yields an optimal control policy for this bandit problem .",
    "section 3 develops the packing heuristic by considering a suitable relaxation of the multi - armed bandit problem .",
    "section 4 introduces a structural property for bandit arms we call the ` decreasing returns ' property .",
    "it is shown that a useful class of bandits , namely the ` coin ' bandits relevant to the applications that motivate us , possess this property .",
    "that section then establishes that the price of irrevocability for bandits possessing the decreasing returns property is uniformly bounded .",
    "section 5 presents very encouraging computational experiments for large scale bandit problems drawn from a generative family of coin type bandits . in the interest of implementability ,",
    "section 6 develops a combinatorial algorithm for the fast computation of packing heuristic policies for multi - armed bandits .",
    "section 7 concludes with a perspective on interesting directions for future work .",
    "we consider a multi - armed bandit problem with multiple simultaneous ` pulls ' permitted at every time step . a single bandit arm ( indexed by @xmath16 ) is a markov decision process ( mdp ) specified by a state space @xmath17 , an action space , @xmath18 , a reward function @xmath19 , and a transition kernel @xmath20 ( where @xmath21 is the @xmath22-dimensional unit simplex ) , yielding a probability distribution over next states",
    "should one choose some action @xmath23 in state @xmath24 .",
    "every bandit arm is endowed with a distinguished ` idle ' action @xmath25 .",
    "should a bandit be idled in some time period , it yields no rewards in that period and transitions to the same state with probability @xmath26 in the next period .",
    "more precisely , @xmath27    we consider a bandit problem with @xmath2 arms .",
    "in each time step one must select a subset of up to @xmath28 arms for which one may pick any action available at those respective arms .",
    "should an action other than the idle action be selected at any of these @xmath4 arms , we refer to such a selection as a ` pull ' of that arm .",
    "that is , any action @xmath29 would be considered a pull of the @xmath16th arm .",
    "one is forced to pick the idle action for the remaining @xmath30 arms .",
    "we wish to find an action selection ( or control ) policy that maximizes expected rewards earned over @xmath6 time periods .",
    "our problem may be cast as an optimal control problem . in particular , we define as our _ state - space _ the set @xmath31 and as our _ action space _ , the set @xmath32 .",
    "we let @xmath33 .",
    "we understand by @xmath34 , the @xmath16th component of @xmath35 and similarly let @xmath36 denote the @xmath16th component of @xmath37 .",
    "a feasible action is one which calls for simultaneously pulling at most @xmath4 arms .",
    "in particular we let @xmath38 denote the set of all feasible actions .",
    "we define a reward function @xmath39 , given by @xmath40 and a system transition kernel @xmath41 , given by @xmath42 .",
    "we now formally develop what we mean by a control policy .",
    "the arm selection policy we will eventually develop will use auxiliary information aside from the current state of the system , and so we require a general definition . let @xmath43 be a random variable that encapsulates any endogenous randomization in selecting an action , and define the filtration generated by @xmath43 and the history of visited states and actions by @xmath44 where @xmath45 and @xmath46 denote the state and action at time @xmath47 , respectively .",
    "we assume that @xmath48 for all @xmath49 and any @xmath50-measurable random variable @xmath51 .",
    "feasible _ policy simply specifies a sequence of @xmath52-valued actions @xmath53 adapted to @xmath50 .",
    "in particular , such a policy may be specified by a collection of @xmath54 measurable , @xmath52-valued random variables , @xmath55 , one for each possible state - action history of the system .",
    "we let @xmath56 denote the set of all such policies @xmath57 , and denote by @xmath58 the expected value of using policy @xmath57 starting in state @xmath59 at time @xmath60 ; in particular @xmath61,\\ ] ] where @xmath62 .",
    "our goal is to compute an optimal admissible policy .",
    "markovian policies , i.e. policies under which @xmath46 is measurable with respect to @xmath63 , are particularly useful . a markovian policy is specified as a collection of independent @xmath52 valued random variables @xmath64 each measurable with respect to @xmath65 .",
    "in particular , assuming the system is in state @xmath59 at time @xmath47 , such a policy selects an action @xmath46 as the random variable @xmath66 , independent of past states and actions .",
    "we let @xmath67 denote the set of all such admissible markovian policies .",
    "every @xmath68 is associated with a value function , @xmath69 which , for every @xmath70 , gives the expected value of using control policy @xmath57 starting at that state : @xmath71.\\ ] ] we denote by @xmath72 the optimal value function .",
    "in particular , @xmath73 . the preceding supremum is always achieved and we denote by @xmath74 a corresponding optimal markovian control policy .",
    "that is , @xmath75 for all @xmath70 .",
    "our restriction to markovian policies is without loss ; @xmath67 always contains an optimal policy among the broader class of admissible policies so that @xmath76 for all states @xmath59 .",
    "we next formulate a mathematical program to compute such an optimal policy .",
    "an optimal policy @xmath74 may be found via the solution of the following linear program , @xmath77 , specified by a parameter @xmath78 that specifies the distribution of arm states at time @xmath79 .",
    "@xmath80 where the variables are the state action frequencies @xmath81 , which give the probability of being in state @xmath59 at time @xmath47 and choosing action @xmath82 .",
    "the first set of constraints in the above program simply enforce the dynamics of the system , while the second set of constraints enforces the requirement that at most @xmath4 arms are simultaneously pulled at any point in time .",
    "an optimal solution to the program above may be used to construct a policy @xmath74 that attains expected value @xmath83 starting at any state @xmath59 for which @xmath84 . in particular , given an optimal solution @xmath85 to @xmath77 , one obtains such a policy by defining @xmath86 as a random variable that takes value @xmath37 with probability @xmath87 . by construction , we have @xmath88 = opt(lp(\\tilde{\\pi}_0))$ ] .",
    "of course , efficient solution of the above program is not a tractable task , which forces us to seek approximations to an optimal policy .",
    "the next section will present one such policy with an appealing structural property we term ` irrevocability ' .",
    "this section develops an approximation to the optimal multi - armed bandit control policy that we will subsequently establish performs adequately relative to the optimal policy .",
    "this approximation will possess a desirable property we term ` irrevocability ' .",
    "in particular , the policy we develop will , at any time , be permitted to pull an arm only if that arm was pulled in the prior time step , or else never pulled in the past .",
    "we first develop a control policy for a related bandit problem , where the requirement that precisely @xmath4 arms be pulled in any time step is relaxed .",
    "as we will see , this is essentially whittle s relaxation and the policy developed for this relaxation is an upper bound to the optimal policy .",
    "we will then use the control policy developed for this relaxed control problem to design a policy for the multi - armed bandit problem that is irrevocable and also offers good performance relative to the optimal policy for a broad class of bandits .    consider the following relaxation of the program @xmath77 , @xmath89 .",
    "@xmath89 may be viewed as a primal formulation of whittle s relaxation :    @xmath90 \\leq kt , \\\\",
    "\\quad & \\sum_{a_i } \\pi_i(s_i , a_i,0 ) = \\sum_{\\bar{s } : \\bar{s}_i = s_i } \\tilde{\\pi}_0(\\bar{s } ) , \\\\",
    "\\quad & \\pi \\geq 0 , \\end{array}\\ ] ] where @xmath91 is the probability of the @xmath16th bandit being in state @xmath34 at time @xmath47 and choosing action @xmath36 .",
    "the program above relaxes the requirement that precisely @xmath4 arms be pulled in a given time step ; instead we now require that over the entire horizon at most @xmath15 arms are pulled in _ expectation _ , where the expectation is over policy randomization and state evolution .",
    "the first set of equality constraints enforce individual arm dynamics whereas the first inequality constraint enforces the requirement that at most @xmath15 arms be pulled in expectation over the entire time horizon .",
    "the following lemma makes the notion of a relaxation to @xmath77 precise ; the proof may be found in the appendix .",
    "[ le : relaxation ] @xmath92    given an optimal solution @xmath93 to @xmath89 , one may consider the policy @xmath94 , that , assuming we are in state @xmath59 at time @xmath47 , selects a random action @xmath95 , where @xmath96 with probability @xmath97 independent of the past .",
    "noting that the action for each arm @xmath16 is chosen independently of all other arms , we use @xmath98 to denote the induced policy for arm @xmath16 . by construction ,",
    "@xmath99 = opt(rlp(\\tilde{\\pi}_0))$ ] .",
    "moreover , we have that @xmath100 satisfies the constraint @xmath101 \\leq kt,\\ ] ] where the expectation is over random state transitions and endogenous policy randomization .    of course",
    ", @xmath94 is not necessarily feasible ; we ultimately require a policy that entails at most @xmath4 arm pulls in any time step .",
    "we will use @xmath94 to construct such a feasible policy .",
    "in addition , we will see that if an arm is pulled and then idled in some subsequent time step , it will never again be pulled , so that the policy we construct will be irrevocable . in what follows",
    "we will assume for convenience that @xmath102 is degenerate and puts mass @xmath26 on a single starting state .",
    "that is , @xmath103 for some @xmath104 for all @xmath16 .",
    "we first introduce some relevant notation . given an optimal solution @xmath93 to @xmath89 , define the value generated by arm @xmath16 as the random variable @xmath105 and the ` active time ' of arm @xmath16 , @xmath106 as the total number of pulls of arm @xmath16 entailed under that policy @xmath107 the expected value of arm @xmath16 , @xmath108 = \\sum_{s_i , a_i , t } \\bar{\\pi}_i(s_i , a_i , t)r_i(s_i , a_i)$ ] , and the expected active time @xmath109 = \\sum_{s_i , a_i , t : a_i \\neq \\phi_i } \\bar{\\pi}_i(s_i , a_i , t)$ ] .",
    "we will assume in what follows that @xmath109 > 0 $ ] for all @xmath16 ; otherwise , we simply consider eliminating those @xmath16 for which @xmath109 = 0 $ ]",
    ". we will also assume for analytical convenience that @xmath110 = kt$ ] .",
    "neither assumption results in a loss of generality .",
    "to motivate our policy we begin with the following analogy with a packing problem : imagine packing @xmath2 objects into a knapsack of size @xmath111 .",
    "each object @xmath16 has size @xmath112 and value @xmath113 .",
    "moreover , we assume that we are allowed to pack fractional quantities of an object into the knapsack and that packing a fraction @xmath114 of the @xmath16th object requires space @xmath115 and generates value @xmath116 .",
    "an optimal policy is then given by the following greedy procedure : select objects in decreasing order of the ratio @xmath117 and place them in to the knapsack to the extent that there is room available . if one had more than a single knapsack and the additional constraint that an item could not be placed in more than a single knapsack , then the situation is more complicated .",
    "one may consider a greedy procedure that , as before , considers items in decreasing order of the ratio @xmath117 and places them ( possibly fractionally ) in sequence , into the least loaded of the bins at that point .",
    "this generalization of the greedy procedure for the simple knapsack is suboptimal , but still a reasonable heuristic .",
    "thus motivated , we begin with a loose high level description of our control policy , which we call the ` packing ' heuristic .",
    "we think of each bandit arm @xmath16 as an ` item ' of value @xmath108 $ ] with size @xmath109 $ ] . for the purposes of this explanation _",
    "alone _ , we will assume for convenience that should policy @xmath94 call for an arm that was pulled in the past to be idled , it will never again call for that arm to be pulled ; we will momentarily remove that assumption . our control policy will operate as follows : we will order arms in decreasing order of the ratio @xmath108/e[t_i]$ ] .",
    "we begin with the top @xmath4 arms according to this ordering . for each such arm we will select an action according to the policy specified for that arm by @xmath118 ; should this policy call for the arm to be idled , we discard that arm and will never again consider pulling it .",
    "we replace the discarded arm with the next available arm ( in order of initial arm rankings ) and select an action for the arm according to @xmath94 .",
    "we repeat this procedure until we have selected non - idle actions for up to @xmath4 arms ( or no arms are available ) .",
    "we then let time advance , earn rewards , and repeat the procedure described above until the end of the time horizon .",
    "algorithm [ alg : kmab ] describes the packing heuristic policy precisely , addressing the fact that @xmath118 may call for an arm to be idled but then pulled in some subsequent time step .",
    "renumber bandits so that @xmath119}{e[{t}_1 ] } \\geq \\frac{e[{r}_2]}{e[{t}_2 ] } \\dots \\geq \\frac{e[{r}_n]}{e[{t}_n]}$ ] .",
    "index bandits by variable @xmath16 .",
    "@xmath120 for all @xmath16 , @xmath121 @xmath122 @xmath123 .",
    "select an @xmath124 with @xmath125 select @xmath126 @xmath127 @xmath128 @xmath129 @xmath130 @xmath131 @xmath132 @xmath133    in the event that we placed no restriction on the time horizon ( i.e. we set @xmath134 in the algorithm above ) , we have by construction , that the expected total reward earned under the above policy is precisely @xmath135 . in essence",
    ", @xmath89 prescribes a policy wherein each arm generates a total reward with mean @xmath136 $ ] using an expected total number of pulls @xmath137 $ ] , independent of other arms .",
    "the above scheme may be visualized as one which ` packs ' as many of the pulls of various arms possible in a manner so as to meet feasibility constraints .",
    "it is clear that the heuristic we have constructed entails a minimal amount of arm ` exploration ' .",
    "in particular , we are guaranteed at most @xmath30 changes to the set of pulled arms .",
    "one may naturally ask what the limited exploration permitted under this policy costs us in terms of performance .",
    "in addition , is this scheme computationally practical ?",
    "in particular , the linear programming relaxation we must solve is still a fairly large program . in subsequent sections we address these issues .",
    "first , we present a theoretical analysis that demonstrates that the price of irrevocability is uniformly bounded for an important general class of bandits .",
    "our analysis sheds light on the structural properties that are likely to afford a low price of irrevocability in practice .",
    "we then present results of computational experiments with a generative family of large - scale problems demonstrating performance losses of up to @xmath138 percent relative to an upper bound on the performance of the optimal policy ( which is potentially non - irreovcable and has no restrictions on exploration ) .",
    "finally , we address computational issues relevant to the packing heuristic and develop a computational scheme that is substantially quicker than heuristics such as whittle s heuristic .",
    "this section establishes a uniform bound on the performance loss incurred in using the irrevocable packing heuristic relative to an optimal , potentially non - irrevocable scheme for a useful family of bandits whose arms exhibit a certain decreasing returns property .",
    "this class includes bandits whose arms are coins of unknown biases  a family particularly relevant to a number of applications including those discussed in the introduction .",
    "we establish that the packing heuristic always earns expected rewards that are within a factor of @xmath1 of an optimal scheme .",
    "our analysis sheds light on those structural properties that likely afford a low price to irrevocability .",
    "in addition to being an indicator of robustness across all parameter regimes , this bound on the price of irrevocability is remarkable for two reasons .",
    "first , it does not rely on an asymptotic scaling of the system ; the performance of the packing heuristic will ` track ' that of an optimal , potentially non - irrevocable heuristic across all regimes .",
    "second , the bound represents a comparison with a system where one is allowed recourse to arms that were pulled in the past and discarded .",
    "in particular , the bound thus highlights the fact that for a useful class of bandits , one may achieve reasonable performance with very limited exploration .",
    "the typical performance we expect from the heuristic is likely to be far superior ( as it generally is in the case of problems for which such worst case guarantees can be established ) ; in a subsequent section we will present computational experiments indicating a performance loss of @xmath139% relative to an optimal policy with no restrictions on exploration .    in what follows we",
    "first specify the decreasing returns property and explicitly identify a class of bandits that possess this property .",
    "we then present our performance analysis which will proceed as follows : we first consider pulling bandit arms _ serially _ , i.e. at most one arm at a time , in order of their rank and show that the total reward earned from bandits that were first pulled within the first @xmath140 pulls is at least within a factor of @xmath1 of an optimal policy .",
    "this result relies on the static ranking of bandit arms used , and a symmetrization idea exploited by @xcite in their result on stochastic packing where rewards are statistically independent of item size .",
    "in contrast to that work , we must address the fact that the rewards earned from a bandit are statistically dependent on the number of pulls of that bandit and to this end we exploit the decreasing returns property that establishes the nature of this correlation .",
    "we then show via a combinatorial sample path argument that the expected reward earned from bandits pulled within the first @xmath141 time steps of the packing heuristic i.e. , with arms being pulled in parallel , is at least as much as that earned in the setting above where arms are pulled serially , thereby establishing our performance guarantee .",
    "define for every @xmath16 and @xmath142 , the random variable @xmath143 @xmath144 tracks the number of times a given arm @xmath16 has been pulled under policy @xmath94 among the first @xmath145 steps of selecting an action for that arm .",
    "further , define @xmath146 @xmath147 is the random reward earned within the first @xmath148 pulls of arm @xmath16 under the policy @xmath94 .",
    "the decreasing returns property roughly states that the expected incremental returns from allowing an additional pull of a bandit arm are , on average , decreasing .",
    "more precisely , we have :    ( decreasing returns)[as : dec_returns ] @xmath149 - e[r_i^{m } ] \\leq e[r_i^m ] - e[r_i^{m-1}]$ ] for all @xmath150 .    one useful class of bandits from a modeling perspective that satisfy this property are bandits whose arms are ` coins ' of unknown bias .",
    "the following discussion makes this notion more precise :      we define a ` coin ' to be any multi - armed bandit for which every arm @xmath16 has action space @xmath151 , with @xmath152 for all @xmath24 , and satisfies the following property : @xmath153 the above sub - martingale characterization of rewards intuitively suggests the decreasing returns property .",
    "in particular , it suggests that the returns from a pull in the current state are at least as large as the expected returns to a pull in a state reached subsequent to the current pull .",
    "the decreasing returns property for coins is established in the following lemma whose proof may be found in the appendix :    [ le : coin_dec_returns ] coins satisfy the decreasing returns property .",
    "that is , if @xmath154 , and @xmath155 then @xmath156 - e[r_i^{m } ] \\leq e[r_i^m ] - e[r_i^{m-1}]\\ ] ] for all @xmath157 .",
    "returning to our motivating example of dynamic product assortment selection , we note that in estimating the bias of a binomial coin of unknown bias given some initial prior on coin bias , bayes rule implies that the estimated bias after @xmath2 observations ( which generate the filtration @xmath158 ) , @xmath159 satisfies @xmath160 = \\mu_n$ ] .",
    "thus , bandits with such arms wherein the reward from an arm is some non - negative scalar times the bias , automatically possess the decreasing returns property .      for convenience of exposition",
    "we assume that @xmath6 is even ; addressing the odd case requires essentially identical proofs but cumbersome notation .    we re - order the bandits in decreasing order of @xmath108/e[t_i]$ ] as in the packing heuristic .",
    "let us define @xmath161 \\geq kt/2\\right\\}.\\ ] ] thus , @xmath162 is the set of bandits that take up approximately half the budget on total expected pulls .",
    "next , let us define for all @xmath16 , random variables @xmath113 and @xmath112 according to @xmath163 for all @xmath164 . we define @xmath165 and @xmath166 , where @xmath167}{e[t_{h^*}]}$ ]",
    "we begin with a preliminary lemma :    [ le : concave ] @xmath168 \\geq \\frac{1}{2 } opt(rlp(\\tilde{\\pi}_0)).\\ ] ]    define a function @xmath169}{e[t_i ] } \\left(\\left(t   - \\sum_{j=1}^{i-1 } e[t_i]\\right)^+ \\wedge e[t_i]\\right),\\ ] ] where @xmath170 . by construction",
    "( i.e. since @xmath171}{e[t_i]}$ ] is non - increasing in @xmath16 ) , we have that @xmath172 is a concave function on @xmath173.$ ] now observe that @xmath168 = \\sum_{i=1}^{h^*-1 } \\frac{e[r_i]}{e[t_i]}e[t_i ] + \\frac{e[r_{h^*}]}{e[t_{h^*}]}\\left(kt/2 - \\sum_{j = 1}^{h^*-1}e[t_i]\\right ) = f(kt/2).\\ ] ] next , observe that @xmath174}{e[t_i ] } e[t_i ] = f(kt).\\ ] ] by the concavity of @xmath172 and since @xmath175 , we have that @xmath176 , which yields the result .    we next compare the expected reward earned by a certain subset of bandits with indices no larger than @xmath162 .",
    "the significance of the subset of bandits we define will be seen later in the proof of lemma [ le : simulation ]  we will see there that all bandits in this subset will begin operation prior to time @xmath141 in a run the packing heuristic .",
    "in particular , define @xmath177    [ le : half_inequality ] @xmath178 \\geq \\frac{1}{4 } opt(rlp(\\tilde{\\pi}_0)).\\ ] ]    we have : @xmath179 & \\stackrel{(a ) } = \\sum_{i=1}^{h^*}{\\rm pr}\\left(\\sum _ { j=1}^{i-1 } t_j < kt/2\\right)e[r_i ] \\\\ & \\stackrel{(b ) } \\geq \\sum_{i=1}^{h^*}{\\rm pr}\\left(\\sum _ { j=1}^{i-1 }",
    "t_j < kt/2\\right)e[\\tilde{r}_i ] \\\\ & \\stackrel{(c ) } = \\sum_{i=1}^{h^*}{\\rm pr}\\left(\\sum",
    "_ { j=1}^{i-1 } \\tilde{t}_j < kt/2\\right)e[\\tilde{r}_i ] \\\\ &",
    "\\stackrel{(d ) } \\geq \\sum_{i=1}^{h^ * } \\left(1 - \\frac{\\sum_{j=1}^{i-1 } e[\\tilde{t}_j]}{kt/2}\\right ) e[\\tilde{r}_i ] \\\\ & = \\sum_{i=1}^{h^*}e[\\tilde{r}_i ]   - \\sum_{i=1}^{h^*}\\frac{\\sum_{j=1}^{i-1 } e[\\tilde{t}_j ] } { kt/2 } e[\\tilde{r}_i ] \\\\ & \\stackrel{(e ) } \\geq \\sum_{i=1}^{h^*}e[\\tilde{r}_i ]   - \\frac{1}{2}\\sum_{i=1}^{h^*}\\frac{\\sum_{j=1 , j \\neq i}^{h^ * } e[\\tilde{t}_j ] } { kt/2 } e[\\tilde{r}_i ] \\\\ & \\stackrel{(f)}\\geq \\frac{1}{2 } \\sum_{i=1}^{h^*}e[\\tilde{r}_i ] \\\\ & \\stackrel{(g)}\\geq \\frac{1}{4 } opt(rlp(\\tilde{\\pi}_0 ) ) \\end{split}\\ ] ] equality  ( a ) follows from the fact that under policy @xmath94 , @xmath180 is independent of @xmath181 for @xmath182 .",
    "inequality  ( b ) follows from our definition of @xmath113 : @xmath183 .",
    "equality  ( c ) follows from the fact that by definition @xmath184 for all @xmath164 .",
    "inequality  ( d ) invokes markov s inequality .",
    "inequality  ( e ) is the critical step in establishing the result and uses the simple symmetrization idea exploited by @xcite : in particular , we observe that since @xmath171}{e[t_i ] } \\leq \\frac{e[r_j]}{e[t_j]}$ ] for @xmath185 , it follows that @xmath108e[t_j ] \\leq \\frac{1}{2 } ( e[r_i]e[t_j ] + e[r_j]e[t_i])$ ] for @xmath185 . replacing every term of the form @xmath108e[t_j]$ ] ( with @xmath185 ) in the expression preceding inequality  ( e ) with the upper bound @xmath186e[t_j ] + e[r_j]e[t_i])$ ] yields inequality  ( e ) .",
    "( f ) follows from the fact that @xmath187 = kt/2 $ ] .",
    "inequality  ( g ) follows from lemma [ le : concave ] .    before moving on to our main lemma that translates the above guarantees to a guarantee on the performance of the packing heuristic",
    ", we need to establish one additional technical fact . recall that @xmath147 is the reward earned by bandit @xmath16 in the first @xmath148 pulls of this bandit .",
    "exploiting the assumed decreasing returns property , we have the following lemma whose proof may be found in the appendix :    [ le : fact1 ] for bandits satisfying the decreasing returns property ( property [ as : dec_returns ] ) , @xmath188 \\geq \\frac{1}{2}e[r_{1/2}].\\ ] ]    we have thus far established estimates for total expected rewards earned assuming implicitly that bandits are pulled in a serial fashion in order of their rank .",
    "the following lemma connects these estimates to the expected reward earned under the @xmath189 policy ( given by the packing heuristic ) using a simple sample path argument .",
    "in particular , the following lemma shows that the expected rewards under the @xmath189 policy are at least as large as @xmath190 $ ] .",
    "[ le : simulation ] @xmath191",
    "\\geq e\\left[\\sum_{i=1}^{h^*}\\mathbf{1}_{\\sum_{j=1}^{i-1}t_j < kt/2 } r_i^{t/2}\\right ] .",
    "$ ]    for a given sample path of the system define @xmath192 on this sample path , it must be that : @xmath193    we claim that arms @xmath194 are all first pulled at times @xmath195 under @xmath189 .",
    "assume to the contrary that this were not the case and recall that arms are considered in order of index under @xmath189 , so that an arm with index @xmath16 is pulled for the first time no later than the first time arm @xmath196 is pulled for @xmath197 .",
    "let @xmath198 be the highest arm index among the arms pulled at time @xmath199 so that @xmath200 .",
    "it must be that @xmath201 .",
    "but then , @xmath202 which is a contradiction .",
    "thus , since every one of the arms @xmath194 is first pulled at times @xmath203 , each such arm may be pulled for at least @xmath141 time steps prior to time @xmath6 ( the horizon ) .",
    "consequently , we have that the total rewards earned on this sample path under policy @xmath189 are at least @xmath204 using identity and taking an expectation over sample paths yields the result .    we are ready to establish our main theorem that provides a uniform bound on the performance loss incurred in using the packing heuristic policy relative to an optimal policy with no restrictions on exploration . in particular , we have that the price of irrevocability is uniformly bounded for bandits satisfying the decreasing returns property .",
    "[ th : unif_bound ] for multi - armed bandits satisfying the decreasing returns property ( property [ as : dec_returns ] ) , @xmath205 \\geq \\frac{1}{8 } e[j^{*}(s,0)| s\\sim \\tilde{\\pi}_0 ] $ ] for all initial state distributions @xmath102 .",
    "we have from lemmas [ le : relaxation],[le : half_inequality],[le : fact1 ] and [ le : simulation ] that @xmath206 \\geq \\frac{1}{8}opt(rlp(\\tilde{\\pi}_0)).\\ ] ] we know from lemma [ le : relaxation ] that @xmath207 $ ] from which the result follows .",
    "our analysis highlighted a structural property  decreasing returns  that is likely to afford a low price of irrevocability .",
    "the next section demonstrates computational results that suggest that in practice we may expect this price to be quite small ( on the order of @xmath139% ) for bandits possessing this property .",
    "this section presents computational experiments with the packing heuristic .",
    "we consider a number of large scale bandit problems drawn from a generative family of problems to be discussed shortly and demonstrate that the packing heuristic consistently demonstrates performance within about @xmath139 % of an upper bound on the performance of an unrestricted ( i.e. potentially _ non_-irrevocable ) optimal solution to the multi - armed bandit problem . in particular , this suggests that the price of irrevocability is likely to be small in practice , at least for models of the type we consider here .",
    "since the bandits considered in our experiments - binomial coins of uncertain bias - are among the most widely used applications of the multi - armed bandit model , we view this to be a positive result .    *",
    "the generative model : * we consider multi - armed bandit problems with @xmath2 arms up to @xmath4 of which may be pulled simultaneously at any time .",
    "the @xmath16th arm corresponds to a binomial@xmath208 coin where @xmath148 is fixed and known , and @xmath209 is unknown but drawn from a dirichlet@xmath210 prior distribution .",
    "assuming we choose to ` pull ' arm @xmath16 at some point , we realize a random outcome @xmath211 .",
    "@xmath212 is a bernoulli@xmath208 random variable where @xmath209 is itself a dirichlet@xmath210 random variable .",
    "we receive a reward of @xmath213 and update the prior distribution parameters according to @xmath214 , @xmath215 . by selecting the initial values of @xmath216 and @xmath217 for each arm",
    "appropriately we can control for the initial uncertainty in the value of @xmath209 .",
    "this model is , for instance , applicable to the dynamic assortment selection problem discussed earlier ( see @xcite ) with each coin representing a product of uncertain popularity and @xmath212 representing the uncertain number of product @xmath16 sales over a single period in which that product is offered for sale .",
    "we recall from our previous discussion that this family of bandits satisfies the decreasing returns property and from our performance analysis we expect a reasonable price of irrevocability .",
    "we consider the following random instances of the above problem .",
    "we consider bandits with @xmath218 .",
    "these dimensions are representative of large scale applications of which the dynamic assortment problem is an example . for each value of @xmath219 we consider time horizons @xmath220 and @xmath221 . for every bandit problem we consider ,",
    "we subdivide the arms of the bandit into @xmath222 groups .",
    "all arms within a group have identical statistical structure , that is , identical @xmath223 values and identical initial values of @xmath216 and @xmath217 . for each value of @xmath224",
    ", we generate a number of problem instances by randomly drawing prior parameters for bandit arms .",
    "in particular , for all arms in a given group we select @xmath216 uniformly in the interval @xmath225 $ ] and then select that value of @xmath217 which results in a prior co - efficient of variation @xmath226 .",
    "these co - efficients of variation represent , respectively , a moderate and high degree of a - priori uncertainty in coin bias ( or in the context of the dynamic assortment application , product popularity ) .",
    "in addition , @xmath223 is drawn uniformly on @xmath227 $ ] and we take @xmath228 .",
    "we generate @xmath229 random problem instances for each co - efficient of variation .",
    "control policies for a given bandit problem instance are evaluated over @xmath230 random state trajectories ( which resulted in @xmath231% confidence intervals that were at least within + /-@xmath26% of the sample average ) .",
    "[ tab : performance ]     coeff . of variation & arms & simultaneous pulls & horizon & performance + @xmath232 & @xmath233 & @xmath234 & @xmath235 & @xmath236 + & 500 & 50 & 25 & 0.91 + & 500 & 50 & 40 & 0.92 + & 500 & 100 & 25 & 0.93 + moderate & 500 & 100 & 40 & 0.94 + @xmath237 & 100 & 10 & 25 & 0.88 + & 100 & 10 & 40 & 0.89 + & 100 & 20 & 25 & 0.91 + & 100 & 20 & 40 & 0.93 + & 500 & 50 & 25 & 0.89 + & 500 & 50 & 40 & 0.90 + & 500 & 100 & 25 & 0.90 + high & 500 & 100 & 40 & 0.91 + @xmath238 & 100 & 10 & 25 & 0.87 + & 100 & 10 & 40 & 0.88 + & 100 & 20 & 25 & 0.90 + & 100 & 20 & 40 & 0.91 +    * evaluating performance : * a striking feature of our performance results is that the price of irrevocability is quite small , a trend that appears to hold over varying parameter regimes .",
    "in particular , we make the following observation :    * consider problems with a small number of arms ( @xmath229 ) with a large number of simultaneous pulls ( @xmath239 ) allowed .",
    "intuitively , an optimal policy could reasonably explore all arms in this setting before settling on the ` best ' arms .",
    "we thus expect the price of irrevocability to be high here . even in this regime",
    "we find that the price of irrevocability is only about @xmath240 % of optimal performance .",
    "* consider problems with a high degree of a - priori uncertainty in coin bias .",
    "mistakes - that is , discarding an arm that is performing reasonably in favor of an unexplored arm that turns out to perform poorly - are particularly expensive in such problems . with a hgh",
    "co - efficient of variation in the prior on initial arm bias , the price of ir - revocability is indeed somewhat higher but continues to remain within @xmath241 % of optimal performance . * for each of our experiments , we observe that keeping all other parameters fixed , relative performance improves with a longer time horizon .",
    "this is intuitive ; with longer horizons , one may delay discarding an arm only once one is sure that the arm performs poorly relative to the expected value of the available alternatives . *",
    "finally , we note that the performance figures we report are relative to an _ upper bound _ on optimal policy performance .",
    "computing the optimal policy is itself an intractable task .",
    "the performance observed here suggests that at least for bandit problems with decreasing returns the packing heuristic is a viable approximation scheme even when irrevocability is not necessarily a concern .",
    "we can thus conclude that the price of irrevocability is small for a useful class of multi - armed bandit problems and that the packing heuristic performs well for this class of problems .",
    "a final concern is computational effort . in particular , for the largest problem instance we considered ( @xmath242 ) , the linear program we need to solve has @xmath243 million variables and about the same number of constraints . even a commercial linear programming solver ( such as cplex ) equipped with the ability to exploit structure in this program will require several hours on a powerful computer to solve this program .",
    "this is in stark contrast with an index based heuristic ( such as whittles heuristic ) that solves a simple dynamic program for each arm at every time step . in the next section we develop an efficient computational algorithm for the solution of @xmath89 that requires substantially less effort than even whittles heuristic and takes a few minutes to solve the aforementioned program on a laptop computer .",
    "this section considers the computational effort required to implement the packing heuristic .",
    "we develop a computational scheme that makes the packing heuristic substantially easier to implement than popular index heuristics such as whittle s heuristic and thus establish that the heuristic is viable from a computational perspective .    the key computational step in implementing",
    "the packing heuristic is the solution of the linear program @xmath89 . assuming that @xmath244 and @xmath245 for all @xmath16 , this linear program has @xmath246 variables and each newton iteration of a general purpose interior point method will require @xmath247 steps .",
    "an interior point method that exploits the fact that bandit arms are coupled via a single constraint will require @xmath248 computational steps at each iteration . we develop a combinatorial scheme to solve this linear program that is in spirit similar to the classical dantzig - wolfe dual decomposition algorithm . in contrast with dantzig - wolfe decomposition ,",
    "our scheme is efficient .",
    "in particular , the scheme requires @xmath249 computational steps to solve @xmath89 making it a significantly faster solution alternative to the schemes alluded to above .",
    "equipped with this fast scheme , it is notable that using the packing heuristic requires @xmath250 computations per time step amortized over the time horizon which will typically be substantially less than the @xmath251 computations required per time step for index policy heuristics such as whittle s heuristic .",
    "our scheme employs a ` dual decomposition ' of @xmath89 .",
    "the key technical difficulty we must overcome in developing our computational scheme for the solution of @xmath89 is the non - differentiability of the dual function corresponding to @xmath89 at an optimal dual solution which prevents us from recovering an optimal or near optimal policy by direct minimization of the dual function .      for each bandit arm @xmath16 ,",
    "define the polytope @xmath252 of permissible state - action frequencies for that bandit arm specified via the constraints of @xmath89 relevant to that arm .",
    "a point within this polytope , @xmath253 , corresponds to a set of valid state - action frequencies for the @xmath16th bandit arm . with some abuse of notation ,",
    "we denote the expected reward from this arm under @xmath253 by the ` value ' function : @xmath254 in addition denote the expected number of pulls of bandit arm @xmath16 under @xmath253 by @xmath255 we understand that both @xmath256 and @xmath257 are defined over the domain @xmath258 .",
    "we may thus rewrite @xmath89 in the following form :    @xmath259    the lagrangian dual of this program is @xmath260 :    @xmath261    the above program is convex . in particular , the objective is a convex function of @xmath262 .",
    "we will show that strong duality applies to the dual pair of programs above , so that the optimal solution to the two programs have identical value .",
    "next , we will observe that for a given value of @xmath262 , it is simple to compute @xmath263 via the solution of a dynamic program over the state space of arm @xmath16 ( a fast procedure ) .",
    "finally it is simple to derive useful a - priori lower and upper bounds on the optimal dual solution @xmath264 .",
    "thus , in order to solve the dual program , one may simply employ a bisection search over @xmath262 . since for a given value of @xmath262",
    ", the objective may be evaluated via the solution of @xmath2 simple dynamic programs , the overall procedure of solving the dual program @xmath260 is fast .",
    "what we ultimately require is the optimal solution to the primal program @xmath89 .",
    "one natural way we might hope to do this ( that ultimately will not work ) is the following : having computed an optimal dual solution @xmath264 , one may hope to recover an optimal primal solution , @xmath265 ( which is what we ultimately want ) , via the solution of the problem @xmath266 for each @xmath16 .",
    "this is the typical dual decomposition procedure .",
    "unfortunately , this last step _ need not _ necessarily yield a feasible solution to @xmath89 .",
    "in particular , solving for @xmath267 may result in an arbitrarily suboptimal solution for any @xmath268 , while solving for a @xmath269 may yield an infeasible solution to @xmath89 .",
    "the technical reason for this is that the lagrangian dual function for @xmath270 may be non - differentiable at @xmath264 .",
    "these difficulties are far from pathological , and example [ eg ] illustrates how they may arise in a very simple example .",
    "[ eg ] the following example illustrates that the dual function may be non - differentiable at an optimal solution , and that it is not sufficient to solve for @xmath269 or @xmath271 for an @xmath272 arbitrarily small .",
    "specifically , consider the case where we have @xmath273 identical bandits , @xmath274 , and @xmath275 .",
    "each bandit starts in state @xmath59 , and two actions can be chosen for it , namely , @xmath82 and the idling action @xmath276 .",
    "the rewards are @xmath277 and @xmath278 .",
    "thus , @xmath89 for this specific case is given by : @xmath279 where @xmath280 , @xmath281 .",
    "clearly , the optimal objective function value for the above optimization problem is 1 .",
    "the lagrangian dual function for the above problem is @xmath282 not the dual function is minimized at @xmath283 , which is a point of non - differentiability .",
    "moreover , solving at @xmath284 for any @xmath272 , gives @xmath285 which is clearly suboptimal .",
    "also , a solution for @xmath286 is @xmath287 , which is clearly infeasible .",
    "notice that in the above example , the average of the solutions to problem for @xmath288 and @xmath289 _ does _ yield a feasible , optimal primal solution , @xmath290 .",
    "we overcome the difficulties presented by the non - differentiability of the dual function by computing both upper and lower approximations to @xmath264 , and computing solutions to for both of these approximations .",
    "we then consider as our candidate solution to @xmath89 , a certain convex combination of the two solutions .",
    "in particular , we propose algorithm [ alg : solver ] , that takes as input the specification of the bandit and a tolerance parameter @xmath291 .",
    "the algorithm produces a feasible solution to @xmath89 that is within an additive factor of @xmath292 of optimal .",
    "@xmath293 . for all @xmath16 , @xmath294 , + @xmath295 .",
    "@xmath296 @xmath297 .",
    "@xmath298 @xmath299 @xmath300 @xmath301 @xmath302    it is clear that the bisection search above will require @xmath303 steps ( where @xmath304 ) . at each step in this search",
    ", we solve @xmath2 problems of the type in , i.e. @xmath305 . these subproblems may be reduced to a dynamic program over the state space of a single arm .",
    "in particular , we define a reward function @xmath306 according to @xmath307 and compute the value of an optimal policy starting at state @xmath308 ( where @xmath309 is that state on which @xmath102 places mass @xmath26 ) assuming @xmath310 as the reward function .",
    "this requires @xmath311 steps per arm .",
    "thus the rlp solver algorithm requires a total of @xmath312 computational steps prior to termination .",
    "the following theorem , proved in the appendix establishes the quality of the solution produced by the rlp solver algorithm :    [ th : solver_correctness ] rlp solver produce a feasible solution to @xmath89 of value at least @xmath313 .",
    "the rlp solver scheme was used for all computational experiments in the previous section . using this scheme ,",
    "the largest problem instances we considered were solved in a few minutes on a laptop computer .",
    "this paper introduced ` irrevocable ' policies for the multi - armed bandit problem .",
    "we hope to draw two main conclusions from our presentation thus far .",
    "first , in addition to being a desirable constraint to impose on a multi - armed bandit policy , irrevocability is frequently a _",
    "cheap _ and thereby practical constraint .",
    "in particular , we have attempted to show via a host of computational results as also a theoretical analysis that the price of irrevocability , i.e. the performance loss incurred relative to an optimal scheme with no restrictions on exploration , is likely to be small in practice .",
    "in fact , the performance of the heuristic we developed suggests that it is likely to be useful even when irrevocability is not a concern .",
    "second , we have shown that computing good irrevocable policies for multi - armed bandit problems is easy .",
    "in particular , we developed a fast computational scheme to accomplish this task .",
    "this scheme is faster than a widely used heuristic for general multi - armed bandit problems .",
    "this research serves as a point of departure for a number of interesting questions that we believe would be interesting to explore :    * the packing heuristic is one of many possible irrevocable heuristics .",
    "it is attractive since it offers satisfactory performance in computational experiments , affords a worst - case price of irrevocability analysis ( and so is theoretically robust ) , and finally can be implemented with less computational efforts than typical index heuristics .",
    "the packing heuristic is , however , by no means the only irrevocable heuristic one may construct .",
    "an alternative for instance , would be to modify whittle s heuristic so as to simply ignore bandits that were discarded in the past or equivalently formulate a corresponding restless bandit problem where discarded arms generate no rewards .",
    "the present work establishes irrevocability as a _ cheap _ constraint to place on many multi - armed bandit problems . moving forward",
    ", one may hope to construct other high performing irrevocable heuristics for the multi - armed bandit problem . * what happens to the price of irrevocability in interesting asymptotic parameter regimes ?",
    "an example of such a regime may include for instance , simultaneously scaling the number of bandits , the number of simultaneous plays allowed , @xmath4 , as also the horizon @xmath6 .",
    "the correlation in the reward earned from a bandit arm and the length of time the arm is pulled preclude a useful , straightforward large deviations type extension to our analysis .",
    "nonetheless , this is an important question to ask .",
    "* we have illustrated the decreasing returns property for coins .",
    "this property is fairly natural though , and there are a number of other types of bandits that may well possess this property . staying in the vain of dynamic assortment selection , it may well be the case that incorporating inventory and pricing decisions for each product may still yield bandits satisfying this property . * in addition to bandits satisfying the decreasing returns property , are there other interesting classes of bandits that afford a low price of irrevocability ?",
    "* is irrevocability a cheap constraint for interesting classes of _ restless _ bandit problems ?",
    "given our results , it is intuitive to expect that this may be the case for bandits with switching costs which represent one simple class of restless bandit problems .",
    "our work thus far suggests that irrevocable policies are an effective means of extending the practical applicability of multi - armed bandit approaches in several interesting scenarios such as dynamic assortment problems or sequential drug trials where recourse to drugs that were discontinued from trials at some point in the past is socially unacceptable .",
    "progress on the questions above will likely further the goal of extending the practical applicability of the multi - armed bandit approach .",
    "@xmath92    let @xmath314 be an optimal solution to @xmath77 . we construct a feasible solution to @xmath89 of equal value .",
    "in particular , define a candidate solution to @xmath89 , @xmath93 according to @xmath315 this solution has value precisely @xmath316 .",
    "it remains to establish feasibility . for this",
    "we first observe that @xmath317    next , we observe that the expected total number of pulls of arm pulls under the policy prescribed by @xmath314 is simply @xmath318 since the total number of pulls in a given time step under @xmath314 is at most @xmath4 , we have @xmath319 but , @xmath320 so that @xmath321    from and , @xmath93 is indeed a feasible solution to @xmath89 .",
    "this completes the proof .",
    "coins satisfy the decreasing returns property .",
    "that is , if @xmath154 , and @xmath155 then @xmath156 - e[r_i^{m } ] \\leq e[r_i^m ] - e[r_i^{m-1}]\\ ] ] for all @xmath157 .    to see that coins satisfy the decreasing returns property , we first introduce some notation .",
    "it is clear that the policy @xmath322 induces a markov process on the state space @xmath17 .",
    "we expand this state space , so as to track the total number of arm pulls so that our state space now become @xmath323 .",
    "the policy @xmath322 induces a distribution over arm @xmath16 states for every time @xmath324 , which we denote by the variable @xmath314 .",
    "thus , @xmath325 will denote the probability of being in state @xmath326 at time @xmath47 and taking action @xmath36 .",
    "now , @xmath327 = \\sum_{s , t < t } \\hat{\\pi}(s_i , m , t , p)r(s , p)\\ ] ] and similarly , for @xmath328 $ ] .",
    "but , @xmath329 \\end{split}\\ ] ] where @xmath330 . here",
    "@xmath331 is the probability of never pulling the arm after reaching state @xmath34 at time @xmath47 so that @xmath332 represents the probability of eventually pulling arm @xmath16 after reaching state @xmath34 at time @xmath47 .",
    "the second inequality follows from the assumption on reward structure in the statement of the lemma .",
    "we thus see that coins satisfy the decreasing returns property .    for bandits satisfying the decreasing returns property ( property [ as : dec_returns ] ) , @xmath188 \\geq \\frac{1}{2}e[r_{1/2}].\\ ] ]",
    "we note that assuming property [ as : dec_returns ] implies that @xmath333 \\geq \\frac{1}{2}e[r_i]$ ] for all @xmath16 .",
    "the assertion of the lemma is then evident  in particular , @xmath179 & = \\sum_{i=1}^{h^*}{\\rm pr}\\left(\\sum _ { j=1}^{i-1 }",
    "t_j < kt/2\\right)e[r_i ] \\\\ & \\leq \\sum_{i=1}^{h^*}{\\rm pr}\\left(\\sum _ { j=1}^{i-1 } t_j < kt/2\\right)2e[r_i^{t/2 } ] \\\\ & = 2 e\\left[\\sum_{i=1}^{h^*}\\mathbf{1}_{\\sum_{j=1}^{i-1}t_j < kt/2 } r_i^{t/2}\\right ] \\end{split}\\ ] ] where the first and second equality use the fact that @xmath180 and @xmath334 are each independent of @xmath181 for @xmath335 .",
    "the following lemma shows that the optimal objective function value of the dual is equal to @xmath135 .",
    "in particular , it shows that slater s constraint qualification condition holds  ( see , for example , @xcite ) .      to show this",
    ", it is sufficient to show that there is a strictly feasible solution to  ( [ eqn : mod_rlp ] ) , i.e. , the inequality is satisfied strictly .",
    "this is straightforward  in particular , for each bandit @xmath16 , set @xmath337 for all @xmath34 and @xmath47 , where @xmath338 is the probability of bandit @xmath16 starting in state @xmath34 .",
    "set @xmath339 for @xmath340 for all @xmath341 .",
    "these state action frequencies belong to @xmath258 , and also give @xmath342 .",
    "we denote the objective function in @xmath260 , i.e. , the dual function by : @xmath347 the slack in the total running time constraint @xmath348 , i.e. @xmath349 , is a subgradient of @xmath350 for any @xmath351 such that @xmath352  ( see @xcite ) .",
    "thus , the set of subgradients of the dual function @xmath350 at @xmath262 are given by @xmath353 then , since @xmath350 is a convex function , it follows that for @xmath345 , @xmath354 the lemma then follows .",
    "we prove the correctness of the @xmath357 solver algorithm separately for the cases when @xmath358 is optimal , and when any optimal solution satisfies @xmath359 .",
    "we denote the values of the bounds on the dual variable that are computed by the _",
    "last iteration _ of the @xmath357 solver algorithm by @xmath360 and @xmath361 .",
    "recall that , @xmath362 we introduce some additional notation : @xmath363 thus , @xmath364      if @xmath358 , it follows from that there is some @xmath367 such that @xmath368 .",
    "hence , it follows from lemma  [ lem : monotone ] that for any @xmath369 , @xmath370 .",
    "hence , line  11 of the @xmath357 solver algorithm is always invoked , and so , the @xmath357 solver algorithm converges to @xmath371 hence , @xmath372 . also , @xmath373 is minimized at @xmath365 .",
    "hence , it follows from lemma  [ lem : slater ] that @xmath374 since , @xmath375 , it follows from @xmath376 .",
    "hence , we now consider the following three cases :    * _ case 1 : _",
    "@xmath377 . + here , @xmath378 , and",
    "hence , using it follows that @xmath379 * _ case 2 : _",
    "+ in this case , @xmath381 satisfy the optimality conditions in .",
    "thus , @xmath382 , and so ( since @xmath383 by ) @xmath384 * _ case 3 : _ @xmath385 .",
    "+ since , @xmath373 is minimized at @xmath386 , @xmath387 since , @xmath388 ( from ) , and using the fact that @xmath389 when @xmath390 , we have @xmath391      the @xmath357 solver algorithm is initialized with @xmath394 . since , @xmath359 , and @xmath395 ( ) , it follows from lemma  [ lem : monotone ] that @xmath396 . but",
    "@xmath397 , else there would be a solution to that satisfies @xmath365 , leading to a contradiction .",
    "thus , @xmath398 , and so lines 812 of the @xmath357 solver algorithm guarantee that @xmath399 using an appropriate modification of the optimality conditions in for the case where the horizon is @xmath400 ( instead of @xmath15 ) , we see that @xmath401 is the maximum reward earned by any policy in @xmath402 . since , @xmath403 is the maximum reward earned by any policy in @xmath404 , @xmath405    we now argue that @xmath406 .",
    "the @xmath357 solver algorithm is initialized with @xmath407 . since , @xmath408 ,",
    "initially , the optimal policy is to idle at all times .",
    "thus , @xmath406 at initialization ; at all other iterations , lines 812 of the algorithm ensure that @xmath406 .",
    "* _ case 1 : _",
    "@xmath380 . + in this case , @xmath381 satisfy the optimality conditions in , and so , @xmath382 .",
    "now , using @xmath409 * _ case 2 : _ @xmath410 . + note that the @xmath357 solver algorithm terminates when @xmath411 + now @xmath410 and @xmath395 . if @xmath412 , it follows from lemma  [ lem : monotone ] that @xmath413 which is a contradiction . hence , @xmath414 + also , since @xmath395 , it follows from lemma  [ lem : monotone ] that for any @xmath415 , @xmath416 .",
    "so , implies that @xmath417 + it follows from , , that @xmath418 + since @xmath373 is minimized at @xmath264 , it follows from and strong duality proved in lemma  [ lem : slater ] that @xmath419 where @xmath420 .",
    "note that the above inequalities also use @xmath421 ( by assumption ) and @xmath422 ( from ) .",
    "thus , @xmath423"
  ],
  "abstract_text": [
    "<S> this paper considers the multi - armed bandit problem with multiple simultaneous arm pulls . </S>",
    "<S> we develop a new ` irrevocable ' heuristic for this problem . in particular </S>",
    "<S> , we do not allow recourse to arms that were pulled at some point in the past but then discarded . </S>",
    "<S> this irrevocable property is highly desirable from a practical perspective . as a consequence of this property , </S>",
    "<S> our heuristic entails a minimum amount of ` exploration ' . at the same time , we find that the price of irrevocability is limited for a broad useful class of bandits we characterize precisely . </S>",
    "<S> this class includes one of the most common applications of the bandit model , namely , bandits whose arms are ` coins ' of unknown biases . </S>",
    "<S> computational experiments with a generative family of large scale problems within this class indicate losses of up to @xmath0 relative to an upper bound on the performance of an optimal policy with no restrictions on exploration . </S>",
    "<S> we also provide a worst - case theoretical analysis that shows that for this class of bandit problems , the price of irrevocability is uniformly bounded : our heuristic earns expected rewards that are always within a factor of @xmath1 of an optimal policy with no restrictions on exploration . </S>",
    "<S> in addition to being an indicator of robustness across all parameter regimes , this analysis sheds light on the structural properties that afford a low price of irrevocability . </S>"
  ]
}