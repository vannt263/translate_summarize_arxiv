{
  "article_text": [
    "it happens often that the physical or mathematical model behind an experiment or dataset is not known .",
    "hence , model selection ( sometimes called subset selection ) becomes an important feature in any data analysis method .",
    "the methodology of selecting the best model has constantly been examined by many authors .",
    "identifying the best subset among many variables is the most difficult part of model construction .",
    "the latter is exacerbated as the number of possible subsets grows exponentially if the number of variables ( parameters ) grows linearly .",
    "there is also a chance that the original input parameters to a model do not convey enough information , and some transformations of the original parameters are needed to make the data more available for information extraction .    in other words , in a supervised learning terminology",
    ", there is a long and unpaved journey between the _ inputs _ ( also called _ predictors , features _ or _ independent variables _ ) and the _ outputs _ ( also called _ responses _ or _ dependent variables _ ) .",
    "thus , the difficulty is not only embedded in picking the right machine learning algorithm for the problem at hand , but also in picking proper transformations of the inputs and their subsets .",
    "there are different methods capable of addressing transformations and subset selection .",
    "however , to the best of our knowledge , none of these methods solves both issues simultaneously .    in our discussions in this paper",
    ", we denote an input variable by an @xmath3 vector @xmath4 as a collection of @xmath5 observations of an input @xmath6 .",
    "the assembly of @xmath7 such inputs and an intercept is denoted by an @xmath8 matrix @xmath9 .",
    "the output is denoted by an @xmath3 vector @xmath10 .",
    "for example , based on this description , a linear model is defined as @xmath11 where @xmath12 is the @xmath3 noise vector , and @xmath13 is a @xmath14 vector of coefficients with the first element being the _ intercept _ ( or _ bias _ ) of the model .    in what follows next , we review a series of methods and algorithms are used to find some subset(s ) of the inputs that could possibly relate the inputs to outputs in an efficient way",
    ".      there are currently various methods for selecting predictors , such as the traditional best subset selection , forward selection , backward selection and stepwise selection methods @xcite . in general",
    ", the best subset procedure finds for each @xmath15 , the subset of inputs of size @xmath16 that minimizes the residual sum of squares ( rss ) @xcite .",
    "there are fast algorithms optimizing the search @xcite .",
    "however , searching through all possible subsets could become laborious as @xmath7 increases .",
    "a number of automatic subset selection methods seek a subset of all inputs , that is as close as possible to the best subset method @xcite .",
    "these methods select a subset of predictors by an automated algorithm that meets a predefined criterion , such as the level of significance ( set by the analyst ) .",
    "for example , the forward selection method @xcite starts with no predictors in the model .",
    "it then adds predictors one at a time until no available predictors can contribute significantly to the response variable .",
    "once a predictor is included in the model , it remains there . on the other hand ,",
    "the backward elimination technique @xcite works in the opposite direction and begins with all the existing predictors in the model , then discards them one after another until all remaining predictors contribute significantly to the response variable .",
    "stepwise subset selection @xcite is a mixture of the forward and backward selection methods .",
    "it modifies the forward selection approach in that variables already in the model do not always remain in the model .",
    "indeed , after each step in which a variable is added , all variables in the model are reevaluated via their partial @xmath17 or @xmath18 statistics and any non - significant variable is removed from the model .",
    "the stepwise regression requires two cutoff values for significance : one for adding variables and one for discarding variables . in general the probability threshold for adding variables should be smaller than the probability threshold for eliminating variables @xcite .",
    "subset selection methods are usually based on targeting models with the largest @xmath19 , or in other words smallest root mean square error ( rmse ) . however , there are other methods in which the selection model is based on mallow s @xmath20 @xcite .",
    "these criteria highlight different aspects of the regression model . as a results",
    ", they can lead to models that are completely different from each other and yet not optimal .",
    "there are also other issues regarding the traditional subset selection regression methods .",
    "they could lead to models that are unreliable for prediction because of over - fitting issues . more specifically",
    ", they could generate models that have variables displaying a high degree of multicollinearity .",
    "such methods can lead to @xmath21 values that are biased and yield to confidence limits that are far too narrow or wide .",
    "moreover , the selection criterion primarily relies on the correlation between the predictor(s ) and the dependent variable .",
    "thus , these methods ( e.g. stepwise method @xcite ) do not take into consideration the correlation within the predictors themselves .",
    "the latter is a source of multicollinearity that is not addressed automatically by these mentioned methods @xcite .",
    "indeed when collinearity among the predictors exists , the variance of the coefficients is inflated , rendering the overall regression equation unstable . to address this issue ,",
    "a number of _ penalized regression _ or _ shrinkage _",
    "approaches are available .",
    "for example , the ridge method tries to eliminate the multicollinearity by imposing a penalty on the size of the regression coefficients @xcite .",
    "indeed , a model is fitted with all the predictors , however , the estimated coefficients are shrunken towards zero relative to the least squared estimates . therefore , biased estimators of regression coefficients are obtained , reducing the variance and thus leading to a more stable equation .    solving for @xmath22 in equation ( [ eq : linear model ] ) using the least squares ( ls ) method would be equivalent to solving @xmath23 ridge regression , on the other hand , places a constraint on the estimator @xmath22 in order to minimize a penalized sum of squares @xcite @xmath24 the complexity parameter @xmath25 controls the amount of shrinkage .",
    "large values of this parameter would result in a large shrinkage .",
    "the value of the constant @xmath26 is predefined by the analyst and is usually selected in order to stabilize the ridge estimators , producing an improved equation with a smaller rmse compared to the least - squares estimates .",
    "one weakness of the ridge method is that it does not select variables . indeed , unlike the subset selection method , it includes all of the predictors in the final model with shrunken coefficients .",
    "the other weakness is that multicollinearity is not addressed .",
    "in fact , the ridge estimate of variables in ( [ eq : ridge least squares ] ) only shrinks the coefficients even for the inputs with multicollinearity .",
    "however , the ridge method does not fix multicollinearity , it only alleviates it .      to obtain variable selection procedures , there are shrinkage methods available such as least absolute shrinkage and selection operator ( lasso ) , where the penalty involves the sum of the absolute values of the coefficients @xmath22 excluding the intercept @xcite .",
    "lasso is closely related to sparse optimization found in works by candes and tao @xcite .",
    "taking @xmath27 , the lasso method can be presented as the following optimization problem @xmath28 where @xmath29 is the @xmath30 norm of @xmath31 and @xmath32 .",
    "the advantage of lasso is that much like the best subset selection method , it performs variable selection .    the parameter @xmath26 is usually selected by cross validation . for a small @xmath26 , the result is equal to the least squares estimates . as the value of @xmath26 augments",
    ", shrinkage happens in such a way that only a sparse number of variables having an active role in the final model would show up .",
    "thus , lasso is a combination of both shrinkage and variables selection .",
    "least angle regression ( lar ) is a new model of automatic subset selection based on a modified version of forward procedure @xcite .",
    "the lar method follows an algorithmic procedure : first , the independent variables are standardized in order to obtain a mean zero . at this stage ,",
    "the @xmath22 coefficients are all equal to zero . then the predictor that most correlates to the response variable",
    "is selected ; its coefficient is then shifted from zero towards its least squares value .",
    "now , once a second predictor becomes as correlated with the existing residual as the first predictor , the procedure is paused .",
    "the second predictor is then added to the model .",
    "this procedure then continues until all desired predictors are included in the model , leading to a full least - squares fit .",
    "the method of least angle regression with lasso modification is very similar to the above procedure , however it includes an extra step : if a coefficient approaches zero , lar excludes its predictor from the model and recalculates the joint least squares path @xcite .",
    "lar methods and its variations are better subset selector algorithms compared to most of the subset selection methods .",
    "another selection approach is the dantzig selector @xcite , which can be formulated as @xmath33 subject to @xmath34 . here , @xmath35 is the @xmath36 norm , that is the maximum of its argument .",
    "the objective of this method is to minimize the maximum inner product of the existing residual with all the independent variables .",
    "this approach has the capacity of recovering an underlying sparse coefficient vector .",
    "lastly , principal component regression ( pcr ) is a method that involves an orthogonal transformation to address multicollinearity @xcite .",
    "this approach is closely related to the singular value decomposition ( svd ) method @xcite .",
    "pcr applies dimensionality reduction and decreases multicollinearity by using a subset of the principal components in the model @xcite .",
    "pcr is one of very few methods that tries to eliminate multicollinearity with linear transformations and , at the same time , perform a regression .",
    "the various approaches described earlier aim to select the best set of relevant variables from an original set . with the exception of the pcr method ,",
    "in which there is linear transformations , variables transformations are not incorporated among predictors in any of the methods mentioned above .",
    "these traditional methods do not offer the option of automatic variable transformation to address polynomial curvilinear relationships .",
    "the analyst usually needs to manually apply polynomial , logarithmic , square - root and interaction - between - variables transformations in order to address non - linearity of the data .",
    "there are a number of non - linear transformation procedures currently available such as box - cox or box - tidwell @xcite .",
    "these methods are relatively efficient in finding the dependent and independent variables transformations . in box - tidwell method",
    "@xcite , independent variables are transformed using a recursive newton algorithm . as a result , it becomes susceptible to round - off errors which would in turn result in unstable and improper transformations @xcite . despite the relative success of these methods",
    ", there is no automatic variable selection embodiment with them .",
    "artificial neural networks ( ann ) are the current state of the art method in transformations and capturing non - linearity @xcite .",
    "ann is a machine learning method that finds some non- linear transformations of the inputs using layers of nodes . despite the efficient performance in capturing the non - linearity of the data ,",
    "the model itself is not comprehensible particularly if there is a physical component to the data that one needs to understand . in other words , ann is a perfect black box model , but not a good interpretation medium for understanding physical and mathematical mechanism(s ) behind the observed data .",
    "as mentioned earlier , only the pcr method performs linear transformations automatically , and also picks variables . however , pcr is not enough when non - linearity is present . on the other hand ,",
    "ann has the best capability in capturing non - linearities , but acts like a black box and does not lend insight into the physical and mathematical mechanism(s ) behind the observed data .    to produce an enhanced subset of the original variables , an ideal selection method should have the potential of adding a supplementary level of regression analysis that would capture complex relationships in the data via mathematical transformation of the predictors and exploration of synergistic effects of combined variables .",
    "the method that we present here has the potential to produce an optimal subset of variables , resulting in a more efficient overall process of model selection .",
    "the core objective of this paper is to introduce a new estimation technique for the classical least square regression framework .",
    "this new automatic variable transformation and model selection method could offer an optimal and stable model that minimizes the mean square error and variability , while combining all possible subset selection methodologies and including variable transformations and interaction .",
    "moreover , this novel method controls multicollinearity , leading to an optimal set of explanatory variables .",
    "the resultant model is also easy to interpret . in other words",
    ", we will depict a method that addresses variable selection and transformation at the same time ; and also helps the analyst make interpretations about the physical and mathematical mechanism(s ) behind the observed data .",
    "as mentioned in the previous section , the analyst usually needs to manually apply polynomial , logarithmic , square - root and interaction between variables in order to address the non - linearity of the data . at the same time",
    ", the analyst should also look for the best model from a subset of the variables .",
    "however , this manual burden can be made automatic .",
    "we will first address the issue of transformations and subset selection separately and then put them together .",
    "there are four important categories of transformations to capture the non - linearity in a data set @xcite .",
    "these transformations are    1 .",
    "logarithm transformation of a positive variable ; i.e. @xmath37 2 .",
    "square - root transformation of a positive variable ; i.e. @xmath38 3 .",
    "integer powers up to a certain amount @xmath39 ; i.e. @xmath40 4 .",
    "interaction between terms created in 1 - 3 to a certain mixture number @xmath41 ; e.g. , for @xmath42 , possible candidates for would be @xmath43 , @xmath44 , @xmath45 , @xmath46 , @xmath47 and @xmath48 .    after the construction of these transformations",
    ", one can start to look for the best model , for @xmath10 , among the set of all mentioned transformations 1 - 4 . in other words , denoting the set of variables created by transformations 1 - 4 as @xmath49 , we are looking for the best sparse model @xmath50 where some elements of @xmath51 are zero .",
    "in fact , some elements of @xmath51 are zero because there is a chance that some columns of @xmath49 are linearly dependent or that they do not contribute to any correlation to @xmath10 .    up to this stage",
    ", we see that using transformations , we can expand the basis of projection .",
    "this would help us to find better correlations between transformed variables and the output @xmath10 .",
    "in other words , using these transformations , we essentially convert the problem into a dictionary search @xcite . as we mentioned before some of the transformations might not be of any importance in ( [ eq : transformation raw ] ) , or could be redundant due to multicollinearity .",
    "we can address these two issues , by a modified dictionary search algorithm , as follows :    * any column of @xmath49 that has a non - significant correlation ( less than @xmath52 ) with @xmath10 can be discarded . in other words , the corresponding elements of @xmath51 , having a small importance , will be set to zero .",
    "* any two columns of @xmath49 that have a high correlation with each other ( greater than @xmath53 ) are redundant columns . between these two",
    ", one should pick the one that has a higher correlation with @xmath10 and discard the other .",
    "in other words , the element in @xmath51 that corresponds to a redundancy with a weaker correlation to @xmath10 can be set to zero .    as a result of this methodology",
    ", we can now solve model ( [ eq : transformation raw ] ) for only a reduced matrix .",
    "hence , although we construct a matrix dictionary of transformations @xmath49 that is larger than the original input matrix @xmath54 , we can reduce the size of @xmath49 as some elements of @xmath51 are now set to zero .",
    "we denote this reduced matrix as @xmath55 and its corresponding vector of coefficients as @xmath56 .",
    "this reduction is similar to basis pursuit ( bp ) method presented in @xcite .    after reducing the matrix of transformations",
    ", the final task is to find the best subset of the columns in @xmath55 to model the data in @xmath10 .",
    "the latter can be done by any method of subset selection ; e.g. best subset selection . for simplicity in referencing to our proposed method",
    ", we denote the whole procedure as parameter selection algorithm ( parseal ) . in the next section , we formalize the methodology that we explained here .",
    "the goal of the parseal is to find the best model explaining @xmath10 from some important transformations on the original observed variables @xmath54 .",
    "the latter is nothing but finding the best subspace using some specific transformations from the original inputs .",
    "the parseal is presented in algorithm [ alg : parseal ] .",
    "this algorithm is equipped with the best subset selection method to find the best model by maximizing the @xmath19 among all possible subset of variables in @xmath55 .",
    "step 1 of this algorithm is input specification .",
    "step 2 is where the dictionary of transformations and interactions is made .",
    "steps 3 and 4 correspond to the elimination of columns of the dictionary of transformations and interactions which involve either a non - significant correlation to the output or multicollinearity between the dictionary elements .",
    "step 5 is where the best model is finally found .",
    "steps 2 , 3 and 5 in this algorithm can be made parallel to decrease the computational time of the method . to our best knowledge ,",
    "algorithm [ alg : parseal ] is the first method that performs both variable transformation and model selection while adding interaction terms and also preventing multi - collinearity , in one package .",
    "the importance limit @xmath52 and independence limit @xmath53 are important factors in controlling the speed of convergence of the parseal . in algorithm",
    "[ alg : parseal ] , the smaller the value of @xmath52 ( similarly , the larger the value of @xmath53 ) , the bigger the space of search in step 5 . as a result ,",
    "the speed of convergence would depend greatly on these two parameters .    in step 5 ,",
    "checking for variation inflation factors ( vifs ) @xcite is a necessary condition to make sure that no multicollinearity is introduced into the final model .",
    "vifs are the elements of the main diagonal of the inverse of the multiplication of the input matrix transposed with the input matrix .",
    "for example if @xmath54 is the input , then @xmath57 and @xmath58 . checking the vif is an essential multicollinearity diagnosis tool @xcite .    1 .",
    "inputs to the algorithm : 1 .",
    "original input matrix @xmath54 2 .",
    "output vector @xmath10 3 .",
    "maximum polynomial power @xmath59 4 .",
    "maximum term interaction number @xmath41 5 .",
    "importance limit @xmath52 6 .",
    "independence limit @xmath53 2 .",
    "construct matrix @xmath49 from @xmath54 by taking logarithm , square - root , polynomial and interactions .",
    "eliminate columns of @xmath49 having an absolute - value correlation less than the importance limit @xmath52 : @xmath60 4 .",
    "for any two columns of @xmath60 having an absolute - value correlation greater than the independence limit @xmath53 , pick the column that has a higher absolute - value correlation with @xmath10 : @xmath55 5 .",
    "find the best subset of variables in @xmath55 against @xmath10 by maximizing @xmath19 while preventing the vif of any variable exceeding 10 .",
    "in fact , the independence limit @xmath53 can be characterized with the vif concept .",
    "as mentioned , @xmath58 , however , this formula can also be written as @xmath61 here , @xmath62 is the multiple @xmath21 for the regression of @xmath63 against other inputs .",
    "hence , if we want two inputs to have a small correlation with each other , we must have a possible vif between them to be less than 10 .",
    "this would impose an @xmath64 between those variables .",
    "hence , a correlation of @xmath65 would say if two inputs are highly correlated or not . on the other hand ,",
    "we know that if we set the independence limit @xmath66 , we would construct a huge dictionary of inputs when transformations are available .",
    "hence , based on our numerical experiments , a value of @xmath67 is more practical .",
    "in this section , we provide a few synthetic examples using parseal . in the following examples ,",
    "we try to show that the algorithm that we have proposed is capable of finding the non - linear transformations in a model .",
    "taking @xmath68 , @xmath69 , @xmath70 to be a uniformly distributed random variable in @xmath71,$ ] we sampled @xmath72 data points and then created the non - linear functional @xmath73 .",
    "we take the original input matrix @xmath54 to be composed of all @xmath68 , @xmath69 , and @xmath70 . using the traditional best subset selection @xcite , accompanied with a control over vifs not to get above @xmath74",
    ", we get the results shown in figure [ fig : ex1 original data vs model data ] . from this figure , it is clear that the best subset selection model is not capable of capturing the correct non - linearity in the model .",
    "the heteroscedasticity of the residual plot can be seen from figure [ fig : ex1 residuals ] .",
    "the found subset of parameters is @xmath75 .    on the other hand ,",
    "if algorithm [ alg : parseal ] is used , the non - linearity is captured completely by our method ( see figures [ fig : ex1 original data vs model data parseal ] and [ fig : ex1 residuals for parseal ] ) .",
    "the subset of parameters found by our method is the model non - linear parameter @xmath76 . here , the importance limit @xmath52 was half of the maximum absolute value of the correlation of all columns in @xmath77 compared to the output .",
    "the independence limit was @xmath78 .",
    "if @xmath79 is a uniform random random variable in @xmath80 $ ] , then we set @xmath81 we sampled @xmath72 data points of @xmath68 , @xmath69 , and @xmath70 and then created the non - linear functional @xmath82 .",
    "we take the original input matrix @xmath54 to be composed of all @xmath68 , @xmath69 , and @xmath70 . using the traditional best subset selection @xcite , accompanied with a control over vifs not to get above @xmath74",
    ", we get the results shown in figure [ fig : ex2 original data vs model data ] . again , from this figure , it is clear that the best subset selection model is not capable of capturing the correct non - linearity in the model .",
    "the heteroscedasticity of the residual plot can be seen from figure [ fig : ex2 residuals ] .",
    "the found subset of parameters is @xmath83 .",
    "on the other hand , if algorithm [ alg : parseal ] is used , the non - linearity is captured completely ( see figures [ fig : ex2 original data vs model data parseal ] and [ fig : ex2 residuals for parseal ] ) .",
    "the subset of parameters found by our proposed method is the model non - linear parameter @xmath84 . here , the importance limit @xmath52 was half of the maximum absolute value of the correlation of all columns in @xmath77 compared to the output .",
    "the independence limit was @xmath85 .",
    "the synthetic examples in the previous section showed the capability of our method in capturing the true non - linearity . in this section , we show a real data case study .    cardiovascular diseases ( cvds ) are the major cause of deaths in the united states , killing more than 350,000 people every year @xcite .",
    "one of the major contributors to cvds is arterial stiffness @xcite .",
    "arterial stiffness can be approximated by carotid - femoral pulse wave velocity ( pwv ) @xcite .",
    "in fact , pwv is one of the most important quantitative index for arterial stiffness @xcite .",
    "pwv measures the speed of the arterial pressure waves traveling along the blood vessels and higher pwv usually highlights stiffer arteries .",
    "increased aortic stiffness is related to many clinically adverse cardiovascular outcomes @xcite .",
    "pwv constitutes an independent and valuable marker for cardiovascular diseases ( cvds ) and its use is crucial as a routine tool for clinical patient assessment .    in this section ,",
    "our aim is not to present the most accurate pwv model .",
    "however , our goal is to show that if our technique of model construction is used ( see algorithm [ alg : parseal ] ) , we are able to find a more physically and statistically valid model .    the data we present is collected from @xmath86 framingham heart study ( fhs ) participants @xcite . each participant had undergone an arterial tonometry data collection .",
    "the participants were part of fhs cohorts gen 3 exam 1 @xcite , offspring exam 7 @xcite , and original exam 26 @xcite . here",
    ", we try to find models for pwv based on the following inputs : age ( @xmath87 ) , pulse duration ( @xmath88 ) , weight ( @xmath89 ) , height ( @xmath90 ) , and body mass index ( @xmath91 ) .",
    "one model is based on the traditional best subset selection method monitored for @xmath92 , and the other based on the parseal method ( algorithm [ alg : parseal ] ) .",
    "the participant characteristics are shown in table [ tbl : participant char ] .",
    ".participant characteristics [ cols=\"^,^,^\",options=\"header \" , ]      figure [ fig : best subset pwv model ] shows the traditional best subset selection method applied on pwv data . as seen in the plot , the best subset selection model can not capture the non - linearity in the data set and completely misses the pwv values above @xmath93 .",
    "the heteroscedasticity of the residual can be seen from the bland - altman plot in figure [ fig : best subset pwv model ba ] and residual plot in figure [ fig : best subset pwv model hete ] .",
    "the @xmath19 of this model is @xmath94 .",
    "the found subset of parameters is @xmath95 .",
    "the p - value of these parameters are @xmath96 , @xmath97 , @xmath98 , and @xmath99 , receptively .",
    "figure [ fig : parseal pwv model ] shows the parseal method applied on pwv data . here , the importance limit @xmath52 was half of the maximum absolute value of the correlation of all columns in @xmath77 compared to the output .",
    "the independence limit was @xmath85 .",
    "as seen on the plot , parseal can fairly capture the non - linearity in the data set .",
    "the residuals can be seen in the bland - altman plot in figure [ fig : parseal pwv model ba ] and residual plot in figure [ fig : parseal pwv model hete ] .",
    "the @xmath19 of the model is @xmath100 .",
    "the found subset of parameters is @xmath101 .",
    "the p - value of these parameters are @xmath102 , @xmath97 , @xmath103 , and @xmath104 , receptively .      comparing figures [ fig : best subset pwv model ] and [ fig :",
    "parseal pwv model ] , it is clear that the parseal method is superior to the best subset selection method .",
    "the @xmath19 of the parseal model is almost @xmath105 better than the best subset selection method .",
    "both methods suffer in capturing all the variation and non - linearity in data ( compare figure [ fig : best subset pwv model ba ] to figure [ fig : parseal pwv model ba ] and figure [ fig : best subset pwv model hete ] to figure [ fig : parseal pwv model hete ] ) . however , parseal is better in this respect .",
    "the heteroscedasticity of the best subset selection method is worse than that of the parseal method ( compare figure [ fig : best subset pwv model hete ] to figure [ fig : parseal pwv model hete ] ) .",
    "the bland - altman limits of agreement of the parseal method is also better than those of the best subset selection method ( compare figure [ fig : best subset pwv model ba ] to figure [ fig : parseal pwv model ba ] ) .",
    "the latter shows that the parseal method is a more precise method than the best subset selection method .",
    "finally , we again mention that our goal is not to show the best possible model for pwv , but rather to show that with the same set of inputs , parseal is superior when compared to other model selection algorithms .",
    "in this paper , we have introduced parseal ( algorithm [ alg : parseal ] ) by which one can simultaneously capture some of the non - linearities of the data into the model and also pick the best model .",
    "this approach minimizes the efforts done by an analyst and is virtually automatic .",
    "so far , up to the best of our knowledge , no other algorithm or method is able to perform these two tasks at the same time automatically .",
    "furthermore , this method prevents multi - collinearity from entering the final model .    from the examples presented in this paper , and also the methodology used by the parseal method , we can claim that our method is one of the best model selector algorithms .",
    "parseal could have versatile applications in biostatistics as shown by one of the examples in this manuscript .    in future works",
    ", we intend to present some statistical analysis of the parseal method . in specific",
    ", we would like to investigate the speed of convergence of our method since parseal relies on creating a large dictionary of inputs .",
    "one of the important factors in creating the mentioned large dictionary of data is the importance limit @xmath52 , algorithm [ alg : parseal ] .",
    "the other is the independence limit @xmath53 .",
    "we would like to quantify optimum values for the importance limit @xmath52 and also the independence limit @xmath53 .",
    "we would also like to extend our method to non - integer powers .",
    "this would make parseal capable of better capturing non - integer non - linearities .",
    "another area of improvement could be adding transformations to the output variable .",
    "the data presented , in this paper , can be found at dryad digital repository .",
    "the unique identifier of the data package is doi:10.5061/dryad.c7s7d .",
    "we would like to thank dr .",
    "niema m. pahlevan and prof .",
    "morteza gharib for giving us the permission to use the framingham heart study data in this paper .",
    "the framingham heart study is conducted and supported by the national heart lung , and blood institute ( nhlbi ) in collaboration with boston university ( contract no .",
    "n01- hc-25195 ) .",
    "this manuscript was not prepared in collaboration with investigators of the framingham heart study and does not necessarily reflect the opinions or conclusions of the framingham heart study or the nhlbi .",
    "the california institute of technology and boston university medical center institutional review boards approved the protocol and all participants gave written informed consent .",
    "this study did not have fieldwork .",
    "this work was not funded .",
    "10    framingham heart study .",
    "https://www.framinghamheartstudy.org/. accessed : 2016 - 07 - 14 .",
    "american  heart association et  al . heart disease and stroke statistics  at - a - glance , 2015 .",
    "george  ep box and david  r cox .",
    "an analysis of transformations . ,",
    "pages 211252 , 1964 .",
    "george  ep box and paul  w tidwell .",
    "transformation of the independent variables .",
    ", 4(4):531550 , 1962 .",
    "candes and t.  tao . near - optimal signal recovery from random projections : universal encoding strategies ? , 52(12):54065425 , 2006 .",
    "emmanuel candes and terence tao .",
    "the dantzig selector : statistical estimation when p is much larger than n. , pages 23132351 , 2007 .",
    "chen , d.l .",
    "donoho , and m.a .",
    "atomic decomposition by basis pursuit .",
    ", 20(1):3361 , 1998 .",
    "thomas  r dawber , gilcin  f meadors , and felix  e moore  jr . epidemiological approaches to heart disease : the framingham study*. , 41(3):279286 , 1951 .",
    "bradley efron , trevor hastie , iain johnstone , robert tibshirani , et  al .",
    "least angle regression .",
    ", 32(2):407499 , 2004 .",
    "ma  efroymson .",
    "multiple regression analysis .",
    ", 1:191203 , 1960 .",
    "jerome friedman , trevor hastie , and robert tibshirani . ,",
    "volume  1 .",
    "springer series in statistics springer , berlin , 2001 .",
    "george  m furnival .",
    "all possible regressions with less computation . , 13(2):403408 , 1971 .",
    "george  m furnival and robert  w wilson .",
    "regressions by leaps and bounds . , 42(1):6979 , 2000 .",
    "mj  garside .",
    "the best sub - set in multiple regression analysis .",
    ", pages 196200 , 1965 .",
    "arthur  e hoerl and robert  w kennard .",
    "ridge regression : applications to nonorthogonal problems .",
    "12(1):6982 , 1970 .    arthur  e hoerl and robert  w kennard .",
    "ridge regression : biased estimation for nonorthogonal problems .",
    ", 12(1):5567 , 1970 .    william  b kannel , manning feinleib , patricia  m mcnamara , robert  j garrison , and william  p castelli .",
    "an investigation of coronary heart disease in families the framingham offspring study .",
    ", 110(3):281290 , 1979 .",
    "david  jc mackay . .",
    "cambridge university press , 2003 .",
    "cl  mallows . choosing variables in a linear regression : a graphical aid . in _ central regional meeting of the institute of mathematical statistics , manhattan , kansas _ , volume  5 , 1964 .",
    "cohn  l mallows .",
    "more comments on cp . , 37(4):362372 , 1995 .",
    "colin  l mallows . choosing a subset regression . in _ technometrics _ , volume  9 , page 190 .",
    "amer statistical assoc 1429 duke st , alexandria , va 22314 , 1967 .",
    "colin  l mallows . some comments on c p. , 15(4):661675 , 1973 .",
    "donald  w marquaridt .",
    "generalized inverses , ridge regression , biased linear estimation , and nonlinear estimation . , 12(3):591612 , 1970 .",
    "gary  f mitchell , shih - jen hwang , ramachandran  s vasan , martin  g larson , michael  j pencina , naomi  m hamburg , joseph  a vita , daniel levy , and emelia  j benjamin .",
    "arterial stiffness and cardiovascular events the framingham heart study .",
    ", 121(4):505511 , 2010 .",
    "gary  f mitchell , helen parise , emelia  j benjamin , martin  g larson , michelle  j keyes , joseph  a vita , ramachandran  s vasan , and daniel levy .",
    "changes in arterial stiffness and wave reflection with advancing age in healthy men and women the framingham heart study . , 43(6):12391245 , 2004 .",
    "douglas  c montgomery , elizabeth  a peck , and g  geoffrey vining . .",
    "john wiley & sons , 2015 .",
    "ja  morgan and jf  tatar .",
    "calculation of the residual sum of squares for all possible regressions .",
    ", 14(2):317325 , 1972 .",
    "akinwande  michael olusegun , hussaini  garba dikko , and shehu  usman gulumbe . identifying the limitation of stepwise selection for variable selection in regression analysis .",
    ", 4(5):414419 , 2015 .",
    "m  e safar , g  m london , et  al . therapeutic studies and arterial stiffness in hypertension :",
    "recommendations of the european society of hypertension . , 18(11):15271535 , 2000 .",
    "m  schatzoff , r  tsao , and s  fienberg .",
    "efficient calculation of all possible regressions .",
    ", 10(4):769779 , 1968 .",
    "greta  lee splansky , diane corey , qiong yang , larry  d atwood , l  adrienne cupples , emelia  j benjamin , ralph  b dagostino , caroline  s fox , martin  g larson , joanne  m murabito , et  al .",
    "the third generation cohort of the national heart , lung , and blood institute s framingham heart study : design , recruitment , and initial examination .",
    ", 165(11):13281335 , 2007 .",
    "mervyn stone and rodney  j brooks .",
    "continuum regression : cross - validated sequentially constructed prediction embracing ordinary least squares , partial least squares and principal components regression .",
    ", pages 237269 , 1990 .",
    "lloyd  n trefethen and david bau  iii . , volume  50 .",
    "siam , 1997 .",
    "hui zou , trevor hastie , and robert tibshirani .",
    "sparse principal component analysis .",
    ", 15(2):265286 , 2006 ."
  ],
  "abstract_text": [
    "<S> in this article , we propose a new algorithm for supervised learning methods , by which one can both capture the non - linearity in data and also find the best subset model . to produce an enhanced subset of the original variables , an ideal selection method should have the potential of adding a supplementary level of regression analysis that would capture complex relationships in the data via mathematical transformation of the predictors and exploration of synergistic effects of combined variables . the method that we present here </S>",
    "<S> has the potential to produce an optimal subset of variables , rendering the overall process of model selection to be more efficient . </S>",
    "<S> the core objective of this paper is to introduce a new estimation technique for the classical least square regression framework . </S>",
    "<S> this new automatic variable transformation and model selection method could offer an optimal and stable model that minimizes the mean square error and variability , while combining all possible subset selection methodology and including variable transformations and interaction . </S>",
    "<S> moreover , this novel method controls multicollinearity , leading to an optimal set of explanatory variables . </S>",
    "<S> * keywords : * automatic model selection , parameter and variable selection , multivariate regression , transformation and interaction of variables , variance inflation factor , biostatistics    @xmath0division of engineering and applied sciences , california institute of technology , 1200 east california boulevard , mc 205 - 45 , pasadena , ca 91125 , usa    @xmath1principium consulting , llc , 556 s. fair oaks ave . , </S>",
    "<S> ste . 101 - 264 , </S>",
    "<S> pasadena , ca 91105    @xmath2corresponding author , email : ptavalla@caltech.edu </S>"
  ]
}