{
  "article_text": [
    "sampling is at the heart of many dbmss , data warehouses , and data streaming systems . it is used both internally , for query optimization , enabling selectivity estimation , and externally , for speeding up query evaluation , and for selecting a representative subset of data for visualization  @xcite .",
    "extensions to sql to support sampling are present in db2 and sqlserver ( the tablesample keyword @xcite ) , oracle ( the sample keyword @xcite ) , and can be simulated for other systems using syntax such as order by random ( ) limit 1 .",
    "users can also ensure sampling is used for query optimization , for example in oracle ( using dynamic - sampling @xcite ) .",
    "mathematically , we are here dealing with a set of weighted items and want to support queries to arbitrary subset sums . with unit weights , we can compute subset sizes which together with the previous sums provide the subset averages .",
    "the question addressed here is which sampling scheme we should use to get the most accurate subset sum estimates .",
    "more precisely , we study the variance of sampling based subset sum estimation .",
    "we note that there has been sevaral previous works in the data base community on sampling based subset sum estimation ( see , e.g. , @xcite ) .",
    "the formal set - up is as follows .",
    "we are dealing with a set of items @xmath0 $ ] with positive weights @xmath1 . here",
    "@xmath2=\\{1, ... ,n\\}$ ] .",
    "a subset @xmath3 $ ] of these are sampled , and each sampled item @xmath4 is given a weight estimate @xmath5 .",
    "unsampled items @xmath6 have a zero weight estimate @xmath7 .",
    "we generally assume that sampling procedures include such weight estimates .",
    "we are mostly interested in unbiased estimation procedures such that @xmath8=w_i\\quad\\quad \\forall i\\in[n].\\ ] ] often one is really interested in estimating the total weight @xmath9 of a subset @xmath10 $ ] of the items , that is , @xmath11 . as an estimate @xmath12",
    ", we then use the sum of the sampled items from the subset , that is , @xmath13 . by linearity of expectation this is also unbiased , that is , from ( [ eq : single ] ) we get @xmath14=w_i \\quad\\quad \\forall i\\subseteq[n].\\ ] ] we are particularly interested in cases where the subset @xmath15 is unknown at the time the sampling decisions are made . for example , in an opinion poll , the subset corresponding to an opinion is only revealed by the persons sampled for the poll . in the context of a huge data base , sampling is used to reduce the data so that we can later support fast approximate aggregations over arbitrary selected subsets @xcite .",
    "applied to internet traffic analysis , the items could be records summarizing the flows streaming by a router",
    ". the weight of a flow would be the number of bytes .",
    "the stream is very high volume so we can only store samples of it efficiently .",
    "a subset of interest could be flow records of a newly discovered worm attack whose signature has just been determined .",
    "the sample is used to estimate the size of the attack even though the worm was unknown at the time the samples were chosen .",
    "this particular example is discussed in @xcite , which also shows how the subset sum sampling can be integrated in a data base style infrastructure for a streaming context . in @xcite",
    "they use the threshold sampling from @xcite which is one the sampling schemes that we will analyze below .",
    "generally there are two things we want to minimize : ( a ) the number of samples viewed as a resource , and ( b ) the variance as a measure for uncertainty in the estimates .    for several sampling schemes ,",
    "we already understand the optimality with respect to the sum of the individual variances @xmath16 } { \\textnormal{var}}[\\hat w_i]\\ ] ] as well as the variance of the total sum @xmath17}]\\quad \\left(={\\textnormal{var}}[\\sum_{i\\in[n ] } \\hat w_i]\\right)\\ ] ] however , what we are really interested in is the estimation of subsets of arbitrary sizes .    before continuing , we note that there is an alternative use of sampling for subset sum estimation in data bases ; namely where data are organized to generate a sample from any selected subset .",
    "generating such samples on - the - fly has been studied with different sampling schemes in @xcite .",
    "when each subset gets its own sample , we are only interested in the variance of totals @xmath18 .    in this paper",
    ", we generate the sample first , and then we use this sample to estimate the weight of arbitrary subsets . as discussed in @xcite ,",
    "this is how we have to do in a high volume streaming context where items arrive faster and in larger quantities than can be saved ; hence where only a sample can be stored efficiently .",
    "the sampling first is also relevant if we want to create a reduced approximate version of a large data ware house that can be downloaded on smaller device .",
    "the purpose of our sampling is later to be able to estimate arbitrary subset sums . with no advance knowledge of the subsets of interest ,",
    "a natural performance measure is the expected variance for a random subset .",
    "we consider two distributions on subsets :    @xmath19 : :    denoting the uniform distribution on subsets of size @xmath20 .",
    "@xmath21 : :    denoting the distribution on subsets where each item is included    independently with probability @xmath22 .",
    "often we are interested in smaller subsets with @xmath23 or @xmath24 .",
    "the corresponding expected variances are denoted @xmath25\\\\ { { w_{{p}}}}&= & { \\textnormal{e}}_{i\\leftarrow { { \\mathcal s_{{p}}}}}[{\\textnormal{var}}[\\hat w_i]]\\end{aligned}\\ ] ] note that @xmath26 and @xmath27 .",
    "we are not aware of any previous analysis of the average variance of subset sum estimation .      our basic theorem below states that our subset sum variances are simple combinations of @xmath28 and @xmath18 .",
    "the quantities @xmath28 and @xmath28 are often quite easy to analyze , and from them we immediately derive any @xmath29 .    [",
    "thm : main ] for any sampling scheme , we have @xmath30    theorem [ thm : main ] holds for arbitrarily correlated random estimators @xmath31 $ ] with @xmath32=w_i$ ] .",
    "that is , we have an arbitrary probability space @xmath33 over functions @xmath34 mapping indices @xmath0 $ ] into estimates @xmath5 .",
    "expectations and variances are all measured with respect to @xmath33 .",
    "the only condition for our theorem to hold true is that the estimate of a subset is obtained by summing the estimates of its element , that is , @xmath35 .",
    "one nice consequence of ( [ eq : main - mn ] ) is that @xmath36 this means that no matter how much negative covariance we have , on the average , it reduces the variance by at most a factor @xmath37 .",
    "a nice application of ( [ eq : main - p ] ) is in connection with a random partition into @xmath38 subsets where each item independently is assigned a random subset .",
    "a given subset includes each item with probability @xmath39 , so by linearity of expectation , the expected total variance over all sets in the partition is @xmath40      we will apply theorem  [ thm : main ] to study the optimality of some known sampling schemes with respect to the average variance of subset sum estimation .",
    "below we first list the schemes and discuss .",
    "what is known about @xmath28 and @xmath18 .",
    "our findings with theorem [ thm : main ] will be summarized in the next subsection .",
    "most of the known sampling schemes use horvitz - thompson estimators : if item @xmath4 was sampled with probability @xmath41 , it is assigned an estimate of @xmath42 .",
    "horvitz - thompson estimators are trivially unbiased .    for",
    "now we assume that the weight @xmath1 is known before the sampling decission is made .",
    "this is typically not the case in survey sampling .",
    "we shall return to this point in section [ sec : survey ] .",
    "[ [ uniform - sampling - without - replacement - ur ] ] uniform sampling without replacement ( u@xmath43r ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in uniform sampling without replacement , we pick a sample of @xmath44 items uniformly at random . if item @xmath4 is sampled it gets weight estimate @xmath45 .",
    "we denote this scheme @xmath46 .",
    "[ [ probability - proportional - to - size - sampling - with - replacement - pr ] ] probability proportional to size sampling with replacement ( p@xmath43r ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in probability proportional to size sampling with replacement , each sample @xmath47 $ ] , @xmath48 $ ] , is independent , and equal to @xmath4 with probability @xmath49}$ ] .",
    "we say that @xmath4 is sampled if @xmath50 for some @xmath48 $ ] .",
    "this happens with probability @xmath51})^k$ ] .",
    "if @xmath4 is now sampled , we use the horvitz - thompson estimator @xmath52 .",
    "we denote this scheme @xmath53 .",
    "[ [ threshold - sampling - thr ] ] threshold sampling ( thr ) + + + + + + + + + + + + + + + + + + + + + + + +    the threshold sampling is a kind of poisson sampling . in poisson sampling ,",
    "each item @xmath4 is picked independently for @xmath54 with some probability @xmath41 . for unbiased estimation",
    ", we use the horvitz - thompson estimate @xmath42 when @xmath4 is picked .    in threshold sampling",
    "we pick a fixed threshold @xmath55 . for the sample @xmath54",
    ", we include all items with weight bigger than @xmath55 .",
    "moreover , we include all smaller items with probability @xmath56 .",
    "sampled items @xmath57 have the horvitz - thompson estimate @xmath58 . with @xmath59",
    "the expected number of samples , we denote this scheme @xmath60 .",
    "threshold sampling is known to minimize @xmath28 relative to the expected number of samples .    in survey sampling ,",
    "one often makes the simplifying assumption that if we want @xmath44 samples , no single weight has more than a fraction @xmath61 of the total weight @xcite . in that case threshold sampling is simply poisson sampling with probability proportional to size as described in @xcite .",
    "more precisely , the threshold becomes @xmath62}/k$ ] , and each item is sampled with probability @xmath56 .",
    "we are , however , interested in the common case of heavy tailed distributions where a one or a few weights dominate the total @xcite . the name `` threshold sampling '' for the general case parameterized by a threshold @xmath55",
    "is taken from @xcite .",
    "[ [ systematic - threshold - sampling - sys ] ] systematic threshold sampling ( sys ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we consider the general version of systematic sampling where each item @xmath4 has an individual sampling probability @xmath41 , and if picked , a weight estimate @xmath63 .",
    "contrasting poisson sampling , the sampling decisions are not independent",
    ". instead we pick a single uniformly random number @xmath64 $ ] , and include @xmath4 in @xmath54 if and only if for some integer @xmath65 , we have @xmath66 it is not hard to see that @xmath67=p_i$ ] .",
    "let @xmath68}p_i$ ] be the expected number of samples . then the actual number of samples is either @xmath69 or @xmath70 . in particular , this number is fixed if @xmath44 is an integer . below",
    "we assume that @xmath44 is integer .    in systematic threshold sampling",
    "we perform systematic sampling with exactly the same sampling probabilities as in threshold sampling , and denote this scheme @xmath71 . hence for each item @xmath4 ,",
    "we have identical marginal distributions @xmath5 with @xmath60 and @xmath71 .",
    "[ [ priority - sampling - pri ] ] priority sampling ( pri ) + + + + + + + + + + + + + + + + + + + + + + +    in priority sampling from @xcite we sample a specified number of @xmath72 samples . for each item ,",
    "a we generate a uniformly random number @xmath73 , and assign it a priority @xmath74 .",
    "we assume these priorities are all distinct .",
    "the @xmath44 highest priority items are sampled .",
    "we call the @xmath75th highest priority the threshold @xmath55",
    ". then @xmath4 is sampled if and only if @xmath76 , and then the weight estimate is @xmath77 .",
    "this scheme is denoted @xmath78 .",
    "note that the weight estimate @xmath77 depends on the random variable @xmath55 which is defined in terms of all the priorities .",
    "this is not a horvitz - thompson estimator . in @xcite",
    "it is proved that this estimator is unbiased , and that there is no covariance between individual estimates for @xmath79 .",
    "below we compare @xmath80 and @xmath81 for the different sampling schemes . using theorem [ thm : main ] most results",
    "are derived quite easily from existing knowledge on @xmath28 and @xmath18 .",
    "the derivation including the relevant existing knowledge will be presented in sections [ s : near - optimal][s : anti - optimal ] .",
    "when comparing different sampling schemes , we use a superscript to specify which sampling scheme is used .",
    "for example @xmath82 means that the sampling scheme @xmath33 obtains a smaller value of @xmath80 than does @xmath83 .    for a given set of input weights @xmath84",
    ", we think abstractly of a sampling scheme as a probability distribution @xmath33 over functions @xmath34 mapping items @xmath4 into estimates @xmath5 .",
    "we require unbiasedness in the sense that @xmath85=w_i$ ] . for a given @xmath86 ,",
    "the number of samples is the number of non - zeroes . for any measure over sampling schemes",
    ", we use a superscript @xmath87 to indicate the optimal value over all sampling schemes using an expected number of at most @xmath44 samples .",
    "for example , @xmath88 is the minimal value of @xmath89 for sampling schemes @xmath33 using an expected number of at most @xmath44 samples .    [ [ optimality - of - sys - thr - and - pri ] ] optimality of sys , thr , and pri + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for any subset size @xmath20 and sample size @xmath44 , we get @xmath90 the input weights @xmath91 where arbitrary , so we conclude that systematic threshold sampling optimizes @xmath29 for any possible input , subset size @xmath20 , and sample size , against any possible sampling scheme .",
    "for contrast , threshold sampling is always off by exactly a factor @xmath37 .",
    "similarly , for any subset inclusion probability @xmath22 , we get that @xmath92    from @xcite , we get that @xmath93 hence , modulo an extra sample , priority sampling is as good as threshold sampling , and hence at most a factor @xmath37 or @xmath94 worse than the optimal systematic threshold sampling .",
    "[ [ anti - optimality - of - u - r - and - pr ] ] anti - optimality of u@xmath95r and p@xmath43r + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we argue that standard sampling schemes such as uniform sampling and probability proportional to size sampling with replacements may be arbitrarily bad compared with the above sampling schemes .",
    "the main problem is in connection with heavy tailed weight distributions where we likely have one or a few dominant weights containing most of the total weight . with uniform sampling , we are likely to miss the dominant weights , and with probability proportional to size sampling with replacement , our sample gets dominated by copies of the dominant weights .",
    "dominant weights are expected in the common case of heavy tailed weight distributions @xcite .",
    "we will analyze a concrete example showing that these classic schemes can be arbitrarily bad compared with the above near - optimal schemes .",
    "the input has a large weight @xmath96 and @xmath97 unit weights @xmath98 , @xmath99 $ ] .",
    "we are aiming at @xmath44 samples .",
    "we assume that @xmath100 and @xmath101 . here",
    "for this concrete example , we will show that @xmath103 > \\\\[-.55em]\\sim\\end{array } } & \\ell^2m / k\\\\ { v_{{m}:n}}^{{\\textnormal{p+r}_{{k}}}}&{\\begin{array}{c}\\\\[-1em ] > \\\\[-.55em]\\sim\\end{array } } & \\ell m / k\\ ] ] here @xmath104 and @xmath105 >",
    "\\\\[-.55em]\\sim\\end{array}}y \\iff x\\geq ( 1-o(1 ) )   y$ ] . a corresponding set of relations can be found in terms of @xmath22 , replacing @xmath106 with @xmath107 and @xmath20 with @xmath108 .",
    "we conclude that uniform sampling with replacement is a factor @xmath109 from optimality while probability proportional to size sampling with replacement is a factor @xmath110 from optimality . since @xmath111 it follows that both schemes can be arbitrarily far from optimal .",
    "one of our conclusions above is that systematic threshold sampling is optimal for the average subset variances no matter the subset size @xmath20 or inclusion probability @xmath22 .",
    "however , there may be scenarios where some sampling schemes are not appropriate . in the section [ sec : appliation ]",
    "we will study a streaming scenario ruling out both threshold and systematic threshold sampling , leaving us with priority sampling among the near - optimal schemes . from ( [ eq : opt - sys - thr - p ] ) and ( [ eq : pri - thr - p ] )",
    ", we get that @xmath112 even if we do nt know what the optimal appropriate scheme is , this inequality provides a limit to the improvement with any possible scheme . in particular ,",
    "if @xmath44 is not too small , and @xmath20 is not too close to @xmath113 , there is only limited scope for improvement .",
    "the rest of the paper is divided as follows : in section [ sec : appliation ] we discuss our results in concrete application scenarios including survey sampling and related experimental work . in section [",
    "s : proof ] we prove theorem [ thm : main ] . in sections",
    "[ s : near - optimal][s : anti - optimal ] we will derive the optimability results . in section",
    "[ sec : bias ] we discuss extensions to biased sampling , and finally we have some concluding remarks in section [ sec : conclussion ] .",
    "in this section , we shall discuss an important internet related application where systematic and threshold sampling are less appropriate , hence where the better choice is to settle for near - optimality of priority sampling .",
    "the setup of the scenario in this section is taken from @xcite .",
    "it serves to contextualize the preceding optimality results in a realistic context",
    ". we will also mention related experimental work from @xcite complementing the analytic results of this paper .",
    "we are here focusing on reservoir sampling ( c.f . @xcite and @xcite ) for a stream of weighted items . in reservoir sampling ,",
    "the items arrive one by one , and a reservoir maintains a sample @xmath54 of the items seen thus far . when a new items arrives , it may be included in the sample @xmath54 and old samples may be dropped from @xmath54 .",
    "old items outside @xmath54 are not reconsidered .",
    "reservoir sampling addresses two issues :    * the streaming issue @xcite where we want to compute a sample from a huge stream that passes by only once , when the memory available to us is limited . *",
    "the incremental data structure issue of maintaining a sample as new items are added . in our case",
    ", we use the sample to provide quick estimates of sums over arbitrary subsets of the items seen thus far .",
    "the reader is referred to @xcite for a description of how reservoir sampling and subset sum estimation can be integrated in a data base style infrastructure for a streaming context .",
    "the above set - up is similar to that of classic survey sampling ( see , e.g. @xcite ) .",
    "however , in survey sampling , typically , we do not know the weight @xmath1 of an item @xmath4 unless we sample it",
    ". instead we have free access to an auxiliary variable @xmath114 that is correlated with @xmath1 , and use @xmath114 to determine the sampling probability @xmath41 for item @xmath4 . for example , if the item @xmath4 is a house and @xmath1 is house hold income , then @xmath114 could be an approximation of @xmath1 based on the address .",
    "we can then use @xmath114 to determine the sampling probability @xmath41 for item @xmath4 .",
    "the weights @xmath1 will only be found for the items sampled .",
    "the previously discussed techniques provide an estimate @xmath115 of the known variable @xmath114 , and then we use @xmath116 as an estimator for @xmath1 . if @xmath115 is an unbiased estimator , then",
    "so is @xmath5 , that is , @xmath117=w_i{\\textnormal{e}}[\\hat u_i]/u_i = w_i$ ] . also , if @xmath115 is a horvitz - thompson estimator , then so is @xmath5 , that is , if @xmath4 is sampled , then @xmath118 .    in survey sampling ,",
    "the main challenge is often to estimate the total @xmath119}$ ] based on the sampled weights .",
    "they often have an analysis of @xmath18 assuming that @xmath120 , and then they use this to indicate that a scheme will be good if @xmath121 . for example , it is known that @xmath122 @xcite , and that threshold sampling minimizes @xmath18 among all poisson sampling schemes @xcite .    knowing the exact weight comes in naturally in computer science when the purpose of the sampling is to reduce a large data set so that we can later support fast approximate aggregations over arbitrary subsets .",
    "for example , this idea is used in data bases @xcite .",
    "nevertheless , there could be cases where sampling is made with one weight @xmath114 in mind , but later used for another weight @xmath1 .",
    "this case is treated as in survey sampling . in case of heavy tailed distributions ,",
    "uniform sampling is basically useless .",
    "hence it is very important that @xmath114 is large when @xmath1 is large .",
    "our context is that of a large stream of weighted items passing by .",
    "when item @xmath4 passes by , we get to see its weight @xmath1 . if our goal was to compute @xmath119}$ ] , we would simply accumulate the weights in a counter .",
    "hence , in our context , the challenge of survey sampling is trivial .",
    "one thing that makes reservoir sampling hard is that sampling decisions are made on - line .",
    "this rules out off - line sampling schemes such as sunter s method @xcite,@xcite where we have to sort all the items before any sampling decisions are made .",
    "a cultural difference between survey sampling and our case is that survey sampling appears less focused on heavy tailed distribution . for threshold or systematic threshold sampling one",
    "can then assume that the threshold is bigger than the maximal weight , hence that these schemes use probabilities proportional to size . in our kind of applications ,",
    "heavy tailed distributions are very prominent @xcite .      with a concrete internet example",
    ", we will now illustrate the selection of subsets and the use of reservoir sampling for estimating the sum over these subsets . for the selection , the basic point is that an item , besides the weight , has other associated information , and selection of an item may be based on all its associated information . as stated in ( [ eq : sum ] ) , to estimate the total weight of all selected items , we sum the weight estimates of all selected sampled items .",
    "internet routers export information about transmissions of data passing through .",
    "these transmissions are called flows .",
    "a flow could be an ftp transfer of a file , an email , or some other collection of related data moving together .",
    "a flow record is exported with statistics such as application type , source and destination ip addresses , and the number of packets and total bytes in the flow .",
    "we think of the byte size as the weight .",
    "we want to sample flow records in such a way that we can answer questions like how many bytes of traffic came from a given customer or how much traffic was generated by a certain application .",
    "both of these questions ask what is the total weight of a certain selection of flows . if we knew in advance of measurement",
    "which selections were of interest , we could have a counter for each selection and increment these as flows passed by .",
    "the challenge here is that we must not be constrained to selections known in advance of the measurements .",
    "this would preclude exploratory studies , and would not allow a change in routine questions to be applied retroactively to the measurements .",
    "a killer example where the selection is not known in advance was the tracing of the _ internet slammer worm _ @xcite .",
    "it turned out to have a simple signature in the flow record ; namely as being udp traffic to port 1434 with a packet size of 404 bytes .",
    "once this signature was identified , the worm could be studied by selecting records of flows matching this signature from the sampled flow records .",
    "we note that data streaming algorithms have been developed that generalize counters to provide answers to a range of selections such as , for example , range queries in a few dimensions @xcite .",
    "however , each such method is still restricted to a limited type of selection to be decided in advance of the measurements .      in @xcite ,",
    "the above internet application is explored with experiments on a stream segment of 85,680 flow records exported from a real internet gateway router .",
    "these items were heavy tailed with a single record representing 80% of the total weight .",
    "subsets considered were entries of an @xmath123 traffic matrix , as well as a partition of traffic into traffic classes such as ftp and dns traffic .",
    "figure [ fig : matrix ] shows the results for the @xmath123 traffic matrix with all the above mentioned sampling schemes ( systematic threshold sampling was not included in @xcite , but is added here for completeness ) .",
    "the figure shows the relative error measured as the sum of errors over all @xmath124 entries divided by the total traffic . the error is a function of the number @xmath44 of samples , except with thr , where @xmath44 represents the expected number of samples .",
    "we note that u@xmath95r is worst .",
    "it has an error close to 100% because it failed to sample the large dominant item .",
    "the p@xmath43r is much better than u@xmath43r , yet much worse than the near - optimal schemes pri , thr , and sys . to qualify the difference , note that p@xmath43r use about 50 times more samples to get safely below a 1% relative error .    among the near - optimal schemes",
    ", there is no clear winner . from our theory",
    ", we would not expect much of a difference . we would expect thr to be ahead of pri by at most one sample .",
    "also , we are studying a partitioning into @xmath124 sets , and then , as noted in section [ s : theorem ] , the average variance advantage of sys is a factor @xmath125 , which is hardly measurable .",
    "the experiments in figure [ fig : matrix ] are thus consistent with our theory .",
    "the strength of our mathematical results is that we now know that no one can ever turn up with a different input or a new sampling scheme and perform better on the average variance .",
    "conversely , experiments with real data could illustrate subsets with relevant special properties that are far from the average behavior .",
    "our analysis names systematic threshold sampling the best possible sampling scheme .",
    "however , in reservoir sampling we often have a resource bound on the number @xmath44 of samples we can store , e.g. , we may only have a certain amount of memory available for the samples .",
    "priority sampling is ideally suited for this context in that a standard priority queue can maintain the @xmath126 items of highest priority ( when a new item arrives , it is first assigned a priority , then it is added to the queue , and finally we remove the item of smallest priority item from the queue in @xmath127 time .    however , with both threshold sampling and priority sampling it appears that we need to know the threshold @xmath55 in advance ( item @xmath4 is sampled with probability @xmath128 ) .",
    "this threshold @xmath55 is a function of all items such that @xmath129 .",
    "hence @xmath55 can only be determined after the whole stream has been investigated .",
    "as described in @xcite it is possible , though a bit more complicated , to adapt threshold sampling for a stream to provide an expected number of @xmath44 samples .",
    "the essential idea is that we increase the threshold as we move along the stream in such away that it always gives an expected number of @xmath44 samples from the items seen thus far .",
    "thus an item @xmath4 gets dropped from the sample when the threshold falls below its priority .",
    "however , if we want to be sure to no more than @xmath44 samples are made , we have to shoot for substantially less than @xmath44 samples .",
    "for example , to stay below @xmath44 with 99% probability , using normal approximation for larger @xmath44 , we should only go for an expected number of @xmath130 samples .",
    "in contrast , with priority sampling , we do better than threshold sampling with an expected number of @xmath131 samples .",
    "thus priority sampling works better when we are allowed at most @xmath44 samples .    for systematic threshold sampling ,",
    "the problem is more severe because if one changes the threshold marginally , it may completely change the set of samples .",
    "one could conceivably resolve this if we only increased the threshold by an exact doubling starting .",
    "however , a doubling of the threshold can be shown to at least double the variance .",
    "another objection to systematic threshold sampling in a streaming context is that we may have a very strong correlations between items in a subset depending on how they are placed in the stream .",
    "normally , it is recommended that the items are appropriately shuffled @xcite , but that is not possible in reservoir sampling from a stream . with threshold and priority sampling",
    "there is no such dependence as there is no covariance between different item estimates . as demonstrated in @xcite",
    ", it is possible to get good confidence bounds with priority sampling and threshold sampling so that we statistically know when we get good estimates for a subset .",
    "the correlation between items in systematic threshold sampling prevents us from providing good confidence intervals , so even if systematic threshold sampling gives better variance on the average , we have no way of knowing if we get these good estimates for a concrete subset .",
    "thus , among our near optimal sampling schemes , priority sampling is the most appropriate for resource constrained reservoir sampling .",
    "recall from ( [ eq : pri - opt ] ) that @xmath132 in our internet application we typically have thousands of samples .",
    "hence we are not concerned about the difference between @xmath44 and @xmath126 samples .",
    "the factor @xmath37 is only significant for larger sets @xmath20 .",
    "however , for larger sets , we expect to do great anyway because they relatively speaking have much smaller errors .",
    "more precisely , we typically expect that we have plenty of samples go get very a good estimate of the total , or in other words , that the relative standard deviation @xmath133}$ ] for the total is very small .    since priority sampling has no covariance , @xmath134 .",
    "at the same time , the average subset sum is @xmath135}$ ] . for a subset achieving both of these averages ,",
    "the relative standard deviation would be @xmath136}}=\\sqrt{n / m}\\,{\\varepsilon}_{n : n}\\ ] ] however , if @xmath137 is big , then the optimality factor @xmath37 is close to @xmath138 .",
    "thus , it is when our variance is expected comparatively small that our relative distance to opt is greatest , the most extreme being in the estimation of the total .",
    "the estimate of the total has the smallest relative standard deviation , but since it is positive , it is infinitely worse than @xmath139 .",
    "another case where we do not need to worry so much about the non - optimality factor @xmath37 is if we are interested in the relative weight of a subset @xmath15 of size @xmath20 . as an estimator , we use @xmath140}$ ] . if @xmath141 , we note that @xmath142}=1-\\hat w_{[n]\\setminus i}/\\hat w_{[n]}$ ] .",
    "most of the error in this estimate stems from the estimate @xmath143\\setminus i}$ ] of the small set @xmath2\\setminus i$ ] , but for this small set size , we are at most a factor @xmath144 from optimality .",
    "as discussed in section [ s : k - sampling ] , we do not know if there is a scheme performing better than priority sampling in practice in the context of resource constrained reservoir sampling .",
    "the conclusion of this section is that even if there is a better scheme , it is not going to help us much .",
    "in this section we prove ( [ eq : main - mn ] ) @xmath145 and ( [ eq : main - p ] ) @xmath146 by the definitions of variance and covariance , for any subset @xmath147 $ ] , @xmath148=a_i+b_i\\ ] ] where @xmath149\\\\ b_i&=&\\sum_{i , j \\in i , i\\neq j } { \\textnormal{cov}}[\\hat w_i,\\hat w_j]\\end{aligned}\\ ] ] suppose @xmath15 is chosen uniformly at random among subsets of @xmath2 $ ] with @xmath20 element . then for any @xmath4 , @xmath150=m / n$ ] , so by linearity of expectation , @xmath151&=&\\sum_{i \\in [ n]}\\pr[i\\in i]{\\textnormal{var}}[\\hat w_i]\\\\ & = & m / n\\cdot a_{[n]}.\\end{aligned}\\ ] ] also , for any @xmath152 , @xmath153=m / n\\cdot(m-1)/(n-1)$ ] , so by linearity of expectation , @xmath154&=&\\sum_{i , j \\in [ n ] , i\\neq j}\\pr[i , j\\in i]{\\textnormal{cov}}[\\hat w_i,\\hat w_j]\\\\ & = & m / n\\cdot ( m-1)/(n-1)\\cdot b_{[n]}.\\end{aligned}\\ ] ] thus @xmath155 = m / n\\cdot a_{[n ] } + m / n\\cdot ( m-1)/(n-1)\\cdot b_{[n]}\\ ] ] by definition , @xmath156}={{\\sigma}v}$ ] . moreover , by ( [ eq : * ] ) , @xmath157 } + b_{[n]}\\ ] ] so @xmath158 } = { { v\\sigma}}-{{\\sigma}v}.\\ ] ] consequently , @xmath159\\\\ & = & \\frac mn\\,{{\\sigma}v}+ \\frac mn\\frac{m-1}{n-1}({{v\\sigma}}-{{\\sigma}v})\\\\ & = & \\frac mn\\,\\left(\\frac{n - m}{n-1}\\,{{\\sigma}v}+\\frac{m-1}{n-1}\\,{{v\\sigma}}\\right)\\end{aligned}\\ ] ] this completes the proof of ( [ eq : main - mn ] ) .",
    "the proof of ( [ eq : main - p ] ) is very similar . in this case , each @xmath160 $ ] is picked independently for @xmath161 with probability @xmath22 . by linearity of expectation , @xmath162=p a_{[n]}.\\ ] ] also , for any @xmath152 , @xmath153=p^2 $ ] , so by linearity of expectation , @xmath163=p^2 b_{[n]}.\\ ] ] thus @xmath164\\\\ & = & p a_{[n ] } + p^2 b_{[n]}\\\\ & = & p { { \\sigma}v}+ p^2({{v\\sigma}}-{{\\sigma}v})\\\\ & = & p((1-p){{\\sigma}v}+ p{{v\\sigma}})\\\\\\end{aligned}\\ ] ] this completes the proof of ( [ eq : main - p ] ) , hence of theorem [ thm : main ] .",
    "we will now use theorem  [ thm : main ] to study the average variance ( near ) optimality of subset sum estimation with threshold sampling , systematic threshold sampling , and priority sampling for any possible set of input weights .",
    "the results are all derived based on existing knowledge on @xmath28 and @xmath18 . below",
    "we will focus on @xmath80 based on random subsets of a given size @xmath20 .",
    "the calculations are very similar for @xmath81 based on the inclusion probability @xmath22 .",
    "it is well - known from survey sampling that @xcite that systematic sampling always provides an exact estimate of the total so @xmath122 .",
    "since variances can not be negative , we have @xmath165 it is also known from survey sampling @xcite that threshold sampling minimize @xmath18 among all poisson sampling schemes .",
    "in @xcite it is further argued that threshold sampling minimizes @xmath28 over all possible sampling schemes , that is , @xmath166 . since systematic threshold sampling uses the same marginal distribution for the items , we have @xmath167 since @xmath71 optimizes both @xmath28 and @xmath18 we conclude ( [ eq : main - mn ] ) that it optimizes @xmath80 for any subset size @xmath20 .",
    "more precisely , using ( [ eq : main - mn ] ) , we get @xmath168 hence @xmath169 as mentioned above we have @xmath170 .",
    "moreover , threshold sampling has no covariance between individual estimates , so @xmath171 but in the previous calculation , we saw that @xmath172 hence we conclude that @xmath173 this completes the proof of ( [ eq : main - mn ] ) .",
    "a very similar calculation establishes ( [ eq : main - p ] ) .",
    "in @xcite it is proved that @xmath174 moreover , for any scheme @xmath33 without covariance , we have @xmath175 since both threshold and priority sampling have no covariance , we conclude ( [ eq : pri - thr - mn ] ) @xmath176 the proof of ( [ eq : pri - thr - p ] ) is similar based on @xmath177 .",
    "below we will analyze a concrete example showing that the classic schemes of uniform sampling without replacement and probability proportional to size sampling with replacement can be arbitrarily bad compared with the above near - optimal schemes .",
    "the concrete example consists of @xmath97 unit weights @xmath98 , @xmath99 $ ] and a large weight @xmath96 .",
    "we are aiming at @xmath44 samples .",
    "we assume that @xmath100 and that @xmath178 .    as in the last section , we focus on the subset size @xmath20 rather than the inclusion probability @xmath22",
    ".      we will now analyze the variance with threshold sampling for the bad example . the variance with systematic and priority sampling",
    "will then follow from ( [ eq : opt - sys - thr - mn ] ) and ( [ eq : pri - thr - mn ] ) .    threshold sampling ( @xmath60 ) will use the threshold @xmath179 .",
    "this will pick the large weight @xmath96 with probability @xmath180 and weight estimate @xmath181 . hence @xmath182=0 $ ] .",
    "each unit weight @xmath1 , @xmath183 , is then picked with probability @xmath184 and estimate @xmath185 .",
    "the variance of the estimate for a unit weight item is then @xmath186 , so the total variance is @xmath187 .",
    "since there is no co - variance , we conclude for any subset size @xmath188 that @xmath189 from ( [ eq : opt - sys - thr - mn ] ) we get that @xmath190 finally , since @xmath191 it follows from ( [ eq : pri - thr - mn ] ) that @xmath192\\approx mn / k.\\ ] ]      in uniform sampling without replacement ( @xmath46 ) , we pick a sample of @xmath44 different items uniformly at random . as we shall see below , the variance of uniform sampling is dominated by the variance of estimating the large weight .",
    "the large weight @xmath96 is picked with probability @xmath193 and estimate @xmath194 .",
    "hence @xmath195=p_n(\\ell / p_n)^2=n\\ell^2/k.\\ ] ] it follows that @xmath196={\\textnormal{e}}[\\hat w_n^2]-w_n^2=n\\ell^2/k-\\ell^2\\approx n\\ell^2/k\\ ] ] hence @xmath197\\approx n\\ell^2/k.\\ ] ] to study the variance @xmath18 of the total sum estimate @xmath198}$ ] , we note that @xmath199}^2]\\geq { \\textnormal{e}}[\\hat w_n^2]= n\\ell^2/k.\\ ] ] hence @xmath200}^2]-w_{[n]}^2\\geq n\\ell^2/k-(\\ell+n-1)^2 \\approx n\\ell^2/k\\ ] ] since @xmath28 and @xmath18 are both lower bounded by @xmath201 , it follows from ( [ eq : main - mn ] ) that for any subset size @xmath188 @xmath202 > \\\\[-.55em]\\sim\\end{array}}(m / n)n\\ell^2/k = m\\ell^2/k\\ ] ] this is roughly a factor @xmath203 worse than what we had with any of the near optimal schemes .      in probability proportional to size sampling with replacement ( @xmath53 ) ,",
    "each sample @xmath47 $ ] , @xmath48 $ ] , is independent , and equal to @xmath4 with probability @xmath49}$ ] .",
    "an item @xmath4 is sampled if @xmath50 for some @xmath48 $ ] .",
    "this happens with probability @xmath51})^k$ ] , and if @xmath4 is sampled , @xmath52 .    in our bad example",
    ", the variance with @xmath53 relates to the fact that we get mostly duplicates of the large item .",
    "the expected number of unit samples is only @xmath204 , and as a result , we get a large variance from the unit items .",
    "each unit item is picked with probability @xmath205 hence @xmath206=(n-1)p_1(1-p_1)/p_1 ^ 2\\approx   n\\ell / k.\\ ] ] this is a factor @xmath110 worse than with threshold sampling .",
    "we will now show that @xmath207 > \\\\[-.55em]\\sim\\end{array}}{{\\sigma}v}$ ] , or equivalently , that @xmath208 by definition @xmath209}]-w_iw_{[i-1]}\\right),\\ ] ] so @xmath210}-{\\textnormal{e}}[\\hat w_i\\hat w_{[i-1]}]\\right)\\nonumber\\\\ & = & 2\\sum_{i>1}\\left(w_i(w_{[i-1]}-{\\textnormal{e}}[\\hat w_{[i-1]}|i\\in s]\\right ) \\label{eq : sum1}\\end{aligned}\\ ] ] to bound this sum , first we consider the term with @xmath211 .",
    "@xmath212}={\\textnormal{e}}[\\hat w_{[n-1]}]=p_n{\\textnormal{e}}[\\hat w_{[n-1]}| n\\in s]+(1-p_n ) { \\textnormal{e}}[\\hat w_{[n-1]}| n\\not\\in s]\\ ] ] so @xmath212}-{\\textnormal{e}}[\\hat w_{[n-1]}| n\\in s]\\geq ( 1-p_n ) { \\textnormal{e}}[\\hat w_{[n-1]}| n\\not\\in s]\\ ] ] here @xmath213=((n-1)/(\\ell+n-1))^k < ( n/\\ell)^k$ ]",
    ". moreover @xmath214}| n\\not\\in s]\\leq k / p_1\\approx \\ell$ ] , so @xmath215}| n\\not\\in s]=o(n / k)$ ] .",
    "n / k).\\ ] ] as desired .",
    "next we consider @xmath183 .",
    "we have @xmath217}-{\\textnormal{e}}[\\hat w_{[i-1]}|i\\in s])}\\\\ & = & ( i-1)-(i-1)\\pr[1\\in s|i\\in s]/p_1)\\end{aligned}\\ ] ] and @xmath218/p_1&\\geq & ( i-1)\\pr[1\\in s|s_k = i]/p_1\\\\ & \\geq & \\pr[1\\in s|s_k = i]/p_1\\\\ & \\geq & \\frac{1-(1 - 1/(\\ell+n-1))^{k-1}}{1-(1 - 1/(\\ell+n-1))^k}\\\\ & \\geq & \\frac{1-(1 - 1/\\ell)^{k-1}}{1-(1 - 1/\\ell)^k}\\\\ & \\geq & \\frac{(k-1)(1-\\frac{k-1}{2\\ell})}\\ell \\frac \\ell k \\\\ & \\geq & 1 - 1/k-\\frac{k-1}{2\\ell}\\\\ & = & 1-o(1/k)\\\\\\end{aligned}\\ ] ] the last derivation follows because @xmath101 . hence @xmath219/p_1)=o(i / k)\\ ] ] so @xmath220}-{\\textnormal{e}}[\\hat w_{[i-1]}|i\\in s]\\right)}\\nonumber\\\\ & = & \\sum_{i=2}^{n-1}o(i / k)=o(n^2/k)=o(n\\ell / k)\\label{eq : units}\\end{aligned}\\ ] ] combining ( [ eq : sv ] ) , ( [ eq : sum1 ] ) , ( [ eq : n ] ) , and ( [ eq : units ] ) , we conclude that @xmath221 , hence that @xmath222 > \\\\[-.55em]\\sim\\end{array}}{{v\\sigma}}{\\begin{array}{c}\\\\[-1em ] > \\\\[-.55em]\\sim\\end{array}}n\\ell / k\\ ] ] together with ( [ eq : sv ] ) and theorem [ thm : main ] , it follows for any set size @xmath20 , that @xmath223 > \\\\[-.55em]\\sim\\end{array}}(m / n ) n\\ell / k = m\\ell / k\\ ] ] this is a factor @xmath110 more than @xmath224 .",
    "so far we have restricted our attention to unbiased estimators . with biased estimators we would consider mean square error ( mse ) instead of just variance .",
    "we note that even though a biased estimator may give a smaller mse than an unbiased one , there are many standard reasons to prefer unbiased estimators . for example , if we want to combine estimates in a sum , we can use linearity of expectation to conclude that the sum of the estimators is unbiased if each estimator is unbiased . also , if we add independent unbiased estimators , the variances are just added . with biases",
    ", we can not just add up the mean square errors .",
    "an example where we wish to combine independent estimators is if we have independent samples from different streams . in the internet application , these streams could be flow records from different routers where would want to combine the information in a global picture @xcite . in other words , a biased estimator may be ok if all we consider is a single isolated estimate .",
    "however , as soon as we start combining estimates , the bias may come back and haunt us .    despite these caveats of biased estimators ,",
    "we discuss them briefly below to see how they fit into subset sum estimation . as a concrete example of biased estimation ,",
    "suppose a sampling scheme does not provide exact estimation of the total , but that the total is known .",
    "then , for each item @xmath4 , we could use the adjusted estimator @xmath225}/\\hat x_{[n]})$ ] .",
    "then the total is right in the sense that @xmath226}=x_{[n]}$ ] . in the case of threshold sampling with no dominant weights ,",
    "this adjusted estimator is equivalent to the estimator suggested in @xcite .",
    "the adjusted threshold sampling estimator will bias towards large weights .",
    "however , the corresponding adjusted uniform sampling estimator will have bias towards smaller weights .    now ,",
    "if we allow bias , how well can we do with respect to our average mean square error ?",
    "it can easily be seen that our main theorem holds for mean square error and not just for variances .",
    "that is , with @xmath227 denoting mean square error instead of @xmath228 for variance , we get the following generalization of ( [ eq : main - mn ] ) : @xmath229 in fact , our formulas generalize to any symmetric quadratic polynomial . as with the variance of unbiased estimators , we can use ( [ eq : mse ] ) to compute @xmath230 for a concrete sampling scheme for which we know @xmath231 and @xmath232 .    now ,",
    "if we want to minimize averages and there is no requirement of unbiasedness , the optimal performance is obtained by a concrete sample , thus with no randomness in the sample .",
    "assume that the weights are in decreasing order so that @xmath233 is the largest weight .",
    "if all we cared about was @xmath232 , we could give some item the total weight , and drop the rest .",
    "if all we cared about was @xmath231 , the optimal choice is to pick the @xmath44 largest weights , using their real weights as the estimate . then @xmath234 .    to optimize @xmath230",
    ", we introduce a parameter @xmath235 for the negative error @xmath119}-\\hat w_{[n]}$ ] in the total",
    ". then @xmath236 . to minimize @xmath231",
    ", the optimal choice is to pick the @xmath44 largest weights , setting the rest to @xmath237 . for the @xmath44 largest weights , we distribute the error equally , setting @xmath238 . then @xmath239 .",
    "the last term is fixed , so to optimize @xmath230 , we should choose @xmath235 so as to minimize @xmath240 for @xmath241 , we choose @xmath242 , and then @xmath243 for @xmath244 as discussed above .",
    "obviously , picking the @xmath44 largest weights and giving them a specific estimate is not a good `` sampling '' scheme .",
    "the above more illustrates the danger of just looking at averages and the deceptiveness of biased estimation . for non - random subsets such as a large set of small items ,",
    "the above scheme would always return a zero .",
    "this kind of unfairness is nt right .",
    "recall that we had a similar criticism of systematic sampling in section [ s : k - sampling ] if we could not shuffle the items .",
    "an ideal sampling scheme should both have a reasonable fairness and perform reasonably well on the average .",
    "threshold and priority sampling have no covariance , so all partitions have the same total variance . here by considering partitions rather than individual subsets , we ensure that each item is counted exactly once .",
    "moreover , among unbiased schemes , they essentially got within a factor @xmath245 from optimality on the average variance for subsets of size @xmath20 , so when @xmath20 is not too close to @xmath113 , this is close to ideal .",
    "as a formal measure for ability to estimate subset sums of a set of @xmath113 weighted items , we suggested for each set size @xmath20 , to study the average variance over all subsets : @xmath246 , |i|=m}\\left[{\\textnormal{var}}[\\hat w_i]/m\\right]\\ ] ] we discovered that @xmath29 was the following simple combination of the sum of variances @xmath28 and the variance of the total sum @xmath18 : @xmath247 a corresponding formula was found for the expected variance @xmath81 for subsets including each item independently with probability @xmath22 .",
    "we then considered different concrete sampling schemes .",
    "the optimality of @xmath28 and @xmath18 was already known for some sampling schemes , and this now allow us to derive the optimality with respect to @xmath29 for arbitrary subset size @xmath20 .",
    "we found that systematic threshold sampling was optimal with respect to @xmath29 , and that threshold sampling was off exactly by a factor @xmath245 .",
    "finally , we know that priority sampling performs like threshold sampling modulo one extra sample . we argued that this distance to optimality is not significant in practice when we use many samples .",
    "this was important to know in the context of resource constrained reservoir sampling , where priority sampling is the better choice for other reasons .",
    "for contrast , we also showed that more classic schemes like uniform sampling with replacement and probability proportional to size sampling without replacement could be arbitrarily far from optimality .",
    "the concrete example was stylistic heavy tailed distribution .",
    "duffield , c.  lund , and m.  thorup .",
    "flow sampling under hard resource constraints . in _ proc .",
    "acm ifip conference on measurement and modeling of computer systems ( sigmetrics / performance ) _ , pages 8596 , 2004 ."
  ],
  "abstract_text": [
    "<S> for high volume data streams and large data warehouses , sampling is used for efficient approximate answers to aggregate queries over selected subsets . </S>",
    "<S> mathematically , we are dealing with a set of weighted items and want to support queries to arbitrary subset sums . with unit weights </S>",
    "<S> , we can compute subset sizes which together with the previous sums provide the subset averages . </S>",
    "<S> the question addressed here is which sampling scheme we should use to get the most accurate subset sum estimates .    </S>",
    "<S> we present a simple theorem on the variance of subset sum estimation and use it to prove variance optimality and near - optimality of subset sum estimation with different known sampling schemes . </S>",
    "<S> this variance is measured as the average over all subsets of any given size . by optimal </S>",
    "<S> we mean there is no set of input weights for which any sampling scheme can have a better average variance . </S>",
    "<S> such powerful results can never be established experimentally . </S>",
    "<S> the results of this paper are derived mathematically . </S>",
    "<S> for example , we show that appropriately weighted systematic sampling is simultaneously optimal for all subset sizes . more standard schemes such as uniform sampling and probability - proportional - to - size sampling with replacement can be arbitrarily bad .    </S>",
    "<S> knowing the variance optimality of different sampling schemes can help deciding which sampling scheme to apply in a given context . </S>"
  ]
}