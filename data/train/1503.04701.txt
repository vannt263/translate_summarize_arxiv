{
  "article_text": [
    "we consider the problem of attitude estimation from vector measurements , where the attitude parameter is static in the first place , and then the dynamic case where it evolves on the special orthogonal group @xmath0 .",
    "attitude estimation , both in the static and dynamic cases , has been the subject of numerous works due to its potential applications to e.g. aerial vehicles ( or satellites ) control .",
    "the references are too numerous to be exhaustively listed , and the reader is referred to e.g. @xcite for an overview of estimation problems on so(3 ) , or the survey @xcite , and e.g. the paper @xcite for a very recent work on the subject .    in the present paper an intrinsic version of the cramr - rao bound on estimation accuracy is established in section [ smith : sec ] on the special orthogonal group @xmath0 .",
    "it is intrinsic in the sense that it does not rely on a specific choice of coordinates on @xmath0 . for any estimator @xmath1 of @xmath2",
    "we give a lower bound on the covariance matrix of @xmath5 , that is , the estimation error expressed in terms of group multiplication , and then projected onto a three dimensional vector space using the logarithmic map of @xmath0 .",
    "this error indeed makes sense as @xmath6 is the rotation that maps the estimated orientation to the true orientation , whereas the usual estimation error @xmath7 is meaningless on @xmath0 , as @xmath7 is not a rotation matrix , and has no intrinsic counterpart .",
    "taking advantage of the lie group structure of the space the calculations are rather simple and direct . viewing @xmath0 as a manifold and choosing an invariant metric , we recover in a simple way the result derived by s. smith in @xcite ( see also the recent work of n. boumal @xcite ) .    as a straightforward application , the result",
    "is first applied to the static attitude estimation problem , also known as whaba s problem @xcite , for which we derive a lower bound .",
    "note a ( classical ) cramr - rao lower bound has already been proposed in @xcite for the linearized problem .",
    "then , we consider in section [ taylor : sec ] the problem of attitude estimation / filtering from vector measurements and angular velocity measurements from a gyroscope ( see e.g. @xcite and @xcite for an implementation ) in the degenerate case where the gyroscope is of much higher quality than the other sensors . for systems possessing deterministic dynamics and stochastic output measurements ,",
    "j. h. taylor proved in @xcite the cramr - rao bound is provided by the extended kalman filter ( ekf ) covariance , linearized around the _ true _ unknown trajectory of the system , and thus can not be computed online . thanks to the invariance properties of the system",
    ", we prove the cramr - rao bound does not depend on the true system s trajectory , and can be computed online .",
    "the invariant extended kalman filter ( iekf ) is a recent methodology to modify the ekf in order to account for the invariance properties of the state space when devising ekfs on lie groups , see @xcite , and more recently @xcite where an iekf is derived on @xmath0 with discrete time observations .",
    "a remarkable property of the iekf , akin to the properties of symmetry - preserving observers @xcite from which the iekf is derived , is that the estimation error system depends on the system s trajectory in a reduced manner , and sometimes does not depend on it at all , a property shared by the intrinsic cramr - rao bound derived in this paper .",
    "in fact , the links between both theories go beyond : in the case considered here on @xmath0 , we prove in section [ kalman : sec ] the _ intrinsic _ cramr - rao bound coincides with the covariance matrix returned by the _ invariant _ ekf ( and thus can be computed online ) .",
    "consider a family of probability densities @xmath8 parameterized by a vector @xmath9 .",
    "consider an unbiased estimator @xmath10 of the parameter @xmath11 from a sample measurement @xmath12 .",
    "the requirement that @xmath13 be unbiased means it must be `` good '' ( for a large sample ) whatever @xmath11 .",
    "because of this requirement , the estimator can not recover @xmath11 exactly , given a single or a finite number of measurements .",
    "this fact is formalized by the well - known existence of a lower bound on the accuracy of the estimator : the so - called cramr - rao bound .",
    "mathematically , it states the average estimation error covariance is lower bounded as follows @xmath14where @xmath15 denotes the expectation with respect to the probability law @xmath8 , and the matrix @xmath16 ( the inequality is in the sense of the loewner order ) is the so - called fisher information matrix , defined as the hessian with respect to @xmath11 of the average log likelihood @xmath17 , where @xmath18 denotes the natural logatithm of @xmath19 .    the interesting question raised by s.t .",
    "smith @xcite , is whether there exists an analogue of this bound for a parameter @xmath11 that belongs to a riemannian manifold , and not a vector space anymore .",
    "this kind of question can arise in signal processing , where one seeks to estimate for instance a subspace , as in principal component analysis , that is , an element of the grassman manifold . to answer this question",
    ", one must first find a way to compare the estimator @xmath20 and the true parameter @xmath11 , as on a manifold the quantity @xmath21 has no meaning .",
    "this can be done through the riemann exponential map , and then , @xcite proves a cramr - rao bound can be produced . adapting the classical proof to the manifold case , he shows that the ( well - defined ) error covariance @xmath22 is lower bounded by a ( well - defined ) information matrix @xmath16 , plus additional terms steming from the curvature of the parameter space .",
    "unfortunately , the formula is not in closed form .",
    "however , for sufficiently small covariance @xmath22 it can be expanded up to terms of order @xmath23 .",
    "the quite inspiring paper @xcite draws new links between statistics and geometry .",
    "it has been in particular adapted to the quotient manifold case in @xcite . in the present paper , we derive similar results for the @xmath0 case , more simply , and we apply them to two attitude estimation problems . note that , a tutorial presentation on the intrinsic cramr - rao bound can be found in e.g. @xcite .",
    "we compute here the intrinsic cramr - rao lower bound ( icrlb ) on @xmath0 , up to the second order terms in the estimation error @xmath24 where @xmath25 denotes the bi - invariant distance on @xmath0 .",
    "this allows to recover in a simple and direct way the result of @xcite taking advantage of the lie group structure of the parameter space ( and thus without having to evaluate riemann s curvature tensor at @xmath26 ) .",
    "@xmath0 is a lie group of dimension @xmath27 , and thus a riemannian manifold .",
    "the tangent space at @xmath28 , the identity rotation , denoted @xmath29 is called the lie algebra of @xmath0 and can be identified with @xmath30 that is @xmath31 using rotation matrices ( i.e. viewing @xmath0 as a submanifold of @xmath32 ) the ( group ) exponential map defined by @xmath33\\end{aligned}\\ ] ] where expm denotes the matrix exponential map , and where @xmath34 for @xmath35 denotes the skew symmetric matrix defined by @xmath36 . in a neighborhood of @xmath28 , the exponential map can be inverted .",
    "the ( group ) logarithmic map @xmath37 is defined as the inverse of exp . for any estimator @xmath1 of a parameter @xmath2",
    ", it allows to measure the mean quadratic estimation error @xmath38 the logarithmic map allows also to define a covariance matrix of the ( right - invariant ) estimation error @xmath39 and we have as usual @xmath40 .    note that , if @xmath26 and @xmath1 denote orientations of bodies in space , the quantity @xmath41 has an intrinsic purely geometrical interpretation as its orientation denotes the axis of rotation and its norm the rotation time around which the body with configuration @xmath1 must rotate in order to reach configuration @xmath26 , and provides us with a natural orientation error in @xmath30 .",
    "consider the family of densities parameterized by an element @xmath26 of so(3 ) @xmath42where the sample space @xmath43 is a measurable space . to fix ideas",
    "we will consider in the sequel that @xmath43 is @xmath44 for @xmath45 , as will be the case in the examples .",
    "using the exponential map , and mimicking its riemannian analogue @xcite , we introduce    the intrinsic fisher information matrix can be defined in a right - invariant basis as follows for any @xmath46 @xmath47and then @xmath48 can be recovered using the standard polarization formulas : @xmath49    besides , note that , using the fact that @xmath50 and differentiating the latter equality w.r.t @xmath51 then reusing that @xmath52 we have @xmath53 allowing to recover an intrinsic version of the classical result according to which the information matrix can be also defined using a second order derivative ( i.e. a hessian ) .",
    "the intrinsic fisher information matrix also writes@xmath54    let @xmath55 be an unbiased estimator of @xmath26 in the sense of centered intrinsic ( right invariant ) error @xmath56 , that is , @xmath57 p \\left(x \\mid r \\right ) dx =   0.\\ ] ]    let @xmath22 be the covariance matrix of the estimation error as defined in .",
    "we have then @xmath58 where we have neglected terms of order @xmath59||^3\\right)$ ] . for small errors",
    ", we can neglect the terms in @xmath22 on the right hand side ( curvature terms ) yielding @xmath60 where c are terms of higher order , linked to the effect of the curvature of the parameter space @xmath0 , hence the letter c , which here stands for  curvature terms \" .      before proving the results",
    "let us give an example application . in this subsection",
    "we assume measurements are of the form @xmath61 where @xmath62 s are some reference vector in @xmath30 , and where @xmath63 s are independent isotropic and gaussian noises with covariance matrices @xmath64 .",
    "this implies the following density form @xmath65 wahba s problem consists in finding the maximum likelihood estimator of @xmath26 and has been solved using a singular value decomposition .",
    "we now give a lower bound on the estimation accuracy .",
    "let @xmath55 be an unbiased estimator of @xmath26 in the sense of the intrinsic ( right invariant ) error @xmath56 .",
    "after @xmath66 measurements , the information matrix is ( using the independence of the noises ) @xmath67 to derive @xmath16 we first note that : @xmath68 we have besides the following taylor expansion @xmath69 so the second detivative at @xmath70 of @xmath71 is @xmath72 . as we have @xmath73",
    "we obtain : @xmath74 let @xmath75 , we have proved the following relation : @xmath76 giving immediatly :    for wahba s problem @xcite , the intrinsic covariance matrix defined by where @xmath1 is any unbiased estimator for a sample of @xmath66 independent measurements , satisfies inequality where @xmath77    note that , the result does not depend on the underlying parameter @xmath26 .",
    "this might be explained using theory of equivariant estimators on lie groups that can be traced back to @xcite ( see also @xcite for a more recent exposure ) .",
    "let @xmath55 be an unbiased estimator of @xmath26 in the sense of the intrinsic ( right invariant ) error @xmath56 , that is , @xmath78 p \\left(x \\mid r \\right ) dx =   0.\\ ] ] so , if we let @xmath79 be any vector of the lie algebra and @xmath80 , differentiating the latter equality we get @xmath81 p \\left ( x \\mid \\exp(t\\xi)r \\right ) dx =   0.\\ ] ] formally , this implies @xmath82 p(x|r ) \\\\&+ \\log ( r \\hat{r}^t(x ) ) d_2p \\left ( x\\mid r \\right ) [ ( \\xi)_\\times r ] \\big ) dx = 0\\end{aligned},\\ ] ] where @xmath83 denotes the differential and @xmath84 the partial differential with respect to the second argument .",
    "for any vector @xmath85 we have thus : @xmath86 \\right\\rangle   p(x|r)dx \\\\   & = \\int_x \\left\\langle u , \\log \\left ( r \\hat{r}^t(x ) \\right ) \\right\\rangle",
    "d_2p(x\\mid r ) \\left [ ( \\xi)_\\times r \\right ] dx \\\\   & \\leqslant \\sqrt { \\int_x \\left\\langle u , \\log \\left ( r \\hat{r}^t(x ) \\right ) \\right\\rangle   ^2 p(x|r ) dx } ~\\cdot\\\\&\\qquad \\sqrt{\\int_x \\left ( d_2 \\ln p(x\\mid r ) [ ( \\xi)_\\times r ] \\right)^2 p(x|r ) dx } \\end{aligned}\\ ] ] where we used the cauchy schwarz inequality and the relationships @xmath87 we introduce a basis of @xmath88 and the matrix @xmath89",
    ". the latter inequality can be re - written as follows : @xmath90 p(x|r)dx \\right]^2   \\nonumber \\\\    & \\leqslant \\left ( u^t \\left [ \\int_x \\tilde{a}(x)^t \\tilde{a}(x ) p(x|r ) dx \\right ] u \\right ) \\xi^t j \\left ( r \\right ) \\xi \\label{cauchy - schwarz}\\end{aligned}\\ ] ]",
    "now we compute a second - order expansion of the left - hand term in the estimation error @xmath91 .",
    "to do so , we note @xmath92=\\log [ \\exp(t\\xi)\\exp(\\log(q ) )   ] $ ] is equal to ( using the baker - campbell - hausdorff formula and keeping only terms up to @xmath93 ) @xmath94 t\\xi\\ ] ] differentiating w.r.t to @xmath51 yields @xmath95 = \\big [ i - \\frac{1}{2 } ad_{\\log(q ) } + & \\frac{1}{12 } ad_{\\log(q)}^2   \\\\   & + o \\left(||\\log(q)||^3",
    "\\right ) \\big ] \\xi \\end{aligned}\\ ] ] neglecting the third - order terms we get : @xmath96 = \\left [ i - \\frac{1}{2 } ( \\log(q))_\\times + \\frac{1}{12 } ( \\log(q))_\\times^2   \\right ] \\xi = \\\\ & \\left [ i - \\frac{1}{2 } ( \\log(q))_\\times + \\frac{1}{12 } \\left ( \\log(q ) \\log(q)^t-||\\log(q)||^2",
    "i_3 \\right ) \\right ] \\xi\\end{aligned}\\ ] ] we introduce the latter second - order expansion in the error in the equation ( note this is only formal as the second - order expansion will appear in an integral ) : @xmath97 p(x|r ) dx \\xi \\bigg]^2 \\\\   & \\leqslant \\left(u^t \\left[\\int_x \\tilde{a}(x)^t \\tilde{a}(x ) p(x|r ) dx \\right]u \\right ) \\left ( \\xi^t j \\left ( r \\right ) \\xi \\right)\\end{aligned}\\ ] ] this writes using the fact that @xmath1 is unbiased ( which makes the term in front of the factor @xmath98 cancel as @xmath99 ) @xmath100 p(x|r ) dx \\xi ] ^2 \\\\   & \\leqslant \\left(u^t \\left[\\int_x \\tilde{a}(x)^t \\tilde{a}(x ) p(x|r ) dx \\right ] u \\right ) \\left ( \\xi^t j \\left ( r \\right ) \\xi \\right)\\end{aligned}\\ ] ] letting @xmath101 we get : @xmath102 i_3 + \\frac{1}{12 } p \\right ) \\xi \\right]^2 \\leqslant ( u^t p u ) ( \\xi^t j \\xi)\\ ] ] the change of variables @xmath103 i_3 + \\frac{1}{12 } p \\right ) \\xi $ ] yields : @xmath104",
    "i_3 + \\frac{1}{12 } p \\right)^{-1 } \\\\ & j \\left ( \\left[1 - \\frac{1}{12 }",
    "tr(p )   \\right ] i_3 + \\frac{1}{12 } p \\right)^{-1 } \\xi ' \\bigg)\\end{aligned}\\ ] ] this inequality is true for any @xmath105 , which implies matricially the following desired result : @xmath106 i_3 + \\frac{1}{12 } p \\right ) j^{-1 }   \\left ( \\left[1 - \\frac{tr(p)}{12 } \\right ] i_3 + \\frac{1}{12 } p \\right)\\ ] ]      the main result of @xcite stipulates that : @xmath107 - \\frac{1}{9 } r_m(p ) j^{-1 } r_m(p ) ] \\succeq j^{-1}\\ ] ] where @xmath108 is defined through riemann s curvature tensor , and making use of the bi - invariance of the right - invariant metric of @xmath0 ( see e.g. @xcite for riemann s sectional curvature formulas on lie groups ) it boils down to @xmath109)_\\times^2 ) u = -\\frac{1}{4 } u^t[p - tr(p)]u$ ] . replacing in we get : @xmath110 j^{-1 } + j^{-1 } [ p - tr(p)i_3 ]",
    "\\right ) \\\\&\\qquad- \\frac{1}{144}[p - tr(p)i_3 ] j^{-1 } [ p - tr(p)i_3 ] \\succeq j^{-1}\\end{aligned}\\ ] ] the same formula is obtained developing , which proves both results coincide .",
    "however , the reader can check that the calculation on a general riemannian manifold is more tedious ( and local ) .",
    "the lie group case is more straithtforward and the results are obtained without using the machinery of riemannian second - order geometry .",
    "we now consider the following system @xmath111 that is , the motion in space of a solid fixed at a point , having deterministic known angular velocity @xmath112 , and noisy measurements at discrete times @xmath113 .",
    "this fits into the general filtering setting of @xcite .",
    "we assume that @xmath114 and @xmath115 are gaussian noises with covariance matrices @xmath116 and @xmath117 .",
    "our goal is to derive an intrinsic lower cramr - rao bound on the estimation error .",
    "we will see it follows from the results of section [ smith : sec ] indeed , thanks to the facts that 1- @xmath118 is a deterministic quantity and 2- due to the invariance of the system , the flow can be explicitly computed .",
    "such problems arise for attitude estimation in the degenerate case where the gyroscope is infinitely better than the vector sensors .",
    "sensors measuring in the body frame vectors from the fixed frame include magnetometers , that measure the earth magnetic field in the body frame , and accelerometers , that measure the earth gravity vector field in the body frame , under static flight assumptions . for each of these sensors , the isotropy assumption of",
    "the noise is reasonable technologically , as the measurements are performed using in each case three orthogonal one - axis sensors ( accelerometers or magnetometers ) .",
    "the gaussianity is more questionable but it is a convenient and widespread assumption about the noise .",
    "the conditional intrinsic information matrix at time @xmath66 is ( using @xcite and the results above ) @xmath119 now , using the invariance of the dynamics , we see there exists a rotation @xmath120 depending only on @xmath121 such that @xmath122 .",
    "indeed @xmath120 is the solution at time @xmath123 to the differential equation on @xmath0 defined by @xmath124 ( see eg .",
    "@xcite ) . as a result",
    ", all the measurements are independent given @xmath125 and we can write : @xmath126 when deriving an intrinsic cramr - rao bound for wahba s problem in section [ smith : sec ] , we have already proved that letting @xmath127 we have for any @xmath128 such that @xmath129 that @xmath130 and the result does not depend on @xmath131 . differentiating @xmath132 twice w.r.t .",
    "@xmath51 and using with @xmath133 ( which is valid as @xmath134 for @xmath135 ) , we get finally : @xmath136      a mere application of theorem 1 implies    for the considered system , at time @xmath123 , the accuracy @xmath137 of any unbiased attitude estimator is lower bounded according to formula , with @xmath138 .",
    "for the filtering problem of section [ taylor : sec ] , on can derive an invariant extended kalman filter ( iekf ) @xcite .",
    "the iekf is a novel methodology for devising ekfs on lie groups , where the ekf is bound to respect the invariances of the problem , and where an intrinsic estimation error is linearized at each step .",
    "moreover , the exponential map allows to map the kalman correction term to the state space .",
    "the iekf for the problem above is derived on @xmath0 in the recent paper @xcite , and we briefly recall the principle here .",
    "the iekf equations write @xmath139 where @xmath140 is the gain matrix to be tuned as follows .",
    "letting @xmath141 be the right invariant estimation error projected in the lie algebra , the error system has the following remarkable autonomous form @xmath142 during the propagation step , the covariance of the linearized estimation error @xmath143 remains fixed , that is , @xmath144 as the linearized dynamics ( for the well - chosen estimation error ) yields a static system and it was assumed there is no process noise . as concerns",
    "the update step , using formula , a _ first order approximation _ to the error update equation above reads @xcite @xmath145 the gain that minimizes the increase in the covariance matrix of the linearized error at the update step is the kalman gain @xmath146 with @xmath147 , leading to the covariance update : @xmath148 as there is no a priori information about the value of @xmath149 , a usual way to initialize the filter is maximum likelihood : @xmath150 as concerns the covariance matrix @xmath151 , where @xmath152 , a first - order expansion of reads @xmath153 , i.e. @xmath154 which gives : @xmath155 .",
    "gathering the previous results we obtain :    the covariance matrix of the error returned by the iekf writes @xmath156 and thus the iekf returns the cramr - rao bound for the associated filtering problem , neglecting the curvature terms .",
    "note that , it is logical that the curvature terms be ignored by the iekf as it is based on a first order approximation of the estimation error .",
    "the result is in sharp contrast with the general theory @xcite that stipulates that the cramr - rao bound is the ekf covariance indeed , but , linearized around the true trajectory , that is unknown to the user .",
    "those bounds are referred to as `` posterior cramr - rao bounds '' in the filtering and hidden markov models ( hmm ) literature ( see e.g. @xcite ) . for invariant systems on so(3 ) , in the case of deterministic dynamics , we have proved the bound can be computed in real time .",
    "this appears as another remarkable feature of dynamical systems defined on lie groups .",
    "in this paper we have derived an intrinsic cramr - rao lower bound ( icrlb ) on @xmath0 in a straightforward way .",
    "we have applied it to derive an icrlb for wahba s problem when the noise is isotropic and gaussian .",
    "then , we have also derived an icrlb for the problem of filtering on @xmath0 a system with deterministic evolution and noisy isotropic gaussian measurements .",
    "we have also proved the _",
    "intrinsic _ crlb is the covariance matrix returned by the _",
    "ekf on @xmath0 .",
    "this is a remarkable result , as generally the crlb can not be computed online as it presupposes to know the true trajectory of the system , which is precisely what one seeks to estimate .",
    "it is thus usually reserved for offline simulations to test filters efficiency , and we generally speak of `` posterior cramr - rao bounds '' @xcite .    in the future , we would like to investigate in what ways the intrinsic gradient methods ( see @xcite ) might asymptotically reach the intrinsic cramr - rao bound . besides , we hope it is possible to derive an icrlb for the filtering problem considered in the present paper , but with a noisy evolution ( that is , using noisy gyroscopes ) . in this case",
    "it will certainly not coincide with the covariance returned by the iekf , but it might be computable online taking advantage of the invariances of the system .",
    "such results could be applied to a wide range of aeronautics estimation problems , like e.g. , @xcite , and may be useful to test efficiency of some other instrinsic filtering methods , such as @xcite .",
    "s.  bonnabel , ph .",
    "martin , and e.  salaun .",
    "invariant extended kalman filter : theory and application to a velocity - aided attitude estimation problem . in _ decision and control ,",
    "2009 held jointly with the 2009 28th chinese control conference .",
    "cdc / ccc 2009 .",
    "proceedings of the 48th ieee conference on _ , pages 12971304 .",
    "ieee , 2009 .",
    "guillaume bourmaud , rmi mgret , audrey giremus , and yannick berthoumieu .",
    "discrete extended kalman filter on lie groups . in _",
    "signal processing conference ( eusipco ) , 2013 proceedings of the 21st european _ , pages 15 .",
    "ieee , 2013 .",
    "r  mahony , tarek hamel , jochen trumpf , and christian lageman .",
    "nonlinear attitude observers on so ( 3 ) for complementary and compatible measurements : a theoretical study . in _ decision and control ,",
    "2009 held jointly with the 2009 28th chinese control conference .",
    "cdc / ccc 2009 .",
    "proceedings of the 48th ieee conference on _ , pages 64076412 .",
    "ieee , 2009 ."
  ],
  "abstract_text": [
    "<S> in this note an intrinsic version of the cramr - rao bound on estimation accuracy is established on the special orthogonal group @xmath0 . </S>",
    "<S> it is intrinsic in the sense that it does not rely on a specific choice of coordinates on @xmath0 : the result is derived using rotation matrices , but remains valid when using other parameterizations , such as quaternions . for any estimator @xmath1 of @xmath2 we give indeed a lower bound on the covariance matrix of @xmath3 , that is , the estimation error expressed in terms of group multiplication , whereas the usual estimation error @xmath4 is meaningless on @xmath0 . </S>",
    "<S> the result is first applied to whaba s problem . </S>",
    "<S> then , we consider the problem of a continuous - time nonlinear _ deterministic _ system on @xmath0 with discrete measurements subject to additive isotropic </S>",
    "<S> _ gaussian noise _ , and we derive a lower bound to the estimation error covariance matrix . we prove the _ intrinsic _ cramr - rao bound coincides with the covariance matrix returned by the _ </S>",
    "<S> invariant _ ekf , and thus _ can _ be computed online . </S>",
    "<S> this is in sharp contrast with the general case , where the bound can only be computed if the true trajectory of the system is known . </S>"
  ]
}