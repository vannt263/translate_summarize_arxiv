{
  "article_text": [
    "recent studies concerning learning processes in neural circuits have highlighted the role of spike timing and synchrony ( e.g.  in sensory systems @xcite ) , leading to a view of the learning devices as a class of time coincidence detectors of a limited number of spikes ( at least under certain circumstances ) .",
    "these observations are at the root of several fundamental questions concerning neural coding , the most important one possibly being how do neurons learn to recognize multiple spatiotemporal patterns .",
    "a very stimulating contribution in this field has been the recent introduction by gutig and sompolinsky @xcite of a perceptron - like model neuron which is able to process spatio - temporal patterns , the so called tempotron . in spite of its simplicity ,",
    "such a device is capable of decoding the information contained in the synchrony of spike patterns through a relatively simple supervised gradient learning rule .",
    "subsequent work @xcite has analyzed by statistical physics techniques the storage capacity ( i.e.  the typical maximum number of distinct input - output associations which the device can in principle be trained to reproduce , assuming that the inputs and the expected responses are drawn from some probability distribution ) and the geometry of the space of solutions of the tempotron for continuous synaptic weights .    in a nutshell , the tempotron is the simplest form of an integrate and fire ( if ) neuron , with @xmath0 input synapses of strength @xmath1 , @xmath2 ( also called synaptic weights ) . in the tempotron , each input corresponds to @xmath0 sequences of spikes , where the set of spiking times is denoted by @xmath3 .",
    "the tempotron performs a binary classification of the inputs depending on whether the membrane potential reaches or not the firing threshold @xmath4 in the given time interval . the potential at time @xmath5 is given by @xmath6 , where @xmath7 is the temporal kernel of the membrane .",
    "a standard choice for the kernel is the exponential one , namely @xmath8 , where @xmath9 and @xmath10 are the membrane and synaptic integration time constants . for this model the precise timings and the number of output spikes ( if greater than one ) play no role in the binary classification , allowing for multiple equivalent output spiking profiles for positive classifications of a given time interval .    as discussed in @xcite ,",
    "a key parameter is the quantity @xmath11 where @xmath12 is the duration of the input pattern .",
    "when both @xmath0 and @xmath13 are large , and @xmath14 , certain time correlations can be neglected and the analysis simplifies .",
    "this has allowed the authors of @xcite to estimate the storage capacity of the device for the case of continuous weights and random i.i.d .",
    "it is interesting to observe that these conditions are not far from being actually realistic @xcite .",
    "the vanishing of the correlations at different times is due to the sparse regime under which the device operates , and it means that the width of the kernel @xmath7 is much shorter than the typical interval between incoming spikes ; this in turn means that , under this regime , only quasi - simultaneous input spikes actually contribute to the depolarization at any given time , which explains why in  @xcite the theoretical and experimental results are closely reproduced with a simplified model in which time is discretized in @xmath15 bins and the output is simply given by a perceptron rule applied on each bin ( see paragraph  [ par : model ] for additional details ) .",
    "basic devices like the temportron have the potential virtue of touching those fundamental questions in neural coding which are preserved in spite of the simplicity of the device itself .",
    "in such a framework , we have approached the problem from a different angle , namely adopting a computational scheme which that is not based on a gradient - like computation but is still fully local and distributed .",
    "we study the simplified ( time - discretized ) tempotron by both the replica method and the so called message - passing approach ( or cavity method ) which allows us to study analytically the storage capacity and at the same time to derive simple learning protocols ( i.e.  training rules for the modifications of the synaptic strengths , which the device applies upon receiving input patterns and being made aware of the expected response , such that at the end of the training the desired set of input - output associations is learned ) which are efficient and do not rely on any continuity condition of the synaptic weights .",
    "we also show how to adapt the simplest of these learning protocols to address the original , continuous - time model . for the sake of simplicity we focus directly on the case of discrete synapses , although the results could be extended to the continuous case .    for the cases of single and multilayer perceptrons with firing rate coding and binary synapses we have shown in previous works @xcite that the message - passing approach is indeed efficient in solving the learning problem for random patterns and that the computational scheme can be simplified to the point of providing extremely simple learning protocols .",
    "these past results together with the novel ones on spatio - temporal coding presented here should be of practical interest for large scale neuromorphic devices and hopefully for providing novel hints on aspects of synaptic plasticity .",
    "[ [ the - modelparmodel ] ] the model[par : model ] + + + + + + + + + + + + + + + + + + + +    we studied two tempotron scenarios , one in which synaptic conductances can take values in @xmath16 , and one in which they can take values in @xmath17 , focusing on the former case in simulations . as in the final paragraphs of  @xcite , we worked under the simplifying assumption that input and output spike patterns can be encoded ( via binning ) as sparse strings of @xmath18 s and @xmath19 s , and that the relationship between the inputs and the output at any given time bin is given by a perceptron rule .",
    "as mentioned in  @xcite and in the introduction , we expect that this simplification does not qualitatively alter the overall picture under the sparse regime considered , since even in the integrate - and - fire model only quasi - simultaneous input spikes affect the overall depolarization at any given time , due to the fast membrane decay constant w.r.t .  the typical inter - spike interval ; indeed ,",
    "our numerical results ( see section [ sub : continuous ] ) show that it is even possible to use a time - discretized learning protocol to address the original continuous - time classification problem .",
    "we thus consider a classification device with @xmath0 binary synapses , @xmath1 with @xmath20 , which has to learn to classify @xmath21 input patterns .",
    "the input patterns are @xmath22 matrices ( where @xmath13 here corresponds to the @xmath23 discussed in the introduction ) whose elements are @xmath24 with @xmath20 , @xmath25 and @xmath26 . for each pattern @xmath27 and at each time step @xmath5 ,",
    "the device response is given by @xmath28 , where @xmath29 is the heaviside step function and @xmath4 is a threshold ; we call the vector @xmath30 the _ internal representation _ for the pattern @xmath27 . finally ,",
    "a pattern @xmath27 is classified according to @xmath31 , i.e.  the overall output @xmath32 equals @xmath18 if the internal representation is a vector of all @xmath18 s , or it equals @xmath19 if at least one element of @xmath33 is @xmath19 .",
    "each pattern has a desired output @xmath34 which is to be compared to the actual output @xmath32 : the classification problem is satisfied when @xmath35 for all @xmath27 . for notational simplicity , we also define @xmath36 and @xmath37 when we need to convert the outputs so that they take values in @xmath16 .",
    "we studied the case in which all inputs and expected outputs are i.i.d .",
    "random variables .",
    "we call @xmath38 the output frequency ( i.e.  @xmath39 with probability @xmath38 ) and @xmath40 the input frequency ( i.e.  @xmath41 with probability @xmath40 )",
    ". we will assume in the following that @xmath42 : this ensures that the probability that a vector @xmath43 is composed of all @xmath18 s is @xmath44 , i.e.  has the same statistics as the internal representations which satisfy the input / output associations .",
    "clearly , the model reduces to a standard perceptron when @xmath45 .",
    "we assume that @xmath46 , @xmath47 and @xmath14 ; in this case , @xmath48 , which shows that the inputs are sparse at large @xmath13 with our choice for @xmath40 . for simplicity , all our theoretical results and simulations will be presented for the case @xmath49 , which is the value that maximizes the capacity .",
    "we studied the device described above within the replica theory , in a replica symmetric ( rs ) setting for the internal representations , in the limit of large @xmath13 , both for the case in which @xmath50 and @xmath51 , and estimated the entropy , the critical capacity , the optimal value for the threshold , and studied the structure of the space of the solutions and the valid internal representations .",
    "the results are almost identical for both @xmath52 and @xmath53 cases , so in the following we will only specify the model when a difference arises .",
    "we confirmed the results , where possible , with the cavity method ( see section  [ sub : cavity - method ] ) .",
    "all details of the calculations are provided in the appendix ( section  [ sec : appendix :- replica - calculations ] ) ; here we summarize the results .",
    "the zero - temperature entropy of the device is defined as @xmath54 , where @xmath55 is the number of solutions ( valid configurations of @xmath56 s ) to the problem associated with some choice of the patterns @xmath57 and their expected outputs @xmath58 ( see also eq .  [",
    "eq : v ] ) , and @xmath59 denotes the average over the patterns .",
    "@xmath60 ca nt be negative ( since the number of valid configurations is an integer number ) ; the value of @xmath61 at which @xmath60 vanishes is called the _",
    "critical capacity _ , and represents the typical number of patterns per synapse which can be correctly classified by the device ( i.e.  stored ) when the patterns are extracted according to the random i.i.d .",
    "distribution which we are studying .",
    "the rs replica calculation predicts @xmath62 , which interestingly does not depend on @xmath13 ( provided @xmath63 ) .",
    "this function goes to zero at @xmath64 , which coincides with the information theoretic upper bound , i.e.  the device is able to store one bit of information per synapse .",
    "this is in contrast with other related architectures , e.g.  the multi - layer perceptron .",
    "after this point , the entropy is negative , and therefore the rs solution is no longer valid .",
    "the typical value of the overlap between two different solutions to the same classification problem , defined as @xmath65 for two solutions @xmath66 and @xmath67 , is constant for all values of @xmath61 , and as low as possible , i.e.  @xmath68 in the @xmath52 case and @xmath69 in the @xmath53 case , where @xmath70 is the typical fraction of non - null synapses ( which we found to be @xmath71 ) . in terms of the structure of the space of the solutions ,",
    "this means that the clusters of solutions are isolated ( point - like ) .",
    "we expand the threshold in series of @xmath72 and write @xmath73 .",
    "the optimal value of @xmath74 is @xmath18 in the @xmath52 case and @xmath75 for the @xmath53 case ; the value of @xmath76 does not affect the capacity , but we can set it so that synaptic values are unbiased : @xmath77{1-f^{\\prime}}\\right)\\label{eq : opt_theta}\\ ] ] where @xmath78 is the inverse of the complementary error function .",
    "we found that the valid internal representations follow a binomial distribution at large @xmath13 , i.e.  that the probability distribution for the value at each time bin is independent of the others .",
    "this fact is in agreement with the continuous model findings @xcite , and it is interesting for two reasons : on one hand , it confirms that different time bins are uncorrelated in the sparse limit , which is important in order to achieve efficiency in applying the cavity method . on the other hand , it means that the distributions of the input and output spike trains are identical ( note that our choice of @xmath40 only ensures that the all - zero string occur with the same probability in input and output , but does not imply that the non - zero strings have the same distribution of the number of spikes ) . in turn , this is a necessary condition for recurrent networks to be built and work under this regime , which would be an interesting direction for future research .",
    "we also computed how the internal representations are partitioned , and found that the rescaled entropy of the dominant internal representations ( i.e.  the logarithm of the number of different internal representations for a pattern which are associated with the largest portions of the solution space ) is given by @xmath79 .",
    "this means that , as @xmath13 increases , the number of valid dominant internal representations increases as @xmath80 , while the number of synaptic states associated with each of them correspondingly shrinks , so that the overall entropy remains constant .",
    "the cavity method has been shown  @xcite to provide an alternative scheme for deriving the results from replica theory in the case of the binary perceptron with binary @xmath52 inputs and binary @xmath52 synapses .",
    "it has also been used on single instances of the learning problem on such devices ( in which case it is known as the belief propagation algorithm , i.e.  bp ) to study the space of the solutions for some particular instance and for deriving heuristic learning algorithms  @xcite . in those studies ,",
    "the problem is represented as a factor graph in which synaptic weights are represented by variable nodes , and input patterns ( and their desired outputs ) are represented by factor nodes ; messages are exchanged between the two types of nodes along the edges of the graph , representing marginal probabilities over the states of the variables ; global thermodinamic quantities such as the entropy can be computed from the messages provided they satisfy the bp equations ( which is typically achieved by reaching a fixed point in an iterative algorithm ) .",
    "replica theory results can then be reproduced numerically with bp by averaging the computed quantities from a large number of samples of sufficient size .    in the case of the present study , however , in which the input patterns take values in @xmath81 ,",
    "the approach used in @xcite can not be applied directly for the sake of reproducing replica theory results , not even in the perceptron limit @xmath45 .",
    "this is due to a violation of the underlying assumption of the cavity method , known as the clustering property , as will be explained in greater detail at the end of this section , and as a result the standard bp equations are approximate , rather than becoming asymptotically exact in the large @xmath0 regime .",
    "thus , in order to reproduce the replica theory results , the standard bp equations must be amended ; however , since the heuristic algorithm described in section  [ sub : reinbp ] is based on the standard bp , which is simpler and more computationally efficient , and proves equally effective to the corrected - bp version , we will first derive the standard bp equations , and describe how to correct them afterwards .",
    "[ [ standard - belief - propagation - algorithmparstandard - bp ] ] standard belief propagation algorithm[par : standard - bp ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as mentioned above , messages on the factor graph represent probabilities over the variable nodes ( i.e.  the synaptic weights ) , and therefore can be represented by a single real value : as in  @xcite , we use average values for this purpose ( also called magnetizations ) .    the bp equations , written in terms of the cavity magnetizations @xmath82 and @xmath83 , read : @xmath84 where @xmath85 are synapse indices and @xmath86 are pattern indices .",
    "the second equation is the difference between the probabilities that the pattern @xmath27 is satisfied when synapse @xmath87 takes the values @xmath19 and @xmath88 , respectively , assuming all other synaptic values are distributed according to the cavity magnetizations @xmath89 ( for @xmath90 ) .",
    "these can be computed from the probability that the internal representation is all zero given the value of @xmath1 : @xmath91    with this , and using the shorthand notation @xmath92 , we can write eq .  [ eq : n1 ] as : @xmath93    in order to compute efficiently the function @xmath94 , we use the central limit theorem , which ensures that for large @xmath0 we have : @xmath95 where @xmath96 is a @xmath13-dimensional multivariate gaussian with mean @xmath97 and covariance matrix @xmath98 , whose elements are given by : @xmath99    the region of integration is a product of semi - bounded intervals : @xmath100 $ ] .",
    "computing this integral in general is very expensive , and rapidly becomes infeasible for large @xmath13 .",
    "however , the sparsity of the input patterns implies that diagonal terms are of order @xmath101 , while off - diagonal terms are of order @xmath102 and can be neglected , simplifying the computation : @xmath103 equations  [ eq : m1],[eq : n2],[eq : a1],[eq : sigma1 ] and  [ eq : b3 ] form a closed system which allows computations to be performed effectively , and which can conveniently be modified to derive a heuristic solver algorithm ( see section  [ sub : reinbp ] ) .",
    "however , as stated at the beginning of this section , these equations fail to exactly reproduce the replica theory results .",
    "[ [ corrected - belief - propagation - algorithmparcorrected - bp ] ] corrected belief propagation algorithm[par : corrected - bp ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the reason for the failure of the standard bp equations to provide correct results ( e.g.  when computing the entropy ) is that when the inputs are unbalanced , i.e.  they do nt average to @xmath18 ( as is necessarily the case when the values are in @xmath17 ) , the clustering property , i.e.  the assumption that that the messages incoming into variable nodes from different factor nodes are uncorrelated , is violated .",
    "this can be seen by considering ( see  @xcite ) : @xmath104 which is @xmath105 unless the average input @xmath106 is zero , in which case it is @xmath107 and becomes negligible .",
    "only in that case , therefore , standard bp equations become asymptotically correct for large @xmath0 ; in all other circumstances , they only provide an approximation ( numerical experiments show that for our model they sistematically predict a slightly lower entropy than the correct one ) .",
    "we also note that , if we define @xmath108 , where @xmath109 and @xmath110 with average @xmath111 , we can split the depolarization as such : @xmath112 where we defined the overall magnetization @xmath113 .",
    "it becomes apparent that the depolarization distributions induced by the different patterns are all correlated via @xmath12 , which is a global quantity .",
    "we can however amend the bp algorithm , and derive correct marginals and therefore correct global thermodynamic quantities , by studying a related problem , in which this contribution is removed from the factor nodes and induced by an external field instead ; this suggests the following modification to the cavity equations : we start by choosing a value for the magnetization , call it @xmath114 , and we consider the problem with patterns @xmath115 instead of @xmath116 , thereby ensuring that the clustering property holds , and with an additional external field @xmath117 applied to each variable node , thus modifying eqs .",
    "[ eq : m1 ] and  [ eq : n1 ] as such : @xmath118 the total magnetization @xmath12 can be obtained from the cavity marginals as : @xmath119 therefore , we can ensure that , at the fixed point , @xmath120 by just adding an extra step to the bp iterative process in which @xmath117 is modified at each iteration according to the difference @xmath121 .    after convergence , we compute the entropy @xmath122 , and via",
    "this define @xmath123 .",
    "then , the marginals computed for the problem defined by @xmath124 are the same as those to the original problem , and are asymptotically correct ( within the rs assumption ) , allowing us to compute all the desired properties on a given instance of the original problem via this modified cavity method . by averaging over many different samples",
    ", we can recover the results of the replica method , as shown for the entropy curves in fig .",
    "[ fig : entropy ] .",
    "entropy vs. @xmath61 as computed by the cavity method at different values of @xmath13 , compared to the one predicted by the replica theory for @xmath47 and @xmath14 .",
    "each point shows the average and standard deviation over @xmath125 random samples with @xmath126 . ]",
    "belief propagation equations , in their message passing form over single problem instances , have repeatedly proven to provide very effective heuristics when modified in order to produce optimal configurations @xcite .",
    "two main ways in which this can be achieved are decimation @xcite and reinforcement @xcite , which can be seen as a `` soft decimation '' process . in decimation ,",
    "cycles are performed which alternate message passing and fixing ( or `` freezing '' , or `` decimating '' ) the most polarized free variables , until all variables are fixed . in reinforcement ,",
    "the iterative equations have an additional term which has the role of a time - dependent external field , and which is computed from the magnetizations obtained at the preceding time step , so that the system is driven towards a completely polarized state .",
    "following @xcite the reinforced bp equations are the same as the normal bp equations with one single difference for eq .",
    "[ eq : m1 ] , which becomes : @xmath127 where @xmath128 is the iteration step , @xmath129 is the total magnetization of variable @xmath87 at iteration @xmath128 , and @xmath130 is an iteration - dependent reinforcement parameter , which we set as @xmath131 .",
    "note that the additional term is proportional to the iteration step , and therefore dominates for large @xmath128 , ensuring that polarization towards one single configuration is eventually achieved ( although in practice computational problems will arise in difficult or unsatisfiable situations , due to the limited precision of the floating point representation ) .",
    "we implemented both the decimation and the reinforcement schemes , for both the standard version of bp ( for which the marginals are approximate due to correlations between different messages ) and the corrected version ( for which marginals are exact , at the cost of increased computational complexity and running time ) .",
    "since we did not find the corrected version to provide any significant advantage over the nave version ( which is not particularly surprising , considering that the approximation provided by standard bp is rather good , and that the introduction of the reinforcement term introduces spurious correlations by itself ) , here we will only present results for the latter case .",
    "the value of @xmath132 is a parameter of the solver algorithm ; higher values of @xmath132 make the algorithm greedier , in that the messages are polarized more quickly but can get trapped into a non - zero - energy state , while reducing @xmath132 improves the accuracy of the algorithm at the cost of requiring more iterations . in practical tests , we found that by using eq .",
    "[ eq : n2 ] we were able to reach values of @xmath61 as high as @xmath133 , but only at the cost of using extremely small values of @xmath132 ( of the order of @xmath134 for @xmath126 and @xmath135 ) , and therefore of an impractically high computational time .    however , we found heuristically that , by detecting when an excessively polarized state was reached , and introducing a `` depolarization event '' triggered by such condition , we could achieve the same results with much higher values of @xmath132 , and therefore in a much shorter computational time .",
    "more in detail , we introduced , at each iteration step , a check to detect cases in which any of the terms in the denominator if eq .",
    "[ eq : n2 ] goes to @xmath18 , indicating a numerical problem due to excessively polarized magnetizations in a state of non - zero energy . whenever this condition is found , we divide all messages @xmath136 and total magnetizations @xmath137 by a positive factor @xmath138 ( thus depolarizing all the messages ) , and reset @xmath130 to @xmath18 . in subsequent iterations , we keep increasing @xmath130 linearly in steps of @xmath132 ( until another event is detected ) .",
    "we obtain good results by setting the factor @xmath138 to @xmath139 initially , and increasing it by one at every invocation of this additional depolarization rule .",
    "indeed , since @xmath130 does not increase monotonically any more in this scheme , this modified algorithm will not be guaranteed to polarize towards a single state , unless the state itself has zero energy and therefore represents a solution to the problem .",
    "[ fig : bprein ] shows the performance of this algorithm for @xmath126 and @xmath135 . setting @xmath140 as the maximum number of iterations , a critical capacity of almost @xmath141",
    "is achieved .",
    "number of iterations until solving ( green curve ) and solving probability ( red curve ) for different values of @xmath61 , for a tempotron device with @xmath126 and @xmath135 , with the reinforced bp algorithm .",
    "the parameter @xmath142 was set to @xmath143 .",
    "for each point , @xmath125 samples were used .",
    "the number of iterations was capped at @xmath140 . ]      as for the case of the simple perceptron @xcite , it is possible to drastically simplify the reinforced bp equations ( in a purely heuristic way ) , and obtain an online algorithm which , when parameters are set to their optimal values , proves to be almost as effective at learning as reinforced bp itself , while dramatically reducing computational requirements .",
    "this algorithm requires a hidden state @xmath144 to be endowed with each synapse .",
    "this hidden state can only assume odd integer values , and is capped by a maximum absolute value @xmath145 , so that each synapse has a total of @xmath146 hidden states .",
    "the hidden state and the synaptic weight @xmath1 are related by the simple expression @xmath147 . in all simulations , we set the initial state of the @xmath144 states by randomly drawing values from @xmath148 .",
    "the learning protocol turns out to be as follows : patterns @xmath149 are presented in random order to the device , computing the depolarization @xmath150 ; from this , we determine @xmath151 and compute @xmath152 ; depending on the value of @xmath153 , we choose one of three actions :    @xmath154 : :    : do nothing @xmath155 : :    : with probability @xmath156 , update synapses for which    @xmath157 and    @xmath158 ; with probability    @xmath159 do nothing @xmath160 : :    : update all synapses for which @xmath157    the synaptic update rule is always of this form : @xmath161 which implies that synaptic values @xmath1 are only updated in the @xmath162 case , and only if @xmath157 and @xmath163 .",
    "as stated above , we impose @xmath164 , so that the update rule is not applied when @xmath165 .",
    "the probability @xmath156 of taking an action in the `` barely correct '' case @xmath166 is a parameter of the algorithm , just as @xmath145 .",
    "we determined empirically the optimal values of @xmath156 and @xmath145 for different values of @xmath0 and @xmath13 by extensively testing the space of the parameters .",
    "our results , shown in fig .",
    "[ fig : simplestats ] , indicate that @xmath167 works best for all values ( we explored the values of @xmath156 in steps of @xmath168 , and that the optimal value of @xmath145 is reasonably well fitted by a function @xmath169 , where @xmath170 .",
    "the capacity decreases with @xmath0 , for fixed @xmath13 , but does not seem to tend to @xmath18 asymptotically ( see inset in fig .",
    "[ fig : simplestats ] ) .    optimal critical capacity ( left ) and optimal value of the number of hidden states @xmath146 ( right ) for various values of @xmath0 and @xmath13 .",
    "critical capacity is defined as the value of @xmath61 for which the probability of successfully solving the problem in @xmath140 iterations or less is @xmath171 .",
    "optimal @xmath145 is defined as the value of @xmath145 which yields the highest critical capacity .",
    "the parameter @xmath156 is set to @xmath172 in all plots shown here , since that was found to be the optimal value independently of other parameters .",
    "at least @xmath173 random samples were generated for each combination of @xmath174 in order to determine the success probability and therefore the critical capacity .",
    "@xmath61 was explored in steps of @xmath175 .",
    "the inset in the left panel shows the critical capacity as a function of @xmath176 for @xmath135 ; the solid black line is a fit by an exponential function .",
    "the solid black lines in the right panel show the fit of @xmath146 as a function of @xmath0 and @xmath13 via @xmath169 . ]      as a way to verify that the model and learning protocols which we studied are relevant in a more biologically realistic setting , we adapted the simplified bp - inspired scheme described in the previous section to address the continuous - time classification problem ( see introduction ) : for a given an instance of the problem , we discretize the time in @xmath13 bins , apply the bp - inspired learning protocol ( slightly modified to use continuous inputs ) , and test the resulting synaptic weights assignments on the continuous device ( see the appendix for details , section  [ sec : appendix :- time - discretization ] ) .",
    "we found that in a device with @xmath126 synapses , with time constants @xmath177 and @xmath178 , tested on @xmath179 long input patterns discretized in @xmath180 bins , this scheme can achieve a classification error lower than 1% up to @xmath181 , demonstatrating that indeed under these conditions not much relevant information is typically lost in the time discretization process , and that the proposed time - discretized learning protocol can be effective even in a continuous - time setting .",
    "we have presented a theoretical analysis of the computational performance of the tempotron model with discretized time and discrete synaptic weights .",
    "the results show that the device is able to learn random spatio - temporal patterns at a learning rate which saturates the information theoretic bounds .",
    "in addition to this , and possibly of more practical interest , we have been able to derive some novel learning protocols which are local and distributed and do not rely on a gradient descent process on the synaptic weights .",
    "these algorithms are based on the message - passing method and extend previous works on rate - coding networks .",
    "specifically , we have shown that the message - passing algorithms can store spatio - temporal patterns at very high loads and that even some extremely simplified versions are still able to store an extensive number of patterns efficiently .",
    "furthermore , we showed that the discretized - time algorithm can even be adapted to effectively address the original , continuous - time version of the problem .",
    "our approach can be applied to both discrete and continuous synapses .",
    "many open problems remain to be studied , starting from how these protocols can be made even simpler in a biologically plausible modeling context .",
    "still we believe that at least as far as artificial neural systems is concerned these results could find direct application in neuromorphic devices .",
    "we will consider the case in which synaptic weights take values in @xmath148 first .",
    "the volume of the space of the solutions for a given instantiation of the patterns can be written as : @xmath182 where index @xmath20 is used for synapses , index @xmath25 is used for time bins , index @xmath183 is used for patterns , the auxiliary variables @xmath184 are the internal representations ( they are equal to @xmath185 , see section  [ sec : introduction ] ) , and@xmath186 is a characteristic function ensuring that the internal representation @xmath187 is compatible with the output @xmath188 .    from here on , for simplicity , we will omit the subscript @xmath189 from the outputs @xmath32 .    in order to compute the entropy",
    ", we need to compute the quenched average @xmath190 ; we do this by using the replica trick:@xcite @xmath191    where we compute @xmath192 for integer values of @xmath83 , and use the analytic continuation to compute the limit @xmath193 .",
    "the average over the replicated volume is : @xmath194 we used the index @xmath195 to denote the replica .",
    "we can now use the integral representation of the @xmath196 function , @xmath197 , and compute the average over the input patterns , using their independence and the @xmath46 limit ( in the following , all integrals are assumed to be on @xmath198 $ ] unless otherwise specified ) : @xmath199 where @xmath109 and @xmath200 are the average value and the variance of the inputs @xmath201 , respectively .",
    "we then introduce order parameters @xmath202 and @xmath203 via dirac - delta functions , and their conjugates @xmath204,@xmath205 via integral expansion of the deltas , and get : @xmath206 where in the second step we dropped indices @xmath87 and @xmath27 .",
    "we expand the threshold @xmath4 in series of @xmath72 : @xmath207 from which we immediately get the relation : @xmath208    this leaves us with : @xmath209    in the @xmath46 limit , this integral can be computed by the saddle point method : we introduce the rs ansatz for the solution : @xmath210 , @xmath211 , @xmath212 , and analogous expressions for the conjugate parameters . therefore : @xmath213 where in the second step we introduced auxiliary gaussian integrals ( we use the shorthand notation @xmath214 and define @xmath215 ) , which allows to drop the replica index @xmath216 , and in the last step we used the @xmath193 limit . finally , we obtain the expression for the entropy : @xmath217 the expression for @xmath218 is the familiar expression for perceptron models , and it can be written more explicitly for the two cases @xmath219 and @xmath220 : @xmath221    the expression for @xmath222 can be manipulated further : @xmath223    in the limit of @xmath47 , we can use the central limit theorem and keep only the higher order terms in @xmath13 , and obtain : @xmath224 where we defined : @xmath225    the saddle point equation for @xmath12 gives : @xmath226    which implies : @xmath227",
    "this in turn puts to zero @xmath228 and @xmath229 : @xmath230    optimizing with respect to @xmath74 , i.e.  imposing @xmath231 , we also get @xmath232 .",
    "the remaining equations are different for the cases @xmath52 and @xmath233 . for the @xmath52 case : @xmath234    the result @xmath235 is obvious . from @xmath232 and @xmath236 , and since @xmath237 , we get @xmath68 and @xmath238 .    for the @xmath233 case:@xcite @xmath239    from @xmath236 and @xmath240 these simplify to : @xmath241    from @xmath242 we see that the cross - overlap is as low as possible , like in the @xmath52 case : the physical interpretation is that clusters of solution are isolated , i.e.  point - like .",
    "the only remaining order parameters are @xmath12 and @xmath76 , which are related by eq .",
    "[ eq : lambda ] and give :    @xmath243{1-f^{\\prime}}\\right)\\right)\\ ] ]    therefore , in order to have unbiased synapses , i.e.  @xmath244 , we may set @xmath76 to : @xmath77{1-f^{\\prime}}\\right)\\label{eq : theta_1}\\ ] ] with our choice for the distribution of the inputs , @xmath245{1-f}$ ] , this formula starts from @xmath18 at @xmath45 , has a maximum for @xmath246 and slowly decreases ( as @xmath247 ) to @xmath18 as @xmath13 diverges ; the reason for this behaviour is that there are two competing tendencies at work as @xmath13 increases : on one hand , the increase in the length of the internal representation while @xmath38 is kept constant requires that more and more individual bins fall below threshold ; on the other hand , the sparsification of the inputs reduces the fluctuations in the depolarization ; this second contribution dominates for large @xmath13 and so the threshold goes to @xmath18 , but for practical purposes ( i.e.  for biologically relevant values of @xmath13 ) it does not become negligible .    from the above results , we can determine the entropy : @xmath248 which goes to @xmath18 at : @xmath249 which coincides with the information theoretic upper bound .",
    "the probability distribution of the number of output spikes ( i.e.  @xmath19 s in the internal representation ) can be obtained by taking the ratio between the volume of the solution space in which one pattern is restricted to produce @xmath250 spikes and the total volume : @xmath251 where @xmath252 is the kronecker delta function : @xmath253    we can write @xmath254 , restrict ourselves to integer @xmath83 and obtain an expression almost identical to eq .",
    "[ eq : vol^n ] , except for the kronecker delta affecting pattern @xmath255 : @xmath256    the computation follows the one for the entropy ; the only affected term is @xmath222 , and only one term survives the limit @xmath193 , giving : @xmath257 where we wrote @xmath258 ( see eq .",
    "[ eq : eta ] ) for short . for large @xmath13 and finite @xmath250 ,",
    "this approximates to : @xmath259 where @xmath260 ( see eq .",
    "[ eq : lambdadef ] ) , and in the second step we used eq .",
    "[ eq : lambda ] from the saddle point solution .",
    "the result is a binomial distribution , in which the probability of producing a spike is @xmath261{1-f^{\\prime}}$ ] , which is our choice for the input frequency @xmath40 .",
    "we can study the structure of the space of the internal representations by following @xcite : we consider the volume of each internal representation @xmath262 , where @xmath263 is an internal representation , such that the overall volume can be written as @xmath264 ; then we define : @xmath265 and study the free energy defined by : @xmath266 which , once known , allows to derive the size of the internal representations from the quantity @xmath267 ( for @xmath268 , @xmath269 where @xmath270 is the typical volume of the dominant internal representations ) , and their number from the micro - canonical entropy @xmath271 ( for @xmath268 , @xmath272 is the logarithm of the typical number of internal representation of size @xmath270 , divided by @xmath0 ) .",
    "the computation is performed by using the replica trick for @xmath156 integer and then performing an analytic continuation :    @xmath273    where we introduced the new internal representation replica index @xmath274 .",
    "the computation follows the steps of the entropy computation of section  [ sub : entropy ] , but requires the introduction of order parameters with 2 replica indices ; in particular @xmath275 , which in the rs anzatz can take 3 values : @xmath276 we obtain , for large @xmath13 : @xmath277    the saddle point equations for @xmath268 give the same results as before , as expected ; in particular , we find @xmath278 in the @xmath52 case and @xmath279 in the @xmath233 case , and @xmath280 as expected .",
    "furthermore , we have : @xmath281 where we used the shorthand notation @xmath282 . from this and from the saddle point equations at @xmath268 , in particular from eq .",
    "[ eq : lambda ] , we obtain the weight and the entropy of the dominant internal representations for @xmath283 : @xmath284    from these , we can find the leading terms of the number of different dominant internal representations : @xmath285 and their volume : @xmath286",
    "the learning protocol presented in section  [ sub : bpi ] can be easily generalized to the case in which the input patterns @xmath57 are not binary , but positive and continuous : the only required change is that the update rules , rather then being applied only to those synapses for which @xmath287 , are applied to all synapses with probability @xmath288 .",
    "therefore , the actions taken upon determining @xmath289 and the value @xmath290 are :    @xmath154 : :    : do nothing @xmath155 : :    : with probability @xmath156 , update synapses for which    @xmath158 , each with probability    @xmath291 ; with probability @xmath159    do nothing @xmath160 : :    : update all synapses , each with probability @xmath292      as an additional generalization , we also introduce a robustness parameter @xmath115 and re - define @xmath293 , where @xmath4 is the firing threshold : this forces the learning algorithm to seek solutions in which the depolarization is far from the threshold . in the numerical experiments described in section  [ sub : continuous ] we used the value @xmath294 , increasing it from @xmath18 in steps of @xmath143 for @xmath295 iterations at each step ; the other parameters of the model used in those tests were @xmath126 , @xmath296 , @xmath297 and @xmath298 .      in this section",
    "we describe the time - discretization process mentioned in section  [ sub : continuous ] : we consider a continuous - time model as described in the introduction ; then , for any given input spike train , we compute the post - synaptic - potential trace @xmath299 , where @xmath7 is the temporal kernel of the membrane .",
    "we divide the time window @xmath12 in @xmath13 equal bins , and for each bin @xmath300 we compute the input @xmath301 as the fraction of the membrane kernel in that bin : @xmath302 where we used the fact that @xmath303 with out choice of @xmath304 .",
    "the time - discretized patterns can then be passed to the discrete algorithm for deriving a vector of synaptic weights , which in turn can be tested on the original model . in our numerical experiments",
    ", we generated input spike trains by a poisson process with a rate chosen as to obtain the correct value of the input frequency @xmath40 ( see section  [ par : model ] ) after the discretization in @xmath13 time bins .",
    "when testing the solution , we used the value of the firing threshold for the continuous unit which gave the lowest number of errors .",
    "baldassi c , braunstein a , brunel n and zecchina r 2007 _ proceedings of the national academy of sciences _ * 104 * 1107911084 issn 0027 - 8424 , 1091 - 6490 pmid : 17581884",
    "bailly - bechet m , borgs c , braunstein a , chayes j , dagkessamanskaia a , franc cois j and zecchina r 2010 _ proceedings of the national academy of sciences _ * 108 * 882887 issn 0027 - 8424 http://www.pnas.org/content/108/2/882.short"
  ],
  "abstract_text": [
    "<S> neural networks are able to extract information from the timing of spikes . here </S>",
    "<S> we provide new results on the behavior of the simplest neuronal model which is able to decode information embedded in temporal spike patterns , the so called tempotron @xcite . using statistical physics techniques </S>",
    "<S> we compute the capacity for the case of sparse , time - discretized input , and `` material '' discrete synapses , showing that the device saturates the information theoretic bounds with a statistics of output spikes that is consistent with the statistics of the inputs . </S>",
    "<S> we also derive two simple and highly efficient learning algorithms which are able to learn a number of associations which are close to the theoretical limit . </S>",
    "<S> the simplest versions of these algorithms correspond to distributed on - line protocols of interest for neuromorphic devices , and can be adapted to address the more biologically relevant continuous - time version of the classification problem , hopefully allowing for the understanding of some aspects of synaptic plasticity . </S>"
  ]
}