{
  "article_text": [
    "in line with information theory , we treat a literary text as the output of a stationary and ergodic source that takes values in a finite alphabet and we look for information about the source through a statistical analysis of the text  @xcite . here",
    "we focus on correlations functions , which are defined after specifying an observable and a product over functions .",
    "in particular , given a symbolic sequence * s * ( the text ) , we denote by @xmath2 the symbol in the @xmath3-th position and by @xmath4 ( @xmath5 ) the substring @xmath6 . as observables , we consider functions @xmath7 that map symbolic sequences * s * into a sequence * x * of numbers ( e.g. , @xmath1 s and @xmath0 s ) .",
    "we restrict to local mappings , namely @xmath8 for any @xmath3 and a finite constant @xmath9 .",
    "its autocorrelation function is defined as : @xmath10 where @xmath11 plays the role of time ( counted in number of symbols ) and @xmath12 denotes an average over sliding windows , see supporting information ( si ) sec .",
    "i for details .",
    "the choice of the observable @xmath7 is crucial in determining whether and which `` memory '' of the source is being quantified . only once a class of observables sharing the same properties is shown to have the same asymptotic autocorrelation , it is possible to think about long - range correlations of the text as a whole . in the past ,",
    "different kinds of observables and encodings ( which also correspond to particular choices of @xmath7 ) were used , from the huffmann code  @xcite , to attributing to each symbol an arbitrary binary sequence ( ascii , unicode , 6-bit tables , dividing letters in groups , etc . )",
    "@xcite , to the use of the frequency - rank  @xcite or parts of speech  @xcite on the level of words .",
    "while the observation of long - range correlations in all cases points towards a fundamental source , it remains unclear which common properties these observables share .",
    "this is essential to determine whether they share a common root ( conjectured in ref .",
    "@xcite ) and to understand the meaning of quantitative changes in the correlations for different encodings ( reported in ref .",
    "@xcite ) . in order to clarify these points we use mappings  @xmath7 that avoid the introduction of spurious correlations .",
    "inspired by voss  @xcite and ebeling _ et al . _",
    "@xcite we use @xmath13 s that transform the text into binary sequences @xmath14 by assigning @xmath15 if and only if a local matching condition @xmath16 is satisfied at the @xmath3-th symbol , and @xmath17 otherwise ( e.g. , @xmath18 _ k - th symbol is a vowel _ ) .",
    "see si - sec .",
    "ii for specific examples .      once equipped with the binary sequence  @xmath19 associated with the chosen condition @xmath16 we can investigate the asymptotic trend of its @xmath20 .",
    "we are particularly interested in the long - range correlated case @xmath21 for which @xmath22 diverges . in this case",
    "the associated random walker @xmath23 spreads super - diffusively as  @xcite @xmath24    in the following we investigate correlations of the binary sequence  @xmath19 using eq .",
    "( [ eq.mu ] ) because integrated indicators lead to more robust numerical estimations of asymptotic quantities  @xcite .",
    "we are mostly interested in the distinction between short- @xmath25 and long- @xmath26 range correlations .",
    "we use normal ( anomalous ) diffusion of @xmath27 interchangeably with short- ( long- ) range correlations of  * x*.    an insightful view on the possible origins of the long - range correlations can be achieved by exploring the relation between the power spectrum @xmath28 at @xmath29 and the statistics of the sequence of inter - event times @xmath30 s ( i.e. , one plus the lengths of the cluster of @xmath1 s between consecutive @xmath0 s ) .",
    "for the short - range correlated case , @xmath31 is finite and given by @xcite : @xmath32 for the long - range correlated case , @xmath33 and eq .  ( [ eq.spectrum ] ) identifies two different origins : ( i ) _ burstiness _ measured as the broad tail of the distribution of inter - event times  @xmath34 ( divergent  @xmath35 ) ; or ( ii ) long - range correlations of the sequence of @xmath30 s ( not summable  @xmath36 ) . in the next section we show how these two terms give different contributions at different linguistic levels of the hierarchy .      building blocks of the hierarchy depicted in fig .",
    "[ fig.1 ] are binary sequences ( organized in levels ) and links between them .",
    "levels are established from sets of semantically or syntactically similar conditions @xmath16 s ( e.g. , vowels / consonants , different letters , different words , different topics ) .",
    "each binary sequence  @xmath14 is obtained by mapping the text using a given @xmath13 , and will be denoted by the relevant condition in @xmath16 .",
    "for instance , * prince * denotes the sequence @xmath14 obtained from the matching condition @xmath37 ``  prince  '' .",
    "a sequence @xmath38 is linked to @xmath14 if for all @xmath39 s such that  @xmath40 we have  @xmath41 , for a fixed constant @xmath42 .",
    "if this condition is fulfilled we say that @xmath14 is _ on top of _ @xmath38 and that @xmath14 belongs to a higher level than @xmath38 . by definition , there are no direct links between sequences at the same level .",
    "a sequence at a given level is on top of all the sequences in lower levels to which there is a direct path . for instance , * prince * is on top of * e * which is on top of * vowel*. as will be clear later from our results , the definition of link can be extended to have a probabilistic meaning , suited for generalizations to high levels ( e.g. , ``  prince  '' is more probable to appear while writing about a topic connected to war ) .",
    "we now show how correlations flow through two linked binary sequences . without loss of generality",
    "we denote @xmath19 a sequence on top of @xmath43 and @xmath44 the unique sequence on top of  @xmath38 such that @xmath45 ( sum and other operations are performed on each symbol : @xmath46 for all @xmath47 ) .",
    "the spreading of the walker  @xmath48 associated with @xmath38 is given by @xmath49 where @xmath50 is the cross - correlation .",
    "using the cauchy - schwarz inequality  @xmath51 we obtain @xmath52 define @xmath53 , as the sequence obtained reverting @xmath54 on each of its elements @xmath55 .",
    "it is easy to see that if @xmath45 then @xmath56 . applying the same arguments above , and using that @xmath57 for any @xmath19",
    ", we obtain @xmath58 and similarly @xmath59 .",
    "suppose now that @xmath60 with @xmath61 .",
    "in order to satisfy simultaneously the three inequalities above , at least two out of the three @xmath62 have to be equal to the largest value @xmath63 . next we discuss the implications of this restriction to the flow of correlations up and down in our hierarchy of levels .    * up . *",
    "suppose that at a given level we have a binary sequence @xmath38 with long - range correlations  @xmath64 .",
    "from our restriction we know that at least one sequence @xmath19 on top of @xmath43 , has long - range correlations with @xmath65 .",
    "this implies , in particular , that if we observe long - range correlations in the binary sequence associated with a given letter then we can argue that its anomaly originates from the anomaly of at least one word where this letter appears , higher in the hierarchy of a word containing the given letter is on top of the sequence @xmath43 of that letter .",
    "if @xmath43 is long range correlated ( lrc ) then either @xmath19 is lrc or @xmath66 is lrc .",
    "being finite the number of words with a given letter , we can recursively apply the argument to @xmath66 and identify at least one lrc word . ] .    * down .",
    "* suppose @xmath19 is long - range correlated  @xmath67 . from eq .",
    "( [ eq.sum ] ) we see that a fine tuning cancellation with cross - correlation must appear in order for their lower - level sequence @xmath43 ( down in the hierarchy ) to have @xmath68 . from the restriction derived above",
    "we know that this is possible only if  @xmath69 , which is unlikely in the typical case of sequences @xmath38 receiving contributions from different sources ( e.g. , a letter receives contribution from different words ) .",
    "typically , @xmath38 is composed by @xmath70 sequences  @xmath71 , with @xmath72 , in which case  @xmath73 .",
    "correlations typically flow down in our hierarchy of levels .",
    "* finite - time effects . *",
    "while the results above are valid asymptotically ( infinitely long sequences ) , in the case of any real text we can only have a finite - time estimate  @xmath74 of the correlations @xmath75 .",
    "already from eq .",
    "( [ eq.sum ] ) we see that the addition of sequences with different  @xmath76 , the mechanism for moving down in the hierarchy , leads to @xmath77 if @xmath78 is computed at a time when the asymptotic regime is still not dominating .",
    "this will play a crucial role in our understanding of long - range correlations in real books . in order to give quantitative estimates",
    ", we consider the case of @xmath43 being the sum of the most long - range correlated sequence @xmath19 ( the one with @xmath79 ) and many other independent non - overlapping and @xmath44 are non - overlapping if for all @xmath47 for which @xmath80 we have @xmath81 . ]",
    "sequences whose combined contribution is written as @xmath82 , with @xmath83 an independent identically distributed binary random variable .",
    "this corresponds to the random addition of @xmath0 s with probability @xmath84 to the @xmath1 s of @xmath19 . in this case",
    "@xmath85 shows a transition from normal @xmath86 to anomalous @xmath87 diffusion .",
    "the asymptotic regime of * z * starts after a time @xmath88 where @xmath89 and @xmath67 are obtained from @xmath90 which asymptotically goes as @xmath91 .",
    "note that the power - law sets at @xmath92 only if @xmath93 .",
    "a similar relation is obtained moving up in the hierarchy , in which case a sequence @xmath19 in a higher level is built by random subtracting @xmath0 s from the lower - level sequence @xmath43 as @xmath94 ( see si - sec .",
    "iii - a for all calculations ) .",
    "* burstiness .",
    "* in contrast to correlations , burstiness due to the tails of the inter - event time distribution  @xmath34 is not always preserved when moving up and down in the hierarchy of levels .",
    "consider first going down by adding sequences with different tails of @xmath34 .",
    "the tail of the combined sequence will be constrained to the shortest tail of the individual sequences . in the random addition example , @xmath95 with @xmath19 having a broad tail in  @xmath34 , the large @xmath96 asymptotic of @xmath43 has short - tails because the cluster of zeros in @xmath19 is cut randomly by @xmath97  @xcite .",
    "going up in the hierarchy , we take a sequence on top of a given bursty binary sequence , e.g. , using the random subtraction @xmath98 mentioned above .",
    "the probability of finding a large inter - event time @xmath96 in @xmath43 is enhanced by the number of times the random deletion merges two or more clusters of @xmath1 s in @xmath19 , and diminished by the number of times the deletion destroys a previously existent inter - event time @xmath96 .",
    "even accounting for the change in @xmath99 , this moves can not lead to a short - ranged  @xmath34 for @xmath19 if @xmath34 of @xmath43 has a long tail ( see si - sec .",
    "iii - b ) .",
    "altogether , we expect burstiness to be preserved moving up , and destroyed moving down in the hierarchy of levels .    * summary . * from eq .",
    "( [ eq.spectrum ] ) the origin of long - range correlations  @xmath100 can be traced back to two different sources : the tail of @xmath34 ( burstiness ) and the tail of @xmath101 .",
    "the computations above reveal their different role at different levels in the hierarchy : @xmath75 is preserved moving down , but there is a transfer of _ information _ from @xmath34 to @xmath101 .",
    "this is better understood by considering the following simplified set - up : suppose at a given level we observe a sequence  @xmath19 coming from a renewal process with broad tails in the inter - event times @xmath102 with @xmath103 leading to  @xmath104  @xcite .",
    "let us now consider what is observed in * z * , at a level below , obtained by adding to  @xmath19 other independent sequences .",
    "the long  @xmath96 s ( a long sequence of 0 s ) in eq .",
    "( [ eq.renewal ] ) will be split in two long sequences introducing at the same time a cut - off @xmath105 in  @xmath34 and non - trivial correlations @xmath106 for large  @xmath3 . in this case , asymptotically the long - range correlations ( @xmath107 ) is solely due to @xmath106 .",
    "burstiness affects only  @xmath74 estimated for times  @xmath108 .",
    "a similar picture is expected in the generic case of a starting sequence  @xmath19 with broad tails in both @xmath34 and @xmath101 .    .",
    "( b , d ) transport  @xmath109 defined in eq .",
    "( [ eq.mu ] ) .",
    "the numerical results show : ( a ) exponential decay of @xmath110 with @xmath111 inset : @xmath34 in log - linear scales ; ( b ) @xmath112 ; ( c ) non - exponential decay of @xmath110 with @xmath113 ; and ( d )  @xmath114 .",
    "all panels show results for the the original and @xmath115-shuffled sequences , see legend . ]",
    "is an indicator of the burstiness of the distribution  @xmath34 .",
    "@xmath74 is a finite time estimator of the global indicator of long - range correlation  @xmath75 .",
    "a poisson process has @xmath116 .",
    "the twenty most frequent symbols ( white circles ) and twenty frequent words ( black circles ) of wrnpc are shown ( see si - tables for all books ) . @xmath117 indicates the case of * vowels * and @xmath118 of * blank space*. the red dashed - line is a lower - bound estimate of @xmath74 due to burstiness ( see si - sec .",
    "vi ) . this diagram is a generalization for long - range correlated sequences of the diagrams in ref .",
    "equipped with previous section s theoretical framework , here we interpret observations in real texts .",
    "we use ten english versions of international novels ( see si - sec .",
    "iv for the list and for the pre - processing applied to the texts ) . for each book @xmath119",
    "binary sequences were analyzed separately : vowel / consonants , @xmath120 at the letter level ( blank space and the @xmath121 most frequent letters ) , and @xmath120 at the word level ( @xmath122 most frequent words , @xmath123 most frequent nouns , and @xmath123 words with frequency matched to the frequency of the nouns ) .",
    "the finite - time estimator of the long - range correlations  @xmath74 was computed fitting eq .",
    "( [ eq.mu ] ) in a broad range of large @xmath124 $ ] ( time lag of correlations ) up to @xmath125 of the book size .",
    "this range was obtained using a conservative procedure designed to robustly distinguish between short and long - range correlations ( see si - sec .",
    "we illustrate the results in our longest novel , `` war and peace '' by l. tolstoy ( wrnpc , in short , see si - tables for the results in all books ) .",
    "one of the main goals of our measurements is to distinguish , at different hierarchy levels , between the two possible sources of long - range correlations in eq .",
    "( [ eq.spectrum ] )  burstiness corresponding to @xmath34 with diverging @xmath35 or diverging @xmath126 . to this end",
    "we compare the results with two null - model binary sequences @xmath127 obtained by applying to @xmath19 the following procedures :    * shuffle the sequence of  @xmath128 s .",
    "destroys all correlations .",
    "* shuffle the sequence of inter - event times  @xmath30 s .",
    "destroys correlations due to @xmath101 but preserves those due to  @xmath34 .    starting from the lowest level of the hierarchy depicted in fig .",
    "[ fig.1 ] , we obtain @xmath129 for the sequence of vowels in wrnpc and  @xmath74 between  @xmath130 and  @xmath131 in the other  @xmath132 books ( see si - fig .",
    "the values for @xmath133 and @xmath134 were compatible ( two error bars ) with the expected value  @xmath135 in all books .",
    "figures  [ fig.2]ab show the computations for the case of the letter  `` e '' : while @xmath34 decays exponentially in all cases ( fig .",
    "[ fig.2]a ) , long - range correlations are present in the original sequence @xmath136 but absent from the a2 shuffled version of @xmath136 ( fig .  [ fig.2]b ) .",
    "this means that burstiness is absent from @xmath136 and does not contribute to its long - range correlations .",
    "in contrast , for the word ``  prince  '' fig .",
    "[ fig.2]c shows a non - exponential  @xmath34 and fig .",
    "[ fig.2]d shows that the original sequence @xmath137 and the  @xmath138 shuffled sequence show similar long - range correlations ( black and red curves , respectively ) .",
    "this means that the origin of the long - range correlations of * prince * are mainly due to burstiness  tails of  @xmath34  and not to correlations in the sequence of  @xmath30 s  @xmath101 .    in fig .",
    "[ fig.3 ] we plot for different sequences the summary quantities @xmath74 and @xmath139  a measure of the burstiness proportional to the relative width of  @xmath34  @xcite .",
    "a poisson process has @xmath140 .",
    "all _ letters _ have @xmath141 , but clear long - range correlations @xmath142 ( left box magnified in fig .",
    "[ fig.3 ] ) .",
    "this means that correlations come from  @xmath101 and not from  @xmath34 , as shown in fig .",
    "[ fig.2](a , b ) for the letter `` e '' .",
    "the situation is more interesting in the higher - level case of _ words_. the most frequent words and the words selected to match the nouns mostly show @xmath141 so that the same conclusions we drew about letters apply to these words .",
    "in contrast to this group of function words are the most frequent _ nouns _ that have large  @xmath139  @xcite and large  @xmath74 , appearing as outliers at the upper right corner of fig .",
    "[ fig.3 ] .",
    "the case of ``  prince  '' shown in fig .",
    "[ fig.2](c , d ) is representative of these words , for which burstiness contributes to the long - range correlations . in order to confirm the generality of fig .",
    "[ fig.3 ] in the @xmath143 books of our database , we performed a pairwise comparison of @xmath74 and @xmath144 between the @xmath123 nouns and their frequency matched words .",
    "overall , the nouns had a larger @xmath74 in @xmath145 and a larger @xmath146 in @xmath147 out of the @xmath148 cases ( p - value  @xmath149 , assuming equal probability ) . in every single book at least @xmath150 out of @xmath123 comparisons",
    "show larger values of @xmath74 and @xmath139 for the nouns .",
    "we now explain a striking feature of the data shown in fig .",
    "[ fig.3 ] : the absence of sequences with low @xmath74 and high @xmath139 ( lower - right corner ) .",
    "this is an evidence of correlation between these two indicators and motivates us to estimate a @xmath139-dependent lower bound for @xmath74 , as shown in fig .",
    "[ fig.3 ] .",
    "note that high values of burstiness are responsible for long - range correlations estimate @xmath151 , as discussed after eq .",
    "( [ eq.renewal ] ) .",
    "for instance , the slow decay of  @xmath34 for intermediate  @xmath96 in * prince * ( fig .  [ fig.2]c ) leads to @xmath152 and an estimate @xmath153 at intermediate times .",
    "burstiness contribution to @xmath74 ( which gets also contributions from long - range correlations in the @xmath30 s ) is measured by @xmath154 , which is usually a lower bound for the total long - range correlations : @xmath155 .",
    "more quantitatively , consider an @xmath138-shuffled sequence with power - law @xmath34  as in eq .",
    "( [ eq.renewal ] )  with an exponential cut - off for @xmath156 . by increasing @xmath157 we have",
    "that @xmath139 monotonously increases [ it can be computed directly from @xmath34 ] . in terms of @xmath154 ,",
    "if the fitting interval @xmath124 $ ] used to compute the finite time @xmath154 is all below @xmath157 ( i.e. @xmath159 ) we have @xmath160 ( see eq .",
    "( [ eq.renewal ] ) ) while if the fitting interval is all beyond the cutoff ( i.e. @xmath161 ) we have @xmath162 .",
    "interpolating linearly between these two values and using @xmath163 we obtain the lower bound for @xmath74 in fig .",
    "[ fig.3 ] .",
    "it strongly restricts the range of possible @xmath164 in agreement with the observations and also with @xmath74 obtained for the @xmath138-shuffled sequences ( see si - sec .",
    "vi for further details ) .      the pre - asymptotic normal diffusion  anticipated in sec .",
    "* finite - time effects *  is clearly seen in fig .",
    "[ fig.4 ] .",
    "our theoretical model explains also other specific observations :    \\1 .",
    "key - words reach higher values of  @xmath74 than letters ( @xmath165 ) .",
    "this observation contradicts our expectation for asymptotic long times : * prince * is on top of * e * and the reasoning after eq .",
    "( [ eq.sum ] ) implies @xmath166 .",
    "this seeming contradiction is solved by our estimate  ( [ eq.tt ] ) of the transition time @xmath167 needed for the finite - time estimate @xmath74 to reach the asymptotic @xmath75 .",
    "this is done imagining a surrogate sequence with the same frequency of `` e '' composed by * prince * and randomly added @xmath0 s . using the fitting values of  @xmath168 for * prince * in eq .",
    "( [ eq.tt ] ) we obtain @xmath169 , which is larger than the maximum time @xmath170 used to obtain  @xmath74 .",
    "conversely , for a sequence with the same frequency of ``  prince  '' built as a random sequence on top of * e * we obtain @xmath171 . these calculations",
    "not only explain @xmath165 , they show that * prince * is a particularly meaningful ( not random ) sequence on top of * e * , and that * e * is necessarily composed by other sequences with  @xmath172 that dominate for shorter times .",
    "more generally , the _ observation _ of long - range correlations at low levels is due to widespread correlations on higher levels .",
    "the sharper transition for keywords .",
    "the addition of many sequences with @xmath100 explains the slow increase in @xmath173 for letters because sequences with increasingly larger @xmath75 dominate for increasingly longer times .",
    "the same reasoning explains the positive correlation between @xmath174 and the length of the book ( pearson correlation  @xmath175 , similar results for other letters ) .",
    "the sequence @xmath176 also shows slow transition and small  @xmath74 , consistent with the interpretation that it is connected to many topics on upper levels .",
    "in contrast , the sharp transition for * prince * indicates the existence of fewer independent contributions on higher levels , consistent with the observation of the onset of burstiness @xmath177 .",
    "altogether , this strongly supports our model of hierarchy of levels with keywords ( but not function words ) strongly connected to specific topics which are the actual correlation carriers .",
    "the sharp transition for the keywords appears systematically roughly at the scale of a paragraph ( @xmath178 symbols ) , in agreement with similar observation in refs .",
    "@xcite .",
    "additional insights on long - range correlations are obtained by investigating whether they are robust under different manipulations of the text  @xcite . here",
    "we focus on two non - trivial shuffling methods ( see si - sec .",
    "vii for simpler cases for which our theory leads to analytic results ) .",
    "consider generating new same - length texts by applying to the original texts the following procedures    * keep the position of all blank spaces fixed and place each word - token randomly in a gap of the size of the word .",
    "* recode each word - type by an equal length random sequence of letters and replace consistently all its tokens .",
    "note that m1 preserve structures ( e.g. , words and letter frequencies ) destroyed by m2 . in terms of our hierarchy , m1 destroys the links to levels above word level while m2 shuffles the links from word- to letter - levels . since according to our picture correlations",
    "originate from high level structures , we predict that m1 destroys and m2 preserves long - range correlations . indeed simulations unequivocally show that long - range correlations present in the original texts ( average  @xmath74 of letters in wrnpc @xmath179 and in all books @xmath180 ) are mostly destroyed by m1 ( @xmath181 and @xmath182 ) and preserved by m2 ( @xmath183 and @xmath184 ( see si - tables for all data ) . at this point",
    "it is interesting to draw a connection to the _ principle of the arbitrariness of the sign _ , according to which the association between a given sign ( e.g. , a word ) and the referent ( e.g. , the object in the real world ) is arbitrary  @xcite .",
    "as confirmed by the m2 shuffling , the long - range correlations of literary texts are invariant under this principle because they are connected to the semantic of the text .",
    "our theory is consistent with this principle .",
    "( local derivative of the transport curve in fig .",
    "[ fig.2]bd ) .",
    "results for three sequences in wrnpc are shown ( from top to bottom ) : the noun ``  prince  '' , the most frequent letter `` e '' , and the word ``  so  '' ( same frequency of `` prince ' ' ) .",
    "the horizontal lines indicate the @xmath74 , the error bars , and the fitting range .",
    "inset ( from top to bottom ) : the @xmath150 other nouns appearing as outliers in fig .",
    "[ fig.3 ] , the @xmath150 most frequent letters after `` e '' , and the @xmath150 words matching the frequency of the outlier - nouns . ]",
    "from an information theory viewpoint , long - range correlations in a symbolic sequence have two different and concurrent sources : the broad distribution of the distances between successive occurrences of the same symbol ( burstiness ) and the correlations of these distances .",
    "we found that the contribution of these two sources is very different for observables of a literary text at different linguist levels .",
    "in particular , our theoretical framework provides a robust mechanism explaining our extensive observations that on relevant semantic levels the text is high - dimensional and bursty while on lower levels successive projections destroy burstiness while preserving the long - range correlations of the encoded text via a flow of information from burstiness to correlations .",
    "the mechanism explaining how correlations cascade from high- to low - levels is generic and extends to levels higher than word - level in the hierarchy in fig .",
    "[ fig.1 ] .",
    "the construction of such levels could be based , e.g. , on techniques devised to extract information on a `` concept space '' @xcite .",
    "while long - range correlations have been observed at the concept level  @xcite , further studies are required to connect to observations made at lower levels and to distinguish between the two sources of correlations .",
    "our results showing that correlation is preserved after random additions / subtractions of @xmath0 s help this connection because they show that words can be linked to concepts even if they are not used every single time the concept appears ( a high probability suffices ) .",
    "for instance , in ref .",
    "@xcite a topic can be associated to an axis of the concept space and be linked to the words used to build it . in this case , when the text is referring to a topic there is a higher probability of using the words linked to it and therefore our results show that correlations will flow from the topic to the word level . in further higher levels , it is insightful to consider as a limit picture the renewal case  eq .",
    "( [ eq.renewal ] )  for which long - range correlations originate only due to burstiness . this _",
    "limit case _ is the simplest toy model compatible with our results .",
    "our theory predicts that correlations take form of a bursty sequence of events once we approach the semantically relevant topics of the text .",
    "our observations show that some highly topical words already show long - range correlations mostly due to burstiness , as expected by observing that topical words are connected to less concepts than function words  @xcite .",
    "this renewal limit case is the desired outcome of successful analysis of anomalous diffusion in dynamical systems and has been speculated to appear in various fields  @xcite . using this limit case as a guideline we can think of an algorithm able to automatically detect the relevant structures in the hierarchy by pushing recursively the long - range correlations into a renewal sequence .",
    "next we discuss how our results improve previous analyses and open new possibilities of applications .",
    "previous methods either worked below the letter level  @xcite or combined the correlations of different letters in such a way that asymptotically the most long - range correlated sequence dominates  @xcite . only through our results",
    "it is possible to understand that indeed a single asymptotic exponent @xmath75 should be expected in all these cases .",
    "however , and more importantly , @xmath75 is usually beyond observational range and an interesting range of finite - time  @xmath74 is obtained depending on the observable or encoding . on the letter level , our analysis ( figs .",
    "[ fig.2 ] and  [ fig.3 ] ) revealed that all of them are long - range correlated with no burstiness ( exponentially distributed inter - event times ) .",
    "this lack of burstiness can be wrongly interpreted as an indication that letters  @xcite and most parts of speech  @xcite are well described by a poisson processes .",
    "our results explain that the non - poissonian ( and thus information rich ) character of the text is preserved in the form of long - range correlations  ( @xmath100 ) , which is observed also for all frequent words ( even in the most frequent word ``  the  ' ' ) .",
    "these observations violate not only the strict assumption of a poisson process , they are incompatible with any finite - state markov chain model .",
    "these models are the basis for numerous applications of automatic semantic information extraction , such as keywords extraction , authorship attribution , plagiarism detection , and automatic summarization  @xcite .",
    "all these applications can potentially benefit from our deeper understanding of the mechanisms leading to long - range correlations in texts .    apart from these applications",
    ", more fundamental extensions of our results should : ( i ) consider the mutual information and similar entropy - related quantities , which have been widely used to quantify long - range correlations  @xcite ( see  @xcite for a comparison to correlations ) ; ( ii ) go beyond the simplest case of the two point autocorrelation function and consider multi - point correlations or higher order entropies  @xcite , which are necessary for the complete characterization of the correlations of a sequence ; and ( iii ) consider the effect of non - stationarity on higher levels , which could cascade to lower levels and affect correlations properties .",
    "finally , we believe that our approach may help to understand long - range correlations in any complex system for which an hierarchy of levels can be identified , such as human activities  @xcite and dna sequences  @xcite .",
    "we thank b. lindner for insightful suggestions and s. graffi for the careful reading of the manuscript .",
    "g.c . acknowledges partial support by the firb - project rbfr08uh60 ( miur , italy ) .",
    "m. d. e. acknowledges partial support by the prin project 2008y4w3cy ( miur , italy )",
    ".    99    schenkel a , zhang j , zhang y ( 1993 ) long range correlation in human writings .",
    "_ fractals _ 1:47 - 55 .",
    "alvarez - lacalle e , dorow b , eckmann jp , moses e , ( 2006 ) hierarchical structures induce long - range dynamical correlations in written texts .",
    "_ proc natl acad sci usa _ 103:7956 - 7961 .",
    "voss r , clarke j ( 1975 ) ` 1/f noise ' in music and speech .",
    "_ nature _ 258:317 - 318 .",
    "gilden d , thornton t , mallon m ( 1995 ) 1/f noise in human cognition .",
    "_ science _ 267:1837 - 1839 .",
    "muchnik l , havlin s , bunde a , stanley he ( 2005 ) scaling and memory in volatility return intervals in financial markets .",
    "_ proc natl acad sci usa _ 102:9424 - 9428 .",
    "rybski d , buldyrev sv , havlin s , liljeros f , makse ha ( 2009 ) scaling laws of human interaction activity .",
    "_ proc natl acad sci _ 106:12640 - 12645 .",
    "kello ct , brown gda , ferrer - i - cancho r , holden jg , linkenkaer - hansen k , rhodes t , van orden gc ( 2010 ) scaling laws in cognitive sciences .",
    "_ trends cogn sci _ 14:223 - 232    press wh ( 1978 ) flicker noises in astronomy and elsewhere .",
    "_ comments on astrophysics _",
    "7:103    li w , kaneko k ( 1992 ) long - range correlation and partial @xmath185 spectrum in a noncoding dna sequence .",
    "_ europhys lett _ 17:655 - 660 .    peng ck , buldyrev s , goldberger a , havlin s , sciortino f , simons m , and stanley he ( 1992 ) long - range correlations in nucleotide sequences .",
    "_ nature _ 356 : 168 - 171 .",
    "voss rf ( 1992 ) evolution of long - range fractal correlations and @xmath186 noise in dna base sequences .",
    "_ phys rev lett _ 68:3805 - 3808 .",
    "c.d . manning , h. schtze ( 1999 ) _ foundations of statistical natural language processing _ , ( the mit press , cambridge , massachusetts , usa ) .",
    "stamatatos e ( 2009 ) a survey of modern authorship attribution methods .",
    "_ journal of the american society for information science and technology _ 60:538 - 556 .",
    "oberlander j and brew c ( 2000 ) stochastic text generation .",
    "_ phil trans r soc lond a _ 358:1373 - 1387 .",
    "o usatenko , v yampolskii ( 2003 ) binary n - step markov chains and long - range correlated systems .",
    "_ phys rev lett _ 90:110601 .",
    "amit m , shmerler y , eisenberg e , abraham m , shnerb n ( 1994 ) language and codification dependence of long - range correlations in texts . _ fractals _ 2:7 - 13    ebeling w , neiman a ( 1995 ) long - range correlations between letters and sentences in texts . _",
    "physica a _",
    "215:233 - 241 .",
    "ebeling w , pschel t ( 1994 ) entropy and long - range correlations in literary english .",
    "_ europhys lett _ 26:241 - 246 .",
    "allegrini p , grigolini p , palatella l ( 2004 ) intermittency and scale - free networks : a dynamical model for human language complexity .",
    "_ chaos , solitons and fractals _ 20:95 - 105 .",
    "melnyk ss , usatenko ov , and yampolskii va ( 2005 ) competition between two kinds of correlations in literary texts .",
    "_ phys rev e _ 72:026140 .",
    "herrera jp , pury pa ( 2008 ) statistical keyword detection in literary corpora .",
    "_ eur phys j b _ 63:135 - 146 .",
    "montemurro ma , zanette d ( 2010 ) towards the quantification of the semantic information encoded in written language .",
    "_ adv comp syst _ 13:135 - 153 .",
    "cover tm , thomas ja ( 2006 ) _ elements of information theory _",
    "( wiley series in telecommunications and signal processing )    herzel h , groe i ( 1995 ) measuring correlations in symbol sequences .",
    "_ physica a : statistical mechanics and its applications _ , 216:518 - 542    grassberger p ( 1989 ) estimating the information content of symbol sequences and efficient codes .",
    "_ ieee transactions on information theory _ ,",
    "35:669 - 675 .",
    "kokol p , podgorelec v ( 2000 ) complexity and human writings .",
    "_ complexity _ 7:1 - 6 .",
    "kanter i , kessler da ( 1995 ) markov processes : linguistics and zipf s law .",
    "_ phys rev lett _ 74:4559 - 4562 .",
    "montemurro ma , pury pa ( 2002 ) long - range fractal correlations in literary corpora .",
    "_ fractals _ 10:451 - 461 .",
    "trefn g , floriani e , west bj and grigolini p , ( 1994 ) dynamical approach to anomalous diffusion : response of levy processes to a perturbation .",
    "_ phys rev e _",
    "50:2564 - 2579 .",
    "cox dr , lewis paw ( 1978 ) _ the statistical analysis of series of events _ ( chapman and hall , london ) .",
    "b. lindner ( 2006 ) superposition of many independent spike trains is generally not a poisson process .",
    "_ phys rev e _ 73:022901 .",
    "allegrini p , menicucci d , bedini r , gemignani a , paradisi p ( 2010 ) complex intermittency blurred by noise : theory and application to neural dynamics .",
    "_ phys rev e _ 82:015103 .",
    "goh k - i , barabasi a - l , burstiness and memory in complex systems .",
    "_ europhys lett _ 81 : 48002 .",
    "ortuno m , carpena p , bernaola - galvan p , munoz e , somoza am ( 2002 ) keyword detection in natural languages and dna .",
    "_ europhys lett _ 57:759 - 764 .    altmann eg , pierrehumbert jb , motter ae ( 2009 ) beyond word frequency : bursts , lulls , and scaling in the temporal distributions of words .",
    "_ plos one _ 4:e7678 .",
    "doxas i , dennis s , oliver wl ( 2009 ) the dimensionality of discourse .",
    "_ proc natl acad science usa _ 107:4866 - 4871 .",
    "saussure f de ( 1983 ) course in general linguistics , eds .",
    "charles bally and albert sechehaye .",
    "roy harris .",
    "la salle , illinois )    badalamenti af ( 2001 ) speech parts as poisson processes .",
    "_ journal of psycholinguistic research _ 30:31 .",
    "schmitt ao , ebeling w , herzel h ( 1996 ) the modular structure of informational sequences .",
    "_ biosystems _ 37:199210 .    supporting information",
    "given an ergodic and stationary stochastic process , correlation functions are defined as @xmath187 where @xmath188 denotes an average over different realizations  @xmath19 of the process . stationarity guarantees that @xmath189 depends on the time lag @xmath11 only . in practice",
    ", one typically has no access to different realizations of the process but only to a single finite sequence . in our case , any binary sequence",
    "* x * is obtained from a single text of length @xmath190 through a given mapping . in such cases",
    "it is possible to use the assumption of ergodicity to approximate the correlation function  ( [ eq.corr ] ) by @xmath191 where @xmath12 means averaging , for each fixed @xmath11 , over all pairs @xmath192 and @xmath193 for @xmath194 as @xmath195",
    "consider the sentence `` this paper is a paper of mine '' . by choosing the condition @xmath16 to be _ the @xmath3-th symbol is a vowel _",
    "the projection @xmath196 maps the sentence into the sequence @xmath197 .",
    "if @xmath16 is _ the k - th symbol is equal to ` e ' _ than we get : @xmath198 .",
    "generally , we can treat any n - gram of letters in the same way , as for example by choosing the condition @xmath16 to be _",
    "the @xmath199-gram starting at the k - th symbol is equal to ` er ' _ , that projects using a sliding window the sentence to:@xmath200 .",
    "words are encoded using their corresponding n - gram , for example @xmath16 could be _ the 7-gram starting at the k - th symbol is equal to ` paper ' ( blank spaces included ) _ that gives : @xmath201 .",
    "it is possible to generalize these procedures to more _ semantic conditions _ @xmath16 that associate @xmath0 to either all or part of the symbols that appears in a sentence that is attached to a specified topic .",
    "these topics can be quantitatively constructed from the frequency of words using methods such as latent semantic analysis  @xcite or the procedures to determine the so - called _ concept space _  @xcite .",
    "we describe two simple procedures to construct two binary sequences * x * and * z * such that * x * is _ on top of _ * z*. these procedures will be based either on the  addition \" of @xmath0 s to * x * or on the  subtraction \" of @xmath0 s of * z*. in the simplest cases of _ random _ addition and subtraction , we explicitly compute how long - range correlations flow from * x * to * z * ( corresponding to a flow from upper to lower levels of the hierarchy ) and how burstiness is preserved when extracting * x * from z ( moving from lower to upper levels in the hierarchy ) .",
    "recall that a sequence * x * is _ on top of _ * z * if for all @xmath39 such that @xmath202 we have @xmath203 , for a fixed constant @xmath204 . without loss of generality in the following calculations we fix for simplicity @xmath205 .",
    "we now define simple operations that map two binary sequences into a third binary sequence :    * given two generic binary sequences * z * and * @xmath97 * we define their multiplication * y*=*@xmath97 * * z * as @xmath206 . by construction",
    "* y * is on top of * z*. * given two non - overlapping sequences * x * and * y * we define their sum * z * = * x*+*y * as @xmath207 . by construction",
    "* x * and * y * are on top of * z*. we say that sequences @xmath14 and @xmath44 are non - overlapping if for all @xmath47 for which @xmath80 we have @xmath81 .    in general , two independent binary sequences * x * and",
    "* @xmath97 * will overlap . a sequence @xmath44 which is non",
    "- overlapping with @xmath14 can be constructed from * @xmath97 * as * y*= * @xmath97*(*1 * -*x * ) , where * 1 * denotes the trivial sequence with all @xmath0 s . in this case",
    ", we say that * z * = * x*+*y * , with * y * = * @xmath97*(*1 * -*x * ) is a sequence lower than * x * in the hierarchy that is constructed by a _ random addition _",
    "( of 1 s ) to * x*. similarly , if * @xmath208 * is independent of * z * , the sequence * @xmath208**z * is a _ random subtraction _ ( of 1 s ) of * z *      consider a sequence * z * constructed as a random addition of @xmath0 s to a given long - range correlated sequence @xmath19 : @xmath209 , with * y * = * @xmath97*(*1 * -*x * ) and @xmath97 a sequence of _ i.i.d . _",
    "binary random variables .",
    "the associated random walker @xmath48 spreads anomalously with the same exponent of @xmath27 .",
    "this asymptotic regime is masked at short times by a pre - asymptotic normal behavior .",
    "here we first compute explicitly the spreading of @xmath48 in terms of that of @xmath27 and @xmath210 and then we compute a bound for the transition time @xmath167 to the asymptotic anomalous diffusion of @xmath48 .    as written in eq .",
    "( 5 ) of the main text we have @xmath211 for our particular case we obtain @xmath212 and    @xmath213    from eqs .",
    "( [ eq.y ] ) and  ( [ squared - app ] )  and noting that @xmath214 and @xmath215  we obtain @xmath216 the correlation term in eq .",
    "( [ eq.sum ] ) can also be obtained through direct calculations : @xmath217\\\\ & = & -\\left < \\xi \\right > \\sigma^2_x(t).\\label{eq.correl}\\end{aligned}\\ ] ]    finally , inserting eqs .  ( [ eq.sigma2 ] ) and ( [ eq.correl ] ) into eq .  ( [ eq.sum ] )",
    "we have @xmath218 as @xmath27 superdiffuses so it will @xmath48 and they both have the same asymptotic behavior . on the other hand the asymptotic regime",
    "is masked at short times by a pre - asymptotic normal behavior , given by the linear term in @xmath11 .",
    "we stress that , even if the non - overlapping condition for * y * forces both @xmath219 and @xmath220 to have the same asymptotic behavior of @xmath109 , their cumulative contributions does not cancel out unless we trivially have @xmath221 .",
    "we now give a bound on the transition time  @xmath167 to the asymptotic anomalous diffusion of eq .",
    "( [ eq.sumsimple ] ) . without loss of generality",
    "consider the case in which even the asymptotic anomalous behavior of @xmath27 is masked by generic pre -",
    "asymptotic @xmath222 such that @xmath223\\ ] ] with @xmath224 and @xmath222 increasing and such that @xmath225 for @xmath226 ( to guarantee that the asymptotic behavior is dominated by @xmath227 ) and @xmath228 ( as @xmath229 ) .",
    "the asymptotic behavior @xmath230 in eq.([eq.sumsimple ] ) dominates only after a time @xmath167 such that : @xmath231 using the fact that the term @xmath232 is positive and that @xmath233 is monotonically increasing we finally have @xmath234 which corresponds to eq .",
    "( 7 ) of the main text . in practice ,",
    "any finite - time estimate @xmath235 is close to the asymptotic @xmath236 only if the estimate is performed for @xmath237 , otherwise @xmath238 ( @xmath239 if @xmath240 ) .",
    "as noted in the main text , if * z * = * x*+*y * then @xmath241 = @xmath242 + * y*. applying to this relation the same arguments above , similar pre - asymptotic normal diffusion and transition time appear in the case of random subtraction , moving up in the hierarchy .",
    "more specifically , starting from a sequence * z * such that asymptotically @xmath243 and constructing  @xmath244 , with * @xmath208 * independent of * z * , we obtain a transition time @xmath167 for @xmath19 given by : @xmath245 which corresponds to eq .",
    "( [ eq.tt ] ) above after properly replacing @xmath246 , @xmath247 and @xmath248 .",
    "we consider the case of sequences as in eq .",
    "( 8) of the main text : @xmath38 is a sequence emerging from a renewal process with algebraically decaying inter - event times , i.e. @xmath249 and @xmath250 .",
    "given now a fixed @xmath251 , we consider the random subtraction @xmath98 where each @xmath252 is eventually set to @xmath253 with probability @xmath254 .",
    "it is easy to see that the inter - event times of the new process will be distributed as : @xmath255 asymptotically @xmath256 is dominated by the long tails of @xmath257 : given a large @xmath96 , fix @xmath258 eventually diverging with @xmath259 and split accordingly the sum over @xmath3 in the second term of the right hand side .",
    "the term corresponding to the sum @xmath260 is exponentially dominated by @xmath261 and arbitrary small , while the remaining finite sum over @xmath262 is controlled again by the tail of @xmath34 .",
    "in our investigations we considered the english version of the @xmath143 popular novels listed in si - tab . books .",
    "the texts were obtained through the gutenberg project ( http://www.gutenberg.org ) .",
    "we implement a very mild pre - processing of the text that reduces the number of different symbols and simplifies our analysis : we consider as valid symbols the letters `` a - z '' , numbers `` 0 - 9 '' , the apostrophe   ``  '' and the blank space ``  '' .",
    "capitalization , punctuations and other markers were removed .",
    "a string of symbols between two consecutive blank spaces is considered to be a word .",
    "no lemmatization was applied to them so that plurals and singular forms are considered to be different words .",
    "as described in the main text , the distinction between long - range and short - range correlation requires a finite - time estimate  @xmath74 of the asymptotic diffusion exponent  @xmath75 of the random - walkers associated to a binary sequence . in practice",
    ", this corresponds to estimate the tails of the @xmath263 relation and it is therefore essential to estimate the upper limit in  @xmath11 , denoted as  @xmath170 , for which we have enough accuracy to provide a reasonable estimate  @xmath74 .",
    "we adopt the following procedure to estimate  @xmath170 .",
    "we consider a surrogate binary sequence with the same length  @xmath190 and fraction of symbols ( @xmath0 s ) , but with the symbols randomly placed in the sequence .",
    "for this sequence we know that  @xmath264 .",
    "we then consider instants of time  @xmath265 equally spaced in a logarithmic scale of  @xmath11 ( in practice we consider @xmath266 , with @xmath47 integer and @xmath267 ) .",
    "we then estimate the local exponent as @xmath268/\\log_{10}(1.2)$ ] . for small  @xmath11 , @xmath269 but for larger  @xmath11 statistical fluctuations",
    "arise due to the finiteness of  @xmath190 , as illustrated in fig .",
    "[ fig.a1](a ) .",
    "we choose  @xmath170 as the smallest  @xmath265 for which @xmath270 are all outside @xmath271 $ ] ( see fig .  [ fig.a1]a ) .",
    "we recall that our primary interest in the distinction between @xmath264 and  @xmath272 .",
    "the procedure described above is particularly suited for this distinction and an exponent  @xmath142 obtained for large  @xmath273 can be confidently regarded as a signature of super diffusion ( long - range correlation ) . in fig .",
    "[ fig.a1 ] we verify that  @xmath170 show no strong dependence on the fraction of @xmath0 s in the binary sequence ( inset ) and that it scales linearly with  @xmath190 . based on these results , a good estimate of  @xmath170 is  @xmath274 , i.e. the safe interval for determining long - range correlation ends two decades before the size of the text .",
    "this phenomenological rule was adopted in the estimate of  @xmath74 for all cases .",
    "the  @xmath170 is only the upper limit and the estimate  @xmath74 is performed through a least - squared fit in the time interval @xmath275 , where @xmath276 . in practice ,",
    "we select @xmath143 different values of @xmath265 around @xmath277 and report the mean and variance over the different fittings as  @xmath74 and its uncertainty , respectively .",
    "we start clarifying the validity of the inequality @xmath278 where @xmath74 is the finite - time estimate of the total long - range correlation  @xmath75 of a binary sequence * x * and @xmath279 is the estimate for the correlation due to the burstiness ( which can be quantified by shuffling * x * using the procedure  @xmath138 of the main text ) . equation ( 4 ) of the main text shows that both burstiness @xmath280 and long - range correlations in the sequence of @xmath30 s contribute to the long - range correlations of a binary sequence @xmath19 .",
    "while the @xmath139 contribution is always positive , the contribution from the correlation in @xmath30 s can be positive or negative . in principle , a negative contribution could precisely cancel the contribution of @xmath139 and violates the inequality  ( [ eq.inequality ] ) .",
    "conversely , this inequality is guaranteed to hold if the asymptotic contribution of the correlation in @xmath30 s of @xmath19 to @xmath281 is positive .",
    "we now show that this is the case for the sequences we have argued to provide a good account of our observations .",
    "consider high in the hierarchy a renewal sequence * x * with a given @xmath282 and broad tail in @xmath34 ( diverging @xmath139 ) .",
    "adding many independent non - overlapping sequences , we construct a lower level sequence that still has long range correlation , with the same exponent @xmath75 ( see sec .  [ sec.operations ] above ) . for this sequence we know that the broad tail in @xmath34 has a cutoff @xmath157 and thus burstiness gives no contribution to @xmath75 . instead , @xmath100 results solely from the correlations in the @xmath96 s , which are therefore necessarily positive .",
    "it is natural to expect that this positiveness of the asymptotic correlation extends to finite times , in which case the ( finite time ) inequality  ( [ eq.inequality ] ) holds .",
    "indeed , for small @xmath283 , the distribution @xmath34 is not strongly affected by the independent additions and thus for @xmath108 a finite time estimate @xmath74 will receive contributions from both burstiness and @xmath284 correlations .",
    "finally , we have directly tested the validity of eq .",
    "( [ eq.inequality ] ) by comparing  @xmath74 of different sequences * x * to the @xmath154 obtained from the corresponding @xmath285 ( a2-shuffled sequences of * x * , see main text ) . the inequality  ( [ eq.inequality ] ) was confirmed for every single sequence we have analyzed , as shown by the fact that @xmath154 ( red symbols ) in fig .",
    "[ fig.3si ] are systematically below their corresponding @xmath235 ( black circles ) .",
    "we now obtain a quantitative lower bound for  @xmath74 using eq .",
    "( [ eq.inequality ] ) .",
    "we consider a renewal sequence ( in which case @xmath286 ) with an inter - event time distribution given by @xmath287 where @xmath157 is the cut - off time , @xmath279 is the anomalous diffusion exponent for a renewal sequence with no cutoff @xmath288 , @xmath289 is a lower cut - off ( we fixed it at @xmath290 ) , and @xmath291 is a normalization constant .",
    "we obtain the lower bound for  @xmath74 as a function of @xmath139 by considering how @xmath154 and @xmath139 change with @xmath157 in the model above . for short times ( @xmath292 )",
    "the corresponding walkers have not seen the cutoff and their diffusion will be anomalous with exponent @xmath293 . at longer time",
    "( @xmath294  @xcite ) the diffusion becomes normal @xmath162 .",
    "correspondingly , if the fitting interval @xmath295 $ ] used to compute the finite time @xmath154 ( see sec .  [ ssec.confidence ] ) is all below @xmath157 ( i.e. @xmath296 ) we have @xmath293 while if the fitting interval is all beyond the cutoff ( i.e. @xmath297 ) we have @xmath162 . when @xmath157 is inside the fitting interval we approximate @xmath154 by linearly interpolating between @xmath279 and @xmath0",
    "finally , we can compute @xmath139 by directly calculating the first and second moments of the distribution  ( [ cutoffpt ] ) .",
    "particularly important are the values @xmath298 and @xmath299 obtained evaluating @xmath139 at the critical values of the cutoff @xmath300 and @xmath301 , respectively . using the fact that @xmath139 is a monotonic increasing function of @xmath157 we can obtain explicitely the @xmath74 dependency on @xmath139 .",
    "the @xmath154 for the case of a binary sequence with distribution  ( [ cutoffpt ] ) is given by @xmath302,\\\\ \\hat{\\gamma}_{a2}&=\\gamma_{a2 } \\qquad \\qquad \\qquad    \\qquad   \\qquad & \\textrm{if } \\qquad \\sm > s_2.\\end{aligned}\\ ] ] the red dashed line in fig .",
    "[ fig.3si ] ( fig . 3 of the main text ) was computed using the fitting range corresponding to the book wrnpc  @xmath303 ( see sec .  [ ssec.confidence ] ) , and @xmath304 ( compatible with @xmath74 observed for words with large @xmath139 ) .",
    "in addition to the shuffling methods presented in the main text , we discuss here briefly two cases :    * * shuffle words * + mixing words order kills correlations for scales larger than the maximum word length  @xcite .",
    "even the blank space sequence  @xmath305 becomes uncorrelated because its original correlations originate ( as in the case of all letters ) from the correlation in  @xmath30 and not from tails in  @xmath34 .",
    "* * keep all blank spaces * in their original positions and fill the empty space between them with : * * two letters @xmath306 , placed randomly with probabilities @xmath307 and @xmath308 . * * the same letters of the book , placed in random positions .",
    "+ by construction , correlation for blank space is trivially preserved .",
    "what do we expect for the other letters ?",
    "the following simple reasoning indicates that long - range correlation should be expected asymptotically in both cases : any letter sequence @xmath19 is on top of the reverted blank space sequence @xmath309 ; the results in sec .",
    "[ sec.operations ] above show that either the selected sequence @xmath19 or its complement  @xmath66 ( such that @xmath310 ) has  @xmath311 ; and eq .",
    "( [ eq.sumsimple ] ) above shows that any randomly chosen @xmath19 on top of @xmath309 has  @xmath311 . in practice",
    "these exponents are relevant only if the subsequence is dense enough in order for @xmath167 in eq .",
    "( [ eq.ttsub ] ) above to be inside the observation range . for the first shuffling method and for our longest book ( wrnpc )",
    ", we obtain that only if @xmath312 one finds @xmath313 book size . since the most frequent letter in a book has much smaller frequency ( around @xmath314 ) , we conclude that in practice all sequences obtained using the second shuffling mehthod have @xmath315 for all books of size smaller than  @xmath316 symbols ( @xmath317 pages ) .",
    "+ these simple calculations show that @xmath318 does not explain the correlations observed in the letters of the original text , as has been speculated in ref .",
    "their origin are the long - range correlations on higher levels .",
    "& & & + sequence & @xmath319 & @xmath144 & error & @xmath74 & error & @xmath74 & error & @xmath74 & error + vowels & 41414 & 0.440 & 0.020 & 1.18 & 0.05 & & & & + _ & 26666 & 0.379 & 0.011 & 1.13 & 0.06 & 1.13 & 0.06 & 1.13 & 0.06 + e & 13545 & 0.812 & 0.003 & 1.20 & 0.04 & 1.11 & 0.04 & 1.01 & 0.04 + t & 10667 & 0.858 & 0.003 & 1.17 & 0.05 & 1.05 & 0.03 & 1.05 & 0.03 + a & 8772 & 0.838 & 0.003 & 1.14 & 0.05 & 1.07 & 0.03 & 0.98 & 0.04 + o & 8128 & 0.920 & 0.002 & 1.25 & 0.05 & 1.13 & 0.04 & 0.99 & 0.04 + i & 7500 & 0.887 & 0.002 & 1.20 & 0.04 & 1.10 & 0.03 & 1.03 & 0.03 + h & 7379 & 0.848 & 0.003 & 1.15 & 0.04 & 1.11 & 0.04 & 1.04 & 0.03 + n & 7001 & 0.895 & 0.002 & 1.09 & 0.03 & 1.13 & 0.04 & 1.02 & 0.03 + s & 6497 & 0.925 & 0.002 & 1.11 & 0.04 & 1.09 & 0.03 & 1.07 & 0.03 + r & 5418 & 0.905 & 0.002 & 1.15 & 0.04 & 1.15 & 0.04 & 1.04 & 0.03 + d & 4928 & 0.878 & 0.003 & 1.06 & 0.03 & 1.10 & 0.04 & 0.97 & 0.04",
    "+ l & 4704 & 1.081 & 0.003 & 1.20 & 0.06 & 1.12 & 0.04 & 1.00 & 0.03 + u & 3469 & 0.901 & 0.004 & 1.15 & 0.04 & 1.15 & 0.04 & 1.07 & 0.03 + w & 2681 & 0.966 & 0.003 & 1.11 & 0.04 & 1.23 & 0.05 & 0.99 & 0.04 + g & 2529 & 0.986 & 0.003 & 1.13 & 0.04 & 1.16 & 0.05 & 0.97 & 0.05 + c & 2397 & 0.980 & 0.005 & 1.15 & 0.05 & 1.11 & 0.04 & 1.00 & 0.03 + y & 2259 & 1.070 & 0.004 & 1.23 & 0.04 & 1.05 & 0.03 & 1.00 & 0.04 + m & 2103 & 1.030 & 0.005 & 1.16 & 0.04 & 1.24 & 0.05 & 0.98 & 0.03 + f & 1988 & 1.089 & 0.006 & 1.17 & 0.05 & 1.14 & 0.04 & 1.02 & 0.04 + p & 1514 & 1.143 & 0.006 & 1.17 & 0.04 & 1.13 & 0.04 & 1.05 & 0.03 + _ the _ & 1635 & 0.971 & 0.005 & 1.29 & 0.07 & & & & + _ and _ & 868 & 0.973 & 0.008 & 1.08 & 0.04 & & & & + _ to _ & 734 & 1.013 & 0.006 & 1.08 & 0.04 & & & & + _ a _ & 624 & 1.057 & 0.007 & 1.08 & 0.03 & & & & + _ she _ & 542 & 1.548 & 0.012 & 1.34 & 0.06 & & & & + _ it _ & 530 & 1.172 & 0.008 & 1.22 & 0.04 & & & & + _ alice _ & 386 & 0.885 & 0.008 & 1.01 & 0.05 & & & & + _ in _ & 367 & 0.959 & 0.008 & 1.04 & 0.03 & & & & + _ way _ & 57 & 1.098 & 0.021 & 1.21 & 0.04 & & & & + _ turtle _ & 57 & 4.066 & 0.039 & 1.47 & 0.12 & & & & + _ hatter _ & 55 & 4.978 & 0.050 & 1.46 & 0.12 & & & & + _ gryphon _ & 55 & 3.541 & 0.038 & 1.43 & 0.10 & & & & + _ quite _ & 55 & 1.290 & 0.025 & 1.16 & 0.04 & & & & + _ mock _ & 55 & 3.919 & 0.045 & 1.45 & 0.12 & & & & + _ are _ & 54 & 1.250 & 0.024 & 1.10 & 0.03 & & & & + _ think _ & 52 & 1.268 & 0.039 & 1.09 & 0.04 & & & & + _ more _ & 49 & 1.040 & 0.023 & 1.11 & 0.04 & & & & + _ head _ & 49 & 1.230 & 0.024 & 1.07 & 0.03 & & & & + _ never _ & 48 & 1.083 & 0.049 & 1.09 & 0.05 & & & & + _ voice _ & 47 & 1.359 & 0.054 & 1.07 & 0.03 & & & & +     & & & + sequence & @xmath319 & @xmath144 & error & @xmath74 & error & @xmath74 & error & @xmath74 & error + vowels & 358397 & 0.454 & 0.020 & 1.25 & 0.04 & & & & + _ & 208375 & 0.434 & 0.009 & 1.34 & 0.04 & 1.34 & 0.04 & 1.34 & 0.04 + e & 123056 & 0.817 & 0.002 & 1.18 & 0.04 & 1.22 & 0.05 & 0.96 & 0.05 + t & 86576 & 0.836 & 0.002 & 1.18 & 0.04 & 1.15 & 0.04 & 0.98 & 0.04 + a & 78914 & 0.847 & 0.002 & 1.11 & 0.04 & 1.11 & 0.04 & 1.03 & 0.03 + o & 67896 & 0.885 & 0.001 & 1.20 & 0.04 & 1.24 & 0.04 & 0.96 & 0.05",
    "+ n & 64597 & 0.865 & 0.002 & 1.13 & 0.04 & 1.18 & 0.04 & 1.02 & 0.04 + i & 63755 & 0.889 & 0.001 & 1.22 & 0.04 & 1.15 & 0.04 & 1.07 & 0.03 + s & 62383 & 0.909 & 0.001 & 1.21 & 0.04 & 1.20 & 0.04 & 1.05 & 0.03 + r & 59027 & 0.870 & 0.001 & 1.17 & 0.04 & 1.09 & 0.04 & 1.05 & 0.03 + h & 54880 & 0.861 & 0.002 & 1.23 & 0.04 & 1.07 & 0.05 & 1.09 & 0.03",
    "+ l & 38467 & 1.033 & 0.001 & 1.28 & 0.05 & 1.10 & 0.04 & 1.03 & 0.03 + d & 37051 & 0.923 & 0.001 & 1.24 & 0.04 & 1.19 & 0.04 & 1.01 & 0.03 + c & 27687 & 0.978 & 0.001 & 1.28 & 0.04 & 1.17 & 0.04 & 1.11 & 0.03 + u & 24776 & 0.957 & 0.001 & 1.15 & 0.04 & 1.12 & 0.04 & 1.04 & 0.03 + f & 24052 & 0.955 & 0.001 & 1.18 & 0.04 & 1.12 & 0.04 & 1.08 & 0.03 + m & 21509 & 0.987 & 0.001 & 1.20 & 0.04 & 1.20 & 0.04 & 1.03 & 0.04 + w & 19172 & 1.010 & 0.001 & 1.36 & 0.05 & 1.21 & 0.04 & 1.07 & 0.03 + g & 18284 & 0.962 & 0.001 & 1.20 & 0.04 & 1.09 & 0.05 & 0.99 & 0.03 + p & 16742 & 1.040 & 0.001 & 1.24 & 0.04 & 1.10 & 0.03 & 1.04 & 0.03 + y & 15700 & 0.993 & 0.002 & 1.26 & 0.04 & 1.15 & 0.04 & 1.03 & 0.04 + _ the _ & 16882 & 0.924 & 0.002 & 1.21 & 0.04 & & & & + _ of _ & 9414 & 0.970 & 0.002 & 1.25 & 0.04 & & & & + _ and _ & 5765 & 0.897 & 0.003 & 1.10 & 0.04 & & & & + _ a _ & 5326 & 1.097 & 0.003 & 1.19 & 0.04 & & & & + _ in _ & 4287 & 1.022 & 0.003 & 1.14 & 0.04 & & & & + _ to _ & 4080 & 1.051 & 0.003 & 1.20 & 0.04 & & & & + _ water _ & 417 & 1.509 & 0.011 & 1.26 & 0.04 & & & & + _",
    "little _ & 412 & 1.117 & 0.011 & 1.08 & 0.03 & & & & + _ where _ & 349 & 1.086 & 0.011 & 1.06 & 0.03 & & & & + _ sea _ & 348 & 1.534 & 0.015 & 1.29 & 0.04 & & & & + _ much _ & 338 & 1.112 & 0.010 & 1.08 & 0.04 & & & & + _ country _ & 337 & 1.519 & 0.011 & 1.28 & 0.05 & & & & + _ land _ & 318 & 1.387 & 0.012 & 1.25 & 0.04 & & & & + _ must _ & 317 & 1.290 & 0.009 & 1.12 & 0.04 & & & & + _ feet _ & 312 & 1.391 & 0.013 & 1.23 & 0.04 & & & & + _ may _ & 311 & 1.118 & 0.010 & 1.09 & 0.03 & & & & + _ species _ & 303 & 2.459 & 0.022 & 1.55 & 0.05 & & & & + _ found _ & 303 & 1.217 & 0.010 & 1.16 & 0.04 & & & & + _ me _ & 301 & 1.206 & 0.012 & 1.07 & 0.04 & & & & + _ day _ & 301 & 1.375 & 0.007 & 1.12 & 0.03 & & & & +     & & & + sequence & @xmath319 & @xmath144 & error & @xmath74 & error & @xmath74 & error & @xmath74 & error + vowels & 237050 & 0.420 & 0.020 & 1.26 & 0.05 & & & & + _ & 151300 & 0.404 & 0.010 & 1.53 & 0.05 & 1.53 & 0.05 & 1.53 & 0.05 + e & 78161 & 0.843 & 0.002 & 1.16 & 0.04 & 1.09 & 0.04 & 1.04 & 0.04 + t & 58475 & 0.873 & 0.002 & 1.32 & 0.05 & 1.19 & 0.04 & 1.04 & 0.03 + a & 53663 & 0.854 & 0.002 & 1.21 & 0.04 & 1.17 & 0.04 & 1.08 & 0.03 + o & 47726 & 0.896 & 0.001 & 1.18 & 0.04 & 1.24 & 0.04 & 0.97 & 0.04 +",
    "n & 44497 & 0.874 & 0.002 & 1.14 & 0.04 & 1.18 & 0.04 & 1.12 & 0.03 + h & 44473 & 0.832 & 0.002 & 1.37 & 0.05 & 1.27 & 0.04 & 1.17 & 0.05 + i & 40025 & 0.906 & 0.001 & 1.34 & 0.05 & 1.23 & 0.05 & 1.13 & 0.04 + s & 37500 & 0.941 & 0.001 & 1.32 & 0.05 & 1.37 & 0.04 & 1.07 & 0.03 + r & 34514 & 0.888 & 0.002 & 1.19 & 0.04 & 1.19 & 0.04 & 1.09 & 0.04 + d & 30491 & 0.929 & 0.001 & 1.31 & 0.06 & 1.22 & 0.04 & 1.00 & 0.04 + l & 24876 & 1.059 & 0.001 & 1.20 & 0.04 & 1.10 & 0.04 & 1.07 & 0.03 + u & 17475 & 0.943 & 0.001 & 1.16 & 0.04 & 1.22 & 0.04 & 0.99 & 0.05 + w & 17213 & 0.948 & 0.002 & 1.36 & 0.06 & 1.30 & 0.05 & 1.06 & 0.03 + m & 14754 & 0.977 & 0.001 & 1.18 & 0.04 & 1.24 & 0.04 & 1.04 & 0.03 + c & 14148 & 1.006 & 0.001 & 1.37 & 0.05 & 1.18 & 0.04 & 1.10 & 0.04 + g & 14069 & 0.994 & 0.002 & 1.28 & 0.06 & 1.27 & 0.04 & 1.06 & 0.03 + f & 13862 & 1.016 & 0.002 & 1.25 & 0.04 & 1.21 & 0.04 & 1.09 & 0.04 + y & 10868 & 1.068 & 0.002 & 1.29 & 0.05 & 1.16 & 0.04 & 0.95 & 0.05 + p & 9940 & 1.074 & 0.002 & 1.24 & 0.05 & 1.24 & 0.04 & 1.06 & 0.04 + _ the _ & 8930 & 1.018 & 0.003 & 1.34 & 0.04 & & & & + _ and _ & 7280 & 0.958 & 0.002 & 1.27 & 0.04 & & & & + _ of _ & 4365 & 1.113 & 0.003 & 1.42 & 0.07 & & & & + _ to _ & 4190 & 1.077 & 0.003 & 1.20 & 0.04 & & & & + _ a _ & 4158 & 1.152 & 0.004 & 1.22 & 0.04 & & & & + _ he _ & 3311 & 2.158 & 0.011 & 1.60 & 0.05 & & & & + _ him _",
    "& 1184 & 2.009 & 0.013 & 1.42 & 0.05 & & & & + _ jurgis _ & 1098 & 2.077 & 0.010 & 1.48 & 0.07 & & & & + _ i _ & 485 & 6.141 & 0.275 & 1.54 & 0.06 & & & & + _ man _ & 463 & 1.301 & 0.013 & 1.27 & 0.04 & & & & + _ said _ & 367 & 1.975 & 0.019 & 1.38 & 0.04 & & & & + _ time _ & 356 & 1.209 & 0.013 & 1.15 & 0.04 & & & & + _ men _ & 329 & 1.768 & 0.011 & 1.33 & 0.05 & & & & + _ now _ & 325 & 1.077 & 0.009 & 1.11 & 0.03 & & & & + _ day _ & 280 & 1.378 & 0.021 & 1.15 & 0.04 & & & & + _ other _ & 279 & 1.244 & 0.014 & 1.16 & 0.04 & & & & + _ place _ & 263 & 1.227 & 0.013 & 1.17 & 0.04 & & & & + _ only _ & 261 & 1.042 & 0.010 & 1.03 & 0.04 & & & & + _ before _ & 235 & 1.117 & 0.010 & 1.09 & 0.03 & & & & + _ home _ & 229 & 1.759 & 0.012 & 1.23 & 0.04 & & & & +     & & & + sequence & @xmath319 & @xmath144 & error & @xmath74 & error & @xmath74 & error & @xmath74 & error + vowels & 235370 & 0.445 & 0.020 & 1.48 & 0.05 & & & & + _ & 146786 & 0.429 & 0.009 & 1.65 & 0.05 & 1.65 & 0.05 & 1.65 & 0.05 + e & 76483 & 0.850 & 0.002 & 1.40 & 0.05 & 1.11 & 0.04 & 1.10 & 0.03 + t & 59660 & 0.858 & 0.002 & 1.24 & 0.04 & 1.18 & 0.04 & 1.02 & 0.04 + a & 51642 & 0.859 & 0.002 & 1.23 & 0.04 & 1.18 & 0.04 & 1.11 & 0.04 + o & 47123 & 0.890 & 0.002 & 1.23 & 0.04 & 1.22 & 0.04 & 1.05 & 0.04 + n & 44064 & 0.869 & 0.002 & 1.24 & 0.04 & 1.24 & 0.04 & 1.08 & 0.03 + i & 42750 & 0.920 & 0.001 & 1.30 & 0.04 & 1.19 & 0.04 & 1.11 & 0.03 + s & 38995 & 0.940 & 0.001 & 1.34 & 0.06 & 1.23 & 0.04 & 1.20 & 0.04 + h & 36904 & 0.859 & 0.002 & 1.40 & 0.05 & 1.13 & 0.04 & 1.20 & 0.04 + r & 35465 & 0.912 & 0.001 & 1.34 & 0.05 & 1.19 & 0.04 & 1.16 & 0.04 + d & 27682 & 0.974 & 0.001 & 1.40 & 0.06 & 1.24 & 0.04 & 1.05 & 0.03",
    "+ l & 24910 & 1.055 & 0.001 & 1.20 & 0.04 & 1.12 & 0.04 & 1.06 & 0.03 + u & 17372 & 0.947 & 0.002 & 1.20 & 0.04 & 1.17 & 0.04 & 1.07 & 0.03 + w & 15554 & 0.996 & 0.002 & 1.30 & 0.04 & 1.25 & 0.04 & 1.10 & 0.03 + m & 14940 & 1.006 & 0.002 & 1.29 & 0.04 & 1.27 & 0.04 & 1.09 & 0.04 + c & 14884 & 1.042 & 0.001 & 1.35 & 0.05 & 1.21 & 0.04 & 1.21 & 0.06 + f & 14234 & 1.006 & 0.001 & 1.24 & 0.05 & 1.14 & 0.04 & 1.04 & 0.03 + g & 12890 & 1.044 & 0.001 & 1.26 & 0.04 & 1.17 & 0.04 & 1.09 & 0.03 + y & 11994 & 1.022 & 0.002 & 1.34 & 0.04 & 1.14 & 0.04 & 1.02 & 0.03 + p & 11087 & 1.093 & 0.002 & 1.30 & 0.05 & 1.17 & 0.04 & 1.16 & 0.05 + _ the _ & 9091 & 1.043 & 0.003 & 1.38 & 0.04 & & & & + _ and _ & 5898 & 0.995 & 0.003 & 1.34 & 0.05 & & & & + _ of _ & 4380 & 1.033 & 0.003 & 1.32 & 0.05 & & & & + _ a _ & 4057 & 1.098 & 0.003 & 1.22 & 0.04 & & & & + _ to _ & 3545 & 1.095 & 0.004 & 1.24 & 0.04 & & & & + _ in _ & 2555 & 1.031 & 0.004 & 1.14 & 0.04 & & & & + _ would _",
    "& 480 & 1.552 & 0.012 & 1.26 & 0.04 & & & & + _ river _ & 478 & 2.176 & 0.014 & 1.43 & 0.06 & & & & + _ water _ & 242 & 1.899 & 0.015 & 1.38 & 0.05 & & & & + _ she _ & 239 & 2.055 & 0.022 & 1.44 & 0.06 & & & & + _ boat _ & 212 & 1.921 & 0.028 & 1.32 & 0.05 & & & & + _ here _ & 210 & 1.508 & 0.015 & 1.24 & 0.04 & & & & + _ night _ & 177 & 1.609 & 0.012 & 1.30 & 0.05 & & & & + _ can _ & 177 & 1.392 & 0.015 & 1.13 & 0.04 & & & & + _ go _ & 176 & 1.275 & 0.010 & 1.16 & 0.04 & & & & + _ head _ & 175 & 1.612 & 0.017 & 1.41 & 0.06 & & & & + _ pilot _ & 172 & 2.652 & 0.047 & 1.40 & 0.05 & & & & + _ long _ & 172 & 1.246 & 0.013 & 1.06 & 0.03 & & & & + _ first _ & 164 & 1.132 & 0.018 & 1.11 & 0.04 & & & & + _ miles _ & 162 & 1.816 & 0.030 & 1.49 & 0.05 & & & & +     & & & + sequence & @xmath319 & @xmath144 & error & @xmath74 & error & @xmath74 & error & @xmath74 & error + vowels & 356037 & 0.441 & 0.020 & 1.45 & 0.05 & & & & + _ & 215939 & 0.424 & 0.009 & 1.54 & 0.05 & 1.54 & 0.05 & 1.54 & 0.05 + e & 116938 & 0.859 & 0.002 & 1.29 & 0.04 & 1.12 & 0.04 & 1.05 & 0.03 + t & 87882 & 0.860 & 0.002 & 1.23 & 0.04 & 1.25 & 0.04 & 1.00 & 0.03 + a & 77820 & 0.851 & 0.002 & 1.24 & 0.04 & 1.22 & 0.05 & 1.05 & 0.03 + o & 69258 & 0.900 & 0.001 & 1.27 & 0.04 & 1.16 & 0.04 & 1.09 & 0.03 +",
    "n & 65552 & 0.886 & 0.001 & 1.20 & 0.04 & 1.20 & 0.04 & 1.07 & 0.03 + i & 65349 & 0.905 & 0.001 & 1.28 & 0.04 & 1.11 & 0.04 & 1.09 & 0.03 + s & 64148 & 0.917 & 0.001 & 1.34 & 0.05 & 1.31 & 0.04 & 1.15 & 0.04 + h & 62824 & 0.856 & 0.002 & 1.32 & 0.04 & 1.38 & 0.06 & 1.21 & 0.04 + r & 52073 & 0.900 & 0.002 & 1.32 & 0.04 & 1.19 & 0.04 & 1.14 & 0.04 + l & 42733 & 1.051 & 0.001 & 1.22 & 0.04 & 1.20 & 0.04 & 0.99 & 0.03 + d & 38192 & 0.969 & 0.001 & 1.42 & 0.05 & 1.19 & 0.04 & 1.05 & 0.03 + u & 26672 & 0.968 & 0.001 & 1.23 & 0.04 & 1.09 & 0.03 & 1.02 & 0.03 + m & 23243 & 0.998 & 0.001 & 1.22 & 0.04 & 1.16 & 0.04 & 0.96 & 0.04 + c & 22482 & 1.031 & 0.001 & 1.32 & 0.04 & 1.30 & 0.04 & 1.15 & 0.04 + w & 22193 & 0.957 & 0.001 & 1.24 & 0.04 & 1.23 & 0.04 & 1.06 & 0.03 + f & 20812 & 0.997 & 0.001 & 1.33 & 0.05 & 1.20 & 0.04 & 1.01 & 0.04 + g & 20801 & 1.009 & 0.001 & 1.32 & 0.04 & 1.11 & 0.03 & 1.07 & 0.04 + p & 17233 & 1.057 & 0.001 & 1.23 & 0.04 & 1.13 & 0.04 & 1.12 & 0.03 + y & 16852 & 1.037 & 0.001 & 1.25 & 0.05 & 1.22 & 0.04 & 0.98 & 0.04 + _ the _ & 14404 & 1.033 & 0.002 & 1.40 & 0.04 & & & & + _ of _ & 6600 & 1.073 & 0.003 & 1.47 & 0.06 & & & & + _ and _ & 6428 & 0.962 & 0.002 & 1.23 & 0.04 & & & & + _ a _ & 4722 & 1.137 & 0.003 & 1.34 & 0.05 & & & & + _ to _ & 4619 & 1.023 & 0.003 & 1.15 & 0.04 & & & & + _ in _ & 4166 & 1.021 & 0.003 & 1.30 & 0.05 & & & & + _ whale _ & 1096 & 2.162 & 0.018 & 1.57 & 0.07 & & & & + _ from _ & 1085 & 1.143 & 0.006 & 1.15 & 0.04 & & & & + _ man _ & 476 & 1.252 & 0.007 & 1.21 & 0.04 & & & & + _ them _ & 474 & 1.214 & 0.012 & 1.16 & 0.04 & & & & + _ sea _ & 453 & 1.311 & 0.009 & 1.24 & 0.04 & & & & + _ old _ & 450 & 1.507 & 0.012 & 1.33 & 0.04 & & & & + _ we _ & 445 & 1.646 & 0.011 & 1.28 & 0.05 & & & & + _ ship _ & 438 & 1.522 & 0.012 & 1.31 & 0.04 & & & & + _ ahab _ & 436 & 3.056 & 0.021 & 1.53 & 0.06 & & & & + _ ye _ & 431 & 2.680 & 0.018 & 1.43 & 0.04 & & & & + _ who _ & 344 & 1.136 & 0.012 & 1.22 & 0.04 & & & & + _ head _ & 342 & 1.346 & 0.012 & 1.35 & 0.05 & & & & + _ time _ & 333 & 1.086 & 0.014 & 1.08 & 0.04 & & & & + _ long _ & 333 & 1.092 & 0.009 & 1.07 & 0.03 & & & & +     & & & + sequence & @xmath319 & @xmath144 & error & @xmath74 & error & @xmath74 & error & @xmath74 & error + vowels & 203916 & 0.437 & 0.020 & 1.20 & 0.04 & & & & + _ & 122194 & 0.450 & 0.008 & 1.41 & 0.05 & 1.41 & 0.05 & 1.41 & 0.05 + e & 69370 & 0.828 & 0.002 & 1.19 & 0.04 & 1.12 & 0.04 & 1.08 & 0.04 + t & 46645 & 0.872 & 0.002 & 1.10 & 0.05 & 1.09 & 0.04 & 1.02 & 0.03 + a & 41688 & 0.849 & 0.002 & 1.11 & 0.03 & 1.18 & 0.04 & 1.04 & 0.04 + o & 40041 & 0.891 & 0.001 & 1.18 & 0.04 & 1.09 & 0.03 & 1.00 & 0.04 + i & 37830 & 0.870 & 0.002 & 1.16 & 0.04 & 1.31 & 0.04 & 1.09 & 0.04 +",
    "n & 37689 & 0.884 & 0.001 & 1.13 & 0.04 & 1.16 & 0.04 & 1.09 & 0.03 + h & 34067 & 0.869 & 0.002 & 1.31 & 0.04 & 1.04 & 0.04 & 1.10 & 0.03 + s & 33114 & 0.956 & 0.001 & 1.06 & 0.03 & 1.11 & 0.03 & 1.06 & 0.03 + r & 32299 & 0.882 & 0.001 & 1.18 & 0.04 & 1.09 & 0.04 & 1.06 & 0.04 + d & 22303 & 0.917 & 0.002 & 1.15 & 0.04 & 1.11 & 0.03 & 1.05 & 0.03 + l & 21594 & 1.036 & 0.001 & 1.19 & 0.05 & 1.10 & 0.04 & 1.03 & 0.04 + u & 14987 & 0.971 & 0.002 & 1.26 & 0.04 & 1.17 & 0.04 & 1.03 & 0.03 + m & 14764 & 0.963 & 0.002 & 1.17 & 0.04 & 1.17 & 0.04 & 1.02 & 0.03 + c & 13461 & 1.005 & 0.002 & 1.27 & 0.06 & 1.20 & 0.04 & 1.04 & 0.04 + y & 12706 & 0.992 & 0.002 & 1.37 & 0.05 & 1.20 & 0.05 & 1.04 & 0.03 + w & 12305 & 0.949 & 0.002 & 1.23 & 0.04 & 1.14 & 0.04 & 1.06 & 0.04 + f & 11998 & 0.988 & 0.002 & 1.23 & 0.04 & 1.05 & 0.03 & 1.05 & 0.03 + g & 10031 & 0.949 & 0.002 & 1.06 & 0.04 & 1.17 & 0.04 & 1.03 & 0.03 + b & 9088 & 0.943 & 0.002 & 1.19 & 0.05 & 1.08 & 0.03 & 0.99 & 0.04 + _ the _ & 4331 & 1.083 & 0.003 & 1.24 & 0.04 & & & & + _ to _ & 4163 & 0.945 & 0.003 & 1.11 & 0.03 & & & & + _ of _ & 3609 & 0.974 & 0.003 & 1.21 & 0.04 & & & & + _ and _ & 3585 & 0.859 & 0.003 & 1.18 & 0.04 & & & & + _ her _ & 2225 & 1.592 & 0.015 & 1.31 & 0.04 & & & & + _ i _ & 2068 & 2.915 & 0.014 & 1.46 & 0.05 & & & & + _ at _ & 788 & 1.071 & 0.006 & 1.10 & 0.04 & & & & + _ mr _ & 786 & 1.218 & 0.007 & 1.32 & 0.06 & & & & + _ they _ & 601 & 1.459 & 0.010 & 1.26 & 0.04 & & & & + _ elizabeth _ & 597 & 1.192 & 0.027 & 1.17 & 0.06 & & & & + _ or _ & 300 & 1.026 & 0.010 & 1.00 & 0.03 & & & & + _ bennet _ & 294 & 2.047 & 0.034 & 1.37 & 0.07 & & & & + _ who _ & 284 & 1.148 & 0.010 & 1.06 & 0.03 & & & & + _ miss _ & 283 & 1.536 & 0.015 & 1.35 & 0.07 & & & & + _ one _ & 268 & 1.066 & 0.009 & 1.06 & 0.04 & & & & + _ jane _ & 264 & 1.741 & 0.016 & 1.29 & 0.06 & & & & + _ bingley _ & 257 & 3.166 & 0.019 & 1.45 & 0.08 & & & & + _ we _ & 253 & 1.546 & 0.013 & 1.26 & 0.04 & & & & + _ own _ & 183 & 1.078 & 0.015 & 1.06 & 0.04 & & & & + _ lady _ & 183 & 1.924 & 0.023 & 1.38 & 0.06 & & & & +     & & & + sequence & @xmath319 & @xmath144 & error & @xmath74 & error & @xmath74 & error & @xmath74 & error + vowels & 638882 & 0.430 & 0.020 & 1.26 & 0.04 & & & & + _ & 402964 & 0.415 & 0.009 & 1.34 & 0.05 & 1.34 & 0.05 & 1.34 & 0.05 + e & 204300 & 0.840 & 0.002 & 1.25 & 0.04 & 1.25 & 0.04 & 1.03 & 0.03 + t & 157193 & 0.867 & 0.002 & 1.26 & 0.04 & 1.25 & 0.04 & 1.03 & 0.03 + a & 138706 & 0.841 & 0.002 & 1.25 & 0.04 & 1.28 & 0.04 & 1.09 & 0.03 + o & 136541 & 0.881 & 0.001 & 1.24 & 0.04 & 1.29 & 0.04 & 1.03 & 0.03 + h & 117821 & 0.852 & 0.002 & 1.23 & 0.04 & 1.10 & 0.04 & 1.06 & 0.03 +",
    "n & 115898 & 0.866 & 0.002 & 1.23 & 0.04 & 1.17 & 0.04 & 1.05 & 0.04 + i & 112746 & 0.881 & 0.001 & 1.26 & 0.04 & 1.19 & 0.04 & 1.04 & 0.03 + s & 106979 & 0.935 & 0.001 & 1.28 & 0.05 & 1.27 & 0.04 & 1.06 & 0.03 + r & 92501 & 0.910 & 0.001 & 1.27 & 0.04 & 1.28 & 0.04 & 1.06 & 0.03 + d & 76655 & 0.929 & 0.001 & 1.34 & 0.04 & 1.22 & 0.04 & 1.03 & 0.03 + l & 62107 & 1.108 & 0.002 & 1.24 & 0.04 & 1.28 & 0.05 & 1.01 & 0.03 + u & 46589 & 0.949 & 0.001 & 1.23 & 0.04 & 1.17 & 0.04 & 1.00 & 0.04 + m & 42945 & 0.992 & 0.001 & 1.21 & 0.04 & 1.25 & 0.04 & 1.04 & 0.05 + f & 38552 & 0.977 & 0.001 & 1.24 & 0.04 & 1.24 & 0.04 & 1.05 & 0.03 + w & 38209 & 0.986 & 0.001 & 1.29 & 0.04 & 1.21 & 0.04 & 1.05 & 0.03 + c & 37602 & 0.984 & 0.001 & 1.21 & 0.04 & 1.26 & 0.04 & 1.08 & 0.03 + g & 31927 & 0.988 & 0.001 & 1.22 & 0.04 & 1.23 & 0.04 & 1.03 & 0.03 + y & 31053 & 1.048 & 0.001 & 1.25 & 0.04 & 1.26 & 0.04 & 1.05 & 0.04 + p & 23880 & 1.069 & 0.001 & 1.23 & 0.04 & 1.26 & 0.04 & 1.11 & 0.03 + _ the _ & 20652 & 1.050 & 0.002 & 1.36 & 0.04 & & & & + _ and _ & 16835 & 0.908 & 0.002 & 1.22 & 0.05 & & & & + _ to _ & 13184 & 1.031 & 0.002 & 1.26 & 0.04 & & & & + _ of _ & 12173 & 1.033 & 0.002 & 1.26 & 0.04 & & & & + _ that _ & 7515 & 1.023 & 0.002 & 1.21 & 0.04 & & & & + _ in _ & 6716 & 1.023 & 0.002 & 1.11 & 0.03 & & & & + _ by _ & 2069 & 1.042 & 0.004 & 1.09 & 0.03 & & & & + _ sancho _ & 2063 & 3.762 & 0.025 & 1.63 & 0.05 & & & & + _ or _ & 2048 & 1.154 & 0.004 & 1.15 & 0.04 & & & & + _ quixote _ & 2002 & 3.214 & 0.016 & 1.55 & 0.06 & & & & + _ other _ & 609 & 1.072 & 0.008 & 1.11 & 0.04 & & & & + _ knight _ & 606 & 2.175 & 0.016 & 1.43 & 0.05 & & & & + _ take _ & 546 & 1.195 & 0.008 & 1.14 & 0.04 & & & & + _ master _ & 545 & 1.720 & 0.013 & 1.38 & 0.04 & & & & + _ thy _ & 510 & 2.252 & 0.017 & 1.35 & 0.05 & & & & + _ senor _ & 509 & 1.632 & 0.009 & 1.28 & 0.04 & & & & + _ worship _ & 470 & 2.337 & 0.012 & 1.37 & 0.04 & & & & + _ here _ & 467 & 1.237 & 0.007 & 1.14 & 0.04 & & & & + _ god _ & 467 & 1.169 & 0.011 & 1.12 & 0.04 & & & & + _ way _ & 466 & 1.056 & 0.006 & 1.07 & 0.04 & & & & +     & & & + sequence & @xmath319 & @xmath144 & error & @xmath74 & error & @xmath74 & error & @xmath74 & error + vowels & 110026 & 0.432 & 0.020 & 1.23 & 0.04 & & & & + _ & 71180 & 0.402 & 0.010 & 1.50 & 0.05 & 1.50 & 0.05 & 1.50 & 0.05 + e & 35603 & 0.864 & 0.002 & 1.30 & 0.04 & 1.05 & 0.05 & 0.99 & 0.04 + t & 28825 & 0.858 & 0.002 & 1.16 & 0.04 & 1.23 & 0.04 & 0.99 & 0.04 + a & 23478 & 0.858 & 0.002 & 1.17 & 0.04 & 1.03 & 0.03 & 1.13 & 0.04 + o & 23192 & 0.898 & 0.001 & 1.22 & 0.04 & 1.06 & 0.03 & 0.98 & 0.04 +",
    "n & 20146 & 0.866 & 0.002 & 1.12 & 0.04 & 1.23 & 0.04 & 1.07 & 0.03 + h & 19565 & 0.861 & 0.002 & 1.18 & 0.04 & 1.11 & 0.04 & 1.07 & 0.04 + i & 18811 & 0.910 & 0.002 & 1.16 & 0.05 & 1.23 & 0.04 & 1.05 & 0.03 + s & 17716 & 0.951 & 0.001 & 1.19 & 0.04 & 1.13 & 0.04 & 1.08 & 0.03 + r & 15247 & 0.917 & 0.002 & 1.30 & 0.05 & 1.13 & 0.04 & 1.10 & 0.05 + d & 14850 & 0.950 & 0.002 & 1.20 & 0.04 & 1.20 & 0.04 & 1.01 & 0.03 + l & 12136 & 1.086 & 0.002 & 1.17 & 0.04 & 1.14 & 0.04 & 1.06 & 0.03 + u & 8942 & 0.949 & 0.002 & 1.18 & 0.04 & 1.07 & 0.04 & 1.06 & 0.03 + w & 8042 & 0.949 & 0.002 & 1.13 & 0.03 & 1.18 & 0.04 & 1.12 & 0.04 + m & 7135 & 0.977 & 0.002 & 1.22 & 0.04 & 1.18 & 0.04 & 1.02 & 0.03 + y & 6725 & 1.043 & 0.002 & 1.36 & 0.04 & 1.04 & 0.03 & 1.00 & 0.04 + g & 6606 & 1.041 & 0.002 & 1.16 & 0.04 & 1.15 & 0.05 & 1.06 & 0.03 + c & 6497 & 1.030 & 0.003 & 1.23 & 0.05 & 1.09 & 0.05 & 1.16 & 0.04 + f & 6004 & 1.047 & 0.003 & 1.22 & 0.04 & 1.11 & 0.03 & 1.02 & 0.03 + b & 4958 & 0.959 & 0.003 & 1.10 & 0.04 & 1.25 & 0.04 & 1.02 & 0.03 + _ the _ & 3703 & 1.154 & 0.004 & 1.35 & 0.04 & & & & + _ and _ & 3105 & 1.008 & 0.003 & 1.21 & 0.04 & & & & + _ a _ & 1863 & 1.085 & 0.005 & 1.20 & 0.04 & & & & + _ to _ & 1727 & 1.054 & 0.004 & 1.14 & 0.03 & & & & + _ of _ & 1436 & 1.127 & 0.005 & 1.21 & 0.04 & & & & + _ he _ & 1197 & 1.770 & 0.015 & 1.40 & 0.04 & & & & + _ tom _ & 689 & 1.740 & 0.014 & 1.39 & 0.06 & & & & + _ with _ & 647 & 1.068 & 0.008 & 1.15 & 0.04 & & & & + _ if _ & 237 & 1.404 & 0.011 & 1.23 & 0.04 & & & & + _ huck _ & 223 & 3.228 & 0.024 & 1.46 & 0.07 & & & & + _ boys _ & 155 & 1.767 & 0.019 & 1.24 & 0.06 & & & & + _ did _ & 150 & 1.336 & 0.018 & 1.22 & 0.04 & & & & + _ joe _ & 133 & 2.248 & 0.051 & 1.38 & 0.06 & & & & + _ never _ & 131 & 1.185 & 0.017 & 1.14 & 0.04 & & & & + _ boy _ & 122 & 1.788 & 0.054 & 1.29 & 0.06 & & & & + _ back _ & 121 & 0.968 & 0.015 & 1.04 & 0.03 & & & & + _ off _ & 99 & 1.335 & 0.019 & 1.12 & 0.04 & & & & + _ night _ & 98 & 2.025 & 0.057 & 1.29 & 0.04 & & & & + _ other _ & 96 & 1.145 & 0.019 & 1.12 & 0.03 & & & & + _ becky _ & 96 & 2.701 & 0.036 & 1.55 & 0.10 & & & & +     & & & + sequence & @xmath319 & @xmath144 & error & @xmath74 & error & @xmath74 & error & @xmath74 & error + vowels & 440676 & 0.456 & 0.020 & 1.61 & 0.06 & & & & + _ & 265304 & 0.436 & 0.009 & 1.78 & 0.06 & 1.78 & 0.06 & 1.78 & 0.06 + e & 141465 & 0.855 & 0.002 & 1.28 & 0.04 & 1.30 & 0.06 & 1.11 & 0.03 + t & 100183 & 0.904 & 0.001 & 1.54 & 0.07 & 1.37 & 0.06 & 1.05 & 0.03 + a & 93129 & 0.877 & 0.001 & 1.32 & 0.05 & 1.25 & 0.06 & 1.06 & 0.03 + o & 91403 & 0.930 & 0.001 & 1.19 & 0.04 & 1.35 & 0.05 & 1.10 & 0.04 + i & 81407 & 0.914 & 0.001 & 1.44 & 0.07 & 1.33 & 0.06 & 1.21 & 0.05 +",
    "n & 80138 & 0.897 & 0.001 & 1.34 & 0.05 & 1.27 & 0.04 & 1.26 & 0.06 + s & 76915 & 0.950 & 0.001 & 1.40 & 0.06 & 1.31 & 0.06 & 1.23 & 0.06 + h & 72550 & 0.906 & 0.002 & 1.61 & 0.06 & 1.23 & 0.05 & 1.44 & 0.08",
    "+ r & 69852 & 0.918 & 0.001 & 1.49 & 0.06 & 1.22 & 0.04 & 1.30 & 0.06 + l & 55052 & 1.074 & 0.001 & 1.41 & 0.06 & 1.39 & 0.06 & 1.19 & 0.05 + d & 49093 & 0.980 & 0.001 & 1.44 & 0.05 & 1.17 & 0.04 & 1.04 & 0.04 + u & 33272 & 0.982 & 0.001 & 1.25 & 0.04 & 1.36 & 0.06 & 1.16 & 0.04 + m & 31535 & 1.025 & 0.001 & 1.29 & 0.05 & 1.35 & 0.05 & 1.05 & 0.03 + c & 29894 & 1.072 & 0.001 & 1.62 & 0.07 & 1.31 & 0.04 & 1.36 & 0.07 + g & 27791 & 1.031 & 0.001 & 1.36 & 0.06 & 1.24 & 0.05 & 1.19 & 0.04 + f & 26638 & 1.025 & 0.001 & 1.30 & 0.05 & 1.22 & 0.04 & 1.12 & 0.03 + w & 26164 & 1.056 & 0.001 & 1.53 & 0.07 & 1.32 & 0.05 & 1.31 & 0.06 + y & 24251 & 1.032 & 0.001 & 1.36 & 0.05 & 1.15 & 0.03 & 1.01 & 0.03 + p & 22440 & 1.124 & 0.002 & 1.46 & 0.06 & 1.27 & 0.05 & 1.25 & 0.05 + _ the _ & 14952 & 1.071 & 0.002 & 1.44 & 0.06 & & & & + _ of _ & 8141 & 1.121 & 0.003 & 1.63 & 0.07 & & & & + _ and _ & 7217 & 1.167 & 0.003 & 1.53 & 0.05 & & & & + _ a _ & 6518 & 1.144 & 0.003 & 1.24 & 0.04 & & & & + _ to _ & 4963 & 1.157 & 0.003 & 1.38 & 0.07 & & & & + _ in _ & 4946 & 1.002 & 0.002 & 1.16 & 0.04 & & & & + _ were _ & 510 & 1.461 & 0.013 & 1.27 & 0.04 & & & & + _ stephen _ & 505 & 4.955 & 0.099 & 1.64 & 0.06 & & & & + _ we _ & 425 & 2.427 & 0.085 & 1.25 & 0.04 & & & & + _ man _ & 415 & 1.388 & 0.019 & 1.16 & 0.04 & & & & + _ into _ & 330 & 1.179 & 0.011 & 1.14 & 0.04 & & & & + _ eyes _ & 329 & 1.921 & 0.013 & 1.21 & 0.04 & & & & + _ where _ & 310 & 1.214 & 0.014 & 1.11 & 0.03 & & & & + _ hand _ & 308 & 1.295 & 0.017 & 1.18 & 0.04 & & & & + _ street _ & 293 & 1.394 & 0.013 & 1.21 & 0.04 & & & & + _ our _ & 291 & 1.556 & 0.018 & 1.23 & 0.04 & & & & + _ first _ & 278 & 1.306 & 0.011 & 1.19 & 0.04 & & & & + _ father _ & 277 & 1.631 & 0.013 & 1.62 & 0.05 & & & & + _ day _ & 250 & 1.131 & 0.012 & 1.10 & 0.03 & & & & + _ just _ & 249 & 2.014 & 0.012 & 1.20 & 0.04 & & & & +     & & & + sequence & @xmath319 & @xmath144 & error & @xmath74 & error & @xmath74 & error & @xmath74 & error + vowels & 945097 & 0.430 & 0.020 & 1.55 & 0.05 & & & & + _ & 565161 & 0.426 & 0.009 & 1.50 & 0.05 & 1.50 & 0.05 & 1.50 & 0.05 + e & 312626 & 0.834 & 0.002 & 1.39 & 0.05 & 1.35 & 0.04 & 1.05 & 0.03 + t & 224180 & 0.886 & 0.001 & 1.40 & 0.04 & 1.27 & 0.04 & 1.09 & 0.03 + a & 204154 & 0.869 & 0.002 & 1.42 & 0.05 & 1.18 & 0.04 & 1.05 & 0.03 + o & 191126 & 0.904 & 0.001 & 1.45 & 0.05 & 1.40 & 0.04 & 1.04 & 0.04 +",
    "n & 182910 & 0.860 & 0.002 & 1.28 & 0.04 & 1.43 & 0.05 & 1.17 & 0.04 + i & 172403 & 0.894 & 0.001 & 1.48 & 0.05 & 1.46 & 0.05 & 1.21 & 0.04 + h & 166290 & 0.852 & 0.002 & 1.50 & 0.05 & 1.26 & 0.04 & 1.28 & 0.05 + s & 161889 & 0.955 & 0.001 & 1.32 & 0.04 & 1.47 & 0.05 & 1.04 & 0.03 + r & 146667 & 0.919 & 0.001 & 1.38 & 0.04 & 1.34 & 0.04 & 1.10 & 0.03 + d & 117632 & 0.923 & 0.001 & 1.48 & 0.05 & 1.33 & 0.04 & 1.05 & 0.03",
    "+ l & 95888 & 1.064 & 0.001 & 1.26 & 0.04 & 1.30 & 0.04 & 1.04 & 0.04 + u & 64788 & 0.971 & 0.001 & 1.26 & 0.04 & 1.26 & 0.04 & 1.04 & 0.03 + m & 61162 & 1.018 & 0.001 & 1.30 & 0.04 & 1.26 & 0.04 & 0.98 & 0.04 + c & 60576 & 1.009 & 0.001 & 1.54 & 0.05 & 1.39 & 0.04 & 1.17 & 0.04 + w & 58852 & 0.978 & 0.001 & 1.28 & 0.04 & 1.29 & 0.04 & 1.24 & 0.05 + f & 54419 & 1.064 & 0.001 & 1.49 & 0.05 & 1.23 & 0.04 & 1.05 & 0.03 + g & 50819 & 1.014 & 0.001 & 1.49 & 0.05 & 1.37 & 0.04 & 1.08 & 0.04 + y & 45847 & 1.035 & 0.001 & 1.36 & 0.04 & 1.34 & 0.04 & 1.01 & 0.04 + p & 44680 & 1.080 & 0.001 & 1.48 & 0.05 & 1.34 & 0.04 & 1.12 & 0.03 + _ the _ & 34495 & 1.128 & 0.002 & 1.59 & 0.05 & & & & + _ and _ & 22217 & 0.874 & 0.002 & 1.22 & 0.04 & & & & + _ to _ & 16640 & 1.056 & 0.001 & 1.24 & 0.04 & & & & + _ of _ & 14864 & 1.168 & 0.002 & 1.59 & 0.05 & & & & + _ a _ & 10525 & 1.119 & 0.002 & 1.21 & 0.04 & & & & + _ he _ & 9860 & 1.893 & 0.006 & 1.44 & 0.06 & & & & + _ so _ & 1900 & 1.180 & 0.005 & 1.14 & 0.03 & & & & + _ prince _ & 1890 & 3.862 & 0.026 & 1.68 & 0.06 & & & & + _ pierre _ & 1796 & 6.563 & 0.042 & 1.78 & 0.06 & & & & + _ an _ & 1625 & 1.131 & 0.004 & 1.17 & 0.04 & & & & + _ could _",
    "& 1115 & 1.285 & 0.005 & 1.15 & 0.05 & & & & + _ natasha _ & 1098 & 6.334 & 0.036 & 1.74 & 0.06 & & & & + _ man _ & 1081 & 1.407 & 0.007 & 1.33 & 0.04 & & & & + _ will _ & 1066 & 1.530 & 0.011 & 1.36 & 0.04 & & & & + _ andrew _ & 1047 & 4.321 & 0.021 & 1.71 & 0.06 & & & & + _ do _ & 1037 & 1.273 & 0.010 & 1.16 & 0.04 & & & & + _ time _ & 926 & 1.100 & 0.008 & 1.13 & 0.03 & & & & + _ princess _ & 915 & 5.431 & 0.033 & 1.72 & 0.06 & & & & + _ face _ & 893 & 1.445 & 0.007 & 1.25 & 0.04 & & & & + _ french _ & 879 & 2.287 & 0.029 & 1.53 & 0.05 & & & & +"
  ],
  "abstract_text": [
    "<S> the complexity of human interactions with social and natural phenomena is mirrored in the way we describe our experiences through natural language . in order to retain and convey such a high dimensional information , </S>",
    "<S> the statistical properties of our linguistic output has to be highly correlated in time . </S>",
    "<S> an example are the robust observations , still largely not understood , of correlations on arbitrary long scales in literary texts . in this paper </S>",
    "<S> we explain how long - range correlations flow from highly structured linguistic levels down to the building blocks of a text ( words , letters , etc .. ) . by combining calculations and data analysis </S>",
    "<S> we show that correlations take form of a bursty sequence of events once we approach the semantically relevant topics of the text . </S>",
    "<S> the mechanisms we identify are fairly general and can be equally applied to other hierarchical settings . + published as : link : dx.doi.org/10.1073/pnas.1117723109[proc . </S>",
    "<S> nat . </S>",
    "<S> acad . </S>",
    "<S> sci . </S>",
    "<S> usa ( 2012 ) doi : 10.1073/pnas.1117723109 ]    literary texts are an expression of the natural language ability to project complex and high - dimensional phenomena into a one - dimensional , semantically meaningful sequence of symbols . for this projection to be successful , such sequences have to encode the information in form of structured patterns , such as correlations on arbitrarily long scales  @xcite . </S>",
    "<S> understanding how language processes long - range correlations , an ubiquitous signature of complexity present in human activities  @xcite and in the natural world  @xcite , is an important task towards comprehending how natural language works and evolves . </S>",
    "<S> this understanding is also crucial to improve the increasingly important applications of information theory and statistical natural language processing , which are mostly based on short - range - correlations methods @xcite .    </S>",
    "<S> take your favorite novel and consider the binary sequence obtained by mapping each vowel into a @xmath0 and all other symbols into a @xmath1 . </S>",
    "<S> one can easily detect structures on neighboring bits , and we certainly expect some repetition patterns on the size of words . </S>",
    "<S> but one should certainly be surprised and intrigued when discovering that there are structures ( or memory ) after several pages or even on arbitrary large scales of this binary sequence . in the last twenty years </S>",
    "<S> , similar observations of long - range correlations in texts have been related to large scales characteristics of the novels such as the story being told , the style of the book , the author , and the language  @xcite . however , the mechanisms explaining these connections are still missing ( see ref .  </S>",
    "<S> @xcite for a recent proposal ) . without such mechanisms , many fundamental questions can not be answered . </S>",
    "<S> for instance , why all previous investigations observed long - range correlations despite their radically different approaches ? </S>",
    "<S> how and which correlations can flow from the high - level semantic structures down to the crude symbolic sequence in the presence of so many arbitrary influences ? </S>",
    "<S> what information is gained on the large structures by looking at smaller ones ? finally , what is the origin of the long - range correlations ?    in this paper we provide answers to these questions by approaching the problem through a novel theoretical framework . </S>",
    "<S> this framework uses the hierarchical organization of natural language to identify a mechanism that links the correlations at different linguistic levels . as schematically depicted in fig .  </S>",
    "<S> [ fig.1 ] , a topic is linked to several words that are used to describe it in the novel . at the lower level </S>",
    "<S> , words are connected to the letters they are formed , and so on . </S>",
    "<S> we calculate how correlations are transported through these different levels and compare the results with a detailed statistical analysis in ten different novels . </S>",
    "<S> our results reveal that while approaching semantically relevant high - level structures , correlations unfold in form of a bursty signal . moving down in levels , </S>",
    "<S> we show that correlations ( but not burstiness ) are preserved , explaining the ubiquitous appearance of long - range correlations in texts .    ) , letters ( a - z ) , words , and topics . ] </S>"
  ]
}