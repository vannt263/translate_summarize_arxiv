{
  "article_text": [
    "many machine learning applications are faced with large and inherently high - dimensional datasets . for example",
    ", @xcite discusses training datasets with ( on average ) @xmath4 items and @xmath5 distinct features . @xcite",
    "experimented with a dataset of potentially 16 trillion ( @xmath6 ) unique features .",
    "interestingly , while large - scale learning has become a very urgent , hot topic , it is usually very difficult for researchers from universities to obtain truly large , high - dimensional datasets from industry .",
    "for example , the experiments in our recent work  @xcite on large - scale learning using @xmath0-bit minwise hashing  @xcite were based on the _ webspam _ dataset ( about 24 gb in libsvm format ) , which may be too small .    to overcome this difficulty , we have generated a dataset of about 200 gb ( in libsvm format ) from the _ rcv1 _",
    "dataset , using the original features + all pairwise combinations of features + 1/30 of the 3-way combinations of features .",
    "we choose 200 gb ( which of course is still very small ) because relatively inexpensive workstations with 192 gb memory are in the market , which may make it possible for liblinear  @xcite , the popular solver for logistic regression and linear svm , to perform the training of the entire dataset in main memory .",
    "we hope in the near future we will be able to purchase such a workstation . of course , in this `` information explosion '' age , the growth of data is always much faster than the growth of memory capacity .",
    "note that the our hashing method is orthogonal to particular solvers of logistic regression and svm .",
    "we have tested @xmath0-bit minwise hashing with other solvers  @xcite and observed substantial improvements .",
    "we choose liblinear  @xcite as the work horse because it is a popular tool and may be familiar to non - experts .",
    "our experiments may be easily validated by simply generating the hashed data off - line and feeding them to liblinear ( or other solvers ) without modification to the code . also , we notice that the source code of liblinear , unlike many other excellent solvers , can be compiled in visual studio without modification . as many practitioners are using windows  , we use liblinear throughout the paper , for the sake of maximizing the repeatability of our work .",
    "+ unsurprisingly , our experimental results agree with our prior studies  @xcite that @xmath0-bit minwise hashing is substantially more accurate than the vowpal wabbit ( vw ) hashing algorithm  @xcite at the same storage .",
    "note that in our paper , vw refers to the particular hashing algorithm in  @xcite , not the online learning platform that the authors of  @xcite have been developing . for evaluation purposes , we must separate out hashing algorithms from learning algorithms because they are orthogonal to each other .",
    "+ all randomized algorithms including minwise hashing and random projections rely on pseudo - random numbers .",
    "a common practice of minwise hashing ( e.g. ,  @xcite ) is to use universal hashing functions to replace perfect random permutations . in this paper",
    ", we also present an empirical study to verify that this common practice does not degrade the learning performance .",
    "minwise hashing has been widely deployed in industry and @xmath0-bit minwise hashing requires only minimal modifications .",
    "it is well - understood at least in the context of search that the ( one time ) preprocessing cost is not a major issue because the preprocessing step , which is trivially parallelizable , can be conducted off - line or combined in the data - collection process . in the context of pure machine learning research",
    ", one thing we notice is that for training truly large - scale datasets , the data loading time is often dominating  @xcite , for online algorithms as well as batch algorithms ( if the data fit in memory ) .",
    "thus , if we have to load the data many times , for example , for testing different `` @xmath7 '' values in svm or running an online algorithms for multiple epoches , then the benefits of data reduction algorithms such as @xmath0-bit minwise hashing would be enormous .",
    "even on our dataset of 200 gb only , we observe that the preprocessing cost is roughly on the same order of magnitude as the data loading time .",
    "furthermore , using a gpu ( which is inexpensive ) for fast hashing , we can reduce the preprocessing cost of @xmath0-bit minwise hashing to a small fraction of the data loading time . in other words ,",
    "the dominating cost is the still the data loading time .",
    "+ we are currently experimenting @xmath0-bit minwise hashing for machine learning with @xmath8 tb datasets and the results will be reported in subsequent technical reports .",
    "it is a very fun process to experiment with @xmath0-bit minwise hashing and we certainly would like to share our experience with the machine learning and data mining community .",
    "_ minwise hashing _  @xcite has been successfully applied to a very wide range of real - world problems especially in the context of search  @xcite , for efficiently computing set similarities .",
    "minwise hashing mainly works well with binary data , which can be viewed either as 0/1 vectors or as sets .",
    "given two sets , @xmath9 , a widely used ( normalized ) measure of similarity is the _ resemblance _ @xmath10 : @xmath11 in this method , one applies a random permutation @xmath12 on @xmath13 and @xmath14 .",
    "the collision probability is simply @xmath15 one can repeat the permutation @xmath16 times : @xmath17 , @xmath18 , ... ,",
    "@xmath19 to estimate @xmath10 without bias , as @xmath20    the common practice of minwise hashing is to store each hashed value , e.g. , @xmath21 and @xmath22 , using 64 bits  @xcite .",
    "the storage ( and computational ) cost will be prohibitive in truly large - scale ( industry ) applications  @xcite .    in order to apply minwise hashing for efficiently training linear learning algorithms such as logistic regression or linear svm",
    ", we need to express the estimator ( [ eqn_rm ] ) as an inner product . for simplicity , we introduce @xmath23 and we hope that the term @xmath24 can be expressed as an inner product . indeed , because @xmath25 we know immediately that the estimator ( [ eqn_rm ] ) for minwise hashing is an inner product between two extremely high - dimensional ( @xmath26 ) vectors .",
    "each vector , which has exactly @xmath16 1 s , is a concatenation of @xmath16 @xmath27-dimensional vectors .",
    "because @xmath28 is possible in industry applications , the total indexing space ( @xmath26 ) may be too high to directly use this representation for training .",
    "+ the recent development of _ b - bit minwise hashing _  @xcite provides a strikingly simple solution by storing only the lowest @xmath0 bits ( instead of 64 bits ) of each hashed value . for convenience ,",
    "we define @xmath29    @xcite[the_basic ] assume @xmath27 is large ( i.e. , @xmath30 ) .",
    "@xmath31^{2^b-1}}{1-\\left[1-r_1\\right]^{2^b}},\\hspace{1 in } a_{2,b } = \\frac{r_2\\left[1-r_2\\right]^{2^b-1}}{1-\\left[1-r_2\\right]^{2^b}}.\\box\\end{aligned}\\ ] ] as @xmath32 and @xmath33 , the limits are @xmath34    the case @xmath35 is very common in practice because the data are often relatively highly sparse ( i.e. , @xmath36 ) , although they can be very large in the absolute scale .",
    "for example , if @xmath28 , then a set @xmath13 with @xmath37 ( which roughly corresponds to the size of a small novel ) is highly sparse ( @xmath38 ) even though @xmath39 is actually very large in the absolute scale .",
    "one can also verify that the error by using ( [ eqn_pb_r->0 ] ) to replace ( [ eqn_basic ] ) is bounded by @xmath40 , which is very small when @xmath35 .",
    "in fact , @xcite extensively used this argument for studying 3-way set similarities .",
    "we can then estimate @xmath41 ( and @xmath10 ) from @xmath16 independent permutations : @xmath17 , @xmath18 , ... , @xmath19 , @xmath42@xmath43 ^ 2 } = \\frac{1}{k}\\frac{\\left[c_{1,b}+(1-c_{2,b})r\\right]\\left[1-c_{1,b}-(1-c_{2,b})r\\right]}{\\left[1-c_{2,b}\\right]^2}\\end{aligned}\\ ] ]    clearly , the similarity ( @xmath10 ) information is adequately encoded in @xmath41 . in other words , often there is no need to explicitly estimate @xmath10 .",
    "the estimator @xmath44 is an inner product between two vectors in @xmath45 dimensions with exactly @xmath16 1 s .",
    "therefore , if @xmath0 is not too large ( e.g. , @xmath46 ) , this intuition provides a simple practical strategy for using @xmath0-bit minwise hashing for large - scale learning .",
    "linear algorithms such as linear svm and logistic regression have become very powerful and extremely popular .",
    "representative software packages include svm@xmath47  @xcite , pegasos  @xcite , bottou s sgd svm  @xcite , and liblinear  @xcite .",
    "given a dataset @xmath48 , @xmath49 , @xmath50 , the @xmath51-regularized linear svm solves the following optimization problem : @xmath52 and the @xmath51-regularized logistic regression solves a similar problem : @xmath53 here @xmath54 is an important penalty parameter .",
    "since our purpose is to demonstrate the effectiveness of our proposed scheme using @xmath0-bit hashing , we simply provide results for a wide range of @xmath7 values and assume that the best performance is achievable if we conduct cross - validations .    in our approach , we apply @xmath16 independent random permutations on each feature vector @xmath55 and store the lowest @xmath0 bits of each hashed value .",
    "this way , we obtain a new dataset which can be stored using merely @xmath56 bits . at run - time , we expand each new data point into a @xmath45-length vector .",
    "+ for example , suppose @xmath57 and the hashed values are originally @xmath58 , whose binary digits are @xmath59 .",
    "consider @xmath60 .",
    "then the binary digits are stored as @xmath61 ( which corresponds to @xmath62 in decimals ) . at run - time , we need to expand them into a vector of length @xmath63 , to be @xmath64 , which will be the new feature vector fed to a solver : @xmath65",
    "in our earlier technical reports  @xcite , our experimental settings closely followed the work in  @xcite by testing @xmath0-bit minwise hashing on the _ webspam _ dataset ( @xmath66 , @xmath67 ) .",
    "following  @xcite , we randomly selected @xmath68 of samples for testing and used the remaining @xmath69 samples for training . since the _ webspam",
    "_ dataset ( 24 gb in libsvm format ) may be too small compared by datasets used in industry , in this paper we present an empirical study on the _ expanded rcv1 _",
    "dataset by using the original features + all pairwise combinations ( products ) of features + 1/30 of 3-way combinations ( products ) of features .",
    "we chose liblinear as the tool to demonstrate the effectiveness of our algorithm .",
    "all experiments were conducted on workstations with xeon(r ) cpu ( w5590@3.33ghz ) and 48 gb ram , under windows 7 system .",
    ".data information [ cols=\"<,<,<,<,<\",options=\"header \" , ]     [ tab_preprocessing ]",
    "conceptually , minwise hashing requires @xmath16 permutation mappings @xmath70 , @xmath71 to @xmath16 , where @xmath72 .",
    "if we are able to store these @xmath16 permutation mappings , then the operation is straightforward . for practical industrial applications , however , storing permutations would be infeasible .",
    "instead , permutations are usually simulated by _ universal hashing _ , which only requires storing very few numbers .    the simplest ( and possibly the most popular ) approach is to use _",
    "hashing_. that is , we define a series of hashing functions @xmath73 to replace @xmath74 : @xmath75 where @xmath76 is a prime number and @xmath77 is chosen uniformly from @xmath78 and @xmath79 is chosen uniformly from @xmath80 . this way , instead of storing @xmath74 , we only need to store @xmath81 numbers , @xmath82 , @xmath83 to @xmath16 .",
    "there are several small `` tricks '' for speeding up 2-universal hashing ( e.g. , avoiding modular arithmetic ) .",
    "an interesting thread might be http://mybiasedcoin.blogspot.com/2009/12/text-book-algorithms-at-soda-guest-post.html    given a feature vector ( e.g. , a document parsed as a list of 1-gram , 2-gram , and 3-grams ) , for any nonzero location @xmath84 in the original feature vector , its new location becomes @xmath85 ; and we walk through all nonzeros locations to find the minimum of the new locations , which will be the @xmath86th hashed value for that feature vector . since the generated parameters , @xmath77 and @xmath79 , are fixed ( and stored ) , this procedure becomes deterministic .",
    "+ our experiments on _ webspam _ can show that even with this simplest hashing method , we can still achieve good performance compared to using perfect random permutations . we can not realistically store @xmath16 permutations for the _ rcv1 _ dataset because its @xmath87 .",
    "thus , we only verify the practice of using 2-universal hashing on the _ webspam _ dataset , as demonstrated in figure  [ fig_spam_acc_2u ] .",
    "it has been a lot of fun to develop @xmath0-bit minwise hashing and apply it to machine learning for training very large - scale datasets .",
    "we hope engineers will find our method applicable to their work .",
    "we also hope this work can draw interests from research groups in statistics , theoretical cs , machine learning , or search technology .",
    "ludmila cherkasova , kave eshghi , charles b.  morrey iii , joseph tucek , and alistair  c. veitch . applying syntactic similarity algorithms for enterprise information management . in _",
    "kdd _ , pages 10871096 , paris , france , 2009 .",
    "cho - jui hsieh , kai - wei chang , chih - jen lin , s.  sathiya keerthi , and s.  sundararajan . a dual coordinate descent method for large - scale linear svm . in _ proceedings of the 25th international conference on machine learning _",
    ", icml , pages 408415 , 2008 ."
  ],
  "abstract_text": [
    "<S> our recent work on large - scale learning using @xmath0-bit minwise hashing  @xcite was tested on the _ webspam _ dataset ( about 24 gb in libsvm format ) , which may be way too small compared to real datasets used in industry . since we could not access the proprietary dataset used in  @xcite for testing the vowpal wabbit ( vw ) hashing algorithm , in this paper we present an experimental study based on the expanded _ </S>",
    "<S> rcv1 _ dataset ( about 200 gb in libsvm format ) .    in our earlier report  @xcite </S>",
    "<S> , the experiments demonstrated that , with merely 200 hashed values per data point , @xmath0-bit minwise hashing can achieve similar test accuracies as vw with @xmath1 hashed values per data point , on the _ webspam _ dataset . in this paper , our new experiments on the ( expanded ) _ </S>",
    "<S> rcv1 _ </S>",
    "<S> dataset clearly agree with our earlier observation that @xmath0-bit minwise hashing algorithm is substantially more accurate than vw hashing algorithm at the same storage . </S>",
    "<S> for example , with @xmath2 ( 16384 ) hashed values per data point , vw achieves similar test accuracies as @xmath0-bit hashing with merely 30 hashed values per data point . </S>",
    "<S> this is of course not surprising as the report  @xcite has already demonstrated that the variance of the vw algorithm can be order of magnitude(s ) larger than the variance of @xmath0-bit minwise hashing . </S>",
    "<S> it was shown in @xcite that vw has the same variance as random projections . </S>",
    "<S> + at least in the context of search , minwise hashing has been widely used in industry . </S>",
    "<S> it is well - understood that the preprocessing cost is not a major issue because the preprocessing is trivially parallelizable and can be conducted off - line or combined with the data - collection process . nevertheless , in this paper , we report that , even merely from the perspective of academic machine learning practice , the preprocessing cost is not a major issue for the following reasons :    * the preprocessing incurs only a one - time cost . the same processed data can be used for many training experiments , for example , for many different `` c '' values in svm cross - validation , or for different combinations of data splitting ( into training and testing sets ) . * for training truly large - scale datasets , </S>",
    "<S> the dominating cost is often the data loading time . in our 200 gb dataset ( which may be still very small according to the industry standard ) , the preprocessing cost of @xmath0-bit minwise hashing is on the same order of magnitude as the data loading time . * using a gpu , the preprocessing cost can be easily reduced to a small fraction ( e.g.,@xmath3 ) of the data loading time .    the standard industry practice of minwise hashing is to use universal hashing to replace permutations . </S>",
    "<S> in other words , there is no need to store any permutation mappings , one of the reasons why minwise hashing is popular . in this paper </S>",
    "<S> , we also provide experiments to verify this practice , based on the simplest 2-universal hashing , and illustrate that the performance of @xmath0-bit minwise hashing does not degrade . </S>"
  ]
}