{
  "article_text": [
    "over the last decade , graphs have emerged as the standard for modeling interactions between entities in a wide variety of applications .",
    "graphs are used to model infrastructure networks , the world wide web , computer traffic , molecular interactions , ecological systems , epidemics , co - authors , citations , and social interactions , among others .",
    "understanding the frequency of small subgraphs has been an important aspect of graph analysis .    despite the differences in the motivating applications ,",
    "some topological structures have emerged to be important across all these domains .",
    "the most important such subgraph is the triangle ( 3-clique ) .",
    "many networks , especially social networks , are known to have many triangles .",
    "this is thought to be because social interactions exhibit homophily ( people befriend similar people ) and transitivity ( friends of friends become friends ) .",
    "the notion of _ clustering coefficient _ is inspired by this observation , and is the standard method of summarizing triangle counts  @xcite .",
    "it is well known that some networks , especially social networks , have much higher clustering coefficients than random networks  @xcite .",
    "triangle measures are important for understanding network structure and evolution @xcite and reproducing the degree - wise clustering coefficients of a network is important for generative models @xcite .      for large graphs , computing triangle - based measures can be expensive .",
    "the standard approach is to find all wedges , i.e. , paths of length 2 , and check to see if they are closed , i.e. , the edge that complete the triangle exists .",
    "previous work presents a wedge - sampling approach for approximating clustering coefficients  @xcite ; this is in contrast to sampling single edges , which is a more obvious but less reliable technique . in @xcite",
    ", it is shown that the wedge - sampling approach is orders of magnitude faster than enumeration and is both faster and has less variance than edge - sampling techniques . in this paper , we show that the wedge - sampling approach scales to massive networks using mapreduce , a framework well - suited to sampling .",
    "previous distributed triangle counting algorithms have had to deal with problems of finding triangles where edges are stored on different processors and skewed degree distributions lead to load balancing issues  @xcite .",
    "in contrast , our wedge sampling approach in mapreduce deals with these issues seamlessly and recommends sampling as a general technique for large graphs since it leads to fast serial algorithms as well as scalable parallel implementations .",
    "we describe our contributions in more detail as follows :    we present a parallelization of our wedge - based sampling algorithm in the mapreduce framework .",
    "the premise of wedge sampling is to set up a distribution on the vertices ( as potential wedge centers ) and use that to sample the actual wedges .",
    "designing a serial algorithm is easier , since the information to compute the distribution and then form the wedges is all local . in the mapreduce implementation , edges are distributed arbitrarily ; therefore , it takes several passes to compute the necessary distribution , create the sample wedges , and finally check if they are closed .",
    "additionally , we show that mapreduce enables computing multiple clustering coefficients for the same graph ( e.g. , binned by degree ) for essentially the same cost as computing the single global clustering coefficient .",
    "since the clustering coefficient generally varies with degree , it is helpful to see the profile versus a single value because these profiles are useful in graph characterization and modeling .",
    "we give extensive experimental results , on both real - world networks from the laboratory for web algorithms as well as artificial networks created according to the graph500 benchmark .",
    "we have multiple examples with more than a billion edges .",
    "to the best of our knowledge , these are the largest triangle computations to date .",
    "results demonstrate the efficiency of our algorithm .",
    "for instance , we estimate the cost of computing clustering coefficients per ( logarithmically ) binned degree to be an average of 0.33 seconds per million edges plus overhead , which is approximately 225 seconds total for our 32-node hadoop cluster .",
    "hence , a graph with over 9b edges requires less than one hour of computation .",
    "note that global clustering coefficient and total triangles are also computed .",
    "a straightforward implementation requires that the entire edge list be `` shuffled '' three times .",
    "we show how to greatly reduce the shuffle volume with some clever implementation strategies that are able to filter the edge list during the `` map '' phase .",
    "we discuss the implementation details and show comparisons of performance .",
    "a feature of wedge - based sampling is that the closed wedges are uniform random triangles .",
    "hence , we also give experimental results characterizing triangles in terms of the their minimum and maximum degrees .",
    "triangles from social networks tend to be somewhat assortative whereas triangles from other types of networks are not .",
    "enumeration algorithms for finding triangles are either node- or edge - centric .",
    "node - centric algorithms iterate over all nodes and , for each node @xmath0 , check all pairs among the neighbors of @xmath0 for being connected .",
    "edge - centric algorithms , on the other hand , go over all edges @xmath1 and seek common neighbors of @xmath2 and @xmath0 .",
    "chiba and nishizeki  @xcite proposed a node - centric algorithm that orders the vertices by degree and processes each edge only once , by its lower - degree vertex .",
    "they showed that this algorithm runs in @xmath3-time , where @xmath4 is the number of edges , and @xmath5 is the arboricity of the graph @xmath6 ( arboricity is defined as the minimum number of forests into which its edges can be partitioned and can be considered as a measure of how dense the graph is ) .",
    "schank and wagner  @xcite used the same idea for their _ forward _ algorithm .",
    "chu and cheng studied an i / o efficient implementation of the same algorithm  @xcite .",
    "latapy proved that the forward algorithm runs in @xmath7-time and proposed improvements that reduce the search space @xcite .",
    "latapy also showed that the runtime of this algorithm becomes @xmath8 for graphs with power - law degree distributions , where @xmath9 is the power - law coefficient and @xmath10 is the number of vertices  @xcite .",
    "arifuzzaman et al .",
    "@xcite give a massively parallel algorithm for computing clustering coefficients .",
    "pearce et al .",
    "@xcite used triangle counting as an application to show the effectiveness external memory algorithms for massive graph analysis .",
    "enumeration algorithms however , can be expensive , due to the extremely large number of triangles ( see e.g. , ) , even for graphs even of moderate size ( millions of vertices ) . much theoretical work has been done on characterizing the hardness of exhausting triangle enumeration and finding weighted triangles  @xcite .",
    "eigenvalue / trace based methods adopted by tsourakakis  @xcite and avron  @xcite compute estimates of the total and per - degree number of triangles .",
    "however , the compute - intensive nature of eigenvalue computations ( even just a few of the largest magnitude ) makes these methods intractable on large graphs .",
    "most relevant to our work are sampling mechanisms .",
    "tsourakakis et al .",
    "@xcite initiated the sparsification methods , the most important of which is doulion  @xcite .",
    "this method sparsifies the graph by retaining each edge with probability @xmath11 ; counts the triangles in the sparsified graph ; and multiplies this count by @xmath12 to predict the number of triangles in the original graph .",
    "one of the main benefits of doulion is its ability to reduce large graphs to smaller ones that can be loaded into memory .",
    "however , the estimates can suffer from high variance  @xcite .",
    "theoretical analyses of this algorithm ( and its variants ) have been the subject of various studies  @xcite .",
    "another sampling approach has been proposed by kolountzakis et al .",
    "@xcite , which involves both edge and triple - node sampling ( a generalization of wedge - sampling ) .",
    "a mapreduce implementation of their method could potentially use many of the same techniques presented here .",
    "alternative sampling mechanisms have been proposed for streaming and semi - streaming algorithms @xcite .",
    "most recently , jha et al .",
    "@xcite showed how wedge - sampling can be performed when the graph is observed as a stream of edges and generalized their method for graphs with repeated edges  @xcite .",
    "an alternative approach and its parallelization were proposed by tangwongsan et al .",
    "many of these sampling procedures given above are by their very nature quite amenable to a mapreduce implementation .",
    "the wedge - sampling approach used in this paper , first discussed by schank and wagner  @xcite , is a sampling approach with the high accuracy and speed advantages of other sampling - based methods ( like doulion ) but a hard bound on the variance .",
    "previous work by a subset of the authors of this paper  @xcite presents a detailed empirical study of wedge sampling .",
    "it was also shown that wedge sampling can compute a variety of triangle - based metrics including degree - wise clustering coefficients and uniform randomly sampled triangles .",
    "this distinguishes wedge sampling from previous sampling methods that can only estimate the total number of triangles .",
    "mapreduce @xcite is a conceptual programming model for processing massive data sets .",
    "the most popular implementation is the open - source apache hadoop @xcite along with the apache hadoop distributed file system ( hdfs ) @xcite , which we have used in our experiments .",
    "mapreduce assumes that the data is distributed across storage in roughly equal - sized blocks .",
    "the mapreduce paradigm divides a parallel program into two parts : a _ map _ step and a _ reduce _ step . during the map step ,",
    "each block of data is assigned to a _ mapper _ which processes the data block to emit key - value pairs .",
    "the mappers run in parallel and are ideally local to the block of data being processed , minimizing communication overhead . in between the map and reduce steps , a parallel _ shuffle",
    "_ takes place in order to group all values for each key together .",
    "this step is hidden from the user and is extremely efficient . for every key",
    ", its values are grouped together and sent to a _",
    "reducer _ , which processes the values for a single key and writes the result to file .",
    "all keys are processed in parallel .",
    "mapreduce has been used for network and graph analysis in a variety of contexts .",
    "it is a natural choice , if for no other reason than the fact that it is widely deployed  @xcite .",
    "pegasus @xcite is a general library for large - scale graph processing ; the largest graph they considered was 1.4 m vertices and 6.6 m edges and the pagerank analytic , but they did not report execution times .",
    "lin and schatz @xcite propose some special techniques for graph algorithms such as pagerank that depend on matrix - vector products .",
    "mapreduce sampling - based techniques that reduce the overall graph size are discussed by lattanzi et al .",
    "@xcite .    in terms of triangle counting and computing clustering coefficients",
    ", cohen @xcite considers several different analytics including triangle and rectangle enumeration .",
    "plantenga @xcite has studied subgraph isomorphism ( i.e. , finding small graph patterns such as triangles ) , including cohen s algorithm as a special case .",
    "( we use plantenga s implementation of cohen s triangle enumeration algorithm for comparison in our subsequent numerical results . ) for a non - triangle pattern , plantenga s sgi code ran on a 7.6b vertex graph with 107b undirected edges in 620 minutes on a 64-node hadoop cluster .",
    "wu et al .",
    "@xcite have also studied triangle enumeration using mapreduce with running times of roughly 175 seconds on a graph with 1.6 m nodes and 5.7 m edges .",
    "suri and vassilvitskii @xcite proposed a mapreduce implementation for exact per - node clustering coefficients .",
    "most naive partitioning schemes do not give efficient parallelization because of high - degree vertices , and their result involves new partitioning methods to avoid this problem .",
    "we discuss how both @xcite and @xcite compare to our method in .",
    "sahad @xcite has a hadoop program that uses sampling techniques based on graph coloring to find subgraphs , but is limited to tree patterns .",
    "ugander et al .",
    "@xcite analyzed the facebook graph with 721 m nodes and 69b edges ( representing friendships ) on a 2,250 node hadoop cluster .",
    "they sampled 500,000 nodes and computed the exact local clustering coefficient for each sampled node .",
    "they reported ( binned ) averages of the local clustering coefficients .",
    "we note that the binned local clustering coefficient is different than the binned global clustering coefficient which we calculate in this paper .",
    "additionally , the approach of computing the exact local clustering coefficient for a set of sample nodes is non - trivial for general graphs since assembling the neighbors for high - degree nodes is extremely expensive . in the case of the facebook graph",
    ", the maximum number of neighbors is only 5,000 ( per facebook policy ) ; compare to our graphs which have nodes with over 1 million neighbors .",
    "let @xmath13 be an undirected graph with @xmath14 nodes and @xmath15 undirected edges .",
    "we assume the vertices are indexed by @xmath16 .",
    "let @xmath17 denote the degree of vertex @xmath18 ; degree - zero vertices are ignored .",
    "wedge _ is a length-2 path .",
    "let @xmath19 denote the number of wedges centered at vertex @xmath18 ; i.e. , @xmath20 .",
    "a wedge is _ closed _ if its endpoints are connected and _",
    "open _ otherwise .",
    "the _ center _ of a wedge is the middle vertex .",
    "a _ triangle _ is a cycle with three vertices .",
    "a closed wedge forms a triangle ; conversely , a triangle corresponds to _ three _ closed wedges .",
    "let @xmath21 denote the number of triangles containing node @xmath18 , which is equal to the number of closed wedges centered at node @xmath18 .",
    "the _ node - level clustering coefficient _ ( first used in @xcite ) is @xmath22 thus , @xmath23 measures how tightly the neighbors of a vertex are connected amongst themselves .",
    "we define @xmath24 to be the set of all wedges in @xmath6 and @xmath25 .",
    "we partition @xmath24 into two disjoint subsets as follows : @xmath26 the subscript of 3 for the closed wedges indicates that each triangle creates 3 wedges in @xmath27 .",
    "let @xmath28 denote the total number of triangles ( since each triangle is counted thrice ) .",
    "the _ ( global ) clustering coefficient _ ( also known as the transitivity ) @xcite of an undirected graph is given by @xmath29 at the global level , @xmath30 is an indicator of how tightly nodes of the graph are connected .      in this paper",
    ", we will be using the binned degree - wise clustering coefficients , which measure how tightly the neighborhood of vertices of a specified degree group are connected .",
    "let @xmath31 be a subset of degrees ( recall that we ignore degree - zero nodes ) .",
    "we define @xmath32 and @xmath33 . in many cases ,",
    "we are interested in a single degree , i.e. , if @xmath34 then @xmath35 is the set of nodes of degree @xmath36 and @xmath37 is the number of nodes of degree @xmath36 .",
    "we define @xmath38 to be the set of all wedges centered at a node in @xmath39 and @xmath40 to be the total number of wedges centered at nodes in @xmath39 , i.e. , @xmath41 .",
    "we partition the set @xmath38 into four disjoint subsets as follows : @xmath42 define @xmath43 for @xmath44 .",
    "since @xmath45 , we can define _ binned degree - wise clustering coefficient _ ,",
    "@xmath46 , as the fraction of closed wedges in @xmath38 ; i.e. , @xmath47 the formula for triangles is more complex and given by @xmath48 since for each triangle there is either one wedge in @xmath49 , two wedges in @xmath50 or three wedges in @xmath51 . shows examples of these quantities when the bins are all singletons : @xmath52 .",
    "\\(1 ) at ( 0,0 ) [ nd ] 1 ; ( 2 ) at ( 3,2 ) [ nd ] 2 ; ( 3 ) at ( 3,-2 ) [ nd ] 3 ; ( 4 ) at ( 6,1 ) [ nd ] 4 ; ( 5 ) at ( 8,-2 ) [ nd ] 5 ; ( 6 ) at ( 9,1.5 ) [ nd ] 6 ; ( 1 ) to ( 2 ) ; ( 1 ) to ( 3 ) ; ( 2 ) to ( 4 ) ; ( 3 ) to ( 4 ) ; ( 3 ) to ( 5 ) ; ( 4 ) to ( 5 ) ; ( 4 ) to ( 6 ) ; at ( 10.25,2.25 ) @xmath53 , @xmath54 , @xmath55 + @xmath56 , @xmath57 + @xmath58 , @xmath59 + @xmath60 , @xmath61 + @xmath62 , @xmath63 + @xmath64 , @xmath65 ;",
    "for a more detailed exposition of wedge sampling and empirical tests of its behavior , we refer the reader to  @xcite . for completeness , we review the relevant concepts and calculations here .",
    "the following result is a simple corollary of hoeffding s inequality @xcite ( refer to theorem 1.5 in @xcite ) ; the proof can be found in @xcite .",
    "we say that @xmath66 is the _ error _ and @xmath67 is the _",
    "confidence_.    [ thm : hoeffding ] let @xmath68 be independent random variables with @xmath69 for all @xmath70 . define @xmath71",
    "let @xmath72}$ ] .",
    "for any positive @xmath73 , setting @xmath74 yields @xmath75      the strategy for computing the clustering coefficient per degree ( or degree range ) is similar to that described for the degree - wise clustering coefficient in @xcite .",
    "[ thm : cd - est ] for @xmath76 , set @xmath77 . for @xmath70 , choose wedge @xmath78 uniformly at random ( with replacement ) from @xmath38 and let @xmath79 be defined as @xmath80 then + @xmath81    observe that @xmath82}$ ] since it is the probability that a random wedge in @xmath38 is closed .",
    "the proof follows immediately from .",
    "[ [ choosing - uniform - random - wedges ] ] choosing uniform random wedges + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we do not want to form all wedges explicitly .",
    "instead , we _ implicitly _ generate random wedges .",
    "observe that the number of wedges centered at vertex @xmath18 is exactly @xmath83 , and @xmath84 .",
    "that leads to the following procedure . to select a random wedge , recall that @xmath85",
    "therefore , first choose vertex @xmath86 with probability @xmath87 .",
    "second , choose two distinct neighbors of vertex @xmath18 to form a random wedge . to set up this distribution , we need to compute the degree distribution .",
    "if @xmath88 ( a singleton ) , then all nodes in @xmath35 are equally probable . if @xmath89 , then the weight of vertex @xmath18 is @xmath90 .",
    "estimating the number of triangles is slightly more complicated since each closed wedge may have 1,2 , or 3 vertices in @xmath39 .",
    "[ thm : td - est ] let the conditions of hold . for each @xmath78 , let @xmath91 be defined as @xmath92 then @xmath93    we claim @xmath94 } = t_d$ ] .",
    "suppose that @xmath95 is selected from @xmath38 uniformly at random .",
    "observe that @xmath96 }       &      = { { \\rm prob}\\left\\{w \\in w_{d,1}\\right\\ } }       + \\frac{{{\\rm prob}\\left\\{w \\in w_{d,2}\\right\\}}}{2 }       + \\frac{{{\\rm prob}\\left\\{w \\in w_{d,3}\\right\\}}}{3 } \\\\      &      = 1 \\cdot \\frac{p_{d,1}}{p_d }      + \\frac{1}{2 } \\cdot \\frac{p_{d,2}}{p_d }      + \\frac{1}{3 } \\cdot \\frac{p_{d,3}}{p_d } \\\\      &",
    "= t_d / p_d ,    \\end{aligned}\\ ] ] per .",
    "hence , from we have @xmath97 and the theorem follows by multiplying the inequality by @xmath40 .",
    "in addition to knowing the number of triangles in a graph , it may also be interesting to consider the properties of those triangles .",
    "for instance , durak et al .",
    "@xcite consider the differences in node degrees in a triangle .",
    "it turns out that the closed wedges discovered during the wedge sampling procedure are triangles sampled uniformly ( with replacements ) .",
    "hence , we can study these randomly sampled triangles to estimate the overall characteristics of triangles in the graph .",
    "let @xmath98 be a random sample of the wedges of a graph @xmath6 , and let @xmath99 triangles that are formed by the closed wedges in @xmath98 .",
    "then each triangle in @xmath100 is a uniform random sample from the triangles of @xmath6 .",
    "the proof depends on observing that a triangle being chosen depends only on one of its 3 wedges being chosen .",
    "since the wedge sample is uniformly random , each triangle is equally likely to be picked , and there is no dependency between any pair of triangles , which implies a uniform sample .",
    "earlier work by a subset of the authors  @xcite provides a thorough study on how the techniques described above perform in practice .",
    "as expected , tremendous improvements are achieved in runtimes compared to full enumeration , especially for large graphs , since the number of samples is independent of graph size . specifically , we see speed - ups of more than 1000x with errors in the clustering coefficient of less than 0.002 .",
    "additionally , in comparison to the doulion method ( an edge - sampling technique ) we obtain speed - ups of 5x or more while obtaining the same accuracy . the ability to adapt our wedge - sampling method to computing binned degree - wise clustering coefficients and triangle sampling",
    "are also benefits in comparison to edge - based sampling .",
    "our goal in this work is to implement the wedge sampling approach within the mapreduce framework and provide evidence that it can scale to much larger problems .",
    "we now present a mapreduce algorithm for estimating the clustering coefficients and number of triangles in a graph . for details on mapreduce",
    ", we refer the reader to lin and dyer @xcite ; we have emulated their style in our algorithm presentations .",
    "we use the open - source hadoop implementation of mapreduce , and the hadoop distributed file system ( hdfs ) for storing data .",
    "each mapreduce job takes one or more distributed files as input .",
    "these files are automatically stored as _",
    "( also known sometimes as blocks ) , and one mapper is launched per split . the mappers produce key - value pairs . all values with the same key are sent to the same reducer .",
    "the number of reducers is specified by the user .",
    "each mapreduce job produces a single hdfs output file .",
    "a mapreduce job accepts _ configuration parameters _ , which are passed along as data to the mapper and reducer functions ; we discuss these in more detail in the sections that follow .",
    "the set of mapreduce jobs in our algorithm is coordinated by a hadoop java program running on a single _",
    "client node_.    in our code , we assume the nodes are binned by degree as discussed in .",
    "computation of the global clustering coefficient is a special case which can be computed by either looking only at a single bin containing all degrees or using a weighted average of the binned clustering coefficients ( see ) .",
    "our input is an undirected edge list where the node identifiers are 64-bit integers ; we assume no duplicates or self - edges and no particular ordering .",
    "we divide our mapreduce algorithm into three _",
    "major phases _ plus post - processing , as presented in .",
    "each major phase makes a complete pass through the edge list .",
    "the first phase sets up the distribution on wedges .",
    "the second phase creates the sample wedges .",
    "finally , the third phase checks whether or not the sample wedges are closed . in all three phases , we have strategies to reduce the data volume in the shuffle phase ( between the map and reduce ) , discussed in detail in the sections that follow .",
    "we define degree bins in a parameterized way as follows .",
    "let @xmath101 be the number of singleton bins , and let @xmath102 be the rate of growth on the bin sizes .",
    "the first @xmath101 bins are singletons containing degrees @xmath103 respectively .",
    "the remaining bins grow exponentially in size .",
    "we describe the lowest degree of bin @xmath104 as @xmath105 the highest degree for bin @xmath104 is just one less than the lowest degree of bin @xmath106 . for a given degree @xmath36",
    ", we can easily look up its bin as @xmath107 in our implementation , @xmath101 and @xmath108 are communicated to each mapreduce job as configuration parameters .    for @xmath109 and @xmath110 ,",
    "the bins are @xmath111 , @xmath112 , @xmath113 , @xmath114 , @xmath115 , @xmath116 , and so on .",
    "note that the bin @xmath111 can not have any wedges , so we just ignore it .",
    "let @xmath117 be an upper bound on the highest degree for a given graph .",
    "then choosing @xmath118 and @xmath119 yields bins @xmath120 . in other words",
    ", we have a single bin containing all vertices ( excepting degree-1 vertices ) . on the other hand , choosing @xmath121 yields @xmath122 . here , every bin is a singleton .",
    "we are not constrained to equation for computing the bins ; we can use any procedure such that each degree is assigned to a single bin .",
    "likewise , is optional and used to reduce the shuffle volume in phase 2c .",
    "phase 1a is a straightforward mapreduce task  computing the degree of each vertex .",
    "the map and reduce functions are described in .",
    "the input is the * edge list file * ; each entry is a pair of vertex ids @xmath123 that define an edge .",
    "the map function is called for each edge @xmath123 and emits two key - value pairs keyed to the vertex ids and having a value of 1 .",
    "the reduce function gathers all the values for each vertex and sums them to determine the degree .",
    "the final output to hdfs is a * vertex degree file * ; each entry is of the form @xmath124 where @xmath0 is a vertex i d and @xmath36 is its degree .",
    "emit@xmath125 emit@xmath126 @xmath127 emit@xmath128    shows a simple version of the code . to make the code more efficient",
    ", we collect local counts within each mapper ( using a java ` map ` container ) and emit the totals .",
    "this technique is called an in - memory combiner @xcite .",
    "we found the in - memory combiner to reduce shuffle volume more than employing the reducer as a combiner .",
    "phase 1b works with the output of phase 1a ( * vertex degree file * ) to compute the number of wedges per bin .",
    "the map and reduce functions for phase 1b are presented in .",
    "the input is the list of degrees per vertex .",
    "the map function is called for each vertex ( with its associated degree ) and emits the number of wedges for that vertex , keyed to the appropriate bin .",
    "the reduce function simply combines the results for each bin .",
    "the final output is a * wedges per bin file * ; each entry is of the form @xmath129 where @xmath130 is the bin i d , @xmath131 is the number of vertices in the bin , and @xmath132 is the number of wedges in the bin .    * parameters : * @xmath133 @xmath134 @xmath135 @xmath136 emit@xmath137 @xmath138",
    "@xmath139 emit@xmath140    once again , we have shown a simple version of the algorithm in . to make the code more efficient",
    ", we collect local counts within each mapper ( using a java ` map ` container ) and emit the totals .    for the case of a single bin , strictly speaking ,",
    "phase 1b is unnecessary .",
    "instead , we could have used a hadoop _ global counter _ to tally the total wedges in the reduce step of phase 1a .      from the * wedges per bin",
    "file * ( output of phase 1b ) , we create a * wedges per bin object * , which acts as a function @xmath141 such that @xmath142 is the number of wedges in bin @xmath130 .",
    "the work is performed entirely in our main program running on the client node .",
    "it reads phase 1b output from hdfs , stores wedges per bin values in a java ` map ` container , and launches the next mapreduce job ( phase 2a ) , sending the _ serialized _ container as a configuration parameter .",
    "the input to phase 2a is the * vertex degree file * along with the * wedges per bin object * , which is passed as a configuration parameter .",
    "phase 2a calculates the number of sample wedges centered at each vertex . the map function is shown in .",
    "the map function is called for each ( vertex i d , degree ) pair . from this",
    ", we can calculate the expected number of wedges that would be sampled from the vertex for a uniform random sample , @xmath143 .",
    "this number is unlikely to be integral .",
    "rounding up would produce far too many wedges .",
    "instead , we use probabilistic rounding .",
    "for instance , if @xmath144 , then there is a 10% change of producing @xmath145 wedges and a 90% chance of producing no wedges , @xmath146 .",
    "we are only off by at most one , so if @xmath147 , then there is a 10% change of producing @xmath148 wedges and a 90% chance of producing @xmath145 wedge .",
    "hence , the expected number of wedges for this vertex is exactly @xmath143",
    ". only vertices with at least one sample wedge are emitted .",
    "the final output is a * wedge centers file * ; each entry is of the form @xmath149 where @xmath0 is the vertex i d , @xmath36 is the vertex degree , @xmath150 is the number of sample wedges centered at that vertex , and @xmath11 is the total number of wedges in the bin containing @xmath0 .",
    "the reduce function is just the identity map and is not shown .    * parameter : * @xmath104 * parameter : * @xmath141 @xmath151 @xmath152 @xmath153 @xmath154)$ ]",
    "@xmath155 emit@xmath149      phase 2b is an optional step that generates a java ` map ` of wedge centers and their bin ids based on the output of phase 2a ( * wedge centers file * ) .",
    "we represent this object as a function @xmath156 such that @xmath157 this * wedge centers object * has one value for every vertex appearing in a wedge center .",
    "it is serialized and passed as a configuration parameter to phase 2c , where it is used to filter the edges that are emitted by the map function .",
    "note that hadoop imposes a limit on the size of the configuration parameters ( 5 mb by default ) .",
    "if the number of wedge centers is too large ( a few hundred thousand samples will exceed 5 mb ) , then other options must be explored .",
    "one alternative is to pass the container to the map tasks using the hadoop distributed cache ; however , we have not implemented this idea .",
    "phase 2b is optional , and can be skipped if there are too many wedge centers .",
    "we demonstrate the benefits of this step in .      in phase 2c ,",
    "the goal is to take each sample wedge center ( from the * wedge center file * ) , collect its neighbors ( from the * edge list file * ) , and create a set of sample wedges .",
    "we merge each vertex and its neighbors at the reduce phase .",
    "if it exists , the optional * wedge center object * is used to filter the edges that are shuffled , ignoring all edges that are not adjacent to a sampled wedge center .",
    "the algorithm is shown in .",
    "for clarity , we give a separate map function for each input type . in the actual implementation",
    ", we have to determine the input type on the fly , because both input files are of hadoop type ` text ` . for input from the * wedge centers",
    "file * , the map function simply passes along its degree and sample wedge count ( i.e. , the number of wedges to be sampled from the vertex ) .    for input from the * edge list file",
    "* , the map function checks to see if the edge is adjacent to a wedge center .",
    "if so , it is passed based on the outcome of a random coin flip .",
    "the aim of the reduce phase is to generate random wedges centered at a vertex ( say @xmath0 ) .",
    "the most nave map implementation would forward all edges incident to @xmath0 , so that wedges can be selected from them .",
    "a major problem with this is that if the number of samples @xmath104 is much less than the degree of @xmath0 , most of the communication is unnecessary .",
    "for example , the highest degree vertices of a social network graph might link to millions of edges , but @xmath104 is in the tens of thousands or less ; therefore , most of the incident edges will not participate in sampled wedges centered at these vertices .",
    "we have a probabilistic fix to address this situation .",
    "we do not have the vertex degree readily available , but we do know its bin and therefore a lower bound on its degree . consider a vertex @xmath0 of degree greater than @xmath158 , where @xmath159 .",
    "we send just some of the incident edges to @xmath0 , with independent probability @xmath160 .",
    "then the expected number of edges to send is @xmath161 .",
    "note that this expectation is at least @xmath162 .",
    "getting less than @xmath163 edges is potentially disastrous , but the probability of this is minuscule . by a multiplicative chernoff bound ( given below ) ,",
    "the probability of such an event is @xmath164 . for @xmath165 ( a tiny sample size ) ,",
    "the probability is less than @xmath166 .",
    "let @xmath167 , where each @xmath79 is independently distributed in @xmath168 $ ] .",
    "then @xmath169 } \\right\\ } } \\leq \\exp(-\\delta^2{\\mathbb{e}[x ] } /2).\\ ] ]    if @xmath170 is not too far from @xmath158 , then the expectation @xmath161 is potentially much smaller than @xmath170 .",
    "hence , we get the desired number of random edges without sending too many .    even with this improvement , the data passed forward may be too large to fit into the reducer s memory .",
    "we use a feature of hadoop called _ secondary sort _ to ensure that the data arrives pre - sorted .",
    "note that the key used for passing along the vertex information is @xmath171 and the key for the edges is of the form @xmath172 , where @xmath173 is a random positive integer .",
    "this data is all mapped to the key @xmath0 , but the values following the colon control the sort of the values associated with @xmath0 .",
    "the secondary key of zero ensures that the degree and wedge count data are first .",
    "the secondary keys for edges ( @xmath173 ) ensure that the adjacent edges are randomly sorted ; otherwise , hadoop would present the edges in their order of arrival , which could bias the selection .",
    "* parameters : * @xmath133 * parameter : * @xmath104 * parameter : * @xmath156 emit@xmath174 edgehelper(@xmath175 ) edgehelper(@xmath176 ) @xmath177 @xmath178 @xmath179 @xmath180 @xmath181 @xmath182)$ ] @xmath183 emit@xmath184 @xmath185 @xmath186 @xmath187 @xmath188 emit@xmath189 @xmath190 @xmath191 @xmath192 @xmath193 define mapping @xmath194 renumber from 1 to @xmath195 @xmath195 and pairs @xmath196    from the secondary sort , the wedge center must be first in the values list at the reduce phase , if it exists . if it does not exist",
    ", then there is nothing to do .",
    "recall that for each wedge center @xmath0 , we have its degree , @xmath36 , and a desired number of wedge samples , @xmath150",
    ". each wedge must be randomly sampled _ with _ replacement .",
    "the two edges of a single wedge are sampled _ without _ replacement .",
    "so , wedge sampling requires a minimum of 2 and a maximum of @xmath197 edges . if @xmath198 , some edges are necessarily reused .",
    "if @xmath199 , it is more likely that every wedge centered at @xmath0 will have two unique edges ; however , due to the birthday paradox , there remains a non - negligible likelihood of wedge overlap even for large @xmath36 .",
    "for these large @xmath36 , we want to avoid reading all neighbors into memory since the list is quite long , but we still want to accurately reproduce uniform sampling with replacement .",
    "we do this by using a simulated sampling procedure explained below .",
    "it only requires reading the first @xmath195 neighbors into memory where @xmath200 .",
    "procedure sampling produces @xmath150 uniform random wedges ( with replacement ) centered at @xmath0 .",
    "number the edges incident to @xmath0 arbitrarily from @xmath201 to @xmath36 .",
    "a uniform random wedge is represented as a uniform random pair of indices @xmath202 ( @xmath203 ) .",
    "we can repeat this random index selection @xmath150 times to implicitly sample @xmath150 random wedges , each of which is just represented as a pair of indices .",
    "observe that the total number of indices in the union of these pairs is at most @xmath195 , so all we need are the the first @xmath195 uniform randomly ordered edges obtained as the output of the map phase . we map these sampled wedge indices randomly to the index set @xmath204 through a permutation .",
    "now , each wedge is indexed as a pair @xmath202 ( @xmath205 ) . from the list of edges",
    "/ neighbors @xmath206 , we can generate these random edges .",
    "this is what is done in sampling and reduce in .",
    "the final output of this phase is a * sample wedge list file * , where each entry is of the form @xmath207 .",
    "the number @xmath208 is a hash of the desired closure edge @xmath209 ( a key which allows the undirected edges from the edge list to be correctly matched with the closure requests in phase 3b.g ) , the wedge is defined by @xmath210 , @xmath11 is the total number of wedges in the bin containing @xmath211 , and @xmath212 is the degree of vertex @xmath211 .    as mentioned above",
    ", phase 2b is optional .",
    "if skipped , the mapreduce shuffle brings adjacent edges of a wedge center together in the reduce phase .",
    "we defer calculation of the number of sample wedges to the reduce phase , but otherwise proceed as defined above .",
    "note that in many cases the reducer collects zero samples and does no work .",
    "phase 3a ( optional ) assembles a list of all the unique edge hashes from the * sample wedges file * and stores it as a java ` set ` object .",
    "we denote this * wedge hashes object * by @xmath213 .",
    "this is similar to the procedure in phase 2b , which assembles the list of wedge centers .",
    "we set @xmath214 if phase 3a is skipped .",
    "phase 3b is the last major step and checks the wedge closures , as shown in .",
    "the inputs are the * sample wedges file * created by phase  2c and the original * edge list file*. we also pass the optional * wedge hashes object * ( @xmath215 ) as a configuration parameter . if @xmath215 is nonempty , it is used to filter the edges passed forward to the reduce function .",
    "( note that we could skip phase 3a and forward every edge forward to the reducers , but this would result in much greater data shuffling in phase 3b . ) note that more than one edge may hash to the same value ; hence , we loop through all edges that arrive at the reducer to verify that there is a match before declaring a wedge as closed .",
    "likewise , more than one wedge may be closed by a single edge .",
    "the output of this phase is the * results file ( ver .",
    "0 ) * ; each entry is of the form ( @xmath216 ) where @xmath217 indicates if the wedge is open or closed and everything else is the same as for the * sample wedges file*.    * parameter : * @xmath218 emit(@xmath208 , @xmath219 ) @xmath220 emit(@xmath208 , @xmath221 ) sort the values @xmath222 into @xmath223 ( edges ) and @xmath224 ( wedges ) @xmath225 @xmath226 @xmath227 @xmath228 emit(@xmath216 )        phases 4a & 4b augment each sample wedge with the degrees of @xmath229 and @xmath230 .",
    "this information is needed for estimating the number of triangles per bin .",
    "if only the clustering coefficients are required , these two steps can be omitted .",
    "shows phase 4a ; the procedure for phase 4b is analogous and so is omitted .",
    "the final output of phase 4b is the * results file ( ver .",
    "2 ) * ; each line is of the form ( @xmath231 ) where @xmath232 and @xmath233 are the degrees of vertices @xmath229 and @xmath230 , respectively , while the remainder is the same as for the * results file ( ver .",
    "0)*.    emit(@xmath234 , @xmath235 ) emit(@xmath236 , @xmath36 ) @xmath237 emit(@xmath238 )      * parameters : * @xmath133 @xmath239 @xmath240 @xmath241 emit(@xmath130 , ( @xmath242 ) ) @xmath243 @xmath244 @xmath245 @xmath246 @xmath247 @xmath248 @xmath249 emit(@xmath250 )    phase 4c tallies the final results per bin , using the logic in .",
    "its output is the * summary file*. each line is of the form @xmath250 where @xmath130 is the bin i d , @xmath251 is the number of open wedges , @xmath252 is the number of closed wedges with @xmath18 vertices in the bin , @xmath30 is the clustering coefficient estimate , @xmath11 is the number of wedges in the bin , and @xmath253 is the estimated number of triangles with one or more vertices in the bin .",
    "we can estimate the global clustering coefficient from the degree - binned clustering coefficients as follows .",
    "let @xmath254 and @xmath132 be the clustering coefficient estimate and total number of wedges for bin @xmath130 .",
    "let @xmath255 be the total number of wedges .",
    "then the estimates for the global clustering coefficient and total number of triangles are given by @xmath256 let @xmath257 denote the total number of bins .",
    "we assume that every bin has @xmath104 samples producing an error bound of @xmath66 with confidence @xmath67 .",
    "then we argue that @xmath258 with confidence @xmath259 .",
    "presents the input , communication , and output volume for each phase .",
    "let @xmath10 denote the number of nodes , @xmath4 denote the number of edges ( each undirected edge is counted just once ) , @xmath150 denote the total number of sampled wedges , and let @xmath257 denote the number of bins .",
    "note that the communications in phases 2c and 3b can be substantially higher ( @xmath260 ) if the phase 2b or 3a is skipped .",
    "our experimental results show that phase 1a is by far the most expensive , which is consistent with our performance analysis because phase 1a communicates the most data , @xmath261 key - values pairs .",
    "all other communications are size @xmath10 or @xmath150 .",
    "1a & @xmath262 : process @xmath4 edges from * edge list file*. & @xmath263 : communicate 2 key - value pairs per edge . &",
    "@xmath264 : output @xmath10 vertex - degree pairs to * vertex degrees file*. + 1b & @xmath264 : process @xmath10 vertex - degree pairs from * vertex degrees file*. & @xmath264 : communicate 1 key - value pair per vertex . & @xmath265 : output data for each bin to * wedges per bin file*. + 2a & @xmath264 : process @xmath10 vertex - degree pairs from * vertex degrees file*. & + 2c & @xmath266 : process @xmath4 edges from * edge list file * and approximately @xmath150 sample wedge centers from * wedge centers file*. & @xmath267 : communicate @xmath268 key - value pair per sample wedge center .",
    "& @xmath269 : output the sample wedges to the * sample wedges file*. + 3b & @xmath270 : process wedges in * sample wedges file * and edges from * edge list file*. & @xmath271 : communicate 1 message per wedge and 1 message per hash - matching edge . &",
    "@xmath269 : output close / open data for each sample wedge into * results file ( ver . 0)*.",
    "+ 4a/4b & @xmath272 : process @xmath150 sampled wedges from * results file ( ver .",
    "0/1 ) * and @xmath10 vertex - degree pairs to * vertex degrees file*. & @xmath272 : communicate 1 key - value pair for each vertex and each edge .",
    "& @xmath269 : output augmented data for each sample wedge into * results file ( ver . 1/2)*. + 4c & @xmath269 : process @xmath150 sampled wedges from * results file ( ver .",
    "2)*. & @xmath273 : communicate 3 key - values pairs per wedge . &",
    "@xmath265 : output results per bin in * summary files*. +",
    "we obtained real - world graphs from the laboratory for web algorithms ( http://law.di.unimi.it/datasets.php ) , which were compressed using llp and webgraph @xcite .",
    "we selected ten larger graphs for which the complete edge lists were available .",
    "we also consider three artificially - generated graphs according to the graph500 benchmark  @xcite , which uses stochastic kronecker graphs ( skg )  @xcite for its graph generator with [ 0.57,0.19;0.19,0.05 ] as the @xmath274 generator matrix .",
    "we have added noise with a parameter of 0.1 , as proposed in  @xcite to avoid oscillatory degree distributions .",
    "these graphs are generated in mapreduce .",
    "all networks are treated as undirected for our study ; in other words , if @xmath275 , @xmath276 , or both , we say that edge @xmath277 exists . briefly , the networks are described as follows .",
    "amazon-2008 @xcite : a graph describing similarity among books as reported by the amazon store .",
    "ljournal-2008 @xcite : nodes represent users on livejournal .",
    "node @xmath278 connects to node @xmath173 if @xmath278 registered @xmath173 as a friend .",
    "hollywood-2009 , hollywood-2011 @xcite : this is a graph of actors .",
    "two actors are joined by an edge whenever they appear in a movie together .",
    "twitter-2010 @xcite : nodes are twitter users , and node @xmath278 links to node @xmath173 if @xmath173 follows @xmath278 .",
    "it-2004 @xcite : links between web pages on the .it domain , provided by iit .",
    "uk-2005 - 05 , uk-2006 - 06 , uk - union-2006 - 06 - 2007 - 05 ( shorted to uk - union ) @xcite : links between web pages on the .uk domain .",
    "( we ignore the time labeling on the links in the last graph . )",
    "sk-2005 @xcite : links between web pages on the .sk domain .",
    "graph500 - 23 , graph500 - 26 , graph500 - 29 @xcite : artificially generated graphs according to the graph500 benchmark using the skg method . the number ( 23 , 26 , 29 ) indicates the number of levels of recursion and the size of the graph .",
    "the properties of the networks are summarized in ; specifically , we report the number of vertices , the number of _ undirected _ edges , the total number of wedges , and estimates for the total number of triangles and the global clustering coefficients , calculated according to .",
    "( to the best of our knowledge , we are the only group that has calculated the last three columns , so these numbers have not been independently validated . )",
    ".network characteristics .",
    "all edges are treated as undirected .",
    "the triangle counts and global clustering coefficients ( gcc ) are our estimates . [ cols=\"^,<,>,>,>,>,^ \" , ]     using these sampled triangles , we can look at the degrees of the vertices . each triangle has a minimum , middle , and maximum degree .",
    "we analyze the _ degree assortativity _ of the vertices of the triangles by comparing the minimum and maximum degrees in .",
    "specifically , we assign each vertex to a degree bin , using with @xmath109 and @xmath110 .",
    "we group all triangles with the same minimum degree bin together .",
    "the box plot shows the statistics of the bin for the maximum degree : the central mark ( red ) is the median max - degree , while the edges of the ( blue ) box are the 25th and 75th percentiles .",
    "the whiskers extend to the most extreme points considered not to be outliers , and the outliers ( red plus marks ) are plotted individually .",
    "observe that the social network , hollywood-2011 , shows an assortative relation between the maximum and minimum for the hollywood graph , since the two quantities rise gradually together .",
    "for the uk - union web graph on the other hand , the average maximum degree is essentially invariant to the minimum degree .",
    "these findings are consistent with the results in  @xcite about networks with high global clustering coefficients having degree assortative triangles , while this assortativity can not be observed in networks with low clustering coefficients . here",
    ", we were able to observe the same trend on these massive graphs using sampling in a much more efficient way , avoiding the enumeration burden .",
    "we see that the graph500 networks also have almost no assortativity between the minimum and maximum degrees and therefore do not have the characteristics of a social network .     +",
    "we have shown that wedge - based sampling can be scaled to massive graphs in the mapreduce framework . on a relatively small mapreduce cluster ( 32 nodes ) , we have analyzed graphs with up to 240 m edges , 8.5b edges , 5.2 t wedges , and 447b triangles .",
    "even the largest graph was analyzed in less than one hour , and most took only a few minutes .",
    "shows a timing analysis of the mapreduce tasks @xcite for phase 1a on the uk - union graph .",
    "mapper tasks run in waves of 128 parallel jobs , equal to the number of mapper slots available on the cluster .",
    "note that a larger cluster would be able to run more map tasks in parallel , decreasing the overall runtime . to the best of our knowledge ,",
    "these are the largest triangle - based calculations performed to date .        unlike enumeration techniques that need to at least validate every triangle and more often have cost proportional to the number of wedges , our method is linear in the number of edges .",
    "the most expensive component of the wedge - based sampling is finding the degree of each vertex ( phase 1a ) ; reducing the time for this is a topic for future study . on our cluster ,",
    "the time is approximately 0.33 seconds per million edges , plus a fixed cost of 225 seconds for overhead . because we are using mapreduce",
    ", we never need to fit the entire graph into memory  we only need to be able to stream through all the edges .",
    "our mapreduce implementation requires a total of eight mapreduce jobs , three of which do most of the work because they read the entire edge list ( phases 1a , 2c , and 3b ) and two of which are optional ( phases 4a and 4b , which are labeling the degrees of the sampled triangles ) .",
    "we have striven to minimize the data being shuffled in each phase by using special data structures to filter the edges .",
    "using our code , we are able to compute the degree distribution , approximate the binned degree - wise clustering coefficient and the number of triangles per bin .",
    "additionally , we can analyze the characteristics of the triangles ( e.g. degree assortativity ) . as part of our analysis",
    ", we have analyzed the graphs used in the graph500 benchmark .",
    "we are able to give a more detailed understanding of the empirical properties of the generator and compare it to real - world data ; this is potentially helpful in determining if performance on the benchmark data is indicative of performance on real - world data .",
    "we gratefully acknowledge the peer reviewers for their constructive comments which have substantially improved the paper and brought several additional references to our attention .",
    "we are thankful to jon berry for conducting the in - memory experiments described in .        , http://dx.doi.org/10.1109/sc.companion.2012.251[_poster : parallel algorithms for counting triangles and computing clustering coefficients _ ] , in high performance computing , networking , storage and analysis ( scc ) , 2012 , pp .",
    "14501450 .    , _",
    "patric : a parallel algorithm for counting triangles and computing clustering coefficients in massive networks _",
    ", ndssl technical report 12 - 042 , network dynamics and simulation science laboratory , virginia polytechnic institute and state university , july 2012 .        , http://dx.doi.org/10.1109/ipdps.2009.5161102[_implementing a portable multi - threaded graph library : the mtgl on qthreads _ ] , in ipdps 2009 : ieee international symposium on parallel & distributed processing , 2009 , pp .",
    "18 .        ,",
    "http://dx.doi.org/10.1145/1963405.1963488[_layered label propagation : a multiresolution coordinate - free ordering for compressing social networks _ ] , in www11 : proceedings of the 20th international conference on world wide web , acm press , 2011 .",
    ", http://dx.doi.org/http://doi.acm.org/10.1145/988672.988752[_the webgraph framework i : compression techniques _ ] , in www04 : proceedings of the 13th international conference on world wide web , acm press , 2004 , pp .  595602 .    , http://dx.doi.org/10.1145/1142351.1142388[_counting triangles in data streams _ ] , in pods06 : proceedings of the twenty - fifth acm sigmod - sigact - sigart symposium on principles of database systems , 2006 , pp .",
    "253262 .      , http://dx.doi.org/10.1145/1557019.1557049[_on compressing social networks _",
    "] , in kdd 09 : proceedings of the 15th acm sigkdd international conference on knowledge discovery and data mining , acm , 2009 , pp .  219228 .    ,",
    "http://dx.doi.org/10.1145/2020408.2020513[_triangle listing in massive networks and its applications _",
    "] , in kdd11 : proceedings of the 17th acm sigkdd international conference on knowledge discovery and data mining , new york , 2011 , acm , pp",
    ".  672680 .          , http://dx.doi.org/10.1145/2396761.2398503[_degree relations of triangles in real - world networks and graph models _ ] , in cikm 12 : proceedings of the 21st acm international conference on information and knowledge management , cikm 12 , new york , ny , usa , 2012 , acm , pp .  17121716 .        , http://dx.doi.org/10.1109/cason.2009.13[_a random network generator with finely tunable clustering coefficient for small - world social networks _ ] , in cason 09 : international conference on computational aspects of social networks , june 2009 , pp",
    ".  1017 .        , http://dx.doi.org/10.1145/2487575.2487678[_a space efficient streaming algorithm for triangle counting using the birthday paradox _ ] , in proceedings of the 19th acm sigkdd international conference on knowledge discovery and data mining , kdd 13 , new york , ny , usa , 2013 , acm , pp .",
    "589597 .      , http://dx.doi.org/10.1007/11533719_72[_new streaming algorithms for counting triangles in graphs _",
    "] , in cocoon05 : computing and combinatorics , l.  wang , ed .",
    "3595 of lecture notes in computer science , springer berlin heidelberg , 2005 , pp .  710716 .    , http://dx.doi.org/10.1109/icdm.2009.14[_pegasus : a peta - scale graph mining system implementation and observations _ ] , in icdm 09 .",
    "ninth ieee international conference on data mining , dec .",
    "2009 , pp .",
    "229238 .",
    ", http://dx.doi.org/10.1145/1772690.1772751[_what is twitter , a social network or a news media ? _ ] , in www 10 : proceedings of the 19th international conference on world wide web , new york , ny , usa , 2010 , acm , pp .",
    "591600 .      , http://dx.doi.org/10.1145/1989493.1989505[_filtering : a method for solving graph problems in mapreduce _ ] , in proceedings of the 23rd acm symposium on parallelism in algorithms and architectures , spaa 11 , new york , ny , usa , 2011 , acm , pp .",
    "8594 .        , _ data - intensive text processing with mapreduce _ , morgan & claypool publishers , 2010 .",
    "citing post - production version of manuscript dated sept . 6 , 2012 , available at http://lintool.github.com/mapreducealgorithms/ed1n.html .",
    ", http://dx.doi.org/10.1145/1830252.1830263[_design patterns for efficient graph algorithms in mapreduce _ ] , in mlg10 : proceedings of the eighth workshop on mining and learning with graphs , mlg 10 , new york , ny , usa , 2010 , acm , pp .  7885 .                , http://dx.doi.org/10.1145/2505515.2505563[_an efficient mapreduce algorithm for counting triangles in a very large graph _ ] , in cikm13 : proceedings of the 22nd acm international conference on conference on information & knowledge management , association for computing machinery , 2013 , pp",
    ".  539548 .            , http://dx.doi.org/10.1145/1772690.1772778[_measurement-calibrated graph models for social network experiments _ ] , in www 10 : proceedings of the 19th international conference on world wide web , 2010 , pp",
    ".  861870 .      height",
    "2pt depth -1.6pt width 23pt , http://dx.doi.org/10.1007/11427186_54[_finding , counting and listing all triangles in large graphs , an experimental study _ ] , in experimental and efficient algorithms , springer berlin / heidelberg , 2005 , pp .",
    "606609 .          , http://dx.doi.org/10.1145/1963405.1963491[_counting triangles and the curse of the last reducer _ ] , in www 11 : proceedings of the 20th international conference on world wide web , new york , ny , usa , 2011 , acm , pp .",
    "607614 .    , http://dx.doi.org/10.1145/2505515.2505741[_parallel triangle counting in massive streaming graphs _ ] , in cikm13 : proceedings of the 22nd acm international conference on conference on information & knowledge management , association for computing machinery , 2013 , pp .",
    "781786 .",
    ", http://dx.doi.org/10.1109/asonam.2009.32[_spectral counting of triangles in power - law networks via element - wise sparsification _ ] , in asonam 09 . international conference on advances in social network analysis and mining , 2009 .",
    ", july 2009 , pp",
    ".  6671 .      , http://dx.doi.org/10.1109/icdm.2008.72 [ _ fast counting of triangles in large real networks , without counting : algorithms and laws _ ] , in icdm 2008 : proceedings of the 8th ieee international conference on data mining , 2008 , pp .",
    "608617 .            , http://dx.doi.org/10.1109/icnc.2011.6022061[_a parallel computing model for large - graph mining with mapreduce _ ] , in icnc11 : 2011 seventh international conference on natural computation , vol .  1 , july 2011 , pp .  4347 .    , http://dx.doi.org/10.1007/978-3-642-24082-9_83[_improved sampling for triangle counting with mapreduce _ ] , in convergence and hybrid information technology , g.  lee , d.  howard , and d.  slezak , eds .",
    "6935 of lecture notes in computer science , springer berlin / heidelberg , 2011 , pp .",
    "685689 .      , http://dx.doi.org/10.1109/ipdps.2012.44[_sahad : subgraph analysis in massive networks using hadoop _ ] , in ipdps12 : 2012 ieee 26th international parallel distributed processing symposium , may 2012 , pp ."
  ],
  "abstract_text": [
    "<S> graphs and networks are used to model interactions in a variety of contexts . </S>",
    "<S> there is a growing need to quickly assess the characteristics of a graph in order to understand its underlying structure . </S>",
    "<S> some of the most useful metrics are triangle - based and give a measure of the connectedness of mutual friends . </S>",
    "<S> this is often summarized in terms of clustering coefficients , which measure the likelihood that two neighbors of a node are themselves connected . </S>",
    "<S> computing these measures exactly for large - scale networks is prohibitively expensive in both memory and time . </S>",
    "<S> however , a recent _ wedge sampling _ algorithm has proved successful in efficiently and accurately estimating clustering coefficients . in this paper , we describe how to implement this approach in mapreduce to deal with massive graphs . </S>",
    "<S> we show results on publicly - available networks , the largest of which is 132 m nodes and 4.7b edges , as well as artificially generated networks ( using the graph500 benchmark ) , the largest of which has 240 m nodes and 8.5b edges . </S>",
    "<S> we can estimate the clustering coefficient by degree bin ( e.g. , we use exponential binning ) and the number of triangles per bin , as well as the global clustering coefficient and total number of triangles , in an average of 0.33 seconds per million edges plus overhead ( approximately 225 seconds total for our configuration ) . </S>",
    "<S> the technique can also be used to study triangle statistics such as the ratio of the highest and lowest degree , and we highlight differences between social and non - social networks . to the best of our knowledge , </S>",
    "<S> these are the largest triangle - based graph computations published to date .    </S>",
    "<S> * keywords : * triangle counting , clustering coefficient , triangle characteristics , large - scale networks , mapreduce    sampling triangles in massive graphs with mapreduce </S>"
  ]
}