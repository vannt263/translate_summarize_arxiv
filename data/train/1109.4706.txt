{
  "article_text": [
    "finite mixture distributions have become increasingly popular in the modelling and analysis of data due to their flexibility .",
    "this use of finite mixture distributions to model heterogeneous data has undergone intensive development in the past decades , as witnessed by the numerous applications in various scientific fields such as bioinformatics , cluster analysis , genetics , information processing , medicine , and pattern recognition .",
    "comprehensive surveys on mixture models and their applications can be found , for example , in the monographs by everitt and hand ( 1981 ) , titterington , smith , and markov ( 1985 ) , lindsay ( 1995 ) , mclachlan and basford ( 1988 ) , and mclachlan and peel ( 2000 ) , among others ; see also the papers by banfield and raftery ( 1993 ) and fraley and raftery ( 1999 ) .",
    "mixtures of multivariate @xmath0-distributions , as proposed by mclachlan and peel ( 1998 , 2000 ) , provide extra flexibility over normal mixtures .",
    "the thickness of tails can be regulated by an additional parameter ",
    "the degrees of freedom , thus enabling it to accommodate outliers better than normal distributions .",
    "however , in many practical problems , the data often involve observations whose distributions are highly asymmetric as well as having longer tails than the normal , for example , datasets from flow cytometry ( pyne et al . , 2009 ) .",
    "azzalini ( 1985 ) introduced the so - called skew - normal ( sn ) distribution for modelling symmetry in data sets .",
    "following the development of the sn and skew @xmath0-mixture models by lin , lee , and yen ( 2007 ) , and lin , lee , and hsieh ( 2007 ) , respectively , basso et al .",
    "( 2010 ) studied a class of mixture models where the components densities are scale mixtures of skew - normal distributions introduced by branco and dey ( 2001 ) , which include the classical skew - normal and skew @xmath0-distributions as special cases . recently ,",
    "cabral , lachos , and prates ( 2012 ) have extended the work of basso et al .",
    "( 2010 ) to the multivariate case .    in a study of automated flow cytometry analysis , pyne et al .",
    "( 2009 ) proposed a finite mixture of multivariate skew @xmath0-distributions based on a ` restricted ' variant of the skew @xmath0-distribution introduced by sahu , dey , and branco ( 2003 ) .",
    "lin ( 2010 ) considered a similar approach , but working with the original ( unrestricted ) characterization by sahu et al .",
    "however , with this more general formulation , maximum likelihood ( ml ) estimation via the em algorithm ( dempster , laird , and rubin , 1977 ) can no longer be implemented in closed form due to the intractability of some of the conditional expectations involved on the e - step . to work around this ,",
    "lin ( 2010 ) proposed a monte carlo ( mc ) version of the e - step .",
    "one potential drawback of this approach is that the model fitting procedure relies on mc estimates which can be computationally expensive .    in this paper , we show how the em algorithm can be implemented exactly to calculate the ml estimates of the parameters for the ( unrestricted ) multivariate skew @xmath0-mixture model , based on analytically reduced expressions for the conditional expectations , suitable for numerical evaluation using readily available software .",
    "a key factor in being able to compute the integrals quickly by numerical means is the recognition that they can be expressed as moments of a truncated multivariate @xmath0-distribution , which in turn can be expressed in terms of the distribution function of a ( non - truncated ) multivariate central @xmath0-random vector , for which fast programs already exist .",
    "we show that the proposed algorithm is highly efficient compared to the version with a mc e - step .",
    "it produces highly accurate results for which , if mc were to achieve comparable accuracy , a large number of draws would be necessary .",
    "the remainder of the paper is organized as follows . in section  [ sec : prelim ] , for the sake of completeness , we include a brief description of the multivariate skew @xmath0-distribution ( mst ) used for defining the multivariate skew @xmath0-mixture model .",
    "we also describe the truncated @xmath0-distribution in the multivariate case , critical for the swift evaluation of the integrals on the e - step occurring in the calculation of some of the conditional expectations .",
    "section [ sec:3 ] presents the development of an em algorithm for obtaining ml estimates for the mst distribution . in the following section , the finite mixture of mst ( fm - mst ) distributions is defined .",
    "section [ sec:5 ] presents an implementation of the em algorithm to the fitting of the fm - mst model .",
    "an approximation to the observed information matrix is discussed in section [ sec:6 ]",
    ". finally , we present some applications of the proposed methodology in section [ sec:7 ] .",
    "we begin by defining the multivariate skew @xmath0-distribution and briefly describing some related properties .",
    "some alternative versions of the distribution are also discussed .",
    "next , we introduce the truncated multivariate @xmath0-distribution and provide some formulas for computing its moments",
    ". these expressions are crucial for the swift evaluation of the conditional expectations on the e - step to be discussed in the next section .      following sahu et al .",
    "( 2003 ) , a random vector @xmath1 is said to follow a @xmath2-dimensional ( unrestricted ) skew @xmath0-distribution with @xmath3 location vector @xmath4 , @xmath5 scale matrix @xmath6 , @xmath3 skewness vector @xmath7 , and scalar degrees of freedom @xmath8 , if its density is given by    @xmath9    where @xmath10 here the operator @xmath11 denotes a diagonal matrix with diagonal elements specifed by the vector @xmath7 .",
    "also , we let @xmath12 be the @xmath2-dimensional @xmath0-density with location vector @xmath4 , scale matrix @xmath6 , and degrees of freedom @xmath8 , and @xmath13 the corresponding ( cumulative ) distribution function . the notation @xmath14 will be used . note that when @xmath15 , ( [ mst ] ) reduces to the symmetric @xmath0-density @xmath16 . also , when @xmath17 , we obtain the skew normal distribution .",
    "the mst distribution admits a convenient hierarchical form , @xmath18 where @xmath19 is the @xmath20 identity matrix , @xmath21 denotes the multivariate normal distribution with mean @xmath4 and covariance matrix @xmath6 , @xmath22 represents the @xmath2-dimensional half - normal distribution with mean @xmath23 and scale matrix @xmath6 , and @xmath24 is the gamma distribution with mean @xmath25 .",
    "we observe from ( [ mst_h ] ) that the mst distribution ( [ mst ] ) has the following stochastic representation .",
    "suppose that conditional on the value @xmath26 of the gamma random variable @xmath27 , @xmath28 where @xmath19 denotes the @xmath20 identity matrix , @xmath23 denotes the zero vector of appropriate dimension , and @xmath29 is a @xmath2-dimensional random vector .",
    "then @xmath30 has the multivariate skew @xmath0-distribution density ( [ mst ] ) . in the above , @xmath31 denotes the vector whose @xmath32th element is the magnitude of the @xmath32th element of the vector @xmath33 .",
    "it is important to note that , although also known as the multivariate skew @xmath0-distribution , the versions considered by azzalini and dalla valle ( 1996 ) , gupta ( 2003 ) , and lachos , ghosh , and arellano - valle ( 2010 ) , among others , are different from ( [ mst ] ) .",
    "these versions are simpler in that the skew @xmath0-density is defined in terms involving only the univariate @xmath0-distribution function instead of the multivariate form of the latter as used in ( [ mst ] ) .",
    "recently , pyne et al . (",
    "2009 ) proposed a simplified version of the skew @xmath0-density given by ( [ mst ] ) by replacing the term @xmath34 in ( [ mst_s ] ) by the term @xmath35 , where @xmath36 is a univariate central @xmath0-random variable with @xmath8 degrees of freedom , leading to the reduced skew @xmath0-density : @xmath37 where @xmath38^{\\frac{1}{2 } } { \\boldsymbol\\delta}^t { \\boldsymbol\\omega}^{-1 } ( { \\mbox{\\boldmath $ y$}}-{\\boldsymbol\\mu})$ ] .",
    "we shall refer to this characterization of skew @xmath0-distribution as the ` restricted ' multivariate skew @xmath0 ( rmst)distribution .",
    "one immediate consequence of this type of ` simplification ' is that the correlation structure of the original symmetric model is affected by the introduction of skewness , whereas for ( [ mst ] ) the correlation structure remains the same , as noted in arellano - valle , bolfarine , and lachos ( 2007 ) .",
    "nevertheless , one major advantage of having simplified forms like ( [ mst_sam ] ) is that calculations on the e - step can be expressed in closed form .",
    "however , the form of skewness is limited in these characterizations . here , we extend their approach to the more general form of the skew @xmath0-density as proposed by sahu et al .",
    "( 2003 ) .",
    "let @xmath39 be a @xmath2-dimensional random variable having a multivariate @xmath0-distribution with location vector @xmath4 , scale matrix @xmath6 , and @xmath8 degrees of freedom .",
    "truncating @xmath40 to the hyperplane region @xmath41 , where @xmath42 means each element @xmath43 is less than or equal to @xmath44 for @xmath45 , results in a right - truncated @xmath0-distribution whose density is given by @xmath46    for a random vector @xmath39 with density ( [ ttden ] ) , we write @xmath47 . for our purposes , we will be concerned with the first two moments of @xmath39 , specifically @xmath48 and @xmath49 .",
    "explicit formulas for the truncated central @xmath0-distribution in the univariate case @xmath50 were provided by ohagan ( 1973 ) , who expressed the moments in terms of the non - truncated @xmath0-distribution .",
    "the multivariate case was studied in ohagan ( 1976 ) , but still considering the central case only .",
    "we will generalize these results to the case with non - zero location vector here .    before presenting the expressions",
    ", it will be convenient to introduce some notation .",
    "let @xmath40 be a vector , where @xmath51 denotes the @xmath32th element and @xmath52 is a two - dimensional vector with elements @xmath51 and @xmath53 .",
    "also , @xmath54 and @xmath55 represents the @xmath56 and @xmath57-dimensional vector , respectively , with the corresponding elements removed . for a matrix @xmath39",
    ", @xmath58 denotes the @xmath59th element , and @xmath60 defines the @xmath61 matrix consisting of the elements @xmath62 , @xmath58 , @xmath63 and @xmath64 .",
    "@xmath65 is created by removing the @xmath32th row and column from @xmath39 .",
    "similarly , @xmath66 is the @xmath57-dimensional square matrix resulting from the removal of the @xmath32th and @xmath67th row and column from @xmath39 .",
    "lastly , @xmath68 is the @xmath32th and @xmath67th column of @xmath39 with the elements of @xmath69 removed , yielding a @xmath70 matrix .",
    "we now proceed to the expressions for the first two moments of @xmath39 .    with some effort",
    ", one can show that the first moment of ( [ ttden ] ) is @xmath71 where @xmath72 , and @xmath73 is a @xmath3 vector with elements @xmath74 for @xmath45 , and where @xmath75    the second moment is given by @xmath76 where @xmath77 is a @xmath20 matrix with off - diagonal elements @xmath78 and diagonal elements , @xmath79    it is worth noting that evaluation of the expressions ( [ ex ] ) and ( [ exx ] ) rely on algorithms for computing the multivariate central @xmath0-distribution function for which highly efficient procedures are readily available in many statistical packages .",
    "for example , an implementation of genz s algorithm ( genz and bretz , 2002 ; kotz and nadarajah , 2004 ) is provided by the ` mvtnorm ` package available from the r website .",
    "in this section , we describe an em algorithm for the ml estimation of the mst distribution specified by ( [ mst ] ) . to apply the em algorithm ,",
    "the observed data vector @xmath80 is regarded as incomplete , and we introduce two latent variables denoted by @xmath81 and @xmath26 , as defined by ( [ mst_h ] ) .",
    "we let @xmath82 be the parameter containing the elements of the location parameter @xmath4 , the distinct elements of the scale matrix @xmath6 , the elements of the skew parameter @xmath7 , and the degrees of freedom @xmath8 .",
    "it follows that the complete - data log - likelihood function for @xmath82 is given by @xmath83 where @xmath84 does not depend on @xmath82 .",
    "the implementation of the em algorithm requires alternating repeatedly the e- and m - steps until convergence in the case where the sequence of the log likelihood values @xmath85 is bounded above . here",
    "@xmath86 denotes the value of @xmath82 after the @xmath87th iteration .    on the @xmath88th iteration ,",
    "the e - step requires the calculation of the conditional expectation of the complete - data log likelihood given the observed data @xmath89 , using the current estimate @xmath86 for @xmath82 .",
    "that is , we have to calculate the so - called @xmath90-function defined by @xmath91 where @xmath92 denotes the expectation operator , using @xmath86 for @xmath82 .",
    "this , in effect , requires the calculation of the conditional expectations @xmath93    note that the @xmath90-function does not admit a closed form expression for this problem , due to the conditional expectations @xmath94 , @xmath95 , and @xmath96 not being able to be evaluated in closed form .    concerning the calculation of the expectation @xmath94 ,",
    "the conditional density of @xmath97 given @xmath98 , is given by @xmath99 where @xmath100 and @xmath23 is the zero vector of appropriate dimension .",
    "the conditional expectation @xmath101 can be reduced to @xmath102 where the last term @xmath103 is given by @xmath104^{-\\frac{p}{2 } }       \\left|{\\boldsymbol\\lambda}\\right|^{-\\frac{1}{2}}}{t_{p,\\nu^{(k)}+p }       \\left(\\boldsymbol    q_j^{(k ) }   \\sqrt{\\frac{\\nu^{(k)}+p}{\\nu^{(k)}+{d{^{(k)}}({\\boldsymbol y_j})}}};\\boldsymbol 0 , { \\boldsymbol\\lambda}^{(k)}\\right ) }      \\frac{\\gamma\\left(\\frac{\\nu^{(k)}}{2}+p\\right)}{\\gamma\\left(\\frac{\\nu^{(k)}+p}{2 } \\right ) } s_{1,j}^{(k ) } ,       \\label{s }    \\end{aligned}\\ ] ] and @xmath105 is an integral given by @xmath106_1 }   \\int_{-\\infty}^{\\left[\\boldsymbol q_{j}^{(k)}\\right]_2 } \\ldots   \\int_{-\\infty}^{\\left[\\boldsymbol q_{j}^{(k)}\\right]_p }   log\\left(1+\\frac{\\boldsymbol s^t{{\\boldsymbol\\lambda}}^{{(k)}^{-1}}\\boldsymbol   s}{\\nu^{(k)}+{d{^{(k)}}({\\boldsymbol y_j})}}\\right ) \\nonumber\\\\      &    & \\left[1+\\frac{\\boldsymbol s^t{{\\boldsymbol\\lambda}}^{{(k)}^{-1 } } \\boldsymbol s}{\\nu^{(k)}+d^{(k)}({\\boldsymbol y_j})}\\right ]       ^{-\\left(\\frac{\\nu^{(k)}}{2}+p\\right ) } ds_1       ds_2 \\ldots ds_p ,      \\label{s1}\\end{aligned}\\ ] ] and @xmath107 denotes the digamma function .    combining ( [ e1a ] ) and ( [ s ] )",
    ", @xmath94 can be reduced to @xmath108    we note that the term @xmath103 will be very small in practice since it would be zero if we adopted a one - step late ( osl ) em algorithm ( green , 1990 ) . in which case , there would be no need to calculate the multiple integral @xmath105 in ( [ s ] ) . hence then",
    ", @xmath94 can be reduced to @xmath109    it can be easily shown that @xmath110 can be written in closed form ( see , for example , lin ( 2010 ) ) , given by @xmath111    to obtain @xmath95 and @xmath96 , first note that the joint density of @xmath98 , @xmath112 , and @xmath113 is given by @xmath114}.   \\label{yuw}\\end{aligned}\\ ] ]    using bayes rule , the conditional density of @xmath112 and @xmath113 given @xmath98 can be written as @xmath115    from ( [ uw_y ] ) , standard conditional expectation calculations yield @xmath116 where @xmath117 represents the expected value of a truncated @xmath2-dimensional @xmath0-variate @xmath118 , which is distributed as , @xmath119 that is , the random vector @xmath118 is truncated to lie in the positive hyperplane @xmath120 .",
    "analogously , @xmath96 can be reduced to @xmath121 where @xmath122 represents the second moment of @xmath118 .",
    "the truncated moments @xmath123 and @xmath124 can be swiftly evaluated using the expressions ( [ ex ] ) and ( [ exx ] ) in section [ sec:2.2 ] .      on the @xmath88th iteration ,",
    "the m - step consists of the maximization of the @xmath90-function ( [ qfun ] ) with respect to @xmath82 . for easier computation",
    ", we employ the ecm extension of the em algorithm , where the m - step is replaced by four conditional  maximization ( cm)-steps , corresponding to the decomposition of @xmath82 into four subvectors , @xmath125 , where @xmath126 , @xmath127 , @xmath128 is the vector containing the distinct elements of @xmath6 , and @xmath129 . to compute @xmath130 , we maximize @xmath131 with respect to @xmath4 , and to compute @xmath132 , we first update @xmath4 to @xmath130 and then maximize @xmath133 with respect to @xmath7 , and so on .",
    "we let @xmath134 denote the operator that produces a vector by extracting the diagonal elements of @xmath135 .",
    "straightforward algebraic manipulations lead to the following closed form expressions for @xmath130 , @xmath136 , and @xmath132 , @xmath137 } { \\sum_{j=1}^n e_{2,j}^{(k ) } } , \\label{mu }   \\displaybreak[0]\\\\ { \\boldsymbol\\delta}^{(k+1 ) } & = \\left({\\boldsymbol\\sigma}^{(k)^{-1 } } \\odot       \\sum_{j=1}^n { \\mbox{\\boldmath $ e$}}_{4.j}^{(k)}\\right)^{-1 }      \\mbox{diag}\\left({\\boldsymbol\\sigma}^{(k)^{-1 } }       \\sum_{j=1}^n ( { \\mbox{\\boldmath $ y$}}_j-{\\boldsymbol\\mu}^{(k ) } ) { \\mbox{\\boldmath $ e$}}_{3,j}^{(k)^t } \\right ) ,       \\label{delta }   \\displaybreak[0]\\\\ \\intertext{and }   { \\boldsymbol\\sigma}^{(k+1 ) } & =   \\frac{1 } { n } \\sum_{j=1}^n   \\left[{\\boldsymbol\\delta}^{(k+1)}{\\mbox{\\boldmath $ e$}}_{4,j}^{(k)^t}{\\boldsymbol\\delta}^{(k+1)^t } \\right",
    ".       - \\left({\\boldsymbol y_j}-{\\boldsymbol\\mu}^{(k+1)}\\right){\\mbox{\\boldmath $ e$}}_{3,j}^{(k)^t}{\\boldsymbol\\delta}^{(k+1 ) }       \\notag\\\\      & + \\left({\\boldsymbol y_j}-{\\boldsymbol\\mu}^{(k+1)}\\right ) \\left({\\boldsymbol",
    "y_j}-{\\boldsymbol\\mu}^{(k+1)}\\right)^t       e_{2,j}^{(k ) } \\left .",
    "- { { \\boldsymbol\\delta}}^{(k+1)}{\\mbox{\\boldmath $ e$}}_{3,j}^{(k ) }       \\left({\\boldsymbol y_j}-{\\boldsymbol\\mu}^{(k+1)}\\right)^t \\right ] ,   \\label{sigma}\\end{aligned}\\ ] ] where @xmath138 denotes the hadamard or element - wise product , and @xmath139 .",
    "an updated estimate of the degrees of freedom @xmath140 is obtained by solving the equation @xmath141    in summary , the ecm algorithm proceeds as follows on the @xmath88th iteration : * e*-step : given @xmath142 , compute the four conditional expectations @xmath94 , @xmath110 , @xmath95 and @xmath96 by using ( [ e1 ] ) , ( [ e2 ] ) , ( [ e3 ] ) , and ( [ e4 ] ) , respectively , for @xmath143 . *",
    "m*-step : update @xmath130 , @xmath132 , @xmath136 and by using ( [ mu ] ) , ( [ delta ] ) , and ( [ sigma ] ) .",
    "calculate @xmath140 by solving ( [ nu ] ) .",
    "the probability density function ( pdf ) of a finite mixture of @xmath144 multivariate skew @xmath0-components , using the notation above , is given by @xmath145 where @xmath146 denotes the @xmath32th mst component of the mixture model as defined by ( [ mst ] ) , with location parameter @xmath147 , scale matrix @xmath148 , skew parameter @xmath149 , and degrees of freedom @xmath150 .",
    "the mixing proportions @xmath151 satisfy @xmath152 @xmath153 and @xmath154 .",
    "we shall denote the model defined by ( [ mstmix ] ) by fm - mst ( finite mixture of mst ) distributions .",
    "let @xmath155 contain all the unknown parameters of the fm - mst model ; that is , @xmath156 where now @xmath157 consists of the unknown parameters of the @xmath32th component density function .    to formulate the estimation of the unknown parameters in the fm - mst model as an incomplete - data problem in the em framework , a set of latent component labels @xmath158 @xmath159 is introduced , where each element @xmath160 is a binary variable defined as @xmath161 and @xmath162 @xmath163 .",
    "hence , the random vector @xmath164 corresponding to @xmath165 follows a multinomial distribution with one trial and cell probabilities @xmath166 ; that is , @xmath167 .",
    "it follows that the fm - mst model can be represented in the hierarchical form given by @xmath168 where @xmath169 and @xmath170 .",
    "from the hierarchical characterization ( [ mstmix_h ] ) of the fm - mst distributions , the complete - data log - likelihood function is given by @xmath171 where @xmath172\\notag\\\\ \\log l_{2c}\\left({\\boldsymbol\\psi}\\right ) & =     \\sum_{h=1}^g \\sum_{j=1}^n z_{hj }   \\left[\\left(\\frac{\\nu_h}{2}\\right ) \\log\\left(\\frac{\\nu_h}{2}\\right ) +   \\left(\\frac{\\nu_h}{2}+p-1\\right)\\log\\left(w_j\\right ) \\right.\\notag\\\\      &        \\left .- \\log\\gamma\\left(\\frac{\\nu_h}{2}\\right ) -       \\left(\\frac{w_j}{2}\\right)\\nu_h\\right ] , \\notag\\displaybreak[0]\\\\ \\log l_{3c}\\left({\\boldsymbol\\psi}\\right ) & =     \\sum_{h=1}^g \\sum_{j=1}^n z_{hj } \\left\\{-   p\\log\\left(2\\pi\\right ) -   \\frac{1}{2}\\log\\left|{{\\boldsymbol\\sigma}}_h\\right| \\right .",
    "\\notag\\\\      &        - \\left .",
    "\\frac{w_j}{2}\\left[d_h\\left({\\boldsymbol y_j}\\right ) +       \\left(\\boldsymbol u_j-\\boldsymbol q_{hj}\\right)^t { \\boldsymbol\\lambda}_h^{-1 }       \\left(\\boldsymbol u_j-\\boldsymbol q_{hj}\\right)\\right]\\right\\ } ,   \\label{lc2}\\end{aligned}\\ ] ] and where @xmath173    it is clear from ( [ logl ] ) that maximization of the @xmath90-function of the complete - data log likelihood ( mclachlan and krishnan , 2008 ) , @xmath174 only requires maximization of the components functions @xmath175 separately @xmath176 . the necessary conditional expectations involved in computing the @xmath90-function with respect to ( [ lc2 ] ) are , namely , @xmath177    the posterior probability of membership of the @xmath178th component by @xmath179 , using the current estimate @xmath180 for @xmath155 , is given using bayes theorem by @xmath181    the other four expectations have analogous expressions to their one - component counterpart given in section [ sec:3 ] .",
    "they are given by @xmath182\\\\ { \\mbox{\\boldmath $ e$}}_{3,hj}^{(k ) }     & =   \\left(\\frac{\\nu_h^{(k)}+p}{\\nu_h^{(k)}+d_h^{(k)}({\\boldsymbol y_j})}\\right ) t_{p ,   \\nu_h^{(k)}+p}^{-1}\\left(y_{hj}^{*(k ) } ;   \\boldsymbol 0 , { \\boldsymbol\\lambda}_h^{(k)}\\right ) \\boldsymbol s_{2,ij}^{(k ) } , \\label{e3 }   \\displaybreak[0]\\\\   { \\mbox{\\boldmath $ e$}}_{4,hj}^{(k ) }     & = \\left(\\frac{\\nu_h^{(k)}+p}{\\nu_h^{(k)}+d_h^{(k)}({\\boldsymbol y_j})}\\right ) t_{p ,   \\nu_h^{(k)}+p}^{-1}\\left(y_{hj}^{*(k ) } ;   \\boldsymbol 0 , { \\boldsymbol\\lambda}_h^{(k)}\\right )   \\boldsymbol s_{3,ij}^{(k ) } , \\label{e4 } \\displaybreak[0]\\end{aligned}\\ ] ] where @xmath183 is a scalar defined by @xmath184_1 }   \\int_{-\\infty}^{\\left[{\\boldsymbol q_{hj}{^{(k)}}}\\right]_2 } \\ldots   \\int_{-\\infty}^{\\left[{\\boldsymbol q_{hj}{^{(k)}}}\\right]_p }   log\\left(1+\\frac{\\boldsymbol s^t{{\\boldsymbol\\lambda}_h}^{-1}\\boldsymbol   s}{\\nu_h^{(k)}+d_h^{(k)}({\\boldsymbol y_j})}\\right ) \\label{s1b}\\\\      &    & \\left[1+\\frac{\\boldsymbol s^t{{\\boldsymbol\\lambda}_h}^{-1}\\boldsymbol       s}{\\nu_h^{(k)}+d_h^{(k)}({\\boldsymbol y_j})}\\right ]       ^{-\\left(\\frac{\\nu_h^{(k)}}{2}+p\\right ) } d\\boldsymbol u , \\nonumber\\end{aligned}\\ ] ] @xmath185 is a @xmath3 vector whose @xmath186th element is @xmath187 and @xmath188 is a @xmath20 matrix whose @xmath189th element is @xmath190 where , for convenience of notation , @xmath191 is used to denote @xmath192 .",
    "it is important to note that @xmath193 and @xmath194 are related to the first two moments of a truncated @xmath2-dimensional @xmath0-variate @xmath195 .",
    "more specifically , let @xmath196 the truncated @xmath0-distribution as defined by ( [ ttden ] ) .",
    "then @xmath197 and hence ( [ e3 ] ) and ( [ e4 ] ) reduces to @xmath198 and @xmath199 respectively , which can be implicitly expressed in terms of the parameters @xmath200 , @xmath201 , @xmath202 , @xmath203 using results ( [ ex ] ) and ( [ exx ] ) .",
    "it is worth emphasizing that computation of @xmath204 and @xmath205 depends on algorithms for evaluating the multivariate @xmath0-distribution function , for which fast procedures are available .    in summary ,",
    "the ecm algorithm is implemented as follows on the @xmath88th iteration :    * e - step : * given @xmath206 , compute @xmath207 using ( [ tau ] ) , and @xmath208 , @xmath209 , @xmath210 , and @xmath211 as described by ( [ e1 ] ) , ( [ e2 ] ) , ( [ e3 ] ) , and ( [ e4 ] ) respectively , for @xmath212 and @xmath213 .    *",
    "m - step : * update the estimate of @xmath155 by calculating for @xmath214 , the following estimates of the parameters in @xmath155 ,    @xmath215 } { \\sum_{j=1}^n       \\tau_{hj}^{(k)}e_{2,hj}^{(k ) } } , \\notag\\displaybreak[0]\\\\ { \\boldsymbol\\delta}^{(k+1 ) } & = \\left({\\boldsymbol\\sigma}_h^{(k)^{-1 } } \\odot       \\sum_{j=1}^n \\tau_{hj}^{(k ) } { \\mbox{\\boldmath $ e$}}_{4,hj}^{(k)}\\right)^{-1 }      \\mbox{diag}\\left({\\boldsymbol\\sigma}_h^{(k)^{-1 } }       \\sum_{j=1}^n \\tau_{hj}^{(k ) } ( { \\mbox{\\boldmath $ y$}}_j-{\\boldsymbol\\mu}_h^{(k ) } ) { \\mbox{\\boldmath $ e$}}_{3,hj}^{(k)^t } \\right ) ,      \\notag\\displaybreak[0]\\\\      \\intertext{and } { { \\boldsymbol\\sigma}}_h^{(k+1 ) } & =   \\frac{1 } { \\sum_{j=1}^n           \\tau_{hj}^{(k ) } } \\sum_{j=1}^n \\tau_{hj}^{(k ) }           \\left[{\\boldsymbol\\delta}_h^{(k+1)}{\\mbox{\\boldmath $ e$}}_{4,hj}^{(k)^t}{\\boldsymbol\\delta}_h^{(k+1)^t } \\right .",
    "\\left({\\boldsymbol y_j}-{\\boldsymbol\\mu}_h^{(k+1)}\\right ) { \\mbox{\\boldmath $ e$}}_{3,hj}^{(k)^t }       { \\boldsymbol\\delta}_h^{(k+1 ) } \\notag\\\\      &    - { \\boldsymbol\\delta}_h^{(k+1)}{\\mbox{\\boldmath $ e$}}_{3,hj}^{(k ) }       \\left({\\boldsymbol y_j}-{\\boldsymbol\\mu}_h^{(k+1)}\\right)^t       \\left .",
    "+ \\left({\\boldsymbol y_j}-{\\boldsymbol\\mu}_h^{(k+1)}\\right ) \\left({\\boldsymbol y_j}-{\\boldsymbol\\mu}_h^{(k+1)}\\right)^t       e_{2,hj}^{(k)}\\right ] .",
    "\\notag\\end{aligned}\\ ] ]    an update @xmath216 of the degrees of freedom is obtained by solving iteratively the equation @xmath217 }       { \\sum_{j=1}^n \\tau_{hj}^{(k)}}.      \\nonumber\\ ] ]    a program for implementing this em algorithm has been written in r.",
    "we consider an approximation to the asymptotic covariance matrix of the ml estimates using the inverse of the empirical information matrix ( basford et al . , 1997 ) .",
    "the empirical information matrix is given by @xmath218 where @xmath219 @xmath163 are the individual scores , consisting of @xmath220 we let @xmath221 denote the complete - data log likelihood formed from the single observation @xmath98 .",
    "an estimate of the covariance matrix of @xmath222 is given by taking the inverse of ( [ ie ] ) .",
    "after some algebraic manipulations , one can show that the elements of @xmath223 are given by the following explicit expressions :    @xmath224\\\\ s_{j,{\\boldsymbol\\mu}_h }     & =   \\tau_{hj}\\hat{{\\boldsymbol\\sigma}}_h^{-1 } \\left[e_{2,ij}\\left({\\boldsymbol y_j}-   \\hat{{\\boldsymbol\\mu}}_h\\right ) - \\hat{{\\boldsymbol\\delta}}_h { \\mbox{\\boldmath $ e$}}_{3,ij}\\right ] ,   \\notag\\displaybreak[0]\\\\ s_{j,{\\boldsymbol\\sigma}_h } & =   \\textstyle \\frac{1}{2 } \\tau_{hj }   \\left[\\left({\\boldsymbol y_j}-\\hat{{\\boldsymbol\\mu}}_h\\right)\\left({\\boldsymbol y_j}-\\hat{{\\boldsymbol\\mu}}_h\\right)^t -   \\left({\\boldsymbol y_j}-\\hat{{\\boldsymbol\\mu}}_h\\right){\\mbox{\\boldmath $ e$}}_{3,hj}^t\\hat{{\\boldsymbol\\delta}}_h   \\right . \\notag\\\\      &    \\left . - \\hat{{\\boldsymbol\\delta}}_h{\\mbox{\\boldmath $ e$}}_{3,hj}\\left({\\boldsymbol y_j}-\\hat{{\\boldsymbol\\mu}}_h\\right ) +       \\hat{{\\boldsymbol\\delta}}_h{\\mbox{\\boldmath $ e$}}_{3,ij}\\left({\\boldsymbol y_j}-\\hat{{\\boldsymbol\\mu}}_h\\right ) +       \\hat{{\\boldsymbol\\delta}}_h{\\mbox{\\boldmath $ e$}}_{4,hj}^t\\hat{{\\boldsymbol\\delta}}_h\\right ] \\hat{{\\boldsymbol\\sigma}}_h^{-1 }       \\notag\\\\      &    - \\textstyle\\frac{1}{2}\\tau_{hj } \\hat{{\\boldsymbol\\sigma}}_h^{-1 } ,       \\notag\\displaybreak[0]\\\\ s_{j,{\\boldsymbol\\delta}_h } & =   \\tau_{hj } \\left [       \\mbox{diag}\\left(\\hat{{\\boldsymbol\\sigma}}_h^{-1 } \\left({\\boldsymbol y_j}-\\hat{{\\boldsymbol\\mu}}_h\\right )      \\right ) { \\mbox{\\boldmath $ e$}}_{3,hj } - \\left(\\hat{{\\boldsymbol\\sigma}}_h^{-1 } \\odot { \\mbox{\\boldmath $ e$}}_{4,hj}\\right )      \\hat{{\\boldsymbol\\delta}}_h \\right ] ,     \\notag\\displaybreak[0]\\\\ s_{j,\\nu_h } & =   \\textstyle\\frac{1}{2 } \\tau_{hj }   \\left[\\log\\left(\\textstyle\\frac{1}{2 } \\hat{\\nu}_h\\right ) + 1 + e_{1,hj } -   \\psi\\left(\\textstyle\\frac{1}{2}\\hat{\\nu}_h\\right ) - e_{2,hj}\\right ] .",
    "\\notag \\ ] ]",
    "in this section , we fit the fm - mst model to three real data sets to demonstrate its usefulness in analyzing and clustering multivariate skewed data . in the first example , we focus on the flexibility of the fm - mst model in capturing the asymmetric shape of flow cytometric data . the next example illustrates the clustering capability of the model . in the final example , we demonstrate the computational efficiency of the proposed algorithm .",
    "we consider a subset of the t - cell phosphorylation data collected by maier et al .",
    "( 2007 ) . in the original data , blood samples from 30 subjects were stained with four fluorophore - labeled antibodies against cd4 , cd45ra , slp76(py128 ) , and zap70(py292 ) before and after an anti - cd3 stimulation . in this example",
    ", we focus on a reduced subset of the data in two variables  cd4 and zap70 . this bivariate sample ( figure [ fig1 ] )",
    "is apparently bimodal and exhibits asymmetric pattern .",
    "hence we fit a two - component fm - mst model to the data .",
    "more specifically , the fitted model can be written as @xmath225 where @xmath226    for comparison , we include the fitting of a two - component mixture of skew @xmath0-distributions from the skew - normal independent ( sni ) family ( lachos , ghosh , and arellano - valle , ( 2010 ) ) , hereafter named the fm - sni - st model . the estimated fm - sni - st density can be computed from the r package ` mixsmsn ` ( cabral , lachos , and prates , ( 2012 ) ) . note that the mst distribution is different to the sni - st distribution since the skewing function is not of dimension one .",
    "note also that the sni - st distribution is equivalent to the restricted mst distribution ( [ mst_sam ] ) after reparametrization .",
    "moreover , under the fm - sni - st settings , the correlation structure of @xmath1 will also be dependent on the skewness parameter , whereas for the fm - mst distributions the covariance structure is not affected by @xmath7 .",
    "the contours of the fitted sni - st and mst component densities are depicted in fig  [ fig1](b ) and fig  [ fig1](c ) , respectively . to better visualize the shape of the fitted models , we display the estimated densities of each component instead of the mixture contours .",
    "it can be seen that the fm - mst model provides a noticeably better fit . from a clustering point of view",
    ", the fm - mst model also shows better performance as it is able to separate the two clusters correctly .",
    "moreover , it adapts to the asymmetric shape of each cluster more adequately .",
    "thus the superiority of fm - mst model is evident in dealing with asymmetric and heavily tailed data in this data set .",
    "-mixtures were fitted to the data restricted in two dimensions cd45 and zap70 .",
    "( a ) hue intensity plot of the lymphoma data set ; ( b ) the contours of the component densities in the fitted two - component skew @xmath0-mixture model fm - sni - st using the r package ` mixsmsn ` ; ( c ) the fitted component contours of the two - component fm - mst model.__,scaledwidth=100.0% ]      our second example concerns a data set collected by brinkman et al .",
    "( 2007 ) , where peripheral blood samples were collected weekly from patients following blood and bone marrow transplant .",
    "the original goal was to identify cellular signatures that can predict or assist in early detection of graft versus host disease ( gvhd ) , a common post - transplantation complication in which the recipient s bone marrow was attacked by the new donor material .",
    "samples were stained with four fluorescence reagents : cd4 fitc , cd8@xmath227 pe , cd3 percp , and cd8 apc .",
    "hence we fit a 4-variate fm - mst model to a case sample with a population of 13773 cells .",
    "the data set is shown in figure [ fig2 ] , where cells are displayed in five different colours according to a manual expert clustering into five clusters .",
    "in addition , we include the results for the fm - sni - st model and the restricted mst mixture model introduced in section [ sec:2.1 ] ( equation [ mst_sam ] ) , hereafter denoted by fm - rmst .",
    "pe ( fl2.h ) , cd3 percp ( fl3.h ) and cd8 apc ( fl4.h).__,scaledwidth=100.0% ]    we compare the performance of the three models fm - mst , fm - sni - st , and fm - rmst in assigning cells to the expert clusters .",
    "gating suggests there are five clusters in this case sample .",
    "hence we applied the algorithm for the fitting of each model with @xmath144 predefined as @xmath228 . for a fair comparison",
    ", we started the three algorithms using the same initial values .",
    "the initial clustering is based on @xmath87-means.the degrees of freedom are set to be identical for all components for the first iteration and assigned a relatively large value .",
    "a similar strategy was described in lin ( 2010 ) .    to assess the performance of these three algorithms ,",
    "we take the manual expert clustering as being the ` true ' class membership and we calculated the error rate of classification against this benchmark result with dead cells removed , measured by choosing among the possible permutations of class labels the one that gives the highest value .",
    "as anticipated , the optimal clustering result was given by the fm - mst model .",
    "it achieved the lowest misclassification rate .",
    "the fm - sni - st model has a higher number of misallocations .",
    "the fm - rmst model has a disappointing performance in terms of clustering .",
    "its error rate is almost double that of its competitors .",
    "it is worth pointing out that both the fm - mst and fm - rmst models have 99 free parameters , while the fm - sni - st model has 95 parameters .",
    "it is evident that these two restricted models have inferior performance .",
    "this reveals some evidence of the extra flexibility offered by the more general fm - mst model .",
    "[ cols=\"^,^,^\",options=\"header \" , ]     [ tab2 ]",
    "we now proceed to two interesting experiments for evaluating the computational cost and accuracy of using the em - exact and em - mc algorithms on high - dimensional data .",
    "as pointed out previously , the main computational cost for em - exact is evaluating the multivariate @xmath0-distribution function .",
    "calculation of the first two moments of a @xmath2-variate truncated @xmath0-distribution requires the evaluation of two @xmath229 functions , @xmath2 evaluations of @xmath230 , and @xmath231 evaluations of @xmath232 .",
    "hence , the computation time will increase substantially with the number of dimensions .",
    "however , with the em - exact algorithm , accuracy can be compromised for time .",
    "we sampled 100 data from a brain tumor dataset supplied by geoff osborne from the queensland brain institute at the university of queensland . in both experiments we varied the dimension @xmath2 of the sample .",
    "the graph in figure [ fig3](a ) shows the typical cpu time per each e - step iteration for various dimensions @xmath2 of the data ; em - mc@xmath233 represents the em - mc algorithm with @xmath234 random draws using the gibbs sampling approach described in lin ( 2010 ) .",
    "it is worth noting that in both experiments em - exact is evaluated with a default tolerance of at least @xmath235 .",
    "as seen in figure [ fig3 ] , em - exact is the fastest among the four versions of the e - step for low dimensions . for example , at @xmath236 , em - exact at least 25 times faster than em - mc(50 ) .",
    "it is important to note that although em - mc(50 ) is slightly faster than em - exact at higher dimensions , em - exact produces results to a significantly higher accuracy , while em - mc requires a large number of draws to achieve comparable results .",
    "we note that in our simulations , for example , at @xmath237 , 50 draws is insufficient to achieve acceptable estimates .",
    "preliminary results suggests that at least 500 draws is required to generate reasonable approximations when @xmath2 is greater than 6 . in this case , em - exact is at least ten times quicker .",
    "furthermore , em - exact also has an additional advantage over the em - mc alternative in that its results are reproducible .    to compare the accuracy of the em - exact and em - mc algorithms , we compute the total absolute error against the baseline em - exact with a maximum tolerance of @xmath238 . for each of the em - mc@xmath233 algorithms ,",
    "the average total absolute error of 100 trials is used . for em - exact ,",
    "the default tolerance is set to @xmath235 .",
    "the results are shown in figure [ fig3](b ) .",
    "not surprisingly , the absolute error of the em - mc algorithm is significantly higher than that of the em - exact algorithm .",
    "it can be observed that the absolute error is very high even for em - mc(500 ) . at @xmath239 , for example , em - exact is at least 30000 times more accurate and takes less than half the time required for em - mc(500 ) .",
    "it is important to emphasize that as the dimension @xmath2 of the data increases , em - mc requires considerably more draws to provide a comparable ( and acceptable ) level of accuracy as em - exact , which can be computationally intensive .",
    "hence we advocate the use of em - exact , especially for applications involving high dimensional data .",
    "we have described an exact em algorithm for evaluating the parameters of a general multivariate skew @xmath0-mixture model .",
    "this model has a more general characterization than various alternative versions of the skew @xmath0-distribution available in the literature and hence offers greater flexibility in capturing the asymmetric shape of skewed data .",
    "our proposed method is based on reduced analytical expressions for the e - step conditional expectations , which can be formulated in terms of the first and second moments of a multivariate truncated @xmath0-distribution .",
    "the latter can then be expressed further in terms of the distribution function of the multivariate central @xmath0-distribution for which fast algorithms capable of producing highly accurate results already exist .",
    "it is demonstrated to have a marked advantage over the em algorithm with a monte carlo e - step . to achieve comparable accuracy to that of the em algorithm with the e - step",
    "implemented using the above numerical approach , the version of the algorithm with a monte carlo e - step would require a large number of draws , which would be computationally expensive .",
    "this work is supported by a grant from the australian research council .",
    "also , we would like to thank professor seung - gu kim for comments and corrections , and dr kui ( sam ) wang for his helpful discussions on this topic .",
    "r.  brinkman , m.  gaspareto , s .- j .",
    "lee , a.  ribickas , j.  perkins , w.  janssen , r.  smiley , and c.  smith .",
    "high content flow cytometry and temporal data analysis for defining a cellular signature of graft versus host disease . , 13:691700 , 2007 .",
    "g.j . mclachlan and d.",
    "robust cluster analysis via mixtures of multivariate @xmath0-distributions . in a.  amin , d.  dori , p.  pudil , and h.  freeman , editors , _ lecture notes in computer science _ ,",
    "volume 1451 , pages 658666 .",
    "springer - verlag , berlin , 1998 .",
    "s.  pyne , x.  hu , k.  wang , e.  rossin , t .-",
    "lin , l.  m. maier , c.  baecher - allan , g.  j. mclachlan , p.  tamayo , d.  a. hafler , p.  l. de  jager , and j.  p. mesirow .",
    "automated high - dimensional flow cytometric data analysis .",
    ", 106:85198524 , 2009 ."
  ],
  "abstract_text": [
    "<S> we show how the expectation - maximization ( em ) algorithm can be applied exactly for the fitting of mixtures of a general multivariate skew @xmath0 ( mst ) distributions , eliminating the need for computationally expensive monte carlo estimation . </S>",
    "<S> finite mixtures of mst distributions have proven to be useful in modelling heterogeneous data with asymmetric and heavy tail behaviour . </S>",
    "<S> recently , they have been exploited as an effective tool for modelling flow cytometric data . however , without restrictions on the the characterizations of the component skew @xmath0-distributions , monte carlo methods have been used to fit these models . in this paper , we show how the em algorithm can be implemented for the iterative computation of the maximum likelihood estimates of the model parameters without resorting to monte carlo methods for mixtures with unrestricted mst components . </S>",
    "<S> the fast calculation of semi - infinite integrals on the e - step of the em algorithm is effected by noting that they can be put in the form of moments of the truncated multivariate @xmath0-distribution , which subsequently can be expressed in terms of the non - truncated form of the @xmath0-distribution function for which fast algorithms are available . </S>",
    "<S> we demonstrate the usefulness of the proposed methodology by some applications to three real data sets . </S>"
  ]
}