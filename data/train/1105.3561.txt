{
  "article_text": [
    "the objective of a classification problem is to classify a subject to one of several classes based on a @xmath0-dimensional vector @xmath1 of characteristics observed from the subject . in most applications ,",
    "variability exists , and hence @xmath1 is random . if the distribution of @xmath1 is known , then we can construct an optimal classification rule that has the smallest possible misclassification rate . however , the distribution of @xmath1 is usually unknown , and a  classification rule has to be constructed using a training sample .",
    "a  statistical issue is how to use the training sample to construct a classification rule that has a misclassification rate close to that of the optimal rule .    in traditional applications , the dimension @xmath0 of @xmath1",
    "is fixed while the training sample size @xmath2 is large . because of the advance in technologies , nowadays a  much larger amount of information can be collected , and the resulting @xmath1 is of a high dimension .",
    "in many recent applications , @xmath0 is much larger than the training sample size , which is referred to as the large-@xmath0-small-@xmath2 problem or ultra - high dimension problem when @xmath3 for some @xmath4 .",
    "an example is a study with genetic or microarray data . in our example",
    "presented in section [ sec5 ] , for instance , a crucial step for a successful chemotherapy treatment is to classify human cancer into two classes of leukemia , acute myeloid leukemia and acute lymphoblastic leukemia , based on @xmath5 genes and a  training sample of 72 patients .",
    "other examples include data from radiology , biomedical imaging , signal processing , climate and finance .",
    "although more information is better when the distribution of @xmath1 is known , a larger dimension @xmath0 produces more uncertainty when the distribution of @xmath1 is unknown and , hence , results in a greater challenge for data analysis since the training sample size @xmath2 can not increase as fast as @xmath0 .",
    "the well - known linear discriminant analysis ( lda ) works well for fixed-@xmath0-large-@xmath2 situations and is asymptotically optimal in the sense that , when @xmath2 increases to infinity , its misclassification rate over that of the optimal rule converges to one .",
    "in fact , we show in this paper that the lda is still asymptotically optimal when @xmath0 diverges to infinity at a rate slower than @xmath6 . on the other hand",
    ", @xcite showed that the lda is asymptotically as bad as random guessing when @xmath7 ; some similar results are also given in this paper .",
    "the main purpose of this paper is to construct a sparse lda and show it is asymptotically optimal under some sparsity conditions on unknown parameters and some condition on the divergence rate of @xmath0 ( e.g. , @xmath8 as @xmath9 ) . our proposed sparse lda is based on the thresholding methodology , which was developed in wavelet shrinkage for function estimation [ @xcite , donoho et al .",
    "( @xcite ) ] and covariance matrix estimation [ @xcite ] .",
    "there exist a few other sparse lda methods , for example , @xcite , @xcite and @xcite .",
    "the key differences between the existing methods and ours are the conditions on sparsity and the construction of sparse estimators of parameters .",
    "however , no asymptotic results were established in the existing papers .    for high - dimensional @xmath1 in regression ,",
    "there exist some variable selection methods [ see a recent review by @xcite ] .",
    "for constructing a  classification rule using variable selection , we must identify not only components of @xmath1 having mean effects for classification , but also components of @xmath1 having effects for classification through their correlations with other components [ see , e.g. , @xcite , @xcite ] .",
    "this may be a  very difficult task when @xmath0 is much larger than @xmath2 , such as @xmath5 and @xmath10 in the leukemia example in section [ sec5 ] . ignoring the correlation , @xcite proposed the features annealed independence rule ( fair ) , which first selects @xmath11 components of @xmath1 having mean effects for classification and then applies the naive bayes rule ( obtained by assuming that components of @xmath1 are independent ) using the selected @xmath11 components of @xmath1 only . although no sparsity condition on the covariance matrix of @xmath12 is required , the fair is not asymptotically optimal because the correlation between components of @xmath1 is ignored .",
    "our approach is not a variable selection approach , that is , we do not try to identify a subset of components of @xmath1 with a size smaller than @xmath2 .",
    "we use thresholding estimators of the mean effects as well as bickel and levina s ( @xcite ) thresholding estimator of the covariance matrix of @xmath1 , but we allow the number of nonzero estimators ( for the mean differences or covariances ) to be much larger than @xmath2 to ensure the asymptotic optimality of the resulting classification rule .    the rest of this paper is organized as follows . in section [ sec2 ] , after introducing some notation and terminology , we establish a sufficient condition on the divergence of @xmath0 under which the lda is still asymptotically close to the optimal rule .",
    "we also show that , when @xmath0 is large compared with @xmath2 ( @xmath13 ) , the performance of the lda is not good even if we know the covariance matrix of @xmath1 , which indicates the need of sparse estimators for both the mean difference and covariance matrix . our main result is given in section [ sec3 ] , along with some discussions about various sparsity conditions and divergence rates of @xmath0 for which the proposed sparse lda performs well asymptotically",
    ". extensions of the main result are discussed in section [ sec4 ] . in section [ sec5 ]",
    ", the proposed sparse lda is illustrated in the example of classifying human cancer into two classes of leukemia , along with some simulation results for examining misclassification rates .",
    "all technical proofs are given in section [ sec6 ] .",
    "we focus on the classification problem with two classes .",
    "the general case with three or more classes is discussed in section [ sec4 ] .",
    "let @xmath1 be a @xmath0-dimensional normal random vector belonging to class @xmath14 if @xmath15 , @xmath16 , where @xmath17 , and @xmath18 is positive definite .",
    "the misclassification rate of any classification rule is the average of the probabilities of making two types of misclassification : classifying @xmath1 to class 1 when @xmath19 and classifying @xmath1 to class 2 when @xmath20 .",
    "if @xmath21 , @xmath22 and @xmath18 are known , then the optimal classification rule , that is , the rule with the smallest misclassification rate , classifies @xmath1 to class 1 if and only if @xmath23 , where @xmath24 , @xmath25 , and @xmath26 denotes the transpose of the vector @xmath27 .",
    "this rule is also the bayes rule with equal prior probabilities for two classes .",
    "let @xmath28 denote the misclassification rate of the optimal rule . using the normal distribution",
    ", we can show that @xmath29 where @xmath30 is the standard normal distribution function .",
    "although @xmath31 , @xmath32 if @xmath33 as @xmath34 and @xmath35 if @xmath36 .",
    "since @xmath37 is the misclassification rate of random guessing , we assume the following regularity conditions : there is a constant @xmath38 ( not depending on @xmath0 ) such that @xmath39 and @xmath40 where @xmath41 is the @xmath42th component of @xmath43 . under ( [ conds])([condd ] ) , @xmath44 , and hence @xmath45",
    "also , @xmath46 and @xmath47 so that the rate of @xmath48 is the same as the rate of @xmath49 , where @xmath50 is the @xmath51-norm of the vector @xmath27 .    in practice , @xmath52 and @xmath18 are typically unknown , and we have a training sample @xmath53 , where @xmath54 is the sample size for class  @xmath14 , @xmath55 , @xmath16 , all @xmath56 s are independent and @xmath57 is independent of @xmath1 to be classified .",
    "the limiting process considered in this paper is the one with @xmath58 .",
    "we assume that @xmath59 converges to a constant strictly between 0 and 1 ; @xmath0 is a function of @xmath2 , but the subscript @xmath2 is omitted for simplicity .",
    "when @xmath9 , @xmath0 may diverge to @xmath60 , and the limit of @xmath61 may be 0 , a positive constant , or @xmath60 .    for a classification rule",
    "@xmath62 constructed using the training sample , its performance can be assessed by the conditional misclassification rate @xmath63 defined as the average of the conditional probabilities of making two types of misclassification , where the conditional probabilities are with respect to  @xmath1 , given the training sample  @xmath57 .",
    "the unconditional misclassification rate is @xmath64 $ ] .",
    "the asymptotic performance of @xmath62 refers to the limiting behavior of @xmath65 or @xmath66 as @xmath67 . since @xmath68 , by the dominated convergence theorem ,",
    "if @xmath69 , where @xmath70 is a constant and @xmath71 denotes convergence in probability , then @xmath72 .",
    "hence , in this paper we focus on the limiting behavior of the conditional misclassification rate @xmath73 .",
    "we hope to find a rule @xmath62 such that @xmath63 converges in probability to the same limit as @xmath28 , the misclassification rate of the optimal rule .",
    "if @xmath32 , however , we hope not only @xmath74 , but also @xmath75 and @xmath28 have the same convergence rate .",
    "this leads to the following definition .",
    "[ defin1 ] let @xmath62 be a classification rule with conditional misclassification rate @xmath75 , given the training sample @xmath57 .",
    "@xmath62 is asymptotically optimal if @xmath76 .",
    "@xmath62 is asymptotically sub - optimal if @xmath77 .",
    "@xmath62 is asymptotically worst if @xmath78 .    if @xmath79 [ i.e. , @xmath80 in ( [ delta ] ) is bounded ] , then the asymptotic sub - optimality is the same as the asymptotic optimality . part ( iii ) of definition [ defin1 ]",
    "comes from the fact that @xmath37 is the misclassification rate of random guessing .    in this paper",
    "we focus on the classification rules of the form @xmath81 where @xmath82 , @xmath83 and @xmath84 are estimators of @xmath43 , @xmath85 and @xmath86 , respectively , constructed using the training sample @xmath57 .",
    "the well - known linear discriminant analysis ( lda ) uses the maximum likelihood estimators @xmath87 , @xmath88 and @xmath89 , where @xmath90 the lda is given by ( [ rule ] ) with @xmath91 , @xmath92 , @xmath93 when @xmath94 exists , and @xmath95 a generalized inverse @xmath96 when @xmath94 does not exist ( e.g. , when @xmath97 ) .",
    "a straightforward calculation shows that , given @xmath57 , the conditional misclassification rate of the lda is @xmath98    is the lda asymptotically optimal or sub - optimal according to definition  [ defin1 ] ?",
    "bickel and levina [ ( @xcite ) , theorem 1 ] showed that , if @xmath99 and @xmath100 , then the unconditional misclassification rate of the lda converges to @xmath37 so that the lda is asymptotically worst .",
    "a natural question is , for what kind of @xmath101 ( which may diverge to @xmath60 ) , is the lda asymptotically optimal or sub - optimal .",
    "the following result provides an answer .",
    "[ thm1 ] suppose that ( [ conds])([condd ] ) hold and @xmath102 .",
    "the conditional misclassification rate of the lda is equal to @xmath103\\zetaps/2 \\bigr).\\ ] ]    if @xmath80 is bounded , then the lda is asymptotically optimal and @xmath104    if @xmath105 , then the lda is asymptotically sub - optimal .    if @xmath105 and @xmath106 , then the lda is asymptotically optimal .    [ rem1 ] since @xmath107 under conditions ( [ conds ] ) and ( [ condd ] ) , when @xmath80 is bounded , @xmath108 is the same as @xmath109 , which is satisfied if @xmath110 with @xmath111 .",
    "when @xmath105 , @xmath108 is stronger than @xmath112 . under ( [ conds])([condd ] ) , @xmath113 .",
    "hence , the extreme case is @xmath114 is a constant times @xmath0 , and the condition in part ( iv ) becomes @xmath115 , which holds when @xmath110 with @xmath116 . in the traditional applications with a fixed @xmath0 , @xmath80 is bounded , @xmath117 as @xmath9 and thus theorem [ thm1 ] proves that the lda is asymptotically optimal .",
    "the proof of part ( iv ) of theorem [ thm1 ] ( see section [ sec6 ] ) utilizes the following lemma , which is also used in the proofs of other results in this paper .",
    "[ lem1 ] let @xmath118 and @xmath119 be two sequences of positive numbers such that @xmath120 and @xmath121 as @xmath9 .",
    "if @xmath122 , where @xmath123 may be 0 , positive , or @xmath60 , then @xmath124    since the lda uses @xmath96 to estimate @xmath86 when @xmath7 and is asymptotically worst as @xcite showed , one may think that the bad performance of the lda is caused by the fact that @xmath96 is not a good estimator of @xmath86 .",
    "our following result shows that the lda may still be asymptotically worst even if we can estimate @xmath86 perfectly .",
    "[ thm2 ] suppose that ( [ conds])([condd ] ) hold , @xmath100 and that @xmath18 is known so that the lda is given by ( [ rule ] ) with @xmath125 , @xmath91 and @xmath126 .    if @xmath127 ( which is true if @xmath128 ) , then @xmath129 .    if @xmath130 with @xmath131 , then @xmath132 a constant strictly between 0 and @xmath37 and @xmath133 .    if @xmath134 , then @xmath135 but @xmath136 .",
    "theorem [ thm2 ] shows that even if @xmath18 is known , the lda may be asymptotically worst and the best we can hope is that the lda is asymptotically sub - optimal .",
    "it can also be shown that , when @xmath21 and @xmath22 are known and we apply the lda with @xmath137 and @xmath138 , the lda is still not asymptotically optimal when @xmath139 , where @xmath140 is any sub - vector of @xmath43 with dimension @xmath2 .",
    "this indicates that , in order to obtain an asymptotically optimal classification rule when @xmath101 is much larger than @xmath2 , we need sparsity conditions on @xmath18 and @xmath43 when both of them are unknown . for bounded @xmath80 ( in which case the asymptotic optimality is the same as the asymptotic sub - optimality ) , by imposing sparsity conditions on @xmath18 , @xmath21 and @xmath22 , theorem 2 of bickel and levina ( @xcite ) shows the existence of an asymptotically optimal classification rule . in the next section ,",
    "we obtain a result by relaxing the boundedness of @xmath80 and by imposing sparsity conditions on @xmath18 and  @xmath43 .",
    "since the difference of the two normal distributions is in @xmath43 , imposing a sparsity condition on @xmath43 is weaker and more reasonable than imposing sparsity conditions on both @xmath21 and @xmath22 .",
    "we focus on the situation where the limit of @xmath61 is positive or @xmath60 .",
    "the following sparsity measure on @xmath18 is considered in @xcite : @xmath141 where @xmath142 is the @xmath143th element of @xmath18 , @xmath144 is a constant not depending on @xmath0 , @xmath145 and @xmath146 is defined to be 0 . in the special case of @xmath147 , @xmath148 in ( [ cp ] ) is the maximum of the numbers of nonzero elements of rows of @xmath18 so that a @xmath148 much smaller than @xmath0 implies many elements of @xmath18 are equal to 0 . if @xmath149 is much smaller than @xmath0 for a constant @xmath150 , then @xmath18 is sparse in the sense that many elements of @xmath18 are very small .",
    "an example of @xmath149 much smaller than @xmath0 is @xmath151 or @xmath152 .    under conditions ( [ conds ] ) and",
    "@xmath153 @xcite showed that @xmath154 where @xmath155 , @xmath156 is @xmath89 thresholded at @xmath157 with a positive constant @xmath158 ; that is , the @xmath143th element of @xmath156 is @xmath159 , @xmath160 is the @xmath143th element of @xmath89 and @xmath161 is the indicator function of the set @xmath162 .",
    "we consider a slight modification , that is , only off - diagonal elements of @xmath89 are thresholded .",
    "the resulting estimator is still denoted by @xmath156 and it has property ( [ bl ] ) under conditions ( [ conds ] ) and ( [ condp ] ) .",
    "we now turn to the sparsity of @xmath43 . on one hand , a large @xmath80 results in a  large difference between @xmath163 and @xmath164 so that the optimal rule has a small misclassification rate . on the other hand , a larger divergence rate of @xmath80 results in a more difficult task of constructing a good classification rule , since @xmath43 has to be estimated based on the training sample @xmath57 of a size much smaller than @xmath0 .",
    "we consider the following sparsity measure on @xmath43 that is similar to the sparsity measure @xmath149 on @xmath18 : @xmath165 where @xmath166 is the @xmath42th component of @xmath43 , @xmath167 is a constant not depending on @xmath0 and @xmath168 .",
    "if @xmath169 is much smaller than @xmath0 for a @xmath170 , then @xmath43 is sparse . for @xmath114",
    "defined in ( [ delta ] ) , under ( [ conds])([condd ] ) , @xmath171 .",
    "hence , the rate of divergence of @xmath114 is always smaller than that of @xmath169 and , in particular , @xmath80 is bounded when @xmath169 is bounded for a @xmath172 .",
    "we consider the sparse estimator @xmath173 that is @xmath82 thresholded at @xmath174 with constants @xmath175 and @xmath176 , that is , the @xmath42th component of @xmath173 is @xmath177 , where @xmath178 is the @xmath42th component of @xmath82 .",
    "the following result is useful .",
    "[ lem2 ] let @xmath41 be the @xmath42th component of @xmath43 , @xmath178 be the @xmath42th component of  @xmath82 , @xmath179 be given by ( [ an ] ) and @xmath180 be a fixed constant .",
    "if ( [ condp ] ) holds , then @xmath181 the number of @xmath42 s with @xmath182 , @xmath183 the number of @xmath42 s with @xmath184 and @xmath185 the number of @xmath42 s with @xmath186 .",
    "if ( [ condp ] ) holds , then @xmath187    we propose a sparse linear discriminant analysis ( slda ) for high - dimension  @xmath0 , which is given by ( [ rule ] ) with @xmath188 , @xmath189 and @xmath190 .",
    "the following result establishes the asymptotic optimality of the slda under some conditions on the rate of divergence of @xmath0 , @xmath149 , @xmath169 , @xmath191 and @xmath114 .",
    "[ thm3 ] let @xmath149 be given by ( [ cp ] ) , @xmath169 be given by ( [ dp ] ) , @xmath179 be given by ( [ an ] ) , @xmath191 be as defined in lemma [ lem2 ] and @xmath155 .",
    "assume that conditions ( [ conds ] ) , ( [ condd ] ) and ( [ condp ] ) hold and @xmath192    the conditional misclassification rate of the slda is equal to @xmath193\\zetaps/ 2 \\bigr ) .\\",
    "] ]    if @xmath80 is bounded , then the slda is asymptotically optimal and @xmath194    if @xmath105 , then the slda is asymptotically sub - optimal .    if @xmath105 and @xmath195 , then the slda is asymptotically optimal .",
    "[ rem2 ] condition ( [ bn ] ) may be achieved by an appropriate choice of @xmath196 in @xmath197 , given the divergence rates of @xmath149 , @xmath169 , @xmath191 and @xmath80 .",
    "[ rem3 ] when @xmath80 is bounded and ( [ conds])([condd ] ) hold , condition ( [ bn ] ) is the same as @xmath198    [ rem4 ] when @xmath105 , condition ( [ bn ] ) , which is sufficient for the asymptotic sub - optimality of the slda , is implied by @xmath199 , @xmath200 and @xmath201 . when @xmath105 , the condition @xmath202 , which is sufficient for the asymptotic optimality of the slda , is the same as @xmath203    we now study when condition ( [ bn ] ) holds and when @xmath202 with @xmath105 . by remarks 3 and 4 , ( [ bn ] )",
    "is the same as condition ( [ cond1 ] ) when @xmath80 is bounded , and @xmath202 is the same as condition ( [ cond2 ] ) when @xmath105 .    1 .   if there are two constants @xmath204 and @xmath205 such that @xmath206 is exactly the number of nonzero @xmath41 s",
    ". under condition",
    "( [ condd ] ) , @xmath114 and @xmath207 have exactly the order @xmath191 .",
    "a.   if @xmath191 is bounded ( e.g. , there are only finitely many nonzero @xmath41 s ) , then @xmath80 is bounded and condition ( [ bn ] ) is the same as condition  ( [ cond1 ] ) .",
    "the last two convergence requirements in ( [ cond1 ] ) are implied by @xmath208 , which is the condition for the consistency of @xmath209 proposed by @xcite .",
    "b.   when @xmath210 ( @xmath105 ) , we assume that @xmath211 and @xmath212 with @xmath213 and @xmath214 .",
    "then , condition ( [ cond2 ] ) is implied  by @xmath215\\\\[-8pt ] n^{2\\eta+ \\gamma-1 } & \\rightarrow&0.\\nonumber\\end{aligned}\\ ] ] if we choose @xmath216 , then condition ( [ cond22 ] ) holds when @xmath217 and @xmath218 . to achieve ( [ cond22 ] )",
    "we need to know the divergence rate of @xmath0 .",
    "if @xmath219 for a @xmath220 , then @xmath221 , and thus condition ( [ cond22 ] ) holds when @xmath222 and @xmath223 . if @xmath224 for a @xmath4 , which is referred to as an ultra - high dimension , then @xmath225 , and condition ( [ cond22 ] ) holds if @xmath226 and @xmath227 .",
    "2 .   since @xmath228 and @xmath229",
    "we conclude that @xmath230 the right - hand side of ( [ qn ] ) can be used as a bound of the divergence rate of @xmath191 when @xmath210 , although it may not be a tight bound .",
    "for example , if @xmath231 and the right - hand side of ( [ qn ] ) is used as a bound for @xmath191 , then the last convergence requirement in ( [ cond1 ] ) or ( [ cond2 ] ) is implied by the first convergence requirement in ( [ cond1 ] ) or ( [ cond2 ] ) when @xmath232 .",
    "3 .   if @xmath233 , then the second convergence requirement in ( [ cond1 ] ) or ( [ cond2 ] ) is implied by the first convergence requirement in ( [ cond1 ] ) or ( [ cond2 ] ) when @xmath234 $ ] .",
    "4 .   consider the case where @xmath235 , @xmath236 and an ultra - high dimension , that is , @xmath224 for a @xmath4 . from the previous discussion ,",
    "condition ( [ cond1 ] ) holds if @xmath199 , and ( [ cond2 ] ) holds if @xmath237 . since @xmath238 , @xmath239 , which converges to 0 if @xmath240 . if @xmath80 is bounded , then @xmath199 is sufficient for condition  ( [ bn ] ) .",
    "if @xmath105 , then the largest divergence rate of @xmath114 is @xmath241 and @xmath242 ( i.e. , the slda is asymptotically optimal ) when @xmath243 .",
    "when @xmath244 , this means @xmath245 .",
    "if the divergence rate of @xmath0 is smaller than @xmath246 then we can afford to have a larger than @xmath247 divergence rate for @xmath149 and @xmath169 .",
    "for  example , if @xmath219 for a @xmath220 and @xmath248 for a @xmath249 and a positive constant @xmath70 , then @xmath250 diverges to @xmath60 at a rate slower than @xmath251 .",
    "we now study when condition ( [ cond1 ] ) holds .",
    "first , @xmath252 , which converges to 0 if @xmath253 .",
    "second , @xmath254 , which converges to 0 if @xmath196 is chosen so that @xmath255 $ ] . finally ,",
    "if we use the right - hand side of ( [ qn ] ) as a bound for @xmath191 , then @xmath256 , which converges to 0 if @xmath257 $ ] .",
    "thus , condition ( [ cond1 ] ) holds if @xmath258 and @xmath259 < \\alpha\\leq(1-\\gamma)/[2(1-g)]$ ] . for condition ( [ cond2 ] )",
    ", we assume that @xmath260 with a @xmath261 $ ] ( @xmath262 corresponds to a bounded @xmath80 ) .",
    "then , a similar analysis leads to the conclusion that condition ( [ cond2 ] ) holds if @xmath263 and @xmath264 < \\alpha\\leq[1-(1+\\rho)\\gamma]/[2(1-g)]$ ] .    to apply the slda",
    ", we need to choose two constants , @xmath158 in the thresholding estimator @xmath156 and @xmath265 in the thresholding estimator @xmath173 .",
    "we suggest a  data - driven method via a cross - validation procedure .",
    "let @xmath266 be the data set containing the entire training sample but with @xmath56 deleted , and let @xmath267 be the slda rule based on @xmath266 , @xmath268 , @xmath16 .",
    "the leave - one - out cross - validation estimator of the misclassification rate of the slda is @xmath269 where @xmath270 is the indicator function of whether @xmath267 classifies @xmath56 incorrectly .",
    "let @xmath271 denote @xmath272 when the sample sizes are @xmath273 and @xmath274 .",
    "then @xmath275 which is close to @xmath276 for large @xmath54 .",
    "let @xmath277 be the cross - validation estimator when @xmath278 is used in thresholding @xmath279 and @xmath82 .",
    "then , a data - driven method of selecting @xmath278 is to minimize @xmath277 over a suitable range of @xmath278 .",
    "the resulting @xmath280 can also be used as an estimate of the misclassification rate of the slda .",
    "we first consider an extension of the main result in section [ sec3 ] to nonnormal @xmath1 and @xmath56 s . for nonnormal @xmath1 , the lda with known @xmath52 and @xmath18 , that is , the rule classifying @xmath1 to class 1 if and only if @xmath23 , is still optimal when @xmath1 has an elliptical distribution [ see , e.g. , @xcite ] with density @xmath281 where @xmath282 is either @xmath21 or @xmath22",
    ", @xmath283 is a monotone function on @xmath284 , and @xmath285 is a  normalizing constant . special cases of ( [ elli ] ) are the multivariate @xmath286-distribution and the multivariate double - exponential distribution .",
    "although this rule is not necessarily optimal when the distribution of @xmath1 is not of the form ( [ elli ] ) , it is still a reasonably good rule when @xmath52 and @xmath18 are known .",
    "thus , when @xmath52 and @xmath18 are unknown , we study whether the misclassification rate of the slda defined in section [ sec3 ] is close to that of the lda with known @xmath52 and @xmath18.=-1    from the proofs for the asymptotic properties of the slda in section [ sec3 ] , the results depending on the normality assumption are :    result ( [ bl ] ) , the consistency of @xmath209 ;    results ( [ prob1 ] ) and ( [ prob2 ] ) in lemma [ lem2 ] ;    the form of the optimal misclassification rate given by ( [ delta ] ) ;    the result in lemma [ lem1 ] .    thus , if we relax the normality assumption , we need to address ( i)(iv ) . for ( i ) , it was discussed in section 2.3 of @xcite that result ( [ bl ] ) still holds when the normality assumption is replaced by one of the following two conditions .",
    "the first condition is @xmath287 for a constant @xmath288 , where @xmath289 is the @xmath42th component of @xmath56 . under condition  ( [ case1 ] ) ,",
    "result ( [ bl ] ) holds without any modification . the second condition is=-1 @xmath290 for a constant @xmath291 . under condition ( [ case2 ] ) ,",
    "result ( [ bl ] ) holds with @xmath292 changed to @xmath293 .",
    "the same argument can be used to address ( ii ) , that is , results ( [ prob1 ] ) and ( [ prob2 ] ) hold under condition ( [ case1 ] ) or condition ( [ case2 ] ) with @xmath292 replaced by @xmath294 . for ( iii ) , the normality of @xmath1 can be relaxed to that , for any @xmath0-dimensional nonrandom vector @xmath295 with @xmath296 and any real number @xmath286 , @xmath297 where @xmath298 is an unknown distribution function symmetric about 0 but it does not depend on @xmath295 .",
    "distributions satisfying ( [ case3 ] ) include elliptical distributions [ e.g. , a distribution of the form ( [ elli ] ) ] and the multivariate scale mixture of normals [ @xcite ] . under ( [ case3 ] ) , when @xmath52 and @xmath18 are known , the lda has misclassification rate @xmath299 with @xmath80 given by ( [ delta ] ) .",
    "it remains to address ( iv ) .",
    "note that the following result , @xmath300 is the key for lemma [ lem1 ] . without assuming normality",
    ", we consider the condition @xmath301 where @xmath302 is a constant , @xmath303 , @xmath304 is a constant and @xmath70 is a positive constant . for the case where @xmath298 is standard normal , condition ( [ case4 ] ) holds with @xmath305 , @xmath306 and @xmath307 . under condition ( [ case4 ] )",
    ", we can show that the result in lemma holds for the case of @xmath308 , which is needed to extend the result in theorem [ thm3](iv ) .",
    "this leads to the following extension .",
    "[ thm4 ] assume condition ( [ case3 ] ) and either condition ( [ case1 ] ) or ( [ case2 ] ) .",
    "when condition ( [ case1 ] ) holds , let @xmath309 be defined by ( [ bn ] ) .",
    "when condition ( [ case2 ] ) holds , let @xmath197 and @xmath309 be defined by ( [ an ] ) and ( [ bn ] ) , respectively , with @xmath310 replaced by @xmath293 .",
    "assume that @xmath311 and @xmath312 .",
    "the conditional misclassification rate of the slda is @xmath313 \\zetaps/2 \\bigr).\\ ] ]    if @xmath80 is bounded , then @xmath314 where @xmath315 is the misclassification rate of the lda when @xmath52 and @xmath18 are known .    if @xmath105 , then @xmath316 .    if @xmath105 and @xmath195 , then @xmath317    we next consider extending the results in sections [ sec2 ] and [ sec3 ] to the classification problem with @xmath318 classes .",
    "let @xmath1 be a @xmath0-dimensional normal random vector belonging to class @xmath14 if @xmath15 , @xmath319 , and the training sample be @xmath320 , where @xmath54 is the sample size for class @xmath14 , @xmath55 , @xmath319 , and all @xmath56 s are independent .",
    "the lda classifies @xmath1 to class @xmath14 if and only if @xmath321 for all @xmath322 , @xmath323 , where @xmath324 , @xmath325 , @xmath326 and @xmath327 is an inverse or a generalized inverse of @xmath328 , and @xmath329 .",
    "the conditional misclassification rate of the lda is @xmath330 where @xmath331 is the probability with respect to @xmath332 , @xmath319 .",
    "the slda and its conditional misclassification rate can be obtained by simply replacing @xmath333 and @xmath334 by their thresholding estimators @xmath156 and @xmath335 , respectively . for simplicity of computation , we suggest the use of the same thresholding constant ( [ an ] ) for all @xmath336 s .    the optimal rate can be calculated as @xmath337 where @xmath338 and @xmath339 , @xmath340 , @xmath341 .",
    "asymptotic properties of the lda and slda can be obtained , under the asymptotic setting with @xmath9 and @xmath342 a constant in @xmath343 for each @xmath14 .",
    "sparsity conditions should be imposed to each @xmath344 .",
    "if the probabilities in expression  ( [ misrate ] ) do not converge to 0 , then the asymptotic optimality of the lda ( under the conditions in theorem [ thm1 ] ) or the slda ( under the conditions in theorem [ thm3 ] ) can be established using the same proofs as those in section [ sec6 ] .",
    "when @xmath28 in ( [ misrate ] ) converges to 0 , to consider convergence rates , the proof of the asymptotic optimality of the lda or slda requires an extension of lemma [ lem1 ] .",
    "specifically , we need an extension of result ( [ normal ] ) to the case of multivariate normal distributions . this technical issue , together with empirical properties of the slda with @xmath318 , will be investigated in our future research .",
    "golub et al . ( @xcite ) applied gene expression microarray techniques to study human acute leukemia and discovered the distinction between acute myeloid leukemia ( aml ) and acute lymphoblastic leukemia ( all ) . distinguishing all from aml is crucial for successful treatment , since chemotherapy regimens for all can be harmful for aml patients .",
    "an accurate classification based solely on gene expression monitoring independent of previous biological knowledge is desired as a general strategy for discovering and predicting cancer classes .",
    "we considered a dataset that was used by many researchers [ see , e.g. , @xcite ] .",
    "it contains the expression levels of @xmath5 genes for @xmath10 patients .",
    "patients in the sample are known to come from two distinct classes of leukemia : @xmath345 are from the all class , and @xmath346 are from the aml class .",
    "figure [ fig1 ] displays the cumulative proportions defined as @xmath347 , @xmath348 , where @xmath349 is the @xmath42th largest value among the squared components of @xmath82 .",
    "these proportions indicate the importance of the contribution of each @xmath350 . it can be seen from figure [ fig1 ] that the first 1,000 @xmath350 s contribute a  cumulative proportion nearly 98% .",
    "figure [ fig2 ] plots the absolute values of the off - diagonal elements of the sample covariance matrix @xmath89 .",
    "it can be seen that many of them are relatively small .",
    "if we ignore a factor of @xmath351 , then among a total of 25,407,756 values in figure [ fig2 ] , only 0.45% of them vary from 0.35 to 9.7 and the rest of them are under 0.35 .    .",
    "]    . ]    for the slda , to construct sparse estimates of @xmath43 and @xmath18 by thresholding , we applied the cross - validation method described in the end of section [ sec3 ] to choose the constants @xmath158 and @xmath265 in the thresholding values @xmath352 and @xmath353 .",
    "figure [ fig3 ] shows the cross validation scores @xmath277 over a range of @xmath278 .",
    "the minimum cross validation score is achieved at @xmath354 and @xmath355 .",
    "these thresholding values resulted in a @xmath173 with exactly 2,492 nonzero components , which is about 35% of all components of @xmath82 , and a @xmath209 with exactly 227,083 nonzero elements , which is about 0.45% of all elements of @xmath89 .",
    "note that the number of nonzero estimates of @xmath43 is still much larger than @xmath10 , but the slda does not require it to be smaller than @xmath2 .",
    "the resulting slda has an estimated ( by cross validation ) misclassification rate 0.0278 .",
    "in fact , 1 of the 47 all cases and 1 of the 25 aml cases are misclassified under the cross validation evaluation of the slda .    for comparison",
    ", we carried out the lda with a generalized inverse @xmath96 . in the leave - one - out cross - validation evaluation of the lda , 2 of the 47 all cases and 5 of the 25 aml cases",
    "are misclassified by the lda , which results in an estimated misclassification rate 0.0972 . compared with the lda",
    ", the slda reduces the misclassification rate by nearly 70% . from figure 5 of @xcite ,",
    "the misclassification rate of the fair method , estimated by the average of 100 randomly constructed cross validations with @xmath356 data points for constructing classifier and @xmath357 data points for validation ( @xmath358 and @xmath359 ) , ranges from 5% to 7% , which is smaller than the misclassification rate of the lda but larger than the misclassification rate of the slda .",
    "we also performed a simulation study on the conditional misclassification rate of slda under a population constructed using estimates from the real data set and a smaller dimension @xmath360 .",
    "the smaller dimension was used to reduce the computational cost and the 1,714 variables were chosen from the 7,129 variables with @xmath0-values ( of the two sample @xmath286-tests for the mean effects ) smaller than 0.05 . in each of the 100 independently generated data sets , independent",
    "@xmath361 and @xmath362 were generated from @xmath363 and @xmath364 , respectively , where @xmath360 and @xmath365 and @xmath209 are estimates from the real data set .",
    "the sparse estimate @xmath209 was used instead of the sample covariance matrix @xmath89 , because @xmath89 is not positive definite .",
    "since the population means and covariance matrix are known in the simulation , we were able to compute the conditional misclassification rate @xmath366 for each generated data set .",
    "a boxplot of 100 values of @xmath367 in the simulation is given in figure [ fig4](a ) .",
    "the unconditional misclassification rate of the slda can be approximated by averaging over the 100 conditional misclassification rates . in this simulation ,",
    "the unconditional misclassification rate for the slda is 0.069 . since the population is known in simulation , the optimal misclassification rate @xmath368 is known to be 0.03 .    for comparison ,",
    "in the simulation we computed the conditional misclassification rates , @xmath369 for the lda and @xmath370 for the shrunken centroids regularized discriminant analysis ( scrda ) proposed by @xcite .",
    "since @xmath370 does not have an explicit form , it is approximated by an independent test data set of size @xmath371 in each simulation run .",
    "boxplots of @xmath372 and @xmath370 for 100 simulated data sets are included in figure [ fig4](a ) .",
    "it can be seen that the conditional misclassification rate of the lda varies more than that of the slda .",
    "the unconditional misclassification rate for the lda , approximated by the 100 simulated @xmath369 values , is 0.152 , which indicates a 53% improvement of the slda over the lda in terms of the unconditional misclassification rate .",
    "the scrda has a simulated unconditional misclassification rate 0.137 and its performance is better than that of the lda but worse than that of the slda . in this simulation",
    ", we also found that the conditional misclassification rate of the fair method was similar to that of the lda .    to examine the performance of these classification methods in the case of nonnormal data",
    ", we repeated the same simulation with the multivariate normal distribution replaced by the multivariate @xmath286-distribution with 3 degrees of freedom .",
    "the boxplots are given in figure [ fig4](b ) and the simulated unconditional misclassification rates are 0.059 , 0.194 and 0.399 for the slda , scrda and lda , respectively .",
    "since the @xmath286-distribution has a larger variability than the normal distribution , all conditional misclassification rates in the @xmath286-distribution case vary more than those in the normal distribution case.=-1",
    "proof of theorem [ thm1 ] ( i ) let @xmath373 and @xmath374 be the @xmath143th elements of @xmath89 and @xmath18 , respectively . from result ( 10 ) in @xcite , @xmath375 . then , @xmath376 where @xmath377 is the norm of the matrix @xmath378 defined as the maximum of all eigenvalues of @xmath378 . by ( [ conds])([condd ] ) and @xmath109 , @xmath94 exists and @xmath379 consequently , @xmath380 = \\hat\\bfdelta { } ' \\bfsigma^{-1 } \\hat\\bfdelta[1+o_p(s_n ) ] .\\ ] ] since @xmath381 = o(p / n)$ ] and @xmath382 ^ 2 \\leq\\zetap e [ ( \\hat\\bfdelta- \\bfdelta ) ' \\bfsigma^{-1 } ( \\hat\\bfdelta- \\bfdelta)]$ ]",
    ", we have @xmath383\\\\ & = & \\zetap[1 + o_p ( s_n ) ] , \\end{aligned}\\ ] ] where the last equality follows from @xmath384 . combining these results",
    ", we obtain that @xmath385 = \\zetap[1+o_p(s_n)]^2\\\\ & = & \\zetap[1+o_p(s_n)].\\end{aligned}\\ ] ] then @xmath386}}{2\\sqrt{1+o_p(s_n ) } } + o_p \\biggl ( \\sqrt{\\frac{p}{n } } \\biggr)\\\\ & = & - \\frac{\\zetaps}{2 } [ 1 + o_p(s_n ) ] + o_p \\biggl ( \\sqrt{\\frac{p}{n } } \\biggr)\\\\ & = & - \\frac{\\zetaps}{2 } \\biggl [ 1 + o_p(s_n ) + o_p\\biggl ( \\frac{\\sqrt{p}}{\\sqrt{n } \\zetaps } \\biggr ) \\biggr]\\\\ & = & - \\frac{\\zetaps}{2 } [ 1 + o_p(s_n ) ] .\\end{aligned}\\ ] ] similarly , we can show that @xmath387.\\ ] ] these results and formula ( [ rate ] ) imply the result in ( i ) .",
    "let @xmath388 be the density of @xmath30 . by the result in ( i ) ,",
    "@xmath389 where @xmath390 is between @xmath391 and @xmath392 \\zetaps/2 $ ] .",
    "since @xmath393 is bounded by a constant , the result follows from the fact that @xmath28 is bounded away from 0 when @xmath80 is bounded .",
    "when @xmath105 , @xmath32 , and , by the result in ( i ) , @xmath394 .",
    "if @xmath105 , then , by lemma [ lem1 ] and the condition @xmath395 , we conclude that @xmath396 .",
    "proof of lemma [ lem1 ] it follows from result ( [ normal ] ) that @xmath397/2 } & \\leq & \\frac{\\phi ( - \\sqrt{\\xi_n } ( 1-\\tau_n ) ) } { \\phi ( - \\sqrt{\\xi_n } ) } \\\\ & \\leq & \\frac{1 + \\xi_n}{\\xi_n ( 1 - \\tau_n ) } e^{[\\xi_n - \\xi_n(1-\\tau_n)^2]/2 } .\\end{aligned}\\ ] ] since @xmath120 and @xmath121 , @xmath398 the result follows from @xmath399/2 = \\xi_n \\tau_n ( 1 - \\tau_n/2 ) \\rightarrow\\gamma$ ] regardless of whether @xmath400 is 0 , positive , or @xmath60 .",
    "proof of theorem [ thm2 ] for simplicity , we prove the case of @xmath401 .",
    "the conditional misclassification rate of the lda in this case is given by ( [ rate ] ) with @xmath333 replaced by @xmath18 .",
    "note that @xmath402 , where @xmath403 is the identity matrix of order @xmath0 .",
    "let @xmath404 be the @xmath42th component of @xmath405 .",
    "then , @xmath406 and the @xmath42th component of @xmath407 is @xmath408 , and the @xmath42th component of @xmath409 is @xmath410 , @xmath411 , where @xmath412 , @xmath411 , @xmath16 , are independent standard normal random variables .",
    "consequently , @xmath413 \\\\ % & = & \\sum_{j=1}^p \\biggl ( - \\frac{\\zeta_j^2}{2 } + \\frac{\\varep_{1j}^2 - \\varep_{2j}^2}{n } + \\frac{\\zeta_j \\varep_{2j}}{\\sqrt{n_1 } } \\biggr)\\\\ & = & - \\frac{\\zetap}{2 } + \\frac{1}{n } \\sum_{j=1}^p ( \\varep_{1j}^2 - \\varep_{2j}^2 ) + \\frac{1}{\\sqrt{n_1 } } \\sum_{j=1}^p \\zeta_j \\varep_{2j}\\\\ & = & - \\frac{\\zetap}{2 } + o_p \\biggl ( \\frac{\\sqrt{p}}{n } \\biggr ) + o_p \\biggl ( \\frac { \\zetaps}{\\sqrt{n } } \\biggr)\\end{aligned}\\ ] ] and @xmath414 + o_p \\biggl ( \\frac { \\zetaps}{\\sqrt{n } } \\biggr)\\\\ & = & \\zetap+ \\frac{4p}{n } [ 1 + o_p(1 ) ] , \\end{aligned}\\ ] ] where the last equality follows from @xmath415 under ( [ conds])([condd ] ) . combining these results , we obtain that @xmath416 } } + o_p(1).\\ ] ] similarly , we can prove that ( [ r1 ] ) still holds if @xmath417 is replaced by @xmath418 . if @xmath127 , then the quantity in ( [ r1 ] ) converges to 0 in probability .",
    "hence , @xmath419 .    since @xmath420 , @xmath421 .",
    "then , the quantity in ( [ r1 ] ) converges to @xmath422 in probability and , hence , @xmath423 , which is a constant between 0 and @xmath37 . since @xmath33 , @xmath32 and , hence , @xmath133 .",
    "when @xmath134 , it follows from ( [ r1 ] ) that the quantity on the left - hand side of ( [ r1 ] ) diverges to @xmath424 in probability .",
    "this proves that @xmath425 . to show @xmath426 @xmath427",
    ", we need a more refined analysis .",
    "the quantity on the left - hand side of ( [ r1 ] ) is equal to @xmath428 } } = - \\frac{\\zetaps}{2 } ( 1-\\tau_n ) , \\ ] ] where @xmath429 } } \\ ] ] and @xmath430 .",
    "note that @xmath431 } } \\\\ & = & \\frac{({4p}/{n } ) [ 1 + o_p(1 ) ] } { \\zetap+ ( { 4p}/{n } ) [ 1 + o_p(1 ) ] + \\zetaps\\sqrt{\\zetap+ ( { 4p}/{n } ) [ 1 + o_p(1 ) ] } } \\end{aligned}\\ ] ] and @xmath432 } } = \\frac { o_p ( \\sqrt{p}/n)}{\\zetap } + \\frac{o_p ( 1/\\sqrt{n } ) } { \\zetaps}\\\\ & = & \\frac{o_p ( \\sqrt{p / n } ) } { \\zetap}\\end{aligned}\\ ] ] under ( [ conds ] ) and ( [ condd ] )",
    ". then @xmath433 if @xmath434 is bounded , then @xmath435 for a constant @xmath436 and @xmath437 which diverges to @xmath60 in probability since @xmath438 . if @xmath439 , then @xmath440 for a constant @xmath436 and @xmath441 which diverges to @xmath60 in probability since @xmath13 .",
    "thus , @xmath442 in probability , and the result follows from lemma [ lem1 ] .",
    "proof of lemma [ lem2 ]",
    "( i ) it follows from ( [ normal ] ) that , for all @xmath286 , @xmath443 where @xmath204 and @xmath205 are positive constants .",
    "then , the probability in ( [ prob1 ] ) is @xmath444 because @xmath445 when @xmath446 , we conclude that @xmath447 , and thus ( [ prob1 ] ) holds .",
    "the proof of ( [ prob2 ] ) is similar since @xmath448 ( ii ) the result follows from results ( [ prob1 ] ) and ( [ prob2 ] ) .",
    "proof of theorem [ thm3 ] the conditional misclassification rate @xmath449 is given by @xmath450 from result ( [ bl ] ) , @xmath451 = \\tilde\\bfdelta { } ' \\bfsigma^{-1 } \\tilde\\bfdelta[1+o_p(d_n ) ] .\\ ] ] without loss of generality , we assume that @xmath452 , where @xmath453 is the @xmath454-vector containing nonzero components of @xmath173 .",
    "let @xmath455 , where @xmath456 has dimension @xmath454 . from lemma [ lem2](ii ) ,",
    "@xmath457 and , with probability tending to 1 , @xmath458 let @xmath459 .",
    ". this together with ( [ conds])([condd ] ) implies that @xmath461 and hence @xmath462\\\\ & = & \\zetap\\bigl [ 1 + o_p \\bigl(\\sqrt{k_n}/\\zetaps\\bigr ) \\bigr].\\end{aligned}\\ ] ] write @xmath463 where @xmath464 , @xmath465 , @xmath466 and @xmath467 are @xmath468 matrices with @xmath191 defined in lemma  [ lem2](ii ) .",
    "then @xmath469 if @xmath470 and @xmath471 , where @xmath472 and @xmath473 have dimension @xmath191 , then @xmath474 since @xmath473 has dimension @xmath191 , @xmath475 and hence @xmath476 since @xmath477 , @xmath478 from result ( [ bl ] ) , @xmath479 .\\ ] ] under condition ( [ conds ] ) , all eigenvalues of sub - matrices of @xmath18 and @xmath86 are bounded by @xmath38 . repeatedly using condition ( [ conds ] ) , we obtain that @xmath480 where @xmath144 and @xmath149 are given in ( [ cp ] ) .",
    "this proves that @xmath481 which also holds when @xmath482 is replaced by @xmath483 or @xmath484 .",
    "note that @xmath485 therefore , @xmath486 \\\\",
    "& = & - \\frac{\\zetaps}{2 } \\biggl [ 1 + o_p \\biggl ( \\frac{\\sqrt { c_{h , p}q_n } } { \\zetaps\\sqrt{n } } \\biggr ) \\\\ & & \\hphantom{- \\frac{\\zetaps}{2 } \\biggl [ } { } + o_p \\biggl(\\frac{\\sqrt{k_n}}{\\zetaps } \\biggr ) + o_p(d_n ) \\biggr]\\\\ & = & - \\frac{\\zetaps}{2 } [ 1+o_p ( b_n ) ] .\\end{aligned}\\ ] ] this proves the result in ( i ) .",
    "the proofs of ( ii)(iv ) are the same as the proofs for theorem [ thm1](ii)(iv ) with @xmath487 replaced by @xmath309 .",
    "this completes the proof .",
    "the authors would like to thank two referees and an associate editor for their helpful comments and suggestions , and dr .",
    "weidong liu for his help in correcting an error in the proof of theorem 3 ."
  ],
  "abstract_text": [
    "<S> in many social , economical , biological and medical studies , one objective is to classify a subject into one of several classes based on a set of variables observed from the subject . because the probability distribution of the variables is usually unknown , the rule of classification is constructed using a training sample . </S>",
    "<S> the well - known linear discriminant analysis ( lda ) works well for the situation where the number of variables used for classification is much smaller than the training sample size . because of the advance in technologies , modern statistical studies often face classification problems with the number of variables much larger than the sample size , and </S>",
    "<S> the lda may perform poorly . </S>",
    "<S> we explore when and why the lda has poor performance and propose a sparse lda that is asymptotically optimal under some sparsity conditions on the unknown parameters . for illustration of application </S>",
    "<S> , we discuss an example of classifying human cancer into two classes of leukemia based on a set of 7,129 genes and a training sample of size 72 . </S>",
    "<S> a simulation is also conducted to check the performance of the proposed method .    ,    ,    and    .    </S>"
  ]
}