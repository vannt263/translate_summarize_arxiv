{
  "article_text": [
    "physiological information about neural structure and activity was employed from the very beginning to construct effective mathematical models of brain functions .",
    "typically , neural networks were introduced as assemblies of elementary dynamical units , that interact with each other through a graph of connections @xcite . under the stimulus of experimental investigations ,",
    "these models have been including finer and finer details .",
    "for instance , the combination of complex single  neuron dynamics , delay and plasticity in synaptic evolution , endogenous noise and specific network topologies revealed quite crucial for reproducing experimental observations , like the spontaneous emergence of synchronized neural activity , both _ in vitro _",
    "( see , e.g. , @xcite ) and _ in vivo _ , and the appearance of peculiar fluctuations , the so  called  up  down \" states , in cortical sensory areas @xcite .",
    "since the brain activity is a dynamical process , its statistical description needs to take into account time as an intrinsic variable .",
    "accordingly , non  equilibrium statistical mechanics should be the proper conceptual frame , where effective models of collective brain activity should be casted in .",
    "moreover , the large number of units and the redundancy of connections suggest that a mean  field approach can be the right mathematical tool for understanding the large  scale dynamics of neural network models .",
    "several analytical and numerical investigations have been devoted to mean field approaches to neural dynamics .",
    "in particular , stability analysis of asynchronous states in globally coupled networks and collective observables in highly connected sparse network can be deduced in relatively simple neural network models through mean field techniques @xcite .    in this paper",
    "we provide a detailed account of a mean  field approach , that has been inspired by the ",
    "heterogeneous mean  field \" ( hmf ) formulation , recently introduced for general interacting networks @xcite .",
    "the overall method is applied here to the simple case of random networks of leaky integrate  and  fire ( lif ) excitatory neurons in the presence of synaptic plasticity .",
    "on the other hand , it can be applied to a much wider class of neural network models , based on a similar mathematical structure .",
    "the main advantages of the hmf method are the following : ( _ i _ ) it can identify the relation between the dynamical properties of the global ( _ synaptic _ ) activity field and the network topology , ( _ ii _ ) it allows one to establish under which conditions partially synchronized or irregular firing events may appear , ( _ iii _ ) it provides a solution to the inverse problem of recovering the network structure from the features of the global activity field .    in section [ sec2 ] , we describe the network model of excitatory lif neurons with short  term plasticity .",
    "the dynamical properties of the model are discussed at the beginning of section [ sec3 ] . in particular , we recall that the random structure of the network is responsible for the spontaneous organization of neurons in two families of _ locked _ and _ unlocked _ ones @xcite . in the rest of this section",
    "we summarize how to define a _ heterogeneous thermodynamic limit _ , that preserves the effects of the network randomness and allows one to transform the original dynamical model into its hmf representation @xcite ) .",
    "the hmf equations provide a relevant computational advantage with respect to the original system .",
    "actually , they describe the dynamics of classes of equal  in  degree neurons , rather than that of individual neurons . in practice",
    ", one can take advantage of a suitable sampling , according to its probability distribution , of the continuous in  degree parameter present in the hmf formulation .",
    "for instance , by properly `` sampling '' the hmf model into 300 equations one can obtain an effective description of the dynamics engendered by a random erds ",
    "renyi network made of @xmath0 neurons .    in section [ sec4 ]",
    "we show that the hmf formulation allows also for a clear interpretation of the presence of classes of _ locked _ and _ unlocked _ neurons in qse : they correspond to the presence of a _ fixed point _ or of an _ intermittent - like _ map of the return time of firing events , respectively .",
    "moreover , we analyze in details the stability properties of the model and we find that any finite sampling of the hmf dynamics is chaotic , i.e. it is characterized by a positive maximum lyapunov exponent , @xmath1 .",
    "its value depends indeed on the finite sampling of the in ",
    "degree parameter . on the other hand",
    ", chaos is found to be relatively weak and , when the number of samples , @xmath2 , is increased , @xmath1 vanishes with a power  law decay , @xmath3 , with @xmath4 .",
    "this is consistent with the mean  field like nature of the hmf equations : in fact , it can be argued that , in the thermodynamic limit , any chaotic component of the dynamics should eventually disappear , as it happens for the original lif model , when a naive thermodynamic limit is performed @xcite .    in section [ sec5 ]",
    "we analyze the hmf dynamics for networks with different topologies ( e.g. , erds  renyi and in particular scale free ) .",
    "we find that the dynamical phase characterized by qse is robust with respect to the network topology and it can be observed only if the variance of the considered in ",
    "degree distributions is sufficiently small .",
    "in fact , quasi - synchronous events are suppressed for too broad in  degree distributions , thus yielding a transition between a fully asynchronous dynamical phase and a quasi - synchronous one , controlled by the variance of the in  degree distribution .",
    "in all the cases analyzed in this section , we find that the global synaptic ",
    "activity field characterizes completely the dynamics in any network topology .",
    "accordingly , the hmf formulation appears as an effective algorithmic tool for solving the following _ inverse problem _ : given a global synaptic  activity field , which kind of network topology has generated it ? in section [ sec6 ] , after a summary of the numerical procedure used to solve such an inverse problem , we analyze the robustness of the method in two circumstances : @xmath5 when a noise is added to the average synaptic  activity field , and @xmath6 when there are noise and disorder in the external currents .",
    "such robustness studies are particularly relevant in view of applying this strategy to real data obtained from experiments .",
    "finally , in section [ sec7 ] we show that a hmf formulation can be straightforwardly extended to non  massive networks , i.e. random networks , where the in  degree does not increase proportionally to the number of neurons . in this case the relevant quantity in the hmf - like formulation is the average value of the in  degree distribution , and the hmf equations are expected to reproduce confidently the dynamics of non  massive networks , provided this average is sufficiently large .",
    "conclusions and perspectives are contained in section [ sec8 ] .",
    "we consider a network of @xmath7 excitatory lif neurons interacting via a synaptic current and regulated by short  term plasticity , according to a model introduced in @xcite .",
    "the membrane potential @xmath8 of each neuron evolves in time following the differential equation @xmath9 where @xmath10 is the membrane time constant , @xmath11 is the membrane resistance , @xmath12 is the synaptic current received by neuron @xmath13 from all its presynaptic neurons ( see below for its mathematical definition ) and @xmath14 is the contribution of an external current ( properly multiplied by a unit resistance ) .    whenever the potential @xmath15 reaches the threshold value @xmath16 , it is reset to @xmath17 , and a spike is sent towards the postsynaptic neurons . for the sake of simplicity",
    "the spike is assumed to be a @xmath18like function of time .",
    "accordingly , the spike ",
    "train @xmath19 produced by neuron @xmath13 , is defined as , @xmath20 where @xmath21 is the time when neuron @xmath13 fires its @xmath22-th spike .    the transmission of the spike ",
    "train @xmath19 is mediated by the synaptic dynamics .",
    "we assume that all efferent synapses of a given neuron follow the same evolution ( this is justified in so far as no inhibitory coupling is supposed to be present ) .",
    "the state of the @xmath23-th synapse is characterized by three variables , @xmath24 , @xmath25 , and @xmath26 , which represent the fractions of synaptic transmitters in the recovered , active , and inactive state , respectively ( @xmath27 ) @xcite . the evolution equations are @xmath28 only the active transmitters react to the incoming spikes : the parameter @xmath29 tunes their effectiveness .",
    "moreover , @xmath30 is the characteristic decay time of the postsynaptic current , while @xmath31 is the recovery time from synaptic depression . for the sake of simplicity , we assume also that all parameters appearing in the above equations are independent of the neuron indices .",
    "the model equations are finally closed , by representing the synaptic current as the sum of all the active transmitters delivered to neuron @xmath13 @xmath32 where @xmath33 is the strength of the synaptic coupling ( that we assume independent of both @xmath23 and @xmath13 ) , while @xmath34 is the directed connectivity matrix whose entries are set equal to 1 or 0 if the presynaptic neuron @xmath23 is connected or disconnected with the postsynaptic neuron @xmath13 , respectively . since we suppose the input resistance @xmath11 independent of @xmath13 , it can be included into @xmath33 . in this paper",
    "we study the case of excitatory coupling between neurons , i.e. @xmath35 .",
    "we assume that each neuron is connected to a macroscopic number , @xmath36 , of pre - synaptic neurons : this is the reason why the sum is divided by the factor @xmath7 .",
    "typical values of the parameters contained in the model have phenomenological origin @xcite . unless otherwise stated , we adopt the following set of values : @xmath37 ms , @xmath38 ms , @xmath39 ms , @xmath40 mv , @xmath41 mv , @xmath42 mv , @xmath43 mv and @xmath44 .",
    "numerical simulations can be performed much more effectively by introducing dimensionless quantities , @xmath45 and by rescaling time , together with all the other temporal parameters , in units of the membrane time constant @xmath10 ( for simplicity , we leave the notation unchanged after rescaling ) .",
    "the values of the rescaled parameters are : @xmath46 , @xmath47 , @xmath48 , @xmath49 , @xmath50 , @xmath51 and @xmath44 .",
    "as to the normalized external current @xmath52 , its value for the first part of our analysis corresponds to the firing regime for neurons . while the rescaled eqs .",
    "( [ dynsyn ] ) and ( [ contz ] ) keep the same form , eq .",
    "( [ eq1 ] ) changes to , @xmath53 a major advantage for numerical simulations comes from the possibility of transforming the set of differential equations ( [ dynsyn])([input ] ) and ( [ eq1n ] ) into an event  driven map ( for details see @xcite and also @xcite ) .",
    "the dynamics of the fully coupled neural network ( i.e. , @xmath54 ) , described by eq.s ( [ eq1n ] ) and ( [ eq2])([input ] ) , converges to a periodic synchronous state , where all neurons fire simultaneously and the period depends on the model parameters @xcite .",
    "a more interesting dynamical regime appears when some disorder is introduced in the network structure .",
    "for instance , this can be obtained by maintaining each link between neurons with probability @xmath55 , so that the in - degree of a neuron ( i.e. the number of presynaptic connections acting on it ) takes the average value @xmath56 , and the standard deviation of the corresponding in - degree distribution is given by the relation @xmath57 .",
    "in such an erds - renyi random network one typically observes quasi ",
    "synchronous events ( qse ) , where a large fraction of neurons fire in a short time interval of a few milliseconds , separated by an irregular firing activity lasting over some tens of ms ( e.g. , see @xcite ) .",
    "this dynamical regime emerges as a collective phenomenon , where neurons separate spontaneously into two different families : the _ locked _ and the _ unlocked _ ones .",
    "locked neurons determine the qse and exhibit a periodic behavior , with a common period but different phases .",
    "degree @xmath58 ranges over a finite interval below the average value @xmath59 .",
    "the unlocked ones participate to the irregular firing activity and exhibit a sort of intermittent evolution @xcite .",
    "their in - degree is either very small or higher than @xmath59 .",
    "as the dynamics is very sensitive to the different values of of @xmath60 , in a recent publication @xcite we have shown that one can design a _ heterogeneous mean - field _ ( hmf ) approach by a suitable thermodynamic limit preserving , for increasing values of @xmath7 , the main features associated with topological disorder .",
    "the basic step of this approach is the introduction of a probability distribution , @xmath61 , for the normalized in - degree variable @xmath62 , where the average @xmath63 and the variance @xmath64 are fixed independently of @xmath7 .",
    "a realization of the random network containing @xmath7 nodes ( neurons ) is obtained by extracting for each neuron @xmath23 ( @xmath65 ) a value @xmath66 from @xmath61 , and by connecting the neuron @xmath23 with @xmath67 randomly chosen neurons ( i.e. , @xmath68 , @xmath69 ) .",
    "for instance , one can consider a suitably normalized gaussian  like distribution defined on the compact support , @xmath70 $ ] , centered around @xmath63 with a sufficiently small value of the standard deviation @xmath71 , so that the tails of the distribution vanish at the boundaries of the support .",
    "is a gaussian with @xmath72 , standard deviation @xmath73 .",
    "a black dot in the raster plot indicates that neuron @xmath74 has fired at time @xmath75 .",
    "the red line is the global field @xmath76 and the green curve is its analytic fit by the function @xmath77 , that repeats over each period of @xmath76 ; the parameter values are @xmath78 , @xmath79 , @xmath80 and @xmath81 .",
    "notice that the amplitude of both @xmath76 and @xmath82 has been suitably rescaled to be appreciated on the same scale of the raster plot . ]    in fig.[rp1 ] we show the raster plot for a network of @xmath83 neurons and a gaussian distribution @xmath61 with @xmath84 and @xmath85 .",
    "one can observe a quasi - synchronous dynamics characterized by the presence of locked and unlocked neurons , and such a distinctive dynamical feature is preserved in the thermodynamic limit @xcite .",
    "for example the time average of the inter  spike time interval between firing events of each neuron , ( in formulae @xmath86 , where the integer @xmath22 labels the @xmath22-th firing event ) as a function of the connectivity @xmath87 is , apart from fluctuations , the same for each network size @xmath7 .",
    "this confirms that the main features of the dynamics are maintained for increasing values of @xmath7 .",
    "the main advantage of this approach is that one can explicitly perform the limit @xmath88 on the set of equations ( [ eq1n ] ) and ( [ eq2])([input ] ) , thus obtaining the corresponding hmf equations :    @xmath89    the dynamical variables depend now on the continuous in  degree index @xmath87 , and this set of equations represents the dynamics of equivalence classes of neurons . in fact , in this hmf formulation , neurons with the same @xmath87 follow the same evolution @xcite . in practice , eq.s ( [ vk])([meanfield ] )",
    "can be integrated numerically by sampling the probability distribution @xmath61 : one can subdivide the support @xmath90 $ ] of @xmath87 by @xmath2 values @xmath91 , in such a way that @xmath92 is constant ( importance sampling ) .",
    "notice that the integration of the discretized hmf equations is much less time consuming than the simulations performed on a random network .",
    "for instance , numerical tests indicate that the dynamics of a network with @xmath93 neurons can be confidently reproduced by an importance sampling with @xmath94",
    ".    the effect of the discretization of @xmath95 on the hmf dynamics can be analyzed by considering the distance @xmath96 between the global activity fields @xmath97 and @xmath98 ( see eq.([meanfield ] ) ) obtained for two different values @xmath99 and @xmath100 of the sampling , i.e. : @xmath101 in general @xmath76 exhibits a quasi periodic behavior and @xmath96 is evaluated over a time interval equal to its period @xmath102 . in order to avoid an overestimation of @xmath96 due to different initial conditions , the field @xmath103 is suitably translated in time in order to make its first maximum coincide with the first maximum of @xmath104 in the time interval @xmath105 $ ] . in fig .",
    "[ scarto ] we plot @xmath106 as a function of @xmath2 .",
    "we find that @xmath107 , thus confirming that the finite size simulation of the hmf dynamics is consistent with the hmf model ( @xmath108 ) .     with @xmath2 classes of neurons in the hmf dynamics .",
    "finite size effects are controlled by plotting the distance between the activity fields obtained for two sampling values @xmath2 and @xmath109 , @xmath110 ( defined in the text ) , vs. @xmath2 .",
    "the red dashed line is the power law @xmath111 .",
    "data is obtained for a gaussian distribution @xmath61 , with @xmath84 and @xmath85 . ]    as a final remark , notice that the presence of short  term synaptic plasticity plays a fundamental role in determining the partially synchronized regime .",
    "in fact , numerical simulations show that the discretized hmf dynamics without plasticity , i.e. @xmath112 , converges to a synchronous periodic dynamics for any value of @xmath2 @xcite .",
    "in the hmf equations ( [ vk])([meanfield ] ) the dynamics of each neuron is determined by its in ",
    "degree @xmath87 and by the global synaptic activity field @xmath76 . for the stability analysis of these equations ,",
    "we follow a procedure introduced in @xcite and employed also in @xcite . for sufficiently large",
    "@xmath2 the discretized hmf dynamics allows one to obtain a precise fit of the periodic function @xmath76 and to estimate its period @xmath102 . as an instance of its periodic behavior , in fig.[rp1 ] we report",
    "also @xmath76 ( red line ) and its fit ( green line and the formula in the caption ) . the fitted field is exactly periodic and is a good approximation of the global field that one expects to observe in the mean field model corresponding to an infinite discretization @xmath2 . as a result ,",
    "the analysis performed using this periodic field are relative to the dynamics of the hmf model , i.e. in the limit @xmath108 . using this fit",
    ", one can represent the dynamics of each class @xmath87 of neurons by the discrete  time map @xmath113,\\ ] ] where @xmath114 is the modulus of the time difference between the @xmath115-th spike of neuron @xmath87 and @xmath116 , i.e. the @xmath115-th qse , that is conventionally identified by the corresponding maximum of @xmath76 ( see fig .",
    "[ rp1 ] ) .     of the rescaled variable @xmath117",
    "( see eq.([mappa ] ) ) for different values of @xmath87 , corresponding to lines of different colors ( see the legend in the inset : the black line is the bisector of the square ) . ]    in fig .",
    "[ maps ] we show @xmath118 for different values of @xmath87 .",
    "the map of each class of _ locked _ neurons has a stable fixed point , whose value decreases with @xmath87 . as a consequence , different classes of _ locked _ neurons share the same periodic behavior , but exhibit different phase shifts with respect to the maximum of @xmath76 .",
    "this analysis describes in a clear mathematical language what is observed in simulations ( see fig [ rp1 ] ) : equally periodic classes of locked neurons determine the qse by firing sequentially , over a very short time interval , that depends on their relative phase shift . in general ,",
    "the values of @xmath87 identifying the family of locked neurons belong to a subinterval @xmath119 of @xmath90 $ ] : the values of @xmath120 and @xmath121 mainly depend on @xmath61 and on its standard deviation @xmath71 ( more details are reported in @xcite ) . for what concerns _ unlocked _ neurons",
    ", @xmath118 exhibits the features of an intermittent - like dynamics .",
    "in fact , unlocked neurons with @xmath87 close to @xmath120 and @xmath121 spend a long time in an almost periodic firing activity , contributing to a qse , then they depart from it , firing irregularly before possibly coming back again close to a qse .",
    "the duration of the irregular firing activity of unlocked neurons typically increases for values of @xmath87 far from the interval @xmath122 .    using the deterministic map ( [ mappa ] )",
    ", one can tackle in full rigor the stability problem of the hmf model .",
    "the existence of stable fixed points for the locked neurons implies that they yield a negative lyapunov exponent associated with their periodic evolution .    as for the unlocked neurons , their lyapunov exponent , @xmath123 ,",
    "can be calculated numerically by the time - averaged expansion rate of nearby orbits of map ( [ mappa ] ) : @xmath124,\\ ] ] where @xmath125 is the initial distance between nearby orbits and @xmath126 is their distance at the @xmath13th iterate , so that @xmath127 if this limit exists . the lyapunov exponents for the unlocked component vanish as @xmath128 . according to these results",
    ", one expects that the maximum lyapunov exponent @xmath129 goes to zero in the limit @xmath130 .",
    "in fact , at each finite @xmath2 , @xmath131 can be evaluated by using the standard algorithm by benettin et al .",
    "@xcite . in fig.[lyup_max ] we plot @xmath131 as a function of the discretization parameter @xmath2 .",
    "thus , @xmath129 is positive , behaving approximately as @xmath3 , with @xmath4 ( actually , we find @xmath132 ) .",
    "the scenario in any discretized version of the hmf dynamics is the following : _",
    "( i ) _ all _ unlocked neurons _",
    "exhibit positive lyapunov exponents , i.e. they represent the chaotic component of the dynamics ; _ ( ii ) _",
    "@xmath131 is typically quite small , and its value depends on the discretization parameter @xmath2 and on @xmath61 ; _ ( iii ) _ in the limit @xmath108 @xmath133 and all @xmath123 s of unlocked neurons vanish , thus converging to a quasi periodic dynamics , while the _ locked neurons _ persist in their periodic behavior .",
    "the same scenario is observed in the dynamics of random networks built with the hmf strategy , where the variance of the distribution @xmath61 is kept independent of the system size @xmath7 , so that the fraction of locked neurons is constant .",
    "for the lif dynamics in an erdos ",
    "renyi random network with @xmath7 neurons , it was found that @xmath134 in the limit @xmath135 @xcite . according to the argument proposed in @xcite",
    ", the value of the power - law exponent is associated to the scaling of the number of unlocked neurons , @xmath136 with the system size @xmath7 , namely @xmath137 .",
    "the same argument applied to hmf dynamics indicates that the exponent @xmath4 , ruling the vanishing of @xmath138 in the limit @xmath139 , stems from the fact that the hmf dynamics keeps the fraction of unlocked neurons constant .     as a function of the sampling parameter @xmath2",
    ": @xmath131 has been averaged also over ten different realizations of the network ( the error bars refer to the maximum deviation from the average ) .",
    "the dashed red line is the powerlaw @xmath3 , with @xmath140 . ]",
    "when the distribution @xmath61 is sufficiently broad , the system becomes asynchronous and locked neurons disappear .",
    "the global field @xmath76 exhibits fluctuations due to finite size effects and in the thermodynamic limit it tends to a constant value @xmath141 . from eq.s ( [ vk])([zk ] ) , one obtains that in this regime each neuron with in  degree @xmath87 fires periodically with a period @xmath142~,\\ ] ] while its phase depends on the initial conditions . in this case",
    "all the lyapunov exponents @xmath123 are negative .",
    "for a given in ",
    "degree probability distribution @xmath61 , the fraction of locked neurons ( i.e. , @xmath143 ) decreases by increasing @xmath71 @xcite . in particular , there is a critical value @xmath144 at which @xmath145 vanishes .",
    "this signals a very interesting dynamical transition between the quasi - synchronous phase ( @xmath146 ) to a multi - periodic phase ( @xmath147 ) , where all neurons are periodic with different periods .",
    "here we focus on the different collective dynamics that may emerge for choices of @xmath61 other than the gaussian case , discussed in the previous section .",
    "first , we consider a power  law distribution @xmath148 where the constant @xmath149 is given by the normalization condition @xmath150 .",
    "the lower bound @xmath151 is introduced in order to maintain @xmath149 finite . for simplicity",
    ", we fix the parameter @xmath151 and analyze the dynamics by varying @xmath152 . notice that the standard deviation @xmath71 of distribution ( [ eqplaw ] ) decreases for increasing values of @xmath152 .",
    "the dynamics for relatively high @xmath152 is very similar to the quasi ",
    "synchronous regime observed for @xmath146 in the gaussian case ( see fig .",
    "[ rp1 ] ) . by decreasing @xmath152",
    "one can observe again a transition to the asynchronous phase observed for @xmath147 in the gaussian case .",
    "accordingly , also for the power  law distribution ( [ eqplaw ] ) a phase with locked neurons may set in only when there is a sufficiently large group of neurons sharing close values of @xmath87 . in fact , the group of locked neurons is concentrated at values of @xmath87 quite close to the lower bound @xmath151 , while in the gaussian case they concentrate at values smaller than @xmath153 .",
    "another distribution , generating an interesting dynamical phase , is @xmath154 i.e. the sum of two gaussians peaked around different values , @xmath155 and @xmath156 , of @xmath87 , with the same variance @xmath157 .",
    "@xmath158 is the normalization constant such that @xmath159 .",
    "we fix @xmath160 and vary both the variance , @xmath161 , and the distance between the peaks , @xmath162 .",
    "if @xmath161 is very large ( @xmath163 ) , the situation is the same observed for a single gaussian with large variance , yielding a multi  periodic asynchronous dynamical phase .    for intermediate values of @xmath161",
    "i.e. @xmath164 , the dynamics of the network can exhibit a quasi ",
    "synchronous phase or a multi  periodic asynchronous phase , depending on the value of @xmath165 .",
    "in fact , one can easily realize that this parameter tunes the standard deviation of the overall distribution : small separations amount to broad distributions .    finally , when @xmath166 , a new dynamical phase appears .",
    "vs. @xmath87 for the probability distribution @xmath61 defined in eq.([dgauss ] ) , with @xmath167 , and @xmath168 .",
    "we have obtained the global field @xmath76 simulating the hmf dynamics with a discretization with @xmath169 classes of neurons .",
    "we have then used @xmath76 to calculate the @xmath170 of neurons evolving eq .",
    "( [ vk ] ) . in the inset",
    "we show the raster plot of the dynamics : as in fig.1 , neurons are ordered along the vertical axis according to their in  degree . ]    for small values of @xmath165 ( e.g. @xmath171 ) , we observe the usual qse scenario with one family of locked neurons ( data not shown ) .",
    "however , when @xmath165 is sufficiently large ( e.g. @xmath172 ) , each peak of the distribution generates its own group of locked neurons .",
    "more precisely , neurons separate into three different sets : two locked groups , that evolve with different periods , @xmath173 and @xmath174 , and the unlocked group . in fig.[rpdue ] we show the dependence of @xmath175 on @xmath87 and the raster plot of the dynamics ( see the inset ) for @xmath176 .",
    "notice that the plateaus of locked neurons extend over values of @xmath87 on the left of @xmath155 and @xmath156 . in the inset of fig .",
    "[ fourier ] we plot the global activity field @xmath76 : the peaks signal the quasi - synchronous firing events of the two groups of locked neurons .",
    "one can also observe that very long oscillations are present over a time scale much larger than @xmath173 and @xmath174 .",
    "they are the effect of the _ firing synchrony _ of the of two locked families .",
    "in fact , the two frequencies @xmath177 and @xmath178 are in general not commensurate , and the resulting global field is a quasi  periodic function",
    ". this can be better appreciated by looking at fig.[fourier ] , where we report the frequency spectrum of the signal @xmath76 ( red curve ) .",
    "we observe peaks at frequencies @xmath179 , for integer values of @xmath115 and @xmath22 . for comparison ,",
    "we report also the spectrum of a periodic @xmath76 , generated by the hmf with power law probability distribution ( [ eqplaw ] ) , with @xmath180 ( black curve ) : in this case the peaks are located at frequencies multiples of the frequency of the locked group of neurons .     for different in  degree probability distributions .",
    "the black spectrum has been obtained for the hmf dynamics with @xmath181 , generated by the power law probability distribution @xmath182 ( see eq.([eqplaw ] ) ) , with @xmath183 : in this case there is a unique family of locked neurons generating a periodic global activity field @xmath76 .",
    "the red spectrum has been obtained for a random network of @xmath184 neurons generated by the double gaussian distribution ( see eq.([dgauss ] ) ) described in fig.s 6 and 7 : in this case two families of locked neurons are present while , as reported in the inset , @xmath76 exhibits a quasi  periodic evolution . ]    on the basis of this analysis , we can conclude that slow oscillations of the global activity field @xmath76 may signal the presence of more than one group of topologically homogeneous ( i.e. locked ) neurons .",
    "moreover , we have also learnt that one can generate a large variety of global synaptic activity fields by selecting suitable in - degree distributions @xmath61 , thus unveiling unexpected perspectives for exploiting a sort of _ topological engineering _ of the neural signals .",
    "for instance , one could investigate which kind of @xmath61 could give rise to an almost resonant dynamics , where @xmath185 is close to a multiple of @xmath186 .",
    "the hmf formulation allows one to define and solve the following global inverse problem : how to recover the in ",
    "degree distribution @xmath61 from the knowledge of the global synaptic activity field @xmath76 @xcite .",
    "here we just sketch the basic steps of the procedure . given @xmath76 , each class of neurons of in - degree @xmath87 evolves according to the hmf equations : @xmath187 the different fonts used here , with respect to eq.s ( [ vk])([meanfield ] ) , point out that in this framework the choice of the initial conditions is arbitrary and the dynamical variables @xmath188 , @xmath189 , @xmath190 in general may take different values from those assumed by @xmath191 , @xmath192 , @xmath193 , i.e. the variables generating @xmath76 in ( [ vk])([meanfield ] ) .",
    "however , one can exploit the self consistent relation for the global field @xmath76 : @xmath194 if @xmath76 and @xmath195 are known , this is a fredholm equation of the first kind for the unknown @xmath61 @xcite . if @xmath76 is a periodic signal , eq .",
    "( [ global ] ) can be easily solved by a functional montecarlo minimization procedure , yielding a faithful reconstruction of @xmath61 @xcite .",
    "this method applies successfully also when @xmath76 is a quasi - periodic signal , like the one generated by in  degree distribution ( [ dgauss ] ) .    in this section",
    "we want to study the robustness of the hmf equations and of the corresponding inverse problem procedure in the presence of noise .",
    "this is quite an important test for the reliability of the overall hmf approach .",
    "in fact , a real neural structure is always affected by some level of noise , that , for instance , may emerge in the form of fluctuations of ionic or synaptic currents .",
    "moreover , it has been observed that noise is crucial for reproducing dynamical phases , that exhibit some peculiar synchronization patterns observed in _ in vitro _",
    "experiments @xcite .    for the sake of simplicity",
    ", here we introduce noise by turning the external current @xmath52 , in eq .",
    "( [ vk ] ) , from a constant to a time and neuron dependent stochastic processes @xmath196 .",
    "precisely , the @xmath196 are assumed to be i.i.d .",
    "stochastic variables , that evolve in time as a random walk with boundaries , @xmath197 and @xmath198 ( the same rule adopted in @xcite ) .",
    "accordingly , the average value , @xmath199 of @xmath196 is given by the expression @xmath200 , while the amplitude of fluctuations is @xmath201 .",
    "at each step of the walk , the values of @xmath196 are independently updated by adding or subtracting , with equal probability , a fixed increment @xmath202 .",
    "whenever the value of @xmath196 crosses one of the boundaries , it is reset to the boundary value .    since the dynamics has lost its deterministic character , its numerical integration can not exploit an event driven algorithm , and one has to integrate eq.s ( [ vk ] ) ([zk ] ) by a scheme based on explicit time discretization .",
    "the results reported hereafter refer to an integration time step @xmath203 , that guarantees an effective sampling of the dynamics over the whole range of parameter values that we have explored .",
    "we have assumed that @xmath204 is also the time step of the stochastic evolution of @xmath196 .    here",
    "we consider the case of uncorrelated noise , that can be obtained by a suitable choice of @xmath202 @xcite . in our simulations",
    "@xmath205 , that yields a value @xmath206 of the correlation time of the random walk with boundaries .",
    "this value , much smaller than the value @xmath207 typical of the isi of neurons , makes the stochastic evolution of the external currents , @xmath196 , an effectively uncorrelated process with respect to the typical time scales of the neural dynamics .     of the hmf dynamics , sampled by @xmath208 classes of neurons , for a gaussian probability distribution @xmath61 , with @xmath209 and @xmath210 .",
    "lines of different colors correspond to different values of the noise amplitude , @xmath18 , added to the external currents @xmath196 : @xmath211 ( black line ) , @xmath212 ( red line ) , @xmath213 ( green line ) , @xmath214 ( blue line ) and @xmath215 ( orange line ) . ]    in fig .",
    "[ camponoise ] we show @xmath76 , produced by the discretized hmf dynamics with @xmath208 and for a gaussian distribution @xmath61 , with @xmath84 and @xmath210",
    ". curves of different colors correspond to different values of @xmath18 .",
    "we have found that up to @xmath216 , i.e. also for non negligible noise amplitudes ( @xmath217 ) , the hmf dynamics is practically unaffected by noise . by further increasing @xmath18 , the amplitude of @xmath76 decreases , as a result of the desynchronization of the network induced by large amplitude noise .",
    "also the inversion procedure exhibits the same robustness with respect to noise . as a crucial test",
    ", we have solved the inverse problem to recover @xmath61 by injecting the noisy signal @xmath76 in the noiseless equations ( [ vktil])([zktil ] ) , where @xmath218 ( see fig.[camponoise ] ) .",
    "the reconstructed distributions @xmath61 , for different @xmath18 , are shown in fig .",
    "[ noise_invert_1 ] .",
    "for relatively small noise amplitudes ( @xmath219 ) the recovered form of @xmath61 is quite close to the original one , as expected because the noisy @xmath76 does not differ significantly from the noiseless one . on the contrary , for relatively large noise amplitudes ( @xmath220 )",
    ", the recovered distribution @xmath61 is broader than the original one and centered around a shifted average value @xmath63 .",
    "the dynamics exhibits much weaker synchrony effects , the same indeed one could observe for the noiseless dynamics on the lattice built up with this broader @xmath61 given by the inversion method .",
    ", the reconstructed probability distribution @xmath61 ( red circles ) with the original gaussian distribution ( black line ) : the upper  left panel corresponds to the noiseless case ( @xmath221 ) , while the upper  right , the lower  left and and the lower ",
    "right correspond to @xmath222 , respectively . ]    as a matter of fact , the global neural activity fields obtained by experimental measurements are unavoidably affected by some level of noise .",
    "accordingly , it is worth investigating the robustness of the inversion method also in the case of noise acting directly on @xmath76 . in order to tackle this problem ,",
    "we have considered a simple noisy version of the global synaptic activity field , defined as @xmath223 , where the random number @xmath224 is uniformly extracted , at each integration time step , in the interval @xmath225 $ ] . in fig .",
    "[ noise_invert_2 ] we show the distributions @xmath61 obtained for different values of @xmath18 .",
    "we can conclude that the inversion method is quite stable with respect to this additive noise .",
    "in fact , even for very large signal  to  noise ratio ( e.g. low ",
    "right panel of fig .",
    "[ noise_invert_2 ] , where @xmath226 ) the main features of the original distribution are still recovered , within a reasonable approximation .     and @xmath227 ( the random variable @xmath224 is extracted from a uniform probability distribution in the interval @xmath228 $ ] ) .",
    "we compare , for different values of the noise amplitude @xmath18 , the reconstructed probability distribution @xmath61 ( red circles ) with the original gaussian distribution ( black line ) : the upper  left , the upper  right , the lower  left and and the lower ",
    "right panels correspond to @xmath229 , respectively . ]",
    "in this section we analyze the effectiveness of the hmf approach for sparse networks , i.e. networks where the neurons degree does not scale linearly with @xmath7 and , in particular , the average degree @xmath230 is independent of the system size . in this context , the coupling term describing the membrane potential of a generic neuron @xmath23 , in a network of @xmath7 neurons , evolves according to the following equation : @xmath231 while the dynamics of @xmath25 is the same of eq.s ( [ dynsyn])([contz ] ) .",
    "the coupling therm is now independent of @xmath7 , and the normalization factor , @xmath232 , has been introduced in order to compare models with different average connectivity .",
    "the structure of the adjacency matrix @xmath34 is determined by choosing for each neuron @xmath23 its in - degree @xmath58 from a probability distribution @xmath233 ( with support over positive integers ) independent of the system size .     from sparse random networks with the same quantity generated by the corresponding hmf dynamics .",
    "we have considered sparse random networks with @xmath234 neurons . in the upper panel",
    "we consider a gaussian probability distributions @xmath235 with different averages @xmath230 and variances @xmath236 , such that @xmath237 : @xmath238 correspond to the violet , orange , red and blue lines , respectively .",
    "the black line represents @xmath76 from the hmf dynamics ( @xmath239 ) , where @xmath240 is a gaussian probability distribution with @xmath241 and @xmath242 . in the lower panel",
    "we consider the scale free case with fixed power exponent @xmath152 and different @xmath243 : @xmath244 correspond to the orange , red and blue lines , respectively .",
    "the black line represents @xmath76 from the hmf dynamics ( @xmath239 ) , where @xmath245 with cutoff @xmath246.,title=\"fig : \" ]   from sparse random networks with the same quantity generated by the corresponding hmf dynamics .",
    "we have considered sparse random networks with @xmath234 neurons . in the upper panel",
    "we consider a gaussian probability distributions @xmath235 with different averages @xmath230 and variances @xmath236 , such that @xmath237 : @xmath238 correspond to the violet , orange , red and blue lines , respectively .",
    "the black line represents @xmath76 from the hmf dynamics ( @xmath239 ) , where @xmath240 is a gaussian probability distribution with @xmath241 and @xmath242 . in the lower panel",
    "we consider the scale free case with fixed power exponent @xmath152 and different @xmath243 : @xmath244 correspond to the orange , red and blue lines , respectively .",
    "the black line represents @xmath76 from the hmf dynamics ( @xmath239 ) , where @xmath245 with cutoff @xmath246.,title=\"fig : \" ]    on sparse networks the hmf model is not recovered in the thermodynamic limit , as the fluctuations of the field received by each neuron of in ",
    "degree @xmath58 do not vanish for @xmath88 .",
    "nevertheless , for large enough values of @xmath58 , one can expect that the fluctuations become negligible in such a limit , i.e. the synaptic activity field received by different neurons with the same in - degree is approximately the same .",
    "( [ vsparsa ] ) can be turned into a mean  field like form as follows @xmath247 where @xmath76 represents the global field , averaged over all neurons in the network .",
    "this implies that the equation is the same for all neurons with in ",
    "degree @xmath248 , depending only on the ratio @xmath249 . consequently ,",
    "also in this case one can read eq .",
    "( [ vsparsa_mean ] ) as a hmf formulation of eq .",
    "( [ vsparsa ] ) , where each class of neurons @xmath250 evolves according to to eq.s ( [ vk])([zk ] ) , with @xmath250 replacing @xmath87 , while the global activity field is given by the relation @xmath251 .    in order to analyze the validity of the hmf as an approximation of models defined on sparse networks , we consider two main cases : ( _ i _ )",
    "@xmath252 is a truncated gaussian with average @xmath253 and standard deviation @xmath254 ; ( _ ii _ ) @xmath255 is a power  law ( i.e. , scale free ) distribution with a lower cutoff @xmath256 .",
    "the gaussian case ( _ i _ ) is an approximation of any sparse model , where @xmath257 is a discretized gaussian distribution with parameters @xmath232 and @xmath236 , chosen in such a way that @xmath258 .",
    "the scale free case ( _ ii _ ) approximates any sparse model , where @xmath257 is a power law with exponent @xmath152 and a generic cutoff .",
    "such an approximation is expected to provide better results the larger is @xmath230 , i.e. the larger is the cutoff @xmath243 of the scale free distribution . in fig .",
    "[ campi_sparsa ] we plot the global field emerging from the hmf model , superposing those coming from a large finite size realization of the sparse network , with different values of @xmath232 for the gaussian case ( upper panel ) and of @xmath243 for the scale free case ( lower panel ) .",
    "the hmf equations exhibit a remarkable agreement with models on sparse network , even for relatively small values of @xmath259 and @xmath243 .",
    "this analysis indicates that the hmf approach works also for non  massive topologies , provided the typical connectivities in the network are large enough , e.g. @xmath260 in a gaussian random network with @xmath93 neurons ( see fig .",
    "( [ campi_sparsa ] ) ) .",
    "for systems with a very large number of components , the effectiveness of a statistical approach , paying the price of some necessary approximation , has been extensively proven , and mean  field methods are typical in this sense . in this paper we discuss how such a method , in the specific form of heterogeneous mean  field , can be defined in order to fit an effective description of neural dynamics on random networks .",
    "the relative simplicity of the model studied here , excitatory leaky  integrate  and fire neurons with short term synaptic plasticity , is also a way of providing a pedagogical description of the hmf and of its potential interest in similar contexts @xcite .",
    "we have reported a detailed study of the hmf approach including investigations on _ ( i ) _ its stability properties , _ ( ii ) _ its effectiveness in describing the dynamics and in solving the associated inverse problem for different network topologies , _ ( iii ) _ its robustness with respect to noise , and _",
    "( iv ) _ its adaptability to different formulations of the model at hand . in the light of _ ( ii )",
    "_ and _ ( iii ) _ , the hmf approach appears quite a promising tool to match experimental situations , such as the identification of topological features of real neural structures , through the inverse analysis of signals extracted as time series from small , but not microscopic , domains . on a mathematical ground ,",
    "the hmf approach is a simple and effective mean  field formulation , that can be extended to other neural network models and also to a wider class of dynamical models on random graphs .",
    "the first step in this direction could be the extension of the hmf method to the more interesting case , where the random network contains excitatory and inhibitory neurons , according to distributions of interest for neurophysiology @xcite",
    ". this will be the subject of our future work .",
    "99 d. de santos - sierra , i. sendia - nadal , i. leyva , j. a. almendral , s. anava , a. ayali , d. papo , and s. boccaletti , plos one 9(1 ) : e85828 ( 2014 ) . v. volman , i. baruchi , e. persi and e. ben - jacob , physica a 335 , 249 ( 2004 ) .",
    "hidalgo j , seoane lf , corts jm , mu@xmath261 oz ma , plos one 7(8 ) , e40710 ( 2012 ) .",
    "j. f. mejias , h. j. kappen and j. j. torres , plos one 5(11 ) , e13651 ( 2010 ) n. brunel , journal of computational neuroscience , 8(3 ) , 183 - 208 ( 2000 ) .",
    "l. calamai , a. politi , a. torcini , phys .",
    "e 80 , 036209 ( 2009 ) d. millman , s. mihalas , a. kirkwood & e. niebur , nature physics , 6(10 ) , 801 - 805 ( 2010 ) .",
    "b. cessac , b. doyon , m. quoy m. & samuelides , physica d : nonlinear phenomena , 74(1 ) , 24 - 44 ( 1994 ) .",
    "bressloff , phys .",
    "e , 60(2 ) , 2160 ( 1999 ) .",
    "a. barrat , m. barthelemy , and a. vespignani , _ dynamical processes on complex networks _",
    ", cambridge university press , cambridge , uk ( 2008 ) .",
    "s. n. dorogovtsev , a.v .",
    "goltsev , and j. f. f. mendes , rev .",
    "mod . phys . 80 , 1275 ( 2008 ) .",
    "m. di volo , r. livi , s. luccioli , a. politi and a. torcini , phys .",
    "e 87 , 032801 ( 2013 ) .",
    "r. burioni , m. di volo , m. casartelli , r. livi and a. vezzani , scientific reports 4 , 4336 ( 2014 ) . m. tsodyks and h. markram , proc .",
    "usa 94 , 719 ( 1997 ) .",
    "m. tsodyks , k. pawelzik and h. markram , neural comput .",
    "10 , 821 , ( 1998 ) .",
    "m. tsodyks , a. uziel and h. markram , the journal of neuroscience 20 , rc1 ( 1 - 5 ) ( 2000 ) .",
    "r. brette , neural comput . 18 , 2004 ( 2006 ) .",
    "r. zillmer , r. livi , a. politi , and a. torcini , phys .",
    "e 76 , 046102 ( 2007 ) .",
    "m. di volo and r. livi , j. of chaos solitons and fractals 57 , 5461 ( 2013 ) . m. tsodyks , i. mitkov , and h. sompolinsky , phys .",
    "71 , 1280 ( 1993 ) .",
    "g. benettin , l. galgani , a. giorgilli and j .- m .",
    "strelcyn , meccanica 15 , 21 ( 1980 ) .",
    "r. kress , _ linear integral equations _",
    "applied numerical sciences , v.82 , springer - verlag , new york , ( 1999 ) .",
    "corticonics_. new york : cambridge up ( 1991 ) .",
    "p. bonifazi , m. goldin , m. a. picardo , i. jorquera , a. cattani , g. bianconi & r. cossart , science , 326(5958 ) , 1419 - 1424 ( 2009 ) ."
  ],
  "abstract_text": [
    "<S> we report about the main dynamical features of a model of leaky - integrate - and fire excitatory neurons with short term plasticity defined on random massive networks . </S>",
    "<S> we investigate the dynamics by a heterogeneous mean  field formulation of the model , that is able to reproduce dynamical phases characterized by the presence of quasi  synchronous events . </S>",
    "<S> this formulation allows one to solve also the inverse problem of reconstructing the in - degree distribution for different network topologies from the knowledge of the global activity field . </S>",
    "<S> we study the robustness of this inversion procedure , by providing numerical evidence that the in - degree distribution can be recovered also in the presence of noise and disorder in the external currents . </S>",
    "<S> finally , we discuss the validity of the heterogeneous mean  field approach for sparse networks , with a sufficiently large average in  degree . </S>"
  ]
}