{
  "article_text": [
    "we consider the model @xmath10 where the random vector @xmath1 and the random @xmath2 matrix @xmath3 are observed , the @xmath2 matrix @xmath4 is unknown , @xmath5 is an @xmath2 random noise matrix , @xmath6 is a random noise vector , @xmath11 is a vector of unknown parameters to be estimated , and @xmath12 is a given subset of @xmath13 .",
    "we consider the problem of estimating an @xmath14-sparse vector @xmath7 ( i.e. , a vector @xmath7 having only @xmath14 non zero components ) , with @xmath8 possibly much larger than @xmath9 . if the matrix @xmath4 in  is observed without error ( @xmath15 ) , this problem has been recently studied in numerous papers .",
    "the proposed estimators mainly rely on @xmath16 minimization techniques .",
    "in particular , this is the case for the widely used lasso and dantzig selector , see among others cands and tao  ( 2007 ) , bunea et al .",
    "( 2007a , b ) , bickel et al .",
    "( 2009 ) , koltchinskii  ( 2009 ) , the book by bhlmann and van de geer  ( 2011 ) , the lecture notes by koltchinskii  ( 2011 ) , belloni and chernozhukov  ( 2011 ) and the references cited therein .    however , it is shown in rosenbaum and tsybakov  ( 2010 ) that dealing with a noisy observation of the regression matrix @xmath4 has severe consequences . in particular",
    ", the lasso and dantzig selector become very unstable in this context .",
    "an alternative procedure , called the matrix uncertainty selector ( mu selector for short ) is proposed in rosenbaum and tsybakov  ( 2010 ) in order to account for the presence of noise @xmath5 .",
    "the mu selector @xmath17 is defined as a solution of the minimization problem @xmath18 where @xmath19 denotes the @xmath20-norm , @xmath21 , @xmath22 is a given subset of @xmath23 characterizing the prior knowledge about @xmath7 , and the constants @xmath24 and @xmath25 depend on the level of the noises @xmath5 and @xmath26 respectively .",
    "if the noise terms @xmath26 and @xmath5 are deterministic , it is suggested in rosenbaum and tsybakov  ( 2010 ) to choose @xmath25 such that @xmath27 and to take @xmath28 with @xmath29 such that @xmath30 where , for a matrix @xmath31 , we denote by @xmath32 its componentwise @xmath33-norm .    in this paper",
    ", we propose a modification of the mu selector for the model where @xmath5 is a random matrix with independent and zero mean entries @xmath34 such that the sums of expectations @xmath35 are finite and admit data - driven estimators .",
    "our main example where such estimators exist is the model with data missing at random ( see below ) .",
    "the idea underlying the new estimator is the following .",
    "in the ideal setting where there is no noise @xmath5 , the estimation strategy for @xmath7 is based on the matrix @xmath4 . when there is noise this is impossible since @xmath4 is not observed and so we have no other choice than using @xmath3 instead of @xmath4 .",
    "however , it is not hard to see that under the above assumptions on @xmath5 , the matrix @xmath36 appearing in contains a bias induced by the diagonal entries of the matrix @xmath37 whose expectations @xmath38 do not vanish .",
    "if @xmath38 can be estimated from the data , it is natural to make a bias correction .",
    "this leads to a new estimator @xmath39 defined as a solution of the minimization problem @xmath40 where @xmath41 is the diagonal matrix with entries @xmath42 , which are estimators of @xmath38 , and @xmath43 and @xmath44 are constants that will be specified later .",
    "this estimator @xmath39 will be called the compensated mu selector . in this paper , we show both theoretically and numerically that the estimator @xmath39 achieves better performance than the original mu selector  @xmath17 . in particular , under natural conditions given below , the bounds on the error of the compensated mu selector decrease as @xmath45 up to logarithmic factors as @xmath46 , whereas for the original mu selector @xmath17 the corresponding bounds do not decrease with @xmath9 and can be only small if the noise @xmath5 is small .",
    "[ rem:1 ] the problem ( [ cmu ] ) is equivalent to @xmath47 where @xmath48 with the same @xmath24 and @xmath25 as in ( [ cmu ] ) ( see the proof in section [ sec : preuve ] ) .",
    "this simplifies in some cases the computation of the solution .",
    "an important example where the values @xmath38 can be estimated is given by the model with missing data .",
    "assume that the elements @xmath49 of the matrix @xmath4 are unobservable , and we can only observe @xmath50 where for each fixed @xmath51 , the factors @xmath52 are i.i.d .",
    "bernoulli random variables taking value 1 with probability @xmath53 and 0 with probability @xmath54 , @xmath55 .",
    "the data @xmath49 is missing if @xmath56 , which happens with probability @xmath54 . we can rewrite ( [ ex1 ] ) in the form @xmath57 where @xmath58 , @xmath59 .",
    "thus , we can reduce the model with missing data ( [ ex1 ] ) to the form ( [ 00 ] ) with a matrix @xmath5 whose elements @xmath34 have zero mean and variance @xmath60 .",
    "so , @xmath61 in section  [ sec : stoch ] below , we show that when the @xmath54 are known , the @xmath38 admit good data - driven estimators @xmath62 .",
    "if the @xmath54 are unknown , they can be readily estimated by the empirical frequencies of 0 that we further denote by @xmath63 .",
    "then the @xmath64 appearing in ( [ ex2 ] ) are not available and should be replaced by @xmath65 .",
    "this slightly changes the model and implies a minor modification of the estimator ( cf .",
    "section  [ sec : stoch]).",
    "consider the following random matrices @xmath66 @xmath67 where @xmath68 is the diagonal matrix with diagonal elements @xmath38 , @xmath51 , and for a square matrix @xmath31 , we denote by @xmath69 the matrix with the same dimensions as @xmath31 , the same diagonal elements as @xmath31 and all off - diagonal elements equal to zero .    under conditions that will be specified below , the entries of the matrices @xmath70 are small with probability close to 1 .",
    "bounds on the @xmath33-norms of the matrices @xmath70 characterize the stochastic error of the estimation .",
    "the accuracy of the estimators is determined by these bounds and by the properties of the gram matrix @xmath71 for a vector @xmath72 , we denote by @xmath73 the vector in @xmath13 that has the same coordinates as @xmath72 on the set of indices @xmath74 and zero coordinates on its complement  @xmath75 .",
    "we denote by @xmath76 the cardinality of @xmath77 .    to state our results in a general form , we follow gautier and tsybakov  ( 2011 ) and introduce the sensitivity characteristics related to the action of the matrix @xmath78 on the cone @xmath79 .",
    "for @xmath80 $ ] and an integer @xmath81 $ ] , we define the _ @xmath82 sensitivity _ as follows : @xmath83 we will also consider the _ coordinate - wise sensitivities _ @xmath84 where @xmath85 is the @xmath86th coordinate of @xmath87 , @xmath88 . to get meaningful bounds for various types of estimation errors , we will need the positivity of @xmath89 or @xmath90 .",
    "as shown in gautier and tsybakov  ( 2011 ) , this requirement is weaker than the usual assumptions related to the structure of the gram matrix @xmath78 , such as the restricted eigenvalue assumption and the coherence assumption . for completeness , we recall these two assumptions .",
    "* assumption re(@xmath14 ) . *",
    "_ let @xmath91 .",
    "there exists a constant @xmath92 such that @xmath93 for all subsets @xmath77 of @xmath94 of cardinality @xmath95 . _",
    "* assumption c. * _ all the diagonal elements of @xmath78 are equal to 1 and all its off - diagonal elements of @xmath96 satisfy the coherence condition : @xmath97 for some @xmath98 . _",
    "+ note that assumption c with @xmath99 implies assumption re(@xmath14 ) with @xmath100 , see bickel et al .",
    "( 2009 ) or lemma 2 in lounici  ( 2008 ) . from proposition  4.2 of gautier and tsybakov  ( 2011 )",
    "we get that , under assumption c with @xmath101 , @xmath102 which yields the control of the sensitivities @xmath103 for all @xmath104 since @xmath105 by proposition  4.1 of gautier and tsybakov  ( 2011 ) .",
    "furthermore , proposition  9.2 of gautier and tsybakov  ( 2011 ) implies that , under assumption re(@xmath14 ) , @xmath106 and by proposition  9.3 of that paper , under assumption re(@xmath107 ) for any @xmath108 and any @xmath109 , we have @xmath110 where @xmath111 .",
    "in this section , we give bounds on the estimation and prediction errors of the compensated mu selector . for @xmath112 , we consider the thresholds @xmath113 and @xmath114 , @xmath115 , such that @xmath116 and @xmath117 define @xmath118 and @xmath119 where @xmath120 and @xmath12 is a given subset of @xmath23 . for @xmath112 ,",
    "the compensated mu selector is defined as a solution of the minimization problem @xmath121 we have the following result .",
    "[ t1 ] assume that model ( [ 0])([00 ] ) is valid with an @xmath14-sparse vector of parameters @xmath122 , where @xmath22 is a given subset of @xmath23 .",
    "for @xmath112 , set @xmath123 then , with probability at least @xmath124 , the set @xmath125 is not empty and for any solution @xmath126 of we have @xmath127    the proof of this theorem is given in section [ sec : preuve ] .    note that ( [ t1:3 ] ) contains a bound on the prediction error under no assumption on @xmath4 : @xmath128 the other bounds in theorem [ t1 ] depend on the sensitivities . using ( [ k1 ] )  ( [ k4 ] ) we obtain the following corollary of theorem [ t1 ] .",
    "[ t2 ] let the assumptions of theorem  [ t1 ] be satisfied .",
    "then , with probability at least @xmath124 , for any solution @xmath126 of we have the following inequalities .",
    "\\(i ) under assumption re(@xmath14 ) : @xmath129 ( ii ) under assumption re(@xmath107 ) , @xmath108 : @xmath130 ( iii ) under assumption c with @xmath131 : @xmath132 where we set @xmath133 .",
    "if the components of @xmath26 and @xmath5 are subgaussian , the values @xmath134 are of order @xmath45 up to logarithmic factors , and the value @xmath135 is of the same order in the model with missing data ( see section [ sec : stoch ] ) .",
    "then , the bounds for the compensated mu selector in theorem  [ t2 ] are decreasing with rate @xmath136 as @xmath137 .",
    "this is an advantage of the compensated mu selector as compared to the original mu selector @xmath17 , for which the corresponding bounds do not decrease with @xmath9 and can be small only if the noise @xmath5 is small ( cf .",
    "rosenbaum and tsybakov  ( 2010 ) ) .    if the matrix @xmath4 is observed without error ( @xmath15 ) , then @xmath138 , @xmath139 , and the compensated mu selector coincides with the dantzig selector . in this particular case , the results ( ii ) and ( iii ) of theorem  [ t2 ] improve , in terms of the constants or the range of validity , upon the corresponding bounds in bickel et al .",
    "( 2009 ) and lounici  ( 2008 ) .",
    "theorems [ t1 ] and [ t2 ] are stated with general thresholds @xmath134 and @xmath135 , and can be used both for random or deterministic noises @xmath140 ( in the latter case , @xmath141 ) and random or deterministic @xmath4 . in this section ,",
    "considering @xmath142 we first derive the values @xmath134 for random @xmath26 and @xmath5 with subgaussian entries , and then we specify @xmath135 and the matrix @xmath143 for the model with missing data .",
    "note that , for random @xmath26 and @xmath5 , the values @xmath134 and @xmath135 characterize the stochastic error of the estimator .",
    "recall that a zero - mean random variable @xmath144 is said to be @xmath145-subgaussian ( @xmath146 ) if , for all @xmath147 , @xmath148\\leq \\text{exp}(\\gamma^2t^2/2).\\ ] ] in particular , if @xmath144 is a zero - mean gaussian or bounded random variable , it is subgaussian .",
    "a zero - mean random variable @xmath144 will be called @xmath149-subexponential if there exist @xmath146 and @xmath150 such that @xmath151\\leq \\text{exp}(\\gamma^2t^2/2 ) , \\quad \\forall \\    @xmath5 satisfy the following assumption .",
    "* assumption n. * _ let @xmath152 , @xmath153 .",
    "the entries @xmath34 , @xmath154 of the matrix @xmath5 are zero - mean @xmath155-subgaussian random variables , the @xmath9 rows of @xmath5 are independent , and @xmath156 for @xmath157 , @xmath158 . the components @xmath159 of the vector @xmath26 are independent zero - mean @xmath160-subgaussian random variables satisfying @xmath161 , @xmath162 . _",
    "assumption n implies that the random variables @xmath163 , @xmath164 are subexponential . indeed , if two random variables @xmath165 and @xmath166 are subgaussian , then for some @xmath167 we have @xmath168 , which implies that ( [ subexp ] ) holds for @xmath169 with some @xmath170 whenever @xmath171 , cf . , e.g. , petrov  ( 1995 ) , page 56 .",
    "next , @xmath172 is a zero - mean subexponential random variable with variance @xmath173 .",
    "it is easy to check that ( [ subexp ] ) holds for @xmath174 with @xmath175 and @xmath176 .    to simplify the notation",
    ", we will use a rougher evaluation valid under assumption n , namely that all @xmath163 , @xmath164 are @xmath177-subexponential with the same @xmath178 and @xmath150 , and all @xmath179 are @xmath180-subexponential . here the constants @xmath181 and @xmath182 depend only on @xmath155 and @xmath160 . for @xmath183 and an integer @xmath184 , set @xmath185    [ lem : subexp ]",
    "let assumption n be satisfied , and let @xmath4 be a deterministic matrix with @xmath186 .",
    "then for any @xmath183 the bound ( [ pr2 ] ) holds with @xmath187    use the union bound and the facts that @xmath188 for a @xmath145-subgaussian @xmath144 , and @xmath189 for a sum of independent @xmath190-subexponential @xmath191 .",
    "consider now the model with missing data ( [ ex1 ] ) and assume that @xmath4 is non - random .",
    "then we have @xmath192 , which implies : @xmath193 = x_{ij}^2(1-\\pi_j)\\ , , \\quad j=1,\\dots , p.\\ ] ] hence , @xmath194 is an unbiased estimator of @xmath195 .",
    "then @xmath196 defined in ( [ ex3 ] ) is naturally estimated by @xmath197 the matrix @xmath143 is then defined as a diagonal matrix with diagonal entries @xmath198 .",
    "it is not hard to prove that @xmath198 approximates @xmath196 in probability with rate @xmath45 up to a logarithmic factor .",
    "for example , let the probability that the data is missing be the same for all @xmath199 : @xmath200 .",
    "then @xmath201 where we have used the fact that @xmath202 , hoeffding s inequality and the notation @xmath203 .",
    "this proves ( [ pr1 ] ) with @xmath204 if @xmath205 is unknown , we replace it by the estimator @xmath206 , where @xmath207 denotes the indicator function .",
    "another difference is that @xmath64 appearing in ( [ ex2 ] ) are not available when @xmath54 s are unknown .",
    "therefore , we slightly modify the estimator using @xmath208 instead of @xmath209 ; we define @xmath210 as a solution of @xmath211 with @xmath212 where @xmath213 and @xmath214 are suitably chosen constants , @xmath215 is the @xmath2 matrix with entries @xmath216 , and @xmath143 is a diagonal matrix with entries @xmath217 this modification introduces in the bounds an additional term proportional to @xmath218 , which is of the order @xmath219 in probability and hence is negligible as compared to the error bound for the compensated mu selector .    in this section ,",
    "we have considered non - random @xmath4 . using the same argument",
    ", it is easy to derive analogous expressions for @xmath220 and @xmath221 when @xmath4 is a random matrix with independent sub - gaussian entries , and @xmath26 , @xmath5 are independent from @xmath4 .",
    "the bounds of theorems [ t1 ] and [ t2 ] depend on the unknown matrix @xmath4 via the sensitivities , and therefore can not be used to provide confidence intervals . in this section ,",
    "we show how to address the issue of confidence intervals by deriving other type of bounds based on the empirical sensitivities .",
    "note first that the matrix @xmath222 is a natural estimator of the unknown gram matrix @xmath78 .",
    "it is @xmath223-consistent in @xmath33-norm under the conditions of the previous section .",
    "therefore , it makes sense to define the empirical counterparts of @xmath89 and @xmath90 by the relations : @xmath224 that we will call the _ empirical sensitivities _ can be efficiently computed for small @xmath14 or , alternatively , one can compute data - driven lower bounds on them for any @xmath14 using linear programming , cf . gautier and tsybakov  ( 2011 ) .",
    "the following theorem establishes confidence intervals for @xmath14-sparse vector @xmath7 based on the empirical sensitivities .",
    "[ t3 ] assume that model ( [ 0])([00 ] ) is valid with an @xmath14-sparse vector of parameters @xmath122 , where @xmath22 is a given subset of @xmath23 .",
    "then , with probability at least @xmath124 , for any solution @xmath126 of we have @xmath225 where @xmath226 , and we set @xmath227 .",
    "set @xmath228 , and write for brevity @xmath229 using lemma  [ lem3 ] in section  [ sec : preuve ] , the fact that @xmath230 where @xmath77 is the set of non - zero components of @xmath231 ( cf .",
    "lemma  1 in rosenbaum and tsybakov  ( 2010 ) ) and the definition of the empirical sensitivity @xmath232 , we find @xmath233 this and the definition of @xmath234 yield ( [ t3:1 ] ) .",
    "the proof of ( [ t3:2 ] ) is analogous , with @xmath235 used instead of @xmath234 .",
    "note that the bounds ( [ t3:1])([t3:2 ] ) remain valid for @xmath236 .",
    "therefore , if one gets an estimator @xmath237 of @xmath14 such that @xmath238 with high probability , it can be plugged in into the bounds in order to get completely feasible confidence intervals .",
    "we consider here the model with missing data ( [ ex1 ] ) .",
    "simulations in rosenbaum and tsybakov  ( 2010 ) indicate that in this model the mu selector achieves better numerical performance than the lasso or the dantzig selector .",
    "here we compare the mu selector with the compensated mu selector .",
    "we design the numerical experiment the following way .",
    "@xmath239 we take a matrix @xmath4 of size @xmath240 ( @xmath241 ) which is the normalized version ( centered and then normalized so that all the diagonal elements of the associated gram matrix @xmath242 are equal to 1 ) of a @xmath243 matrix with i.i.d .",
    "standard gaussian entries .",
    "+ @xmath239 for a given integer @xmath14 , we randomly ( uniformly ) choose @xmath14 non - zero elements in a vector @xmath7 of size @xmath244 .",
    "the associated coefficients @xmath245 are set to @xmath246 , and all other coefficients are set to 0 .",
    "we take @xmath247 .",
    "+ @xmath239 we set @xmath248 , where @xmath26 a vector with i.i.d .",
    "zero mean and variance @xmath249 normal components , @xmath250 .",
    "+ @xmath239 we compute the values @xmath251 with @xmath208 as in , . ] and @xmath252 for all @xmath199 .",
    "( the value @xmath205 rather than its empirical counterpart , which is very close to @xmath205 , is used in the algorithm to simplify the computations ) .",
    "+ @xmath239 we run a linear programming algorithm to compute the solutions of and where we optimize over @xmath253 . to simplify the comparison with rosenbaum and tsybakov  ( 2010 )",
    ", we write @xmath24 in the form @xmath254 with @xmath255 .",
    "in particular , @xmath256 corresponds to the dantzig selector based on the noisy matrix  @xmath3 . in practice",
    ", one can use an empirical procedure of the choice of @xmath257 described in rosenbaum and tsybakov  ( 2010 ) .",
    "the choice of @xmath25 is not crucial and influences only slightly the output of the algorithm .",
    "the results presented below correspond to @xmath25 chosen in the same way as in the numerical study in rosenbaum and tsybakov  ( 2010 ) .",
    "+ @xmath239 we compute the error measures @xmath258 .",
    "+ @xmath239 for each value of @xmath14 we run @xmath259 monte carlo simulations",
    ".    tables 15 present the empirical averages and standard deviations ( in brackets ) of @xmath260 , @xmath261 , of the number of non - zero coefficients in @xmath262 ( @xmath263 ) and of the number of non - zero coefficients in @xmath262 belonging to the true sparsity pattern ( @xmath264 ) .",
    "we also present the total number of simulations where the sparsity pattern is exactly retrieved ( exact ) .",
    "the lines with  @xmath265 \" for @xmath266 correspond to the mu selector and those with  @xmath267 \" to the compensated mu selector . +    [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     the results of the simulations are quite convincing .",
    "indeed , the compensated mu selector improves upon the mu selector with respect to all the considered criteria , in particular when @xmath7 is very sparse ( @xmath268 ) .",
    "the order of magnitude of the improvement is such that , for the best @xmath257 , the errors @xmath260 and @xmath261 are divided by @xmath269 .",
    "the improvement is not so significant for larger @xmath14 , especially for @xmath270 when the model starts to be not very sparse .",
    "for all the values of @xmath14 , the non - zero coefficients of @xmath7 are systematically in the sparsity pattern both of the mu selector and of the compensated mu selector .",
    "the total number of non - zero coefficients is always smaller ( i.e. , closer to the correct one ) for the compensated mu selector .",
    "finally , note that the best results for the error measures @xmath260 and @xmath261 are obtained with @xmath271 , while the sparsity pattern is better retrieved for @xmath272 .",
    "this reflects a trade - off between estimation and selection .",
    "* proof of remark [ rem:1 ] . * it is enough to show that @xmath273 where @xmath274 let first @xmath275 . using the triangle inequality , we easily get that @xmath276 .",
    "now take @xmath276 .",
    "we set @xmath277 and consider @xmath278 defined by @xmath279 for @xmath280 , where @xmath281 and @xmath282 are the @xmath283th components of @xmath284 and @xmath184 respectively .",
    "it is easy to check that @xmath285 , which concludes the proof .",
    "we first write that @xmath290 is equal to @xmath291 by definition of the @xmath134 and @xmath135 , with probability at least @xmath124 we have @xmath292 therefore @xmath289 with probability at least @xmath124 .      throughout the proof",
    ", we assume that we are on event of probability at least @xmath124 where inequalities  hold and @xmath289 .",
    "we have @xmath295 consequently , @xmath296 using that @xmath297 , we easily get that @xmath298 is not greater than @xmath299 now remark that @xmath300 finally , using that @xmath301 together with the fact that @xmath302 , we obtain the result .",
    "we now proceed to the proof of theorem [ t1 ] .",
    "the bounds ( [ t1:1 ] ) and ( [ t1:2 ] ) follow from lemma [ lem4 ] , the fact that @xmath230 where @xmath77 is the set of non - zero components of @xmath231 ( cf . lemma  1 in rosenbaum and tsybakov  ( 2010 ) ) and the definition of the sensitivities @xmath103 , @xmath303 . to prove ( [ t1:3 ] ) , first note that @xmath304 and use ( [ t1:1 ] ) with @xmath305 and lemma [ lem4 ] .",
    "this yields the first term under the minimum on the right hand side of ( [ t1:3 ] ) .",
    "the second term is obtained again from ( [ p1 ] ) , lemma  [ lem4 ] and the inequality @xmath306 .",
    "* the bounds ( [ t2:1 ] ) and ( [ t2:4 ] ) follow by combining ( [ t1:1 ] ) with ( [ k3 ] ) and with ( [ k1 ] )  ( [ k2 ] ) respectively .",
    "next , ( [ t2:2 ] ) follows from ( [ t1:3 ] ) and ( [ k3 ] ) . also , as an easy consequence of ( [ t1:1 ] ) and ( [ k4 ] ) with @xmath307 we get @xmath308 finally , ( [ t2:3 ] ) follows from this inequality and ( [ t2:1 ] ) using the interpolation formula @xmath309 , and the fact that @xmath310 .",
    "belloni , a. , and chernozhukov , v. ( 2011 ) .",
    "high dimensional sparse econometric models : an introduction . in : _ inverse problems and high dimensional estimation , stats in the chteau 2009 _ , alquier , p. , e. gautier , and g. stoltz , eds . , _ lecture notes in statistics _ , * 203 * 127162 , springer , berlin ."
  ],
  "abstract_text": [
    "<S> we consider the regression model with observation error in the design : @xmath0 here the random vector @xmath1 and the random @xmath2 matrix @xmath3 are observed , the @xmath2 matrix @xmath4 is unknown , @xmath5 is an @xmath2 random noise matrix , @xmath6 is a random noise vector , and @xmath7 is a vector of unknown parameters to be estimated . </S>",
    "<S> we consider the setting where the dimension @xmath8 can be much larger than the sample size @xmath9 and @xmath7 is sparse . because of the presence of the noise matrix @xmath5 , the commonly used lasso and dantzig selector are unstable . </S>",
    "<S> an alternative procedure called the matrix uncertainty ( mu ) selector has been proposed in rosenbaum and tsybakov  ( 2010 ) in order to account for the noise . </S>",
    "<S> the properties of the mu selector have been studied in rosenbaum and tsybakov  ( 2010 ) for sparse @xmath7 under the assumption that the noise matrix @xmath5 is deterministic and its values are small . in this paper , we propose a modification of the mu selector when @xmath5 is a random matrix with zero - mean entries having the variances that can be estimated . </S>",
    "<S> this is , for example , the case in the model where the entries of @xmath4 are missing at random . </S>",
    "<S> we show both theoretically and numerically that , under these conditions , the new estimator called the compensated mu selector achieves better accuracy of estimation than the original mu selector . </S>"
  ]
}