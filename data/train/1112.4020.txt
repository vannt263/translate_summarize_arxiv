{
  "article_text": [
    "nonnegative datasets are everywhere ; from term by document matrix induced from a document corpus @xcite , gene expression datasets @xcite , pixels in digital images @xcite , disease patterns @xcite , to spectral signatures from astronomical spectrometers @xcite among others .",
    "even though diverse , they have one thing in common : all can be represented by using nonnegative matrices induced from the datasets .",
    "this allows many well - established mathematical techniques to be applied in order to anayze the datasets .",
    "there are many common tasks associated with these datasets , for example : grouping the similar data points ( _ clustering _ ) , finding patterns in the datasets , identifying important or interesting features , and finding sets of relevant data points to queries ( _ information retrieval _ ) . in this paper",
    ", we will focus on two tasks : clustering and latent semantic indexing  a technique that can be used for improving recall and precision of an information retrieval ( ir ) system .",
    "clustering is the task of assigning data points into clusters such that similar points are in the same clusters and dissimilar points are in the different clusters .",
    "there are many types of clustering , for example supervised / unsu - pervised , hierarchical / partitional , hard / soft , and one - way / many - way ( two - way clustering is known as co - clustering or bi - clustering ) among others . in this paper ,",
    "clustering term refers to unsupervised , partitional , hard , and one - way clustering .",
    "further , the number of cluster is given beforehand .",
    "the nmf as a clustering method can be traced back to the work by lee & seung @xcite .",
    "but , the first work that explicitly demonstrates it is the work by xu et al .",
    "@xcite in which they show that the nmf outperforms the spectral methods in term of _ purity _ and _ mutual information _ measures for reuters and tdt2 datasets .",
    "clustering aspect of the nmf , even though numerically well studied , is not theoretically well explained .",
    "usually this aspect is explained by showing the equivalence between nmf objective to either k - means clustering objective @xcite or spectral clustering objective @xcite .",
    "the problem with the first approach is there is no obvious way to incorporate the nonnegativity constraints into k - means clustering objective . and",
    "the problem with the second approach is it discards the nonnegativity constraints , thus is equivalent to finding stationary points on unbounded region .",
    "accordingly , the nmf which is a bound - constrained optimization turns into an unbounded optimization , so there is no guarantee the stationary point being utilized in proving the equivalence is located on the feasible region indicated by the constraints .    in the first part of this paper",
    ", we will provide a theoretical support for clustering aspect of the nmf by analyzing the objective at the stationary point using the karush - kuhn - tucker ( kkt ) conditions without setting the kkt multipliers to zeros .",
    "thus , the stationary point under investigation is guaranteed to be located on the feasible region .",
    "latent semantic indexing ( lsi ) is a method introduced by deerwester et al .",
    "@xcite to improve recall and precision of an ir system using truncated singular value decomposition ( svd ) of the term - by - document matrix to reveal hidden relationship between documents by indexing terms that are present in the similar documents and weakening the influences of terms that are mutually present in the dissimilar documents .",
    "the first capability can solve the _",
    "synonymy_different words with similar meaning ",
    "problem , and the second capability can solve the _ polysemy_words with multiple unrelated meanings  problem .",
    "thus , lsi not only is able to retrieve relevant documents that do not contain terms in the query , but also can filter out irrelevant documents that contain terms in the query .",
    "lsi aspect of the nmf is not well studied .",
    "there are some works that discuss the relationship between the nmf and probabilistic lsi , e.g. , @xcite .",
    "but the emphasize is in clustering capability of probabilistic lsi , not lsi aspect of the nmf .",
    "motivated by the svd which is the standard method in clustering and lsi , in the second part of this paper , lsi aspect of the nmf will be studied , and the results will be compared to the results of the svd .",
    "the nmf was popularized by the work of lee & seung @xcite in which they showed that this technique can be used to learn parts of faces and semantic features of text .",
    "previously , it has been studied under the term positive matrix factorization @xcite .",
    "mathematically , the nmf is a technique that decomposes a nonnegative data matrix into a pair of other nonnegative matrices : @xmath0 where @xmath1 $ ] denotes the data matrix , @xmath2 $ ] denotes the basis matrix , @xmath3 $ ] denotes the coefficient matrix , and @xmath4 denotes the number of factors which usually is chosen so that @xmath5 .",
    "note that the definitions of @xmath6 , @xmath7 , and @xmath8 are chosen to simplify the interpretation of the nmf .    to compute @xmath7 and @xmath8 , usually eq .",
    "[ ch3:eq1 ] is rewritten into a minimization problem in frobenius norm .",
    "@xmath9 in addition to the usual frobenius norm , family of bregman divergences  which frobenius norm and kullback - leibler divergence are part of it  can also be used as the distance measures . detailed discussion on bregman divergences can be found in , e.g. , ref .  @xcite . in this work , we will consider only frobenius norm .",
    "all nmf algorithms are formulated in the alternating fashion , fixing one matrix while solving the other ( the popular lee & seung algorithms @xcite and their derivatives , e.g. , @xcite also use the alternating strategy , but can not be represented by generic algorithm below ) .",
    "this strategy is employed because the nmf is nonconvex with respect to @xmath7 and @xmath8 , but is convex with respect to @xmath7 or @xmath8 @xcite .",
    "thus , the alternating strategy transforms nmf problem into a pair of convex subproblems . transforming a nonconvex problem into the corresponding convex subproblems",
    "is a common practice in optimization researches because : ( 1 ) convex optimization is more tractable , ( 2 ) usually convex methods are more efficient , ( 3 ) any local optimum is necessarily a global optimum , and ( 4 ) the algorithms are easy to initialize @xcite .",
    "initialization : @xmath10 .",
    "@xmath11    algorithm [ ch3:alg1 ] solves nmf problem in the alternating fashion which will generate a solution sequence @xmath12 .",
    "this algorithm is known as alternating nonnegativity - constrained least square ( anls ) algorithm , and usually is solved by decomposing each subproblem into the corresponding nonnegativity - constrained least square ( nnls ) problems , where there are many algorithms that guarantee the global - optimality of the nnls problems .",
    "the following equations are the nnls versions of the anls in algorithm [ ch3:alg1 ] : @xmath13 where @xmath14 is the @xmath15-th row of @xmath16 .    according to grippo & sciandrone @xcite , any limit point of @xmath17 , @xmath18generated by any anls algorithm that optimally solves the convex subproblem eq .",
    "[ ch3:eq3 ] and eq .",
    "[ ch3:eq4]is a stationary point . and",
    "such anls based nmf algorithms exist , e.g. , @xcite , therefore there is guarantee that the stationary points are reachable . and as nnls is the building block for anls , any nnls algorithm that guarantees to find optimal solutions of eq .",
    "[ ch3:eq5 ] and eq .",
    "[ ch3:eq6 ] , e.g. , @xcite can also be employed to search for the stationary points . and as will be shown in section [ ch3:clusteringnmf ] , nmf objective ( eq .  [ ch3:eq2 ] ) implicitly puts upper bounds on the feasible region ( the lower bounds are explicit : the nonnegativity constraints ) . thus the nmf is bound - constrained optimization problem , consequently @xmath17 , @xmath18 has at least one limit point @xcite .",
    "this completes the conditions for any nmf algorithm that optimally solves subproblem eq .",
    "[ ch3:eq3 ] and eq .",
    "[ ch3:eq4 ] to have convergence guarantee .",
    "this section is the first part of this paper in which a theoretical framework for supporting clustering aspect of the nmf will be provided .",
    "the strict kkt optimality conditions will be utilized to derive the equivalence between nmf objective to graph clustering objective . unlike previous approaches where the kkt multipliers are set to zeros @xcite",
    ", we will make no assumption about the kkt multipliers , thus the stationary point under investigation is guaranteed to be located on the feasible region in the nonnegative orthant .",
    "we will also show that the feasible region is bounded , with the lower bounds are explicitly bounded by the nonnegativity constraints , and the upper bounds are implicitly bounded by the objective . as stated in section [ ch3:limitpoint ] ,",
    "the boundedness of the feasible region is the necessary condition for guaranteeing the existence of limit point of @xmath19 . and",
    "for interpretability reason , the data matrix @xmath6 will be considered as a feature - by - item data matrix unless stated differently .",
    "the following proposition gives the theoretical support for clustering aspect of the nmf .",
    "[ ch3:prop1 ] minimizing the following objective @xmath20 leads to the feature clustering indicator matrix @xmath7 and the item clustering indicator matrix @xmath8 .",
    "@xmath21    since @xmath6 is constant , minimizing @xmath22 is equivalent to simultaneously optimizing : @xmath23 note that because @xmath24 , minimizing eq .",
    "[ ch3:eq11 ] is equivalent to : @xmath25    the kkt function of objective in eq .",
    "[ ch3:eq9 ] is : @xmath26 where @xmath27 and @xmath28 are the kkt multipliers . by applying the kkt optimality conditions to @xmath29",
    "we get : @xmath30 with complementary slackness : @xmath31 where @xmath32 denotes component - wise multiplications .",
    "eq .  [ ch3:eq15 ] and eq .  [ ch3:eq16 ]",
    "lead to : @xmath33 substituting eq .",
    "[ ch3:eq18 ] to eq .",
    "[ ch3:eq10 ] leads to : @xmath34 which is equivalent to simultaneously optimizing : @xmath35 similarly , substituting eq .",
    "[ ch3:eq17 ] to eq .",
    "[ ch3:eq10 ] leads to : @xmath36 which is equivalent to simultaneously optimizing : @xmath37 as shown , eq .",
    "[ ch3:eq22 ] and eq .",
    "[ ch3:eq26 ] recover eq .",
    "[ ch3:eq12 ] and eq .",
    "[ ch3:eq13 ] respectively , so there is no need to substituting eq .  [ ch3:eq17 ] and eq .  [ ch3:eq18 ] into eq .",
    "[ ch3:eq11 ] .",
    "now we concentrate on the basis matrix @xmath7 first . eq .",
    "[ ch3:eq20 ]  [ ch3:eq22 ] give alternative objectives to the original nmf objective that contain only @xmath7 . note that if we consider @xmath6 to be an affinity matrix induced from bipartite graph @xmath38 ( which is a reasonable thought since any feature - by - item matrix can be modeled by a bipartite graph ) , then @xmath39 is the feature graph where edge weights describe the similarity between corresponding vertex pairs .",
    "so , eq .  [ ch3:eq20 ] looks like _ ratio association _ applied to @xmath39 . but without orthogonality constraint @xmath40 ( which is the part of _ ratio association _ objective ) , one can optimize eq .  [ ch3:eq20 ] by setting @xmath7 to an infinity matrix",
    ". however , this violates eq .",
    "[ ch3:eq22 ] which favours small @xmath7 .",
    "similarly , one can optimize eq .",
    "[ ch3:eq22 ] by setting @xmath7 to a zero matrix . but",
    "again , this violates eq .",
    "[ ch3:eq20 ] .",
    "thus , eq .  [ ch3:eq20 ] and eq .  [ ch3:eq22 ] create implicit lower and upper bound constraints on @xmath7 : @xmath41 .    for convenience , eq .",
    "[ ch3:eq22 ] can be restated as : @xmath42 by using the fact @xmath43 , eq .",
    "[ ch3:eq27 ] can be rewritten into : @xmath44 therefore , eq .  [ ch3:eq20 ]  [ ch3:eq22 ] can be restated as : @xmath45 even though @xmath7 is now bounded , since there is no column - orthogonality constraint , maximizing eq .",
    "[ ch3:eq29 ] can be easily done by setting each entry of @xmath7 to the corresponding largest possible value ( in graph term this means to only create one partition on @xmath46 ) . but",
    "this scenario results in a large value of eq .",
    "[ ch3:eq31 ] , which violates the objective .",
    "similarly , minimizing eq .",
    "[ ch3:eq31 ] to the smallest possible value violates eq .",
    "[ ch3:eq29 ] . since minimizing @xmath47 implies minimizing @xmath48 , but not vice versa , simultaneously optimizing eq .",
    "[ ch3:eq29 ] and eq .",
    "[ ch3:eq31 ] can be done by setting @xmath48 as small as possible and balancing @xmath47 with eq .",
    "[ ch3:eq29 ] .",
    "this scenario is the relaxed _ ratio association _ applied to @xmath46 , and as long as vertices in @xmath46 are clustered , it leads to the grouping of related features .",
    "the remaining problem is eq .",
    "[ ch3:eq30 ] .",
    "since we know nothing about @xmath49 , the best bet will be making each entry of @xmath50 as large as possible .",
    "this can be done by setting @xmath7 to the largest possible values , but this scenario violates eq .",
    "[ ch3:eq31 ] .",
    "so , the most reasonable scenario will be making the entries near diagonal region of @xmath50 as large as possible .",
    "this can be achieved by using @xmath7 from previous discussion . as @xmath7 is the feature clustering indicator matrix , multiplying @xmath51 with @xmath7 will result in a matrix that has larger entries near diagonal region ,",
    "therefore it can be expected that eq .",
    "[ ch3:eq30 ] will have good optimality .",
    "thus simultaneously optimizing eq .",
    "[ ch3:eq29 ]  [ ch3:eq31 ] leads to the feature clustering indicator matrix @xmath7 .    by applying the similar approach to the coefficient matrix @xmath8 , optimizing eq .",
    "[ ch3:eq24 ]  [ ch3:eq26 ] is equivalent to optimizing : @xmath52 where @xmath53 denotes @xmath15-th row of @xmath8 . by following the previous discussion on @xmath7",
    ", it can be shown that as long as vertices in @xmath54 are clustered , simultaneously optimizing eq .",
    "[ ch3:eq32 ]  [ ch3:eq34 ] leads to the item clustering indicator matrix @xmath8 .      as shown in the proof of proposition [ ch3:prop1 ] , optimizing nmf objective is equivalent to applying the relaxed _ ratio association _ to the item graph @xmath54 and the feature graph @xmath39 simultaneously . and because in the nmf , clustering membership of each point is directly determined by finding the largest projection on the axis of the decomposition rank subspace ( @xmath4 subspace ) @xcite , the nmf can only offer good results if the data points are linearly separable .",
    "this is not the case with the spectral clustering , where the memberships are indirectly determined by applying k - means clustering on the resulting factors .",
    "this additional step can sometimes find correct assignments even though the data points are not linearly separable . and",
    "unfortunately , since the factors produced by the nmf are nonnegative and directly point to the cluster s centers @xcite , applying k - means clustering on the factors wo nt change the clustering assignments .",
    "the following examples show the limitation of the nmf in clustering linearly inseparable data points . and for comparison , the spectral clustering is used .",
    "for the spectral clustering , we use ng et al .",
    "algorithm ( njw ) @xcite , and for the nmf , we use lee & seung algorithm ( nmfls ) @xcite , and kim & park algorithm ( nmfjk ) @xcite .",
    "njw and nmfls are the standard algorithm for the spectral clustering and the nmf respectively , and nmfjk is the nmf algorithm that has convergence guarantee .",
    "algorithm [ ch2:alg1 ] describes njw algorithm , and algorithm [ ch3:alg2 ] describes clustering using the nmf .",
    "note that we wrote codes for njw and nmfls by ourselves , and use codes from the authors website for nmfjk . to get the same treatment as in njw",
    ", we use the same kernel strategy for nmfls and nmfjk .",
    "the adjustable parameter @xmath55 is learned directly from the datasets , and the results are displayed in figure [ ch2:fig1 ] , [ ch3:fig1 ] , and [ ch3:fig2 ] .    1 .",
    "input : rectangular data matrix @xmath56 with @xmath57 data points , # cluster @xmath4 , and gaussian kernel parameter @xmath55 .",
    "2 .   construct symmetric affinity matrix @xmath58 from @xmath6 by using gaussian kernel .",
    "3 .   normalize @xmath59 by @xmath60 where @xmath61 is a diagonal matrix with @xmath62 .",
    "4 .   compute the @xmath4 largest eigenvectors of @xmath59 , and form @xmath63 $ ] , where @xmath64 is the @xmath65-th largest eigenvector of @xmath59 .",
    "normalize every row of @xmath66 , i.e. , @xmath67 .",
    "apply k - means clustering on the row of @xmath66 to obtain the clustering indicator matrix @xmath68 .    1 .",
    "input : rectangular data matrix @xmath56 with @xmath57 data points , # cluster @xmath4 , and gaussian kernel parameter @xmath55 .",
    "2 .   construct symmetric affinity matrix @xmath58 from @xmath6 by using gaussian kernel .",
    "3 .   compute @xmath7 and @xmath8 by using nmf algorithm ( nmfls or nmfjk ) so that @xmath69 .",
    "assume @xmath8 is used , then clustering assignment of data point @xmath70 , @xmath71 , can be computed by @xmath72 .    as shown in figure [ ch2:fig1 ] , [ ch3:fig1 ] , and [ ch3:fig2 ] , while the spectral clustering can correctly find the clustering assignments for all datasets , the nmfs can only compete with the spectral clustering for the last dataset which is rather linearly separable .",
    "these results are in accord with the proof of proposition [ ch3:prop1 ] ( that states as long as vertices on the feature @xmath73item@xmath74 graph are clustered , optimizing the nmf objective leads to the feature @xmath73item@xmath74 clustering indicator matrix ) .",
    "thus , it seems that as a clustering method , the nmf is more similar to k - means clustering or support vector machine ( svm ) which also can only cluster linearly separable datasets , than to the spectral methods , even though both clustering using the nmf and the spectral methods are based on matrix decomposition techniques .",
    "accordingly , clustering performances of the nmf can probably be improved by using appropriate kernel methods as in k - means clustering and svm .",
    "the experiments are conducted to evaluate the performances of the nmf as a clustering method .",
    "all algorithms are developed in gnu octave under linux platform using a notebook with 1.86 ghz intel processor and 2 gb ram .",
    "reuters-21578 document corpus , the standard dataset for testing learning algorithms and other text - based processing methods , is used for this purpose .",
    "this dataset contains 21578 documents ( divided into 22 files with each file contains 1000 documents and the last file contains 578 documents ) with 135 topics created manually with each document is assigned to one or more topics based on its content .",
    "the dataset is available in sgml and xml format , we use the xml version .",
    "we use all but the 18@xmath75 file because this file is invalid both in its sgml and xml version .",
    "we use only documents that belong to exclusively one class ( we use `` classes '' for refeering to the original grouping , and `` clusters '' for referring to groups resulted from the clustering algorithms ) .",
    "further , we remove the common english stop words , stem the remaining words using porter stemmer @xcite , and then remove words that belong to only one document . and also , we normalize the term - by document matrix @xmath6 by : @xmath76 where @xmath77 as suggested by xu et al .",
    "we form test datasets by combining top 2 , 4 , 6 , 8 , 10 , and 12 classes from the corpus .",
    "table [ ch2:table3 ] summarizes the statistics of these test datasets , where # doc , # word , % nnz , max , and min refer to the number of document , the number of word , percentage of nonzero entry , maximum cluster size , and minimum cluster size respectively . and table [ ch2:table4 ] gives the sizes ( # doc ) of these top 12 classes .",
    ".statistics of the test datasets . [ cols=\"<,>,>,>,>,>\",options=\"header \" , ]     [ ch3:table13 ]",
    "we have presented a theoretical framework for supporting clustering aspect of the nmf without setting the kkt multipliers to zeros . thus the stationary point used in proving this aspect",
    "is guaranteed to be on the nonnegative orthant which is the feasible region of the nmf .",
    "our theoretical work implies a limitation of the nmf as a clustering method in which it can not be used in clustering linearly inseparable datasets .",
    "so , the nmf as a clustering method is more resembling k - means clustering or svm than the spectral clustering , even though both the nmf and the spectral methods utilize matrix decomposition techniques . as the clustering capabilities of k - means and svm usually can be improved by using the kernel methods , probably the same approach can also be employed in the nmf .",
    "we will address this issue in our future researches .    clustering capability of nmfjk is comparable to the svd in reuters datasets with nmfjk tends to be better for small # cluster and the svd for big # cluster .",
    "but unfortunately , nmfls which is the standard nmf algorithm can not outperform the svd .",
    "these results imply clustering aspect of the nmf is algorithm - dependent , a fact that seems to be overlooked in the nmf researches .",
    "lsi aspect of the nmf seems to be comparable to the svd in its power for solving synonymy and polysemy problems for datasets with clear semantic structures that allowed these problems to be revealed . in real datasets , however , the nmf generally can not outperform the svd .",
    "but an interesting fact comes into sight ; in some cases , the nmf can outperform the svd , even though when the computations are repeated and averaged over the number of trials , these advantages vanish . because the nmf can offer different results depending on the algorithms , the initializations , the objectives , and the problems , improving lsi capability of the nmf is possible .",
    "we will address this problem in our future researches .",
    "h.  kim and h.  park , `` sparse non - negative matrix factorizations via alternating non - negativity constrained least squares for microarray data analysis , '' bioinformatics , vol .",
    "23(12 ) , pp .  1495 - 502 , 2007 .                  c.  ding , t.  li , and w.  peng , `` on the equivalence between non - negative matrix factorization and probabilistic latent semantic indexing , '' computational statistics & data analysis , vol .",
    "52(8 ) pp .  3913 - 27 , 2008 .                          h.  kim and h.  park , `` nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method , '' siam .",
    "j. matrix anal . & appl .",
    "30(2 ) , pp .  713 - 30 , 2008 .",
    "r.  albright , j.  cox , d.  duling , a.  langville , and c.  meyer , `` algorithms , initializations , and convergence for the nonnegative matrix factorization , '' ncsu technical report math 81706 , north carolina state university , 2006 ."
  ],
  "abstract_text": [
    "<S> this paper provides a theoretical support for clustering aspect of the nonnegative matrix factorization ( nmf ) . by utilizing the karush - kuhn - tucker optimality conditions , </S>",
    "<S> we show that nmf objective is equivalent to graph clustering objective , so clustering aspect of the nmf has a solid justification . </S>",
    "<S> different from previous approaches which usually discard the nonnegativity constraints , our approach guarantees the stationary point being used in deriving the equivalence is located on the feasible region in the nonnegative orthant . </S>",
    "<S> additionally , since clustering capability of a matrix decomposition technique can sometimes imply its latent semantic indexing ( lsi ) aspect , we will also evaluate lsi aspect of the nmf by showing its capability in solving the synonymy and polysemy problems in synthetic datasets . and more extensive evaluation will be conducted by comparing lsi performances of the nmf and the singular value decomposition ( svd)the standard lsi method  using some standard datasets .    </S>",
    "<S> bound - constrained optimization , clustering method , nonnegative matrix factorization , karush - kuhn - tucker optimality conditions , latent semantic indexing .    15a23 , 68r10 . </S>"
  ]
}