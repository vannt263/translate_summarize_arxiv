{
  "article_text": [
    "the recurrent neural network ( rnn ) model that we study in this paper is an echo state network ( esn ) with linear activation function .",
    "this system consist of an input driven recurrent network of size @xmath5 , and a linear readout layer trained to calculate a desired function of the input .",
    "let @xmath0 and @xmath11 indicate a one - dimensional input at time @xmath69 and an input weight vector respectively . let @xmath7 be a @xmath8 recurrent weight matrix , @xmath70 be an @xmath5-dimensional network state at time @xmath69 , and @xmath22 be the readout weight vector .",
    "the dynamics of the network and output is described by : @xmath71 where the readout weights are given by @xcite : @xmath72    the value of the optimal readout weights depend on the covariance and cross - covariance components @xmath73 and @xmath74 . here",
    "we show that these can be computed exactly for any arbitrary system given by @xmath7 and @xmath11 and autocorrelation of the input @xmath75 and cross - correlation of input and output @xmath76 .",
    "we begin by noting that the explicit expression for the system state is given by : @xmath77 calculating @xmath78 for a given problem requires the following input - output - dependent evaluations : @xmath79",
    "here we compute the memory function and the total memory of the recurrent neural network described in appendix  [ app : computing_xx_xy ] for exponentially correlated input where @xmath80 .",
    "the total memory of the system is given by the following summation over the memory function @xcite :    @xmath81    where @xmath34 is the input with lag @xmath2 , @xmath35 .",
    "computing @xmath55 requires the evaluation of : @xmath82 this assumes an even correlation function , i.e. , @xmath83 . for numerical computation",
    "it is more convenient to perform the calculation as follows : @xmath84 where @xmath85 is a partial sum of @xmath55 satisfying @xmath86 , @xmath87 is a partial sum of @xmath55 satisfying @xmath88 , and @xmath89 is a partial sum of @xmath55 satisfying @xmath90 , which is double counted and must be subtracted",
    ". we can substitute @xmath91 and evaluate @xmath85 and @xmath89 as follows : @xmath92    @xmath93    here @xmath94 is the identity of the hadamard product denoted by @xmath95 , and @xmath96 is a matrix inverse with respect to the hadamard product . here",
    "the trick is that @xmath97 takes the input to the basis of the connection matrix @xmath7 allowing the dynamics to be described by the powers of the eigenvalues of @xmath7 , i.e. , @xmath98 .",
    "since @xmath98 is symmetric we can use the matrix identity @xmath99 , where @xmath100 is the main diagonal of @xmath98 .",
    "summing over the powers of @xmath98 gives us @xmath101 .",
    "the covariance of the network states and the expected output is given by : @xmath102 for @xmath103 , the signal becomes i.i.d . and",
    "the calculations simplify as follows @xcite : @xmath104    the total memory capacity can be calculated by summing over @xmath105 : @xmath81",
    "for our experiment with memory capacity of network under exponentially correlated input we used the following setup .",
    "we generated @xmath106 long sample inputs with autocorrelation function @xmath107 .",
    "to generate exponentially correlated input we draw @xmath46 samples @xmath108 from a uniform distribution over the interval @xmath109 $ ] .",
    "the samples are passed through a low - pass filter with a smoothing factor @xmath50 .",
    "we normalize and center @xmath10 so that @xmath110 and @xmath111 .",
    "the resulting normalized samples @xmath0 have exponential autocorrelation with decay exponent @xmath50 , i.e. , @xmath112 .",
    "to validate our calculations , we use a network of @xmath57 nodes in a ring topology and identical weights . the spectral radius @xmath49 .",
    "the input weights @xmath11 are created by sampling the binomial distribution and multiplying with @xmath113",
    ". the scale of the input weights does not affect the memory and the performance in linear systems and therefore we adopt this convention for generating @xmath11 throughout the paper .",
    "we also assumed @xmath114 , the number of samples @xmath115 , washout period of @xmath116 steps , and regularization factor @xmath117 .",
    "a long standing question in recurrent neural network is how its structure effect its memory and task solving performance .",
    "our derivation lets us compute optimal readout layer for arbitrary network .",
    "here we describe the calculations we performed to examine the effect of structure of the network on its memory and task solving performance . to this end , we use networks of size @xmath118 , @xmath119 , and @xmath120 and we systematically study the randomness and spectral radius .",
    "we start from a uniform weight ring topology and incrementally add randomness from @xmath121 to @xmath122 .",
    "the results for each value of @xmath61 and @xmath48 are averaged over @xmath123 instances .",
    "this averaging is necessary even for @xmath121 because the input weights are randomly generated and although their scaling does not affect the result their exact values do  @xcite .",
    "the calculations in appendix  [ app : computing_xx_xy ] for optimal layer of a recurrent network may be described in a more generally in terms of power spectrum of the input signal . here",
    "we assume the setup in appendix  [ app : computing_xx_xy ] and derive an expressions for optimal readout layer using its the power spectrum of the input and output .",
    "we start by the standard calculation of @xmath55 and @xmath56 : @xmath79    we replace @xmath124 and @xmath125 which gives    @xmath126    and    @xmath127",
    "here we use the derivation in appendix  [ appsec : analyticspsdgeneric ] and compute the memory function and the total memory of the system .",
    "let @xmath128 and @xmath37 so that @xmath129    we find that @xmath130 the matrix @xmath40 is given by @xmath41\\otimes[\\frac{\\bar{\\boldsymbol\\omega}}{1-e^{-if } { \\bf d } } ] s_{uu}(f ) df,\\end{aligned}\\ ] ] and the matrix @xmath42 is given by : @xmath43 \\otimes [ \\int_{-t}^{t}\\frac{s_{u\\widehat{y}}(f)e^{if\\tau}}{1-e^{if } { \\bf d}}df].\\end{aligned}\\ ] ] the total memory is then given by : @xmath131 where @xmath45 \\otimes [ \\frac{s_{u\\widehat{y}}(f')}{1-e^{if'}{\\bf d}}].\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> recurrent neural networks ( rnn ) are simple dynamical systems whose computational power has been attributed to their short - term memory </S>",
    "<S> . short - term memory of rnns has been previously studied analytically only for the case of orthogonal networks , and only under annealed approximation , and uncorrelated input . here for the first time , we present an exact solution to the memory capacity and the task - solving performance as a function of the structure of a given network instance , enabling direct determination of the function  structure relation in rnns . </S>",
    "<S> we calculate the memory capacity for arbitrary networks with exponentially correlated input and further related it to the performance of the system on signal processing tasks in a supervised learning setup . </S>",
    "<S> we compute the expected error and the worst - case error bound as a function of the spectra of the network and the correlation structure of its inputs and outputs . </S>",
    "<S> our results give an explanation for learning and generalization of task solving using short - term memory , which is crucial for building alternative computer architectures using physical phenomena based on the short - term memory principle .    </S>",
    "<S> excitable dynamical systems , or reservoirs , store a short - term memory of a driving input signal in their instantaneous state  @xcite . </S>",
    "<S> this memory can produce a desired output in a linear readout layer , which can be trained efficiently using ordinary linear regression or gradient descent . </S>",
    "<S> this paradigm , called   ( rc ) , was originally proposed as a simplified model of information processing in the prefrontal cortex  @xcite . </S>",
    "<S> it was later generalized to explain computation in cortical microcircuits  @xcite and to facilitate training in recurrent neural networks  @xcite . </S>",
    "<S> a central feature of rc is the lack of fine tuning of the underlying dynamical system : any random structure that guarantees a stable dynamics gives rise to short - term memory  @xcite . </S>",
    "<S> analogous behavior has also been observed in selective response in random neural populations  @xcite . </S>",
    "<S> furthermore , fixed underlying structure in rc makes it suitable for implementing computation using spatially distributed physical phenomena@xcite . </S>",
    "<S> such approaches can give us a way to store and process information more efficiently than with von neumann architecture  @xcite .    </S>",
    "<S> short - term memory in neural networks has been studied for uncorrelated input @xmath0 under annealed approximation , i.e. , connectivity is resampled independently at each time step  @xcite . </S>",
    "<S> that study considered only linear orthogonal networks , where the columns of the connectivity matrix are pairwise orthogonal and the node transfer functions are linear . </S>",
    "<S> a memory function @xmath1 was defined to measure the ability of the system to reconstruct input from @xmath2 time steps ago , i.e. , @xmath3 , from the present system state @xmath4 . </S>",
    "<S> it was shown that the total memory capacity can not exceed the @xmath5 degrees of freedom in the system . for networks with saturating nonlinearity , </S>",
    "<S> the memory scales with @xmath6  @xcite ; however , by fine - tuning the nonlinearity one can achieve near - linear scaling of memory capacity  @xcite . </S>",
    "<S> in nonlinear networks , it is very difficult to analyze the complete memory function and even harder to relate it to the performance on computational tasks , as is evident from many works in this area with hard - to - reconcile conclusions ( see ref .  </S>",
    "<S> @xcite ) .    </S>",
    "<S>  consider a discrete - time network of @xmath5 nodes . </S>",
    "<S> the network weight matrix @xmath7 is @xmath8 with spectral radius @xmath9 . </S>",
    "<S> a time - dependent scalar input signal @xmath10 is fed to the network using the input weight vector @xmath11 . </S>",
    "<S> the evolution of the network state @xmath12 and the output @xmath13 is governed by @xmath14 where @xmath15 is an @xmath5-dimensional column vector calculated for a desired output @xmath16 . here , </S>",
    "<S> each column of @xmath17 is the state of the network at time @xmath12 and each column of @xmath18 is the corresponding desired output at each time step . in practice </S>",
    "<S> it is sometimes necessary to use tikhonov regularization to calculate the readout weights , i.e. , @xmath19 , where @xmath20 is a regularization factor that needs to be adjusted depending on @xmath21 , and @xmath10 @xcite . </S>",
    "<S> + calculating @xmath22 for a given problem requires the following input - output - dependent evaluations ( appendix  [ app : computing_xx_xy ] ) : @xmath23 where @xmath24 and @xmath25 are the autocorrelation of the input and the cross - correlation of the input and target output . </S>",
    "<S> this may also be expressed more generally in terms of the power spectrum of the input and the target : @xmath26 where @xmath27 and @xmath28 , @xmath29 is the power spectral density of the input , and @xmath30 is the cross - spectral density of the input and the target output .    </S>",
    "<S> the performance can be evaluated by the mean - squared - error ( mse ) as follows : @xmath31    the mse gives us a distribution - independent upper bound on the instantaneous - squared - error through the application of markov inequality : @xmath32 </S>",
    "<S> \\le \\frac{\\langle e^2\\rangle}{a}.\\end{aligned}\\ ] ] the existence of a worst - case bound is important for engineering applications of rc .     </S>",
    "<S> the memory function of the system is defined as  @xcite @xmath33 where @xmath34 is the input with lag @xmath2 , @xmath35 .    </S>",
    "<S> the exact evaluation of this function has previously proved elusive for arbitrary @xmath7 and @xmath11 . here </S>",
    "<S> we provide a solution using eigendecomposition of @xmath7 and the power spectral density of the input signal . </S>",
    "<S> the solution may also be described directly in terms of the autocorrelation of the input , as in appendix  [ app : computing_mcac ] .    </S>",
    "<S> let @xmath36 and @xmath37 so that @xmath38 the memory function is reduced to @xmath39 where the matrix @xmath40 is given by @xmath41\\otimes[\\frac{\\bar{\\boldsymbol\\omega}}{1-e^{-if } { \\bf d } } ] s_{uu}(f ) df,\\end{aligned}\\ ] ] and the matrix @xmath42 is given by @xmath43 \\otimes [ \\int_{-t}^{t}\\frac{s_{u\\widehat{y}}(f)e^{if\\tau}}{1-e^{if } { \\bf d}}df].\\end{aligned}\\ ] ]    the total memory is then given by @xmath44 where @xmath45 \\otimes [ \\frac{s_{u\\widehat{y}}(f')}{1-e^{if'}{\\bf d}}].\\end{aligned}\\ ] ]     +     +     +    we validate our formula by comparing analytical and empirical evaluation of the memory curve . </S>",
    "<S> the input is assumed to be a sequence of length @xmath46 with autocorrelation function @xmath47 ( appendix  [ appsec : setup ] ) . </S>",
    "<S> fig  [ fig : correlated_input](a ) shows the result of the single - instance calculation of the analytical and the empirical memory curves for different @xmath48 ( appendix  [ appsec : setup ] ) . </S>",
    "<S> as expected , the analytical and empirical results agree .    </S>",
    "<S> we also study the memory capacity for different levels of structure in the input signal . here </S>",
    "<S> , we use simple ring topologies with @xmath49 and vary the decay exponent @xmath50 , fig  [ fig : correlated_input](b ) . for a fixed system size , </S>",
    "<S> decreasing @xmath50 exponentially increases the correlation in the input , which increases the memory capacity .    </S>",
    "<S> next , we use our method to calculate the optimal output layer , the expected average error , and bounds on worst - case error for a common narma10 benchmark task : @xmath51 where @xmath52 , @xmath53 . the input @xmath10 is drawn from a uniform distribution over the interval @xmath54 $ ] . here </S>",
    "<S> the evaluation of @xmath55 follows the same calculation as for the memory capacity for the uniform distribution . for @xmath56 </S>",
    "<S> we must estimate the cross - correlation of @xmath13 and @xmath10 and substitute it into equation  [ eq : basicsums ] . </S>",
    "<S> fig  [ fig : narma10corr](a ) shows the output of a network of @xmath57 nodes and @xmath49 with analytically calculated optimal output layer . </S>",
    "<S> the output agrees with the correct output of the narma10 system . </S>",
    "<S> the cross - correlation of the system used for the calculation is shown in the inset . </S>",
    "<S> fig  [ fig : narma10corr](b ) shows the worst - case error bound for this system and the empirical errors generated from the system output , showing that the bound we derived is tight .     </S>",
    "<S> +   +    finally , we show how the framework can be used in a prediction scenario , namely the prediction of the mackey - glass system . </S>",
    "<S> the mackey - glass system @xcite was first proposed as a model for feedback systems that may show different dynamical regimes . </S>",
    "<S> the system is a one - dimensional delayed feedback differential equation and manifests a wide range of dynamics , from fixed points to strange attractors with varying divergence rates ( lyapunov exponent ) . </S>",
    "<S> this system has been used as a benchmark task for chaotic signal prediction and generation  @xcite . </S>",
    "<S> it is defined as : @xmath58 where @xmath59 ensure the chaoticity of the dynamics  @xcite .    </S>",
    "<S> fig  [ fig : narma10corr](c ) shows the prediction result for 10 time steps ahead and the inset shows the autocorrelation at different lags . </S>",
    "<S> the autocorrelation is characterized by a long correlation length evident from non - zero correlation values for large @xmath2 . </S>",
    "<S> this long memory is a hallmark of chaotic systems . </S>",
    "<S> we use this information to evaluate equation  [ eq : genericxx ] and equation  [ eq : basicsums ] , where for predicting @xmath2 steps ahead we have @xmath60     the effect of randomness and sparsity of reservoir connectivity has been a subject of debate  @xcite . to study the effect of network structure on memory and performance , we systematically explore the range between sparse deterministic uniform networks and random graphs . </S>",
    "<S> we start from a simple ring topology with identical weights and induce noise to @xmath61 random links by sampling the normal distribution @xmath62 . </S>",
    "<S> we then re - evaluate the memory and task solving performance keeping the weight matrix fixed . </S>",
    "<S> we evaluate system performance on a memory task with exponentially correlated inputs , the nonlinear autoregressive narma10 task , and the mackey - glass chaotic time series prediction . </S>",
    "<S> we systematically explore the effects of @xmath48 , @xmath5 , and @xmath61 on the performance of the system .    </S>",
    "<S> fig  [ fig : toplogyresult](a ) shows the resulting total memory capacity normalized by @xmath5 as a function of increasing randomness @xmath63 for different spectral radii @xmath48 . </S>",
    "<S> the expected theoretical total memory capacity for an uncorrelated signal is @xmath64 . here </S>",
    "<S> the system exploits the structure of the input signal to store longer input sequences , i.e. , @xmath65 . </S>",
    "<S> this effect has been studied previously under annealed approximation and in a compressive sensing setup  @xcite . however , here we see that even without the sparse input assumption and @xmath66 optimization in the output ( a computationally expensive optimization used in compressive sensing ) the network can achieve capacity greater than its degrees of freedom @xmath5 . </S>",
    "<S> fig  [ fig : toplogyresult](b ) and ( c ) show the error in the narma10 and the mackey - glass prediction tasks . here , best performance is achieved for a regular architecture . </S>",
    "<S> a slight randomness significantly increases error at first , but additional irregularity will decrease it . </S>",
    "<S> this can be observed for the narma10 task at @xmath67 and for the mackey - glass prediction task at @xmath49 .     </S>",
    "<S> although memory capacity of rnns has been studied before , it is learning and generalization ability in a task solving setup is not discussed . </S>",
    "<S> our derivation allows us to relate the memory capacity to task solving performance for arbitrary rnns and reason about their generalization . in empirical experiments with systems presented here , the training and testing are done with finite input sequences that are sampled independently for each experiment , so the statistics of the training and testing inputs vary according to a gaussian distribution around their true values and one expects these estimates to approach their true values with increasing sample size . </S>",
    "<S> hence , the mean - squared - error @xmath68 , which is linear in the input and output statistics , is also distributed as a gaussian for repeated experiments . by the law of large numbers </S>",
    "<S> , the difference between testing and training mean - squared - error tends to zero in the limit . </S>",
    "<S> this explains the ability of the system to generalize its computation from training to test samples .     </S>",
    "<S> the computational power of reservoir computing networks has been attributed to their memory capacity . </S>",
    "<S> while their memory properties have been studied under annealed approximation , thus far no direct mathematical connection to their signal processing performance had been made . </S>",
    "<S> we developed a mathematical framework to exactly calculate the memory capacity of rc systems and extended the framework to study their expected and worst - case errors on a given task in a supervised learning setup . </S>",
    "<S> our result confirms previous studies that the upper bound for memory capacity for uncorrelated inputs is @xmath5 . </S>",
    "<S> we further show that the memory capacity monotonically increases with correlation in the input . </S>",
    "<S> intuitively , the output exploits the redundant structure of the inputs to retrieve longer sequences . </S>",
    "<S> moreover , we generalize our derivation to task solving performance . </S>",
    "<S> our derivation help us reason about the memory and performance of arbitrary systems directly in terms of their structure . </S>",
    "<S> we showed that networks with regular structure have a higher memory capacity but are very sensitive to slight changes in the structure , while irregular networks are robust to variation in their structure .    </S>",
    "<S> _ acknowledgments . </S>",
    "<S> _ this work was partly supported by nsf grants # 1028238 and # 1028120 , # 1518861 , and # 1525553 .    99    s. ganguli , d. huh , h. sompolinsky , proc .  </S>",
    "<S> natl .  </S>",
    "<S> acad .  </S>",
    "<S> sci .  </S>",
    "<S> usa  * 105 * , 1897018975 ( 2008 )    p. dominey , m. arbib , and j.  p. joseph , j.  cogn .  </S>",
    "<S> neurosci .  </S>",
    "<S> * 7 * , 311336 ( 1995 )    w. maass , t. natschlger , and h. markram , neural comput .  </S>",
    "<S> * 14 * , 253160 , ( 2002 )    h. jaeger and h. haas , science  * 304 * , 148102 ( 2004 )    lukoeviius , h. jaeger , and b. schrauwen , k \" unstliche intelligenz  * 26 * , 365371 ( 2012 )    d. hansel and c. van vreeswijk , j.  neurosci .  </S>",
    "<S> * 32 * , 4049 - 4064 ( 2012 )    j.  p. crutchfield , w.  l. ditto , and s. sinha , chaos * 20 * , 037101 ( 2010 )    o.  l. white , d.  d. lee , and h. sompolinsky , phys .  </S>",
    "<S> rev .  </S>",
    "<S> lett .  </S>",
    "<S> * 92 * , 148102 ( 2004 )    t. toyoizumi , neural comput .  * </S>",
    "<S> 24 * , 267899 , ( 2012 )    l. bsing , b. schrauwen , and r. legenstein , neural comput .  </S>",
    "<S> * 22 * , 12721311 ( 2010 )    a. rodan and p. tio , neural networks , ieee transactions on * 22 * , 131 - 144 ( 2011 )    s. ganguli and h. sompolinsky , advances in neural information processing systems , * 23 * , 667675 , ( 2010 )    a. goudarzi , c. teuscher , n. gulbahce , and t. rohlf , phys .  rev </S>",
    "<S> .  lett .  * </S>",
    "<S> 108 * , 128702 ( 2012 )    d. snyder , a. goudarzi , and c. teuscher , phys .  </S>",
    "<S> rev .  </S>",
    "<S> e  * 87 * , 042808 ( 2013 )    h. o. sillin , r. aguilera , h. shieh , a. v. avizienis , m aono , a. z. stieg , and j. k. gimzewski , nanotechnology , * 24 * , 384004 , ( 2013 )    a. goudarzi and d. stefanovic , procedia computer science * 41 * , 176181 , ( 2014 )    n. d. haynes , m. c. soriano , d. p. rosin , i. fischer , and d. j. gauthier , phys . rev . </S>",
    "<S> e * 91 * , 020801 , ( 2015 )    k. nakajima , t. li , h. hauser , and r. pfiefer , j. r. soc . </S>",
    "<S> interface * 11 * , 20140437 , ( 2014 )    j. b \" urger , a. goudarzi , d. stefanovic , and c. teuscher , aims materials science * 2 * , 530545 , ( 2015 )    y. katayama , t. yamane , d. nakano , r. nakane , and g. tanaka , proceedings of the 2015 ieee / acm international symposium on nanoscale architectures ( nanoarch 15 ) , ieee , p. 23 </S>",
    "<S> - 24 , ( 2015 )    k. vandoorne , p. mechet , t. van vaerenbergh , m. fiers , g. morthier , d. verstraeten , b. schrauwen , j. dambre , and p. bienstman , nat </S>",
    "<S> . commun . * 5 * , 3541 , ( 2014 )    m. c. mackey and l. glass , science * 197 * , 287289 , ( 1977 ) </S>"
  ]
}