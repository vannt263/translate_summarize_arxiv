{
  "article_text": [
    "the notion of directed information introduced by massey in  @xcite assesses the amount of information that causally `` flows '' from a given random and ordered sequence to another . for this reason",
    ", it has increasingly found use in diverse applications , from characterizing the capacity of channels with feedback  @xcite , the rate distortion function under causality constraints  @xcite , establishing some of the fundamental limitations in networked control  @xcite , determining causal relationships in neural networks  @xcite , to portfolio theory and hypothesis testing  @xcite , to name a few .    the directed information from a random ) for random variables , denoting a particular realization by the corresponding italic character , @xmath0 . ]",
    "sequence @xmath1 to a random sequence @xmath2 is defined as @xmath3 where the notation @xmath4 represents the sequence @xmath5 .",
    "the causality inherent in this definition becomes evident when comparing it with the mutual information between @xmath1 and @xmath2 , given by @xmath6 . in the latter sum ,",
    "what matters is the amount of information about the _",
    "sequence @xmath1 present in @xmath7 , given the past values @xmath8 .",
    "by contrast , in the conditional mutual informations in the sum of  , only the past and current values of @xmath1 are considered , that is , @xmath4 .",
    "thus , @xmath9 represents the amount of information causally conveyed from @xmath1 to @xmath2 .    there",
    "exist several results characterizing the relationship between @xmath9 and @xmath10 .",
    "first , it is well known that @xmath11 , with equality if and only if @xmath2 is causally related to @xmath1  @xcite . a conservation law of mutual and directed information has been found in  @xcite , which asserts that @xmath12 , where @xmath13 denotes the concatenation @xmath14 .",
    "given its prominence in settings involving feedback , it is perhaps in these scenarios where the directed information becomes most important .",
    "for instance , the directed information has been instrumental in characterizing the capacity of channels with feedback ( see , e.g. ,  @xcite and the references therein ) , as well as the rate - distortion function in setups involving feedback  @xcite .    in this paper , our focus is on the relationships ( inequalities and identities ) involving directed and mutual informations within feedback systems , as well as between directed informations involving different signals within the corresponding feedback loop . in order to discuss some of the existing results related to this problem , it is convenient to consider the general feedback system shown in fig .",
    "[ fig : diagramas]-(a ) . in this diagram",
    ", the blocks @xmath15 represent possibly non - linear and time - varying causal systems such that the total delay of the loop is at least one sample . in the same figure ,",
    "@xmath16 are exogenous random signals ( scalars , vectors or sequences ) , which could represent , for example , any combination of disturbances , noises , random initial states or side informations .",
    "we note that any of these exogenous signals , in combination with its corresponding deterministic mapping @xmath17 , can also yield any desired stochastic causal mapping .    for the simple case in which all the systems @xmath18",
    "are linear time invariant ( lti ) and stable , and assuming @xmath19 ( deterministically ) , it was shown in  @xcite that @xmath20 does not depend on whether there is feedback from @xmath21 to @xmath22 or not .",
    "inequalities between mutual and directed informations in a less restricted setup , shown in fig .",
    "[ fig : diagramas]-(b ) , have been found in  @xcite . in that setting ( a networked - control system ) , @xmath23 is a strictly causal lti dynamic system having ( vector ) state sequence @xmath24 , with @xmath25 being the random initial state in its state - space representation .",
    "the external signal @xmath26 ( which could correspond to a disturbance ) is statistically independent of @xmath27 , the latter corresponding to , for example , side information or channel noise . both are also statistically independent of @xmath28 .",
    "the blocks labeled @xmath29 , @xmath30 and @xmath31 correspond to an encoder , a decoder and a channel , respectively , all of which are causal .",
    "the channel @xmath31 maps @xmath32 and @xmath1 to @xmath33 in a possibly time - varying manner , i.e. , @xmath34 similarly , the concatenation of the encoder , the channel and the decoder , maps @xmath32 and @xmath35 to @xmath36 as a possibly time - dependent function @xmath37 under these assumptions , the following fundamental result was shown in  ( * ? ? ?",
    "* lemma  5.1 ) : @xmath38 by further assuming in  @xcite that the decoder @xmath30 in fig .",
    "[ fig : diagramas]-(b ) is deterministic , the following markov chain naturally holds , @xmath39 leading directly to @xmath40 which is found in the proof of  ( * ? ? ?",
    "* corollary  5.3 ) .",
    "the deterministic nature of the decoder @xmath30 played a crucial role in the proof of this result , since otherwise the markov chain   does not hold , in general , due to the feedback from @xmath22 to @xmath41 .",
    "notice that both   and   provide lower bounds to the difference between two mutual informations , each of them relating a signal _ external _ to the loop ( such as @xmath42 ) to a signal _ internal _ to the loop ( such as @xmath43 or @xmath2 ) . instead , the inequality @xmath44 which holds for the system in fig",
    ".  [ fig : diagramas]-(a ) and appears in  ( * ? ? ? * theorem  3 ) ( and rediscovered later in  ( * ? ? ?",
    "* lemma  4.8.1 ) ) , involves the directed information between two internal signals and the mutual information between the second of these and an external sequence .",
    "a related bound , similar to   but involving information rates and with the leftmost mutual information replaced by the directed information from @xmath1 to @xmath2 ( which are two signals internal to the loop ) , has been obtained in  ( * ? ? ? * lemma  4.1 ) : @xmath45 with @xmath46 and @xmath47 , provided @xmath48 .",
    "this result relies on three assumptions : a ) that the channel @xmath31 is memory - less and satisfies a `` conditional invertibility '' property , b ) a finite - memory condition , and c ) a fading - memory condition , these two related to the decoder @xmath30 ( see fig .",
    "[ fig : diagramas ] ) .",
    "it is worth noting that , as defined in  @xcite , these assumptions upon @xmath30 exclude the use of side information by the decoder and/or the possibility of @xmath30 being affected by random noise or having a random internal state which is non - observable ( please see  @xcite for a detailed description of these assumptions ) .",
    "the inequality   has recently been extended in  ( * ? ? ? * theorem  1 ) , for the case of discrete - valued random variables and assuming @xmath49 , as the following identity ( written in terms of the signals and setup shown in fig .",
    "[ fig : diagramas]-(a ) ) : @xmath50 letting @xmath51 in fig .",
    "[ fig : diagramas]-(a ) and with the additional assumption that @xmath52 , it was also shown in  ( * ? ? ? * theorem  1 ) that @xmath53 for the cases in which @xmath54 ( i.e. , when the concatenation of @xmath55 and @xmath56 corresponds to a summing node ) . in  @xcite ,   and   play important roles in characterizing the capacity of channels with noisy feedback .    to the best of our knowledge ,  ,  ,  ,   and   are the only results available in the literature which lower bound the difference between an internal - to - internal directed information and an external - to - internal mutual information .",
    "there exist even fewer published results in relation to inequalities between two directed informations involving only signals internal to the loop . to the best of our knowledge ,",
    "the only inequality of this type in the literature is the one found in the proof of theorem  4.1 of  @xcite .",
    "the latter takes the form of a ( quasi ) data - processing inequality for directed informations in closed - loop systems , and states that @xmath57 provided to mean `` @xmath58 is independent of @xmath41 '' . ] @xmath59 and if @xmath55 is such that @xmath60 is a function of @xmath61 ( i.e. , if @xmath55 is conditionally invertible ) @xmath62 . in",
    ", @xmath63 corresponds to the causally conditioned directed information defined in  @xcite .",
    "inequality   plays a crucial role  @xcite , since it allowed lower bounding the average data rate across a digital error - free channel by a directed information .",
    "( in  @xcite , @xmath64 corresponded to a random dither signal in an entropy - coded dithered quantizer . )    in this paper , we derive a set of information identities and inequalities involving pairs of sequences ( internal or external to the loop ) in feedback systems .",
    "the first of these is an identity which , under an independence condition , can be interpreted as a law of conservation of information flows . the latter identity is the starting point for most of the results which follow it . among other things , we extend   and   to the general setup depicted in fig .  [",
    "fig : diagramas]-(a ) , where _ none of the assumptions made in  @xcite ( except causality ) needs to hold_. moreover , we will prove the validity of   without assuming the conditional invertibility of @xmath55 nor that @xmath65 .",
    "the latter result is one of four novel data - processing inequalities derived in section  [ ssec : nested_directed ] , each involving two nested directed informations valid for the system depicted in fig .",
    "[ fig : diagramas]-(a ) .",
    "the last of these is a complete closed - loop counterpart of the traditional open - loop data - processing inequality .",
    "the remainder of this paper begins with a description of the systems under study and the extension of massey s directed information to the case in which each of the blocks in the loop may introduce an arbitrary , non - negative delay ( i.e. , we do not allow for anticipation ) .",
    "the information identities and inequalities are presented in section  [ sec : results ] .",
    "for clarity of the exposition , all the proofs are deferred to section  [ sec : proofs ] .",
    "a brief discussion of potential applications of our results is presented in section  [ sec : possible_applications ] , which is followed by the conclusions in section  [ sec : conclusions ] .",
    "we begin by providing a formal description of the systems labeled @xmath66 in fig .  [",
    "fig : diagramas]-(a ) .",
    "their input - output relationships are given by the possibly - varying deterministic mappings    [ eq : block_defs ] @xmath67    where @xmath16 are exogenous random signals and the ( possibly time - varying ) delays @xmath68 are such that @xmath69 that is , the concatenation of @xmath70 has a delay of at least one sample .",
    "for every @xmath71 , @xmath72 , i.e. , @xmath73 is a real random vector whose dimension is given by some function @xmath74 .",
    "the other sequences ( @xmath75 ) are defined likewise .      as stated in  @xcite , the directed information ( as defined in  )",
    "is a more meaningful measure of the flow of information between @xmath1 and @xmath2 than the conventional mutual information @xmath76 when there exists causal feedback from @xmath41 to @xmath58 . in particular , if @xmath1 and @xmath2 are discrete - valued sequences , input and output , respectively , of a forward channel , and if there exists _ strictly causal _ , perfect feedback ,",
    "so that @xmath77 ( a scenario utilized in  @xcite as part of an argument in favor of the directed information ) , then the mutual information becomes @xmath78 thus , when strictly causal feedback is present , @xmath10 fails to account for how much information about @xmath1 has been conveyed to @xmath2 through the forward channel that lies between them .",
    "it is important to note that , in  @xcite ( as well as in many works concerned with communications ) , the forward channel is instantaneous , i.e. , it has no delay .",
    "therefore , if a feedback channel is utilized , then this feedback channel must have a delay of at least one sample , as in the example above .",
    "however , when studying the system in fig .",
    "[ fig : diagramas]-(a ) , we may need to evaluate the directed information between signals @xmath1 and @xmath2 which are , respectively , input and output of a _ strictly casual _",
    "forward channel ( i.e. , with a delay of at least one sample ) , whose output is instantaneously fed back to its input . in such case , if one further assumes perfect feedback and sets @xmath79 , then , in the same spirit as before , @xmath80 = h(\\rvay^{k } ) .",
    "\\end{aligned}\\ ] ] as one can see , massey s definition of directed information ceases to be meaningful if instantaneous feedback is utilized .",
    "it is natural to solve this problem by recalling that , in the latter example , the forward channel had a delay , say @xmath81 , greater than one sample .",
    "therefore , if we are interested in measuring how much of the information in @xmath33 , not present in @xmath8 , was conveyed from @xmath4 through the forward channel , we should look at the mutual information @xmath82 , because only the input samples @xmath83 can have an influence on @xmath7 .",
    "for this reason , we introduce the following , modified notion of directed information    _ in this paper , the directed information from @xmath1 to @xmath2 through a forward channel with a non - negative time varying delay of @xmath84 samples is defined as @xmath85 _    for a zero - delay forward channel ,",
    "the latter definition coincides with massey s .",
    "likewise , we adapt the definition of causally - conditioned directed information to the definition @xmath86 when the signals @xmath21 , @xmath58 and @xmath41 are related according to  .    before finishing this section ,",
    "it is convenient to recall the following identity ( a particular case of the chain rule of conditional mutual information  @xcite ) , which will be extensively utilized in the proofs of our results : @xmath87",
    "we begin by stating a fundamental result , _ which relates the directed information between two signals within a feedback loop , say @xmath58 and @xmath41 , to the mutual information between an external set of signals and @xmath41 _ :    [ thm : main ] _ in the system shown in fig .",
    "[ fig : diagramas]-(a ) , it holds that @xmath88 with equality achieved if @xmath27 is independent of @xmath89 .",
    "_    this fundamental result , which for the cases in which @xmath90 can be understood as a _ law of conservation of information flow _ , is illustrated in fig .",
    "[ fig : information_flow ] . for such cases ,",
    "the information causally conveyed from @xmath58 to @xmath41 equals the information flow from @xmath91 to @xmath41 .",
    "when @xmath89 are not independent of @xmath27 , part of the mutual information between @xmath89 and @xmath41 ( corresponding to the term @xmath92 ) can be thought of as being `` leaked '' through @xmath27 , thus bypassing the forward link from @xmath58 to @xmath41 .",
    "this provides an intuitive interpretation for  .",
    "theorem  [ thm : main ] implies that @xmath93 is only a part of ( or at most equal to ) the information `` flow '' between all the exogenous signals entering the loop outside the link @xmath94 ( namely @xmath91 ) , and @xmath41 . in particular , if @xmath89 were deterministic , then @xmath95 , regardless of the blocks @xmath70 and irrespective of the nature of @xmath27 .    by using  , @xmath96 .",
    "then , applying theorem  [ thm : main ] , we recover  , whenever @xmath97",
    ". thus ,  ( * ? ? ?",
    "* theorem  3 ) and  ( * ? ? ?",
    "* lemma  4.8.1 ) ) can be obtained as a corollary of theorem  [ thm : main ] .",
    "the following result provides an inequality relating @xmath93 with the separate flows of information @xmath98 and @xmath99 .",
    "[ thm : from_splitting_more_precise ] _ for the system shown in fig .",
    "[ fig : diagramas]-(a ) , if @xmath100 and @xmath101 , then @xmath102 with equality if and only if the markov chain @xmath103 holds .",
    "_    theorem  [ thm : from_splitting_more_precise ] shows that , provided @xmath104 , @xmath93 is lower bounded by the sum of the individual flows from all the subsets in any given partition of @xmath105 , to @xmath2 , provided these subsets are mutually independent .",
    "indeed , both theorems  [ thm : main ] and  [ thm : from_splitting_more_precise ] can be generalized for any appropriate choice of external and internal signals .",
    "more precisely , let @xmath106 be the set of all external signals in a feedback system .",
    "let @xmath107 and @xmath108 be two internal signals in the loop .",
    "define @xmath109 as the set of exogenous signals which are introduced to the loop at every subsystem @xmath17 that lies in the path going from @xmath107 to @xmath108 .",
    "thus , for any @xmath110 , if @xmath111 , we have that   and   become @xmath112 respectively .",
    "to finish this section , we present a stronger , non - asymptotic version of inequality  :    [ thm : three_full_loops ] _ in the system shown in fig .",
    "[ fig : diagramas]-(a ) , if @xmath113 are mutually independent , then _",
    "@xmath114    as anticipated , theorem  [ thm : three_full_loops ] can be seen as an extension of   to the more general setup shown in fig .",
    "[ fig : diagramas]-(a ) , where the assumptions made in  ( * ? ? ? * lemma  4.1 ) do not need to hold .",
    "in particular , letting the decoder @xmath30 and @xmath115 in fig .",
    "[ fig : diagramas]-(b ) correspond to @xmath55 and @xmath116 in fig .  [",
    "fig : diagramas]-(a ) , respectively , we see that inequality   holds even if @xmath30 and @xmath29 have dependent initial states , or if the internal state of @xmath30 is not observable  @xcite .",
    "theorem  [ thm : three_full_loops ] also admits an interpretation in terms of information flows . this can be appreciated in the diagram shown in fig .",
    "[ fig : flujos2 ] , which depicts the individual full - turn flows ( around the entire feedback loop ) stemming from @xmath64 , @xmath26 and @xmath117 . theorem  [ thm : three_full_loops ] states that the sum of these individual flows is a lower bound for the directed information from @xmath58 to @xmath41 , provided @xmath118 are independent .",
    "this section presents three closed - loop versions of the data processing inequality _ relating two directed informations _ , both between pairs of signals _ internal _ to the loop . as already mentioned in section  [ sec : intro ] , to the best of our knowledge ,",
    "the first inequality of this type to appear in the literature is the one in theorem  4.1 in  @xcite ( see  ) .",
    "recall that the latter result stated that @xmath119 , requiring @xmath55 to be such that @xmath60 is a deterministic function of @xmath61 and that @xmath59 .",
    "the following result presents another inequality which also relates two nested directed informations , namely , @xmath120 and @xmath121 , but requiring only that @xmath122 .",
    "[ thm : dpi_dir_dir ] _ for the closed - loop system in fig .",
    "[ fig : diagramas]-(b ) , if @xmath123 , then @xmath124 _    notice that theorem  [ thm : dpi_dir_dir ] does not require @xmath117 to be independent of @xmath26 or @xmath64 .",
    "this may seem counter - intuitive upon noting that @xmath117 enters the loop between the link from @xmath21 to @xmath58 .",
    "the following theorem is an identity between two directed informations involving only internal signals .",
    "it can also be seen as a complement to theorem  [ thm : dpi_dir_dir ] , since it can be directly applied to establish the relationship between @xmath125 and @xmath126 .",
    "[ thm : finally ] _ for the system shown in fig .",
    "[ fig : diagramas]-(a ) , if @xmath127 , then @xmath128 with equality if , in addition , @xmath129 . in the latter case , it holds that @xmath130",
    "_    notice that , by requiring additional independence conditions upon the exogenous signals ( specifically , @xmath129 ) , theorem  [ thm : finally ] ( and , in particular ,  ) yields @xmath131 which strengthens the inequality in  ( * ? ? ?",
    "* theorem  4.1 ) ( stated above in  ) .",
    "more precisely ,   does not require conditioning one of the directed informations and holds irrespective of the invertibility of the mappings in the loop .",
    "a closer counterpart of   ( i.e. , of  ( * ? ? ?",
    "* theorem  4.1 ) ) , involving @xmath132 , is presented next .",
    "[ thm : xtoycond ] _ for the system shown in fig .",
    "[ fig : diagramas]-(a ) , if @xmath127 , then @xmath133 _ _ where the equality labeled @xmath134 hods if , in addition , the markov chain @xmath135 is satisfied for all @xmath71 . _",
    "thus , provided @xmath127 ,   yields that   holds regardless of the invertibility of @xmath55 , requiring instead that , for all @xmath71 , any statistical dependence between @xmath136 and @xmath137 resides only in @xmath138 ( i.e. , that markov chain   holds ) .",
    "the results derived so far relate directed informations having either the same `` starting '' sequence or the same `` destination '' sequence .",
    "we finish this section with the following corollary , which follows directly by combining theorems  [ thm : dpi_dir_dir ] and  [ thm : finally ] and relates directed informations involving four different sequences internal to the loop .",
    "[ coro : full_d - dpi ] _ for the system shown in fig .",
    "[ fig : diagramas]-(a ) , if @xmath127 and @xmath129 , then @xmath139 equality holds in @xmath140 if , in addition , @xmath141 ( i.e. , if @xmath142 are mutually independent ) . _    to the best of our knowledge , corollary  [ coro : full_d - dpi ] is the first result available in the literature providing a lower bound to the gap between two nested directed informations , involving four different signals inside the feedback loop .",
    "this result can be seen as the first full extension of the open - loop ( traditional ) data - processing inequality , to arbitrary closed - loop scenarios .",
    "( notice that there is no need to consider systems with more than four mappings , since all external signals entering the loop between a given pair of internal signals can be regarded as exogenous inputs to a single equivalent deterministic mapping . )",
    "we start with the proof of theorem  [ thm : main ] .",
    "it is clear from fig .",
    "[ fig : diagramas]-(a ) and from   that the relationship between @xmath26 , @xmath117 , @xmath64 , @xmath27 , @xmath58 and @xmath41 can be represented by the diagram shown in fig .",
    "[ fig : key ] .    from this diagram and lemma  [ lem : not_so_obvious ] ( in the appendix )",
    "it follows that if @xmath27 is independent of @xmath143 , then the following markov chain holds : @xmath144 denoting the triad of exogenous signals @xmath145 by @xmath146 we have the following    [ eq : lanueva ] @xmath147 \\nonumber\\\\ &   \\overset{(a)}{= } \\sumfromto{i=1}{k } \\left [ i(\\theta^{i};\\rvay(i)|\\rvay^{i-1 } ) - i(\\theta^{i};\\rvay(i)|\\rvax^{i - d_{3}(i)},\\rvay^{i-1 } ) \\right]\\label{eq : solo_directeds } \\\\ &   \\overset{(b)}{\\leq } \\sumfromto{i=1}{k } i(\\theta^{i};\\rvay(i)|\\rvay^{i-1 } ) \\overset{(c)}{\\leq } \\sumfromto{i=1}{k } i(\\theta^{k};\\rvay(i)|\\rvay^{i-1 } ) \\\\&= i(\\theta^{k};\\rvay^{k}).\\end{aligned}\\ ] ]    in the above , @xmath140 follows from the fact that , if @xmath8 is known , then @xmath148 is a deterministic function of @xmath149 .",
    "the resulting sums on the right - hand side of   correspond to @xmath150 , and thereby proving the first part of the theorem , i.e. , the equality in  . in turn",
    ", @xmath151 stems from the non - negativity of mutual informations , turning into equality if @xmath49 , as a direct consequence of the markov chain in  .",
    "finally , equality holds in  @xmath152 if @xmath97 , since @xmath41 depends causally upon @xmath153 .",
    "this shows that equality in   is achieved if @xmath97 , completing the proof .",
    "apply the chain - rule identity   to the rhs of   to obtain @xmath154 now , applying   twice , one can express the term @xmath155 as follows : @xmath156 where the second equality follows since @xmath157 .",
    "the result then follows directly by combining   with   and  .    since @xmath158 , @xmath159 where @xmath140 is due to theorem  [ thm : finally ]",
    ", @xmath151 follows from theorem  [ thm : main ] and the fact that @xmath160 and @xmath152 from the chain rule of mutual information . for the second term on the rhs of the last equation ,",
    "we have @xmath161 where @xmath140 holds since @xmath141 , @xmath151 , @xmath162 and @xmath163 stem from the chain rule of mutual information  , and @xmath152 is a consequence of the markov chain @xmath164 which is due to the fact that @xmath165 .",
    "finally , @xmath166 is due to the markov chain @xmath167 , which holds because @xmath168 as a consequence of lemma  [ lem : not_so_obvious ] in the appendix ( see also fig .  [ fig : diagramas]-(a ) ) .",
    "substitution of   into   yields  , thereby completing the proof .    since @xmath104",
    ", we can apply   ( where now @xmath169 plays the role of @xmath26 ) , and obtain @xmath170 now , we apply theorem  [ thm : main ] , which gives @xmath171 completing the proof .    applying theorem  [ thm : main ] , since @xmath172 , @xmath173 for",
    "the other directed information , we have that @xmath174 where @xmath140 follows from theorem  [ thm : main ] , which also states that equality is reached if and only if @xmath175 . in turn , @xmath151 is due to the fact that @xmath43 is a deterministic function of @xmath176 .",
    "equality @xmath152 holds if and only if @xmath177 . finally ,",
    "from lemma  [ lem : not_so_obvious ] ( in the appendix ) , @xmath162 turns into equality if @xmath178 .",
    "substitution of   into   yields  , completing the proof .",
    "we begin with the second part of the theorem , proving the validity of the equality @xmath134 in  .",
    "we have the following : @xmath179 \\\\ &   \\overset{(a)}{\\leq } \\sumfromto{i=1}{k } i(\\rvar^{i},\\rvap^{i},\\rvax^{i - d_{3}(i)};\\rvay(i)|\\rvay^{i-1},\\rvaq^{i } ) \\\\ &   \\overset{(b)}{= } \\sumfromto{i=1}{k } i(\\rvar^{i},\\rvap^{i};\\rvay(i)|\\rvay^{i-1},\\rvaq^{i } ) \\\\ &   \\overset{\\eqref{eq : chainrule_i}}{= } \\sumfromto{i=1}{k } \\left [ i(\\rvar^{i},\\rvap^{i},\\rvaq_{i+1}^{k};\\rvay(i)|\\rvay^{i-1},\\rvaq^{i } ) - i(\\rvaq_{i+1}^{k};\\rvay(i)|\\rvay^{i-1},\\rvaq^{i},\\rvar^{i},\\rvap^{i } ) \\right ] \\\\ &   \\overset{(c)}{= } \\sumfromto{i=1}{k } \\left [ i(\\rvar^{i},\\rvap^{i},\\rvaq_{i+1}^{k};\\rvay(i)|\\rvay^{i-1},\\rvaq^{i } ) \\right ] \\\\ &   \\overset{\\eqref{eq : chainrule_i}}{= } \\sumfromto{i=1}{k } \\left [ i(\\rvar^{i},\\rvap^{i};\\rvay(i)|\\rvay^{i-1},\\rvaq^{k } ) + i(\\rvaq_{i+1}^{k};\\rvay(i)|\\rvay^{i-1},\\rvaq^{i } ) \\right ] \\\\ &   \\overset{(d)}{= } \\sumfromto{i=1}{k } i(\\rvar^{i},\\rvap^{i};\\rvay(i)|\\rvay^{i-1},\\rvaq^{k } ) \\label{eq : directed_normal_cond } \\\\ &   \\overset{(e)}{\\leq } \\sumfromto{i=1}{k } i(\\rvar^{k},\\rvap^{k};\\rvay(i)|\\rvay^{i-1},\\rvaq^{k } ) = i(\\rvar^{k},\\rvap^{k};\\rvay^{i}|\\rvaq^{k } ) \\label{eq : irprgivenq}\\end{aligned}\\ ] ] where equality holds in @xmath140 if and only if the markov chain @xmath180 holds for all @xmath181 ( as a straightforward extension of lemma  [ lem : not_so_obvious ] ) . in our case ,",
    "the latter markov chain holds since we are assuming @xmath182 . in turn",
    ", @xmath151 stems from the fact that , for all @xmath181 , @xmath148 is a function of @xmath183 . to prove  @xmath152 , we resort to   and write @xmath184 from the definitions of the blocks ( in  ) , it can be seen that , given @xmath138 , the triad of random sequences @xmath185 is a deterministic function of ( at most ) @xmath186 . recalling that @xmath187 and that @xmath188 ( see  ) , it readily follows that @xmath189 , and thus each of the mutual informations on the right - hand - side of   is zero . to verify the validity of @xmath162",
    ", we use   and obtain @xmath190 where @xmath162 now follows since @xmath191 , where the last term in this chain of inequalities was shown to be zero in the proof of @xmath162 .",
    "equality holds in @xmath163 if and only if @xmath192 , a markov chain which is satisfied in our case from the fact that @xmath193 and from lemma  [ lem : not_so_obvious ] .    finally , since @xmath194",
    ", we have that the chain of equalities from   to   holds , from which we conclude that @xmath195 inserting this result into   and invoking theorem  [ thm : main ] we arrive at equality @xmath134 in  .",
    "to prove the first equality the  , it suffices to notice that @xmath196 corresponds to the sum on the right - hand - side of  , from where we proceed as with the first part .",
    "this completes the proof of the theorem .",
    "information inequalities and , in particular , the data - processing inequality , have played a fundamental role in information theory and its applications  @xcite .",
    "it is perhaps the lack of a similar body of results associated with the directed information ( and with non - asymptotic , causal information transmission ) which has limited the extension of many important information - theoretic ideas and insights to situations involving feedback or causality constraints  @xcite .",
    "two such areas , already mentioned in this paper , are the understanding of the fundamental limitations arising in networked control systems over noiseless digital channels , and causal rate distortion problems . in those contexts",
    ", causality is of paramount relevance an thus the directed information appears , naturally , as the appropriate measure of information flow ( see , for example ,  @xcite and  @xcite ) .",
    "we believe that our results might help gaining insights into the fundamental trade - offs underpinning those problems , and might also allow for the solution of open problems such as , for instance , characterizing the minimal average data - rate that guarantees a given performance level  @xcite ( an improved version of the latter paper , which extensively uses the results derived here , is currently under preparation by the authors ) . on a different vein , directed mutual information plays a role akin to that of ( standard ) mutual information when characterizing channel feedback capacity ( see , e.g. ,  @xcite and the references therein ) .",
    "our results may also play a role in expanding the understanding of communication problems over channels used with feedback , particularly when including in the analysis additional exogenous signals such as a random channel state , interference and , in general , any form of side information .",
    "thus , we hope that the inequalities and identities presented in section  [ sec : results ] may help in extending results such as dirty - paper coding  @xcite , watermarking  @xcite , distributed source coding  @xcite , multi - terminal coding  @xcite , and data encryption  @xcite , to scenarios involving causal feedback .",
    "in this paper , we have derived fundamental relations between mutual and directed informations in general discrete - time systems with feedback .",
    "the first of these is an inequality between the directed information between to signals inside the feedback loop and the mutual information involving a subset of all the exogenous incoming signals .",
    "the latter result can be interpreted as a law of conservation of information flows for closed - loop systems .",
    "crucial to establishing these bounds was the repeated use of chain rules for conditional mutual information as well as the development of new markov chains .",
    "the proof techniques do not rely upon properties of entropies or distributions , and the results hold in very general cases including non - linear , time - varying and stochastic systems with arbitrarily distributed signals .",
    "indeed , the only restriction is that all blocks within the system must be causal mappings , and that their combined delay must be at least one sample .",
    "a new generalized data processing inequality was also proved , which is valid for nested directed informations within the loop .",
    "a key insight to be gained from this inequality was that the further apart the signals are in the loop , the lower is the directed information between them .",
    "this closely resembles the behavior of mutual information in open loop systems , where it is well known that any independent processing of the signals can only reduce their mutual information .",
    "[ lem : not_so_obvious ] in the system shown in fig .",
    "[ fig:2systems ] , the exogenous signals @xmath197 are mutually independent and @xmath198 are deterministic ( possibly time - varying ) causal maps characterized by @xmath199 , @xmath200 , @xmath201 , for some @xmath202 .      since @xmath204 and @xmath205 are deterministic functions , it follows that for every possible pair of sequences @xmath206 , the sets @xmath207 and @xmath208 are also deterministic .",
    "thus , @xmath209 and @xmath210 .",
    "this means that for every pair of borel sets @xmath211 of appropriate dimensions , @xmath212 where @xmath140 follows from the fact that @xmath213 . this completes the proof .",
    "m.  s. derpich and j.  stergaard , `` improved upper bounds to the causal quadratic rate - distortion function for gaussian stationary sources , '' _ ieee trans .",
    "inf . theory _ ,",
    "58 , no .  5 , pp .",
    "31313152 , may 2012 .",
    "n.  martins and m.  dahleh , `` feedback control in the presence of noisy channels :  bode - like  fundamental limitations of performance , '' _ ieee trans .",
    "53 , no .  7 , pp . 16041615 , aug .",
    "e.  i. silva , m.  s. derpich , and j.  stergaard , `` a framework for control system design subject to average data - rate constraints , '' _ ieee trans .",
    "control _ ,",
    "56 , no .  8 , pp . 18861899 , june 2011 .",
    " , `` on the minimal average data - rate that guarantees a given closed loop performance level , '' in _ proc .",
    "2nd ifac workshop on distributed estimation and control in networked systems , necsys _ , annecy , france , 2010 , pp .",
    "6772 .",
    "h.  h. permuter , y .- h .",
    "kim , and t.  weissman , `` interpretations of directed information in portfolio theory , data compression , and hypothesis testing , '' _ ieee trans .",
    "inf . theory _ ,",
    "57 , no .",
    "32483259 , june 2011 .",
    "h.  zhang and y .- x .",
    "sun , `` directed information and mutual information in linear feedback tracking systems , '' in _ proc .",
    "6-th world congress on intelligent control and automation _",
    ", june 2006 , pp . 723727 .",
    "j.  zola , m.  aluru , a.  sarje , and s.  aluru , `` parallel information - theory - based construction of genome - wide gene regulatory networks , '' _ ieee transactions on parallel and distributed systems _ , vol .",
    "21 , no .  12 , pp .",
    "17211733 , dec . 2010 .",
    "n.  merhav , `` data - processing inequalities based on a certain structured class of information measures with application to estimation theory , '' _ ieee trans .",
    "inf . theory _ ,",
    "58 , no .",
    "52875301 , aug .",
    "2012 .",
    "r.  zamir , s.  shamai , and u.  erez , `` nested linear / lattice codes for structured multiterminal binning , '' _ ieee trans .",
    "inf . theory _",
    "special a.d .",
    "wyner issue , pp .",
    "1250 - 1276 , june 2002 . , pp .",
    "12501276 , june 2002 ."
  ],
  "abstract_text": [
    "<S> we present several novel identities and inequalities relating the mutual information and the directed information in systems with feedback . </S>",
    "<S> the internal blocks within such systems are restricted only to be causal mappings , but are allowed to be non - linear , stochastic and time varying . moreover , the involved signals can be arbitrarily distributed . </S>",
    "<S> we bound the directed information between signals inside the feedback loop by the mutual information between signals inside and outside the feedback loop . </S>",
    "<S> this fundamental result has an interesting interpretation as a law of conservation of information flow . building upon it </S>",
    "<S> , we derive several novel identities and inequalities , which allow us to prove some existing information inequalities under less restrictive assumptions . </S>",
    "<S> finally , we establish new relationships between nested directed informations inside a feedback loop . </S>",
    "<S> this yields a new and general data - processing inequality for systems with feedback . </S>"
  ]
}