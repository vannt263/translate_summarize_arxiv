{
  "article_text": [
    "a graph is an ordered pair @xmath0 comprising a set @xmath1 of nodes or vertices and a set @xmath2 of edges or links , which are 2-element subsets of @xmath1 .",
    "a graph @xmath3 is _ labeled _ when the vertices are distinguished by names such as @xmath4 with @xmath5 .    graphs @xmath3 and @xmath6 are said to be _ isomorphic _ if there is a bijection between the vertex sets of @xmath3 and @xmath6 , @xmath7 such that any two vertices @xmath8 and @xmath9 are adjacent in @xmath3 if and only if @xmath10 and @xmath11 are adjacent in @xmath6 .",
    "the _ degree _ of a node @xmath12 , denoted by @xmath13 , is the number of ( incoming and outgoing ) links to other nodes .",
    "an e - r graph @xmath14 is a graph of size @xmath15 constructed by connecting nodes randomly with probability @xmath16 independent from every other edge .",
    "usually e - r graphs are assumed to be non - recursive ( i.e. truly random ) , but e - r graphs can be constructed recursively using pseudo - random generating algorithms .",
    "measurement of the information content and randomness of a graph has been largely dominated by graph - theoretic approaches ( e.g. comparisons to randomized graphs  @xcite ) or the application of classical information theory  @xcite .",
    "common approaches include calculating the entropy of the adjacency matrix of a graph or estimating the entropy or entropy rate of its degree sequence .",
    "one of the main objectives behind the application of some measures , such as shannon entropy , is the quantification of the randomness and information content of an object .",
    "here we introduce graphs with interesting deceptive properties , particularly disparate entropy values for the same object when looked at from different perspectives , revealing the inadequacy of classical information - theoretic approaches to graph complexity .",
    "central to information theory is the concept of shannon s information entropy , which quantifies the average number of bits needed to store or communicate the statistical description of an object .    for an ensemble @xmath17 , where @xmath18 is the set of possible outcomes ( the random variable ) , @xmath19 and @xmath20 is the probability of an outcome in @xmath18 .",
    "the shannon entropy of @xmath21 is then given by    @xmath22    which implies that to calculate @xmath23 one has to know or assume the mass distribution probability of ensemble @xmath21 .",
    "one caveat regarding shannon s entropy is that one is forced to make an arbitrary choice regarding granularity .",
    "take for example the bit string 01010101010101 .",
    "the shannon entropy of the string at the level of single bits is maximal , as there are the same number of 1s and 0s , but the string is clearly regular when two - bit blocks are taken as basic units , in which instance the string has minimal complexity because it contains only 1 symbol ( 01 ) from among the 4 possible ones ( 00,01,10,11 ) . one way to overcome",
    "this problem is to take into consideration all possible `` granularities '' or entropy rate :    let @xmath24 with @xmath25 denote the joint probability over blocks of @xmath26 consecutive symbols .",
    "shannon entropy rate _",
    "@xcite ( aka granular entropy , @xmath15-gram entropy ) of a block of @xmath26 consecutive symbols  denoted by @xmath27be : @xmath28    thus to determine the entropy rate of the sequence , we estimate the limit when @xmath29 .",
    "it is not hard to see , however , that @xmath30 will diverge as @xmath26 tends to infinity if the number of symbols increases , but if applied to a binary string @xmath30 , it will reach a minimum for the granularity in which a statistical regularity is revealed .",
    "the shannon entropy  @xcite of an object @xmath31 is simply @xmath30 for fixed block size @xmath32 , so we can drop the subscript .",
    "we can define the shannon entropy of a graph @xmath3 by :    @xmath33    where @xmath34 is a probability distribution of @xmath35 , @xmath35 is a feature of interest @xmath36 of @xmath3 , e.g. edge density , degree sequence , number of over - represented subgraphs / graphlets ( graph motifs ) , and so on . when @xmath34 is the uniform distribution ( every graph of the same size is equally likely ) , it is usually omitted as a parameter of @xmath6 .    as we will see , the same graph that may have a maximal entropy rate for a degree distribution can have an asymptotic zero entropy for an adjacency matrix .",
    "however , unlike graph entropy , the algorithmic complexity of a graph is very robust , in accordance with the invariance theorem  @xcite , though in practice it does have some limitations that have recently been explored , with interesting results  @xcite .    generally , graph entropy is a function of edge density or degree sequence , both graph invariants .",
    "it is a common practice in molecular biology to count the number of ` branchings '  @xcite per node by , for example , randomly traversing a graph starting from a random point .",
    "the more extensive the branching , the greater the uncertainty of a graph s path being traversed in a unique fashion , and the higher the entropy .",
    "thorough surveys of graph entropy are available in  @xcite , so we will avoid providing another one . in most , if not all of these applications of entropy , very little attention is paid to the fact that entropy can lead to completely disparate results depending on the ways in which the same objects of study are described , that is , to the fact that entropy is not invariant vis -  - vis object description , a major drawback for a complexity measure  @xcite . in the survey",
    "@xcite , the author suggests that there may be no ` right ' definition .",
    "here we formally confirm this to be the case .",
    "entropy requires pre - selection of a feature of interest from a probability distribution and can not therefore be considered universal in any fundamental sense for , for example , graph profiling .",
    "this is because ignorance of the probability distribution makes entropy necessarily object - description dependent , there being no such thing as an _ invariance theorem _  @xcite as there is in algorithmic information theory for algorithmic ( kolmogorov - chaitin ) complexity .",
    "algorithmic complexity defines the complexity of an object by the length of its shortest computational description , so that a computer program can fully reconstruct the original object using such a description  @xcite .",
    "one can construct pseudo - random graphs by using integer sequences , in particular borel - normal irrational numbers , to construct networks .    a real number @xmath37 is said to be normal if its @xmath15-tuplet digits are equally likely , thereby of natural maximal @xmath15-order entropy rate by definition of borel normality .",
    "for example , the mathematical constant @xmath38 is believed to be an absolute borel normal number ( borel normal in every base ) , and so one can take the digits of @xmath38 in any base and take @xmath39 digits as the entries for a graph adjacency matrix of size @xmath39 by taking @xmath15 consecutive segments of @xmath15 digits @xmath38",
    ". the resulting graph will have @xmath15 nodes and an edge density 0.5 because the occurrence of 1 or 0 in @xmath38 in binary has the probability 0.5 ( the same as @xmath38 in decimal after transformation of digits to 0 if digit @xmath40 and 1 otherwise , or @xmath41 and 1 otherwise in general for any base @xmath42 ) , thus complying with the definition of an erds - rnyi ( e - r ) graph ( even if of high density ) .    * a * +     +    * b * +     +    * c * +    as theoretically expected and numerically demonstrated in fig .",
    "[ pigraphs](a and b ) , the degree distribution will approximate a normal distribution around @xmath15 .",
    "this means that the graph adjacency matrix will have maximal entropy ( if @xmath38 is borel normal ) but low degree - sequence entropy because all values are around @xmath15 and they do not span all the possible node degrees ( in particular , low degrees ) .",
    "this means that algorithmically constructing a graph can give rise to an object with a different entropy when the feature of interest of the said graph is changed .",
    "a graph does not even have to be of low algorithmic complexity to yield incompatible description - dependent entropy values .",
    "one can take the digits of an @xmath43 chaitin number ( the halting probabilities of optimal turing machines with prefix - free domains ) , some of the digits of which are uncomputable .",
    "but in fig .",
    "[ pigraphs](c ) we show a graph based on the first 64 digits of an @xmath43 chaitin number  @xcite , thus a highest algorithmic complexity graph in the long run ( it is ultimately uncomputable ) .",
    "since randomness implies normality  @xcite , the adjacency matrix has maximal entropy , but for the same reasons as obtain in the case of the @xmath38 graphs , it will have low degree - sequence entropy .",
    "for algorithmic complexity , in contrast , as we will see in theorem  [ algo ] , all graphs have the same algorithmic complexity regardless of their ( lossless ) descriptions ( e.g. adjacency matrix or degree sequence ) , as long as the same and only the same graph ( up to an isomorphism ) can be reconstructed from their descriptions .",
    "one can also start from completely different graphs .",
    "for example , fig .",
    "[ entropy ] shows how shannon entropy is applied directly to the adjacency matrix as a function of edge density , with the same entropy values retrieved despite their very different ( dis)organization",
    ".    the entropy rate will be low for the regular antelope graph , and higher , but still very removed from randomness for the e - r , because by definition the degree- sequence variation of an e - r graph is small .",
    "however , in scale - free graphs degree distribution is artificially scaled , spanning a large number of different degrees as a function of number of connected edges per added node , and resulting in an over - estimation of their degree - sequence entropy , as can be numerically verified in fig .",
    "[ distentropy ] .",
    "degree - sequence entropy points in the opposite direction to the entropic estimation of the same graphs arrived at by looking at their adjacency matrices , when in reality , scale - free networks produced by , e.g. , barabasi - albert s preferential attachment algorithm  @xcite , are recursive ( algorithmic and deterministic , even if probabilities are involved ) , as opposed to the e - r construction built ( pseudo-)randomly .",
    "the entropy of the degree - sequence of scale - free graphs would suggest that they are almost as , or even more random than e - r graphs for exactly the same edge densities . to circumvent this ,",
    "ad - hoc measures of modularity have been introduced  @xcite , to precisely capture how removed a graph is from ` scale - freeness ' by comparing any graph to a scale - free randomized version of itself , and thereby compelling consideration of a pre - selected feature of interest ( ` scale - freeness ' ) .    furthermore , an e - r graph can be recursively ( algorithmically ) generated or not , and so its shannon entropy has no connection to the causal , algorithmic information content of the graph and can only provide clues for low entropy graphs that can be characterized by other graph - theoretic properties , without need of an entropic characterization .     +      we introduce a method to build a family of recursive graphs with maximal entropy but low algorithmic complexity , hence graphs that appear statistically random but are , however , of low algorithmic randomness and thus causally ( recursively ) generated . moreover ,",
    "these graphs may have maximal entropy for some lossless descriptions but minimal entropy for other lossless descriptions of exactly the same objects , with both descriptions characterizing the same object and only that object , thereby demonstrating how entropy fails at unequivocally and unambiguously characterizing a graph independent of a particular feature of interest .",
    "we denote by ` zk ' the graph ( unequivocally ) constructed as follows :    1 .",
    "let @xmath44 be a starting graph @xmath3 connecting a node with label 1 to a node with label 2 .",
    "if a node with label @xmath15 has degree @xmath15 , we call it a _ core node _ , otherwise , we call it a _ supportive node_. 2",
    ".   iteratively add a node @xmath45 to @xmath3 such that the number of core nodes in @xmath3 is maximized .",
    "the resulting graph is typified by the one in figs .",
    "[ zkgraph ] that we denote by zk .    *",
    "a * +   + * b * +      the degree sequence of the labeled nodes @xmath46 is the champernowne constant  @xcite in base 10 , a transcendental real whose decimal expansion is borel normal  @xcite , constructed by concatenating representations of successive integers    @xmath47    whose digits are the labelled node degrees of @xmath3 for @xmath48 iterations ( sequence a033307 in the oeis ) .",
    "the sequence of edges is a recurrence relation built upon values of previous iteration values between core and supportive nodes , defined by :    @xmath49+[2/r]+ \\ldots + [ n / r]\\ ] ]    where @xmath50 is the golden ratio and @xmath51=$ ] the floor function ( sequence a183136 in the oeis ) whose values are 1 , 2 , 4 , 7 , 10 , 14 , 18 , 23 , 29 , 35 , 42 , 50 , 58 , 67 , 76 , 86 , 97 , 108 , 120 , 132 , 145 for @xmath48 .    [ d1 ] @xmath52 is a graph with at least one node with degree @xmath37 where @xmath53 .",
    "@xmath54 has been used where we want to emphasize the number of generation- or time - steps in the process of constructing @xmath52 .",
    "the symbol @xmath55 denotes the maximum degree of the graph .",
    "nodes in the zk graph belong to two types : core and supportive nodes .",
    "[ d2 ] node @xmath37is a core node iff @xmath56 such that @xmath57 .",
    "otherwise it is a supportive node .",
    "[ t1 ] to convert @xmath58 to @xmath59 , we need to add two supportive nodes to @xmath58 if @xmath60 is odd or one supportive node if @xmath60 is even .    _ by induction _ + * the basis * : @xmath61 has 3 core nodes denoted by @xmath62 and two supportive nodes denoted by @xmath63 . as described in the construction procedure , to convert @xmath61 to @xmath64 , we choose a supportive node with maximum degree . here",
    ", since we have only @xmath63 nodes their degree is one .",
    "so we need to connect to 3 other supportive nodes .",
    "as we have only one left , we need to add two supportive nodes .",
    "now , @xmath64 has 3 supportive nodes , two of them are new , @xmath65 , and one old , @xmath63 .",
    "the old one is of degree two , and we need to convert it to 5 ; we have two other supportive nodes left , so we need a new supportive node @xmath65 .",
    "therefore , the assumption is true for @xmath61 and @xmath64 ( the basis ) . + *",
    "inductive step * : now , if we assume that it is true for @xmath66 , then it is true for @xmath52 .",
    "+ we consider two cases :    1 .",
    "@xmath67 is odd 2 .",
    "@xmath67 is even    * case one * if @xmath67 is odd then @xmath68 is even , which means we have added one supportive node with degree one , and to convert @xmath66 to @xmath52 we need to have a core node with degree @xmath15 .",
    "the maximum degree of a supportive node is @xmath69 and we have only one supportive node which is not connected to the core candidate node , which implies that the core candidate node will be @xmath68 , and we would need to add 2 extra supportive nodes to our graph . +",
    "* case two * if @xmath67 is even then @xmath68 is odd , and therefore @xmath66 has two supportive nodes with degree one ( they have only been connected to the last core nodes ) .",
    "so we would need to add only one node to convert the supportive node with maximum degree to a core node with degree @xmath15 .    1 .",
    "if @xmath15 is odd then @xmath70 2 .",
    "@xmath67 is even @xmath71    [ t4 ] @xmath72 there is a maximum of 3 nodes with degree @xmath60 in @xmath52    _ by induction _ + * the basis * : the assumption is true for @xmath61 + * inductive step * : if we assume @xmath73 have @xmath74 there is a maximum of 3 nodes with degree @xmath75 then @xmath72 , then there is a maximum of 3 nodes with degree @xmath76 .",
    "the proof is direct using theorem  [ t1 ] . to generate @xmath52",
    ", we add a maximum of two supportive nodes .",
    "these nodes have degree one and there is no node with degree one except the first core node ( core node with degree 1 ) .",
    "thus we have a maximum of 3 nodes with degree one .",
    "the degree of all other supportive nodes will be increased by one , which , based on the hypothesis of induction , has not been repeated more than 3 times .",
    "[ maxentropy ] zk is of maximal degree - sequence entropy .",
    "the degree sequence of the zk graph can be divided into 2 parts :    1 .",
    "a dominating degree subsequence associated with the core nodes ( always longer than subsequence 2 of supporting nodes ) generated by the infinite series : @xmath77 that produces the champernowne constant @xmath78 , which is borel normal  @xcite .",
    "1 .   a second degree sequence associated with the supportive nodes , whose digits do not repeat more than 3 times , and therefore , by theorem  [ t4 ] , has a maximal @xmath15-order entropy rate for @xmath79 and a high entropy rate for @xmath80 .",
    "therefore , the degree sequence of zk is asymptotically of maximal entropy rate .",
    "[ lowk ] the zk graph is of low algorithmic ( kolmogorov - chaitin ) complexity .    by demonstration .",
    "the computer generated program of the zk graph written in the wolfram language , is :    .... addedges[graph _ ] : =    edgeadd[graph ,     rule@@@distribute[{max[vertexdegree[graph ] ] + 1 ,        table[i , { i , ( max[vertexdegree[graph ] ] +            2 ) , ( max[vertexdegree[graph ] ] +             1 ) + ( max[vertexdegree[graph ] ] + 1 ) -           vertexdegree[graph , max[vertexdegree[graph ] ] + 1 ] } ] } , list ] ] ....    the graph can be constructed recursively for any number of nodes @xmath15 by nesting the addedges [ ] function as follows :    .... nest[addedges , graph[{1 - > 2 } , n ] ....    starting from the graph defined by @xmath81 as initial condition .    the length of nestlist with addedges and the initial condition in bytes is the algorithmic complexity of zk , which grows by only @xmath82 and is therefore of low algorithmic randomness .    we now show that we can fully reconstruct zk from the degree sequence . as we know",
    "that we could also reconstruct zk from its adjacency matrix , we therefore have it that both are lossless descriptions from which zk can be fully reconstructed and for which entropy provides contradictory values depending on the feature of interest .    [ t5 ] @xmath83 , all instances of @xmath52 are isomorphic",
    ".    the only degree of freedom in graph reconstruction is the selection of a _ supportive node _ to convert to a _",
    "core node _ when there are several supportive nodes of maximal degree .",
    "as has been proven in theorem  [ t1 ] , the number of nodes which are added to a graph is independent of the supportive node selected for conversion to a core node .",
    "in any instance of a graph the number of nodes and edges are equal , and it is clear that by mapping the selected node in each step in any instance of a graph to the selected node in the corresponding step in another instance @xmath84 we get @xmath85 , such that @xmath86 is a bijection ( both one - one and superimposed one on the other ) .    and finally , we have to prove that all isomorphic graphs have about the same ( e.g. low ) algorithmic complexity :    [ algo ] let @xmath87 be an _ isomorphic graph _ of @xmath3",
    ". then @xmath88 for all @xmath89 , where @xmath90 is the automorphism group of @xmath3 .",
    "the idea is that if there is a significantly shorter program @xmath91 for generating @xmath3 compared to a program @xmath16 generating @xmath90 , we can use @xmath91 to generate @xmath90 via @xmath3 and a relatively short program @xmath92 that tries , e.g. , all permutations , and checks for isomorphism . let s assume that there exists a program @xmath91 such that @xmath93 , i.e. the difference is not bounded by any constant , and that @xmath94 .",
    "we can replace @xmath16 by @xmath95 to generate @xmath90 such that @xmath96 , where @xmath92 is a constant independent of @xmath87 that represents the size of the shortest program that generates @xmath97 , given any @xmath3 .",
    "then we have it that @xmath98 , which is contrary to the assumption .",
    "the number of borel - normal numbers that can be used as the degree sequence of a graph is determined by the necessary and sufficient conditions in  @xcite and is numerable infinite .",
    "taking advantage of the correlation between 2 variables @xmath99 , @xmath100 ( starting independently ) with the same probability distribution , let @xmath101 be a @xmath102 matrix with rows normalized to 1 . consider the random variables @xmath103 , @xmath104 which satisfy    @xmath105    the correlation between @xmath103 and @xmath104 is just the inner product between the two rows of @xmath101 .",
    "this can be used to generate a degree distribution of a graph with any particular entropy , provided the resulting degree sequence complies or is completed according to the necessary and sufficient conditions for building a graph  @xcite .",
    "while this paper does not focus on alternatives to graph entropy , we point out alternative directions for exploring robust ( if semi - computable ) approaches to graph complexity .",
    "these new developments have been introduced together with actual numerical tools showing that one can not only robustly define the algorithmic complexity of labelled graphs independent of description language , but also of unlabelled graphs , as set forth in  @xcite , in particular :    algorithmic complexity of unlabelled graphs : let @xmath106 be a lossless description of @xmath3 and @xmath90 its automorphism group . then , @xmath107    where @xmath108 is the algorithmic ( kolmogorov - chaitin ) complexity of the graph @xmath3 as introduced in  @xcite ( the shortest computer program that produces @xmath3 upon halting ) and @xmath109 is the set of all @xmath110 descriptions for all graphs in @xmath90 , independent of @xmath110 ( per the invariance theorem ) . which , unlike graph entropy , is robust  @xcite . in  @xcite , it was in fact shown that a labelled graph algorithmic complexity estimation is a good approximation of the algorithmic complexity of the graph automorphism group ( i.e. the unlabelled graph complexity ) , and is correlated in one direction to the automorphism group count .",
    "in contrast to algorithmic complexity , no computable measure of complexity can test for all ( turing ) computable regularities in a dataset  @xcite .",
    "that is , there is no test that can be implemented as a turing machine that takes the data as input and indicates whether the data has a regularity upon halting ( regularities such as `` every 5th place is occupied by a consecutive prime number '' , to mention one example among an infinite number of possibilities ) .",
    "a computable regularity is a regularity for which a test can be set as a computer program running on a specific - purpose turing machine testing for that regularity .",
    "common statistical tests , for example , are computable because they are designed to be effective , but no computable _ universal _ measure of complexity can test for every computable regularity . in other words , for every computable measure capturing a data feature @xmath21 intended to quantify the random content of the data , one can devise a mechanistic procedure producing @xmath21 that deceptively simulates the said measure for all other features .",
    "moreover , for every effective feature , one can devise / conceive an effective measure to test for it , but there is no computable measure able to implement a universal statistical test  @xcite .",
    "this means that for every effective ( computable ) property / feature @xmath21 of a computable object @xmath111 , there is a computable measure @xmath112 to test for @xmath21 in @xmath111 ( or any object like @xmath111 ) , but no computable measure @xmath113 exists to test for every feature @xmath21 in @xmath111 ( and all the effectively enumerable computable objects like @xmath111 ) .",
    "let @xmath110 be a lossless description of an object @xmath3 .",
    "then @xmath110 can be used to reconstruct @xmath3 , and in fact there is no essential distinction between @xmath110 and @xmath3 , merely a conventional one .",
    "we have shown that , unlike @xmath114 and its invariance theorem :    for a computable measure @xmath6 , such as shannon entropy , there is no constant @xmath92 or logarithmic term such that @xmath115 or @xmath116 as a function of the size of @xmath3 bounding the difference .    in other words , the shannon entropy @xmath6 values for",
    "the same object diverge for different object lossless descriptions and is therefore not a robust measure of complexity .",
    "the methods introduced here allow the construction of ` borel - normal pseudo - random graphs ' , uncomputable number - based graphs and algorithmically produced graphs , illustrating the shortcomings of graph - theoretic and entropy approaches to graph complexity beyond random feature selection , and their deficiency when it comes to profiling graph randomness .",
    "we have shown that entropy fails to characterize absolute randomness , citing specific complexity - deceiving graphs for which entropy retrieves disparate values when the same object is described differently , even if the descriptions allow the reconstruction of exactly the same object and therefore essentially are the same object up to a simple algorithmic translation / transformation .",
    "this drawback of shannon entropy is all the more serious because it is usually overlooked , unlike the traditional challenge of dependence on probability distribution .",
    "this means that one needs to choose a description of interest to apply a definition of entropy , such as the adjacency matrix of a network or its degree sequence , but as soon as the choice is made , entropy becomes a trivial function only characterizing the description of interest and nothing else . in the case of , for example , the adjacency matrix of a network , entropy becomes a function of edge density , while for degree sequence entropy becomes a function of sequence normality .",
    "entropy can thus trivially be replaced by such functions without any loss .",
    "new developments , however , promise better routes to universal measures of ( graph ) complexity more independent of object description and therefore better equipped to profile general properties that may have been or not previously identified  @xcite .    the methods introduced here may also help to build more robust models of randomized graphs for use in comparing properties of graphs that may or may not have a property of interest against an agnostic model with stable measures of complexity .",
    "zenil , h. , soler - toscano , f. , kiani , n.a .",
    ", hernndez - orozco and s. , rueda - toicen , a. , a decomposition method for global evaluation of shannon entropy and local estimations of algorithmic complexity , submitted .",
    "zenil , h. , soler - toscano , f. , dingle , k. , and louis , a. , correlation of automorphism group size and topological properties with program - size complexity evaluations of graphs and complex networks .",
    "physica a : statistical mechanics and its applications , vol .",
    "404 , pp . 341-358 , 2014 .",
    "zenil , h. , soler - toscano , f. , delahaye , j.p . and gauvrit , n. , two - dimensional kolmogorov complexity and an empirical validation of the coding theorem method by compressibility .",
    "peerj computer science , 1 , p.e23 , 2015 ."
  ],
  "abstract_text": [
    "<S> a common practice in the estimation of the complexity of objects , in particular of graphs , is to rely on graph- and information - theoretic measures . here , </S>",
    "<S> using integer sequences with properties such as borel normality , we explain how these measures are not independent of the way in which a single object , such a graph , can be described . from descriptions that can reconstruct the same graph and are therefore essentially translations of the same description </S>",
    "<S> , we will see that not only is it necessary to pre - select a feature of interest where there is one when applying a computable measure such as shannon entropy , and to make an arbitrary selection where there is not , but that more general properties , such as the causal likeliness of a graph as a measure ( opposed to randomness ) , can be largely misrepresented by computable measures such as entropy and entropy rate . </S>",
    "<S> we introduce recursive and non - recursive ( uncomputable ) graphs and graph constructions based on integer sequences , whose different lossless descriptions have disparate entropy values , thereby enabling the study and exploration of a measure s range of applications and demonstrating the weaknesses of computable measures of complexity .    algorithmic complexity , graph complexity , shannon entropy , kolmogorov - chaitin complexity , borel normality , algorithmic randomness </S>"
  ]
}