{
  "article_text": [
    "we consider principal component analysis ( pca ) in gaussian graphical models .",
    "pca is a classical dimensionality reduction method which is frequently used in statistics and machine learning @xcite .",
    "the first principal components of a multivariate are its orthogonal linear combinations which preserve most of the variance . in the gaussian case",
    ", pca has special properties which make it especially favorable : it is the best linear approximation of the data and it provides independent components . on the other hand , gaussian graphical models , also known as covariance selection models , provide a graphical representation of the conditional independence structure within the gaussian distribution @xcite . exploiting the extensive knowledge and literature on graph theory , graphical models allow for efficient distributed implementation of statistical inference algorithms , e.g. , the well known belief propagation method and the junction tree algorithm @xcite . in particular , decomposable graphs , also known as chordal or triangulated graphs ,",
    "provide simple and intuitive inference methods due to their appealing structure .",
    "our main contribution is the application of decomposable graphical models to pca which we nickname dpca , where d denotes both _",
    "decomposable _ and _",
    "distributed_.    the main motivation for distributed pca is decentralized dimensionality reduction .",
    "it plays a leading role in distributed estimation and compression theory in wireless sensor networks @xcite , and decentralized data mining techniques @xcite .",
    "it is also used in anomaly detection in computer networks @xcite . in particular",
    ", @xcite proposed to approximate the global pca using a sequence of conditional local pca solutions .",
    "alternatively , an approximate solution which allows a tradeoff between performance and communication requirements was proposed in @xcite using eigenvalue perturbation theory .",
    "dpca is an efficient implementation of distributed pca based on a prior graphical model . unlike the above references it does not try to approximate pca , but yields an exact solution up to on any given tolerance .",
    "on the other hand , it assumes additional prior knowledge in the form of a graphical model which previous works did not take into account .",
    "although , it is interesting to note that the gauss markov source example in @xcite is probably the most celebrated decomposable graphical model .",
    "therefore , we now address the availability of such prior information .",
    "in general , practical applications do not necessarily satisfy any obvious conditional independence structure . in such scenarios , dpca can be interpreted as an approximate pca method that allows a tradeoff between accuracy and decentralization by introducing sparsity . in other problems",
    "it is reasonable to assume that an unknown structure exists and can be learned from the observed data using existing methods such as @xcite .",
    "alternatively , a graphical model can be derived from non - statistical prior knowledge on the specific application .",
    "an intuitive example is distributed networks in which the topology of the network suggests a statistical graph as exploited in @xcite .",
    "finally , we emphasize that even if a prior graphical model is available , it does not necessarily satisfy a decomposable form . in this case",
    ", a decomposable approximation can be obtained using classical graph theory algorithms @xcite .",
    "pca can be interpreted as maximum likelihood ( ml ) estimation of the covariance using the available data followed by its eigenvalue decomposition .",
    "when a prior graphical model is available , pca can still be easily obtained by adjusting the ml estimation phase to incorporate the prior conditional independence structure using existing methods @xcite , and then computing the eigenvalue decomposition ( evd ) .",
    "the drawback to this approach is that it does not exploit the structure of the graph in the evd phase .",
    "this disadvantage is the primary motivation to dpca which is specifically designed to utilize the structure of gaussian graphical models .",
    "decomposable covariance selection models result in sparse concentration ( inverse covariance ) matrices which can be estimated in a decentralized manner .",
    "therefore , we propose to reformulate dpca in the concentration domain and solve the global evd using a sequence of local evd problems in each of the cliques of the decomposable graph with a small amount of message passing .",
    "this allows for distributed implementation according to the topology of the graph and reduces the need to collect all the observed data in a centralized processing unit .",
    "when the algorithm terminates , each clique obtains its own local version of the principal components .    to illustrate dpca we apply it to distributed anomaly detection in computer networks @xcite . in this context",
    ", dpca learns a low dimensional model of the normal traffic behavior and allows for simple outlier detection .",
    "this application is natural since the network s topology provides a physical basis for constructing an approximate a graphical model .",
    "for example , consider two nodes which are geographically distant and linked only through a long path of nodes .",
    "it is reasonable to believe that these two sensors are independent conditioned on the path , but a theoretical justification of this assertion is difficult and depends on the specific problem formulation .",
    "we examine the validity of this claim in the context of anomaly detection in the abilene network using a real - world dataset .",
    "we propose an approximate decomposition of the abilene network , enable the use of dpca and obtain a fully distributed anomaly detection method .",
    "the outline of the paper is as follows .",
    "decomposable graphs are easy to explain using a special graph of two cliques which is their main building block .",
    "therefore , we begin in section [ sec_2dpca ] by introducing the problem formulation and solution to dpca in this simple case .",
    "the generalization to decomposable graphs is presented in section [ sec_dpca ] which consists of their technical definitions followed by a recursive application of the two cliques solution .",
    "we demonstrate the use of dpca using two numerical examples .",
    "first , in section [ sec_num ] we simulate our proposed algorithm in a synthetic tracking scenario .",
    "second , in section [ sec_abilene ] we illustrate its application to anomaly detection using a real - world dataset from the abilene backbone network .",
    "finally , in section [ sec_conc ] we provide concluding remarks and address future work .",
    "the following notation is used .",
    "boldface upper case letters denote matrices , boldface lower case letters denote column vectors , and standard lower case letters denote scalars .",
    "the superscripts @xmath0 and @xmath1 denote the transpose and matrix inverse , respectively .",
    "the cardinality of a set @xmath2 is denoted by @xmath3 .",
    "the matrix @xmath4 denotes the identity , @xmath5 is the minimum eigenvalue of square symmetric matrix @xmath6 , @xmath7 is a null vector of @xmath6 , @xmath8 is the maximum eigenvalue of @xmath6 , and @xmath9 means that @xmath6 is positive definite . finally , we use indices in the subscript @xmath10}_a$ ] or @xmath11}_{a , b}$ ] to denote sub - vectors or sub - matrices , respectively , and @xmath11}_{a,:}$ ] denotes the sub - matrix formed by the @xmath2th rows in @xmath6 .",
    "where possible , we omit the brackets and use @xmath12 or @xmath13 instead .",
    "in this section , we introduce dpca for a simple case which will be the building block for the general algorithm .     and @xmath14 are conditionally independent given @xmath15 .",
    "[ fig_twoclique],scaledwidth=40.0% ]      let @xmath16}^t$ ] be a length @xmath17 , zero mean gaussian random vector in which @xmath10}_a$ ] and @xmath10}_b$ ] are independent conditionally on @xmath10}_c$ ] where @xmath2 , @xmath15 and @xmath14 are disjoint subsets of indices . for later use , we use graph terminology and define two cliques of indices @xmath18 and @xmath19 coupled through the separator @xmath20 ( see fig . [ fig_twoclique ] ) .",
    "we assume that the covariance matrix of @xmath21 is unknown , but the conditional independence structure ( defined through index sets @xmath22 and @xmath23 ) is known .",
    "the input to dpca is a set of @xmath24 independent and identically distributed realizations of @xmath21 denoted by @xmath25 for @xmath26 .",
    "more specifically , this input is distributed in the sense that the first clique has access to @xmath27}_{c_1}$ ] for @xmath26 , whereas the second clique has access only to @xmath27}_{c_2}$ ] for @xmath26 . using this data and minimal message passing between the two cliques , dpca searches for the linear combination @xmath28 having maximal variance .",
    "when the algorithm terminates , each of the cliques obtains its own local version of @xmath29 , i.e. , the sub - vectors @xmath30}_{c_1}$ ] and @xmath30}_{c_2}$ ] .",
    "the following subsections present the proposed solution to dpca .",
    "it involves two main stages : covariance estimation and principal components computation .",
    "first , the covariance matrix of @xmath21 is estimated using the maximum likelihood ( ml ) technique .",
    "due to the known conditional independence structure , the ml estimate has a simple closed form solution which can be computed in a distributed manner ( more details about this procedure can be found in @xcite ) . each clique and",
    "the separator computes their own local sample covariance matrices @xmath31}_{c_1}{\\left[}{\\mathbf{x}}_i{\\right]}_{c_1}^t\\\\",
    "\\tilde{\\mathbf{s}}^{c_2,c_2}&=&\\frac{1}{n}\\sum_{i=1}^n{\\left[}{\\mathbf{x}}_i{\\right]}_{c_2}{\\left[}{\\mathbf{x}}_i{\\right]}_{c_2}^t\\\\    \\tilde{\\mathbf{s}}^{s , s}&=&\\frac{1}{n}\\sum_{i=1}^n{\\left[}{\\mathbf{x}}_i{\\right]}_{s}{\\left[}{\\mathbf{x}}_i{\\right]}_{s}^t,\\end{aligned}\\ ] ] where the tilde and the superscripts are used to emphasize that these are local estimates .",
    "similarly , the local concentration matrices , also known as precision matrices , are defined as @xmath32 where it is assumed that the matrices are non - singular ( otherwise , the ml estimate does not exist ) .",
    "next , the global ml concentration matrix @xmath33 is obtained by requiring @xmath34 due to the conditional independence of @xmath12 and @xmath35 given @xmath36 .",
    "the global solution is @xmath37}+{\\left[}\\begin{array}{cc }           { \\mathbf{0 } } & \\begin{array}{cc }                 { \\mathbf{0 } } & { \\mathbf{0}}\\end{array }            \\\\",
    "\\begin{array}{c }             { \\mathbf{0}}\\\\             { \\mathbf{0}}\\end{array }            & \\tilde{\\mathbf{k}}^{c_2,c_2 }         \\end{array }    { \\right]}-{\\left[}\\begin{array}{ccc }           { \\mathbf{0 } } & { \\mathbf{0 } } & { \\mathbf{0}}\\\\           { \\mathbf{0 } } & \\tilde{\\mathbf{k}}^{s , s } & { \\mathbf{0}}\\\\           { \\mathbf{0 } } & { \\mathbf{0 } } & { \\mathbf{0}}\\end{array }    { \\right]}.\\end{aligned}\\ ] ] it is easy to see that the sub - matrices associated with the cliques are perturbations of to their local versions : @xmath38}_{c_1,c_1}&=&\\tilde{\\mathbf{k}}^{c_1,c_1}+{\\left[}\\begin{array}{cc }                                                 { \\mathbf{0 } } & { \\mathbf{0}}\\\\                                                 { \\mathbf{0 } } & { \\mathbf{m}}_{b }                                               \\end{array }    { \\right]}\\\\ \\label{ksubblocks2 }   { \\left[}{\\mathbf{k}}{\\right]}_{c_2,c_2}&=&\\tilde{\\mathbf{k}}^{c_2,c_2}+{\\left[}\\begin{array}{cc }                                                 { \\mathbf{m}}_{a } & { \\mathbf{0}}\\\\                                                 { \\mathbf{0 } } & { \\mathbf{0}}\\end{array}{\\right]}\\end{aligned}\\ ] ] and require only message passing via @xmath39 and @xmath40 : @xmath41}_{s , s}-\\tilde{\\mathbf{k}}^{s , s}\\\\    { \\mathbf{m}}_{a}&=&{\\left[}\\tilde{\\mathbf{k}}^{c_1,c_1}{\\right]}_{s , s}-\\tilde{\\mathbf{k}}^{s , s}.\\end{aligned}\\ ] ] the dimension of these messages is equal to @xmath42 which is presumably small .",
    "thus , the global ml concentration matrix can be easily found in a distributed manner .",
    "the global covariance estimate is simply defined as the inverse of its concentration @xmath43 .",
    "it is consistent with the local estimates of its sub - matrices : @xmath44}_{c_1,c_1}&=&\\tilde{\\mathbf{s}}^{c_1,c_2}\\\\    { \\left[}{\\mathbf{s}}{\\right]}_{c_2,c_2}&=&\\tilde{\\mathbf{s}}^{c_2,c_2},\\end{aligned}\\ ] ] but there is no special intuition regarding its @xmath45}_{a , b}$ ] and @xmath45}_{b , a}$ ] sub - blocks .",
    "given the global ml covariance estimate @xmath46 , the pca objective function is estimated as @xmath47 which is maximized subject to a norm constraint to yield @xmath48 this optimization gives both the maximal eigenvalue of @xmath46 and the its eigenvector @xmath29 .",
    "the drawback to the above solution is that the evd computation requires centralized processing and does not exploit the structure of @xmath33 .",
    "each clique needs to send its local covariance to a central processing unit which constructs @xmath46 and computes its maximal eigenvalue and eigenvector .",
    "we will now provide an alternative distributed dpca algorithm in which each clique uses only local information along with minimal message passing in order to calculate its local version of @xmath49 and @xmath29 .",
    "our first observation is that dpca can be equivalently solved in the concentration domain instead of the covariance domain .",
    "indeed , it is well known that @xmath50 when the inverse @xmath51 exists .",
    "the corresponding eigenvectors are also identical .",
    "the advantage of working with @xmath33 instead of @xmath46 is that we can directly exploit @xmath33 s sparsity as expressed in ( [ kab ] ) .    before continuing",
    "it is important to address the question of singularity of @xmath46 .",
    "one may claim that working in the concentration domain is problematic since @xmath46 may be singular .",
    "this is indeed true but is not a critical disadvantage since graphical models allow for well conditioned estimates under small sample sizes .",
    "for example , classical ml exists only if @xmath52 , whereas the ml described above requires the less stringent condition @xmath53 @xcite .",
    "in fact , the ml covariance is defined as the inverse of its concentration , and thus the issue of singularity is an intrinsic problem of ml estimation rather than the dpca solution .",
    "we now return to the problem of finding @xmath54 in a distributed manner .",
    "we begin by expressing @xmath55 as a trivial line - search problem : @xmath56 and note that the objective is linear and the constraint set is convex .",
    "it can be solved using any standard line - search algorithm , e.g. bisection . at first",
    ", this representation seems useless as we still need to evaluate @xmath57 which was our original goal .",
    "however , the following proposition shows that checking the feasibility of a given @xmath58 can be done in a distributed manner .",
    "let @xmath33 be a symmetric matrix with @xmath59 .",
    "then , the constraint @xmath60}{\\right)}\\end{aligned}\\ ] ] is equivalent to the following pair of constraints @xmath61}{\\right)}\\end{aligned}\\ ] ] with the _ message matrix _ defined as @xmath62    the proof is obtained by rewriting ( [ eigasopt1 ] ) as a linear matrix inequality @xmath63}-t{\\mathbf{i}}\\succ{\\mathbf{0}}\\end{aligned}\\ ] ] and decoupling this inequality using the following lemma :    [ schur ] let @xmath6 be a symmetric matrix partitioned as @xmath64}.\\end{aligned}\\ ] ] then , @xmath9 if and only if @xmath65 and @xmath66 .    applying schur s lemma to ( [ eig2lmi ] ) with @xmath67 and rearranging yields @xmath68}.\\end{aligned}\\ ] ] finally , ( [ prop2 ] ) and ( [ prop1 ] ) are obtained by rewriting ( [ tikbb ] ) and ( [ tikac ] ) as eigenvalue inequalities , respectively .",
    "proposition 1 provides an intuitive distributed solution to ( [ eigasopt ] ) . for",
    "any given @xmath58 we can check the feasibility by solving local eigenvalue problems and message passing via @xmath69 whose dimension is equal to the cardinality of the separator .",
    "the optimal global eigenvalue is then defined as the maximal globally feasible @xmath58 .",
    "we note that the solution in proposition 1 is asymmetric with respect to the cliques .",
    "the global constraint is replaced by two local constraints regarding clique @xmath18 and the remainder @xmath70 .",
    "alternatively , we can exchange the order and partition the indices into @xmath71 and @xmath19 .",
    "this asymmetry will become important in the next section when we extend the results to general decomposable graphs .",
    "after we obtain the minimal eigenvalue @xmath55 , we can easily recover its corresponding eigenvector @xmath29 .",
    "for this purpose , we define @xmath72 and obtain @xmath73 .",
    "the matrix @xmath74 follows the same block sparse structure as @xmath33 , and the linear set of equations @xmath75 can be solved in a distributed manner .",
    "there are two possible solutions .",
    "usually , @xmath76 is non - singular in which case the solution is @xmath77}_{c_1}&=&{\\mathbf{u}}_{\\rm{null}}{\\left(}{\\mathbf{q}}_{c_1,c_1}-{\\left[}\\begin{array}{cc }                                                 { \\mathbf{0 } } & { \\mathbf{0}}\\\\                                                 { \\mathbf{0 } } & { \\mathbf{m}}\\end{array }    { \\right]}{\\right)}\\\\ \\label{ub }   { \\left[}{\\mathbf{u}}{\\right]}_b&=&-{\\mathbf{q}}_{b , b}^{-1}{\\mathbf{q}}_{b , c}{\\left[}{\\mathbf{u}}{\\right]}_c,\\end{aligned}\\ ] ] where the _ message _ @xmath78 is defined as @xmath79 otherwise , if @xmath80 is singular then the solution is simply @xmath81}_{c_1}&=&{\\mathbf{0}}\\\\    { \\left[}{\\mathbf{u}}{\\right]}_b&=&{\\mathbf{u}}_{\\rm{null}}{\\left(}{\\mathbf{k}}_{b , b}{\\right)}.\\end{aligned}\\ ] ] this singular case is highly unlikely as the probability of ( [ ac0 ] ) in continuous models is zero .",
    "however , it should be checked for completeness .      in practice ,",
    "dimensionality reduction involves the projection of the data into the subspace of a few of the first principal components .",
    "we now show that the algorithm in [ sec_eigval ] can be extended to provide higher order components .",
    "the @xmath82th principal component is defined as the linear transformation which is orthogonal to the preceding components and preserves maximal variance .",
    "similarly to the first component it is given by @xmath83 where @xmath84 is the @xmath82th principal eigenvector of @xmath46 . in the concentration domain , @xmath84 is the eigenvector associated with @xmath85 , the @xmath82th smallest eigenvalue of @xmath33 .    in order to distribute the computation of @xmath85 ,",
    "we adjust ( [ eigasopt ] ) using the following lemma :    let @xmath33 be a symmetric matrix with eigenvalues @xmath86 and eigenvectors @xmath87 . then , @xmath88 the optimal @xmath89 are any values which satisfy @xmath90 for @xmath91 .    the proof is based on the recursive variational characterization of of the @xmath82th smallest eigenvalue : @xmath92 where @xmath93 are the preceding eigenvectors , and the optimal solution @xmath94 is the eigenvector associated with @xmath85 .",
    "a dual representation can be obtained using lagrange duality .",
    "we rewrite the orthogonality restrictions as quadratic constraints @xmath95 and eliminate them using lagrange multipliers : @xmath96}{\\mathbf{u}}\\end{aligned}\\ ] ] where the inequality is due to the weak duality @xcite .",
    "the inner minimization is unbounded unless @xmath97 therefore , @xmath98 lagrange duality does not guaranty an equality in ( [ duallambdak ] ) since ( [ dualuk ] ) is not convex .",
    "however , it is easy to see that the inequality is tight and can be attained by choosing @xmath99 .",
    "finally , ( [ suptv ] ) is obtained by replacing the maximum with a supremum and relaxing the constraint .",
    "lemma 2 allows us to find @xmath85 in a distributed manner .",
    "we replace @xmath33 with @xmath100 where @xmath101 is a @xmath102 matrix with the preceding eigenvectors as its columns and @xmath103 is a @xmath104 diagonal matrix with sufficiently high constants on its diagonal , and search for its principal component . the matrix @xmath105 does not necessarily satisfy the sparse block structure of @xmath33 so we can not use the solution in proposition 1 directly .",
    "fortunately , it can be easily adjusted since the modification to @xmath33 is of low rank .",
    "let @xmath33 be a symmetric matrix with @xmath59 .",
    "then , the constraint @xmath106}+{\\mathbf{u}}{\\mathbf{d}}{\\mathbf{u}}^t{\\right)}\\end{aligned}\\ ] ] is equivalent to the following pair of constraints @xmath107}_{b,:}{\\mathbf{d}}{\\left[}{\\mathbf{u}}{\\right]}_{b,:}^t{\\right)}\\\\ \\label{prop1u }     t&<&{\\rm{eig}}_{\\min}{\\left(}{\\mathbf{k}}_{c_1,c_1}+{\\left[}\\overline{\\mathbf{u}}{\\right]}_{c_1,:}\\overline{\\mathbf{d}}{\\left[}\\overline{\\mathbf{u}}{\\right]}_{c_1,:}^t{\\right)}\\end{aligned}\\ ] ] where @xmath108}_{c_1,:}&=&{\\left[}\\begin{array}{cc }             \\begin{array}{c }               { \\mathbf{0}}\\\\               { \\mathbf{i}}\\end{array }              & { \\left[}{\\mathbf{u}}{\\right]}_{c_1 } \\\\           \\end{array }      { \\right]}\\\\      \\overline{{\\mathbf{d}}}&=&{\\left[}\\begin{array}{cc }                        { \\mathbf{0 } } & { \\mathbf{0}}\\\\                        { \\mathbf{0 } } & { \\mathbf{d}}\\end{array }      { \\right]}-{\\mathbf{m}}_{\\mathbf{u}}{\\left(}t{\\right)}\\end{aligned}\\ ] ] and the _ message matrix _ @xmath109 is defined as @xmath110}_{b,:}^t                               \\end{array }      { \\right]}{\\left(}{\\mathbf{k}}_{b , b}+{\\left[}{\\mathbf{u}}{\\right]}_{b,:}{\\mathbf{d}}{\\left[}{\\mathbf{u}}{\\right]}_{b,:}^t - t{\\mathbf{i}}{\\right)}^{-1}{\\left[}\\begin{array}{cc }                                 { \\mathbf{k}}_{b , c } & { \\left[}{\\mathbf{u}}{\\right]}_{b,:}{\\mathbf{d}}\\end{array }      { \\right]}.    \\end{aligned}\\ ] ]    the proof is similar to that of proposition 1 and therefore omitted .",
    "thus , the solution to the @xmath82th largest eigenvalue is similar to the method in section [ sec_eigval ] .",
    "the only difference is that the messages are slightly larger .",
    "each message is a matrix of size @xmath111 . in practice ,",
    "dimensionality reduction involves only a few principal components and this method is efficient when @xmath112 is considerably less than @xmath17 ( the size of the messages in a fully centralized protocol ) .",
    "the higher order components can therefore be found in a distributed manner as detailed in section [ sec_eigvec ] above .",
    "we now proceed to the general problem of dpca in decomposable graphs . in the previous section",
    ", we showed that dpca can be computed in a distributed manner if it is a priori known that @xmath12 and @xmath35 are conditionally independent given @xmath36 .",
    "graphical models are intuitive characterizations of such conditional independence structures . in particular , decomposable models are graphs that can be recursively subdivided into the two cliques graph in fig .",
    "1 . therefore , this section consists of numerous technical definitions taken from @xcite followed by a recursive application of the previous results .",
    "an undirected graph @xmath113 is a set of nodes connected by undirected edges .",
    "a random vector @xmath21 satisfies the markov property with respect to @xmath113 , if for any pair of non - adjacent nodes the corresponding pair of random variables are conditionally independent on the rest of the elements in @xmath21 . in the gaussian distribution ,",
    "this definition results in sparsity in the concentration domain . if @xmath33 is the concentration matrix of a jointly gaussian multivariate @xmath21 that satisfies @xmath113 , then @xmath114}_{i , j}=0 $ ] for any pair @xmath115 of non - adjacent nodes .",
    "decomposable graphs are a specific type of graph which possess an appealing structure .",
    "a graph is decomposable if it can be recursively be subdivided into disjoint sets of nodes @xmath2 , @xmath14 and @xmath15 , where @xmath15 separates @xmath2 and @xmath14 , and @xmath15 is complete , i.e. , there are no edges between @xmath2 and @xmath14 and all the nodes within @xmath15 are connected by an edge .",
    "clearly , the simplest non - trivial decomposable graph is the two cliques graph in fig .",
    "[ fig_twoclique ] .",
    "a clique is a maximal subset of nodes which is fully connected .",
    "it is convenient to represent a decomposable graph using a sequence of cliques @xmath116 which satisfy a _",
    "perfect elimination order_. an important property of this order is that @xmath117 separates @xmath118 from @xmath119 where @xmath120 note that this perfect elimination order induces an inherent asymmetry between the cliques which will be used in our recursive solution below .",
    "the two cliques graph in fig .",
    "[ fig_twoclique ] is a simple example of a decomposable graph with @xmath18 , @xmath19 , @xmath121 , @xmath122 , @xmath123 and @xmath124 .",
    "accordingly , @xmath121 separates @xmath125 from @xmath126 .",
    "similarly to the previous section , global ml estimation of the concentration matrix in decomposable gaussian graphical model has a simple closed form .",
    "it can be computed in a distributed manner : @xmath127}^0-\\sum_{k=2}^k{\\left[}\\tilde{\\mathbf{k}}^{s_k , s_k}{\\right]}^0\\end{aligned}\\ ] ] where the local estimates are defined as : @xmath128 and @xmath129}_{c_k}{\\left[}{\\mathbf{x}}_i{\\right]}_{c_k}^t,\\quad k=1,\\cdots , k\\\\ \\label{decomp_ml4 }   \\tilde{\\mathbf{s}}^{s_k , s_k}=\\frac{1}{n}\\sum_{i=1}^n{\\left[}{\\mathbf{x}}_i{\\right]}_{s_k}{\\left[}{\\mathbf{x}}_i{\\right]}_{s_k}^t,\\quad k=2,\\cdots , k.\\end{aligned}\\ ] ] the zero fill - in operator @xmath130}^0 $ ] in ( [ decomp_ml ] ) outputs a matrix of the same dimension as @xmath33 where the argument occupies the appropriate sub - block and the rest of the matrix has zero valued elements ( see ( [ kabc ] ) for a two clique example , and @xcite for the exact definition of this operator ) .",
    "dpca can be recursively implemented by using the previous two clique solution .",
    "indeed , proposition 1 shows that the eigenvalue inequality @xmath131 is equivalent to two adjusted local eigenvalue inequalities @xmath132 where @xmath133}^0.\\end{aligned}\\ ] ] where @xmath134 is a message as in ( [ message ] ) and @xmath130}^0 $ ] is the zero fill - in operator .",
    "next , we can apply schur s lemma again and replace ( [ eigminhk1 ] ) with two additional inequalities : @xmath135 where @xmath136 and @xmath137 are similarly defined .",
    "we continue in an iterative fashion until we obtain @xmath138 decoupled eigenvalue inequalities .",
    "thus , the feasibility of a given @xmath58 can be checked in a distributed manner with minimal message passing between the cliques , and any line - search can efficiently solve dpca .",
    "specifically , in algorithm 1 displayed below we provide a pseudo code for dpca that solves for @xmath58 using the bisection method .",
    "given initial bounds @xmath139 algorithm 1 is guaranteed to find the minimal eigenvalue up to any required tolerance @xmath140 within @xmath141 iterations .",
    "each iteration consists of up to @xmath142 messages through the matrices @xmath134 whose dimensions are equal to the cardinalities of @xmath143 for @xmath144 .",
    "a simple choice for the bounds is @xmath145 since @xmath33 is positive definite , and @xmath146 as proved in the appendix .",
    "given a principal eigenvalue @xmath55 , its corresponding eigenvector can be computed by solving @xmath75 where @xmath72 as detailed in section [ sec_eigvec ] . beginning with @xmath147 we partition @xmath148 into @xmath149 and @xmath150 and test the singularity of @xmath151 .",
    "if it is singular , then @xmath55 is associated with @xmath149 .",
    "otherwise , we send the message @xmath152 to @xmath153 and repartition it .",
    "we continue until we find the associated remainder @xmath149 or reach the first clique .",
    "then , we compute the corresponding local null vector and begin propagating it to the higher remainders as expressed in ( [ ub ] ) .",
    "a pseudo code of this method is provided in algorithm 2 below .",
    "@xmath154 @xmath155 @xmath147 @xmath156    algorithm 1 can be easily extended to compute higher order eigenvalues through application of proposition 2 . for this purpose ,",
    "note that the inequality in ( [ prop1u ] ) has the same structure as ( [ eigasopt2 ] ) and therefore can be recursively partitioned again .",
    "the only difference is that the rank of the modification is increased at each clique and requires larger message matrices .",
    "thus , the algorithm is efficient as long as the size of the separators ( @xmath157 ) , the number of cliques ( @xmath138 ) and the number of required eigenvalues ( @xmath82 ) are all relatively small in comparison to @xmath17 . given any eigenvalue ( first or high order ) , algorithm 2 finds the associated eigenvector in a distributed and efficient manner .",
    "we now illustrate the performance of dpca using a synthetic numerical example .",
    "specifically , we use dpca to track the first principle component in a slowly time varying setting .",
    "we define a simple graphical model with @xmath158 nodes representing three fully connected networks with only 5 coupling nodes , i.e. , @xmath159 , @xmath160 , and @xmath161 .",
    "we generate @xmath162 length @xmath163 vectors @xmath25 of zero mean , unit variance and independent gaussian random variables . at each time point",
    ", we define @xmath33 through ( [ decomp_ml])-([decomp_ml4 ] ) using a sliding window of @xmath164 realizations with @xmath165 samples overlap .",
    "next , we run dpca using algorithm 1 .",
    "due to slow time variation , we define the lower ( @xmath166 ) and upper ( @xmath167 ) bounds as the value of the previous time point minus and plus 0.1 , respectively .",
    "we define the tolerance as @xmath168 corresponding to 8 iterations .",
    "figure [ timevarying ] shows the exact value of the minimal eigenvalue as a function of time along with its dpca estimates at the @xmath169th , @xmath170th and @xmath171th iterations .",
    "it is easy to see that a few iterations suffice for tracking the maximal eigenvalue at high accuracy .",
    "each iteration involves three evds of approximately @xmath172 matrices and communication through two messages of size @xmath173 . for comparison , a centralized solution would require sending a set of @xmath174 length @xmath158 vectors to a central processing unit which computes an evd of a matrix of size @xmath175 .    , scaledwidth=40.0% ]",
    "a promising application for dpca is distributed anomaly detection in computer networks . in this context , pca is used for learning a low dimensional model for normal behavior of the traffic in the network .",
    "the samples are projected into the subspace associated with the first principal components .",
    "anomalies are then easily detected by examining the residual norm .",
    "our hypothesis is that the connectivity map of the network is related to its statistical graphical model .",
    "the intuition is that two distant links in the network are ( approximately ) independent conditioned on the links connecting them and therefore define a graphical model .",
    "we do not rigorously support this claim but rather apply it in a heuristic manner in order to illustrate dpca .",
    "following @xcite , we consider a real world dataset of abilene , the internet2 backbone network .",
    "this network carries traffic from universities in the united states .",
    "figure [ abilene_map ] shows its connectivity map consisting of 11 routers and 41 links ( each edge corresponds to two links and there are additional links from each of the nodes to itself ) . examining the network",
    "it is easy to see that the links on the east and west sides of the map are separated through six coupling links : dnvr - kscy , snva - kscy and losa - hstn .",
    "thus , our first approximated decomposable graph , denoted by @xmath176 , consists of two cliques : an eastern clique and a western clique coupled by these six links .",
    "graph @xmath176 corresponds to a decomposable concentration matrix with a sparsity level of @xmath177 .",
    "our second decomposable graph denoted by @xmath178 is obtained by redividing the eastern clique again into two cliques separated through the four coupling links : ipls - chin and atla - wash .",
    "its corresponding concentration matrix has a sparsity level of @xmath179 .",
    "finally , for comparison we randomly generate an arbitrary graph @xmath180 over the abilene nodes , with an identical structure as @xmath178 ( three cliques of the same cardinalities ) , which is not associated with the topology of the abilene network .    in our experiments ,",
    "we learn the @xmath181 covariance matrix from a @xmath182 data matrix representing 1008 samples of the load on each of the 41 abilene links during april 7 - 13 , 2003 . we compute pca and project each of the @xmath183 samples of dimension @xmath184 into the null space of the first four principal components .",
    "the norm of these residual samples is plotted in the top plot of fig .",
    "[ abilene_fig1 ] .",
    "it is easy to see the spikes putatively associated with anomalies .",
    "next , we examine the residuals using dpca with @xmath176 , @xmath178 and @xmath180 .",
    "the norms of the residuals are plotted in the three lower plots of fig .",
    "[ abilene_fig1 ] .",
    ", respectively .",
    "as expected , the topology based plots are quite similar with spikes occurring at the times of these anomalies .",
    "thus , we conclude that the decomposable graphical model for abilene is a good approximation and does not cause substantial loss of information ( at least for the purpose of anomaly detection ) . on the other hand , the residual norm using the random graph is a poor approximation as it does not preserve the anomalies detected by the full non - distributed pca .",
    "these conclusions are supported in fig .",
    "[ abilene_fig2 ] where we show the absolute errors of dpca with respect to pca using the different graphical models .",
    "it is easy to see that @xmath176 results in minimal error , @xmath178 provides a reasonable tradeoff between performance and computational complexity ( through its increased sparsity level ) , while graph @xmath180 is clearly a mismatched graphical model and results in significant increase in error .",
    ", scaledwidth=40.0% ]    , scaledwidth=40.0% ]    , scaledwidth=40.0% ]",
    "in this paper , we introduced dpca and derived a decentralized method for its computation .",
    "we proposed distributed anomaly detection in communication networks as a motivating application for dpca and investigated possible graphical models for such settings .",
    "future work should examine the statistical properties of dpca . from a statistical perspective",
    ", dpca is an extension of classical pca to incorporate additional prior information .",
    "thus , it would be interesting to analyze the distribution of its components and quantify their significance , both under the true graphical model and under mismatched models .",
    "in addition , dpca is based on the intimate relation between the inverse covariance and the conditional gaussian distribution .",
    "therefore , it is also important to assess its sensitivity to non - gaussian sources .",
    "finally , alternative methods to ml in singular and ill conditioned scenarios should be considered .",
    "the authors would like to thank clayton scott for providing the abilene data , and arnau tibau puig for stimulating discussions and his help with the abilene dataset .    in this appendix , we prove that the minimal eigenvalue of a @xmath185 symmetric matrix @xmath33 is less than or equal to the minimal eigenvalue of any of its sub - matrices , say @xmath186 for some set of indices @xmath2 . for simplicity",
    ", we assume that @xmath187 for some integer @xmath188 . the proof is a simple application of the rayleigh quotient characterization of the minimal eigenvalues : @xmath189}{\\mathbf{k}}{\\left[}\\begin{array}{c }            { \\mathbf{v}}\\\\            { \\mathbf{0}}\\end{array }    { \\right]}}{{\\left[}\\begin{array}{cc }                    { \\mathbf{v}}^t & { \\mathbf{0}}^t                  \\end{array }    { \\right]}{\\left[}\\begin{array}{c }            { \\mathbf{v}}\\\\            { \\mathbf{0}}\\end{array }    { \\right]}}\\\\    & = & \\frac{{\\mathbf{v}}^t{\\mathbf{k}}_{a , a}{\\mathbf{v}}}{{\\mathbf{v}}^t{\\mathbf{v}}}\\\\ \\label{mineigv }   & = & \\min_{\\mathbf{u}}\\frac{{\\mathbf{u}}^t{\\mathbf{k}}_{a , a}{\\mathbf{u}}}{{\\mathbf{u}}^t{\\mathbf{u}}}\\\\    & = & { \\rm{eig}}_{\\min}\\{{\\mathbf{k}}_{a , a}\\}\\end{aligned}\\ ] ] where @xmath190 is the optimal solution to ( [ mineigv ] ) .",
    "y.  qu , g.  ostrouchovz , n.  samatovaz , and a.  geist . principal component analysis for dimensions reduction in massive distributed data sets . in _ ieee international conference on data mining ( icdm )"
  ],
  "abstract_text": [
    "<S> we consider principal component analysis ( pca ) in decomposable gaussian graphical models . we exploit the prior information in these models in order to distribute its computation . for this purpose , </S>",
    "<S> we reformulate the problem in the sparse inverse covariance ( concentration ) domain and solve the global eigenvalue problem using a sequence of local eigenvalue problems in each of the cliques of the decomposable graph . </S>",
    "<S> we demonstrate the application of our methodology in the context of decentralized anomaly detection in the abilene backbone network . </S>",
    "<S> based on the topology of the network , we propose an approximate statistical graphical model and distribute the computation of pca . </S>"
  ]
}