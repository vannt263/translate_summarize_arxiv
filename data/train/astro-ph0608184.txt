{
  "article_text": [
    "cosmologists are becoming very good at determining the parameters of the universe . within the last few years",
    "observational results , exemplified by the microwave background anisotropy measurements by the wilkinson microwave anisotropy probe ( wmap ) , have introduced precision into cosmological modelling .",
    "considerable sophistication is now required both in deriving theoretical predictions from models and in carrying out data analysis procedures able to squeeze the best from the data .    within a cosmological model ,",
    "the parameters indicate the importance of different effects .",
    "for instance , they describe the relative amounts of different types of material in the universe , the geometry and expansion rate of the universe , and the properties of the initial irregularities in the universe which led to the formation of structure .",
    "such parameters are not predicted by fundamental theories , but rather must be fit from data in order to decide which combination , if any , is capable of describing our universe .",
    "a variety of cosmological data are currently well fit by a model of the universe that is homogeneous , isotropic and spatially flat , contains cold dark matter in greater proportions than ordinary baryonic matter , and in which tiny initial perturbations evolved under einstein s theory of general relativity into the structures of today .",
    "current data constrain the parameters of this model rather well , many at the 10% level or better .",
    "however , the presence of good data leads to a different problem , one of knowing when to stop fitting .",
    "two different and competing models of the universe may explain the data equally well , so how do we choose between them ?",
    "the solution is one proposed by william of occam , that the simpler model should be preferred .",
    "this is known as _ occam s",
    "razor_. so a complicated model that explains the data slightly better than a simple one should be penalized for the extra parameters it introduces , because the extra parameters bring with them a lack of predictability . on the other hand , if a model is too simple , and can not fit certain data well , then it can be discarded .",
    "this is a rather common type of statistical problem , both in cosmology and in other fields of astrophysics : each available parameter within a model describes some piece of physics that might be relevant to our universe , but until measurements are made we do nt know which .",
    "cosmological _ model selection _ refers to comparing different model descriptions of the data .",
    "it does nt care particularly about the actual values of parameters , but rather aims to determine which _ set _ of parameters gives the preferred fit to observational data .",
    "model selection is an extremely widespread challenge throughout science ; how do you fit to data when you are unsure about the set of parameters that you should be deploying .",
    "you can not just include every parameter you can think of in a fit to data , because inclusion of extra unnecessary parameters worsens the determination of those that are essential , so that very soon you can end up learning nothing about anything .",
    "moreover , you ca nt just use goodness of fit to the data , because typically inclusion of a new parameter will improve the goodness of fit even if that parameter has absolutely no actual relevance to the universe . typical attempts to avoid these problems involve _ ad hoc _ criteria such as ` chi - squared per degrees of freedom ' arguments or the ` likelihood ratio test ' , in which arbitrary thresholds have to be invoked to decide which way the verdict is supposed to go .",
    "model selection aims to put this practice on a firmer footing .",
    "model selection problems are ones in which the parameter set necessary to describe a given dataset is unknown , the question typically being whether new data justifies inclusion of a new physical parameter . many of the most pressing questions in astrophysics are of this form .",
    "cosmological examples would include whether the spatial curvature is non - zero , whether the dark energy density evolves , and whether the initial perturbation spectrum has an amplitude which varies with length scale .",
    "the generic purpose of a model selection statistic is to set up a tension between the predictiveness of a model ( for instance indicated by the number of free parameters ) and its ability to fit observational data .",
    "oversimplistic models offering a poor fit should of course be thrown out , but so should more complex models which offer poor predictive power .",
    "there are two main types of model selection statistic that have been used in the literature so far .",
    "* information criteria * look at the best - fitting parameter values and attach a penalty for the number of parameters ; they are essentially a technical formulation of ` chi - squared per degrees of freedom ' arguments . by contrast , the * bayesian evidence * applies the same type of likelihood analysis familiar from parameter estimation , but at the level of models rather than parameters .",
    "it depends on goodness of fit across the entire model parameter space .    here",
    "we discuss three possible statistics . in each case",
    ", the statistic is a single number that is a property of the model , and having computed it the models can be placed in a rank - ordered list .",
    "akaike information criterion ( aic ) : : :    this was derived by hirotugu akaike in 1974 , and takes the form    @xmath0 where    @xmath1 is the likelihood ( @xmath2    is often called @xmath3 though it generalizes it to    non - gaussian distributions ) and @xmath4 is the number of    parameters in the model .",
    "the subscript ` max ' indicates that one should    find the parameter values yielding the highest possible likelihood    within the model .",
    "it is obvious that this second term acts as a kind    of ` occam factor ' ; initially as parameters are added the fit to data    improves rapidly until a reasonable fit is achieved , but further    parameters then add little and the penalty term @xmath5 takes    over .",
    "the generic shape of the aic as a function of number of    parameters is therefore a rapid fall , a minimum , and then a rise .",
    "the    preferred model sits at the minimum .",
    "+    the aic was derived from information - theoretic considerations ,    specifically an approximate minimization of the kullback ",
    "leibler    information entropy which measures the distance between two    probability distributions .",
    "bayesian information criterion ( bic ) : : :    this was derived by gideon schwarz in 1978 , and strongly resembles the    aic .",
    "it is given by    @xmath6    where @xmath7 is the number of datapoints . since",
    "a typical    dataset will have @xmath8 , the bic imposes a stricter    penalty against extra parameters than the aic .",
    "+    it was derived as an approximation to the bayesian evidence , to be    discussed next , but the assumptions required are very restrictive and    unlikely to hold in practice , rendering the approximation quite crude .",
    "bayesian evidence : : :    the bayesian evidence looks rather different , being defined as    @xmath9    here @xmath10 is the vector of parameters of the model ,    and @xmath11 is the prior distribution of those parameters _ before _    the data were obtained .",
    "the prior is an essential part of the    definition of a model , upon which the evidence will ultimately depend ,    and might for instance be a set of ranges within which parameters are    assumed to be uniformly distributed .",
    "+    the evidence of a model is thus the average likelihood of the model in    the prior . unlike the statistics above",
    ", it does not focus on the    best - fitting parameters of the model , but rather asks `` of all the    parameter values you thought were viable before the data came along ,    how well on average did they fit the data ? '' . literally , it is the    likelihood of the model given the data . given bayes theorem    @xmath12 (",
    "here m is the    model , d is the data , and the vertical bar is read as ` given ' ) , the    evidence updates the prior model probability @xmath13 to the    posterior model probability @xmath14 , i.e.  the probability    of the model given the data .",
    "+    the evidence rewards predictability of models , provided they give a    good fit to the data , and hence gives an axiomatic realization of    occam s razor .",
    "a model with little parameter freedom is likely to fit    data over much of its parameter space , whereas a model which could    match pretty much any data that might have cropped up will give a    better fit to the actual data but only in a small region of its larger    parameter space , pulling the average likelihood down .",
    "+    the evidence is also known as the marginalized likelihood or , more    accurately , the model likelihood . the ratio of evidences for two    models is known as the bayes factor .    of these statistics , we would advocate using , wherever possible , the bayesian evidence which is a full implementation of bayesian inference and can be directly interpreted in terms of model probabilities .",
    "it is computationally challenging to compute , being a highly - peaked multi - dimensional integral , but recent algorithm development has made it feasible in cosmological contexts .",
    "we discuss it further in the next section .",
    "if the bayesian evidence can not be computed , the bic can be deployed as a substitute .",
    "it is much simpler to compute as one need only find the point of maximum likelihood for each model .",
    "however interpreting it can be difficult .",
    "its main usefulness is as an approximation to the evidence , but this holds only for gaussian likelihoods and provided the datapoints are independent and identically distributed .",
    "the latter condition holds poorly for the current global cosmological dataset , though it can potentially be improved by binning of the data hence decreasing the @xmath7 in the penalty term .",
    "the aic has been widely used outside astrophysics , but is of debatable utility . sometime after it was first derived , it was shown to be ` dimensionally inconsistent ' , a statistical term meaning that it is not guaranteed to give the right result even in the limit of infinite unbiased data .",
    "it may however be useful for checking the robustness of conclusions drawn using the bic .",
    "the evidence and bic are dimensionally consistent .",
    "computing the evidence in realistic problems is challenging , particularly in cosmology where evaluating theoretical predictions at just a single parameter point requires several seconds of cpu time with state - of - the - art codes such as ` cmbfast ` or ` camb ` .",
    "markov chain monte carlo ( mcmc ) methods are now commonplace in cosmological parameter estimation , and efficiently trace the posterior probability distribution of the parameters of a model in the vicinity of the best - fit region .",
    "however a different sampling strategy is needed to evaluate the evidence .",
    "it can receive a large contribution from the tails of the posterior distribution of the parameters , because even though the likelihoods there are small , this region occupies a large volume of the prior probability space . therefore the sampling strategy must effectively sample the entire prior volume to evaluate the integral ( eqn 3 ) accurately . until recently , the best available strategy for evidence calculation , known as thermodynamic integration or simulated annealing , required around @xmath15 likelihood evaluations for an accurate answer for a five - parameter cosmological model , placing the problem at the limit of current supercomputer power .",
    "fortunately , a powerful new algorithm for evidence evaluation , known as * nested sampling * , was recently invented by john skilling ( 2004 ) . at sussex",
    "we have implemented this algorithm for cosmology in a code named ` cosmonest ` , which we recently made publically available .",
    "it has proven to be one to two orders of magnitude more efficient than thermodynamic integration , meaning that evidence calculations can now be run on a small computing cluster .    to set up the algorithm , the evidence integral",
    "is first recast as a one - dimensional integral in terms of the prior mass @xmath16 , where @xmath17 with @xmath16 running from 0 to 1 .",
    "[ a mental image to accompany this is to consider the prior parameter space as a cube , and to smash it with a large hammer .",
    "the fragments are then arranged in a line in order of increasing likelihood . ]",
    "the algorithm samples the prior a large number of times , assigning a ` prior mass ' to each sample .",
    "the samples are ordered by likelihood , and the integration follows as the sum of the sequence , @xmath18 this is shown in figure  [ fig : nested ] .    in order to compute the integral accurately the prior mass is logarithmically sampled .",
    "we start by randomly placing a set of @xmath7 points within the prior parameter space , where in a typical cosmological application @xmath19 .",
    "we then iteratively discard the lowest likelihood point @xmath20 , replacing it with a new point uniformly sampled from the remaining prior mass ( i.e.  with likelihood greater than @xmath20 ) .",
    "each time a point is discarded the prior mass remaining , @xmath21 , shrinks by a factor that is known probabilistically , and the evidence is incremented accordingly . in this way the algorithm works its way towards the higher likelihood regions .",
    "the process is illustrated in figure [ fig : timeseries ] .",
    "additional details of the algorithm are in mukherjee , parkinson & liddle ( 2006a ) .",
    "the algorithm is simple , works accurately even in high dimensions , and should be generally applicable in a number of areas even outside of astrophysics .",
    "although the evidence gives a rank - ordered list of models , it is still necessary to decide how big a difference in evidence is needed to be significant .",
    "if the prior probabilities of the models are assumed equal , the difference in log(evidence ) can be directly interpreted as the relative probabilities of the models after the data . even if people disagree on the relative prior probabilities",
    ", they will all agree on the direction in which the data , represented by the evidence , has shifted the balance .",
    "the usual interpretational scale employed is due to sir harold jeffreys ( from his classic 1961 book ` theory of probability ' ) , which , given a difference @xmath22 between the evidences @xmath23 of two models , states that    [ cols=\"^ , < \" , ]     in practice we find the divisions at 2.5 ( corresponding to posterior odds of about 13:1 ) and 5 ( corresponding to posterior odds of about 150:1 ) the most useful .    when should model selection be deployed ?",
    "if the data indicates something strongly enough , it does nt really matter how the statistical analysis is done .",
    "the main zone of interest is where a new parameter is ` detected ' at between two and four ` sigmas ' via parameter estimation techniques .",
    "these overestimate the significance of a detection because they ignore model dimensionality , and there is a well - known ( in the statistics literature anyway ) phenomenon called lindley s paradox , whereby model selection considerations can overturn an analysis based on a ` number of sigmas ' argument . a nice discussion of lindley s paradox is given in trotta ( 2005 ) .",
    "one of the bugbears of bayesian methods is the requirement to specify priors explicitly , with the evidence depending on the choice of priors .",
    "if the data have low informative content ( technically defined via the ratio of prior and posterior parameter volumes ) , this can be a serious issue , but it becomes less so if the data are constraining so that the posterior is well localized within any conceivable prior . in that case",
    "the evidence becomes proportional to the prior volume , and quite a substantial change in volume is needed to move models significantly around the jeffreys scale .",
    "there are several areas of application of model selection techniques , the main two being as follows :    application to data : : :    with real data , one can assess the viability of different models under    consideration .",
    "in this case one simply computes the evidence for each    model of interest and ranks them .",
    "model selection forecasting : : :    this application aims to compare the power of different experiments    before they are carried out .",
    "many proposed experiments seek to answer    model selection questions , but their capabilities are often quantified    using parameter estimation projections , such as fisher matrix    forecasting . for instance , a dark energy experiment may be advertized    as able to measure the equation of state parameter @xmath24    with an uncertainty of @xmath25 , the aim being to detect    deviations of @xmath24 from @xmath26 , which    characterizes the cosmological constant or vacuum zero - point energy .",
    "one can instead forecast experiments ability to carry out model    selection tests . in this case data",
    "must be simulated for a range of    different assumed models , in order to investigate where in the    available parameter space a given experiment can make a strong or    decisive model comparison between a dynamical dark energy model and    the cosmological constant .",
    "this gives a powerful tool for comparing    the statistical power of competing experiments .",
    "it should also be    possible to extend this concept to survey optimization , whereby one    tunes survey parameters to optimize the ability to carry out model    selection tests , but it is less clear that this will be fruitful .",
    "we have extensively discussed the philosophy of model selection forecasting , with specific application to dark energy experiments , in mukherjee et al .",
    "( 2006b ) . in pahud",
    "et al .  (",
    "2006 ) we applied these ideas to determination of the nature of the primordial power spectrum of density perturbations , focussing on the ability of the planck satellite mission to perform model selection of this type . in this article",
    "we will focus on applications to real data .      to help understand what is going on",
    ", we can carry out a simple toy model investigation into the spatial curvature of the universe . according to the three - year data from wmap ( henceforth wmap3 ) ,",
    "the total density , in units of the critical density , is @xmath27 ( where we took the liberty of symmetrizing the uncertainty and where the hubble key project determination of the hubble parameter is also used ) .",
    "given this , how likely is it that the universe is flat ?",
    "for simplicity we ll assume a gaussian likelihood corresponding to this measurement , and ignore dependence on other parameters , so we have @xmath28\\ ] ] we also have to choose a prior range for @xmath29 ; let s say @xmath30 representing some plausible range people might have considered long before precision data emerged . now",
    "the calculation , remembering that the evidence is just the average likelihood over the prior .",
    "flat model : : :    we just have to evaluate the likelihood at @xmath31 .",
    "it is @xmath32 .",
    "curved model : : :    now we have to integrate the likelihood over the prior , being sure to    normalize the prior properly .",
    "this gives    @xmath33 .",
    "the conclusion is that , under these assumptions , the flat model is preferred at odds of approximately 50:1 .    that example was pretty boring , since @xmath34 lies almost in the middle of the measured range . but",
    "suppose the result had been @xmath35 , a putative three - sigma detection of spatial curvature .",
    "the evidence for the curved model is unchanged ( it does nt care what the measured value is provided it is well within the prior ) while that of the flat model shrinks .",
    "nevertheless , the end result is an odds ratio of only 2:1 in favour of the curved model . in this case",
    ", three - sigma is nowhere near enough to convincingly indicate that space is curved .",
    "physically , the evidence is allowing for it being _ a priori _ very unlikely that @xmath29 could be so close to one as to give such a low - confidence ` detection ' , yet still not be equal to one .",
    "put another way , the @xmath22 would be 0.7 which according to jeffreys is hardly worth mentioning .",
    "now we turn to a real cosmological example .",
    "the new three - year data from wmap ( see figures  [ fig : ilc ] and [ fig : spectrum ] ) is for the most part uncontroversial from a model selection point of view , with parameters either being definitely required or clearly unnecessary .",
    "the exception is the scale dependence of primordial density perturbations , defined by the spectral index @xmath36 .",
    "these perturbations are usually considered to have been generated by inflation , a period of rapid acceleration in the early universe . as well as solving some of the problems with the traditional hot big bang model , inflation",
    "also generically predicts the kind of observations that we now see .",
    "the many models of inflation predict a wide range of possible values for @xmath36 , which one should then try and fit from the data .    however , a decade before inflation was invented , harrison and zeldovich independently proposed that @xmath36 should be precisely one , corresponding to perturbations whose amplitude is independent of scale .",
    "at least until this year s publication of the three - year wmap data , the harrison ",
    "zeldovich spectrum always gave a good fit to existing data . from a model selection perspective it benefits from having one parameter less than a model where @xmath37 varies , and indeed we showed in a paper last year predating wmap3 that the harrison ",
    "zeldovich model had the highest evidence , though other models including varying @xmath36 were not strongly excluded .",
    "wmap3 gave , for the first time , indications that @xmath36 might be less than one , with their main paper quoting the results ( from wmap3 data alone ) @xmath38 , which thus appears to be over 3-sigma away from unity .",
    "a similar result is found when wmap data is combined with other independent datasets , such as the power spectrum of the large - scale distribution of galaxies and the redshift ",
    "luminosity relation of distant type ia supernovae .",
    "so far , parameter estimation analyses performed on available data taken together seem to indicate that @xmath39 at about 3 to 4-sigma .",
    "this significance level is exactly where lindley s paradox is at its strongest , making the use of model selection techniques imperative , as acknowledged in the wmap3 papers .",
    "we have carried out such an analysis .",
    "we chose a prior on @xmath36 uniform between 0.8 between 1.2 ; most inflationary models give @xmath36 in this range and this is what was believed to be the possible range for it before the data came along .",
    "evidences were computed using ` cosmonest ` , with the calculations taking a few days on a multi - processor cluster .    according to our model selection analysis ,",
    "the evidence for the @xmath36 varying model is significant , but not strong or decisive .",
    "wmap3 data on its own gives a bayes factor of only @xmath40 , indicating that this data alone is unable to distinguish the two models .",
    "when wmap3 data are used together with external data sets we estimate a @xmath22 of @xmath41 , corresponding to an odds ratio of 8 to 1 in favour of the @xmath36 varying model . adding the external datasets improves the constraining power on @xmath36 , as they significantly extend the scales over which the primordial power spectrum affects the data .",
    "nevertheless , the support for varying @xmath36 is clearly tentative rather than compelling .",
    "there is additional reason for some caution at present because there may be residual systematics in the data that could affect our conclusions regarding @xmath36 ; the evidence calculation concerns statistical uncertainties only .",
    "for example , the effect of varying @xmath36 in determining the power spectra shown in figure  [ fig : spectrum ] is somewhat degenerate with the signature of the relatively recent reionization of the universe , which is mainly inferred from polarization data which is difficult to handle .",
    "there are also uncertainties associated with the modelling of the instrument beam profiles , and in whether one should attempt to model out a possible contribution to the cmb anisotropies from the sunyaev ",
    "zeldovich effect .",
    "the situation will be improved with higher signal - to - noise data from additional years of wmap observations and future experiments .",
    "many of the most interesting cosmological questions are ones of model selection , not parameter estimation . with the growing precision of cosmological data , it is imperative to deploy proper model selection techniques to extract the best robust conclusions from data .",
    "application to the post - wmap3 cosmological data compilation continues to indicate that the data can be well fit by quite minimal cosmological models .",
    "five fundamental parameters are definitely required , and wmap3 has provided suggestive indications that a sixth , the density perturbation spectral index , needs to be added to the set . according to the bayesian evidence , however , the case for inclusion of the spectral index has yet to become compelling .",
    "as the data improve in sensitivity we expect new model selection based questions to be both raised and answered in the next decade",
    ". these may be about the nature of dark energy , the model for reionization , the nature of inflation , the case for primordial gravitational waves , or the nature of cosmic topology .",
    "model selection , of course , will have further applications in astrophysics and beyond .",
    "the authors were supported by pparc .",
    "we thank pier stefano corasaniti , mike hobson , andrew jaffe , martin kunz , cdric pahud , john peacock , john skilling , and roberto trotta for discussion relating to these ideas .    `",
    "cosmonest ` is available for download at http://www.cosmonest.org .",
    "* jeffreys h * 1961 , _ theory of probability _ , 3rd edition [ oup , oxford ] ."
  ],
  "abstract_text": [
    "<S> model selection aims to determine which theoretical models are most plausible given some data , without necessarily asking about the preferred values of the model parameters . </S>",
    "<S> a common model selection question is to ask when new data require introduction of an additional parameter , describing a newly - discovered physical effect . </S>",
    "<S> we review several model selection statistics , and then focus on use of the bayesian evidence , which implements the usual bayesian analysis framework at the level of models rather than parameters . </S>",
    "<S> we describe our _ cosmonest _ code , which is the first computationally - efficient implementation of bayesian model selection in a cosmological context . </S>",
    "<S> we apply it to recent wmap satellite data , examining the need for a perturbation spectral index differing from the scale - invariant ( harrison  zeldovich ) case </S>",
    "<S> .    cosmological model selection     + astronomy centre , university of sussex , brighton bn1 9qh </S>"
  ]
}