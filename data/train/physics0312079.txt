{
  "article_text": [
    "we take the view that the most effective form of inference is provided by the observed likelihood function along with the associated @xmath0-value function . in the case of a scalar parameter",
    "the likelihood function is simply proportional to the density function .",
    "the @xmath0-value function can be obtained exactly if there is a one - dimensional statistic that measures the parameter . if not",
    ", the @xmath0-value can be obtained to a high order of approximation using recently developed methods of likelihood asymptotics . in the presence of nuisance parameters , the likelihood function for a ( one - dimensional ) parameter of interest",
    "is obtained via an adjustment to the profile likelihood function .",
    "the @xmath0-value function is obtained from quantities computed from the likelihood function using a canonical parametrization @xmath1 , which is computed locally at the data point .",
    "this generalizes the method of eliminating nuisance parameters by conditioning or marginalizing to more general contexts . in section 2",
    "we give some background notation and introduce the notion of orthogonal parameters . in section 3",
    "we illustrate the @xmath0-value function approach in a simple model with no nuisance parameters .",
    "profile likelihood and adjustments to profile likelihood are described in section 4 .",
    "third order @xmath0-values for problems with nuisance parameters are described in section 5 .",
    "section 6 describes the classical conditional and marginal likelihood approach .",
    "we assume our measurement(s ) @xmath2 can be modelled as coming from a probability distribution with density or mass function @xmath3 , where @xmath4 takes values in @xmath5 .",
    "we assume @xmath6 is a one - dimensional parameter of interest , and @xmath7 is a vector of nuisance parameters .",
    "if there is interest in more than one component of @xmath8 , the methods described here can be applied to each component of interest in turn .",
    "the likelihood function is @xmath9 it is defined only up to arbitrary multiples which may depend on @xmath2 but not on @xmath8 .",
    "this ensures in particular that the likelihood function is invariant to one - to - one transformations of the measurement(s ) @xmath2 . in the context of independent ,",
    "identically distributed sampling , where @xmath10 and each @xmath11 follows the model @xmath3 the likelihood function is proportional to @xmath12 and the log - likelihood function becomes a sum of independent and identically distributed components : @xmath13 the maximum likelihood estimate @xmath14 is the value of @xmath8 at which the likelihood takes its maximum , and in regular models is defined by the score equation @xmath15 the observed fisher information function @xmath16 is the curvature of the log - likelihood : @xmath17 and the expected fisher information is the model quantity @xmath18 if @xmath2 is a sample of size @xmath19 then @xmath20 .    in accord with the partitioning of @xmath8 we partition",
    "the observed and expected information matrices and use the notation @xmath21 and @xmath22 we say @xmath6 is _ orthogonal _ to @xmath7 ( with respect to expected fisher information ) if @xmath23 .",
    "when @xmath6 is scalar a transformation from @xmath24 to @xmath25 such that @xmath6 is orthogonal to @xmath26 can always be found ( cox and reid , [ 1 ] ) .",
    "the most directly interpreted consequence of parameter orthogonality is that the maximum likelihood estimates of orthogonal components are asymptotically independent .",
    "* example 1 : ratio of poisson means * suppose @xmath27 and @xmath28 are independent counts modelled as poisson with mean @xmath7 and @xmath29 , respectively",
    ". then the likelihood function is @xmath30 and @xmath6 is orthogonal to @xmath31 .",
    "in fact in this example the likelihood function factors as @xmath32 , which is a stronger property than parameter orthogonality . the first factor is the likelihood for a binomial distribution with index @xmath33 and probability of success @xmath34 , and the second is that for a poisson distribution with mean @xmath26 .",
    "* example 2 : exponential regression * suppose @xmath35 are independent observations , each from an exponential distribution with mean @xmath36 , where @xmath37 is known .",
    "the log - likelihood function is @xmath38 and @xmath23 if and only if @xmath39 . the stronger property of factorization of the likelihood does not hold .",
    "we assume now that @xmath8 is one - dimensional .",
    "a plot of the log - likelihood function as a function of @xmath8 can quickly reveal irregularities in the model , such as a non - unique maximum , or a maximum on the boundary , and can also provide a visual guide to deviance from normality , as the log - likelihood function for a normal distribution is a parabola and hence symmetric about the maximum . in order to calibrate the log - likelihood function",
    "we can use the approximation @xmath40^{1/2 } \\dotsim n(0,1),\\ ] ] which is equivalent to the result that twice the log likelihood ratio is approximately @xmath41 .",
    "this will typically provide a better approximation than the asymptotically equivalent result that @xmath42 as it partially accommodates the potential asymmetry in the log - likelihood function .",
    "these two approximations are sometimes called first order approximations because in the context where the log - likelihood is @xmath43 , we have ( under regularity conditions ) results such as @xmath44 where @xmath45 follows a standard normal distribution .",
    "it is relatively simple to improve the approximation to third order , i.e. with relative error @xmath46 , using the so - called @xmath47 approximation @xmath48 where @xmath49 is a likelihood - based statistic and a generalization of the wald statistic @xmath50 ; see fraser [ 2 ] .",
    "* example 3 : truncated poisson *    suppose that @xmath2 follows a poisson distribution with mean @xmath51 , where @xmath52 is a background rate that is assumed known",
    ". in this model the @xmath0-value function can be computed exactly simply by summing the poisson probabilities . because the poisson distribution is discrete , the @xmath0-value could reasonably be defined as either @xmath53 or @xmath54 sometimes called the upper and lower @xmath0-values , respectively .    for the values @xmath55 , @xmath56 , figure 1 shows the likelihood function as a function of @xmath57 and the @xmath0-value function @xmath58 computed using both the upper and lower @xmath0-values . in figure 2 we plot the _ mid _",
    "@xmath0-value , which is @xmath59 the approximation based on @xmath47 is nearly identical to the mid-@xmath0-value ; the difference can not be seen on figure 2 .",
    "table 1 compares the @xmath0-values at @xmath60 .",
    "this example is taken from fraser , reid and wong [ 3 ] .",
    "-value function ( bottom ) for the poisson model , with @xmath56 and @xmath55 .",
    "for @xmath60 the @xmath0-value interval is @xmath61.,width=307,height=403 ]    -value functions and the mid-@xmath0-value function for the poisson model , with @xmath56 and @xmath55 .",
    "the approximation based on @xmath62 is identical to the mid-@xmath0-value function to the drawing accuracy.,width=307,height=307 ]    .the @xmath0-values for testing @xmath60 , i.e. that the number of observed events is consistent with the background .",
    "[ cols= \" < , < \" , ]     -value function for the log - odds ratio , @xmath6 , for the data of table ii .",
    "the value @xmath63 corresponds to the hypothesis that the probabilities of leaving are equal for men and women.,width=307,height=307 ]    * example 7 : poisson with estimated background * suppose in the context of example 3 that we allow for imprecision in the background , replacing @xmath52 by an unknown parameter @xmath64 with estimated value @xmath65 .",
    "we assume that the background estimate is obtained from a poisson count @xmath66 , which has mean @xmath67 , and the signal measurement is an independent poisson count , @xmath2 , with mean @xmath68 .",
    "we have @xmath69 and @xmath70 , so the estimated precision of the background gives us a value for @xmath71 .",
    "for example , if the background is estimated to be @xmath72 this implies a value for @xmath71 of @xmath73 .",
    "uncertainty in the standard error of the background is ignored here .",
    "we now outline the steps in the computation of the @xmath47 approximation ( [ rstar ] ) .",
    "the log - likelihood function based on the two independent observations @xmath66 and @xmath2 is @xmath74 with canonical parameter @xmath75",
    ".    then @xmath76 @xmath77 from which @xmath78 then we have @xmath79 @xmath80 the likelihood root is @xmath81\\\\ & = & { \\rm sign}(q)\\surd(2[k\\hat\\beta\\log\\{\\hat\\beta/\\hat\\beta_\\mu\\ } ) + ( \\hat\\beta+\\hat\\mu )   \\nonumber \\\\ & & \\quad \\log\\{(\\hat\\beta+\\hat\\mu)/(\\hat\\beta_\\mu+\\mu)\\ } \\nonumber   \\\\ & & -k(\\hat\\beta-\\hat\\beta_\\mu)-\\{\\hat\\beta+\\hat\\mu-(\\hat\\beta_\\mu+\\mu)\\}]).\\end{aligned}\\ ] ] the third order approximation to the @xmath0-value function is @xmath82 , where @xmath83    figure 5 shows the @xmath0-value function for @xmath57 using the mid-@xmath0-value function from the poisson with no adjustment for the error in the background , and the @xmath0-value function from @xmath82 .",
    "the @xmath0-value for testing @xmath60 is 0.00464 , allowing for the uncertainty in the background , whereas it is 0.000408 ignoring this uncertainty .",
    "the hypothesis @xmath84 could also be tested by modelling the mean of @xmath2 as @xmath85 , say , and testing the value @xmath86 . in this formulation",
    "we can eliminate the nuisance parameter exactly by using the binomial distribution of @xmath2 conditioned on the total @xmath87 , as described in example 1 .",
    "this gives a mid-@xmath0-value of 0.00521 .",
    "the computation is much easier than that outlined above , and seems quite appropriate for testing the equality of the two means .",
    "however if inference about the mean of the signal is needed , in the form of a point estimate or confidence bounds , then the formulation as a ratio seems less natural at least in the context of hep experiments .",
    "a more complete comparison of methods for this problem is given in linnemann [ 8 ] .",
    "in special model classes , it is possible to eliminate nuisance parameters by either _ conditioning _ or _ marginalizing_.",
    "the conditional or marginal likelihood then gives essentially exact inference for the parameter of interest , if this likelihood can itself be computed exactly .",
    "in example 1 above , @xmath88 is the density for @xmath28 conditional on @xmath33 , so is a conditional likelihood for @xmath6 .",
    "this is an example of the more general class of linear exponential families : @xmath89 in which @xmath90 defines the conditional likelihood .",
    "the comparison of two binomials in example 6 is in this class , with @xmath6 as defined at ( 30 ) and @xmath91 .",
    "the difference of two poisson means , in example 7 , can not be formulated this way , however , even though the poisson distribution is an exponential family , because the parameter of interest @xmath6 is not a component of the canonical parameter .",
    "it can be shown that in models of the form ( [ expon ] ) the log - likelihood @xmath92 approximates the conditional log - likelihood @xmath93 , and that @xmath94 where @xmath95^{1/2}\\\\ q & = & ( \\hat\\psi_a-\\psi)\\{j_a(\\hat\\psi)\\}^{1/2}\\end{aligned}\\ ] ] approximates the @xmath0-value function with relative error @xmath46 in i.i.d .",
    "an asymptotically equivalent approximation based on the profile log - likelihood is @xmath94 where @xmath96^{1/2}\\\\ q & = & ( \\hat\\psi-\\psi)\\{j_p(\\hat\\psi)\\}^{1/2 } \\frac{|j_{\\lambda\\lambda}(\\psi,\\hat\\lambda_\\psi)|^{1/2 } } { |j_{\\lambda\\lambda}(\\hat\\psi,\\hat\\lambda)|^{1/2 } } .\\end{aligned}\\ ] ] in the latter approximation an adjustment for nuisance parameters is made to @xmath97 , whereas in the former the adjustment is built into the likelihood function .",
    "approximation ( 46 ) was used in figure 3 ."
  ],
  "abstract_text": [
    "<S> we describe some recent approaches to likelihood based inference in the presence of nuisance parameters . </S>",
    "<S> our approach is based on plotting the likelihood function and the @xmath0-value function , using recently developed third order approximations . </S>",
    "<S> orthogonal parameters and adjustments to profile likelihood are also discussed . </S>",
    "<S> connections to classical approaches of conditional and marginal inference are outlined . </S>"
  ]
}