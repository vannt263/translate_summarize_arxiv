{
  "article_text": [
    "we consider development of a multilevel iterative solver for large - scale sparse linear systems corresponding to graph laplacian problems for graphs with balanced vertex degrees .",
    "a typical example is furnished by the matrices corresponding to the ( finite difference)/(finite volume)/(finite element ) discretizations of scalar elliptic equation with mildly varying coefficients on unstructured grids .",
    "multigrid ( mg ) methods have been shown to be very efficient iterative solvers for graph laplacian problems and numerous parallel mg solvers have been developed for such systems .",
    "our aim here is to design an algebraic multigrid ( amg ) method for solving the graph laplacian system and discuss the implementation of such methods on multi - processor parallel architectures , with an emphasis on implementation on graphical processing units ( gpus ) .",
    "the programming environment which we use in this paper is the compute unified device architecture ( cuda ) toolkit introduced in 2006 by nvidia which provides a framework for programming on gpus . using this framework in the last 5 years",
    "several variants of geometric multigrid ( gmg ) methods have been implemented on gpus @xcite and a high level of parallel performance for the gmg algorithms on cuda - enabled gpus has been demonstrated in these works .    on the other hand , designing amg methods for massively parallel heterogenous computing platforms , e.g. , for clusters of gpus , is very challenging mainly due to the sequential nature of the coarsening processes ( setup phase ) used in amg methods . in most amg algorithms , coarse - grid points or basis",
    "are selected sequentially using graph theoretical tools ( such as maximal independent sets and graph partitioning algorithms ) .",
    "although extensive research has been devoted to improving the performance of parallel coarsening algorithms , leading to notable improvements on cpu architectures @xcite , on a single gpu @xcite , and on multiple gpus @xcite , the setup phase is still considered a bottleneck in parallel amg methods .",
    "we mention the work in  @xcite , where a smoothed aggregation setup is developed in cuda for gpus .    in this paper , we describe a parallel amg method based on the un - smoothed aggregation amg ( ua - amg ) method .",
    "the setup algorithm we develop and implement has several notable design features .",
    "a key feature of our parallel aggregation algorithm is that it first chooses coarse vertices using a parallel maximal independent set algorithm @xcite and then forms aggregates by grouping coarse level vertices with their neighboring fine level vertices , which , in turn , avoids ambiguity in choosing fine level vertices to form aggregates .",
    "such a design eliminates both the memory write conflicts and conforms to the cuda programming model .",
    "the triple matrix product needed to compute the coarse - level matrix ( a main bottleneck in parallel amg setup algorithms ) simplifies significantly in the ua - amg setting , reducing to summations of entries in the matrix on the finer level .",
    "the parallel reduction sums available in cuda are quite an efficient tool for this task during the amg setup phase .",
    "additionally , the ua - amg setup typically leads to low grid and operator complexities .    in the solve phase of the proposed algorithm ,",
    "a k - cycle  @xcite is used to accelerate the convergence rate of the multilevel ua - amg method . such multilevel method optimizes the coarse grid correction and results in an approximate two - level method .",
    "two parallel relaxation schemes considered in our amg implementation are a damped jacobi smoother and a parameter free @xmath0-jacobi smoother introduced in  @xcite and its weighted version in  @xcite . to further accelerate the convergence rate of the resulting k - cycle method we apply it as a preconditioner to a nonlinear conjugate gradient method .",
    "the remainder of the paper is organized as follows . in section [ sec : ua - amg ] , we review the ua - amg method",
    ". then , in section [ sec : setup ] , a parallel graph aggregation method is introduced , which is our main contribution .",
    "the parallelization of the solve phase is discussed in section [ sec : solve ] . in section [ sec : numerics ] , we present some numerical results to demonstrate the efficiency of the parallel ua - amg method .",
    "the linear system of interest has as coefficient matrix the graph laplacian corresponding to an undirected connected graph @xmath1 . here",
    ", @xmath2 denotes the set of vertices and @xmath3 denotes the set of edges of @xmath4 .",
    "we set @xmath5 ( cardinality of @xmath2 ) . by @xmath6",
    "we denote the inner product in @xmath7 and the superscript @xmath8 denotes the adjoint with respect to this inner product .",
    "graph laplacian _",
    "@xmath9 is then defined via the following bilinear form @xmath10 we assume that the weights @xmath11 , and @xmath12 are strictly positive for all @xmath13 and @xmath14 .",
    "the first summation is over the set of edges @xmath3 ( over @xmath15 connecting the vertices @xmath13 and @xmath14 ) , and @xmath16 and @xmath17 are the @xmath13-th and @xmath14-th coordinate of the vector @xmath18 , respectively .",
    "we also assume that the subset of vertices @xmath19 is such that the resulting matrix @xmath20 is symmetric positive definite ( spd ) .",
    "if the graph is connected @xmath19 could contain only one vertex and @xmath20 will be spd . for matrices corresponding to the discretization scalar elliptic equation on unstructured grids",
    "@xmath19 is the set of vertices near ( one edge away from ) the boundary of the computational domain .",
    "the linear system of interest is then @xmath21    with this system of equation we associate a multilevel hierarchy which consists of spaces @xmath22 , each of the spaces is defined as the range of interpolation / prolongation operator @xmath23 with @xmath24 .",
    "given the @xmath25-th level matrix @xmath26 , the aggregation - based prolongation matrix @xmath27 is defined in terms of a non - overlapping partition of the @xmath28 unknowns at level @xmath25 into the @xmath29 nonempty disjoint sets @xmath30 , @xmath31 , called aggregates .",
    "an algorithm for choosing such aggregates is presented in the next section .",
    "the prolongation @xmath27 is the @xmath32 matrix with columns defined by partitioning the constant vector , @xmath33 , with respect to the aggregates : @xmath34 the resulting coarse - level matrix @xmath35 is then defined by the so called `` triple matrix product '' , namely , @xmath36 note that since we consider ua - amg , the interpolation operators are boolean matrices such that the entries in the coarse - grid matrix @xmath37 can be obtained from a simple summation process : @xmath38 thus , the triple matrix product , typically _ the _ costly procedure in an amg setup , simplifies significantly for ua - amg to reduction sums .",
    "we now introduce a general ua - amg method ( see algorithm [ alg : ua - amg ] ) and in the subsequent sections we describe the implementation of each of the components of algorithm  [ alg : ua - amg ] for gpus .    given @xmath39 ( size of the coarsest level ) and @xmath40 ( maximum levels ) @xmath41 , construct the aggregation @xmath42 , @xmath43 based on @xmath44 , compute @xmath37 by  , @xmath45 ,    * solve phase : *    solve @xmath46 exactly , pre - smoothing : @xmath47 , restriction : compute @xmath48 , coarse grid correction : solve @xmath49 approximately by recursively calling the amg on coarser level @xmath50 and get @xmath51 , prolongation : compute @xmath52 , post - smoothing : @xmath53 .",
    "consider the system of linear equations   corresponding to an unweighted graph @xmath54 partitioned into two subgraphs @xmath55 .",
    "further assume that the two subgraphs are stored on separate computes . to implement a jacobi or gauss - seidel smoother for the graph laplacian equation with respect to @xmath4 ,",
    "the communication between the two computers is proportional to the number of edge cuts of such a partitioning , given by @xmath56 therefore , a partition corresponding to the minimal edge cut in the graph results in the fastest implementation of such smoothers .",
    "this in turn gives a heuristic argument , as also suggested in @xcite , @xcite , that when partitioning the graph in subgraphs ( aggregates ) the subgraphs should have a similar number of vertices and have a small `` perimeter . ''",
    "such a partitioning can be constructed by choosing any vertex in the graph , naming it as a coarse vertex , and then aggregating it with its neighboring vertices .",
    "this heuristic motivates our aggregation method .",
    "the algorithm consists of a sequence of two subroutines : first , a parallel maximal independent set algorithm is applied to identify coarse vertices ; then a parallel graph aggregation algorithm follows , so that subgraphs ( aggregates ) centered at the coarse vertices are formed .    in the algorithm , to reduce repeated global memory read access and write conflicts , we impose explicit manual scheduling on data caching and flow control in the implementations of both algorithms ; the aim is to achieve the following goals :    1 .",
    "( read access coalescence ) : to store the data that a node uses frequently locally or on a fast connecting neighboring node .",
    "( write conflicting avoidance ) : to reduce , or eliminate the situation that several nodes need to communicate with a center node simultaneously .",
    "the idea behind such algorithm is to simplify the memory coalescence , and design a random aggregation algorithm where there are as many as possible threads loading from a same memory location , while as few as possible threads writing to a same memory location .",
    "therefore , it is natural to have one vertex per thread when choosing the coarse vertices . for vertices that are connected the corresponding processing threads should be wrapped together in a group . by doing so ,",
    "repeated memory loads from the global memory can be avoided .",
    "however , we also need to ensure that no two coarse vertices compete for a fine level point , because either atomic operations as well as inter - thread communication is costly on a gpu .",
    "therefore , the coarse vertices are chosen in a way that any two of them are of distance 3 or more , which is the same as finding a maximal independent set of vertices for the graph corresponding to @xmath57 , where @xmath20 is the graph laplacian of a given graph @xmath4 , so that each fine level vertex can be determined independently which coarse vertex it associates with .    given an undirected unweighted graph @xmath54 , we first find a set @xmath58 of coarse vertices such that @xmath59 here , @xmath60 is the graph distance function defined recursively as @xmath61 assume we obtain such set @xmath58 , or even a subset of @xmath58 , we can then form aggregates , by picking up a vertex @xmath13 in @xmath58 and defining an aggregate as a set containing @xmath13 and its neighbors .",
    "the condition guarantees that two distinct vertices in @xmath58 , do not share any neighbors .",
    "the operation of marking the numberings of subgraphs on the fine grid vertices is write conflict free , and the restriction imposed by ensures that aggregates can be formed independently , and simultaneously .",
    "the rationale of the independent set algorithm is as follows : first , a random vector @xmath62 is generated , each component of which corresponds to a vertex in the graph .",
    "then we define the set @xmath58 as the following : @xmath63 if @xmath58 is not empty , then such construction results in a collection of vertices in @xmath58 is of distance 3 or more .",
    "indeed , assume that @xmath64 for @xmath65 , let @xmath66 . from the definition of the set @xmath58",
    ", we immediately conclude that @xmath67 .",
    "of course , more caution is needed when @xmath58 defined above is empty ( a situation that may occur depending on the vector @xmath62 ) . however ,",
    "this can be remedied , by assuming that the vector @xmath62 ( with random entries ) has a global maximum , which is also a local maximum .",
    "the @xmath58 contains at least this vertex .",
    "the same algorithm can be applied then recursively to the remaining graph ( after this vertex is removed ) . in practice",
    ", @xmath58 does not contain one but more vertices .",
    "we here give a description of the parallel aggregation algorithm , running the exact copies of the code on each thread .    1 .",
    "generate a quasi - random number and store it in @xmath68 , as @xmath69 mark vertex @xmath13 as `` unprocessed '' ; wait until all threads complete these operations .",
    "2 .   1 .   goto ( 2d ) if @xmath13 is marked `` processed '' , otherwise continue to ( 2b ) .",
    "2 .   determine if the vertex @xmath13 is a coarse vertex , by check if the following is true .",
    "@xmath70 if so , continue to ( 2c ) ; if not , goto ( 2d ) .",
    "3 .   form an aggregate centered at @xmath13 .",
    "let @xmath71 be a set of vertices defined as @xmath72 define a column vector @xmath73 such that @xmath74 mark vertices @xmath75 `` processed '' and request an atomic operation to update the prolongator @xmath76 as @xmath77 \\;.\\ ] ] 4 .",
    "synchronize all threads ( meaning : wait until all threads reach this step ) .",
    "stop if @xmath13 is marked `` processed '' , otherwise goto step ( 2a ) .    within each pass of the parallel aggregation algorithm ( paa , algorithm [ alg : paa ] ) ,",
    "the following two steps are applied to each vertex @xmath13 .    1 .",
    "construct a set @xmath58 which contains coarse vertices .",
    "2 .   construct an aggregate for each vertex in @xmath58 .",
    "note that these two subroutines can be executed in a parallel fashion . indeed ,",
    "step ( a ) does not need to be applied to the whole graph before starting step ( b ) .",
    "even if @xmath58 is partially completed , any operation in step ( b ) will not interfere step ( a ) , running on the neighboring vertices and completing the construction of @xmath58 .",
    "a problem for this approach is that it usually can not give a set of aggregates that cover the vertex set @xmath78 after 1 pass of step ( a ) and step ( b ) .",
    "we thus run several passes and the algorithm terminates when a complete cover is obtained .",
    "the number of passes is reduced if we make the set @xmath58 as large as possible in each pass , therefore the quasi - random vector @xmath62 needs to have a lot of local maximums .",
    "another heuristic argument is that @xmath58 needs to be constructed in a way that every coarse vertex has a large number of neighboring vertices .",
    "numerical experiments suggest that the following is a good way of generating the vector @xmath62 with the desired properties .",
    "@xmath79 where @xmath80 is the degree of the vertex @xmath13 , and @xmath81 generates a random number uniformly distributed on the interval @xmath82 $ ] .      to improve the quality of the aggregates",
    ", we can either impose some constrains during the aggregation procedure ( which we call in - line optimization ) , or introduce a post - process an existing aggregation in order to improve it .",
    "one in - line strategy that we use to improve the quality of the aggregation is to limit the number of vertices in an aggregate during the aggregation procedure .",
    "however , such limitations may result in a small coarsening ratio . in such case , and",
    "numerical results suggest that applying aggregation process twice , which is equivalent to in skipping a level in a multilevel hierarchy , can compensate that .",
    "our focus is on a post - processing strategy , which we name `` rank one optimization '' .",
    "it uses an _ a priori _ estimate to adjust the interface ( boundary ) of a pair of aggregates , so that the aggregation based two level method , with a fixed smoother , converges fast locally on those two aggregates .",
    "we consider the connected graph formed by a union of aggregates ( say a pair of them , which will be the case of interest later ) , and let @xmath83 be the dimension of the underlying vector space .",
    "let @xmath84 be a semidefinite weighted graph laplacian ( representing a local sub - problem ) and @xmath85 be a given local smoother . as is usual for semidefinite graph laplacians",
    ", we consider the subspace @xmath86-orthogonal to the null space of @xmath87 and we denote it by @xmath78 . the @xmath86 orthogonal projection on @xmath78",
    "is denoted here by @xmath88 .",
    "let @xmath89 be the error propagation operator for the smoother @xmath90 .",
    "we consider the two level method whose error propagation matrix is @xmath91 here @xmath92 is a subspace and @xmath93 is the @xmath94-orthogonal projection of the elements of @xmath78 onto the coarse space @xmath95 . in",
    "what follows we use the notation @xmath96 when we want to emphasize the dependence on @xmath97 .",
    "we note that @xmath98 is well defined on @xmath78 because @xmath87 is spd on @xmath78 and hence it @xmath99 is an inner product on @xmath78 .",
    "we also have that @xmath98 self - adjoint on @xmath78 and under the assumption @xmath92 , we obtain @xmath100 and @xmath101 . also , @xmath102 is self - adjoint on @xmath78 in the @xmath99 inner product iff @xmath90 is self - adjoint in the @xmath86-inner product on @xmath103 .",
    "we now introduce the operator @xmath104 ( recall that @xmath92 ) @xmath105 and from the definition of @xmath106 for all @xmath107 we have @xmath108 we note the following identities which follow directly from the definitions above and the assumption @xmath92 : @xmath109 with respect to the coarse space @xmath95 , we need to make @xmath110 maximal .",
    "the following lemma quantifies this observation and is instrumental in showing how to optimize locally the convergence rate when the subspaces @xmath95 are one dimensional . in the statement of the lemma we use @xmath111 to denote a subset of minimizers of",
    "a given , not necessarily linear , functional @xmath112 on a space @xmath113 .",
    "more precisely , we set @xmath114 we have similar definition ( with obvious changes ) for the set @xmath115 .",
    "[ the - only - lemma ] let @xmath102 , be the projection of the local smoother on @xmath78 , and @xmath116 be the set of all one dimensional subspaces of @xmath78 .",
    "then we have the following : @xmath117 where @xmath118 and @xmath119 .    * proof . * from the identities   it follows that we can restrict our considerations on @xmath120 and that we only need to prove the lemma with @xmath121 and @xmath122 .",
    "in order to make the presentation more transparent , we denote @xmath123 , @xmath124 .",
    "let us mention also that by orthogonality in this proof we mean orthogonality in the @xmath99 inner product on @xmath78 .",
    "the proof then proceeds as follows .",
    "let @xmath125 be such that @xmath126 .",
    "we set @xmath127 .",
    "note that for such choice of @xmath128 we have @xmath129 and hence @xmath130 on the other hand , for all @xmath131 we have @xmath132 and we then conclude that @xmath133 in , we conclude the following thus prove .",
    "@xmath134    to prove , we observe that for any @xmath135 the inequalities in   become equalities and hence @xmath136 this implies that @xmath137 .",
    "it is also clear that @xmath138 for all @xmath139 , because @xmath128 is one dimensional .",
    "in addition , since @xmath140 is self - adjoint , it follows that @xmath128 is the span of the eigenvector of @xmath140 with eigenvalue of magnitude @xmath141 .",
    "next , for any @xmath142 we have @xmath143 , where @xmath144 is the second largest singular value of @xmath140 and with equality holding iff @xmath145 .",
    "this completes the proof .",
    "we now move on to consider a pair of aggregates .",
    "let @xmath87 be the graph laplacian of a connected positively weighted graph @xmath147 which is union of two aggregates @xmath148 and @xmath149 .",
    "furthermore , let @xmath150 be the characteristic vector for @xmath151 , namely a vector with components equal to @xmath152 at the vertices of @xmath151 and equal to zero at the vertices of @xmath153 .",
    "analogously we have a characteristic vector @xmath154 for @xmath153 .",
    "finally , let @xmath155 be the space of vectors that are linear combinations of @xmath150 and @xmath154 .",
    "more specifically , the subspace @xmath95 is defined as @xmath156 let @xmath116 be the set of subspaces defined above for all possible pairs of @xmath148 and @xmath149 , such that @xmath157 .",
    "note that by the definition above , every pair @xmath158 gives us a space @xmath159 which is orthogonal to the null space of @xmath87 , i.e. orthogonal to @xmath160 .",
    "we now apply the result of lemma  [ the - only - lemma ] and show how to improve locally the quality of the partition ( the convergence rate @xmath161 ) by reducing the problem of minimizing the @xmath87-norm of @xmath162 to the problem of finding the maximum of the @xmath87-norm of the rank one transformation @xmath163 . under the assumption that the spaces @xmath95 are orthogonal to the null space of @xmath87 ( which they satisfy by construction ) from lemma  [ the - only - lemma ]",
    "we conclude that the spaces @xmath128 which minimize @xmath164 also maximize @xmath110 .    for the pair of aggregates , @xmath165 is the largest eigenvalue of @xmath166 , where @xmath167 is the pseudo inverse of @xmath20 . clearly , the matrix @xmath166 , is also a rank one matrix and hence @xmath168    during optimization steps , we calculate the trace using the fact that for any rank one matrix @xmath169 we have @xmath170 where @xmath171 is a nonzero diagonal entry ( any nonzero diagonal entry ) and @xmath172 is the @xmath173-th column of @xmath169 .",
    "the formula   is straightforward to prove if we set @xmath174 for two column vectors @xmath175 and @xmath62 , and also suggests a numerical algorithm .",
    "we devise a loop computing @xmath176 , and @xmath177 , for @xmath178 , where @xmath179 is the dimension of @xmath169 ; the loop is terminated whenever @xmath180 , and we compute the trace via for this @xmath173 . in particular for the examples we have tested , @xmath181 is usually a full matrix and we observed that the loop almost always terminated when @xmath182 .    the algorithm which traverses all pairs of neighboring aggregates and optimizes their shape is as follows .    1 .   _ input _ : two set of vertices , @xmath148 and @xmath149 , corresponding to a pair of neighboring subgraphs .",
    "_ output _ : two sets of vertices , @xmath183 and @xmath184 satisfying that @xmath185 and the subgraphs corresponding to @xmath183 and @xmath184 are both connected .",
    "2 .   let @xmath186 , then compute @xmath187 .",
    "3 .   run in parallel to generate all partitionings such that the vertices set latexmath:[\\[\\tilde { \\mathcal{v}}_{1 } \\cup \\tilde { \\mathcal{v}}_{2 } = { \\mathcal{v}}_{1 } \\cup { \\mathcal{v}}_{2 }   , \\qquad     @xmath183 and @xmath184 are connected .",
    "4 .   run in parallel to compute the norm @xmath165 for all partitionings get from step ( 2 ) , and return the partitioning that results in maximal @xmath165 .",
    "the subgraph reshaping algorithm fits well the programming model of a multicore gpu .",
    "we demonstrate this algorithm on two example problems , and later show its potential as a post - process for the parallel aggregation algorithm ( algorithm  [ alg : paa ] ) outlined in the previous section . in the examples that follow next we use the rank one optimization and then measure the quality of the coarse space also by computing the energy norm of the @xmath189 , where @xmath190 is the @xmath191-orthogonal projection to the space @xmath128 .",
    "[ example-1 ] : consider a graph laplacian @xmath94 corresponding to a graph which is a @xmath192 square grid .",
    "the weights on the edges are all equal to @xmath152 .",
    "we start with an obviously non - optimal partitioning as shown on the left of figure [ fig : isotropic ] , of which the resulting two level method , consisting of @xmath0-jacobi pre- and post - smoothers and an exact coarse level solver , has a convergence rate @xmath193 , and @xmath194 . after applying algorithm [ alg : sra ] ,",
    "the refined aggregates have the shapes shown on the right of figure [ fig : isotropic ] , of which the two level method has the same convergence rate @xmath193 but the square of the energy seminorm is reduced to @xmath195",
    ".    @xmath196    [ example-2 ] consider a graph laplacian @xmath94 corresponding to a graph which is a @xmath192 square grid , on which all horizontal edges are weighted @xmath152 while all vertical edges are weighted @xmath197 .",
    "such graph laplacian represents anisotropic coefficient elliptic equations with neumann boundary conditions .",
    "start with a non - optimal partitioning as shown on the left of figure [ fig : anisotropic ] , of which the resulting two level method has a convergence rate @xmath198 and @xmath199 . after applying algorithm [ alg : sra ] ,",
    "the refined aggregates have the shapes shown on the right of figure [ fig : anisotropic ] , of which the two level convergence rate is reduced to @xmath200 and the energy of the coarse level projection is also reduced as @xmath195 .",
    "in this section , we discuss the parallelization of the solver phase on gpu .",
    "more precisely , we will focus on the parallel smoother , prolongation / restriction , mg cycle , and sparse matrix - vector multiplication .",
    "an efficient parallel smoother is crucial for the parallel amg method . for the sequential amg method , gauss - seidel relaxation is widely used and has been shown to have a good smoothing property . however the standard gauss - seidel is a sequential procedure that does not allow efficient parallel implementation . to improve the arithmetic intensity of the smoother and",
    "make it work better with simt based gpus , we adopt the well - known jacobi relaxation , and introducing a damping factor to improve the performance of the jacobi smoother . for a matrix @xmath201 and its diagonals",
    "are denoted by @xmath202 , the jacobi smoother can be written in the following matrix form @xmath203 or component - wise @xmath204 this procedure can be implemented efficiently on gpus by assigning one thread to each component , and update the corresponding components locally and simultaneously .",
    "we also consider the so - called @xmath0 jacobi smoother , which is parameter free .",
    "define @xmath205 where @xmath206 with @xmath207 , and the @xmath0 jacobi has the following matrix form @xmath208 or component - wise @xmath209 in  @xcite it has been show that if @xmath20 is symmetric positive definite , the smoother is always convergent and has multigrid smoothing properties comparable to full gauss - seidel smoother if @xmath210 and @xmath211 is bounded away from zero .",
    "moreover , because its formula is very similar to the jacobi smoother , it can also be implemented efficiently on gpus by assigning one thread to each component , and update the corresponding the component locally and simultaneously .      for ua - amg method ,",
    "the prolongation and restriction matrices are piecewise constant and characterize the aggregates .",
    "therefore , we can preform the prolongation and restriction efficiently in ua - amg method . here",
    ", the output array @xmath212 ( column index of @xmath76 ) , which contains the information of aggregates , plays an important rule .    * * prolongation : * let @xmath213 , so that the action @xmath214 can be written component - wise as follows : @xmath215 assign each thread to one element of @xmath216 , and the array ` aggregation ` can be used to obtain information about @xmath217 , i.e. , @xmath218 $ ] , so that prolongation can be efficiently implemented in parallel . *",
    "* restriction : * let @xmath219 , so that the action @xmath220 can be written component - wise as follows : @xmath221 therefore , each thread is assigned to an element of @xmath222 , and the array ` aggregation ` can be used to obtain information about @xmath223 , i.e. , to find all @xmath14 such that @xmath224 = i$ ] . by doing so , the action of restriction can also be implemented in parallel .      unfortunately , in general , ua - amg with v - cycle is not an optimal algorithm in terms of convergence rate .",
    "but on the other hand , in many cases , ua - amg using two - grid solver phase gives optimal convergence rate for graph laplacian problems .",
    "this motivated us to use other cycles instead of v - cycle to mimic the two - grid algorithm .",
    "the idea is to invest more works on the coarse grid , and make the method become closer to an exact two - level method , then hopefully , the resulting cycle will have optimal convergence rate .",
    "the particular cycle we will discuss here is the so - called k - cycle ( nonlinear amli - cycle ) and we refer to  @xcite for details on its implementation in general .",
    "as the k - cycle will be used as a preconditioner for nonlinear preconditioned conjugate gradient ( npcg ) method , the sparse matrix - vector multiplication ( spmv ) has major contribution to the computational work involved .",
    "an efficient spmv algorithm on gpu requires a suitable sparse matrix storage format .",
    "how different storage formats perform in spmv is extensively studied in @xcite .",
    "this study shows that the need for coalesce accessing of the memory makes ellpack ( ell ) format one of the most efficient sparse matrix storage formats on gpus when each row of the sparse matrix has roughly the same nonzeros . in our study , because our main focus is on the parallel aggregation algorithm and the performance of the ua - amg method , we still use the compressed row storage ( csr ) format , which has been widely used for the iterative linear solvers on cpu .",
    "although this is not an ideal choice for gpu implementation , the numerical results in the next section already show the efficiency of our parallel amg method .",
    "in this section , we present numerical tests using the proposed parallel amg methods .",
    "whenever possible we compare the results with the cusp libraries @xcite .",
    "cusp is an open source c++ library of generic parallel algorithms for sparse linear algebra and graph computations on cuda - enabled gpus .",
    "all cusp s algorithms and implementations have been optimized for gpu by nvidia s research group . to the best of our knowledge ,",
    "the parallel amg method implemented in the cusp package is the state - of - the - art amg method on gpu .",
    "we use as test problems several discretizations of the laplace equation .      define @xmath190 , the @xmath86 projection on the piece - wise constant space @xmath225 , as the following : @xmath226 we present several tests showing how the energy norm of this projection changes with respect to different parameters used in the parallel aggregation algorithm , since the convergence rate is an increasing function of @xmath227 .",
    "the tests involving @xmath227 further suggest two additional features necessary to get a multigrid hierarchy with predictable results .",
    "first , the sizes of aggregates need to be limited , and second , the columns of the prolongator @xmath76 need to be ordered in a deterministic way , regardless of the order that aggregates are formed .",
    "the first requirement can be fulfilled simply by limiting the sizes of the aggregates in each pass of the parallel aggregation algorithm .",
    "we make the second requirement more specific .",
    "let @xmath228 to be the index of the coarse vertex of the @xmath173-th aggregate .",
    "we require that @xmath228 should be an increasing sequence and then use the @xmath173-th column of @xmath76 to record the aggregate with the coarse vertex numbered @xmath228 .",
    "this can be done by using a generalized version of the prefix sum algorithm @xcite .",
    "we first show in table  [ square - dirichlet ] the coarsening ratios ( in the parenthesis in the table ) and the energy norms @xmath229 of a two grid hierarchy , for a laplace equation with dirichlet boundary conditions on a structured grid containing @xmath230 vertices .",
    "the limit on the size of an aggregate is denoted by @xmath8 , which suggests that , any aggregate can include @xmath8 vertices or less , which directly implies that the resulting coarsening ratio is less or equal to @xmath8 .",
    "m.  brezina and p.  s. vassilevski . smoothed aggregation spectral element agglomeration amg : sa-@xmath231amge . in",
    "_ proceedings of the 8th international conference on large - scale scientific computing _ , volume 7116 of _ lecture notes in computer science _ , pages 315 .",
    "springer - verlag , 2012 .",
    "e.  chow , r.  falgout , j.  hu , r.  tuminaro , and u.  yang .",
    "a survey of parallelization techniques for multigrid solvers . in",
    "_ parallel processing for scientific computing _ , volume  20 , pages 179201 .",
    "siam , 2006 .",
    "a.  j. cleary , r.  d. falgout , v.  e. henson , and j.  e. jones .",
    "coarse - grid selection for parallel algebraic multigrid . in",
    "_ solving irregularly structured problems in parallel _ , volume 1457 of _ lecture notes in computer science _ , pages 104115 .",
    "springer , 1998 .",
    "z.  feng and z.  zeng .",
    "parallel multigrid preconditioning on graphics processing units ( gpus ) for robust power grid analysis . in _ proceedings of the 47th design automation conference _ , dac 10 , pages 661666 .",
    "acm , 2010 .",
    "j.  kraus and m.  frster .",
    "efficient amg on heterogeneous systems . in",
    "_ facing the multicore - challenge ii _ , volume 7174 of _ lecture notes in computer science _ , pages 133146",
    "springer berlin heidelberg , 2012 ."
  ],
  "abstract_text": [
    "<S> we design and implement a parallel algebraic multigrid method for isotropic graph laplacian problems on multicore graphical processing units ( gpus ) . </S>",
    "<S> the proposed amg method is based on the aggregation framework . </S>",
    "<S> the setup phase of the algorithm uses a parallel maximal independent set algorithm in forming aggregates and the resulting coarse level hierarchy is then used in a k - cycle iteration solve phase with a @xmath0-jacobi smoother . </S>",
    "<S> numerical tests of a parallel implementation of the method for graphics processors are presented to demonstrate its effectiveness . </S>"
  ]
}