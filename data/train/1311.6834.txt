{
  "article_text": [
    "coding ( sc ) @xcite has been a popular and effective data representation method for many applications , including pattern recognition @xcite , bioinformatics @xcite and computer vision @xcite . given a data sample with its feature vector , sc tries to learn a codebook with some codeworks , and approximate the data sample as the linear combination of the codewords .",
    "sc assume that only a few codewords in the codebook are enough to represent the data sample , thus the combination coefficients should be sparse , i.e. most of the coefficients are zeros , leaving only a few of them non - zeros .",
    "the linear combination coefficients of the data sample could be its new representation .",
    "because they are sparse , the coefficient vector is often referred to as the sparse code . to solve the sparse code ,",
    "one usually minimizes the approximation error with regard to the codebook and the sparse code , and at the same time seeks the sparsity of the sparse code .",
    "although sc has been used in many pattern recognition applications , such as palmprint recognition @xcite , dynamic texture recognition @xcite , human action recognition @xcite , speech recognition @xcite , digit recognition @xcite , image annotation @xcite , and face recognition @xcite , in most cases , sc is used as an unsupervised learning method .",
    "when sc is performed to the training data set , it is assumed that the class labels of the training samples are unavailable .",
    "then after the sparse codes are learned , they will be used to learn a classifier .",
    "thus the class labels are ignored during the sparse coding procedure . however , in most pattern recognition problems , the class labels of the training samples are given .",
    "it is thus natural to improve the discriminative ability of the learned sparse codes for the classification purpose . to solve this problem ,",
    "a few supervised sc methods were proposed to include the class labels during the coding of the samples .",
    "for example , mairal et al .",
    "@xcite proposed to learn the sparse codes of the samples and a classifier in the sparse code space simultaneously , by constructing and optimizing a unified objective function for the sc parameters and the classification parameters .",
    "wang et al .",
    "@xcite proposed the discriminative sc method based on multi - manifolds , by learning discriminative class - conditioned codebooks and sparse codes from both data feature spaces and class labels .",
    "though these methods use the class labels , they require that all the training samples are labeled .",
    "however , in some real - world applications , there are only very few training samples labeled , while the remaining training samples are unlabeled .",
    "learning from such a training set is called semi - supervised learning @xcite .",
    "semi - supervised learning , compared to the supervised learning , can explore both the labels of the labeled samples and the distribution of the overall data set containing labeled and unlabeled samples .",
    "when there are few labeled samples , they are not sufficient to learn an effective classifier using a supervised learning algorithm . in this case , it is necessary to include the unlabeled samples to explore the overall distribution .",
    "many semi - supervised learning algorithm has been proposed to learn classifier from both labeled and unlabeled samples ( inductive learning ) @xcite , or to learn the labels of the unlabeled samples from the labeled samples ( transductive learning ) @xcite .",
    "however , surprisingly , no work has been done to learn discriminate sparse codes from partially labeled data set by utilizing both the labels and the feature vectors of the labeled samples , and the feature vectors of the unlabeled data samples .",
    "it is interesting to note that he et al .",
    "@xcite proposed to use the sc method to construct a sparse graph from the data set for the transductive learning problem , so that the class labels could be prorogated from the labeled samples to the unlabeled samples via the sparse code . however , during the sparse graph learning procedure using sc , the class labels of the labeled samples were ignored .",
    "thus in he et al.s work @xcite , sc was also performed in an unsupervised way .",
    "similarly , sc was also used to construct a sparse graph for the transductive learning problem @xcite .    to fill this gap",
    ", we propose a semi - supervised sc method in this paper . given a data set with only few of the samples labeled , besides conducting sc for all the samples",
    ", we also assume that the class labels for all the samples could be learned from their sparse codes . to do this",
    ", we define variable class labels for all the samples , and a classifier to predict the variable class labels .",
    "the variable class label learning is regularized by the manifold of the data set and the labels of the labeled samples . to learn the codebook , sparse codes , variable class labels , and the classifier parameters simultaneously",
    ", we propose a unified objective function . in the objective function , besides the approximation error term and the sparsity term for sc , we also introduce the class label approximation error term and the manifold regularization term for variable class labels . by optimizing this objective function , we try to predict the variable class label from the sparse codes , thus the learned sparse code is naturally discriminative since it has the ability to predict the class labels .",
    "moreover , the learning of the class labels of the unlabeled samples is regularized by the known labels of the labeled samples , the sparse codes and the manifold structure of the data set .",
    "the contributions of this paper are in two folds :    1 .",
    "we propose a discriminative sc method which could learn from semi - supervised data set .",
    "it is a discriminative representation and both labeled and unlabeled data samples could be used to improve its discriminative power .",
    "2 .   moreover , it is also an inductive learning method since it learns a codebook and a classifier from the semi - supervised training set , which could be further used to code and classify the test samples .",
    "the rest parts of this paper is organized as follows : in section [ sec : method ] , we introduce the proposed semi - supervised sc method ; in section [ sec : exp ] , the experiment results on two data sets are reported ; and finally in section [ sec : conclusion ] the paper is concluded .",
    "in this section , we introduce the proposed semi - supervised learning method",
    ". an objective function is firstly constructed , and then an iterative algorithm is developed to optimize it .",
    "we assume that we have a training data set of @xmath0 training samples , denoted as @xmath1 , where @xmath2 is the @xmath3-dimensional feature vector for the @xmath4-th sample .",
    "the data set is further denoted as a data matrix as @xmath5\\in r^{d\\times n}$ ] , where the @xmath4-th column is the feature vector of the @xmath4-th sample .",
    "we assume that we are dealing with a @xmath6-class semi - supervised classification problem , and only the first @xmath7 samples are labeled , while the remaining samples are unlabeled . for a labeled sample @xmath2",
    ", we define a @xmath6-dimensional binary class label vector @xmath8 , with its @xmath9-th element equal to one if it is labeled as the @xmath9-th class , and the reminding elements equal to zero .",
    "the class label vector set of the labeled samples are denoted as @xmath10 , and they are further organized as a matrix @xmath11\\in \\{1,0\\}^{c\\times l}$ ] , with its @xmath4-th column as the label vector of the @xmath4-th sample . to construct the objective function , we consider the following three problems :    * * sparse coding * : given a sample @xmath2 , sparse coding tries to learn a codebook matrix @xmath12\\in   \\mathbb{r}^{d\\times m}$ ] , where its columns are @xmath13 codewords , and an @xmath13-dimensional coding vector @xmath14 , so that @xmath2 could be approximated as the linear combination of the codewords , + @xmath15 + and at the same time , @xmath16 should be as sparse as possible .",
    "thus we also call @xmath16 sparse code .",
    "the sparse code @xmath16 is a new representation of @xmath2 .",
    "the sparse codes of the training samples are organized in a sparse code matrix @xmath17\\in r^{m\\times n}$ ] , with its @xmath4-th column as the sparse code of the @xmath4-th sample . to learn the codebook and the sparse codes from the training set",
    ", the following optimization problem is proposed , + @xmath18 + where the first term @xmath19 is the approximation error term , the second term @xmath20 is introduced to encourage the sparsity of each @xmath2 , and @xmath21 is a trade - off parameter .",
    "moreover , @xmath22 is imposed to to reduce the complexity of each codeword . * * class label learning * : we also propose to learn the class label vectors from the sparse code space for all the training samples by a linear function . to do this ,",
    "we introduce a variable label vector for each sample @xmath2 as @xmath23",
    ". please note that we relax it as a real value vector instead of a binary vector , and each element presents its membership of each class . the variable class label vector set for all the training samples are denoted as @xmath24 , and further organized as a variable class label matrix , @xmath25\\in \\mathbb{r}^{c\\times n}$ ] .",
    "we assume that its class label vector could be approximated from its sparse code by a linear classifier , + @xmath26 + where @xmath27 is the classifier parameter matrix . to learn the class labels and the classifier parameter matrix , we propose the following optimization problem , + @xmath28 + as we can see from the above objective function , we use the squared @xmath29 norm distance @xmath30 as the approximation error for the @xmath4-th sample .",
    "moreover , @xmath31 constrain is introduced to reduce the complexity of the classifier , and @xmath32 constrains are introduced so that the learned labels could respect the known labels of the labeled samples . *",
    "* manifold label regularization * : we also hope the learned class labels could respect the manifold structure of the data set .",
    "we assume that for each sample @xmath2 , its class label vector @xmath33 could be reconstructed by the class labels of its nearest neighbors @xmath34 , + @xmath35 + where @xmath36 is the reconstruction coefficient , which could be solved in the same way as locally linear embedding ( lle ) @xcite by minimizing the reconstruction error in the original feature space , + @xmath37 + with the solved reconstruction coefficient matrix @xmath38\\in \\mathbb{r}_+^{n\\times n}$ ] , we regularize the class label learning with the following optimization problem , + @xmath39 + by doing this , we assume that label space and the data space share the same local linear reconstruction coefficients .    the overall optimization problem is formulated by combining the three problems in ( [ equ : sc ] ) , ( [ equ : label ] ) and ( [ equ : manifold ] ) , and the following optimization problem is obtained ,    @xmath40    where @xmath41 and @xmath42 are the tradeoff parameters , which are selected by cross - validation .",
    "please note that in this formulation , we do not use the class labels to regularize the sparse codes directly .",
    "instead , a classifier is learned to assign the class label from the sparse codes , so that the class labels , the classifiers , and the sparse codes could be learned together and regularize each other .",
    "it is difficult to find a closed - form solution for the problem in ( [ equ : objective ] ) .",
    "thus we use the alternate optimization strategy to optimize it in an iterative algorithm . in each iteration",
    ", the variables are optimized by turn .",
    "when one of the variables is optimized , the others are fixed .",
    "we first discuss the optimization of @xmath43 and @xmath44 .",
    "as we show later , they could be solved together as different parts of an generalized codebook . by removing the terms irrelevant to @xmath43 and @xmath44 , and fixing @xmath45 and @xmath46",
    ", we obtain the following optimization problem ,    @xmath47    we define an extended data matrix by catenating @xmath48 and @xmath46 as @xmath49 , and an extended codebook matrix by catenating @xmath43 and @xmath44 as @xmath50 .",
    "moreover , we combine the two constrains @xmath22 and @xmath31 to one single constraint @xmath51 .",
    "this constrain could be rewritten as @xmath52 , where @xmath53 is the @xmath54-th column of the @xmath55 matrix . in this way ,",
    "the optimization is rewritten as    @xmath56    this problem could be solved using the lagrange dual method proposed in @xcite .",
    "after @xmath55 is solved , @xmath43 and @xmath44 could be recovered from it as    @xmath57    where @xmath58 is the frist @xmath3 rows of the matrix @xmath55 , and @xmath59 is the @xmath60 to @xmath61 rows of matrix @xmath55 .      to solve the sparse codes in @xmath45 , we fix @xmath55 , remove the terms irrelevant to @xmath45 , and the following problem is obtained ,    @xmath62    similarly , this problem could be solved efficiently by the feature - sign search algorithm proposed in @xcite .      to solve the class label vectors in @xmath46 , we fix @xmath43 , @xmath45 and @xmath44 , remove the terms irrelevant to @xmath46 , and get the following optimization problem ,    @xmath63    we separate the class label matrix to to sub - matrices as @xmath64 $ ] , where @xmath65 contains the first @xmath7 columns of @xmath46 , which are the variable class label vectors of the labeled samples , while @xmath66 contains the remaining columns which are the variable class label vectors of the unlabeled samples .",
    "similarly , we also separate @xmath45 to two sub - matrices as @xmath67 $ ] , where @xmath68 contains the sparse codes of the labeled samples , while @xmath69 contains the sparse codes of the labeled samples . moreover",
    ", we define matrix @xmath70 for convenience , and also separate it to two sub - matrices as @xmath71 where @xmath72 contains its first @xmath7 rows and @xmath73 contains its remaining rows . with these definitions , we could rewrite the objective function in ( [ equ : obj_y ] ) as    @xmath74 \\begin{bmatrix } q_{l }",
    "\\\\ q_{u } \\end{bmatrix } \\right",
    "\\|_2 ^ 2\\\\ & = \\beta \\left \\| y_l - ws_l \\right \\|^2_2 + \\beta \\left \\| y_u - ws_u \\right \\|^2_2 + \\gamma \\left \\| y_l q_l + y_u q_u \\right \\|_2 ^ 2 \\end{aligned}\\ ] ]    since it is constrained that @xmath75 for any @xmath76 , @xmath77 and it is actually not a variable .",
    "thus we substitute @xmath77 to ( [ equ : obj_y1 ] ) by only treating @xmath66 as variable to solve , and obtain the following optimization problem with regard to @xmath66 ,    @xmath78    to solve this problem , we simply set the derivative of the objective function @xmath79 with regard to @xmath66 to zero , and obtain the solution for @xmath66 ,    @xmath80      we summarize the iterative learning algorithm for semi - supervised sparse coding ( sssc ) in algorithm [ alg : learning ] . as we can see from the algorithm",
    ", we employ the original sparse coding algorithm to initialize the sparse code matrix , and employ the linear neighborhood propagation ( lnp ) algorithm @xcite to initialize the class label matrix .",
    "the iterations are repeated for @xmath81 times and the updated solutions for @xmath43 , @xmath45 , @xmath44 and @xmath66 are outputted .",
    "* input * : training data matrix @xmath48 ; * input * : training data label matrix for labeled samples @xmath82 ; * input * : tradeoff parameters @xmath21 , @xmath41 and @xmath42 . ; * input * : iteration number @xmath81 .",
    "initialize the sparse code matrix @xmath83 by performing original sparse coding to @xmath48 ; initialize the class label matrix @xmath84 ;    update codebook matrix @xmath85 and the classifier parameter matrix @xmath86 as in ( [ equ : b ] ) by fixing @xmath87 and @xmath88 ;    update sparse code matrix @xmath89 as in ( [ equ : s ] ) by fixing @xmath90 and @xmath88 ;    update the variable class label matrix @xmath91 as in ( [ equ : y ] ) by fixing @xmath90 and @xmath92 ;    * output * : the codebook matrix @xmath93 , the sparse code matrix @xmath94 , the classifier parameter matrix @xmath95 , and the class label matrix for the unlabeled samples @xmath96 .",
    "when a new test sample @xmath97 comes , we first find its nearest neighbors @xmath98 from the training set , and we assume that it could be reconstructed by these nearest neighbors .",
    "the reconstruction coefficients @xmath99 are computed by solving a problem in ( [ equ : qp ] ) . to solve its sparse code vector @xmath100 , and its class label vector @xmath101",
    ", we use the codebook @xmath43 , classifier parameter matrix @xmath44 , and the class label matrix @xmath46 learned from the training set .",
    "the optimization problem is formulated as    @xmath102    where @xmath33 is the class label vector of the @xmath4-th training sample . to solve this problem",
    ", we also adopt the alternate optimization strategy . in an iterative algorithm ,",
    "we optimize @xmath100 and @xmath101 in turn .    * * solving @xmath100 * : when @xmath100 is optimized , @xmath101 is fixed , and the following problem is solved , + @xmath103 + where @xmath104 .",
    "this problem could be solved using the feature - sign search algorithm proposed in @xcite . * * solving @xmath101 * : when @xmath100 is fixed and @xmath101 is optimized , we have the following problem , + @xmath105 + it could be solved easily by setting the derivative with regard to @xmath101 to zero , and the solution is obtained as + @xmath106    by repeating the above two procedures for @xmath81 times , we could obtain the optimal sparse code @xmath100 and the class label vector @xmath101 for the test sample @xmath97 .",
    "it will be further classifier to the @xmath107-th class with the largest value in the class label vector @xmath101 ,    @xmath108    where @xmath109 is the @xmath9-th element of @xmath101 .",
    "in this section , we evaluate the performance of the proposed semi - supervised sparse coding algorithm on two real - world data sets .      the cytochromes p450 is a family of enzymes which are involved in the metabolism of most modern drugs @xcite .",
    "there are five major isoforms of cytochromes p450 , which are 1a2 , 2c9 , 2c19 , 2d6 , and 3a4 @xcite .",
    "it is very important to model the interactions of the cytochromes p450 with the drug - like compounds in drug - drug interaction studies . in this case , predicting if a given compound can inhibit these isoforms plays an important role in the drug design @xcite . here , we evaluated the proposed algorithm in the problem of cytochromes p450 inhibition prediction .",
    "we collected a data set of compounds for each isoform , and each compound is an inhibitor or a non - inhibitor of the isoform . the numbers of inhibitors and non - inhibitors of each isoform are given in figure [ fig : figp450_data ] . as we can see from the figure ,",
    "the data sets are not balanced . for each isoform ,",
    "non - inhibitors are usually more than inhibitors . to represent each compound , we extracted the molecular signatures as features , which were computed from the atomic signatures of circular atomic fragments @xcite .",
    "the problem of cytochromes p450 inhibition prediction is to learn a predictor from the given data set to predict whether a candidate compound is an inhibitor or a non - inhibitor .",
    "thus it is a binary classification problem .",
    "+    to conduct the experiment , for each isoform , we performed the 10-fold cross - validation @xcite to the data set .",
    "each data set of an isoform was split into ten folds , and each fold was used as the test set in turn , while the remaining nine folds were used as the training set . for each taining set",
    ", we only randomly labeled a small part ( about 20%)of the compounds with the class labels ( inhibitors or non - inhibitors ) , while leaving the remaining part as unlabeled compounds .",
    "the proposed learning algorithm was performed to the molecular signatures of the training compounds to learn the codebook , the classifier and the labels of the unlabeled compounds .",
    "then the compounds in the test set were used as test sample one by one .",
    "the learned codebook and the classifier were used to code and classify the test compound .    to evaluate the prediction performance ,",
    "we used the following performance measures as prediction performance metrics : sensitivity ( sen ) , specificity ( spc ) , accuracy ( acc ) , and f1 score ( f1 ) . to calculate these metrics , we first calculate the following values for each test set : true positive ( tp ) which is the number of inhibitor compounds that were correctly predicted , true negative ( tn ) which is the number of non - inhibitor compounds that were correctly predicted , false positive ( fp ) which is the number of non - inhibitor compounds wrongly predicted as inhibitor compounds , and false negative ( fn ) which is the number of inhibitor compounds wrongly predicted as non - inhibitor compounds . with these values computed from the test set ,",
    "the performance measures are defined as ,    @xmath110    please note that the ranges of sen , spc , acc and f1 values are all from @xmath111 to @xmath112 , and a larger value indicates a better prediction performance .       +     +     +     +     +    since the proposed algorithm is the first semi - supervised sparse coding algorithm , we compared it to some unsupervised and supervised sparse coding algorithms .",
    "for the unsupervised sparse coding algorithms , we compared the proposed sssc against the original sparse coding ( sc ) algorithm proposed in @xcite , and the popular manifold regularized sparse coding ( mrsc ) algorithm proposed in @xcite .",
    "for the supervised sparse coding algorithm , we compared it against the unified classifier learning and sparse coding ( uclsc ) algorithm proposed in @xcite , and the discriminative sparse coding on multi - manifold ( dscmm ) algorithm proposed in @xcite",
    ". please note that for the supervised sparse coding algorithms , it is required that all the training samples are labeled . in this case",
    ", we only used the labeled samples in the training set , while the unlabeled samples were ignored .",
    "the experiment results of four different performance measures on the five data sets are given in fig .",
    "[ fig : fig1a2 ] - [ fig : fig3a4 ] .",
    "it is clear that our sssc algorithm consistently outperforms all other supervised and unsupervised sparse coding algorithms , namely dscmm , uclsc , mrsc and sc , in terms of the sen , spc , acc and f1 measures .",
    "this implies that sssc is able to learn more discriminative sparse codes to distinguish inhibitors from non - inhibitors by learning discriminative codebooks and classifiers .",
    "the performance of supervised methods , dscmm and uclsc , is comparable to that of unsupervised methods , mrsc and sc .",
    "we should note that only labels are used by the supervised sparse coding methods , while unsupervised methods can explore all samples . however , supervised methods include class labels to improve the discriminative ability of the sparse codes during learning , but unsupervised methods simply ignore them . only the proposed semi - supervised method ,",
    "sssc , can use both the labels and all samples .",
    "thus it is not surprising that it archives the best performance .      in this experiment",
    ", we evaluate the proposed algorithm on the problem of wireless sensor fault diagnosis for wireless networks @xcite .      we collected a data set of 300 samples of wireless sensors .",
    "the samples were classified to four fault types , including shock , biasing , short circuit , and shifting .",
    "we also included the normal type , making it five types in total .",
    "for each type , there are 60 samples . for each sample",
    ", we used the output signal of wireless sensors as the feature to predict its state type .    to conduct the experiment",
    ", we also employed the 10-fold cross validation .",
    "the entire data set was split to 10 folds randomly .",
    "each fold was used as the test set in turn , and the remaining nine folds were combined and used as the training set to train the diagnosis model .",
    "most of the training samples were unlabeled while only a small portion of the training samples was labeled .",
    "we performed the proposed algorithm to learn the codebook , classifier , sparse codes and class labels of the unlabeled training samples .",
    "the learned codebook and classifier are used to represent and classify the test samples .",
    "the classification performance is measured by the classification accuracy ( acc ) for multi - class problem , which is defined as follows ,    @xmath113    the value of acc also varies from 0 to 1 , and a larger acc indicates better classification performance .",
    "+    the boxplots of the accuracy of 10-fold cross validations are given in fig .",
    "[ fig : figwireless131219 ] . from this figure",
    ", we can see that the proposed semi - supervised sparse coding and classification method sssc significantly outperforms the other sparse coding methods on the wireless sensor fault diagnosis task .",
    "this is because our method utilizes both the labeled and unlabeled samples in learning the sparse code , while others do not effectively use such information .",
    "again , the supervised methods dscmm and uclsc do not show much better improvement over the unsupervised methods mrsc and sc .",
    "it is clear that the proposed sssc combines the advantages of both supervised and unsupervised methods .",
    "the codebook and the class labels of unlabeled samples are directly learned from training samples . thus it is better adaptive to the data and higher classification accuracy can be achieved .",
    "we have proposed a sparse coding method for the semi - supervised data representation and classification task . to the best of our knowledge",
    ", this paper is the first attempt to learn sparse code on partially labeled data sets .",
    "experimental results have shown that our proposed method sssc are not only significantly better than state - of - the - art unsupervised sparse coding methods , but also outperforms supervised sparse coding methods .",
    "how to explore more discriminative information from both labeled and unlabeled , and combine them with our proposed semi - supervised sparse coding algorithm to further improve the learning performance appears to be an interesting direction in machine learning and pattern recognition communities . in the future",
    ", we will investigate the usage of the proposed method in applications of bioinformatics @xcite .",
    "z.  lu and y.  peng , `` heterogeneous constraint propagation with constrained sparse representation , '' in _ proceedings of ieee international conference on data mining_.1em plus 0.5em minus 0.4emieee computer society , 2012 , pp .",
    "10021007 .",
    "l.  liu , m.  esmalifalak , q.  ding , v.  emesih , and z.  han , `` detecting false data injection attacks on power grid by sparse optimization , '' _ ieee transactions on smart grid _ ,",
    "vol .  5 , no .  2 , pp .",
    "612621 , march 2014 .",
    "l.  zhou , z.  lu , h.  leung , and l.  shang , `` spatial temporal pyramid matching using temporal sparse representation for human motion retrieval , '' _ the visual computer _",
    "6 - 8 , pp . 845854 , 2014 .",
    "l.  wang , z.  lu , and h.  h. ip , `` image categorization based on a hierarchical spatial markov model , '' in _ international conference on computer analysis of images and patterns_.1em plus 0.5em minus 0.4em springer berlin heidelberg , 2009 , pp .",
    "766773 .",
    "z.  lu , y.  peng , and j.  xiao , `` unsupervised learning of finite mixtures using entropy regularization and its application to image segmentation , '' in _ ieee conference on computer vision and pattern recognition_.1em plus 0.5em minus 0.4emieee , 2008 , pp .",
    "s.  chen , c .- y .",
    "zhang , and k.  song , `` recognizing short coding sequences of prokaryotic genome using a novel iteratively adaptive sparse partial least squares algorithm , '' _ biology direct _ , vol .  8 , no .  1 , 2013 .",
    "z.  lei , k.  chen , h.  li , h.  liu , and a.  guo , `` the gaba system regulates the sparse coding of odors in the mushroom bodies of drosophila , '' _ biochemical and biophysical research communications _ , vol .",
    "436 , no .  1 ,",
    "pp . 3540 , 2013 .",
    "c.  wang , s.  yan , l.  zhang , and h .- j .",
    "zhang , `` multi - label sparse coding for automatic image annotation , '' in _ 2009 ieee computer society conference on computer vision and pattern recognition workshops , cvpr workshops 2009 _ , 2009 , pp .",
    "16431650 .",
    "j.  yang , k.  yu , y.  gong , and t.  huang , `` linear spatial pyramid matching using sparse coding for image classification , '' in _ 2009 ieee computer society conference on computer vision and pattern recognition workshops , cvpr workshops 2009 _ , 2009 , pp . 17941801 .",
    "z.  lu and y.  peng , `` exhaustive and efficient constraint propagation : a graph - based learning approach and its applications , '' _ international journal of computer vision _ , vol .",
    "103 , no .  3 , pp .",
    "306325 , 2013 .",
    "z.  lu and h.  h. ip , `` generalized relevance models for automatic image annotation , '' in _ advances in multimedia information processing - pcm 2009_.1em plus 0.5em minus 0.4emspringer berlin heidelberg , 2009 , pp .",
    "245255 .",
    "l.  shang , w.  huai , g.  dai , j.  chen , and j.  du , `` palmprint recognition using 2d - gabor wavelet based sparse coding and rbpnn classifier , '' _ lecture notes in computer science ( including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics ) _ , vol .",
    "6064 lncs , no .",
    "part 2 , pp . 112119 , 2010 .",
    "b.  ghanem and n.  ahuja , `` sparse coding of linear dynamical systems with an application to dynamic texture recognition , '' in _ proceedings - international conference on pattern recognition _ , 2010 , pp",
    ". 987990 .",
    "z.  lu , y.  peng , and h.  h .- s .",
    "ip , `` spectral learning of latent semantics for action recognition , '' in _ ieee international conference oncomputer vision ( iccv)_.1em plus 0.5em minus 0.4emieee , 2011 , pp .",
    "15031510 .",
    "g.  sivaram , s.  nemala , m.  elhilali , t.  tran , and h.  hermansky , `` sparse coding for speech recognition , '' in _ icassp , ieee international conference on acoustics , speech and signal processing - proceedings _ , 2010 , pp .",
    "43464349 .",
    "z.  lu and y.  peng , `` image annotation by semantic sparse recoding of visual content , '' in _ proceedings of the 20th acm international conference on multimedia_.1em plus 0.5em minus 0.4emacm , 2012 , pp .",
    "499508 .",
    "z.  lu , h.  h. ip , and q.  he , `` context - based multi - label image annotation , '' in _ proceedings of the acm international conference on image and video retrieval_.1em plus 0.5em minus 0.4emacm , 2009 , p.  30 .",
    "m.  yang , l.  zhang , j.  yang , and d.  zhang , `` robust sparse coding for face recognition , '' in _ proceedings of the ieee computer society conference on computer vision and pattern recognition _ , 2011 ,",
    ". 625632 .    j.  mairal , f.  bach , j.  ponce , g.  sapiro , and a.  zisserman , `` supervised dictionary learning , '' in _ advances in neural information processing systems 21 - proceedings of the 2008 conference _ , 2009 , pp .",
    "10331040 .",
    "m.  bilenko , s.  basu , and r.  mooney , `` integrating constraints and metric learning in semi - supervised clustering , '' in _ proceedings , twenty - first international conference on machine learning , icml 2004 _ , 2004 , pp . 8188 .",
    "j.  y. ching , a.  k. wong , and k.  c. chan , `` class - dependent discretization for inductive learning from continuous and mixed - mode data , '' _ ieee transactions on pattern analysis and machine intelligence _ , vol .",
    "17 , no .  7 , pp . 641651 , 1995 .    k.  yu , j.  bi , and v.  tresp , `` active learning via transductive experimental design , '' in _",
    "icml 2006 - proceedings of the 23rd international conference on machine learning _ ,",
    "2006 , 2006 , pp . 10811088 .",
    "r.  he , w .- s .",
    "zheng , b .-",
    "hu , and x .- w .",
    "kong , `` nonnegative sparse coding for discriminative semi - supervised learning , '' in _ proceedings of the ieee computer society conference on computer vision and pattern recognition _ , 2011 ,",
    ". 28492856 .",
    "s.  yang , x.  wang , l.  yang , y.  han , and l.  jiao , `` semi - supervised action recognition in video via labeled kernel sparse coding and sparse l1 graph , '' _ pattern recognition letters _ , vol .",
    "33 , no .",
    "1951  6 , 2012/10/15 .",
    "j.  wang , f.  wang , c.  zhang , h.  c. shen , and l.  quan , `` linear neighborhood propagation and its applications , '' _ pattern analysis and machine intelligence , ieee transactions on _ , vol .",
    "31 , no .  9 , pp . 16001615 , 2009 .",
    "e.  simpson , m.  mahendroo , g.  means , m.  kilgore , m.  hinshelwood , s.  graham - lorence , b.  amarneh , y.  ito , c.  fisher , m.  michael , c.  mendelson , and s.  bulun , `` aromatase cytochrome p450 , the enzyme responsible for estrogen biosynthesis , '' _ endocrine reviews _ , vol .",
    "15 , no .  3 , pp .",
    "342355 , 1994 .    c.  baj - rossi , t.  rezzonico  jost , a.  cavallini , f.  grassi , g.  de  micheli , and s.  carrara , `` continuous monitoring of naproxen by a cytochrome p450-based electrochemical sensor , '' _ biosensors and bioelectronics _ , vol .",
    "53 , pp . 283287 , 2014 .",
    "m.  rasmussen , c.  klausen , and b.  ekstrand , `` regulation of cytochrome p450 mrna expression in primary porcine hepatocytes by selected secondary plant metabolites from chicory ( cichorium intybus l. ) , '' _ food chemistry _",
    "146 , pp . 255263 , 2014 .",
    "faulon , m.  misra , s.  martin , k.  sale , and r.  sapra , `` genome scale enzyme - metabolite and drug - target interaction predictions using the signature molecular descriptor , '' _ bioinformatics _ , vol .",
    "24 , no .  2 ,",
    "pp . 225233 , 2008 .",
    "faulon , c.  churchwell , and d.  visco  jr .",
    ", `` the signature molecular descriptor .",
    "2 . enumerating molecules from their extended valence sequences , '' _ journal of chemical information and computer sciences _ , vol .",
    "43 , no .  3 , pp .",
    "721734 , 2003 .",
    "faulon , d.  visco  jr . , and r.  pophale , `` the signature molecular descriptor .",
    "1 . using extended valence sequences in qsar and qspr studies , '' _ journal of chemical information and computer sciences _ , vol .",
    "43 , no .  3 , pp .",
    "707720 , 2003 .",
    "d.  rojatkar , k.  chinchkhede , and g.  sarate , `` handwritten devnagari consonants recognition using mlpnn with five fold cross validation , '' in _ proceedings of ieee international conference on circuit , power and computing technologies , iccpct 2013 _ , 2013 , pp .",
    "12221226 .",
    "s.  gao , i.  w. tsang , l .- t .",
    "chia , and p.  zhao , `` local features are not lonely  laplacian sparse coding for image classification , '' in _ computer vision and pattern recognition ( cvpr ) , 2010 ieee conference on_.1em plus 0.5em minus 0.4emieee , 2010 , pp .",
    "35553561 .",
    "y.  tian , b.  zhang , e.  p. hoffman , r.  clarke , z.  zhang , i .-",
    "m . shih , j.  xuan , d.  m. herrington , and y.  wang , `` kddn : an open - source cytoscape app for constructing differential dependency networks with significant rewiring , '' _ bioinformatics _ , 2014 ."
  ],
  "abstract_text": [
    "<S> sparse coding approximates the data sample as a sparse linear combination of some basic codewords and uses the sparse codes as new presentations . in this paper , we investigate learning discriminative sparse codes by sparse coding in a semi - supervised manner , where only a few training samples are labeled . by using the manifold structure spanned by the data set of both labeled and unlabeled samples and the constraints provided by the labels of the labeled samples , we learn the variable class labels for all the samples . furthermore , to improve the discriminative ability of the learned sparse codes , we assume that the class labels could be predicted from the sparse codes directly using a linear classifier . by solving the codebook , sparse codes , class labels and classifier parameters simultaneously in a unified objective function , </S>",
    "<S> we develop a semi - supervised sparse coding algorithm . </S>",
    "<S> experiments on two real - world pattern recognition problems demonstrate the advantage of the proposed methods over supervised sparse coding methods on partially labeled data sets . </S>"
  ]
}