{
  "article_text": [
    "the _ planck _ satellite mission , launched in may 2009 , is measuring the polarization signal of the cmb in seven channels over the frequency range 30 - 353 ghz .",
    "it is now in normal operation and performing as expected @xcite . with a sensitivity of @xmath11 in polarization and an angular resolution down to @xmath12 @xcite , _ planck _ will produce polarization data which offer a multitude of opportunities including : possible recovery of inflationary b - modes at large scales , tighter constraints on the parameters describing the epoch of reionization , and greater understanding of the polarized nature of galactic foregrounds ( particularly dust , which dominates at the high frequency channels of _ planck _ ) .    in this paper , we adapt existing foreground separation tools to explore the prospects of estimating low - resolution polarized cmb maps from the _ planck _ mission .",
    "we separate the polarization from the temperature signal and focus on the problem of estimating the q and u component maps . from these estimated cmb maps and their covariance",
    ", we then use a low-@xmath13 pixel likelihood estimator to get estimates on the optical depth to reionization , @xmath1 , and the tensor - to - scalar ratio , @xmath2 , and to construct low-@xmath13 conditional likelihood slices of @xmath3 and @xmath4 .    for an all - sky experiment like _ planck _ , component separation of the polarization signal",
    "will be much more difficult than for the temperature counterpart , in part , because the ratio of the foreground signal to cmb signal is higher .",
    "there is a large body of work on cmb foreground subtraction for temperature measurements ( see reviews by @xcite and @xcite ) , and a growing body of work on the problem of polarized foreground subtraction , including applications to the _ wmap _ data and _ planck _ simulated data .",
    "the five - year analysis of the _ wmap _ data included results from two approaches to component separation : template cleaning @xcite and a bayesian parameter estimation method @xcite which we use in this paper . for _ planck _",
    ", @xcite compare an internal linear combination technique with their template - fitting scheme to assess the impact of foregrounds on b - mode detection at low @xmath13s .",
    "@xcite provide constraints on @xmath2 with _ planck _ data and future cmb experiments using a component separation method based on smica and @xcite describe a correlated component analysis for _ planck_. finally , forecasts on polarization foreground removal for a future cmbpol mission have been presented in @xcite .    in  [ sec : bayes_est ] , we provide details of our bayesian parameter technique , gibbs sampling , and our model of the data . in  [ sec : psm ] , we describe the experimental specifications of _ planck _ and our simulated maps .",
    "we present the estimated maps from the parametric technique , the estimated @xmath1 and @xmath2 likelihoods , and the low-@xmath13 conditional likelihood slices on @xmath3 and @xmath4 in  [ sec : results ] .",
    "in the bayesian parameter estimation method of foreground separation , the emission models of the cmb and foregrounds are parametrized based on our understanding of their frequency dependence .",
    "we then use a sampling method to estimate the marginalized cmb q and u maps ( and additionally the marginalized foreground maps ) in every pixel . in this analysis , we use healpix @xcite @xmath14 maps containing @xmath15 pixels .",
    "we use two different implementations of gibbs sampling to estimate the maps and compare their results in the case of a simplified diagonal noise model .",
    "the first is a code called _ commander _ which we review in  [ sec : commander ] and refer the reader to @xcite and @xcite for more details .",
    "the second is a code called _ galclean _ which we review in  [ sec : galclean ] and refer the reader to @xcite for more details .",
    "consider the simple case in which the data model is given by @xmath16 where @xmath17 are the observed data , consisting of q and u polarization maps observed by _ planck _",
    ", @xmath18 is the sky signal , and @xmath19 is the instrument noise .",
    "we wish to estimate the sky signal @xmath18 which is achieved by computing the posterior distribution @xmath20 . by bayes theorem ,",
    "this distribution can be written as @xmath21 with prior distribution @xmath22 and gaussian likelihood @xmath23 with @xmath24 and a normalization term c. it is straightforward to generalize to multi - frequency data . in the case",
    "that the noise @xmath25 is uncorrelated between channels , the likelihood can be written as @xmath26^t\\mathbf{n}_{\\nu}^{-1}[\\mathbf{d}_{\\nu}-\\mathbf{s}_{\\nu } ] \\label{eq : likelihood}\\ ] ] where @xmath27 is the observed sky map at frequency @xmath28 and @xmath29 is its covariance matrix .",
    "we define the parametric model for the total sky signal in antenna temperature for our three - component model ( @xmath30 for cmb , @xmath31 for synchrotron emission , and @xmath32 for thermal dust emission ) as @xmath33 where @xmath34 are amplitude vectors of length @xmath35 and @xmath36 are diagonal coefficient matrices of side @xmath35 at each frequency .",
    "given that the cmb radiation is blackbody and assuming that the spectral index of the galactic components do not vary over our frequency range , the coefficients are given by @xmath37   \\\\ \\label{eq : alpha3 } \\bm\\alpha_3(\\nu,\\beta_3 ) & = \\rm{diag}[(\\nu/\\nu_{353})^{\\bm\\beta_3}].\\end{aligned}\\ ] ] here we have defined the function @xmath38 which converts the cmb signal @xmath39 from thermodynamic to antenna temperature , and the two spectral index vectors @xmath40 and @xmath41 for synchrotron and dust , respectively .",
    "we also set the pivot frequencies to 30 ghz and 353 ghz .",
    "though the spectral indices for q and u in a given pixel are expected to be similar ( following from the assumption that the polarization angle does not change with frequency ) , we allow the option for the indices to be sampled independently for q and for u. thus , our model is completely described by @xmath42 amplitude parameters @xmath43 and @xmath44 spectral index parameters @xmath45 .",
    "we impose a flat prior on amplitude - type parameters and gaussian priors on the spectral index parameters of @xmath46 for synchrotron and @xmath47 for dust .",
    "the priors we have chosen have central value and standard deviation at approximately the average and range of values typically observed and simulated ( see , for example , @xcite for further discussion ) .",
    "with our data model and priors defined , our aim is to estimate the joint cmb - foreground posterior @xmath48 from which we can then obtain the marginalized distribution for the cmb amplitude vector as @xmath49 and similarly for the other model parameters .    for the multivariate problem that we are considering",
    ", gibbs sampling draws from the joint distribution by sampling each parameter conditionally as follows @xmath50 in the next two sections , we compare and contrast the different methods that _ commander _ and _ galclean _ implement to sample the amplitude - type and spectral index parameters .",
    "_ commander _ is a flexible code for joint component separation and cmb power spectrum estimation @xcite . _",
    "commander _ has typically been used in its full implementation , in which the parametric model of the total sky signal is sampled jointly with the cmb power spectrum .",
    "we describe here how to use _ commander _ to do sampling of the sky signal only .",
    "the theory of gibbs sampling allows the joint density @xmath51 to be sampled by alternately sampling from the two conditional densities @xmath52 the conditional sky signal distribution can be written as @xmath53 where @xmath54 and @xmath25 are the signal and noise covariance matrices .",
    "since we are only interested in doing only the component separation part , this is akin to ignoring the the @xmath55 term in the above algorithm , giving the following distribution @xmath56 for our purposes of low-@xmath13 component separation , @xmath1 and @xmath2 estimation , and evaluation of low-@xmath13 @xmath3 and @xmath4 conditional slices , it is arguably optimal to  switch off \" the @xmath57 sampling step in _ commander_. this is because the @xmath57 sampling step is time - consuming and ( in the case that the cmb map has approximately gaussian uncertainties after marginalizing over foregrounds ) equivalent estimates on the spectra can be obtained quickly by using an exact pixel likelihood given the estimated cmb map and cmb covariance .    alternatively , and without dropping the @xmath55 term in eq .",
    "[ eq : cond_dist ] , we can say that we are conditioning on @xmath58 for the correlated cmb component , while simultaneously allowing for an uncorrelated cmb component with a separate value in each pixel .",
    "the net result is that the cmb amplitudes are sampled in the same way as the foreground amplitudes , and the @xmath57 sampling step has been omitted entirely .",
    "the conditional distribution @xmath59 for fixed @xmath60 is a @xmath61-dimensional gaussian from which it is straightforward to sample .",
    "first , we define the data model as @xmath62 the conditional distribution is @xmath63^t \\mathbf{n}^{-1}_{\\nu}[\\mathbf{d}_{\\nu}-\\sum\\limits_k \\bm\\alpha_k(\\nu;\\beta_k)\\mathbf{a}_k]}\\\\     & \\propto e^{-\\frac{1}{2}(\\mathbf{a}-\\hat{\\mathbf{a}})^t \\mathbf{f}^{-1}(\\mathbf{a}-\\hat{\\mathbf{a}})}\\end{aligned}\\ ] ] where @xmath64 is the wiener - filter mean given by @xmath65 with elements @xmath66    the sampling algorithm that _ commander _ employs solves the set of linear equations @xmath67 where @xmath68 is the wiener - filter mean plus random fluctuations ( given by white noise maps @xmath69 ) @xmath70 the solution vector @xmath71 has mean @xmath72 and covariance matrix @xmath73 , and the next amplitude sample is given by @xmath74 .      for fixed amplitude ,",
    "the spectral index sampler in _ commander _ is a standard inversion sampler .",
    "this algorithm first maps out the conditional probability distribution @xmath75 by evaluating the likelihood ( given by eq .",
    "[ eq : likelihood ] ) at 500 points between the upper and lower @xmath76 limits and then computes the corresponding cumulative probability distribution @xmath77 .",
    "next , a random number @xmath78 is drawn from the uniform distribution @xmath79 $ ] .",
    "thus , the sample from @xmath75 is given by @xmath80 .",
    "in commander , eq .  [ eq : beta_samp ] can be iterated more than once in each main gibbs loop as an inexpensive way to reduce correlations between consecutive samples .",
    "we allow three spectral index iterations for each main gibbs iteration .",
    "_ galclean _ , the gibbs sampling algorithm described in @xcite , solves the same set of equations outlined in ",
    "[ sec : bayes_est ] and defines the same parametric model given in eqs .",
    "[ eq : model]-[eq : alpha3 ] .",
    "the method that _ galclean _ uses to do the amplitude sampling , described below in  [ sec : gal_ampsamp ] is similar to that of _ commander _ , with a different technique for drawing a sample . the main difference between _",
    "galclean _ and _ commander _ is in the spectral index sampling , where _ galclean _ implements a metropolis - hastings algorithm as outlined in  [ sec : gal_specsamp ] . given enough time to converge",
    ", both methods should give the same estimates .",
    "_ galclean _ solves the set of linear equations given by @xmath81 , where @xmath73 and @xmath82 are given by eqs .",
    "[ eq : fisher ] and [ eq : x ] , for the wiener - filtered mean @xmath72 .",
    "the _ galclean _",
    "code then draws a gaussian sample @xmath83 and constructs the next amplitude sample as @xmath84 where @xmath85 is the lower cholesky decomposition of the fisher matrix @xmath86 .",
    "_ galclean _ uses the metropolis - hastings algorithm ( see , for example , @xcite ) to sample the index vector .",
    "briefly , a trial step @xmath87 is drawn from a gaussian distribution of width @xmath88 and centred on the current @xmath89 vector .",
    "the current and trial vectors are used to construct the current and trial posteriors @xmath90 where @xmath91 is given by eq .",
    "[ eq : likelihood ] , which includes the full noise correlation matrix . then the ratio of the trial to current posterior , @xmath2 , determines whether to move to the trial position ( with probability @xmath2 ) , or to stay at the original position ( with probability 1-@xmath2 ) .",
    "we examine the results from the _ commander _ and _ galclean _ gibbs sampling chains by forming the mean map and covariance matrix .",
    "the mean map can be found from the marginalized distribution as @xmath92 where we sum over the @xmath93 elements in the gibbs chain for the @xmath94th component of the model .",
    "we find that a typical gibbs chain takes roughly 100 iterations to  burn - in \" ( or converge to the equilibrium distribution ) when the spectral indices are initialized at random values drawn from the gaussian prior .",
    "therefore , we remove the first 100 elements in each gibbs chain before processing the samples .",
    "it should be noted that the marginalized distribution @xmath95 may not be an exact gaussian , but we assume it to be to good enough approximation .",
    "similarly , the covariance matrix for @xmath96 , is estimated by summing over chain elements , as shown here for the general noise case , for the covariance between pixel @xmath97 and @xmath98 for component @xmath94 @xmath99 for diagonal noise , the covariance in pixel @xmath97 for component @xmath100 reduces to @xmath101 note that while we found @xmath102 samples to be enough to ensure convergence of the chains for the case of diagonal noise , we expect to need an order of magnitude more samples to construct the full covariance matrix in the case of correlated noise .",
    "the product of a bayesian parametric map estimation method is both a cmb map and a covariance matrix ( which can be estimated from the marginalized posterior distribution ) and together these products can be used to place constraints on cosmological parameters .",
    "we compute the likelihood of the estimated maps , given a theoretical angular power spectrum , using the method described in @xcite .",
    "summarized here , the standard likelihood is given by @xmath103}{|s+n|^{1/2}}\\ ] ] where @xmath104 is the data vector containing the temperature , @xmath105 , and polarization maps , @xmath106 and @xmath107 , @xmath108 is the number of pixels in each map , and @xmath109 and @xmath110 are the signal and noise covariance matrices . under the assumption that noise in the temperature can be ignored  which holds at low multipoles where the signal dominates ",
    "the standard likelihood can be simplified to @xmath111}{|\\tilde{s}_p+n_p|^{1/2 } } \\\\ & \\frac{\\exp[-\\frac{1}{2}\\mathbf{t}^ts^{-1}_t \\mathbf{t}]}{|s_t|^{1/2 } } \\end{split}\\ ] ] where @xmath112 is the temperature signal matrix , @xmath113 is the new polarization data vector given by @xmath114 and @xmath115 is the signal matrix for the new polarization vector .",
    "this new form of the likelihood allows us to factorize it into the likelihood of temperature and polarization , with information about their cross - correlation preserved .",
    "the two cosmological parameters most closely linked with the large scale cmb polarization signal are the optical depth to reionization , @xmath1 , and the tensor - to - scalar ratio , @xmath2 .",
    "the signature of reionization is at @xmath116 in @xmath3 where the amplitude of the reionization signal is proportional to @xmath117 .",
    "the tensor - to - scalar ratio @xmath2 directly scales the @xmath4 power spectrum and is best probed at low @xmath13 s before @xmath4 due to lensing dominates .    by varying only the optical depth to reionization @xmath1 and the power spectrum amplitude ( such that the temperature anisotropy power at @xmath118 is held constant )",
    ", we can calculate the likelihood at each value of @xmath1 .",
    "this allows us to estimate limits on the optical depth to reionization including foreground uncertainty .",
    "similarly , we can vary only the tensor - to - scalar ratio @xmath2 , fixing all other parameters at concordance values , and calculate the likelihood at each value of @xmath2 .",
    "the standard likelihood estimator that we use here assumes that the estimated cmb map has approximately gaussian uncertainties after marginalizing over foregrounds . in practice ,",
    "marginalizing over foregrounds may induce non - gaussianities and there are several ways of addressing this issue .",
    "one option is to modify the standard likelihood to include a non - gaussian term .",
    "alternatively , the @xmath57 s can be sampled jointly with the maps in a full bayesian framework .",
    "this type of scheme is implemented in the _ commander _ code , in which the problem of sampling from the joint density @xmath51 is reduced to that of sampling from the two conditional densities @xmath119 and @xmath120 .",
    "we have already described the sampling algorithm for the first conditional distribution @xmath119 in  [ sec : amp_samp ] .",
    "the second conditional distribution , @xmath120 , reduces to @xmath121 since the data does not further constrain the power spectrum if we already know the cmb sky signal",
    ". then the distribution reads @xmath122 for which there is a simple textbook sampling algorithm detailed in @xcite .    in  [ sec :",
    "cl_comparison ] , we will compare foreground - marginalized @xmath57 estimates from our standard pixel - likelihood code with those from the _ commander _ gibbs sampler .",
    "template cleaning is an alternative method of component separation that assumes that the sky at any frequency can be modeled as a linear sum of fixed spatial templates . in the regime of perfect templates and no spatial spectral index variations",
    ", template fitting would give the optimal foreground subtraction and marginalization .",
    "we do a comparison of our results to a simple template cleaning method which is implemented in _",
    "commander_. the data model is given by @xmath123 where the first term on the right - hand side is the cmb sky signal , the second term represents the sum over @xmath110 templates @xmath124 with fixed frequency scaling @xmath125 and overall amplitude @xmath126 .",
    "this implementation of template cleaning assumes no spatial variation in the frequency scaling and is therefore limited in usefulness to cases where the spectral index variations in the data are small . however , it is a fast method that has been successfully used for the analysis of _ wmap _ data , for example .",
    "we use the _ wmap _ 23 ghz map for the low - frequency synchrotron template and the _ planck _ simulated 353 ghz map as the high - frequency dust template .",
    "this leaves six remaining channels of _ planck _ ( 30 - 217 ghz ) to be used as data for the template fitting .",
    "these templates are fitted to the data , the best fit coefficients for each component are found and the templates are subtracted from the map using these coefficients in order to obtain a clean cmb map at each frequency .",
    "these can then be optimally combined through inverse variance weighting , and the likelihood computed using the method in  [ sec : likelihood ] .",
    "this method assumes that the templates have no associated uncertainties ; methods to propagate template uncertainty have been considered in e.g. @xcite .",
    "we use a software package called the planck sky model ( psm , version 1.6.6 ) developed by the _ planck _",
    "working group 2 to generate our simulated cmb and synchrotron maps .",
    "we generate maps at the seven polarized _ planck",
    "_ frequency channels ( 30 , 44 , 70 , 100 , 143 , 217 , and 353 ghz ) . in our analysis",
    ", we do not apply beams or smoothing to the data and we leave the sky unmasked in the gibbs sampling step ; these issues should be included in a more realistic analysis .",
    "we use the psm @xmath127 option to generate realizations of the cmb .",
    "the psm simulates the cmb by feeding a set of standard @xmath128cdm cosmological parameters to camb which produces a set of corresponding @xmath57s . a gaussian random cmb temperature and polarization field",
    "is then drawn according to this spectra .",
    "we use psm model 6 for the synchrotron emission .",
    "the synchrotron q and u emission maps are given as an extrapolation in frequency of the polarized 23 ghz _",
    "wmap _ map : @xmath129 @xmath130    the model for the spectral index is taken to be model 4 of @xcite , given by @xmath131 where @xmath132 is the _ wmap _ polarization map at 23 ghz , @xmath133 is a geometrical reduction factor ( reflecting depolarization due to magnetic field structure ) , @xmath134 is the intrinsic polarization fraction from the cosmic rays energy spectrum , and @xmath135 is the 408 mhz map of @xcite . as an initial test , we generate our own simple power - law dust model , extrapolating from the predicted 94 ghz map in @xcite : @xmath136 .",
    "we set the dust spectral index to @xmath137 uniformly over the whole sky .",
    "although the observed dust emission is known to fit better to a multi - component model , it is reasonable to approximate it with a single - component model at frequencies below 300 ghz where the dust polarization is likely to be dominated by a single component .",
    "we generate diagonal white noise realizations based on the noise levels listed in table [ tab : planck_specs ] , taken from the _ planck _ bluebook @xcite , and scale the given noise levels at beam - sized pixels to the corresponding noise level at @xmath138 sized pixels .",
    "our simplified noise model contains no @xmath139-noise or other correlations , the addition of which would increase effective noise levels .",
    "._planck _ specifications .",
    "the sensitivity is specified for a pixel of area @xmath140 ( area of pixel of at healpix nside=16 , npix = 3072 ) .",
    "[ cols=\"^,^,^ \" , ]     our pixel likelihood code can be used not only to constrain parameters , but also to find the power at each multipole in the polarized power spectra .",
    "at each multipole , we compute the conditional likelihood as a function of @xmath3 and @xmath4 for @xmath141 with all other multipoles held fixed at the fiducial @xmath128cdm values , using the method described in @xcite .",
    "for example , the conditional likelihood of @xmath142 is @xmath143 .",
    "we compare the power at each multipole in the gibbs cmb map to the template - cleaned case ( described in  [ sec : template_cleaning ] ) and to the foreground - free case , shown in fig .",
    "[ fig : clee ] and fig .",
    "[ fig : clbb ] for the @xmath8 simulation . for @xmath3 ,",
    "the results are consistent between the three cases . for the @xmath4 spectra , we find that the template - cleaned conditional slices agree with the foreground - free curves as well , or better than , the gibbs slices , indicating that the more economical template - cleaning method is an effective ( and fast ) option for foreground removal in the case of low spectral index variation established in our data model . however , we argue that gibbs sampling should be used instead of , or in addtition to , template cleaning , in order to benefit from the gibbs feature that the inclusion of foreground uncertainties in the covariance matrix can be propagated to the limits on @xmath2 .",
    "this effect appears as the inflation in the gibbs @xmath4 distributions over the template distributions for the @xmath10 simulation , shown , in particular for @xmath144 and 5 in fig .",
    "[ fig : clbb_r0 ] .                in fig .",
    "[ fig : template_coeff ] , we plot the results from the _ commander _ template fitting .",
    "the data points are the best - fit template coefficients for the dust and synchrotron emission at 30 , 44 , 70 , 100 , 143 , and 217 ghz .",
    "the dashed curves show the emission , in antenna units , of the thermal dust for @xmath137 and of the synchrotron for @xmath145 .",
    "the curves are normalized to the r.m.s .",
    "values of the 23 ghz and 353 ghz template maps for synchrotron and dust , respectively .          in  [ sec : likelihood ]",
    "we discussed a potential issue with our standard pixel likelihood code in the case that the marginalized distributions @xmath146 contain non - gaussianities .",
    "we investigate our cmb marginal posteriors and do find a small level of non - gaussianity particularly in regions where the foreground signal is large .",
    "we proposed several options for addressing this issue in  [",
    "sec : likelihood ] , and in this section we show a comparison between our standard pixel - likelihood and gibbs sampled @xmath57 estimates in order to assess the level of non - gaussianity seen in the cmb marginal posteriors .",
    "we run the pixel - likelihood code to compute the conditional likelihood as a function of @xmath3 and @xmath4 for @xmath147 with all other multipoles held fixed at the fiducial @xmath128cdm values .",
    "we additionally marginalize over @xmath148 and @xmath149 when computing the @xmath3 likelihood in order to account for correlations between the tt , te , and ee components .",
    "we neglect correlations between @xmath13 values .",
    "we run the gibbs sampler ( _ commander _ ) in the mode in which the cmb power spectra is sampled simultaneously with the foreground components , as described in  [ sec : likelihood ] . in fig .",
    "[ fig : clslices ] we show slices through the @xmath57 distribution obtained from the gibbs estimator compared to the pixel likelihood .",
    "we find the estimates from the two methods to be equivalent up to small differences .",
    "the small discrepancies between the gibbs and pixel likelihood estimates are due to the pixel likelihood code using @xmath150 , compared to the higher resolution @xmath151 used for the gibbs code .",
    "another source of differences may be from @xmath152 correlations present in the gibbs samples but not in the pixel likelihood which estimates slices of @xmath57 for all other multipoles fixed .",
    "these results indicate that it is reasonable to approximate the foreground - marginalized cmb pixel amplitudes as gaussian in the pixel - based likelihood .",
    "we have investigated two independent gibbs sampling codes for polarized cmb foreground separation in the case of diagonal noise , and power law dust and synchrotron models .",
    "we have constructed the large - scale posterior cmb and foreground amplitude maps as well as the dust and synchrotron spectral index maps using the _ planck _ sky model and noise levels , and without masking the galactic plane .",
    "we explored constraints on @xmath1 and @xmath2 for our _ planck _ model and found that our bayesian algorithms produced results consistent with @xmath6 and @xmath8 at 1- and 2@xmath153 .",
    "we find @xmath154 and @xmath155 .",
    "we find a 95% cut - off limit on an @xmath156 detection at @xmath9 .",
    "while our specific predictions on @xmath157 and @xmath158 are limited by our simplified noise and data models , we have shown that the gibbs - estimated cmb maps and errors capture the additional uncertainty due to the presence and removal of foregrounds , which is then translated into an error inflation on @xmath1 and @xmath2 .",
    "there are many interesting extensions to this work that can be further explored using the tools described in this paper .",
    "more realistic data models can be considered , including two - component dust and a synchrotron curvature model .",
    "other modelling effects can be considered , such as polarized free - free emission and polarized spinning dust .",
    "mismatches between data and model should be investigated in terms of the amplification factor or biases that they impart to estimates on @xmath1 and @xmath2 .",
    "the noise model can be extended to include the full q / u noise covariances and also @xmath139 noise .",
    "the tools adapted and developed in this work can also be used to estimate parameters for small - scale versus large - scale experiments ( e.  g.  core )    we acknowledge the use of the planck sky model , developed by the component separation working group ( wg2 ) of the _ planck _ collaboration .",
    "we thank the _ wmap _ team for the use of their polarization code .",
    "this work was performed using the darwin supercomputer of the university of cambridge high performance computing service ( http://www.hpc.cam.ac.uk/ ) , provided by dell inc .",
    "using strategic research infrastructure funding from the higher education funding council for england .",
    "cd acknowledges an stfc advanced fellowship and erc grant under the fp7 .",
    "baumann d. et al .",
    "2009 , aip conf .",
    "1141:10 - 120 bersanelli , m. et al .",
    "2010 , a&a , available on line , arxiv:1001.3321 betoule , m. et al .",
    "2009 , a&a , 503 , 691 delabrouille , j. & cardoso , j .- f .",
    "2009 , lnp , 665 , 159 dunkley , j. 2005 , mnras , 356 , 925 dunkley , j. et al .",
    "2009 , apj , 701 , 1804 dunkley , j. et al .",
    "2009 , aip conference proceedings , 1141 , 222 efstathiou , g. , gratton , s. , & paci , f. 2009 , mnras , 397 , 1355 eriksen , h.k .",
    "et al . 2004 , apjs , 155 , 227 eriksen , h.k .",
    "et al . 2006 , new astron .",
    "50 , 861 eriksen , h.k .",
    "2008 , apj , 676 , 10 finkbeiner , d.  p. , davis , m. & schlegel , d.  j. 1999 , apj , 524 , 876 fraisse , a.  .a . ,",
    "2008 , arxiv:0811.3920v1 gold , b. et al .",
    "2009 , apjs , 180 , 265 haslam et al . 1982 , a&as , 47 , 1 grski , k. et al .",
    "2005 , `` the healpix primer '' ( version 2.15 ) , available at http://healpix.jpl.nasa.gov jewell , j. , levin s. , & anderson , c.h . 2004 , apj , 609 , 1 knox , l. , christensen , n. , & skordis , c. 2001 , apj , 563 , l95    larson , d. et al .",
    "2007 , apj , 656 , 653 leach , s. et al .",
    "2008 , a&a , 491 , 597 lewis , a. & bridle , s. 2002 , phys .",
    "d , 66 , 103511 mandolesi , n. et al .",
    "2010 , a&a , available on line , arxiv:1001.2657 miville - deschenes , m.   -a .",
    "2008 , arxiv:0802.3345v1 nolta , m.  r. et al .",
    "2009 , apjs , 180 , 296 page , l. et al .",
    "2007 , apjs , 170 , 335 planck collaboration 2006 , the scientific programme of _ planck _ , astro - ph/0604069 ricciardi , s. et al .",
    "2010 , arxiv:1006.2326v1 rosset , c. et al .",
    "2010 , a&a , available on line , arxiv:1004.2595 planck collaboration , 2011 , arxiv:1101.2022 wandelt , b. et al .",
    "2004 , phys .",
    "d , 70 , 083511"
  ],
  "abstract_text": [
    "<S> we use bayesian component estimation methods to examine the prospects for large - scale polarized map and cosmological parameter estimation with simulated _ planck _ data assuming simplified white noise properties . </S>",
    "<S> the sky signal is parametrized as the sum of the cmb , synchrotron emission , and thermal dust emission . </S>",
    "<S> the synchrotron and dust components are modelled as power - laws , with a spatially varying spectral index for synchrotron and a uniform index for dust . using the gibbs sampling technique , we estimate the linear polarisation q and u posterior amplitudes of the cmb , synchrotron and dust maps as well as the two spectral indices in @xmath0 pixels . </S>",
    "<S> we use the recovered cmb map and its covariance in an exact pixel likelihood algorithm to estimate the optical depth to reionization @xmath1 , the tensor - to - scalar ratio @xmath2 , and to construct conditional likelihood slices for @xmath3 and @xmath4 . </S>",
    "<S> given our foreground model , we find @xmath5 for @xmath6 , @xmath7 for a model with @xmath8 , and a 95% upper limit of @xmath9 for @xmath10 .    </S>",
    "<S> [ firstpage ] </S>"
  ]
}