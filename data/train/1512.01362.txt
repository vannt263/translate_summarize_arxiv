{
  "article_text": [
    "decision - making and data analysis tasks are made nontrivial by the presence of missing data in a database .",
    "the decisions made by decision makers are likely to be more accurate and reliable with complete datasets than with incomplete datasets containing missing data entries .",
    "also , data analysis and data mining tasks yield more representative results and statistics when all the required data is available . as a result",
    ", there has been a lot of research interest in the domain of missing data imputation with researchers developing novel techniques to perform this task accurately and in a reasonable amount of time due to the time sensitive nature of some real life applications [ @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and @xcite ] .",
    "applications such as in medicine , manufacturing or energy that use sensors in instruments to report vital information that makes time sensitive decisions , may fail when there are missing data in the database . in such cases",
    ", it is very important to have a system capable of imputing the missing data from the failed sensors with high accuracy as quickly as possible .",
    "the imputation procedure in such cases requires the approximation of the missing value taking into account the interrelationships that exist between the values of other sensors in the system .",
    "there are several reasons that could lead to data being missing in a dataset",
    ". these could be as a result of data entry errors or respondents not answering certain questions in a survey during the data collection phase .",
    "furthermore , failure in instruments and sensors could be a reason for missing data entries .",
    "the table below depicts a database consisting of seven feature variables with the values of some of the variables missing .",
    "the variables are @xmath0 , @xmath1 , @xmath2 , @xmath3 , @xmath4 , @xmath5 and @xmath6 .",
    ".database with missing data entries [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     consider that the database in question has several records of the seven variables with some of the data entries for some variables not available .",
    "the question of interest is , can we say with some degree of certainty what the missing data entries are ? furthermore , can we introduce techniques for approximation of the missing data when correlation and interrelationships between the variables in the database are considered ?",
    "we aim to use deep learning techniques , genetic algorithms ( gas ) , maximum likelihood estimator ( mle ) and swarm intelligence ( si ) techniques to approximate the missing data in databases with the different models created catering to the different missing data mechanisms and patterns .",
    "therefore , with knowledge of the presence of interrelationships or lack thereof between feature variables in the respective datasets , one will know exactly what model is relevant to the imputation task at hand .",
    "also we plan to use fuzzy logic with deep learning techniques to perform the imputation tasks .",
    "in this section , we present details on the problem of missing data and the deep learning techniques we aim to use to solve the problem .",
    "missing data is a scenario in which some of the components of the dataset are not available for all feature variables , or may not even be defined within the problem domain in the sense that the values do not match the problem definition by either being outliers or inaccurate @xcite .",
    "this produces a variety of problems in several application domains that rely on the access to complete and quality data . as a result",
    ", techniques aimed at handling the problem have been an area of research for a while in several disciplines [ @xcite , @xcite and @xcite ] .",
    "missing data may occur in several ways in a dataset .",
    "for example , it may occur due to several participants non - response to questions in the data collection process or data entry process .",
    "there are also other situations in which missing data may occur due to failures of sensors or instruments in the data recording process for sectors that use these .",
    "the following subsubsections present the different missing data mechanisms .",
    "the way to handle missing data in a reasonable manner depends on how the data points go missing . according to @xcite ,",
    "there exist three missing data mechanisms .",
    "they are : missing completely at random ( * mcar * ) , missing at random ( * mar * ) , and a missing not at random or non - ignorable case(*mnar or ni * ) .",
    "mcar scenario arises when the chances of there being a missing data entry for a feature variable is not dependent on the feature variable itself or on any of the other feature variables in the dataset @xcite .",
    "this implies that the missing value is independent of the feature variable being considered or the other feature variables within the dataset @xcite . in table",
    "[ tab : table1 ] , the nature of the missing value in @xmath1 for row 5 is said to be mcar if the nature of this missing value does not depend on @xmath0 , @xmath2 , @xmath3 , @xmath4 , @xmath5 and @xmath6 and the variable @xmath1 itself .",
    "mar occurs if the chances of there being a missing value in a specific feature variable depends on all the other feature variables within the dataset , but not on the feature variable of interest @xcite .",
    "mar means the value for the feature variable is missing , but conditional on some other feature variable observed in the dataset , although not on the feature variable of interest @xcite . in table [ tab : table1 ] , the nature of the missing value in @xmath1 is said to be mar if the missing nature of the value depends on @xmath0 , @xmath2 , @xmath3 , @xmath4 , @xmath5 and @xmath6 but not on @xmath1 itself .",
    "the third type of missing data mechanism is the non - ignorable case .",
    "the non - ignorable case occurs when the chances of there being a missing entry in variable , @xmath1 for example , is influenced by the value of the variable @xmath1 regardless of whether or not the other variables in the dataset are altered and modified [ @xcite , @xcite ] . in this case , the pattern of missing data is not random and it is impossible to predict this missing data using the rest of the variables in the dataset .",
    "non - ignorable missing data is the most difficult to approximate and model than the other two missing data mechanisms @xcite . in table",
    "[ tab : table1 ] the nature of the missing value in @xmath1 is said to be non - ignorable if the missing value in @xmath1 depends on the variable itself and not on the other variables .",
    "there are two main missing data patterns defined by @xcite .",
    "these patterns are the arbitrary and monotone missing data patterns . in the arbitrary missing data pattern ,",
    "missing observations may occur anywhere and the ordering of the variables is of no importance as in rows 1 to 5 . in monotone missing patterns",
    ", the ordering of the variables is of importance and occurrence is not random . in this case , if we have a dataset with variables as in table [ tab : table1 ] , it is said to be a monotone missing pattern if a variable @xmath7 is observed for a particular scenario , and this implies that all the previous variables @xmath8 , where @xmath9 , are also observed for that scenario @xcite .",
    "table [ tab : table1 ] shows an arbitrary missing data pattern from rows 1 to 5 and a monotone missing data pattern from rows 6 to 9 . in table",
    "[ tab : table1 ] the missing values are random and can happen at any point in the dataset from rows 1 to 5 while it can be seen that missing values have some common order in rows 6 to 9 .",
    "this means that if the values for a variable @xmath7 are missing , so are the values for other variables @xmath10 , where @xmath11 .",
    "deep learning comprises of several algorithms in machine learning that make use of a cataract of nonlinear processing units organized into a number of layers that extract and transform features from the input data [ @xcite , @xcite ] .",
    "each of the layers use the output from the previous layer as input and a supervised or unsupervised algorithm could be used in the training or building phase . with these come applications in",
    "supervised and unsupervised problems like classification and pattern analysis respectively .",
    "it is also based on the unsupervised learning of multiple levels of features or representations of the input data whereby higher - level features are obtained from lower level features to yield a hierarchical representation of the data @xcite . by learning multiple levels of representations that depict different levels of abstraction of the data , we obtain a hierarchy of concepts .",
    "there are different types of deep learning architectures such as convolutional neural networks ( cnn ) , convolutional deep belief networks ( cdbn ) , deep neural networks ( dnn ) , deep belief networks ( dbn ) , stacked ( denoising ) auto - encoders ( sae / sdae ) and deep / stacked restricted boltzmann machines ( dbm ) .",
    "we intend to make use of dnns and saes predominantly , and the others with the exception of cnns and cdbns .",
    "dnns are commonly understood in terms of the universal approximation theorem , probabilistic inference or discrete signal processing .",
    "an artificial neural network ( ann ) with numerous hidden layers of nodes between the input layer and the output layer is known as a dnn .",
    "they are typically designed as feed forward networks and can be trained discriminatively utilizing standard back propagation with updates of the weights being done by use of stochastic gradient descent .",
    "typical choices for the activation and cost functions are the softmax and cross entropy functions for classification tasks , with sigmoid and standard error functions used for regression or prediction tasks with normalized inputs . in figures [ fig : figure1]-[fig : figure3 ] , the architectures of four deep learning techniques are depicted .",
    "figure [ fig : figure1 ] shows a dnn with eight input nodes in the input layer , three hidden layers each with nine nodes and an output layer with four nodes .",
    "the nodes from each layer are connected with those from the subsequent and preceding layers .",
    "figure [ fig : figure2 ] shows a dbn and a dbm whereby the first layer of nodes ( bottom - up ) is the input layer ( v ) with visible units representing the database feature variables and the subsequent layers of nodes are binary hidden nodes ( h ) .",
    "the arrows in the dbn indicate that the training is a top - down approach while the lack of arrows in the dbm is a result of the training being both top - down and bottom - up . in figure",
    "[ fig : figure3 ] , we see individual rbms being stacked together to form the encoder part of an autoencoder , which is transposed to yield the decoder part .",
    "the autoencoder is then fine - tuned using back propagation to modify the interconnecting weights with the aim being to minimize the network error .      in this section ,",
    "we present some of the work that has been done by researchers to address the problem of missing data . in @xcite",
    ", it is suggested that information within incomplete cases , that is , instances with missing values be used when estimating missing values .",
    "a nonparametric iterative imputation algorithm ( niia ) is proposed that leads to a root mean squared error value of at least 0.5 on the imputation of continuous values and a classification accuracy of at most 87.3% on the imputation of discrete values with varying ratios of missingness .",
    "@xcite present a multi - objective genetic algorithm approach for missing data imputation .",
    "it is observed that the results obtained outperform some of the well known missing data methods with accuracies in the 90 percentile . in @xcite ,",
    "the shell - neighbor method is applied in missing data imputation by means of the shell - neighbor imputation ( sni ) algorithm which is observed to perform better than the k - nearest neighbor imputation method in terms of imputation and classification accuracy as it takes into account the left and right nearest neighbors of the missing data as well as varying number of nearest neighbors contrary to k - nn that considers just fixed k nearest neighbors .",
    "@xcite use robust regression imputation for missing data in the presence of outliers and investigate its effectiveness .",
    "@xcite implement a hybrid genetic algorithm - neural network system to perform missing data imputation tasks with varying number of missing values within a single instance while @xcite create a hybrid k - nearest neighbor - neural network system for the same purpose . in some cases ,",
    "neural networks were used with principal component analysis ( pca ) and genetic algorithm as in @xcite , @xcite and @xcite .",
    "@xcite use a hybrid of auto - associative neural networks or autoencoders with genetic algorithm , simulated annealing and particle swarm optimization to impute missing data with high levels of accuracy in cases where just one feature variable has missing input entries .",
    "novel algorithms for missing data imputation and comparisons between existing techniques can be found in papers such as @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and @xcite .",
    "in this section , we outline the methodology used to address the problem of missing data . the approach used to design the novel imputation techniques with sae / sdae involves the following six steps which are depicted in figure [ fig : figure4 ] :    1 .",
    "train the deep neural network with a complete set of records to recall the inputs as the outputs .",
    "inputs are the dataset feature variables , for example @xmath0 to @xmath6 in table [ tab : table1 ] , and the outputs are these same feature variables as the aim is to reproduce these inputs at the output layer . for the network to be able to do this",
    ", it needs to extract information from the input data , which is captured in the updated network weights and biases .",
    "the extraction of information is done during the training phase whereby lower level features are extracted from the input data after which low - level features are extracted till high - level features are obtained yielding a hierarchical representation of the input data .",
    "the overall idea is that features are extracted from features to get as good a representation of the data as possible . in the encoder phase",
    "mentioned in the previous section , a deterministic mapping function , @xmath12 , creates a hidden representation , @xmath13 , of the input data @xmath14 .",
    "it is typically represented by an affine mapping and subsequently a nonlinearity , @xmath15 @xcite .",
    "the @xmath16 parameter comprises of the matrix of weights @xmath17 and the vector of offsets / biases @xmath18 . in the decoder phase ,",
    "@xmath13 being the hidden representation is remapped to @xmath19 which is a vector reconstruction in the input space with @xmath20 @xcite .",
    "the function @xmath21 is the decoder function which is an affine mapping deliberately ensued by a non - linearity with squashing traits that either follows the form @xmath22 or @xmath23 with the parameter set @xmath24 comprising of the transpose of the weights and biases from the encoder @xcite .",
    "2 .   obtain the objective function from step 1 as depicted in figure [ fig : figure4 ] as input to the optimization techniques .",
    "the updated weights and biases mentioned in step 1 are gotten by back propagating the error at the output layer obtained by comparing the actual output to the network output through the network .",
    "the function or equation used to compare the actual output to the network output is used as the objective function .",
    "@xmath19 from step 1 is not explained as a rigorous regeneration of @xmath14 but rather as the parameters of a distribution @xmath25 in probabilistic terms , that may yield @xmath14 with high probability @xcite .",
    "this thus leads to @xmath26 . from this",
    ", we obtain an associated reconstruction error which is to be optimized by the optimization techniques and is of the form @xmath27 .",
    "this equation could also be written as @xmath28 @xcite . for a denoising autoencoder ,",
    "the reconstruction error to be optimized is expressed as @xmath29 $ ] where @xmath30 $ ] averages over the corrupted examples @xmath31 drawn from a corruption process @xmath32 @xcite .",
    "3 .   approximate the missing data entries using the approximation techniques .",
    "mcar , mar and mnar missing data mechanisms will be considered as well as arbitrary and monotone missing data patterns .",
    "different models will be created to experiment with these and test the hypothesis . in testing the hypothesis",
    ", we use the test set of data which consist of known feature variable values @xmath8 and unknown or missing feature variable values @xmath33 as input to the trained deep learning technique .",
    "the @xmath8 values are passed as input to the network while the @xmath33 values are first estimated by the approximation techniques before being passed into the network as input .",
    "the optimal @xmath33 value is obtained when the objective function from step 2 is minimized .",
    "4 .   use the now completed database with the approximated missing values in the trained deep learning method from step 1 to observe whether or not the objective has been minimized . in this case",
    ", that will be checking if the error is minimized as we attempt to reconstruct the input .",
    "if so , the complete dataset is presented as output .",
    "6 .   if not , do step 3",
    "in this research , we are introducing novel data imputation techniques which we expect will be of benefit to the research community interested in missing data imputation .",
    "some of these are :    * with the techniques introduced , we expect to yield improved missing data imputation accuracies compared against existing methods by looking at the relative prediction accuracy , correlation coefficient , standard square error , mean and root mean squared errors and other relevant representative metrics in comparison to existing techniques .",
    "this expectation stems from the manner in which deep learning methods extract information and features from the input data . * with literature stating that deep neural networks are capable of representing and approximating more complex functions and relations than simple neural networks , we hope these techniques will be applicable in a variety of sectors regardless of the complexity of the problem with high accuracy .",
    "this will be tested against existing techniques and the aforementioned . *",
    "possible parallelization of the imputation tasks using the methods to be introduced could lead to faster imputed missing values which benefits time sensitive applications .",
    "although there are possible benefits to using the novel techniques to be introduced , there could possibly be limitations observed , for example :    * using deep neural networks could possibly lead to a lot of time being required to do the imputations and obtaining a complete dataset due to the number of parameters that need to be optimized during training and also the number of computations done during testing .",
    "the full effect of long computation times could be felt in time sensitive applications such as in medicine , finance or manufacturing . the slow computation time",
    "could be addressed by parallelizing the processes on a multicore system .",
    "each core could handle the imputation of the missing data value(s ) in different rows depending on the number of cores .",
    "also , dynamic programming could be used to speed up the computation time . * besides time being a factor , there could also be a problem of space required to do the computations . to address these two drawbacks , a complexity analysis will be done to verify the time and space complexities of the proposed methods .",
    "anything less than @xmath34 will be preferable with @xmath35 being regarded as the ideal complexity for both .",
    "in this article , we propose a new hypothesis that the use of deep learning techniques in conjunction with swarm intelligence , genetic algorithms and maximum likelihood estimator methods will lead to better imputations due to the fact that a hierarchical representation of the input data is obtained as higher level features are further extracted from lower level features in deep learning methods .",
    "this hypothesis is investigated by taking into account a comparison between the techniques to be introduced and the existing methods like neural networks with genetic algorithm , auto - associative neural network with genetic algorithm , k - nearest neighbor with neural networks , neural networks with principal component analysis and genetic algorithm and so on .",
    "the main motivation behind this hypothesis is the need to provide datasets with highly representative and accurate feature values from which trustworthy decisions and data analytics and statistics will emerge .",
    "abdella , m. and marwala , t. the use of genetic algorithms and neural networks to approximate missing data in database . _ computational cybernetics , 2005 .",
    "iccc 2005 .",
    "ieee 3rd international conference on_. ieee .",
    "pp.207 - 212 .",
    "2005 .",
    "arel , i. , rose , d. c. and karnowski , t. p. deep machine learning - a new frontier in artificial intelligence research [ research frontier ] . _ computational intelligence magazine , ieee_. ieee .",
    "5(4):13 - 18",
    ". 2010 .",
    "aydilek , i. b. and arslan , a. a novel hybrid approach to estimating missing values in databases using k - nearest neighbors and neural networks .",
    "_ international journal of innovative computing , information and control_. 7(8):4705 - 4717 .",
    "2012 .",
    "deng , l. , li , j. , huang , j .-",
    ", yao , k. , yu , d. , seide , f. , seltzer , m. , zweig , g. , he , x. , williams , j. and others .",
    "recent advances in deep learning for speech research at microsoft .",
    "_ acoustics , speech and signal processing ( icassp ) , 2013 ieee international conference on_. ieee . pp.8604 - 8608 . 2013 .",
    "donders , a. r. t. , van der heijden , g. j. , stijnen , t. , moons , k. g. review : a gentle introduction to imputation of missing values .",
    "_ journal of clinical epidemiology_. elsevier .",
    "59(10):1087 - 1091 .",
    "erhan , d. , bengio , y. , courville , a. , manzagol , p .- a .",
    ", vincent , p. and bengio , s. why does unsupervised pre - training help deep learning ? _ the journal of machine learning research_. jmlr .",
    "( 11):625 - 660 . 2010 .",
    "jerez , j. m. , molina , i. , garca - laencina , p. j. , alba , e. , ribelles , n. , martn , m. and franco , l. missing data imputation using statistical and machine learning methods in a real breast cancer problem . _ artificial intelligence in medicine_. elsevier . 50(2):105 - 115 .",
    "kalaycioglu , o. , copas , a. , king , m. and omar , r. z. a comparison of multiple - imputation methods for handling missing data in repeated measurements observational studies . _",
    "journal of the royal statistical society : series a ( statistics in society ) _ , wiley online library .",
    "2015 .",
    "lee , k. j. and carlin , j. b. multiple imputation for missing data : fully conditional specification versus multivariate normal imputation . _ american journal of epidemiology_. oxford univ press .",
    "171(5):624 - 632 .",
    "leke , c. , twala , b. and marwala , t. modeling of missing data prediction : computational intelligence and optimization algorithms .",
    "_ systems , man and cybernetics ( smc ) , 2014 ieee international conference on_. ieee . pp.1400 - 1404",
    ". 2014 .",
    "liew , a. w .- c . , law , n .- f . and",
    "yan , h. missing value imputation for gene expression data : computational techniques to recover missing data from available information .",
    "_ briefings in bioinformatics_. oxford univ press . 12(5):498 - 513 .",
    "2011 .",
    "mistry , f. j. , nelwamondo , f. v. and marwala , t. missing data estimation using principle component analysis and autoassociative neural networks . _ journal of systemics , cybernatics and informatics_. 7(3):72 - 79 . 2009 .",
    "mohamed , a. k. , nelwamondo , f. v. , marwala , t. estimating missing data using neural network techniques , principal component analysis and genetic algorithms .",
    "_ proceedings of the eighteenth annual symposium of the pattern recognition association of south africa_. 2007 .",
    "myers , t. a. goodbye , listwise deletion : presenting hot deck imputation as an easy and effective tool for handling missing data .",
    "_ communication methods and measures_. taylor & francis . 5(4):297 - 310 .",
    "2011 .",
    "rana , s. , john , a. h. , midi , h. , and imon , a. robust regression imputation for missing data in the presence of outliers .",
    "_ far east journal of mathematical sciences_. pushpa publishing house . 97(2):183 .",
    "rubin , donald b. multiple imputations in sample surveys - a phenomenological bayesian approach to nonresponse .",
    "_ proceedings of the survey research methods section of the american statistical association_. american statistical association .",
    "1:20 - 34 ."
  ],
  "abstract_text": [
    "<S> in the last couple of decades , there has been major advancements in the domain of missing data imputation . </S>",
    "<S> the techniques in the domain include amongst others : expectation maximization , neural networks with evolutionary algorithms or optimization techniques and k - nearest neighbor approaches to solve the problem . </S>",
    "<S> the presence of missing data entries in databases render the tasks of decision - making and data analysis nontrivial . as a result </S>",
    "<S> this area has attracted a lot of research interest with the aim being to yield accurate and time efficient and sensitive missing data imputation techniques especially when time sensitive applications are concerned like power plants and winding processes . in this article , considering arbitrary and monotone missing data patterns , we hypothesize that the use of deep neural networks built using autoencoders and denoising autoencoders in conjunction with genetic algorithms , swarm intelligence and maximum likelihood estimator methods as novel data imputation techniques will lead to better imputed values than existing techniques . </S>",
    "<S> also considered are the missing at random , missing completely at random and missing not at random missing data mechanisms . </S>",
    "<S> we also intend to use fuzzy logic in tandem with deep neural networks to perform the missing data imputation tasks , as well as different building blocks for the deep neural networks like stacked restricted boltzmann machines and deep belief networks to test our hypothesis . </S>",
    "<S> the motivation behind this article is the need for missing data imputation techniques that lead to better imputed values than existing methods with higher accuracies and lower errors .    </S>",
    "<S> deep neural network , stacked autoencoder , stacked denoising autoencoder , stacked restricted boltzmann machine , swarm intelligence , genetic algorithms , maximum likelihood estimator , fuzzy logic , missing data </S>"
  ]
}