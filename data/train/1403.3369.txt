{
  "article_text": [
    "[ [ scientific - context . ] ] scientific context .",
    "+ + + + + + + + + + + + + + + + + + +    research on brains and cognition unfolds in two directions . _",
    "top - down _ oriented research starts from the `` higher '' levels of cognitive performance , like rational reasoning , conceptual knowledge representation , command of language .",
    "these phenomena are typically described in symbolic formalisms developed in mathematical logic , artificial intelligence ( ai ) , computer science and linguistics . in the _ bottom - up _ direction ,",
    "one departs from `` low - level '' sensor data processing and motor control , using the analytical tools offered by dynamical systems theory , signal processing and control theory , statistics and information theory .",
    "the human brain obviously has found a way to implement high - level logical reasoning on the basis of low - level neuro - dynamical processes . how this is possible , and how the top - down and bottom - up research directions can be united",
    ", has largely remained an open question despite long - standing efforts in neural networks research and computational neuroscience @xcite , machine learning @xcite , robotics @xcite , artificial intelligence @xcite , dynamical systems modeling of cognitive processes @xcite , cognitive science and linguistics @xcite , or cognitive neuroscience @xcite .",
    "[ [ summary - of - contribution . ] ] summary of contribution .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    here i establish a fresh view on the neuro - symbolic integration problem .",
    "i show how dynamical neural activation patterns can be characterized by certain neural filters which i call _ conceptors_. conceptors derive naturally from the following key observation .",
    "when a recurrent neural network ( rnn ) is actively generating , or is passively being driven by different dynamical patterns ( say @xmath0 ) , its neural states populate different regions @xmath1 of neural state space .",
    "these regions are characteristic of the respective patterns .",
    "for these regions , neural filters @xmath2 ( the conceptors ) can be incrementally learnt . a conceptor @xmath3 representing a pattern @xmath4",
    "can then be invoked after learning to constrain the neural dynamics to the state region @xmath5 , and the network will select and re - generate pattern @xmath4 .",
    "learnt conceptors can be blended , combined by boolean operations , specialized or abstracted in various ways , yielding novel patterns on the fly .",
    "conceptors can be economically represented by single neurons ( addressing patterns by neurons , leading to explicit command over pattern generation ) , or they may be constituted spontaneously upon the presentation of cue patterns ( content - addressing , leading to pattern imitation ) .",
    "the logical operations on conceptors admit a rigorous semantical interpretation ; conceptors can be arranged in conceptual hierarchies which are structured like semantic networks known from artificial intelligence .",
    "conceptors can be economically implemented by single neurons ( addressing patterns by neurons , leading to explicit command over pattern generation ) , or they may self - organize spontaneously and quickly upon the presentation of cue patterns ( content - addressing , leading to pattern imitation ) .",
    "conceptors can also be employed to `` allocate free memory space '' when new patterns are learnt and stored in long - term memory , enabling incremental life - long learning without the danger of freshly learnt patterns disrupting already acquired ones .",
    "conceptors are robust against neural noise and parameter variations .",
    "the basic mechanisms are generic and can be realized in any kind of dynamical neural network .",
    "all taken together , conceptors offer a principled , transparent , and computationally efficient account of how neural dynamics can self - organize in conceptual structures .    [ [ going - bottom - up - from - neural - dynamics - to - conceptors . ] ] going bottom - up : from neural dynamics to conceptors .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the neural model system in this report are standard recurrent neural networks ( rnns , figure [ fig1main ] * a * ) whose dynamics is mathematically described be the state update equations @xmath6 time here progresses in unit steps @xmath7 .",
    "the network consists of @xmath8 neurons ( typically in the order of a hundred in this report ) , whose activations @xmath9 at time @xmath10 are collected in an @xmath8-dimensional _ state vector _ @xmath11 .",
    "the neurons are linked by random synaptic connections , whose strengths are collected in a _ weight matrix _ @xmath12 of size @xmath13 .",
    "an input signal @xmath14 is fed to the network through synaptic input connections assembled in the _ input weight",
    "_ matrix @xmath15 .",
    "the `` s - shaped '' function @xmath16 squashes the neuronal activation values into a range between @xmath17 and @xmath18 .",
    "the second equation specifies that an _ ouput signal _",
    "@xmath19 can be read from the network activation state @xmath11 by means of _ output weights _ @xmath20 .",
    "these weights are pre - computed such that the output signal @xmath19 just repeats the input signal @xmath14 .",
    "the output signal plays no functional role in what follows ; it merely serves as a convenient 1-dimensional observer of the high - dimensional network dynamics .    the network - internal neuron - to - neuron connections @xmath12 are created at random .",
    "this will lead to the existence of cyclic ( `` recurrent '' ) connection pathways inside the network .",
    "neural activation can reverberate inside the network along these cyclic pathways .",
    "the network therefore can autonomously generate complex neurodynamical patterns even when it receives no input .",
    "following the terminology of the _ reservoir computing _",
    "@xcite , i refer to such randomly connected neural networks as _",
    "reservoirs_.     ( black thin line ) and conceptor - controlled output @xmath19 ( bold light gray ) .",
    "second column : 20 timesteps of traces @xmath21 of two randomly picked reservoir neurons .",
    "third column : the singular values @xmath22 of the reservoir state correlation matrix @xmath23 in logarithmic scale .",
    "last column : the singular values @xmath24 of the conceptors @xmath25 in linear plotting scale . *",
    "c. * from pattern to conceptor . left : plots of value pairs @xmath26 ( dots ) of the two neurons shown in first row of * b * and the resulting ellipse with axis lengths @xmath27 .",
    "right : from @xmath23 ( thin light gray ) to conceptor @xmath25 ( bold dark gray ) by normalizing axis lengths @xmath27 to @xmath28 .",
    ", width=100 ]    for the sake of introducing conceptors by way of an example , consider a reservoir with @xmath29 neurons .",
    "i drive this system with a simple sinewave input @xmath14 ( first panel in first row in fig .",
    "[ fig1main ] * b * ) .",
    "the reservoir becomes entrained to this input , each neuron showing individual variations thereof ( fig .",
    "[ fig1main ] * b * second panel ) .",
    "the resulting reservoir state sequence @xmath30 can be represented as a cloud of points in the 100-dimensional reservoir state space .",
    "the dots in the first panel of fig .",
    "[ fig1main ] * c * show a 2-dimensional projection of this point cloud . by a statistical method known as principal component analysis , the shape of this point cloud can be captured by an @xmath8-dimensional ellipsoid whose main axes point in the main scattering directions of the point cloud .",
    "this ellipsoid is a geometrical representation of the _ correlation matrix _",
    "@xmath23 of the state points .",
    "the lengths @xmath31 of the ellipsoid axes are known as the _ singular values _ of @xmath23 .",
    "the directions and lengths of these axes provide a succinct characterization of the geometry of the state point cloud .",
    "the @xmath32 lengths @xmath22 resulting in this example are log - plotted in fig .",
    "[ fig1main ] * b * , third column , revealing an exponential fall - off in this case .    as a next step ,",
    "these lengths @xmath22 are normalized to become @xmath33 , where @xmath34 is a design parameter that i call _ aperture_. this normalization ensures that all @xmath24 are not larger than 1 ( last column in fig .",
    "[ fig1main ] * b * ) .",
    "a new ellipsoid is obtained ( fig .",
    "[ fig1main ] * c * right ) which is located inside the unit sphere .",
    "the normalized ellipsoid can be described by a @xmath8-dimensional matrix @xmath25 , which i call a _ conceptor _ matrix .",
    "@xmath25 can be directly expressed in terms of @xmath23 by @xmath35 , where @xmath36 is the identity matrix .    when a different driving pattern @xmath37 is used , the shape of the state point cloud , and subsequently the conceptor matrix @xmath25 , will be characteristically different . in the example",
    ", i drove the reservoir with four patterns @xmath38  @xmath39 ( rows in fig",
    ".  [ fig1main ] * b * ) .",
    "the first two patterns were sines of slightly different frequencies , the last two patterns were minor variations of a 5-periodic random pattern .",
    "the conceptors derived from the two sine patterns differ considerably from the conceptors induced by the two 5-periodic patterns ( last column in fig .",
    "[ fig1main]*b * ) . within each of these two pairs ,",
    "the conceptor differences are too small to become visible in the plots .",
    "there is an instructive alternative way to define conceptors . given a sequence of reservoir states @xmath40 , the conceptor @xmath25 which characterizes this state point cloud is the unique matrix which minimizes the cost function @xmath41 , where @xmath42 is the sum of all squared matrix entries .",
    "the first term in this cost would become minimal if @xmath25 were the identity map , the second term would become minimal if @xmath25 would be the all - zero map .",
    "the aperture @xmath43 strikes a balance between these two competing cost components . for increasing apertures",
    ", @xmath25 will tend toward the identity matrix @xmath36 ; for shrinking apertures it will come out closer to the zero matrix . in the terminology of machine learning",
    ", @xmath25 is hereby defined as a _",
    "regularized identity map_. the explicit solution to this minimization problem is again given by the formula @xmath44 .",
    "summing up : if a reservoir is driven by a pattern @xmath14 , a conceptor matrix @xmath25 can be obtained from the driven reservoir states @xmath11 as the regularized identity map on these states .",
    "@xmath25 can be likewise seen as a normalized ellipsoid characterization of the shape of the @xmath11 point cloud .",
    "i write @xmath45 to denote a conceptor derived from a pattern @xmath37 using aperture @xmath43 , or @xmath46 to denote that @xmath25 was obtained from a state correlation matrix @xmath23 .",
    "[ [ loading - a - reservoir . ] ] loading a reservoir .",
    "+ + + + + + + + + + + + + + + + + + + +    with the aid of conceptors a reservoir can re - generate a number of different patterns @xmath47 that it has previously been driven with .",
    "for this to work , these patterns have to be learnt by the reservoir in a special sense , which i call _ loading _ a reservoir with patterns .",
    "the loading procedure works as follows .",
    "first , drive the reservoir with the patterns @xmath47 in turn , collecting reservoir states @xmath48 ( where @xmath49 ) . then , recompute the reservoir connection weights @xmath12 into @xmath50 such that @xmath50 optimally balances between the following two goals .",
    "first , @xmath50 should be such that @xmath51 for all times @xmath10 and patterns @xmath52 .",
    "that is , @xmath50 should allow the reservoir to `` simulate '' the driving input in the absence of the same .",
    "second , @xmath50 should be such that the weights collected in this matrix become as small as possible . technically this compromise - seeking learning task amounts to computing what is known as a regularized linear regression , a standard and simple computational task .",
    "this idea of `` internalizing '' a driven dynamics into a reservoir has been independently ( re-)introduced under different names and for a variety of purposes ( _ self - prediction _",
    "@xcite , _ equilibration _",
    "@xcite , _ reservoir regularization _",
    "@xcite , _ self - sensing networks _",
    "@xcite , _ innate training _ @xcite ) and appears to be a fundamental rnn adaptation principle .    [ [ going - top - down - from - conceptors - to - neural - dynamics . ] ] going top - down : from conceptors to neural dynamics .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    assume that conceptors @xmath53 have been derived for patterns @xmath47 , and that these patterns have been loaded into the reservoir , replacing the original random weights @xmath12 by @xmath50 .",
    "intuitively , the loaded reservoir , when it is run using @xmath54 ( no input ! ) should behave exactly as when it was driven with input earlier , because @xmath50 has been trained such that @xmath55 .",
    "in fact , if only a single pattern had been loaded , the loaded reservoir would readily re - generate it .",
    "but if more than one patter had been loaded , the autonomous ( input - free ) update @xmath56 will lead to an entirely unpredictable dynamics : the network ca nt `` decide '' which of the loaded patterns it should re - generate !",
    "this is where conceptors come in .",
    "the reservoir dynamics is filtered through @xmath57 .",
    "this is effected by using the augmented update rule @xmath58 . by virtue of inserting @xmath57 into the feedback loop ,",
    "the reservoir states become clipped to fall within the ellipsoid associated with @xmath57 . as a result",
    ", the pattern @xmath59 will be re - generated : when the reservoir is observed through the previously trained output weights , one gets @xmath60 .",
    "the first column of panels in fig .",
    "[ fig1main ] * b * shows an overlay of the four autonomously re - generated patterns @xmath19 with the original drivers @xmath59 used in that example .",
    "the recovery of the originals is quite accurate ( mean square errors 3.3e-05 , 1.4e-05 , 0.0040 , 0.0019 for the four loaded patterns ) .",
    "note that the first two and the last two patterns are rather similar to each other .",
    "the filtering afforded by the respective conceptors is `` sharp '' enough to separate these twin pairs .",
    "i will later demonstrate that in this way a remarkably large number of patterns can be faithfully re - generated by a single reservoir .",
    "[ [ morphing - and - generalization . ] ] morphing and generalization .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    given a reservoir loaded with @xmath61 patterns @xmath59 , the associated conceptors @xmath57 can be linearly combined by creating mixture conceptors @xmath62 , where the mixing coefficients @xmath63 must sum to 1 .",
    "when the reservoir is run under the control of such a _ morphed _ conceptor @xmath64 , the resulting generated pattern is a morph between the original `` pure '' patterns @xmath59 .",
    "if all @xmath63 are non - negative , the morph can be considered an _ interpolation _ between the pure patterns ; if some @xmath63 are negative , the morph _ extrapolates _ beyond the loaded pure patterns .",
    "i demonstrate this with the four patterns used in the example above , setting @xmath65 , and letting @xmath66 vary from @xmath67 to @xmath68 in increments of @xmath69 .",
    "[ figmorphmain ] shows plots of observer signals @xmath19 obtained when the reservoir is generating patterns under the control of these morphed conceptors .",
    "the innermost 5 by 5 panels show interpolations between the four pure patterns , all other panels show extrapolations .    in machine learning terms ,",
    "both interpolation and extrapolation are cases of _",
    "generalization_. a standard opinion in the field states that generalization by interpolation is what one may expect from learning algorithms , while extrapolation beyond the training data is hard to achieve .",
    "morphing and generalizing dynamical patterns is a common but nontrivial task for training motor patterns in robots .",
    "it typically requires training demonstrations of numerous interpolating patterns @xcite .",
    "conceptor - based pattern morphing appears promising for flexible robot motor pattern learning from a very small number of demonstrations .     and @xmath70 ) .",
    "panels with bold frames : the four loaded prototype patterns ( same patterns as in fig .  [ fig1main ] * b*. ) , width=120 ]    [ [ aperture - adaptation . ] ] aperture adaptation .",
    "+ + + + + + + + + + + + + + + + + + + +    choosing the aperture @xmath43 appropriately is crucial for re - generating patterns in a stable and accurate way . to demonstrate this ,",
    "i loaded a 500-neuron reservoir with signals @xmath38 ",
    "@xmath39 derived from four classical chaotic attractors : the lorenz , rssler , mackey - glass , and hnon attractors .",
    "note that it used to be a challenging task to make an rnn learn any single of these attractors @xcite ; to my knowledge , training a single rnn to generate several different chaotic attractors has not been attempted before .",
    "after loading the reservoir , the re - generation was tested using conceptors @xmath71 where for each attractor pattern @xmath59 a number of different values for @xmath43 were tried .",
    "[ figchaosapmain ] * a * shows the resulting re - generated patterns for five apertures for the lorenz attractor .",
    "when the aperture is too small , the reservoir - conceptor feedback loop becomes too constrained and the produced patterns de - differentiate .",
    "when the aperture is too large , the feedback loop becomes over - excited .",
    "+   +     an optimal aperture can be found by experimentation , but this will not be an option in many engineering applications or in biological neural systems . an intrinsic criterion for optimizing @xmath43 is afforded by a quantity that i call _ attenuation _ : the damping ratio which the conceptor imposes on the reservoir signal .",
    "[ figchaosapmain ] * c * plots the attenuation against the aperture for the four chaotic signals .",
    "the minimum of this curve marks a good aperture value : when the conceptor dampens out a minimal fraction of the reservoir signal , conceptor and reservoir are in good `` resonance '' .",
    "the chaotic attractor re - generations shown in fig .",
    "[ figchaosapmain ] * b * were obtained by using this minimum - attenuation criterion .    the aperture range which yields visibly good attractor re - generations in this demonstration spans about one order of magnitude . with further refinements ( zeroing small singular values in conceptors",
    "is particularly effective ) , the viable aperture range can be expanded to about three orders of magnitude .",
    "while setting the aperture right is generally important , fine - tuning is unnecessary .    [",
    "[ boolean - operations - and - conceptor - abstraction . ] ] boolean operations and conceptor abstraction .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    assume that a reservoir is driven by a pattern @xmath72 which consists of randomly alternating epochs of two patterns @xmath37 and @xmath73 .",
    "if one does nt know which of the two patterns is active at a given time , all one can say is that the pattern @xmath72 currently is @xmath37 or it is @xmath73 .",
    "let @xmath74 be conceptors derived from the two partial patterns @xmath75 and the `` or '' pattern @xmath72 , respectively .",
    "then it holds that @xmath76 .",
    "dropping the division by 2 , this motivates to define an or ( mathematical notation : @xmath77 ) operation on conceptors @xmath78 by putting @xmath79 .",
    "the logical operations not ( @xmath80 ) and and ( @xmath81 ) can be defined along similar lines .",
    "[ figbooleansmain ] shows two - dimensional examples of applying the three operations .    .",
    "magenta ( thick ) ellipses show @xmath82 , @xmath83 , @xmath84 ( from left to right ) . , width=140 ]    boolean logic is the mathematical theory of @xmath85",
    ". many laws of boolean logic also hold for the @xmath85 operations on conceptors : the laws of associativity , commutativity , double negation , de morgan s rules , some absorption rules . furthermore ,",
    "numerous simple laws connect aperture adaptation to boolean operations .",
    "last but not least , by defining @xmath86 if and only if there exists a conceptor @xmath87 such that @xmath88 , an _ abstraction ordering _ is created on the set of all conceptors of dimension @xmath8 .    [ [ neural - memory - management . ] ] neural memory management .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    boolean conceptor operations afford unprecedented flexibility of organizing and controlling the nonlinear dynamics of recurrent neural networks . here",
    "i demonstrate how a sequence of patterns @xmath89 can be _ incrementally _ loaded into a reservoir , such that ( i ) loading a new pattern @xmath90 does not interfere with previously loaded @xmath91 ; ( ii ) if a new pattern @xmath90 is similar to already loaded ones , the redundancies are automatically detected and exploited , saving memory capacity ; ( iii ) the amount of still `` free '' memory space can be logged .",
    "let @xmath57 be the conceptor associated with pattern @xmath59 .",
    "three ideas are combined to implement the memory management scheme .",
    "first , keep track of the `` already used '' memory space by maintaining a conceptor @xmath92 .",
    "the sum of all singular values of @xmath93 , divided by the reservoir size , gives a number that ranges between 0 and 1 .",
    "it is an indicator of the portion of reservoir `` space '' which has been used up by loading @xmath94 , and i call it the _ quota _ claimed by @xmath95",
    ". second , characterize what is `` new '' about @xmath96 ( not being already represented by previously loaded patterns ) by considering the conceptor @xmath97 .",
    "the _ logical difference _",
    "operator @xmath98 can be re - written as @xmath99 .",
    "third , load only that which is new about @xmath96 into the still unclaimed reservoir space , that is , into @xmath100 .",
    "these three ideas can be straightforwardly turned into a modification of the basic pattern loading algorithm .    for a demonstration ,",
    "i created a series of periodic patterns @xmath101 whose integer period lengths were picked randomly between 3 and 15 , some of these patterns being sines , others random patterns .",
    "these patterns were incrementally loaded in a 100-neuron reservoir , one by one .",
    "[ memmanmain ] shows the result .",
    "the `` used space '' panels monitor the successive filling - up of reservoir space .",
    "since patterns @xmath102 were identical replicas of patterns @xmath103 , no additional space was consumed when these patterns were ( re-)loaded . the `` driver and @xmath104 '' panels document the accuracy of autonomously re - generating patterns using conceptors @xmath57 .",
    "accuracy was measured by the _",
    "normalized root mean square error _ ( nrmse ) , a standard criterion for comparing the similarity between two signals .",
    "the nrmse jumps from very small values to a high value when the last pattern is loaded ; the quota of 0.98 at this point indicates that the reservoir is `` full '' .",
    "the re - generation testing and nrmse computation was done after all patterns had been loaded .",
    "an attempt to load further patterns would be unsuccessful , but it also would not harm the re - generation quality of the already loaded ones .",
    "( black line ) overlaid on its reproduction ( green line ) .",
    "the memory fraction used up until pattern @xmath52 is indicated by the panel fraction filled in red ; the quota value is printed in the left bottom corner of each panel .",
    ", width=145 ]    this ability to load patterns incrementally solves a notorious problem in neural network training , known as _",
    "catastrophic forgetting _ , which manifests itself in a disruption of previously learnt functionality when learning new functionality .",
    "although a number of proposals have been made which partially alleviate the problem in special circumstances @xcite , catastrophic forgetting was still listed as an open challenge in an expert s report solicited by the nsf in 2007 @xcite which collected the main future challenges in learning theory .",
    "[ [ recognizing - dynamical - patterns . ] ] recognizing dynamical patterns .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    boolean conceptor operations enable the combination of positive and negative evidence in a neural architecture for dynamical pattern recognition . for a demonstration",
    "i use a common benchmark , the _",
    "japanese vowel _ recognition task @xcite . the data of this benchmark consist in preprocessed audiorecordings of nine male native speakers pronouncing the japanese di - vowel /ae/. the training data consist of 30 recordings per speaker , the test data consist of altogether 370 recordings , and the task is to train a recognizer which has to recognize the speakers of the test recordings .",
    "this kind of data differs from the periodic or chaotic patterns that i have been using so far , in that the patterns are non - stationary ( changing in their structure from beginning to end ) , multi - dimensional ( each recording consisting of 12 frequency band signals ) , stochastic , and of finite duration .",
    "this example thus also demonstrates that conceptors can be put to work with data other than single - channel stationary patterns .    a small ( 10 neurons ) reservoir",
    "was created .",
    "it was driven with all training recordings from each speaker @xmath52 in turn ( @xmath105 ) , collecting reservoir response signals , from which a conceptor @xmath57 characteristic of speaker @xmath52 was computed .",
    "in addition , for each speaker @xmath52 , a conceptor @xmath106 was computed .",
    "@xmath107 characterizes the condition `` this speaker is not any of the other eight speakers '' .",
    "patterns need not to be loaded into the reservoir for this application , because they need not be re - generated .    in testing ,",
    "a recording @xmath37 from the test set was fed to the reservoir , collecting a reservoir response signal @xmath4 . for each of the conceptors ,",
    "a _ positive evidence _",
    "@xmath108 was computed .",
    "@xmath109 is a non - negative number indicating how well the signal @xmath4 fits into the ellipsoid of @xmath57 .",
    "likewise , the _ negative evidence _ @xmath110 that the sample @xmath37 was not uttered by any of the eight speakers other than speaker @xmath52 was computed . finally , the _ combined evidence _",
    "@xmath111 was computed .",
    "this gave nine combined evidences @xmath112 .",
    "the pattern @xmath37 was then classified as speaker @xmath52 by choosing the speaker index @xmath52 whose combined evidence @xmath113 was the greatest among the nine collected evidences .    in order to check for the impact of the random selection of the underlying reservoir",
    ", this whole procedure was repeated 50 times , using a freshly created random reservoir in each trial .",
    "averaged over these 50 trials , the number of test misclassifications was 3.4 .",
    "if the classification would have been based solely on the positive or negative evidences , the average test misclassification numbers would have been 8.4 and 5.9 respectively .",
    "the combination of positive and negative evidence , which was enabled by boolean operations , was crucial .",
    "state - of - the - art machine learning methods achieve between 4 and 10 misclassifications on the test set ( for instance @xcite ) .",
    "the boolean - logic - conceptor - based classifier thus compares favorably with existing methods in terms of classification performance .",
    "the method is computationally cheap , with the entire learning procedure taking a fraction of a second only on a standard notebook computer .",
    "the most distinctive benefit however is incremental extensibility . if new training data become available , or if a new speaker would be incorporated into the recognition repertoire , the additional training can be done using only the new data without having to re - run previous training data .",
    "this feature is highly relevant in engineering applications and in cognitive modeling and missing from almost all state - of - the - art classification methods .",
    "[ [ autoconceptors - and - content - addressable - memories . ] ] autoconceptors and content - addressable memories .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    so far i have been describing examples where conceptors @xmath57 associated with patterns @xmath59 were computed at training time , to be later plugged in to re - generate or classify patterns .",
    "a conceptor @xmath25 matrix has the same size as the reservoir connection matrix @xmath50 .",
    "storing conceptor matrices means to store network - sized objects .",
    "this is implausible under aspects of biological modeling . here",
    "i describe how conceptors can be created on the fly , without having to store them , leading to content - addressable neural memories",
    ".    if the system has no pre - computed conceptors at its disposal , loaded patterns can still be re - generated in a two - stage process .",
    "first , the target pattern @xmath37 is selected by driving the system with a brief initial `` cueing '' presentation of the pattern ( possibly in a noisy version ) . during this phase ,",
    "a preliminary conceptor @xmath114 is created by an online adaptation process .",
    "this preliminary @xmath114 already enables the system to re - generate an imperfect version of the pattern @xmath37 .",
    "second , after the cueing phase has ended , the system continues to run in an autonomous mode ( no external cue signal ) , initially using @xmath114 , to continuously generate a pattern .",
    "while this process is running , the conceptor in the loop is continuously adapted by a simple online adaptation rule .",
    "this rule can be described in geometrical terms as `` adapt the current conceptor @xmath115 such that its ellipsoid matches better the shape of the point cloud of the current reservoir state dynamics '' . under this rule one",
    "obtains a reliable convergence of the generated pattern toward a highly accurate replica of the target pattern @xmath37 that was given as a cue .",
    "( black ) and @xmath116 ( gray ) .",
    "right panels show an overlay of the original driver pattern ( black , thin ) and the reconstruction at the end of auto - adaptation ( gray , thick ) . *",
    "b * pattern reconstruction errors directly after cueing ( black squares ) and at end of auto - adaptation ( gray crosses ) . *",
    "c * reconstruction error of loaded patterns ( black ) and novel patterns drawn from the same parametric family ( gray ) versus the number of loaded patterns , averaged over 5 repetitions of the entire experiment and 10 patterns per plotting point .",
    "error bars indicate standard deviations . *",
    "d * autoconceptor adaptation dynamics described as evolution toward a plane attractor @xmath64 ( schematic).,title=\"fig:\",width=181 ]   ( black ) and @xmath116 ( gray ) .",
    "right panels show an overlay of the original driver pattern ( black , thin ) and the reconstruction at the end of auto - adaptation ( gray , thick ) . *",
    "b * pattern reconstruction errors directly after cueing ( black squares ) and at end of auto - adaptation ( gray crosses ) .",
    "* c * reconstruction error of loaded patterns ( black ) and novel patterns drawn from the same parametric family ( gray ) versus the number of loaded patterns , averaged over 5 repetitions of the entire experiment and 10 patterns per plotting point .",
    "error bars indicate standard deviations . *",
    "d * autoconceptor adaptation dynamics described as evolution toward a plane attractor @xmath64 ( schematic).,title=\"fig:\",width=181 ] + * c *   ( black ) and @xmath116 ( gray ) .",
    "right panels show an overlay of the original driver pattern ( black , thin ) and the reconstruction at the end of auto - adaptation ( gray , thick ) . *",
    "b * pattern reconstruction errors directly after cueing ( black squares ) and at end of auto - adaptation ( gray crosses ) . *",
    "c * reconstruction error of loaded patterns ( black ) and novel patterns drawn from the same parametric family ( gray ) versus the number of loaded patterns , averaged over 5 repetitions of the entire experiment and 10 patterns per plotting point .",
    "error bars indicate standard deviations . *",
    "d * autoconceptor adaptation dynamics described as evolution toward a plane attractor @xmath64 ( schematic).,title=\"fig:\",width=181 ]   ( black ) and @xmath116 ( gray ) .",
    "right panels show an overlay of the original driver pattern ( black , thin ) and the reconstruction at the end of auto - adaptation ( gray , thick ) . *",
    "b * pattern reconstruction errors directly after cueing ( black squares ) and at end of auto - adaptation ( gray crosses ) . *",
    "c * reconstruction error of loaded patterns ( black ) and novel patterns drawn from the same parametric family ( gray ) versus the number of loaded patterns , averaged over 5 repetitions of the entire experiment and 10 patterns per plotting point .",
    "error bars indicate standard deviations . *",
    "d * autoconceptor adaptation dynamics described as evolution toward a plane attractor @xmath64 ( schematic).,title=\"fig:\",width=181 ]    results of a demonstration are illustrated in figure [ figcontaddressmain ] .",
    "a 200-neuron reservoir was loaded with 5 patterns consisting of a weighted sum of two irrational - period sines , sampled at integer timesteps .",
    "the weight ratio and the phaseshift were chosen at random ; the patterns thus came from a family of patterns parametrized by two parameters .",
    "the cueing time was 30 timesteps , the free - running auto - adaptation time was 10,000 timesteps , leading to an auto - adapted conceptor @xmath116 at the end of this process . on average ,",
    "the reconstruction error improved from about -0.4 ( log10 nrmse measured directly after the cueing ) to -1.1 ( at the end of auto - adaptation ) .",
    "it can be shown analytically that the auto - adaptation process pulls many singular values down to zero .",
    "this effect renders the combined reservoir - conceptor loop very robust against noise , because all noise components in the directions of the nulled singular values become completely suppressed .",
    "in fact , all results shown in figure [ figcontaddressmain ] were obtained with strong state noise ( signal - to - noise ratio equal to 1 ) inserted into the reservoir during the post - cue auto - adaptation .",
    "the system functions as a _ content - addressable memory _ ( cam ) : loaded items can be recalled by cueing them .",
    "the paradigmatic example of a neural cam are auto - associative neural networks ( aanns ) , pioneered by palm @xcite and hopfield @xcite .",
    "in contrast to conceptor - based cam , which store and re - generate dynamical patterns , aanns store and cue - recall static patterns . furthermore , aanns do not admit an incremental storing of new patterns , which is possible in conceptor - based cams .",
    "the latter thus represent an advance in neural cams in two fundamental aspects .    to further elucidate the properties of conceptor cams",
    ", i ran a suite of simulations where the same reservoir was loaded with increasing numbers of patterns , chosen at random from the same 2-parametric family ( figure [ figcontaddressmain ] * c * ) . after loading with @xmath117 patterns ,",
    "the reconstruction accuracy was measured at the end of the auto - adaptation . not surprisingly , it deteriorated with increasing memory load @xmath118 ( black line ) .",
    "in addition , i also cued the loaded reservoir with patterns that were _ not _ loaded , but were drawn from the same family . as one would expect , the re - construction accuracy of these novel patterns was worse than for the loaded patterns  but only for small @xmath118 .",
    "when the number of loaded patterns exceeded a certain threshold , recall accuracy became essentially equal for loaded and novel patterns .",
    "these findings can be explained in intuitive terms as follows . when few patterns are loaded , the network memorizes individual patterns by `` rote learning '' , and subsequently can recall these patterns better than other patterns from the family . when more patterns are loaded , the network learns a representation of the entire parametric class of patterns .",
    "i call this the _ class learning effect_.    the class learning effect can be geometrically interpreted in terms of a _ plane attractor _ @xcite arising in the space of conceptor matrices @xmath25 ( figure [ figcontaddressmain ] * d * ) . the learnt parametric class of patterns is represented by a @xmath119-dimensional manifold @xmath120 in this space , where @xmath119 is the number of defining parameters for the pattern family ( in our example , @xmath121 ) .",
    "the cueing procedure creates an initial conceptor @xmath122 in the vicinity of @xmath120 , which is then attracted toward @xmath120 by the auto - adaptation dynamics . while an in - depth analysis of this situation reveals that this picture is not mathematically correct in some detail , the plane attractor metaphor yields a good phenomenal description of conceptor cam class learning .",
    "plane attractors have been invoked as an explanation for a number of biological phenomena , most prominently gaze direction control @xcite . in such phenomena ,",
    "points on the plane attractor correspond to _ static _",
    "fixed points ( for instance , a direction of gaze ) .",
    "in contrast , points on @xmath64 correspond to conceptors which in turn define _ temporal _ patterns .",
    "again , the conceptor framework `` dynamifies '' concepts that have previously been worked out for static patterns only .    [ [ toward - biological - feasibility - random - feature - conceptors . ] ] toward biological feasibility : random feature conceptors .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    several computations involved in adapting conceptor matrices are non - local and therefore biologically infeasible .",
    "it is however possible to approximate matrix conceptors with another mechanism which only requires local computations .",
    "the idea is to project ( via random projection weights @xmath123 ) the reservoir state into a _",
    "random feature space _ which is populated by a large number of neurons @xmath124 ; execute the conceptor operations individually on each of these neurons by multiplying a _ conception weight _",
    "@xmath125 into its state ; and finally to project back to the reservoir by another set of random projection weights @xmath126 ( figure [ figrandfeatures ] ) .",
    "the original reservoir - internal random connection weigths @xmath50 are replaced by a dyade of two random projections of first @xmath123 , then @xmath126 , and the original reservoir state @xmath4 segregates into a reservoir state @xmath72 and a random feature state @xmath127 .",
    "the conception weights @xmath125 assume the role of conceptors .",
    "they can be learnt and adapted by procedures which are directly analog to the matrix conceptor case . what had to be non - local matrix computations before now turns into local , one - dimensional ( scalar ) operations .",
    "these operations are biologically feasible in the modest sense that any information needed to adapt a synaptic weight is locally available at that synapse .",
    "all laws and constructions concerning boolean operations and aperture carry over",
    ".        a set of conception weights @xmath125 corresponding to a particular pattern can be neurally represented and `` stored '' in the form of the connections of a single neuron to the feature space . a dynamical pattern thus can be represented by a single neuron .",
    "this enables a highly compact neural representation of dynamical patterns .",
    "a machine learning application is presented below .",
    "i re - ran with such random feature conceptors a choice of the simulations that i did with matrix conceptors , using a number of random features that was two to five times as large as the reservoir .",
    "the outcome of these simulations : the accuracy of pattern re - generation is essentially the same as with matrix conceptors , but setting the aperture is more sensitive .    [ [ a - hierarchical - classification - and - de - noising - architecture . ] ] a hierarchical classification and de - noising architecture . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    here i present a system which combines in a multi - layer neural architecture many of the items introduced so far .",
    "the input to this system is a ( very ) noisy signal which at a given time is being generated by one out of a number of possible candidate pattern generators .",
    "the task is to recognize the current generator , and simultaneously to re - generate a clean version of the noisy input pattern .    }",
    "( n)$ ] in the three layers .",
    "color coding : @xmath38 blue , @xmath128 green , @xmath129 red , @xmath39 cyan .",
    "fourth panel : trust variables @xmath130}(n)$ ] ( blue ) and @xmath131}(n)$ ] ( green ) .",
    "fifth panel : signal reconstruction errors ( log10 nrmse ) of @xmath132}$ ] ( blue ) , @xmath133}$ ] ( green ) and @xmath134}$ ] ( red ) versus clean signal @xmath59 .",
    "black line : linear baseline filter .",
    "bottom panels : 20-step samples from the end of the two presentation periods .",
    "red : noisy input ; black : clean input ; thick gray : cleaned output signal @xmath134}$ ] . , title=\"fig:\",width=321 ] * b * } ( n)$ ] in the three layers .",
    "color coding : @xmath38 blue , @xmath128 green , @xmath129 red , @xmath39 cyan .",
    "fourth panel : trust variables @xmath130}(n)$ ] ( blue ) and @xmath131}(n)$ ] ( green ) .",
    "fifth panel : signal reconstruction errors ( log10 nrmse ) of @xmath132}$ ] ( blue ) , @xmath133}$ ] ( green ) and @xmath134}$ ] ( red ) versus clean signal @xmath59 .",
    "black line : linear baseline filter .",
    "bottom panels : 20-step samples from the end of the two presentation periods .",
    "red : noisy input ; black : clean input ; thick gray : cleaned output signal @xmath134}$ ] . , title=\"fig:\",width=188 ] +    i explain the architecture with an example .",
    "it uses three processing layers to de - noise an input signal @xmath135}(n ) = p^j(n ) + noise$ ] , with @xmath59 being one of the four patterns @xmath136 used before in this report ( shown for instance in figure [ fig1main ] * b * ) .",
    "the architecture implements the following design principles ( figure [ fig3layer ] * a * ) .",
    "( i ) each layer is a random feature based conceptor system ( as in figure [ figrandfeatures ] * b * ) .",
    "the four patterns @xmath137 are initially loaded into each of the layers , and four _ prototype _ conceptor weight vectors @xmath138 corresponding to the patterns are computed and stored .",
    "( ii ) in a bottom - up processing pathway , the noisy external input signal @xmath135}(n ) = p^j(n ) + noise$ ] is stagewise de - noised , leading to signals @xmath132 } , y_{[2]},y_{[3]}$ ] on layers @xmath139 , where @xmath134}$ ] should be a highly cleaned - up version of the input ( subscripts @xmath140 $ ] refer to layers , bottom layer is @xmath141 ) .",
    "( iii ) the top layer auto - adapts a conceptor @xmath142}$ ] which is constrained to be a weighted or combination of the four prototype conceptors . in a suggestive notation",
    "this can be written as @xmath142}(n ) = \\gamma^1_{[3]}(n)\\ , c^1 \\vee",
    "\\ldots \\vee \\gamma^4_{[3]}(n ) \\,c^4 $ ] .",
    "the four weights @xmath143}$ ] sum to one and represent a _ hypothesis _ vector expressing the system s current belief about the current driver @xmath59 .",
    "if one of these @xmath143}$ ] approaches 1 , the system has settled on a firm classification of the current driving pattern .",
    "( iv ) in a top - down pathway , conceptors @xmath144}$ ] from layers @xmath145 are passed down to the respective layers @xmath146 below . because higher layers should have a clearer conception of the current noisy driver pattern than lower layers , this passing - down of conceptors `` primes '' the processing in layer @xmath146 with valuable contextual information .",
    "( v ) between each pair of layers @xmath147 , a _ trust _ variable @xmath148}(n)$ ] is adapted by an online procedure .",
    "these trust variables range between 0 and 1 .",
    "a value of @xmath148}(n ) = 1 $ ] indicates maximal confidence that the signal @xmath149}(n)$ ] comes closer to the clean driver @xmath150 than the signal @xmath151}(n)$ ] does , that is , the stage - wise denoising actually functions well when progressing from layer @xmath145 to @xmath152 .",
    "the trust @xmath148}(n)$ ] evolves by comparing certain noise ratios that are observable locally in layers @xmath145 and @xmath152 .",
    "( vi ) within layer @xmath145 , an internal auto - adaptation process generates a candidate de - noised signal @xmath151}^{\\mbox{\\scriptsize auto}}$ ] and a candidate local autoconceptor @xmath144}^{\\mbox{\\scriptsize auto}}$ ] .",
    "the local estimate @xmath151}^{\\mbox{\\scriptsize auto}}$ ] is linearly mixed with the signal @xmath153}$ ] , where the trust @xmath154}$ ] sets the mixing rate .",
    "the mixture @xmath155 } = \\tau_{[l-1,l]}\\ , y_{[l]}^{\\mbox{\\scriptsize      auto } } + ( 1- \\tau_{[l-1,l ] } ) \\ , y_{[l-1]}$ ] is the effective signal input to layer @xmath145 .",
    "if the trust @xmath154}$ ] reaches its maximal value of 1 , layer @xmath145 will ignore the signal from below and work entirely by self - generating a pattern .",
    "( vii ) in a similar way , the effective conceptor in layer @xmath145 is a trust - negotiated mixture @xmath144 } = ( 1-\\tau_{[l , l+1]})\\ , c_{[l]}^{\\mbox{\\scriptsize auto } } + \\tau_{[l , l+1 ] } \\ , c_{[l+1]}$ ]",
    ". thus if the trust @xmath148}$ ] is maximal , layer @xmath145 will be governed entirely by the passed - down conceptor @xmath156}$ ] .",
    "summarizing , the higher the trusts inside the hierarchy , the more will the system be auto - generating conceptor - shaped signals , or conversely , at low trust values the system will be strongly permeated from below by the outside driver .",
    "if the trust variables reach their maximum value of 1 , the system will run in a pure `` confabulation '' mode and generate an entirely noise - free signal @xmath134}$ ]  at the risk of doing this under an entirely misguided hypothesis @xmath142}$ ] .",
    "the key to make this architecture work thus lies in the trust variables .",
    "it seems to me that maintaining a measure of trust ( or call it confidence , certainty , etc . )",
    "is an intrinsically necessary component in any signal processing architecture which hosts a top - down pathway of guiding hypotheses ( or call them context , priors , bias , etc . ) .",
    "figure [ fig3layer ] * b * shows an excerpt from a simulation run . the system was driven first by an initial 4000 step period of @xmath157 , followed by 4000 steps of @xmath158 .",
    "the signal - to - noise ratio was 0.5 ( noise twice as strong as signal ) .",
    "the system successfully settles on the right hypothesis ( top panel ) and generates very clean de - noised signal versions ( bottom panel ) .",
    "the crucial item in this figure is the development of the trust variable @xmath131}$ ] . at the beginning of each 4000 step period",
    "it briefly drops , allowing the external signal to permeate upwards through the layers , thus informing the local auto - adaptation loops about `` what is going on outside '' .",
    "after these initial drops the trust rises to almost 1 , indicating that the system firmly `` believes '' to have detected the right pattern .",
    "it then generates pattern versions that have almost no mix - in from the noisy external driver .    as a baseline comparison",
    "i also trained a standard linear transversal filter which computed a de - noised input pattern point based on the preceding @xmath159 input values .",
    "the filter length @xmath61 was set equal to the number of trainable parameters in the neural architecture .",
    "the performance of this linear de - noising filter ( black line in figure [ fig3layer ] ) is inferior to the architecture s performance both in terms of accuracy and response time .",
    "it is widely believed that top - down hypothesis - passing through a processing hierarchy plays a fundamental role in biological cognitive systems @xcite .",
    "however , the current best artificial pattern recognition systems @xcite use purely bottom - up processing  leaving room for further improvement by including top - down guidance .",
    "a few hierarchical architectures which exploit top - down hypothesis - passing have been proposed @xcite .",
    "all of these are designed for recognizing static patterns , especially images .",
    "the conceptor - based architecture presented here appears to be the first hierarchical system which targets dynamical patterns and uses top - down hypothesis - passing .",
    "furthermore , in contrast to state - of - the - art pattern recognizers , it admits an incremental extension of the pattern repertoire .",
    "[ [ intrinsic - conceptor - logic . ] ] intrinsic conceptor logic .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + +    in mathematical logics the _ semantics _ ( `` meaning '' ) of a symbol or operator is formalized as its _",
    "extension_. for instance , the symbol cow in a logic - based knowledge representation system in ai is semantically interpreted by the set of all ( physical ) cows , and the or - operator @xmath77 is interpreted as set union : cow @xmath77 horse would refer to the set comprising all cows and horses .",
    "similarly , in cognitive science , _ concepts _ are semantically referring to their extensions , usually called _ categories _ in this context @xcite .",
    "both in mathematical logic and cognitive science , extensions need not be confined to physical objects ; the modeler may also define extensions in terms of mathematical structures , sensory perceptions , hypothetical worlds , ideas or facts .",
    "but at any rate , there is an ontological difference between the two ends of the semantic relationship .",
    "this ontological gap dissolves in the case of conceptors .",
    "the natural account of the `` meaning '' of a matrix conceptor @xmath25 is the shape of the neural state cloud it is derived from .",
    "this shape is given by the correlation matrix @xmath23 of neural states .",
    "both @xmath25 and @xmath23 have the same mathematical format : positive semi - definite matrices of identical dimension .",
    "figure [ figsemanticsmain ] visualizes the difference between classical extensional semantics of logics and the system - internal conceptor semantics .",
    "the symbol @xmath160 is the standard mathematical notation for the semantical meaning relationship .",
    "i have cast these intuitions into a formal specification of an _ intrinsic conceptor logic _ ( icl ) , where the semantic relationship outlined above",
    "is formalized within the framework of _ institutions _ @xcite .",
    "this framework has been developed in mathematics and computer science to provide a unified view on the multitude of existing `` logics '' . by formalizing icl as an institution ,",
    "conceptor logic can be rigorously compared to other existing logics .",
    "i highlight two findings .",
    "first , an icl cast as an institution is a dynamcial system in its own right : the symbols used in this logic evolve over time .",
    "this is very much different from traditional views on logic , where symbols are static tokens .",
    "second , it turns out that icl is a logic which is _ decidable_. stated in intuitive terms , in a decidable logic it can be calculated whether a `` concept '' @xmath161 subsumes a concept @xmath162 ( as in `` a cow is an animal '' ) .",
    "deciding concept subsumption is a core task in ai systems and human cognition . in most logic - based ai systems ,",
    "deciding concept subsumption can become computationally expensive or even impossible . in icl",
    "it boils down to determining whether all components of a certain conception weight vector @xmath125 are smaller or equal to the corresponding components @xmath163 of another such vector , which can be done in a single processing step .",
    "this may help explaining why humans can make classification judgements almost instantaneously .",
    "[ [ discussion . ] ] discussion .",
    "+ + + + + + + + + + +    the human brain is a neurodynamical system which evidently supports logico - rational reasoning @xcite .",
    "since long this has challenged scientists to find computational models which connect neural dynamics with logic .",
    "very different solutions have been suggested . at the dawn of computational neuroscience , mcculloch and pitts",
    "have already interpreted networks of binary - state neurons as carrying out boolean operations @xcite .",
    "logical inferences of various kinds have been realized in localist connectionist networks where neurons are labelled by concept names @xcite . in neurofuzzy modeling ,",
    "feedforward neural networks are trained to carry out operations of fuzzy logic on their inputs @xcite . in a field",
    "known as neuro - symbolic computation , deduction rules of certain formal logic systems are coded or trained into neural networks @xcite .",
    "the combinatorial / compositional structure of symbolic knowledge has been captured by dedicated neural circuits to enable tree - structured representations @xcite or variable - binding functionality @xcite .",
    "all of these approaches require _ interface _ mechanisms .",
    "these interface mechanisms are non - neural and code symbolic knowledge representations into the numerical activation values of neurons and/or the topological structure of networks .",
    "one could say , previous approaches _ code _ logic _ into _ specialized neural networks , while conceptors _ instantiate _ the logic _ of _ generic recurrent neural networks .",
    "this novel , simple , versatile , computationally efficient , neurally not infeasible , bi - directional connection between logic and neural dynamics opens new perspectives for computational neuroscience and machine learning .",
    "in this section i expand on the brief characterization of the scientific context given in section [ secoverview ] , and introduce mathematical notation .",
    "intelligent behavior is desired for robots , demonstrated by humans , and studied in a wide array of scientific disciplines .",
    "this research unfolds in two directions . in `` top - down '' oriented research ,",
    "one starts from the `` higher '' levels of cognitive performance , like rational reasoning , conceptual knowledge representation , planning and decision - making , command of language .",
    "these phenomena are described in symbolic formalisms developed in mathematical logic , artificial intelligence ( ai ) , computer science and linguistics . in the `` bottom - up '' direction ,",
    "one departs from `` low - level '' sensor data processing and motor control , using the analytical tools offered by dynamical systems theory , signal processing and control theory , statistics and information theory . for brevity",
    "i will refer to these two directions as the _ conceptual - symbolic _ and the _ data - dynamical _ sets of phenomena , and levels of description .",
    "the two interact bi - directionally .",
    "higher - level symbolic concepts arise from low - level sensorimotor data streams in short - term pattern recognition and long - term learning processes .",
    "conversely , low - level processing is modulated , filtered and steered by processes of attention , expectations , and goal - setting in a top - down fashion .",
    "several schools of thought ( and strands of dispute ) have evolved in a decades - long quest for a unification of the conceptual - symbolic and the data - dynamical approaches to intelligent behavior .",
    "the nature of symbols in cognitive processes has been cast as a philosophical issue @xcite . in localist",
    "connectionistic models , symbolically labelled abstract processing units interact by nonlinear _ spreading activation _",
    ". a basic tenet of behavior - based ai is that higher cognitive functions _",
    "emerge _ from low - level sensori - motor processing loops which couple a behaving agent into its environment @xcite . within cognitive science , a number of cognitive pheneomena have been described in terms of _ self - organization _ in nonlinear dynamical systems @xcite .",
    "a pervasive idea in theoretical neuroscience is to interpret _ attractors _ in nonlinear neural dynamics as the carriers of conceptual - symbolic representations .",
    "this idea can be traced back at least to the notion of cell assemblies formulated by hebb @xcite , reached a first culmination in the formal analysis of associative memories @xcite , and has since then diversified into a range of increasingly complex models of interacting ( partial ) neural attractors @xcite .",
    "another pervasive idea in theoretical neuroscience and machine learning is to consider _ hierarchical _ neural architectures , which are driven by external data at the bottom layer and transform this raw signal into increasingly abstract feature representations , arriving at conceptual representations at the top layer of the hierarchy .",
    "such hierarchical architectures mark the state of the art in pattern recognition technology @xcite .",
    "many of these systems process their input data in a uni - directional , bottom - to - top fashion .",
    "two notable exceptions are systems where each processing layer is designed according to statistical principles from _",
    "bayes rule _",
    "@xcite , and models based on the iterative linear maps of _ map seeking circuits _",
    "@xcite , both of which enable top - down guidance of recognition by expectation generation .",
    "more generally , leading actors in theoretical neuroscience have characterized large parts of their field as an effort to understand how cognitive phenomena arise from neural dynamics @xcite .",
    "finally , i point out two singular scientific efforts to design comprehensive cognitive brain models , the act - r architectures developed by anderson et al . @xcite and the spaun model of eliasmith et al",
    "both systems can simulate a broad selection of cognitive behaviors .",
    "they integrate numerous subsystems and processing mechanisms , where act - r is inspired by a top - down modeling approach , starting from cognitive operations , and spaun from a bottom - up strategy , starting from neurodynamical processing principles .    despite this extensive research , the problem of integrating the conceptual - symbolic with the data - dynamical aspects of cognitive behavior",
    "can not be considered solved .",
    "quite to the contrary , two of the largest current research initiatives worldwide , the human brain project @xcite and the nih brain initiative @xcite , are ultimately driven by this problem .",
    "there are many reasons why this question is hard , ranging from experimental challenges of gathering relevant brain data to fundamental oppositions of philosophical paradigms .",
    "an obstinate stumbling block is the different mathematical nature of the fundamental formalisms which appear most natural for describing conceptual - symbolic versus data - dynamical phenomena : symbolic logic versus nonlinear dynamics .",
    "logic - oriented formalisms can easily capture all that is combinatorially constructive and hierarchically organized in cognition : building new concepts by logical definitions , describing nested plans for action , organizing conceptual knowledge in large and easily extensible abstraction hierarchies .",
    "but logic is inherently non - temporal , and in order to capture cognitive _ processes _ , additional , heuristic `` scheduling '' routines have to be introduced which control the order in which logical rules are executed .",
    "this is how act - r architectures cope with the integration problem .",
    "conversely , dynamical systems formalisms are predestined for modeling all that is continuously changing in the sensori - motor interface layers of a cognitive system , driven by sensor data streams .",
    "but when dynamical processing modules have to be combined into compounds that can solve complex tasks , again additional design elements have to be inserted , usually by manually coupling dynamical modules in ways that are informed by biological or engineering insight on the side of the researcher .",
    "this is how the spaun model has been designed to realize its repertoire of cognitive functions .",
    "two important modeling approaches venture to escape from the logic - dynamics integration problem by taking resort to an altogether different mathematical framework which can accomodate both sensor data processing and concept - level representations : the framework of bayesian statistics and the framework of iterated linear maps mentioned above .",
    "both approaches lead to a unified formal description across processing and representation levels , but at the price of a double weakness in accounting for the embodiment of an agent in a dynamical environment , and for the combinatorial aspects of cognitive operations .",
    "it appears that current mathematical methods can instantiate only one of the three : continuous dynamics , combinatorial productivity , or a unified level - crossing description format .",
    "the conceptor mechanisms introduced in this report bi - directionally connect the data - dynamical workings of a recurrent neural network ( rnn ) with a conceptual - symbolic representation of different functional modes of the rnn .",
    "mathematically , conceptors are linear operators which characterize classes of signals that are being processed in the rnn .",
    "conceptors can be represented as matrices ( convenient in machine learning applications ) or as neural subnetworks ( appropriate from a computational neuroscience viewpoint ) . in a bottom - up way , starting from an operating rnn , conceptors can be learnt and stored , or quickly generated on - the - fly , by what may be considered the simplest of all adaptation rules : learning a regularized identity map .",
    "conceptors can be combined by elementary logical operations ( and , or , not ) , and can be ordered by a natural abstraction relationship .",
    "these logical operations and relations are defined via a formal semantics .",
    "thus , an rnn engaged in a variety of tasks leads to a learnable representation of these operations in a logic formalism which can be neurally implemented .",
    "conversely , in a top - down direction , conceptors can be inserted into the rnn s feedback loop , where they robustly steer the rnn s processing mode . due to their linear algebra nature",
    ", conceptors can be continuously morphed and `` sharpened '' or `` defocussed '' , which extends the discrete operations that are customary in logics into the domain of continuous `` mental '' transformations .",
    "i highlight the versatility of conceptors in a series of demonstrations : generating and morphing many different dynamical patterns with a single rnn ; managing and monitoring the storing of patterns in a memory rnn ; learning a class of dynamical patterns from presentations of a small number of examples ( with extrapolation far beyond the training examples ) ; classification of temporal patterns ; de - noising of temporal patterns ; and content - addressable memory systems .",
    "the logical conceptor operations enable an incremental extension of a trained system by incorporating new patterns without interfering with already learnt ones .",
    "conceptors also suggest a novel answer to a perennial problem of attractor - based models of concept representations , namely the question of how a cognitive trajectory can leave an attractor ( which is at odds with the very nature of an attractor ) .",
    "finally , i outline a version of conceptors which is biologically plausible in the modest sense that only local computations and no information copying are needed .",
    "i assume that the reader is familiar with properties of positive semidefinite matrices , the singular value decomposition , and ( in some of the analysis of adaptation dynamics ) the usage of the jacobian of a dynamical system for analysing stability properties .",
    "@xmath164 , ( a , b ) , ( a , b ] , [ a , b)$ ] denote the closed ( open , half - open ) interval between real numbers @xmath165 and @xmath166 .",
    "@xmath167 or @xmath168 denotes the transpose of a matrix @xmath169 or vector @xmath4 .",
    "@xmath36 is the identity matrix ( the size will be clear from the context or be expressed as @xmath170 ) .",
    "the @xmath171th unit vector is denoted by @xmath172 ( dimension will be clear in context ) .",
    "the trace of a square matrix @xmath169 is denoted by @xmath173 .",
    "the singular value decomposition of a matrix @xmath169 is written as @xmath174 , where @xmath175 are orthonormal and @xmath176 is the diagonal matrix containing the singular values of @xmath169 , assumed to be in descending order unless stated otherwise .",
    "@xmath177 is the pseudoinverse of @xmath169 .",
    "all matrices and vectors will be real and this will not be explicitly mentioned .",
    "i use the matlab notation to address parts of vectors and matrices , for instance @xmath178 is the third column of a matrix @xmath64 and @xmath179 picks from @xmath64 the submatrix consisting of rows 2 to 4 .",
    "furthermore , again like in matlab , i use the operator diag in a `` toggling '' mode : @xmath180 returns the diagonal vector of a square matrix @xmath169 , and @xmath181 constructs a diagonal matrix from a vector @xmath119 of diagonal elements . another matlab notation that will be used is `` @xmath182 '' for the element - wise multiplication of vectors and matrices of the same size , and `` @xmath183 '' for element - wise exponentation of vectors and matrices .",
    "@xmath184 and @xmath185 denote the range and null space of a matrix @xmath169 . for linear subspaces @xmath186 of @xmath187",
    ", @xmath188 is the orthogonal complement space of @xmath189 and @xmath190 is the direct sum @xmath191 of @xmath189 and @xmath192 .",
    "@xmath193 is the @xmath194 dimensional projection matrix on a linear subspace @xmath189 of @xmath187 . for a @xmath118-dimensional linear subspace @xmath189 of @xmath187",
    ", @xmath195 denotes any @xmath196 dimensional matrix whose columns form an orthonormal basis of @xmath189 . such matrices @xmath195 will occur only in contexts where the choice of basis can be arbitrary .",
    "it holds that @xmath197 .",
    "@xmath198 $ ] denotes the expectation ( temporal average ) of a stationary signal @xmath11 ( assuming it is well - defined , for instance , coming from an ergodic source ) .    for a matrix @xmath64",
    ", @xmath199 is the frobenius norm of @xmath64 . for real @xmath64 ,",
    "it is the square root of the summed squared elements of @xmath64 .",
    "if @xmath64 is positive semidefinite with svd @xmath200 , @xmath199 is the same as the 2-norm of the diagonal vector of @xmath176 , i.e.  @xmath201 .",
    "since in this report i will exclusively use the frobenius norm for matrices , i sometimes omit the subscript and write @xmath202 for simplicity .    in a number of simulation experiments",
    ", a network - generated signal @xmath19 will be matched against a target pattern @xmath14 .",
    "the accuracy of the match will be quantified by the normalized root mean square error ( nrmse ) , @xmath203 / [ ( p(n)^2]}$ ] , where @xmath204 $ ] is the mean operator over data points @xmath10 .    the symbol @xmath8 is reserved for the size of a reservoir (= number of neurons ) throughout .",
    "this is the main section of this report . here",
    "i develop in detail the concepts , mathematical analysis , and algorithms , and i illustrate various aspects in computer simulations .",
    "figure [ figoverviewtextsections ] gives a navigation guide through the dependency tree of the components of this section .",
    ".,width=548 ]    the program code ( matlab ) for all simulations can be retrieved from + http://minds.jacobs-university.de/sites/default/files/uploads/ ...",
    "+ ... sw / conceptorstechrepmatlab.zip .      throughout this report",
    ", i will be using discrete - time recurrent neural networks made of simple @xmath16 neurons , which will be driven by an input time series @xmath14 . in the case of 1-dimensional input , these networks consist of ( i ) a `` reservoir '' of @xmath8 recurrently connected neurons whose activations form a state vector @xmath205 , ( ii ) one external input neuron that serves to drive the reservoir with training or cueing signals @xmath14 and ( iii ) another external neuron which serves to read out a scalar target signal @xmath19 from the reservoir ( fig .  [ fig0 ] ) .",
    "the system operates in discrete timesteps @xmath206 according to the update equations @xmath207 where @xmath50 is the @xmath13 matrix of reservoir - internal connection weights , @xmath15 is the @xmath208 sized vector of input connection weights , @xmath209 is the @xmath210 vector of readout weights , and @xmath166 is a bias .",
    "the @xmath16 is a sigmoidal function that is applied to the network state @xmath4 component - wise . due to the @xmath16 ,",
    "the _ reservoir state space _ or simply _ state space _ is @xmath211 .",
    "the input weights and the bias are fixed at random values and are not subject to modification through training .",
    "the output weights @xmath212 are learnt .",
    "the reservoir weights @xmath50 are learnt in some of the case studies below , in others they remain fixed at their initial random values .",
    "if they are learnt , they are adapted from a random initialization denoted by @xmath12 .",
    "figure [ fig0 ] * a * illustrates the basic setup .",
    "i will call the driving signals @xmath14 _ patterns_. in most parts of this report , patterns will be periodic .",
    "periodicity comes in two variants .",
    "integer - periodic _ patterns have the property that @xmath213 for some positive integer @xmath118 .",
    "second , _ irrational - periodic _ patterns are discretely sampled from continuous - time periodic signals , where the sampling interval and the period length of the continuous - time signal have an irrational ratio .",
    "an example is @xmath214 .",
    "these two sorts of drivers will eventually lead to different kinds of attractors trained into reservoirs : integer - periodic signals with period length @xmath215 yield attractors consisting of @xmath215 points in reservoir state space , while irrational - periodic signals give rise to attracting sets which can be topologically characterized as one - dimensional cycles that are homeomorphic to the unit cycle in @xmath216 .",
    ", an input neuron feeds a driving signal @xmath37 to a `` reservoir '' of @xmath29 neurons which are recurrently connected to each other through connections @xmath50 . from the @xmath8-dimensional neuronal activation state @xmath4 ,",
    "an output signal @xmath104 is read out by connections @xmath212 .",
    "all broken connections are trainable . *",
    "b. * during the initial driving of the reservoir with driver @xmath59 , using initial random weights @xmath12 , neuron @xmath217 produces its signal ( thick gray line ) based on external driving input @xmath37 and feeds from other neurons @xmath4 from within the reservoir ( three shown ) . * c. * after training new reservoir weights @xmath50 , the same neuron should produce the same signal based only on the feeds from other reservoir neurons.,width=140 ]      a basic theme in this report is to develop methods by which a collection of different patterns can be loaded in , and retrieved from , a single reservoir .",
    "the key for these methods is an elementary dynamical phenomenon : if a reservoir is driven by a pattern , the entrained network states are confined to a linear subspace of network state space which is characteristic of the pattern . in this subsection",
    "i illuminate this phenomenon by a concrete example .",
    "this example will be re - used and extended on several occasions throughout this report .",
    "i use four patterns .",
    "the first two are irrational periodic and the last two are integer - periodic : ( 1 ) a sinewave of period @xmath218 sampled at integer times ( pattern @xmath219 ) ( 2 ) a sinewave @xmath220 of period @xmath221 ( period of @xmath219 plus 1 ) , ( 3 ) a random 5-periodic pattern @xmath222 and ( 4 ) a slight variation @xmath223 thereof ( fig . [ fig1 ] left column ) .    a reservoir with @xmath29 neurons is randomly created . at creation time the input weights @xmath15 and the bias @xmath166 are fixed at random values ; these will never be modified thereafter .",
    "the reservoir weights are initialized to random values @xmath12 ; in this first demonstration they will not be subsequently modified either .",
    "the readout weights are initially undefined ( details in section [ secgeneralsetupexpdetail ] ) .    in four successive and independent runs ,",
    "the network is driven by feeding the respective pattern @xmath150 as input ( @xmath224 ) , using the update rule @xmath225 after an initial washout time , the reservoir dynamics becomes entrained to the driver and the reservoir state @xmath48 exhibits an involved nonlinear response to the driver @xmath59 .",
    "after this washout , the reservoir run is continued for @xmath226 steps , and the obtained states @xmath48 are collected into @xmath227 sized state collection matrices @xmath228 for subsequent use .    the second column in fig .",
    "[ fig1 ] shows traces of three randomly chosen reservoir neurons in the four driving conditions .",
    "it is apparent that the reservoir has become entrained to the driving input .",
    "mathematically , this entrainment is captured by the concept of the _ echo state property _ : any random initial state of a reservoir is `` forgotten '' , such that after a washout period the current network state is a function of the driver .",
    "the echo state property is a fundamental condition for rnns to be useful in learning tasks @xcite .",
    "it can be ensured by an appropriate scaling of the reservoir weight matrix .",
    "all networks employed in this report possess the echo state property .     of reservoir signal energies in the principal component directions .",
    "`` leading pc energy '' : close - up on first ten signal energies in linear scale .",
    "notice that the first two panels in each row show discrete - time signals ; points are connected by lines only for better visual appearance.,width=145 ]    a principal component analysis ( pca ) of the 100 reservoir signals reveals that the driven reservoir signals are concentrated on a few principal directions .",
    "concretely , for each of the four driving conditions , the reservoir state correlation matrix was estimated by @xmath229 , and its svd @xmath230 was computed , where the columns of @xmath231 are orthonormal eigenvectors of @xmath232 ( the principal component ( pc ) vectors ) , and the diagonal of @xmath233 contains the singular values of @xmath232 , i.e.  the energies ( mean squared amplitudes ) of the principal signal components . figure [ fig1 ] ( third and last column ) shows a plot of these principal component energies .",
    "the energy spectra induced by the two irrational - period sines look markedly different from the spectra obtained from the two 5-periodic signals .",
    "the latter lead to nonzero energies in exactly 5 principal directions because the driven reservoir dynamics periodically visits 5 states ( the small but nonzero values in the @xmath234 plots in figure [ fig1 ] are artefacts earned from rounding errors in the svd computation ) .",
    "in contrast , the irrational - periodic drivers lead to reservoir states which linearly span all of @xmath235 ( figure [ fig1 ] , upper two @xmath234 plots ) .",
    "all four drivers however share a relevant characteristic ( figure [ fig1 ] , right column ) : the total reservoir energy is concentrated in a quite small number of leading principal directions .    when one inspects the excited reservoir dynamics in these four driving conditions , there is little surprise that the neuronal activation traces look similar to each other for the first two and in the second two cases ( figure [ fig1 ] , second column ) .",
    "this `` similarity '' can be quantified in a number of ways .",
    "noting that the geometry of the `` reservoir excitation space '' in driving condition @xmath52 is characterized by a hyperellipsoid with main axes @xmath231 and axis lengths @xmath236 , a natural way to define a similarity between two such ellipsoids @xmath237 is to put @xmath238 the measure @xmath239 ranges in @xmath240 $ ] .",
    "it is 0 if and only if the reservoir signals @xmath241 populate orthogonal linear subspaces , and it is 1 if and only if @xmath242 for some scaling factor @xmath165 .",
    "the measure @xmath239 can be understood as a generalized squared cosine between @xmath243 and @xmath232 .",
    "figure [ figcompare ] * a * shows the similarity matrix @xmath244 obtained from ( [ eqsimr ] ) .",
    "the similarity values contained in this matrix appear somewhat counter - intuitive , inasmuch as the reservoir responses to the sinewave patterns come out as having similarities of about 0.6 with the 5-periodic driven reservoir signals ; this does not agree with the strong visual dissimilarity apparent in the state plots in figure [ fig1 ] . in section [ sec : simmeasure ] i will introduce another similarity measure which agrees better with intuitive judgement .",
    "based on the data correlation matrices @xmath243 . *",
    "b , c : * similarities based on conceptors @xmath245 for two different values of aperture @xmath43 .",
    "for explanation see text.,title=\"fig:\",width=40 ]   based on the data correlation matrices @xmath243 . *",
    "b , c : * similarities based on conceptors @xmath245 for two different values of aperture @xmath43 . for explanation see text.,title=\"fig:\",width=40 ]    based on the data correlation matrices @xmath243 . *",
    "b , c : * similarities based on conceptors @xmath245 for two different values of aperture @xmath43 .",
    "for explanation see text.,title=\"fig:\",width=40 ]      one of the objectives of this report is a method for storing several driving patterns in a single reservoir , such that these stored patterns can later be retrieved and otherwise be controlled or manipulated . in this subsection",
    "i explain how the initial `` raw '' reservoir weights @xmath12 are adapted in order to `` store '' or `` memorize '' the drivers , leading to a new reservoir weight matrix @xmath50 .",
    "i continue with the four - pattern - example used above .",
    "the guiding idea is to enable the reservoir to re - generate the driven responses @xmath48 _ in the absence of the driving input_. consider any neuron @xmath217 ( fig .",
    "[ fig0]*b * ) . during the driven runs @xmath224 , it has been updated per @xmath246 where @xmath247 is the @xmath171-th row in @xmath12 , @xmath248 is the @xmath171-th element of @xmath15 , and @xmath249 is the @xmath171th bias component .",
    "the objective for determining new reservoir weights @xmath50 is that the trained reservoir should be able to oscillate in the same four ways as in the external driving conditions , but without the driving input .",
    "that is , the new weights @xmath250 leading to neuron @xmath171 should approximate @xmath251 as accurately as possible , for @xmath224 . concretely ,",
    "we optimize a mean square error criterion and compute @xmath252 where @xmath61 is the number of patterns to be stored ( in this example @xmath253 ) .",
    "this is a linear regression task , for which a number of standard algorithms are available .",
    "i employ ridge regression ( details in section [ secgeneralsetupexpdetail ] ) .",
    "the readout neuron @xmath104 serves as passive observer of the reservoir dynamics . the objective to determine its connection weights",
    "@xmath209 is simply to replicate the driving input , that is , @xmath209 is computed ( again by ridge regression ) such that it minimizes the squared error @xmath254 , averaged over time and the four driving conditions .",
    "i will refer to this preparatory training as _ storing _ patterns @xmath59 in a reservoir , and call a reservoir _ loaded _ after patterns have been stored .",
    "how can these stored patterns be individually _ retrieved _ again ?",
    "after all , the storing process has superimposed impressions of all patterns on all of the re - computed connection weights @xmath50 of the network  very much like the pixel - wise addition of different images would yield a mixture image in which the individual original images are hard to discern .",
    "one would need some sort of filter which can disentangle again the superimposed components in the connection weights . in this section",
    "i explain how such filters can be obtained .",
    "the guiding idea is that for retrieving pattern @xmath52 from a loaded reservoir , the reservoir dynamics should be restricted to the linear subspace which is characteristic for that pattern . for didactic reasons",
    "i start with a simplifying assumption ( to be dropped later ) .",
    "assume that there exists a ( low - dimensional ) linear subspace @xmath255 such that all state vectors contained in the driven state collection @xmath228 lie in @xmath256 .",
    "in our example , this is actually the case for the two 5-periodic patterns .",
    "let @xmath257 be the projector matrix which projects @xmath235 on @xmath256",
    ". we may then hope that if we run the loaded reservoir autonomously ( no input ) , constraining its states to @xmath256 using the update rule @xmath258 it will oscillate in a way that is closely related to the way how it oscillated when it was originally driven by @xmath59 .",
    "however , it is not typically the case that the states obtained in the original driving conditions are confined to a proper linear subspace of the reservoir state space . consider the sine driver @xmath38 in our example .",
    "the linear span of the reservoir response state is all of @xmath235 ( compare the @xmath234 pc energy plots in figure [ fig1 ] ) .",
    "the associated projector would be the identity , which would not help to single out an individual pattern in retrieval .",
    "but actually we are not interested in those principal directions of reservoir state space whose excitation energies are negligibly small ( inspect again the quick drop of these energies in the third column , top panel in figure [ fig1 ]  it is roughly exponential over most of the spectrum , except for an even faster decrease for the very first few singular values ) .",
    "still considering the sinewave pattern @xmath38 : instead of @xmath259 we would want a projector that projects on the subspace spanned by a `` small '' number of leading principal components of the `` excitation ellipsoid '' described by the sine - driver - induced correlation matrix @xmath260 .",
    "what qualifies as a `` small '' number is , however , essentially arbitrary .",
    "so we want a method to shape projector - like matrices from reservoir state correlation matrices @xmath232 in a way that we can adjust , with a control parameter , how many of the leading principal components should become registered in the projector - like matrix .    at this point",
    "i give names to the projector - like matrices and the adjustment parameter .",
    "i call the latter the _ aperture _ parameter , denoted by @xmath43 .",
    "the projector - like matrices will be called _",
    "conceptors _ and generally be denoted by the symbol @xmath25 .",
    "since conceptors are derived from the ellipsoid characterized by a reservoir state corrlation matrix @xmath232 , and parametrized by the aperture parameter , i also sometimes write @xmath261 to make this dependency transparent .",
    "there is a natural and convenient solution to meet all the intuitive objectives for conceptors that i discussed up to this point .",
    "consider a reservoir driven by a pattern @xmath150 , leading to driven states @xmath48 collected ( as columns ) in a state collection matrix @xmath228 , which in turn yields a reservoir state correlation matrix @xmath262 .",
    "we define a conceptor @xmath261 with the aid of a cost function @xmath263 , whose minimization yields @xmath261 .",
    "the cost function has two components .",
    "the first component reflects the objective that @xmath25 should behave as a projector matrix for the states that occur in the pattern - driven run of the reservoir .",
    "this component is @xmath264 $ ] , the time - averaged deviation of projections @xmath265 from the state vectors @xmath266 .",
    "the second component of @xmath267 adjusts how many of the leading directions of @xmath232 should become effective for the projection .",
    "this component is @xmath268 .",
    "this leads to the following definition .",
    "[ defconceptor ] let @xmath269 $ ] be an @xmath13 correlation matrix and @xmath270 .",
    "the _ conceptor matrix _",
    "@xmath271 _ associated with _",
    "@xmath23 _ and _ @xmath43 is @xmath272 + \\alpha^{-2}\\,\\|c\\|^2_{\\mbox{\\scriptsize \\emph{fro}}}.\\ ] ]    the minimization criterion ( [ eqdefconceptor ] ) uniquely specifies @xmath46 .",
    "the conceptor matrix can be effectively computed from @xmath23 and @xmath43 .",
    "this is spelled out in the following proposition , which also lists elementary algebraic properties of conceptor matrices :    [ propcompconceptor ] let @xmath273 $ ] be a correlation matrix and @xmath270 .",
    "then ,    1",
    ".   @xmath46 can be directly computed from @xmath23 and @xmath43 by @xmath274 2 .",
    "if @xmath275 is the svd of @xmath23 , then the svd of @xmath46 can be written as @xmath276 , i.e. @xmath25 has the same principal component vector orientation as @xmath23 , 3 .",
    "the singular values @xmath24 of @xmath25 relate to the singular values @xmath22 of @xmath23 by @xmath33 , 4 .",
    "the singular values of @xmath25 range in @xmath277 , 5 .",
    "@xmath23 can be recovered from @xmath25 and @xmath43 by @xmath278    the proof is given in section [ secproofpropcompconceptor ] .",
    "notice that all inverses appearing in this proposition are well - defined because @xmath279 is assumed , which implies that all singular values of @xmath46 are properly smaller than 1 .",
    "i will later generalize conceptors to include the limiting cases @xmath280 and @xmath281 ( section [ secaperturesemantics ] ) .    in practice",
    ", the correlation matrix @xmath269 $ ] is estimated from a finite sample @xmath282 , which leads to the approximation @xmath283 , where @xmath284 is a matrix containing reservoir states @xmath11 collected during a learning run .",
    "figure [ figsingvalsfalloff ] shows the singular value spectra of @xmath46 for various values of @xmath43 , for our example cases of @xmath285 ( irrational - period sine driver ) and @xmath286 ( 5-periodic driver ) .",
    "we find that the nonlinearity inherent in ( [ eq : compconceptor ] ) makes the conceptor matrices come out `` almost '' as projector matrices : the singular values of @xmath25 are mostly close to 1 or close to 0 . in the case of the 5-periodic driver , where the excited network states populate a 5-dimensional subspace of @xmath235",
    ", increasing @xmath43 lets @xmath46 converge to a projector onto that subspace .    .",
    "singular value spectra are shown for the first sinewave pattern and the first 5-periodic random pattern .",
    "for explanation see text.,width=140 ]    if one has a conceptor matrix @xmath287 derived from a pattern @xmath59 through the reservoir state correlation matrix @xmath232 associated with that pattern , the conceptor matrix can be used in an autonomous run ( no external input ) using the update rule @xmath288 where the weight matrix @xmath50 has been shaped by storing patterns among which there was @xmath59 . returning to our example , four conceptors",
    "@xmath289 were computed with @xmath290 and the loaded reservoir was run under rule ( [ eqcconstrainedrule ] ) from a random initial state @xmath291 . after a short washout period , the network settled on stable periodic dynamics which were closely related to the original driving patterns .",
    "the network dynamics was observed through the previously trained output neuron .",
    "the left column in figure [ fig1 ] shows the autonomous network output as a light bold gray line underneath the original driver . to measure the achieved accuracy ,",
    "the autonomous output signal was phase - aligned with the driver ( details in section [ secgeneralsetupexpdetail ] ) and then the nrmse was computed ( insets in figure panels ) .",
    "the nrmses indicate that the conceptor - constrained autonomous runs could successfully separate from each other even the closely related pattern pairs @xmath38 versus @xmath128 and @xmath129 versus @xmath39 .",
    "[ [ a - note - on - terminology . ] ] a note on terminology .",
    "+ + + + + + + + + + + + + + + + + + + + + +    equation ( [ eqcconstrainedrule ] ) shows a main usage of conceptor matrices : they are inserted into the reservoir state feedback loop and cancel ( respectively , dampen ) those reservoir state components which correspond to directions in state space associated with zero ( or small , respectively ) singular values in the conceptor matrix . in most of this report , such a direction - selective damping in the reservoir feedback loop will be effected by way of inserting matrices @xmath25 like in equation ( [ eqcconstrainedrule ] ) . however , inserting a matrix is not the only way by which such a direction - selective damping can be achieved . in section [ secbiolplausible ] , which deals with biological plausibility issues , i will propose a neural circuit which achieves a similar functionality of direction - specific damping of reservoir state components by other means and with slightly differing mathematical properties .",
    "i understand the concept of a `` conceptor '' as comprising any mechanism which effects a pattern - specific damping of reservoir signal components . since in most parts of this report",
    "this will be achieved with conceptor matrices , as in ( [ eqcconstrainedrule ] ) , i will often refer to these @xmath25 matrices as `` conceptors '' for simplicity .",
    "the reader should however bear in mind that the notion of a conceptor is more comprehensive than the notion of a conceptor matrix .",
    "i will not spell out a formal definition of a `` conceptor '' , deliberately leaving this concept open to become instantiated by a variety of computational mechanisms of which only two are formally defined in this report ( via conceptor matrices , and via the neural circuit given in section [ secbiolplausible ] ) .      in figure [ figcompare ] * a * a similarity matrix is presented which compares the excitation ellipsoids represented by the correlation matrices @xmath232 by the similarity metric ( [ eqsimr ] ) .",
    "i remarked at that time that this is not a fully satisfactory metric , because it does not agree well with intuition .",
    "we obtain a more intuitively adequate similiarity metric if conceptor matrices are used as descriptors of `` subspace ellipsoid geometry '' instead of the raw correlation matrices , i.e.  if we employ the metric @xmath292 where @xmath293 is the svd of @xmath261 .",
    "figure [ figcompare ] * b*,*c * shows the similarity matrices arising in our standard example for @xmath290 and @xmath294 . the intuitive dissimilarity between the sinewave and the 5-periodic patterns , and the intuitive similarity between the two sines ( and the two 5-periodic pattern versions , respectively ) is revealed much more clearly than on the basis of @xmath239 .",
    "when interpreting similarities @xmath239 or @xmath295 one should bear in mind that one is not comparing the original driving patterns but the excited reservoir responses .",
    "the minimization criterion ( [ eqdefconceptor ] ) immediately leads to a stochastic gradient online method for adapting @xmath25 :    [ propstochadaptc ] assume that a stationary source @xmath11 of @xmath8-dimensional reservoir states is available .",
    "let @xmath296 be any @xmath13 matrix , and @xmath297 a learning rate .",
    "then the stochastic gradient adaptation @xmath298 will lead to @xmath299 , \\alpha)$ ] .",
    "the proof is straightforward if one employs generally known facts about stochastic gradient descent and the fact that @xmath300 + \\alpha^{-2}\\,\\|c\\|^2_{\\mbox{\\scriptsize { fro}}}$ ] is positive definite quadratic in the @xmath301-dimensional space of elements of @xmath25 ( shown in the proof of proposition [ propcompconceptor ] ) , and hence provides a lyapunov function for the gradient descent ( [ eqstochadaptc ] ) .",
    "the gradient of @xmath300 + \\alpha^{-2}\\,\\|c\\|^2_{\\mbox{\\scriptsize { fro}}}$ ] with respect to @xmath25 is @xmath302 + \\alpha^{-2}\\,\\|c\\|^2_{\\mbox{\\scriptsize { fro } } } = ( i - c)\\,e[xx ' ] - \\alpha^{-2}\\,c,\\ ] ] which immediately yields ( [ eqstochadaptc ] ) .",
    "the stochastic update rule ( [ eqstochadaptc ] ) is very elementary .",
    "it is driven by two components , ( i ) an error signal @xmath303 which simply compares the current state with its @xmath25-mapped value , and ( ii ) a linear decay term .",
    "we will make heavy use of this adaptive mechanism in sections [ secautoc ] _ ff .",
    "_ this observation is also illuminating the intuitions behind the definition of conceptors .",
    "the two components strike a compromise ( balanced by @xmath43 ) between ( i ) the objective that @xmath25 should leave reservoir states from the target pattern unchanged , and ( ii ) @xmath25 should have small weights . in the terminology of machine learning",
    "one could say , `` a conceptor is a regularized identity map '' .",
    "conceptor matrices offer a way to morph rnn dynamics .",
    "suppose that a reservoir has been loaded with some patterns , among which there are @xmath304 and @xmath59 with corresponding conceptors @xmath305 .",
    "patterns that are intermediate between @xmath304 and @xmath59 can be obtained by running the reservoir via ( [ eqcconstrainedrule ] ) , using a linear mixture between @xmath306 and @xmath57 : @xmath307 still using our four - pattern example , i demonstrate how this morphing works out for morphing ( i ) between the two sines , ( ii ) between the two 5-periodic patterns , ( iii ) between a sine and a 5-periodic pattern .",
    "[ [ frequency - morphing - of - sines ] ] frequency morphing of sines + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this demonstration , the morphing was done for the two sinewave conceptors @xmath308 and @xmath309 .",
    "the morphing parameter @xmath310 was allowed to range from @xmath311 to @xmath312 ( ! ) .",
    "the four - pattern - loaded reservoir was run from a random initial state for 500 washout steps , using ( [ eqmorph ] ) with @xmath313 .",
    "then recording was started .",
    "first , the run was continued with the intial @xmath313 for 50 steps .",
    "then , @xmath310 was linearly ramped up from @xmath313 to @xmath314 during 200 steps .",
    "finally , another 50 steps were run with the final setting @xmath314 .",
    "note that morph values @xmath315 and @xmath316 correspond to situations where the reservoir is constrained by the original conceptors @xmath317 and @xmath318 , respectively .",
    "values @xmath319 correspond to interpolation .",
    "values @xmath320 and @xmath321 correspond to extrapolation .",
    "the extrapolation range on either side is twice as long as the interpolation range .",
    "in addition , for eight equidistant values @xmath322 in @xmath323 , the reservoir was run with a mixed conceptor @xmath324 for 500 steps , and the obtained observation signal @xmath19 was plotted in a delay - embedded representation , yielding `` snapshots '' of the reservoir dynamics at these @xmath310 values ( a delay - embedding plot of a 1-dimensional signal @xmath19 creates a 2-dimensional plot by plotting value pairs @xmath325 with a delay @xmath119 chosen to yield an appealing visual appearance ) .    .",
    "black circular dots in the two bottom panels mark the points @xmath315 and @xmath326 , corresponding to situations where the two original conceptors @xmath327 were active in unadulterated form .",
    "top : delay - embedding plots of network observation signal @xmath19 ( delay = 1 step ) .",
    "thick points show 25 plotted points , thin points show 500 points ( appearing as connected line ) .",
    "the eight panels have a plot range of @xmath328 \\times [ -1.4 , 1.4]$ ] .",
    "triangles in center panel mark the morph positions corresponding to the delay embedding `` snapshots '' . center : the network observation signal @xmath19 of a morph run .",
    "bottom : thin black line : the period length obtained from morphing between ( and extrapolating beyond ) the original period lengths .",
    "bold gray line : period lengths measured from the observation signal @xmath19.,width=145 ]    figure [ figmorphsines ] shows the findings .",
    "the reservoir oscillates over the entire inter / extrapolation range with a waveform that is approximately equal to a sampled sine . at the morph values @xmath315 and @xmath316 ( indicated by dots in the figure ) , the system is in exactly the same modes as they were plotted earlier in the first two panels of the left column in figure [ fig1 ] .",
    "accordingly the fit between the original driver s period lenghtes and the autonomously re - played oscillations is as good as it was reported there ( i.e. corresponding to a steady - state nrmse of about 0.01 ) . in the extrapolation range , while the linear morphing of the mixing parameter @xmath310 does not lead to an exact linear morphing of the observed period lengths , still the obtained period lengths steadily continue to decrease ( going left from @xmath315 ) and to increase ( going right from @xmath316 ) .    in sum",
    ", it is possible to use conceptor - morphing to extend sine - oscillatory reservoir dynamics from two learnt oscillations of periods @xmath329 to a range between @xmath330 ( minimal and maximal values of period lengths shown in the figure ) . the post - training sinewave generation thus _ extrapolated _ beyond the period range spanned by the two training samples by a factor of about 4.4 . from a perspective of machine learning",
    "this extrapolation is remarkable . generally speaking , when neural pattern generators are trained from demonstration data ( often done in robotics , e.g. @xcite ) , _",
    "interpolation _ of recallable patterns is what one expects to achieve , while extrapolation is deemed hard .",
    "from a perspective of neurodynamics , it is furthermore remarkable that the dimension of interpolation / extrapolation was the _ speed _ of the oscillation . among the infinity of potential generalization dimensions of patterns , speedup / slowdown of pattern generation has a singular role and is particularly difficult to achieve .",
    "the reason is that speed can not be modulated by postprocessing of some underlying generator s output  the prime generator itself must be modulated @xcite .",
    "frequency adaptation of neural oscillators is an important theme in research on biological pattern generators ( cpgs ) ( reviews : @xcite ) .",
    "frequency adaptation has been modeled in a number of ways , among which ( i ) to use a highly abstracted cpg model in the form of an ode , and regulate speed by changing the ode s time constant ; ( ii ) to use a cpg model which includes a pacemaker neuron whose pace is adaptive ; ( iii ) to use complex , biologically quite detailed , modular neural architectures in which frequency adapatation arises from interactions between modules , sensor - motoric feedback cycles , and tonic top - down input .",
    "however , the fact that humans can execute essentially arbitrary motor patterns at different speeds is not explained by these models .",
    "presumably this requires a generic speed control mechanism which takes effect already at higher ( cortical , planning ) layers in the motor control hierarchy .",
    "conceptor - controlled frequency adaptation might be of interest as a candidate mechanism for such a `` cognitive - level '' generic speed control mechanism .",
    "[ [ shape - morphing - of - an - integer - periodic - pattern ] ] shape morphing of an integer - periodic pattern + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this demonstration , the conceptors @xmath331 and @xmath332 from the 5-periodic patterns @xmath129 and @xmath39 were morphed , again with @xmath333 .",
    "figure [ figmorphrandpattern ] depicts the network observer @xmath19 for a morph run of 95 steps which was started with @xmath313 and ended with @xmath314 , with a linear @xmath310 ramping in between .",
    "it can be seen that the differences between the two reference patterns ( located at the points marked by dots ) become increasingly magnified in both extrapolation segments . at each of the different points in each 5-cycle ,",
    "the `` sweep '' induced by the morphing is however neither linear nor of the same type across all 5 points of the period ( right panel ) .",
    "a simple algebraic rule that would describe the geometric characteristics of such morphings can not be given .",
    "i would like to say , it is `` up to the discretion of the network s nonlinear dynamics '' how the morphing command is interpreted ; this is especially true for the extrapolation range . if reservoirs with a different initial random @xmath12 are used , different morphing geometries arise , especially at the far ends of the extrapolation range ( not shown ) .    .",
    "bottom : network observation from a morphing run .",
    "dots mark the points @xmath315 and @xmath326 , corresponding to situations where the two original conceptors @xmath327 were active in unadulterated form .",
    "the network observation signal @xmath19 is shown .",
    "top : delay - embedding `` snapshots '' .",
    "figure layout similar to figure [ figmorphsines].,width=145 ]    the snapshots displayed in figure [ figmorphrandpattern ] reveal that the morphing sweep takes the reservoir through two bifurcations ( apparent in the transition from snapshot 2 to 3 , and from 7 to 8) . in the intermediate morphing range ( snapshots 3  7 )",
    ", we observe a discrete periodic attractor of 5 points . in the ranges beyond , on both sides",
    "the attracting set becomes topologically homomorphic to a continuous cycle . from a visual inspection",
    ", it appears that these bifurcations `` smoothly '' preserve some geometrical characteristics of the observed signal @xmath19 .",
    "a mathematical characterisation of this phenomenological continuity across bifurcations remains for future investigations .",
    "[ [ heterogeneous - pattern - morphing ] ] heterogeneous pattern morphing + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    figure [ figmorphrand2sine ] shows a morph from the 5-periodic pattern @xmath129 to the irrational - periodic sine @xmath128 ( period length @xmath221 ) .",
    "this time the morphing range was @xmath319 , ( no extrapolation ) .",
    "the figure shows a run with an initial 25 steps of @xmath315 , followed by a 50-step ramp to @xmath316 and a tail of 25 steps at the same @xmath310 level .",
    "one observes a gradual change of signal shape and period along the morph . from a dynamical systems point of view",
    "this gradual change is unexpected .",
    "the reservoir is , mathematically speaking , an autonomous system under the influence of a slowly changing control parameter @xmath310 . on both ends of the morph ,",
    "the system is in an attractor .",
    "the topological nature of the attractors ( seen as subsets of state space ) is different ( 5 isolated points vs.  a homolog of a 1-dim circle ) , so there must be a at least one bifurcation taking place along the morphing route .",
    "such a bifurcations would usually be accompanied by a sudden change of some qualitative characteristic of the system trajectory .",
    "we find however no trace of a dynamic rupture , at least not by visual inspection of the output trajectory .",
    "again , a more in - depth formal characterization of what geometric properties are `` smoothly '' carried through these bifurcations is left for future work .",
    "figure [ figmorphmain ] in section [ secoverview ] is a compound demonstration of the three types of pattern morphing that i here discussed individually .",
    "a possible application for the pattern morphing by conceptors is to effect smooth gait changes in walking robots , a problem that is receiving some attention in that field .    .",
    "figure layout otherwise is as in figure [ figmorphsines ] .",
    ", width=145 ]        here i show how the parameter @xmath43 can be interpreted as a scaling of signal energy , and motivate why i call it `` aperture '' .",
    "we can rewrite @xmath334,\\alpha)$ ] as follows :    @xmath335,\\alpha ) & = & e[xx ' ] ( e[xx ' ] + \\alpha^{-2}i)^{-1 }   = e[(\\alpha x)(\\alpha x ) ' ] ( e[(\\alpha x ) ( \\alpha x ) ' ] + i)^{-1}\\nonumber\\\\ & = & c(e[(\\alpha x)(\\alpha x)'],1 ) = c ( \\alpha^2\\,e[xx'],1).\\end{aligned}\\ ] ]    thus , changing from @xmath336 to @xmath46 can be interpreted as scaling the reservoir data by a factor of @xmath43 , or expressed in another way , as scaling the signal energy of the reservoir signals by a factor of @xmath337 .",
    "this is directly analog to what adjusting the aperture effects in an optical camera . in optics ,",
    "the term _ aperture _ denotes the diameter of the effective lens opening , and the amount of light energy that reaches the film is proportional to the squared aperture .",
    "this has motivated the naming of the parameter @xmath43 as _",
    "it is easy to verify that if @xmath338 and @xmath339 are two versions of a conceptor @xmath25 differing in their apertures @xmath340 , they are related to each other by @xmath341 where we note that @xmath342 is always invertible .",
    "@xmath343 is thus a function of @xmath344 and the ratio @xmath345 .",
    "this motivates to introduce an _ aperture adaptation _",
    "operation @xmath162 on conceptors @xmath25 , as follows : @xmath346 where @xmath347 is the conceptor version obtained from @xmath25 by adjusting the aperture of @xmath25 by a factor of @xmath348 .",
    "specifically , it holds that @xmath349 .",
    "we introduce the notation @xmath350 , which leads to the following easily verified data - based version of ( [ eqsemalpha3 ] ) : @xmath351    when we treat boolean operations further below , it will turn out that the not operation will flip zero singular values of @xmath25 to unit singular values .",
    "because of this circumstance , we admit unit singular values in conceptors and formally define    [ defconceptor ] a _ conceptor matrix _ is a positive semidefinite matrix whose singular values range in @xmath240 $ ] .",
    "we denote the set of all @xmath13 conceptor matrices by @xmath352 .",
    "note that definition [ defconceptor ] defined the concept of a _ conceptor matrix associated with a state correlation matrix _",
    "@xmath23 _ and an aperture _ @xmath43 , while definition [ defconceptor ] specifies the more general class of _ conceptor matrices_. mathematically , conceptor matrices ( as in definition [ defconceptor ] ) are more general than the conceptor matrices associated with a state correlation matrix @xmath23 , in that the former may contain unit singular values .",
    "furthermore , in the context of boolean operations it will also become natural to admit aperture adaptations of sizes @xmath353 and @xmath354 . the inversion in equation ( [ eqsemalpha3 ] )",
    "is not in general well - defined for such @xmath348 and/or conceptors with unit singular values , but we can generalize those relationships to the more general versions of conceptors and aperture adaptations by a limit construction :    [ deflimitphi ] let @xmath25 be a conceptor and @xmath355 $ ] .",
    "then @xmath356    it is a mechanical exercise to show that the limits in ( [ eq : deflimitphi ] ) exist , and to calculate the singular values for @xmath347 .",
    "the results are collected in the following proposition .",
    "[ propapadapt ] let @xmath276 be a conceptor and @xmath357 the vector of its singular values .",
    "let @xmath355 $ ] .",
    "then @xmath358 is the conceptor with singular values @xmath359 , where    @xmath360    since aperture adaptation of @xmath361 only changes the singular values of @xmath25 , the following fact is obvious :    [ proptransforminvarianceapadapt ] if @xmath362 is orthonormal , then @xmath363 .",
    "iterated application of aperture adaptation corresponds to multiplying the adaptation factors :    [ propiterateapadapt ] let @xmath25 be a conceptor and @xmath364 $ ]",
    ". then @xmath365 .",
    "the proof is a straightforward algebraic verification using ( [ eqpropadapt ] ) .",
    "borrowing again terminology from photography , i call a conceptor with svd @xmath276 _ hard _ if all singular values in @xmath176 are 0 or 1 ( in photography , a film with an extremely `` hard '' gradation yields pure black - white images with no gray tones . )",
    "note that @xmath25 is hard if and only if it is a projector matrix .",
    "if @xmath25 is hard , the following holds : @xmath366.\\label{eqsemalpha3c}\\end{aligned}\\ ] ] the first claim amounts to stating that @xmath25 is a projection operator , which is obviously the case ; the second claim follows directly from ( [ eqpropadapt ] ) .",
    "besides the aperture , another illuminating characteristic of a conceptor matrix is the mean value of its singular values , i.e.  its normalized trace @xmath367 .",
    "it ranges in @xmath240 $ ] .",
    "intuitively , this quantity measures the fraction of dimensions from the @xmath8-dimensional reservoir state space that is claimed by @xmath25 .",
    "i will call it the _ quota _ of @xmath25 .      in applications",
    "one will often need to adapt the aperture to optimize the quality of @xmath25 .",
    "what `` quality '' means depends on the task at hand .",
    "i present an illustrative example , where the reservoir is loaded with very fragile patterns . retrieving them",
    "requires a prudent choice of @xmath43 . specifically , i loaded a reservoir of size @xmath368 with four chaotic patterns , derived from the well - known rssler , lorenz , mackey - glass , and hnon attractors ( details of this example are given in section [ secexpdetailapertureadapt ] ) .",
    "four conceptors @xmath369 were computed , one for each attractor , using @xmath370 . then , in four retrieval experiments , the aperture of each of these was adapted using ( [ eqsemalpha3 ] ) in a geometric succession of five different @xmath348 , yielding five versions of each of the @xmath371 .",
    "each of these was used in turn for a constrained run of the reservoir according to the state update rule @xmath372 , and the resulting output observation was plotted in a delay - embedding format .",
    "the plot range is @xmath240\\times[0,1]$ ] in every panel . *",
    "a *  * c * are attractors derived from differential equations , hence subsequent points are joined with lines ; * d * derives from an iterated map where joining lines has no meaning .",
    "each 6-panel block shows five patterns generated by the reservoir under the control of differently aperture - adapted versions of a conceptor ( blue , first five panels ) and a plot of the original chaotic reference signal ( green , last panel ) .",
    "empty panels indicate that the @xmath19 signal was outside the @xmath240 $ ] range .",
    "the right upper panel in each block shows a version which , judged by visual inspection , comes satisfactorily close to the original .",
    "first number given in a panel : aperture @xmath43 ; second number : quota @xmath374.,title=\"fig:\",width=65 ] @xmath373@xmath373 .",
    "the plot range is @xmath240\\times[0,1]$ ] in every panel . *",
    "a *  * c * are attractors derived from differential equations , hence subsequent points are joined with lines ; * d * derives from an iterated map where joining lines has no meaning .",
    "each 6-panel block shows five patterns generated by the reservoir under the control of differently aperture - adapted versions of a conceptor ( blue , first five panels ) and a plot of the original chaotic reference signal ( green , last panel ) .",
    "empty panels indicate that the @xmath19 signal was outside the @xmath240 $ ] range .",
    "the right upper panel in each block shows a version which , judged by visual inspection , comes satisfactorily close to the original .",
    "first number given in a panel : aperture @xmath43 ; second number : quota @xmath374.,title=\"fig:\",width=65 ] @xmath375 @xmath375 @xmath373 .",
    "the plot range is @xmath240\\times[0,1]$ ] in every panel . *",
    "a *  * c * are attractors derived from differential equations , hence subsequent points are joined with lines ; * d * derives from an iterated map where joining lines has no meaning .",
    "each 6-panel block shows five patterns generated by the reservoir under the control of differently aperture - adapted versions of a conceptor ( blue , first five panels ) and a plot of the original chaotic reference signal ( green , last panel ) .",
    "empty panels indicate that the @xmath19 signal was outside the @xmath240 $ ] range .",
    "the right upper panel in each block shows a version which , judged by visual inspection , comes satisfactorily close to the original .",
    "first number given in a panel : aperture @xmath43 ; second number : quota @xmath374.,title=\"fig:\",width=65 ] @xmath373@xmath373    figure [ figchaosclover ] displays the findings . per each attractor , the five apertures were hand - selected such that the middle one ( the third ) best re - generated the original chaotic signal , while the first failed to recover the original .",
    "one should mention that it is not trivial in the first place to train an rnn to stably generate any single chaotic attractor timeseries , but here we require the loaded network to be able to generate any one out of four such signals , only by constraining the reservoir by a conceptor with a suitably adapted aperture .",
    "number insets in the panels of figure [ figchaosclover ] indicate the apertures and quotas used per run .",
    "the four chaotic attractors considered in the previous subsection were `` best '' ( according to visual inspection ) reconstructed with apertures between 630 and 1000 . a well - chosen aperture is clearly important for working with conceptors . in all demonstrations reported",
    "so far i chose a `` good '' aperture based on experimentation and human judgement . in practice one",
    "will often need automated criteria for optimizing the aperture which do not rely on human inspection . in this subsection",
    "i propose two measures which can serve as such a guiding criterion .    *",
    "a criterion based on reservoir - conceptor interaction . * introducing an interim state variable @xmath376 by splitting the conceptor - constrained reservoir update equation ( [ eqcconstrainedrule ] ) into @xmath377",
    "i define the _ attenuation _ measurable @xmath165 as @xmath378 / e[\\|z(n ) \\|^2],\\ ] ] where the states @xmath379 are understood to result from a reservoir constrained by @xmath46 .",
    "the attenuation is the fraction of the reservoir signal energy which is suppressed by applying the conceptor .",
    "another useful way to conceive of this quantity is to view it as noise - to - signal ratio , where the `` noise '' is the component @xmath380 which is filtered away from the unconstrained reservoir signal @xmath376 .",
    "it turns out in simulation experiments that when the aperture is varied , the attenuation @xmath381 passes through a minimum , and at this minimum , the pattern reconstruction performance peaks .    in figure",
    "[ figblindout]*a * the @xmath234 of @xmath381 is plotted for a sweep through a range of apertures @xmath43 , for each of the four chaotic attractor conceptors ( details in section [ secexpdetailapertureadapt ] ) .",
    "as @xmath43 grows , the attenuation @xmath381 first declines roughly linearly in the log - log plots , that is , by a power law of the form @xmath382 .",
    "then it enters or passes through a trough .",
    "the aperture values that yielded visually optimal reproductions of the chaotic patterns coincide with the point where the bottom of the trough is reached .",
    "* b * dependancy of attenuation on aperture for the two sinewaves ( top panels ) and the two 5-point periodic patterns ( bottom ) used in sections [ sec : initialdrivingdemo]_ff .",
    "_ these plots also provide the nrmses for the accuracy of the reconstructed patterns ( gray ) . for explanation see text .",
    ", title=\"fig:\",width=65 ] @xmath373@xmath383 . *",
    "b * dependancy of attenuation on aperture for the two sinewaves ( top panels ) and the two 5-point periodic patterns ( bottom ) used in sections [ sec : initialdrivingdemo]_ff . _ these plots also provide the nrmses for the accuracy of the reconstructed patterns ( gray ) . for explanation see text .",
    ", title=\"fig:\",width=65 ]    figure [ figblindout]*b * gives similar plots for the two irrational - period sines and the two 5-point periodic patterns treated in earlier sections .",
    "the same reservoir and storing procedures as described at that place were utilized again here .",
    "the dependence of attenuation on aperture is qualitatively the same as in the chaotic attractor example .",
    "the attenuation plots are overlaid with the nrmses of the original drivers vs.  the conceptor - constrained reservoir readout signals .",
    "again , the `` best '' aperture  here quantified by the nrmse  coincides remarkably well with the trough minimum of the attenuation .",
    "some peculiarities visible in the plots * b * deserve a short comment .",
    "( i ) the initial constant plateaus in all four plots result from @xmath384 for the very small apertures in this region , which leads to @xmath385 .",
    "( ii ) the jittery climb of the attenuation towards the end of the plotting range in the two bottom panels is an artefact due to roundoff errors in svd computations which blows up singular values in conceptors which in theory should be zero . without rounding error involved",
    ", the attenuation plots would remain at their bottom value once it is reached .",
    "( iii ) in the top two panels , some time after having passed through the trough the attenuation value starts to decrease again .",
    "this is due to the fact that for the irrational - period sinewave signals , all singular values of the conceptors are nonzero . as a consequence , for increasingly large apertures the conceptors will converge to the identity matrix , which would have zero attenuation .    *",
    "a criterion based on conceptor matrix properties . *",
    "a very simple criterion for aperture - related `` goodness '' of a conceptor can be obtained from monitoring the gradient of the squared frobenius norm @xmath386 with respect to the logarithm of @xmath348 . to get an intuition about the semantics of this criterion , assume that @xmath25 has been obtained from data with a correlation matrix @xmath23 with svd @xmath275 .",
    "then @xmath387 and @xmath388 .",
    "that is , @xmath389 can be seen as obtained from data scaled by a factor of @xmath348 compared to @xmath390 . the criterion @xmath391 therefore measures the sensitivity of ( the squared norm of ) @xmath25 on ( expontential ) scalings of data .",
    "using again photography as a metaphor : if the aperture of a lens is set to the value where @xmath391 is maximal , the sensitivity of the image (= conceptor ) to changes in brightness (= data scaling ) is maximal .",
    "figure [ figapvianorm ] shows the behavior of this criterion again for the standard example of loading two irrational sines and two integer - periodic random patterns .",
    "its maxima coincide largely with the minima of the attenuation criterion , and both with what was `` best '' performance of the respective pattern generator .",
    "the exception is the two integer - periodic patterns ( figure [ figapvianorm ] * b * bottom panels ) where the @xmath392 criterion would suggest a slightly too small aperture .     and [ sec : retrievegeneric ] .",
    "plots show @xmath391 against the @xmath393 of aperture @xmath348 .",
    "figure layout similar as in figure [ figblindout ] * b*. for explanation see text .",
    ", width=90 ]    comments on criteria for guiding aperture selection :    * the two presented criteria based on attenuation and norm gradient are purely heuristic .",
    "a theoretical analysis would require a rigorous definition of `` goodness '' .",
    "since tasks vary in their objectives , such an analysis would have to be carried out on a case - by - case basis for varying `` goodness '' characterizations .",
    "other formal criteria besides the two presented here can easily be construed ( i experimented with dozens of alternatives ( not documented ) , some of which performed as well as the two instances reported here ) .",
    "altogether this appears to be a wide field for experimentation . *",
    "the attenuation - based criterion needs trial runs with the reservoir to be calculated , while the norm - gradient criterion can be computed offline .",
    "the former seems to be particularly suited for pattern - generation tasks where the conceptor - reservoir feedback loop is critical ( for instance , with respect to stability )",
    ". the latter may be more appropriate in machine learning tasks where conceptors are used for classifying reservoir dynamics in a `` passive '' way without coupling the conceptors into the network updates .",
    "i will give an example in section [ subsec : japvow ] .",
    "conceptor matrices can be submitted to operations that can be meaningfully called and , or , and not .",
    "there are two justifications for using these classical logical terms :    syntactical / algebraic : : :    many algebraic laws governing boolean algebras are preserved ; for hard    conceptor matrices the preservation is exact .",
    "semantical : : :    these operations on conceptor matrices correspond dually to operations    on the data that give rise to the conceptors via ( [ eq : compconceptor ] ) .",
    "specifically , the or operation can be semantically interpreted on the    data level by merging two datasets , and the not operation by inverting    the principal component weights of a dataset . the and operation can be    interpreted on the data level by combining de morgan s rule ( which    states that @xmath394 ) with the semantic interpretations of or and not .    the mathematical structures over conceptors that arise from the boolean operations are richer than standard boolean logic , in that aperture adaptation operations can be included in the picture .",
    "one obtains a formal framework which one might call `` adaptive boolean logic '' .",
    "there are two major ways how such a theory of conceptor logic may be useful :    a logic for information processing in rnns ( cognitive and neuroscience ) : : :    the dynamics of any @xmath8-dimensional rnn ( of any kind ,    autonomously active or driven by external input ) , when monitored for    some time period @xmath395 , yields an @xmath396    sized state collection matrix @xmath282 and its corresponding    @xmath13 correlation matrix @xmath23 , from    which a conceptor matrix @xmath397 can be obtained    which is a `` fingerprint '' of the activity of the network for this    period .",
    "the boolean theory of conceptors can be employed to analyse    the relationships between such `` activity fingerprints '' obtained at    different intervals , different durations , or from different driving    input .",
    "an interesting long - term research goal for cognitive    neuroscience would be to map the logical structuring described on the    network data level , to boolean operations carried out by    task - performing subjects .",
    "an algorithmical tool for rnn control ( machine learning ) : : :    by controlling the ongoing activity of an rnn in a task through    conceptors which are derived from logical operations , one can    implement `` logic control '' strategies for rnns .",
    "examples will be    given in section [ subsec : memmanage ] , where boolean operations on    conceptors will be key for an efficient memory management in rnns ; in    section [ subsec : japvow ] , where boolean operations will enable to    combine positive and negative evidences for finite - duration pattern    recognition ; and in section [ sechierarchicalarchitecture ] , where    boolean operations will help to simultaneously de - noise and classify    signals .",
    "defining boolean operators through their data semantics is transparent and simple when the concerned data correlation matrices are nonsingular . in this case , the resulting conceptor matrices are nonsingular too and have singular values ranging in the open interval @xmath398 .",
    "i treat this situation in this subsection .",
    "however , conceptor matrices with a singular value range of @xmath240 $ ] frequently arise in practice .",
    "this leads to technical complications which will be treated in the next subsection .",
    "the definitions given in the present subsection are preliminary and serve expository purposes .    in the remainder of this subsection",
    ", conceptor matrices @xmath399 are assumed to derive from nonsingular correlation matrices .",
    "i begin with or .",
    "recall that a conceptor matrix @xmath25 ( with aperture 1 ) derives from a data source ( network states ) @xmath4 through @xmath269 , c = c(r,1 ) = r(r+i)^{-1}$ ] . now consider a second conceptor @xmath87 of the same dimension @xmath8 as @xmath25 , derived from another data source @xmath104 by @xmath400 , b = b(q,1 ) = q(q+i)^{-1}$ ] .",
    "i define    @xmath401    and name this the or operation .",
    "observe that @xmath402[x , y]']$ ] , where @xmath403 $ ] is the @xmath404 matrix made of vectors @xmath405 .",
    "@xmath406 is thus obtained by a merge of the two data sources which previously went into @xmath25 and @xmath87 , respectively .",
    "this provides a semantic interpretation of the or operation .    using ( [ eq : compconceptor ] ) , it is straightforward to verify that @xmath406 can be directly computed from @xmath25 and @xmath87 by @xmath407 where the assumption of nonsingular @xmath408 warrants that all inverses in this equation are well - defined .",
    "i now turn to the not operation . for @xmath409 with nonsingular @xmath23",
    "i define it by @xmath410    again this can be semantically interpreted on the data level .",
    "consider the svds @xmath411 and @xmath412 . @xmath23 and",
    "@xmath413 have the same principal components @xmath414 , but the variances @xmath415 of data that would give rise to @xmath23 and @xmath413 are inverse to each other .",
    "in informal terms , @xmath416 can be seen as arising from data which co - vary inversely compared to data giving rise to @xmath25 .",
    "like in the case of or , the negation of @xmath25 can be computed directly from @xmath25 .",
    "it is easy to see that @xmath417    finally , i consider and .",
    "again , we introduce it on the data level .",
    "let again @xmath418 .",
    "the or operation was introduced as addition on data correlation matrices , and the not operation as inversion .",
    "guided by de morgan s law @xmath419 from boolean logic , we obtain a correlation matrix @xmath420 for @xmath421 . via ( [ eq : compconceptor ] ) , from this correlation matrix we are led to    @xmath422    re - expressing @xmath423 in terms of @xmath399 in this equation , elementary transformations ( using ( [ eq : compconceptor ] ) ) again allow us to compute and directly :    @xmath424    by a routine transformation of equations , it can be verified that the de morgan s laws @xmath425 and @xmath426 hold for the direct computation expressions ( [ eqcompor ] ) , ( [ eqcompnot ] ) and ( [ eqcompand ] ) .",
    "we notice that the direct computations ( [ eqcompor ] ) and ( [ eqcompand ] ) for or and and are only well - defined for conceptor matrices whose singular values range in @xmath398 .",
    "i now generalize the definitions for and and or to cases where the concerned conceptors may contain singular values 0 or 1 .",
    "since the direct computation ( [ eqcompand ] ) of and is simpler than the direct computation ( [ eqcompor ] ) of or , i carry out the generalization for and and then transfer it to or through de morgan s rule .",
    "assume that @xmath427 are the svds of conceptors @xmath428 , where @xmath176 and/or @xmath429 may contain zero singular values .",
    "the direct computation ( [ eqcompand ] ) is then not well - defined .",
    "specifically , assume that @xmath430 contains @xmath431 nonzero singular values and that @xmath432 contains @xmath433 nonzero singular values , i.e.  @xmath434 and @xmath435 .",
    "let @xmath436 be a positive real number .",
    "define @xmath437 to be the diagonal matrix which has a diagonal @xmath438 , and similarly @xmath439 to have diagonal @xmath440 .",
    "put @xmath441 .",
    "then @xmath442 is well - defined .",
    "we now define    @xmath443    the limit in this equation is well - defined and can be resolved into an efficient algebraic computation :    [ propanddef ] let @xmath444 be a matrix whose columns form an arbitrary orthonormal basis of @xmath445 . then , the matrix @xmath446 is invertible , and the limit ( [ eqcompandgendef ] ) exists and is equal to @xmath447 equivalently , let @xmath448 be the projector matrix on @xmath449 . then @xmath450 can also be written as @xmath451    the proof and an algorithm to compute a basis matrix @xmath444 are given in section [ secproofanddef ] .    the formulas ( [ eqcompandgencomp ] ) and ( [ eqcompandgencomp_1 ] ) not only extend the formula ( [ eqcompand ] ) to cases where @xmath25 or @xmath87 are non - invertible , but also ensures numerical stability in cases where @xmath25 or @xmath87 are ill - conditioned .",
    "in that situation , the pseudoinverses appearing in ( [ eqcompandgencomp ] ) , ( [ eqcompandgencomp_1 ] ) should be computed with appropriate settings of the numerical tolerance which one can specify in common implementations ( for instance in matlab ) of the svd .",
    "one should generally favor ( [ eqcompandgencomp ] ) over ( [ eqcompand ] ) unless one can be sure that @xmath25 and @xmath87 are well - conditioned .",
    "the direct computation ( [ eqcompnot ] ) of not is well - defined for @xmath25 with a singular value range @xmath240 $ ] , thus nothing remains to be done here .",
    "having available the general and numerically robust computations of and via ( [ eqcompandgencomp ] ) or ( [ eqcompandgencomp_1 ] ) and of not via ( [ eqcompnot ] ) , we invoke de morgan s rule @xmath452 to obtain a general and robust computation for or on the basis of ( [ eqcompandgencomp ] ) resp .",
    "( [ eqcompandgencomp_1 ] ) and ( [ eqcompnot ] ) . summarizing",
    ", we obtain the final definitions for boolean operations on conceptors :    [ def : finalboolean ] @xmath453 where @xmath454 is the projector matrix on @xmath445 .",
    "this definition is consistent with the preliminary definitions given in the previous subsection . for and",
    "this is clear : if @xmath25 and @xmath87 are nonsingular , @xmath455 is the identity and the pseudoinverse is the inverse , hence ( [ eqcompand ] ) is recovered ( fact 1 ) .",
    "we noted in the previous subsection that de morgan s laws hold for conceptors derived from nonsingular correlation matrices ( fact 2 ) .",
    "furthermore , if @xmath25 is derived from a nonsingular correlation matrix , then @xmath416 also corresponds to a nonsingular correlation matrix ( fact 3 ) . combining facts 1  3 yields that the way of defining or via de morgan s rule from and and not in definition [ def : finalboolean ] generalises ( [ eqdefor])/([eqcompor ] ) .    for later use i state a technical result which gives a characterization of or in terms of a limit over correlation matrices :    [ proplimitor ] for a conceptor matrix @xmath25 with svd @xmath361",
    "let @xmath456 be a version of @xmath176 where all unit singular values ( if any ) have been replaced by @xmath457 , and let @xmath458 .",
    "let @xmath459 .",
    "similarly , for another conceptor @xmath87 let @xmath460 . then @xmath461    the proof is given in section [ secproofproplimitor ] .",
    "finally i note that de morgan s rule also holds for and ( proof in section [ secproofpropdemorganand ] ) :    [ propdemorganand ] @xmath462      for an @xmath13 matrix @xmath64 , let @xmath463 be the _ identity space _ of @xmath64 .",
    "this is the eigenspace of @xmath64 to the eigenvalue 1 , a linear subspace of @xmath235 .",
    "the identity spaces , null spaces , and ranges of conceptors are related to boolean operations in various ways .",
    "the facts collected here are technical , but will be useful in deriving further results later .",
    "[ propspaces ] let @xmath428 be any conceptor matrices , and @xmath464 hard conceptor matrices of the same dimension . then the following facts hold :    1 .   @xmath465 .",
    "2 .   @xmath466 and @xmath467 and @xmath468 .",
    "3 .   @xmath469 and @xmath470 and @xmath471 .",
    "4 .   @xmath472 and @xmath473 .",
    "5 .   @xmath474 and @xmath475 . 6 .",
    "@xmath476 and @xmath477 . 7 .",
    "@xmath478 for @xmath479 and @xmath480 for @xmath481 $ ] and @xmath482 for @xmath481 $ ]",
    "@xmath483 and @xmath484 . 9 .",
    "@xmath485 and @xmath486 are hard .",
    "@xmath487 and @xmath488 . 11 .",
    "@xmath489 . 12 .",
    "@xmath490 . 13 .",
    "@xmath491 . 14 .",
    "@xmath492 . 15 . @xmath493 .    the proof is given in section [ secproofpropspaces ] .",
    "the boolean operations are related to aperture adaptation in a number of ways :    [ propbooleanaperture ] let @xmath428 be @xmath13 sized conceptor matrices and @xmath494 $ ] .",
    "we declare @xmath495 and @xmath496 .",
    "then ,    1 .",
    "@xmath497 , 2 .",
    "@xmath498 , 3 .",
    "@xmath499 , 4 .",
    "@xmath500 , 5 .",
    "@xmath501 .",
    "the proof can be found in section [ secproofpropbooleanaperture ] .",
    "furthermore , with the aid of aperture adaptation and or it is possible to implement an incremental model extension , as follows .",
    "assume that conceptor @xmath25 has been obtained from a dataset @xmath282 comprised of @xmath502 data vectors @xmath4 , via @xmath503 .",
    "then , @xmath10 new data vectors @xmath104 become available , collected as columns in a data matrix @xmath504 .",
    "one wishes to update the original conceptor @xmath25 such that it also incorporates the information from @xmath504 , that is , one wishes to obtain @xmath505 where @xmath506 is the updated correlation matrix obtained by @xmath507 , \\tilde{r } = zz'/(m+n)$ ] .",
    "but now furthermore assume that the original training data @xmath282 are no longer available .",
    "this situation will not be uncommon in applications .",
    "the way to a direct computation of ( [ eqtargetupdatec ] ) is barred . in this situation ,",
    "the extended model @xmath508 can be computed from @xmath509 as follows .",
    "let @xmath510 . then",
    ", @xmath511    these formulas can be verified by elementary transformations using ( [ eq : compconceptor ] ) , ( [ eqrecoverr ] ) , ( [ eqdefor ] ) and ( [ eqsemalpha0c1 ] ) , noting that @xmath25 can not have unit singular values because it is obtained from a bounded correlation matrix @xmath23 , thus @xmath512 is invertible .",
    "many laws from boolean logic carry over to the operations and , or , not defined for conceptors , sometimes with modifications .",
    "[ propbooleanelementarylaws ] let @xmath36 be the @xmath13 identity matrix , @xmath513 the zero matrix , and @xmath514 any conceptor matrices of size @xmath13 ( including @xmath36 or @xmath513 ) .",
    "then the following laws hold :    1 .",
    "de morgan s rules : @xmath515 and @xmath516 .",
    "associativity : @xmath517 and @xmath518 .",
    "commutativity : @xmath519 and @xmath520 .",
    "double negation : @xmath521 .",
    "5 .   neutrality of @xmath513 and @xmath36 : @xmath522 and @xmath523",
    "globality of @xmath513 and @xmath36 : @xmath524 and @xmath525",
    "weighted self - absorption for or : @xmath526 and @xmath527 .",
    "weighted self - absorption for and : @xmath528 and @xmath529 .",
    "the proofs are given in section [ secproofpropbooleanelementarylaws ] . from among the classical laws of boolean logic , the general absorption rules @xmath530 and",
    "the laws of distributivity do _ not _ hold in general for conceptors . however , they hold for hard conceptors , which indeed form a boolean algebra :    [ propboolealg ] the set of hard @xmath13 conceptor matrices , equipped with the operations @xmath85 defined in definition [ def : finalboolean ] , is a boolean algebra with maximal element @xmath531 and minimal element @xmath532 .    the proof is obvious , exploiting the fact that hard conceptors can be identified with ( projectors on ) their identity subspaces ( proposition [ propspaces ] _ 11 .",
    "_ ) , and that the operations @xmath533 correspond to subspace operations @xmath534 ( proposition [ propspaces ] _ 13 . ",
    "it is well known that the linear subspaces of @xmath235 form a boolean algebra with respect to these subspace operations .",
    "while the absorption rules @xmath535 are not valid for conceptor matrices , it is possible to `` invert '' @xmath77 by @xmath81 and vice versa in a way that is reminiscent of absorption rules :    [ proppseudoabsorbtion ] let @xmath536 be conceptor matrices of size @xmath13 .",
    "then ,    1 .",
    "@xmath537 is a conceptor matrix and @xmath538 2 .",
    "@xmath539 is a conceptor matrix and @xmath540    the proof is given in section [ secproofproppseudoabsorbtion ] .",
    "the existence of ( almost ) boolean operations between conceptors suggests that conceptors may be useful as models of _ concepts _ ( extensive discussion in section [ seclogic ] ) . in this subsection",
    "i add substance to this interpretation by introducing an abstraction relationship between conceptors , which allows one to organize a set of conceptors in an abstraction hierarchy .    in order to equip the set @xmath352 of @xmath13 conceptors with an `` abstraction '' relationship",
    ", we need to identify a partial ordering on @xmath352 which meets our intuitive expectations concerning the structure of `` abstraction '' .",
    "a natural candidate is the partial order @xmath541 defined on the set of @xmath13 real matrices by @xmath542 if @xmath543 is positive semidefinite .",
    "this ordering is often called the _",
    "lwner ordering_. i will interpret and employ the lwner ordering as an abstraction relation . the key facts which connect this ordering to boolean operations , and which justify to interpret @xmath541 as a form of logical abstraction , are collected in the following    [ propbasicabstraction ]",
    "let @xmath352 be the set of conceptor matrices of size @xmath8 .",
    "then the following facts hold .    1 .",
    "an @xmath13 matrix @xmath169 is a conceptor matrix if and only if @xmath544 .",
    "@xmath532 is the global minimal element and @xmath545 the global maximal element of @xmath546 .",
    "3 .   @xmath547 if and only if @xmath548 .",
    "4 .   let @xmath549 and @xmath550 .",
    "then @xmath551 is a conceptor matrix and @xmath552 5 .",
    "let again @xmath549 and @xmath547 .",
    "then @xmath553 is a conceptor matrix and @xmath554 6 .   if for @xmath555 it holds that @xmath556 , then @xmath550 .",
    "if for @xmath555 it holds that @xmath557 , then @xmath547 . 8 .   for @xmath558 and",
    "@xmath559 it holds that @xmath560 ; for @xmath561 it holds that @xmath562 .",
    "if @xmath547 , then @xmath563 for @xmath355 $ ] .    the proof is given in section [ secproofpropbasicabstraction ] . the essence of this proposition can be re - expressed succinctly as follows :    [ proploewnerboolean ] for conceptors @xmath536 the following conditions are equivalent :    1 .",
    "2 .   there exists a conceptor @xmath25 such that @xmath564 .",
    "3 .   there exists a conceptor @xmath25 such that @xmath565 .",
    "thus , there is an equivalence between `` going upwards '' in the @xmath541 ordering on the one hand , and merging conceptors by or on the other hand . in standard logic - based knowledge representation formalisms ,",
    "a _ concept _ ( or _ class _ ) @xmath87 is defined to be more _ abstract _ than some other concept / class @xmath169 exactly if there is some concept / class @xmath25 such that @xmath564 .",
    "this motivates me to interpret @xmath541 as an _ abstraction _ ordering on @xmath352 .      in this subsection",
    "i demonstrate the usefulness of boolean operations by introducing a memory management scheme for rnns .",
    "i will show how it is possible    1 .   to store patterns in an rnn _ incrementally",
    "_ : if patterns @xmath566 have already been stored , a new pattern @xmath567 can be stored in addition without interfering with the previously stored patterns , and without having to know them ; 2 .   to maintain a measure of the _ remaining memory capacity _ of the rnn which indicates how many more patterns can still be stored ; 3 .",
    "to _ exploit redundancies _ : if the new pattern is similar in a certain sense to already stored ones , loading it consumes less memory capacity than when the new pattern is dissimilar to the already stored ones .",
    "recall that in the original pattern storing procedure , the initial random weight matrix @xmath12 is recomputed to obtain the weight matrix @xmath50 of the loaded reservoir , such that @xmath568 where @xmath150 is the @xmath52-th pattern signal and @xmath48 is the reservoir state signal obtained when the reservoir is driven by the @xmath52-th pattern . for a transparent memory management ,",
    "it is more convenient to keep the original @xmath12 and record the weight changes into an _ input simulation matrix _ @xmath569 , such that @xmath570 in a non - incremental batch training mode , @xmath569 would be computed by regularized linear regression to minimize the following squared error : @xmath571 where @xmath61 is the number of patterns and @xmath395 is the length of training samples ( after subtracting an initial washout period ) .",
    "trained in this way , the sum @xmath572 would be essentially identical ( up to differences due to using another regularization scheme ) to the weight @xmath50 matrix obtained in the original storing procedure .",
    "in fact , the performance of loading a reservoir with patterns via an input simulation matrix @xmath569 as in ( [ eqdmatupdate ] ) is indistinguishable from what is obtained in the original procedure ( not reported ) .",
    "the present objective is to find a way to compute @xmath569 incrementally , leading to a sequence @xmath573 ( @xmath49 ) of input simulation matrices such that the following conditions are satisfied :    1 .",
    "when the @xmath52-th input simulation matrix @xmath573 is used in conjunction with a conceptor @xmath306 associated with an already stored pattern @xmath304 ( i.e. , @xmath574 ) , the autonomous dynamics @xmath575 re - generates the @xmath171-th pattern .",
    "2 .   in order to compute @xmath576 , one must not use explicit knowledge of the already stored patterns or their conceptors , and one only needs to drive the network a single time with the new pattern @xmath90 in order to obtain the requisite training data .",
    "if two training patterns @xmath577 are identical ( where @xmath578 ) , @xmath579 is obtained .",
    "the network already has stored @xmath304 and does not need to change in order to accomodate to this pattern when it is presented a second time .",
    "we will see that , as a side effect , the third condition also allows the network to exploit redundancies when @xmath59 is similar but not identical to an earlier @xmath304 ; the additional memory consumption is reduced in such cases .    the key idea is to keep track of what parts of the reservoir memory space have already been claimed by already stored patterns , or conversely , which parts of the memory space are still freely disposable .",
    "each pattern @xmath59 is associated with a conceptor @xmath57 .",
    "the `` used - up '' memory space after having stored patterns @xmath91 will be characterized by the disjunction @xmath580 , and the `` still disposable '' space by its complement @xmath100 .",
    "let a raw reservoir with an initial weight matrix @xmath12 be given , as well as training pattern timeseries @xmath150 where @xmath581 and @xmath582 .",
    "then the incremental storing algorithm proceeds as follows .",
    "initialization ( no pattern yet stored ) : : :    @xmath583 .",
    "choose an aperture @xmath43 to be used for all    patterns .",
    "incremental storage of patterns : : :    for @xmath584 do :    +    1 .",
    "drive reservoir with pattern @xmath59 for @xmath395    timesteps using state updates    @xmath585 and collect the states @xmath586    into a @xmath587 sized state collection matrix    @xmath228 . put @xmath588 .",
    "likewise , collect the patterns @xmath589 into    a row vector @xmath590 of length @xmath591 .",
    "( here ,    like elsewhere in this report , i tacitly assume that before one    collects data , the network state has been purged by driving through an    initial washout period ) .    2 .",
    "compute a conceptor for this pattern by    @xmath592 .",
    "compute an @xmath13 matrix    @xmath593 ( subsequently to be added    as an increment to @xmath594 , yielding @xmath595 ) by putting    1 .",
    "@xmath596 ( _ comment : this conceptor    characterizes the `` still disposable '' memory space for learning    @xmath59 _ ) ,    2 .",
    "@xmath597    ( _ comment : this @xmath587 sized matrix contains the    targets for a linear regression to compute _",
    "@xmath598 ) ,    3 .   @xmath599 ( _ comment : this    @xmath587 sized matrix contains the arguments for    the linear regression _ ) ,    4 .",
    "@xmath600 ( _ comment : carry out the regression ,    regularized by @xmath601 _ ) .    4 .",
    "update @xmath569 :    @xmath602 .    5 .",
    "update @xmath169 : @xmath603 .    here is an intuitive explanation of the core ideas in the update step @xmath604 in this algorithm :    1 .   _",
    "step 3(a ) : _",
    "@xmath605 keeps track of the subspace directions that have already been `` consumed '' by the patterns already stored , and its complement @xmath596 represents the `` still unused '' directions of reservoir state space .",
    "step 3(b ) : _ the regression targets for @xmath593 are given by the state contributions of the driving input ( @xmath606 ) , minus what the already installed input simulation matrix already contributes to predicting the input effects from the previous network state ( @xmath607 ) . setting the regression target in this way gives rise to the desired exploitation of redundancies .",
    "if pattern @xmath59 had been learnt before ( i.e. , a copy of it was already presented earlier ) , @xmath608 will result in this step , and hence , @xmath609 .",
    "step 3(c ) : _ the arguments for the linear regression are confined to the projection @xmath610 of the @xmath59-driven network states @xmath228 on the still unused reservoir directions @xmath611 . in this way",
    ", @xmath593 is decoupled from the already installed @xmath594 : @xmath593 exploits for generating its output state only such information that was not used by @xmath594 .",
    "step 3(d ) : _ this is the well - known tychonov - regularized wiener - hopf formula for computing a linear regression of targets @xmath429 on arguments @xmath176 , also known as `` ridge regression '' @xcite . setting the tychonov regularizer to the inverse squared",
    "aperture @xmath601 is not accidental .",
    "it results from the mathematically identical roles of tychonov regularizers and apertures .",
    "the sketched algorithm provides a basic format .",
    "it can be easily extended / refined , for instance by using different apertures for different patterns , or multidimensional input .",
    "it is interesting to note that it is intrinsically impossible to _ unlearn _ patterns selectively and `` decrementally '' .",
    "assume that patterns @xmath612 have been trained , resulting in @xmath613 .",
    "assume that one wishes to unlearn again @xmath614 .",
    "as a result of this unlearning one would want to obtain @xmath615 .",
    "thus one would have to compute @xmath616 from @xmath613 , @xmath617 and @xmath614 ( that is , from @xmath613 and @xmath618 ) , in order to recover @xmath619 .",
    "however , the way to identify @xmath616 from @xmath613 , @xmath617 and @xmath618 is barred because of the redundancy exploitation inherent in step 3(b ) . given only @xmath613 , and not knowing the patterns @xmath620 which must be preserved , there is no way to identify which directions of reservoir space must be preserved to preserve those other patterns .",
    "the best one can do is to put @xmath621 and re - run step 3 using @xmath622 instead of @xmath623 and putting @xmath624 in step 3(b ) .",
    "this leads to a version @xmath625 which coincides with the true @xmath616 only if there was no directional overlap between @xmath618 and the other @xmath57 , i.e. if @xmath626 in the original incremental learning procedure .",
    "to the extent that @xmath614 shared state directions with the other patterns , i.e. to the extent that there was redundancy , unlearning @xmath614 will degrade or destroy patterns that share state directions with @xmath614 .",
    "the incremental pattern learning method offers the commodity to measure how much `` memory space '' has already been used after the first @xmath52 patterns have been stored .",
    "this quantity is the quota @xmath627 . when it approaches 1 , the reservoir is `` full '' and an attempt to store another pattern will fail because the @xmath123 matrix from step 3(a ) will be close to zero .",
    "two demonstrations illustrate various aspects of the incremental storing procedure .",
    "both demonstrations used an @xmath29 sized reservoir , and @xmath628 patterns were stored . in the first demonstration , the patterns ( sines or random patterns ) were periodic with integer period lengths ranging between 3 and 15 . in the second demonstration ,",
    "the patterns were sinewaves with irrational period lengths chosen between 4 and 20 .",
    "details are documented in section [ secexpdetailmemmanage ] .",
    "figures [ memmanfig1 ] and [ memmanfig2 ] plot characteristic impressions from these demonstrations .     for convenience ) .",
    "13 patterns with integer period lengths ranging between 3 and 15 were stored .",
    "patterns were sinewaves with integer periods or random .",
    "patterns @xmath629 are identical to @xmath103 .",
    "each panel shows a 20-timestep sample of the correct training pattern @xmath59 ( black line ) overlaid on its reproduction ( green line ) .",
    "the memory fraction used up until pattern @xmath52 is indicated by the panel fraction filled in red ; the quota value is printed in the left bottom corner of each panel .",
    "the red areas in each panel in fact show the singular value spectrum of @xmath93 ( 100 values , @xmath4 scale not shown ) .",
    "the nrmse is inserted in the bottom right corners of the panels .",
    "for detail see text.,width=145 ]    .",
    "for detail see text.,width=145 ]    comments on the demonstrations :    demonstration 1 .",
    ": :    when a reservoir is driven with a signal that has an integer period ,    the reservoir states ( after an initial washout time ) will entrain to    this period , i.e.  every neuron likewise will exhibit an    integer - periodic activation signal .",
    "thus , if the period length of    driver @xmath52 is @xmath502 , the correlation matrix    @xmath232 as well as the conceptor @xmath57 will be    matrices of rank @xmath502 .",
    "an aperture @xmath630 was used in this demonstration .",
    "the large size of this    aperture and the fact that @xmath232 has rank @xmath502    leads to a conceptor @xmath57 which comes close to a    projector matrix , i.e.  it has @xmath502 singular values that are    close to one and @xmath631 zero singular values .",
    "furthermore ,    if a new pattern @xmath90 is presented , the periodic    reservoir state vectors arising from it will generically be linearly    independent of all state vectors that arose from earlier drivers .",
    "both    effects together ( almost projector @xmath57 and linear    independence of nonzero principal directions of these    @xmath57 ) imply that the sequence    @xmath632 will essentially be a sequence of    projectors , where @xmath633 will comprise    @xmath502 more dimensions than @xmath634    ( where @xmath502 is the period length of @xmath90 ) .",
    "this becomes clearly apparent in figure [ memmanfig1 ] : the area under    the singular value plot of @xmath93 has an almost rectangular    shape , and the increments from one plot to the next match the periods    of the respective drivers , except for the last pattern , where the    network capacity is almost exhausted .",
    "+    patterns @xmath635 were identical to    @xmath103 . as a consequence ,",
    "when the storage    procedure is run for @xmath635 , @xmath93    remains essentially unchanged  no further memory space is allocated .",
    "+    when the network s capacity is almost exhausted in the sense that the    quota @xmath627 approaches 1 , storing another pattern    becomes inaccurate . in this demo",
    ", this happens for that last pattern    @xmath636 ( see figure [ memmanfig1 ] ) .",
    "demonstration 2 .",
    ": :    when the driver has an irrational period length , the excited reservoir    states will span the available reservoir space    @xmath235 .",
    "each @xmath232 will have only    nonzero singular values , albeit of rapidly decreasing magnitude ( these    tails are so small in magnitude that they are not visible in the first    few plots of @xmath93 in figure [ memmanfig2 ] ) .",
    "the fact that    each driving pattern excites the reservoir in all directions leads to    the `` reverse sigmoid '' kind of shapes of the singular values of the    @xmath93 visible in figure [ memmanfig2 ] .",
    "+    the sinewave drivers @xmath59 were presented in an order with    randomly shuffled period lengths .",
    "a redundancy exploitation effect    becomes apparent : while for the first 8 patterns altogether a quota    @xmath637 was allocated , the next 8 patterns only needed an additional    quota of @xmath638 .",
    "stated in suggestive    terms , at later stages of the storing sequence the network had already    learnt how to oscillate in sinewaves in general , and only needed to    learn in addition how to oscillate at the particular newly presented    frequencies .",
    "an aperture of size @xmath639 was used in    the second demonstration .",
    "the two demonstrations showed that when @xmath10-point periodic patterns are stored , each new pattern essentially claims a new @xmath10-dimensional subspace .",
    "in contrast , each of the irrational - periodic sines affected all of the available reservoir dimensions , albeit to different degrees .",
    "this leads to problems when one tries to store @xmath10-periodic patterns _ after _ irrational - periodic patterns have already been stored .",
    "the latter already occupy all available reservoir state directions , and the new @xmath10-periodic storage candidates can not find completely `` free '' directions .",
    "this leads to poor retrieval qualities for @xmath10-periodic patterns stored on top of irrational - periodic ones ( not shown ) .",
    "the other order  storing irrational - periodic patterns on top of @xmath10-periodic ones  is not problematic .",
    "this problem can be mitigated with the aid of _ autoconceptors _ , which are developed in subsequent sections .",
    "they typically lead to almost hard conceptors with a majority of singular values being zero , and thus  like @xmath10-point periodic patterns  only claim low - dimensional subspaces , leaving unoccupied `` dimension space '' for loading further patterns .    [ [ an - architecture - variant . ] ] an architecture variant .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    when one numerically computes the rank of the input simulation matrix @xmath569 obtained after storing all patterns , one will find that it has rank 1 .",
    "this can be readily explained as follows .",
    "the desired functionality of @xmath569 is to replace the @xmath8-dimensional signals @xmath640 by @xmath641 ( in a condition where the network state @xmath4 is governed by the conceptor @xmath57 ) .",
    "the signal @xmath642 has a rank-1 autocorrelation matrix @xmath643 $ ] .",
    "therefore , also @xmath644 $ ] should have unit rank . since the reservoir states @xmath11 will span all of @xmath235 across the different patterns @xmath52 , a unit rank of @xmath645 $ ] implies a unit rank of @xmath569 .",
    "in fact , the columns of @xmath569 turn out to be scaled versions of @xmath15 , i.e.  @xmath646 for some @xmath8-dimensional _ row _ vector @xmath119 .    furthermore , from @xmath647 we infer @xmath648 : projections of network states on @xmath119 _ predict _ the next input .",
    "this suggests an alternative way to design and train pattern - storing rnns .",
    "the signal @xmath649 is assigned to a newly introduced single neuron @xmath650 .",
    "the system equation for the new design in external driving conditions remains unchanged : @xmath651 however , the autonomous dynamics ( [ eqrunwithd ] ) is replaced by @xmath652 or equivalently @xmath653 the original readout @xmath654 becomes superfluous , since @xmath655 .",
    "figure [ fig : altarchitecture ] is a diagram of the alternative architecture .",
    "note that @xmath119 is the only trainable item in this architecture",
    ".     serves as input unit in external driving conditions : its value is then forced by the external driver @xmath37 . in autonomous runs when @xmath37 is absent ,",
    "the value of @xmath650 is determined by its readout weights @xmath119 from the reservoir .",
    "non - trainable connections are drawn as solid arrows , trainable ones are broken .",
    "the bias vector and the action of conceptors are omitted in this diagram.,width=60 ]    the incremental storing procedure is adapted to the alternative architecture as follows .",
    "initialization ( no pattern yet stored ) : : :    @xmath656 .",
    "choose an aperture @xmath43 to    be used for all patterns .",
    "incremental storage of patterns : : :    for @xmath584 do :    +    1 .   _",
    "( unchanged from original procedure ) _    2 .   _",
    "( unchanged ) _    3 .",
    "compute an @xmath210 vector    @xmath657 by putting    1 .",
    "@xmath596 _ ( unchanged ) _ ,    2 .",
    "@xmath658 _ ( the essential change ) _ ,    3 .",
    "@xmath599 _ ( unchanged ) _ ,    4 .",
    "@xmath659 .    4 .",
    "update @xmath119 :    @xmath660 .    5 .",
    "update @xmath169 : @xmath603    _",
    "( unchanged)_.    for deterministic patterns ( as they were used in the two demonstrations above ) , the alternative procedure should , and does , behave identically to the original one ( not shown ) . for stochastic patterns",
    "it can be expected to be statistically more efficient ( needing less training data for achieving same accuracy ) since it exploits the valuable structural bias of knowing beforehand that @xmath569 should have unit rank and have scaled versions of @xmath15 as columns ( remains to be explored ) .",
    "our demonstrations used 1-dimensional drivers . for @xmath502-dimensional drivers ,",
    "the alternative architecture can be designed in an obvious way using @xmath502 additional neurons @xmath661 .",
    "the alternative architecture has maximal representational efficiency in the following sense . for storing integer - periodic signals @xmath59 , where the sum of periods of the stored signals approaches the network size @xmath8 ( as in demonstration 1 )",
    ", only @xmath8 parameters ( namely , @xmath119 ) have to be trained .",
    "one may object that one also has to store the conceptors @xmath57 in order to retrieve the patterns , i.e.  one has to learn and store also the large number of parameters contained in the conceptors .",
    "we will however see in section [ subseccad ] that conceptors can be re - built on the fly in retrieval situations and need not be stored .",
    "the alternative architecture also lends itself to storing arbitrarily large numbers of patterns on the basis of a single reservoir .",
    "if the used memory quota @xmath627 approaches 1 and the above storing procedure starts stalling , one can add another @xmath650 neuron and continue storing patterns using it .",
    "this however necessitates an additional switching mechanism to select between different available such @xmath650 neurons in training and retrieval ( not yet explored ) .      in this subsection",
    "i present another demonstration of the usefulness of boolean operations on conceptor matrices .",
    "i describe a training scheme for a pattern recognition system which reaches ( or surpasses ) the classification test performance of state - of - the - art recognizers on a widely used benchmark task .",
    "most high - performing existing classifiers are trained in discriminative training schemes .",
    "discriminative classifier training exploits the contrasting differences between the pattern classes .",
    "this implies that if the repertoire of to - be - distinguished patterns becomes extended by a new pattern , the classifier has to be re - trained on the entire dataset , re - visiting training data from the previous repertoire .",
    "in contrast , the system that i present is trained in a `` pattern - local '' scheme which admits an incremental extension of the recognizer if new pattern types were to be included in its repertoire . furthermore , the classifier can be improved in its exploitation phase by incrementally incorporating novel information contained in a newly incoming test pattern .",
    "the key to this local - incremental classifier training is agin boolean operations on conceptors .    unlike in the rest of this report ,",
    "where i restrict the presentation to stationary and potentially infinite - duration signals , the patterns here are nonstationary and of short duration .",
    "this subsection thus also serves as a demonstration how conceptors function with short nonstationary patterns .",
    "i use is the _ japanese vowels _ benchmark dataset .",
    "it has been donated by @xcite and is publicly available at the uci knowledge discovery in databases archive ( http://kdd.ics.uci.edu/ ) .",
    "this dataset has been used in dozens of articles in machine learning as a reference demonstration and thus provides a quick first orientation about the positioning of a new classification learning method .",
    "the dataset consists of 640 recordings of utterances of two successive japanese vowels /ae/ from nine male speakers .",
    "it is grouped in a training set ( 30 recordings from each of the speakers = 270 samples ) and a test set ( 370 further recordings , with different numbers of recordings per speaker ) .",
    "each sample utterance is given in the form of a 12-dimensional timeseries made from the 12 lpc cepstrum coefficients .",
    "the durations of these recordings range between 7 and 29 sampling timesteps .",
    "the task is to classify the speakers in the test data , using the training data to learn a classifier .",
    "figure [ figjapdata ] ( top row ) gives an impression of the original data .",
    "i preprocessed the raw data into a standardized format by ( 1 ) shift - scaling each of the twelve channels such that per channel , the minimum / maximum value across all training samples was 0/1 ; ( 2 ) interpolating each channel trace in each sample by a cubic polynomial ; ( 3 ) subsampling these on four equidistant support points .",
    "the same transformations were applied to the test data .",
    "figure [ figjapdata ] ( bottom row ) illustrates the normalized data format .",
    "the results reported in the literature for this benchmark typically reach an error rate ( percentage of misclassifications on the test set ) of about 5  10 test errors ( for instance , @xcite report from 5  12 test misclassifications , all using specialized versions of temporal support vector machines ) .",
    "the best result that i am aware of outside my own earlier attempts @xcite is reported by @xcite who reaches about 4 errors , using refined hidden markov models in a non - discriminative training scheme .",
    "it is however possible to reach zero errors , albeit with an extraordinary effort : in own work @xcite this was robustly achieved by combining the votes of 1,000 rnns which were each independently trained in a discriminative scheme .    here",
    "i present a `` pocket - size '' conceptor - based classification learning scheme which can be outlined as follows :    1 .   a single , small ( @xmath662 units )",
    "random reservoir network is initially created .",
    "this reservoir is driven , in nine independent sessions , with the 30 preprocessed training samples of each speaker @xmath52 ( @xmath663 , and a conceptor @xmath664 is created from the network response ( no `` loading '' of patterns ; the reservoir remains unchanged throughout ) .",
    "3 .   in exploitation , a preprocessed sample @xmath665 from the test set",
    "is fed to the reservoir and the induced reservoir states @xmath11 are recorded and transformed into a single vector @xmath127 . for each conceptor then the _ positive evidence _",
    "quantity @xmath666 is computed .",
    "this leads to a classification by deciding for @xmath667 as the speaker of @xmath665 .",
    "the idea behind this procedure is that if the reservoir is driven by a signal from speaker @xmath52 , the resulting response @xmath127 signal will be located in a linear subspace of the ( transformed , see below ) reservoir state space whose overlap with the ellipsoids given by the @xmath668 is largest for @xmath669 .",
    "4 .   in order to further improve the classification quality , for each speaker @xmath52 also a conceptor @xmath670 is computed .",
    "this conceptor can be understood as representing the event `` not any of the other speakers '' .",
    "this leads to a _ negative evidence _ quantity @xmath671 which can likewise be used as a basis for classification .",
    "5 .   by adding the positive and negative evidences , a _",
    "combined evidence _ is obtained which can be paraphrased as `` this test sample seems to be from speaker @xmath52 and seems not to be from any of the others '' .    in more detail ,",
    "the procedure was implemented as follows .",
    "a 10-unit reservoir system with 12 input units and a constant bias term with the update equation @xmath672 was created by randomly creating the @xmath673 reservoir weight matrix @xmath50 , the @xmath674 input weight matrix @xmath15 and the bias vector @xmath166 ( full specification in section [ subsecjapvowexp ] ) .",
    "furthermore , a random starting state @xmath675 , to be used in every run in training and testing , was created .",
    "then , for each speaker @xmath52 , the conceptor @xmath664 was learnt from the 30 preprocessed training samples @xmath676 ( where @xmath677 ) of this speaker , as follows :    1 .   for each training sample @xmath678 ( @xmath679 ) of this speaker , the system ( [ eqjapvrnnupdate ] )",
    "was run with this input , starting from @xmath680 , yielding four network states @xmath681 .",
    "these states were concatenated with each other and with the driver input into a @xmath682 dimensional vector @xmath683 $ ] .",
    "this vector contains the entire network response to the input @xmath678 and the input itself .",
    "the 30 @xmath684 were assembled as columns into a @xmath685 matrix @xmath686 from which a correlation matrix @xmath687 was obtained .",
    "a preliminary conceptor @xmath688 was computed from @xmath689 ( preliminary because in a later step the aperture is optimized ) .",
    "note that @xmath690 has size @xmath691 .",
    "after all `` positive evidence '' conceptors @xmath690 had been created , preliminary `` negative evidence '' conceptors @xmath692 were computed as @xmath693    an important factor for good classification performance is to find optimal apertures for the conceptors , that is , to find aperture adaptation factors @xmath694 such that final conceptors @xmath695 function well for classification .",
    "a common practice in machine learning would be to optimize @xmath348 by cross - validation on the training data .",
    "this , however , is expensive , and more crucially , it would defy the purpose to design a learning procedure which can be incrementally extended by novel pattern classes without having to re - inspect all training data . instead of cross - validation",
    "i used the @xmath392 criterion described in section [ secapadjustguide ] to find a good aperture .",
    "figure [ figgammaoptjapvow ] shows how this criterion varies with @xmath348 for an exemplary case of a @xmath696 sweep . for each of the nine @xmath690 , the value @xmath697 which maximized @xmath392 was numerically computed , and the mean of these nine values was taken as the common @xmath698 to get the nine @xmath699 .",
    "a similar procedure was carried out to arrive at @xmath700 .     from an exemplary conceptor plotted against the log 2 of candidate aperture adaptations @xmath348.,width=80 ]    the conceptors @xmath701 were then used for classification as follows .",
    "assume @xmath127 is an 88-dimensional combined states - and - input vector as described above , obtained from driving the reservoir with a preprocessed test sample .",
    "three kinds of classification hypotheses were computed , the first only based on @xmath664 , the second based on @xmath702 , and one based on a combination of both . each classification hypothesis is a 9-dimensional vector with `` evidences '' for the nine speakers . call these evidence vectors @xmath703 for the three kinds of classifications .",
    "the first of these was computed by setting @xmath704 , then normalizing @xmath705 to a range of @xmath240 $ ] to obtain @xmath706 .",
    "similarly @xmath707 was obtained from using @xmath708 , and @xmath709 was simply the mean of the two former .",
    "each hypothesis vector leads to a classification decision by opting for the speaker @xmath52 corresponding to the largest component in the hypothesis vector .",
    "this classification procedure was carried out for all of the 370 test cases , giving 370 hypothesis vectors of each of the three kinds .",
    "figure [ figjapvowevidences ] gives an impression .",
    "obtained in a classification learning experiment .",
    "grayscale coding : white = 0 , black = 1 .",
    "each panel shows 370 evidence vectors .",
    "the ( mostly ) black segments along the diagonal correspond to the correct classifications ( test samples were sorted by speaker ) . for explanation see text.,width=150 ]    [ [ results ] ] results : + + + + + + + +    the outlined classification experiment was repeated 50 times with random new reservoirs . on average across the 50 trials ,",
    "the optimal apertures @xmath710 were found as 25.0 / 27.0 ( standard deviations 0.48 / 0.75 ) .",
    "the number of misclassifications for the three types of classification ( positive , negative , combined evidence ) were 8.5 / 5.9 / 4.9 ( standard deviations 1.0 / 0.91 / 0.85 ) .",
    "the training errors for the combined classification ( obtained from applying the classification procedure on the training samples ) was zero in all 50 trials . for comparison ,",
    "a carefully regularized linear classifier based on the same @xmath127 vectors ( detail in section [ subsecjapvowexp ] ) reached 5.1 misclassifications across the 50 trials .    while these results are at the level of state - of - the - art classifiers on this benchmark , this basic procedure can be refined , yielding a significant improvement",
    "the idea is to compute the evidence for speaker @xmath52 based on a conceptor @xmath711 which itself is based on the assumption that the test sample @xmath665 belongs to the class @xmath52 , that is , the computed evidence should reflect a quantity `` if @xmath665 belonged to class @xmath52 , what evidence can we collect under this assumption ? '' . recall that @xmath664 is obtained from the 30 training samples through @xmath712 , where @xmath713 is the correlation matrix of the 30 training coding vectors belonging to speaker @xmath52 . now add the test vector @xmath127 to @xmath686 , obtaining @xmath714 , \\bar{r } = \\bar{z}\\bar{z}'/31 , \\bar{c}^+_j = \\bar{r } ( \\bar{r } + ( \\gamma^+)^{-2}i)^{-1}$ ] , and use @xmath711 in the procedure outlined above instead of @xmath664 .",
    "note that , in application scenarios where the original training data @xmath686 are no longer available at test time , @xmath711 can be directly computed from @xmath664 and @xmath127 through the model update formulas ( [ equpdatec1 ] ) or ( [ equpdatec2 ] ) .",
    "the negative evidence conceptor is accordingly obtained by @xmath715 .",
    "[ [ results - of - refined - classification - procedure ] ] results of refined classification procedure : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    averaged over 50 learn - test trials with independently sampled reservoir weights , the number of misclassifications for the three types of classification ( positive , negative , combined evidence ) were 8.4 / 5.9 / 3.4 ( standard deviations 0.99 / 0.93 / 0.61 ) .",
    "the training misclassification errors for the combined classification was zero in all 50 trials .",
    "the detection of good apertures through the @xmath392 criterion worked well .",
    "a manual grid search through candidate apertures found that a minimum test misclassification rate of 3.0 ( average over the 50 trials ) from the combined classificator was obtained with an aperture @xmath716 for both the positive and negative conceptors .",
    "the automated aperture detection yielded apertures @xmath717 and a ( combined classificator ) misclassification rate of 3.4 , close to the optimum .",
    "* discussion . *",
    "the following observations are worth noting",
    ".    method also applies to static pattern classification .",
    ": :    in the presented classification method , temporal input samples    @xmath665 ( short preprocessed nonstationary timeseries ) were    transformed into static coding vectors @xmath127 as a basis for    constructing conceptors .",
    "these @xmath127 contained the original    input signal @xmath665 plus the state response from a small    reservoir driven by @xmath665 .",
    "the reservoir was only used to    augment @xmath665 by some random nonlinear interaction terms    between the entries in @xmath665 .",
    "conceptors were created and    used in classification without referring back to the reservoir    dynamics .",
    "this shows that conceptors can also be useful in _ static _    pattern classification .",
    ": :    a classification model consisting of learnt conceptors    @xmath701 for @xmath118 classes can be easily    _ extended by new classes _ , because the computations needed for new    @xmath718 only require positive training    samples of the new class .",
    "similarly , an existing model    @xmath701 can be _ extended by new training samples _    without re - visiting original training data by an application of the    model extension formulae ( [ equpdatec1 ] ) or ( [ equpdatec2 ] ) .",
    "in fact ,    the refined classification procedure given above can be seen as an    ad - hoc conditional model extension by the test sample . including an `` other '' class .",
    ": :    given a learnt classification model @xmath701 for    @xmath118 classes it appears straightforward to include an    `` other '' class by including    @xmath719 and recomputing the negative    evidence conceptors from the set @xmath720 via ( [ eqnegconceptor ] ) .",
    "i have not    tried this out yet .",
    "discriminative nature of combined classification .",
    ": :    the classification of the combined type , paraphrased above as `` sample    seems to be from class @xmath52 and seems not to be from any of    the others '' , combines information from all classes into an evidence    vote for a candidate class @xmath52 . generally , in    discriminative learning schemes for classifiers , too , contrasting    information _ between _ the classes",
    "is exploited .",
    "the difference is that    in those schemes , these differences are worked in at learning time ,    whereas in the presented conceptor - based scheme they are evaluated at    test time .",
    "benefits of boolean operations . : :    the three aforementioned points  extensibility ,",
    "`` other '' class ,    discriminative classification  all hinge on the availability of the    not and or operations , in particular , on the associativity of the    latter . computational efficiency .",
    ": :    the computational steps involved in learning and applying conceptors    are constructive .",
    "no iterative optimization steps are involved ( except    that standard implementations of matrix inversion are iterative ) .",
    "this    leads to short computation times .",
    "learning conceptors from the 270    preprocessed data samples , including determining good apertures , took    650 ms and classifying a test sample took 0.7 ms for the basic and 64    ms for the refined procedure ( on a dual - core 2ghz macintosh notebook    computer , using matlab ) . competitiveness .",
    ": :    the test misclassification rate of 3.4 is slightly better than the    best rate of about 4 that i am aware of in the literature outside own    work @xcite . given that the zero error performance in @xcite was    achieved with an exceptionally expensive model ( combining 1,000    independently sampled classifiers ) , which furthermore is trained in a    discriminative setup and thus is not extensible , the attained    performance level , the computational efficiency , and the extensibility    of the conceptor - base model render it a competitive alternative to    existing classification learning methods .",
    "it remains to be seen though    how it performs on other datasets .",
    "regularization by aperture adaptation ?",
    ": :    in supervised classification learning tasks , it is generally important    to regularize models to find the best balance between overfitting and    under - exploiting training data .",
    "it appears that the role of    regularization is here played by the aperture adaptation , though a    theoretical analysis remains to be done .",
    "early stage of research .",
    ": :    the proposed classifier learning scheme was based on numerous ad - hoc    design decisions , and quite different ways to exploit conceptors for    classification are easily envisioned .",
    "thus , in sum , the presented    study should be regarded as no more than a first demonstration of the    basic usefulness of conceptors for classification tasks .        in the preceding sections",
    "i have defined conceptors as transforms @xmath721 of reservoir state correlation matrices @xmath23 . in order to obtain some conceptor @xmath57 which captures a driving pattern @xmath59 , the network was driven by @xmath59 via @xmath722",
    ", the obtained reservoir states were used to compute @xmath232 , from which @xmath57 was computed .",
    "the conceptor @xmath57 could then later be exploited via the conceptor - constrained update rule @xmath723 or its variant @xmath724 .",
    "this way of using conceptors , however , requires that the conceptor matrices @xmath57 are computed at learning time ( when the original drivers are active ) , and they have to be _ stored _ for later usage .",
    "such a procedure is useful and feasible in engineering or machine learning applications , where the conceptors @xmath57 may be written to file for later use .",
    "it is also adequate for theoretical investigations of reservoir dynamics , and logical analyses of relationships between reservoir dynamics induced by different drivers , or constrained by different conceptors .",
    "however , storing conceptor matrices is entirely implausible from a perspective of neuroscience .",
    "a conceptor matrix has the same size as the original reservoir weight matrix , that is , it is as large an entire network ( up to a saving factor of one half due to the symmetry of conceptor matrices ) .",
    "it is hard to envision plausible models for computational neuroscience where learning a new pattern by some rnn essentially would amount to creating an entire new network .",
    "this motivates to look for ways of how conceptors can be used for constraining reservoir dynamics without the necessity to store conceptors in the first place .",
    "the network would have to create conceptors `` on the fly '' while it is performing some relevant task . specifically , we are interested in tasks or functionalities which are relevant from a computational neuroscience point of view .",
    "this objective also motivates to focus on algorithms which are not immediately biologically implausible . in my opinion",
    ", this largely excludes computations which explicitly exploit the svd of a matrix ( although it has been tentatively argued that neural networks can perform principal component analysis @xcite using biologically observed mechanisms ) .    in the next subsections",
    "i investigate a version of conceptors with associated modes of usage where there is no need to store conceptors and where computations are online adaptive and local in the sense that the information necessary for adapting a synaptic weight is available at the concerned neuron .",
    "i will demonstrate the workings of these conceptors and algorithms in two functionalities , ( i ) content - addressable memory ( section [ secautocmemexample ] ) and ( ii ) simultaneous de - noising and classification of a signal ( section [ sechierarchicalarchitecture ] ) .    in this line of modeling ,",
    "the conceptors are created by the reservoir itself at the time of usage .",
    "there is no role for an external engineer or superordinate control algorithm to `` plug in '' a conceptor .",
    "i will speak of _ autoconceptors _ to distinguish these autonomously network - generated conceptors from the conceptors that are externally stored and externally inserted into the reservoir dynamics . in discussions i will sometimes refer to those `` externalistic '' conceptors as _ alloconceptors_.    autoconceptors , like alloconceptors , are positive semidefinite matrices with singular values in the unit interval .",
    "the semantic relationship to data , aperture operations , and boolean operations are identical for allo- and autoconceptors .",
    "however , the way how autoconceptors are generated is different from alloconceptors , which leads to additional constraints on their algebraic characteristics .",
    "the set of autoconceptor matrices is a proper subset of the conceptor matrices in general , as they were defined in definition [ defconceptor ] , i.e. the class of positive semidefinite matrices with singular values ranging in the unit interval .",
    "the additional constraints arise from the circumstance that the reservoir states @xmath11 which shape an autoconceptor @xmath25 are themselves depending on @xmath25 .",
    "the treatment of autoconceptors will be structured as follows .",
    "i will first introduce the basic equations of autoconceptors and their adaptation dynamics ( section [ secautobasiceqs ] ) , demonstrate their working in a of content - addressable memory task ( section [ secautocmemexample ] ) and mathematically analyse central properties of the adaptation dynamics ( section [ subseccad ] ) .",
    "the adaptation dynamics however has non - local aspects which render it biologically implausible . in order to progress toward biologically feasible autoconceptor mechanisms ,",
    "i will propose neural circuits which implement autoconceptor dynamics in ways that require only local information for synaptic weight changes ( section [ secbiolplausible ] ) .",
    "the basic system equation for autoconceptor systems is @xmath725 or variants thereof , like @xmath726 or @xmath727 the latter two for the situation after having patterns stored .",
    "the important novel element in these equations is that @xmath115 is time - dependent .",
    "its evolution will be governed by adaptation rules that i will describe presently .",
    "@xmath115 need not be positive semidefinite at all times ; only when the adaptation of @xmath115 converges , the resulting @xmath25 matrices will have the algebraic properties of conceptors .",
    "one can conceive of the system ( [ eqbasicautoc1 ] ) as a two - layered neural network , where the two layers have the same number of neurons , and where the layers are reciprocally connected by the connection matrices @xmath25 and @xmath50 ( figure [ fig : autocarchitecture ] ) .",
    "the two layers have states @xmath728 the @xmath72 layer has sigmoidal ( here : @xmath16 ) units and the @xmath127 layer has linear ones . the customary reservoir state @xmath4 becomes split into two states @xmath72 and @xmath127 , which can be conceived of as states of two pools of neurons .     and optional readout mechanisms are omitted .",
    "the broken arrow indicates that @xmath25 connections are online adaptive . for explanation see text .",
    ", width=70 ]    in order to determine an adaptation law for @xmath115 , i replicate the line of reasoning that was employed to motivate the design of alloconceptors in section [ sec : retrievegeneric ] .",
    "alloconceptors were designed to act as `` regularized identity maps '' , which led to the defining criterion ( [ eqdefconceptor ] ) in definition [ defconceptor ] : @xmath729 + \\alpha^{-2}\\,\\|c\\|^2_{\\mbox{\\scriptsize fro}}.\\ ] ] the reservoir states @xmath4 that appear in this criterion resulted from the update equation @xmath730 .",
    "this led to the explicit solution ( [ eq : compconceptor ] ) stated in proposition [ propcompconceptor ] : @xmath731 where @xmath23 was the reservoir state correlation matrix @xmath732 $ ] .",
    "i re - use this criterion ( [ eqdefconceptor ] ) , which leads to an identical formula @xmath733 for autoconceptors .",
    "the crucial difference is that now the state correlation matrix @xmath23 depends on @xmath25 : @xmath734 = e[cr(cr ) ' ] = c\\,e[rr']\\,c = : cqc,\\ ] ] where we introduce @xmath735 $ ] .",
    "this transforms the direct computation formula ( [ eq : compconceptor ] ) to a fixed - point equation : @xmath736 which is equivalent to @xmath737 since @xmath738 depends on @xmath72 states , which in turn depend on @xmath127 states , which in turn depend on @xmath25 again , @xmath738 depends on @xmath25 and should be more appropriately be written as @xmath739 . analysing the fixed - point equation @xmath740 is a little inconvenient , and i defer this to section [ subseccad ] .",
    "when one uses autoconceptors , however , one does not need to explicitly solve ( [ eqcfixedpoint ] ) .",
    "instead , one can resort to a version of the incremental adaptation rule ( [ eqstochadaptc ] ) from proposition [ propstochadaptc ] : @xmath741 which implements a stochastic gradient descent with respect to the cost function @xmath742 + \\alpha^{-2}\\,\\|c\\|^2_{\\mbox{\\scriptsize fro}}$ ] . in the new sitation given by ( [ eqbasicautoc1 ] ) ,",
    "the state @xmath127 here depends on @xmath25 .",
    "this is , however , of no concern for using ( [ eqstochadaptc ] ) in practice .",
    "we thus complement the reservoir state update rule ( [ eqbasicautoc1 ] ) with the conceptor update rule ( [ eqstochadaptc ] ) and comprise this in a definition :    [ defautocrnn ] an _ autoconceptive reservoir network _ is a two - layered rnn with fixed weights @xmath743 and online adaptive weights @xmath25 , whose dynamics are given by @xmath744 where @xmath745 is a learning rate and @xmath14 an input signal .",
    "likewise , when the update equation ( [ eqautocrnn1 ] ) is replaced by variants of the kind ( [ eqbasicautoc2 ] ) or ( [ eqbasicautoc3 ] ) , we will speak of autoconceptive networks .",
    "i will derive in section [ subseccad ] that if the driver @xmath37 is stationary and if @xmath115 converges under these rules , then the limit @xmath25 is positive semidefinite with singular values in the set @xmath746 .",
    "singular values asymptotically obtained under the evolution ( [ eqautocrnn2 ] ) are either `` large '' ( that is , greater than 1/2 ) or they are zero , but they can not be `` small '' but nonzero . if the aperture @xmath43 is fixed at increasingly smaller values , increasingly many singular values will be forced to zero . furthermore , the analysis in section [ subseccad ] will also reveal that among the nonzero singular values , the majority will be close to 1 .",
    "both effects together mean that autoconceptors are typically approximately hard conceptors , which can be regarded as an intrinsic mechanism of contrast enhancement , or noise suppression .      in previous sections",
    "i demonstrated how loaded patterns can be retrieved if the associated conceptors are plugged into the network dynamics .",
    "these conceptors must have been stored beforehand .",
    "the actual memory functionality thus resides in whatever mechanism is used to store the conceptors ; furthermore , a conceptor is a heavyweight object with the size of the reservoir itself .",
    "it is biologically implausible to create and `` store '' such a network - like object for every pattern that is to be recalled .    in this section",
    "i describe how autoconceptor dynamics can be used to create content - addressable memory systems . in such systems ,",
    "recall is triggered by a cue presentation of the item that is to be recalled .",
    "the memory system then should in some way autonomously `` lock into '' a state or dynamics which autonomously re - creates the cue item . in the model that will be described below , this `` locking into '' spells out as running the reservoir in autoconceptive mode ( using equations",
    "( [ eqbasicautoc3 ] ) and ( [ eqautocrnn2 ] ) ) , by which process a conceptor corresponding to the cue pattern shapes itself and enables the reservoir to autonomously re - generate the cue .    the archetype of content - addressable neural memories is the hopfield network @xcite . in these networks , the cue is a static pattern ( technically a vector , in demonstrations often an image ) , which typically is corrupted by noise or incomplete .",
    "if the hopfield network has been previously trained on the uncorrupted complete pattern , its recurrent dynamics will converge to an attractor state which re - creates the trained original from the corrupted cue .",
    "this _ pattern completion _",
    "characteristic is the essence of the memory functionality in hopfield networks . in the autoconceptive model ,",
    "the aspect of completion manifests itself in that the cue is a _ brief _ presentation of a dynamic pattern , too short for a conceptor to be properly adapted .",
    "after the cue is switched off , the autoconceptive dynamics continues to shape the conceptor in an entirely autonomous way , until it is properly developed and the reservoir re - creates the cue .",
    "this autoconceptive adaptation is superficially analog to the convergence to an attractor point in hopfield networks .",
    "however , there are important conceptual and mathematical differences between the two models .",
    "i will discuss them at the end of this section .",
    "[ [ demonstration - of - basic - architecture . ] ] demonstration of basic architecture . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to display the core idea of a content - addressable memory , i ran simulations according to the following scheme :    1 .   * loading .",
    "* a collection of @xmath118 patterns @xmath59 ( @xmath747 ) was loaded in an @xmath8-dimensional reservoir , yielding an input simulation matrix @xmath569 as described in equation ( [ eqdmatupdate ] ) , and readout weights @xmath748 , as described in section [ sec : storinggeneric ] .",
    "no conceptors are stored .",
    "* for each pattern @xmath59 , a recall run was executed which consisted of three stages : 1 .",
    "* initial washout . * starting from a zero network state , the reservoir was driven with @xmath59 for @xmath749 steps , in order to obtain a task - related reservoir state .",
    "* cueing . *",
    "the reservoir was continued to be driven with @xmath59 for another @xmath750 steps . during this cueing period ,",
    "@xmath57 was adapted by using @xmath751 , @xmath752 . at the beginning of this period ,",
    "@xmath57 was initialized to the zero matrix . at the end of this period ,",
    "a conceptor @xmath753 was obtained .",
    "* autonomous recall . *",
    "the network run was continued for another @xmath754 steps in a mode where the input was switched off and replaced by the input simulation matrix @xmath569 , and where the conceptor @xmath753 was further adapted autonomously by the autoconceptive update mechanism , via @xmath755 , @xmath756 . at the end of this period ,",
    "a conceptor @xmath757 was available .",
    "* measuring quality of conceptors .",
    "* the quality of the conceptors @xmath758 and @xmath759 was measured by separate offline runs without conceptor adaptation using @xmath760 ; @xmath761 ( or @xmath762 , respectively ) . a reconstructed pattern @xmath763 was obtained and its similarity with the original pattern @xmath59 was quantified in terms of a nrmse .",
    "i carried out two instances of this experiment , using two different kinds of patterns and parametrizations :    4-periodic pattern .",
    ": :    the patterns were random integer - periodic patterns of period 4 , where    per pattern the four pattern points were sampled from a uniform    distribution and then shift - scaled such that the range became    @xmath764 $ ] .",
    "this normalization implies that the patterns    are drawn from an essentially 3-parametric family ( 2 real - valued    parameters for fixing the two pattern values not equal to    @xmath17 or @xmath18 ; one integer parameter for fixing    the relative temporal positioning of the @xmath17 and    @xmath18 values ) .",
    "experiment parameters :    @xmath765 ( full detail in section    [ seccontentaddressableexperiment ] ) .",
    "mix of 2 irrational - period sines .",
    ": :    two sines of period lengths @xmath766 and    @xmath767 were added with random phase angles and    random amplitudes , where however the two amplitudes were constrained    to sum to 1 .",
    "this means that patterns were drawn from a 2-parametric    family .",
    "parameters : @xmath768 .",
    "furthermore ,    during the auto - adaptation period , strong gaussian iid noise was added    to the reservoir state before applying the @xmath16 , with a    signal - to - noise rate of 1 .",
    "( black ) and @xmath759 ( light gray ) .",
    "the `` y and p '' panels show the reconstructed pattern @xmath104 obtained with @xmath759 ( bold light gray ) and the original training pattern @xmath59 ( broken black ) , after optimal phase - alignment . *",
    "b * , * d * plot the pattern reconstruction nrmses in log10 scale for the reconstructions obtained from @xmath758 ( black squares ) and from @xmath759 ( gray crosses ) . for",
    "explanation see text . , title=\"fig:\",width=245 ] * b. * ( black ) and @xmath759 ( light gray ) . the `` y and p ''",
    "panels show the reconstructed pattern @xmath104 obtained with @xmath759 ( bold light gray ) and the original training pattern @xmath59 ( broken black ) , after optimal phase - alignment . *",
    "b * , * d * plot the pattern reconstruction nrmses in log10 scale for the reconstructions obtained from @xmath758 ( black squares ) and from @xmath759 ( gray crosses ) . for",
    "explanation see text .",
    ", title=\"fig:\",width=245 ] + * c. * ( black ) and @xmath759 ( light gray ) . the `` y and p ''",
    "panels show the reconstructed pattern @xmath104 obtained with @xmath759 ( bold light gray ) and the original training pattern @xmath59 ( broken black ) , after optimal phase - alignment . *",
    "b * , * d * plot the pattern reconstruction nrmses in log10 scale for the reconstructions obtained from @xmath758 ( black squares ) and from @xmath759 ( gray crosses ) . for",
    "explanation see text . , title=\"fig:\",width=245 ] * d. * ( black ) and @xmath759 ( light gray ) . the `` y and p ''",
    "panels show the reconstructed pattern @xmath104 obtained with @xmath759 ( bold light gray ) and the original training pattern @xmath59 ( broken black ) , after optimal phase - alignment . *",
    "b * , * d * plot the pattern reconstruction nrmses in log10 scale for the reconstructions obtained from @xmath758 ( black squares ) and from @xmath759 ( gray crosses ) . for",
    "explanation see text .",
    ", title=\"fig:\",width=245 ]    figure [ figcontadress ] illustrates the outcomes of these two experiments .",
    "main observations :    1 .   in all cases , the quality of the preliminary conceptor @xmath758 was very much improved by the subsequent auto - adaptation ( panels * b * , * d * ) , leading to an ultimate pattern reconstruction whose quality is similar to the one that would be obtained from precomputed / stored conceptors .",
    "the effects of the autoconceptive adaptation are reflected in the singular value profiles of @xmath769 versus @xmath757 ( panels * a * , * c * ) .",
    "this is especially well visible in the case of the sine mix patterns ( for the period-4 patterns the effect is too small to show up in the plotting resolution ) . during the short cueing time",
    ", the online adaptation of the conceptor from a zero matrix to @xmath758 only manages to build up a preliminary profile that could be intuitively called `` nascent '' , which then `` matures '' in the ensuing network - conceptor interaction during the autoconceptive recall period .",
    "3 .   the conceptors @xmath757 have an almost rectangular singular value profile . in the next section",
    "i will show that if autoconceptive adaptation converges , singular values are either exactly zero or greater than 0.5 ( in fact , typically close to 1 ) , in agreement with what can be seen here .",
    "autoconceptive adaptation has a strong tendency to lead to almost hard conceptors .",
    "4 .   the fact that adapted autoconceptors typically have a close to rectangular singular value spectrum renders the auto - adaptation process quite immune against even strong state noise .",
    "reservoir state noise components in directions of the nulled eigenvectors are entirely suppressed in the conceptor - reservoir loop , and state noise components within the nonzero conceptor eigenspace do not impede the development of a `` clean '' rectangular profile .",
    "in fact , state noise is even beneficial : it speeds up the auto - adaptation process without a noticeable loss in final pattern reconstruction accuracy ( comparative simulations not documented here ) .",
    "+ this noise robustness however depends on the existence of zero singular values in the adapting autoconceptor @xmath25 . in the simulations reported above , such zeros were present from the start because the conceptor was initialized as the zero matrix .",
    "if it had been initialized differently ( for instance , as identity matrix ) , the auto - adaptation would only asymptotically pull ( the majority of ) singular values to zero , with noise robustness only gradually increasing to the degree that the singular value spectrum of @xmath25 becomes increasingly rectangular .",
    "if noise robustness is desired , it can be reached by additional adaptation mechanisms for @xmath25 . in particular , it is helpful to include a thresholding mechanism : all singular values of @xmath115 exceeding a suitable threshold are set to 1 , all singular values dropping below a certain cutoff are zeroed ( not shown ) .    [ [ exploring - the - effects - of - increasing - memory - load - patterns - from - a - parametrized - family . ] ] exploring the effects of increasing memory load ",
    "patterns from a parametrized family .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a central theme in neural memory research is the capacity of a neural storage system . in order to explore how recall accuracy depends on the memory load , i carried out two further experiments , one for each pattern type .",
    "each of these experiments went along the following scheme :    1 .",
    "create a reservoir .",
    "2 .   in separate trials ,",
    "load this reservoir with an increasing number @xmath118 of patterns ( ranging from @xmath770 to @xmath771 for the 4-period and from @xmath770 to @xmath772 for the mixed sines ) .",
    "3 .   after loading ,",
    "repeat the recall scheme described above , with the same parameters . monitor the recall accuracy obtained from @xmath757 for",
    "the first 10 of the loaded patterns ( if less than 10 were loaded , do it only for these ) .",
    "4 .   in addition , per trial , also try to cue and `` recall '' 10 novel patterns that were drawn randomly from the 4-periodic and mixed - sine family , respectively , and which were not part of the collection loaded into the reservoir . monitor the `` recall '' accuracy of these novel patterns as well .",
    "5 .   repeat this entire scheme 5 times , with freshly created patterns , but re - using always the same reservoir .",
    "( black solid line ) and with @xmath773 ( black broken line ) , as well as of `` recalling '' patterns not contained in the loaded set , again obtained from @xmath114 ( gray solid line ) and with @xmath773 ( gray broken line ) .",
    "error bars indicate 95 % confidence intervals .",
    "both axes are in logarithmic scaling . for explanation see text . , title=\"fig:\",width=245 ] * b. * ( black solid line ) and with @xmath773 ( black broken line ) , as well as of `` recalling '' patterns not contained in the loaded set , again obtained from @xmath114 ( gray solid line ) and with @xmath773 ( gray broken line ) .",
    "error bars indicate 95 % confidence intervals .",
    "both axes are in logarithmic scaling . for explanation see text .",
    ", title=\"fig:\",width=245 ]    figure [ figcontadressloadsweep ] shows the results of these experiments .",
    "the plotted curves are the summary averages over the 10 recall targets and the 5 experiment repetitions .",
    "each plot point in the diagrams thus reflects an average over 50 nrmse values ( except in cases where @xmath774 patterns were stored ; then plotted values correspond to averages over @xmath775 nrmse values for recalling of loaded patterns ) .",
    "i list the main findings :    1 .   for all numbers of stored patterns , and for both",
    "the _ recall loaded patterns _ and _ recall novel patterns _ conditions , the autoconceptive `` maturation '' from @xmath114 to @xmath773 with an improvement of recall accuracy is found again .",
    "2 .   the final @xmath773-based recall accuracy in the _ recall loaded pattern _",
    "condition has a sigmoid shape for both pattern types .",
    "the steepest ascent of the sigmoid ( fastest deterioration of recall accuracy with increase of memory load ) occurs at about the point where the summed quota of all @xmath114 reaches the reservoir size @xmath8  the point where the network is `` full '' according to this criterion ( a related effect was encountered in the incremental loading study reported in section [ subsec : memmanage ] ) . when the memory load is further increased beyond this point ( one might say the network becomes `` overloaded '' ) , the recall accuracy does not break down but levels out on a plateau which still translates into a recall performance where there is a strong similarity between the target signal and the reconstructed one .",
    "3 .   in the _ recall novel patterns _ conditions ,",
    "one finds a steady improvement of recall accuracy with increasing memory load . for large memory loads , the accuracy in the _ recall novel patterns _ condition is virtually the same as in the _ recall loaded patterns _ conditions .",
    "similar findings were obtained in other simulation studies ( not documented here ) with other types of patterns , where in each study the patterns were drawn from a parametrized family .",
    "a crucial characteristic of these experiments is that the patterns were samples from a parametrized family .",
    "they shared a family resemblance .",
    "this mutual relatedness of patterns is exploited by the network : for large numbers @xmath118 of stored patterns , the storing / recall mechanism effectively acquires a model of the entire parametric family , a circumstance revealed by the essentially equal recall accuracy in the _ recall loaded patterns _ and _ recall novel patterns _ conditions .",
    "in contrast , for small @xmath118 , the _ recall loaded patterns _ condition enables a recall accuracy which is superior to the _ recall novel patterns _ condition : the memory system stores / recalls individual patterns .",
    "i find this worth a special emphasis :    * for small numbers of loaded patterns ( before the point of network overloading ) the system stores and recalls individual patterns . _",
    "the input simulation matrix @xmath569 represents * individual * patterns . _ * for large numbers of loaded patterns ( overloading the network ) , the system learns a representation of the parametrized pattern family and can be cued with , and will `` recall '' , any pattern from that family .",
    "_ the input simulation matrix @xmath569 represents the * class * of patterns .",
    "_    at around the point of overloading , the system , in a sense , changes its nature from a mere storing - of - individuals device to a learning - of - class mechanism .",
    "i call this the _ class learning effect_.    [ [ effects - of - increasing - memory - load - mutually - unrelated - patterns . ] ] effects of increasing memory load  mutually unrelated patterns .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a precondition for the class learning effect is that the parametric pattern family is simple enough to become represented by the network .",
    "if the pattern family is too richly structured to be captured by a given network size , or if patterns do not have a family resemblance at all , the effect can not arise . if a network is loaded with such patterns , and then cued with novel patterns , the `` recall '' accuracy will be on chance level ; furthermore , as @xmath118 increases beyond the overloading region , the recall accuracy of patterns contained in the loaded collection will decline to chance level too .    in order to demonstrate this ,",
    "i loaded the same 100-unit reservoir that was used in the 4-periodic pattern experiments with random periodic patterns whose periods ranged from 3 through 9 . while technically this is still a parametric family , the number of parameters needed to characterize a sample pattern is 8 , which renders this family far too complex for a 100-unit reservoir .",
    "figure [ figcontadressloadsweepunrelated ] illustrates what , expectedly , happens when one loads increasingly large numbers of such effectively unrelated patterns .",
    "the nrmse for the _ recall novel patterns _ condition is about 1 throughout , which corresponds to entirely uncorrelated pattern versus reconstruction pairs ; and this nrmse is also approached for large @xmath118 in the _ recall loaded patterns _ condition .    .",
    "for explanation see text .",
    ", width=264 ]    [ [ to - be - or - not - to - be - an - attractor . ] ] to be or not to be an attractor .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    ( this part addresses readers who are familiar with dynamical systems theory . ) in the light of basic concepts provided by dynamical systems theory , and in the light of how hopfield networks are known to function as content - addressable memories , one might conjecture that the cueing / recalling dynamics should be mathematically described as follows :    * the loading process creates a number of attractors ( periodic attractors in our demonstrations ) in the combined conceptor / reservoir dynamics . *",
    "each loaded pattern becomes represented by such an attractor .",
    "* during the cueing phase , the combined conceptor / network state becomes located in the basin of the attractor corresponding to the cue pattern .",
    "* in the remaining recall phase , the combined conceptor / network dynamics converges toward this attractor .",
    "this , roughly , would be the picture if our content - addressable memory would function in an analog way as hopfield networks , with the only difference being that instead of point attractors ( in hopfield networks ) we are now witnessing cyclic attractors . to submit this picture to a more in - depth test",
    ", i re - ran the 4-periodic recall experiment documented in figure [ figcontadressloadsweep ] * a * with some modifications .",
    "only a single experiment was run ( no averaging over different collections of loaded patterns ) , and only patterns contained in the loaded collection were cued / recalled .",
    "five of the loaded patterns were cued ( or less if fewer had been loaded ) and the found accuracy nrmses were averaged .",
    "the recall accuracy was measured offline after 200 , 2000 , and 20000 steps of the post - cue autoconceptive adaptation .",
    "the development of accuracy with recall runtime should be illustrative of convergence characteristics .",
    "this experiment was done in two versions , first with clean cue signals and then again with cues that were overlaid with relatively weak noise ( sampled from the standard normal distribution , scaled by 1/20 ) . if the autoconceptive adaptation would obey the attractor interpretation",
    ", one would expect that both in the noise - free and the noisy cue conditions , a convergence to identical good recall patterns is obtained .",
    "the outcome of this experiment , illustrated in figure [ figcontadressloadsweepconverge ] , reveals that this is not the case . for clean cues ( panel * a * ) , the recall accuracies after 200 , 2000 , 20000 are virtually identical ( and very good for small numbers of loaded patterns )",
    "this is consistent with an interpretation of ( fast ) convergence to an attractor .",
    "if this interpretation were correct , then adding a small amount of noise to the cue should not affect the system s convergence to the same attractor .",
    "however this is not what is obtained experimentally ( panel * b * ) .",
    "the curves for increasing post - cue runlengths cross each other and do not reach the accuracy levels from the clean cue condition . while it is not clear how to interpret this outcome in terms of dynamical systems convergence , it certainly is not convergence to the same attractors ( if they exist ) as in the clean cue case .",
    "* b. *    a tentative explanation of these findings could be attempted as follows ( a refined account will be given in section [ subseccad ] ) .",
    "first , a simplification .",
    "if the adaptation rate @xmath348 in ( [ eqautocrnn2 ] ) is sufficiently small , a separation of timescales between a fast network state dynamics ( [ eqautocrnn1 ] ) and a slow conceptor adaptation dynamics occurs .",
    "it then makes sense to average over states @xmath376 .",
    "let @xmath776 be the fast - timescale average of network states under the governance of a conceptor @xmath25 .",
    "this results in an autonomous continuous - time @xmath301-dimensional dynamical system in @xmath25 parameters , @xmath777 where @xmath778 is a time constant which is of no concern for the qualitative properties of the dynamics .",
    "the dynamics ( [ eqcaveraged ] ) represents the adaptation dynamics of an autoconceptor .",
    "note that this adaptation dynamics does not necessarily preserve @xmath25 as a positive semidefinite matrix , that is , matrices @xmath25 obtained during adaptation need not be conceptor matrices .",
    "however , as proved in the next section , fixed point solutions of ( [ eqcaveraged ] ) are conceptor matrices .",
    "let us consider first a situation where a reservoir has been `` overloaded '' with a finite but large number of patterns from a simple parametric family . according to our simulation findings ,",
    "it is then possible to re - generate with high accuracy any of the infinitely many patterns from the family by cued auto - adaptation of conceptors .",
    "a natural candidate to explain this behavior is to assume that in the dynamical system ( [ eqcaveraged ] ) governing the @xmath25 evolution , the loading procedure has established an instance of what is known as _ line attractor _ or _",
    "plane attractor _ @xcite .",
    "an attractor of this kind is characterized by a manifold ( line or plane or higher - dimensional surface ) , consisting of fixed points of the system dynamics , which attracts trajectories from its neighborhood .",
    "figure [ figplaneattractor ] attempts to visualize this situation . in our case",
    ", the plane attractor would consist of fixed point solutions of ( [ eqcaveraged ] ) , that is , conceptors that will ultimately be obtained when the recall adaptation converges .",
    "the arrows in this figure represent trajectories of ( [ eqcaveraged ] ) .",
    "-dimensional invariant manifold consisting of neutrally stable fixed points is embedded in an @xmath10-dimensional state space of a dynamical system .",
    "the figure shows an example where @xmath779 ( a `` line attractor '' , left ) and an example where @xmath780 ( `` plane attractor '' ) .",
    "trajectories in a neighborhood of the fixed - point manifold are attracted toward it ( arrows show selected trajectories ) . for explanations",
    "see text.,width=453 ]    in cases where the reservoir has been loaded with only a small number of patterns ( from the same parametric family or otherwise ) , no such plane attractor would be created . instead ,",
    "each of the loaded patterns becomes represented by an isolated point attractor in ( [ eqcaveraged ] ) .",
    "i mention again that this picture , while it is in agreement with the simulations done so far , is preliminary and will be refined in section [ subseccad ] .",
    "[ [ discussion.-1 ] ] discussion .",
    "+ + + + + + + + + + +    neural memory mechanisms  how to store patterns in , and retrieve from , neural networks  is obviously an important topic of research .",
    "conceptor - based mechanisms bring novel aspects to this widely studied field .",
    "the paradigmatic model for content - addressable storage of patterns in a neural network is undoubtedly the family of auto - associative neural networks ( aanns ) whose analysis and design was pioneered by palm @xcite and hopfield @xcite ( with a rich history in theoretical neuroscience , referenced in @xcite ) .",
    "most of these models are characterized by the following properties :    * aanns with @xmath8 units are used to store static patterns which are themselves @xmath8-dimensional vectors .",
    "the activity profile of the entire network coincides with the very patterns . in many demonstrations ,",
    "these patterns are rendered as 2-dimensional images . *",
    "the networks are typically employed , after training , in pattern completion or restauration tasks , where an incomplete or distorted @xmath8-dimensional pattern is set as the initial @xmath8-dimensional network state .",
    "the network then should evolve toward a completed or restored pattern state .",
    "* aanns have symmetric connections and ( typically ) binary neurons .",
    "their recurrent dynamics can be formalized as a descent along an energy function , which leads to convergence to fixed points which are determined by the input pattern . *",
    "an auto - associative network is trained from a set of @xmath118 reference patterns , where the network weights are adapted such that the network state energy associated with each training pattern is minimized .",
    "if successful , this leads to an energy landscape over state space which assumes local minima at the network states that are identical to the reference patterns .",
    "the comprehensive and transparent mathematical theory available for aanns has left a strong imprint on our preconceptions of what are essential features of a content - addressable neural memory .",
    "specifically , aann research has settled the way how the task of storing items in an associative memory is framed in the first place : `` given @xmath118 reference patterns , train a network such that in exploitation , these patterns can be reconstructed from incomplete cues '' .",
    "this leads naturally to identifying stored memory items with attractors in the network dynamics .",
    "importantly , memory items are seen as _ discrete _ , individual entities .",
    "for convenience i will call this the `` discrete items stored as attractors '' ( disa ) paradigm .    beyond modeling memory functionality proper",
    ", the disa paradigm is historically and conceptually connected to a wide range of models of neural representations of conceptual knowledge , where attractors are taken as the neural representatives of discrete concepts . to name only three kinds of such models : point attractors ( cell assemblies and bistable neurons ) in the working memory literature @xcite ; spatiotemporal attractors in neural field theories of cortical representation @xcite ; ( lobes of ) chaotic attractors as richly structured object and percept representations @xcite .",
    "attractors , by definition , keep the system trajectory confined within them . since clearly cognitive processes do not become ultimately trapped in attractors , it has been a long - standing modeling challenge to account for `` attractors that can be left again ''  that is , to partly disengage from a strict disa paradigm .",
    "many answers have been proposed .",
    "neural noise is a plausible agent to `` kick '' a trajectory out of an attractor , but a problem with noise is its unspecificity which is not easily reconciled with systematic information processing . a number of alternative `` attractor - like '' phenomena have been considered that may arise in high - dimensional nonlinear dynamics and offer escapes from the trapping problem : _ saddle point dynamics _ or _ homoclinic cycles _",
    "@xcite ; _ chaotic itinerancy _",
    "@xcite ; _ attractor relics _ , _ attractor ruins _ , or _ attractor ghosts _",
    "@xcite ; _ transient attractors _ @xcite ; _ unstable attractors _ @xcite ; _ high - dimensional attractors _ ( initially named _ partial attractors _ ) @xcite ; _ attractor landscapes _ @xcite .",
    "all of these lines of work revolve around a fundamental conundrum : on the one hand , neural representations of conceptual entities need to have some kind of stability  this renders them identifiable , noise - robust , and temporally persistent when needed . on the other hand",
    ", there must be cognitively meaningful mechanisms for a fast switching between neural representational states or modes .",
    "this riddle is not yet solved in a widely accepted way .",
    "autoconceptive plane attractor dynamics may lead to yet another answer .",
    "this kind of dynamics intrinsically combines dynamical stability ( in directions complementary to the plane of attraction ) with dynamical neutrality ( within the plane attractor ) .",
    "however , in the next section we will see that this picture , while giving a good approximation , is too simple .      here",
    "i present a formal analysis of some asymptotic properties of the conceptor adaptation dynamics .",
    "we consider the system of the coupled fast network state updates and slow conceptor adaptation given by    @xmath781    and    @xmath782    where @xmath745 is a learning rate .",
    "when @xmath745 is small enough , the instantaneous state correlation @xmath783 in ( [ som1eq2 ] ) can be replaced by its expectation under @xmath25 fixed at @xmath115 , that is , we consider the dynamical system in time @xmath118 @xmath784 and take the expectation of @xmath785 under this dynamics , @xmath786 & : = & e_k[c(n ) \\tanh(w\\,z(k ) ) \\tanh(w\\,z(k ) ) ' c(n ) ' ] \\\\ & = & c(n ) \\ , e_k[\\tanh(w\\,z(k ) ) \\tanh(w\\,z(k ) ) ' ] c(n ) ' \\;\\;= : \\;\\;c(n)q(n)c(n ) ' , \\end{aligned}\\ ] ] where @xmath787 is a positive semi - definite correlation matrix .",
    "note that @xmath787 is a function of @xmath25 and itself changes on the slow timescale of the @xmath25 adaptation . for further analysis",
    "it is convenient to change to continuous time and instead of ( [ som1eq2 ] ) consider @xmath788    i now investigate the nature of potential fixed point solutions under this dynamics .",
    "if @xmath25 is a fixed point of this dynamics , @xmath789 is constant . in order to investigate the nature of such fixed point solutions , we analyse solutions in @xmath25 for the general fixed point equation associated with ( [ som1eq3 ] ) , i.e.  solutions in @xmath25 of @xmath790 where @xmath738 is some positive semidefinite matrix .",
    "we will denote the dimension of @xmath25 by @xmath8 throughout the remainder of this section .",
    "let @xmath791 be the svd of @xmath738 , where @xmath569 is a diagonal matrix containing the singular values of @xmath738 on its diagonal , without loss of generality in descending order .",
    "then ( [ som1eq4 ] ) is equivalent to @xmath792    we may therefore assume that @xmath738 is in descending diagonal form @xmath569 , analyse solutions of @xmath793 and then transform these solutions @xmath25 of ( [ som1eq6 ] ) back to solutions of ( [ som1eq4 ] ) by @xmath794 . in the remainder we will only consider solutions of ( [ som1eq6 ] ) .",
    "i will characterize the fixed points of this system and analyse their stability properties .",
    "[ [ the - case - alpha-0 . ] ] the case @xmath280 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this degenerate case , neither the discrete - time update rule ( [ som1eq2 ] ) nor the dynamical equation ( [ som1eq3 ] ) is well - defined .",
    "the aperture can not be set to zero in practical applications where ( [ som1eq2 ] ) is used for conceptor adaptation .",
    "however , it is clear that ( i ) for any @xmath279 , @xmath795 is a fixed point solution of ( [ som1eq3 ] ) , and that ( ii ) if we define @xmath796 , then @xmath797 .",
    "this justifies to set , by convention , @xmath795 as the unique fixed point of ( [ som1eq3 ] ) in the case @xmath280 . in practical applications this could be implemented by a reset mechanism : whenever @xmath280 is set by some superordinate control mechanism ,",
    "the online adaptation ( [ som1eq2 ] ) is over - ruled and @xmath115 is immediately set to @xmath513 .",
    "[ [ the - case - alpha - infty . ] ] the case @xmath281 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in the case @xmath281 ( i.e. , @xmath798 ) our task is to characterize the solutions @xmath25 of @xmath799    we first assume that @xmath569 has full rank . fix some @xmath800 .",
    "we proceed to characterize rank-@xmath118 solutions @xmath25 of ( [ som1eq12 ] ) .",
    "@xmath801 is positive semidefinite and has rank @xmath118 , thus it has an svd @xmath802 where @xmath803 is diagonal nonnegative and can be assumed to be in descending order , i.e.  its diagonal is @xmath804 with @xmath805 .",
    "any solution @xmath25 of ( [ som1eq12 ] ) must satisfy @xmath806 , or equivalently , @xmath807 .",
    "it is easy to see that this entails that @xmath808 is of the form @xmath809 for some arbitrary @xmath810 submatrix @xmath169 .",
    "requesting @xmath811 implies @xmath812 and hence @xmath813    since conversely , if @xmath414 is any orthonormal matrix , a matrix @xmath25 of the form given in ( [ som1eq13 ] ) satisfies @xmath814 , any such @xmath25 solves ( [ som1eq12 ] ) .",
    "therefore , the rank-@xmath118 solutions of ( [ som1eq12 ] ) are exactly the matrices of type ( [ som1eq13 ] ) .    if @xmath569 has rank @xmath815 , again we fix a desired rank @xmath118 for solutions @xmath25",
    ". again let @xmath816 , with @xmath803 in descending order .",
    "@xmath803 has a rank @xmath502 which satisfies @xmath817 . from considering @xmath807",
    "it follows that @xmath818 has the form @xmath819 for some @xmath820 submatrix @xmath169 .",
    "since we prescribed @xmath25 to have rank @xmath118 , the rank of @xmath169 is @xmath821 .",
    "let @xmath822 be the @xmath823 submatrix of @xmath414 made from the columns with indices greater than @xmath502 .",
    "we rearrange @xmath824 to @xmath825 from which it follows ( since the diagonal of @xmath803 is zero at positions greater than @xmath502 ) that @xmath826 .",
    "since the diagonal of @xmath569 is zero exactly on positions @xmath827 , this is equivalent to @xmath828    we now find that ( [ som1eq14 ] ) is already sufficient to make @xmath829 solve ( [ som1eq12 ] ) , because a simple algebraic calculation yields @xmath830    we thus have determined the rank-@xmath118 solutions of ( [ som1eq12 ] ) to be all matrices of form @xmath831 , subject to ( i ) @xmath832 , ( ii ) @xmath833 , ( iii ) @xmath834 . elementary considerations",
    "( omitted here ) lead to the following generative procedure to obtain all of these matrices :    1 .",
    "choose @xmath502 satisfying @xmath835 .",
    "2 .   choose a size @xmath836 matrix @xmath837 made from orthonormal columns which is zero in the last @xmath821 rows ( this is possible due to the choice of @xmath502 ) .",
    "3 .   choose an arbitrary @xmath820 matrix @xmath169 of svd form @xmath838 where the diagonal matrix @xmath839 is in ascending order and is zero exactly on the first @xmath840 diagonal positions ( hence @xmath833 ) .",
    "4 .   put @xmath841 this preserves orthonormality of columns , i.e.@xmath842 is still made of orthonormal columns .",
    "furthermore , it holds that @xmath843 .",
    "pad @xmath842 by adding arbitrary @xmath844 further orthonormal colums to the right , obtaining an @xmath13 orthonormal @xmath845",
    "we have now obtained a rank-@xmath118 solution @xmath846 where we have put @xmath414 to be the transpose of the matrix @xmath845 that was previously constructed .",
    "[ [ the - case-0-alpha - infty . ] ] the case @xmath847 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we proceed under the assumption that @xmath848 , that is , @xmath849 .",
    "i first show that any solution @xmath25 of ( [ som1eq6 ] ) is a positive semidefinite matrix .",
    "the matrix @xmath801 is positive semidefinite and therefore has a svd of the form @xmath850 , where @xmath414 is orthonormal and real and @xmath803 is the diagonal matrix with the singular values of @xmath801 on its diagonal , without loss of generality in descending order .",
    "from @xmath851 it follows that @xmath852    @xmath853 and hence @xmath854 are nonsingular because @xmath855 , therefore @xmath856 where @xmath857 is a diagonal matrix , and in descending order since @xmath803 was in descending order .",
    "we therefore know that any solution @xmath25 of ( [ som1eq6 ] ) is of the form @xmath276 , where @xmath414 is the same as in @xmath858 .",
    "from @xmath857 it furthermore follows that @xmath859 for all singular values @xmath24 of @xmath25 , that is , @xmath25 is a conceptor matrix .",
    "we now want to obtain a complete overview of all solutions @xmath361 of ( [ som1eq6 ] ) , expressed in terms of an orthonormal real matrix @xmath414 and a nonnegative real diagonal matrix @xmath176 .",
    "this amounts to finding the solutions in @xmath176 and @xmath414 of @xmath860 subject to @xmath176 being nonnegative real diagonal and @xmath414 being real orthonormal . without loss of generality we furthermore may assume that the entries in @xmath176 are in descending order .",
    "some observations are immediate .",
    "first , the rank of @xmath176 is bounded by the rank of @xmath569 , that is , the number of nonzero diagonal elements in @xmath176 can not exceed the number of nonzero elements in @xmath569 .",
    "second , if @xmath861 is a solution , and @xmath862 is the same as @xmath176 except that some nonzero elements in @xmath176 are nulled , then @xmath863 is also a solution ( to see this , left - right multiply both sides of ( [ som1eq7 ] ) with a thinned - out identity matrix that has zeros on the diagonal positions which one wishes to null ) .",
    "fix some @xmath864 .",
    "we want to determine all rank-@xmath118 solutions @xmath861 , i.e.  where @xmath176 has exactly @xmath118 nonzero elements that appear in descending order in the first @xmath118 diagonal positions .",
    "we write @xmath865 to denote diagonal real matrices of size @xmath866 whose diagonal entries are all positive . furthermore , we write @xmath867 to denote any @xmath868 matrix whose columns are real orthonormal .    it is clear that if @xmath869 solve ( [ som1eq7 ] ) and @xmath870 ( and @xmath176 is in descending order ) , and if @xmath871 differs from @xmath414 only in the last @xmath840 columns , then also @xmath872 solve ( [ som1eq7 ] ) .",
    "thus , if we have all solutions @xmath873 of @xmath874 then we get all rank-@xmath118 solutions @xmath875 to ( [ som1eq6 ] ) by padding @xmath865 with @xmath840 zero rows / columns , and extending @xmath867 to full size @xmath876 by appending any choice of orthonormal columns from the orthogonal complement of @xmath867 .",
    "we therefore only have to characterize the solutions @xmath873 of ( [ som1eq8 ] ) , or equivalently , of @xmath877    to find such @xmath873 , we first consider solutions @xmath878 of @xmath879 subject to @xmath880 being diagonal with positive diagonal elements . for this",
    "we employ the cauchy interlacing theorem and its converse .",
    "i restate , in a simple special case adapted to the needs at hand , this result from @xcite where it is presented in greater generality .",
    "( adapted from theorem 1 in @xcite , see remark of author at the end of the proof of that theorem for a justification of the version that i render here . )",
    "let @xmath536 be two symmetric real matrices with @xmath881 , and singular values @xmath882 and @xmath883 ( in descending order ) .",
    "then there exists a real @xmath196 matrix @xmath414 with @xmath884 and @xmath885 if and only if for @xmath886 it holds that @xmath887    this theorem implies that if @xmath888 is any solution of ( [ som1eq10 ] ) , with @xmath867 made of @xmath118 orthonormal columns and @xmath880 diagonal with diagonal elements @xmath889 ( where @xmath890 , and the enumeration is in descending order ) , then the latter `` interlace '' with the diagonal entries @xmath891 of @xmath569 per @xmath892 and conversely , any diagonal matrix @xmath880 , whose elements interlace with the diagonal elements of @xmath569 , appears in a solution @xmath888 of ( [ som1eq10 ] ) .",
    "equipped with this overview of solutions to ( [ som1eq10 ] ) , we revert from ( [ som1eq10 ] ) to ( [ som1eq9 ] ) . solving @xmath893 for @xmath865",
    "we find that the diagonal elements @xmath24 of @xmath865 relate to the @xmath889 by @xmath894    since @xmath24 must be positive real and smaller than @xmath18 , only such solutions @xmath880 to ( [ som1eq10 ] ) whose entries are all greater than @xmath895 yield admissible solutions to our original problem ( [ som1eq9 ] ) .",
    "the interlacing condition then teaches us that the possible rank of solutions @xmath25 of ( [ som1eq6 ] ) is bounded from above by the number of entries in @xmath569 greater than @xmath895 .    for each value @xmath896 , ( [ som1eq11 ] )",
    "gives two solutions @xmath897 .",
    "we will show further below that the solutions smaller than @xmath898 are unstable while the solutions greater than @xmath898 are stable in a certain sense .    summarizing and adding algorithmic detail",
    ", we obtain all rank-@xmath118 solutions @xmath361 for ( [ som1eq6 ] ) as follows :    1 .",
    "check whether @xmath569 has at least @xmath118 entries greater than @xmath895 .",
    "if not , there are no rank-@xmath118 solutions . if yes , proceed",
    "find a solution in @xmath888 of @xmath899 , with @xmath880 being diagonal with diagonal elements greater than @xmath895 , and interlacing with the elements of @xmath569 .",
    "( note : the proof of theorem 1 in @xcite is constructive and could be used for finding @xmath867 given @xmath880 . )",
    "compute @xmath865 via ( [ som1eq11 ] ) , choosing between the @xmath900 options at will .",
    "pad @xmath867 with any orthogonal complement and @xmath865 with further zero rows and columns to full @xmath194 sized @xmath861 , to finally obtain a rank-@xmath118 solution @xmath361 for ( [ som1eq6 ] ) .",
    "[ [ the - case - alpha - infty.-1 ] ] the case @xmath281 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    note again that @xmath281 is the same as @xmath798 .",
    "we consider the time evolution of the quantity @xmath901 as @xmath25 evolves under the zero-@xmath601 version of ( [ som1eq3 ] ) : @xmath902    we obtain @xmath903 where in the last line we use that the trace of a positive semidefinite matrix is nonnegative .",
    "this finding instructs us that no other than the identity @xmath904 can be a stable solution of ( [ someq7 ] ) , in the sense that all eigenvalues of the associated jacobian are negative .",
    "if @xmath789 has full rank for all @xmath905 , then indeed this is the case ( it is easy to show that @xmath906 is strictly decreasing , hence a lyapunov function in a neighborhood of @xmath904 ) .",
    "the stability characteristics of other ( not full - rank ) fixed points of ( [ someq7 ] ) are intricate .",
    "if one computes the eigenvalues of the jacobian at rank-@xmath118 fixed points @xmath25 ( i.e.  solutions of sort @xmath907 , see ( [ som1eq13 ] ) ) , where @xmath908 , one finds negative values and zeros , but no positive values . ( the computation of the jacobian follows the pattern of the jacobian for the case @xmath848 , see below , but is simpler ; it is omitted here ) .",
    "some of the zeros correspond to perturbation directions of @xmath25 which change only the coordinate transforming matrices @xmath414 .",
    "these perturbations are neutrally stable in the sense of leading from one fixed point solution to another one , and satisfy @xmath909 .",
    "however , other perturbations @xmath910 with the property that @xmath911 lead to @xmath912 .",
    "after such a perturbation , the matrix @xmath910 will evolve toward @xmath36 in the frobenius norm .",
    "since the jacobian of @xmath25 has no positive eigenvalues , this instability is non - hyperbolic . in simulations",
    "one accordingly finds that after a small perturbation @xmath839 is added , the divergence away from @xmath25 is initially extremely slow , and prone to be numerically misjudged to be zero .    for rank - deficient @xmath738 , which leads to fixed points of sort @xmath913 ,",
    "the computation of jacobians becomes involved ( mainly because @xmath169 may be non - symmetric ) and i did not construct them . in our context , where @xmath738 derives from a random rnn , @xmath738 can be expected to have full rank , so a detailed investigation of the rank - deficient case would be an academic exercise .    [ [ the - case-0-alpha - infty.-1 ] ] the case @xmath847 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    this is the case of greatest practical relevance , and i spent a considerable effort on elucidating it .",
    "note that @xmath848 is equivalent to @xmath855 .",
    "let @xmath914 be a rank-@xmath118 fixed point of @xmath915 , where @xmath855 , @xmath916 is the svd of @xmath25 and without loss of generality the singular values @xmath917 in the diagonal matrix @xmath176 are in descending order , with @xmath918 and @xmath919 for @xmath920 ( where @xmath921 ) . in order to understand the stability properties of the dynamics @xmath922 in a neighborhood of @xmath923 , we compute the eigenvalues of the jacobian @xmath924 at point @xmath923 . notice that @xmath25 is an @xmath13 matrix",
    "whose entries must be rearranged into a vector of size @xmath925 in order to arrive at the customary representation of a jacobian .",
    "@xmath926 is thus an @xmath927 matrix which should be more correctly written as @xmath928 , where @xmath929 is the rearrangement operator ( @xmath930 are the indices of the matrix @xmath926 ) .",
    "details are given in section [ secproofprop1 ] within the proof of the following central proposition :    [ prop1 ] the jacobian @xmath931 of a rank-@xmath118 fixed point of ( [ som1eq3 ] ) has the following multiset of eigenvalues :    1 .",
    "@xmath932 instances of 0 , 2 .",
    "@xmath933 instances of @xmath934 , 3 .",
    "@xmath118 eigenvalues @xmath935 , where @xmath936 , 4 .",
    "@xmath937 eigenvalues which come in pairs of the form @xmath938 where @xmath939 .",
    "an inspection of sort _ 3 .",
    "_  eigenvalues reveals that whenever one of the @xmath940 is smaller than @xmath898 , this eigenvalue is positive and hence the fixed point @xmath923 is unstable .",
    "if some @xmath940 is exactly equal to @xmath898 , one obtains additional zero eigenvalues by _ 3 .",
    "_ i will exclude such cases in the following discussion , considering them to be non - generic .",
    "if all @xmath940 are greater than @xmath898 , it is straightforward to show that the values of sorts _ 3 . _  and _ 4 .",
    "_  are negative .",
    "altogether , @xmath941 thus has @xmath932 times the eigenvalue @xmath513 and otherwise negative ones .",
    "i will call such solutions _",
    "1/2-generic_. all solutions that one will effectively obtain when conceptor auto - adaptation converges are of this kind .",
    "this characterization of the eigenvalue spectrum of 1/2-generic solutions does not yet allow us to draw firm conclusions about how such a solution will react to perturbations .",
    "there are two reasons why proposition [ prop1 ] affords but a partial insight in the stability of 1/2-generic solutions .",
    "( a ) the directions connected to zero eigenvalues span a @xmath932-dimensional center manifold whose dynamics remains un - analysed .",
    "it may be stable , unstable , or neutral .",
    "( b ) when a 1/2-generic solution is perturbed , the matrix @xmath569 which reflects the conceptor - reservoir interaction will change : @xmath569 is in fact a function of @xmath25 and should be more correctly written @xmath942 . in our linearization around fixed point solutions we implicitly considered @xmath569 to be constant .",
    "it is unclear whether a full treatment using @xmath943 would lead to a different qualitative picture .",
    "furthermore , ( a ) and ( b ) are liable to combine their effects .",
    "this is especially relevant for the dynamics on the center manifold , because its qualitative dynamics is determined by components from higher - order approximations to ( [ som1eq3 ] ) which are more susceptible to become qualitatively changed by non - constant @xmath569 than the dynamical components of ( [ som1eq3 ] ) orthogonal to the center manifold .    taking ( a ) and ( b ) into account",
    ", i now outline a hypothetical picture of the dynamics of ( [ som1eq3 ] ) in the vicinity of 1/2-generic fixed - point solutions .",
    "this picture is based only on plausibility considerations , but it is in agreement with what i observe in simulations .",
    "first , a dimensional argument sheds more light on the nature of the dynamics in the @xmath932-dimensional center manifold . consider a 1/2-generic rank-@xmath118 solution @xmath361 of ( [ som1eq7 ] ) .",
    "recall that the singular values @xmath24 in @xmath176 were derived from @xmath889 which interlace with the diagonal elements @xmath944 of @xmath569 by @xmath945 , and where @xmath946 ( equation ( [ som1eq10 ] ) ) .",
    "i call @xmath25 a _ 1/2&interlacing - generic _ solution if the interlacing is proper , i.e.  if @xmath947 .",
    "assume furthermore that @xmath948 is constant in a neighborhood of @xmath25 .",
    "in this case , differential changes to @xmath867 in ( [ som1eq10 ] ) will lead to differential changes in @xmath880 . if these changes to @xmath867 respect the conditions ( i ) that @xmath867 remains orthonormal and ( ii ) that @xmath880 remains diagonal , the changes to @xmath867 lead to new fixed point solutions . the first constraint ( i )",
    "allows us to change @xmath867 with @xmath949 degrees of freedom . the second constraint ( ii )",
    "reduces this by @xmath950 degrees of freedom .",
    "altogether we have @xmath951 differential directions of change of @xmath25 that lead to new fixed points .",
    "this coincides with the dimension of the center manifold associated with @xmath25 .",
    "we can conclude that the center manifold of a 1/2&interlacing - generic @xmath25 extends exactly in the directions of neighboring fixed point solutions .",
    "this picture is based however on the assumption of constant @xmath569 .",
    "if the dependency of @xmath569 on @xmath25 is included in the picture , we would not expect to find any other fixed point solutions at all in a small enough neighborhood of @xmath25 .",
    "generically , fixed point solutions of an ode are isolated .",
    "therefore , in the light of the considerations made so far , we would expect to find isolated fixed point solutions @xmath25 , corresponding to close approximations of stored patterns . in a local vicinity of such solutions ,",
    "the autoconceptor adaptation would presumably progress on two timescales : a fast convergence toward the center manifold @xmath952 associated with the fixed point @xmath25 , superimposed on a slow convergence toward @xmath25 within @xmath952 ( figure [ figautoadaptcentermanifoldsketch ] * a * ) .",
    "autoadaptation in the parameter space of @xmath25 ( schematic ) .",
    "blue points show stable fixed point solutions @xmath25 .",
    "gray plane represents the merged center manifold @xmath952 .",
    "green arrows represent sample trajectories of @xmath25 adaptation .",
    "* a. * when a small number of patterns has been loaded , individual stable fixed point conceptors @xmath25 are created . * b. * in the case of learning a @xmath119-parametric pattern class , fixed point solutions @xmath953 become located within a @xmath119-dimensional pattern manifold @xmath954 ( bold magenta line ) . for explanation see text .",
    ", width=491 ]    the situation becomes particularly interesting when many patterns from a @xmath119-parametric class have been stored .",
    "when i first discussed this situation ( section [ secautocmemexample ] ) , i tentatively described the resulting autoadaptation dynamics as convergence toward a plane attractor ( figure [ figplaneattractor ] ) .",
    "however , plane attractors can not be expected to exist in generic dynamical systems .",
    "taking into account what the stability analysis above has revealed about center manifolds of fixed points @xmath25 , i would like to propose the following picture as a working hypothesis for the geometry of conceptor adaptation dynamics that arises when a @xmath119-parametric pattern class has been stored by overloading :    * the storing procedure leads to a number of stable fixed point solutions @xmath953 for the autoconceptor adaptation ( blue dots in figure [ figautoadaptcentermanifoldsketch ] * b * ) .",
    "these @xmath953 are associated with patterns from the pattern family , but need not coincide with the sample patterns that were loaded .",
    "* the @xmath932-dimensional center manifolds of the @xmath953 merge into a comprehensive manifold @xmath952 of the same dimension . in the vicinity of @xmath952",
    ", the autoadaptive @xmath25 evolution leads to a convergence toward @xmath952 . * within @xmath952",
    "a @xmath119-dimensional submanifold @xmath954 is embedded , representing the learnt class of patterns .",
    "notice that we would typically expect @xmath955 ( examples in the previous section had @xmath956 or @xmath957 , but @xmath932 in the order of several 100 ) .",
    "conceptor matrices located on @xmath954 correspond to patterns from the learnt class . * the convergence of @xmath25 adaptation trajectories toward @xmath952",
    "is superimposed with a slower contractive dynamics within @xmath952 toward the class submanifold @xmath954 . *",
    "the combined effects of the attraction toward @xmath952 and furthermore toward @xmath954 appear in simulations as if @xmath954 were acting as a plane attractor . * on an even slower timescale , within @xmath954 there is an attraction toward the isolated fixed point solutions @xmath953 .",
    "this timescale is so slow that the motion within @xmath954 toward the fixed points @xmath953 will be hardly observed in simulations .    in order to corroborate this refined picture , and especially to confirm the last point from the list above",
    ", i carried out a long - duration content - addressable memory simulation along the lines described in section [ secautocmemexample ] .",
    "ten 5-periodic patterns were loaded into a small ( 50 units ) reservoir .",
    "these patterns represented ten stages of a linear morph between two similar patterns @xmath38 and @xmath958 , resulting in a morph sequence @xmath959 where @xmath960 , thus representing instances from a 1-parametric family .",
    "considering what was found in section [ secautocmemexample ] , loading these ten patterns should enable the system to re - generate by auto - adaptation any linear morph @xmath961 between @xmath38 and @xmath958 after being cued with @xmath961 .",
    "auto - adaptation .",
    "each panel shows pairwise distances of 20 conceptor matrices obtained after @xmath10 auto - adaptation steps , after being cued along a 20-step morph sequence of cue signals .",
    "color coding : blue  zero distance ; red  maximum distance . for",
    "explanation see text .",
    ", width=548 ]    after loading , the system was cued with 20 different cues . in each of these @xmath962 conditions ,",
    "the cueing pattern @xmath963 was the @xmath52-th linear interpolation between the stored @xmath38 and @xmath958 .",
    "the cueing was done for 20 steps , following the procedure given at the beginning of section [ secautocmemexample ] . at the end of the cueing",
    ", the system will be securely driven into a state @xmath127 that is very accurately connected to re - generating the pattern @xmath963 , and the conceptor matrix that has developed by the end of the cueing would enable the system to re - generate a close simile of @xmath963 ( a post - cue log10 nrmse of about @xmath964 was obtained in this simulation ) .",
    "after cueing , the system was left running in conceptor auto - adaptation mode using ( [ eqautocrnn2 ] ) for 1 mio timesteps , with an adaptation rate of @xmath965 .    at times",
    "@xmath966 the situation of convergence was assessed as follows .",
    "the pairwise distances between the current twenty autoconceptors @xmath967 were compared , resulting in a @xmath968 distance matrix @xmath969 .",
    "figure [ figautoadaptcentermanifold ] shows color plots of these distance matrices .",
    "the outcome : at the beginning of autoadaptation ( @xmath970 ) , the 20 autoconceptors are spaced almost equally widely from each other . in terms of the schematic in figure [ figautoadaptcentermanifoldsketch ] * b * , they would all be almost equi - distantly lined up close to @xmath954 . then",
    ", as the adaptation time @xmath10 grows , they contract toward three point attractors within @xmath954 ( which would correspond to a version of [ figautoadaptcentermanifoldsketch ] * b * with three blue dots ) .",
    "these three point attractors correspond to the three dark blue squares on the diagonal of the last distance matrix shown in figure [ figautoadaptcentermanifold ] .",
    "this singular simulation can not , of course , provide conclusive evidence that the qualitative picture proposed in figure [ figautoadaptcentermanifoldsketch ] is correct .",
    "a rigorous mathematical characterization of the hypothetical manifold @xmath954 and its relation to the center manifolds of fixed point solutions of the adaptation dynamics needs to be worked out .",
    "plane attractors have been proposed as models for a number of biological neural adaptation processes ( summarized in @xcite ) .",
    "a classical example is gaze direction control .",
    "the fact that animals can fix their gaze in arbitrary ( continuously many ) directions has been modelled by plane attractors in the oculomotoric neural control system .",
    "each gaze direction corresponds to a ( controlled ) constant neural activation profile .",
    "in contrast to and beyond such models , conceptor auto - adaptation organized along a manifold @xmath954 leads not to a continuum of _ constant _ neural activity profiles , but explains how a continuum of _ dynamical _ patterns connected by continuous morphs can be generated and controlled .    in sum ,",
    "the first steps toward an analysis of autoconceptor adaptation have revealed that this adaptation dynamics is more involved than either the classical fixed - point dynamics in autoassociative memories or the plane attractor models suggested in computational neuroscience . for small numbers of stored patterns",
    ", the picture bears some analogies with autoassociative memories in that stable fixed points of the autonomous adaptation correspond to stored patterns . for larger numbers of stored patterns ( class learning ) ,",
    "the plane attractor metaphor captures essential aspects of phenomena seen in simulations of not too long duration .",
    "the autoconceptive update equations @xmath971 could hardly be realized in biological neural systems .",
    "one problem is that the @xmath25 update needs to evaluate @xmath972 , but @xmath376 is not an _ input _ to @xmath115 in the @xmath127 update but the _ outcome _ of applying @xmath25 .",
    "the input to @xmath25 is instead the state @xmath973 . in order to have both computations carried out by the same @xmath25",
    ", it seems that biologically hardly feasible schemes of installing two weight - sharing copies of @xmath25 would be required .",
    "another problem is that the update of @xmath25 is nonlocal : the information needed for updating a `` synapse '' @xmath974 ( that is , an element of @xmath25 ) is not entirely contained in the presynaptic or postsynaptic signals available at this synapse .    here",
    "i propose an architecture which solves these problems , and which i think has a natural biological `` feel '' .",
    "the basic idea is to ( i ) randomly expand the reservoir state @xmath72 into a ( much ) higher - dimensional _ random feature space _ , ( ii ) carry out the conceptor operations in that random feature space , but in a simplified version that only uses scalar operations on individual state components , and ( iii ) project the conceptor - modulated high - dimensional feature space state back to the reservoir by another random projection .",
    "the reservoir - conceptor loop is replaced by a two - stage loop , which first leads from the reservoir to the feature space ( through connection weight vectors @xmath975 , collected column - wise in a random neural projection matrix @xmath123 ) , and then back to the reservoir through a likewise random set of backprojection weights @xmath126 ( figure [ figbiolarch ] * a * ) .",
    "the reservoir - internal connection weights @xmath50 are replaced by the combination of @xmath123 and @xmath126 , and the original reservoir state @xmath4 known from the basic matrix conceptor framework is split into a reservoir state vector @xmath72 and a feature space state vector @xmath127 with components @xmath976 .",
    "the conception weights @xmath125 take over the role of conceptors . in full detail ,    1 .",
    "expand the @xmath8-dimensional reservoir state @xmath977 into the @xmath64-dimensional random feature space by a random feature map @xmath978 ( a synaptic connection weight matrix of size @xmath979 ) by computing the @xmath64-dimensional feature vector @xmath980 , 2 .",
    "multiply each of the @xmath64 feature projections @xmath981 with an adaptive _ conception weight _",
    "@xmath125 to get a conceptor - weighted feature state @xmath982 , where the _ conception vector _ @xmath983 is made of the conception weights , 3 .",
    "project @xmath127 back to the reservoir by a random @xmath984 backprojection matrix @xmath985 , closing the loop .    since",
    "both @xmath50 and @xmath985 are random , they can be joined in a single random map @xmath986 .",
    "this leads to the following consolidated state update cycle of a _ random feature conception _ ( rfc ) architecture : @xmath987 where @xmath988 and @xmath989 .    from a biological modeling perspective there",
    "exist a number of concrete candidate mechanisms by which the mathematical operation of multiplying - in the conception weights could conceivably be realized .",
    "i will discuss these later and for the time being remain on this abstract mathematical level of description .",
    "is projected by a feature map @xmath990 into a higher - dimensional feature space with states @xmath127 , from where it is back - projected by @xmath126 into the reservoir .",
    "the conceptor dynamics is realized by unit - wise multiplying conception weights @xmath125 into @xmath991 to obtain the @xmath127 state .",
    "the input unit @xmath992 is fed by external input @xmath37 or by learnt input simulation weights @xmath569 . *",
    "b * basic idea ( schematic ) .",
    "black ellipse : reservoir state @xmath72 correlation matrix @xmath23 .",
    "magenta dumbbell : scaling sample points @xmath975 from the unit sphere by their mean squared projection on reservoir states .",
    "green dumbbell : feature vectors @xmath975 scaled by auto - adapted conception weights @xmath125 .",
    "red ellipse : the resulting virtual conceptor @xmath993 .",
    "for detail see text.,title=\"fig:\",width=302 ]   is projected by a feature map @xmath990 into a higher - dimensional feature space with states @xmath127 , from where it is back - projected by @xmath126 into the reservoir .",
    "the conceptor dynamics is realized by unit - wise multiplying conception weights @xmath125 into @xmath991 to obtain the @xmath127 state .",
    "the input unit @xmath992 is fed by external input @xmath37 or by learnt input simulation weights @xmath569 . *",
    "b * basic idea ( schematic ) .",
    "black ellipse : reservoir state @xmath72 correlation matrix @xmath23 .",
    "magenta dumbbell : scaling sample points @xmath975 from the unit sphere by their mean squared projection on reservoir states .",
    "green dumbbell : feature vectors @xmath975 scaled by auto - adapted conception weights @xmath125 .",
    "red ellipse : the resulting virtual conceptor @xmath993 .",
    "for detail see text.,title=\"fig:\",width=151 ]    the conception vector @xmath994 is adapted online and element - wise in a way that is analog to the adaptation of matrix autoconceptors given in definition [ defautocrnn ] . per each element @xmath125 of @xmath995",
    ", the adaptation aims at minimizing the objective function @xmath996 + \\alpha^{-2}c_i^2,\\ ] ] which leads to fixed point solutions satisfying @xmath997 ( e[z_i^2 ] + \\alpha^{-2})^{-1}\\ ] ] and a stochastic gradient descent online adaptation rule @xmath998 where @xmath999 , @xmath1000 is an adaptation rate , and @xmath124 is the @xmath171-the component of @xmath127 . in computer simulations",
    "one will implement this adaptation not element - wise but in an obvious vectorized fashion .",
    "if ( [ eqcadapt ] ) converges , the converged fixed point is either @xmath1001 , which always is a possible and stable solution , or it is of the form @xmath1002 which is another possible stable solution provided that @xmath1003 . in this formula , @xmath1004 denotes the expectation @xmath1005 $ ] , the mean energy of the feature signal @xmath981 .",
    "these possible values of stable solutions can be derived in a similar way as was done for the singular values of autoconceptive matrix @xmath25 in section [ subseccad ] , but the derivation is by far simpler ( because it can be done element - wise for each @xmath125 and thus entails only scalars , not matrices ) and is left as an exercise . like the singular values of stable autoconceptors",
    "@xmath25 , the possible stable value range for conception weights obtainable through ( [ eqcadapt ] ) is thus @xmath1006 .",
    "some geometric properties of random feature conceptors are illustrated in figure [ figbiolarch ] * b*. the black ellipse represents the state correlation matrix @xmath1007 $ ] of a hypothetical 2-dimensional reservoir .",
    "the random feature vectors @xmath975 are assumed to have unit norm in this schematic and therefore sample from the surface of the unit sphere .",
    "the magenta - colored dumbbell - shaped surface represents the weigthing of the random feature vectors @xmath975 by the mean energies @xmath1008 $ ] of the feature signals @xmath981 . under the autoconception adaptation",
    "they give rise to conception weights @xmath125 according to ( [ eqsolrfcc ] ) ( green dumbbell surface ) . for values",
    "@xmath1009 one obtains @xmath1001 , which shows up in the illustration as the wedge - shaped indentation in the green curve .",
    "the red ellipse renders the virtual conceptor @xmath993 ( see below ) which results from the random feature conception weights .",
    "two properties of this rfc architecture are worth pointing out .",
    "first , conceptor matrices @xmath25 for an @xmath8-dimensional reservoir have @xmath1010 degrees of freedom .",
    "if , using conception vectors @xmath995 instead , one wishes to attain a performance level of pattern reconstruction accuracy that is comparable to what can be achieved with conceptor matrices @xmath25 , one would expect that @xmath64 should be in the order of @xmath1010 . at any rate , this is an indication that @xmath64 should be significantly larger than @xmath8 . in the simulations below i used @xmath1011 , which worked robustly well .",
    "in contrast , trying @xmath1012 ( not documented ) , while likewise yielding good accuracies , resulted in systems that were rather sensitive to parameter settings .",
    "second , the individual adaptation rates @xmath1000 can be chosen much larger than the global adaptation rate @xmath745 used for matrix conceptors , without putting stability at risk .",
    "the reason is that the original adpatation rate @xmath745 in the stochastic gradient descent formula for matrix conceptors given in definition [ defautocrnn ] is constrained by the highest local curvature in the gradient landscape , which leads to slow convergence in the directions of lower curvature .",
    "this is a notorious general characteristic of multidimensional gradient descent optimization , see for instance @xcite .",
    "this problem becomes irrelevant for the individual @xmath125 updates in ( [ eqcadapt ] ) . in the simulations presented below",
    ", i could safely select the @xmath1000 as large as @xmath1013 , whereas when i was using the original conceptor matrix autoadaption rules , @xmath965 was often the fastest rate possible . if adaptive individual adaptation rates @xmath1000 would be implemented ( not explored ) , very fast convergence of ( [ eqcadapt ] )",
    "should become feasible .",
    "[ [ geometry - of - feature - based - conceptors . ] ] geometry of feature - based conceptors .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    before i report on simulation experiments , it may be helpful to contrast geometrical properties of the rfc architecture and with the geometry of matrix autoconceptors .    for the sake of discussion ,",
    "i split the backprojection @xmath984 matrix @xmath126 in ( [ eqrfc1 ] ) into a product @xmath1014 where the `` virtual '' reservoir weight matrix @xmath1015 has size @xmath1016 .",
    "that is , i consider a system @xmath1017 equivalent to ( [ eqrfc1 ] ) and ( [ eqrfc2 ] ) , where @xmath995 is updated according to ( [ eqcadapt ] ) . for the sake of simplicity",
    "i omit input terms and bias in this discussion . the map @xmath1018 then plugs into the place that the conceptor matrix @xmath25 held in the conceptor systems @xmath1019 discussed in previous sections .",
    "the question i want to explore is how @xmath1020 compares to @xmath25 in geometrical terms .",
    "a conceptor matrix @xmath25 has an svd @xmath276 , where @xmath414 is orthonormal . in order to make the two systems directly comparable , i assume that all feature vectors @xmath975 in @xmath123 have unit norm .",
    "then @xmath1021 is positive semidefinite with 2-norm less or equal to 1 , in other words it is an @xmath1022 conceptor matrix .",
    "now furthermore assume that the adaptation ( [ eqcadapt ] ) has converged .",
    "the adaptation loop ( [ eqrfc1 ] , [ eqrfc2 ] , [ eqcadapt ] ) is then a stationary process and the expectations @xmath1005 $ ] are well - defined .",
    "note that these expectations can equivalently be written as @xmath1023 , where @xmath1024 $ ] .",
    "according to what i remarked earlier , after convergence to a stable fixed point solution we have , for all @xmath1025 , @xmath1026    again for the sake of discussion i restrict my considerations to converged solutions where all @xmath125 that _ can _ be nonzero ( that is , @xmath1003 ) are indeed nonzero",
    ".    it would be desirable to have an analytical result which gives the svd of the @xmath1022 conceptor @xmath1027 under these assumptions .",
    "unfortunately this analysis appears to be involved and at this point i can not deliver it . in order to still obtain some insight into the geometry of @xmath993 , i computed a number of such matrices numerically and",
    "compared them to matrix - based autoconceptors @xmath25 that were derived from the same assumed stationary reservoir state process .",
    "the outcome is displayed in figure [ figcversusc ] .",
    "( red ellipses ) . broken lines",
    "mark principal directions of the reservoir state correlation matrix @xmath23 .",
    "each panel corresponds to a particular combination of aperture and the second singular value @xmath1028 of @xmath23 .",
    "the dumbbell - shaped surfaces ( green line ) represent the values of the conception weights @xmath125 .",
    "for explanation see text.,width=415 ]    concretely , these numerical investigations were set up as follows .",
    "the reservoir dimension was chosen as @xmath1029 to admit plotting .",
    "the number of features was @xmath1030 .",
    "the feature vectors @xmath975 were chosen as @xmath1031 ( where @xmath1032 ) , that is , the unit vector @xmath1033 rotated in increments of @xmath1034 .",
    "this choice mirrors a situation where a very large number of @xmath975 would be randomly sampled ; this would likewise result in an essentially uniform coverage of the unit circle .",
    "the conception weights @xmath125 ( and hence @xmath993 ) are determined by the reservoir state correlation matrix @xmath1007 $ ] .",
    "the same holds for autoconceptor matrices @xmath25 . for an exploration of the @xmath993 versus @xmath25 geometries , i thus systematically varied @xmath275 .",
    "the principal directions @xmath414 were randomly chosen and remained the same through all variations .",
    "the singular values @xmath1035 were chosen as @xmath1036 , which gave three versions of @xmath23 .",
    "the aperture @xmath43 was selected in three variants as @xmath1037 , which altogether resulted in nine @xmath1038 combinations .",
    "for each of these combinations , conception weights @xmath125 were computed via ( [ eqcconvergedvalue ] ) , from which @xmath993 were obtained .",
    "each of these maps the unit circle on an ellipse , plotted in figure [ figcversusc ] in red .",
    "the values of the @xmath125 are represented in the figure as the dumbbell - shaped curve ( green ) connecting the vectors @xmath1039 .",
    "the wedge - shaped constriction to zero in some of these curves corresponds to angular values of @xmath975 where @xmath1001 .    for comparison , for each of the same @xmath1038 combinations",
    "also an autoconceptor matrix @xmath25 was computed using the results from section [ subseccad ] .",
    "we saw on that occasion that nonzero singular values of @xmath25 are not uniquely determined by @xmath23 ; they are merely constrained by certain interlacing bounds . to break this indeterminacy",
    ", i selected those @xmath25 that had the maximal admissible singular values . according to ( [ som1eq11 ] ) , this means that the singular values of @xmath25 were set to @xmath1040 ( where @xmath1041 ) provided the root argument was positive , else @xmath919 .",
    "here are the main findings that can be collected from figure [ figcversusc ] :    1 .",
    "the principal directions of @xmath1042 and @xmath23 coincide .",
    "the fact that @xmath993 and @xmath23 have the same orientation can also be shown analytically , but the argument that i have found is ( too ) involved and not given here .",
    "this orientation of @xmath993 hinges on the circumstance that the @xmath975 were chosen to uniformly sample the unit sphere .",
    "the @xmath993 ellipses are all non - degenerate , that is , @xmath993 has no zero singular values ( although many of the @xmath125 may be zero as becomes apparent in the wedge constrictions in the dumbbell - shaped representation of these values ) . in particular , the @xmath993 are also non - degenerate in cases where the matrix autoconceptors @xmath25 are ( panels in left column and center top panel ) .",
    "the finding that @xmath993 has no zero singular values can be regarded as a disadvantage compared to matrix autoconceptors , because it implies that no signal direction in the @xmath8-dimensional reservoir signal space can be completely suppressed by @xmath993 . however , in the @xmath64-dimensional feature signal space , we do have nulled directions .",
    "since the experiments reported below exhibit good stability properties in pattern reconstruction , it appears that this `` purging '' of signals in the feature space segment of the complete reservoir - feature loop is effective enough .",
    "3 .   call the ratio of the largest over the smallest singular value of @xmath993 or @xmath25 the _ sharpness _ of a conceptor ( also known as eigenvalue spread in the signal processing literature ) . then sometimes @xmath993 is sharper than @xmath25 , and sometimes the reverse is true .",
    "if sharpness is considered a desirable feature of concepors ( which i think it often is ) , then there is no universal advantage of @xmath25 over @xmath993 or vice versa .",
    "[ [ system - initialization - and - loading - patterns - generic - description . ] ] system initialization and loading patterns : generic description .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    returning from this inspection of geometrical properties to the system ( [ eqrfc1 ] )  ( [ eqcadapt ] ) , i proceed to describe the initial network creation and pattern loading procedure in generic terms . like with the matrix conceptor systems considered earlier in this report",
    ", there are two variants which are the analogs of ( i ) recomputing the reservoir weight matrix @xmath12 , as in section [ sec : storinggeneric ] , as opposed to ( ii ) training an additional input simulation matrix @xmath569 , as in section [ subsec : memmanage ] . in the basic experiments reported below i found that both work equally well . here",
    "i document the second option . a readout weight vector @xmath212 is likewise computed during loading .",
    "let @xmath61 target patterns @xmath59 be given ( @xmath1043 , which are to be loaded .",
    "here is an outline :    network creation .",
    ": :    a random feature map @xmath123 , random input weights    @xmath15 , a random bias vector    @xmath166 , and a random backprojection matrix    @xmath1044 are generated .",
    "@xmath123 and    @xmath1044 are suitably scaled such that the combined    @xmath13 map @xmath1045 attains a    prescribed spectral radius .",
    "this spectral radius is a crucial system    parameter and plays the same role as the spectral radius in reservoir    computing in general ( see for instance @xcite ) .",
    "all conception    weights are initialized to @xmath1046 , that is ,    @xmath1047 .",
    "conception weight adaptation .",
    ": :    the system is driven with each pattern @xmath59 in turn for    @xmath1048 steps ( discarding an    initial washout ) , while @xmath1049 is being adapted per    @xmath1050    leading to conception vectors @xmath1049 at the end of this    period .",
    "state harvesting for computing @xmath569 and @xmath1051 , and for recomputing @xmath126 .",
    ": :    the conception vectors @xmath1049 obtained from the previous    step are kept fixed , and for each pattern @xmath59 the    input - driven system @xmath1052 ;    @xmath1053 is run for    @xmath1054 time steps , collecting states @xmath1055 and    @xmath1056",
    ". computing weights . : :    the @xmath984 input simulation matrix @xmath569    is computed by solving the regularized linear regression    @xmath1057 where @xmath1058    is a suitably chosen tychonov regularizer .",
    "this means that the    autonomous system update @xmath1059 should be able to simulate input - driven updates    @xmath1060 +    @xmath1061 .",
    "@xmath212 is similarly computed by    solving    @xmath1062    +    optionally one may also recompute @xmath1044 by solving the    trivial regularized linear regression    @xmath1063    for a suitably chosen tychonov regularizer @xmath1064 .",
    "while @xmath1044 and @xmath126 should behave virtually    identically on the training inputs , the average absolute size of    entries in @xmath126 will be ( typically much ) smaller than the    original weights in @xmath1044 as a result of the    regularization .",
    "such regularized auto - adaptations have been found to    be beneficial in pattern - generating recurrent neural networks @xcite ,    and in the experiments to be reported presently i took advantage of    this scheme .",
    "the feature vectors @xmath975 that make up @xmath123 can optionally be normalized such that they all have the same norm . in my experiments",
    "this was not found to have a noticeable effect .",
    "if a stored pattern @xmath59 is to be retrieved , the only item that needs to be changed is the conception vector @xmath1049 .",
    "this vector can either be obtained by re - activating that @xmath1049 which was adapted during the loading ( which implies that it needs to be stored in some way ) .",
    "alternatively , it can be obtained by autoadaptation without being previously stored , as in sections [ secautoc ]  [ subseccad ] .",
    "i now describe two simulation studies which demonstrate how this scheme functions ( simulation detail documented in section [ secexpbiolplausible ] ) .",
    "the first study uses stored conception vectors , the second demonstrates autoconceptive adaptation .    [",
    "[ example-1-pattern - retrieval - with - stored - conception - vectors - cj . ] ] example 1 : pattern retrieval with stored conception vectors @xmath1049 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    this simulation re - used the @xmath29 reservoir from sections [ sec : initialdrivingdemo ] _ ff . _ and the four driver patterns ( two irrational - period sines , two very similar 5-periodic random patterns ) .",
    "the results are displayed in figure [ figcstored ] .",
    ", with @xmath1065 random feature vectors @xmath975 .",
    "first column : sorted conception vectors @xmath1049 . second column : spectra of virtual conceptors @xmath993 .",
    "third column : reconstructed patterns ( bold light gray ) and original patterns ( thin black ) after phase alignment .",
    "nrmses are given in insets .",
    "last column : the adaptation of @xmath1049 during the 2000 step runs carried out in parallel to the loading process .",
    "50 of 500 traces are shown .",
    "for explanation see text.,width=491 ]    the loading procedure followed the generic scheme described above ( details in section [ secdocexp ] ) , with @xmath1066 , @xmath1067 , @xmath1068 and @xmath1069 .",
    "the aperture was set to @xmath1070 .",
    "the left column in figure [ figcstored ] shows the resulting @xmath1049 spectra , and the right column shows the evolution of @xmath1049 during this adaptation .",
    "notice that a considerable portion of the conception weights evolved toward zero , and that none ended in the range @xmath1071 , in agreement with theory .",
    "for additional insight into the dynamics of this system i also computed `` virtual '' matrix conceptors @xmath1072 by @xmath1073 , \\ ; c_f^j = r^j \\ , ( r^j + \\alpha^{-2})^{-1}$ ] ( second column ) .",
    "the singular value spectrum of @xmath1072 reveals that the autocorrelation spectra of @xmath1074 signals in rfc systems is almost identical to the singular value spectra obtained with matrix conceptors on earlier occasions ( compare figure [ figsingvalsfalloff ] ) .",
    "the settings of matrix scalings and aperture were quite robust ; variations in a range of about @xmath90050% about the chosen values preserved stability and accuracy of pattern recall ( detail in section [ secexpbiolplausible ] ) .    for testing the recall of pattern @xmath59",
    ", the loaded system was run using the update routine @xmath1075 starting from a random starting state @xmath1076 which was sampled from the normal distribution , scaled by @xmath898 . after a washout of 200 steps ,",
    "the reconstructed pattern @xmath1077 was recorded for 500 steps and compared to a 20-step segment of the target pattern @xmath59 .",
    "the second column in figure [ figcstored ] shows an overlay of @xmath1077 with @xmath59 and gives the nrmses .",
    "the reconstruction is of a similar quality as was found in section [ sec : retrievegeneric ] where full conceptor matrices @xmath25 were used .",
    "[ [ example-2-content - addressed - pattern - retrieval . ] ] example 2 : content - addressed pattern retrieval .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for a demonstration of content - addressed recall similar to the studies reported in section [ secautocmemexample ] , i re - used the @xmath1065 system described above .",
    "reservoir scaling parameters and the loading procedure were identical except that conception vectors @xmath1049 were not stored .",
    "results are collected in figure [ figccontaddress ] .",
    "the cue and recall procedure for a pattern @xmath59 was carried out as follows :    1 .",
    "starting from a random reservoir state , the loaded reservoir was driven with the cue pattern for a washout time of 200 steps by @xmath1078 .",
    "2 .   then , for a cue period of 800 steps , the system was updated with @xmath1049 adaptation by @xmath1079 starting from an all - ones @xmath1049 , with an adaptation rate @xmath1080 for all @xmath171 . at the end of this period , a conception vector @xmath1081 was obtained .",
    "3 .   to measure the quality of @xmath1081 , a separate run of 500 steps without @xmath995 adapation",
    "was done using @xmath1082 obtaining a pattern reconstruction @xmath1083 .",
    "this was phase - aligned with the original pattern @xmath59 and an nrmse was computed ( figure [ figccontaddress ] , third column ) .",
    "the recall run was resumed after the cueing period and continued for another 10,000 steps in auto - adaptation mode , using @xmath1084 leading to a final @xmath1085 at the end of this period .",
    "another quality measurement run was done identical to the post - cue measurement run , using @xmath1085 ( nrmse results in figure [ figccontaddress ] , third column ) .",
    "feature vectors @xmath975 .",
    "first column : sorted feature projection weight vectors @xmath1049 after the cue phase ( black ) and after 10,000 steps of autoadaptation ( gray ) .",
    "second column : spectra of virtual conceptors @xmath993 after cue ( black ) and at the end of autonomous adaptation ( gray ) .",
    "both spectra are almost identical .",
    "third column : reconstructed patterns ( bold light gray : after cue , bold dark gray : after autoadaptation ; the latter are mostly covered by the former ) and original patterns ( thin black ) .",
    "nrmses are given in insets ( top : after cue , bottom : after autoadaptation ) .",
    "fourth column : the adaptation of @xmath1049 during the cueing period .",
    "last column : same , during the 10000 autoadaptation steps .",
    "50 of 500 traces are shown .",
    "note the different timescales in column 4 versus column 5 .",
    "for explanation see text.,width=548 ]    like in the matrix-@xmath25-based content - addressing experiments from section [ secautocmemexample ] , the recall quality directly after the cue further improved during the autoconceptive adaption afterwards , except for the first pattern .",
    "pending a more detailed investigation , this may be attributed to the `` maturation '' of the @xmath1049 during autoadaption which reveals itself in the convergence of a number of @xmath1086 to zero during autoadaptation ( first and last column in figure [ figccontaddress ] ) .",
    "we have seen similar effects in section [ secautocmemexample ] .",
    "an obvious difference to those earlier experiments is that the cueing period is much longer now ( 800 versus 15  30 steps ) .",
    "this is owed to the circumstance that now the conceptor adaptation during cueing started from an all - ones @xmath1049 , whereas in section [ secautocmemexample ] it was started from a zero @xmath25 . in the latter case ,",
    "singular values of @xmath25 had to grow away from zero toward one during cueing , whereas here they had to sink away from one toward zero .",
    "the effects of this mirror situation are not symmetrical . in",
    "an `` immature '' post - cue conceptor matrix @xmath25 started from a zero @xmath25 , all the singular values which eventually should converge to zero are already at zero at start time and remain there .",
    "conversely , the post - cue feature projection weights @xmath1081 , which _ should _ eventually become zero , have not come close to this destination even after the 800 cue steps that were allotted here ( left panels in figure [ figccontaddress ] ) .",
    "this tail of `` immature '' nonzero elements in @xmath1081 leads to an insufficient filtering - out of reservoir state components which do not belong to the target pattern dynamics .",
    "the development of the @xmath1086 during the autonomous post - cue adapation is not monotonous ( right panels ) .",
    "some of these weights meander for a while before they settle to what appear stable final values .",
    "this is due to the transient nonlinear reservoir@xmath1049 interactions which remain to be mathematically analyzed .",
    "a potentially important advantage of using random feature conceptors @xmath995 rather than matrix conceptors @xmath25 in machine learning applications is the faster convergence of the former in online adaptation scenarios . while an dedicated comparison of convergence properties between @xmath995 and @xmath25 conceptors remains to be done , one may naturally expect that stochastic gradient descent works more efficiently for random feature conceptors than for matrix conceptors , because the gradient can be followed individually for each coordinate @xmath125 , unencumbered by the second - order curvature interactions which notoriously slow down simple gradient descent in multidimensional systems .",
    "this is one of the reasons why in the complex hierarchical signal filtering architecture to be presented below in section [ sechierarchicalarchitecture ] i opted for random feature conceptors .",
    "[ [ algebraic - and - logical - rules - for - conception - weights . ] ] algebraic and logical rules for conception weights .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the various definitions and rules for aperture adaptation , boolean operations , and abstraction introduced previously for matrix conceptors directly carry over to random feature conceptor .",
    "the new definitions and rules are simpler than for conceptor matrices because they all apply to the individual , scalar conception weights .",
    "i present these items without detailed derivations ( easy exercises ) . in the following ,",
    "let @xmath1087 be two conception weight vectors .",
    "aperture adaptation ( compare definition [ deflimitphi ] ) becomes    [ def : apadaptcvecs ] @xmath1088    transferring the matrix - based definition of boolean operations ( definition [ def : finalboolean ] ) to conception weight vectors leads to the following laws :    [ def : booleancvecs ] @xmath1089    the matrix - conceptor properties connecting aperture adaptation with boolen operations ( proposition [ propbooleanaperture ] ) and the logic laws ( propositions [ propbooleanelementarylaws ] , [ propboolealg ] ) remain valid after the obvious modifications of notation .",
    "we define @xmath1090 if for all @xmath999 it holds that @xmath1091 .",
    "the main elements of proposition [ propbasicabstraction ] turn into    [ propcabstraction ] let @xmath1092 be conception weight vectors .",
    "then the following facts hold .    1 .   if @xmath1093 , then @xmath1094 , where @xmath995 is the conception weight vector with entries @xmath1095 2 .   if @xmath1096 , then @xmath1097 , where @xmath995 is the conception weight vector with entries @xmath1098 3 .   if @xmath1099 , then @xmath1093 .",
    "4 .   if @xmath1100 , then @xmath1096 .",
    "note that all of these definitions and rules can be considered as restrictions of the matrix conceptor items on the special case of diagonal conceptor matrices .",
    "the diagonal elements of such diagonal conceptor matrices can be identified with conception weights .",
    "[ [ aspects - of - biological - plausibility . ] ] aspects of biological plausibility .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    `` biological plausibility '' is a vague term inviting abuse .",
    "theoretical neuroscientists develop mathematical or computational models of neural systems which range from fine - grained compartment models of single neurons to abstract flowchart models of cognitive processes . assessing",
    "the methodological role of formal models in neuroscience is a complex and sometimes controversial issue @xcite .",
    "when i speak of biological plausibility in connection with conceptor models , i do not claim to offer a blueprint that can be directly mapped to biological systems .",
    "all that i want to achieve in this section is to show that conceptor systems may be conceived which do _ not _ have characteristics that are decidedly _ not _ biologically feasible . in particular , ( all ) i wanted is a conceptor system variant which can be implemented without state memorizing or weight copying , and which only needs locally available information for its computational operations . in the remainder of this section",
    "i explain how these design goals may be satisfied by rfc conceptor architectures .",
    "i will discuss only the adaptation of conception weights and the learning of the input simulation weights @xmath569 , leaving cueing mechanisms aside .",
    "the latter was implemented in the second examples above in an ad - hoc way just to set the stage and would require a separate treatment under the premises of biological plausibility .    in my discussion",
    "i will continue to use the discrete - time update dynamics that was used throughout this report .",
    "biological systems are not updated according to a globally clocked cycle , so this clearly departs from biology . yet , even a synchronous - update discrete - time model can offer relevant insight .",
    "the critical issues that i want to illuminate ",
    "namely , no state / weight copying and locality of computations  are independent of choosing a discrete or continuous time setting .",
    "i first consider the adaptation of the input simulation weights @xmath569 . in",
    "situated , life - long learning systems one may assume that at given point in ( life-)time , some version of @xmath569 is already present and active , reflecting the system s learning history up to that point . in the content - addressable memory example above , at the end of the cueing period there was an abrupt switch from driving the system with external input to an autonomous dynamics using the system s own input simulation via @xmath569 .",
    "such binary instantaneous switching can hardly be expected from biological systems .",
    "it seems more adequate to consider a gradual blending between the input - driven and the autonomous input simulation mode , as per @xmath1101 where a mixing between the two modes is mediated by a `` slide ruler '' parameter @xmath778 which may range between 0 and 1 ( a blending of this kind will be used in the architecture in section [ sechierarchicalarchitecture ] ) .    as a side remark",
    ", i mention that when one considers comprehensive neural architectures , the question of negotiating between an input - driven and an autonomous processing mode arises quite generically . a point in case",
    "are `` bayesian brain '' models of pattern recognition and control , which currently receive much attention @xcite . in those models ,",
    "a neural processing layer is driven both from `` lower '' ( input - related ) layers and from `` higher '' layers which autonomously generate predictions .",
    "both influences are merged in the target layer by some neural implementation of bayes rule .",
    "other approaches that i would like to point out in this context are layered restricted boltzmann machines @xcite , which likewise can be regarded as a neural implementation of bayes rule ; hierarchical neural field models of object recognition @xcite which are based on arathorn s `` map seeking circuit '' model of combining bottom - up and top - down inputs to a neural processing layer @xcite ; and mixture of experts models for motor control ( for example @xcite ) where a `` responsibility '' signal comparable in its function to the @xmath778 parameter negotiates a blending of different control signals .",
    "negotiation between driving a reservoir with external input @xmath37 versus with simulated input @xmath1102 .",
    "abstract neurons are marked by filled gray circles .",
    "connections that solely copy a neural state forward are marked with `` 1 '' .",
    "connections marked @xmath1103 refer to multiplicative modulation .",
    "connections that are inhibitory by their nature are represented by @xmath1104 .",
    "broken arrows indicate a controlling influence on the weight adaptation of @xmath569 .",
    "for explanation see text.,width=415 ]    returning to ( [ eqetamediation ] ) , the mathematical formula could be implemented in an abstract neural circuit as drawn in figure [ figmicrocircuitd ] .",
    "explanation of this diagram : gray dots represent abstract firing - rate neurons ( biologically realized by individual neurons or collectives ) .",
    "all neurons are linear .",
    "activation of neuron @xmath119 : simulated input @xmath1105 ; of @xmath37 : external driver @xmath14 .",
    "neuron @xmath1106 maintains the value of the `` error '' @xmath1107 . @xmath119 and @xmath37 project their activation values to @xmath1108 and @xmath1109 , whose activations are multiplicatively modulated by the activations of neurons @xmath778 and @xmath1110 .",
    "the latter maintain the values of @xmath778 and @xmath1111 from ( [ eqetamediation ] ) .",
    "the activations @xmath1112 and @xmath1113 are additively combined in @xmath992 , which finally feeds to the reservoir through @xmath15 .    for a multiplicative modulation of neuronal activity",
    "a number of biological mechanisms have been proposed , for example @xcite .",
    "the abstract model given here is not committed to a specific such mechanism .",
    "likewise i do not further specify the biological mechanism which balances between @xmath778 and @xmath1110 , maintaining a relationship @xmath1114 ; it seems natural to see this as a suitable version of mutual inhibition .",
    "an in - depth discussion by which mechanisms and for which purposes @xmath778 is administered is beyond the scope of this report .",
    "many scenarios are conceivable . for the specific purpose of content - addressable memory recall ,",
    "the setting considered in this section , a natural option to regulate @xmath778 would be to identify it with the ( 0 - 1-normalized and time - averaged ) error signal @xmath1106 . in the architecture presented in section [ sechierarchicalarchitecture ] below",
    ", regulating @xmath778 assumes a key role and will be guided by novel principles .",
    "the sole point that i want to make is that this abstract architecture ( or similar ones ) requires only local information for the adaptation / learning of @xmath569 .",
    "consider a synaptic connection @xmath1115 from a feature neuron @xmath124 to @xmath119 .",
    "the learning objective ( [ eqadaptdcriterion ] ) can be achieved , for instance , by the stochastic gradient descent mechanism @xmath1116 where the `` error '' @xmath1117 is available in the activity of the @xmath1106 neuron .",
    "the learning rate @xmath745 could be fixed , but a more suggestive option would be to scale it by @xmath1118 , as indicated in the diagram .",
    "that is , @xmath1115 would be adapted with an efficacy proportional to the degree that the system is currently being externally driven .",
    "i now turn to the action and the adaptation of the conception weights , stated in mathematical terms in equations ( [ eqrfc2 ] ) and ( [ eqcadapt ] ) .",
    "there are a number of possibilities to implement these formulae in a model expressed on the level of abstract firing - rate neurons .",
    "i inspect three of them .",
    "they are sketched in figure [ figmicrocircuitsc ] .",
    "is an abstract neuron whose activation is @xmath1119 . in * b * and * c * , @xmath1120 has activation @xmath1121 . in * c * , @xmath1122 has activation @xmath125 .",
    "for explanation see text.,width=529 ]    the simplest model ( figure [ figmicrocircuitsc ] * a * ) represents the quantity @xmath1123 by the activation of a single neuron @xmath1124 .",
    "it receives synaptic input @xmath1121 through connections @xmath975 and feeds to the reservoir ( or to an input gating circuit as discussed above ) through the single synaptic connection @xmath1115 .",
    "the weighting of @xmath1121 with the factor @xmath125 is effected by some self - regulated modulation of synaptic gain . taking into account that @xmath125 changes on a slower timescale than @xmath1121 , the information needed to adapt",
    "the strength @xmath125 of this modulation ( [ eqcadapt ] ) is a moving average of the neuron s own activation energy @xmath1125 and the current synaptic gain @xmath1126 , which are characteristics of the neuron @xmath1124 itself and thus are trivially locally available .    in the next model ( figure [ figmicrocircuitsc ]",
    "* b * ) , there is a division of labor between a neuron @xmath1124 which again represents @xmath1127 and a preceding neuron @xmath1120 which represents @xmath1121 . the latter feeds into the former through a single synaptic connection weighted by @xmath125 .",
    "the adaptation of the synaptic strength @xmath125 here is based on the ( locally time - averaged ) squared activity of the postsynaptic neuron @xmath1124 , which again is information locally available at the synaptic link @xmath125 .    finally , the most involved circuit offered in figure [ figmicrocircuitsc ] * c * delegates the representation of @xmath125 to a separate neuron @xmath1122 . like in the second model , a neuron @xmath1120 which represents @xmath1121 feeds to @xmath1124 , this time copying its own activation through a unit connection .",
    "the @xmath1122 neuron multiplicatively modulates @xmath1124 by its activation @xmath125 . like in the @xmath569 adaptation proposal described in figure [ figmicrocircuitd",
    "] , i do not commit to a specific biological mechanism for such a multiplicative modulation .",
    "the information needed to adapt the activation @xmath125 of neuron @xmath1122 according to ( [ eqcadapt ] ) is , besides @xmath125 itself , the quantity @xmath1128 .",
    "the latter is represented in @xmath1124 which is postsynaptic from the perspective of @xmath1122 and therefore not directly accessible . however , the input @xmath1121 from neuron @xmath1120 is available at @xmath1122 , from which the quantity @xmath1129 can be inferred by neuron @xmath1122 .",
    "the neuron @xmath1122 thus needs to instantiate an intricate activation dynamics which combines local temporal averaging of @xmath1130 with an execution of ( [ eqcadapt ] ) .",
    "a potential benefit of this third neural circuit over the preceding two is that a representation of @xmath125 by a neural activation can presumably be biologically adapted on a faster timescale than the neuron auto - modulation in system * a * or the synaptic strength adaptation in * b*.    when i first considered content - addressable memories in this report ( section [ secautocmemexample ] ) , an important motivation for doing so was that storing entire conceptor matrices @xmath25 for later use in retrieval is hardly an option for biological systems .",
    "this may be different for conception vectors : it indeed becomes possible to `` store '' conceptors without having to store network - sized objects .",
    "staying with the notation used in figure [ figmicrocircuitsc ] : a single neuron @xmath1131 might suffice to represent and `` store '' a conception vector @xmath1049 associated with a pattern @xmath59 .",
    "the neuron @xmath1131 would project to all @xmath1124 neurons whose states correspond to the signals @xmath124 , with synaptic connection weights @xmath1086 , and effecting a multiplicative modulation of the activation of the @xmath1124 neurons proportional to these connection weights .",
    "i am not in a position to judge whether this is really an option in natural brains . for applications in machine learning however , using stored conception vectors @xmath1049 in conjunction with rfc systems may be a relevant alternative to using stored matrix conceptors , because vectors @xmath1049 can be stored much more cheaply in computer systems than matrices .    _ a speculative outlook . _",
    "i allow myself to indulge in a brief speculation of how rfc conceptor systems might come to the surface  literally  in mammalian brains .",
    "the idea is to interpret the activations of ( groups of ) neurons in the neocortical sheet as representing conception factors @xmath125 or @xmath124 values , in one of the versions shown in figure [ figmicrocircuitsc ] or some other concrete realization of rfc conceptors .",
    "the `` reservoir '' part of rfc systems might be found in deeper brain structures . when some patches of the neocortical sheet are activated and others not ( revealed for instance through fmri imaging or electro - sensitive dyes ) , this",
    "may then be interpreted as a specific @xmath1049 vector being active . in geometrical terms , the surface of the hyperellipsoid of the `` virtual '' conceptor would be mapped to the neocortical sheet . since this implies a reduction of dimension from a hypothetical reservoir dimension @xmath8 to the 2-dimensional cortical surface , a dimension folding as in self - organizing feature maps @xcite would be necessary .",
    "what a cognitive scientist would call an `` activation of a concept '' would find its neural expression in such an activation of a dimensionally folded ellipsoid pertaining to a `` virtual '' conceptor @xmath993 in the cortical sheet .",
    "an intriguing further step down speculation road is to think about boolean operations on concepts as being neurally realized through the conceptor operations described in sections [ secboolean ]  [ subsec : japvow ] .",
    "all of this is still too vague .",
    "still , some aspects of this picture have already been explored in some detail in other contexts .",
    "specifically , the series of neural models for processing serial cognitive tasks in primate brains developed by dominey et al .  @xcite combine a reservoir dynamics located in striatal nuclei with cortical context - providing activation patterns which shares some characteristics with the speculations offered here .",
    "a reservoir equipped with some conceptor mechanism does not by itself serve a purpose .",
    "if this computational - dynamical principle is to be made useful for practical purposes like prediction , classification , control or others , or if it is to be used in cognitive systems modeling , conceptor - reservoir modules need to be integrated into more comprehensive architectures .",
    "these architectures take care of which data are fed to the conceptor modules , where their output is channelled , how apertures are adapted , and everything else that is needed to manage a conceptor module for the overall system purpose . in this section",
    "i present a particular architecture for the purpose of combined signal denoising and classification as an example .",
    "this ( still simple ) example introduces a number of features which may be of more general use when conceptor modules are integrated into architectures :    arranging conceptor systems in bidirectional hierarchies : : :    a higher conceptor module is fed from a lower one by the output of the    latter ( bottom - up data flow ) , while at the same time the higher module    co - determines the conceptors associated with the lower one ( top - down    `` conceptional bias '' control ) .",
    "neural instantiations of individual conceptors : : :    using random feature conceptors , it becomes possible to economically    store and address individual conceptors .",
    "self - regulating balance between perception and action modes : : :    a conceptor - reservoir module is made to run in any mixture of two    fundamental processing modes , ( i ) being passively driven by external    input and ( ii ) actively generating an output pattern . the balance    between these modes is autonomously steered by a criterion that arises    naturally in hierarchical conceptor systems .    a personal remark : the first and last of these three items constituted the original research questions which ultimately guided me to conceptors .",
    "_ the task .",
    "_ the input to the system is a timeseries made of alternating sections of the four patterns @xmath136 used variously before in this report : two sines of irrational period lengths , and two slightly differing 5-periodic patterns .",
    "this signal is corrupted by strong gaussian i.i.d .",
    "noise ( signal - to - noise ratio = 0.5 )  see bottom panels in fig .",
    "[ figcarch ] .",
    "the task is to classify which of the four patterns is currently active in the input stream , and generate a clean version of it .",
    "this task is a simple instance of the generic task `` classify and clean a signal that intermittently comes from different , but familiar , sources '' .        _",
    "architecture . _",
    "the basic idea is to stack copies of a reservoir - conceptor loop , giving a hierarchy of such modules ( compare figure [ fig3layerarchitecture ] ) . here",
    "i present an example with three layers , having essentially identical modules @xmath1132}$ ] on the lowest , @xmath1133}$ ] on the middle , and @xmath1134}$ ] on the highest layer ( i use subscript square brackets @xmath140 $ ] to denote levels in the hierarchy ) .",
    "each module is a reservoir - conceptor loop .",
    "the conceptor is implemented here through the @xmath64-dimensional feature space expansion described in the previous section , where a high - dimensional conception weight vector @xmath995 is multiplied into the feature state ( as in figure [ figbiolarch ] ) . at exploitation time",
    "the state update equations are @xmath1135}(n+1 ) & = &    ( 1-\\tau_{[l-1,l]}(n))\\,y_{[l-1]}(n+1 ) + \\tau_{[l-1,l]}(n ) \\,d\\,z_{[l]}(n),\\\\   r_{[l]}(n+1 ) & = & \\tanh(g \\ , z_{[l]}(n ) + w^{\\mbox{\\scriptsize in}}\\,u_{[l]}(n+1 )   + b    ) , \\\\",
    "z_{[l]}(n+1 ) & = &   c_{[l]}(n ) \\,.\\!\\ast \\ , f ' r_{[l]}(n+1),\\\\ y_{[l]}(n+1 ) & = &   w^{\\mbox{\\scriptsize out}}\\ ,   r_{[l]}(n+1),\\end{aligned}\\ ] ] where @xmath155}$ ] is the effective signal input to module @xmath1136}$ ] , @xmath151}$ ] is the output signal from that module , and the @xmath778 are mixing parameters which play a crucial role here and will be detailed later .",
    "in addition to these fast timescale state updates there are several online adaptation processes , to be described later , which adapt @xmath778 s and @xmath995 s on slower timescales .",
    "the weight matrices @xmath1137 are identical in all modules .",
    "@xmath1138 are created randomly at design time and remain unchanged .",
    "@xmath569 and @xmath212 are trained on samples of clean `` prototype '' patterns in an initial pattern loading procedure .",
    "@xmath126 is first created randomly as @xmath1044 and then is regularized using white noise ( all detail in section [ secdochierarchical ] ) .",
    "the effective input signal @xmath155}(n+1)$ ] to @xmath1136}$ ] is thus a mixture mediated by a `` trust '' variable @xmath154}$ ] of a module - external external input @xmath153}(n+1)$ ] and a module - internal input simulation signal @xmath1139}(n)$ ] . on the bottom layer , @xmath1140 } \\equiv 0 $ ] and @xmath1141}(n ) = p(n)$ ] , that is , this layer has no self - feedback input simulation and is entirely driven by the external input signal @xmath14 .",
    "higher modules receive the output @xmath153}(n+1)$ ] of the respective lower module as their external input .",
    "both input mix components @xmath153}$ ] and @xmath1139}$ ] represent partially denoised versions of the external pattern input .",
    "the component @xmath153}$ ] from the module below will typically be noisier than the component @xmath1142}$ ] that is cycled back within the module , because each module is supposed to de - noise the signal further in its internal reservoir - conceptor feedback loop .",
    "if @xmath154}$ ] were to be 1 , the module would be running in an autonomous pattern generation mode and would be expected to re - generate a very clean version of a stored pattern  which might however be a wrong one .",
    "if @xmath154}$ ] were to be 0 , the module would be running in an entirely externally driven mode , with no `` cleaning '' in effect .",
    "it is crucial for the success of this system that these mixing weights @xmath154}$ ] are appropriately set .",
    "they reflect a `` trust '' of the system in its current hypothesis about the type of the driving pattern @xmath37 , hence i call them _ trust _ variables .",
    "i mention at this point that when the external input @xmath37 changes from one pattern type to another , the trust variables must quickly decrease in order to temporarily admit the architecture to be in an altogether more input - driven , and less self - generating , mode .",
    "all in all this constitutes a bottom - up flow of information , whereby the raw input @xmath37 is cleaned stage - wise with an amount of cleaning determined by the current trust of the system that it is applying the right conceptor to effect the cleaning .",
    "the output signals @xmath151}$ ] of the three modules are computed from the reservoir states @xmath1143}$ ] by output weights @xmath20 , which are the same on all layers .",
    "these output weights are initially trained in the standard supervised way of reservoir computing to recover the input signal given to the reservoir from the reservoir state .",
    "the 3-rd layer output @xmath134}$ ] also is the ultimate output of the entire architecture and should give a largely denoised version of the external driver @xmath37 .",
    "besides this bottom - up flow of information there is a top - down flow of information .",
    "this top - down pathway affects the conception weight vectors @xmath144}$ ] which are applied in each module .",
    "the guiding idea here is that on the highest layer ( @xmath1144 in our example ) , the conceptor @xmath142}$ ] is of the form @xmath1145}(n ) = \\bigvee_{j=1,\\ldots,4 } \\varphi(c^j , \\gamma^j(n)),\\ ] ] where @xmath1146 are _ prototype _ conception weight vectors corresponding to the four training patterns .",
    "these prototype vectors are computed and stored at training time . in words , at the highest layer the conception weight vector is constrained to be a disjunction of aperture - adapted versions of the prototype conception weight vectors . imposing this constraint on the highest layer",
    "can be regarded as inserting a qualitative bias in the ensuing classification and denoising process .",
    "adapting @xmath142}(n)$ ] amounts to adjusting the aperture adaptation factors @xmath1147 .    at any time during processing external input , the current composition of @xmath142}$ ] as a @xmath1131-weighted disjunction of the four prototypes reflects the system s current _ hypothesis _ about the type of the current external input .",
    "this hypothesis is stagewise passed downwards through the lower layers , again mediated by the trust variables .",
    "this top - down pathway is realized as follows .",
    "assume that @xmath142}(n)$ ] has been computed .",
    "in each of the two modules below ( @xmath1148 ) , an ( auto-)conception weight vector @xmath1149}(n)$ ] is computed by a module - internal execution of the standard autoconception adaptation described in the previous section ( equation ( [ eqcadapt ] ) ) .",
    "to arrive at the effective conception weight vector @xmath144}(n)$ ] , this @xmath1150}(n)$ ] is then blended with the current conception weight vector @xmath156}(n)$ ] from the next higher layer , again using the respective trust variable as mixing coefficient : @xmath1151}(n ) = ( 1-\\tau_{[l , l+1]}(n ) ) \\ , c^{\\mbox{\\scriptsize aut}}_{[l]}(n ) + \\tau_{[l , l+1]}(n ) \\",
    ", c_{[l+1]}(n).\\ ] ]    in the demo task reported here , the raw input @xmath37 comes from either of four sources @xmath137 ( the familiar two sines and 5-periodic patterns ) , with additive noise .",
    "these four patterns are initially stored in each of the modules @xmath1136}$ ] by training input simulation weights @xmath569 , as described in section [ secbiolplausible ] .",
    "the same @xmath569 is used in all layers .    in the exploitation phase (",
    "after patterns have been loaded into @xmath569 , output weights have been learnt , and prototype conceptors @xmath1049 have been learnt ) , the architecture is driven with a long input sequence composed of intermittent periods where the current input is chosen from the patterns @xmath59 in turn . while being driven with @xmath59 , the system must autonomously determine which of the four stored pattern is currently driving it , assign trusts to this judgement , and accordingly tune the degree of how strongly the overall processing mode is autonomously generative ( high degree of cleaning , high danger of `` hallucinating '' ) vs.  passively input - driven ( weak cleaning , reliable coupling to external driver ) .    summing up",
    ", the overall functioning of the trained architecture is governed by two pathways of information flow ,    * a bottom - up pathway where the external noisy input @xmath37 is successively de - noised , * a top - down pathway where hypotheses about the current pattern type , expressed in terms of conception weight vectors , are passed downwards ,    and by two online adaptation processes ,    * adjusting the trust variables @xmath154}$ ] , and * adjusting the conception weight vector @xmath144}$ ] in the top module .",
    "i now describe the two adaptation processes in more detail .    _ adapting the trust variables .",
    "_ before i enter technicalities i want to emphasize that here we are confronted with a fundamental problem of information processing in situated intelligent agents ( `` sia '' : animals , humans , robots ) .",
    "a sia continuously has to `` make sense '' of incoming sensor data , by matching them to the agent s learnt / stored concepts .",
    "this is a multi - faceted task , which appears in many different instantiations which likely require specialized processing strategies .",
    "examples include online speech understanding , navigation , or visual scene interpretation .",
    "for the sake of this discussion i will lump them all together and call them `` online data interpretation '' ( odi ) tasks .",
    "odi tasks variously will involve subtasks like de - noising , figure - ground separation , temporal segmentation , attention control , or novelty detection .",
    "the demo architecture described in this section only addresses de - noising and temporal segmentation . in many cognitive architectures in the literature ,",
    "odi tasks are addressed by maintaining an online representation of a `` current best '' interpretation of the input data .",
    "this representation is generated in `` higher '' levels of a processing hierarchy and is used in a top - down fashion to assist lower levels , for instance by way of providing statistical bias or predictions ( discussion in @xcite ) .",
    "this top - down information then tunes the processing in lower levels in some way that enables them to extract from their respective bottom - up input specific features while suppressing others  generally speaking , by making them selective .",
    "an inherent problem in such architectures is that the agent must not grow overly confident in its top - down pre - conditioning of lower processing layers . in the extreme case of relying entirely on the current interpretation of data ( instead of on the input data themselves )",
    ", the sia will be hallucinating .",
    "conversely , the sia will perform poorly when it relies entirely on the input data : then it will not `` understand '' much of it , becoming unfocussed and overwhelmed by noise .",
    "a good example is semantic speech or text understanding .",
    "linguistic research has suggested that fast _ forward inferences _",
    "are involved which predispose the sia to interpret next input in terms of a current representation of semantic context ( for instance , @xcite ) .",
    "as long as this current interpretation is appropriate , it enables fast semantic processing of new input ; but when it is inappropriate , it sends the sia on erroneous tracks which linguists call `` garden paths '' . generally speaking ,",
    "for robust odi an agent should maintain a reliable measure of the degree of trust that the sia has in its current high - level interpretation . when the trust level is high",
    ", the sia will heavily tune lower levels by higher - level interpretations ( top - down dominance ) , while when trust levels are low , it should operate in a bottom - up dominated mode .",
    "maintaining an adaptive measure of trust is thus a crucial objective for an sia . in bayesian architectures ( including kalman filter observers in control engineering ) ,",
    "such a measure of trust is directly provided by the posterior probability @xmath1152 .",
    "a drawback here is that a number of potentially complex probability distributions have to be learnt beforehand and may need extensive training data , especially when prior hyperdistributions have to be learnt instead of being donated by an oracle . in mixture of predictive expert models ( for instance @xcite ) , competing interpretation models are evaluated online in parallel and are assigned relative trust levels according to how precisely they can predict the current input .",
    "a problem that i see here is computational cost , besides the biological implausibility of executing numerous predictors in parallel . in adaptive resonance theory @xcite ,",
    "the role of a trust measure is filled by the ratio between the norm of a top - down pattern interpretation over the norm of an input pattern ; the functional effects of this ratio for further processing depends on whether that ratio is less or greater than a certain `` vigilance '' parameter .",
    "adaptive resonance theory however is primarily a static pattern processing architecture not designed for online processing of temporal data .",
    "returning to our demo architecture , here is how trust variables are computed .",
    "they are based on auxiliary quantities @xmath1153}(n)$ ] which are computed within each module @xmath145 .",
    "intuitively , @xmath1153}(n)$ ] measures the ( temporally smoothed ) discrepancy between the external input signal fed to the module and the self - generated , conceptor - cleaned version of it . for layers",
    "@xmath1154 the external input signal is the bottom - up passed output @xmath153}$ ] of the lower layer .",
    "the conceptor - cleaned , module - generated version is the signal @xmath1155}(n)$ ] extracted from the conception - weighted feature space signal @xmath1156}(n ) =   c_{[l]}(n ) \\,.\\!\\ast\\ , f ' r_{[l]}(n)$ ] by the input simulation weights @xmath569 , where @xmath1143}(n)$ ] is the reservoir state in layer @xmath145 .",
    "applying exponential smoothing with smoothing rate @xmath1157 , and normalizing by the likewise smoothed variance of @xmath153}$ ] , gives update equations @xmath1158}(n+1 ) & = &   \\sigma \\ , \\bar{y}_{[l-1]}(n ) + ( 1-\\sigma)\\ , y_{[l-1]}(n+1 ) ,    \\mbox { ( running average)}\\label{equpdatediscrepancy1}\\\\   \\overline{\\mbox{var}}\\,y_{[l-1]}(n+1 ) & = & \\nonumber\\\\ & & \\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\sigma \\ , \\overline{\\mbox{var}}\\,y_{[l-1]}(n+1 ) + ( 1-\\sigma)\\ , ( y_{[l-1]}(n+1 ) - \\bar{y}_{[l-1]}(n+1))^2 , \\label{equpdatediscrepancy2 }   \\\\",
    "\\delta_{[l]}(n+1 ) & = & \\sigma \\,\\delta_{[l]}(n ) +   ( 1-\\sigma)\\ , \\frac{(y_{[l-1]}(n+1 ) -     dz_{[l]}(n+1))^2}{\\overline{\\mbox{var}}\\,y_{[l-1]}(n+1)}\\label{equpdatediscrepancy } \\end{aligned}\\ ] ] for the module - internal detected discrepancies @xmath1153}$ ] . in the bottom module @xmath1132}$ ] , the same procedure is applied to obtain @xmath1159}$ ] except that the module input is here the external driver @xmath14 instead the output @xmath153}(n)$ ] from the level below .    from these three discrepancy signals",
    "@xmath1153}(n)$ ] two trust variables @xmath1160 } , \\tau_{[23]}$ ] are derived .",
    "the intended semantics of @xmath148}$ ] can be stated as `` measuring the degree by which the discrepancy is reduced when going upwards from level @xmath145 to level @xmath152 '' .",
    "the rationale behind this is that when the currently active conception weights in modules @xmath145 and @xmath152 are appropriate for the current drive entering module @xmath171 from below ( or from the outside when @xmath141 ) , the discrepancy should _ decrease _ when going from level @xmath145 to level @xmath152 , while if the the currently applied conception weights are the wrong ones , the discrepancy should _ increase _ when going upwards .",
    "the core of measuring trust is thus the difference @xmath1153}(n ) - \\delta_{[l+1]}(n)$ ] , or rather ( since we want the same sensitivity across all levels of absolute values of @xmath436 ) the difference @xmath1161}(n ) ) - log(\\delta_{[l+1]}(n))$ ] .",
    "normalizing this to a range of @xmath398 by applying a logistic sigmoid with steepness @xmath1162}$ ] finally gives @xmath1163}(n ) = \\left(1 + \\left(\\frac{\\delta_{[l+1]}(n ) } { \\delta_{[l]}(n ) } \\right)^{d_{[l , l+1]}}\\right)^{-1}.\\ ] ] the steepness @xmath1162}$ ] of the trust sigmoid is an important design parameter , which currently i set manually .",
    "stated in intuitive terms it determines how `` decisively '' the system follows its own trust judgement .",
    "it could be rightfully called a `` meta - trust '' variable , and should itself be adaptive . as will become clear in the demonstrations below , large values of this _ decisiveness _ leads the system to make fast decisions regarding the type of the current driving input , at an increased risk of settling down prematurely on a wrong decision .",
    "low values of @xmath1162}$ ] allow the system to take more time for making a decision , consolidating information acquired over longer periods of possibly very noisy and only weakly pattern - characteristic input .",
    "my current view on the regulation of decisiveness is that it can not be regulated on the sole basis of the information contained in input data , but reflects higher cognitive capacities ( connected to mental attitudes like `` doubt '' , `` confidence '' , or even `` stubbornness '' ... ) which are intrinsically not entirely data - dependent .",
    "_ adapting the top - level conception weight vectors @xmath144}$ ] .",
    "_ for clarity of notation i will omit the level index @xmath140 $ ] in what follows , assuming throughout @xmath1144 . by equation ( [ eqarchcor ] ) , the effective conception weight vector used in the top module will be constrained to be a disjunction @xmath1164 , where the @xmath1049 are prototype conception weight vectors , computed at training time .",
    "adapting @xmath994 amounts to adjusting the apertures of the disjunctive components @xmath1049 via @xmath1147 .",
    "this is done indirectly .",
    "the training of the prototype conception weights ( and of the input simulation matrix @xmath569 and of the readout weights @xmath20 ) is done with a single module that is driven by the clean patterns @xmath59 .",
    "details of the training procedure are given in the experiments and methods section [ secdochierarchical ] .",
    "the prototype conception weight vectors can be written as @xmath1165 \\ , .\\!\\ast \\ , ( e[(z^j)^{.\\wedge 2 } ] + \\alpha^{-2})^{.\\wedge -1},\\ ] ] where @xmath1166 is the @xmath64-dimensional signal fed back from the feature space to the reservoir while the module is being driven with pattern @xmath52 during training , and the aperture @xmath43 is a design parameter .",
    "technically , we do not actually store the @xmath1049 but their constitutents @xmath43 and the corresponding mean signal energy vectors @xmath1167 $ ] , the latter of which are collected in an @xmath1168 _ prototype _ matrix @xmath1169)_{i = 1,\\ldots , m ; \\ ; j = 1,\\ldots,4}.\\ ] ] i return to the conceptor adaptation dynamics in the top module at exploitation time .",
    "using results from previous sections , equation ( [ eqarchcor ] ) can be re - written as @xmath1170\\right ) \\ , .\\!\\ast \\ , \\left(\\sum_j    ( \\gamma^j(n))^{.\\wedge 2 } \\ , .\\!\\ast \\ , e[(z^j)^{.\\wedge 2 } ] +    \\alpha^{-2}\\right)^{.\\wedge -1},\\ ] ] where the @xmath1171 operation is applied component - wise to its argument vector . the strategy for adapting the factors @xmath1147 is to minimize the loss function @xmath1172 - e[z^{.\\wedge 2 } ]   \\|^2,\\ ] ] where @xmath127 is the feature space output signal @xmath1173 available during exploitation time in the top module . in words ,",
    "the adaptation of @xmath995 aims at finding a weighted disjunction of prototype vectors which optimally matches the currently observed mean energies of the @xmath127 signal .",
    "it is straightforward to derive a stochastic gradient descent adaptation rule for minimizing the loss ( [ eqgammaloss ] ) .",
    "let @xmath1174 be the row vector made from the @xmath1131 , and let @xmath1175 denote element - wise squaring of a vector .",
    "then @xmath1176 implements the stochastic gradient of @xmath267 with respect to @xmath348 , where @xmath1177 is an adaptation rate .",
    "in fact i do not use this formula as is , but add two helper mechanisms , effectively carrying out @xmath1178 the addition of the term @xmath1179 pulls the @xmath1131 away from the possible extremes @xmath513 and @xmath18 toward @xmath898 with a _ drift _ force @xmath119 , which is a design parameter .",
    "this is helpful to escape from extreme values ( notice that if @xmath1180 , then @xmath1131 would forever remain trapped at that value in the absence of the drift force ) .",
    "the normalization ( [ eqgammaadapt2 ] ) to a unit sum of the @xmath1131 greatly reduces adaptation jitter .",
    "i found both amendments crucial for a reliable performance of the @xmath348 adaptation .",
    "given the @xmath1181 vector , the top - module @xmath994 is obtained by @xmath1182     for 4000 timesteps each .",
    "top row : evolution of @xmath1183 ( blue ) , @xmath1184 ( green ) , @xmath1185 ( red ) , @xmath1186 ( cyan ) in top layer module .",
    "rows 2 and 3 : same in modules 2 and 1 .",
    "fourth row : trust variables @xmath1160}$ ] ( blue ) and @xmath1187}$ ] ( green ) .",
    "fifth row : nrmses for reconstructed signals @xmath132}$ ] ( blue ) , @xmath133}$ ] ( green ) and @xmath134}$ ] ( red ) .",
    "black line shows the linear filter reference nrmse .",
    "thin red line : nrmse of phase - aligned @xmath134}$ ] .",
    "the plotting scale is logarithmic base 10 .",
    "bottom : pattern reconstruction snapshots from the last 20 timesteps in each pattern presentation block , showing the noisy input ( red ) , the layer-3 output @xmath134}$ ] ( thick gray ) and the clean signal ( thin black ) .",
    "for explanation see text.,title=\"fig:\",width=529 ] +   for 4000 timesteps each .",
    "top row : evolution of @xmath1183 ( blue ) , @xmath1184 ( green ) , @xmath1185 ( red ) , @xmath1186 ( cyan ) in top layer module .",
    "rows 2 and 3 : same in modules 2 and 1 .",
    "fourth row : trust variables @xmath1160}$ ] ( blue ) and @xmath1187}$ ] ( green ) .",
    "fifth row : nrmses for reconstructed signals @xmath132}$ ] ( blue ) , @xmath133}$ ] ( green ) and @xmath134}$ ] ( red ) .",
    "black line shows the linear filter reference nrmse .",
    "thin red line : nrmse of phase - aligned @xmath134}$ ] .",
    "the plotting scale is logarithmic base 10 .",
    "bottom : pattern reconstruction snapshots from the last 20 timesteps in each pattern presentation block , showing the noisy input ( red ) , the layer-3 output @xmath134}$ ] ( thick gray ) and the clean signal ( thin black ) .",
    "for explanation see text.,title=\"fig:\",width=510 ]    _ simulation experiment 1 : online classification and de - noising .",
    "_ please consult figure [ figcarch ] for a graphical display of this experiment .",
    "details ( training , initalization , parameter settings ) are provided in the experiments and methods section [ secdochierarchical ] .",
    "the trained 3-layer architecture was driven with a 16,000 step input signal composed of four blocks of 4000 steps each . in these blocks ,",
    "the input was generated from the patterns @xmath1188 in turn ( black lines in bottom row of figure [ figcarch ] ) , with additive gaussian noise scaled such that a signal - to - noise ratio of 1/2 was obtained ( red lines in bottom row of figure [ figcarch ] ) .",
    "the 3-layer architecture was run for the 16,000 steps without external intervention .",
    "the evolution of the four @xmath1131 weights in the top layer represent the system s classification hypotheses concerning the type of the current driver ( top row in figure [ figcarch ] ) . in all four blocks ,",
    "the correct decision is reached after an initial `` re - thinking '' episode .",
    "the trust variable @xmath1187}$ ] quickly approaches 1 after taking a drop at the block beginnings ( green line in fourth row of figure ) .",
    "this drop allows the system to partially pass through the external driver signal up to the top module , de - stabilizing the hypothesis established in the preceding block .",
    "the trust variable @xmath1160}$ ] ( blue line in fourth row ) oscillates more irregularly but also takes its steepest drops at the block beginnings .    for diagnostic purposes , @xmath1189}$ ]",
    "weights were also computed on the lower layers @xmath1190 , using ( [ eqgammaadapt1 ] ) and ( [ eqgammaadapt2 ] ) .",
    "these quantities , which were not entering the system s processing , are indicators of the `` classification belief states '' in lower modules ( second and third row in the figure ) . a stagewise consolidation of these hypotheses can be observed as one passes upwards through the layers .",
    "the four patterns fall in two natural classes , `` sinewaves '' and `` period-5 '' . inspecting the top - layer @xmath1131 it can be seen that within each block , the two hypothesis indicators associated with the `` wrong '' class are quickly suppressed to almost zero , while the two indicators of the current driver s class quickly dominate the picture ( summing to close to 1 ) but take a while to level out to their relative final values . since the top - level @xmath994 is formally a @xmath1131-weighted disjunction of the four prototype @xmath1049 conception vectors , what happens in the first block ( for instance ) can also be rephrased as , `` after the initial re - thinking , the system is confident that the current driver is @xmath38 or @xmath129 , while it is quite sure that it is not ( @xmath128 or @xmath39 ) '' .",
    "another way to look at the same phenomena is to say , `` it is easy for the system to quickly decide between the two classes , but it takes more time to distinguish the rather similar patterns within each class '' .",
    "the fifth row in figure [ figcarch ] shows the log10 nrmse ( running smoothed average ) of the three module outputs @xmath151}(n)$ ] with respect to a clean version of the driver ( thick lines ; blue = @xmath132}$ ] , green = @xmath133}$ ] , red = @xmath134}$ ] ) . for the 5-periodic patterns ( blocks 2 and 4 ) there is a large increase in accuracy from @xmath132}$ ] to @xmath133}$ ] to @xmath134}$ ] . for the sinewave patterns this is not the case , especially not in the first block",
    "the reason is that the re - generated sines @xmath133}$ ] and @xmath134}$ ] are not perfectly phase - aligned to the clean version of the driver .",
    "this has to be expected because such relative phase shifts are typical for coupled oscillator systems ; each module can be regarded as an oscillator .",
    "after optimal phase - alignment ( details in the experiments and methods section ) , the top - level sine re - generation matches the clean driver very accurately ( thin red line ) .",
    "the 5-periodic signal behaves differently in this respect .",
    "mathematically , an 5-periodic discrete - time dynamical system attractor is not an oscillation but a fixed point of the 5-fold iterated map , not admitting anything like a gradual phase shift .    as a baseline reference",
    ", i also trained a linear transversal filter ( wiener filter , see @xcite for a textbook treatment ) on the task of predicting the next clean input value ( details in the experiments and methods section ) .",
    "the length of this filter was 2600 , matching the number of trained parameters in the conceptor architecture ( @xmath215 has 500 @xmath1191 4 learnt parameters , @xmath569 has 500 , @xmath1192 has 100 ) .",
    "the smoothed log10 nrmse of this linear predictor is plotted as a black line .",
    "it naturally can reach its best prediction levels only after 2600 steps in each block , much more slowly than the conceptor architecture .",
    "furthermore , the ultimate accuracy is inferior for all four patterns .    _ simulation experiment 2 : tracking signal morphs .",
    "_ our architecture can be characterized as a signal cleaning - and - interpretation systems which guides itselft by allowing top - down hypotheses to make lower processing layers selective .",
    "an inherent problem in such systems is that that they may erroneously lock themselves on false hypotheses .",
    "top - down hypotheses are self - reinforcing to a certain degree because they cause lower layers to filter out data components that do not agree with the hypothesis  which is the essence of de - noising after all .    in order to test how",
    "our architecture fares with respect to this `` self - locking fallacy '' , i re - ran the simulation with an input sequence that was organized as a linear morph from @xmath38 to @xmath128 in the first 4000 steps ( linearly ramping up the sine frequency ) , then in the next block back to @xmath38 ; this was followed by a morph from @xmath129 to @xmath39 and back again .",
    "the task now is to keep track of the morph mixture in the top - level @xmath1131 .",
    "this is a greatly more difficult task than the previous one because the system does not have to just decide between 4 patterns , but has to keep track of minute changes in relative mixtures .",
    "the signal - to - noise ratio of the external input was kept at 0.5 .",
    "the outcome reveals an interesting qualitative difference in how the system copes with the sine morph as opposed to the 5-periodic morph . as can be seen in figure [ figcarchmorph ] , the highest - layer hypothesis indicators @xmath1131 can track the frequency morph of the sines ( albeit with a lag ) , but get caught in a constant hypothesis for the 5-period morph .",
    "this once again illustrates that irrational - period sines are treated qualitatively differently from integer - periodic signals in conceptor systems .",
    "i can not offer a mathematical analysis , only an intuition . in the sinewave tracking",
    ", the overall architecture can be described as a chain of three coupled oscillators , where the bottom oscillator is externally driven by a frequency - ramping sine .",
    "in such a driven chain of coupled oscillators , one can expect either chaos , the occurrence of natural harmonics , or frequency - locking across the elements of the chain . chaos and",
    "harmonics are ruled out in our architecture because the prototype conceptors and the loading of two related basic sines prevent it . only frequency - locking remains as an option , which is indeed what we find .",
    "the 5-periodic morph can not benefit from this oscillation entrainment .",
    "the minute differences in shape between the two involved prototypes do not stand out strongly enough from the noise background to induce a noticable decline in the trust variable @xmath1187}$ ] : once established , a single hypothesis persists .    on a side note it is interesting to notice that the linear filter that was used as a baseline can not at all cope with the frequency sweep , but for the 5-periodic morph it performs as well as in the previous simulation .",
    "both effects can easily be deduced from the nature of such filters .    only when i used a much cleaner input signal ( signal - to - noise ratio of 10 ) , and after the decisiveness @xmath119 was reduced to @xmath1013 , it became possible for the system to also track the 5-period pattern morph , albeit less precisely than it could track the sines ( not shown ) .",
    "to @xmath128 , back again in next 4000 steps ; steps 8000  16,000 : morphing from @xmath129 to @xmath39 and back again .",
    "figure layout same as in figure [ figcarch].,title=\"fig:\",width=529 ] +   to @xmath128 , back again in next 4000 steps ; steps 8000  16,000 : morphing from @xmath129 to @xmath39 and back again .",
    "figure layout same as in figure [ figcarch].,title=\"fig:\",width=510 ]    _ variants and extensions .",
    "_ when i first experimented with architectures of the kind proposed above , i computed the module - internal conception weight vectors @xmath144}^{\\mbox{\\scriptsize      aut}}$ ] ( compare equation ( [ eqarchmixc ] ) ) on the two lower levels not via the autoconception mechanism , but in a way that was similar to how i computed the top - level conception weight vector @xmath142}$ ] , that is , optimizing a fit to a disjunction of the four prototypes .",
    "abstractly speaking , this meant that a powerful piece of prior information , namely of knowing that the driver was one of the four prototypes , was inserted in all processing layers .",
    "this led to a better system performance than what i reported above ( especially , faster decisions in the sense of faster convergence of the @xmath1193}$ ] ) .",
    "however i subsequently renounced this `` trick '' because the differences in performance were only slight , and from a cognitive modelling perspective i found it more appealing to insert such a valuable prior only in the top layer ( motto : the retina does not conceptually understand what it sees ) .",
    "inspecting again the top row in figure [ figcarch ] , one finds fast initial decision between the alternatives `` pattern 1 or 2 '' versus `` pattern 3 or 4 '' , followed by a much slower differentation within these two classes .",
    "this suggests architecture variants where all layers are informed by priors of the kind as in equation ( [ eqarchcor ] ) , that is , the local conceptor on a layer is constrained to an aperture - weighted disjunction of a finite number of prototype conceptors .",
    "however , the number of prototype conceptors would shrink as one goes upwards in the hierarchy . the reduction in number",
    "would be effected by merging several distinct prototype conception vectors @xmath1194 in layer @xmath145 into a single prototype vector @xmath1195 . in terms of classical ai knowledge representation formalisms",
    "this would mean to implement an abstraction hierarchy .",
    "a further refinement that suggests itself would be to install a top - down processing pathway by which the current hypothesis on layer @xmath152 selects which finer - grained disjunction of prototypes on layer @xmath145 is chosen .",
    "for instance , when @xmath156}(n ) = c^j$ ] and @xmath1196 , then the conception weight vector @xmath144}(n)$ ] is constrained to be of the form @xmath1197}(n))$ ] .",
    "this remains for future work .",
    "the architecture presented above is replete with ad - hoc design decisions .",
    "numerous details could have been realized differently .",
    "there is no unified theory which could inform a system designer what are the `` right '' design decisions .",
    "a complete sia architecture must provide a plethora of dynamical mechanisms for learning , adaptation , stabilization , control , attention and so forth , and each of them in multiple versions tailored to different subtasks and temporal scales .",
    "i do not see even a theoretical possibility for an overarching , principled theory which could afford us with rigorous design principles for all of these .",
    "the hierarchical conceptor architecture presented here is far from realizing a complete sia system , but repercussions of that under - constrainedness of design already show .",
    "_ discussion .",
    "_ hierarchical neural learning architectures for pattern recognition have been proposed in many variants ( examples : @xcite ) , albeit almost always for static patterns .",
    "the only example of hierarchical neural architectures for temporal pattern recognition that i am aware of are the localist - connectionistic shruti networks for text understanding @xcite .",
    "inherently temporal hierarchical pattern classification is however realized in standard hidden - markov - model ( hmm ) based models for speech recognition .",
    "there is one common characteristic across all of these hierarchical recognition systems ( neural or otherwise , static or temporal ) .",
    "this shared trait is that when one goes upward through the processing layers , increasingly `` global '' or `` coarse - grained '' or `` compound '' features are extracted ( for instance , local edge detection in early visual processing leading through several stages to object recognition in the highest layer ) . while the concrete nature of this layer - wise integration of information differs between approaches , at any rate there is change of represented categories across layers . for the sake of discussion ,",
    "let me refer to this as the _ feature integration _ principle .    from the point of view of logic - based knowldege representation ,",
    "another important trait is shared by hierarchical pattern recognition systems : abstraction .",
    "the desired highest - level output is a class labelling of the input data .",
    "the recognition architecture has to be able to _ generalize _ from the particular input instance .",
    "this abstraction function is not explicitly implemented in the layers of standard neural ( or hmm ) recognizers . in rule - based decision - tree classification systems ( textbook :",
    "@xcite ) , which can also be regarded as hierarchical recognition systems , the hierarchical levels however directly implement a series of class abstractions .",
    "i will refer to the abstraction aspect of pattern recognition as the _ categorical abstraction _ principle .    the conceptor - based architecture presented in this section implements categorical abstraction through the @xmath348 variables in the highest layer .",
    "they yield ( graded ) class judgements similar to what is delivered by the class indicator variables in the top layers of typical neural pattern recognizers .",
    "the conceptor - based architecture is different from the typical neural recognition systems in that it does not implement the feature integration principle .",
    "as one progresses upwards through the layers , always the same dynamic item is represented , namely , the current periodic pattern , albeit in increasingly denoised versions .",
    "i will call this the _ pattern integrity _ principle .",
    "the pattern integrity principle is inherently conflicting with the feature integration principle .    by decades of theoretical research and successful pattern recognition applications , we have become accustomed to the feature integration principle .",
    "i want to argue that the pattern integrity principle has some cognitive plausibility and should be considered when one designs sia architectures .",
    "consider the example of listening to a familiar piece of music from a cd player in a noisy party environment .",
    "the listener is capable of two things .",
    "firstly , s / he can classify the piece of music , for instance by naming the title or the band .",
    "this corresponds to performing categorical abstraction , and this is what standard pattern recognition architectures would aim for .",
    "but secondly , the listener can also overtly sing ( or whistle or hum ) along with the melody , or s / he can mentally entrain to the melody .",
    "this overt or covert accompaniment has strong de - noising characteristics ",
    "party talk fragments are filtered out in the `` mental tracing '' of the melody .",
    "furthermore , the mental trace is temporally entrained to the source signal , and captures much temporal and dynamical detail ( single - note - level of accuracy , stressed versus unstressed beats , etc ) . that is an indication of pattern integrity .",
    "another example of pattern integrity : viewing the face of a conversation partner during a person - to - person meeting , say anne meeting tom . throughout the conversation , anne knows that the visual impression of tom s face is indeed _ tom _",
    "s face : categorical abstraction is happening .",
    "but also , just like a listener to noise - overlaid music can trace online the clean melody , anne maintains a `` clean video '' representation of tom s face as it undergoes a succession of facial expressions and head motions .",
    "anne experiences more of tom s face than just the top - level abstraction that it is tom s ; and this online experience is entrained to anne s visual input stream .",
    "pattern integrity could be said to be realized in standard hierarchical neural architectures to the extent that they are _",
    "generative_. generative models afford of mechanisms by which example instances of a recognition class can be actively produced by the system .",
    "prime examples are the architectures of adaptive resonance theory @xcite , the boltzmann machine @xcite and the restricted boltzmann machine / deep belief networks @xcite . in systems of this type ,",
    "the reconstruction of pattern instances occurs ( only ) in the input layer , which can be made to `` confabulate '' or `` hallucinate '' ( both terms are used as technical terms in the concerned literature ) pattern instances when primed with the right bias from higher layers .    projecting such architectures to the human brain ( a daring enterprise ) and returning to the two examples above , this would correspond to re - generating melodies in the early auditory cortex or facial expressions in the early visual cortex ( or even in the retina ) .",
    "but i do not find this a convincing model of what happens in a human brain .",
    "certainly i am not a neuroscientist and not qualified to make scientific claims here .",
    "my doubts rest on introspection ( forbidden !",
    "i know ) and on a computational argument .",
    "introspection : when i am mentally humming along with a melody at a party , i still do _ hear _ the partytalk ",
    "i dare say my early auditory modules keep on being excited by the entire auditory input signal .",
    "i do nt feel like i was hallucinating a clean version of the piece of music , making up an auditory reality that consists only of clean music .",
    "but i do not _ listen _ to the talk noise , i listen only to the music components of the auditory signal .",
    "the reconstruction of a clean version of the music happens  as far as i can trust my introspection ",
    "`` higher up '' in my brain s hierarchy , closer to the quarters where consciously controllable cognition resides .",
    "the computational argument : generative models , such as the ones mentioned , can not ( in their current versions at least ) generate clean versions of noisy input patterns while the input is presented .",
    "they either produce a high - level classification response while being exposed to input ( bottom - up processing mode ) , or they generate patterns in their lowest layer while being primed to a particular class in their highest layer ( top - down mode ) .",
    "they ca nt do both at the same time . but humans can : while being exposed to input , a cleaned - up version of the input is being maintained .",
    "furthermore , humans ( and the conceptor architecture ) can operate in an online - entrained mode when driven by temporal data , while almost all existing recognition architectures in machine learning are designed for static patterns .",
    "unfortunately i can not offer a clear definition of `` pattern integrity '' . an aspect of pattern integrity that i find important if not defining is that some temporal and spatial detail of a recognized pattern is preserved across processing layers . even at the highest layers ,",
    "a `` complete '' representation of the pattern should be available .",
    "this seems to agree with cognitive theories positing that humans represent concepts by _ prototypes _ , and more specifically , by",
    "_ exemplars _ ( critical discussion @xcite ) .",
    "however , these cognitive theories relate to empirical findings on human classification of static , not temporal , patterns .",
    "i am aware that i am vague .",
    "one reason for this is that we lack a scientific terminology , mathematical models , and standard sets of examples for discussing phenomena connected with pattern integrity .",
    "all i can bring to the table at this point is just a new architecture that has some extravagant processing characteristics .",
    "this is , i hope , relevant , but it is premature to connect this in any detail to empirical cognitive phenomena .      in this subsection",
    "i assume a basic acquaintance of the reader with boolean and first - order predicate logic .",
    "so far , i have established that conceptor matrices can be combined with ( almost ) boolean operations , and can be ordered by ( a version of ) abstraction . in this subsection",
    "i explore a way to extend these observations into a formal `` conceptor logic '' .",
    "before i describe the formal apparatus , i will comment on how i will be understanding the notions of `` concept '' and `` logic '' .",
    "such a preliminary clarification is necessary because these two terms are interpreted quite differently in different contexts .",
    "_ `` concepts '' in the cognitive sciences . _",
    "i start with a quote from a recent survey on research on concepts and categories in the cognitive sciences @xcite : _ `` the concept of concepts is difficult to define , but no one doubts that concepts are fundamental to mental life and human communication .",
    "cognitive scientists generally agree that a concept is a mental representation that picks out a set of entities , or a category .",
    "that is , concepts _ refer _ , and what they refer to are categories .",
    "it is also commonly assumed that category membership is not arbitrary but rather a principled matter .",
    "what goes into a category belongs there by virtue of some law - like regularities .",
    "but beyond these sparse facts , the concept concept is up for grabs . '' _ within this research tradition ,",
    "one early strand @xcite posited that the overall organization of a human s conceptual representations , his / her _ semantic memory _",
    ", can be formally well captured by ai representation formalisms called _ semantic networks _ in later years . in semantic network formalisms ,",
    "concepts are ordered in abstraction hierarchies , where a more abstract concept refers to a more comprehensive category . in subsequent research this formally clear - cut way of defining and organizing concepts largely dissolved under the impact of multi - faceted empirical findings . among other things",
    ", it turned out that human concepts are graded , adaptive , and depend on features which evolve by learning .",
    "such findings led to a diversity of enriched models of human concepts and their organization ( my favourites : @xcite ) , and many fundamental questions remain controversial .",
    "still , across all diversity and dispute , the basic conception of concepts spelled out in the initial quote remains largely intact , namely that concepts are mental representations of categories , and categories are defined _ extensionally _ as a set of `` entities '' .",
    "the nature of these entities is however `` up to grabs '' .",
    "for instance , the concept named `` blue '' might be referring to the set of blue physical objects , to a set of wavelengths of light , or to a set of sensory experiences , depending on the epistemic approach that is taken .",
    "_ `` concepts '' in logic formalisms . _",
    "i first note that the word `` concept '' is not commonly used in logics .",
    "however , it is quite clear what elements of logical systems are equated with concepts when such systems are employed as models of semantic memory in cognitive science , or as knowledge representation frameworks in ai .",
    "there is a large variety of logic formalisms , but almost all of them employ typed symbols , specifically unary predicate symbols , relation symbols of higher arity , constant symbols , and function symbols . in the model - theoretic view on logic ,",
    "such symbols become extensionally _ interpreted _ by sets of elements defined over the domain set of a set - theoretic model .",
    "unary predicate symbols become interpreted by sets of elements ; @xmath10-ary relation symbols become interpreted by @xmath10-tuples of such elements ; function symbols by sets of argument - value pairs ; constant symbols by individual elements .",
    "a logic _ theory _ uses a fixed set of such symbols called the theory s _",
    "signature_. within a theory , the interpretation of the signature symbols becomes constrained by the axioms of the theory . in ai knowledge representation systems , this set of axioms can be very large , forming a _ world model _ and _ situation model _",
    "( sometimes called `` t - box '' and `` a - box '' ) . in the parlance of logic - oriented ai ,",
    "the extension of unary predicate symbols are often called _ classes _ instead of `` categories '' .    in ai applications , the world model",
    "is often implemented in the structure of a _ semantic network _",
    "@xcite , where the classes are represented by nodes labelled by predicate symbols .",
    "these nodes are arranged in a hierarchy with more abstract class nodes in higher levels .",
    "this allows the computer program to exploit inheritance of properties and relations down the hierarchy , reducing storage requirements and directly enabling many elementary inferences .",
    "class nodes in semantic networks can be laterally linked by relation links , which are labelled by relation symbols . at the bottom of such a hierarchy one may locate _",
    "nodes labelled by constant symbols .",
    "a cognitive scientist employing such a semantic network representation would consider the class nodes , individual nodes , and relation links as computer implementations or formal models of class concepts , individual concepts , and relational concepts , respectively . also , semantic network specification languages are sometimes called _ concept description languages _ in ai programming . on this background",
    ", i will understand the symbols contained in a logic signature as names of concepts .",
    "furthermore , a logical _ expression _ @xmath1198 $ ] containing @xmath10 free ( first - order ) variables can be interpreted by the set of all @xmath10-tuples satisfying this expression .",
    "@xmath1198 $ ] thus defines an @xmath10-ary relation .",
    "for example , @xmath1199 = \\mbox{\\sc fruit}(x ) \\wedge \\mbox{\\sc yellow}(x ) \\wedge \\mbox{\\sc longish}(x ) \\wedge \\mbox{\\sc curved}(x)$ ] would represent a class ( seems to be the class of bananas ) .",
    "quite generally , logical expressions formed according to the syntax of a logic formalism can build representations of new concepts from given ones .",
    "there is an important difference between how `` concepts '' are viewed in cognitive modeling versus logic - based ai . in the latter field",
    ", concepts are typically _ named _ by symbols , and the formal treatment of semantics is based on a reference relationship between the symbols of a signature and their interpretations .",
    "however , even in logic - based knowledge representation formalisms there can be un - named concepts which are formally represented as logic expressions with free variables , as for instance the banana formula above . in cognitive science",
    ", concepts are not primarily or necessarily named , although a concept can be optionally labelled with a name .",
    "cognitive modeling can deal with conceptual systems that have not a single symbol , for instance when modeling animal cognition .",
    "by contrast , ai - style logic modeling typically is strongly relying on symbols ( the only exception being mathematical theories built on the empty signature ; this is of interest only for intra - mathematical investigations ) .    _ remarks on `` logics '' . _ in writing this paragraph , i follow the leads of the phd thesis @xcite of florian rabe which gives a comprehensive and illuminating account of today s world of formal logic research .",
    "the field of mathematical logics has grown and diversified enormously in the last three decades . while formal logic historically has been developed within and for pure mathematics ,",
    "much of this recent boost was driven by demands from theoretical computer science , ai , and semantic web technologies .",
    "this has led to a cosmos populated by a multitude of `` logics '' which sometimes differ from each other even in basic premises of what , actually , qualifies a formal system as a `` logic '' . in turn , this situation has led to _ meta - logical _ research , where one develops formal _ logical frameworks _ in order to systematically categorize and compare different logics .    among the existing such logical frameworks ,",
    "i choose the framework of _ institutions _",
    "@xcite , because it has been devised as an abstraction of model - theoretic accounts of logics , which allows me to connect quite directly to concepts , categories , and the semantic reference link between these two .",
    "put briefly , a formal system qualifies as a logic within this framework if it can be formulated as an institution .",
    "the framework of institutions is quite general : all logics used in ai , linguistics and theoretical cognitive sciences can be characterized as institutions .",
    "the framework of institutions uses tools from category theory . in this section",
    "i do not assume that the reader is familiar with category theory , and therefore will give only an intuitive account of how `` conceptor logic '' can be cast as an institution .",
    "a full categorical treatment is given in section [ sec : clogiccategorytheory ] .",
    "an institution is made of three main components , familiar from the model theory of standard logics like first - order predicate logic :    1 .   a collection @xmath1200 of _ signatures _ , where each signature @xmath803 is a set of _ symbols _ , 2 .   for each signature @xmath803 , a set @xmath1201 of @xmath803-_sentences _ that can be formed using the symbols of @xmath803 , 3 .",
    "again for each signature @xmath803 , a collection @xmath1202 of @xmath803-_models _ , where a @xmath803-model is a mathematical structure in which the symbols from @xmath803 are interpreted .",
    "furthermore , for every signature @xmath803 there is a _ model relation _ @xmath1203 . for a @xmath803-model @xmath502 and a @xmath803-sentence @xmath1204",
    ", we write infix notation @xmath1205 for @xmath1206 , and say `` @xmath502 is a model of @xmath1204 '' , with the understanding that the sentence @xmath1204 makes a true statement about @xmath502 .",
    "the relationsships between the main elements of an institution can be visualized as in figure [ figinstitution ] .",
    "the full definition of an institution includes a mechanism for symbol re - naming .",
    "the intuitive picture is the following .",
    "if a mathematician or an ai engineer writes down a set of axioms , expressed as sentences in a logic , the choice of symbols should be of no concern whatsoever .",
    "as hilbert allegedly put it , the mathematical theory of geometry should remain intact if instead of `` points , lines , surfaces '' one would speak of `` tables , chairs , beer mugs '' . in the framework of institutions",
    "this is reflected by formalizing how a signature @xmath803 may be transformed into another signature @xmath1207 by a _ signature morphism _ @xmath1208 , and how signature morphisms are extended to sentences ( by re - naming symbols in a sentence according to the signature morphism ) and to models ( by interpreting the re - named symbols by the same elements of a model that were previously used for interpreting the original symbols ) .",
    "then , if @xmath1209 denote the re - named model @xmath502 and sentence @xmath1204 , an institution essentially demands that @xmath1210 if and only if @xmath1211 .    for example , first - order logic ( fol ) can be cast as an institution by taking for @xmath1200 the class of all fol signatures , that is the class of all sets containing typed predicate , relation , function and constant symbols ; @xmath1212 maps a signature @xmath803 to the set of all closed ( that is , having no free variables ) @xmath803-expressions ( usually called _ sentences _ ) ; @xmath1213 assigns to each signature the class of all set - theoretic @xmath803-structures ; and @xmath160 is the satisfaction relation of fol ( also called model relation ) .",
    "for another example , boolean logic can be interpreted as an institution in several ways , for instance by declaring @xmath1200 as the class of all totally ordered countable sets ( the elements of which would be seen as boolean variables ) ; for each signature @xmath803 of boolean variables , @xmath1201 is the set of all boolean expressions @xmath1214 $ ] over @xmath803 and @xmath1202 is the set of all truth value assignments @xmath1215 to the boolean variables in @xmath803 ; and @xmath1216 if @xmath162 evaluates to @xmath429 under the assignment @xmath778 .    in an institution",
    ", one can define _ logical entailment _ between @xmath803-sentences in the familiar way , by declaring that @xmath1204 logically entails @xmath1217 ( where @xmath1218 are @xmath803-sentences ) if and only if for all @xmath803-models @xmath502 it holds that @xmath1205 implies @xmath1219 . by a standard abuse of notation , this",
    "is also written as @xmath1220 or @xmath1221 .",
    "i will sketch two entirely different approaches to define a `` conceptor logic '' .",
    "the first follows in the footsteps of familiar logics .",
    "conceptors can be named by arbitrary symbols , sentences are built by an inductive procedure which specifies how more complex sentences can be constructed from simpler ones by similar syntax rules as in first - order logic , and models are designated as certain mathematical structures built up from named conceptors .",
    "this leads to a logic that essentially represents a version of first - order logic constrained to conceptor domains .",
    "it would be a logic useful for mathematicians to investigate `` logical '' characteristics of conceptor mathematics , especially whether there are complete calculi that allow one to systematically prove all true facts concerning conceptors .",
    "i call such logics _ extrinsic conceptor logics_. extrinsic conceptor logics are tools for mathematicians to reason _ about _ conceptors",
    ". a particular extrinsic conceptor logic as an institution is detailed in section [ sec : clogiccategorytheory ] .",
    "the other approach aims at a conceptor logic that , instead of being a tool for mathematicians to reason _ about _ conceptors , is a model of how a situated intelligent agent does `` logical reasoning '' _ with _ conceptors .",
    "i call this _ intrinsic conceptor logic _ ( icl ) .",
    "an icl has a number of unconventional properties :    * an icl should function as a model of a situated agent s conceptor - based information processing .",
    "agents are bound to differ widely in their structure and their concrete lifetime learning histories .",
    "therefore i do not attempt to design a general `` fits - all - agents '' icl . instead",
    ", for every single , concrete agent life history there will be an icl , _ the _ private icl of _ that _ agent life .",
    "* an agent with a personal learning history is bound to develop its private `` logic '' over time .",
    "the icl of an agent life thus becomes a dynamical system in its own right .",
    "the framework of institutions was not intended by its designers to model temporally evolving objects .",
    "specifying an institution such that it can be considered a dynamical system leads to some particularly unconventional characteristics of an agent life icl .",
    "specifically , signatures become time - varying objects , and signature morphisms ( recall that these model the `` renaming '' of symbols ) are used to capture the temporal evolution of signatures .    an agent s lifetime icl",
    "is formalized differently according to whether the agent is based on matrix conceptors or random feature conceptors . here",
    "i work out only the second case .    in the following outline",
    "i use the concrete three - layer de - noising and classification architecture from section [ sechierarchicalarchitecture ] as a reference example to fill the abstract components of icl with life .",
    "even more concretely , i use the specific `` lifetime history '' of the 16000-step adaptation run illustrated in figure [ figcarch ] as demonstration example .",
    "for simplicity i will refer to that particular de - noising and classification architecture run as `` dca '' .",
    "here is a simplified sketch of the main components of an agent s lifetime icl ( full treatment in section [ sec : clogiccategorytheory ] ) :    1 .",
    "an icl is designed to model a particular agent lifetime history .",
    "a specification of such an icl requires that a formal model of such an _ agent life _ is available beforehand .",
    "the core part of an agent life model @xmath1222 is a set of @xmath502 conceptor adaptation sequences @xmath1223 , where each @xmath1224 is an @xmath64-dimensional conception weight vector .",
    "it is up to the modeler s discretion which conceptors in a modeled agent become included in the agent life model @xmath1222 . in the dca example",
    "i choose the four prototype conception weight vectors @xmath1225 and the two auto - adapted @xmath144}^{\\mbox{\\scriptsize aut}}$ ] on layers @xmath1226 . in this example",
    ", the core constitutent of the agent life model @xmath1222 is thus the set of @xmath1227 conceptor adaptation trajectories @xmath1228}^{\\mbox{\\scriptsize",
    "aut}}(n ) ,",
    "c_{[2]}^{\\mbox{\\scriptsize        aut}}(n)$ ] , where @xmath1229 .",
    "the first four trajectories @xmath1230 are constant over time because these prototype conceptors are not adapted ; the last two evolve over time . another part of an agent life model is the _ lifetime _",
    "@xmath429 , which is just the interval of timepoints @xmath10 for which the adaptation sequences @xmath1224 are defined . in the dca example , @xmath1231 .",
    "2 .   a signature is a finite non - empty set @xmath1232 of @xmath502 time - indexed symbols @xmath1233 . for every @xmath1234",
    "there is a signature @xmath1235 .",
    "+ _ dca example : _ in the icl of this example agent life , the collection @xmath1200 of signatures is made of 16000 signatures @xmath1236 containing six symbols each , with the understanding that the first four symbols refer to the prototype conceptors @xmath1230 and the last two refer to the auto - adapted conceptors @xmath1237}^{\\mbox{\\scriptsize    aut}}(n ) , c_{[2]}^{\\mbox{\\scriptsize aut}}(n)$ ] .",
    "3 .   for every pair @xmath1238 of signatures ,",
    "where @xmath1239 , there is a signature morphism @xmath1240 which maps @xmath1241 to @xmath1242 .",
    "these signature morphisms introduce a time arrow into @xmath1200 . this time arrow `` points backwards '' , leading from later times @xmath1243 to earlier times @xmath10 .",
    "there is a good reason for this backward direction .",
    "logic is all about describing facts . in a historically evolving system , facts @xmath1244 established at some later time @xmath1243 can be explained in terms of facts @xmath1245 at preceding times @xmath10 , but not vice versa .",
    "motto : `` the future can be explained in terms of the past , but the past can not be reduced to facts from the future '' .",
    "signature morphisms are a technical vehicle to re - formulate descriptions of facts .",
    "they must point backwards in time in order to allow facts at later times to become re - expressed in terms of facts stated for earlier times .",
    "figure [ figiclsigmorph ] illustrates the signatures and their morphisms in an icl .",
    "+    4 .   given a signature @xmath1232 , the set of sentences @xmath1246 which can be expressed with the symbols of this signature is the set of syntactic expressions defined inductively by the following rules ( incomplete , full treatment in next subsection ) : 1 .",
    "@xmath1247 are sentences in @xmath1248 .",
    "2 .   for @xmath1239",
    "such that @xmath1249 , for @xmath1250 , @xmath1251 is in @xmath1248 .",
    "3 .   if @xmath1252 , then @xmath1253 .",
    "4 .   if @xmath1254 , then @xmath1255 for every @xmath1256 $ ] ( this captures aperture adaptation ) . 5 .   if @xmath1252 and @xmath1257 , then @xmath1258 ( this will take care of linear blends @xmath1259 ) .",
    "+ in words , sentences express how new conceptors can be built from existing ones by boolean operations , aperture adaptation , and linear blends .",
    "the `` seed '' set for these inductive constructions is provided by the conceptors that can be directly identified by the symbols in @xmath1235 .",
    "+ the sentences of form @xmath1251 deserve a special comment .",
    "the operators @xmath1260 are time evolution operators . a sentence @xmath1251 will be made to refer to the conceptor version @xmath1261 at time @xmath1243 which has evolved from @xmath1224 .",
    "5 .   for every time @xmath10 ,",
    "the set @xmath1262 of @xmath1235-models is the set @xmath1263 of @xmath64-dimensional nonnegative vectors .",
    "+ _ remarks : _",
    "( i ) the idea for these models is that they represent mean energy vectors @xmath1264 $ ] of feature space states .",
    "( ii ) the set of models @xmath1262 is the same for every signature @xmath1235 .",
    "+ _ dca example : _ such feature space signal energy vectors occur at various places in the dca , for instance in equations ( [ eqzmatrix ] ) , ( [ eqcn ] ) , and conception weight vectors which appear in the dca evolution are all defined or adapted in one way or other on the basis of such feature space signal energy vectors .",
    "every @xmath1235-sentence @xmath1204 is associated with a concrete conception weight vector @xmath1265 by means of the following inductive definition : 1 .",
    "2 .   @xmath1267 .",
    "case @xmath1268 : @xmath1269 ( compare definition [ def : booleancvecs ] ) .",
    "case @xmath1270 : @xmath1271 .",
    "case @xmath1272 : @xmath1273",
    "case @xmath1274 : @xmath1275 ( compare definition [ def : apadaptcvecs ] ) .",
    "case @xmath1276 : @xmath1277 .",
    "+ _ remark : _ this statement of the interpretation operator @xmath1278 is suggestive only . the rigorous definition ( given in the next section ) involves additional nontrivial mechanisms to establish the connection between the symbol @xmath1279 and the concrete conceptor version @xmath1224 in the agent life .",
    "here i simply appeal to the reader s understanding that symbol @xmath1233 refers to object @xmath1280 .",
    "7 .   for @xmath1281 and @xmath1282 ,",
    "the model relationship is defined by @xmath1283 _ remark : _ this definition in essence just repeats how a conception weight vector is derived from a feature space signal energy vector .",
    "when all category - theoretical details are filled in which i have omitted here , one obtains a formal definition of an institution which represents _ the _ icl of an agent life @xmath1222 .",
    "it can be shown that in an icl , for all @xmath1284 it holds that @xmath1285 by virtue of this fact , logical entailment becomes _ decidable _ in an icl : if one wishes to determine whether @xmath1286 is implied by @xmath1287 , one can effectively compute the vectors @xmath1288 and then check in constant time whether @xmath1289 .    returning to the dca example ( with lifetime history shown in figure [ figcarch ] ) , its icl identiefies over time the four prototype conceptors @xmath1290 and the two auto - adapted conceptors @xmath1291}^{\\mbox{\\scriptsize auto } } , c_{[2]}^{\\mbox{\\scriptsize auto}}$ ] by temporally evolving symbols @xmath1292 @xmath1293 .",
    "all other conceptors that are computed in this architecture can be defined in terms of these six ones .",
    "for instance , the top - level conceptor @xmath142}(n)$ ] can be expressed in terms of the identifiable four prototype conceptors by @xmath142}(n ) = \\bigvee_{j=1,\\ldots,4 } \\varphi(c^j , \\gamma^j(n))$ ] by combining the operations of disjunction and aperture adaptation . in icl syntax",
    "this construction would be expressible by a @xmath1235 sentence , for instance by @xmath1294    a typical adaptation objective of a conception vector @xmath994 occurring in an agent life is to minimize a loss of the form ( see definition [ eqcadaptobjective ] ) @xmath1295 + \\alpha^{-2}\\,\\|c(n)\\|^2,\\ ] ] or equivalently , the objective is to converge to @xmath1296 \\,.\\!\\ast\\ , ( e[\\alpha^2 z^{.\\wedge    2 } ] + 1)^{.\\wedge -1}.\\ ] ] this can be re - expressed in icl terminology as `` adapt @xmath994 such that @xmath1297 \\models_{\\sigma^{(n ) } } \\chi_{c(n)}$ ] , and such that not @xmath1298 for any @xmath1299 $ ] '' ( here @xmath1300 is an adhoc notation for an icl sentence @xmath1301 specifying @xmath994 ) . in more abstract terms , the typical adaptation of random feature conceptors can be understood as an attempt to converge toward the conceptor that is maximally @xmath160-specific under a certain constraint .    _ discussion . _",
    "i started this section by a rehearsal of how the notion of `` concept '' is understood in cognitive science and logic - based ai formalisms . according to this understanding",
    ", a concept _ refers _ to a category ( terminology of cognitive science ) ; or a class symbol or logical expression with free variables is _ interpreted by _",
    "its set - theoretical extension ( logic terminology ) .",
    "usually , but not necessarily , the concepts / logical expressions are regarded as belonging to an `` ontological '' domain that is different from the domain of their respective referents . for instance , consider a human maintaining a concept named cow in his / her mind .",
    "then many cognitive scientists would identifiy the category that is referred to by this concept with the some set of physical cows .",
    "similarly , an ai expert system set up as a farm management system would contain a symbol cow in its signature , and this symbol would be deemed to refer to a collection of physical cows . in both cases ,",
    "the concept / symbolic expression cow is ontologically different from a set of physical cows .",
    "however , both in cognitive science and ai , concepts / symbolic expressions are sometimes brought together with their referents much more closely . in some perspectives taken in cognitive science",
    ", concepts are posited to refer to other mental items , for instance to sensory perceptions . in most current ai proof calculi",
    "( `` inference engines '' ) , models of symbolic expressions are created which are assembled not from external physical objects but from symbolic expressions ( `` herbrand universe '' constructions ) .",
    "symbols from a signature @xmath803 then refer to sets of @xmath803-terms . in sum , fixing the ontological nature of referents is ultimately left to the modeling scientist in cognitive science or ai .",
    "in contrast , icl is committed to one particular view on the semantic relationship : @xmath1235-sentences are always describing conception weight vectors , and refer to neural activation energy vectors @xmath1302 . in the case of matrix conceptor",
    "based agents , @xmath1235-sentences describe conceptor matrices and refer to neural activation correlation matrices @xmath23 by the following variant of ( [ eqsemmodels ] ) :    @xmath1303    in figure [ figsemantics ] i try to visualize this difference between the classical , extensional view on symbols and their referents , and the view adopted by icl .",
    "this figure contrasts how classical logicians and cognitive scientists would usually model an agent s representation of farm livestock , as opposed to how icl renders that situation .",
    "the semantic relation is here established between the physical world on the one side , and symbols and logical expressions on the other side .",
    "the world is idealized as a set of individuals ( individual animals in this example ) , and symbols for concepts ( predicate symbols in logic ) are semantically interpreted by sets of individuals . in the farmlife example , a logician might introduce a symbol lifestock which would denote the set of all economically relevant animals grown in farms , and one might introduce another symbol poultry to denote the subset of all feathered such animals . the operator that creates `` meaning '' for concept symbols is the grouping of individuals into sets ( the bold `` @xmath1304 '' in figure [ figsemantics ] ) .    with conceptors ,",
    "the semantic relation connects neural activity patterns triggered by perceiving animals on the one side , with conceptors acting on neural dynamics on the other side .",
    "the core operator that creates meaning is the condensation of the incoming data into a neural activation energy pattern @xmath1302 ( or correlation matrix @xmath23 for matrix conceptors ) from which conceptors are generated via the fundamental construction @xmath1305 \\,.\\!\\ast\\ , ( e[z^{.\\wedge 2}]+1)^{.\\wedge -1}$ ] or @xmath397 ( figure [ figsemantics ] depicts the latter case ) .",
    "icl , as presented here , can not claim to be a model of all `` logical reasoning '' in a neural agent .",
    "specifically , humans sometimes engage in reasoning activities which are very similar to how syntactic logic calculi are executed in automated theorem proving .",
    "such activities include the build - up and traversal of search trees , creating and testing hypotheses , variable binding and renaming , and more .",
    "a standard example is the step - by - step exploration of move options done by a human chess novice .",
    "icl is not designed to capture such conscious combinatorial logical reasoning .",
    "rather , icl is intended to capture the automated aspects of neural information processing of a situated agent , where incoming ( sensor ) information is immediately transformed into perceptions and maybe situation representations in a tight dynamical coupling with the external driving signals .",
    "the material presented in this and the next section is purely theoretical and offers no computational add - on benefits over the material presented in earlier sections .",
    "there are three reasons why nonetheless i invested the effort of defininig icls :    * by casting conceptor logic rigorously as an institution , i wanted to substantiate my claim that conceptors are `` logical '' in nature , beyond a mere appeal to the intuition that anything admitting boolean operations is logic . * the institutional definition given here provides a consistent formal picture of the semantics of conceptors . a conceptor @xmath995 identified by an icl sentence",
    "@xmath1306 `` means '' neural activation energy vectors @xmath1302 .",
    "conceptors and their meanings are both neural objects of the same mathematical format , @xmath64-dimensional nonnegative vectors .",
    "having a clear view on this circumstance helps to relate conceptors to the notions of concepts and their referents , which are so far from being fully understood in the cognitive sciences . * some of the design ideas that went into casting icls as institutions may be of more general interest for mathematical logic research .",
    "specifically , making signatures to evolve over time  and hence , turn an institution into a dynamical system  might be found a mechanism worth considering in scenarios , unconnected with conceptor theory or neural networks , where one wants to analyse complex dynamical systems by means of formal logics .      in this section",
    "i provide a formal specification of conceptor logic as an institution .",
    "this section addresses only readers with a dedicated interest in formal logic .",
    "i assume that the reader is familiar with the institution framework for representing logics ( introduced in @xcite and explained in much more detail in section 2 in @xcite ) and with basic elements of category theory .",
    "i first repeat almost verbatim the categorical definition of an institution from @xcite .",
    "[ definstitutioncatth ] an _ institution _ @xmath1307 consists of    1 .   a category @xmath1200 , whose objects @xmath803 are called _ signatures _ and whose arrows are called _ signature morphisms _ , 2 .",
    "a functor @xmath1308 , giving for each signature a set whose elements are called _ sentences _ over that signature , 3 .   a functor @xmath1309 , giving for each signature @xmath803 a category @xmath1202 whose objects are called @xmath803-_models _ , and whose arrows are called @xmath803-(model ) _ morphisms _ , and 4 .   a relation @xmath1310 for each @xmath1311 , called @xmath803-_satisfaction _ ,    such that for each morphism @xmath1312 in @xmath1200 , the _ satisfaction condition _ @xmath1313 holds for each @xmath1314 and each @xmath1315 .",
    "the interrelations of these items are visualized in figure [ figinstitutioncattheory ] .",
    "_ remarks : _    1 .",
    "the morphisms in @xmath1200 are the categorical model of re - naming the symbols in a logic .",
    "the essence of the entire apparatus given in definition [ definstitutioncatth ] is to capture the condition that the model - theoretic semantics of a logic is invariant to renamings of symbols , or , as goguen and burstall state it , `` truth is invariant under change of notation '' .",
    "the intuition behind @xmath803-model - morphisms , that is , maps @xmath1316 , where @xmath1317 are two @xmath803-models , is that @xmath310 is an embedding of @xmath1318 in @xmath1319 .",
    "if we take first - order logic as an example , with @xmath1317 being two @xmath803-structures , then @xmath1320 would be a map from the domain of @xmath1318 to the domain of @xmath1319 which preserves functional and relational relationships specified under the interpretations of @xmath1318 and @xmath1319 .",
    "3 .   in their original 1992 paper @xcite ,",
    "the authors show how a number of standard logics can be represented as institutions . in the time that has passed since then",
    ", institutions have become an important `` workhorse '' for software specification in computer science and for semantic knowledge management systems in ai , especially for managing mathematical knowledge , and several families of programming toolboxes have been built on institutions ( overview in @xcite ) . alongside with",
    "the model - theoretic spirit of institutions , this proven usefulness of institutions has motivated me to adopt them as a logical framework for conceptor logic .",
    "logical entailment between sentences is defined in institutions in the traditional way :    [ defentailment ] let @xmath1321 .",
    "then @xmath1322 _ entails _",
    "@xmath1323 , written @xmath1324 , if for all @xmath1325 it holds that @xmath1326 .",
    "institutions are flexible and offering many ways for defining logics .",
    "i will frame two entirely different kinds of conceptor logics .",
    "the first kind follows the intuitions behind the familiar first - order predicate logic , and should function as a formal tool for mathematicians to reason about ( and with ) conceptors",
    ". since it looks at conceptors `` from the outside '' i will call it _ extrinsic conceptor logic _ ( ecl ) .",
    "although ecl follows the footsteps of familiar logics in many respects , in some aspects it deviates from tradition .",
    "the other kind aims at modeling the `` logical '' operations that an intelligent neural agent can perform whose `` brain '' implements conceptors .",
    "i find this the more interesting formalization ; certainly it is the more exotic one",
    ". i will call it _ intrinsic conceptor logic _ ( icl ) .",
    "_ extrinsic conceptor logic . _",
    "i first give an intuitive outline .",
    "i treat only the case of matrix - based conceptors .",
    "an ecl concerns conceptors of a fixed dimension @xmath8 and their logical interrelationships , so one should more precisely speak of @xmath8-dimensional ecl .",
    "i assume some @xmath8 is fixed .",
    "sentences of ecl should enable a mathematician to talk about conceptors in a similar way as familiar predicate logics allow a mathematician to describe facts about other mathematical objects .",
    "for example , `` for all conceptors @xmath282 , @xmath504 it holds that @xmath1327 and @xmath1328 '' should be formalizable as an ecl sentence .",
    "a little notational hurdle arises because boolean operations appear in two roles : as operators acting on conceptors ( the `` @xmath81 '' in the sentence above ) , and as constitutents of the logic language ( the `` and '' in that sentence ) . to keep these two roles notationally apart",
    ", i will use @xmath1329 ( allowing infix notation ) for the role as operators , and @xmath1330 for the logic language .",
    "the above sentence would then be formally written as `` @xmath1331 '' .",
    "the definition of signatures and ecl - sentences in many respects follows standard customs ( with significant simplifications to be explained after the definition ) and is the same for any conceptor dimension @xmath8 :    [ defsyntaxecl ] let @xmath1332 be a fixed countable indexed set of",
    "_ variables_.    1 .",
    "( ecl - signatures ) the objects ( signatures ) of @xmath1333 are all countable sets , whose elements are called _",
    "symbols_. for signatures @xmath1334 , the set of morphisms @xmath1335 is the set of all functions @xmath1336 .",
    "( ecl - terms ) given a signature @xmath803 , the set of @xmath803-_terms _ is defined inductively by 1 .",
    "every variable @xmath217 , every symbol @xmath1337 , and @xmath36 is a @xmath803-term .",
    "2 .   for @xmath803-terms @xmath1338 and @xmath355 $ ] , the following are @xmath803-terms : @xmath1339 , @xmath1340 , @xmath1341 , and @xmath1342 .",
    "( ecl - expressions ) given a signature @xmath803 , the set @xmath1343 of @xmath803-_expressions _ is defined inductively by 1 .   if @xmath1338 are @xmath803-terms , then @xmath1344 is a @xmath803-expression .",
    "2 .   if @xmath1345 are @xmath803-expressions , and @xmath217 a variable , then the following are @xmath803-expressions : @xmath1346 , @xmath1347 , @xmath1348 , @xmath1349 .",
    "( ecl - sentences ) a @xmath803-expression that contains no free variables is a @xmath803-_sentence _ ( free occurrence of variables to be defined as usual , omitted here . )    given a @xmath803-morphism @xmath1312 , its image @xmath1350 under the functor @xmath1351 is the map which sends every @xmath1352-sentence @xmath1322 to the @xmath1353-sentence @xmath1323 obtained from @xmath1322 by replacing all occurrences of @xmath1352 symbols in @xmath1322 by their images under @xmath1354 .",
    "i omit the obvious inductive definition of this replacement construction .",
    "notes :    * ecl only has a single sort of symbols with arity 0 , namely constant symbols ( which will be made to refer to conceptors later ) .",
    "this renders the categorical treatment of ecl much simpler than it is for logics with sorted symbols of varying arities . * the operator symbols @xmath1355 , @xmath1356 , @xmath1357 , the parametrized operation symbol @xmath1358 and the relation symbol @xmath541",
    "are not made part of signatures , but become universal elements in the construction of sentences .",
    "the models of ecl are quite simple . for a signature @xmath803 , the objects of @xmath1359 are the sets of @xmath803-indexed @xmath8-dimensional conceptor matrices @xmath1360 where @xmath1361 is the set of all @xmath8-dimensional conceptor matrices .",
    "the objects of @xmath1359 are thus the graph sets of the functions from @xmath803 to the set of @xmath8-dimensional conceptor matrices .",
    "the model morphisms of @xmath1359 are canonically given by the index - preserving maps @xmath1362 clearly , @xmath1363 contains exactly one element .",
    "given a signature morphism @xmath1364 , then @xmath1365 is defined to be a map from @xmath1366 to @xmath1367 as follows . for",
    "a @xmath1353-model @xmath1368 let @xmath1369\\!]^{m_2}$ ] denote the interpretation of @xmath1028 in @xmath1370 , that is , @xmath1369\\!]^{m_2}$ ] is the conceptor matrix @xmath1371 for which @xmath1372 .",
    "then @xmath1365 assigns to to @xmath1370 the @xmath1352-model @xmath1373\\!]^{m_2 } , \\sigma_1)\\ } \\in \\mbox{\\emph{mod}}(\\sigma_1)$ ] .",
    "the model relations @xmath1374 are defined in the same way as in the familiar first - order logic .",
    "omitting some detail , here is how :    preliminaries : a map @xmath1375 is called a _",
    "variable assignment_. @xmath1376 is the set of all variable assignments .",
    "we denote by @xmath1377 the variable assignment that is identical to @xmath1378 except that @xmath217 is mapped to @xmath25 .",
    "a @xmath803-_interpretation _ is a pair @xmath1379 consisting of a @xmath803-model @xmath502 and a variable assignment @xmath1378 . by @xmath1380",
    "we denote the interpretation @xmath1381 . for a @xmath803-term @xmath905 , the interpretation @xmath1382 is defined in the obvious way .",
    "then @xmath1383 is defined inductively by    1 .",
    "@xmath1384 iff @xmath1385 , 2 .",
    "@xmath1386 iff @xmath1387 , 3 .",
    "@xmath1388 iff @xmath1389 and @xmath1390 , 4 .",
    "@xmath1391 iff @xmath1389 or @xmath1390 , 5 .",
    "@xmath1392 iff @xmath1393 for all @xmath1394 it holds that @xmath1395 .",
    "@xmath1374 then is the restriction of @xmath1396 on sentences .",
    "this completes the definition of ecl as an institution .",
    "the satisfaction condition obviously holds .",
    "while in many respects ecl follows the role model of first - order logic , the associated model theory is much more restricted in that only @xmath8-dimensional conceptors are admitted as interpretations of symbols .",
    "the natural next step would be to design calculi for ecl and investigate whether this logic is complete or even decidable .",
    "clarity on this point would amount to an insight in the computational tractability of knowledge representation based on matrix conceptors with boolean and aperture adaptation opertors .",
    "_ intrinsic conceptor logic . _",
    "i want to present icl as a model of the `` logics '' which might unfold _ inside _ a neural agent .",
    "all constitutents of icl should be realizable in terms of neurodynamical processes , giving a logic not for reasoning _ about _ conceptors , but _ with _ conceptors .",
    "taking the idea of placing `` logics '' _ inside _ an agent seriously has a number of consequences which lead quite far away from traditional intuitions about `` logics '' :    * different agents may have different logics .",
    "i will therefore not try to define a general icl that would fit any neural agent .",
    "instead every concrete agent with a concrete lifetime learning history will need his / her / its own individual conceptor logic .",
    "i will use the signal de - noising and classification architecture from section [ sechierarchicalarchitecture ] as an example `` agent '' and describe how an icl can be formulated as an institution for this particular case .",
    "some general design principles will however become clear from this case study .",
    "* conceptors are all about temporal processes , learning and adaptation .",
    "an agent s private icl will have to possess an eminently dynamical character .",
    "concepts will change their meaning over time in an agent .",
    "this `` personal history dynamics '' is quintessential for modeling an agent and should become reflected in making an icl a dynamical object itself  as opposed to introducing time through descriptive syntactical elements in an otherwise static logic , like it is traditionally done by means of modal operators or axioms describing a timeline . in my proposal of icls , time enters the picture through the central constitutent of an institution , signature morphisms .",
    "these maps between signatures ( all commanded by the same agent ) will model time , and an agent s lifetime history of adaptation will be modeled by an evolution of signatures . where the original core motif for casting logics as institutions was that `` truth is invariant under change of notation '' ( @xcite ) , the main point of icls could be contrasted as `` concepts and their meaning change with time '' .",
    "the role of signature morphisms in icls is fundamentally different in icls compared to customary formalizations of logics . in the latter",
    ", signature changes should leave meaning invariant ; in the former , adaptive changes in conceptors are reflected by temporally indexed changes in signature . * making an icl private to an agent implies that the model relation @xmath160 becomes agent - specific .",
    "an icl can not be specified as an abstract object in isolation .",
    "before it can be defined , one first needs to have a formal model of a particular agent with a particular lifelong adaptation history .    in sum , an icl ( formalized as institution ) itself becomes a dynamical system , defined relative to an existing ( conceptor - based ) neural agent with a particular adaptation history .",
    "the `` state space '' of an icl will be the set of signatures .",
    "a `` trajectory '' of the temporal evolution of an icl will essentially be a sequence of signatures , enriched with information pertaining to forming sentences and models .",
    "for an illustration , assume that a neural agent adapts two random feature conceptors @xmath1397 .",
    "these are named by two _ temporally indexed _",
    "symbols @xmath1398 .",
    "a signature will be a timeslice of these , @xmath1399 .",
    "for every pair of integer timepoints @xmath1400 ( where @xmath1401 ) there will be a signature morphism @xmath1402 .",
    "the ( strong ) reason why signature morphisms point backwards in time will become clear later .",
    "figure [ figiclsigmorph ] visualizes the components of this example .",
    "the dotted lines connecting the @xmath1242 are suggestive graphical hints that the symbols @xmath1242 all name the `` same '' conceptor @xmath1280 . how this `` sameness of identity over time '' can be captured in the institution formalism",
    "will become clear presently .",
    "formal definitions of icls will vary depending on what kind of conceptors are used ( for instance , matrix or random feature based ) , or whether time is taken to be discrete or continuous .",
    "i give a definition for discrete - time , random feature conceptor based icls .",
    "because icls will be models of an agent s private logic which evolves over the agent s lifetime , the definition of an icl is stated relative to an agent s lifetime conceptor adaptation history .",
    "the only property of such an agent that is needed for defining an icl is the existence of temporally adapted conceptors owned by the agent . putting this into a formal definition :    [ deficlagent ] an _ agent life _ ( here : @xmath64-dimensional random feature conceptor based , discrete time ) is a structure @xmath1403 , where    1 .",
    "@xmath1404 is an interval ( finite or infinite ) of the integers , the _ lifetime _ of @xmath1222 , 2 .",
    "@xmath1405 is a finite nonempty set of _ conceptor identifiers _ , 3 .",
    "@xmath1406^m , ( a_i , n ) \\mapsto a_i(n)$ ] assigns to every time point and conceptor identifier an _ adaptation version _",
    "@xmath1224 of the conceptor identified by the symbol @xmath1233 .    as an example of an agent",
    "consider the signal de - noising and classification architecture ( dca ) presented in section [ sechierarchicalarchitecture ] , with a `` life '' being the concrete 16000-step adaptation run illustrated in figure [ figcarch ] . in this example , the lifetime is @xmath1407 .",
    "i will identify by symbols the four prototype conceptors @xmath1290 and the two auto - adapted conceptors @xmath1291}^{\\mbox{\\scriptsize auto } } , c_{[2]}^{\\mbox{\\scriptsize auto}}$ ] . accordingly",
    "i choose @xmath803 to be @xmath1408 .",
    "the map @xmath1409 is constant in time for the four protype conceptors : @xmath1410 for all @xmath1411 . for the remaining two conceptors , @xmath1412}^{\\mbox{\\scriptsize auto}}(n)$ ] .",
    "the stage is now prepared to spell out the definition of an agent s lifetime icl ( for the case of an agent based on @xmath64-dimensional random feature conceptor and discrete time ) :    [ deficl ] the _ _ intrinsic conceptor logic ( icl ) of an agent life__@xmath1413 is an institution whose components obey the following conditions :    1 .",
    "the objects ( signatures ) of @xmath1414 are the pairs @xmath1415 , where @xmath1416 , and @xmath1417 is a bijection .",
    "+ * dca example : * the lifetime of this example is @xmath1418 . a signature @xmath1419 at time @xmath1234 will later be employed to denote some of the conceptors in the dca in their adapted versions at time @xmath10 .",
    "these conceptor adaptation versions will thus become identifiable by symbols from @xmath1235 .",
    "for @xmath1420 i take the natural projection @xmath1421 .",
    "2 .   for every @xmath1422 ( where @xmath1239 ) , @xmath1423 is a morphism in @xmath1414 .",
    "there are no other morphisms in @xmath1200 besides these . _",
    "remark : _ at first sight this might seem unneccessarily complicated .",
    "why not simply require @xmath1424 ?",
    "the reason is that the set of symbols @xmath1425 of @xmath1235 is just that , a set of symbols .",
    "that over time @xmath1279 should correspond to @xmath1426 is only visually suggested to us , the mathematicians , by the chosen notation for these symbols , but by no means does it actually follow from that notation .",
    "@xmath1248 is inductively defined as follows :",
    "1 .   @xmath1247 and @xmath36 and @xmath513 are sentences in @xmath1248 .",
    "2 .   for @xmath1239",
    "such that @xmath1249 , for @xmath1250 , @xmath1251 is in @xmath1248 . _",
    "remark : _ the @xmath436 operators capture the temporal adaptation of conceptors . the symbol @xmath1279 will be used to denote a conceptor @xmath1224 in its adaptation version at time @xmath10 , and the sentence @xmath1251 will be made to refer to @xmath1261 .",
    "3 .   if @xmath1252 , then @xmath1253 . _",
    "remark : _ unlike in ecl there is no need for a notational distinction between @xmath81 and @xmath1427 etc .",
    "4 .   if @xmath1254 , then @xmath1255 for every @xmath1256 $ ] ( this captures aperture adaptation ) . 5 .   if @xmath1252 and @xmath1257 , then @xmath1258 ( this will take care of linear blends @xmath1259 ) . + _ remark : _ including @xmath36 and @xmath513 in the sentence syntax is a convenience item",
    ". @xmath513 could be defined in terms of any @xmath1279 by @xmath1428 , and @xmath36 by @xmath1429 .",
    "likewise , @xmath77 ( or @xmath81 ) could be dismissed because it can be expressed in terms of @xmath81 and @xmath80 ( @xmath77 and @xmath80 , respectively ) .",
    "4 .   for a signature morphism @xmath1430 , @xmath1431 is the map defined inductively as follows : 1 .",
    "2 .   @xmath1433 .",
    "_ remark 1 : _ when we use the natural projections @xmath1434 , this rule could be more simply written as @xmath1435 . _",
    "remark 2 : _ this is the pivotal point in this entire definition , and the point where the difference to customary views on logics comes to the surface most conspicuously .",
    "usually signature morphisms act on sentences by simply re - naming all signature symbols that occur in a sentence .",
    "the structure of a sentence remains unaffected , in agreement with the motto `` truth is invariant under change of notation '' . by contrast , here a signature symbol @xmath1426 is replaced by an temporal change operator term @xmath1436 , reflecting the new motto `` meaning changes with time '' .",
    "the fact that @xmath1437 leads from @xmath1426 to @xmath1279 establishes `` sameness of identity over time '' between @xmath1426 and @xmath1279 .",
    "usually one would formally express sameness of identity of some mathematical entity by using the same symbol to name that entity at different time points . here",
    "different symbols are used , and thus another mechanism has to be found in order to establish that an entity named by different symbols at different times remains `` the same '' .",
    "the dotted `` identity '' lines in figure [ figiclsigmorph ] are fixed by the signature morphisms @xmath1437 , not by using the same symbol over time .",
    "_ remark 3 : _ at this point",
    "it also becomes clear why the signature morphisms @xmath1430 lead backwards in time .",
    "a conceptor @xmath1261 in its time-(@xmath1243 ) version can be expressed in terms of the earlier version @xmath1224 with the aid of the temporal evolution operator @xmath436 , but in general an earlier version @xmath1224 can not be expressed in terms of a later @xmath1261 .",
    "this reflects the fact that , seen as a trajectory of an input - driven dynamical system , an agent life is ( typically ) irreversible . to put it into everyday language , `` the future can be explained from the past , but not vice versa '' .",
    "3 .   @xmath1438 .",
    "4 .   for @xmath1439 , put @xmath1440 5 .",
    "for every signature @xmath1441 , @xmath1262 is always the same category @xmath1263 with objects all non - negative @xmath64-dimensional vectors @xmath1302 .",
    "there are no model morphisms except the identity morphisms @xmath1442 .",
    "6 .   for every morphism @xmath1443",
    ", @xmath1444 is the identity morphism of @xmath1263 .",
    "7 .   as a preparation for defining the model relationships",
    "@xmath1445 we assign by induction to every sentence @xmath1282 an @xmath64-dimensional conception weight vector @xmath1265 as follows : 1 .",
    "@xmath1446 and @xmath1447 .",
    "2 .   @xmath1448 .",
    "3 .   @xmath1449 .",
    "case @xmath1268 : @xmath1269 ( compare definition [ def : booleancvecs ] ) .",
    "case @xmath1270 : @xmath1271",
    "case @xmath1272 : @xmath1273",
    "case @xmath1274 : @xmath1275 ( compare definition [ def : apadaptcvecs ] ) .",
    "case @xmath1276 : @xmath1277",
    ". 8 .   for @xmath1450 and @xmath1282 ,",
    "the model relationship is defined by @xmath1451    the satisfaction condition ( [ eqsatisfactioncondition ] ) requires that for @xmath1452 and @xmath1453 it holds that @xmath1454 which is equivalent to @xmath1455 because @xmath1456 is the identity morphism on @xmath1263 .",
    "this follows directly from the following fact :    [ lemmasatcon ] @xmath1457 ,    which in turn can be established by an obvious induction on the structure of sentences , where the crucial steps are the cases ( i ) @xmath1458 and ( ii ) @xmath1459 .    in case ( i ) , @xmath1460 .    in case ( ii ) , conclude @xmath1461 + @xmath1462 .",
    "an important difference between `` traditional '' logics and the icl of an agent @xmath1222 concerns different intuitions about semantics .",
    "taking first - order logic as an example of a traditional logic , the `` meaning '' of a first - order sentence @xmath1204 with signature @xmath803 is the class of all of its models . whether a set is a model of @xmath1204 depends on how the symbols from @xmath803 are extensionally interpreted over that set .",
    "first - order logic by itself does not prescribe how the symbols of a signature have to be interpreted over some domain .",
    "in contrast , an icl is defined with respect to a concrete agent @xmath1222 , which in turn uniquely fixes how the symbols from an icl signature @xmath1235 _ must _ be interpreted  this is the essence of points _ 7 .",
    "( a  c ) _ in definition [ deficl ] .    logical entailment in an icl coincides with abstraction of conceptors :    [ propentailicl ] in an icl , for all @xmath1284 it holds that @xmath1463    the simple proof is given in section [ proofrentailment ] . in an agent s lifetime icl , for any sentence @xmath1464 the concrete interpretation @xmath1465^m$ ] can be effectively computed via the rules stated in nr .",
    "_ 7 _ in definition [ deficl ] , provided one has access to the identifiable conceptors @xmath1224 in the agent s life . since for two vectors @xmath1466^m$",
    "] it can be effectively checked whether @xmath1096 , it is decidable whether @xmath1467 .",
    "an icl is therefore decidable .    seen from a categorical point of view , an icl is a particularly small - size institution . since the ( only ) category in the image of @xmath1213 is a set , we can regard the functor @xmath1213 as having codomain @xmath1468 instead of @xmath1469 .",
    "also the category @xmath1200 is small , that is , a set .",
    "altogether , an icl institution nowhere needs proper classes .",
    "the icl definition i gave here is very elementary .",
    "it could easily be augmented in various natural ways , for instance by admitting permutations and/or projections of conceptor vector components as model morphisms in @xmath1263 , or allowing conceptors of different dimensions in the makeup of an agent life .",
    "likewise it is straightforward to spell out definitions for an agent life and its icl for matrix - based conceptors . in the latter case ,",
    "the referents of sentences are correlation matrices @xmath23 , and the defining equation for the model relationship appears as    @xmath1470    in traditional logics and their applications , an important role is played by _ calculi_. a calculus is a set of syntactic transformation rules operating on sentences ( and expressions containing free variables ) which allows one to derive purely syntactical proofs of logical entailment statements @xmath1471",
    ". fundamental properties of familiar logics , completeness and decidability in particular , are defined in terms of calculi .",
    "while it may be possible and mathematically interesting to design syntactical calculi for icls , they are not needed because @xmath1472 is decidable via the semantic equivalent @xmath1473 .",
    "furthermore , the icl decision procedure is computationally very effective : only time @xmath1474 is needed ( admitting a parallel execution ) to determine whether some conception vector is at most as large as another in all components .",
    "this may help to explain why humans can so quickly carry out many concept subsumption judgements ( `` this looks like a cow to me '' ) .",
    "abstracting from all technical detail , here is a summary account of the conceptor approach :    from neural dynamics to conceptors .",
    ": :    conceptors capture the shape of a neural state cloud by a positive    semi - definite operator . from conceptors to neural dynamics .",
    ": :    inserting a conceptor into a neurodynamical state update loop allows    to select and stabilize a previously stored neural activation pattern .",
    "matrix conceptors : :    are useful in machine learning applications and as mathematical tools    for analysing patterns emerging in nonlinear neural dynamics .",
    "random feature conceptors : :    are not biologically apriori implausible and can be neurally coded by    single neurons .",
    "autoconceptor : :    adaptation dynamics leads to content - addressable neural memories of    dynamical patterns and to signal filtering and classification systems .",
    "boolean operations and the abstraction ordering on conceptors : :    establish a bi - directional connection between logic and neural    dynamics . from static to dynamic models . : :    conceptors allow to understand and control dynamical patterns in    scenarios that previously have been mostly restricted to static    patterns .",
    "specifically this concerns content - addressable memories ,    morphing , ordering concepts in logically structured abstraction    hierarchies , and top - down hypothesis control in hierarchical    architectures .",
    "the study of conceptors is at an early stage .",
    "there are numerous natural directions for next steps in conceptor research :    affine conceptor maps . : :    conceptors constrain reservoir states by applying a positive    semi - definite map .",
    "this map is adapted to the `` soft - bounded '' linear    subspace visited by a reservoir when it is driven through a pattern .",
    "if the pattern - driven reservoir states do not have zero mean , it seems    natural to first subtract the mean before applying the conceptor .",
    "if    the mean is @xmath310 , this would result in an update loop of    the form @xmath1475 . while this    may be expected to improve control characteristics of pattern    re - generation , it is not immediately clear how boolean operations    transfer to such affine conceptors which are characterized by pairs    @xmath1476 .",
    "nonlinear conceptor filters . : :    even more generally , one might conceive of conceptors which take the    form of nonlinear filters , for instance instantiated as feedforward    neural networks . such filters @xmath123 could be trained on the    same objective function as i used for conceptors , namely minimizing    @xmath1477 +    \\alpha^{-2}\\|f\\|^2 $ ] , where @xmath1478 is a suitably chosen    norm defined for the filter . like with affine conceptor maps ,",
    "the    pattern specificity of such nonlinear conceptor filters would be    greater than for our standard matrix conceptors , but again it is not    clear how logical operations would extend to such filters .",
    "basic mathematical properties . : :    from a mathematics viewpoint , there are some elementary questions    about conceptors which should be better understood , for instance :    +    * what is the relationship between boolean operations , aperture    adaptation , and linear blending ?",
    "in particular , can the latter be    expressed in terms of the two former ?    * given a dimension @xmath8 , what is the minimial number of    `` basis '' conceptors such that the transitive closure under boolean    operations , aperture adaptation , and/or linear blending is the set of    all @xmath8-dimensional conceptors ?    * are there normal forms for expressions which compose conceptors by    boolean operations , aperture adaptation , and/or linear blending ?",
    "* find conceptor analogs of standard structure - building mathematical    operations , especially products .",
    "these would be needed to design    architectures with several conceptor modules ( likely of different    dimension ) where the `` soft subspace constraining '' of the overall    dynamics works on the total architecture state space .",
    "presumably this    leads to tensor variants of conceptors .",
    "this may turn out to be a    challenge because a mathematical theory of `` semi positive - definite    tensors '' seems to be only in its infancy ( compare @xcite ) .",
    "neural realization of boolean operations .",
    ": :    how can boolean operations be implemented in biologically not    impossible neural circuits ?",
    "complete analysis of autoconceptor adaptation .",
    ": :    the analysis of autoconceptor adaptation in section [ subseccad ] is    preliminary and incomplete .",
    "it only characterizes certain aspects of    fixed - point solutions of this adaptation dynamics but remains ignorant    about the effects that the combined , nonlinear conceptor - reservoir    update dynamics may have when such a fixed - point solution is    perturbed .",
    "specifically , this reservoir - conceptor interaction will    have to be taken into account in order to understand the dynamics in    the center manifold of fixed - point solutions .",
    "applications . : :    the usefulness of conceptors as a practical tool for machine learning    and as a modeling tool in the cognitive and computational    neurosciences will only be established by a suite of successful    applications .",
    "in this section i provide details of all simulation experiments reported in sections [ secoverview ] and [ sectheorydemos ] .",
    "a reservoir with @xmath29 neurons , plus one input unit and one output unit was created with a random input weight vector @xmath15 , a random bias @xmath166 and preliminary reservoir weights @xmath12 , to be run according to the update equations    @xmath1479    initially @xmath212 was left undefined .",
    "the input weights were sampled from a normal distribution @xmath1480 and then rescaled by a factor of 1.5 .",
    "the bias was likewise sampled from @xmath1480 and then rescaled by 0.2 .",
    "the reservoir weight matrix @xmath12 was first created as a sparse random matrix with an approximate density of 10% , then scaled to obtain a spectral radius ( largest absolute eigenvalue ) of 1.5 .",
    "these scalings are typical in the field of reservoir computing @xcite for networks to be employed in signal - generation tasks .    for each of the four driving signals @xmath59 a training time series of length 1500",
    "was generated .",
    "the reservoir was driven with these @xmath150 in turn , starting each run from a zero initial reservoir state .",
    "this resulted in reservoir state responses @xmath48 .",
    "the first 500 steps were discarded in order to exclude data influenced by the arbitrary starting condition , leading to four 100-dimensional reservoir state time series of length @xmath226 , which were recorded into four @xmath1481 state collection matrices @xmath228 , where @xmath1482 ( @xmath1483 ) .",
    "likewise , the corresponding driver signals were recorded into four pattern collection ( row ) vectors @xmath590 of size @xmath1484 .",
    "in addition to this , a version @xmath1485 of @xmath228 was built , identical to @xmath228 except that it was delayed by one step : @xmath1486 .",
    "these collections were then concatenated to obtain @xmath1487 , \\tilde{x } = [ \\tilde{x}^1 | \\tilde{x}^2 | \\tilde{x}^3 | \\tilde{x}^4 ] ,",
    "p = [ p^1 | p^2 | p^3 | p^4]$ ] .",
    "the `` pc energy '' plots in figure [ fig1 ] render the singular values of the correlation matrices @xmath1488 .",
    "the output weights @xmath212 were computed as the regularized wiener - hopf solution ( also known as ridge regression , or tychonov regularization )    @xmath1489    where the regularizer @xmath1490 was set to 0.01 .",
    "loading : after loading , the reservoir weights @xmath50 should lead to the approximate equality @xmath1491 , across all patterns @xmath52 , which leads to the objective of minimizing the squared error @xmath1492 , averaged over all four @xmath52 and training time points . writing @xmath87 for the @xmath1493 matrix whose columns are all identical equal to @xmath166 ,",
    "this has the ridge regression solution    @xmath1494    where the regularizer @xmath1495 was set to 0.0001 . to assess the accuracy of the weight computations , the training normalized root mean square error ( nrmse )",
    "was computed . for the readout weights ,",
    "the nrmse between @xmath1496 and the target @xmath215 was 0.00068 .",
    "for the reservoir weights , the average ( over reservoir neurons @xmath171 , times @xmath10 and patterns @xmath52 ) nrmse between @xmath1497 and the target @xmath1498 was 0.0011 .    in order to determine the accuracy of fit between the original driving signals @xmath59 and the network observation outputs @xmath1499 in the conceptor - constrained autonomous runs , the driver signals and the @xmath1077 signals were first interpolated with cubic splines ( oversampling by a factor of 20 ) .",
    "then a segment length 400 of the oversampled driver ( corresponding to 20 timesteps before interpolation ) was shifted over the oversampled @xmath1077 in search of a position of best fit .",
    "this is necessary to compensate for the indeterminate phaseshift between the driver data and the network outputs .",
    "the nrmses given in figure [ fig1 ] were calculated from the best - fit phaseshift position , and the optimally phase - shifted version of @xmath1077 was also used for the plot .      _ data generation .",
    "_ for the rssler attractor , training time series were obtained from running simple euler approximations of the following odes :    @xmath1500    using parameters @xmath1501 .",
    "the evolution of this system was euler approximated with stepsize @xmath1502 and the resulting discrete time series was then subsampled by 150 .",
    "the @xmath4 and @xmath104 coordinates were assembled in a 2-dimensional driving sequence , where each of the two channels was shifted / scaled to a range of @xmath240 $ ] . for the lorenz attractor ,",
    "the ode    @xmath1503    with @xmath1504 was euler - approximated with stepsize @xmath1502 and subsequent subsampling by 15 .",
    "the @xmath4 and @xmath127 coordinates were collected in a 2-dimensional driving sequence , again each channel normalized to a range of @xmath240 $ ] .",
    "the mackey glass timeseries was obtained from the delay differential equation    @xmath1505    with @xmath1506 , a customary setting when this attractor is used in neural network demonstrations .",
    "an euler approximation with stepsize @xmath1507 was used . to obtain a 2-dim timeseries that could be fed to the reservoir through the same two input channels as the other attractor data , pairs @xmath1508",
    "were combined into 2-dim vectors .",
    "again , these two signals were normalized to the @xmath240 $ ] range .",
    "the hnon attractor is governed by the iterated map    @xmath1509    where i used @xmath1510 .",
    "the two components were filed into a 2-dim timeseries @xmath1511 with no further subsampling , and again normalization to a range of @xmath240 $ ] in each component .",
    "_ reservoir setup . _",
    "a 500-unit reservoir rnn was created with a normal distributed , 10%-sparse weight matrix @xmath12 scaled to a spectral radius of @xmath1512 .",
    "the bias vector @xmath166 and input weights @xmath15 ( sized @xmath1513 for two input channels ) were sampled from standard normal distribution and then scaled by @xmath1514 and @xmath1515 , respectively .",
    "these scaling parameters were found by a ( very coarse ) manual optimization of the performance of the pattern storing process .",
    "the network size was chosen large enough to warrant a robust trainability of the four chaotic patterns . repeated executions of the experiment with",
    "different randomly initialized weights ( not documented ) showed no significant differences .    _ pattern storing . _",
    "the @xmath12 reservoir was driven , in turn , by 2500 timesteps of each of the four chaotic timeseries .",
    "the first 500 steps were discarded to account for initial reservoir state washout , and the remaining 4 @xmath1516 2000 reservoir states were collected in a @xmath1517 matrix @xmath282 . from this , the new reservoir weights @xmath50 were computed as in ( [ someq04 ] ) , with a regularizer @xmath1518 .",
    "the readout weights were computed as in ( [ someq03 ] ) with regularizer @xmath1519 .",
    "the average nrmses obtained for the reservoir and readout weights were @xmath1520 and @xmath1521 , respectively .",
    "_ computing conceptors .",
    "_ from each of the four @xmath1522 step reservoir state sequences @xmath282 recorded in the storing procedure , obtained from driving the reservoir with one of the four chaotic signals , a preliminary correlation matrix @xmath1523 and its svd @xmath1524 were computed .",
    "this correlation matrix was then used to obtain a conceptor associated with the respective chaotic signal , using an aperture @xmath370 . from these unit - aperture conceptors ,",
    "versions with differing @xmath43 were obtained through aperture adaptation per ( [ eqsemalpha3 ] ) .    in passing i note that the overall stability and parameter robustness of this simulation can be much improved if small singular values in @xmath1525 ( for instance , with values smaller than 1e-06 ) are zeroed , obtaining a clipped @xmath176 , from which a `` cleaned - up '' correlation matrix @xmath1526 would be computed .",
    "this would lead to a range of well - working apertures spanning three orders of magnitude ( not shown ) .",
    "i did not do this in the reported simulation in order to illustrate the effects of too large apertures ; these effects would be partly suppressed when the spectrum of @xmath23 is clipped .",
    "_ pattern retrieval and plotting .",
    "_ the loaded network was run using the conceptor - constrained update rule @xmath372 with various @xmath1527 ( @xmath1528 ) for 800 steps each time , of which the first 100 were discarded to account for initial state washout .",
    "the delay embedding plots in figure [ figchaosclover ] were generated from the remaining 700 steps .",
    "embedding delays of 2 , 2 , 3 , 1 respectively were used for plotting the four attractors .    for each of the four 6-panel blocks in figure [ figchaosclover ] , the five aperture adaptation factors @xmath1529 were determined in the following way .",
    "first , by visual inspection , the middle @xmath1530 was determined to fall in the trough center of the attenuation plot of fig",
    ".  [ figblindout ] * a*. then the remaining @xmath1531 were set in a way that ( i ) the entire @xmath348 sequence was a geometrical progression , and ( ii ) that the plot obtained from the first @xmath1532 was visually strongly corrupted",
    ".      for both reported simulation studies , the same basic reservoir was used : size @xmath29 , @xmath12 sparse with approximately 10% nonzero entries , these sampled from a standard normal distribution and later rescaled to obtain a spectral radius of 1.5 .",
    "the input weights @xmath15 were non - sparse , sampled from a standard normal distribution and then scaled by a factor of 1.5 . the bias vector @xmath166 was sampled from a standard normal distribution and scaled by a factor of 0.25 .",
    "the bias serves the important function to break the sign symmetry of the reservoir dynamics . without",
    "it , in autonomous pattern generation runs a pattern @xmath59 or its sign - reversed version @xmath1533 could be generated equally well , which is undesirable .",
    "the driver patterns used in the first demonstration were either sinewaves with integer period lengths , or random periodic patterns scaled to a range of @xmath1534 $ ] .",
    "period lengths were picked randomly between 3 and 15 .",
    "the drivers for the second demonstrations were the sinewaves @xmath1535 ( where @xmath1536 ) , leading to periods exponentially spread over the range @xmath1537 $ ] .",
    "these signals were randomly permuted to yield the presentation order @xmath1538 .",
    "the lengths of pattern signals used for training were @xmath1539 in demo 1 and @xmath226 in demo 2 , with additional washouts of the same lengths .",
    "the readout weights @xmath212 were trained by the generic procedure detailed in section [ secgeneralsetupexpdetail ] after all patterns had been used to drive the network in turn for the incremental storage , and thus having available all state collections @xmath228 .    for testing ,",
    "the conceptors @xmath57 obtained during the storage procedure were used by running the reservoir via @xmath1540 from random initial states . after a washout time",
    ", the network outputs @xmath1083 were recorded for durations of 200 and 400 , respectively , in the two experiments .",
    "a 20-step portion of the original driver pattern was interpolated by a factor of 10 and shifted over a likewise interpolation - refined version of these outputs .",
    "the best matching shift position was used for plotting and nrmse computation ( such shift - search for a good fit is necessary because the autonomous runs are not phase - synchronized to the original drivers , and irrational - period sinewaves need interpolation because they may be phase - shifted by noninteger amounts ) .      _",
    "network setup .",
    "_ reservoir network matrices @xmath50 were sampled sparsely ( 10% nonzero weights ) from a normal distribution , then scaled to a spectral radius of 1.5 in all experiments of this section .",
    "reservoir size was @xmath29 for all period-4 experiments and the unrelated patterns experiment , and @xmath1541 for all experiments that used mixtures - of - sines patterns .",
    "for all experiments in the section , input weights @xmath15 were randomly sampled from the standard normal distribution and rescaled by a factor of 1.5 .",
    "the bias vector @xmath166 was likewise sampled from the standard normal distribution and rescaled by 0.5 .",
    "these scaling parameters had been determined by a coarse manual search for a well - working configuration when this suite experiments was set up .",
    "the experiments are remarkably insensitive to these parameters .",
    "_ storing patterns . _",
    "the storage procedure was set up identically for all experiments in this section .",
    "the reservoir was driven by the @xmath118 loading patterns in turn for @xmath1542 steps ( period-4 and unrelated patterns ) or @xmath1543 steps ( mix - of - sines ) time steps , plus a preceding 100 step initial washout .",
    "the observed network states @xmath11 were concatenated into a @xmath1544 sized state collection matrix @xmath282 , and the one - step earlier states @xmath1545 into a matrix @xmath1546 of same size .",
    "the driver pattern signals were concatenated into a @xmath1547 sized row vector @xmath215 .",
    "the readout weights @xmath212 were then obtained by ridge regression via @xmath1548 , and @xmath569 by @xmath1549 .    _ quality measurements .",
    "_ after the cueing , and at the end of the recall period ( or at the ends of the three interim intervals for some of the experiments ) , the current conceptor @xmath25 was tested for retrieval accuracy as follows . starting from the current network state @xmath11 ,",
    "the reservoir network was run for 550 steps , constrained by @xmath25 .",
    "the first 50 steps served as washout and were discarded .",
    "the states @xmath4 from the last 500 steps were transformed to patterns by applying @xmath212 , yielding a 500-step pattern reconstruction .",
    "this was interpolated with cubic splines and then sampled at double resolution , leading to a 999-step pattern reconstruction @xmath1550 .",
    "a 20-step template sample of the original pattern was similarly interpolated - resampled and then passed over @xmath1550 , detecting the best - fitting position where the the nrmse between the target template and @xmath1550 was minimal ( this shift - search accomodated for unknown phase shifts between the target template and @xmath1550 ) .",
    "this minimal nrmse was returned as measurement result .",
    "_ irrational - period sines .",
    "_ this simulation was done exactly as the one before , using twelve irrational - period sines as reference patterns .",
    "_ network setup .",
    "_ in each of the 50 trials , the weights in a fully connected , @xmath673 reservoir weight matrix , a 12-dimensional input weight vector , a 10-dimensional bias vector , and a 10-dimensional start state were first sampled from a normal distribution , then rescaled to a spectral radius of 1.2 for @xmath50 , and by factors of 0.2 , 1 , 1 for @xmath1551 respectively .",
    "_ numerical determination of best aperture . _ to determine @xmath1552 , the quantities @xmath1553 were computed for @xmath1554 .",
    "these values were interpolated on a 0.01 raster with cubic splines , the support point @xmath1555 of the maximum of the interpolation curve was detected , returning @xmath1556 .",
    "_ linear classifier training .",
    "_ the linear classifier that serves as a baseline comparison was designed in essentially the same way as the echo state networks based classifiers which in @xcite yielded zero test misclassifications ( when combining 1,000 such classifiers made from 4-unit networks ) and 2 test misclassifications ( when a single such classifier was based on a 1,000 unit reservoir ) , respectively .",
    "thus , linear classifiers based on reservoir responses outperform all other reported methods on this benchmark and therefore provide a substantive baseline information .    in detail , the linear classifier was learnt from 270 training data pairs of the form @xmath1557 , where the @xmath127 were the same 88-dimensional vectors used for constructing conceptors , and the @xmath1558 were 9-dimensional , binary speaker indicator vectors with a `` 1 '' in the position of the speaker of @xmath127 .",
    "the classifier consists in a @xmath1559 sized weight matrix @xmath362 , and the cost function was the quadratic error @xmath1560 .",
    "the classifier weight matrix @xmath362 which minimized this cost function on average over all training samples was computed by linear regression with tychonov regularization , also known as ridge regression @xcite . the tychonov parameter which determines the degree of regularization",
    "was determined by a grid search over a 5-fold cross - validation on the training data . across the 50 trials",
    "it was found to vary quite widely in a range between 0.0001 and 0.25 ; in a separate auxiliary investigation it was also found that the effect of variation of the regularizer within this range was very small and the training of the linear classifier can therefore be considered robust .    in testing , @xmath362 was used to determine classification decisions by computing @xmath1561 and opting for the index of the largest entry in @xmath1562 as the speaker .",
    "_ network setup .",
    "_ in both experiments reported in this section , the same reservoir made from @xmath29 units was used . @xmath123 and @xmath126 were full matrices with entries first sampled from the standard normal distribution .",
    "then they were both scaled by an identical factor @xmath165 such that the product @xmath1563 attained a spectral radius of 1.4 .",
    "the input weight vector @xmath15 was sampled from the standard normal distribution and then scaled by 1.2 .",
    "the bias @xmath166 was likewise sampled from the normal distribution and then scaled by 0.2 .",
    "these values were determined by coarse manual search , where the main guiding criterion was recall accuracy .",
    "the settings were rather robust in the first experiment which used stored @xmath1049 .",
    "the spectral radius could be _ individually _ varied from 0.6 to 1.45 , the input weight scaling from 0.3 to 1.5 , the bias scaling from 0.1 to 0.4 , and the aperture from 3 to 8.5 , while always keeping the final recall nrmses for all four patterns below 0.1 .",
    "furthermore , much larger _ combined _ variations of these scalings were also possible ( not documented ) .    in the second experiment with content - addressed recall ,",
    "the functional parameter range was much narrower .",
    "individual parameter variation beyond @xmath9005% was disruptive .",
    "specifically , i observed a close inverse coupling between spectral radius and aperture : if one of the two was raised , the other had to be lowered",
    ".    _ loading procedure . _",
    "the loading procedure is described in some detail in the report text .",
    "the mean nrmses on training data was 0.00081 for recomputing @xmath126 , 0.0011 for @xmath569 , and 0.0029 for @xmath212 .",
    "the mean absolute size of matrix elements in @xmath126 was 0.021 , about a third of the mean absolute size of elements of @xmath1044 .",
    "_ computing nrmses . _",
    "the nrmse comparison between the re - generated patterns at the end of the @xmath995 adaptation and the original drivers was done in the same way as reported on earlier occasions ( section [ seccontentaddressableexperiment ] ) , that is , invoking spline interpolation of the comparison patterns and optimal phase - alignment .",
    "_ module setup .",
    "_ the three modules are identical copies of each other .",
    "the reservoir had @xmath29 units and the feature space had a dimension of @xmath1065 .",
    "the input weight matrix @xmath1564 was sampled from the standard normal distribution and rescaled by a factor of 1.2 . the reservoir - featurespace projection and backprojection matrices @xmath1565 ( sized @xmath984 )",
    "were first sampled from the standard normal distribution , then linearly rescaled by a common factor such that the @xmath13 matrix @xmath1566 ( which functionally corresponds to an internal reservoir weight matrix ) had a spectral radius of 1.4 .",
    "the bias @xmath166 was likewise sampled from the standard normal distribution and then scaled by 0.2 .",
    "a regularization procedure was then applied to @xmath1044 to give @xmath126 as follows .",
    "the preliminary module was driven per @xmath1567 with an i.i.d .",
    "input signal @xmath1568 sampled uniformly from @xmath1569 $ ] , for 1600 steps ( after discarding an initial washout ) . the values obtained for @xmath1570 were collected as columns in a @xmath1571 matrix @xmath686 .",
    "the final @xmath126 was then computed by a ridge regression with a regularizer @xmath1572 by @xmath1573 in words , @xmath126 should behave as the initially sampled @xmath1044 in a randomly driven module , but do so with minimized weight sizes .",
    "this regularization was found to be important for a stable working of the final architecture .",
    "_ training .",
    "_ the input simulation weights @xmath569 ( size @xmath1574 ) and @xmath212 ( size @xmath210 ) were trained by driving a single module with the four target patterns , as follows .",
    "the module was driven with clean @xmath137 in turn , with auto - adaptation of conception weights @xmath995 activated : @xmath1575 where the @xmath995 adaptation rate was set to @xmath1576 , and an aperture @xmath1070 was used . after discarding initial washouts in each of the four driving conditions ( long enough for @xmath994 to stabilize ) , 400 reservoir state vectors @xmath1577 , 400 @xmath127 vectors @xmath376 and 400 input values @xmath150 were collected for @xmath1578 , and collected column - wise in matrices @xmath23 ( size @xmath1579 ) , @xmath686 ( size @xmath1571 ) and @xmath738 ( size @xmath1580 ) , respectively .",
    "in addition , the 400-step submatrices @xmath1581 of @xmath686 containing the @xmath127-responses of the module when driven with @xmath59 were registered separately .",
    "the output weights were then computed by ridge regression on the objective to recover @xmath150 from @xmath1577 by @xmath1582 using a regularizer @xmath1572 . in a similar way",
    ", the input simulation weights were obtained as @xmath1583 with a regularizer @xmath1572 again .",
    "the training nrmses for @xmath212 and @xmath569 were 0.0018 and 0.0042 , respectively .",
    "the @xmath1168 prototype matrix @xmath215 was computed as follows .",
    "first , a preliminary version @xmath1584 was constructed whose @xmath52-the column vector was the mean of the element - wise squared column vectors in @xmath1581 .",
    "the four column vectors of @xmath1584 were then normalized such that the norm of each of them was the mean of the norms of columns in @xmath1584 .",
    "this gave @xmath215 .",
    "this normalization is important for the performance of the architecture , because without it the optimization criterion ( [ eqgammaloss ] ) would systematically lead to smaller values for those @xmath1131 that are associated with smaller - norm columns in @xmath215 .",
    "_ baseline linear filter .",
    "_ the transversal filter that served as a baseline was a row vector @xmath1585 of size 2600 .",
    "it was computed to minimize the loss function @xmath1586 + a^2 \\|w\\|^2,\\ ] ] where @xmath150 were clean versions of the four patterns .",
    "400 timesteps per pattern were used for training , and @xmath165 was set to 1.0 .",
    "the setting of @xmath165 was very robust .",
    "changing @xmath165 in either direction by factors of 100 changed the resulting test nrmses at levels below the plotting accuracy in figure [ figcarch ] .    _ parameter settings in testing .",
    "_ the adaptation rate @xmath1177 was set to 0.002 for the classification simulation and to 0.004 for the morph - tracking case study .",
    "the other global control parameters were identical in both simulations : trust smoothing rate @xmath1587 , decisiveness @xmath1588 } = d_{[23 ] } = 8 $ ] , drift @xmath1589 , @xmath1150}$ ] adaptation rate @xmath1590 ( compare equation ( [ eqcadapt ] ) ; i used the same adaptation rate @xmath1591 for all of the 500 feature units ) .    _ computing and plotting running nrmse estimates .",
    "_ for the nrmse plots in the fifth rows in figures [ figcarch ] and [ figcarchmorph ] , a running estimate of the nrmse between the module outputs @xmath151}$ ] and the clean input patterns @xmath37 ( unknown to the system ) was computed as follows .",
    "a running estimate @xmath1592 of the variance of the clean pattern was maintained like it was done for @xmath1593}(n)$ ] in ( [ equpdatediscrepancy1 ] ) and ( [ equpdatediscrepancy2 ] ) , using an exponential smoothing rate of @xmath1594 .",
    "then the running nrmse was computed by another exponential smoothing per @xmath1595}(n+1 ) = \\sigma\\,\\overline{\\mbox{nrmse}}\\,y_{[l]}(n ) + ( 1-\\sigma)\\ , \\left(\\frac{(p(n+1 ) -      y_{[l]}(n+1))^{2}}{\\overline{\\mbox{var}}\\,p(n+1)}\\right)^{1/2}.\\ ] ] the running nrmse for the baseline transversal filter were obtained in a similar fashion .",
    "_ claim 1_. we first re - write the minimization quantity , using @xmath1596 $ ] : @xmath1597 + \\alpha^{-2}\\,\\|c\\|^2_{\\mbox{\\scriptsize { fro } } } = } \\\\ & = & e[\\mbox{tr}\\ , ( x'(i - c')(i - c ) x ) ] + \\alpha^{-2}\\,\\|c\\|^2_{\\mbox{\\scriptsize { fro}}}\\\\ & = & \\mbox{tr}\\ , ( ( i - c')(i - c)r + \\alpha^{-2}\\,c'c)\\\\   & = & \\mbox{tr}\\ , ( r - c'r - cr + c'c(r + \\alpha^{-2}\\,i ) ) \\\\ & = & \\sum_{i=1,\\ldots , n } e'_i ( r - c'r - cr + c'c(r + \\alpha^{-2}\\,i ) ) e_i.\\end{aligned}\\ ] ] this quantity is quadratic in the parameters of @xmath25 and non - negative . because @xmath1598 and @xmath1599 is positive definite",
    ", @xmath1600 is positive definite in the @xmath301-dimensional space of @xmath25 elements . therefore @xmath300",
    "+ \\alpha^{-2}\\,\\|c\\|^2_{\\mbox{\\scriptsize { fro}}}$ ] has a unique minimum in @xmath25 space . to locate it we compute the derivative of the @xmath171-th component of this sum with respect to the entry @xmath1601 : @xmath1602 where we used the abbreviation @xmath1603 , observing in the last line that @xmath1604 .",
    "summing over @xmath171 yields @xmath1605 + \\alpha^{-2}\\,\\|c\\|^2_{\\mbox{\\scriptsize \\emph{fro } } } = -2\\,n\\,r_{kl } + 2n\\,(ca)_{kl},\\ ] ] which in matrix form is @xmath1605 + \\alpha^{-2}\\,\\|c\\|^2_{\\mbox{\\scriptsize \\emph{fro } } } = -2\\,n\\ , r + 2n\\,c ( r + \\alpha^{-2}\\,i),\\ ] ] where we re - inserted the expression for @xmath169 . setting this to zero yields the claim _ 1 . _",
    "stated in the proposition .",
    "@xmath1608 can be written as @xmath1609 , where the diagonal of @xmath1610 is @xmath1611 .",
    "putting @xmath1612 and @xmath1613 , and similarly @xmath1614 @xmath1615 , we can express the limit @xmath1616 equivalently as @xmath1617 .",
    "note that @xmath1618 is invertible for sufficiently large @xmath1106 .",
    "let @xmath1619 be the @xmath1620 submatrix of @xmath414 made from the last @xmath1621 columns of @xmath414 ( spanning the null space of @xmath25 ) , and let @xmath1622 be @xmath1623 submatrix of @xmath362 made from the last @xmath631 columns of @xmath362 .",
    "the @xmath13 matrix @xmath1624 is positive semidefinite .",
    "let @xmath1625 be its svd , with singular values in @xmath803 in descending order .",
    "noting that @xmath1626 , and @xmath1627 , we can rewrite                    the rows of the @xmath1642 sized matrix @xmath1643 are orthonormal , which also follows from ( [ eqproof1 - 2 ] ) . the cauchy interlacing theorem ( stated in section [ subseccad ] in another context )",
    "then implies that all singular values of @xmath1644 are greater or equal to @xmath1645 .",
    "since all @xmath24 are smaller or equal to @xmath18 , all singular values of @xmath1644 are greater or equal to 1 .",
    "the singular values of the matrix @xmath1646 are therefore greater or equal to @xmath898 .",
    "specifically , @xmath1646 is positive definite . by a similar argument",
    ", @xmath1647 is positive definite .",
    "the matrix @xmath169 from claim 1 is therefore revealed as the sum of two positive definite matrices , and hence is invertible .            where @xmath686 is assumed to be invertible and @xmath1653 is assumed to be invertible .",
    "block - structuring @xmath1654 analogously to this representation , where @xmath1655 is identified with @xmath686 , then easily leads to the claim ( [ eqproof1 - 4 ] ) .    applying claims 1 and 2 to the limit expression @xmath1656 in ( [ eqtrans ] ) , where the matrix @xmath64 in claim 2 is identified with @xmath1657 and the matrix @xmath686 from claim 2",
    "is identified with the matrix @xmath1658 from claim 1 , yields @xmath1659 which , combined with ( [ eqtrans ] ) , leads to @xmath1660 @xmath1631 is an @xmath1661 size matrix whose columns are orthonormal . its range is @xmath1662 that is , @xmath1631 is a matrix whose columns form an orthonormal basis of @xmath1663 .",
    "let @xmath444 be any matrix whose columns form an orthonormal basis of @xmath1664 .",
    "then there exists a unique orthonormal matrix @xmath429 of size @xmath1665 such that @xmath1666 .",
    "it holds that @xmath1667 which gives the final form of the claim in the proposition .    for showing equivalence of ( [ eqcompandgencomp_1 ] ) with ( [ eqcompandinproof ] )",
    ", i exploit a fact known in matrix theory ( @xcite , fact 6.4.16 ) : for two real matrices @xmath1668 of sizes @xmath1669 the following two condition are equivalent : ( i ) @xmath1670 , and ( ii ) @xmath1671 and @xmath1672 . observing that @xmath1673 and that @xmath1674 , setting @xmath1675 and @xmath1676 in ( [ eqcompandgencomp_1 ] ) yields @xmath1677 where condition ( ii ) from the abovementioned fact is easily verified . in a second , entirely analog",
    "step one can pull apart @xmath1678 into @xmath1679      1 .   compute the svds @xmath1681 and @xmath1682 @xmath1683 .",
    "2 .   let @xmath1619 be the submatrix of @xmath414 made from the last @xmath1621 columns in @xmath414 , and similarly let @xmath1622 consist of the last @xmath631 columns in @xmath362 .",
    "3 .   compute the svd @xmath1684 , where + @xmath1685 .",
    "4 .   let @xmath1631 be the submatrix of @xmath50 consisting of the last @xmath840 columns of @xmath50",
    ". then @xmath1686 .        by definition [ def : finalboolean ] , proposition [ propanddef ] , and equation ( [ eq : compconceptor ] ) , @xmath1687 it is easy to check that @xmath1688 holds for any positive semidefinite matrix @xmath169 .",
    "therefore , @xmath1689 furthermore , for positive semidefinite @xmath536 it generally holds that @xmath1690 , and hence @xmath1691              _ claim 5a : _ @xmath1701 .",
    "we have , by definition , @xmath1696 for shorter notation put @xmath1702 and @xmath1703 .",
    "note ( from proof of proposition [ propanddef ] ) that @xmath282 is invertible .",
    "we characterize the unit eigenvectors of @xmath450 .",
    "it holds that @xmath1704 if and only if @xmath1705 .",
    "we need to show that the conjunction @xmath1706 and @xmath1707 is equivalent to @xmath1708 .",
    "case 2 : @xmath1720 .",
    "we first show an auxiliary claim : @xmath1721 .",
    "let @xmath1722 . @xmath923 and @xmath1723 are positive semidefinite because the nonzero singular values of @xmath1724 are greater or equal to 1 .",
    "furthermore , @xmath1725 .",
    "thus , @xmath1726 , i.e.  @xmath1727 . from not @xmath1706 or",
    "not @xmath1728 it follows that @xmath1729 or @xmath1730 .",
    "we infer                    let @xmath1741 be the svd of @xmath169 , and assume @xmath169 has rank @xmath1742 , that is , exactly the first @xmath118 singular values in @xmath176 are nonzero .",
    "let @xmath867 be the @xmath868 matrix consisting of the first @xmath118 columns of @xmath414 .",
    "it holds that @xmath1743 .",
    "we obtain      where @xmath1745 is the @xmath868 matrix consisting of the first @xmath118 columns of @xmath36 .",
    "let @xmath865 be the @xmath866 upper left submatrix of @xmath176",
    ". then @xmath1746 and @xmath1747 hence @xmath1748 or equivalently , @xmath1749 .",
    "this implies @xmath1750 .",
    "_ claim 1 : _ @xmath1759 .",
    "notation : all of the matrices @xmath1760 @xmath1761 are positive semidefinite and have svds with identical principal component matrix @xmath414 . for any matrix @xmath282 among these ,",
    "let @xmath1762 be its svd .",
    "we write @xmath1763 for the @xmath171th singular value in @xmath1764 .",
    "we have to show that @xmath1765 . in the derivations below",
    ", we use various facts from proposition [ propspaces ] and equation ( [ eqpropadapt ] ) without explicit reference .",
    "_ case _ @xmath1775 : using concepts and notation from proposition [ proplimitor ] , it is easy to check that any conceptor @xmath169 can be written as @xmath1776 and its aperture adapted versions as @xmath1777 using proposition [ proplimitor ] and ( [ eqproof4 - 1 ] ) we thus have @xmath1778 furthermore , again by proposition [ proplimitor ] and by ( [ eqproof4 - 2 ] ) , @xmath1779 and @xmath1780 using ( [ eqsemalpha0c1 ] ) , it follows for any conceptor @xmath169 that @xmath1781 applying this to ( [ eqproof4 - 5 ] ) and observing that @xmath1782 yields @xmath1783 we now exploit the following auxiliary fact which can be checked by elementary means : if @xmath1784 is a @xmath436-indexed family of positive semidefinite matrices whose eigenvectors are identical for different @xmath436 , and similarly the members of the familiy @xmath1785 have identical eigenvectors , and if the limits @xmath1786 , @xmath1787 exist and are equal , then the limits @xmath1788 , @xmath1789 exist and are equal , too . putting @xmath1790 and @xmath1791 , combining ( [ eqproof4 - 3 ] )",
    ", ( [ eqproof4 - 4 ] ) , ( [ eqproof4 - 6 ] ) and ( [ eqproof4 - 8 ] ) with this auxiliary fact yields @xmath1792 .",
    "case @xmath1796 : using proposition [ propspaces ] and equations ( [ eqproof4 - 7 ] ) , ( [ eqproof4 - 3 ] ) , ( [ eqproof4 - 2 ] ) , we obtain @xmath1797 where in step ( * ) we exploit the fact that the singular values of @xmath1798 corresponding to eigenvectors whose eigenvalues in @xmath25 are less than unity are identical to the singular values of @xmath1799 at the analog positions .",
    "case @xmath1800 : using proposition [ proplimitor ] , facts from proposition [ propspaces ] , and equation ( [ eqproof4 - 1 ] ) , we obtain @xmath1801 where step ( * ) is obtained by observing @xmath1802 and applying the definition of @xmath1803 given in the statement of proposition [ proplimitor ] .",
    "de morgan s rules : _ by definition [ def : finalboolean ] and proposition [ propdemorganand ] .",
    "associativity : _ from equations ( [ eqproof4 - 3 ] ) and ( [ eqproof4 - 4 ] ) it follows that for any conceptors @xmath1809 it holds that @xmath1810 . employing this fact and using proposition [ proplimitor ] yields associativity of or . applying de morgan s law then transfers associativity to and .",
    "commutativity and 4 .",
    "double negation _ are clear .",
    "neutrality : _ neutrality of @xmath36 : observing that @xmath1811 and @xmath1812 , starting from the definition of @xmath77 we obtain @xmath1813 neutrality of @xmath513 can be obtained from neutrality of @xmath36 via de morgan s rules .",
    "globality : _ @xmath525 follows immediately from the definition of @xmath81 given in [ def : finalboolean ] , observing that @xmath1814 .",
    "the dual @xmath1815 is obtained by applying de morgan s rule on @xmath1816 .",
    "let @xmath541 denote the well - known _ lwner _ ordering on the set of real @xmath13 matrices defined by @xmath547 if @xmath1817 is positive semidefinite .",
    "note that a matrix @xmath25 is a conceptor matrix if and only if @xmath1818 .",
    "i first show the following      _ proof of lemma .",
    "_ @xmath177 and @xmath1820 can be written as @xmath1821 from @xmath547 it follows that @xmath1822 , that is , @xmath1823 , which in turn yields @xmath1824 @xmath547 entails @xmath1825 , which is equivalent to @xmath1826 ( see @xcite , fact 8.21.11 ) , which implies @xmath1827 see proposition 8.1.2 ( xii ) in @xcite . taking the limits ( [ eqlemmal1 ] ) and ( [ eqlemmal2 ] ) leads to the claim of the lemma ( see fact 8.10.1 in @xcite ) .",
    "_ proof of claim 1 .",
    "_ let @xmath536 be conceptor matrices of size @xmath1016 . according to proposition [ proploewnerboolean ] ( which is proven independently of the results stated in proposition [ proppseudoabsorbtion ] )",
    ", it holds that @xmath1828 , which combined with the lemma above establishes @xmath1829 from which it follows that @xmath1830 is positive semidefinite with all singular values greater or equal to one .",
    "therefore , @xmath1831 is positive semidefinite with all nonzero singular values greater or equal to one .",
    "hence @xmath1832 is a conceptor matrix .",
    "it is furthermore obvious that @xmath1833 .    from @xmath1828",
    "it follows that @xmath1834 , which together with @xmath1833 leads to @xmath1835 . exploiting this fact , starting from the definition of and in def .",
    "[ def : finalboolean ] , we conclude @xmath1836    _ proof of claim 2 .",
    "_ this claim is the boolean dual to claim 1 and can be straightforwardly derived by transformation from claim 1 , using de morgan s rules and observing that @xmath1837 ( see prop .  [ propspaces ] item 3 ) , and that @xmath1838 .",
    "_ claim 1 : _",
    "@xmath1839 is equivalent to @xmath169 being positive semidefinite , and for a positive semidefinite matrix @xmath169 , the condition @xmath1840 is equivalent to all singular values of @xmath169 being at most one .",
    "both together yield the claim .",
    "_ proof of lemma .",
    "_ let @xmath1845 be the projection space of @xmath1843 .",
    "it is clear that @xmath1846 and @xmath1847 , hence @xmath1848 is a bijection on @xmath414 .",
    "also @xmath1843 is a bijection on @xmath414 .",
    "we now call upon the following well - known property of the pseudoinverse ( see @xcite , fact 6.4.16 ) :        now let @xmath549 and @xmath550 .",
    "by lemma [ lemmaloewner1 ] , @xmath1854 is positive semidefinite , hence @xmath1855 is positive definite with singular values greater or equal to one , hence invertible .",
    "the singular values of @xmath1856 are thus at most one , hence @xmath1857 is a conceptor matrix .",
    "obviously @xmath1858 .",
    "_ claim 5 : _ this claim is the boolean dual to the previous claim .",
    "it follows by a straightforward transformation applying de morgan s rules and observing that @xmath1860 ( see prop .  [ propspaces ] item 3 ) , and that @xmath1838 .",
    "_ claim 6 : _ let @xmath1861 . using the notation and claim from prop .",
    "[ propanddef ] rewrite @xmath1862 . similarly ,",
    "obviously we can also rewrite @xmath1863 . since @xmath1864 ,",
    "conclude @xmath1865 where use is made of the fact that taking limits preserves @xmath541 ( see @xcite fact 8.10.1 ) .",
    "_ claim 8 : _ let @xmath1867 , where @xmath1868 . by proposition [ propbooleanaperture ]",
    "_ we get @xmath1869 , hence @xmath1870 .",
    "the dual version is obtained from this result by using proposition [ propiterateapadapt ] : let @xmath561 , hence @xmath1871 . then @xmath1872 .",
    "_ claim 9 : _",
    "if @xmath353 , then from proposition [ propapadapt ] it is clear that @xmath1873 is the projector matrix on @xmath1874 and @xmath1875 is the projector matrix on @xmath1876 . from @xmath547 and",
    "the fact that @xmath169 and @xmath87 do not have singular values exceeding 1 it is clear that @xmath1877 , thus @xmath1878 .      it remains to treat the case @xmath1775 .",
    "assume @xmath547 , that is , there exists a positive semidefinite matrix @xmath569 such that @xmath1882 .",
    "clearly @xmath569 can not have singular values exceeding one , so @xmath569 is a conceptor matrix . for ( small ) @xmath1883 ,",
    "let @xmath1884 .",
    "then @xmath1885 can be written as @xmath1886 for a positive semidefinite @xmath1887 , and it holds that @xmath1888 and furthermore @xmath1889 similarly , let @xmath1890 , with @xmath1891 , and observe again @xmath1892 finally , define @xmath1893 , where @xmath1894 .",
    "then @xmath1895 because of @xmath1896 we have @xmath1897 we next state a lemma which is of interest in its own right too .",
    "_ proof of lemma .",
    "_ assume @xmath1900 .",
    "by claim _ 4 . _  of this proposition",
    ", there is a conceptor matrix @xmath25 such that @xmath1901 .",
    "since @xmath1902 , @xmath25 has no unit singular values and thus can be written as @xmath1903 , where @xmath176 is a correlation matrix . therefore ,",
    "@xmath1904 , hence @xmath1905 , that is , @xmath1906",
    ".            _ proof of lemma .",
    "_ since all @xmath1887 ( and hence , all @xmath1915 and @xmath1916 ) have the same eigenvectors as @xmath169 , it suffices to show the convergence claim on the level of individual singular values of the concerned matrices .",
    "let @xmath1917 denote a singular value of @xmath1918 , respectively ( all these versions referring to the same eigenvector in @xmath414 ) .",
    "for convenience i restate from proposition [ propapadapt ] that @xmath1919 it holds that @xmath1920 and @xmath1921 , and similarly @xmath1922 .",
    "it needs to be shown that @xmath1923 .",
    "after these preparations , we can finalize the proof of claim _ 9 .",
    "_  as follows .",
    "from lemma [ lemmaproofadapt2 ] we know that @xmath1930 and @xmath1931 from ( [ eqproofadapt4 ] ) and the fact that @xmath541 is preserved under limits we obtain @xmath563 .",
    "we use the following notation for matrix - vector transforms .",
    "we sort the entries of an @xmath13 matrix @xmath64 into an @xmath301-dimensional vector @xmath1932 row - wise ( ! ) .",
    "that is , @xmath1933 + @xmath1934 , where the ceiling @xmath1935 of a real number @xmath4 is the smallest integer greater or equal to @xmath4 , and @xmath1936 is the modulus function except for arguments of the form @xmath1937 , where we replace the standard value @xmath1938 by @xmath1939 .",
    "conversely , @xmath1940 .",
    "the jacobian @xmath926 can thus be written as a @xmath927 matrix @xmath931 .",
    "the natural parametrization of matrices @xmath25 by their matrix elements does not lend itself easily to an eigenvalue analysis . assuming that a reference solution @xmath914 is fixed",
    ", any @xmath13 matrix @xmath25 is uniquely represented by a parameter matrix @xmath215 through @xmath1941 , with @xmath1942 if and only if @xmath1943 .",
    "conversely , any parameter matrix @xmath215 yields a unique @xmath1944 .",
    "now we consider the jacobian @xmath1945 . by",
    "using that ( i ) @xmath1946 , ( ii ) @xmath1947 for square matrices @xmath1668 , and ( iii ) @xmath1948 for invertible @xmath169 , ( iv ) @xmath1949 , one obtains that @xmath1950 and hence @xmath941 and @xmath926 have the same eigenvalues .        where @xmath1954 is the @xmath145-th unit vector and @xmath1955 .",
    "depending on how @xmath1956 relate to @xmath118 and to each other , calculating ( [ someq1 ] ) leads to numerous case distinctions .",
    "each of the cases concerns entries in a specific subarea of @xmath941 .",
    "these subareas are depicted in fig .",
    "[ somfig1 ] , which shows @xmath941 in an instance with @xmath1957 .    .",
    "an instance with @xmath1957 is shown .",
    "areas are denoted by a , ... ,",
    "l ; same color = same area .",
    "@xmath941 has size @xmath1958 .",
    "its structure is largely organized by a @xmath1959-dimensional and a @xmath1960 submatrix on the diagonal ( areas abefk and hijl , respectively ) .",
    "column / row indices are denoted by @xmath1961 .",
    "area specifications : a : @xmath1962 b : @xmath1963 c : @xmath1964 d : @xmath1965 e : @xmath1966 f : @xmath1967 g : @xmath1968 h : @xmath1969 i : @xmath1970 j : @xmath1971 k : @xmath1972",
    "l : @xmath1973,width=80 ]      the case a concerns all entries @xmath1974 with @xmath1975 translating indices @xmath1961 back to indices @xmath1956 via @xmath1976 + @xmath1977 yields conditions ( i ) @xmath1978 ( from @xmath1979 and @xmath1980 ) , ( ii ) @xmath1981 ( from @xmath1982 ) and ( iii.a ) @xmath1983 or ( iii.b ) @xmath1984 ( from @xmath1985 ) .        where @xmath1989 is the @xmath145-th column in @xmath414 and @xmath1990 if and only if @xmath1991 ( else @xmath513 ) is the kronecker delta .",
    "the value @xmath601 noted for subcase a3 is obtained through @xmath1992 .",
    "note that since @xmath1993 in subcase a3 it holds that @xmath1994 .",
    "the case b concerns all entries @xmath1974 with @xmath2000 like in case a above , this yields conditions on the p - matrix indices : ( i ) @xmath1978 , ( ii ) @xmath2001 , ( iii.a ) @xmath1983 or ( iii.b ) @xmath1984 .",
    "where in the step from the first to the second line one exploits @xmath2001 , hence @xmath2005 ; and @xmath2006 , hence @xmath2007 .",
    "note that since @xmath1993 it holds that @xmath2008 ; that @xmath2009 implies @xmath2010 and that @xmath2011 implies @xmath2012 .",
    "most of the other cases c  l listed in fig .",
    "[ somfig1 ] divide into subcases like a and b. the calculations are similar to the ones above and involve no new ideas . table [ somtable1 ] collects all findings .",
    "it only shows subcases for nonzero entries of @xmath941 .",
    "[ somfig2 ] depicts the locations of these nonzero areas .       .",
    "an instance with @xmath1957 is shown .",
    "areas denotations correspond to table 1 .",
    "same color = same area .",
    "values : areas a3 , c1 , e1 : @xmath601 ; b2 , d1 , f1 : @xmath2013 ; b3 : @xmath2014 ; b5 : @xmath2015 ; j1 : @xmath2016 ; k1 : @xmath2017 ; k2 , l1 : @xmath934 ; k3 : @xmath2018 . *",
    "b. * the left upper principal submatrix re - arranged by simultaneous row / column permutations . * c. * one of the @xmath1016 submatrices @xmath2019 from the diagonal of the @xmath2020 right bottom submatrix of @xmath941 . for explanations see text.,width=130 ]        where @xmath61 and @xmath64 are square , are the eigenvalues collected from the principal submatrices @xmath61 and @xmath64 . in @xmath941",
    "we therefore only need to consider the leading @xmath1959 and the trailing @xmath2022 submatrices ; call them @xmath61 and @xmath64 .      by simultaneous permutations of rows and columns (",
    "which leave eigenvalues unchanged ) in @xmath61 we can bring it to the form @xmath2025 shown in fig .",
    "[ somfig2]*b*. a block structure argument as before informs us that the eigenvalues of @xmath2025 fall into three groups .",
    "there are @xmath932 eigenvalues @xmath934 ( corresponding to the lower right diagonal submatrix of @xmath2025 , denoted as area k2 ) , @xmath118 eigenvalues @xmath935 , where @xmath936 earned from the @xmath118 leading diagonal elements of @xmath2025 ( stemming from the k1 entries in @xmath941 ) , plus there are the eigenvalues of @xmath2026 twodimensional submatrices , each of which is of the form            1 .",
    "@xmath932 instances of 0 , 2 .",
    "@xmath933 instances of @xmath934 , 3 .",
    "@xmath118 eigenvalues @xmath935 , where @xmath936 , 4 .",
    "@xmath937 eigenvalues which come in pairs of the form given in eqn .",
    "( [ someq6 ] ) .        where the last step rests on the fact that all vector components of @xmath2033 are at most 1 and that the set of vectors of the form @xmath2034 is the set of nonnegative vectors with components less than 1 .",
    "l.  appeltant , m.  c. soriano , g.  van  der sande , j.  danckaert , s.  massar , j.  dambre , b.  schrauwen , c.  r. mirasso , and i.  fischer .",
    "information processing using a single dynamical node as complex system .",
    ", 2(468 ) , 2011 .",
    "doi : 10.1038/ncomms1476 .",
    "s.  grossberg .",
    "linking attention to learning , expectation , competition , and consciousness . in l.",
    "itti , g.  rees , and j.  tsotsos , editors , _ neurobiology of attention _ , chapter 107 , pages 652662 .",
    "san diego : elsevier , 2005 .",
    "l.  c. lamb .",
    "the grand challenges and myths of neural - symbolic computation . in l.  de  raedt , b.  hammer , p.  hitzler , and w.  maass , editors , _ recurrent neural networks- models , capacities , and applications _ , number 08041 in dagstuhl seminar proceedings , dagstuhl , germany , 2008 .",
    "internationales begegnungs- und forschungszentrum fr informatik ( ibfi ) , schloss dagstuhl , germany .",
    "l.  lukic , j.  santos - victor , and a.  billard .",
    "learning coupled dynamical systems from human demonstration for robotic eye - arm - hand coordination . in _ proc .",
    "ieee - ras international conference on humanoid robots , osaka 2012 _ , 2012 .",
    "n.  m. mayer and m.  browne .",
    "echo state networks and self - prediction . in",
    "_ biologically inspired approaches to advanced information technology _ , volume 3141 of _ lncs _ , pages 4048 .",
    "springer verlag berlin / heidelberg , 2004 .",
    "d.  l. medin and l.  j. rips .",
    "concepts and categories : memory , meaning , and metaphysics . in k.",
    "j. holyoak and r.  g. morrison , editors , _ the cambridge handbook of thinking and reasoning _ , chapter  3 , pages 3772 .",
    "cambridge university press , 2005 .",
    "g.  pinkas .",
    "propositional non - monotonic reasoning and inconsistency in symmetric neural networks . in _ proc .",
    "12th international joint conference on artificial intelligence - volume 1 _ , pages 525530 , 1991 .",
    "f.  r. reinhart and j.  j. steil .",
    "recurrent neural associative learning of forward and inverse kinematics for movement generation of the redundant pa-10 robot . in a.",
    "stoica , e.  tunsel , t.  huntsberger , t.  arslan , s.  vijayakumar , and a.  o. el - rayis , editors , _ proc .",
    "lab - rs 2008 , vol . 1 _ , pages 3540 , 2008",
    ".    r.  f. reinhart , a.  lemme , and j.  j. steil . representation and generalization of bi - manual skills from kinesthetic teaching . in _ proc . of ieee - ras international conference on humanoid robots , osaka , 2012",
    "_ , in press ."
  ],
  "abstract_text": [
    "<S> the human brain is a dynamical system whose extremely complex sensor - driven neural processes give rise to conceptual , logical cognition . </S>",
    "<S> understanding the interplay between nonlinear neural dynamics and concept - level cognition remains a major scientific challenge . here </S>",
    "<S> i propose a mechanism of neurodynamical organization , called _ </S>",
    "<S> conceptors _ , which unites nonlinear dynamics with basic principles of conceptual abstraction and logic . </S>",
    "<S> it becomes possible to learn , store , abstract , focus , morph , generalize , de - noise and recognize a large number of dynamical patterns within a single neural system ; novel patterns can be added without interfering with previously acquired ones ; neural noise is automatically filtered . </S>",
    "<S> conceptors help explaining how conceptual - level information processing emerges naturally and robustly in neural systems , and remove a number of roadblocks in the theory and applications of recurrent neural networks .     +   + * * +     +   +    ' '' ''     +   +     +    * herbert jaeger * +   + _ jacobs university bremen + school of engineering and science + campus ring + 28759 bremen + germany +   + e - mail : h.jaeger@jacobs-university.de + http://minds.jacobs-university.de_   +    [ [ notes - on - the - structure - of - this - report . ] ] notes on the structure of this report . </S>",
    "<S> + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    this report introduces several novel analytical concepts describing neural dynamics ; develops the corresponding mathematical theory under aspects of linear algebra , dynamical systems theory , and formal logic ; introduces a number of novel learning , adaptation and control algorithms for recurrent neural networks ; demonstrates these in a number of case studies ; proposes biologically ( not too im-)plausible realizations of the dynamical mechanisms ; and discusses relationships to other work . </S>",
    "<S> said shortly , it s long . </S>",
    "<S> not all parts will be of interest to all readers . in order to facilitate navigation through the text and selection of relevant components , </S>",
    "<S> i start with an overview section which gives an intuitive explanation of the novel concepts and informal sketches of the main results and demonstrations ( section 1 ) . after this overview , </S>",
    "<S> the material is presented in detail , starting with an introduction ( section 2 ) which relates this contribution to other research . </S>",
    "<S> the main part is section 3 , where i systematically develop the theory and algorithms , interspersed with simulation demos . </S>",
    "<S> a graphical dependency map for this section is given at the beginning of section 3 . </S>",
    "<S> the technical documentation of the computer simulations is provided in section 4 , and mathematical proofs are collected in section 5 . </S>",
    "<S> the detailed presentation in sections 2  5 is self - contained . </S>",
    "<S> reading the overview in section 1 may be helpful but is not necessary for reading these sections . for convenience </S>",
    "<S> some figures from the overview section are repeated in section 3 .    </S>",
    "<S> [ [ acknowledgements . ] ] acknowledgements . </S>",
    "<S> + + + + + + + + + + + + + + + + +    the work described in this report was partly funded through the european fp7 project amarsi ( www.amarsi-project.eu ) . </S>",
    "<S> the author is indebted to dr . </S>",
    "<S> mathieu galtier and dr . </S>",
    "<S> manjunath ghandi for careful proofreading ( not an easy task ) . </S>"
  ]
}