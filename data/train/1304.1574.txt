{
  "article_text": [
    "the generalization bound measures the probability that a function , chosen from a function class by an algorithm , has a sufficiently small error and plays an important role in statistical learning theory ( see * ? ? ? * ; * ? ? ? * ) .",
    "the generalization bounds have been widely used to study the consistency of the erm - based learning process @xcite , the asymptotic convergence of empirical process @xcite and the learnability of learning models @xcite .",
    "generally , there are three essential aspects to obtain the generalization bounds of a specific learning process : complexity measures of function classes , deviation ( or concentration ) inequalities and symmetrization inequalities related to the learning process . for example",
    ", @xcite presented the generalization bounds based on the rademacher complexity and the covering number , respectively .",
    "@xcite gave the generalization bounds based on the vapnik - chervonenkis ( vc ) dimension .",
    "@xcite proposed the local rademacher complexity and obtained a sharp generalization bound for a particular function class @xmath0 .",
    "@xcite showed improved loss bounds for multiple kernel learning .",
    "it is noteworthy that the aforementioned results of statistical learning theory are all built under the assumption that training and test data are drawn from the same distribution ( or briefly called the assumption of same distribution ) .",
    "this assumption may not be valid in the situation that training and test data have different distributions , which will arise in many practical applications including speech recognition @xcite and natural language processing @xcite .",
    "domain adaptation has recently been proposed to handle this situation and it is aimed to apply a learning model , trained by using the samples drawn from a certain domain ( _ source domain _ ) , to the samples drawn from another domain ( _ target domain _ ) with a different distribution ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) .    without loss of generality , this paper",
    "is mainly concerned with two types of representative domain adaptation . in the first type , the learner receives training data from several source domains , known as _",
    "domain adaptation with multiple sources _",
    "( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "in the second type , the learner minimizes a convex combination of the source and the target empirical risks , termed as _",
    "domain adaptation combining source and target data _",
    "( see * ? ? ?",
    "* ; * ? ? ?",
    "* ) .      in this paper , we present a new framework to obtain the generalization bounds of the learning process for the aforementioned two kinds of representative domain adaptation , respectively . based on the resultant bounds",
    ", we then analyze the asymptotical properties of the learning processes for the two types of domain adaptation .",
    "there are four major aspects in the framework :    * the quantity measuring the difference between two domains ; * the complexity measure of function class ; * the deviation inequalities of the learning process for domain adaptation ; * the symmetrization inequality of the learning precess for domain adaptation .    generally , in order to obtain the generalization bounds of a learning process",
    ", one needs to develop the related deviation ( or concentration ) inequalities of the learning process . for either kind of domain adaptation",
    ", we use a martingale method to develop the related hoeffding - type deviation inequality . moreover , in the situation of domain adaptation , since the source domain differs from the target domain , the desired symmetrization inequality for domain adaptation should incorporate some quantity to reflect the difference . from the point of this view",
    ", we then obtain the related symmetrization inequality incorporating the integral probability metric that measures the difference between the distributions of the source and the target domains .",
    "next , we present the generalization bounds based on the uniform entropy number for both kinds of domain adaptation . also , we generalize the classical mcdiarmid s inequality to a more general setting , where independent random variables take values from different domains . by using the derived inequality",
    ", we obtain the generalization bounds based on the rademacher complexity . following the resultant bounds , we study the asymptotic convergence and the rate of convergence of the learning process in addition to a discussion on factors that affect the asymptotic behaviors .",
    "the numerical experiments support our theoretical findings as well .",
    "meanwhile , we give a comparison with the related results under the assumption of same distribution .",
    "the rest of this paper is organized as follows .",
    "section [ sec : notation - setup ] introduces the problems studied in this paper .",
    "section [ sec : distance ] introduces the integral probability metric to measure the difference between two domains . in section [ sec : uen ] , we introduce two kinds of complexity measures of function classes including the uniform entropy number and the rademacher complexity . in section [ sec : ms ] ( resp .",
    "section [ sec : cstd ] ) , we present the generalization bounds of the learning process for domain adaptation with multiple sources ( resp . combining source and target data ) , and then analyze the asymptotic behavior of the learning process in addition to the related numerical experiment supporting our findings . in section [ sec :",
    "compare ] , we list the existing works on the theoretical analysis of domain adaptation as a comparison and the last section concludes the paper . in the appendices , we prove main results of this paper . for clarity of presentation , we also postpone the discussion of the deviation inequalities and the symmetrization inequalities in the appendices .",
    "in this section , we formalize the main issues of this paper by introducing some necessary notations      we denote @xmath1 @xmath2 and @xmath3 as the @xmath4-th source domain and the target domain , respectively .",
    "set @xmath5 .",
    "let @xmath6 and @xmath7 stand for the distributions of the input spaces @xmath8 @xmath2 and @xmath9 , respectively .",
    "denote @xmath10 and @xmath11 as the labeling functions of @xmath12 ( @xmath13 ) and @xmath14 , respectively .",
    "in the situation of domain adaptation with multiple sources , the input - space distributions @xmath6 @xmath2 and @xmath7 differ from each other , or @xmath15 @xmath2 and @xmath16 differ from each other , or both of the cases occur .",
    "there are sufficient amounts of i.i.d .",
    "samples @xmath17 drawn from each source domain @xmath12 @xmath2 but little or no labeled samples drawn from the target domain @xmath14 .    given @xmath18^{k}$ ] with @xmath19 , let @xmath20 be the function that minimizes the empirical risk @xmath21 over @xmath22 with respect to sample sets @xmath23 , and it is expected that @xmath24 will perform well on the target expected risk : @xmath25 _ i.e. , _",
    "@xmath24 approximates the labeling @xmath16 as precisely as possible .    in the learning process of domain adaptation with multiple sources ,",
    "we are mainly interested in the following two types of quantities :    * @xmath26 , which corresponds to the estimation of the expected risk in the target domain @xmath14 from the empirical quantity that is the weighted combination of the empirical risks in the multiple sources @xmath27 ; * @xmath28 , which corresponds to the performance of the algorithm for domain adaptation with multiple sources ,    where @xmath29 is the function that minimizes the expected risk @xmath30 over @xmath22 .    recalling and , since @xmath31 we have @xmath32 and",
    "thus @xmath33 this shows that the asymptotic behaviors of the aforementioned two quantities when the sample numbers @xmath34 go to _ infinity _ can both be described by the supremum @xmath35 which is the so - called generalization bound of the learning process for domain adaptation with multiple sources .    for convenience ,",
    "we define the loss function as class @xmath36 and call @xmath37 as the function class in the rest of this paper . by and ,",
    "given sample sets @xmath23 drawn from @xmath27 respectively , we briefly denote for any @xmath38 , @xmath39 and @xmath40 thus , we rewrite the generalization bound for domain adaptation with multiple sources as @xmath41      denote @xmath42 and @xmath3 as the source domain and the target domain , respectively .",
    "let @xmath43 and @xmath7 stand for the distributions of the input spaces @xmath44 and @xmath9 , respectively .",
    "denote @xmath45 and @xmath11 as the labeling functions of @xmath46 and @xmath14 , respectively .",
    "in the situation of domain adaptation combining source and target data ( see * ? ? ? * ; * ? ? ?",
    "* ) , the input - space distributions @xmath43 and @xmath7 differ from each other , or the labeling functions @xmath47 and @xmath16 differ from each other , or both cases occur .",
    "there are some ( but not enough ) samples @xmath48 drawn from the target domain @xmath14 in addition to a large amount of samples @xmath49 drawn from the source domain @xmath46 with @xmath50 . given a @xmath51 ,",
    "we denote @xmath52 as the function that minimizes the convex combination of the source and the target empirical risks over @xmath22 : @xmath53 and it is expected that @xmath54 will perform well for any pair @xmath55 , _ i.e. _ , @xmath54 approximates the labeling function @xmath16 as precisely as possible .    as mentioned by @xcite",
    ", setting @xmath56 involves a tradeoff between the source data that are sufficient but not accurate and the target data that are accurate but not sufficient .",
    "especially , setting @xmath57 provides a learning process of the basic domain adaptation with one single source ( see * ? ? ?",
    "similar to the situation of domain adaptation with multiple sources , two types of quantities : @xmath58 and @xmath59 also play an essential role in analyzing the asymptotic behavior of the learning process for domain adaptation combining source and target data . by the similar way of",
    ", we need to consider the supremum @xmath60 which is the so - called generalization bound of the learning process for domain adaptation combining source and target data . following the notation of and taking @xmath61 , we can equivalently rewrite the generalization bound as @xmath62",
    "as shown in some existing works ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , one of major challenges in the theoretical analysis of domain adaptation is to find a quantity to measure the difference between the source domain @xmath46 and the target domain @xmath14 .",
    "then , one can use the quantity to achieve generalization bounds for domain adaptation . in this section",
    ", we use the integral probability metric to measure the difference between the distributions of @xmath46 and @xmath14 , and then discuss the relationship between the integral probability metric and other quantities proposed in existing works , _",
    "e.g. _ , the _ @xmath63-divergence _ and the _ discrepancy distance _ ( see * ? ? ?",
    "* ; * ? ? ?",
    "moreover , we will show that there is a special situation of domain adaptation , where the integral probability metric performs better than other quantities ( see remark [ rem : metric ] )      in @xcite , the _ @xmath63-divergence _ was introduced to derive the generalization bounds based on the vc dimension under the condition of  @xmath64-close \" .",
    "@xcite obtained the generalization bounds based on the rademacher complexity by using the _ discrepancy distance_. both quantities are aimed to measure the difference between two input - space distributions @xmath43 and @xmath7 .",
    "moreover , @xcite used the rnyi divergence to measure the distance between two distributions . in this paper , we use the following quantity to measure the difference between the distributions of the source and the target domains :    [ def : distance ] given two domains @xmath65 , let @xmath66 and @xmath67 be the random variables taking values from @xmath46 and @xmath14 , respectively .",
    "let @xmath68 be a function class .",
    "we define @xmath69 where the expectations @xmath70 and @xmath71 are taken on the distributions @xmath46 and @xmath14 , respectively .",
    "the quantity @xmath72 is termed as the integral probability metric that has played an important role in probability theory for measuring the difference between the two probability distributions ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "recently , @xcite gave the further investigation and proposed an empirical method to compute the integral probability metric .",
    "as mentioned by @xcite[page 432 ] , the quantity @xmath72 is a semimetric and it is a metric if and only if the function class @xmath37 separates the set of all signed measures with @xmath73 .",
    "namely , according to definition [ def : distance ] , given a non - trivial function class @xmath37 , the integral probability metric @xmath72 is equal to _ zero _ if the domains @xmath46 and @xmath14 have the same distribution . by",
    ", the quantity @xmath72 can be equivalently rewritten as @xmath74 next , based on the equivalent form , we discuss the relationships between the quantity @xmath72 and other quantities including the _ @xmath63-divergence _ and the _ discrepancy distance_.      before the formal discussion , we briefly introduce the related quantities proposed in existing works ( see * ? ? ? * ; * ? ? ?",
    "* ) .      in classification tasks , by setting @xmath75 as the absolute - value loss function ( @xmath76 )",
    ", @xcite introduced a variant of the _ @xmath63-divergence _ : @xmath77 to achieve vc - dimension - based generalization bounds for domain adaptation under the condition of  @xmath64-close \" : there exists a @xmath78 such that @xmath79    in both of the classification and regression tasks , given a function class @xmath22 and a loss function @xmath75 , @xcite defined the _ discrepancy distance _ as @xmath80 and then used this quantity to obtain the generalization bounds based on the rademacher complexity .    as mentioned by @xcite , the quantities and match in the setting of classification tasks by setting @xmath75 as the absolute - value loss function , while the usage of does not require the condition of",
    " @xmath64-close \" but the usage of does .",
    "recalling definition [ def : distance ] , since there is no limitation on the function class @xmath37 , the integral probability metric @xmath72 can be used in both classification and regression tasks .",
    "therefore , we only consider the relationship between the integral probability metric @xmath72 and the _ discrepancy distance _ @xmath81 .      from definition [ def : distance ] and , we can find that the integral probability metric @xmath72 measures the difference between the distributions of the two domains @xmath46 and @xmath14 .",
    "however , as addressed in section [ sec : notation - setup ] ,",
    "if a domain @xmath46 differs from another domain @xmath14 , there are three possibilities : the input - space distribution @xmath43 differs from @xmath7 , or @xmath82 differs from @xmath83 , or both of them occur .",
    "therefore , it is necessary to consider two kinds of differences : the difference between the input - space distributions @xmath43 and @xmath7 and the difference between the labeling functions @xmath82 and @xmath83 .",
    "next , we will show that the integral probability metric @xmath72 can be bounded by using two separate quantities that can measure the difference between @xmath43 and @xmath7 and the difference between @xmath82 and @xmath83 , respectively .    as shown in",
    ", the quantity @xmath81 actually measures the difference between the input - space distributions @xmath43 and @xmath7 .",
    "moreover , we introduce another quantity to measure the difference between the labeling functions @xmath82 and @xmath83 :    [ def : distance2 ] given a loss function @xmath75 and a function class @xmath22 , we define @xmath84    note that if the loss function @xmath75 and the function class @xmath22 are both non - trivial ( _ i.e. _ , @xmath37 is non - trivial ) , the quantity @xmath85 is a ( semi)metric between the labeling functions @xmath82 and @xmath83 . in fact , it is not hard to verify that @xmath85 satisfies the triangle inequality and is equal to _ zero _ if @xmath82 and @xmath83 match .    by combining , and",
    ", we have @xmath86 and thus @xmath87 which implies that the integral probability metric @xmath72 can be bounded by the summation of the _ discrepancy distance _ @xmath81 and the quantity @xmath85 , which measure the difference between the input - space distributions @xmath43 and @xmath7 and the difference between the labeling functions @xmath82 and @xmath83 , respectively .",
    "[ rem : metric ] note that there is a specific case in the situation of domain adaptation : @xmath43 differs from @xmath7 and meanwhile @xmath82 differs from @xmath83 , while the distribution of the domain @xmath46 matches with that of the domain @xmath14 . in this case , the integral probability metric @xmath88 equals to _ zero _ , but @xmath81 or @xmath85 neither equals to _ zero_. therefore , the integral probability metric @xmath88 is more suitable for this case than the discrepancy distance @xmath81 .",
    "generally , the generalization bound of a certain learning process is achieved by incorporating some complexity measure of the function class , _",
    "e.g. _ , the covering number , the vc dimension and the rademacher complexity . in this paper , we are mainly concerned with the uniform entropy number and the rademacher complexity .",
    "the uniform entropy number is derived from the concept of the covering number and we refer to @xcite for details .",
    "the covering number of a function class @xmath37 is defined as follows :    [ def : covnum ] let @xmath37 be a function class and @xmath89 be a metric on @xmath37 . for any @xmath90 , the covering number of @xmath37 at radius @xmath91 with respect to the metric @xmath89 , denoted by @xmath92 is the minimum size of a cover of radius @xmath91 .    in some classical results of statistical learning theory ,",
    "the covering number is applied by letting @xmath89 be the distribution - dependent metric .",
    "for example , as shown in theorem 2.3 of @xcite , one can set @xmath89 as the norm @xmath93 and then derive the generalization bound of the i.i.d .",
    "learning process by incorporating the expectation of the covering number , _",
    "i.e. , _ @xmath94 .",
    "however , in the situation of domain adaptation , we only know the information of the source domain , while the expectation @xmath94 is dependent on the distributions of the source and the target domains because @xmath95 . therefore , the covering number is no longer suitable for our framework to obtain the generalization bounds for domain adaptation .",
    "in contrast , the uniform entropy number is distribution - free and thus we choose it as the complexity measure of function classes to derive the generalization bounds for domain adaptation .",
    "next , we will consider the uniform entropy number of @xmath37 in the situations of two types of domain adaptation : ( i ) domain adaptation with multiple sources ; ( ii ) domain adaptation combining source and target data , respectively .      for clarity of presentation , we give a useful notation for the following discussion .",
    "let @xmath96 be the collection of sample sets drawn from multiple sources @xmath27 , respectively .",
    "denote @xmath97 as the collection of the ghost sample sets drawn from @xmath27 such that the ghost sample @xmath98 has the same distribution as @xmath99 for any @xmath13 and any @xmath100 .",
    "denote @xmath101 for any @xmath13 .",
    "moreover , given an @xmath38 and a @xmath18^k$ ] with @xmath19 , we introduce a variant of the @xmath102 norm : @xmath103 it is noteworthy that the variant @xmath104 of the @xmath102 norm is still a norm on the functional space , which can be directly verified by using the definition of norm , so we omit it here .",
    "in the situation of domain adaptation with multiple sources , by setting the metric @xmath89 as @xmath105 , we then define the uniform entropy number of @xmath37 with respect to the metric @xmath105 as @xmath106      in the situation of domain adaptation combining source and target data , we have to introduce another variant of the @xmath102 norm on @xmath37 .",
    "let @xmath107 and @xmath108 be two sets of samples drawn from the domains @xmath46 and @xmath14 , respectively .",
    "given an @xmath38 , we define for any @xmath51 , @xmath109 note that the variant @xmath110 ( @xmath51 ) of the norm @xmath102 is still a norm on the functional space , which can be easily verified by using the definition of norm , so we omit it here",
    ".    moreover , let @xmath111 and @xmath112 be the ghost sample sets of @xmath113 and @xmath114 , respectively .",
    "denote @xmath115 and @xmath116 , respectively .",
    "then , the uniform entropy number of @xmath37 with respect to the metric @xmath117 is defined as @xmath118 where @xmath119 .",
    "the rademacher complexity is one of the most frequently used complexity measures of function classes and we refer to @xcite for details .",
    "[ def : rade ] let @xmath37 be a function class and @xmath120 be a sample set drawn from @xmath121 . denote @xmath122 be a set of random variables independently taking either value from @xmath123 with equal probability .",
    "rademacher complexity of @xmath37 is defined as @xmath124 with its empirical version @xmath125 where @xmath126 stands for the expectation taken with respect to all random variables @xmath120 and @xmath122 , and @xmath127 stands for the expectation only taken with respect to the random variables @xmath122 .",
    "in this section , we present two generalization bounds of the learning process for domain adaptation with multiple sources .",
    "they are based on the uniform entropy number and the rademacher complexity , respectively . by using the derived bounds based on the uniform entropy number",
    ", we then analyze the asymptotic convergence and the rate of convergence of the learning process .",
    "the numerical experiment supports our theoretical analysis as well .",
    "based on the uniform entropy number defined in , a generalization bound for domain adaptation with multiple sources is presented in the following theorem .",
    "[ thm : crate.m ] assume that @xmath37 is a function class consisting of bounded functions with the range @xmath128 $ ] . let @xmath18^k$ ] with @xmath19 .",
    "then , given an arbitrary @xmath129 , we have for any @xmath130 and any @xmath131 , with probability at least @xmath132 , @xmath133 where @xmath134 and @xmath135    in the above theorem , we show that the generalization bound @xmath136 can be bounded by the right - hand side of . compared to the classical result under the assumption of same distribution ( see * ? ? ?",
    "* theorem 2.3 and definition 2.5 ) : with probability at least @xmath132 , @xmath137 with @xmath138 being the empirical risk with respect to the sample set @xmath139 , there is a discrepancy quantity @xmath140 that is determined by the two factors : the choice of @xmath141 and the integral probability metrics @xmath142 ( @xmath13 ) .",
    "the two results will coincide if any source domain and the target domain match , _",
    "i.e. _ , @xmath143 holds for any @xmath13 .    in order to prove this result",
    ", we develop the specific hoeffding - type deviation inequality and the symmetrization inequality for domain adaptation with multiple sources , respectively . the detailed proof is arranged in appendix [ app : proof1 ] .",
    "subsequently , we give another generalization bound based on the rademacher complexity :    [ thm : rb.rade ] assume that @xmath37 is a function class consisting of bounded functions with the range @xmath128 $ ] .",
    "let @xmath18^k$ ] with @xmath19 .",
    "then , we have with probability at least @xmath132 , @xmath144 where @xmath140 is defined in and @xmath145 ( @xmath13 ) are the rademacher complexities on the source domains @xmath12 , respectively .",
    "similarly , the derived bound coincides with the related classical result under the assumption of same distribution ( see * ? ? ?",
    "* theorem 5 ) , when any source domain of @xmath27 and the target domain @xmath14 match , _",
    "i.e. _ , @xmath146 holds for any @xmath13 .",
    "the proof of this theorem is processed by introducing a generalized version of mcdiarmid s inequality which allows independent random variables to take values from different domains ( see appendix [ app : proof3 ] ) .",
    "subsequently , based on the derived bound , we can analyze the asymptotic behavior of the learning process for domain adaptation with multiple sources .      in statistical learning theory ,",
    "it is well - known that the complexity of function class is one of main factors to the asymptotic convergence of the learning process under the assumption of same distribution @xcite .    from theorem [ thm : crate.m ]",
    ", we can directly arrive at the following result showing that the asymptotic convergence of the learning process for domain adaptation with multiple sources is affected by the three aspects : the choice of @xmath141 , the discrepancy quantity @xmath140 and the uniform entropy number @xmath147 .",
    "[ thm : converge.m ] assume that @xmath37 is a function class consisting of the bounded functions with the range @xmath128 $ ] .",
    "let @xmath18^k$ ] with @xmath19 .",
    "if the following condition holds : @xmath148 then we have for any @xmath129 , @xmath149    as shown in theorem [ thm : converge.m ] , if the choice of @xmath150^k$ ] and the uniform entropy number @xmath151 satisfy the condition with @xmath19 , the probability of the event that @xmath152 will converge to _ zero _ for any @xmath129 , when the sample numbers @xmath34 of multiple sources go to _ infinity _ , respectively .",
    "this is partially in accordance with the classical result of the asymptotic convergence of the learning process under the assumption of same distribution ( _ cf . _",
    "theorem 2.3 and definition 2.5 of [ 22 ] ) : the probability of the event that @xmath153 will converge to _ zero _ for any @xmath90 , if the uniform entropy number @xmath154 satisfies the following : @xmath155    note that in the learning process of domain adaptation with multiple sources , the uniform convergence of the empirical risk on the source domains to the expected risk on the target domain may not hold , because the limit does not hold for any @xmath90 but for any @xmath129 .",
    "by contrast , the limit holds for all @xmath90 in the learning process under the assumption of same distribution , if the condition is satisfied .",
    "again , these two results coincide when any source domain and the target domain match , _",
    "i.e. _ , @xmath146 holds for any @xmath13 .",
    "next , we study the rate of convergence of the learning process for domain adaptation with multiple sources .      recalling",
    ", we can find that the rate of convergence is affected by the choice of @xmath141 . according to the cauchy - schwarz inequality , setting @xmath156 ( @xmath13 )",
    ", we have @xmath157 which minimizes the second term of the right - hand side of .",
    "thus , by , and , we find that the fastest rate of convergence of the learning process is up to @xmath158 which is the same as the classical result of the learning process under the assumption of same distribution if the discrepancy @xmath140 is ignored .",
    "in addition , the bound based on the rademacher complexity also implies that the rate of convergence of the learning process is affected by the choice of @xmath141 .",
    "again , according to cauchy - schwarz inequality , setting @xmath159 ( @xmath13 ) leads to the fastest rate of convergence : @xmath160 which is in accordance with the aforementioned analysis .",
    "the following numerical experiments support our theoretical findings ( see fig .",
    "[ fig : fig1 ] ) .",
    "we have performed the numerical experiments to verify the theoretic analysis of the asymptotic convergence of the learning processes for domain adaptation with multiple sources . without loss of generality",
    ", we only consider the case of @xmath161 , _",
    "i.e. , _ there are two source domains and one target domain .",
    "the experiment data are generated in the following way :    for the target domain @xmath162 , we consider @xmath9 as a gaussian distribution @xmath163 and draw @xmath164 ( @xmath165 ) from @xmath9 randomly and independently .",
    "let @xmath166 be a random vector of a gaussian distribution @xmath167 , and let the random vector @xmath168 be a noise term with @xmath169 .",
    "for any @xmath170 , we randomly draw @xmath171 and @xmath172 from @xmath167 and @xmath173 respectively , and then generate @xmath174 as follows : @xmath175 the derived @xmath176 ( @xmath165 ) are the samples of the target domain @xmath14 and will be used as the test data .    in the similar way",
    ", we generate the sample set @xmath177 ( @xmath178 ) of the source domain @xmath179 : for any @xmath180 , @xmath181 where @xmath182 , @xmath183 and @xmath169 .    for the source domain @xmath184 , the samples @xmath185 ( @xmath186 )",
    "are derived in the following way : for any @xmath187 , @xmath188 where @xmath189 , @xmath183 and @xmath169 .    in this experiment",
    ", we use the method of least square regressionjye02/software / slep / index.htm ] to minimize the empirical risk @xmath190 for different combination coefficients @xmath191 and then compute the discrepancy @xmath192 for each @xmath193 .",
    "the initial @xmath194 and @xmath195 both equal to @xmath196 .",
    "each test is repeated @xmath197 times and the final result is the average of the @xmath197 results . after each test , we increment @xmath194 and @xmath195 by @xmath196 until @xmath198 .",
    "the experiment results are shown in fig .",
    "[ fig : fig1 ] .        from fig .",
    "[ fig : fig1 ] , we can find that for any choice of @xmath199 , the curve of @xmath192 is decreasing when @xmath193 increases , which is in accordance with the results presented in theorems [ thm : crate.m ] & [ thm : converge.m ] .",
    "moreover , when @xmath200 , the discrepancy @xmath192 has the fastest rate of convergence , and the rate becomes slower as @xmath199 is further away from @xmath201 . in this experiment",
    ", we set @xmath202 that implies that @xmath203 . recalling",
    ", we have shown that @xmath204 will provide the fastest rate of convergence and this proposition is supported by the experiment results shown in fig .",
    "[ fig : fig1 ] .",
    "in this section , we present two generalization bounds of the learning process for domain adaptation combining source and target data , which are based on the uniform entropy number and the rademacher complexity , respectively .",
    "we then analyze the asymptotic convergence and the rate of convergence of the learning process in addition to the numerical experiments supporting our theoretical analysis .",
    "the following theorem provides a generalization bound based on the uniform entropy number with respect to the metric @xmath205 defined in .",
    "similar to the situation of domain adaptation with multiple sources , the proof of this theorem is achieved by using a specific hoeffding - type deviation inequality and a symmetrization inequality for domain adaptation combining source and target data ( see appendix [ app : proof2 ] ) .",
    "[ thm : crate.c ] assume that @xmath37 is a function class consisting of the bounded functions with the range @xmath128 $ ] .",
    "let @xmath107 and @xmath108 be two sets of i.i.d .",
    "samples drawn from domains @xmath46 and @xmath14 , respectively .",
    "then , for any @xmath51 and given an arbitrary @xmath206 , we have for any @xmath207 , with probability at least @xmath132 , @xmath208 where @xmath72 is defined in and @xmath209 .    compared to the classical result under the assumption of same distribution , the derived bound contains a term of discrepancy quantity @xmath210 that is determined by two factors : the combination coefficient @xmath56 and the quantity @xmath72 .",
    "the two results coincide when the source domain @xmath46 and the target domain @xmath14 match , _",
    "i.e. , _ @xmath211 .    based on the rademacher complexity",
    ", we then get another generalization bound of the learning process for domain adaptation combining source and target data .",
    "its proof is postponed in appendix [ app : proof3 ] .",
    "[ thm : rb.rade.c ] assume that @xmath37 is a function class consisting of the bounded functions with the range @xmath128 $ ] .",
    "let @xmath107 and @xmath212 be two sets of i.i.d .",
    "samples drawn from the domains @xmath46 and @xmath14 , respectively . then , given @xmath51 and for any @xmath131 , we have with probability at least @xmath132 , @xmath213 where @xmath72 is defined in .    note that in the derived bound , we adopt an empirical rademacher complexity @xmath214 that is based on the data drawn from the target domain @xmath14 , because the distribution of @xmath14 is unknown in the situation of domain adaptation .",
    "similar to the aforementioned discussion , the generalization bound coincides with the result under the assumption of same distribution ( see * ? ? ?",
    "* theorem 5 ) , when the source domain of @xmath46 and the target domain @xmath14 match , _",
    "i.e. _ , @xmath211 .    the two results and exhibit a tradeoff between the sample numbers @xmath215 and @xmath216 , which is associated with the choice of @xmath56 .",
    "although the tradeoff has been mentioned in some previous works ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ) , the following will show a rigorous theoretical analysis of the tradeoff .",
    "following theorem [ thm : crate.c ] , we can directly obtain the concerning result pointing out that the asymptotic convergence of the learning process for domain adaptation combining source and target data is affected by three factors : the uniform entropy number @xmath217 , the integral probability metric @xmath72 and the choice of @xmath51 .",
    "[ thm : converge.c ] assume that @xmath37 is a function class consisting of bounded functions with the range @xmath128 $ ] .",
    "given a @xmath51 , if the following condition holds : @xmath218 with @xmath209 , then we have for any @xmath206 , @xmath219    as shown in theorem [ thm : converge.c ] , if the choice of @xmath51 and the uniform entropy number @xmath220 satisfy the condition , the probability of the event @xmath221 will converge to _ zero _ for any @xmath206 , when @xmath215 goes to _",
    "infinity_. this is partially in accordance with the classical result under the assumption of same distributions derived from the combination of theorem 2.3 and definition 2.5 of @xcite .",
    "note that in the learning process for domain adaptation combining source and target data , the uniform convergence of the empirical risk @xmath222 to the expected risk @xmath223 may not hold , because the limit does not hold for any @xmath90 but for any @xmath206 . by contrast",
    ", the limit holds for all @xmath90 in the learning process under the assumption of same distribution , if the condition is satisfied .",
    "the two results coincide when the source domain @xmath46 and the target domain @xmath14 match , _",
    "i.e. , _ @xmath211 .",
    "we consider the choice of @xmath56 that is an essential factor to the rate of convergence for the learning process and is associated with the tradeoff between the sample numbers @xmath215 and @xmath216 .",
    "recalling , if we fix the value of @xmath220 , setting @xmath224 minimizes the second term of the right - hand side of and then we arrive at @xmath225 which implies that setting @xmath224 can result in the fastest rate of convergence , while it can also cause the relatively larger discrepancy between the empirical risk @xmath222 and the expected risk @xmath223 , because the situation of domain adaptation is set up in the condition that @xmath226 , which implies that @xmath227 .",
    "moreover , this choice of @xmath56 associated with a trade off between sample numbers @xmath215 and @xmath216 is also suitable to the rademacher - complexity - based bound .",
    "it is noteworthy that the value @xmath224 has been mentioned in the section of  experimental results \" in @xcite . here , we show a rigorous theoretical analysis of this value and the following numerical experiment also supports this finding ( see fig .",
    "[ fig : fig2 ] ) .",
    "in the situation of domain adaptation combining source and target data , the samples @xmath176 ( @xmath165 ) of the target domain @xmath14 are generated in the aforementioned way ( see ) .",
    "we randomly pick @xmath228 samples from them to form the objective function and the rest @xmath229 are used to test .    in the similar way ,",
    "the samples @xmath230 ( @xmath231 ) of the source domain @xmath46 are generated as follows : for any @xmath232 , @xmath233 where @xmath234 , @xmath183 and @xmath169 .",
    "we also use the method of least square regression to minimize the empirical risk @xmath235 for different combination coefficients @xmath236 and then compute the discrepancy @xmath237 for each @xmath215 . since it has to be satisfied that @xmath238 , the initial @xmath215 is set to be @xmath196 .",
    "each test is repeated @xmath239 times and the final result is the average of the @xmath239 results . after each test ,",
    "we increment @xmath215 by @xmath196 until @xmath231 .",
    "the experiment results are shown in fig .",
    "[ fig : fig2 ] .",
    "figure illustrates that for any choice of @xmath236 , the curve of @xmath240 is decreasing as @xmath215 increases .",
    "this is in accordance with our results of the asymptotic convergence of the learning process for domain adaptation with multiple sources ( see theorems [ thm : crate.c ] and [ thm : converge.c ] ) .",
    "furthermore , fig .",
    "[ fig : fig2 ] also shows that when @xmath241 , the discrepancy @xmath242 has the fastest rate of convergence , and the rate becomes slower as @xmath56 is further away from @xmath243 .",
    "thus , this is in accordance with the theoretical analysis of the asymptotic convergence presented above .",
    "there have been some previous works on the theoretical analysis of domain adaptation with multiple sources ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) and domain adaptation combining source and target data ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ) .    in @xcite , the function class and the loss function",
    "are assumed to satisfy the conditions of  @xmath244-triangle inequality \" and  uniform convergence bound \" .",
    "moreover , one has to get some prior information about the disparity between any source domain and the target domain . under these conditions ,",
    "some generalization bounds were obtained by using the classical techniques developed under the assumption of same distribution .",
    "@xcite proposed another framework to study the problem of domain adaptation with multiple sources . in this framework",
    ", one has to know some prior knowledge including the exact distributions of the source domains and the hypothesis function with a small loss on each source domain .",
    "furthermore , the target domain and the hypothesis function on the target domain were deemed as the mixture of the source domains and the mixture of the hypothesis functions on the source domains , respectively .",
    "then , by introducing the rnyi divergence , @xcite extended their previous work @xcite to a more general setting , where the distribution of the target domain can be arbitrary and one only needs to know an approximation of the exact distribution of each source domain . @xcite",
    "also discussed the situation of domain adaptation with the mixture of source domains .    in @xcite ,",
    "domain adaptation combining source and target data was originally proposed and meanwhile a theoretical framework was presented to analyze its properties for the classification tasks by introducing the @xmath63-divergence . under the condition of  @xmath64-close \" ,",
    "the authors applied the classical techniques developed under the assumption of same distribution to achieve the generalization bounds based on the vc dimension .    @xcite",
    "introduced the _ discrepancy distance _",
    "@xmath81 to capture the difference between domains and this quantity can be used in both classification and regression tasks . by applying the classical results of statistical learning theory",
    ", the authors obtained the generalization bounds based on the rademacher complexity .",
    "in this paper , we propose a new framework to obtain generalization bounds of the learning process for two representative types of domain adaptation : domain adaptation with multiple sources and domain adaptation combining source and target data .",
    "this framework is suitable for a variant of learning tasks including classification and regression .",
    "based on the derived bounds , we theoretically analyze the asymptotic convergence and the rate of convergence of the learning process for domain adaptation",
    ". there are four important aspects of this framework : the quantity measuring the difference between two domains ; the complexity measure of function class , the deviation inequality and the symmetrization inequality for domain adaptation .",
    "* we use the integral probability metric @xmath72 to measure the difference between two domains @xmath46 and @xmath14 .",
    "we show that the integral probability metric is well - defined and is a ( semi)metric on the space of the probability distributions .",
    "it can be bounded by the summation of the _ discrepancy distance _ @xmath81 and the quantity @xmath85 , which measure the difference between the input - space distributions @xmath43 and @xmath7 and the difference between labeling functions @xmath82 and @xmath83 , respectively .",
    "note that there is a special case that is more suitable to the integral probability metric @xmath72 than other quantities ( see remark [ rem : metric ] ) . *",
    "the uniform entropy number and the rademacher complexity are adopted to achieved the generalization bounds ; and ; , respectively .",
    "it is noteworthy that the generalization bounds and can lead to the results based on the fat - shattering dimension , respectively ( see * ? ? ?",
    "* theorem 2.18 ) . according to theorem 2.6.4 of @xcite",
    ", the bounds based on the vc dimension can also be obtained from the results and , respectively . * instead of directly applying the classical techniques , we present the specific deviation inequalities for the learning process of domain adaptation . in order to obtain the generalization bounds based on the uniform entropy numbers , we develop the specific hoeffding - type deviation inequalities for the two types of domain adaptation , respectively ( see appendices [ app : proof1 ] & [ app : proof2 ] ) .",
    "furthermore , we also generalize the classical mcdiarmid s inequality to a more general setting where the independent random variables can take value from different domains ( see appendix [ app : proof3 ] ) .",
    "* we also develop the related symmetrization inequalities of the learning process for domain adaptation .",
    "the derived inequalities incorporate the discrepancy term that is determined by the difference between the source and the target domains and reflects the learning - transfering from the source to the target domains .",
    "based on the derived generalization bounds , we provide a rigorous theoretical analysis of the asymptotic convergence and the rate of convergence of the learning process for either kind of domain adaptation .",
    "we also consider the choices of @xmath141 and @xmath56 that affect the rate of convergence of the learning processes for the two types of domain adaptation , respectively .",
    "moreover , we give a comparison with the previous works @xcite as well as the related results of the learning process under the assumption of same distribution ( see * ? ? ?",
    "* ; * ? ? ?",
    "the numerical experiments support our theoretical findings as well .    in our future work , we will attempt to find a new distance between distributions to develop the generalization bounds based on other complexity measures , and analyze other theoretical properties of domain adaptation .    0.2 in    99    v. n. vapnik .",
    "_ the nature of statistical learning theory ( information science and statistics)_. springer , new york , 1999 .",
    "o. bousquet , s. boucheron and g. lugosi .",
    "introduction to statistical learning theory .",
    "_ lecture notes in artificial intelligence _ , 3176:169 - 207 , 2004 .",
    "a. van der vaart and j. wellner .",
    "_ weak convergence and empirical processes : with applications to statistics ( springer series in statistics)_. springer , 1996 .",
    "a. blumer , a. ehrenfeucht , d. haussler , and m.k .",
    "learnability and the vapnik - chervonenkis dimension .",
    "_ journal of the acm _ , 36(4):929 - 965 , 1989 .",
    "bartlett , o. bousquet and s. mendelson .",
    "local rademacher complexities . _",
    "annals of statistics _ , 33:1497 - 1537 , 2005 .",
    "z. hussain and j. shawe - taylor . improved loss bounds for multiple kernel learning . _ journal of machine learning research _ , * 15(w&cp)*:370 - 377 , 2011 .",
    "j. jiang , and c. zhai .",
    "instance weighting for domain adaptation in nlp .",
    "_ proceedings of the 45th annual meeting of the association of computational linguistics ( acl ) _ , 264 - 271 , 2007 .",
    "j. blitzer , m. dredze and f. pereira .",
    "biographies , bollywood , boomboxes and blenders : domain adaptation for sentiment classification .",
    "_ proceedings of the 45th annual meeting of the association of computational linguistics ( acl ) _ , 440 - 447 , 2007 .",
    "s. bickel , m. brckner and t. scheffer .",
    "discriminative learning for differing training and test distributions .",
    "_ proceedings of the 24th international conference on machine learning ( icml ) _ , 81 - 88 , 2007 .    p. wu and t.g .",
    "improving svm accuracy by training on auxiliary data sources .",
    "_ proceedings of the twenty - first international conference on machine learning , _ 2004 .",
    "j. blitzer , r. mcdonald and f. pereira .",
    "domain adaptation with structural correspondence learning . _",
    "conference on empirical methods in natural language processing ( emnlp ) _ , 2006 .",
    "s. ben - david , j. blitzer , k. crammer , a. kulesza , f. pereira and j. wortman . a theory of learning from different domains",
    ". _ machine learning _ , 79:151 - 175 , 2010 .",
    "w. bian , d. tao and y. rui .",
    "cross - domain human action recognition .",
    "_ ieee transactions on systems , man , and cybernetics , part b : cybernetics _ , 42(2):298 - 307 , 2012 .",
    "k. crammer , m. kearns and j. wortman .",
    "learning from multiple sources .",
    "_ advances in neural information processing systems ( nips ) _ , 2006 .",
    "k. crammer , m. kearns and j. wortman .",
    "learning from multiple sources .",
    "_ journal of machine learning research _ , 9:1757 - 1774 , 2008 .",
    "y. mansour , m. mohri and a. rostamizadeh .",
    "domain adaptation with multiple sources .",
    "_ advances in neural information processing systems ( nips ) _ , 2008 .",
    "y. mansour , m. mohri and a. rostamizadeh .",
    "multiple source adaptation and the rnyi divergence .",
    "_ proceedings of the twenty - fifth conference on uncertainty in artificial intelligence ( uai ) _ , 2000 .",
    "j. blitzer , k. crammer , a. kulesza , f. pereira and j. wortman .",
    "learning bounds for domain adaptation .",
    "_ advances in neural information processing systems ( nips ) _ , 2008 .",
    "y. mansour , m. mohri and a. rostamizadeh .",
    "domain adaptation : learning bounds and algorithms .",
    "_ conference on learning theory ( colt ) _ , 2009 .",
    "s. ben - david , j. blitzer , k. crammer and f. pereira .",
    "analysis of representations for domain adaptation .",
    "_ advances in neural information processing systems ( nips ) _ , 2006 .",
    "probability metrics . _",
    "theory of probability and its application _ , 28(1):278 - 302 , 1984 .",
    "_ probability metrics and the stability of stochastic models .",
    "_ john wiley and sons , 1991 .    a. mller .",
    "integral probability metrics and their generating classes of functions .",
    "_ advances in applied probability _ , 29(2):429 - 443 , 1997 .",
    "reid and r.c .",
    "information , divergence and risk for binary experiments .",
    "_ journal of machine learning research , _ 12:731 - 817 , 2011 .",
    "sriperumbudur , a. gretton , k. fukumizu , g.r.g .",
    "lanckriet and b. schlkopf . a note on integral probability metrics and @xmath245-divergences .",
    "_ corr _ , abs/0901.2698 , 2009 .",
    "sriperumbudur , k. fukumizu , a. gretton , b. schlkopf and g.r.g . lanckriet . on the empirical estimation of integral probability metrics .",
    "_ electron .",
    "j. statist .",
    "_ , 6:1550 - 1599 , 2012 .",
    "s. mendelson .",
    "a few notes on statistical learning theory . _",
    "advanced lectures on machine learning _ , 2600:1 - 40 , 2003 .",
    "w. hoeffding .",
    "probability inequalities for sums of bounded random variables .",
    "_ journal of the american statistical association _",
    ", 58(301):13 - 30 , 1963 .",
    "in this appendix , we provide the proof of theorem [ thm : crate.m ] . in order to achieve the proof ,",
    "we need to develop the specific hoeffding - type deviation inequality and the symmetrization inequality for domain adaptation with multiple sources .",
    "deviation ( or concentration ) inequalities play an essential role in obtaining the generalization bounds for a certain learning process . generally , specific deviation inequalities need to be developed for different learning processes .",
    "there are many popular deviation and concentration inequalities , _ e.g. _ , hoeffding s inequality , mcdiarmid s inequality , bennett s inequality , bernstein s inequality and talagrand s inequality .",
    "these results are all built under the assumption of same distribution , and thus they are not applicable ( or at least can not be directly applied ) to the setting of multiple sources .",
    "next , based on hoeffding s inequality @xcite , we present a deviation inequality for multiple sources .    [ thm : dineq.m ] assume that @xmath37 is a function class consisting of bounded functions with the range @xmath128 $ ] .",
    "let @xmath17 be the set of i.i.d .",
    "samples drawn from the source domain @xmath246 @xmath2 . given @xmath150^{k}$ ] with @xmath19 and for any @xmath38",
    ", we define a function @xmath247 as @xmath248 where for any @xmath13 and given @xmath249 , the set @xmath250 is denoted as @xmath251 then , we have for any @xmath90 , @xmath252 where @xmath70 stands for the expectation taken on all source domains @xmath27 .    this result is an extension of the classical hoeffding - type deviation inequality under the assumption of same distribution ( see * ? ? ?",
    "* theorem 1 ) . compared to the classical result , the resultant deviation inequality is suitable to the setting of multiple sources .",
    "these two inequalities coincide when there is only one source , _ i.e. _ , @xmath253    the proof of theorem [ thm : dineq.m ] is processed by a martingale method . before the formal proof , we introduce some essential notations .",
    "let @xmath23 be sample sets drawn from multiple sources @xmath27 , respectively .",
    "define a random variable @xmath254 where @xmath255 it is clear that @xmath256 where @xmath70 stands for the expectation taken on all source domains @xmath27 .",
    "then , according to and , we have for any @xmath13 and @xmath100 , @xmath257    to prove theorem [ thm : dineq.m ] , we need the following inequality resulted from hoeffding s lemma .",
    "[ lem : hoeffding ] let @xmath258 be a function with the range @xmath128 $ ] .",
    "then , the following holds for any @xmath259 : @xmath260    _ proof .",
    "_ we consider @xmath261 as a random variable",
    ". then , it is clear that @xmath262 since the value of @xmath263 is a constant denoted as @xmath264 , we have @xmath265 according to hoeffding s lemma , we then have @xmath266 this completes the proof .",
    "we are now ready to prove theorem [ thm : dineq.m ] .",
    "_ proof of theorem [ thm : dineq.m ] .",
    "_ according to , , lemma [ lem : hoeffding ] , markov s inequality , jensen s inequality and the law of iterated expectation , we have for any @xmath259 , @xmath268 where @xmath269 .",
    "therefore , we have @xmath270 where @xmath271 similarly , we can obtain @xmath272 note that @xmath273 is a quadratic function with respect to @xmath259 and thus the minimum value  @xmath274 \" is achieved when @xmath275 by combining , and , we arrive at @xmath276 this completes the proof .",
    "@xmath267    in the following subsection , we present a symmetrization inequality for domain adaptation with multiple sources .",
    "symmetrization inequalities are mainly used to replace the expected risk by an empirical risk computed on another sample set that is independent of the given sample set but has the same distribution . in this manner",
    ", the generalization bounds can be achieved by applying some kinds of complexity measures , _",
    "e.g. _ , the covering number and the vc dimension .",
    "however , the classical symmetrization results are built under the assumption of same distribution ( see * ? ? ?",
    "the symmetrization inequality for domain adaptation with multiple sources is presented in the following theorem :    [ thm : sym.m ] assume that @xmath37 is a function class with the range @xmath128 $ ] .",
    "let sample sets @xmath277 and @xmath278 be drawn from the source domains @xmath27 . then , given an arbitrary @xmath279 and @xmath18^{k}$ ] with @xmath19 , we have for any @xmath280 , @xmath281 where @xmath282    this theorem shows that given @xmath279 , the probability of the event : @xmath283 can be bounded by using the probability of the event : @xmath284 that is only determined by the characteristics of the source domains @xmath27 when @xmath285 with @xmath286 .",
    "compared to the classical symmetrization result under the assumption of same distribution ( see * ? ? ?",
    "* ) , there is a discrepancy term @xmath287 in the derived inequality .",
    "especially , the two results coincide when any source domain and the target domain match , _",
    "@xmath143 holds for any @xmath13 .",
    "the following is the proof of theorem [ thm : sym.m ] .",
    "_ proof of theorem [ thm : sym.m ] .",
    "_ let @xmath288 be the function achieving the supremum : @xmath289 with respect to the sample set @xmath277 . according to , , and , we arrive at @xmath290 and thus , @xmath291 where the expectation @xmath292 is defined as @xmath293    let @xmath294 and denote @xmath295 as the conjunction of two events . according to the triangle inequality , we have @xmath296",
    ", @xmath297 then , taking the expectation with respect to @xmath278 gives @xmath298    by chebyshev s inequality , since @xmath278 are the sets of i.i.d .",
    "samples drawn from the multiple sources @xmath27 respectively , we have for any @xmath299 , @xmath300    subsequently , according to and , we have for any @xmath299 , @xmath301 by combining , and , taking the expectation with respect to @xmath277 and letting @xmath302 can lead to : for any @xmath303 , @xmath304 with @xmath305 .",
    "this completes the proof .",
    "@xmath267    by using the resultant deviation inequality and the symmetrization inequality , we can achieve the proof of theorem [ thm : crate.m ] .",
    "_ proof of theorem [ thm : crate.m ] .",
    "_ consider @xmath306 as an independent rademacher random variables , _",
    "i.e. _ , an independent @xmath123-valued random variable with equal probability of taking either value",
    ". given sample sets @xmath307 , denote for any @xmath38 and @xmath13 , @xmath308 and for any @xmath38 , @xmath309    according to , and theorem [ thm : sym.m ] , given an arbitrary @xmath303 , we have for any @xmath310 such that @xmath311 with @xmath305 , @xmath312    fix a realization of @xmath307 and let @xmath313 be a @xmath314-radius cover of @xmath37 with respect to the @xmath315 norm . since @xmath37 is composed of the bounded functions with the range @xmath128 $ ]",
    ", we assume that the same holds for any @xmath316 . if @xmath317 is the function that achieves the following supremum @xmath318 there must be an @xmath319 that satisfies @xmath320 and meanwhile , @xmath321 therefore , for the realization of @xmath307 , we arrive at @xmath322    moreover , we denote the event @xmath323 and let @xmath324 be the characteristic function of the event @xmath325 . by fubini s theorem , we have @xmath326    fix a realization of @xmath307 again . according to , and theorem [ thm : dineq.m ] , we have @xmath327 where the expectation @xmath328 is defined in .",
    "the combination of , and leads to the result : given an arbitrary @xmath129 and for any @xmath329 with @xmath134 , @xmath330 according to , letting @xmath331 we then arrive at with probability at least @xmath132 , @xmath332 where @xmath134 .",
    "this completes the proof .",
    "@xmath267      here",
    ", we provide the proof of theorem [ thm : crate.c ] .",
    "similar to the situation of domain adaptation with multiple sources , we need to develop the related hoeffding - type deviation inequality and the symmetrization inequality for domain adaptation combining source and target data .      based on hoeffding s inequality @xcite",
    ", we derive a deviation inequality for the combination of the source and the target domains .",
    "[ thm : dineq.c ] assume that @xmath37 is a function class consisting of bounded functions with the range @xmath128 $ ] .",
    "let @xmath49 and @xmath333 be sets of i.i.d .",
    "samples drawn from the source domain @xmath334 and the target domain @xmath335 , respectively . for any @xmath51 , define a function @xmath336 as @xmath337 where @xmath338 then , we have for any @xmath51 and any @xmath90 , @xmath339 where the expectation @xmath340 is taken on both of the source domain @xmath46 and the target domain @xmath14 .    in this theorem",
    ", we present a deviation inequality for the combination of source and target domains , which is an extension of the classical hoeffding - type deviation inequality under the assumption of same distribution ( see * ? ? ?",
    "* theorem 1 ) .",
    "compare to the classical result , the resultant deviation inequality allows the random variables to take values from different domains .",
    "the two inequalities coincide when the source domain @xmath46 and the target domain @xmath14 match , _",
    "i.e. , _ @xmath211 .",
    "the proof of theorem [ thm : dineq.c ] is also processed by a martingale method . before the formal proof , we introduce some essential notations .    for any @xmath51 ,",
    "we denote @xmath341 recalling , it is evident that @xmath342 .",
    "we then define two random variables : @xmath343 where @xmath344 it is clear that @xmath345 ; @xmath346 and @xmath347 ; @xmath348 .",
    "according to and , we have for any @xmath232 and any @xmath51 , @xmath349    similarly , we also have for any @xmath170 , @xmath350    we are now ready to prove theorem [ thm : dineq.c ] .    _ proof of theorem [ thm : dineq.c ] . _ according to and , we have @xmath351    according to lemma [ lem : hoeffding ] , , , , markov s inequality , jensen s inequality and the law of iterated expectation , we have for any @xmath259 and any @xmath352 , @xmath353 then , we have @xmath354 where @xmath355 similarly , we can arrive at @xmath356 note that @xmath273 is a quadratic function with respect to @xmath259 and thus the minimum value @xmath357 is achieved when @xmath358 by combining , and , we arrive at @xmath359 this completes the proof .",
    "@xmath267      in the following theorem , we present the symmetrization inequality for domain adaptation combining source and target data .",
    "[ thm : sym.c ] assume that @xmath37 is a function class with the range @xmath128 $ ] .",
    "let @xmath113 and @xmath111 be drawn from the source domain @xmath46 , and @xmath114 and @xmath360 be drawn from the target domain @xmath14 .",
    "then , for any @xmath51 and given an arbitrary @xmath361 , we have for any @xmath207 , @xmath362 with @xmath363    this theorem shows that for any @xmath361 , the probability of the event : @xmath364 can be bounded by using the probability of the event : @xmath365 that is only determined by the samples drawn from the source domain @xmath46 and the target domain @xmath14 , when @xmath366 . compared to the classical symmetrization result under the assumption of same distribution ( see * ? ? ?",
    "* ) , there is a discrepancy term @xmath367 .",
    "the two results will coincide when the source and the target domains match , _",
    "i.e. _ , @xmath368 .",
    "the following is the proof of theorem [ thm : sym.c ] .    _",
    "proof of theorem [ thm : sym.c ] .",
    "_ let @xmath288 be the function achieving the supremum : @xmath369 with respect to @xmath113 and @xmath114 .",
    "according to and , we arrive at @xmath370 and thus @xmath371 where @xmath372    let @xmath373 and denote @xmath295 as the conjunction of two events . according to the triangle inequality , we have @xmath374 then , taking the expectation with respect to @xmath111 and @xmath360 gives @xmath375    by chebyshev s inequality , since @xmath376 and @xmath377 are sets of i.i.d .",
    "samples drawn from the source domain @xmath46 and the target domain @xmath14 respectively , we have for any @xmath299 and any @xmath51 , @xmath378 where @xmath379 and @xmath380 stand for the ghost random variables taking values from the source domain @xmath46 and the target domain @xmath14 , respectively .    subsequently , according to and , we have for any @xmath299 , @xmath381    according to , and , by letting @xmath382 and taking the expectation with respect to @xmath113 and @xmath114 , we have for any @xmath299 , @xmath383 with @xmath384 .",
    "this completes the proof .",
    "we are now ready to prove theorem [ thm : crate.c ] .      _",
    "proof of theorem [ thm : crate.c ] .",
    "_ consider @xmath385 as independent rademacher random variables , _",
    "i.e. _ , independent @xmath386-valued random variables with equal probability of taking either value .",
    "given @xmath387 , @xmath388 , @xmath389 and @xmath390 , denote @xmath391 and for any @xmath38 , @xmath392^{2n_s};\\nonumber\\\\",
    "\\overrightarrow{f}({\\bf z}_{1}^{2n_t}):=&\\big(f({\\bf z}'_1),\\cdots , f({\\bf z}'_{n_t}),f({\\bf z}_1),\\cdots , f({\\bf z}_{n_t})\\big)\\in[a , b]^{2n_t}.\\end{aligned}\\ ] ] we also denote @xmath393^{4n_sn_t}.\\end{aligned}\\ ] ]    according to , and theorem [ thm : sym.c ] , for any @xmath51 and given an arbitrary @xmath361 , we have for any @xmath394 with @xmath395 , @xmath396 given a @xmath51 , fix a realization of @xmath397 and let @xmath313 be a @xmath314-radius cover of @xmath37 with respect to the @xmath117 norm .",
    "since @xmath37 is composed of bounded functions with the range @xmath128 $ ] , we assume that the same holds for any @xmath316 . if @xmath317 is the function that achieves the following supremum @xmath398 there must be an @xmath319 that satisfies that @xmath399 and meanwhile , @xmath400 therefore , for the realization of @xmath397 , we arrive at @xmath401    moreover , we denote the event @xmath402 and let @xmath324 be the characteristic function of the event @xmath325 .",
    "by fubini s theorem , we have @xmath403    fix a realization of @xmath397 again . according to , , and theorem [ thm : dineq.c ] , for any @xmath51 and given an arbitrary @xmath299 , we have for any @xmath366 , @xmath404 where @xmath405 .",
    "the combination of , , and leads to the following result : for any @xmath51 and given an arbitrary @xmath206 , we have for any @xmath366 , @xmath406    according to , letting @xmath407 we have given an arbitrary @xmath206 and for any @xmath207 , with probability at least @xmath132 , @xmath408 this completes the proof .",
    "in this appendix , we will prove theorem [ thm : rb.rade ] and theorem [ thm : rb.rade.c ] . in order to achieve the proofs",
    ", we need to generalize the classical mcdiarmid s inequality ( see * ? ? ?",
    "* theorem 6 ) to a more general setting where independent random variables can independently take values from different domains .",
    "the following is the classical mcdiarmid s inequality that is one of the most frequently used deviation inequalities in statistical learning theory and has been widely used to obtain generalization bounds based on the rademacher complexity under the assumption of same distribution ( see * ? ? ?",
    "* theorem 6 ) .",
    "[ thm : mcdiarmid0 ] let @xmath410 be @xmath411 independent random variables taking value from the domain @xmath121 .",
    "assume that the function @xmath412 satisfies the condition of bounded difference : for all @xmath413 , @xmath414 then , for any @xmath90 @xmath415    as shown in theorem [ thm : mcdiarmid0 ] , the classical mcdiarmid s inequality is valid under the condition that random variables @xmath410 are independent and drawn from the same domain .",
    "next , we generalize this inequality to a more general setting , where the independent random variables can take values from different domains .",
    "[ thm : mcdiarmid ] given independent domains @xmath12 ( @xmath13 ) , for any @xmath13 , let @xmath416 be @xmath417 independent random variables taking values from the domain @xmath12 .",
    "assume that the function @xmath418 satisfies the condition of bounded difference : for all @xmath13 and @xmath100 , @xmath419 then , for any @xmath90 @xmath420      denote for any @xmath13 and @xmath100 , @xmath424 it follows from the definition of that @xmath425 and thus results in @xmath426 moreover , by the law of iterated expectation , we also have for any @xmath13 and @xmath100 @xmath427 according to hoeffding inequality ( see * ? ? ?",
    "* ) , given an @xmath259 , the condition leads to for any @xmath13 and @xmath100 , @xmath428    subsequently , according to markov s inequality , , , and , we have for any @xmath259 , @xmath429 the above bound is minimized by setting @xmath430 and its minimum value is @xmath431 this completes the proof . @xmath267        * proof of theorem [ thm : rb.rade ] * assume that @xmath37 is a function class @xmath37 consisting of bounded functions with the range @xmath128 $ ] . let sample sets @xmath96 be drawn from multiple sources @xmath12 ( @xmath13 ) , respectively . given a choice of @xmath150^{k}$ ] with @xmath19 ,",
    "denote @xmath432 by , we have @xmath433 where @xmath434 .",
    "therefore , it is clear that such @xmath435 satisfies the condition of bounded difference with @xmath436 for all @xmath13 and @xmath100 .",
    "thus , according to theorem [ thm : mcdiarmid ] , we have for any @xmath90 , @xmath437",
    "which can be equivalently rewritten as with probability at least @xmath132 , @xmath438     } \\nonumber\\\\     & + \\sqrt{\\sum_{k=1}^k\\frac{(b - a)^2w_k^2\\ln(1/\\epsilon)}{2n_k}}.\\end{aligned}\\ ] ]              * proof of theorem [ thm : rb.rade.c ] * we only consider the result of theorem [ thm : mcdiarmid ] in a special setting of @xmath161 , @xmath443 , @xmath444 , @xmath445 and @xmath446 .",
    "given a choice of @xmath51 , denote @xmath447 by , we have @xmath448"
  ],
  "abstract_text": [
    "<S> in this paper , we provide a new framework to obtain the generalization bounds of the learning process for domain adaptation , and then apply the derived bounds to analyze the asymptotical convergence of the learning process . without loss of generality , we consider two kinds of representative domain adaptation : one is with multiple sources and the other is combining source and target data . + </S>",
    "<S> in particular , we use the integral probability metric to measure the difference between two domains . for either kind of domain adaptation </S>",
    "<S> , we develop a related hoeffding - type deviation inequality and a symmetrization inequality to achieve the corresponding generalization bound based on the uniform entropy number . </S>",
    "<S> we also generalized the classical mcdiarmid s inequality to a more general setting where independent random variables can take values from different domains . by using this inequality , </S>",
    "<S> we then obtain generalization bounds based on the rademacher complexity . </S>",
    "<S> afterwards , we analyze the asymptotic convergence and the rate of convergence of the learning process for such kind of domain adaptation . meanwhile , we discuss the factors that affect the asymptotic behavior of the learning process and the numerical experiments support our theoretical findings as well . </S>"
  ]
}