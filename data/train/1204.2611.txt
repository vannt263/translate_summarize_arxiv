{
  "article_text": [
    "since many systems in science and engineering are approximately linear , linear inverse problems have attracted great attention in the signal processing community . an input signal @xmath0",
    "is recorded via a linear operator under additive noise : @xmath1 where @xmath2 is an @xmath3 matrix and @xmath4 denotes the noise .",
    "the goal is to estimate @xmath5 from the measurements @xmath6 given knowledge of @xmath2 and a model for the noise @xmath7 .",
    "when @xmath8 , the setup is known as compressed sensing ( cs ) and the estimation problem is commonly referred to as recovery or reconstruction ; by posing a sparsity or compressibility requirement on the signal and using this requirement as a prior during recovery , it is indeed possible to accurately estimate @xmath5 from @xmath6  @xcite . on the other hand , we might need more measurements than the signal length when the signal is dense or the noise is substantial .",
    "wu and verd  @xcite have shown that independent and identically distributed ( i.i.d . )",
    "gaussian sensing matrices achieve the same phase - transition threshold as optimal nonlinear encoding , for any discrete continuous mixture .",
    "hence , in cs the acquisition can be designed independently of the particular signal prior through the use of randomized gaussian matrices @xmath2 .",
    "nevertheless , the majority of ( if not all ) existing recovery algorithms require knowledge of the sparsity structure of @xmath5 , i.e. , the choice of a _ sparsifying transform _ @xmath9 that renders a sparse coefficient vector @xmath10 for the signal .",
    "the large majority of recovery algorithms pose a sparsity prior on the signal @xmath5 , e.g. ,  @xcite .",
    "a second , separate class of bayesian cs recovery algorithms poses a probabilistic prior for the coefficients of @xmath5 in a known transform domain  @xcite .",
    "given a probabilistic model , some related message passing approaches learn the parameters of the signal model and achieve the minimum mean squared error ( mmse ) in some settings ; examples include em - gm - amp - mos  @xcite , turbogamp  @xcite , and amp - mixd  @xcite . as a third alternative , complexity - penalized least square methods  @xcite can use arbitrary prior information on the signal model and provide analytical guarantees , but are only computationally efficient for specific signal models , such as the independent - entry laplacian model  @xcite .",
    "for example , donoho et al .",
    "@xcite relies on kolmogorov complexity , which can not be computed  @xcite . as a fourth alternative",
    ", there exist algorithms that can formulate dictionaries that yield sparse representations for the signals of interest when a large amount of training data is available  @xcite . when the signal is non - i.i.d .",
    ", existing algorithms require either prior knowledge of the probabilistic model  @xcite or the use of training data  @xcite .    in certain cases , one might not be certain about the structure or statistics of the source prior to recovery .",
    "uncertainty about such structure may result in a sub - optimal choice of the sparsifying transform @xmath9 , yielding a coefficient vector @xmath11 that requires more measurements to achieve reasonable estimation quality ; uncertainty about the statistics of the source will make it difficult to select a prior or model for bayesian algorithms .",
    "thus , it would be desirable to formulate algorithms to estimate @xmath5 that are agnostic to the particular statistics of the signal . therefore , we shift our focus from the standard sparsity or compressibility priors to _ universal _ priors  @xcite .",
    "such concepts have been previously leveraged in the kolmogorov sampler universal denoising algorithm  @xcite , which minimizes kolmogorov complexity  @xcite .",
    "related approaches based on minimum description length ( mdl )  @xcite minimize the complexity of the estimated signal with respect to some class of parametric sources .",
    "unfortunately , mdl can provide a suitable algorithmic recovery framework primarily for parametric sources  @xcite .",
    "alternative approaches for non - parametric sources based on kolmogorov complexity are not computable in practice  @xcite . to address this computational problem , we confine our attention to stationary ergodic sources and develop an algorithmic framework for universal signal estimation in cs systems , which can be applied to general linear inverse problems where more measurements might be needed .",
    "our framework leverages the fact that for stationary ergodic sources , both the per - symbol empirical entropy and kolmogorov complexity converge asymptotically almost surely to the entropy rate of the source  @xcite .",
    "we aim to minimize the empirical entropy ; our minimization is regularized by introducing a log likelihood for the noise model , which is equivalent to the standard least squares under additive white gaussian noise .",
    "other noise distributions are readily supported .",
    "we make several contributions toward our universal cs framework .",
    "first , we apply a specific quantization grid to a maximum _ a posteriori _",
    "( map ) estimator driven by a universal prior , providing a finite - computation universal estimation scheme ; our scheme can also be applied to general linear inverse problems where more measurements might be needed .",
    "second , we propose a recovery algorithm based on markov chain monte carlo ( mcmc )  @xcite to approximate this estimation procedure .",
    "third , we prove that for a sufficiently large number of iterations the output of our mcmc recovery algorithm converges to the correct map estimate .",
    "fourth , we identify computational bottlenecks in the implementation of our mcmc estimator and show approaches to reduce their complexity .",
    "fifth , we develop an adaptive quantization scheme that tailors a set of reproduction levels to minimize the quantization error within the mcmc iterations and that provides an accelerated implementation .",
    "sixth , we propose a framework that adaptively adjusts the cardinality ( or size for short ) of the adaptive quantizer to match the complexity of the input signal , in order to further reduce the quantization error and computation .",
    "we note in passing that averaging over the outputs of different runs of the same signal with the same measurements will yield lower mean squared error ( mse ) for our proposed algorithm .    to showcase the potential of our universal estimation approach , fig .",
    "[ fig : m4 ] illustrates recovery results from gaussian measurement matrices for a four - state markov source of length @xmath12 that generates the pattern @xmath13 with 3% errors in state transitions , resulting in the signal switching from @xmath14 to @xmath15 or vice versa either too early or too late ( see section  [ sec : numerical ] for details on the simulation setup ) .",
    "note that the reconstruction algorithm does not know that this source is a binary source . while it is well known that sparsity - promoting recovery algorithms  @xcite can recover sparse sources from linear measurements , the aforementioned switching source is not sparse in conventional sparsifying bases ( e.g. , fourier , wavelet , and discrete cosine transforms ) , rendering such sparsifying transforms not applicable .",
    "signals generated by this markov source can be sparsified using an averaging analysis matrix  @xcite whose diagonal and first three lower sub - diagonals are filled with @xmath15 , and all other entries are @xmath16 ; this transform yields @xmath17 non - zeros in the sparse coefficient vector . however , even if this matrix is known _ a priori _ , existing algorithms based on analysis sparsity  @xcite do not perform satisfactorily , yielding mean signal - to - distortion ratios below @xmath18  db ( cf . section  [ sec : numerical ] ) .",
    "in contrast , our size- and level - adaptive mcmc ( sla - mcmc , cf .",
    "section  [ sec : adaptive ] ) algorithm estimates this source with high fidelity when a moderate number of measurements @xmath19 are available .",
    "we provide more experimental results in section  [ sec : numerical ] to show that the performance of mcmc is comparable to and in many cases better than existing algorithms .     for different signal - to - noise ratio ( snr ) values ( @xmath12 ) .",
    "existing cs algorithms fail at reconstructing this signal , because it is not sparse . _",
    "[ fig : m4],width=302 ]    this paper is organized as follows .",
    "section  [ sec : setting ] provides background content .",
    "section  [ sec : theory ] overviews map estimation , quantization , and introduces universal map estimation .",
    "section  [ sec : mcmc ] formulates an initial mcmc algorithm for universal map estimation , section  [ sec : adaptive ] describes several improvements to this initial algorithm , and section  [ sec : numerical ] presents experimental results .",
    "we conclude in section  [ sec : conclusions ] .",
    "the proof of our main theoretical result appears in the appendix .",
    "consider the noisy measurement setup via a linear operator ( [ eq : def_y ] ) .",
    "the input signal @xmath20 is generated by a stationary ergodic source @xmath21 , and must be estimated from @xmath6 and @xmath2 . _",
    "the distribution @xmath22 that generates @xmath5 is unknown . _",
    "the matrix @xmath23 has i.i.d .",
    "gaussian entries , @xmath24 .",
    "is not dependent on a particular choice for the matrix @xmath2 . ] these moments ensure that the columns of the matrix have unit norm on average . for concrete analysis , we assume that the noise @xmath25 is i.i.d .",
    "gaussian , with mean zero and known variance @xmath26 for simplicity .",
    "we focus on the setting where @xmath27 and the aspect ratio is positive : @xmath28 similar settings have been discussed in the literature  @xcite . when @xmath29 , this setup is known as cs ; otherwise , it is a general linear inverse problem setting .",
    "since @xmath5 is generated by an unknown source , we must search for an estimation mechanism that is agnostic to the specific distribution @xmath22 .      for a scalar channel with a discrete - valued signal @xmath5 , e.g. ,",
    "@xmath2 is an identity matrix and @xmath30 , donoho proposed the kolmogorov sampler ( ks ) for denoising  @xcite , @xmath31 where @xmath32 denotes the kolmogorov complexity of @xmath5 , defined as the length of the shortest input to a turing machine  @xcite that generates the output @xmath5 and then halts , , kolmogorov complexity ( kc ) can be approximated using a fine quantizer .",
    "note that the algorithm developed in this paper uses a coarse quantizer and does not rely on kc due to the absence of a feasible method for its computation  @xcite ( cf .",
    "section  [ sec : adaptive ] ) . ] and @xmath33 controls for the presence of noise .",
    "it can be shown that @xmath32 asymptotically captures the statistics of the stationary ergodic source @xmath34 , and the per - symbol complexity achieves the entropy rate @xmath35 , i.e. , @xmath36 almost surely  ( * ? ? ?",
    "* , theorem  7.3.1 ) . noting that universal lossless compression algorithms  @xcite achieve the entropy rate for any discrete - valued finite state machine source @xmath34",
    ", we see that these algorithms achieve the per - symbol kolmogorov complexity almost surely .",
    "donoho et al . expanded ks to the linear cs measurement setting @xmath37 but did not consider measurement noise  @xcite .",
    "recent papers by jalali and coauthors  @xcite , which appeared simultaneously with our work  @xcite , provide an analysis of a modified ks suitable for measurements corrupted by noise of bounded magnitude .",
    "inspired by donoho et al .",
    "@xcite , we estimate @xmath5 from noisy measurements @xmath6 using the empirical entropy as a proxy for the kolmogorov complexity ( cf . section  [ sec : compressor ] ) .",
    "separate notions of complexity - penalized least squares have also been shown to be well suited for denoising and cs recovery  @xcite .",
    "for example , minimum description length ( mdl )  @xcite provides a framework composed of classes of models for which the signal complexity can be defined sharply . in general ,",
    "complexity - penalized least square approaches can yield mdl - flavored cs recovery algorithms that are adaptive to parametric classes of sources  @xcite .",
    "an alternative universal denoising approach computes the universal conditional expectation of the signal  @xcite .",
    "this section briefly reviews map estimation and then applies it over a quantization grid , where a universal prior is used for the signal . additionally , we provide a conjecture for the mse achieved by our universal map scheme .      in this subsection , we assume for exposition purposes that we know the signal statistics @xmath22 .",
    "given the measurements @xmath6 , the map estimator for @xmath5 has the form @xmath38 because @xmath7 is i.i.d .",
    "gaussian with mean zero and known variance @xmath26 , @xmath39 , where @xmath40 and @xmath41 are constants , and @xmath42 denotes the euclidean norm .",
    "norm to an @xmath43 norm and adjust @xmath44 and @xmath45 accordingly . ]",
    "plugging into ( [ eq : map ] ) and taking log likelihoods , we obtain @xmath46 , where @xmath47 denotes the objective function ( risk ) @xmath48 ; our ideal risk would be @xmath49 .    instead of performing continuous - valued map estimation ,",
    "we optimize for the map in the discretized domain @xmath50 , with @xmath51 being defined as follows . adapting the approach of baron and weissman  @xcite",
    ", we define the set of data - independent reproduction levels for quantizing @xmath5 as @xmath52 where @xmath53 . as @xmath54 increases",
    ", @xmath51 will quantize @xmath5 to a greater resolution .",
    "these reproduction levels simplify the estimation problem from continuous to discrete .",
    "having discussed our reproduction levels in the set @xmath51 , we provide a technical condition on boundedness of the signal .",
    "[ cond : tech1 ] we require that the probability density @xmath22 has bounded support , i.e. , there exists @xmath55 $ ] such that @xmath56 for @xmath57 .",
    "a limitation of the data - independent reproduction level set ( [ eq : def : replevels ] ) is that @xmath51 has infinite cardinality ( or size for short ) .",
    "thanks to condition  [ cond : tech1 ] , for each value of @xmath58 there exists a constant @xmath59 such that a finite set of reproduction levels @xmath60 will quantize the range of values @xmath61 to the same accuracy as that of ( [ eq : def : replevels ] ) .",
    "we call @xmath62 the _ reproduction alphabet _ , and each element in it a ( _ reproduction _ ) _ level_. this finite quantizer reduces the complexity of the estimation problem from infinite to combinatorial .",
    "in fact , @xmath63 $ ] under condition  [ cond : tech1 ] . therefore , for all @xmath64 and sufficiently large @xmath54 , this set of levels will cover the range @xmath65 $ ] .",
    "the resulting reduction in complexity is due to the structure in @xmath66 and independent of the particular statistics of the source @xmath34 .    now that we have set up a quantization grid @xmath67 for @xmath5 , we convert the distribution @xmath22 to a probability mass function ( pmf ) @xmath68 over @xmath67 .",
    "let @xmath69 , and define the pmf @xmath70 as @xmath71 .",
    "then @xmath72 gives the map estimate of @xmath5 over @xmath67 .",
    "as @xmath54 increases , @xmath68 will approximate @xmath22 more closely under ( [ eq : def : replevels2 ] ) .",
    "we now describe a universal estimator for cs over a quantized grid .",
    "consider a prior @xmath73 that might involve kolmogorov complexity  @xcite , e.g. , @xmath74 , or mdl complexity with respect to some class of parametric sources  @xcite .",
    "we call @xmath73 a _ universal prior _ if it has the fortuitous property that for every stationary ergodic source @xmath34 and fixed @xmath75 , there exists some minimum @xmath76 such that @xmath77 for all @xmath78 and @xmath79  @xcite .",
    "we optimize over an objective function that incorporates @xmath73 and the presence of additive white gaussian noise in the measurements : @xmath80 resulting in corresponds to a lagrangian relaxation of the approach studied in  @xcite . ]",
    "our universal map estimator does not require @xmath29 , and @xmath82 can be used in general linear inverse problems .",
    "donoho  @xcite showed for the scalar channel @xmath30 that : ( @xmath83 ) the kolmogorov sampler @xmath84 ( [ eq : x_ks ] ) is drawn from the posterior distribution @xmath85 ; and ( @xmath86 ) the mse of this estimate @xmath87 $ ] is no greater than twice the mmse .",
    "based on this result , which requires a large reproduction alphabet , we now present a conjecture on the quality of the estimation @xmath88 .",
    "our conjecture is based on observing that @xmath88 also samples from the posterior distribution ; some experimental evidence to assess this conjecture is presented in figs .",
    "[ fig : ber ] and  [ fig : sparsel ] .",
    "[ conj : double_mmse ] assume that @xmath89 is an i.i.d .",
    "gaussian measurement matrix where each entry has mean zero and variance @xmath90 .",
    "suppose that condition  [ cond : tech1 ] holds , the aspect ratio @xmath91 in  ( [ eq : delta ] ) , and the noise @xmath25 is i.i.d .",
    "zero - mean gaussian with finite variance .",
    "then for all @xmath92 , the mean squared error of the universal map estimator @xmath88 satisfies @xmath93}{n } < \\frac{2 e_{x , z,\\phi}\\left[\\|x - e_{x}[x|y,\\phi]\\|^2\\right]}{n } + \\epsilon\\ ] ] for sufficiently large @xmath54 .",
    "although the results of the previous section are theoretically appealing , a brute force optimization of @xmath82 is computationally intractable .",
    "instead , we propose an algorithmic approach based on mcmc methods  @xcite .",
    "our approach is reminiscent of the framework for lossy data compression  @xcite .",
    "we propose a universal lossless compression formulation following the conventions of weissman and coauthors  @xcite .",
    "we refer to the estimate as @xmath94 in our algorithm .",
    "our goal is to characterize @xmath95 , cf .",
    "( [ eq : psidef ] ) .",
    "although we are inspired by the kolmogorov sampler approach  @xcite , kc can not be computed  @xcite , and we instead use empirical entropy . for stationary ergodic sources , the empirical entropy converges to the per - symbol entropy rate almost surely  @xcite .    to define the empirical entropy",
    ", we first define the empirical symbol counts : @xmath96 \\triangleq | \\",
    "{ i \\in [ \\q+1,n ] : w_{i-\\q}^{i-1}=\\alpha , w_i=\\beta \\",
    "} | , \\label{eq : nq}\\ ] ] where @xmath97 is the context depth  @xcite , @xmath98 , @xmath99 , @xmath100 is the @xmath101 symbol of @xmath94 , and @xmath102 is the string comprising symbols @xmath83 through @xmath103 within @xmath94 .",
    "we now define the order @xmath97 conditional empirical probability for the context @xmath104 as @xmath105 \\triangleq   \\frac {   n_\\q(w,\\alpha)[\\beta ] } { \\sum_{\\beta ' \\in \\replevels_f } n_\\q(w,\\alpha)[\\beta ' ] } , \\ ] ] and the order @xmath97 conditional empirical entropy , @xmath106 \\log_2\\left ( \\mathbb{p}_\\q(w,\\alpha)[\\beta ] \\right),\\ ] ] where the sum is only over non - zero counts and probabilities .",
    "allowing the context depth @xmath107 to grow slowly with @xmath54 , various universal compression algorithms can achieve the empirical entropy @xmath108 asymptotically  @xcite . on the other hand , no compressor can outperform the entropy rate .",
    "additionally , for large @xmath54 , the empirical symbol counts with context depth @xmath97 provide a sufficiently precise characterization of the source statistics .",
    "therefore , @xmath109 provides a concise approximation to the per - symbol coding length of a universal compressor .",
    "having approximated the coding length , we now describe how to optimize our objective function .",
    "we define the energy @xmath110 in an analogous manner to @xmath111 ( [ eq : psidef ] ) , using @xmath112 as our universal coding length : @xmath113 where @xmath114 .",
    "the minimization of this energy is analogous to minimizing @xmath111 .",
    "ideally , our goal is to compute the globally minimum energy solution @xmath115 .",
    "we use a stochastic mcmc relaxation  @xcite to achieve the globally minimum solution in the limit of infinite computation . to assist the reader in appreciating how mcmc is used to compute @xmath116 , we include pseudocode for our approach in algorithm  [ alg : mcmc ]",
    ". the algorithm , called basic mcmc ( b - mcmc ) , will be used as a building block for our latter algorithms  [ alg : mcmcal ] and  3 in section  [ sec : adaptive ] .",
    "the initial estimate @xmath94 is obtained by quantizing the _ initial point _",
    "@xmath117 to @xmath118 .",
    "the initial point @xmath119 could be the output of any signal reconstruction algorithm , and because @xmath119 is a preliminary estimate of the signal that does not require high fidelity , we let @xmath120 for simplicity , where @xmath121 denotes transpose . we refer to the processing of a single entry of @xmath94 as an iteration and group the processing of all entries of @xmath94 , randomly permuted , into super - iterations .",
    "the boltzmann pmf is defined as @xmath122 where @xmath123 is inversely related to the temperature in simulated annealing and @xmath124 is a normalization constant .",
    "mcmc samples from the boltzmann pmf ( [ eq : def_boltzmann ] ) using a _ gibbs sampler _ : in each iteration , a single element @xmath125 is generated while the rest of @xmath94 , @xmath126 , remains unchanged .",
    "we denote by @xmath127 the concatenation of the initial portion of the output vector @xmath128 , the symbol @xmath98 , and the latter portion of the output @xmath129 .",
    "the gibbs sampler updates @xmath125 by resampling from the pmf : @xmath130 \\right ) } \\label{eqn : gibbs},\\end{aligned}\\ ] ] where @xmath131 is the change in empirical entropy @xmath112 ( [ eq : def : h ] ) when @xmath132 is replaced by @xmath133 , and @xmath134 is the change in @xmath135 when @xmath132 is replaced by @xmath133 . the maximum change in the energy within an iteration of algorithm  [ alg : mcmc ]",
    "is then bounded by @xmath136 note that @xmath5 is assumed bounded ( cf",
    ".  condition  [ cond : tech1 ] ) so that ( [ eq : deltad][eq : deltaq ] ) are bounded as well .    in mcmc , the space @xmath137 is analogous to a statistical mechanical system , and at low temperatures the system tends toward low energies .",
    "therefore , during the execution of the algorithm , we set a sequence of decreasing temperatures that takes into account the maximum change given in ( [ eq : deltaq ] ) : @xmath138where @xmath139 is a temperature offset . at low temperatures , i.e. , large @xmath140 ,",
    "a small difference in energy @xmath110 drives a big difference in probability , cf .",
    "( [ eq : def_boltzmann ] ) .",
    "therefore , we begin at a high temperature where the gibbs sampler can freely move around @xmath118 . as the temperature",
    "is reduced , the pmf becomes more sensitive to changes in energy ( [ eq : def_boltzmann ] ) , and the trend toward @xmath94 with lower energy grows stronger . in each iteration , the gibbs sampler modifies @xmath125 in a random manner that resembles heat bath concepts in statistical mechanics .",
    "although mcmc could sink into a local minimum , geman and geman  @xcite proved that if we decrease the temperature according to ( [ eq : st ] ) , then the randomness of gibbs sampling will eventually drive mcmc out of the local minimum toward the globally optimal @xmath82 . in order to help b - mcmc approach the global minimum with reasonable runtime",
    ", we will refine b - mcmc in section  [ sec : adaptive ] .    :",
    "initial estimate @xmath94 , reproduction alphabet @xmath62 , noise variance @xmath26 , number of super  iterations",
    "@xmath141 , temperature constant @xmath142 , and context depth @xmath143 compute @xmath144,~\\forall~\\alpha \\in ( \\replevels_f)^\\q$ ] , @xmath98 @xmath145 draw permutation @xmath146 at random let  @xmath147 be component @xmath148 in permutation compute @xmath149 [ sudo : fixedr_deltah ] compute @xmath150 [ sudo : fixedr_deltad ] compute @xmath151 [ sudo : fixedr : fs ] generate @xmath125 using @xmath152 update @xmath144,~\\forall~\\alpha \\in ( \\replevels_f)^\\q$ ] , @xmath98 [ sudo : fixedr : update ]  return approximation @xmath94 of @xmath88    the following theorem is proven in  [ ap : th : conv ] , following the framework espoused by jalali and weissman  @xcite .",
    "let @xmath34 be a stationary ergodic source that obeys condition  [ cond : tech1 ] .",
    "then the outcome @xmath153 of algorithm  [ alg : mcmc ] in the limit of an infinite number of super - iterations @xmath141 obeys @xmath154 .",
    "[ th : conv ]    theorem  [ th : conv ] shows that algorithm  [ alg : mcmc ] matches the best - possible performance of the universal map estimator as measured by the objective function @xmath155 , which should yield an mse that is twice the mmse ( cf .",
    "conjecture  [ conj : double_mmse ] ) . to gain some insight about the convergence process of mcmc , we focus on a fixed arbitrary sub - optimal sequence @xmath137 .",
    "suppose that at super - iteration @xmath156 the energy for the algorithm s output @xmath110 has converged to the steady state ( see  [ ap : th : conv ] for details on convergence ) .",
    "we can then focus on the probability ratio @xmath157 ; @xmath158 because @xmath159 is the global minimum and has the largest boltzmann probability over all @xmath137 , whereas @xmath94 is sub - optimal .",
    "we then consider the same sequence @xmath94 at super - iteration @xmath160 ; the inverse temperature is @xmath161 and the corresponding ratio at super - iteration @xmath160 is ( cf .",
    "( [ eq : def_boltzmann ] ) ) @xmath162 that is , between super - iterations @xmath156 and @xmath160 the probability ratio @xmath163 is also squared , and the gibbs sampler is less likely to generate samples that differ from @xmath159 .",
    "we infer from this argument that the probability concentration of our algorithm around the globally optimal @xmath159 is linear in the number of super - iterations .",
    "studying the pseudocode of algorithm  [ alg : mcmc ] , we recognize that lines  [ sudo : fixedr_deltah][sudo : fixedr : fs ] must be implemented efficiently , as they run @xmath164 times .",
    "lines  [ sudo : fixedr_deltah ] and  [ sudo : fixedr_deltad ] are especially challenging .    for line  [ sudo : fixedr_deltah ] ,",
    "a naive update of @xmath165 has complexity @xmath166 , cf .",
    "( [ eq : def : h ] ) . to address this problem , jalali and weissman  @xcite recompute the empirical conditional entropy in @xmath167 time only for the @xmath168 contexts whose",
    "corresponding counts are modified  @xcite .",
    "the same approach can be used in line  [ sudo : fixedr : update ] , again reducing computation from @xmath166 to @xmath167 .",
    "some straightforward algebra allows us to convert line  [ sudo : fixedr_deltad ] to a form that requires aggregate runtime of @xmath169 .",
    "combined with the computation for line  [ sudo : fixedr_deltah ] , and since @xmath170 ( because @xmath171 , and @xmath172 ) in practice , the entire runtime of our algorithm is @xmath173 .",
    "the practical value of algorithm  [ alg : mcmc ] may be reduced due to its high computational cost , dictated by the number of super - iterations @xmath141 required for convergence to @xmath159 and the large size of the reproduction alphabet .",
    "nonetheless , algorithm  [ alg : mcmc ] provides a starting point toward further performance gains of more practical algorithms for computing @xmath159 , which are presented in section  [ sec : adaptive ] . furthermore",
    ", our experiments in section  [ sec : numerical ] will show that the performance of the algorithm of section  [ sec : adaptive ] is comparable to and in many cases better than existing algorithms .",
    "while algorithm  [ alg : mcmc ] is a first step toward universal signal estimation in cs , @xmath54 must be large enough to ensure that @xmath62 quantizes a broad enough range of values of @xmath174 finely enough to represent the estimate @xmath159 well . for large @xmath54",
    ", the estimation performance using the reproduction alphabet  ( [ eq : def : replevels2 ] ) could suffer from high computational complexity . on the other hand , for small @xmath54 the number of reproduction levels employed is insufficient to obtain acceptable performance .",
    "nevertheless , using an excessive number of levels will slow down the convergence .",
    "therefore , in this section , we explore techniques that tailor the reproduction alphabet adaptively to the signal being observed .",
    "to estimate better with finite @xmath54 , we utilize reproduction levels that are _ adaptive _ instead of the fixed levels in @xmath62 .",
    "to do so , instead of @xmath175 , we optimize over a sequence @xmath176 , where @xmath177 and @xmath178 denotes the size .",
    "the new reproduction alphabet @xmath179 does not directly correspond to real numbers .",
    "instead , there is an adaptive mapping @xmath180 , and the reproduction levels are @xmath181 .",
    "therefore , we call @xmath179 the _ adaptive _ reproduction alphabet . since the mapping @xmath182 is one - to - one , we also refer to @xmath179 as reproduction levels . considering the energy function ( [ eq : mcmc_energy ] ) , we now compute the empirical symbol counts @xmath183 $ ] , order @xmath97 conditional empirical probabilities @xmath184 $ ] , and order @xmath97 conditional empirical entropy @xmath185 using @xmath176 , @xmath186 , and @xmath187 , cf .",
    "( [ eq : nq ] ) , ( [ eq : def : pcond ] ) , and  ( [ eq : def : h ] ) .",
    "similarly , we use @xmath188 instead of @xmath189 , where @xmath190 is the straightforward vector extension of @xmath182 .",
    "these modifications yield an adaptive energy function @xmath191 .",
    "we choose @xmath192 to optimize for minimum squared error , @xmath193_m)^2\\right],\\end{aligned}\\ ] ] where @xmath194_m$ ] denotes the @xmath195 entry of the vector @xmath196 .",
    "the optimal mapping depends entirely on @xmath6 , @xmath2 , and @xmath197 . from a coding perspective",
    ", describing @xmath198 requires @xmath199 bits for @xmath197 and @xmath200 bits for @xmath192 to match the resolution of the non - adaptive @xmath62 , with @xmath201 an arbitrary constant  @xcite .",
    "the resulting coding length defines our universal prior .    *",
    "optimization of reproduction levels : * we now describe the optimization procedure for @xmath192 , which must be computationally efficient .",
    "write @xmath202 where @xmath203 is the entry of @xmath2 at row @xmath204 and column @xmath147 .",
    "for @xmath205 to be minimum , we need zero - valued derivatives @xmath206 where @xmath207 is the indicator function for event @xmath208 .",
    "define the location sets @xmath209 for each @xmath187 , and rewrite the derivatives of @xmath205 , @xmath210 let the per - character sum column values be @xmath211 for each @xmath212 and @xmath213 .",
    "we desire the derivatives to be zero , cf .",
    "( [ eq : derivative2 ] ) : @xmath214 thus , the system of equations must be satisfied , @xmath215 for each @xmath187 .",
    "consider now the right hand side , @xmath216 for each @xmath187 .",
    "the system of equations can be described in matrix form @xmath217 } ^{\\omega } \\overbrace { \\left[\\begin{array}{c } \\map(\\beta_1)\\\\ \\vdots\\\\ \\map(\\beta_{|\\alphabet| } ) \\end{array}\\right ] } ^{\\map(\\z)}= \\overbrace { \\left[\\begin{array}{c } \\sum_{m=1}^m y_m \\mu_{m\\beta_1}\\\\ \\vdots\\\\ \\sum_{m=1}^m y_m \\mu_{m\\beta_{|\\alphabet| } } \\end{array } \\right ] } ^{\\theta}.\\end{aligned}\\ ] ] note that by writing @xmath218 as a matrix with entries indexed by row @xmath204 and column @xmath219 given by ( [ eq : mu_def ] ) , we can write @xmath220 as a gram matrix , @xmath221 , and we also have @xmath222 , cf .",
    "( [ eq : system_equation ] ) .",
    "the optimal @xmath182 can be computed as a @xmath223 vector @xmath224 if @xmath225 is invertible .",
    "we note in passing that numerical stability can be improved by regularizing @xmath220 .",
    "note also that @xmath226 which can be computed in @xmath227 time instead of @xmath228 .    *",
    "computational complexity : * pseudocode for level - adaptive mcmc ( l - mcmc ) appears in algorithm  [ alg : mcmcal ] , which resembles algorithm  [ alg : mcmc ] .",
    "the initial mapping @xmath182 is inherited from a quantization of the initial point @xmath119 , @xmath229 ( @xmath139 takes different values in section  [ sec : adaptive_size ] ) , and other minor differences between b - mcmc and l - mcmc appear in lines marked by asterisks .",
    "we discuss computational requirements for each line of the pseudocode that is run within the inner loop .",
    "* line  [ algo2:deltah ] can be computed in @xmath230 time ( see discussion of line  [ sudo : fixedr_deltah ] of b - mcmc in section  [ sec : b - mcmc_complexity ] ) .",
    "* line  [ algo2:mu ] updates @xmath231 for @xmath232 in @xmath233 time .",
    "* line  [ algo2:omega ] updates @xmath220 .",
    "because we only need to update @xmath234 columns and @xmath234 rows , each such column and row contains @xmath235 entries , and each entry is a sum over @xmath233 terms , we need @xmath227 time .",
    "* line  [ algo2:map_opt ] requires inverting @xmath220 in @xmath236 time .",
    "* line  [ algo2:ell2 ] requires @xmath227 time , cf .",
    "( [ eqn : compute_ell2 ] ) .",
    "* line  [ algo2:distribution ] requires @xmath235 time .    in practice",
    "we typically have @xmath237 , and so the aggregate complexity is @xmath238 , which is greater than the computational complexity of algorithm  [ alg : mcmc ] by a factor of @xmath235 .    :",
    "initial mapping @xmath182 , sequence @xmath197 , adaptive alphabet @xmath179 , noise variance @xmath26 , number of super - iterations @xmath141 , temperature constant @xmath142 , context depth @xmath143 , and temperature offset @xmath139 compute @xmath183,~\\forall~\\alpha \\in \\z^\\q$ ] , @xmath187 initialize @xmath220[algo2:init_omega ] @xmath239 draw permutation @xmath146 at random let  @xmath147 be component @xmath148 in permutation compute @xmath240 [ algo2:deltah ] compute @xmath241 [ algo2:mu ] update @xmath220 [ algo2:omega ] compute @xmath192 [ algo2:map_opt ] compute @xmath242 [ algo2:ell2 ] compute @xmath243 [ algo2:distribution ] @xmath244 [ algo2:save_sequence ] generate @xmath245 using @xmath246 update @xmath247 $ ] at @xmath168 relevant locations update @xmath248 , @xmath249[algo2:update_mu ] update @xmath220 [ algo2:update_omega ] : return approximation @xmath190 of @xmath250 , and temperature offset @xmath251          while algorithm  [ alg : mcmcal ] adaptively maps @xmath197 to @xmath252 , the signal estimation quality heavily depends on @xmath253 . denote the true alphabet of the signal by @xmath254 ; if the signal is continuous - valued , then @xmath255 is infinite .",
    "ideally we want to employ as many levels as the runtime allows for continuous - valued signals , whereas for discrete - valued signals we want @xmath256 . inspired by this observation , we propose to begin with some initial @xmath253 , and then adaptively adjust @xmath253 hoping to match @xmath255 .",
    "hence , we propose the size- and level - adaptive mcmc algorithm ( algorithm  3 ) , which invokes l - mcmc ( algorithm  [ alg : mcmcal ] ) several times .",
    "* three basic procedures : * in order to describe the size- and level - adaptive mcmc ( sla - mcmc ) algorithm in detail , we introduce three alphabet adaptation procedures as follows .",
    "* _ merge _ : first , find the closest adjacent levels @xmath257 . create a new level @xmath258 and add it to @xmath179 .",
    "let @xmath259 .",
    "replace @xmath260 by @xmath258 whenever @xmath261 .",
    "next , remove @xmath262 and @xmath263 from @xmath179 .",
    "* _ add - out _ : define the range @xmath264 @xmath265 $ ] , and @xmath266 . add a _ lower _ level @xmath258 and/or _ upper level _",
    "@xmath267 to @xmath179 with @xmath268 .",
    "note that @xmath269 , i.e. , the new levels are empty . *",
    "_ add - in _ : first , find the most distant adjacent levels , @xmath262 and @xmath263 . then , add a level @xmath258 to @xmath179 with @xmath259 . for @xmath270 s.t .",
    "@xmath271 , replace @xmath260 by @xmath258 with probability @xmath272 , where @xmath273 is given in  ( [ eq : def_boltzmann ] ) ; for @xmath270 s.t .",
    "@xmath274 , replace @xmath260 by @xmath258 with probability @xmath275 . note that @xmath276 is typically non - zero , i.e. , @xmath258 tends not to be empty .",
    "we call the process of running one of these procedures followed by running l - mcmc a _ round_.    * size- and level - adaptive mcmc : * sla - mcmc is conceptually illustrated in the flowchart in fig .",
    "[ fig : sla - mcmc ] .",
    "it has four stages , and in each stage we will run l - mcmc for several super - iterations ; we denote the execution of l - mcmc for @xmath141 super - iterations by l(@xmath141 ) .",
    "the parameters @xmath277 , and @xmath278 are the number of super - iterations used in stages  1 through  4 , respectively .",
    "the choice of these parameters reflects a trade - off between runtime and estimation quality .    in stage  1 ,",
    "sla - mcmc uses a fixed - size adaptive reproduction alphabet @xmath179 to tentatively estimate the signal . the initial point of stage  1",
    "is obtained in the same way as l - mcmc .",
    "after stage  1 , the initial point and temperature offset for each instance of l - mcmc correspond to the respective outputs of the previous instance of l - mcmc .",
    "if the source is discrete - valued and @xmath279 in stage  1 , then multiple levels in the output @xmath179 of stage  1 may correspond to a single level in @xmath280 . to alleviate this problem , in stage  2",
    "we merge levels closer than @xmath281 , where @xmath282 is a parameter .",
    "however , @xmath253 might still be larger than needed ; hence in stage  3 we tentatively merge the closest adjacent levels . the criterion",
    "@xmath283 evaluates whether the current objective function is lower ( better ) than in the previous round ; we do not leave stage  3 until @xmath283 is violated .",
    "note that if @xmath284 ( this always holds for continuous - valued signals ) , then ideally sla - mcmc should not merge any levels in stage  3 , because the objective function would increase if we merge any levels .",
    "define the outlier set @xmath285 . under condition  [ cond :",
    "tech1 ] , @xmath286 might be small or even empty . when @xmath286 is small , l - mcmc might not assign levels to represent the entries of @xmath286 . to make sla - mcmc more robust to outliers , in stage  4a we add empty levels outside the range @xmath287 and then allow l - mcmc to change entries of @xmath197 to the new levels during gibbs sampling ; we call this _ populating _ the new levels .",
    "if a newly added outside level is not populated , then we remove it from @xmath179 .",
    "seeing that the optimal mapping @xmath192 in l - mcmc tends not to map symbols to levels with low population , we consider a criterion @xmath288 where we will will add an outside upper ( lower ) level if the population of the current upper ( lower ) level is smaller than @xmath289 , where @xmath290 is a parameter .",
    "that is , the criterion @xmath288 is violated if both populations of the current upper and lower levels are sufficient ( at least @xmath289 ) ; in this case we do not need to add outside levels because @xmath192 will map some of the current levels to represent the entries in @xmath286 .",
    "the criterion @xmath291 is violated if all levels added outside are not populated by the end of the round .",
    "sla - mcmc keeps adding levels outside @xmath287 until it is wide enough to cover most of the entries of @xmath5 .",
    "next , sla - mcmc considers adding levels inside @xmath287 ( stage  4b ) .",
    "if the signal is discrete - valued , this stage should stop when @xmath256 .",
    "else , for continuous - valued signals sla - mcmc can add levels until the runtime expires .    in practice ,",
    "sla - mcmc runs l - mcmc at most a constant number of times , and the computational complexity is in the same order of l - mcmc , i.e. , @xmath238 . on the other hand ,",
    "sla - mcmc allows varying @xmath253 , which often improves the estimation quality .",
    "donoho proved for the scalar channel setting that @xmath84 is sampled from the posterior @xmath85  @xcite .",
    "seeing that the gibbs sampler used by mcmc ( cf .",
    "section  [ sec : b - mcmc ] ) generates random samples , and the outputs of our algorithm will be different if its random number generator is initialized with different _ random seeds _ , we speculate that running sla - mcmc several times will also yield independent samples from the posterior , where we note that the runtime grows linearly in the number of times that we run sla - mcmc . by mixing",
    "( averaging over ) several outputs of sla - mcmc , we obtain @xmath292 , which may have lower squared error with respect to the true @xmath5 than the average squared error obtained by a single sla - mcmc output .",
    "numerical results suggest that mixing indeed reduces the mse ( cf .",
    "[ fig : munif_algos ] ) ; this observation suggests that mixing the outputs of multiple algorithms , including running a random reconstruction algorithm several times , may reduce the squared error .",
    "in this section , we demonstrate that sla - mcmc is comparable and in many cases better than existing algorithms in reconstruction quality , and that sla - mcmc is applicable when @xmath293 . additionally , some numerical evidence is provided to justify conjecture  [ conj : double_mmse ] in section  [ sec : conjecture ] .",
    "then , the advantage of sla - mcmc in estimating low - complexity signals is demonstrated .",
    "finally , we compare b - mcmc , l - mcmc , and sla - mcmc performance .",
    "we implemented sla - mcmc in matlab and tested it using several stationary ergodic sources .",
    "except when noted , for each source , signals @xmath5 of length @xmath12 were generated .",
    "each such @xmath5 was multiplied by a gaussian random matrix @xmath2 with normalized columns and corrupted by i.i.d .",
    "gaussian measurement noise @xmath7 .",
    "except when noted , the number of measurements @xmath19 varied between 2000 and 7000 .",
    "the noise variance @xmath26 was selected to ensure that the signal - to - noise ratio ( snr ) was @xmath18 or @xmath294  db ; snr was defined as @xmath295)/(m\\sigma_z^2)\\right]$ ] .",
    "we set the context depth @xmath296 , the number of super - iterations in different stages of sla - mcmc @xmath297 and @xmath298 , the maximum total number of super - iterations to be @xmath299 , the initial number of levels @xmath300 , and the tuning parameter from section  [ sec : adaptive_size ] @xmath301 .",
    "sla - mcmc was not given the true alphabet @xmath280 for any of the sources presented in this paper ; our expectation is that it should adaptively adjust @xmath253 to match @xmath255 .",
    "the final estimate @xmath292 of each signal was obtained by averaging over the outputs @xmath302 of @xmath18 runs of sla - mcmc , where in each run we initialized the random number generator with another random seed , cf .",
    "section  [ sec : mix ] .",
    "these choices of parameters seemed to provide a reasonable compromise between runtime and estimation quality .",
    "we chose our performance metric as the mean signal - to - distortion ratio ( msdr ) defined as @xmath303/\\mbox{mse}\\right).\\ ] ] for each @xmath19 and snr , the mse was obtained after averaging over the squared errors of @xmath292 for 50 draws of @xmath5 , @xmath2 , and @xmath7 .",
    "we compared the performance of sla - mcmc to that of ( _ i _ ) compressive sensing matching pursuit ( cosamp )  @xcite , a greedy method ; ( _ ii _ ) gradient projection for sparse reconstruction ( gpsr )  @xcite , an optimization - based method ; and ( _ iii _ ) message passing approaches ( for each source , we chose best - matched algorithms between em - gm - amp - mos ( egam for short )  @xcite and turbogamp ( tg for short )  @xcite ) .",
    "typical runtimes are @xmath304 hour ( for continuous - valued signals ) and @xmath305 minutes ( discrete - valued ) per random seed for sla - mcmc , @xmath306 minutes for egam  @xcite and tg  @xcite , and @xmath294 minutes for cosamp  @xcite and gpsr  @xcite .    among these baseline algorithms designed for i.i.d .",
    "signals , gpsr  @xcite and egam  @xcite only need @xmath6 and @xmath2 , and cosamp  @xcite also needs the number of non - zeros in @xmath5 . only tg  @xcite is designed for non - i.i.d .",
    "signals ; however , it must be aware of the probabilistic model of the source .",
    "finally , gpsr  @xcite performance was similar to that of cosamp  @xcite for all sources considered in this section , and thus is not plotted .     as a function of the number of gaussian random measurements @xmath19 for different snr values ( @xmath12 ) .",
    "_ [ fig : ber],width=302 ]    ) distribution as a function of the number of gaussian random measurements @xmath19 for different snr values ( @xmath307 ) .",
    "_ [ fig : densemrad],width=302 ]      * bernoulli source : * we first present results for an i.i.d .",
    "bernoulli source .",
    "the bernoulli source followed the distribution @xmath308 , where @xmath309 is the dirac delta function .",
    "note that sla - mcmc did not know @xmath310 and had to estimate it on the fly .",
    "we chose egam  @xcite for message passing algorithms because it fits the signal with gaussian mixtures ( gm ) , which can accurately characterize signals from an i.i.d .",
    "bernoulli source .",
    "the resulting msdrs for sla - mcmc , egam  @xcite , and cosamp  @xcite are plotted in fig .",
    "[ fig : ber ] .",
    "we can see that when @xmath311 , egam  @xcite approaches the mmse  @xcite performance for low to medium @xmath19 ; although sla - mcmc is often worse than egam  @xcite , it is within @xmath312  db of the mmse performance .",
    "this observation that sla - mcmc approaches the mmse for @xmath311 partially substantiates conjecture  [ conj : double_mmse ] in section  [ sec : conjecture ] .",
    "when @xmath313 , sla - mcmc is comparable to egam  @xcite when @xmath314 .",
    "cosamp  @xcite has worse msdr .     for different snr values ( @xmath12 ) .",
    "_ [ fig : sparsel],width=302 ]    $ ] as a function of the number of gaussian random measurements @xmath19 for different snr values ( @xmath12 ) . _",
    "[ fig : munif],width=302 ]    * dense markov - rademacher source : * considering that most algorithms are designed for i.i.d .",
    "sources , we now illustrate the performance of sla - mcmc on non - i.i.d .",
    "sources by simulating a dense markov - rademacher ( mrad for short ) source .",
    "the non - zero entries of the dense mrad signal were generated by a two - state markov state machine ( non - zero and zero states ) .",
    "the transition from zero to non - zero state for adjacent entries had probability @xmath315 , while the transition from non - zero to zero state for adjacent entries had probability @xmath316 ; these parameters yielded @xmath317 non - zero entries on average .",
    "the non - zeros were drawn from a rademacher distribution , which took values @xmath318 with equal probability . with such denser signals , we may need to take more measurements and/or require higher snrs to achieve similar performance to previous examples .",
    "seeing that the runtime of cosamp  @xcite is cubic in @xmath19 and the number of non - zeros in the signal , we only generated dense mrad signals with length @xmath307 to save runtime .",
    "the number of measurements varied from @xmath319 to @xmath320 , with @xmath321 .",
    "although tg  @xcite does not provide an option that accurately characterize the mrad source , we still chose to compare against its performance because it is applicable to non - i.i.d . signals .",
    "the msdrs for sla - mcmc , tg  @xcite , and cosamp  @xcite are plotted in fig .",
    "[ fig : densemrad ] .",
    "we can see that cosamp  @xcite has poor performance because it does not consider the source memory ; although tg  @xcite is designed for non - i.i.d .",
    "sources , it is nonetheless outperformed by sla - mcmc .",
    "this example shows that sla - mcmc reconstructs non - i.i.d .",
    "signals well and is applicable to general linear inverse problems .",
    "we now discuss the performance of sla - mcmc in estimating continuous sources .",
    "* sparse laplace ( i.i.d . ) source : * for unbounded continuous - valued signals , which do not adhere to condition  [ cond : tech1 ] , we simulated an i.i.d .",
    "sparse laplace source following the distribution @xmath322 , where @xmath323 denotes a laplacian distribution with mean zero and variance one .",
    "we chose egam  @xcite for message passing algorithms because it fits the signal with gm , which can accurately characterize signals from an i.i.d .",
    "sparse laplace source .",
    "the msdrs for sla - mcmc , egam  @xcite , and cosamp  @xcite are plotted in fig .",
    "[ fig : sparsel ] .",
    "we can see that egam  @xcite approaches the mmse  @xcite performance in all settings ; sla - mcmc outperforms cosamp  @xcite , while it is approximately @xmath324  db worse than the mmse .",
    "recall from conjecture  [ conj : double_mmse ] that we expect to achieve twice the mmse , which is approximately @xmath312  db below the signal - to - distortion ratio of mmse , and thus sla - mcmc performance is reasonable .",
    "this example of sla - mcmc performance approaching the mmse further substantiates conjecture  [ conj : double_mmse ] .",
    "* markov - uniform source : * for bounded continuous - valued signals , which adhere to condition  [ cond : tech1 ] , we simulated a markov - uniform ( munif for short ) source , whose non - zero entries were generated by a two - state markov state machine ( non - zero and zero states ) with @xmath325 and @xmath316 ; these parameters yielded @xmath326 non - zeros entries on average .",
    "the non - zero entries were drawn from a uniform distribution between @xmath16 and @xmath304 .",
    "we chose tg with markov support and gm model options  @xcite for message passing algorithms .",
    "we plot the resulting msdrs for sla - mcmc , tg  @xcite , and cosamp  @xcite in fig .",
    "[ fig : munif ] .",
    "we can see that the cosamp  @xcite lags behind in msdr .",
    "the sla - mcmc curve is close to that of tg  @xcite when @xmath313 , and it is slightly better than tg  @xcite when @xmath311 .    when the signal model is known , the message passing approaches egam  @xcite and tg  @xcite achieve quite low mse s , because they can get close to the bayesian mmse .",
    "sometimes the model is only known imprecisely , and sla - mcmc can improve over message passing ; for example , it is better than tg  @xcite in estimating munif signals ( fig .",
    "[ fig : munif ] ) , because tg  @xcite approximates the uniformly distributed non - zeros by gm .",
    "when the source is continuous ( figs .  [ fig : sparsel ] and  [ fig : munif ] ) , sla - mcmc might be worse than the existing message passing approaches ( egam  @xcite and tg  @xcite ) .",
    "one reason for the under - performance of sla - mcmc is the @xmath312  db gap of conjecture  [ conj : double_mmse ] .",
    "the second reason is that sla - mcmc can only assign finitely many levels to approximate continuous - valued signals , leading to under - representation of the signal .",
    "however , when it comes to discrete - valued signals that have finite size alphabets ( figs .",
    "[ fig : ber ] and  [ fig : densemrad ] ) , sla - mcmc is comparable to and in many cases better than existing algorithms .",
    "sla - mcmc promotes low complexity due to the complexity - penalized term in the objective function  ( [ eq : mcmc_energy ] ) .",
    "hence , it tends to perform well for signals with low complexity such as the signals in figs .",
    "[ fig : ber ] and  [ fig : densemrad ] ( note that the bernoulli signal is sparse while the mrad signal is denser ) . in this subsection",
    ", we simulated a non - sparse low - complexity signal .",
    "we show that complexity - penalized approaches such as sla - mcmc might estimate low - complexity signals well .     as a function of the number of gaussian random measurements @xmath19 for different snr values ( @xmath12 ) . _",
    "[ fig : munif_algos],width=302 ]    * four - state markov source : * to evaluate the performance of sla - mcmc for discrete - valued non - i.i.d .  and non - sparse signals",
    ", we examined a four - state markov source ( markov4 for short ) , which has been discussed in section  [ sec : intro ] and fig .",
    "[ fig : m4 ] .",
    "we tried conventional sparsifying transforms , but none of them yielded coefficients that were reasonably sparse .",
    "as pointed out in section  [ sec : intro ] , there exists a special transform that yields a sparse representation of the markov4 signals , but using specialized transforms requires prior knowledge of the source .",
    "despite knowing the sparsifying matrix , all our baseline algorithms performed poorly ( yielding msdrs below @xmath18  db ) .",
    "thus , we did not include the results for these baseline algorithms in fig .",
    "[ fig : m4 ] .",
    "on the other hand , markov4 signals have low complexity in the time domain , and hence , sla - mcmc successfully reconstructed markov4 signals with reasonable quality even when @xmath19 was relatively small .",
    "this markov4 source highlights the special advantage of our approach in reconstructing low - complexity signals .",
    "we compare the performance of b - mcmc , l - mcmc , and sla - mcmc with different numbers of seeds ( cf .",
    "section  [ sec : mix ] ) by examining the munif source ( cf .",
    "section  [ sec : bvsub ] ) .",
    "we ran b - mcmc with the fixed uniform alphabet @xmath62 in  ( [ eq : def : replevels2 ] ) with @xmath327 levels .",
    "l - mcmc was initialized in the same way as stage  1 of sla - mcmc .",
    "b - mcmc and l - mcmc ran for @xmath328 super - iterations before outputting the estimates ; this number of super - iterations was sufficient because it was greater than @xmath329 in stage  1 of sla - mcmc .",
    "the results are plotted in fig .",
    "[ fig : munif_algos ] .",
    "b - mcmc did not perform well given the @xmath62 in  ( [ eq : def : replevels2 ] ) and is not plotted .",
    "we can see that sla - mcmc outperforms l - mcmc . averaging over more seeds provides an increase of @xmath304  db in msdr .",
    "db . ] finally , we tried a  good \" reproduction alphabet in b - mcmc , @xmath330 , and the results were close to those of sla - mcmc . indeed ,",
    "b - mcmc is quite sensitive to the reproduction alphabet , and stages  @xmath324@xmath331 of sla - mcmc find a good set of levels .",
    "example output levels @xmath181 of sla - mcmc were : @xmath332 for bernoulli signals , @xmath333 for dense mrad signals , @xmath334 levels spread in the range @xmath335 $ ] for i.i.d .",
    "sparse laplace signals , @xmath336 levels spread in the range @xmath337 $ ] for munif signals , and @xmath338 for markov4 signals ; we can see that sla - mcmc adaptively adjusted @xmath253 to match @xmath255 so that these levels represented each signal well .",
    "also , we can see from figs .",
    "[ fig : ber][fig : sparsel ] that sla - mcmc did not perform well in the low measurements and high snr setting , which was due to mismatch between @xmath253 and @xmath255 .",
    "this paper provides universal algorithms for signal estimation from linear measurements . here , universality denotes the property that the algorithm need not be informed of the probability distribution for the recorded signal prior to acquisition ; rather , the algorithm simultaneously builds estimates both of the observed signal and its distribution .",
    "inspired by the kolmogorov sampler  @xcite and motivated by the need for a computationally tractable framework , our contribution focuses on stationary ergodic signal sources and relies on a map estimation algorithm .",
    "the algorithm is then implemented via a mcmc formulation that is proven to be convergent in the limit of infinite computation .",
    "we reduce the computation and improve the estimation quality of the proposed algorithm by adapting the reproduction alphabet to match the complexity of the input signal .",
    "our experiments have shown that the performance of the proposed algorithm is comparable to and in many cases better than existing algorithms , particularly for low - complexity sources that do not exhibit standard sparsity or compressibility .    as we were finishing this paper , jalali and poor  @xcite",
    "have independently shown that our formulation  ( [ eq : mcmc_energy ] ) also provides an implementable version of rnyi entropy minimization .",
    "their theoretical findings further motivate our proposed universal mcmc formulation .",
    "our proof mimics a very similar proof presented in  @xcite for lossy source coding ; we include all details for completeness .",
    "the proof technique relies on mathematical properties of non - homogeneous ( e.g. , time - varying ) markov chains ( mcs )  @xcite . through the proof",
    ", @xmath339 denotes the state space of the mc of codewords generated by algorithm  [ alg : mcmc ] , with size @xmath340 .",
    "we define a stochastic transition matrix @xmath341 from @xmath342 to itself given by the boltzmann distribution for super - iteration @xmath156 in algorithm  [ alg : mcmc ] .",
    "similarly , @xmath343 defines the stable - state distribution on @xmath342 for @xmath341 , satisfying @xmath344 .",
    "@xcite _ dobrushin s ergodic coefficient _ of a mc transition matrix @xmath345 is denoted by @xmath346 and defined as @xmath347 , where @xmath348 denotes row @xmath83 of @xmath345 .    from the definition , @xmath349 .",
    "moreover , the ergodic coefficient can be rewritten as @xmath350 where @xmath351 denotes the entry of @xmath345 at row @xmath83 and column @xmath103 .",
    "we group the product of transition matrices across super - iterations as @xmath352 .",
    "there are two common characterizations for the stable - state behavior of a non - homogeneous mc .",
    "@xcite a non - homogeneous mc is called _ weakly ergodic _ if for any distributions @xmath353 and @xmath354 over the state space @xmath342 , and any @xmath355 , @xmath356 , where @xmath357 denotes the @xmath43 norm .",
    "similarly , a non - homogeneous mc is called _ strongly ergodic _ if there exists a distribution @xmath358 over the state space @xmath342 such that for any distribution @xmath353 over @xmath342 , and any @xmath355 , @xmath359 .",
    "we will use the following two theorems from  @xcite in our proof .",
    "@xcite a mc is weakly ergodic if and only if there exists a sequence of integers @xmath360 such that @xmath361 .",
    "[ th : block ]    @xcite let a mc be weakly ergodic .",
    "assume that there exists a sequence of probability distributions @xmath362 on the state space @xmath342 such that @xmath344 .",
    "then the mc is strongly ergodic if @xmath363 . [ th : weak ]    the rest of proof is structured as follows .",
    "first , we show that the sequence of stable - state distributions for the mc used by algorithm  [ alg : mcmc ] converges to a uniform distribution over the set of sequences that minimize the energy function as the iteration count @xmath156 increases .",
    "then , we show using theorems  [ th : block ] and  [ th : weak ] that the non - homogeneous mc used in algorithm  [ alg : mcmc ] is strongly ergodic , which by the definition of strong ergodicity implies that algorithm  [ alg : mcmc ] always converges to the stable distribution found above .",
    "this implies that the outcome of algorithm  [ alg : mcmc ] converges to a minimum - energy solution as @xmath364 , completing the proof of theorem  [ th : conv ] .",
    "we therefore begin by finding the stable - state distribution for the non - homogeneous mc used by algorithm  [ alg : mcmc ] . at each super - iteration @xmath156 ,",
    "the distribution defined as @xmath365 satisfies @xmath366 , cf .",
    "( [ eqn : gibbs ] ) .",
    "we can show that the distribution @xmath343 converges to a uniform distribution over the set of sequences that minimize the energy function , i.e. , @xmath367 where @xmath368 . to show ( [ eq : stable ] )",
    ", we will show that @xmath369 is increasing for @xmath370 and eventually decreasing for @xmath371 . since for @xmath370 and @xmath372 we have @xmath373 , for @xmath374 we have @xmath375 which together with ( [ eq : pidist ] ) implies @xmath376 . on the other hand ,",
    "if @xmath371 , then we obtain @xmath377^{-1}. \\label{eq : hlower}\\end{aligned}\\ ] ] for sufficiently large @xmath140 , the denominator of ( [ eq : hlower ] ) is dominated by the second term , which increases when @xmath140 increases , and therefore @xmath369 decreases for @xmath371 as @xmath156 increases . finally , since all sequences",
    "@xmath370 have the same energy @xmath110 , it follows that the distribution is uniform over the symbols in @xmath378 .    having shown convergence of the non - homogenous mc s stable - state distributions",
    ", we now show that the non - homogeneous mc is strongly ergodic .",
    "the transition matrix @xmath341 of the mc at iteration @xmath156 depends on the temperature @xmath140 in ( [ eq : st ] ) used within algorithm  [ alg : mcmc ] .",
    "we first show that the mc used in algorithm  [ alg : mcmc ] is weakly ergodic via theorem  [ th : block ] ; the proof of the following lemma is given in  [ app : lemm : ergcoefbound ] .",
    "the ergodic coefficient of @xmath341 for any @xmath379 is upper bounded by @xmath380 , where @xmath381 is defined in ( [ eq : deltaq ] ) .",
    "[ lemm : ergcoefbound ]    we note in passing that condition  [ cond : tech1 ] ensures that @xmath381 is finite .",
    "using lemma  [ lemm : ergcoefbound ] and ( [ eq : st ] ) , we can evaluate the sum given in theorem  [ th : block ] as @xmath382 and so the non - homogeneous mc defined by @xmath383 is weakly ergodic .",
    "now we use theorem  [ th : weak ] to show that the mc is strongly ergodic by proving that @xmath384 .",
    "since we know from earlier in the proof that @xmath369 is increasing for @xmath370 and eventually decreasing for @xmath371 , there exists a @xmath385 such that for any @xmath386 , we have @xmath387 since the right hand side does not depend on @xmath388 , we have that @xmath389 .",
    "this implies that the non - homogeneous mc used by algorithm  [ alg : mcmc ] is strongly ergodic , and thus completes the proof of theorem  [ th : conv ] .",
    "let @xmath390 be two arbitrary sequences in @xmath342 .",
    "the probability of transitioning from a given state to a neighboring state in an iteration within iteration @xmath148 of super - iteration @xmath156 of algorithm  [ alg : mcmc ] is given by ( [ eqn : gibbs ] ) , and can be rewritten as @xmath391 where @xmath392 . therefore , the smallest probability of transition from @xmath393 to @xmath394 within super - iteration @xmath156 of algorithm  [ alg : mcmc ] is bounded by @xmath395 using the alternative definition of the ergodic coefficient ( [ eq : ergcoef ] ) , @xmath396 proving the lemma .",
    "preliminary conversations with deanna needell and tsachy weissman framed our thinking about universal compressed sensing .",
    "phil schniter was instrumental in formulating the proposed framework and shepherding our progress through detailed conversations , feedback on our drafts , and probing questions .",
    "gary howell provided invaluable guidance on using north carolina state university s high performance computing resources .",
    "final thanks to jin tan , yanting ma , and nikhil krishnan for thoroughly proofreading our manuscript .",
    "m.  zhou , h.  chen , j.  paisley , l.  ren , l.  li , z.  xing , d.  dunson , g.  sapiro , and l.  carin , `` nonparametric bayesian dictionary learning for analysis of noisy and incomplete images , '' , vol .",
    "1 , pp . 130144 , jan ."
  ],
  "abstract_text": [
    "<S> we study the compressed sensing ( cs ) signal estimation problem where an input signal is measured via a linear matrix multiplication under additive noise . while this setup usually assumes sparsity or compressibility in the input signal during recovery , </S>",
    "<S> the signal structure that can be leveraged is often not known _ a priori_. in this paper , we consider _ universal _ cs recovery , where the statistics of a stationary ergodic signal source are estimated simultaneously with the signal itself . </S>",
    "<S> inspired by kolmogorov complexity and minimum description length , we focus on a maximum _ a posteriori _ ( map ) estimation framework that leverages universal priors to match the complexity of the source . </S>",
    "<S> our framework can also be applied to general linear inverse problems where more measurements than in cs might be needed . </S>",
    "<S> we provide theoretical results that support the algorithmic feasibility of universal map estimation using a markov chain monte carlo implementation , which is computationally challenging . </S>",
    "<S> we incorporate some techniques to accelerate the algorithm while providing comparable and in many cases better reconstruction quality than existing algorithms . </S>",
    "<S> experimental results show the promise of universality in cs , particularly for low - complexity sources that do not exhibit standard sparsity or compressibility .    </S>",
    "<S> compressed sensing , map estimation , markov chain monte carlo , universal algorithms . </S>"
  ]
}