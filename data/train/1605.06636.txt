{
  "article_text": [
    "deep networks have significantly improved the state of the arts for diverse machine learning problems and applications .",
    "unfortunately , the impressive performance gains come only when massive amounts of labeled data are available for supervised training .",
    "since manual labeling of sufficient training data for diverse application domains on - the - fly is often prohibitive , for a target task short of labeled data , there is strong motivation to build effective learners that can leverage rich labeled data in a different source domain . however",
    ", this learning paradigm suffers from the shift in data distributions across different domains , which poses a major obstacle in adapting predictive models for the target task @xcite .",
    "learning a discriminative model in the presence of the shift between training and test distributions is known as transfer learning or domain adaptation @xcite .",
    "previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels @xcite .",
    "recent deep transfer learning methods leverage deep networks to learn more transferable feature representations by embedding domain adaptation in the pipeline of deep learning , which can simultaneously disentangle the explanatory factors of variations behind data and match the marginal distributions across domains @xcite .",
    "transfer learning becomes more challenging when the domains may change by the joint distributions @xmath0 of features and labels , which is a natural scenario in practical applications .",
    "another line of work @xcite gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions @xcite . as target labels are unavailable , the adaptation is executed by minimizing the discrepancy between marginal distributions instead of joint distributions . how to address a seamless adaptation of joint distributions remains an open problem .    in this paper ,",
    "we address deep transfer learning under the general scenario that the joint distributions of features and labels may change substantially across domains . instead of separate adaptations of marginal and conditional distributions which often require strong independence and/or smoothness assumptions on the factorized distributions @xcite , we propose a novel joint distribution discrepancy that can directly compare joint distributions by embedding them into reproducing kernel hilbert spaces , which eliminates the need of marginal - conditional factorization for separate adaptation .",
    "transfer learning is enabled in deep convolutional networks , where the dataset shifts may linger in multiple task - specific feature layers and the classifier layer , because deep features eventually transition from general to specific along the network and the transferability of features and classifiers decreases when the domain discrepancy increases @xcite .",
    "we craft a family of joint adaptation networks to match the joint distributions of all the task - specific layers of deep networks across domains by minimizing the joint distribution discrepancy , which can be trained efficiently using back - propagation .",
    "empirical evidence shows that the approach yields state of the art results on standard domain adaptation datasets .",
    "transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions @xcite , which enables learning on novel domains without labeled data and finds wide applications in computer vision @xcite and natural language processing @xcite .",
    "the main technical difficulty of transfer learning is how to reduce the shifts in data distributions across domains .",
    "most existing methods learn a shallow representation model by which domain discrepancy is minimized , which however can not suppress domain - specific exploratory factors of variations .",
    "deep networks learn abstract representations that disentangle the explanatory factors of variations in data @xcite and extract transferable factors underlying different populations @xcite , which can only reduce , but not remove , the cross - domain discrepancy @xcite .",
    "the state of the art work on deep domain adaptation bridges deep learning and transfer learning @xcite by integrating domain - adaptation modules into deep network architectures to boost transfer performance .",
    "transfer learning from different domains in practical applications is more challenging as the domains may change by the joint distributions @xmath0 of features and labels .",
    "previous deep transfer learning methods @xcite mainly correct the shifts in the marginal distributions .",
    "note that @xcite partially align the source and target classes based on pseudo labels , but both rely on a rather strong assumption that the adaptations of marginal distributions @xmath1 and target distributions @xmath2 can be independent .",
    "another line of work @xcite gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions @xcite , which is a remarkable step towards full - transfer of joint distributions .",
    "since the target labels are unavailable in unsupervised domain adaptation , adaptation is performed by minimizing the discrepancy between the marginal distributions instead of the joint distributions , while the conditional shift is taken into account through @xmath3 .",
    "this paper addresses the challenge of joint distribution adaptation by minimizing a novel joint distribution discrepancy , which is naturally implemented in deep networks .",
    "in domain adaptation problems , we are given a source domain @xmath4 of @xmath5 labeled examples and a target domain @xmath6 of @xmath7 unlabeled examples .",
    "the source domain and target domain are sampled from joint distributions @xmath8 and @xmath9 respectively , and @xmath10 .",
    "the goal of this paper is to craft a deep neural network @xmath11 that formally reduces the shifts in joint distributions across domains and enables learning transferable features and classifiers , such that the target risk @xmath12 $ ] can be minimized via the source domain supervision .",
    "transfer learning is a challenging machine learning paradigm due to the scarcity of labeled data in the target domain , and the classifier trained on the source domain can not generalize to the target domain due to the substantial shifts in joint distributions @xmath13 .",
    "the distribution shifts may stem from the marginal distributions @xmath14",
    "covariate shift @xcite ) , the conditional distributions @xmath15 ( a.k.a .",
    "conditional shift @xcite ) , or both ( a.k.a .",
    "dataset shift @xcite ) .",
    "in general , the presence of conditional shift leads to an ill - posed problem , and the additional assumption that the conditional distribution changes only under location - scale transformations on @xmath16 is commonly imposed to make the problem tractable @xcite .",
    "however , this assumption is still very strong for practical domain adaptation problems and it is nontrivial to justify when and how this assumption can hold .",
    "also , how to seamlessly address covariate shift and conditional shift remains an open problem .",
    "many existing methods address the transfer learning problem mainly by bounding the target error with the source error plus a discrepancy metric between the marginal distributions @xmath1 and @xmath17 of the source and target domains @xcite .",
    "two classes of statistics have been explored for two - sample testing , which accepts or rejects the null hypothesis @xmath18 based on the two samples respectively generated from @xmath1 and @xmath17 : energy distance ( ed ) and maximum mean discrepancy ( mmd ) @xcite .",
    "as these statistics are defined for the marginal distributions , they are only applied to covariate shift adaptation problems @xcite , which include state of the art deep adaptation methods @xcite .    in this paper , we propose to directly measure the discrepancy between joint distributions @xmath0 and @xmath19 ( denoted by @xmath20 and @xmath21 for short ) based on the theory of kernel embedding of distributions @xcite , in which a probability distribution is represented as an element of a reproducing kernel hilbert space ( rkhs ) .",
    "let @xmath16 and @xmath22 be random variables with codomains @xmath23 and @xmath24 and joint distribution @xmath0 . given kernels @xmath25 on @xmath26 and @xmath27 on @xmath28 , the moore - aronszajn theorem asserts the existence of corresponding rkhss @xmath29 and @xmath30 in which the element @xmath31 and @xmath32 satisfy the reproducing property @xmath33 , @xmath34 for all @xmath35 , @xmath36 , @xmath37 , where @xmath38 and @xmath39 are the feature mappings , @xmath40 and @xmath41 .",
    "assume the tensor product of two hilbert spaces @xmath42 is also a hilbert space that satisfies @xmath43 .",
    "the kernel embedding @xmath44 of joint distribution @xmath0 in the hilbert space @xmath45 is defined as @xmath46 = \\int\\nolimits_{{\\omega _ { \\mathbf{x } } } \\times { \\omega _ { \\mathbf{y } } } } { \\phi \\left ( { \\mathbf{x } } \\right ) \\otimes \\psi \\left ( { \\mathbf{y } } \\right){\\text{d}}p\\left ( { { \\mathbf{x}},{\\mathbf{y } } } \\right)},\\ ] ] where @xmath47 $ ] denotes taking expectation over random variable that follows distribution @xmath20 . given @xmath48 pairs of training examples",
    "@xmath49 drawn i.i.d . from @xmath20 , the empirical estimate of @xmath44",
    "is @xmath50 we require characteristic kernels @xmath51 and @xmath52 such that the kernel embedding @xmath53 is injective , and that the embedding of distributions into infinite - dimensional feature spaces can preserve all of the statistical features of arbitrary distributions , which removes the necessity of density estimation of @xmath20 .",
    "we extend the maximum mean discrepancy ( mmd ) @xcite to measure the discrepancy between joint distributions .",
    "mmd is a distance - measure between marginal distributions @xmath54 and @xmath55 , which is defined as the squared distance between the kernel embeddings of marginal distributions in rkhs , @xmath56 - { \\mathbb{e}_{{q_{{{\\mathbf{x}}^t}}}}}\\left [ { \\phi \\left ( { { { \\mathbf{x}}^t } } \\right ) } \\right ] } \\right\\|_{{\\mathcal{h}_\\phi } } ^2 $ ] .",
    "we propose a novel joint distribution discrepancy ( jdd ) , which measures the discrepancy between joint distributions @xmath57 and @xmath58 as the squared distance between the kernel embeddings of the joint distributions in rkhs , @xmath59 - { \\mathbb{e}_q}\\left [ { \\phi \\left ( { { { \\mathbf{x}}^t } } \\right ) \\otimes \\psi \\left ( { { { \\mathbf{y}}^t } } \\right ) } \\right ] } \\right\\|_\\mathcal{h}^2 , \\end{aligned}\\ ] ] where @xmath42 is the tensor product hilbert space that satisfies the reproducing property . based on the theory of kernel two - sample testing @xcite , we have @xmath60 if and only if @xmath61 .",
    "given source domain @xmath4 of @xmath5 labeled examples and target domain @xmath62 of @xmath7 unlabeled points drawn i.i.d . from @xmath20 and @xmath21 respectively , the empirical estimate of @xmath63 is @xmath64 note that for unsupervised domain adaptation the target labels @xmath65 are unavailable and we have to infer the target labels by a transductive learning paradigm , of which the details will be forthcoming .",
    "in the presence of deep neural networks , the training data may be represented by the features generated by multiple hidden layers as @xmath66 .",
    "since deep features eventually transition from general to specific along the network @xcite , transferability of features at different layers may be task - dependent and it is safer to model the joint distribution discrepancy based on multiple task - specific layers .",
    "we extend jdd to measure the discrepancy in task - specific features @xmath66 as the squared distance between the kernel embeddings of joint distributions @xmath67 and @xmath68 as @xmath69 - { \\mathbb{e}_q}\\left [ { \\mathop   \\bigotimes \\nolimits_{\\ell   \\in \\mathcal{l } } \\phi \\left ( { { { \\mathbf{x}}^{t\\ell } } } \\right ) \\otimes \\psi \\left ( { { { \\mathbf{y}}^t } } \\right ) } \\right ] } \\right\\|_\\mathcal{h}^2,\\ ] ] where @xmath70 is the tensor product of hilbert spaces that satisfies reproducing property @xmath71 .",
    "for source domain @xmath72 of @xmath5 labeled examples and target domain @xmath73 of @xmath7 unlabeled examples drawn i.i.d . from @xmath20 and @xmath21 respectively , the empirical estimate of @xmath74 is @xmath75 this multilayer jdd is designed to seamlessly match the deep architectures for network adaptation .",
    "the building modules for deep networks to perform joint distribution adaptation are shown in figure  [ fig : jan ] .",
    "deep networks @xcite can learn distributed , compositional , and abstract representations for natural data such as image and text .",
    "this paper addresses joint distribution adaptation within deep networks for learning transferable features and classifiers .",
    "we extend deep convolutional networks ( cnns ) , i.e. alexnet @xcite and googlenet @xcite , to novel joint adaptation networks ( jans ) as shown in figure  [ fig : jan ] .",
    "denote by @xmath76 the cnn classifier , and the empirical error of cnn on the source domain data @xmath77 is @xmath78 where @xmath79 is the cross - entropy loss function .",
    "the deep features learned by cnns can disentangle the exploratory factors of variations in data distributions and promote knowledge adaptation @xcite .",
    "however , the latest literature findings also reveal that the deep features can reduce , but not remove , the cross - domain distribution discrepancy @xcite .",
    "the deep features in standard cnns must eventually transition from general to specific along the network , and the transferability of features and classifiers decreases when the cross - domain discrepancy increases @xcite . in other words , the shifts in the joint distributions linger even after multilayer feature abstractions . in this paper , we fine - tune cnns on labeled source examples and require the joint distributions of the source and target domains to become similar under multiple task - specific feature layers @xmath80 and the classifier layer .",
    "this is implemented by jointly minimizing a penalty that substitutes multilayer activations @xmath81 to jdd   as @xmath82 where @xmath83 is a tradeoff parameter for the jdd penalty , @xmath72 and @xmath73 are the source and target domains represented by features of multiple layers @xmath84 .",
    "for the jan model based on alexnet we set @xmath85 while for the jan model based on googlenet we set @xmath86 , as these layers are tailored to task - specific structures , which are not safely transferable and should be jointly adapted by minimizing both the empirical error and jdd .",
    "it is worth noting that for unsupervised domain adaptation the target labels @xmath65 are unavailable . in order to compute jdd",
    ", we adopt a transductive learning paradigm which is a natural option for semi - supervised learning : we substitute the ground - truth labels @xmath87 in with the predictions made by the cnn classifier @xmath88 , where @xmath89 denotes a data point from either the source or target domain .",
    "note that , @xmath88 is not a single number indicating the assigned category of @xmath90 ( which is rather inaccurate ) ; it is the distribution over the label space , indicating the probability of assigning @xmath90 to each category .",
    "hence , the jdd penalty simultaneously regularizes both the features and the classifier such that the shifts in joint distributions across domains can be formally corrected in rkhss , which is made end - to - end learnable by deep networks .",
    "the iterative nature of the back - propagation algorithm nicely fits the transductive learning paradigm , such that jdd can be computed in each feed - forward pass .",
    "a limitation of jdd is quadratic complexity , which is inefficient for scalable deep transfer learning .",
    "motivated by the unbiased estimate of mmd @xcite , we devise a similar linear - time estimate of jdd as @xmath91 this linear - time estimate nicely fits into the mini - batch stochastic gradient descent ( sgd ) algorithm implemented in @xcite , based on which the training of the jan models can scale linearly to large samples .",
    "we evaluate the joint adaptation networks against state of the art transfer learning and deep learning methods on standard domain adaptation benchmarks . the codes and datasets will be available online .      * office-31 * @xcite is a standard benchmark for domain adaptation in computer vision , comprising 4,652 images and 31 categories collected from three distinct domains : _ amazon _ ( * a * ) , which contains images downloaded from amazon.com , _ webcam _ ( * w * ) and _ dslr _ ( * d * ) , which contain images taken by web camera and digital slr camera with different settings , respectively .",
    "we evaluate all methods across three transfer tasks * a * @xmath92 * w * , * d * @xmath92 * w * and * w * @xmath92 * d * , which are widely adopted by prior deep transfer learning methods @xcite , and another three transfer tasks * a * @xmath92 * d * , * d * @xmath92 * a * and * w * @xmath92 * a * as in @xcite .    * * imageclef - da * * is a benchmark dataset for the imageclef 2014 domain adaptation challenge , which is organized by selecting the 12 common categories shared by the following four public datasets , each is considered as a domain : _ caltech-256 _ ( * c * ) , _ imagenet ilsvrc 2012 _ ( * i * ) , _ pascal voc 2012 _ ( * p * ) , and _ bing _ ( * b * ) .",
    "we permute all domain combinations and build 12 transfer tasks : * c * @xmath92 * i * , * c * @xmath92 * p * , * c * @xmath92 * b * , * i * @xmath92 * c * , * i * @xmath92 * p * , * i * @xmath92 * b * ,",
    "* p * @xmath92 * c * , * p * @xmath92 * i * , * p * @xmath92 * b * , * b * @xmath92 * c * , * b * @xmath92 * i * , and * b * @xmath92 * p*. it is worth noting that , some domains ( e.g. * b * ) contain low - quality images that are more difficult to categorize than the other domains ( e.g. * c * ) .",
    "this makes this dataset a good compliment to the _ office-31 _ dataset .",
    "we compare with both conventional and the state of the art transfer learning and deep learning methods : transfer component analysis ( * tca * ) @xcite , geodesic flow kernel ( * gfk * ) @xcite , convolutional neural network ( * alexnet * @xcite and * googlenet * @xcite ) , deep domain confusion ( * ddc * ) @xcite , deep adaptation network ( * dan * ) @xcite , and reverse gradient ( * revgrad * ) @xcite .",
    "tca is a conventional transfer learning method based on mmd - regularized kernel pca .",
    "gfk is a manifold learning method that interpolates across an infinite number of intermediate subspaces to bridge domains .",
    "ddc is the first method that maximizes domain invariance by adding to alexnet an adaptation layer using linear - kernel mmd @xcite .",
    "dan learns more transferable features by embedding deep features of multiple task - specific layers to reproducing kernel hilbert spaces ( rkhss ) and matching different distributions optimally using multi - kernel mmd .",
    "revgrad improves domain adaptation by making the source and target domains indistinguishable for a discriminative domain classifier via an adversarial training paradigm .",
    "we examine the influence of deep representations for domain adaptation by employing the breakthrough * alexnet * @xcite and the state of the art * googlenet * @xcite for learning deep representations . for alexnet",
    ", we follow decaf @xcite and use the activations of the @xmath93 layer as image representation . for googlenet",
    ", we use the activations of the last inception layer @xmath94 as image representation . to go deeper with the efficacy of _ joint adaptation _ based on minimizing the joint distribution discrepancy ( jdd ) , we evaluate two variants of joint adaptation networks ( jans ) : * jan - xy * and * jan - xxy * , by using the two - layer adaptation module and the three - layer adaptation module in figure  [ fig : jan ] , respectively .",
    "we follow standard evaluation protocols for unsupervised domain adaptation @xcite .",
    "office-31 _ and _ imageclef - da _ datasets , we use all labeled source examples and all unlabeled target examples .",
    "we compare the average classification accuracy of each method on five random experiments , and report the standard error of the classification accuracies by different experiments of the same transfer task .",
    "we perform model selection by tuning the hyper - parameters using cross - validation on labeled source data . for mmd - based methods ( tca , ddc , and dan ) and jan , we use the gaussian kernel with bandwidth set to the median pairwise squared distances on training data , i.e. median trick @xcite .",
    "we implement all deep methods based on the * caffe * framework , and fine - tune from caffe - trained models of alexnet @xcite and googlenet @xcite , both are pre - trained on the imagenet dataset . due to limited training points in our datasets , ( 1 ) for alexnet , we fix convolutional layers @xmath95@xmath96 copied from pre - trained model , fine - tune @xmath97@xmath98 and fully connected layers @xmath99@xmath93 , and train classifier layer @xmath100 , all via back propagation ; ( 2 ) for googlenet , we fix inception ( convolution - pooling ) layers @xmath101@xmath102 copied from pre - trained model , fine - tune @xmath103@xmath104 and train classifier layer @xmath105 , all via back propagation . since the classifier is trained from scratch , we set its learning rate to be 10 times that of the lower layers .",
    "we use mini - batch stochastic gradient descent ( sgd ) with momentum of 0.9 and the learning rate annealing strategy implemented in revgrad @xcite : the learning rate is not selected through a grid search due to high computational cost  it is adjusted during sgd using the following formula : @xmath106 , where @xmath107 is the training progress linearly changing from @xmath108 to @xmath109 , @xmath110 and @xmath111 , which is optimized to promote convergence and low error on the source domain .",
    "we set @xmath112 within @xmath113 iterations and then set it to the cross - validated value , which allows the jdd penalty to be less sensitive to noisy signals at early stages of training process .",
    "the classification accuracy results on the _ office-31 _",
    "dataset for unsupervised domain adaptation based on alexnet and googlenet are shown in table [ table : office31-alexnet ] and table [ table : office31-googlenet ] , respectively .",
    "note that for fair comparison , the results of the state of the art methods , dan @xcite and revgrad @xcite , are directly reported from their original papers .",
    "the proposed jan models , jan - xy and jan - xxy , outperform all comparison methods on most transfer tasks . in particular , jans promote the classification accuracy substantially on hard transfer tasks , e.g. * a @xmath92 w * and * a @xmath92 d * , where the source and target domains are substantially different , and produce comparable classification accuracy on easy transfer tasks , * d @xmath92 w * and * w @xmath92 d * , where the source and target domains are similar @xcite .",
    "the encouraging results highlight the vital importance of joint distribution adaptation in deep neural networks , and suggest that jans are able to learn more transferable features and classifiers for effective domain adaptation .",
    "cccccccc method & a @xmath92 w & d @xmath92 w & w @xmath92 d & a @xmath92 d & d @xmath92 a & w @xmath92 a & avg + tca @xcite & 61.0@xmath1140.0 & 93.2@xmath1140.0 & 95.2@xmath1140.0 & 60.8@xmath1140.0 & 51.6@xmath1140.0 & 50.9@xmath1140.0 & 68.8 + gfk @xcite & 60.4@xmath1140.0 & 95.6@xmath1140.0 & 95.0@xmath1140.0 & 60.6@xmath1140.0 & 52.4@xmath1140.0 & 48.1@xmath1140.0 & 68.7 + alexnet @xcite & 61.6@xmath1140.5 & 95.4@xmath1140.3 & 99.0@xmath1140.2 & 63.8@xmath1140.5 & 51.1@xmath1140.6 & 49.8@xmath1140.4 & 70.1 + ddc @xcite & 61.8@xmath1140.4 & 95.0@xmath1140.5 & 98.5@xmath1140.4 & 64.4@xmath1140.3 & 52.1@xmath1140.8 & 52.2@xmath1140.4 & 70.6 + revgrad @xcite & 73.0@xmath1140.5 & 96.4@xmath1140.3 & 99.2@xmath1140.3 & - & - & - & - + dan @xcite & 68.5@xmath1140.5 & 96.0@xmath1140.3 & 99.0@xmath1140.3 & 67.0@xmath1140.4 & 54.0@xmath1140.5 & 53.1@xmath1140.5 & 72.9 + jan - xy & 74.0@xmath1140.3 & 96.5@xmath1140.3 & 99.4@xmath1140.1 & 71.0@xmath1140.4 & 56.6@xmath1140.2 & * * 55.1**@xmath1140.3 & 75.4 + jan - xxy & * * 74.9**@xmath1140.3 & * * 96.6**@xmath1140.2 & * * 99.5**@xmath1140.2 & * * 71.8**@xmath1140.3 & * * 58.3**@xmath1140.3 & 55.0@xmath1140.4 & * 76.0 * +    cccccccc method & a @xmath92 w & d @xmath92 w & w @xmath92 d & a @xmath92 d & d @xmath92 a & w @xmath92 a & avg + tca @xcite & 68.6@xmath1140.0 & 94.0@xmath1140.0 & 97.4@xmath1140.0 & 69.5@xmath1140.0 & 61.7@xmath1140.0 & 61.4@xmath1140.0 & 75.4 + gfk @xcite & 71.2@xmath1140.0 & * * 96.4**@xmath1140.0 & 99.0@xmath1140.0 & 70.3@xmath1140.0 & 62.7@xmath1140.0 & 60.7@xmath1140.0 & 76.7 + googlenet @xcite & 71.4@xmath1140.5 & 95.8@xmath1140.2 & 98.3@xmath1140.2 & 72.2@xmath1140.3 & 61.0@xmath1140.3 & 60.5@xmath1140.3 & 76.5 + ddc @xcite & 72.5@xmath1140.4 & 95.5@xmath1140.2 & 98.1@xmath1140.1 & 73.2@xmath1140.3 & 61.6@xmath1140.3 & 61.6@xmath1140.3 & 77.1 + dan @xcite & 76.0@xmath1140.3 & 95.9@xmath1140.2 & 98.6@xmath1140.1 & 74.4@xmath1140.2 & 61.5@xmath1140.3 & 60.3@xmath1140.2 & 77.8 + jan - xy & * * 78.1**@xmath1140.4 & * * 96.4**@xmath1140.2 & * * 99.3**@xmath1140.1 & * * 77.5**@xmath1140.2 & * * 68.4**@xmath1140.2 & * * 65.0**@xmath1140.4 & * 80.8 * +    from the results , we can make several interesting observations .",
    "( 1 ) standard deep - learning methods ( alexnet and googlenet ) perform comparably with traditional shallow transfer - learning methods ( tca and gfk ) that use deep - network features ( alexnet - fc7 and googlenet - in9 ) as input .",
    "the only difference between these two sets of methods is that deep networks can further take the advantage of supervised fine - tuning on the source - labeled data , while tca and gfk can individually benefit from their domain adaptation procedures .",
    "these results confirm the current practice that deep networks learn abstract feature representations , which can only reduce , but not remove , the domain discrepancy @xcite .",
    "( 2 ) deep - transfer learning methods that reduce the domain discrepancy by domain - adaptive deep networks ( ddc , dan and revgrad ) substantially outperform both standard deep - learning methods and traditional shallow transfer - learning methods .",
    "this validates that incorporating domain - adaptation modules into deep networks can improve domain adaptation performance .",
    "( 3 ) the proposed joint adaptation network ( jan ) models outperform previous methods by large margins and set new state of the art results on these benchmark datasets .",
    "different from all previous deep - transfer learning methods that only adapt the marginal distributions across domains based on feature layers ( one layer for revgrad and multilayer for dan ) , jan initiates a principled methodology for adapting the joint distributions across domains based on both feature layers and classifier layer in an end - to - end deep learning framework , which can fully correct the dataset shifts in joint distributions across domains .",
    "we go deeper with the efficacy of jan by showing results of its variants , * jan - xy * and * jan - xxy * , which use the two - layer adaptation module and the three - layer adaptation module in figure  [ fig : jan ] , respectively .",
    "the three - layer adaptation model jan - xxy achieves significantly better results than the two - layer adaptation model jan - xy .",
    "this highlights the importance of _ deeper _ adaptation of joint distributions .",
    "since deep features eventually transition from general to specific along the network , there may be multiple layers where the features are not safely transferable @xcite , hence it is safer to adapt the joint distributions based on multi - layer features instead of single - layer features .",
    "it is noteworthy that this paper initiates a principled way to model multi - layer features in a kernel embedding framework @xcite .",
    "cccccccccccccc method & c@xmath92i & c@xmath92p & c@xmath92b & i@xmath92c & i@xmath92p & i@xmath92b & p@xmath92c & p@xmath92i & p@xmath92b & b@xmath92c & b@xmath92i & b@xmath92p & avg + tca @xcite & 83.2 & 69.8 & 58.8 & 93.7 & 76.0 & 59.2 & 87.9 & 84.0 & 56.8 & 87.0 & 80.3 & 67.0 & 75.3 + gfk @xcite & 84.2 & 69.8 & 58.3 & 92.5 & 75.8 & 57.3 & 82.2 & 78.7 & 48.8 & 81.2 & 73.5 & 61.5 & 72.0 + googlenet @xcite & 85.2 & 66.8 & 58.6 & 92.2 & 76.5 & 57.2 & 91.3 & 85.0 & * 58.7 * & 87.0 & 81.3 & 65.2 & 75.4 + ddc @xcite & 86.3 & 71.5 & 58.9 & 92.8 & 77.7 & 59.6 & 89.9 & 83.7 & 55.5 & 87.6 & 80.4 & 69.1 & 76.1 + dan @xcite & 90.0 & 73.5 & 60.8 & 95.0 & 78.5 & 58.6 & 92.1 & 84.7 & 56.3 & 90.0 & 82.4 & 72.0 & 77.8 +",
    "jan - xy & * 91.0 * & * 75.3 * & * 62.5 * & * 96.1 * & * 78.9 * & * 61.3 * & * 92.7 * & * 87.0 * & 57.5 & * 90.8 * & * 83.1 * & * 73.7 * & * 79.2 * +    the four domains in _ imageclef - da _ are more balanced than that of _ office-31 _ , but some domains ( e.g. * b * ) comprise low - quality images that are much more difficult to categorize . with more difficult transfer tasks , we are expecting to testify whether transfer learning works on harder problems .",
    "since the transfer tasks are more difficult , we report the results with googlenet only , which are shown in table  [ table : imageclef - da ] . the jan model outperforms all comparison methods on most transfer tasks , including the harder ones , e.g. * c@xmath92b * and * i@xmath92b*. this evidence on diverse datasets validates that the jan model can learn transferable features robust to joint distribution shifts for domain adaptation on hard problems .",
    "* feature visualization : * we visualize in figures  [ fig : dans][fig : jant ] the activations of task * a * @xmath92 * w * learned by dan and jan respectively using the t - sne embeddings @xcite . compared with the activations given by dan in figure  [ fig : dans][fig : dant ] , the activations given by jan in figures  [ fig : jans][fig : jant ]",
    "show that the target categories are discriminated much more clearly by the source classifier of jan , which suggests that the joint adaptation of deep features and labels is a powerful approach to effective domain adaptation .",
    "* distribution discrepancy : * the theory of domain adaptation  @xcite suggests the @xmath115-distance as a measure of cross - domain discrepancy , which , together with the source risk , will bound the target risk .",
    "the proxy @xmath116-distance is defined as @xmath117 , where @xmath118 is the generalization error of a domain classifier ( e.g. kernel svm ) trained on the binary problem of classifying source and target .",
    "figure  [ fig : adist ] shows @xmath119 on tasks * a * @xmath92 * w * and * w * @xmath92 * d * with features of cnn , dan , and jan , respectively .",
    "we observe that @xmath119 using jan features is much smaller than @xmath119 using cnn and dan features , which suggests that jan features are more transferable . as domains * w * and * d * are very similar , @xmath119 of task * w * @xmath92 * d * is much smaller than @xmath119 of task * a * @xmath92 * w * , which explains the better performance of * w * @xmath92 * d*.    a limitation of @xmath120-distance is that it can not measure the cross - domain discrepancy of joint distributions , which is addressed by the proposed joint distribution discrepancy ( jdd ) measure .",
    "we compute jdd across domains using cnn , dan and jan activations respectively , based on the features in @xmath93 and labels in @xmath100 under the alexnet architecture .",
    "the results in figure  [ fig : jdde ] show that jdd using jan features is much smaller than jdd using cnn and dan features , which evidence that jans can successfully reduce the shifts in joint distributions to learn more transferable features and classifiers .    *",
    "parameter sensitivity : * we investigate the effects of the jdd penalty parameter @xmath121 . figure  [ fig : sensitivity ] shows the transfer performance of jan based on alexnet and googlenet respectively , by varying @xmath122 on task * a * @xmath92 * d*. the accuracy of jan first increases and then decreases as @xmath121 varies and shows a bell - shaped curve .",
    "this confirms the motivation of learning deep features and adapting joint distributions , since a good trade - off between them can enhance network transferability .",
    "this paper presented a novel approach to deep transfer learning , which enables direct adaptation of joint distributions across domains .",
    "different from previous methods , we eliminate the requirement for separate adaptations of marginal and conditional distributions , which are often subject to rather strong independence assumptions .",
    "the discrepancy between joint distributions of multiple features and labels can be computed by embedding the joint distributions in a tensor - product hilbert space , which can be naturally implemented through most deep networks .",
    "we will constitute as future work the further evaluations on larger - scale datasets and the semi - supervised domain adaptation scenarios ."
  ],
  "abstract_text": [
    "<S> deep networks rely on massive amounts of labeled data to learn powerful models . for a target task short of labeled data , </S>",
    "<S> transfer learning enables model adaptation from a different source domain . </S>",
    "<S> this paper addresses deep transfer learning under a more general scenario that the joint distributions of features and labels may change substantially across domains . </S>",
    "<S> based on the theory of hilbert space embedding of distributions , a novel joint distribution discrepancy is proposed to directly compare joint distributions across domains , eliminating the need of marginal - conditional factorization . </S>",
    "<S> transfer learning is enabled in deep convolutional networks , where the dataset shifts may linger in multiple task - specific feature layers and the classifier layer . </S>",
    "<S> a set of joint adaptation networks are crafted to match the joint distributions of these layers across domains by minimizing the joint distribution discrepancy , which can be trained efficiently using back - propagation . </S>",
    "<S> experiments show that the new approach yields state of the art results on standard domain adaptation datasets . </S>"
  ]
}