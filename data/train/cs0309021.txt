{
  "article_text": [
    "the growing number of multimedia contents available via the world wide web , cd - roms , and dvds has made information technologies incorporating speech , image , and text processing crucial .",
    "of the various types of contents , lectures ( audio / video ) are typical and a valuable multimedia resource , in which speeches ( i.e. , oral presentations ) are usually organized based on text materials , such as resumes , slides , and textbooks . in lecture videos , image information , such as flip charts , is often also used . in other words ,",
    "a single lecture consists of different types of compatible multimedia contents .    because a single lecture often refers to several topics and takes a long time , it is useful to obtain specific segments ( passages ) selectively so that the audience can satisfy their information needs at minimum cost . to resolve this problem ,",
    "in this paper we propose a lecture - on - demand system that retrieves relevant video / audio passages in response to user queries .",
    "for this purpose , we utilize the benefits of different media types to improve retrieval performance .    on the one hand ,",
    "text has the advantage that users can view / scan the entire contents quickly and can easily identify relevant passages using the layout information ( e.g. , text structures based on sections and paragraphs ) .",
    "in other words , text contents can be used for random - access purposes . on the other hand",
    ", speech is used mainly for sequential - access purposes .",
    "therefore , it is difficult to identify relevant passages unless target video / audio data includes additional annotation , such as indexes . even if the target data are indexed , users are not necessarily able to provide effective queries . to resolve this problem ,",
    "textbooks are desirable materials from which users can extract effective keywords and phrases .",
    "however , while textbooks are usually concise , speech has a high degree of redundancy and therefore is easier to understand than textbooks , especially where additional image information is provided .    in view of the above ,",
    "we model our lecture - on - demand ( lod ) system as follows .",
    "a user selects text segments ( keywords , phrases , sentences , and paragraphs ) that are relevant to their information needs from a textbook for a target lecture . by using selected segments ,",
    "a text query is generated automatically .",
    "that is , queries can be formulated even if users can not provide effective keywords .",
    "users can also submit additional keywords as queries , if necessary .",
    "video passages relevant to a given query are retrieved and presented to the user . to retrieve the video passages in response to text queries",
    ", we extract the audio track from a lecture video , generate a transcription by means of large vocabulary continuous speech recognition , and produce a text index , prior to system use .",
    "our system is a cross - media system in the sense that users can retrieve video and audio information by means of text queries .",
    "figure  [ fig : system ] depicts the overall design of our lecture - on - demand system , in which the left and right regions correspond to the on - line and off - line processes , respectively .",
    "although our system is currently implemented for japanese , our methodology is fundamentally language independent . for the purpose of research and development , we tentatively target lecture programs on tv for which textbooks are published .",
    "we explain the basis of our system using figure  [ fig : system ] .    in the off - line process , given the video data of a target lecture , audio data are extracted and segmented into a number of passages .",
    "then , a speech recognition system transcribes each passage .",
    "finally , the transcribed passages are indexed as in conventional text retrieval systems , so that each passage can be retrieved efficiently in response to text queries . to adapt speech recognition to a specific lecturer , we perform unsupervised speaker adaptation using an initial speech recognition result ( i.e. , a transcription ) . to adapt speech recognition to a specific topic , we perform language model adaptation , for which we search a general corpus for documents relevant to the textbook related to a target lecture . then , retrieved documents ( i.e. , a topic - specific corpus ) are used to produce a word - based n - gram language model .",
    "we also perform image analysis to extract text ( e.g. , keywords and phrases ) from flip charts .",
    "these contents are also used to improve our language model .    in the on - line process",
    ", a user can view specific video passages by submitting any text queries , i.e. , keywords , phrases , sentences , and paragraphs , extracted from the textbook .",
    "any queries not in the textbook can also be used .",
    "the current implementation is based on a client - server system on the web .",
    "both the off - line and on - line processes are performed on servers , but users can access our system using web browsers on their own pcs .",
    "figure  [ fig : lodem ] depicts a prototype interface of our lod system , in which a lecture associated with `` nonlinear multivariate analysis '' is given . in this interface , an electronic version of a textbook",
    "is displayed on the left side , and a lecture video is played on the right side . in addition , users can submit any text queries in the input box , which is not depicted in figure  [ fig : lodem ] . in this scenario , a text paragraph related to `` discriminant analysis '' was copied and pasted into the query input box , and top - ranked transcribed passages for the query were listed according to the degree of relevance ( in the lower part of figure  [ fig : lodem ] ) .",
    "users can select ( click on ) transcriptions to play the corresponding video passage .",
    "it should be noted that unlike conventional keyword - based retrieval systems , in which users usually submit a small number of keywords , in our system users can easily submit longer queries using textbooks . where submitted keywords are misrecognized in transcriptions , the retrieval accuracy decreases",
    ". however , longer queries are relatively robust for speech recognition errors , because the effect of misrecognized words is overshadowed by the large number of words correctly recognized .",
    "the basis of passage segmentation is to divide the entire video data for a single lecture into more than one unit to be retrieved .",
    "we call these smaller units `` passages '' . for this purpose",
    ", both speech and image data can provide promising clues .",
    "however , in lecture tv programs , it is often the case that a lecturer sitting still is the main focus and a small number of flip charts are used occasionally . in such cases ,",
    "image data is less informative .",
    "therefore , tentatively we use only speech data for the passage segmentation process .",
    "however , segmentation can potentially vary depending on the user query .",
    "thus , it is difficult to predetermine a desirable segmentation in the off - line process .",
    "because of the above problems , we first extract the audio track from a target video and use a simple pause - based segmentation method to obtain minimal speech units , such as sentences and long phrases . in other words ,",
    "speech units are continuous audio segments that do not include pauses longer than a certain threshold . finally , we generate variable - length passages from one or more speech units . to put it more precisely , we combine @xmath0 speech units into a single passage , with @xmath0 ranging from 1 to 5 in the current implementation .",
    "the speech recognition module generates word sequence @xmath1 , given phone sequence @xmath2 . in a stochastic framework",
    ", the task is to select the @xmath1 maximizing @xmath3 , which is transformed as in equation  ( [ eq : bayes ] ) through the bayesian theorem .",
    "@xmath4 @xmath5 models the probability that the word sequence @xmath1 is transformed into the phone sequence @xmath2 , and @xmath6 models the probability that @xmath1 is linguistically acceptable .",
    "these factors are called the acoustic and language models , respectively .",
    "we use the japanese dictation toolkit , which includes the julius decoder and acoustic / language models .",
    "julius performs a two - pass ( forward - backward ) search using word - based forward bigrams and backward trigrams .",
    "the acoustic model was produced from the asj speech database , which contains approximately 20,000 sentences uttered by 132 speakers including both gender groups .",
    "a 16-mixture gaussian distribution triphone hidden markov model , in which states are clustered into 2,000 groups by a state - tying method , is used .",
    "we adapt the provided acoustic model by means of an mllr - based unsupervised speaker adaptation method , for which in practice we use the htk toolkit .",
    "existing methods to adapt language models can be classified into two fundamental categories . in the first category  the _ integration _ approach ",
    "general and topic - specific corpora are integrated to produce a topic - specific language model  @xcite . because the sizes of those corpora differ , n - gram statistics are calculated using the weighted average of the statistics extracted independently from those corpora . however , it is difficult to determine the optimal weight depending on the topic . in the second category ",
    "the _ selection _ approach  a topic - specific subset is selected from a general corpus and is used to produce a language model .",
    "this approach is effective if general corpora contain documents associated with target topics , but n - gram statistics in those documents are overshadowed by other documents in resultant language models .",
    "we followed the selection approach , because the 10 m web page corpus  @xcite containing mainly japanese pages associated with various topics was publicly available .",
    "the quality of the selection approach depends on the method of selecting topic - specific subsets .",
    "an existing method  @xcite uses hypotheses in the initial speech recognition phase as queries to retrieve topic - specific documents from a general corpus .",
    "however , errors in the initial hypotheses have the potential to decrease the retrieval accuracy . instead",
    ", we use textbooks related to target lectures as queries to improve the retrieval accuracy and consequently the quality of the language model adaptation .",
    "given transcribed passages and text queries , the basis of the retrieval module is the same as that for text retrieval .",
    "we use an existing probabilistic text retrieval method  @xcite to compute the relevance score between the query and each passage in the database .",
    "the relevance score for passage @xmath7 is computed by equation  ( [ eq : okapi ] ) .",
    "@xmath8 where @xmath9 and @xmath10 denote the frequency with which term @xmath11 appears in query @xmath12 and passage @xmath7 , respectively . @xmath0 and @xmath13 denote the total number of passages in the database and the number of passages containing term @xmath11 , respectively .",
    "@xmath14 denotes the length of passage @xmath7 , and @xmath15 denotes the average length of passages in the database .",
    "we empirically set @xmath16 and @xmath17 , respectively .",
    "we use content words , such as nouns , extracted from transcribed passages as index terms , and perform word - based indexing .",
    "we use the chasen morphological analyzer to extract content words .",
    "the same method is used to extract terms from queries .",
    "however , retrieved passages are not disjoint , because top - ranked passages often overlap with one another in terms of the temporal axis .",
    "it is redundant simply to list the top - ranked retrieved passages as they are .",
    "therefore , we reorganize those overlapped passages into a single passage .",
    "the relevance score for a group ( a new passage ) is computed by averaging the scores of all passages belonging to the group .",
    "new passages are sorted according to the degree of relevance and are presented to users as the final result .",
    "to evaluate the performance of our lod system , we produced a test collection ( as a benchmark data set ) and performed experiments partially resembling a task performed in the trec spoken document retrieval ( sdr ) track  @xcite .",
    "five lecture programs on tv ( each lecture was 45 minutes long ) , for which printed textbooks were also published , were videotaped in dv and were used as target lectures . each lecture was manually transcribed and sentence boundaries with temporal information ( i.e. , correct speech units ) were also identified manually . each paragraph in the corresponding textbook was used as a query independently . for each query ,",
    "a human assessor ( a graduate student not an author of this paper ) identified one or more relevant sentences in the human transcription .    using our test collection",
    ", we evaluated the accuracy of speech recognition and passage retrieval .",
    "for the five lectures , our system used the sentence boundaries in human transcriptions to identify speech units , and performed speech recognition .",
    "we also used human transcriptions as perfect speech recognition results and investigated the extent to which speech recognition errors affect the retrieval accuracy .",
    "our system retrieved top - ranked passages in response to each query .",
    "note that the passages here are those grouped based on the temporal axis , which should not be confused with those obtained from the passage segmentation method .",
    "[ cols=\"<,<,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ tab : results ]    to evaluate the accuracy of speech recognition , we used the word error rate ( wer ) , which is the ratio of the number of word errors ( deletion , insertion , and substitution ) to the total number of words .",
    "we also used test - set out - of - vocabulary rate ( oov ) and trigram test - set perplexity ( pp ) to evaluate the extent to which our language model adapted to the target topics .",
    "we used human transcriptions as test set data .",
    "for example , oov is the ratio of the number of word tokens not contained in the language model for speech recognition to the total number of word tokens in the transcription .",
    "note that smaller values of oov , pp , and wer are obtained with better methods .",
    "the final outputs ( i.e. , retrieved passages ) were evaluated based on recall and precision , averaged over all queries . recall ( r )",
    "is the ratio of the number of correct speech units retrieved by our system to the total number of correct speech units for the query in question . precision ( p ) is the ratio of the number of correct speech units retrieved by our system to the total number of speech units retrieved by our system . to summarize recall and precision into a single measure",
    ", we used the f - measure ( f ) .",
    "table  [ tab : results ] shows the accuracy of speech recognition ( wer ) and passage retrieval ( r , p , and f ) , for each lecture . in this table",
    ", the columns `` hum '' and `` asr '' correspond to the results obtained with human transcriptions and automatic speech recognition , respectively .",
    "the column `` + la '' denotes results for asr combined with language model adaptation .",
    "the column `` topic '' denotes topics for the five lectures .    to adapt language models",
    ", we used the textbook corresponding to a target lecture and searched the 10 m web page corpus for 2,000 relevant pages , which were used as a source corpus . in the case where the language model adaptation was not performed ,",
    "all 10 m web pages were used as a source corpus . in either case ,",
    "20,000 high frequency words were selected from a source corpus to produce a word - based trigram language model .",
    "we used the chasen morphological analyzer to extract words ( morphemes ) from the source corpora , because japanese sentences lack lexical segmentation .    in passage retrieval , we regarded the top @xmath0 passages as the final outputs . in table",
    "[ tab : results ] , the value of @xmath0 ranges from 1 to 3 . as the value of @xmath0 increases , the recall improves , but potentially sacrificing precision .      by comparing the results of asr and +",
    "la in table  [ tab : results ] , for some cases oov and pp increased by adapting language models .",
    "however , wer decreased by adapting language models to target topics , irrespective of the lecture .",
    "the values of oov , pp , and wer for lecture  # 1 were generally smaller than those for the other lectures .",
    "one possible reason is that the lecturer of # 1 spoke more fluently and made fewer erroneous utterances than the other lecturers .",
    "recall , precision , and f - measure increased by adapting language models for lectures  # 2 - 5 , irrespective of the number of passages retrieved . for lecture  # 1 , the retrieval accuracy did not significantly differ whether or not we adapted the language model to the topic .",
    "one possible reason is that the wer of lecture  # 1 without language model adaptation ( 20.9% ) was sufficiently small to obtain a retrieval accuracy comparable with the text retrieval  @xcite .",
    "the difference between hum and asr was marginal in terms of the retrieval accuracy .",
    "therefore , the effect of the language model adaptation method was overshadowed in passage retrieval .",
    "the retrieval accuracy for lecture  # 1 was higher than those for the other lectures .",
    "the story of lecture  # 1 was organized based primarily on the textbook , when compared with the other lectures .",
    "this suggests that the performance of our lod system is dependent of the organization of target lectures .",
    "surprisingly , for lectures  # 1 and # 2 , recall , precision , and f - measure of + la were better than those of hum .",
    "this means that the automatic transcription was more effective than human transcription for passage retrieval purposes .",
    "one possible reason is the existence of japanese variants ( i.e. , more than one spelling form corresponding to the same word ) , such as `` _ girisha_/ _ girishia _  ( greece ) '' .",
    "because the language model was adapted by means of the textbook for a target lecture , the spelling in automatic transcriptions systematically resembled that in the queries extracted from the textbooks .",
    "in contrast , it is difficult to standardize the spelling in human transcriptions .",
    "therefore , relevant passages in automatic transcriptions were more likely to be retrieved than passages in the human transcriptions .",
    "we conclude that our language model adaptation method was effective for both speech recognition and passage retrieval .",
    "reflecting the rapid growth in the use of multimedia contents , information technologies appropriate to speech , image , and text processing are crucial . of the various content types in this paper we focused on the video data of lectures with their organization based on textbooks , and proposed a system for cross - media on - demand lectures , in which users can formulate text queries using the textbook for a target lecture to retrieve specific video passages .",
    "to retrieve video passages in response to text queries , we extract the audio track from a lecture video , generate a transcription by large vocabulary continuous speech recognition , and produce a text index , prior to system use .",
    "we evaluated the performance of our system experimentally , for which five tv lecture programs in various topics were used .",
    "the experimental results showed that the accuracy of speech recognition varied depending on the topic and presentation style of the lecturers .",
    "however , the accuracy of speech recognition and passage retrieval was improved by adapting language models to the topic of the target lecture .",
    "even if the word error rate was approximately 40% , the accuracy of retrieval was comparable with that obtained by human transcription .      c.  auzanne , j.  s. garofolo , j.  g. fiscus , and w.  m. fisher , `` automatic language model adaptation for spoken document retrieval , '' in _ proceedings of riao 2000 conference on content - based multimedia information access _",
    ", 2000 .",
    "k.  eguchi , k.  oyama , k.  kuriyama , and n.  kando , `` the web retrieval task and its evaluation in the third ntcir workshop , '' in _ proceedings of the 25th annual international acm sigir conference on research and development in information retrieval _ , 2002 , pp .",
    "375376 .",
    "l.  chen , j .- l .",
    "gauvain , l.  lamel , g.  adda , and m.  adda , `` language model adaptation for broadcast news transcription , '' in _ proceedings of isca workshop on adaptation methods for speech recognition _ , 2001 .",
    "s.  robertson and s.  walker , `` some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval , '' in _ proceedings of the 17th annual international acm sigir conference on research and development in information retrieval _ , 1994 , pp",
    ". 232241 .",
    "j.  s. garofolo , e.  m. voorhees , v.  m. stanford , and k.  s. jones , `` trec-6 1997 spoken document retrieval track overview and results , '' in _ proceedings of the 6th text retrieval conference _ , 1997 , pp ."
  ],
  "abstract_text": [
    "<S> we propose a cross - media lecture - on - demand system , in which users can selectively view specific segments of lecture videos by submitting text queries . </S>",
    "<S> users can easily formulate queries by using the textbook associated with a target lecture , even if they can not come up with effective keywords . </S>",
    "<S> our system extracts the audio track from a target lecture video , generates a transcription by large vocabulary continuous speech recognition , and produces a text index . </S>",
    "<S> experimental results showed that by adapting speech recognition to the topic of the lecture , the recognition accuracy increased and the retrieval accuracy was comparable with that obtained by human transcription . </S>"
  ]
}