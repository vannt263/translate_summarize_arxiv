{
  "article_text": [
    "[ [ differential - privacy . ] ] differential privacy .",
    "+ + + + + + + + + + + + + + + + + + + + +    social and communication networks have been the subject of intense study over the last few years .",
    "however , while these networks comprise a rich source of information for science , they also contain highly sensitive private information .",
    "what kinds of information can we release about these networks while preserving the privacy of their users ?",
    "simple measures , such as removing obvious identifiers , do not work ; for example , several studies ( e.g. , @xcite ) reidentified individuals in the graph of a social network even after all vertex and edge attributes were removed .",
    "such attacks highlight the need for statistical and learning algorithms that provide rigorous privacy guarantees .",
    "differential privacy @xcite , which emerged from a line of work started by @xcite , provides meaningful guarantees in the presence of arbitrary side information . in a traditional statistical data set , where each person corresponds to a single record ( or row of a table )",
    ", differential privacy guarantees that adding or removing any particular person s data will not noticeably change the distribution on the analysis outcome .",
    "there is now a rich and deep literature on differentially private methodology for learning and other algorithmic tasks ; see @xcite for a recent tutorial .",
    "by contrast , differential privacy in the context of graph data is much less developed .",
    "there are two main variants of graph differential privacy : _ edge _ and _ node _ differential privacy .",
    "intuitively , edge differential privacy ensures that an algorithm s output does not reveal the inclusion or removal of a particular edge in the graph , while node differential privacy hides the inclusion or removal of a node together with all its adjacent edges .",
    "edge privacy is a weaker notion ( hence easier to achieve ) and has been studied more extensively , with particular emphasis on the release of individual graph statistics @xcite , the degree distribution @xcite , and data structures for estimating the edge density of all cuts in a graph @xcite .",
    "several authors designed edge - differentially private algorithms for fitting generative graph models @xcite , but these do not appear to generalize to node privacy with meaningful accuracy guarantees .    the stronger notion , node privacy , corresponds more closely to what was achieved in the case of traditional data sets , and to what one would want to protect an individual s data : it ensures that _ no matter what an analyst observing the released information knows ahead of time _ , she learns the same things about an individual alice regardless of whether alice s data are used or not .",
    "in particular , no assumptions are needed on the way the individuals data are generated ( they need not even be independent ) .",
    "node privacy was studied more recently @xcite , with a focus on on the release of descriptive statistics ( such as the number of triangles in a graph ) .",
    "unfortunately , differential privacy s stringency makes the design of accurate , node - private algorithms challenging .    in this work",
    ", we provide the first algorithms for node - private inference of a high - dimensional statistical model that does not admit simple sufficient statistics .    [",
    "[ modeling - large - graphs - via - graphons . ] ] modeling large graphs via graphons .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    traditionally , large graphs have been modeled using various parametric models , one of the most popular being the stochastic block model @xcite .",
    "here one postulates that an observed graph was generated by first assigning vertices at random to one of @xmath3 groups , and then connecting two vertices with a probability that depends on the groups the two vertices are members of .",
    "as the number of vertices of the graph in question grows , we do not expect the graph to be well described by a stochastic block model with a fixed number of blocks . in this paper",
    "we consider nonparametric models ( where the number of parameters need not be fixed or even finite ) given in terms of a _",
    "graphon_. a graphon is a measurable , bounded function @xmath4 ^ 2\\to [ 0,\\infty)$ ] such that @xmath5 , which for convenience we take to be normalized : @xmath6 . given a graphon , we generate a graph on @xmath7 vertices by first assigning i.i.d .",
    "uniform labels in @xmath8 $ ] to the vertices , and then connecting vertices with labels @xmath9 with probability @xmath10 , where @xmath11 is a parameter determining the density of the generated graph @xmath12 with @xmath13 .",
    "we call @xmath12 a @xmath1-random graph with target density @xmath11 ( or simply a @xmath14-random graph ) .    to our knowledge ,",
    "random graph models of the above form were first introduced under the name latent position graphs @xcite , and are special cases of a more general model of `` inhomogeneous random graphs '' defined in @xcite , which is the first place were @xmath7-dependent target densities @xmath11 were considered . for both dense graphs ( whose target density does not depend on the number of vertices ) and sparse graphs ( those for which @xmath15 as @xmath16 ) , this model is related to the theory of convergent graph sequences @xcite . for dense graphs it was first explicitly proposed in @xcite , though it can be implicitly traced back to @xcite , where models of this form appear as extremal points of two - dimensional exchangeable arrays ; see @xcite ( roughly , their results relate graphons to exchangeable arrays the way de finetti s theorem relates i.i.d .",
    "distributions to exchangeable sequences ) . for sparse graphs",
    ", @xcite offers a different nonparametric approach .",
    "[ [ estimation - and - identifiability . ] ] estimation and identifiability .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    assuming that @xmath12 is generated in this way , we are then faced with the task of estimating @xmath1 from a _ single observation _ of a graph @xmath12 . to our knowledge , this task was first explicitly considered in @xcite , which considered graphons describing stochastic block models with a fixed number of blocks .",
    "this was generalized to models with a growing number of blocks @xcite , while the first estimation of the nonparametric model was proposed in @xcite .",
    "various other estimation methods were proposed recently , for example @xcite .",
    "these works make various assumptions on the function @xmath1 , the most common one being that after a measure - preserving transformation , the integral of @xmath1 over one variable is a strictly monotone function of the other , corresponding to an asymptotically strictly monotone degree distribution of @xmath12 .",
    "( this assumption is quite restrictive : in particular , such results do not apply to graphons that represent block models . ) for our purposes , the most relevant works are @xcite and @xcite , which provide consistent estimators without monotonicity assumptions ( see `` comparison to nonprivate bounds '' , below ) .",
    "one issue that makes estimation of graphons challenging is _ identifiability _ : multiple graphons can lead to the same distribution on @xmath12 .",
    "specifically , two graphons @xmath1 and @xmath17 lead to the same distribution on @xmath1-random graphs if and only if there are measure preserving maps @xmath18\\to[0,1]$ ] such that @xmath19 , where @xmath20 is defined by @xmath21 @xcite .",
    "hence , there is no `` canonical graphon '' that an estimation procedure can output , but rather an equivalence class of graphons .",
    "some of the literature circumvents identifiability by making strong additional assumptions , such as strict monotonicity , that imply the existence of canonical equivalent class representatives .",
    "we make no such assumptions , but instead define consistency in terms of a metric on these equivalence classes , rather than on graphons as functions .",
    "we use a variant of the @xmath2 metric , @xmath22\\to[0,1 ] }   \\|w^\\phi - w'\\|_2\\ , .\\ ] ] where @xmath23 ranges over measure - preserving bijections .      in this paper",
    "we construct an algorithm that produces an estimate @xmath24 from a single instance @xmath12 of a @xmath1-random graph with target density @xmath11 ( or simply @xmath25 , when @xmath7 is clear from the context ) .",
    "we aim for several properties :    @xmath24 is differentially private ;    @xmath24 is consistent , in the sense that @xmath26 in probability as @xmath16 ;    @xmath24 has a compact representation ( in our case , as a matrix with @xmath27 entries ) ;    the procedure works for sparse graphs , that is , when the density @xmath25 is small ;    on input @xmath12 , @xmath24 can be calculated efficiently .",
    "here we give an estimation procedure that obeys the first four properties , leaving the question of polynomial - time algorithms for future work . given an input graph @xmath12 , a privacy - parameter @xmath28 and a target number @xmath3 of blocks , our algorithm @xmath29 produces a @xmath3-block graphon @xmath30 such that    @xmath29 is @xmath28-differentially node private .",
    "the privacy guarantee holds for all inputs , independent of modeling assumptions .",
    "assume that    @xmath1 is an arbitrary graphon , normalized so @xmath31 ;    the expected average degree @xmath32 grows at least as fast as @xmath33 ; and    @xmath3 goes to infinity sufficiently slowly with @xmath7 .",
    "then when @xmath12 is @xmath34-random , the estimate @xmath24 for @xmath1 is _ consistent _ ( that is , @xmath35 , both in probability and almost surely ) .",
    "combined with the general theory of convergent graphs sequences , these result in particular give a node - private procedure for estimating the edge density of all cuts in a @xmath34-random graph , see in section  [ sec : w - rand ] below .",
    "the main idea of our algorithm is to use the exponential mechanism of @xcite to select a block model which approximately minimizes the @xmath36 distance to the observed adjacency matrix of @xmath0 , under the best possible assignment of nodes to blocks ( this explicit search over assignments makes the algorithm take exponential time ) . in order to get an algorithm that is accurate on sparse graphs ,",
    "we need several nontrivial extensions of current techniques . to achieve privacy",
    ", we use a new variation of the lipschitz extension technique of @xcite to reduce the sensitivity of the @xmath37 distance . while those works used lipschitz extensions for noise addition , we use of lipshitz extensions inside the `` exponential mechanism ''  @xcite ( to control the sensitivity of the score functions ) .",
    "to bound our algorithm s error , we provide a new analysis of the @xmath36-minimization algorithm ; we show that approximate minimizers are not too far from the actual minimizer ( a `` stability '' property ) .",
    "both aspects of our work are enabled by restricting the @xmath38-minimization to a set of block models whose density ( in fact , @xmath39 norm ) is not much larger than that of the underlying graph .",
    "the algorithm is presented in section  [ sec : estimation ] .",
    "our most general result proves consistency for arbitrary graphons @xmath1 but does not provides a concrete rate of convergence .",
    "however , we provide explicit rates under various assumptions on @xmath1 .",
    "specifically , we relate the error of our estimator to two natural error terms involving the graphon @xmath1 : the error @xmath40 of the best @xmath3-block approximation to @xmath1 in the @xmath2 norm ( see below ) and an error term @xmath41 measuring the @xmath2-distance between the graphon @xmath1 and the matrix of probabilities @xmath42 generating the graph @xmath12 ( see below . ) in terms of these error terms , theorem  [ thm : final ] shows @xmath43{\\frac{\\log k}{\\rho n } }   + \\sqrt{\\frac{k^2\\log n}{n\\eps } } + \\frac{1}{\\rho \\eps n } } \\right)}}.\\label{eq : main - bound}\\ ] ]    along the way , we provide a novel analysis of a straightforward , nonprivate least - squares estimator , whose error bound has a better dependence on @xmath3 : @xmath44{\\frac{\\log k}{\\rho",
    "n }         + \\frac { k^2}{\\rho n^2 } } } \\right)}}. \\label{eq : main - bound - nonprivate}\\ ] ]    it follows from the theory of graph convergence that for all graphons @xmath1 , we have @xmath45 as @xmath46 and @xmath47 almost surely as @xmath48 .",
    "as proven in appendix  [ sec : sampling - k - block ] , we also have @xmath49{k / n})$ ] , though this upper bound is loose in many cases .    as a specific instantiation of these bounds ,",
    "let us consider the case that @xmath1 is exactly described by a @xmath3-block model , in which case @xmath50 and @xmath51{k / n})$ ] ( see appendix  [ sec : sampling - k - block ] ) .",
    "for @xmath52 , @xmath53 and constant @xmath28 , our private estimator has an asymptotic error that is dominated by the ( unavoidable ) error of @xmath54{k / n}$ ] , showing that we do not lose anything due to privacy in this special case .",
    "another special case is when @xmath1 is @xmath55-hlder continuous , in which case @xmath56 and @xmath57 ; see remark  [ rem : hoelder - cont - w ] below .",
    "[ [ comparison - to - previous - nonprivate - bounds . ] ] comparison to previous nonprivate bounds .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we provide the first consistency bounds for estimation of a nonparametric graph model subject to node differential privacy . along the way , for sparse graphs",
    ", we provide more general consistency results than were previously known , regardless of privacy .",
    "in particular , to the best of our knowledge , _ no prior results give a consistent estimator for @xmath1 that works for sparse graphs without any additional assumptions besides boundedness .",
    "when compared to results for nonprivate algorithms applied to graphons obeying additional assumptions , our bounds are often incomparable , and in other cases match the existing bounds .",
    "we start by considering graphons which are themselves step functions with a known number of steps @xmath3 . in the dense case ,",
    "the nonprivate algorithms of @xcite and @xcite , as well as our nonprivate algorithm , give an asymptotic error that is dominated by the term @xmath58{k / n})$ ] , which is of the same order as our private estimator as long as @xmath59 .",
    "@xcite provided the first convergence results for estimating graphons in the sparse regime . assuming that @xmath1 is bounded above and below ( so it takes values in a range @xmath60 $ ] where @xmath61 )",
    ", they analyze an inefficient algorithm ( the mle ) .",
    "the bounds of @xcite are incomparable to ours , though for the case of @xmath3-block graphons , both their bounds and our nonprivate bound are dominated by the term @xmath62{k / n}$ ] when @xmath63 and @xmath64 .",
    "a different sequence of works shows how to consistently estimate the underlying block model with a _",
    "fixed _ number of blocks @xmath3 in polynomial time for very sparse graphs ( as for our non - private algorithm , the only thing which is needed is that @xmath65 ) @xcite ; we are not aware of concrete bounds on the convergence rate .    for the case of _ dense _ @xmath55-hlder - continuous graphons , the results of @xcite give an error which is dominated by the term @xmath57 . for @xmath66 ,",
    "our nonprivate bound matches this bound , while for @xmath67 it is worse .",
    "@xcite consider the sparse case .",
    "the rate of their estimator is incomparable to the rate of our estimator ; further , their analysis requires a lower bound on the edge probabilities , while ours does not .",
    "see appendix  [ sec : comparisons ] for a more detailed discussion of the previous literature .",
    "for a graph @xmath0 on @xmath68=\\{1,\\dots , n\\}$ ] , we use @xmath69 and @xmath70 to denote the edge set and the adjacency matrix of @xmath0 , respectively . the edge density @xmath71 is defined as the number of edges divided by @xmath72 .",
    "finally the degree @xmath73 of a vertex @xmath74 in @xmath0 is the number of edges containing @xmath74 .",
    "we use the same notation for a weighted graph with nonnegative edge weights @xmath75 , where now @xmath76 , and @xmath77 .",
    "we use @xmath78 to denote the set of weighted graphs on @xmath7 vertices with weights in @xmath8 $ ] , and @xmath79 to denote the set of all graphs in @xmath78 that have maximal degree at most @xmath80 .",
    "[ [ from - matrices - to - graphons . ] ] from matrices to graphons .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + +    we define a graphon to be a bounded , measurable function @xmath4 ^ 2\\to { \\mathbb{r}}_+$ ] such that @xmath5 for all @xmath81 $ ]",
    ". it will be convenient to embed the set of a symmetric @xmath82 matrix with nonnegative entries into graphons as follows : let @xmath83 be the partition of @xmath8 $ ] into adjacent intervals of lengths @xmath84 .",
    "define @xmath85}}$ ] to be the step function which equals @xmath86 on @xmath87 .",
    "if @xmath88 is the adjacency matrix of an unweighted graph @xmath0 , we use @xmath89}}$ ] for @xmath85}}$ ] .",
    "[ [ distances . ] ] distances .",
    "+ + + + + + + + + +    for @xmath90 we define the @xmath91 norm of an @xmath82 matrix @xmath88 by @xmath92 , and the @xmath91 norm of a ( borel)-measurable function @xmath4 ^ 2\\to { \\mathbb{r}}$ ] by @xmath93 .",
    "associated with the @xmath2-norm is a scalar product , defined as @xmath94 for two @xmath82 matrices @xmath88 and @xmath95 , and @xmath96 for two square integrable functions",
    "@xmath97 ^ 2\\to{\\mathbb{r}}$ ] .",
    "note that with this notation , the edge density and the @xmath98 norm are related by @xmath99 .",
    "recalling , we define the @xmath37 distance between two matrices @xmath100 , or between a matrix @xmath88 and a graphon @xmath1 by @xmath101}},{{w[{b}]}})$ ] and @xmath102}},w)$ ] . in addition , we will also use the in general larger distances @xmath103 and @xmath104 , defined by taking a minimum over matrices @xmath105 which are obtained from @xmath88 by a relabelling of the indices : @xmath106 and @xmath107 .",
    "[ [ w - random - graphs - and - stochastic - block - models . ] ] w - random graphs and stochastic block models .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given a graphon @xmath1 we define a random @xmath82 matrix @xmath108 by choosing @xmath7 `` positions '' @xmath109 i.i.d .",
    "uniformly at random from @xmath8 $ ] and then setting @xmath110 . if @xmath111 , then @xmath42 has entries in @xmath8 $ ] , and we can form a random graph @xmath112 on @xmath7-vertices by choosing an edge between two vertices @xmath113 with probability @xmath114 , independently for all @xmath113 . following",
    "@xcite we call @xmath115 a @xmath1-random graph and @xmath42 a @xmath1-weighted random graph .",
    "we incorporate a target density @xmath11 ( or simply @xmath25 , when @xmath7 is clear from the context ) by normalizing @xmath1 so that @xmath6 and taking @xmath0 to be a sample from @xmath116 . in other words",
    ", we set @xmath117 and then connect @xmath74 to @xmath118 with probability @xmath119 , independently for all @xmath113 .",
    "the error from our main estimates measuring the distance between @xmath42 and @xmath1 is defined as @xmath120 and goes to zero as @xmath16 by the following lemma , which follows easily from the results of @xcite .",
    "[ lem : h_n - convergence ] let @xmath1 be a graphon with @xmath121 .",
    "with probability one , @xmath122 and @xmath123    stochastic block models are specific examples of @xmath1-random graph in which @xmath1 is constant on sets of the form @xmath87 , where @xmath124 is a partition of @xmath8 $ ] into intervals of possibly different lengths .",
    "[ [ approximation - by - block - models . ] ] approximation by block models .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in the opposite direction , we can map a function @xmath1 to a matrix @xmath95 by the following procedure .",
    "starting from an arbitrary partition @xmath125 of @xmath8 $ ] into sets of equal lebesgues measure , define @xmath126 to be the matrix obtained by averaging over sets of the form @xmath127 , @xmath128 where @xmath129 denotes the lebesgue measure . finally , we will use @xmath130 to denote the step function @xmath131}(w/{\\mathcal{p}})_{ij}1_{y_i}\\times 1_{y_j}.\\ ] ] using the above averaging procedure ,    it is easy to see that any graphon @xmath1 can be well approximated by a block model .",
    "indeed , let @xmath132}}\\|_2\\ ] ] where the minimum goes over all @xmath133 matrices @xmath95 .",
    "given that we are minimizing the @xmath2-distance , the minimimizer can easily be calculated , and is equal to @xmath134 , where @xmath135 is a partition of @xmath8 $ ] into adjacent intervals of lengths @xmath136 .",
    "it then follows from the lebesgue density theorem ( see , e.g. , @xcite for details ) that @xmath137 as @xmath138 .",
    "we will take the above approximation as a benchmark for our approach , and consider it the error an `` oracle '' could obtain ( hence the superscript @xmath139 ) .",
    "[ [ convergence . ] ] convergence .",
    "+ + + + + + + + + + + +    [ sec : convergence ]    the theory of graph convergence was first developed @xcite , where it was formulated for dense graphs , and then generalized to sparse graphs in @xcite .",
    "one of the notions of graph convergence considered in these papers is the notion of convergence in metric .",
    "the metric in question is similar to the metric @xmath37 , but instead of the @xmath2-norm , one starts from the cut - norm @xmath140 first defined in @xcite , @xmath141 } \\bigl| \\int_{s\\times t}w    \\bigr|,\\ ] ] where the supremum goes over all measurable sets @xmath142 $ ] .",
    "the cut - distance @xmath143 between two integrable functions @xmath144",
    "^ 2\\to{\\mathbb{r}}$ ] is then defined as @xmath145 where the inf goes over all measure preserving bijections on @xmath8 $ ] .",
    "we will also need the following variations : a distance @xmath146 between two graphs on the same node set , as well as a distance @xmath147 between a graph @xmath0 and a graphon @xmath1 , defined as @xmath148}}-{{w[{g}]}}\\|_\\square $ ] and @xmath149}}-w\\|_\\square , $ ] respectively , where the minimum goes over graphs @xmath150 isomorphic to @xmath0 .",
    "given these notions , we say a ( random or deterministic ) sequence @xmath12 of graphs converges to a graphon @xmath1 in the cut metric if , as @xmath16 , @xmath151}},w } \\right)}}\\to 0\\ , .\\ ] ]    with this notion of convergence , for any graphon @xmath1 with @xmath6 , a sequence of @xmath1-random graphs @xmath12 with target density @xmath11 converges to the generating graphon @xmath1 .",
    "this    was shown for bounded @xmath1 and @xmath7-independent target densities @xmath25 with @xmath152 in @xcite , but the statement is much more general , and in particular holds for arbitrary target densities @xmath11 as long as @xmath153 and @xmath154 @xcite .    [ [ estimation - of - multi - way - cuts . ] ] estimation of multi - way cuts .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    using the results of @xcite , the convergence of @xmath12 in the cut - metric @xmath143 implies many interesting results for estimating various quantities defined on the graph @xmath12 . indeed , a consistent approximation @xmath24 to @xmath1 in the metric @xmath37 is clearly consistent in the weaker metric @xmath143 .",
    "but this distance controls various quantities of interest to computer scientists , e.g. , the size of all multi - way cuts , implying that a consistent estimator for @xmath1 also gives consistent estimators for all multi - way cuts . to formalize this ,",
    "we need some notation . given a weighted graph @xmath0 on @xmath68 $ ] with node - weights one and edge - weights @xmath155 , and given a partition @xmath156 of @xmath68 $ ] into @xmath157 groups , let @xmath158 be the weighted graph with weights @xmath159 we call @xmath158 a @xmath157-quotient or @xmath157-way cut of @xmath0 , and denote the set of all @xmath157-way cuts by @xmath160 : @xmath161 $ into $ q$      sets } \\}\\ , .\\ ] ] we also consider the set of _ fractional @xmath157-way cuts _",
    ", @xmath162 , defined in terms of _ fractional @xmath157-partitions _",
    "a fractional @xmath157-partition of @xmath163 is a map @xmath164 , where @xmath165 is the simplex @xmath166^q\\colon \\sum_i\\rho_i=1\\}$ ] , and the corresponding fractional quotient @xmath167 is the weighted graph with weights @xmath168 and @xmath169 .",
    "the set of fractional @xmath157-partitions of a graphon @xmath1 , @xmath170 , is defined similarly : @xmath171\\to\\delta_q\\ }    $ ] , with a fractional partition now a measurable function @xmath172\\to \\delta_q$ ] , and @xmath173 given in terms of the weights @xmath174 to measure the distance between the various sets of @xmath157-way cuts , we use the hausdorff distance between sets @xmath175 , @xmath176 where @xmath177 is the @xmath39-norm @xmath178 .",
    "it was shown in @xcite that if @xmath12 converges to @xmath1 in the cut - metric , then @xmath179 .",
    "in particular , a consistent estimator @xmath24 for the generating graphon @xmath1 of a @xmath1-random graph @xmath12 leads to a consistent estimator @xmath180 for the cuts @xmath181 , in the sense that @xmath182 . with a little more work",
    ", we can give quantitative bounds ; see theorem  [ thm : cuts ] below .",
    "the goal of this paper is the development of a differentially private algorithm for graphon estimation .",
    "the privacy guarantees are formulated for worst - case inputs  we do not assume that @xmath0 is generated from a graphon when analyzing privacy .",
    "this ensures that the guarantee remains meaningful no matter what an analyst knows ahead of time about @xmath0 .    in this paper",
    ", we consider the notion of node privacy .",
    "we call two graphs @xmath0 and @xmath183 _ node neighbors _ if one can be obtained from the other by removing one node and its adjacent edges .",
    "[ def : dif - privacy ] a randomized algorithm @xmath29 is _ @xmath28-node - private _ if for all events @xmath184 in the output space of @xmath29 , and node neighbors @xmath185 , @xmath186 \\leq \\exp(\\eps)\\times \\pr [ { { \\mathcal{a}}}(g ' ) \\in s]\\,.\\ ] ]    we also need the notion of the _ node - sensitivity _ of a function @xmath187 , defined as maximum @xmath188 , where the maximum goes over node - neighbors .",
    "this constant is often called the lipshitz constant of @xmath189    finally , we need a lemma concerning the extension of functions @xmath190 to functions @xmath191 .",
    "we say a function on adjacency matrices is _ nondecreasing _ if adding an edge to the adjacency matrix does not increase the value of the function .",
    "[ lem : lip - extension ] for every function @xmath190 , there is an extension @xmath191 of @xmath189 with the same node - sensitivity as @xmath189 .",
    "if @xmath189 is a nondecreasing linear function of the adjacency matrix , then we can select @xmath192 to be nondecreasing and computable in polynomial - time and so that @xmath193 for all graphs @xmath194 .",
    "the lemma is proved in appendix  [ app : private ] .",
    "given a graph as input generated by an unknown graphon @xmath1 , our goal is to recover a block - model approximation to @xmath1 .",
    "the basic nonprivate algorithm we emulate is least squares estimation , which outputs the @xmath133 matrix @xmath95 which is closest to the input adjacency matrix @xmath88 in the distance @xmath195 where the minimum runs over all equipartitions @xmath196 of @xmath68 $ ] into @xmath3 classes , i.e. , over all maps @xmath197\\to [ k]$ ] such that all classes have size as close to @xmath198 as possible , i.e. , such that @xmath199 for all @xmath74 , and @xmath200 is the @xmath82 block - matrix with entries @xmath201 .",
    "if @xmath88 is the adjacency matrix of a graph @xmath0 , we write @xmath202 instead of @xmath203 . in the above notation , the basic algorithm we would want to emulate is then the algorithm which outputs the least square fit @xmath204 , where the @xmath205 runs over all symmetric @xmath133 matrices @xmath95 .",
    "our main idea to turn the least square algorithm into a private algorithm is to use the so - called exponential mechanism of mcsherry and talwar @xcite .",
    "applied naively , we would therefore want to output a random @xmath133 matrix @xmath95 according to the probability distribution @xmath206 with @xmath207 chosen small enough to guarantee differential privacy .",
    "it is a standard fact from the theory of differential privacy , that @xmath207 should be at most @xmath28 over twice the node - sensitivity of the `` score function '' , here @xmath208 .",
    "but this value of @xmath207 turns out to be too small to produce an output that is a good approximation to the least square estimator .",
    "indeed , for a given matrix @xmath95 and equipartition @xmath196 , the distance @xmath209 can change by as much as @xmath210 when @xmath0 is replaced by a node - neighbor , regardless of the magnitude of the entries of @xmath95 . to obtain differential privacy , we then would need to choose @xmath211 , which turns out to not produce useful results when the input graph @xmath0 is sparse , since small values of @xmath207 will lead to large errors relative to the least square estimator .    to address this",
    ", we first note that we can work with an equivalent score that is much less sensitive .",
    "given @xmath95 and @xmath196 , we subtract off the squared norm of @xmath0 to obtain the following : @xmath212 where the @xmath213 ranges over equipartitions @xmath197\\to[k]$ ] . for a fixed input graph @xmath0 , maximizing",
    "the score is the same as minimizing the distance , i.e. @xmath214 the sensitivity of the new score is then bounded by @xmath215 times the maximum degree in @xmath0 ( since @xmath0 only affects the score via the inner product @xmath216 ) .",
    "but this is still problematic since , a priori , we have no control over either the size of @xmath217 or the maximal degree of @xmath0 .",
    "to keep the sensitivity low , we make two modifications : first , we only optimize over matrices @xmath95 whose entries are of order @xmath11 ( in the end , we expect that a good estimator will have entries which are not much larger than @xmath218 , which is of order @xmath11 ) , and second we restrict ourselves to graphs @xmath0 whose maximum degree is not much larger than one would expect for graphs generated from a bounded graphon , namely a constant times the average degree . while the first restriction is something we can just implement in our algorithm , unfortunately the second is something we have no control over : we need to choose @xmath207 small enough to guarantee privacy for all input graphs , and we have set out to guarantee privacy in the worst case , which includes graphs with maximal degree @xmath219 . here",
    ", we employ an idea from @xcite : we first consider the restriction of @xmath220 to @xmath221 where @xmath222 will be chosen to be of the order of the average degree of @xmath0 , and then extend it back to all graphs while keeping the sensitivity low .      after these motivations , we are now ready to define our algorithm .",
    "it takes as input the privacy parameter @xmath28 , the graph @xmath0 , a number @xmath3 of blocks , and a constant @xmath223 that will have to be chosen large enough to guarantee consistency of the algorithm .",
    "it outputs a matrix @xmath95 from the set of matrices @xmath224^{k\\times k } : \\text{all entries            } b_{i , j}\\text { are multiples of } \\frac 1 n\\}.\\ ] ] inside our algorithm , we use an @xmath225-private algorithm to get an estimate @xmath226 for the edge density of @xmath0 .",
    "we do so by setting @xmath227 , where @xmath228 is a laplace random variable with density @xmath229 .",
    "the existence of the lipschitz extension used in the algorithm follows from lemma  [ lem : lip - extension ] .",
    "[ step : rho - approx]compute an @xmath230-node - private density approximation @xmath227    @xmath231 ( the target maximum degree ) @xmath232 ( the target @xmath39 norm for the matrix @xmath95 )    for each @xmath95 and @xmath196 , let @xmath233 denote a nondecreasing lipschitz extension of @xmath220 from @xmath79 to @xmath78 such that for all matrices @xmath88 , @xmath234 , and define @xmath235    @xmath236 , sampled from the distribution @xmath237 where @xmath95 ranges over matrices in @xmath238 and @xmath239    [ lem : privacy - main ] algorithm  [ alg : main - algo ] is @xmath28-node private .    by lemma  [ lem : dmns ] from appendix [ app : private ] , the estimate @xmath226 is @xmath225-private , so we want to prove that the exponential mechanism itself is @xmath225-private as well . in view of lemma",
    "[ lem : exp - mech ] from appendix [ app : private ] , all we need to show is that the the vertex sensitivity of @xmath240 is at most @xmath241 . to this end , we first bound the vertex sensitivity of the original score when restricted to graphs with degree @xmath80 .",
    "let @xmath242 be node neighbors . from",
    ", we see that @xmath243 } ( a_{xy}-a'_{xy})b_{\\pi(x)\\pi(y)}\\ , , \\ ] ] where @xmath244 are the adjacency matrices of @xmath0 and @xmath183 . since @xmath88 and @xmath105 differ in at most @xmath245 entries , the score differs by at most @xmath246 .",
    "this is at most @xmath241 , since @xmath247 .",
    "since @xmath248 is a lipschitz extension of @xmath249 , the vertex sensitivity of @xmath248 ( over _ all _ neighboring graphs ) is at most @xmath241 , as required .",
    "[ thm : final ] let @xmath4 ^ 2\\to [ 0,\\lambda]$ ] be a normalized graphon , let @xmath250 , let @xmath251 , @xmath223 , and @xmath3 be an integer .",
    "assume that @xmath252 and @xmath253 , @xmath254 .",
    "then the algorithm  [ alg : main - algo ] outputs an approximation @xmath255 such that @xmath256}}\\bigr )    & \\leq { \\eps_k^{(o)}}(w ) +   2\\eps_n(w )   +   o_p{{\\left ( { \\sqrt[4]{\\frac{\\lambda^2\\log k}{\\rho n } }   + \\lambda\\sqrt{\\frac{k^2\\log n}{n\\eps}}+\\frac { { \\lambda}}{n\\rho\\eps } } \\right)}}.    \\end{aligned}\\ ] ]    the theorem will be proven in section  [ sec : private - analysis ] .    in the course of the proof",
    ", we will prove results on the performance of a non - private algorithm , which is a variant of the standard least square algorithm , the main difference being that instead of minimizing @xmath203 over all matrices @xmath95 , we only optimize it over matrices whose entries are bounded by a constant times the density of @xmath0 .",
    "@xmath257 ( the target @xmath39 norm for the matrix @xmath95 )    @xmath258 .",
    "[ thm : final ] let @xmath4 ^ 2\\to [ 0,\\lambda]$ ] be a normalized graphon , let @xmath250 , let @xmath251 , @xmath223 , and @xmath3 be an integer . if @xmath236 is the least - squares estimator ( algorithm  [ alg : main - algo ] ) , @xmath259 , @xmath260 , then @xmath261}}\\bigr )    \\leq { \\eps_k^{(o)}}(w)+ 2\\eps_n(w )    +   o_p{{\\left ( { \\sqrt[4]{\\lambda^2{{\\left ( { \\frac{\\log k}{\\rho n }    + \\frac { k^2}{\\rho n^2 } } \\right ) } } } } \\right)}}.\\ ] ] in particular , @xmath262}}\\bigr)\\to 0 $ ] in probability if @xmath138 and @xmath263 .    theorem  [ thm : final ] is proven in section  [ sec : least-2-error ] .",
    "while theorem [ thm : final ] and theorem  [ thm : final ] are stated in term of bounds which hold in probability , our proofs give slightly more , and allow us in particular to prove statements which hold almost surely as @xmath16 .",
    "namely , they show that under the assumptions of theorem  [ thm : final ] , the output @xmath236 of the nonprivate algorithm is such that @xmath264}}\\bigr )    & \\leq { \\eps_k^{(o)}}(w)+o{{\\left ( { \\sqrt[4]{\\frac{\\lambda^2\\log k}{\\rho n}+\\frac { \\lambda^2k^2}{\\rho n^2 } } } \\right)}}+o(1 ) ;    \\end{aligned}\\ ] ] they also show that if we replace the assumption @xmath265 in theorem  [ thm : final ] by the stronger assumption @xmath266 , then the output @xmath236 of the private algorithm is such that @xmath256}}\\bigr )   \\leq { \\eps_k^{(o)}}(w )   +   o{{\\left ( { \\sqrt[4]{\\frac{\\lambda^2\\log k}{\\rho n } }   + \\lambda\\sqrt{\\frac{k^2\\log n}{n\\eps } }   { + \\frac { \\sqrt\\lambda}{n\\rho\\eps } }   } \\right)}}+o(1 )    \\end{aligned}\\ ] ] where in both expressions , @xmath267 is a term which goes to zero with probability one as @xmath16 .",
    "thus for both algorithm , as long as @xmath3 grows sufficiently slowly with @xmath7 , with probability one , the asymptotic error is of the form @xmath268 , which is best possible , since we ca nt do better than the best oracle block model approximation .",
    "[ rem : hoelder - cont - w ] under additional assumptions on the graphon @xmath1 , we can say a little more .",
    "for example , if we assume that @xmath1 is hlder continuous , i.e , if we assume that the exists constants @xmath269 $ ] and @xmath270 such that @xmath271 whenever @xmath272 , then we have that @xmath56 and @xmath57 .",
    "see appendix  [ sec : holder ] for details .",
    "theorems  [ thm : final ] and  [ thm : final ] imply that the sets of fractional @xmath157-way cuts of the estimator @xmath236 from these theorems provide good approximations to the @xmath157-way cuts of the graph @xmath0 ( as defined in section  [ sec : w - rand ] ) . specifically :    [ thm : cuts ] let @xmath273 be an integer .",
    "\\(i ) under the assumptions of theorem  [ thm : final ] , @xmath274{\\lambda^2{{\\left ( { { \\frac{\\log k}{\\rho n }    + \\frac { k^2}{\\rho n^2 } } } \\right ) } } } } \\right)}}.\\ ] ]    \\(ii ) under the assumptions of theorem  [ thm : final ] , @xmath275{\\frac{\\lambda^2\\log k}{\\rho n } }   + \\lambda\\sqrt{\\frac{k^2\\log n}{n\\eps}}+\\frac { { \\lambda}}{n\\rho\\eps } } \\right)}}.\\ ] ]    the proof of the theorem relies on the theory of graph convergence , in particular the results of @xcite , and is given in appendix  [ app : cuts ] .    when considering the `` best '' block model approximation to @xmath1 , one might want to consider block models with unequal block sizes ; in a similar way , one might want to construct a private algorithm that outputs a block model with unequal size blocks , and produces a bound in terms of this best block model approximation instead of @xmath40 . with more cumbersome notation ,",
    "this can be easily proved with our methods , with the minimal block size taking the role of @xmath136 in all our proofs .",
    "we leave the details to a journal version .",
    "at a high level , our proofs of theorems  [ thm : final ] and of [ thm : final ] follow from the fact that for all @xmath95 and @xmath196 , the expected score @xmath276 $ ] is equal to the score @xmath277 , combined with a concentration argument . as a consequence",
    ", the maximizer @xmath236 of @xmath278 will approximately minimize the @xmath2-distance @xmath279 , which in turn will approximately minimize @xmath280}}- w\\|_2 $ ] , thus relating the @xmath2-error of our estimator @xmath236 to the `` oracle error '' @xmath40 defined in .",
    "in this section we present the analysis of exact and approximate least squares .",
    "this allows us to analyze the nonprivate algorithm .",
    "the analysis of the private algorithm ( theorem [ thm : final ] ) requires additional arguments relating the private approximate maximizer to the nonprivate one ; we present these in section  [ sec : private - analysis ] ) .",
    "our main concentration statement is contained in the following proposition , which we prove in section  [ sec : concentration ] below . to state it",
    ", we define , for every symmetric @xmath82 matrix @xmath281 with vanishing diagonal , @xmath282 to be the distribution over symmetric matrices @xmath88 with zero diagonal such that the entries @xmath283 are independent bernouilli random variables with @xmath284 .",
    "[ cor : concentration ] let @xmath285 , @xmath286^{n\\times n}$ ] be a symmetric matrix with vanishing diagonal , and @xmath287 .",
    "if @xmath288 and @xmath289 is such that @xmath290 for some @xmath291 , then with probability at least @xmath292 , @xmath293{\\mu^2 \\rho({q}){{\\left ( { \\frac{k^2 } { n^2 } + \\frac { \\log k } { n } } \\right ) } } } } \\right)}}\\ ] ] and in particular @xmath294{\\mu^2 \\rho({q}){{\\left ( { \\frac{k^2 } { n^2 } + \\frac { \\log k } { n } } \\right ) } } } } \\right ) } } \\\\ & \\leq ( 2\\|{q}\\|_2 + { { \\nu}}){{\\left ( { 1+\\frac { 2k}n } \\right)}}+ o{{\\left ( { \\sqrt{\\mu \\rho({q } ) } } \\right ) } } \\end{aligned}\\ ] ]    morally , the proposition contains almost all that is needed to establish the bound proving consistency of the standard least squares algorithm ( which , in fact , only involves the case @xmath295 ) , even though there are several additional steps needed to complete the proof ( see sections  [ app : prop - proof ] and [ app : prop - proof - completed ] below ) .",
    "the proposition also contains an extra ingredient which is a crucial input for the analysis of the private algorithm : it states that if instead of an optimal , least square estimator , we output an estimator whose score is only approximately maximal , then the excess error introduced by the approximation is small . to apply the proposition , we then establish a a lemma which gives us a lower bound on the score of the output @xmath236 in terms of the maximal score and an excess error @xmath296 .",
    "there are several steps needed to execute this strategy , the most important ones involving a rigorous control of the error introduced by the lipschitz extension inside the exponential algorithm ( which in turn requires estimating the deviation of the maximal degree from the expected degree , a step where the condition that @xmath297 has to grow like @xmath298 is needed ) .",
    "the excess error @xmath296 eventually turns into the second to last error term in , while the difference between our private estimator @xmath226 for the edge density and the actual edge density of @xmath0 is responsible for the last one .",
    "the analysis of the private algorithm is presented in section  [ sec : private - analysis ] ; the remainder of this section presents the detailed analysis of the least squares estimator .",
    "[ rem : h - bound ] note that for both the non - private algorithm and the private algorithm , the above proposition naturally gives a bound for the @xmath2 estimation error for matrix of probabilities @xmath281 . in fact , our proofs provide error bounds on @xmath299 which differ from , and the bounds in theorem  [ thm : cuts ] in that ( i ) the error term @xmath300 is absent , and ( ii ) the oracle error @xmath40 is replaced by an oracle error @xmath301 for @xmath302 , see theorems  [ thm : h ] , [ thm : h ] and [ thm : cuts - from - h ] .",
    "converting these bounds into bounds on @xmath303}})$ ] and expressing the result in terms of @xmath40 instead of @xmath301 then introduces the error term @xmath300 in , and the bounds in theorem  [ thm : cuts ] .",
    "the following two lemmas contain the core of the argument outlined at the beginning of this section .",
    "[ lem : expectations ] let @xmath304^{n\\times n}$ ] be a symmetric matrix with vanishing diagonal , let @xmath287 , and let @xmath305 be @xmath133 matrices .",
    "then @xmath306-\\max_{\\pi}{\\mathbb{e}}[score ( b,\\pi;g)],\\ ] ] where the two max s go over equipartitions @xmath307\\to [ k]$ ] .",
    "by linearity of expectation , we have @xmath308 taking into account the definition of @xmath279 , the lemma follows .",
    "our second lemma states that the realized scores are close to their expected values .",
    "the proof is based on a careful application of the concentration bounds .",
    "the argument is delicate because we must take advantage of the low density ( when @xmath25 is small ) .",
    "[ lem : concentration ] let @xmath285 , let @xmath309^{n\\times        n}$ ] be a symmetric matrix with vanishing diagonal and let @xmath287 . if @xmath310 , then , with probability at least @xmath292 @xmath311}}| =        o{{\\left ( { \\mu \\,\\sqrt{\\rho({q}){{\\left ( { \\frac{k^2 } { n^2 } + \\frac { \\log k } { n } } \\right ) } } } } \\right)}}\\ ] ] for all equipartitions @xmath196 and all @xmath312^{k\\times k}$ ] .",
    "first , consider a specific pair @xmath313 . recall that @xmath314 } }",
    "= 2{{\\left \\langle { a-{q } } , { b_\\pi}\\right\\rangle}}\\,.\\ ] ] we wish to bound the deviation of @xmath315 from its mean .",
    "set @xmath316 .",
    "the quantity @xmath317 is a sum of @xmath72 independent random variables in @xmath8 $ ] with expectation @xmath318 . using a slight variation on the standard chernoff bound , which we state in lemma  [ lem : chernoff - mult ]",
    ", we will bound the probability that @xmath184 deviates from its mean by at most @xmath319 , where @xmath320 $ ] will be chosen in a moment .",
    "setting @xmath321 and @xmath322 the assumption @xmath323 implies @xmath324 , and setting @xmath325 , the bound from lemma  [ lem : chernoff - mult ] becomes @xmath326 implying that @xmath327    finally , we observe that for any @xmath88 , the maximum of @xmath328 over all @xmath312^{k\\times k}$ ] is the same as the maximum over all @xmath329 .",
    "taking a union bound over the ( at most @xmath330 ) pairs @xmath313 and observing that @xmath331 , we get the statement of the lemma .",
    "let @xmath289 be as specified , let @xmath332 arbitrary , and let @xmath333 be two equipartitions . by lemmas  [ lem : expectations ] and [ lem : concentration ] , @xmath334-\\max_{\\pi}{\\mathbb{e}}[score(\\hat b,\\pi;g ) ] \\\\",
    "& \\leq score(b';g)-score(\\hat b;g)+ o{{\\left ( { \\mu \\,\\sqrt{\\rho({q}){{\\left ( { \\frac{k^2 } { n^2 } + \\frac { \\log k } { n } } \\right ) } } } } \\right ) } } \\\\ & \\leq { { \\nu}}^2 + o{{\\left ( { \\sqrt{\\mu^2 \\rho({q}){{\\left ( { \\frac{k^2 } { n^2 } + \\frac { \\log k } { n } } \\right ) } } } } \\right ) } } \\end{aligned}\\ ] ] which implies the bound .",
    "( taking square roots works since @xmath335 as long as @xmath336 . ) to prove , we use that for an arbitrary equipartition @xmath196 @xmath337 and that @xmath338 . inserting the definition of @xmath299 and using the main statement plus the assumptions @xmath310",
    ", we obtain .",
    "up to technical details , proposition  [ cor : concentration ] contains all that is needed to prove consistency of the least square algorithm .",
    "as indicated in remark  [ rem : h - bound ] , we will first prove that the algorithm gives a consistent estimator for the matrix @xmath281 , and then use this prove that the output also gives a consistent estimator for @xmath1 .",
    "the first statement is formalized in the following theorem .",
    "[ thm : h ] under the assumptions of theorem  [ thm : final ] , @xmath339 where @xmath340 with the @xmath341 going over all symmetric @xmath133 matrices @xmath95 .",
    "moreover , a.s . as @xmath16 , @xmath342 {          \\lambda^2{{\\left ( {             \\frac{k^2 } { n^2\\rho }              + \\frac{\\log k } { n\\rho }          } \\right ) } } } } \\right)}}+o(1 ) .",
    "\\end{aligned}\\ ] ]    as a first step , we will bound the left hand side of by conditioning on the event that @xmath343 by a concentration argument very similar to the proof of lemma  [ lem : concentration ] above ( in fact , it is easier , see lemma  [ lem : good - trho ] ( part 3 ) in appendix  [ app : aux ] ) , we have that , with probability at least @xmath292 , @xmath344 we can now apply proposition  [ cor : concentration ] with @xmath295 ( since the nonprivate algorithm returns an exact minimizer ) .",
    "recall that @xmath345 and @xmath346 .",
    "we get that , with probability at least @xmath347 , @xmath348 { { \\lambda^2 }          { { \\left ( {              \\frac{k^2 } { n^2\\rho }              + \\frac{\\log k } { n\\rho }          } \\right ) } } } } \\right)}}.\\ ] ]    in the remainder of the proof , we bound the first term on the left - hand side above by relating it to the `` oracle error '' @xmath349 .",
    "let @xmath350 and @xmath196 be such that @xmath351 .",
    "it is easy to see that then @xmath352 is obtained from @xmath42 by averaging over the classes of @xmath196 , which in turn implies that @xmath353 and @xmath354 .",
    "define @xmath95 by rounding all entries of @xmath355 down to the nearest multiple of @xmath84 , adding a rounding error of at most @xmath84 , so that @xmath356 .",
    "note that @xmath350 is on the scale of @xmath1 and @xmath42 ( that is , we expect @xmath357 ) , while @xmath95 is on the scale of @xmath34 and @xmath281 ; hence , @xmath358 .",
    "now @xmath359 , @xmath360 , and @xmath361 .",
    "thus , @xmath95 is in the set @xmath362 that the algorithm searches over .",
    "we can bound the first term in the left - hand side of by @xmath363 combined with our previous two bounds and the fact that by , we can bound @xmath364 by @xmath365 , this implies that @xmath366{\\lambda^2          { { \\left ( {              \\frac{k^2 } { n^2\\rho }              + \\frac{\\log k } { n\\rho }          } \\right ) } }         } } \\right ) } } \\\\ & \\leq { \\hat \\eps_k^{(o)}}({{h_n}(w)})+ \\sqrt{\\lambda}\\bigl|1-\\frac{\\rho({q})}\\rho\\bigr|+ o{{\\left ( {      \\sqrt[4 ] {          \\lambda^2{{\\left ( {              \\frac{k^2 } { n^2\\rho }              + \\frac{\\log k } { n\\rho }          } \\right ) } }          } } \\right)}}.   \\end{aligned}\\ ] ] we can now use to bound @xmath367 by @xmath368 and thus @xmath369 by @xmath370 plus an error which can be absorbed into the error term above .",
    "we obtain that , conditioned on , with probability at least @xmath347 , we have @xmath371 {          \\lambda^2{{\\left ( {              \\bigl|1-\\frac{\\rho({q})}\\rho\\bigr|^4              + \\frac{k^2 } { n^2\\rho }              + \\frac{\\log k } { n\\rho }          } \\right ) } } } } \\right)}}.   \\end{aligned}\\ ] ] by lemma  [ lem : good - trho ] from appendix  [ app : aux ] , @xmath372 , implying that @xmath373 on the other hand , again by lemma  [ lem : good - trho ] from appendix  [ app : aux ] , the probability that does not hold is @xmath374 , showing that with probability @xmath375 , @xmath376 {          \\lambda^2{{\\left ( { \\frac{k^2 } { n^2\\rho }              + \\frac{\\log k } { n\\rho }          } \\right ) } } } } \\right)}}.   \\end{aligned}\\ ] ] this holds conditioned on an event @xmath377 of probability @xmath378 . to bound the contribution of @xmath377 to the overall error",
    ", we bound @xmath379 and @xmath380 , giving an error contribution of @xmath381{\\lambda^2/\\rho n})$ ] which we can absorb into the error already present .    to prove the almost sure statement",
    ", we use that @xmath47 almost surely , which by lemma  [ lem : good - trho ] ( part 2 ) from appendix  [ app : aux ] implies that @xmath382 almost surely . since the error probability in is exponentially small",
    ", we can use the borel - cantelli lemma to obtain the a.s .",
    "statement .      to deduce theorem  [ thm : final ] from theorem  [ thm : h ]",
    ", we will bound @xmath349 in terms of @xmath40 , and @xmath383 in terms of @xmath384 .",
    "we will show that the leading error in both cases is an additive error of @xmath41 . to do this ,",
    "we need two lemmas .",
    "[ lem : equi - part ] fix @xmath7 and @xmath385 .    1 .   for each equipartition @xmath197\\to [ k]$ ] and each permutation @xmath386\\to [ n]$ ]",
    ", @xmath387 is an equipartition .",
    "2 .   for all equipartitions @xmath307\\to [ k]$ ]",
    "there exists a permutation @xmath386\\to [ n]$ ] such that @xmath388 .",
    "any equipartition must have exactly @xmath389 classes of size @xmath390 and @xmath391 classes of size @xmath392 , where @xmath393 are determined by the equations @xmath394 , @xmath395 ; and any partition with these properties is an equipartition .",
    "the statement follows .    to state the next lemma",
    ", we define the _ standard equipartition _",
    "@xmath196 of @xmath68 $ ] into @xmath3 classes to be the partition into the classes @xmath396 , @xmath397 $ ] , where @xmath398 .",
    "note that @xmath399 , @xmath400 , and @xmath401 .",
    "[ lem : equi - part - bd ] let @xmath95 be a symmetric @xmath133 matrix with nonnegative entries , and let @xmath196 be the standard equipartition of @xmath68 $ ] into @xmath3 classes .",
    "then @xmath402}}-{{w[{b_\\pi}]}}\\|_2\\leq \\sqrt{\\frac { 4k}n}\\|b\\|_2.\\ ] ]    let @xmath403 be adjacent intervals of length @xmath136 , and for @xmath404 , let @xmath405 be the set of point in @xmath406 such that @xmath407 , and let @xmath408",
    ". then @xmath409}}-{{w[{b_\\pi}]}})(x , y)=0 $ ] unless @xmath410 lies in one of the @xmath411 sets @xmath412 , @xmath413 or @xmath414 , @xmath415 $ ] . taking , e.g. , @xmath416 we have that @xmath417}}-{{w[{b_\\pi}]}})(x , y)|^2=|b_{ij}-b_{i+1,j+1}|^2\\leq b_{ij}^2+b_{i+1,j+1}^2 $ ] ( note that the set @xmath418 is empty , so that here we only have to consider @xmath419 ) . in a similar way ,",
    "the difference in @xmath420 is bounded by @xmath421 , and the difference in @xmath422 is bounded by @xmath423 .",
    "the total contribution of all these sets can then be bounded by @xmath424    we start by bounding @xmath383 .",
    "let @xmath197\\to [ k]$ ] be a standard equipartition , and let @xmath425 be a partition of @xmath8 $ ] into adjacent intervals of lengths @xmath84 . by the triangle inequality",
    ", the fact that the set of measure preserving bijections @xmath426\\to [ 0,1]$ ] contains all bijections which just permute the intervals @xmath427 and lemma  [ lem : equi - part ] @xmath428}},{{w[{\\hat b_\\pi}]}})+ \\delta_2{{\\left ( { \\frac 1{\\rho(g)}{{w[{\\hat b}]}},{{w[{{{h_n}(w ) } } ] } } } \\right ) } } + \\delta_2{{\\left ( { { { w[{{{h_n}(w)}}]}},w } \\right ) } } \\\\ & \\leq \\frac 1{\\rho(g)}\\|{{w[{\\hat b}]}}-{{w[{\\hat b_\\pi}]}}\\|_2 + \\hat\\delta_2{{\\left ( { \\frac 1{\\rho(g)}\\hat b,{{h_n}(w ) } } \\right ) } } + \\hat\\delta_2{{\\left ( { { { h_n}(w)},w } \\right ) } } .",
    "\\end{aligned}\\ ] ] the third term is equal to @xmath41 . to bound the first term , we first condition on the event , and then use together with lemma  [ lem : equi - part - bd ] to conclude that conditioned on , with probability at least @xmath347 , @xmath429}},{{w[{\\hat b_\\pi}]}})\\leq o{{\\left ( { \\frac{\\rho}{\\rho(g)}\\sqrt { \\frac{\\lambda k}{n } } } \\right)}}=o{{\\left ( { \\sqrt[4 ] { \\lambda^2\\frac { k^2}{\\rho n^2 } } } \\right)}}.\\ ] ] in view of lemma  [ lem : good - trho ] from appendix  [ app : aux ] , the probability that this bound does not hold is bounded by @xmath430 , so in view of the fact that @xmath431 , which shows that @xmath432 , we see that the contribution of the failure event is again bounded by @xmath433{\\lambda^2\\frac k{\\rho n } } } \\right)}}$ ] . all together , this proves that @xmath434 {          \\lambda^2{{\\left ( { \\frac{k^2 } { n^2\\rho }              + \\frac{\\log k } { n\\rho }          } \\right ) } } } } \\right)}}.\\ ] ] the corresponding a.s . bound follows again from the fact that @xmath435 a.s . , and the fact that all other failure probabilities are exponentially small .",
    "next fix @xmath95 such that it is a minimizer in .",
    "that implies that @xmath95 is obtained from @xmath1 by averaging over a partition of @xmath1 into @xmath3 classes , which in particular implies that @xmath436 .",
    "together with lemma  [ lem : equi - part - bd ] this implies that there is an equipartition @xmath197\\to [ k]$ ] such that @xmath437}}\\|_2 \\\\ & \\geq \\|w-{{w[{b_\\pi}]}}\\|_2 - \\sqrt{\\frac { 4k}\\lambda n } \\\\ & \\geq \\hat\\delta_2(b_\\pi , w)- \\sqrt{\\frac { 4k}\\lambda n } \\end{aligned}\\ ] ] using lemma  [ lem : equi - part ] to express @xmath438 as a minimum over permutations @xmath386\\to [ n]$ ] , we then bound @xmath439^\\sigma\\|_2 \\\\ & \\leq \\|{{w[{b_\\pi } ] } } - w\\|_2+\\hat\\delta_2({{h_n}(w)},w ) \\\\ & \\leq { \\eps_k^{(o)}}(w)+\\eps_n(w)+\\sqrt{\\frac { 4k}\\lambda n } , \\end{aligned}\\ ] ] where in the first line we use @xmath440^\\sigma$ ] to denote the matrix with entries @xmath440_{\\sigma(x),\\sigma(y)}$ ] .",
    "together with this completes the proof of the theorem .",
    "in this section we prove consistency of the private algorithms .",
    "our analysis relies on some basic results on differentially private algorithms from previous work , which are collected in appendix  [ app : private ] .",
    "compared to the analysis of the non - private algorithms , we need to control several additional error sources which were not present for the nonprivate algorithm .",
    "in particular , we will have to control the error between @xmath441 and @xmath71 , the fact that the algorithm ( approximately ) maximizes @xmath442 instead of @xmath443 , and the error introduced by the exponential sampling error .",
    "the necessary bounds are given by the following lemma .",
    "to state it , we denote the maximal degree in @xmath0 by @xmath444 .",
    "[ lem : private - output ] let @xmath255 be the output of the randomized algorithm  [ alg : main - algo ] . then the following properties hold with probability at least @xmath445 with respect to the coin flips of the algorithm :    1 )    @xmath446 .",
    "\\2 ) if @xmath447 and @xmath448 , then @xmath449    observing that @xmath450 , we get that @xmath451 which immediately gives ( 1 ) .    to prove ( 2 ) , we first use ( 1 ) and the assumptions on @xmath71 and @xmath444 to bound @xmath452 this implies that the extended score is equal to the original score .",
    "we conclude the proof by using lemma  [ lem : exp - mech ] to show that with probability at least @xmath453 , the exponential mechanism returns a matrix @xmath236 such that @xmath454 where @xmath455 . bounding @xmath456 , this completes the proof of the lemma .",
    "theorem  [ thm : final ] will follow from the following theorem in the same way as theorem  [ thm : final ] followed from theorem  [ thm : h ] .",
    "[ thm : h ] under the assumptions of theorem  [ thm : final ] , @xmath457 {          \\frac{\\lambda^2\\log k}{\\rho n }              }      + \\lambda\\sqrt{\\frac{k^2\\log n}{n\\eps } }      + \\frac { { \\lambda}}{n\\rho\\eps }            } \\right)}}.\\ ] ] moreover , if we replace the assumption @xmath265 in theorem  [ thm : final ] by the stronger assumption @xmath266 , then a.s . as @xmath16 , @xmath458 {          \\frac{\\lambda^2\\log k}{\\rho n }              }      + \\lambda\\sqrt{\\frac{k^2\\log n}{n\\eps } }      + \\frac { \\sqrt\\lambda}{n\\rho\\eps }            } \\right ) } } + o(1 ) .",
    "\\end{aligned}\\ ] ]    with probability at least @xmath459 , we may assume that the output of the private algorithm obeys the conclusions of lemma  [ lem : private - output ] . with a decrement in probability of at most @xmath460",
    ", we then have that @xmath461 next use the assumption @xmath252 , the fact that @xmath462 , and lemma  [ lem : max - degree ] from appendix  [ app : aux ] with @xmath463 to show that at a decrement in probability of at most @xmath464 , the maximal degree in @xmath0 is at most @xmath465 . lemma  [ lem : private - output ] then allows us to use proposition  [ cor : concentration ] with @xmath466 this introduces an additional error term @xmath467 into the bound and an extra error term of order @xmath468 in the upper bound , leading to the estimate that , with probability at least @xmath469 , @xmath470 { { \\lambda^2 }          { { \\left ( {              \\frac{k^2}{n^2\\rho }              + \\frac{\\log k}{n\\rho }                 } \\right ) } }                }      + \\lambda\\sqrt{\\frac{k^2\\log n}{n\\eps } }              } \\right)}}\\ ] ] and @xmath471 from here on we proceed as in the proof of , except that we now move from the minimizer @xmath350 for @xmath349 to a matrix @xmath472 by rounding the entries of @xmath473 down to the nearest multiple of @xmath84 . instead of",
    ", we now obtain the bound @xmath474 {          \\lambda^2          { { \\left ( {              \\frac{k^2}{n^2\\rho }              + \\frac{\\log k}{n\\rho }                 } \\right ) } }                }      + \\lambda\\sqrt{\\frac{k^2\\log n}{n\\eps } }              } \\right ) } } + o{{\\left ( { \\sqrt\\lambda\\bigl|\\frac{\\hat\\rho}\\rho-1\\bigr| } \\right ) } } , \\label{eq : oneofourbounds}\\ ] ] a bound which is valid with probability at least @xmath475 . now the fact that @xmath476 and @xmath477 implies that @xmath478{\\frac{\\lambda^2 } { n } } } \\right)}}+ o_p{{\\left ( { \\frac { \\sqrt\\lambda}{n\\rho\\eps } } \\right)}}.\\ ] ] combining this with , we obtain that with probability at least @xmath475 , @xmath479 {          \\lambda^2          { { \\left ( {              \\frac{k^2}{n^2\\rho }              + \\frac{\\log k}{n\\rho }                 } \\right ) } }                }      + \\lambda\\sqrt{\\frac{k^2\\log n}{n\\eps } }      + \\frac { \\sqrt\\lambda}{n\\rho\\eps }              } \\right ) } }              .",
    "\\end{aligned}\\ ] ] the contribution of the failure event can now be bounded by @xmath480 to complete the proof of the bound in probability , we have to add the error terms from and .",
    "we can simplify the resulting expression somewhat by first noting that the left hand side of eq .",
    "is of order at most @xmath481 , which shows that for the bound on @xmath482 not to be vacuous , we need @xmath483 .",
    "we can therefore drop the term @xmath484 inside the fourth root of . furthermore , by the assumption of the theorem , @xmath485 , which shows that the first term in is @xmath486{\\lambda^2/n})$ ] and can hence be absorbed into the error terms in .",
    "this gives us the main theorem statement .",
    "to prove bounds which hold a.s .",
    ", we note that for @xmath266 , the error probability in is summable ( that is , the probability of error @xmath487 satisfies @xmath488 ) for all @xmath489 , which together with our previous results implies that @xmath490 with probability one .",
    "since the probability of failure for all other events necessary for to hold is summable as well , we get that a.s .",
    ", @xmath491 {          \\frac{\\lambda^2\\log k}{\\rho n }              }      + \\lambda\\sqrt {          \\frac{k^2\\log n}{n\\eps }                  }      { + \\frac { \\sqrt\\lambda}{n\\rho\\eps } }          } \\right ) } } + o(1 ) ,    \\end{aligned}\\ ] ] where again @xmath267 is a term which goes to zero with probability one as @xmath16 .",
    "the proof of theorem  [ thm : final ] follows from theorem  [ thm : h ] in essentially same way as theorem  [ thm : final ] followed from theorem  [ thm : h ] .",
    "the only modification needed is that we now have to bound @xmath492}},{{w[{\\hat b_\\pi}]}})$ ] instead of @xmath493}},{{w[{\\hat b_\\pi}]}})$ ] .",
    "but this is even easier , since here we wo nt need to distinguish several cases .",
    "instead , we just use that @xmath289 implies @xmath494 . with the help of lemma  [ lem : equi - part - bd ]",
    ", we then bound this error term by @xmath495 a term which can be incorporated into the error term @xmath496 .",
    "the most relevant previous works are those of @xcite and @xcite .",
    "we provide comparisons for two types of bounded graphons :    @xmath3-block graphons , and    @xmath55-hlder graphons .",
    "[ [ k - block - graphons . ] ] @xmath3-block graphons .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a @xmath3-block graphon is a function on @xmath8 ^ 2 $ ] that is constant on rectangles of the form @xmath87 , where @xmath497 form a partition of the interval @xmath8 $ ] .    in this setting , the oracle error @xmath50 and @xmath51{k / n})$ ] ( see appendix  [ sec : sampling - k - block ] ) .",
    "our nonprivate estimator then has asymptotic error @xmath62{\\frac k n + \\frac{\\log k}{\\rho n }         + \\frac { k^2}{\\rho n^2 } } $ ] ; this is dominated by @xmath41 as long as @xmath498 and @xmath499 . for constant @xmath28 ,",
    "our private estimator has asymptotic error at most @xmath62{\\frac k n +    \\frac{\\log k}{\\rho n } + \\frac{k^4 \\log^3 n}{n^2}}$ ] . for @xmath500 and density @xmath499 , the error of our estimator",
    "is again dominated by the ( unavoidable ) error of @xmath54{k / n}$ ] .",
    "several works analyze procedures for estimating the edge - probability matrix @xmath281 assuming that it is ( exactly ) a @xmath3-block matrix . in the dense case @xmath501 , ( * ? ? ?",
    "* theorem 1.1 ) show that the least squares estimator achieves error @xmath502 .",
    "they also give a matching lower bound , which shows that the mle is optimal with respect to @xmath36 estimation of @xmath281 .",
    "* theorem 2.3 ) gives a polynomial - time algorithm with higher error @xmath503{\\frac k n})$ ] .",
    "these bounds apply to estimating the edge - probability matrix @xmath281 , but do not apply directly to estimating an underlying block graphon @xmath1 .",
    "lemma  [ lem : sampling - k - block ] shows that @xmath504 converges to @xmath1 in the @xmath37 metric at a rate of @xmath505{k / n})$ ] .",
    "using either of the algorithms above for estimating @xmath1 gives a net error rate of @xmath506{k / n})$ ] for @xmath37 estimation of @xmath1 .",
    "this is the best known nonprivate rate , and is matched by our nonprivate rate .    in the sparse case , where @xmath507 as @xmath48 , wolfe and olhede showed under additional assumptions ( roughly , that entries of @xmath281 are bounded above and below by multiples of @xmath25 ) that the mle produces an estimate @xmath508 of @xmath281 that satisfies @xmath509{\\frac{\\log^2(1/\\rho)\\log(k)}{n\\rho } } } \\right)}}$ ] (",
    "* theorem 5.1 ) using the fact that @xmath510 when @xmath511 is small relative to @xmath512 . ] .",
    "again , one can combine these with lemma  [ lem : sampling - k - block ] to get a rate of @xmath62{\\frac k n } + \\frac{k}{n } \\sqrt{\\frac{\\log(n)}{\\rho } } +    \\sqrt[4]{\\frac{\\log^2(1/\\rho)\\log(k)}{n\\rho } } $ ] for estimating an underlying @xmath3-block graphon @xmath1 . note that when @xmath25 is small , any of these three terms may dominate the rate .    [ [ hlder - continuous - graphons . ] ] hlder - continuous graphons .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    the known algorithms for estimating continuous graphons proceed by fitting a @xmath3-block model to the observed data , and arguing that this model approximates the underlying graphon .",
    "our results show that if @xmath28 is constant and @xmath1 is @xmath55-hlder continuous ( lipschitz continuity corresponds to @xmath513 ) , then the nonprivate error scales as @xmath514{\\frac{\\log n     } { \\rho n } } + n^{-\\alpha/2}$ ] for an appropriate choice of @xmath3 , while the private error scales as @xmath515{\\frac{\\log n     } { \\rho n } } + n^{-\\alpha/2}$ ] for an appropriate choice of @xmath3 .",
    "see remark  [ rem : hoelder - cont - w ] for details .    in the dense case ( @xmath501 ) , @xcite",
    "show that one can estimate a @xmath55-hlder continuous graphon by a @xmath3-block graphon with error @xmath516 with the last term accounting for the difference between estimating @xmath281 and @xmath1 . setting @xmath3 to the optimal value of @xmath517",
    "gives a rate which except for the case @xmath513 is dominated by the term @xmath518 .",
    "our nonprivate bound matches this bound for @xmath66 , and is worth for @xmath519 , while the private one is always worth .",
    "@xcite analyse the mle in the sparse case , again restricting to @xmath3-block models .",
    "they show - block graphons in which all intervals have size @xmath520 ( since allowing nonuniformly sized blocks only makes their bounds worse ) , the original graphon takes values in a range @xmath521 $ ] defined by two constants such that @xmath522 , and @xmath3 is polynomially smaller than @xmath7 . ]",
    "@xmath523{\\frac{\\log^2(1/\\rho)\\log(k)}{n\\rho } } + k^{-\\alpha } + \\frac{\\sqrt{\\log(n/\\rho)}}{n^{\\alpha/4 } } } \\right ) } } \\label{eq : wobound - k}\\ ] ]    the value of @xmath3 that maximizes this expression ( asymptotically ) can be found by setting the first and third terms to be equal ; we get @xmath524 and a resulting bound of @xmath525{\\frac{\\log^2(1/\\rho)\\log(n)}{n\\rho } } + \\frac{\\sqrt{\\log(n/\\rho)}}{n^{\\alpha/4 } } } \\right ) } } .\\ ] ]    next , note that @xmath25 must be @xmath526 for the middle term to be less than 1 .",
    "this means that the first term is @xmath527 . for @xmath528 ,",
    "this is never larger than the third term .",
    "we may therefore simplify the bound to @xmath529{\\frac{\\log^2(1/\\rho)\\log(n)}{n\\rho } } + \\frac{\\sqrt{\\log(n/\\rho)}}{n^{\\alpha/4 } } } \\right)}}$ ] , as stated in the introduction .",
    "the notion of node - privacy defined in section  [ sec : diff - p ] `` composes '' well , in the sense that privacy is preserved ( albeit with slowly degrading parameters ) even when the adversary gets to see the outcome of an adaptively chosen sequence of differentially private algorithms run on the same data set .",
    "[ lem : composition ] if an algorithm @xmath29 runs @xmath530 randomized algorithms @xmath531 , each of which is @xmath28-differentially private , and applies an arbitrary ( randomized ) algorithm @xmath532 to their results , i.e. , @xmath533 then @xmath29 is @xmath534-differentially private .",
    "this holds even if for each @xmath535 , @xmath536 is selected adaptively based on @xmath537 .",
    "[ [ output - perturbation . ] ] output perturbation .",
    "+ + + + + + + + + + + + + + + + + + + +    [ sec : sens ] one common method for obtaining efficient differentially private algorithms for approximating real - valued functions is based on adding a small amount of random noise to the true answer .",
    "a _ laplace _ random variable with mean @xmath538 and standard deviation",
    "@xmath539 has density @xmath540 .",
    "we denote it by @xmath541 .    in the most basic framework for achieving differential privacy , laplace noise",
    "is scaled according to the _ global sensitivity _ of the desired statistic @xmath189 .",
    "this technique extends directly to graphs as long as we measure sensitivity with respect to the metric used in the definition of the corresponding variant of differential privacy .",
    "below , we explain this ( standard ) framework in terms of node privacy .",
    "let @xmath542 denote the set of all graphs .",
    "[ def : global - sensitivity ] the @xmath543-global node sensitivity of a function @xmath544 is : @xmath545    for example , the edge density of an @xmath7-node graph has node sensitivity @xmath546 , since adding or deleting a node and its adjacent edges can add or remove at most @xmath219 edges .",
    "in contrast , the number of nodes in a graph has node sensitivity @xmath547 .",
    "[ laplace mechanism  @xcite][lem : dmns ] the algorithm @xmath548 ( which adds i.i.d .",
    "noise @xmath549 to each entry of @xmath550 ) is @xmath28-node - private .",
    "thus , we can release the number of nodes , @xmath551 , in a graph @xmath0 with noise of expected magnitude @xmath552 while satisfying node differential privacy . given a public bound @xmath7 on @xmath551",
    ", we can release the number of edges , @xmath553 , with additive noise of expected magnitude @xmath554 .",
    "[ [ exponential - mechanism . ] ] exponential mechanism .",
    "+ + + + + + + + + + + + + + + + + + + + + +    sensitivity plays a crucial role in another basic design tool for differentially private algorithms , called the _ exponential mechanism_.",
    "suppose we are given a collection of @xmath555 functions , @xmath556 from @xmath78 to @xmath557 , each with sensitivity at most @xmath241 .",
    "the exponential mechanism , due to @xcite , takes a data set ( in our case , a graph @xmath0 ) and aims to output the index @xmath558 of a function in the collection which has nearly maximal value at @xmath0 , that is , such that @xmath559 .",
    "the algorithm @xmath29 samples an index @xmath74 such that @xmath560    [ lem : exp - mech ] the algorithm @xmath29 is @xmath28-differentially private .",
    "moreover , with probability at least @xmath561 , its output @xmath562 satisfies @xmath563    [ [ lipschitz - extensions . ] ] lipschitz extensions .",
    "+ + + + + + + + + + + + + + + + + + + + +    there are cases ( and we will encounter them in this paper ) , where the sensitivity of a function can only be guaranteed to be low if the graph in question has sufficiently low degrees .",
    "in this situation , it is useful to consider extensions of these functions from graphs obeying a certain degree bound to those without this restriction",
    ".    let @xmath79 denote the set of graphs with degree at most @xmath80 .",
    "given functions @xmath564 and @xmath565 , we say @xmath192 is a vertex lipschitz extension of @xmath189 from @xmath79 to @xmath78 if @xmath192 agrees with @xmath189 on @xmath79 and @xmath192 has the same node - sensitivity as @xmath189 , that is @xmath566    we close this section with the proof of lemma  [ lem : lip - extension ] .",
    "the existence of @xmath192 follows from a very general result ( e.g. , @xcite ) , which states that for any metric spaces @xmath567 and @xmath568 such that @xmath569 , and any lipschitz function @xmath570 , there exists an extension @xmath571 with the same lipschitz constant .",
    "the explicit , efficient construction of extensions for linear functions is due to @xcite .",
    "the idea is to replace @xmath550 with the maximum of @xmath572 where @xmath207 ranges over weighted subgraphs of @xmath0 with ( weighted ) degree at most @xmath80 .",
    "it is the value of the following linear program : @xmath573^{n\\times",
    "n }    \\text { is symmetric , and } \\\\    c_{i ,",
    "j } \\leq a(g)_{i , j } \\text { for all } i , j \\text { , and }",
    "\\\\    \\sum_{j \\neq i } c_{i , j } \\leq d \\text { for all } i \\in [ n]\\ , . \\end{cases}\\ ] ] see @xcite for the analysis of this program s properties .",
    "[ lem : good - trho ] let @xmath4 ^ 2\\to[0,\\lambda]$ ] be a normalized graphon , let @xmath574 $ ] , let @xmath575 , let @xmath251 and assume that @xmath297 is bounded away from zero",
    ". then    1 .",
    "@xmath576 , @xmath577 and @xmath578 , so in particular @xmath579 for any @xmath489 .",
    "2 .   let @xmath580}}\\|_2 $ ]",
    ". then @xmath581 3 .",
    "let @xmath582 .",
    "with probability at least @xmath583 , @xmath584    \\1 . clearly , @xmath576 .    to bound the variance of @xmath585 , we expand @xmath586 as a sum of @xmath587 terms of the form @xmath588-{\\mathbb{e}}[w(x_i , x_j)]{\\mathbb{e}}[w(x_k , x_\\ell)]$ ] with @xmath113 and @xmath589 .",
    "observing that only those terms contribute for which either @xmath590 , @xmath591 or @xmath592 , and bounding @xmath588\\leq \\|w\\|_2 ^",
    "2\\leq \\|w\\|_\\infty \\|w\\|_1 $ ] , we obtain that the variance of @xmath593 is @xmath594 .    to bound the variance of @xmath71 , we first condition on @xmath595 , and bound @xmath596 = \\rho^2({q})+\\frac 4{n^2(n-1)^2}\\sum_{i < j}\\bigl({q}_{ij}-{q}^2_{ij}\\bigr )",
    "\\leq \\rho^2({q})+\\frac 2{n(n-1)}\\rho({q}).\\ ] ] taking the expectation over @xmath567 and using the bound on the variance of @xmath585 , we obtain that @xmath597 where in the last step we used the assumption that @xmath297 is bounded away from zero .",
    "note that @xmath598 .",
    "next , we use the triangle inequality and the fact that the @xmath98-norm is bounded by the @xmath2-norm to see that @xmath599}}- w\\|_1 \\leq \\|\\frac 1\\rho { { w[{{q}}]}}- w\\|_2=\\eps_n(w)\\ ] ]    \\3 .",
    "conditioned on @xmath567 , @xmath600 is a sum of bernouilli random variables with mean @xmath601 . by the multiplicative chernov bound from lemma  [ lem : chernoff - mult ] , we have that for all @xmath324 @xmath602    next we bound the maximal degree in @xmath251 .",
    "[ lem : max - degree ] let @xmath1 be a normalized graphon with @xmath603 , let @xmath250 , and let @xmath604 .",
    "then with probability at least @xmath605 , the maximal degree in @xmath116 is bounded by @xmath606 .",
    "note that the degrees in @xmath12 are stochastically dominated by those in an erdos - renyi random graph where edges are chosen i.i.e . with probability @xmath607 .",
    "in such a graph , the degree of a given vertex @xmath74 is a sum of @xmath219 i.i.d bernouilli random variables , and the standard chernov bound implies that for all @xmath604 @xmath608 taking the union bound over all @xmath7 vertices in @xmath116 this proves the claim .",
    "the sampling error @xmath41 plays a key role in the statements of theorems [ thm : final ] and [ thm : final ] . in some settings such as hlder - continuous graphons ,",
    "this sampling error is dominated by the oracle error @xmath40 . for @xmath3-block graphons ,",
    "however , @xmath50 and @xmath41 becomes more significant .",
    "[ lem : sampling - k - block ] if @xmath1 is a @xmath3-block graphon , then @xmath609 and @xmath610{k / n})$ ] where @xmath611 .",
    "fix a @xmath3-block graphon @xmath1 , and let @xmath612 be the lengths of the `` blocks '' , that is the intervals defining the block representation ( so that @xmath613 and @xmath614 ) . given a sample @xmath615 of i.i.d .",
    "uniform values in @xmath8 $ ] , let @xmath616 denote the fraction of the @xmath617 that land in each block @xmath530 .    aligning @xmath618}$ ] with @xmath1",
    "consists of finding a permutation @xmath196 of @xmath68 $ ] .",
    "this maps each @xmath617 to one of the intervals @xmath619 where @xmath620 $ ] .",
    "we say @xmath617 is correctly aligned if its interval @xmath621 is contained in the block in which @xmath617 landed .",
    "for each block @xmath530 , we can ensure that @xmath622 of the points @xmath617 that landed in @xmath530 get aligned with @xmath530 ( the @xmath623 term accounts for the fact that up to @xmath84 of the length at each end of the interval does not line up exactly with one of the intervals @xmath624 ) .",
    "thus , the number of points @xmath617 that get incorrectly aligned is at most @xmath625 .",
    "each misaligned point contributes at most @xmath626 to the total squared error @xmath627 , so we have @xmath628 each term @xmath629 in the @xmath543 norm on the right - hand side is the deviation of a binomial from it s mean , and has standard deviation @xmath630 . this upper bounds the expected absolute deviation by jensen s inequality .",
    "thus , @xmath631 . the sum @xmath632 is maximized when @xmath633 for all @xmath530 ; it then takes the value @xmath634 .",
    "hence @xmath635 . by jensen s inequality ,",
    "@xmath636{k / n}$ ] , as desired .    for any graphon @xmath1 ,",
    "we have @xmath637{k / n})$ ] .",
    "fix a matrix @xmath1 , and let @xmath638 denote the best @xmath3-block approximation to @xmath1 in the @xmath2 norm ( that is , the minimizer of @xmath40 ) . given a uniform i.i.d . sample in @xmath615 , let @xmath42 denote the matrix @xmath639}$ ] , and let @xmath640 denote @xmath641}$ ] . by the triangle inequality , @xmath642    lemma  [ lem : sampling - k - block ] bounds @xmath643 by @xmath62{k / n}$ ] .",
    "it remains to bound the last term . squaring it ,",
    "we have @xmath644 . these terms are not independent , but each individually has expectation @xmath645 . by linearity of expectation , @xmath646 , and hence @xmath647 .",
    "in this section we prove remark  [ rem : hoelder - cont - w ] . throughout this section",
    "we assume that @xmath4 ^ 2\\to [ 0,1]$ ] is @xmath55-hlder continuous for some @xmath648 $ ] , i.e. , we assume that there exists a constant @xmath270 such that @xmath649    [ lem : w - wp ] let @xmath650 , and assume that the vertices of @xmath651 are reordered in such a way that @xmath652",
    ". then @xmath653}}\\|_2=o_p(n^{-\\alpha/2}).\\ ] ]    we first approximate @xmath1 in terms of the weighted graph @xmath654 with weights @xmath655 , where @xmath656 is the expectation of @xmath617 .",
    "since @xmath657 when @xmath658 $ ] , we can use the hlder continuity of @xmath1 to conclude that @xmath659}}\\|_2\\leq \\|w-{{w[{\\tilde h}]}}\\|_\\infty\\leq c\\bigl(\\frac 2n\\bigr)^\\alpha.\\ ] ] to prove the lemma , it is therefore enough to prove that @xmath660}}-{{w[{h}]}}\\|_2 ^ 2\\bigr]=o(n^{-\\alpha } ) , \\ ] ] where @xmath661 $ ] denotes expectations with respect to the random variables @xmath662 . using the hlder continuity of @xmath1 together with jensen s inequality , we bound @xmath663}}-{{w[{h}]}}\\|_2 ^ 2\\bigr ] & = \\frac 1{n^2}\\sum_{i , j\\in [ n]}{\\mathbb{e}}\\bigl[\\bigr ( w(\\bar x_i,\\bar x_j)-w(x_i , x_j)\\bigl)^2\\bigr ] \\\\ & \\leq \\frac { c^2}{n^2}\\sum_{i , j\\in [ n ] } { \\mathbb{e}}\\bigl [ \\bigl(|\\bar x_i - x_i|+|\\bar x_j - x_j|\\bigr)^{2\\alpha}\\bigr ] \\\\&\\leq \\frac { c^2}{n^2}\\sum_{i , j\\in [ n ] } { \\mathbb{e}}\\bigl [ \\bigl(2|\\bar x_i - x_i|^2 + 2|\\bar x_j - x_j|^2\\bigr)^{\\alpha}\\bigr ] \\\\ & \\leq \\frac { c^2}{n^2}\\sum_{i , j\\in [ n]}\\bigl ( 2{\\mathbb{e}}\\bigl[|\\bar x_i - x_i|^2\\bigr]+ 2{\\mathbb{e}}\\bigl[|\\bar x_j - x_j|^2\\bigr]\\bigr)^{\\alpha } \\end{aligned}\\ ] ] using the fact that @xmath617 has expectation @xmath664 and variance @xmath665 , we bound the right hand side by @xmath666 , completing the proof .",
    "[ lem : eps - k ] let @xmath135 be a partition of @xmath8 $ ] into adjacent intervals of lengths @xmath136 .",
    "then @xmath667    let @xmath668 . for @xmath669",
    ", @xmath670 is an average over points in @xmath87 , implying that @xmath671",
    "in this appendix , we prove the following theorem which implies theorem  [ thm : cuts ] by the same arguments as those which lead from theorems  [ thm : h ] and [ thm : h ] to theorems  [ thm : final ] and [ thm : final ] .",
    "[ thm : cuts - from - h ] let @xmath273 be an integer .",
    "\\(i ) under the assumptions of theorem  [ thm : final ] , @xmath672 {          \\lambda^2{{\\left ( { \\frac{k^2 } { n^2\\rho }              + \\frac{\\log k } { n\\rho }          } \\right ) } } } } \\right)}}.\\ ] ]    \\(ii ) under the assumptions of theorem  [ thm : final ] , @xmath673{\\frac{\\lambda^2\\log k}{\\rho n } }   + \\lambda\\sqrt{\\frac{k^2\\log n}{n\\eps}}+\\frac { { \\lambda}}{n\\rho\\eps } } \\right)}}.\\ ] ]    before we prove the theorem , we start with a few bounds on the hausdorff distance of various sets of @xmath157-way cuts . first , using the definition of the cut - distance ( and the fact that the set of @xmath157-way cuts of a graph is invariant under relabelings ) , it is easy to see ( see also @xcite ) that whenever @xmath0 and @xmath183 are weighted graphs on @xmath68 $ ] and @xmath674 is a partition of @xmath68 $ ] , then @xmath675 implying that @xmath676 in a similar way , we have that for two graphons @xmath677 , @xmath678 we will also need to compare the fractional and integer cuts , @xmath679 and @xmath680 . to do so , one can use a simple rounding argument , as in theorem 5.4 and its proof from @xcite .",
    "this gives the bound instead of @xmath681 ( leading to the factor @xmath682 on the right hand side of ) , and that hausdorff distances were defined with respect to the @xmath98-norm ( leading to a bound which is better by a factor @xmath157 than the bounds in @xcite ) . ]",
    "@xmath683 valid for any weighted graph @xmath0 with node weights @xmath547 on @xmath68 $ ] and maximal edge - weight @xmath547 .",
    "we also note that for any weighted graph @xmath281 , @xmath684}})\\ ] ] ( see ( * ? ? ?",
    "* proposition 5.3 ) ) . finally , we note that @xmath685 .",
    "in particular , @xmath686    let @xmath575 and @xmath687 . by lemma  [ lem : good - trho",
    "] , we have that @xmath688 $ ] with probability at least @xmath689 . by the assumptions of the two theorems , @xmath690 .",
    "we apply to show that @xmath691 with probability at least @xmath689 . as a consequence , again with probability @xmath689 ,    @xmath692 by and , this implies that with the same probability @xmath693    next we apply to the weighted graph @xmath694 . since @xmath695",
    ", we conclude that with probability at least @xmath689 , @xmath696 since @xmath697 for all @xmath698 and all @xmath699 , we can easily absorb the failure event , getting @xmath700 to complete the proof , we proceed as in the proof of to show that @xmath701 {          \\lambda^2{{\\left ( { \\frac{k^2 } { n^2\\rho }              + \\frac{\\log k } { n\\rho }          } \\right ) } } } } \\right)}}.\\ ] ] combined with the bound from theorem  [ thm : final ] , the fact that the cut - norm is bounded by the @xmath2-norm , and the fact that @xmath702 , we conclude that    @xmath703}}\\bigr ) \\leq { \\hat \\eps_k^{(o)}}({{h_n}(w ) } ) + o_p{{\\left ( { \\sqrt[4 ] {          \\lambda^2{{\\left ( { \\frac{k^2 } { n^2\\rho }              + \\frac{\\log k } { n\\rho }          } \\right ) } } } } \\right)}}.\\ ] ] combined with , , , and the bound , this proves .",
    "the proof of is essentially identical , except that now we use theorem  [ thm : final ] .",
    "[ lem : chernoff - mult ] let @xmath704 be independent random variables taking values in @xmath8 $ ] , and let @xmath705 .",
    "if @xmath706 and @xmath324 , then @xmath707 for @xmath708 , the probability is at most @xmath709    let @xmath710 denotes the exact mean of @xmath567 ( so @xmath711 ) .",
    "the standard multiplicative form of the chernoff bound states that for @xmath489 ( not necessarily less than 1 ) , we have @xmath712 setting @xmath713 ( that is , @xmath714 ) , the bound above becomes @xmath715 both of these terms are bounded above by @xmath716 : the first , since @xmath717 ; and the second , since @xmath324 ."
  ],
  "abstract_text": [
    "<S> we design algorithms for fitting a high - dimensional statistical model to a large , sparse network without revealing sensitive information of individual members . given a sparse input graph @xmath0 , our algorithms output a node - differentially - private nonparametric block model approximation . </S>",
    "<S> by node - differentially - private , we mean that our output hides the insertion or removal of a vertex and all its adjacent edges . </S>",
    "<S> if @xmath0 is an instance of the network obtained from a generative nonparametric model defined in terms of a graphon @xmath1 , our model guarantees consistency , in the sense that as the number of vertices tends to infinity , the output of our algorithm converges to @xmath1 in an appropriate version of the @xmath2 norm . in particular , this means we can estimate the sizes of all multi - way cuts in @xmath0 .    </S>",
    "<S> our results hold as long as @xmath1 is bounded , the average degree of @xmath0 grows at least like the log of the number of vertices , and the number of blocks goes to infinity at an appropriate rate . </S>",
    "<S> we give explicit error bounds in terms of the parameters of the model ; in several settings , our bounds improve on or match known nonprivate results .    </S>",
    "<S> = 1 </S>"
  ]
}