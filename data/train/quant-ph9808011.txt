{
  "article_text": [
    "the term ` animal faith ' in our subtitle is taken from the title of santayana s book  skepticism and animal faith\"@xcite ; it refers to what gets us through the day and keeps our thought processes going even through our spells of radical doubt .",
    "santayana was concerned to delimit animal faith and to contrast it with other , presumably higher , things .",
    "our agenda here is quite different and is closer to kant s : it is to articulate and transform into explicit principles the animal faith implicit in certain of our concepts that play a key role in science .",
    "once this is done , once these principles become explicit , we ll find that they take on a new life of their own , and are full of surprises .",
    "first , they reveal the simple mathematical structure that unifies quantum and classical .",
    "but then they strangely turn against themselves , revealing their own limitations , and even rather impolitely suggesting that perhaps we should find better ways to get through the day .",
    "this paper , however , is about their more cheerful messages .    like",
    "freud s  talking cure \" , our analysis will start by paying attention to how we talk about everyday things , and then go on to explore subterranean labyrinths in search of hidden meanings .",
    "but it is not a search for what has been repressed , or for what has fallen into the unconsciousness of habit , but for that ancient animal heritage of  know how \" that we , as thinking people , unconsciously draw upon in formulating our most sophisticated thoughts .",
    "lest this sound too ambitious , let me add that we are confining our analysis to what underlies certain commonplace scientific words , notably : _ information , part , place , event , variable , process , procedure , system , input , output _ and _ cause_. the unconscious beliefs we are searching for are those that belong to the smoothly working hidden machinery behind the easy flow of thoughts in which these words occur .",
    "to put it another way , without certain implicitly held principles these commonplace words , which are indispensable in any discussion of scientific matters , would be quite meaningless , and these are the principles we are trying to capture and articulate .",
    "we shall see that , when these principles are precisely articulated , they become a tightly knit system that has some surprising consequences , including , as mentioned , the two ` core laws ' of quantum mechanics . more generally , the consequences of an analysis of animal faith are of two kinds  call them kantian and freudian ; the kantian kind reinforce our animal faith , while the freudian kind force us to question it .",
    "let s begin our analysis with information .",
    "suppose you see something and write down what you see ; this is a _",
    "datum_. _ data _ is what has been seen and noted . to qualify as _ information _ , however , data must have some element of surprise . if you know that a is going to happen , seeing it happen may be gratifying or reassuring , but it is not _",
    "informative_. thus _ information _ is what you have seen , contrasted with what you _ might _ have seen . when you _ describe _ something , you normally supply a number of connected pieces of information , so _ description _ is information broken up into _ connected parts_.    these very simple and ordinary observations are beginning to give hints of a pattern , but they come to a halt with the difficult word ` _ parts _ ' .",
    "it s curious how in the long and contentious history of speculations about what the world is made of , almost all the debate has been about what are its _ ingredients _ , and almost none about the more basic problem of what it means for these proposed ingredients to fit together as parts .",
    "taking ordinary material things apart and putting them together is such a natural activity that we blithely extend it to entities of every kind , without ever imagining that this might lead to problems .    and",
    "this is certainly the case with information , as when we speak confidently of  partial information \" ,  the whole story \" , etc .",
    "no doubt we are at ease with such talk because the parts and wholes of information so often coincide with the parts and wholes of the language that conveys it . but",
    "underlying this linguistic idea of part and whole there is a deeper level of meaning , and excavating that deeper level will be the main task of our analysis .",
    "first of all , we must carefully distinguish between two kinds of part- whole relationship : that which we find in space and space - time , and that which we find in material structures like buildings and computers and molecules .",
    "let s call the first relationship _ extension _ and the second _ composition_. the parts of an extension are its _ regions _ , and the identity of a region , i.e. that which makes it different from other regions , is its _ place_. the parts of a composition , on the other hand , are its _ components _ , and the identity of a component is not its place but its _",
    "type_. components are _ interchangeable _ parts ; you can duplicate them , remove them from their places and use them elsewhere , etc . but a region , far from being an interchangeable part , is by its nature unique ; the people of palo alto can leave california , even the buildings of palo alto can leave california , but palo alto itself ca nt leave california since it s a _ place _ in california .",
    "matter , as we currently conceive of it , is composed of elementary particles , the most perfectly interchangeable of parts imaginable , which makes matter a composition par excellence , which is to say , its essential self is nowhere .",
    "there are those today who speak of making matter out of space or space out of matter , but this is nonsense ; you ca nt make extension out of composition nor composition out of extension .",
    "those who claim to have a theory of _ everything _ , even if they are right , have only found half of the holy grail ; the other half is a theory of _ everywhere _ !",
    "the material objects we encounter in everyday life are always both regions and components ; they have both places and types .",
    "thought is a constant dialogue between extension and composition . practically speaking , the important distinction to keep in mind",
    "is between parts that do nt keep their identity when you remove them from their context , like random variables in a joint probability distribution , and those that do , like logic gates in a computer .",
    "only the latter can be used to _ construct _ things .    in mathematics ,",
    "the dialogue is between geometry and algebra .",
    "think of analytic geometry , which is just such a dialogue .",
    "in pure euclidean metric geometry , we start out with a homogeneous space whose places are _ points _",
    ", i.e. we can only identify them by _ pointing _ to them as  here \" or  there \" or  there \" etc . in order to keep track of these points ,",
    "we move into algebra .",
    "we do this by giving each point a unique identity as a composition of an x - vector , a y - vector and a z - vector ; this identification scheme is known as a coordinate system .",
    "notice that vectors themselves are not regions or places ; you specify a vector as a certain _ type _ of thing that can be found anywhere .",
    "notice also that we must carefully specify just what it means for one vector to be a part , i.e. a component , of another ; this particular kind of part - whole relation is what defines linear algebra as opposed to other kinds of algebra .",
    "finally , notice that giving coordinates to points does not entirely get rid of _ here _ and _ there _ ; thus the dialogue continues since we must locate the 0 vector _ here _ , and we must then point  there \" ,  there \" ,  there \" in the directions of x , y and z. nor can we completely get rid of _ this kind _ and _ that kind _ in geometry , since geometric structure involves numerical ratios of distance , so we have _ this _ and _ that _ number , and onward to _ this _ and _ that _ figure , etc .    how does the distinction between extension and composition apply to informational structures ?",
    "information accumulates as a progressive _ extension _ of the  body of knowledge \" .",
    "incoming items of information build on each other , qualify each other , and in general , can only be understood in relationship to their  neighbors \" .",
    "_ this _ happened , and then _ this _ happened and then _ this _ happened .. \" etc . ,",
    "where in each case to know _",
    "what _ happened we must know something about what happened previously . from time to time , though , we _",
    "abstract _ an item of information from its place and give it a kind of autonomy by turning it into a story , a design , a warning , an example , a rule , a law , a procedure etc .",
    "that can be retold or reused in other contexts ; in short , we turn it into a component .",
    "one way to analyze a process is to systematically reconstruct it from components that have been so abstracted ; we ll call this a _ reductive analysis _ and the resulting composition a _",
    "system_. let s reflect a bit on the distinction between process and system , which will play an important role in interpreting our mathematical results .",
    "the word ` process ' , in its most general sense , means something that ` proceeds or moves along ' .",
    "but the word also has the connotation of _ procedure _ as in _",
    "due process_. thus a process of a certain kind refers to that which is allowed to happen , or which can happen , under certain specified conditions .",
    "this is how we shall construe the present technical meaning of the word .",
    "if the possibilities under the specified conditions are assigned probabilities we ll call the process a _ stochastic _ process .",
    "more exactly , a stochastic process is a joint probability distribution on a set of so - called stochastic variables . in chapter 3",
    ", we ll focus on markov processes , which are the simplest and best known stochastic processes , and we ll see that the core quantum laws as they occur in markov processes take the form most familiar to physicists .",
    "a system is a process analyzed into interchangeable parts .",
    "a good system brings order to a complex whole by portraying it as a regular arrangement of a small variety of such parts .",
    "for stochastic systems the paradigm case is a markov chain , which is a  chain \" of connected copies of a single part called a _",
    "transition matrix _ t@xmath0 .",
    "we can think of t@xmath0 as a representation of a stochastic variable j ( the column variable ) whose probability distribution p(j ) is a function of a free parameter i ( the row variable ) .",
    "connecting means assigning the free parameter i to the stochastic variable j of the prior transition matrix , thereby turning the numbers t@xmath0 into conditional probabilities .",
    "if we assign the free parameter of the first component to an _ unconditioned _ variable , then chaining transmits the definiteness of the first variable down the line to create a joint probability distribution on all the succeeding variables , and the resulting process is a markov process .",
    "the contrast between process and system is roughly that between extension and composition .",
    "a process is extended , its regions being _",
    "events_. like many english nouns ( ` noun ' , for instance ) , the word ` event ' can be taken either in the definite or the indefinite sense ; an event can be something in particular that happened , in which case it has its unique place in space - time , or it can be a particular _ kind _ of happening , as when we speak of  the event heads \" in probability theory .",
    "one way to describe a process is as an arrangement of events in the second sense , which we can think of as labeling its extended parts by certain of their qualities ; another word for this is _ map - making_. a map is a composition of essences , or predicates , to use the modern term , but the parts of the process that these essences identify need not be removable components , so a map does not in general represent the process itself as a composition .    thus we see that there are two ways to analyze a process , or any other extended whole : we can _ map _ it ; this is the way of the naturalist , and also of the explorer , the historian and the astronomer . or we can _ reduce _ it to a heap of autonomous parts , which we then reassemble into a composition that has the same map the naturalist would draw .",
    "nowadays we often hear about the naturalist as the good guy and the reductionist as the bad guy . but building things and taking things apart belong to life as much as exploring and drawing pictures .",
    "when the reductionist goes bad , it s usually because the naturalist hasnt given him a good enough map ; what the reductionist then recreates from his storehouse of interchangeable parts may resemble the naturalist s whole , but the essence of the original is missing .    to give to a part the autonomy of a component , we must usually do more than just copy it as it appears in place .",
    "rather , we must  de - install \" it from its original context , which is to say , we must transform it into a new entity whose features are no longer conditioned by place . to reductively analyze a process is to de - install its partial regions , at least in our imagination , turning them into items that can stand alone , and that can be duplicated in such a way that the duplicates can be reassembled into a duplicate of the original whole . reductive analysis gives the observer a more penetrating gaze , which sees not only what and where things are but where and how they come apart .",
    "the metaphor of de - installing and re - installing components should be a vivid one for anyone who has had to wrestle with the problem of de - installing an app from windows .",
    "in fact , de - installing a computer program is literally a special case of the operation we shall describe here of _ disconnecting _ a component .",
    "we ll go into all this in detail later , but for now let it just serve as a reminder of how much hard work goes into seeing our ordinary world as full of _",
    "things_. this hard work is largely unconscious , and is based on  know - how \" dating from our dim animal past .",
    "our problem today is that , with the progress of scientific thought , we have wandered into domains where this unconscious skill no longer serves all our needs ; for instance , it falls far short of telling us how to de - install a quark from a nucleus , or an event from a time loop , or for that matter , how to de - install a thought from the stream of consciousness .",
    "thus it has become essential to dredge up and articulate the principles we unconsciously rely on in this work of de - installing things , and consciously learn to do it better , for otherwise we risk disastrous encounters with things we ca nt imagine and therefore ca nt see .",
    "the naturalist , in contrast to the reductionist , sees things in place ; he sees what and where they are in the context of the process as a whole .",
    "this involves seeing extensional separations and boundaries , but it also involves seeing how things _ function _ within the whole . actually , as we ll see , functional structure is best described in terms of the following three - place relation :    * separability * : we say that b _ separates _ a and c , or that a and c are _ separable _ at b , if fixing b makes the uncertainty or indefiniteness as to what is the case with a independent of the uncertainty or indefiniteness as to what is the case with c.    the most familiar example of this notion is the separability of the future from the past by the present : if we know everything about the present , then getting more information about the past does not give us any more information about the future , and vice versa , or so we suppose .",
    "( remember , we are now examining our presuppositions , not looking for objective truths ) .",
    "note that this says nothing about the determinism of the future by the present  all it says is that the past and future , however else they may be related , only  communicate \" with each other via the present . which brings up another familiar example : the separability of two communicating parties by their line of communication .",
    "if a and c are talking by phone , tapping their phone line ( b ) can tell you everything that s going on between them ; whatever else is going on with the two of them at the time is going on with each of them independently .    another term for separability is _",
    "conditional independence_. in the theory of stochastic processes the two terms have essentially the same meaning , though we ll keep them both to refer to slightly different mathematical formulations ( see chapter 2 . )    * independence , unconditional and conditional * : events a and c are called _ independent _ if the probability of a&c is the probability of a times the probability of c. a and c are called _ conditionally independent _ given condition b if they are independent in the probability distribution conditioned by b , i.e. if p(a&c@xmath1b ) = p(a@xmath1b)p(c@xmath1b ) .",
    "if for some random variable x , a and c are independent given condition x = k , where k is any value of x , then we say that x _ separates _ a and c. in the above telephone example , if we regard b as a random variable , then b separates a and c.    since every probability distribution is conditioned by something , there is a sense in which all independence is conditional .",
    "the relation of independence , like every notion of being different or separate , is really a three - term relation : a and c are independent in a certain context b. we have just considered the case where that context is the result of placing a certain condition on a probability distribution .",
    "but the more familiar case is that in which the context is the result of _ removing _ a certain condition on a probability distribution .",
    "notice that although there is only one way to impose a condition , there are many ways to remove a condition ; this is one reason why there are so many different ways to represent a given process as a system .",
    "the converse of  deconditioned \" independence is :    * conditional dependence * : we say that a and c are _ conditionally dependent _ given b if p(a&c@xmath1b ) is unequal to p(a@xmath1b)p(c@xmath1b ) .",
    "a very important kind of conditional dependence is that in which the condition b is of the form x = y , where , without b , a&x is independent of y&c .",
    "this is called linking , and we ll return to it shortly .",
    "there is a fundamental theorem that relates linking to separability :    * disconnection theorem * : process a&x&c is separable at x if and only if there exists a process in which a&x is independent of y&c which reduces to a&x&c under the condition x = y . this , and other theorems stated in this chapter , are proved in chapter 3 .",
    "speaking more generally , what makes separability so important is that it always coincides with the possibility of breaking a process into two de - installed components of some kind .",
    "we separate a and c by listening in on their phone line b , which is at the same time their connection and their common functional boundary , at least as far as their conversation goes . on the other hand",
    "we _ disconnect _ a and c , turning them into autonomous components , by _ cutting _ their phone line ; this ends their conversation because their phone line is their separation boundary .",
    "when we mark the separation boundaries in a process , we may be functioning as naturalists , but we are also like the butcher with his blue pen marking the chops in a carcass .    to reconnect a and c so as to restore the whole process is to equate what is the case with b1 and what is the case with b2 , thus restoring the original situation of a&b&c .",
    "just what it means to equate b1 with b2 turns out to be less obvious than it sounds , and in fact can be understood in two very different ways ; we ll call these two ways _ i - o connection _ and _ linking_. understanding exactly how an i - o connection differs from a link is the key to understanding how quantum processes differ from classical processes such as phone conversations .",
    "let s start with i - o connection .",
    "we ll simplify slightly by supposing that a is talking to c on a one - way line b. if that line is cut , what happens to the signal voltage at the two cut ends b1 and b2 ?",
    "the cut line b1 from a is an _ output_.",
    "this means , ideally , that what comes down the wire depends only on a , so the signal at b1 is unchanged by the cut .",
    "the cut line b2 from c is an _ input _",
    ", however , and our description of the situation does nt specify how inputs behave when they are disconnected .",
    "an open line will normally produce white noise , sometimes mixed with hum and faint radio signals .",
    "however , and this is a crucial point , the designer of a system usually does nt have to take into account disconnected inputs ; he only needs to think about how the system behaves when inputs are  driven \" by outputs , whether from the user or from other components . to put it another way , the designer can regard inputs as",
    "_ free variables _ ; his components are _ open processes _ , analogous to open formulae in the predicate calculus .",
    "open processes are interconnected by equating inputs to outputs , which means _ assigning _ free variables to non - free or _ bound",
    "_ variables ( assigning variables is a familiar concept for computer programmers . ) a composition created in this way will be called an _",
    "i - o system_. if all the free variables of the components are assigned , we call the system _ closed _ ; otherwise it s an _ open _ system .",
    "we are on familiar ground here ; this is how engineers and programmers are taught to think these days . once again , here are the main ideas :    * input * : a free variable in an open process .    * i - o connection * : the assignment of a free variable to a bound variable .    *",
    "output * : the bound variable to which a free variable is assigned .",
    "disconnecting is just the reverse of connecting .",
    "more exactly :    * i - o disconnection * : a and c are disconnected at variable b means we are given two processes a&b1 and b2&c with b2 free such that when b2 is assigned to b1 , the result is the original process a&b&c .    * i - o system * : a composition of connected open processes .",
    "every i - o system represents a process , but we must be careful to distinguish this process from the system , just as we must be careful to distinguish a vector from its representation as a sum in a particular basis .",
    "there s something not quite right about all this .",
    "after all , a free variable is only an abstract linguistic entity , whereas our cut wire , lying there on the ground and picking up white noise , remains very much a real material object .",
    "the parts of a telephone system are not  indefinite processes \" they are just as definite as houses and trees and stones . or so says animal faith .",
    "there is in fact a way to describe the situation that s more in keeping with this bit of animal faith , which is to replace the concept of assigning a free variable by the concept of _ linking _ a so - called _ white _ variable .    *",
    "white variable * : a random variable on which there is a uniform probability distribution . in the case of a discrete variable , whiteness means that every value is equally probable , and hence the variable has a finite number of values , since their probabilities must add up to 1 .",
    "a white real variable has a uniform probability density which must integrate to 1 , so it s range is bounded .",
    "* theorem * : if x and y are independent random variables with the same range , and y is white , then the condition x = y on their joint probability distribution does not change the distribution on x.    in our telephone example , let s now forget about free variables and think of the components a&b1 and b2&c as closed processes . taken together ,",
    "the two can be regarded as independent regions in the process a&b1&b2&c .",
    "when we connect b1 to b2 , the result is a new process a&b&c .",
    "how does this new process differ from the old ?",
    "the obvious answer is that it is just like the old one except for its behavior being restricted by the condition that b1 equals b2 .",
    "connecting two wire ends means making their signals equal .",
    "it is not immediately clear how this answer , which is couched in the physical language of broken wires , signal voltages etc .",
    ", applies to processes as such .",
    "( remember , a process , as we are now defining it , is simply a joint probability distribution on a set of variables . ) .",
    "however , the phrase  .. restricted by the condition .. \" gives a strong hint .",
    "to say  the probability distribution d1 is just like the probability distribution d2 restricted by the condition c \" means that you get d2 from d1 by conditioning all event probabilities in d1 by event c. thus our answer is basically this : to connect any two variables x and y , condition all of the probabilities of the disconnected system by the event x = y . this particular bit of animal faith brought to light will turn out to be the key to making sense out of quantum mechanics , so let s forthwith coin words that nail it down :    * links * : place a condition of the form x = y on two variables x and y of a stochastic process , thereby creating a new process in which the unconditional probability p(e ) of any event e is p(e @xmath1 x = y ) in the old process .",
    "this condition is called a link .    * link system * : a stochastic process plus a set of links .",
    "we ll go over all of this in considerably more detail in chapters 2 and 3 .",
    "a summary of the relevant concepts , terminology and notation of probability theory will be found in chapter 3 , section 1 .",
    "let s now briefly look at some high spots .",
    "first , if x and y are independent and y is white , then the link between them behaves exactly like an i - o connection , with y acting like the input ; this follows immediately from the above theorem .",
    "we ll show in chapter 3 that we can neatly translate any i - o system into a link system which represents the same process and has essentially the same formal description .",
    "engineers should have no problem adapting to link descriptions .",
    "second , and this is extremely important , we ll prove that this natural mapping of i - o connections to links does not go backward !",
    "though every i - o system is equivalent to a link system , most link systems have no i - o equivalents , and even when one can make an i - o model of a link process , that model is usually exponentially more complicated than some link representation of the same process .",
    "quantum systems are among those having no i - o counterparts , though quantum measurement systems do , as we ll see in section 3.7 .",
    "if technology can learn to really deal with this larger class of non i - o systems , it will turn into a fundamentally new kind of enterprise .",
    "third , a crucial definition , that of a _ link state _ :    * link state * : _ given _ any link x = y , the _ state _ of that link is defined as the matrix representing the joint probability distribution on x and y without the link ( actually , it s this matrix normalized by dividing by its trace , but that s a small detail . )",
    "this is the concept that replaces the quantum wave function , and does away with the problem of the collapse of the wave function and other related quantum nuisances , as we discuss later .    in the introduction",
    "we briefly mentioned caloric .",
    "heat , in the eighteenth century , was regarded as a fluid called caloric which was somehow related to the mechanical behavior of objects , but subject to its own non - mechanical laws .",
    "a series of experiments during the first half of the nineteenth century , starting with count rumford s crude qualitative observations , and ending with the definitive work of joule and helmholz , convinced physicists that they did nt actually need to postulate a new substance called caloric , since heat is simply energy .",
    "when it was further realized that this energy is the random kinetic energy of molecular motion , thermodynamics became statistical mechanics .",
    "history to some extent repeated itself in the early twentieth century with de broglie s wave mechanics .",
    "his  matter waves \" were at first thought to be waves in some new kind of fluid , the quantum analogue of caloric , but after born discovered his quantum probability rule , they lost most of their substantiality , and physicists began to call them  probability waves \" .",
    "quantum mechanics , however , did nt go the way of thermodynamics .",
    "because the so - called probability wave is really a wave of the square root of probability ,  quantum caloric \" did nt seem to be reducible to anything already known . and",
    ", alas , it is still with us , ignored by most working physicists , but hanging around the edge of physics in a kind of limbo where from time to time it is reworked and touted as a wonderful new discovery .",
    "but fortunately this unhappy state of affairs is almost over .    here , in a nutshell ,",
    "is how link states reduce  quantum caloric \" to something already known , namely probability :    suppose x and y are independent random variables .",
    "then the probability that x and y will both have some value k is the product of the probabilities that they will have k separately , i.e. p(x = k & y = k ) = p(x = k)p(y = k ) .",
    "let s call p(x = k ) and p(y = k ) the unlinked probabilities of x being k and y being k. now suppose we impose the link condition x = y .",
    "the linked probability of x = k is then the probability , conditioned by x = y , that both x = k and y = k .",
    "the probability that x is k , as a function of k , is proportional to p(x = k)p(y = k ) .    in short , linked probabilities",
    "are always quadratic in unlinked probabilities . if the distributions on x and y are identical , which is the quantum situation , then linked probabilities are the squares of unlinked probabilities .",
    "that s essentially all there is to it !",
    "quantum amplitudes are not the intensities of some mysterious new fluid , but are simply unlinked probabilities .",
    "one might ask  probabilities of what ? \" , but we ll see that we do nt have to answer this question , just as we do nt have to answer the question of what sets of things have numbers x and y in order to understand the meaning and truth of xy = yx",
    ".    a generalized form of quantum amplitudes will be found in any statistical situation whatsoever if we subject that situation to link analysis .",
    "quantum amplitudes proper will show up if probabilities can go negative ( we ll return to this in a minute ) and we de - install components in such a way that the link states possess quantum symmetries ; as we ll see in chapter 3 , this is always possible , though not always advisable .",
    "link states are quantum states represented as von neumann density matrices .",
    "however , we can start in the more familiar way with quantum states as vectors over the amplitude field , and when we apply link analysis to the schrodinger equation , amplitudes still turn out to be unlinked probabilities . here , very roughly , is why this is so ; the details are in chapter 3 .    in a continuous markov chain , regarded as an i - o system",
    ", the probability distribution on the state variable can be represented as a vector which evolves in time according to the law v = t(v ) , where v is the distribution vector at time t , v the vector at t+dt , and t the differential transition matrix .",
    "the schrodinger equation , expressed in the language of hilbert space , has exactly the same form , the difference being that the components of v are amplitudes rather than probabilities , and t is a unitary matrix rather than a transition matrix .",
    "let s now represent the markov chain as a link system rather than an i - o system .",
    "the changing state vector is now the diagonal of a link matrix s that changes according to the law s = tst@xmath2 .",
    "if we think of s and s as density operators , this is the von neumann generalization of v = t(v ) .",
    "an immediate corollary is the born probability rule in the generalized von neumann form prob(p ) = trace(ps ) .",
    "the rule for turning an i - o system into a link system gives s the form @xmath1 v @xmath3 w @xmath1 , where w is a  white \" vector , i.e. it represents a uniform probability distribution , so it is only v that varies with time .",
    "the diagonal entries are quadratic in the entries of v and w , but since the constant entries of w are equal they factor out , so the probability vector is just v , the same probability vector that occurs in the i - o system .    next ,",
    "let s do the same with the quantum chain .",
    "again we have s = tst@xmath2 , but now t is unitary . and",
    "again we have s = @xmath1v@xmath3w@xmath1 , but now instead of w being white , we have w = v , which makes the diagonal entries into the squares of the entries in v , i.e. state probabilities are the squares of  amplitudes \" ! the amplitudes are no longer the values of some mysterious wave function but are simply the probabilities in the joint distribution on a pair of unlinked variables .",
    "quantum caloric is nowhere in sight !    that takes care of the main obstacle to a statistical interpretation of quantum mechanics .",
    "the square law is no longer a reason not to regard probability waves as true waves of probability .",
    "it s true , the wave amplitudes are nt the probabilities that we measure ; rather , they are the probabilities that we would measure if the quantum process were disconnected at a separation point .",
    "but the same is true in a markov chain , analyzed as a link process , the only difference being that in the markov chain the output probabilities do nt change when the output is linked to an input . in a quantum chain , ",
    "input \" and  output \" have identical distributions , and since unlinked state variables are independent , linking yields the unlinked probabilities squared .",
    "there is still the problem of amplitudes going negative ( they can also go imaginary , but as mackey observed , the scalar i can be interpreted as a unitary symmetry in real quantum mechanics)@xcite . what could possibly be the meaning of negative probabilities",
    "? how can there be fewer than 0 things having a certain property p ? how can an event e occur less than 0 times ?",
    "these are not questions that can be answered by constructing some ingenious mechanism , since they involve logic itself .",
    "earlier we compared negative probabilities to negative piles of gold , my point being that there is no problem in constructing a useful mathematics that incorporates them .",
    "but that does nt tell us what they really are , and we now doubt very much that their deeper significance can be understood at all within the scope of science as we know it .",
    "this could be enough to drive many people back to quantum caloric .",
    "but before you join the stampede , be aware that the logical mysteries in quantum mechanics do nt come from the link state interpretation .",
    "they come from the bare fact of amplitude cancellation , and practically stare you in the face with epr",
    ". how can it be that a is possible , and also that b is possible , but that ( a or b ) is impossible ?",
    "that s the mystery of the two - slit experiment . here `` or '' is the logician s `` or '' which allows a and b both to be true , a true and b false , a false and b true , but requires ( a or b ) to be false if both are false .",
    "the double slit example we have is mind is : case a : slit a open leading to a detection ; case b : slit b open leading to a detection ; case ( a or b ) : both slits open but there is no detection .",
    "von neumann was well aware of the logical strangeness of quantum phenomena , and tried to cope with it by means of a non - boolean construction that he called quantum logic .",
    "this turned out to be a dead end , but other strange logics dating from the period of quantum logic show more promise .",
    "enough of this for now , however .",
    "we ll return briefly to the logic of negative case - counting in chapter 2 , but it s too big a subject to do justice to here ; suffice to say that the logical anomalies do nt vitiate our simple mathematics .",
    "so where do we stand with quantum mechanics ?",
    "we ve gotten rid of  matter waves \" but the mystery of quantum interference remains , since to derive the schrodinger equation in link theory does require negative probabilities , even in the process itself .",
    "but now , at least , the mystery of interference has become a _",
    "simple _ mystery , and there is no longer any reason to think of it as belonging to physics in particular .",
    "once one has really grasped how universal the two generalized quantum laws are at the case - counting level , it s as hard to take seriously the arcane  models \" of quantum mechanics that abound today as it is to take seriously the ancient sheep farmer s  spirit of tranquillity \" after one has understood that xy = yx .",
    "but we re afraid we are getting somewhat ahead of our story .",
    "we started out to psychoanalyze animal faith , to dredge up some of the unconscious beliefs that underlie our scientific thinking .",
    " wait a minute ! \" , you protest ,",
    " i do nt know about you , but i ca nt find any of this fancy mathematics in my unconscious ! \"",
    "fair enough , neither of the authors can either .",
    "it s the analyst s job to propose , not to pronounce , and his proposals must always be confirmed by the analysand , who in this case is all of us .",
    "however , what we re looking for in our unconscious is certainly not fancy mathematics .",
    "animal faith in itself , i.e. in place , ca nt even be put into words .",
    "to  find \" it means to articulate something that does the same job , namely that of supporting our habitual modes of conscious thinking . in our current jargon , to articulate an item of animal faith means to de - install it , i.e. to transform it into a proposition that can stand alone .",
    "however , once articulated and understood , this new creation can then be put to the test by re - installing it in our intuition : does it ring a bell ?",
    "do we think  ah , yes , that s what i ve always thought , even if perhaps i never quite said it that way . \" ?",
    "if the items we have articulated pass this kind of test , then we can confidently move on to the interesting surprises .",
    "quantum mechanics is one such interesting surprise ; another , which is the topic of chapter 2 , is a very kantian answer to hume s skepticism about the existence of causes .",
    "kant s own answer was that we necessarily see causes everywhere because , to paraphrase the way bertrand russell put it , we wear  causal - colored glasses \" @xcite . by combining the philosopher von wright s insight into how we operationally define causality with our analysis of process and system , we ll bring this statement up to date : we necessarily see input - output systems everywhere .",
    "since input - output systems are closely related to computer models , it could be said that we necessarily see computers everywhere - we all wear computer - colored glasses !",
    "if all this sounds like the grand march of progress , be aware that the two surprises above are not entirely harmonious .",
    "what makes our kantian analysis of causality possible is the fact that the separability structure of a process only partly determines the form of the link matrix s , and we can show that it is always possible to choose a causal s ( i.e. , an s with an input ) .",
    "however , quantum s s are not causal ! furthermore , and this is crucial , the choice of s for a given disconnection will , in general , affect the separability structure of the disconnected parts .",
    "thus things that come apart neatly with one type of s may exhibit bad  non - localities \" for another type of s. epr comes apart very neatly with a quantum analysis , but any causal analysis compatible with the data will exhibit non - locality , as bell s theorem shows .",
    "thus , though we are always perfectly free to choose causal s s and thus turn the universe into a giant computer , this computer may have a gosh - awful tangle of long distance wiring that could be eliminated by using other s forms .",
    "kant has had his say  now it s freud s turn .",
    "freud s project was not to justify our beliefs but to change our minds .",
    "he was , of course , working with people whose minds were very much in need of changing ( which of course does nt include you and me ) .",
    "still , he may be able to give us a few useful tips .",
    "as long as our assumptions remain unconscious , we have no choice but to believe them , or rather , we have no choice but to act as if we believed them",
    ". the question of belief does nt actually arise until we are able to contemplate and weigh alternatives .",
    "for this to happen with some article of animal faith , we must first of all raise it up from the unconscious depths in such a way that it becomes a proposition , i.e. we must de - install it .",
    "then one of two things may happen .",
    "we may take a clear look at this proposition and recognize its absurdity :  wait a minute , my mother is nt really threatening to lock me in my crib if i try to go outdoors ! \" , or whatnot . or , we may see that the proposition is obviously true .",
    "if this happens , then another question arises : is this truth  objective \" , or does it depend on how you look at it ?    how do we answer this last question ? how do we find out whether something depends on how we look at it ?",
    "one way is to see whether we can look at it differently .",
    "if we believe that all print is too fuzzy to read , we might try on glasses . of course we may have to experiment to find the right glasses .",
    "kant s error was to confuse our glasses with our eyes , so - to - speak .",
    "the kantian conclusion of chapters 2 and 3 is that we all wear causal glasses , which turn out to be the same as computer - colored glasses .",
    "does this mean that we are forever fated to see everything as a computer , or can we take these glasses off ? as mentioned ,  causal glasses \"",
    "correspond to a particular kind of link matrix .",
    "we can work mathematically with very different kinds of link matrices , and indeed we must do so in order to practice quantum physics .",
    "chapter 3 will make this very clear , and will point us in the direction of an expanded kind of science that works with the full range of acausal state matrices .",
    "but the question still remains : can we , as human beings living our daily lives , ever take off our causal glasses ?    for mathematicians , all things are possible",
    ". the experimenter who is willing to push hard enough may occasionally force nature to reveal its unnatural proclivities .",
    "but you and i , when we are just trying to get on with it , must constantly ask questions whose answers begin with  because \" . that s our human nature . but what will be the effect on how we live and act if we can learn from the coming science how to go beyond  why \" and  because \" ? what sort of beings might we someday become ?",
    "all modern discussions of causality begin with hume . it was hume who first clearly pointed out that merely knowing the regular succession of events does not tell us what causes what .",
    "the fact that b always follows a gives us no grounds for concluding that a  forces \" b to happen .",
    "why speak of causes at all , then ?",
    "one school of thought says we should stop doing so ; here is the young bertrand russell on the subject@xcite :     ... the reason why physics has ceased to look for causes is that , in fact , there are no such things .",
    "the law of causality , i believe , like much that passes muster among philosophers , is a relic of a bygone age , surviving , like the monarchy , only because it is erroneously supposed to do no harm . \"",
    "the mature bertrand russell , however , found out that like the rest of us he was unable to say very much outside of logic and pure mathematics without bringing in causes .",
    "cause and effect pervades everyday life ; the solution to hume s problem ca nt be confined to the ivory tower of philosophy .",
    "a recent news item reported the discovery of a correlation in mexico city between heavy chili pepper consumption and stomach cancer . for us",
    "chili pepper lovers , this was most unwelcome news : do chilies cause cancer , or might there be some other explanation ? what s at stake here is no mere philosophical abstraction ; it s whether we have to stop eating chilies !",
    "if there is no causal link between a and b , then , even if there is a high correlation between a and b , our wanting to bring about or prevent b is no reason to do anything about a. but if we learn that a causes b , or that it significantly increases the probability of b , then whatever power we may have over a becomes power over b too .",
    "the understanding of cause and effect is essential for practical life , indeed it is essential for our very survival , because it is the knowledge of means and ends .",
    "this obvious fact of life has been curiously neglected by philosophers .",
    "the logician g. h. von wright , however , was an exception ; he actually went so far as to propose that the ends - means relation be used as the defining criterion of causality @xcite .",
    "that is , he would have us say that , by definition , a causes b if and only if a can be used as a means to b , and more generally , that there is a causal connection from a to b if and only if we can change b by changing a. notice that this is an operational definition : it says that the presence or absence of a causal connection between a and b can be detected by performing a certain operation , which is to change a and notice whether there is a concomitant change in b. that is , of course , just how diet scientists would go about seeing whether chilies cause cancer .",
    "does von wright s definition solve hume s problem ?",
    "it certainly gives causality an empirical anchor that can not be neglected in any proposed solution .",
    "however , a very important question still remains : is it possible to find formal patterns in the data itself that correspond to the causal connections revealed by the dispositional relation of means - to - ends ?",
    "this is a difficult and subtle question , and the search for its answer ultimately takes us into territory where the question itself essentially disappears .",
    "first , though , there is a lot of work to be done closer to home .",
    "the people who really live with causality are not philosophers but engineers , and curiously enough , they rarely use the word  cause \" . that s not because , as russell said ,  there is no such thing \" , but because they have developed a much richer and more discriminating vocabulary for the various causal structures and relationships they deal with .",
    " variable , ` input ' , ` output ' , ` process ' , and ` function ' are standard causal terms .",
    "computer science has added some important newcomers such as , ` assignment ' , ` call ' , and ` subroutine ' .",
    "indeed , the concept of subroutine neatly revives the whole aristotelian foursome of causes : the material cause of a subroutine is the computer it runs in , the formal cause is its source code , the efficient cause is its call , and the final cause is the purpose it serves in the calling program .",
    "it s clear from this brief survey of terms that , for the engineer , causality is an aspect of how something is composed , not of how it is extended .",
    "the engineer s job is to put together certain components into a system that realizes a causal process , i.e. a process having inputs that produce certain desired outputs .",
    "furthermore , the components themselves are causal processes , and putting components together means using the outputs of certain components to causally control the inputs of others . as remarked in chapter 1 , the information engineer does not normally concern himself with the behavior of unconnected inputs . for him , and more generally for low power transfer , the behavior of input b connected to output a is usually the behavior of a in the absence of the connection , so b can be regarded as an indeterminate or _ free _ parameter .",
    "thus , what the engineer designs , in our current jargon , is a functional i - o system ; its components are what we called _",
    "open processes_. the functional dependence of outputs on inputs is called a _",
    "transfer function_. the concept of a transfer function clearly matches von wright s concept of causality as the potential for the relationship of means to ends , so it makes sense to refer to functional i - o systems as _",
    "causal systems_.    in a causal system , so defined , the outputs of each component are functions of its inputs , and their composition by assignment of inputs to outputs is functional composition .",
    "however , we now know that , in the case of macroscopic components , the functional dependence of output on input is a large - number effect resting on the statistical behavior of atoms .",
    "a more accurate , and more general , account of the engineer s systems makes the output of a component into a _",
    "function of the input , with the causal transfer function replaced by a transition matrix of conditional probabilities ; these are the i - o systems of chapter 1 .",
    "the composition of transfer  functions \" is no longer functional composition but matrix multiplication , which reduces to functional composition in the special cases where the transition matrices contain only 0 s and 1 s . having carefully noted the distinction between the mathematician s function and the engineer s transfer function , we ll now relax our language a bit and use the word s  cause \" and  causal \" in connection with both .",
    "thus we ll often refer to i - o systems as causal systems and to i - o states as causal states , especially in a context where we are distinguishing these from quantum systems and states , which are essentially a - causal .",
    "a more accurate qualifier would be statistico - causal , but it s too much of a mouthful . since `` classical physics '' is often taken to imply strict determinism , we emphasize that the `` classical '' systems we consider in this paper _ always _ contain stochastic elements .",
    "suppose an input is connected to an output ",
    "how do you know which is which ? to find out you disconnect them ; the output is the one that continues to do by itself what they did together when connected - another operational test of causality . but",
    "suppose you ca nt disconnect them , and suppose also that you ca nt manipulate the inputs to the system ; all you can do is watch .",
    "then how do you identify causality ?",
    "is there nothing we can learn about what causes what by just watching ?",
    "hume s problem is still with us .",
    "here s another take on hume s problem : are there patterns in the joint probability distribution of an observed process that mark the causal relations we would see if we were to experiment with its disconnected parts ? to put it another way , do cause and effect belong to the process itself , or are they artifacts that result from taking the process apart ?    the theory of link systems throws new light on this question .",
    "recall the disconnection theorem , which says that a process can be disconnected or  de - linked \" at x if and only if it is separable at x. this shows us a way to transform a process into a link system : first disconnect x into x1 and x2 , where x2 is  white \" ( corresponding to an input ) , which produces two independent processes , call them a and b. then link a and b by imposing the condition x1=x2 ; this gives us a two - component link system exhibiting our original process .",
    "now do the same for separating variables in a and b etc .",
    "until there are no separation points left .",
    "the result is a system which  factors \" the original process into  prime \" components that can not be further subdivided .",
    "recall that separability belongs to the process itself , i.e. it is intrinsic to the probability distribution .",
    "thus the disconnection theorem reveals severe limitations on how we can construe the correlations in a process as causal .",
    "a highly correlated joint probability distribution need not have any separating variables at all , so causality requires something besides mere correlation in the data itself .",
    "we have at least a partial answer to hume .",
    "we ll see in chapter 3 that the earmarks of causality in the data itself can actually be quite distinctive .",
    "given the temporal order of the events in the process , it turns out that there is a unique prime factorization into components with white inputs .",
    "we learned in chapter 1 , and it will be proved in chapter 3 , that such systems are in natural 1 - 1 correspondence with i - o systems representing the same processes . thus from a knowledge of the given alone ,",
    "i.e. of the statistical behavior of the system without our interference , we can completely describe the i - o components we would get if we took the process apart at its ( intrinsic ) separation boundaries .",
    "the cause and effect relation , which we first located in the transfer functions and interconnections of the parts , is already _ there _ in the process itself !",
    "hume , what do you say to that ?     not so fast , sir ; you were too glib with your prime factorization .",
    "why should the x2 s be white ? and , if you regard what is given to our understanding to be only the probability of certain events , then on what basis do you bring in temporal order ? if you must call upon such invisible help to reveal causality in the bare order of correlation , you do but reinforce my conviction that causes are phantoms of the imagination . \"    good points",
    "let s translate them into the modern idiom .",
    "it will help to focus our ideas if we confine ourselves to processes in which the separating boundaries are all in a line ; these are called markov processes .",
    "their definition can be stated in terms of a three - term relation among events called the markov property , which is defined as follows :    * markov property * : to say that the triple of events ( a , b , c ) have the markov property means that they satisfy the equation p(c",
    "@xmath1 a&b ) = p(c @xmath1 b ) , which we ll refer to as the _ markov equation_.    * markov process * : a process in which the variables are indexed by a time parameter such that , at any time , if a is any event not involving future variables , and b is the state of the present , and c is any event not involving past variables , then a , b and c satisfy the markov equation .",
    "recall that the probability p(x @xmath1 y ) of x conditioned by y is defined as p(x&y)/p(y ) .",
    "bearing this in mind , notice that if we multiply both sides of the markov equation by p(a&b)/p(b ) we get the equation p(a&c @xmath1 b ) = p(a @xmath1 b)p(c @xmath1 b ) .",
    "this relationship among a , b and c is known as _ conditional independence _ ; in words , a is independent of c , given b. recall that in chapter 1 we referred to separability as conditional independence ; this equation defines what we were referring to .",
    "event b separates events a and c means that , given b , the probability of a&c is the product of the probabilities of a and c. it turns out to be more convenient to define separability by a slightly different equation , which is gotten by multiplying both sides of the markov equation by p(a&b)p(b ) .",
    "* separability * : to say that b separates a and c means that p(a&b&c)p(b ) = p(a&b)p(b&c ) .    if none of the joint probabilities of a , b and c are zero , then the markov property , conditional independence , and separability are all logically equivalent .",
    "however , for conditional independence to make sense , p(b ) can not be zero , while for the markov equation to make sense p(a&b ) must also not be zero . since separability always makes sense , whatever the probabilities , it s the most convenient of the three to work with .",
    "such fine points need nt concern us now , however ; the really important point is this :    conditional independence and separability are symmetrical in a and c , which means that the defining property of a markov process is symmetrical in time ! think about that for a minute .",
    "the markov equation seems to have a temporal arrow , since it s about conditioning of the future by the present and past , but we now see that this is an illusion .",
    "our argument for causality being in the process itself rested on a theorem that assumes a given temporal order of events , and in the usual discussions of markov processes , temporal order is taken for granted .",
    "but hume s challenge was to find causality in the bare order of correlation .",
    "we now see that this bare order does not distinguish past from future , so it would seem that mr .",
    "hume is right : cause and effect really are in the eye of the beholder .",
    "kant , who has been listening to this discussion with growing impatience , can restrain himself no longer .",
    "since his style is notoriously obscure , we will freely translate his remarks , with a little help from the eminent historian of philosophy harald hoffding@xcite .    ",
    "we gather that by ` process ' you mean something that is accessible to experience .",
    "now experience , in contrast to mere imagination , is a complex composed of elements some of which are due to the faculty of knowledge itself , while others are the result of the way in which this faculty is determined to activity from without . as i have shown , space and time are forms of our perception . for whatever",
    "the nature of our sensations , and however much they may change , the spatial and temporal relations in which their content is presented to us remain the same ; a space or a time does not change , whatever be its filling out .",
    "it follows that the temporal order of the process is given , since it belongs to the form whereby the outer determinant of our experience of the process can manifest itself in experience as a process .",
    "hume is correct that cause and effect are in the ` eye of the beholder ' , but wrong to imagine that we could see otherwise than as we do .",
    "\"    kant s ambition was to describe once and for all the  forms \" that belong to the  faculty of knowledge \" , leaving the  outer determinants \" of experience in a limbo where they have become known as  things in themselves \" .",
    "today we are more modest . for one thing",
    ", we no longer make his absolute distinction between the subjective and the objective ; rather we see objectivity as a matter of degree . also , instead of trying to directly analyze the faculty of knowledge , we often rest content with characterizing the contribution of the subject to an experience in terms of a  choice of viewpoint \" , and of the object as a particular feature of the experience that is invariant under the choice of viewpoint .",
    "the subjective - objective boundary is of course quite mobile , and depends on the range of possible viewpoints we are considering .",
    "even transposed into a modern setting , kant s observations about time direction are telling .",
    "so far we have been taking the notion of process as simply given .",
    "but we must ask : given to what ? if given merely to imagination , then of course processes can be anything we want them to be , and they can go backwards or forwards or any which way in time .",
    "but if we are empirical scientists , then processes are something given to _ experience _ , at least potentially , which means that in thinking about processes going backwards we must consider whether we could actually observe such a process . and",
    "here we must heartily agree with kant that the answer is no , since experience itself has an inexorable temporal arrow , whatever its content .",
    "indeed the process as something given to experience is given one piece at a time , and the order in which these pieces are given is precisely the temporal order that we find in the process itself .",
    "this being the case , what could it possibly mean that time goes backwards ?    we ve been describing the process itself as what is given , but here s something else to keep in mind : the time reversal of the _ given _ is the _ taken away _ !",
    "let s be more concrete .",
    "suppose we are observing a process and writing down what we observe .",
    "our observations accumulate in a diary which starts with 100 blank pages and progressively fills with writing . now",
    "each letter that we write reduces the nominal possibilities for what the diary _ could _",
    "contain by a factor of 26 ; this is the shannonian sense in which it constitutes information .",
    "thus the process itself proceeds step - by - step alongside of a process of information gathering , which progressively reduces the vast initial range of possible states of the diary to one state , the chronicle of _",
    "what actually happened_.    we decided in chapter 1 that it is not a particular chronology that we will call a process , but rather the conditions and rules that govern it .",
    "let us then imagine an enormous series of repetitions governed by the same conditions and rules , each producing a diary of what actually happened .",
    "we now are given an enormous heap of diaries from whose statistics we abstract the joint probability distribution that we having been calling a stochastic process . the question is whether the stochastic process so abstracted exists as a thing - in itself apart from its potentiality of  determining our faculty of knowledge to activity \" , namely , the activity that produces this heap of diaries .",
    "the kantian answer is no .",
    "the very _ idea _ of process is inseparable from the idea of a progressive accumulation of observed information in a diary .",
    "such a progression has of course a time arrow , that of the progressive narrowing of a range of possibilities .",
    "thus a markov process backwards in time is nonsense . to be given",
    "a process without a time arrow is inconceivable , since the given - ness of a process is conceptually tied to the given - ness of successive stages of its presentation to a potential observer .",
    "it took a lot of fancy footwork , but time and with it causality seem at last to have returned to their familiar shapes .",
    "finally we are back safe in the bosom of animal faith . or are we ?",
    "kant says it s ok to believe what our animal faith tells us , because we ca nt help it such beliefs belong to the very  faculty of knowledge \" . darwin s theory of evolution also suggests that it s ok , but for a somewhat different reason , namely that our faculty of knowledge evolved to enable us to survive in an all- too - real world of hostile and dangerous ",
    "things - in - themselves \" . the two messages , despite their opposite philosophical starting points , are actually not all that far apart .",
    "darwin s message applies an important corrective to kant s , however , which is that in analyzing our faculty of knowledge we must take into account the special conditions that obtained in the environment where that faculty evolved .",
    "thus , the belief that the world is flat is certainly  true enough \" for creatures who never travel more than a few miles from where they are born , but is badly wrong for long - distance navigators .",
    "homo sapiens is perhaps alone among the animals in being able to modify his animal beliefs in response to drastic changes in the environment , including those of his own making .",
    "furthermore , and in this we humans are probably unique , our faculty of knowledge includes a faculty of abstract reasoning that can extrapolate to conditions far beyond our present environment .",
    "sometimes this precedes a big move : think how we used every branch of modern science to prepare ourselves for exploring and colonizing outer space .",
    "kant lived in the age of enlightenment whose idols were euclid and newton , and rather naively incorporated their systems into his  intrinsic \" faculty of knowledge , thus confusing a brief phase of human culture with something final and absolute .",
    "one wonders what he would have said about einstein s theory of relativity .",
    "kant thought it was impossible to experience space as non - euclidean , and yet today we have good reason to believe that certain regions of space are quite non - euclidean , and we know at least some of the ways in which this impinges on experience .",
    "the most drastic departures from euclid involve warps not only in space but in time , producing loops where time closes back on itself . for the inhabitants of such a loop ,",
    "if there be such , the future is also the past !    needless to say , time loops pose a serious problem for our neat kantian explanation of time - direction and causality .",
    "if the future is the past , which way does our experience go ? how can there be a diary today about the future ? in brief , _",
    "what happens _ ?",
    "if what happens is what could be experienced by someone involved in the happening , what would such experiences in a time loop be like ? and what would it mean for experience itself to happen ? before getting further into this quagmire , let s briefly check in with the authorities to see whether time loops are anything more than wild science fiction .",
    "we ll start with stephen hawking .",
    "here is a recent item from the new york sunday times :     october 1 , 1995 . in a u - turn that has sent shock waves through the universe , professor stephen hawking , britain s leading cosmic physicist ,",
    "has accepted the possibility of time travel .",
    "having ridiculed the concept for years , hawking now says that it is not just a possibility but one on which the government should spend money . \"",
    "hawking gives a quick summary of the reasons for his u - turn in the preface to a new book called _ the physics of star trek _ by astronomer lawrence krauss@xcite .",
    "first he addresses the problem of faster - than - light travel , a staple of space opera despite its being forbidden by special relativity , and concludes that  fortunately , einstein s general theory of relativity allows the possibility for a way around this difficulty . \"",
    "he then turns to time travel :  one of the consequences of rapid interstellar travel is that one could also travel backwards in time .",
    "imagine the outcry about the waste of taxpayer s money if it were known that the national science foundation were supporting research on time travel . for that reason , scientists working in this field have to disguise their real interest by using technical terms like ` closed timelike curves ' that are code for time travel .",
    "nevertheless , today s science fiction is often tomorrow s science fact . \"",
    "one of the scientists working in this field is the super - string theorist michio kaku . here are two brief excerpts from his recent book _",
    "hyperspace_@xcite :     in june 1988 , three physicists ( kip thorne and michale morris at the california institute of technology and ulvi yurtsever at the university of michigan ) made the first serious proposal for a time machine .",
    "they convinced the editors of physical review letters , one of the most distinguished publications in the world , that their work merited serious consideration . \" ....     if time travel is possible , then the laws of causality crumble .",
    "in fact , all of history as we know it might collapse as well .",
    "\"    whatever may be the prospects for a time machine , and we personally do nt think they are very good , the conceptual problem posed by time loops in outer space is problem enough in itself . even if we remain comfortably at home in our causal near - euclidean space - time and never venture near a cosmic worm - hole , we still live in the same universe with such aberrations . in the larger picture ,",
    "their time is of a piece with our own , and thus the question still remains : what happens there ?",
    "since this question ca nt be answered by a narrative , what kind of answer should we look for ?    here s another brief quote from kaku .",
    " however , the concentrations of matter - energy necessary to bend time backward are so vast that general relativity breaks down and quantum corrections begin to dominate over relativity.\"@xcite a complementary point is made in an article called  the quantum physics of time travel \" by david deutch and michael lockwood in the scientific american , march 1994@xcite :  quantum mechanics may necessitate the presence of closed time - like curves . these , while hard to find on large scales , may well be plentiful at the submicroscopic scale , where the effects of quantum mechanics predominate . \"",
    "these are very interesting points , since , even without time loops , quantum mechanics on the microscopic scale has always given a lot of trouble to those who ask ",
    "what happens there ? \" .",
    "it seems that we are bracketed by temporal and causal weirdness from both the very small and the very large .",
    "the realm of causality and process , or at any rate due process , appears to be confined to a middle scale , like sentient life itself . but are we so perfectly balanced in this middle that the temporal weirdness at the extremes has no bearing at all on our situation ?",
    "not so long ago we seemed to be bracketed by weirdness of another kind : though pebbles are round and stars and planets are round , the earth we live on is flat .",
    "today we forget how incredibly hard it was for our ancestors to give up their flat earth .",
    "the ancients managed to live precariously on a round earth for almost nine centuries , but by the fourth century ad the earth had become flat again , and flat even for educated people who knew their aristotle , ptolemy and aristarchus .",
    "curiously enough , it was nt flattened by the barbarian invaders , but by learned scholars who were well grounded in alexandrian mechanical technology which they used ( or misused ) to construct ingenious models of the cosmos as a planetarium @xcite .    what drove our educated ancestors to such absurdities ?",
    "biblical faith is the usual suspect , but we think animal faith is the real culprit .",
    "animal faith says that up is up and down is down , and never the twain shall meet .",
    "up and down are kantian - darwinian categories , so deeply entrenched in our psyches , our language and our genes that they pervade all thought , as shown by expressions like getting high , feeling let down , being well grounded , etc . etc .",
    "the trouble with a round earth is not that we ca nt imagine its shape but that we ca nt imagine a universe where unsupported things do nt fall , and falling means going down .",
    "the aristotelian and medieval solution to the problem of falling was to have everything fall towards the center of the universe , which was located in the center of the earth .",
    "but after the copernican revolution , the concept of falling once again became quite mysterious .",
    "which way do things fall when they are well away from earth , and why ?",
    "it was newton who solved this mystery by realizing that falling is not something that happens to objects in isolation but is a relationship between two or more objects . with this insight , falling was subsumed into another and very different concept , that of mutual attraction .",
    "past and future are even more ingrained in our genes , psyches , and language than up and down , so quantum superposition and black holes are even greater heresies for animal faith than a round earth wandering around the sun . today it s not falling that is mysterious but _ happening_. does newton s solution offer us any help in making it less so ?",
    "is it conceivable that we could subsume the concept of happening into some more general concept ?",
    "if one is prepared to accept kaku s reading of the current astronomical evidence , this larger concept ca nt have an absolute temporal arrow anymore than newton s concept could have an absolute arrow of up and down .",
    "the past and future have become poles in a relationship . but",
    "a relationship between what and what ? and described how ?",
    "we ll return to this in a moment .",
    "we agreed with kant that _ happening _ is an empirical concept .",
    "we then simplified the meaning of _ empirical _ into being able to keep a diary of observed events .",
    "let s now concentrate on the concept of a _",
    "diary_.    to keep a diary , you start out with a stack of white paper and write down your experiences in the order in which they occur . as we noted above , each time you write something down ,",
    "you narrow the set of possibilities for what could end up on the paper , i.e. you take away a subset of _ cases_. each item in your diary corresponds to a certain set of cases taken away , so the act of writing produces a sequence -c1 , -c2 , -c3 , ... of such departing case sets .",
    "this account of writing is incomplete , however , since it only shows cases departing , not cases arriving .",
    "our description of your diary began after you had already acquired a stack of paper . and yet , properly speaking , your first act of diary keeping was to acquire that stack of paper , since it is blank paper that _ supplies _ the set of cases from which the members of c1 , c2 , c3 etc . are taken away when you write .",
    "furthermore , this first act may well be repeated , since you may run out of paper before you are done .",
    "thus a more complete description of your diary writing would go like this : + c1 , -c2 , -c3 , + c4 , -c5 etc .",
    ", where a + c is a set of cases added to those at hand when you get a new sheet of paper , and a -c is a set of cases taken away when paper meets pencil",
    ".    now let s address the question : what would it mean for a record of events to be reversed in time ?",
    "what comes to mind is a movie run backward : spilled salt flying from the floor into its shaker , the phoenix rising from its ashes etc .",
    "however , simply running the frames of a movie backwards does not reverse the crucial order of events that creates each frame , namely , light going from object to camera and then leaving its imprint on the blank film .",
    "a backwards movie is only a forward record read backwards ; it s not the record of backwards events .",
    "let s approach our question on the more fundamental level of information and entropy , which is to say , on the level of departing and arriving cases .",
    "once we take this course , its answer is immediate : time reversal turns departing cases into arriving cases and vice versa .",
    "what goes backwards is not the order of movie frames or some other kind of  state descriptions \" but rather the order of the case sets c1 , c2 , c3 etc . , and this is accompanied by a reversal of their signs .",
    "thus our diary above would go + c5 , -c4 , + c3 , + c2 , -c1 .    reversing past and future",
    "turns writing into paper and paper into writing !",
    "time reversal is indeed very simple , but it s also pretty weird .",
    "here s another bit of weirdness .",
    "seen forward in time , you take a piece of paper supplying 100 cases and write something on it that takes away 50 of them .",
    "seen backward , you take a piece of paper supplying 50 cases and take away 100 of them ! what is the meaning of the  remaining \" minus 50 cases ?",
    "it s certainly nothing familiar .",
    "however , this example shows that negative cases , and thus negative probabilities , ca nt be avoided in describing time - reversal , and if the speculative cosmologists and astrophysicists are right about the circular warping of time , we will somehow have to make our peace with them .    in short , the generalized concept of happening we have been looking for , whatever else it may require ,",
    "does require a generalized theory of probability in which we can countenance debts as well as surpluses of cases .",
    "in chapter 1 we mentioned that negative cases play havoc with logic .",
    "let s very briefly see how this happens , since the mathematical  illogic \" of negative cases is actually a useful tool . a more extended discussion of the following points is given in `` boolean geometry '' , where the `` boolean cube '' is pictured @xcite .",
    "define the logic of a set of cases to be the set of all of its subsets .",
    "consider the logic of a set of three cases .",
    "it has eight members , which can be arranged in a natural way as the vertices of a cube , called the _",
    "boolean cube_. if we stand the boolean cube on the vertex representing the null set 0 , the top vertex becomes the universal set 1 containing all three members , i.e. the principle axis containing 0 and 1 is vertical .",
    "this same construction can be carried out with n cases to give an n - dimensional hypercube , which is also called a boolean cube .",
    "ascending an edge of a boolean cube ( or hypercube ) represents adding a case to the lower vertex , where parallel edges always add the same case .",
    "it follows that going down an edge represents taking away a case .",
    "since writing a diary means successively adding and taking away cases , we can represent the course of writing by an edge path that starts with the vertex representing the initial case set .",
    "suppose now that we stand the boolean cube upside down , putting 1 at the bottom and 0 at the top .",
    "each step in the diary edge path then reverses its sign , so this transformation of the cube represents time - reversal .",
    "it also has a logical meaning , which is negation ; more exactly , a self - reflection of the cube in the vertical direction maps each vertex into its negation .    turning the cube upside down is only one of its many geometric symmetries@xcite .",
    "we can classify these according to how far they rotate the 0 - 1 arrow from its normal vertical orientation , the extreme being 180 degrees , as in negation .",
    "the intermediate angles represent partial time reversal .",
    "we can imagine the movable 0 - 1 arrow as a throttle that ranges from full speed ahead in time when it is pointing up to full speed astern when it is pointing down .",
    "if the throttle is exactly halfway between , the diary goes ",
    "sideways \" in time , so - to - speak ( this can only happen in a hypercube with an even number of dimensions . ) among such sideways transformations are square roots of negation , which we ll come back to in a minute .",
    "it was our aim to come up with a concept of diary that is invariant under time reversal .",
    "the most natural interpretation of its invariant structure is a path on the boolean cube , now understood as a purely geometric structure without an intrinsic up - down direction .",
    "this conception relativizes the logic of the diary , since the operations and , or etc .",
    "depend on the setting of the time throttle .",
    "recall in chapter 1 that we noted the weird logic of interference , where a is possible and b is possible but ( a or b ) is impossible .",
    "relativizing logic explains this puzzling situation : the particle that  interferes with itself \" is in an eigenstate of an observable for which or has a different meaning than it does in the frame of the classical apparatus .    back to the square root of negation .",
    "when we base probability theory on boolean geometry , this transformation gives us imaginary probabilities , just as negation gives us negative probabilities ; link theory turns these into the imaginary and negative amplitudes of quantum mechanics .",
    "let us now go out on a limb and conjecture that the direction  sideways in time \" is a direction in space .",
    "this could mean that the square root of negation is also behind the i of minkowski space ( see section 3.8 for more on this topic . )",
    "if so , it could be crucial for unifying our theories of space and matter .",
    "let us go even further out on that limb .",
    "first , we ll call on pauli for a bit of support . towards the end of his life pauli",
    "had a mystical vision of the quantum i as the key to unifying physics with psychology @xcite .",
    "it turns out that complex quantum mechanics is mathematically equivalent to a real quantum mechanics ( see section 3.6 ) in which every object contains a particular two - state object , which has been called the janus particle @xcite , whose state is unmeasurable . te s conjecture",
    "is that choosing between the two states of the janus particle corresponds to choosing between the heisenberg and schrodinger representations .",
    "the fact that the absolute phase of amplitude is unobservable then means that quantum matter in itself ( i.e. unmeasured ) is not only symmetrical with respect to time direction but also with respect to _ subject _ and _ object_. the differentiation of the world into particular pieces of matter , particular minds , and particular space warps can thus be understood as three different manifestations of the breaking of the primal symmetry represented by i.    as we said before , logic is really out of bounds for this paper .",
    "the last few paragraphs must have seemed pretty cryptic , but we wanted to give a bit of the flavor of things to come .",
    "we ll return to such speculations in the last section of chapter 3 , but until then our business will be with matters closer to home .",
    "time reversal does more than just change the sign of the events in our diary writing ",
    "it also reverses their order .",
    "now it s reasonable to formalize the process of registering and recording information as an i - o system . as we saw , the laws of the underlying markov process are symmetrical in time , which means that we could also represent it by an i - o system going backwards in time .",
    "but what sort of system would represent  sideways \" time ?",
    "here we must turn from i - o systems to link systems .",
    "there turns out to be a whole spectrum of these ranging from full speed ahead to full speed astern , and it is those exactly half - way between that give quantum states ( see section 3.6 ) . note that the  state throttle \" here is very different from the  phase throttle \" that rotates the boolean cube , and indeed it need not even involve negative probabilities .",
    "we asked above whether _ happening _ might resemble _ falling _ in being a special case of a more general situation analogous to motion in a gravitational field . as we ll see , the mathematics of the state throttle supports this analogy .",
    "we should speak of rising , however , rather than falling , in the sense that events _ arise _ from the  ground \" of past conditions . with the state throttle in full reverse ,",
    "events arise backward in time from the ground of future conditions . in the more general case ,",
    "events are  repelled \" by both past and future , like helium balloons between two massive bodies . at half throttle , when the balloons are exactly balanced in between , we get quantum mechanics .",
    "we ve come a long way from our starting place , which was to respond to hume s challenge to find causality in the  bare order of correlation \" . in effect",
    ", hume s problem effectively disappeared , giving way to others more puzzling and serious .",
    "the problem with hume s problem is that it assumes we are simply given a world where things happen .",
    "this is indeed true in everyday life , but when we start to think about what happens in time loops or atomic nuclei , we find that it s the concept of happening itself that is the problem .",
    "causality , time direction , and history all seem to be bound up together in a way that makes it hard to say which is the more fundamental .",
    "this tightly knit complex of ideas is a familiar one , and we have made some progress in analyzing it .",
    "but we have also discovered that it does nt transfer very well to either the domain of the very large or of the very small . to make sense of modern physics",
    "seems to require that we give up not only causality and history but even logic .",
    "the next chapter will lay out in a more systematic way the kind of mathematical theory that must take their place .",
    "the main goal of this paper is to show that basic quantum mechanics can best be understood as a branch of the theory of probability . for this to happen",
    ", probability theory must be expanded in two ways : first and foremost , probabilities must be allowed to go negative .",
    "second , and this is more a matter of technique than a change in our assumptions , the composition of transition matrices must be subsumed into a more general method of composition based on the linking of variables .",
    "we ll see that _ in this expanded theory _ , probability amplitude is not some new kind of  fluid \" or physical field , but simply probability itself . by regarding it as such , we turn some of the most striking basic discoveries of modern physics into pure mathematics , thereby freeing them from their purely physical context to become tools available to every branch of science .",
    "just how much more of physics as we know it will follow in this course remains to be seen .",
    "_ event , sample space , case , variable , range , joint variable , logic , atom , probability measure , probability distribution , random variable , summation theorem , marginal , stochastic process , independence , conditional probability , conditional independence , separability , markov property , markov process .",
    "_    * event*. following feller@xcite we ll take _ event _ to be our basic undefined concept .",
    "we ll use the word in pretty much the ordinary sense . like most english nouns",
    ", it can be either definite or indefinite .",
    "* sample space , case * : a _ sample space _ is a set of mutually exclusive possibilities called _",
    "cases_. we say that an event e _ belongs _ to a sample space l if for every case of l , it makes sense to ask whether e is true or false .",
    "another word for event is _",
    "proposition_. examples : the event _ heads _ belongs to the sample space whose cases are the possibilities heads and tails . the event _ seven _ belongs to the sample space whose cases are the 36 possible throws of a pair of dice .",
    "* variable * : the term will be used in the scientist s sense , meaning something that can vary , rather than in the logician s sense of a place - holder for a term .",
    "a variable x is said to belong to a sample space l if for any value k of x , the event x = k belongs to l. for example , let l be the set of all cases for ten coin tosses , and let x be the number of heads ; then x=1 , x=2 , x=3 ... x=10 , are events in l and x is a variable in l. more abstractly , a variable is simply a function on l ; when we regard x in this way , the proper notation is x(l ) .",
    "the set of all possible values of a variable is called its range .    *",
    "joint variable x @xmath4 y * : though variables are often numerical , we ll place no restrictions on what kind of things their values can be . for instance , a variable can range over all of the joint values of several other variables .",
    "the notation x @xmath4 y will refer to a variable that ranges over all joint values of x and y.    there is an ambiguity in the term  joint value \" , which is whether or not it involves the order of things joined .",
    "is x @xmath4 y the same variable as y @xmath4 x ? and is ( x @xmath4 y ) @xmath4 z the same variable as x @xmath4 ( y @xmath4 z )",
    "? actually , there are two concepts of joining , which in everyday language we would render as  x and y \" and  x , then y \" .",
    "the first and simplest , which is unstructured joining , is commutative and associative , so we can write x @xmath4 y @xmath4 z @xmath4 ... etc ... without parentheses and with the variables in any order .",
    "the second , which is the one we ll use here , takes joint values to be _ ordered _ n - tuples ; it has the decisive advantage over the first that variables of the same type are in natural 1 - 1 correspondence . there",
    "s actually a third concept called _ list structured _ joining that is very useful in constructing complicated link diagrams , but it belongs to a more advanced and specialized chapter of our theory .    * logic , 1 and 0 , generating * : events can be combined with and , or and not , and a set of events closed under these operations will be called a _ logic _ ( this use of the word ` logic ' comes from von neumann ) . we ll abbreviate and by `` & '' and not by `` @xmath5 '' .",
    "a logic is of course a boolean algebra .",
    "null _ element of a logic , abbreviated 0 , is the element a & @xmath5a , which is the same for any a. the _ universal _ element , abbreviated 1 , is the element @xmath50 = ( a or @xmath5a ) .",
    "given any set k of events , the set of all events that result from applying and , or and not to the members of k is a logic , and it will be called the logic _ generated _ by k. given a variable x , the logic generated by all events of the form x = k , where k is a value of x , is called the logic of x , and the members of that logic are called events in x. more generally , the logic of x , y , z ... is the logic generated by all events of the form x = i , y = j , z = k etc . , and",
    "the members of that logic are called events _ in _ x , y , z etc ..    * atom*. the atoms of a logic are the cases of that logic .",
    "to put it another way , an _ atom _ of a logic is defined as an event that can not be further refined by any other event of the logic , i.e. a is an atom if , for any e in the logic , either a&e = a or a&e = 0 .",
    "atoms are mutually exclusive possibilities , and the set of all atoms is exhaustive within the logic . in the logic of the variable x ,",
    "the atoms are the events of the form x = k for all k , while in the logic of x , y , z ... the atoms are the events of the form ( x = i)&(y = j)&(z = k ) ... for all i , j , k .... if the number of atoms is finite , then the subsets of atoms uniquely correspond to the events in the logic .",
    "the cases of the sample space are the atoms of its logic , so atom and case are basically synonymous terms . however , we ll generally reserve the word ` case ' for atoms that we count , such as the atoms of the sample space .",
    "* probability*. underlying all our more sophisticated ideas about probability is pascal s classic definition of the probability p(e ) of an event e as the number of cases favorable to e divided by the total number of cases .",
    "if the cases of a sample space can be regarded as equally probable , then this definition defines a so - called _ probability measure _ on the events of that sample space , satisfying the following three axioms :          for now we are dealing with _ classical _ probabilities , which are never negative . for these we can always imagine an underlying sample space with equiprobable cases that define p(e ) according to pascal s definition . in dealing with negative probabilities ,",
    "the notion of an underlying case set must be generalized so that it can contain both positive and negative cases . (",
    "the notion of negative set membership has been carefully investigated by mathematicians , and blizard has shown that the zermello - fraenkel axioms can be generalized to allow for it @xcite . ) in this more general form , the case set has a very important role to play in link theory , since it enables us to distinguish among several quite different conceptions of independence ; the details here will be found in future papers .",
    "* probability distribution on x * : a function defined on the range of x which for each k in that range has the value p(x = k ) .",
    "the standard notation for this function is p(x ) , which can be very confusing , since it conflates the scientist s and logician s two very different notions of variable .",
    "however , it s such a well - established tradition that we ll have to live with it ; when you get confused , just remember to replace x by something that looks like a sentence .",
    "* stochastic , or random variable * : a variable on which is given a probability distribution .",
    "the qualifiers ` stochastic ' and ` random ' are only needed when a variable must be distinguished from variables of other kinds , like free variables . since our results have nothing to do with randomness",
    ", stochastic is the preferred qualifier .",
    "* summation theorem and marginals * : the summation theorem says that given a joint probability distribution on variables x and y , we can find the probability distribution on x alone by summing over all values of y ; i.e .. p(x = i ) = @xmath7p((x = i)&(y = j ) ) , or , in our abbreviated notation , p(x ) = @xmath8p(x , y ) .",
    "this is a theorem we ll use repeatedly .",
    "a corollary is that a joint probability distribution uniquely defines the joint probability distribution on any subset of its variables ; such partial joint probability distributions are called _ marginals_.    * stochastic process*. define a _ stochastic process _ , or _",
    "process _ for short , as a set of variables on which is given a joint probability distribution .",
    "in the usual definition it is also assumed that these _ stochastic variables _ involve events which can be ordered in time .",
    "this wo nt do for our present purposes , though , since , we need a new concept of process general enough to apply to time loops . we can start out by thinking of process variables as having at least approximate locations in the space - time of current physics .",
    "however , since we aim to eventually construct space - time out of probabilities , this is only a temporary expedient .",
    "* independence*. events e , f , g , ... are called independent if p(e&f&g ... ) = p(e)p(f)p(g ) .... note that for a set of events to be independent , it s not enough that every pair of events in that set be independent ; independence is a joint condition of all the events .",
    "two variables are called independent if events in the first are independent of events in the second .",
    "a set of variables are called independent if events in different variables are independent .",
    "this definition pretty much captures our intuitive concept of independence as lack of correlation , although there is one thing about it that is not so intuitive : suppose that either p(e)=1 or p(e)=0 .",
    "then for any f , p(e&f ) is either p(f ) or 0 ; in either case it is p(e)p(f ) .",
    "such a  definite \" e is independent of all events , including itself !",
    "this oddity will turn out to be important for the concept of probability space ( see section 3.8 ) .    given a finite sample space of equiprobable cases",
    ", there is another way to define the independence of e and f , which is that the sample space can be arranged as a rectangular set in which the cases of e are a vertical stripe and the cases of f are a horizontal stripe . for ordinary probability theory",
    "in which cases always count positively , the two definitions are equivalent .",
    "however , with both negative and positive cases , the two diverge , since p(e ) can be 0 even though its cases set is not ",
    "rectangular \" with respect to that of f. it turns out that there is even a third definition of independence which applies to imaginary and complex probabilities ( see section 3.6 ) when these are defined in terms of real probabilities .",
    "the above product rule is the definition we ll adopt here , however .      since a conditional probability results from dividing by p(f ) ,",
    "it is only well - defined if p(f ) is not 0 .",
    "negative probabilities are constantly producing 0 s in weird places , so it s best to avoid conditional probabilities if there s an alternative ; fortunately , there usually is .    *",
    "condition * : an event c that is assumed to be true , thereby reducing the sample spaces to the set of those cases favorable to it .",
    "axioms for a process are often given as conditions on a  free \" process whose variables are independent .",
    "a very important kind of condition for our present work is the _ link _ , which is an event of the form x = y .",
    "the probability of e in the reduced sample space conditioned by c is p(e @xmath1 c ) in the unconditioned sample space .    *",
    "conditional independence * : events e and f are called _ conditionally independent _ given c if they are independent in the sample space conditioned by c. the usual algebraic formulation of this relationship is p(e&f @xmath1 c ) = p(e @xmath1 c)p(f @xmath1 c ) . more generally , a set of events e1 , e2 , e3 ... is conditionally independent given c if p(e1&e2&e3 ...",
    "@xmath1 c ) = p(e1@xmath1c)p(e2@xmath1c)p(e3@xmath1c ) ...    as we noted in chapter 2 , these equations becomes meaningless when the probability of the condition is 0 .",
    "the following definition is equivalent to conditional independence when p(b ) is not 0 , but also makes sense when it is 0 :    * separability*. events a and c are called separable by event b if p(a&b&c)p(b ) = p(a&b)p(b&c ) .",
    "this definition comes from noting that it can be obtained by multiplying the conditional independence condition , p(a & c @xmath1 b)= p(a @xmath1 b)p(c @xmath1 b ) , on both sides by p(b)@xmath9 .",
    "the equation defining the separability of n events by b is derived by multiplying both sides of the appropriate equation for conditional independence by p(b)@xmath10 .",
    "we say that variable y separates variables x and z if p(x , y , z)p(y ) = p(x , y)p(y , z ) . which is an elliptical way of saying that if for any values i , j , and k of variables x , y and z we have p((x = i)&(y = j)&(z = k))p(y = j ) = p((x = i)&(y = j))p((y = j)&(z = k ) ) .",
    "much of our work will be with markov processes .",
    "feller@xcite begins his chapter on markov processes with the concept of markov chain , which he defines as a series of trials in which the probabilities at each trial depend on the outcome of the previous trial , the rule of dependence being the same for all trials .",
    "he then goes on to say that a markov chain is characterized by a matrix of conditional probabilities , called the _ transition matrix _ , together with an initial  state vector \" giving the probability distribution at the initial trial .",
    "there are problems with this definition .",
    "part of what motivates it is the need to separate out the _ law _ of the process , given by the transition matrix , from the _ boundary condition _ on the process given by the initial state vector .",
    "and yet there is no reason why the initial vector should not contain 0 s , for which conditional probabilities are meaningless .",
    "thus the markovian law of the process , which makes perfect intuitive sense whatever the initial state , can not properly be described by a matrix of conditional probabilities .",
    "the so - called transition matrix must be defined in some other way .",
    "the remedy for this and related confusions is one we have discussed at length in chapters 1 and 2 ; this remedy is to clearly distinguish between the process itself and its representation as a _",
    "system_. feller began by describing a process and then switched over to describing it as a system , without noticing the change .",
    "section 3.6 will be devoted to markov systems , so here we ll define only the process .",
    "first a more general concept , which we ve already met in chapter 2 :    * markov property * : three events a , b and c are said to have the markov property if p(c @xmath1 a&b ) = p(c @xmath1 b ) . multiplying both sides by p(a&b)p(b ) , we see that this is simply separation , i.e. it says that b separates a from c      * markov transition matrix * : abstractly , a matrix in which the sum of every column is 1 ( just to confuse matters , such matrices are sometimes called _",
    "stochastic matrices_. ) more concretely , the matrix of conditional probabilities from variable x(t ) to variable x(t+1 ) in a markov process .",
    "* primary variables * : in section 3.1 , a process was defined as a set of random variables on which we are given a joint probability distribution ; these variables will be called the _ primary _ variables of the process .",
    "we will assume that the primary variables are given in a certain linear order , which may or may not be related to the probability distribution .    * secondary variables * : a secondary variable is a joint variable in a set of primary variables , called its members .",
    "we will assume that the members always occur in their natural order , so there is one and only one secondary variable for every subset of primary variables .",
    "we ll speak of secondary variables as also belonging to the process .",
    "both primary and secondary variables will be denoted by small letters .",
    "* global variable * : the secondary variable whose membership includes all primary variables . the process itself will be denoted by capitalizing the global variable , e.g. , w is the process whose global variable is w.    * independent part , component * : a sub - process u of w is called an independent part of w , or a _ component _ of w , if the primary variables of u are independent of all other primary variables of w. the primary variables of u are also called its _",
    "members_.        one often hears that the whole is more than the sum of its parts .",
    "what is generally meant is that the whole is more than the mere heap of its parts , which is true unless the whole happens to be a heap of parts .",
    "a process is indeed the heap of its prime parts , which are in fact components , in the sense that the description of each does not require that we take into account the individual peculiarities of the others .",
    "however , we saw that one of the surprises of negative and imaginary probability theory is that there are several quite different definitions of independence , and thus the members of a heap share , as it were , the  nationality \" of their heap , which involves the particular kind of independence that its members have from each other .",
    "step 1 : apply the condition x = y to w to obtain a new _ conditioned _ process w with the same variables as w but with a new joint probability distribution w in which the probability p(e ) of an event e is p(e@xmath1x = y ) in w.      we can think of a link as an operator l(w , x , y ) on w , x and y that produces a new process w , in which the probability of an event e is its probability in w conditioned by the event x = y , and in which the variable y is dropped .",
    "we ll write w@xmath1(x = y ) for this operation , i.e. w = w@xmath1(x = y ) ; more generally , w@xmath1lmn will mean applying links l , m and n to process w.    there is nothing in the definition of link that says x and y must be different variables ; w@xmath1(x = x ) is a perfectly legitimate operator .",
    "what , then , does it do ?",
    "it does nothing to the probabilities , since the condition x = x is already in effect .",
    "it does , however , drop the second variable , namely x. to link a variable to itself simply means to drop it from w.    x and y can be either primary or secondary variables . if they are secondary , however , they must contain corresponding members that are also implicitly linked .",
    "thus if x = x1&x2 then y must be of the form y1&y2 , and the condition x = y implies also that x1 = y1 and x2 = y2 .",
    "this means that when y is dropped , we must also drop all of its primary members , and with them all other secondary variables containing any of these members , which are no longer needed , since they are duplicated by secondary variables with corresponding members in x. dropping a variable is a bit more complicated than the name suggests .",
    "the converse of linking could be called de - linking , but the term does nt really fit , since it implies that there are two things to be de - linked , whereas in fact there is only one . to restore the original w",
    ", we must first turn x in two equal variables , and then remove the condition that equates the two .",
    "we ll call this operation :                    * density matrix * : the joint probabilities on x and y arranged in a matrix with x as the horizontal index and y as the vertical . following _ one _ quantum tradition , we ll use the terms ` state ' and ` density matrix ' interchangeably .",
    "* state of a variable x * : the density matrix of the link x = x .",
    "notice that the off - diagonal elements of this matrix are all 0 , since it s impossible for its two indices to differ , and impossibility has probability 0 !",
    "a more rigorous definition of the state of a variable is the diagonal matrix whose indices range over the range of that variable , and whose diagonal is the probability distribution on that variable .",
    "for instance , if x ranges from 1 to 3 , then its state is the 3 by 3 matrix m@xmath0 such that m@xmath11 is the probability that x=1 , m@xmath12 the probability that x=2 , m@xmath13 the probability that x=3 , and all other entries are 0 .",
    "we ll find that the state of x encompasses all of the classical meanings of the word ` state ' , including markov states and deterministic states like those of a computer .",
    "a markov state is usually represented by a vector ; this traditional vector can now be defined as the diagonal of the state matrix of the time - dependent random variable of the process ( we ll see the details of this later ) .        * proposition * : literally , a synonym for event .",
    "( we are borrowing the term from von neumann , who applied it to quantum variables . )",
    "there is a difference in connotation , though .",
    "events are often described by noun phrases , whereas one always represents propositions by sentences .",
    "we can think of propositions as sentences in the predicate calculus whose free variables are assigned to process variables .    * predicate * : a  propositional function \" whose free variables are unassigned . a predicate becomes a proposition when",
    "all of its variables are either assigned or quantified .",
    "a link , for instance , is a proposition resulting from the assignment of the equality predicate to a pair of variables .",
    "* projection * : in linear algebra , a projection p is defined as a linear operator satisfying the idempotent law pp = p. it can be shown that we can always find a basis in which such a p is represented by a diagonal bit matrix .",
    "for now we ll confine ourselves to diagonal projections , i.e. a _ projection _ will be defined as a matrix in which all off - diagonal elements are 0 and the diagonal elements are either 0 or 1 .",
    "von neumann showed that propositions about the measurement of a quantum observable x can be represented by projections that commute with the eigenvalues of that observable .",
    "this led him to an elegant generalization of the born probability rule : if p is a projection representing a certain proposition , and d is a density matrix representing a quantum state , then , given d , the born probability of that proposition is tr(pd ) .    * representing predicates by projections * : if p(x ) is a proposition about x , define the _ characteristic function _ of p(x ) to be the bit - valued function b(x ) which is 1 or 0 corresponding to whether p(x ) is true or false for x. the diagonal matrix p having b(x ) as its diagonal is the projection that we ll use to represent the proposition p(x ) .",
    "we ll normally use the same letter p for both the predicate and the projection , and when the predicate is assigned , for the resulting proposition ; it s usually clear from the context which of these things we mean .",
    "if we multiply the projection p by the state matrix d of x , we clearly knock out all cases for which the predicate p is false , so the sum of the remaining entries in d is the probability of proposition p , i.e. p(p ) = tr(pd ) .",
    "this makes born s probability rule start to look a bit less mysterious .",
    "however , what really demystifies born s rule is that it holds not just for the state of a variable but for any link state .",
    "more exactly :      proof : first note that the state s of x in w@xmath1x = y is s with all off - diagonal elements set to 0 , normalized by dividing by p(x = y ) .",
    "since p is diagonal , ps is the result of multiplying the rows of s by corresponding diagonal elements of p. thus the diagonal elements of s are simply those of s divided by p(x = y ) = tr(s ) .",
    "since we just saw that p(p ) = tr(ps ) , we conclude that p(p ) = tr(ps ) .",
    "our result differs from von neumann s by the factor tr(s ) .",
    "had we defined a link state as s / tr(s ) , thus making tr(s ) always 1 , we would have gotten von neumann s result exactly .",
    "this was our original definition of state , and it works fines for classical and quantum states .",
    "however , we would like our definition of state to still make sense even as we push our theory into new domains where , because of the possible cancelation of plus and minus cases , the trace of s can be 0 . for this reason , we believe that our present unnormalized state is preferable in the long run .",
    "it follows directly from the definition of independence that the density matrix of a pure state has the form @xmath1v@xmath3u@xmath1 in dirac notation .",
    "if u is white , the state will be called _",
    "causal_. on the other hand , if u = v , the state is quantum . causal and quantum overlap if u and v are both white . , but in no other case .",
    "proof : to say that w is white means that for any i , p(w = i ) = 1/n .",
    "let s be the state of link w@xmath1x = w . since s is pure , p((x = i)&(w = i ) ) = p(x = i)p(w = i ) = ( 1/n)p(x = i ) . but",
    "p((x = i)&(w = i ) ) is s@xmath14 , the ith diagonal term of s. thus tr(s )",
    "= @xmath15 s@xmath14 = ( 1/n)@xmath15 p(x = i ) = 1/n .    *",
    "white connection theorem * : roughly : pure causal links do nt affect the component on the  output \" side .",
    "more exactly , if x is in a component a that is independent of y , and y is white , then the link w@xmath1x = y does nt change the joint probability distribution on a.    proof : let z be the join of the variables of a other than x , i.e. a = x&z . then p(x , z , y ) = p(x , z)p(y ) = ( 1/n)p(x , z ) in w. by the above trace theorem , p(x = y ) = 1/n .",
    "thus p(x , z , y@xmath1x = y ) , which , ignoring y , is the joint probability distribution on x and z in w@xmath1x = y , is equal to ( 1/n)p(x , z)/p(x = y ) = p(x , z ) , which is the joint probability distribution on x and z in w. qed .",
    "this theorem is the key to mapping the engineer s input - output systems into link theory .",
    "remember , we asked the question : when an input is wired to an output , how do you know which is which ?",
    "the answer is : cut the wire and see which loose end is unaffected by the cut .",
    "this answer can be amplified a bit .",
    "if the wire is from box a to box b which is otherwise unconnected to a , then cutting it not only leaves the output of a unaffected , but all of a unaffected .",
    "this is essentially the content of the white connection theorem , if we regard the causal link as a connecting wire .",
    "the word  causal \" means what it says !",
    "* mixed states * : needless to say , pure states play a very important role in our theory . in quantum mechanics ,",
    "impure states are called _ mixed _ states ; this is because they can always be written as linear combinations of pure states .",
    "this turns out to be true quite generally of link states , so we ll borrow the quantum term _",
    "mixed state _ for any state that is not pure .",
    "let us point out , though , that the decomposition into pure states of a mixed classical state , such as one might find in a computer program , is generally quite artificial , since the pure components will seldom be classical and will often involve negative probabilities .",
    "_ disconnection , fundamental theorem of disconnection , link systems , proper systems , predicates as components , relational composition , abstract links and their states , contraction , product theorem , tensors , public variables , systems , transition matrices , causal boxes and systems .",
    "_        * fundamental theorem of disconnection * : let w be the join of three variables x , y and z , i.e. w = x&y&z , , where p(y ) is not 0 for any y. then x can be disconnected from z at y if and only if y separates x and z. ( recall that separation means p(x , y , z)p(y ) = p(x , y)p(y , z ) . )      suppose there exists a disconnection of w that produces a new process w. let y be the new variable of w , i.e. w = w@xmath1(y = y ) . to avoid confusion ,",
    "let s use p for probabilities in w and p for probabilities in w. then in w we have p(x , y , y,z ) = p(x , y)p(y,z ) .",
    "this implies that , in w , x is independent of z for any values of y and y. in particular , x is independent of z if y = k and y=k .",
    "since this is true for any k , it is consistent with the linking condition y = y , which is what turns w into w. this shows that y separates x and z in w. notice that going from disconnection to separation does nt require the assumption that p(y ) is never 0 .",
    "going backwards is a bit harder .",
    "suppose y separates x and z in w. we need to construct a new w containing x , y and z plus a new variable y such that x&y is independent of y&z in w , and such that w = w@xmath1(y = y ) .",
    "separability in w means that p(x , y , z)p(y ) = p(x , y)p(y , z ) .",
    "let w be a process consisting of two independent parts w1 and w2 , where w1 contains x and y , and w2 contains y and z. let p(x , y ) = p(x , y ) .",
    "we construct p(y,z ) as p(z@xmath1 y)/n where n is the number of cases of z for which p(z@xmath1 y ) is non - null .",
    "then p(z@xmath1 y ) = p(z&y)/p(y ) where p(y ) = @xmath16 p(z@xmath1 y ) .",
    "note that this is where the requirement p(y ) @xmath17 0 is needed.w      * lemma 1 * : p(x , y , z ) = p(x , y)p(z@xmath1y ) .",
    "proof : because p(y ) is never 0 , we can freely speak of probabilities conditioned by y. thus we can write p(x , y , z ) as p(x , z@xmath1y)p(y ) . we can also write separability as conditional independence , i.e. p(x , z@xmath1y ) = p(x@xmath1y)p(z@xmath1y ) . combining the two we get : p(x , y , z ) = p(x , y)p(y)p(z , y ) = p(x , y)p(z@xmath1y ) .",
    "qed    * lemma 2*. the joint probability distribution on x , y , y and z in w is p(x , y)p(z@xmath1y)/n .",
    "proof : by the independence of w1 and w2 , we have p(x , y , y,z ) = p(x , y)p(y,z ) . since p(x , y )",
    "was defined as p(x , y ) this becomes p(x , y)p(y,z ) .",
    "= p(x , y)p(z@xmath1y)/n , where n is the number of rows in the transition matrix p(y@xmath1z ) .",
    "qed    now let s link y and y. this involves two steps : first we identify the variables y and y , and then we normalize the resulting distribution by dividing by p(x = y ) .",
    "we see by lemma 2 that the first step turns the joint distribution on w into p(x , y)p(z@xmath1y)/n .",
    "now p(x = y ) is the trace of the state matrix of the link , which we can easily see is 1/n ( the steps showing this are spelled out in the proof of the causal link chain theorem in the next section . ) thus dividing by p(y = y ) yields p(x , y)p(z@xmath1y ) , which by lemma 1 is p(x , y , z ) .",
    "thus p(x , y , z ) = p(x , y , z ) .",
    "the above disconnection theorem is what tells us how to mark a process for dissection , so - to - speak .",
    "first , we find a variable x , primary or secondary , which separates the process into two conditionally independent parts .",
    "we then disconnect the process at x into two unconditionally independent parts , and then repeat for these two parts etc .",
    "until we can repeat no more , at which point we have arrived at a prime factorization .",
    "these prime factors , as such , constitute a mere heap of independent parts .",
    "the disconnection procedure , however , leaves a record of past connections , a kind of wiring diagram , that can be used to reconstitute the original process .",
    "the question arises whether such a prime factorization is unique .",
    "there are actually two questions here : first , whether the factors themselves are unique as processes , to which the answer is definitely no ( except in the trivial case where the process has no factors except itself ) and second , whether the prime  boundaries \" are unique , or to put it another way , whether every factoring procedure ends up with the same sets of primary variables in its factors .",
    "the second question has no simple yes or no answer .",
    "often it is yes , but there are important cases ( epr , for instance ) where a seemingly arbitrary choice in making the first cut will determine whether or not any further cuts are even possible .          proof : divide the linked variables into two sets x and y , where x contains the first variable of each link , and y contains the second .",
    "let x be the join of variables in x , y the join of variables in y. then the link x = y clearly encompasses the links of all the members of x and y , whose order is immaterial as long as they are properly matched .",
    "the linking order can indeed make a difference if the linked pairs have variables in common .",
    "if you link y to z and then x to y , all three of x , y and z will be tied together and called x. however , if you first link x to y , y disappears , so you when then you try to link y to z , there is no process variable to link .",
    "this sort of thing can not happen in a proper system , of course .    but suppose you need to link all three of x , y and z together ?",
    "indeed , this is exactly what an engineer does all the time when he  branches \" an output to go to several inputs .",
    "must the engineer then work with an improper system ?",
    "the answer is no .",
    "there are in fact several ways to branch properly .",
    "one rather interesting way is to extend link theory by treating predicates as  abstract \" components which , when linked to random variables , become conditions on the process . thus to connect x , y and z",
    ", we introduce the abstract component x=y=z plus the three proper links .",
    "the resulting more general theory merges stochastic composition with the predicate calculus , and may be of mathematical interest .",
    "however , much of what it accomplishes can be done in other ways ; for instance , we can regard x=y=z as a stochastic process with three equal white variables , and the links x = x , y = y and z = z also connect x , y and z. at any rate , abstract components wo nt concern us here . with one exception ,",
    "that is ; links themselves should definitely be regarded as abstract components .",
    "but they are the only ones we need for now .    not having to worry about",
    "the order and grouping of links is such a plus that we ll assume that all our systems are proper . under this assumption",
    ", there is a very readable and natural diagrammatic notation for link systems , which uses icons to represent component processes , and arrows between them going from x s to y s to represent links . indeed ,",
    "if we draw and label the icons right , we can often avoid variable names altogether , since the variables will be identified by their places .",
    "unfortunately , the practicalities of word processing , e - mail , etc . make it expedient to have a linear backup notation .",
    "boxes are written thus : a[y1,y2 , .. ; x1 , x2 , .. ] , where the variables before the semicolon are the ingoing arrows , those after , the outgoing .",
    "a link is shown by an equation , so a(y ; x ) ( x = x ) b(x ; z ) links x in a to x in b. if there is no need to reconstruct the particular lost variables in w , we can omit the equation and simply use the same letter in a and b , e.g. a(y ; x ) b(x ; z ) .",
    "needless to say , diagrams are much easier to understand , and it s often best to turn this linear notation into actual diagrams rather than trying to decipher it as it is",
    ". there are more powerful diagrammatic notations than this for dealing with complex structures , but they are nt needed for the simple examples we ll be studying here .",
    "it s worth introducing one new concept from this extended notation , though :    * box * : an icon that represents a component of a system , but that may also be a container , implicit or explicit , for a more detailed system . from the outside",
    "a box is to be regarded simply as a process , but it s permissible to look inside it for smaller components ( think of the transistors inside an integrated circuit or the electrons inside an atom . ) in the next two sections we ll show all component icons as boxes , and sometimes refer to them as such",
    ".    * transformation * : a two - variable sub - process t of a link system in which both variables are linked to others .",
    "if the variables of t are x and y , in that order , and if x = x is the link to x and y = y is the link to y , then the link state ( density matrix ) of x = x is called the state _ before _",
    "t , while the link state of y = y is called the state _ after _",
    "t. t is called _ causal _ if x is white .",
    "* transformation product theorem*. roughly , to combine two successive transformations , multiply their matrices .",
    "more exactly , if t and u are transformations with variables x,y and y,z , then linking y and y and then ignoring y produces the transformation nut , where n is a normalizer .      * corollary * : equivalent cut theorem .",
    "suppose that cut c1 in w produces the process w1 with variables x and y. define w2 as the process that results from multiplying x by diagonal matrix d and y by d@xmath2 .",
    "then the cut w,w2 is equivalent to the cut w,w1 .          _",
    "the rule of law , boundary conditions , markov chains , generators , time - reversed markov chains , link chains , the link dynamical rule , differential markov chains , inverse markov chains , double boundary conditions , chains , sharp states , schrodinger s equation , complex amplitudes , the phase particle , the janus particle .",
    "_    the ancient and medieval worlds observed nature as carefully as we do , hoping as we do to discover what remains fixed in the midst of change . but science , as we know it , began when our focus shifted from changeless _",
    "things _ to the changeless laws of _ change itself_. the goal of  natural philosophy \" then became to find those universal ",
    "laws of causality \" that determine the future from the present .",
    "the amazing success of newtonian mechanics convinced many people that this goal had actually been attained , and so went the scientific consensus for 200 years . but then came quantum mechanics .",
    "what most shocked people about quantum mechanics was its seeming break with newtonian determinism .",
    "chance , seemingly exiled to the limbo of ancient superstition , had , like napoleon returning from elba , once again forced itself upon the civilized world . could chaos be far behind ?",
    "the ideal goal of scientific theory had to retrench from predicting certainties to predicting probabilities .",
    "our ultimate ",
    "law of causality \" was an illusion ; the best we could hope for was to find the ultimate rule giving the best _ probability distribution _ on the state variable x(t+1 ) of the world at time t+1 as a function of the state variable x(t ) of the world at time t. which brings us to _ markov chains_.    a markov chain is a markov process whose transition matrix is the same at all times t ( see section 3.1 for the definition of a markov transition matrix . )",
    "call this unchanging matrix g the generator of the process . given g plus the state at time 0",
    ", we can calculate p(x(1 ) ) , p(x(2 ) ) .. etc .",
    ", and also p(x1,x2 ) and p(x2,x3 ) .. etc .",
    "it follows from the markov property that we can calculate the whole joint distribution ( this is a well - known theorem . )    a generator , in contrast to an initial state , is the _ law _ of a process .",
    "it s important to realize that not every markov process has such a law .",
    "given the markov property alone , the matrix t of conditional probabilities between successive states can be an arbitrary function of time , in which case the distinction between law and initial condition , between  essence \" and  accident \" , is artificial at best .    what happens to the generator if we reverse time ?",
    "except in very special cases , it does not exist .",
    "the transition matrices of a time - reversed markov chain are almost always time - dependent .",
    "this makes the chain property a good indicator of time direction ",
    "score one more point against hume !",
    "it s far from being a definitive indicator , though , since this reversed time - dependence is always lawful , and the reversed chain can always be embedded in a larger process with the chain property .",
    "we have not yet mentioned systems ; the concept of generator applies only to the process itself .",
    "however , it follows quickly from our results in section 3.5 that we can always represent a markov chain by a link system whose components , except for the first one , look and behave just like g. more exactly :    * causal link chain theorem * : given a markov chain c with generator g and initial vector v , there is a link system whose first component is v and whose successive components all have the matrix ( 1/n)g , where n is the * dimension * of v etc .",
    ", i.e. the number of values in the range of the state variables .",
    "let x be the initial chain variable , and x,y be the variables ( indices ) of g. then we must show that the joint distribution on x , y that results from assigning x , the conditioning index in g , to x , is the same as that resulting from linking the joint distribution in x , y having the joint distribution matrix ( 1/n)g to the independent variable x with the link ( x = x ) .",
    "recall that the sum of every column of a transition matrix is 1 , so ( 1/n)g is a joint distribution matrix in which the variable x is white .",
    "thus , by the white connection theorem , linking x and x does not change the distribution on x , and so multiplies the ith column of ( 1/n)g by p(x)/p(x = y ) = np(x ) .",
    "but this is just what you get when you multiply the initial vector by the transition matrix g.        * causal transformation * : a transformation t in which the probability distribution on the row index is white , i.e. all columns have the same sum .",
    "the transformations in a causal link chain are of course causal .",
    "a transition matrix as ordinarily defined is a causal transformation for which all elements are non - negative .",
    "we have now mapped markov chains onto link chains in such a way that their generators turn into causal transformations with the same matrices , except for a normalizing factor n. it was important to keep track of such _",
    "normalizers _ in proving our basic theorems , but in the future it will make life easier to ignore them until it comes time to compute actual probabilities .",
    "this is almost always possible because , if we know any multiple of a probability distribution on x , we can divide by its sum to get the probabilities themselves .",
    "when it comes to dealing with negative probabilities , there may be no sensible normalizer at all .",
    "for instance , it can happen that even though all the case counts we need are well defined , the total case count is 0 .",
    "the passage of time in a markov chain can be represented by a time - dependent transition matrix t(t ) , where t(t+1 ) = t(t)g . by the product theorem in section 3.5 , this same  transition law \" ( modulo a normalizer )",
    "is true in a causal link chain .",
    "the two concepts differ radically , however , when it comes to the meaning of the word ` state ' . in a markov chain ,",
    "the states are what we have called the state vectors of the variables , which directly represent their probability distributions .",
    "these state vectors are indeed still well - defined in link theory , and have the same meaning .",
    "however , the word ` state ' itself has been given a very new meaning , and now refers to the density matrix of a link . since the state vector is the diagonal of the state matrix , probabilities are gotten by the rule p(p ) = tr(ps ) , as we saw in the last section .    in markov chains , the law governing change of state , or as we ll call it , the _ dynamical rule _ , is v = t(v ) , where t is the time - dependent transition matrix , v the initial state , and v the final state .",
    "things are very different when we come to link states .",
    "* link dynamical rule*. let t be a causal transformation .",
    "if the matrix of t has an inverse , then s = tst@xmath2 , where s is the state before t , s the state after . if t does not have an inverse ,",
    "then the most that can be said is that ts = st    proof .",
    "let x and y be the variables of t , where the links to t are x = x and y = y for external variables x and y. now consider the join of the components other than t , and abstract from this the sub - process with variables x and y. cut the link x = x. by the product theorem , the matrix s in the unlinked variables x and x is ut . similarly , the matrix s in the unlinked variables y and y is tu .",
    "we now have s = ut and s = tu .",
    "thus st = tut = ts .",
    "if t has an invertible matrix , then we can multiply on the right by t@xmath2 giving the stronger rule s = tst@xmath2 .",
    "the classical dynamical rule is the offspring of newtonian determinism , and is based on a certain concept of explanation , which equates our understanding of a phenomenon with our ability to reliably predict it . with the return of chance , reliability becomes a matter of degree .",
    "the best laws are now those that give us the best estimates of probability .",
    "nevertheless , the aim of classical explanation remains the same , namely reducing information about the future to information about the past .",
    "thus the form of an explanatory law is that of a function that yields a probability distribution on the future as a function of a probability distribution on the present .",
    "notice that all this changes with the link dynamical rule .",
    "s is only a function of s if t is invertible .",
    "otherwise , the dynamical rule only specifies a certain relationship between future and past , in which the two enter symmetrically ( note that if t is invertible , we can also write s= tst@xmath2 . ) with this shift come new questions about the very meaning of dynamical explanation .",
    "the traditional scientist asks _ how _ the past dominates the future , but never _",
    "whether_. but link dynamics reveals many other kinds of relational structure besides domination .",
    "we are beginning to see hints of how link theory could enable us to remove our  causal colored glasses \" .",
    "it greatly simplifies many calculations to write the generator of a differential markov chain in the form g = 1+hdt , where 1 is the identity matrix . ignoring second - order effects",
    ", we then have g@xmath9 = 1 + 2hdt etc .",
    ", so we can calculate t(t ) by integration rather than by repeated matrix multiplication . in differential markov chains , it s customary to speak of h rather than g as the generator of the process",
    ". let @xmath18(t ) be the state vector of the chain at t , and define d@xmath18(t ) as @xmath18(t+dt)-@xmath18(t ) ; then for any t we have d@xmath18 = g(@xmath18 ) - @xmath18 = h(@xmath18)dt .",
    "many people have remarked on the resemblance between the equation d@xmath18 = h(@xmath18)dt and schrodinger s equation .",
    "we ll now see that this resemblance is no accident .",
    "first , note that 1+hdt is always invertible , it s inverse being 1-hdt ( we are ignoring second - order terms , so ( 1+hdt)(1-hdt ) = 1-h@xmath9dt@xmath9 = 1 ) . treating 1-hdt as a generator would give us our original markov chain reversed in time !",
    "but does nt this clash with our earlier observation that the time - reversal of a markov chain is nt a markov chain ?",
    "no , because closer inspection reveals that 1-hdt always contains negative  probabilities \" .",
    "of course if we allow negative probabilities , then the answer is yes , and we have a useful tool for retrodicting state vectors .",
    "however , be careful here . though the state evolution is reversed by the inverse process",
    ", the forward process itself , which includes the joint distribution on all variables , is not , since the inverse process always has negative joint probabilities .",
    "if we encounter a time - reversed markov chain , there are two things we can do in order to preserve the chain property : we can turn ourselves around in time , in effect putting the boundary condition on the future , or we can allow negative probabilities and work with the inverse chain , which means giving up studying the transitions and contenting ourselves with state vector dynamics .",
    "suppose , though , that we encountered a chain c made up of two chains c1 and c2 , the one forward and the other reversed . then it would seem that we only have the second option",
    ". however , link theory gives us a third , which is to put a separate condition on both the past and future of c , where the initial condition is white for c1 , and the final condition is white for c2 .",
    "this is illustrated in fig .",
    "the  generator \" g = g1&g2 of c is of course not a causal transformation , so c is not a markov chain .",
    "but g persists throughout the time of the process and thus should qualify as a law .",
    "what is essentially new is that in order to define the process as satisfying this law we must now be given two boundary conditions , one initial and one final , and that without this final condition , the process has no law ! this observation leads us to a basic generalization of the concept of markov chain :        we ve been focusing on transformations ; let s now turn to states . as we saw above , the link states of a causal chain are of the form @xmath1v@xmath3w@xmath1 , where w is white . we know from quantum physics",
    "that quantum density matrices are self - adjoint , which means that pure quantum states are of the form @xmath1v@xmath3v@xmath1 .",
    "what kind of link chains have quantum states ?",
    "suppose the initial link state of a quantum chain is @xmath1v@xmath3v@xmath1 . by the dynamical rule",
    ", the second link state will then be g@xmath1v@xmath3v@xmath1g@xmath2 .",
    "for the second state to be quantum , i.e. to be of the form @xmath1v@xmath3v@xmath1 , we must have @xmath19v@xmath1g@xmath2 = ( g@xmath1v@xmath20)@xmath21 = @xmath19v@xmath1g@xmath21 .",
    "( recall that t@xmath21 is the adjoint of t , which for real matrices is just t with rows and columns reversed . )",
    "if this equality is to be true for any initial state vector @xmath1v@xmath20 , i.e. if we are to be able to separate the law of the process from its boundary conditions , then we must have g@xmath2 = g@xmath21 .",
    "but a transformation whose inverse is its adjoint is a unitary transformation .",
    "thus we see that the generator of a quantum chain is unitary .",
    "one question remains ; given the initial vector @xmath1v@xmath20 , what must be the final state vector @xmath19w@xmath1 ? let t be the product gggg ... of all the g s .",
    "then @xmath1w@xmath3w@xmath1= t@xmath1v@xmath3v@xmath1t@xmath2 , so @xmath19w@xmath1 = @xmath19v@xmath1 t@xmath2 . we can write t@xmath2 as g@xmath2g@xmath2g@xmath2g@xmath2 .... which leads to a way of diagramming quantum chains that is very useful in analyzing quantum measurements , as we ll see in the next section . first an important definition :      since s is self - adjoint , its non - zero entry must be in its diagonal , which means it is a pure state whose vector components are also sharp .",
    "this leads to the diagram of a quantum system which is prepared in a sharp state given in fig .",
    "3 .    as with a differential markov chain , we can write the equation of a quantum chain in the form d@xmath18 = h(@xmath18)dt , where @xmath18 is the state vector .",
    "but where is the i ? you ask .",
    "we still havent arrived at physics yet ; what we have been studying is a more general mathematical structure known as _ real _ quantum mechanics . as mentioned , mackey showed how to get _ complex _ quantum mechanics as a special case of real quantum mechanics by introducing a ( real ) linear operator , which we ll call i , that commutes with every other operator .",
    "we ll take a slightly different route .    instead of starting out with i as an operator",
    ", we ll start with a matrix representation of the complex numbers , where a+ib is represented as a 2x2 matrix c@xmath22 , with c@xmath11 = c@xmath12 = a , and c@xmath23 = -c@xmath24 = b. one can quickly verify that these matrices behave like complex numbers under addition and multiplication .",
    "a complex  probability measure \" is then simply an additive measure on a boolean algebra having these matrices as its values .",
    "let x be a variable on which there is such a complex probability distribution c(x ) .",
    "we can map this onto a real distribution on three variables x , j and k such that for any x , p(j , k ) is the j , kth entry of c(x ) . with this mapping ,",
    "a complex probability acquires a definite meaning in terms of real probabilities : it is a joint distribution on two binary variables j and k satisfying p(j=1 & k=1 ) = p(j=2 & k=2 ) and p(j=2 & k=1 ) = - p(j=1 & k=2 ) .",
    "a complex variable x is then a process in x , j and k such that , for any m , p(j , k@xmath1x = m ) is a complex probability .",
    "suppose e and e are independent events with complex probabilities c and c. what , then , is the probability of e&e ? we would of course like it to be cc. however , if c and c are independent real 2x2 matrices , i.e. independent joint distributions on i , j and i,j , then combining e and e produces a distribution p(j , k , j,k ) = p(j , k)p(j,k ) which does not even make sense as a complex probability .",
    "this presents us with a choice : we must either give up trying to reduce complex probabilities to real probabilities , or else give up our old notion of independence , putting the complex product rule c(e&e ) = cc in its place .",
    "since quantum mechanics uses this complex product rule in defining the inner product of two quantum objects , we ll take the second course , and ask what it means when translated into real probability theory .    to multiply c and c means to _ contract _ k and j",
    ", i.e. , to link k to j and then ignore k , or equivalently , since cc = cc , to contract j and k. thus to combine a complex process w with an independent complex process w into a complex process w&w we must always link a binary variable of one with a binary variable of the other . in quantum mechanics ,",
    "an n - dimensional unitary transformation t can be thought of as operation on an object which is an entangled composite of an n - dimensional real object r and a two - dimensional real object j , where the entanglements satisfies the skew - symmetry rules above .",
    "the 2x2 pieces of t can be thought of as operators on j. suppose t operates on an independent r&j. to bring t and t into the same universe of discourse , we must link j and j , thus multiplying the transformations on j times those on j. but it only makes sense to multiply transformations if they operate on the same object , so we conclude that j = j !    in other words",
    ", there is a particular two - state particle that belongs to every quantum object ; this is what we referred to earlier as the janus particle .",
    "that name was based on a conjecture relating its states to the equivalence of the heisenberg and schrodinger representations , so for present purposes lets give it another name :      as is well known , only the relative phase of quantum amplitude can be observed , the absolute phase being unobservable .",
    "this is equivalent to saying that the state of j is unobservable .",
    "note that this means that  pure \" quantum states , regarded as real states , are always mixtures with two - dimensional degeneracy .",
    "an interesting way to construct complex quantum mechanics is to start with real objects ri , add j to each of them , and then confine ourselves of those states of ri&j in which either j is independent of ri ( in the real sense ) , or is entangled in such a way that changing the state of j does not affect the probabilities in measurements of ri ..      * janus particle * : a hypothetical two - state particle belonging to all quantum objects , among whose symmetries are the reversal of the heisenberg and schrodinger representations .",
    "let us here go on record with our conjecture that the janus particle is the phase particle , and that resulting  subject - object symmetry \" is the conjugation symmetry of quantum mechanics .      _ the measurement problem in link theory , the paradoxes of negative case counting , standard measurement theory , the projection postulate , the disturbance rule , the three axioms that define laboratory objects , functional objects , experiments , confined quantum objects , quantum time vs. laboratory time , prepared quantum chains , definite present and open future , the projection postulate in link theory , the collapse of the wavefront , the confined quantum object is a laboratory object , the white state , time loops revisited , macroscopic matter .",
    "_    the born rule tells us that , for any proposition p and quantum variable x , the probability of a measurement of x satisfying p is tr(ps ) , where s is the link state of x. the dynamical rule tells how the state s changes with the passage of time .",
    "so does nt that wrap it up ? what more is there to be said about quantum observation ? in the standard hilbert space approach , there is the problem of how to construe our  classical \" measuring instruments as quantum objects , which led von neumann to his famous construction of the movable boundary between object and observer . but this problem does nt arise in link theory , since quantum and classical can perfectly well coexist in the same joint distribution . indeed , as we have seen , the very distinction between classical and quantum rests in part on arbitrary decisions about how to analyze a process .",
    "so what , then , is left of the infamous quantum measurement problem ? have we in fact solved it ?    not quite .",
    "in fact , the measurement problem takes a particularly acute form in link theory , as we ll now see .",
    "4a shows a single complete measurement of a variable x in a pure state @xmath1v@xmath3v@xmath1 ; we simply draw an arrow from x to a recorder , linking x to an input variable of the recorder which is assumed to be white .",
    "so far so good ; there s no measurement problem yet .",
    "if we wait until time t(y ) to measure ( fig .",
    "4b ) , then the state will have been transformed into tst@xmath21 , and an arrow from y to the recorder will again give results in agreement with standard theory .",
    "still no measurement problem .    but now let s consider two complete measurements , one before and one after transformation t ( fig .",
    "each of these measurements will give the same probabilities that it did when it was the sole measurement .",
    "however , that s not what the standard theory predicts .",
    "what went wrong ?    here",
    "s one thing that went wrong .",
    "consider a _ selection _ from among all the records of just those records for which the first measurement had the value k. within this subset , every event in x will have probability 1 or 0 , so x is independent of all other events , and v , which we ll assume is not sharp , is replaced by a sharp vector v. the result will be in most cases that the state of y is no longer quantum .",
    "in other words , measuring the object throws it out of a quantum state .",
    "but we re in worse trouble than that . in the system of fig .",
    "4c , it may well happen that some of the joint probabilities on x and y are negative , which means our recorder has recorded  negative \" cases ! so what s wrong with recording negative cases ?",
    "you may ask . remember , recorded cases are not just alternatives ,",
    "they are _ instances _ ; they are actual things , marks on a tape , whatnot .",
    "suppose your closet contains ten positive white shirts and ten negative green shirts .",
    "if you reach into your closet for a shirt , you will come out empty handed , since there are 0 shirts there . on the other hand ,",
    "if you reach in for a white shirt , you ll have ten to choose from . that s quantum logic .    to see the problem of repeated measurement more sharply ,",
    "consider a third measurement on z which comes after a second transformation t@xmath21 ( remember , t is unitary , so t@xmath21 = t@xmath2 ) ; this is shown in fig .",
    "5 .    as before",
    ", we ll run a long series of recorded experiments . since tt@xmath21 = 1 , x and z",
    "are perfectly correlated in this process .",
    "since the recorder is white and therefore does nt effect the probabilities of x , y and z , this perfect correlation will show up in the total set of records , i.e. x and z are equal in every record . but now consider the subset s@xmath25 of all records for which y = k .",
    "since y separates x and z , we know that x and z are conditionally independent given y , which means that they are independent within s@xmath25 .",
    "thus the records in s@xmath25 will in most cases show x and z as having different values !",
    "s@xmath25 is like your white shirts in the closet ",
    "it s a non - null subset of the null set . but",
    "real shirts and real records just do nt behave like that .",
    "obviously we ve got to take another tack .    up to now in this paper",
    "we have managed to avoid the logical anomalies that result from negative case counting by carefully steering away from them , the trick here being to think only about the arithmetic of ratios . but this wo nt work forever . sooner or later we have to face the paradox of the white shirts .",
    "this is the paradox of the two - slit experiment seen straight on . with both slits open ,",
    "the case counts cancel at the null in the  wave interference pattern \" where the detector lies , and no electrons are detected . but with one slit closed , the  white shirt \" electrons alone are selected , and they make it through .",
    "we ll see that the solution of the measurement problem involves transforming this seeming paradox into a mathematical account of what it means for the future to be _",
    "since the predictions of the standard theory of quantum measurement are in close agreement with experiment , we must represent measurement by link systems that make these same predictions . to see just what is required , here",
    "is a brief summary of the standard theory , which is based on an assumption that von neumann called the _",
    "projection postulate_.    * projection postulate*. given a state s and a proposition ( projection ) p , there is an ideal measurement m to test for the truth of p that leaves those things that pass the test in the state psp / p , and those things that fail in the state psp/(1-p ) , where p = tr(ps ) and p is the negation 1-p of p.    it follows directly that m leaves the object in the state psp+psp. let q be any observable with eigenvalues q@xmath26 , q@xmath27 , .. q@xmath28 , and let p@xmath26 , p@xmath27 .. p@xmath28 be the  projectors \" of these eigenvalues , i.e. , the ( ortho ) projections whose ranges are the eigenspaces of q@xmath26 , q@xmath27 .. q@xmath28 .",
    "it is easy to show from the projection postulate that we can measure q by performing a series of tests for , p@xmath26,p@xmath27 ..",
    ". the object will only pass one of these tests , p@xmath29 , which tells us that q has the value q@xmath29 .",
    "the ensemble of objects thus tested will be in the mixture @xmath15p@xmath29sp@xmath29 , which gives us the general rule for the minimum  disturbance \" of a state by a measurement .",
    "* disturbance rule * : given an object in state s , the least disturbing measurement we can make of quantity q is one which leaves the object in the state @xmath15p@xmath29sp@xmath29 , where the p@xmath29 are the projectors of q.    the disturbance rule gives us a way to prepare a beam of objects in any state s , which is to start with a beam in any state and measure it in a way that yields the components of s , and then select out a subset of objects from each component to give the proper weight of that component in s.    practically speaking , the standard theory of measurement works very well , at least for simple systems . but the projection postulate has caused a lot of trouble for people concerned with quantum foundations , since , by  collapsing the wavefront \" , it messes up what is otherwise a beautifully clean and simple unitary dynamics .",
    "since link theory has no wavefront to collapse , it ought to be able to do better , and indeed it does ; in fact , we count its success in this regard as its major accomplishment to date .",
    "we ll see in a while what the projection postulate means in terms of linking .",
    "but first we need a description in our current language of the kinds of things that go on in a physics laboratory .",
    "the first thing to be said about our idealized and simplified laboratory is that there is a clock on the wall .",
    "someone , or something , is always watching that clock and logging every event , so that at the end of every experiment there is a diary of the experimental procedures and the outcomes of measurement .. the second thing to be said is that these experimental procedures can be repeated as often as necessary to obtain meaningful relative frequencies .",
    "let s now characterize the _ objects _ in our lab .",
    "* laboratory object * : a process k in which we distinguish two kinds of _ external _ variables , called _ input _ and _ output _ variables .",
    "each such variable exists for only one clock tick ; for instance x could be the keyboard input to a computer at time t ; we ll write the time at which x occurs as t(x ) .",
    "input variables are those that we control , while output variables are those that we measure , or use to control inputs .",
    "k may have other variables besides its inputs and outputs , and it need not have inputs at all , though it must always have at least one output .",
    "when we speak simply of _ the input _ to k , we ll mean all of its input variables together , or more exactly , the join of all these variables .",
    "similarly for _ the output _ from k. the input ( output ) _ before _ or _ after _ time t@xmath26 is the join of input ( output ) variables x@xmath29 such that t(x@xmath29 ) @xmath19 t@xmath26 , or t(x@xmath29 ) @xmath20 t@xmath26 .",
    "these three axioms broadly capture our common - sense notion of processes that we can observe and manipulate .",
    "notice that they have nothing whatsoever to do with classical mechanics .",
    "bohr characterized the laboratory as the place where newtonian mechanics reigns , but this was a mistake .",
    "newton s laws neither imply nor are implied by the above axioms ; indeed they belong to a wholly different universe of discourse ( that s one reason why the young russell , who admired physics more than common sense , was so eager to jettison causality . )    * functional object * : one in which the output is a function of the input .",
    "the requirement of whiteness on the input imposes the condition on the joint distribution that all probabilities are either 0 or 1/n , where n is dimension ( total number of cases . )",
    "it s convenient to ignore normalization and represent such a joint distribution by the bit array which is the characteristic function of the object s function regarded as a relation .",
    "* experiment * : a link system whose components are laboratory objects , one of which is a functional object known as a _ recorder_. the output of the recorder is a variable at the end of the process whose value is a record of all inputs and outputs to the object on which the experiment is performed .",
    "it is assumed that every input variable in an experiment is linked to a prior output variable , and from this it follows that the process represented by the experiment is itself a laboratory object .",
    "let s now turn to the construction of quantum objects .",
    "we ll show these as enclosed in a dotted box , as in fig . 4 and fig .",
    "our aim now is to design dotted boxes that enclose their quantum weirdness , so - to - speak , i.e. , boxes whose in - going and outgoing arrows satisfy our definition of a laboratory object .",
    "such confined quantum objects are , properly speaking , the true objects of scrutiny in a quantum physics lab .",
    "notice that the dotted boxes in fig .",
    "4 are confined quantum objects ; they satisfy 1 ) and 2 ) because they have no inputs , and 3 ) because their probabilities satisfy born s rule and are therefore non - negative .",
    "the dotted boxes in fig .",
    "5 are not confined objects , however , since their outputs , i.e. the joint distributions on x , y and z , do have negative probabilities .",
    "our challenge now is to redraw fig .",
    "5 as a confined object that gives the right numbers .",
    "there s one thing does nt look right about the box in fig . 5 , which is that to prepare it in state @xmath1v@xmath3v@xmath1 requires placing a boundary condition both before time t(x ) and after time t(y ) .",
    "if we think of these conditions as inputs from the lab , the second condition violates 2 ) , the causality principle .",
    "this is assuming that the time sequences inside and outside the box are the same .",
    "but there is another possibility , and a more likely one , which is that  time \" in the quantum domain is somewhat autonomous from laboratory time , or to put it more abstractly , that the chains of quantum separability that we compared to markov chains are in themselves not temporal sequences at all . under this assumption",
    ", it is reasonable to define a _ prepared _",
    "quantum chain as one whose beginning and end are both at the initial laboratory time of the experiment ( fig 6a ) .",
    "this construction looks more natural when we draw it with t@xmath21 factored out of w ( fig .",
    "reversing rows and columns of t@xmath21 turns 6b into the equivalent form 6c .",
    "let s then take this as our picture of a chain prepared in a pure state ; notice that the states s and s are identical .",
    "more generally :      suppose that when we measure x , the future measurement of y is indeterminate , in the sense that all outcomes are possible .. this indeed is our experience of time , which we express by saying that there does not exist at time t(x ) a record of the outcome of the measurement at t(y ) .",
    "thus it makes no sense to speak at t(x ) of the subset of records for which y has a certain value . but",
    "this implies that x and x are linked at time t(x ) by the matrix tt@xmath21 = 1 , which means that recording the value of x at t(x ) also records the value of x at t(x ) . since we must sum over all values of y to evaluate probabilities at t(x ) , this avoids the paradox of fig .",
    "now what happens when we measure y ?",
    "in our earlier example , this measurement unlinked x and z , making them in fact independent .",
    "however , in the present case , x is already on record as having the same value as x , so it of course can not become independent .",
    "how do we show this in our diagram ? simply by drawing a line between x and x ( fig .",
    "recall that a recording device is a laboratory object having ( white ) inputs whose output at the final time is a 1 - 1 function of the history of its inputs .",
    "before y is measured , this line merely duplicates tt@xmath21 , so drawing it would be redundant .",
    "but after y has acquired a definite value , this line in effect remembers the measurement of x , thus holding the fort against the disruption that would otherwise be produced by fixing y. therefore there will be no records at all where x and x are unequal , and the paradox of a non - null subset of the null set is avoided .        * projection postulate for a complete measurement * : to make a complete measurement of the state of a prepared quantum chain at time t(x ) , insert a link between x and x and measure the linked variable x , i.e. _ probe _",
    "x.    the choice of whether or not to probe x can be thought of as a double - pole switch .",
    "just what do we mean by the term ` switch ' ? by a _",
    "single - pole switch _ we mean a three - variable component sw[s , x , y ] where s is a binary variable with values on and off , sw[on , x , y ] is the identity matrix , and sw[off , x , y ] is the white matrix in which every element equals 1/n , n being the dimension of x and y. notice that for unlinked x and y , s is white , since the sum of matrix elements in the on condition is the same as that in the off condition . by a _",
    "double - pole switch _ we mean a pair of single - pole switches whose s s are linked .",
    "the crucial question now arises whether this switch input satisfies axioms 1 ) and 2 ) , i.e. whether it is white and causal .",
    "if so , then the dotted box qualifies as a laboratory object . to say that the switch is white means that the probability p of the process is unchanged by drawing the line . by the summation theorem , p is the sum of the joint distribution on x and x. drawing the line eliminates all off - diagonal terms in this sum .",
    "but since tt@xmath21 = 1 , these off - diagonal terms are all 0 anyway , so the line does nt affect p. this takes care of 1 ) ; we ll come to 2 ) shortly .    looking at fig .",
    "8b , we see that closing the switch breaks the diagram into two parts separated by x. the second part reduces to a circle , as far as x is concerned , so its presence at x is white .",
    "thus the state s of the link from v to x is unchanged by the measurement of x ; it is still @xmath1v@xmath3v@xmath1 , and the probability distribution on x is still the diagonal of s , in accordance with born s rule .",
    "but what is state s@xmath26 ?",
    "clearly it has the same diagonal as s. but , since tt@xmath21 = 1 , its off - diagonal elements are all 0 .",
    "s@xmath26 is then the mixed state having sharp components weighted by the probability distribution on x. thus we see that this change of state from s to s1 satisfies the standard theory of measurement , and is indeed nothing else but our old friend the collapse of the wavefront ! of course there is now no longer a wavefront to collapse , so we ll have to find another name for it .",
    "whatever we call it , though , it s the crucial event that links the micro to the macro , and it is brought about by the _ passive _ act of recording the state of x. our probe does not disturb what it sees ; its effect comes from seeing alone .    that seeing alone is sufficient to collapse the wave function was a point made by hpn in the context of the double - slit experiment performed with low - velocity cannon balls , armor plate and mechanical detectors @xcite .",
    "this point needs a little further discussion here in anticipation of both more abstract _ and _ more physical discussions of the measurement problem which are in preparation .",
    "`` seeing '' the low - velocity cannon ball go through one slit or the other requires illumination , and reflection of at least one photon from the cannon ball itself , together with reflection of enough light from the two slits to distinguish which it went through .",
    "hence there is actual energy transfer from the ball to the observer .",
    "this energy transfer has to be recorded before this particular case can be used to enter as a record in the compilation of single slit cases in contrast with double slit cases .",
    "the act is `` passive '' but not devoid of physical content and ( pace von neumann and wigner ) does _ not _ require a",
    "_ conscious _ observer to `` collapse the wave function '' , as we hope the analysis made in terms of the abstract theory presented above has made clear . more generally , making the `` quantum system '' into a `` laboratory object '' requires conscious experimenters to set up the apparatus and to interpret the results after they are recorded , but this in no way distinguishes it from the protocols of classical experimentation where no question of quantum effects arises .",
    "when such processes are studied in enough detail to observe the progressive ( and predicted ) `` randomization '' of mesoscopic quantum states  as has recently been done @xcite with two separating rydberg atoms in @xmath30 states  the quantum _ theoretical _ treatment must include many more degrees of freedom than are usually included when discussing quantum measurement and the projection postulate .",
    "we believe that it might prove instructive to see how much of the analysis of such systems depends on how they are `` carved up '' between classical and quantum degrees of freedom and where considerations of experimental accuracy have to enter .    closing the  record",
    "\" switch does not change the observed probability distribution on x , and in this sense it does not disturb the measurement .",
    "however , it does affect the state of y , which would be tst@xmath21 with the switch open but is t s@xmath26t@xmath21 with the switch closed .",
    "from the point of view of the laboratory , closing the switch causes a change in the state of y. inside the dotted box , though , there is nothing resembling cause and effect ; indeed , nothing even happens inside the box , since closing the switch merely duplicates an existing link .",
    "measuring y is just like measuring x , the only difference being that s@xmath26 is mixed ( fig .",
    "thus we see that throwing the switch at y does nt affect the measurement of x , or the whiteness of the x switch , so the switch inputs satisfy 2 ) , the causality axiom .",
    "this shows that the dotted box is a classical object , and the sequence of measured variables is in fact a markov process .",
    "so far we ve only been considering complete measurements .",
    "an incomplete measurement of x is represented by a functional box f(x ) going from x to the recorder , where f is not 1 - 1 .",
    "recall that , in a complete measurement , connecting x to x means that by recording x at t(x ) we have also recorded x at t(x ) .",
    "however , in the case of a partial measurement , it s not x but only f(x ) that gets recorded at t(x ) , so this connection should go from f(x ) to f(x ) rather than from x to x ( fig .",
    "this arrangement satisfies the standard general rule s = @xmath31p@xmath25sp@xmath25 for the change of state due to a partial measurement , where the p@xmath25 are the projectors of the measured observable .",
    "what about preparation ?",
    "as remarked , a preparation is always equivalent to a selection based on a measurement .",
    "but we can also represent a preparation as an input to an unprepared system , i.e. a system in the white state ( fig .",
    "10a ) ; clearly such an input is always white .    the white state , whose matrix is 1 , has a unique role in quantum theory , since it is unchanged by measurement ( i.e. , if s=1 , then @xmath31p@xmath25sp@xmath25 = @xmath31p@xmath25 = s ) .",
    "the diagram of an unprepared and unmeasured quantum object is simply a circle ( fig .",
    "10b ) ; we can think of it as representing pure entropy .",
    "perhaps this is the only quantum diagram that makes sense in isolation from the classical world .",
    "our conclusion that the  collapse of the wavefront \" requires an open future seems to reintroduce an absolute arrow of time .",
    "but this is an illusion , since an unprepared system is symmetrical with respect to past and future , whether it is measured or not .",
    "time s arrow comes not from quantum openness but from classical preparation , which we have conventionally placed at the beginning of the process , but which could just as well occur at the end , or at both the beginning and end , since it is equivalent to a selection of records .",
    "thus our present analysis is consistent with our previous notion of the  state throttle \" in a time loop .",
    "the topic of measurement , especially partial measurement , deserves a much fuller treatment than we have given it here .",
    "however , even the present abbreviated account calls for a few more remarks concerning macroscopic matter :    in our analysis above , we have treated quantum and laboratory objects as essentially different kinds of things . and",
    "yet sooner or later we must face the fact that laboratory objects are made out of atoms , and try to render an account of just what this means . in standard quantum mechanics ,",
    "when you assemble quantum objects you always get other quantum objects , and this indeed does create insoluble problems , since there s no way to  bend \" quantum - ness into anything else . the present approach does nt have that problem , of course , since it treats quantum - ness as a mode of analysis rather than an intrinsic property of things , and there is no reason why the appropriate mode of analysis should be the same at different scales .",
    "but we can hardly let things go at that , since there is a huge body of empirical knowledge about the relationship between micro and macro that needs to be translated into our present conceptual scheme .    in the last century",
    ", the laws of mechanics were thought to form a closed system that completely governs physical change , all other physical facts being relegated to the category of  accidents \" of the world state .",
    "how , in such a scheme , is one to deal with the undoubtedly lawful character of thermodynamic events like heat flow ?",
    "the nineteenth - century answer , which never quite satisfied anyone , was to impose a secondary layer of ",
    "laws among the accidents \" , so - to - speak , these being somehow based on the laws of probability . with the advent of quantum mechanics",
    "this idea becomes pretty far - fetched , since the equation governing quantum mechanical change has , except for the factor i , exactly the same form as the diffusion equation , which is one of the alleged laws among the accidents .",
    "let us hope that this classical two - layer concept of law is now well behind us .",
    "how , then , do we describe the relationship of micro to macro without it ?",
    "let s start at the level of process .",
    "remember , in a process as such there are no states until you decide how to carve it up , only correlated variables , so it literally makes no sense to ask whether a process is quantum or causal .",
    "it does , however , make sense to ask whether or not its joint distribution contains negative probabilities .",
    "let s pretend for the moment that it does nt .",
    "what we are then given is a huge number of micro - variables associated with individual atoms plus a much smaller number of macro variables associated with regions large enough to contain many atoms , the latter often closely corresponding to sums or averages of the micro variables .",
    "the macro - variables can be directly and repeatedly measured without disturbing them , and they seem to satisfy pretty well our axioms for laboratory objects , at least for simple inorganic objects .",
    "we implicitly assume that spatial boundaries inside the object correspond to separation boundaries in our present sense ",
    "one sometimes speaks of this assumption as non - locality .",
    "applying our fundamental theorem makes it possible to define states at these boundaries , thus turning the macro - behavior of the object into a composition .",
    "this is essentially how an scientist carves something up when he wants to make a computer model of it , and there is no doubt that it works very well .",
    "the reason it works is that macro - behavior has a good deal of autonomy from what we assume about the behavior of the unmeasured micro - variables , and for most purposes we can ignore quantum effects completely , at least under average terrestrial conditions .",
    "it might be supposed that this is simply a consequence of locality plus the complexity of the processes the macro - variables are averaging over , and indeed it would be , if the micro - probabilities were always positive .",
    "however , with negative micro - probabilities the situation is quite different .",
    "consider a chain whose generator is periodic , i.e. , whose nth power is the identity for some n. with only positive probabilities allowed , such a generator would have to be deterministic . but this is not true with negative probabilities , and physicists are well acquainted with periodic unitary transformations ( indeed , any self - adjoint unitary transformation has period two . ) now a probabilistic chain of period",
    "n is of course separable at every variable , and would thus satisfy locality if it were strung through a material object .",
    "however , its first and last variables are perfectly correlated , just as if they were directly linked ! from the point of view of common sense , this is non - locality with a vengeance .    clearly we need a new assumption that keeps such non - local  virtual links \" from playing a significant role in macro - behavior .",
    "physicists already have a name for what is probably the right assumption : decoherence .",
    "since this concept is defined in terms of wave functions , it needs to be reformulated in the language of probability . and",
    "it undoubtedly needs to be generalized to deal with matter at low temperatures , and perhaps with organic matter as well .",
    "though all this could become technically complicated , it looks straightforward enough .    there is a much more serious problem which is not solved by decoherence , however , and that is to explain why negative probabilities remain confined to the micro - realm .",
    "what prevents them from  breaking out \" into the everyday world and playing havoc with logic ?",
    "we avoided this problem in the case of quantum measurements by assuming that making a measurement replaces certain  virtual links \" due to quantum coherence by actual links .",
    "something of the same sort must apply to the relationship between a macro - variable and the quantum average to which it corresponds .",
    "that is , a macro - variable , by being linked to the world at large , actualizes the virtual link on the quantum level between an average variable and its time - reversed dual . to put it more simply , macro - regions are self - measuring .",
    "notice that measuring an average of many quantum micro - variables extracts only a minute fraction of the information in a pure joint state of these micro - variables , so these  macro - measurements \" are essentially non - disturbing .",
    "but before we break out the champagne to celebrate the final triumph of logic and common sense , there are a few more questions .",
    "why does nt the time symmetry of the microscopic level carry over to the macro - level , at least to the extent of showing up as independent macroscopic boundary constraints on past and future ? to what extent does our reasoning in terms of states bear on the process itself rather than on how we choose to carve it up ? might it be that hanging on to our causal framework , even for everyday objects , exacts a large price in complexity , thus blinding us to simple and essential forms in nature ? and",
    "finally , why should we assume that we can always find ways to keep the irrationality of negative probability out of our experience ?",
    "this has been a long paper , but we fear it has only scratched the surface of its subject .",
    "we ve concentrated here on the basics rather than on results .",
    "we believe this was the best course , since these basics take some getting used to .",
    "but in the long run , it s results that win the prize .",
    "the history of science is littered with the remains of seductive theories that led nowhere , von neumann s quantum logic being a recent case in point .",
    "the first place to look for results in link theory is obviously physics , about which we ve said relatively little , so here , in conclusion , is a brief prospectus on several new items of physics that we see on the horizon .",
    "first , some general remarks about space .",
    "so far , we ve mostly been drawing diagrams rather than maps ; in the dialogue between composition and extension , composition has had by far the louder voice .",
    "but the world does nt present itself to raw experience as a neat arrangement of interchangeable parts  first we have to carve it up . and before we can carve it up , we have to map it , or ",
    "mark the carcass \" , as we put it earlier .",
    "the places where we mark are the places that separate the information in the extended structure , the boundaries through which correlation flows , and which , when rendered  immobile \" by fixing the values of their variables , break the whole into independently variable parts .",
    "the question naturally arises as to how the structure of the probabilistic separating relation p(a&b&c)p(b ) = p(a&b)p(b&c ) is related to more familiar geometric structures like metrics and topologies and linear orders .",
    "consider , for instance , a markov process .",
    "we always start out by giving its variables in a certain order , but is this order contained in the probabilities or is it arbitrary ?",
    "we saw that the arrow is arbitrary , so we are really talking about the between - ness relation .",
    "let s define b(x , y , z ) , read ",
    "y is between x and z \" , to mean p(x , y , z)p(y ) = p(x , y)p(y , z ) .",
    "we know that separation is equivalent to the markov property , which holds for any x , y and z in the given order . thus the question is whether b(x , y , z ) always excludes b(y , x , z ) and b(x , z , y ) ; let s call this the order property . if a process has the order property , then the linear order of its variables is indeed intrinsic to that process itself .",
    "there are some obvious instances of markov processes in which the order property fails .",
    "for instance , a sequence of independent variables is a markov process , but since any one of the variables separates any two others , the variables can be rearranged in any order ; intrinsically , they are a mere heap . at the opposite extreme",
    "is the case where the variables are perfectly correlated , i.e. , where the transition matrices are permutation matrices .",
    "if you fix any variable in such a process , you fix all the others too .",
    "but , as we saw in section 1 , a fixed variable is independent of all others , so , again , any variable separates any other two .",
    "it may seem odd to think of laplace s perfect newtonian clockwork universe as a mere disordered heap of states , but there it is !",
    "the true opposite of the independent variable case is the differential process given by a system where g = 1 + hdt .",
    "if h is not 0 and there are no negative transition probabilities , such a process is always ordered .",
    "hdt does nt have to be truly infinitesimal to guarantee this , only rather small .",
    "if g is constant in time , the probabilities not only define an order but a metric , at least up to a scale constant .",
    "the local order of a continuous process is rather curious .",
    "since it approaches the deterministic case in arbitrarily small neighborhoods , it is , so - to - speak , locally disordered .",
    "that is , it would require arbitrarily many repetitions of the process to statistically determine the order of points that are arbitrarily close together ; this is rather suggestive of the time - energy uncertainty principle .",
    "the same kind of reasoning can be applied to higher - dimensional continuous processes , whose probability distributions in general contain the intrinsic geometry of the space that is used to define them , or at least its topology .",
    "keith bowden@xcite has suggested that if we apply suitable perturbing random fields to classical fields , we can recover their spaces from the resulting joint probability distributions .",
    "it would seem natural in a  probability space \" for the local structure to be quite different from the global structure , which might have some bearing on super - string theory .",
    "what s the simplest random process of all ?",
    "surely it s a coin toss .",
    "let v be a coin tossing process whose heads / tails variable is x , and define the velocity v of v as the probability of x = heads minus the probability of x = tails ( we re not assuming it s a fair coin . )",
    "notice that if you step one meter to the right or left every second according to the outcome of a toss of v , then v is your average velocity to the right in meters per second .",
    "what s the simplest of all systems with more than one component ?",
    "surely it s a pair of linked coin tosses .",
    "call these components v and v , with binary variables x and x and velocities v and v. prior to linking , v and v are independent . linking x and",
    "x produces the single - variable binary process u = v&v@xmath1x = x , which we can think of as a coin toss too .",
    "what , then , is the velocity u of u in terms of v and v ?",
    "proof : let p and q be the probabilities of heads and tails for v , and similarly let p and q for v. then v = p - q and v = p-q , and from the definition of linking one can quickly verify that u = ( pp-qq)/(pp+qq ) .",
    "thus we must show that ( pp-qq)/(pp+qq ) = ( p - q+p-q)/[1-(p - q)(p-q ) ] .",
    "now in fact these two expressions are not identical as they stand , but only become identical when we bring in the additional fact that probabilities add up to one , i.e. p+q = p+q = 1 .",
    "the easiest way to take these conditions into account is to note that v = ( p - q)/(p+q ) and v = ( p-q)/(p+q ) and substitute these expressions for v and v in ( v+v)/(1+vv ) ; the resulting expression then reduces to ( pp-qq)/(pp+qq ) .",
    "qed .    applied to observer and object ,",
    "the boost law implies the lorentz transformation thus we find that the concept of linking , which before led us immediately to the heart of quantum mechanics , has now led us immediately to the heart of relativity !    there is still a lot of work to be done to relate the above theorem to the concept of  probability space \" based on separability .",
    "one approach here may be to interpret  time lines \" as binary markov chains from which the left - right variables are abstracted statistically .",
    "1x1 space - time would then be the indefinite process that results from linking these velocity variables in an unspecified collection of such chains .",
    "notice the formal resemblance here to our construction of complex amplitudes , which also resulted from linking an indefinite set of processes via a binary phase variable .",
    "the question arises whether this resemblance is more than just an analogy .",
    "could it be that at some fundamental level , the phase particle and the  velocity particle \" are one and the same ?",
    "let s briefly consider where this would lead . since in ( complex )",
    "minkowski space boosts are rotations of the complex plane , this identity would make the relativity of amplitude phase into a generalization of the relativity of motion . even more important for the science of the future is that the conjugation symmetry of the phase particle would become the symmetry of v and -v , which is the symmetry that results from reversing object and observer .",
    "this supports our conjecture that the phase particle is indeed the janus particle , and makes more sense of pauli s mystical vision of i as the key to uniting physics with psychology , as described in chapter 2 .",
    "but of course we are now looking rather far ahead .",
    "noyes , h. p. [ 1980",
    "] , `` an operational analysis of the double slit experiment '' , in _ studies in the foundations of quantum mechanics _ , p. suppes ed . , philosophy of science association , east lansing , michigan , p 103 .",
    "a markov process in time can be generated either from a vector given when the process starts and propagate forward in time or from a vector given when the process ends and propagate backward in time ( see text ) ."
  ],
  "abstract_text": [
    "<S> we shall argue in this paper that a central piece of modern physics does not really belong to physics at all but to elementary probability theory . given a joint probability distribution j on a set of random variables containing x and y , </S>",
    "<S> define a link between x and y to be the condition x = y on j. define the _ state _ d of a link x = y as the joint probability distribution matrix on x and y without the link . </S>",
    "<S> the two core laws of quantum mechanics are the born probability rule , and the unitary dynamical law whose best known form is the schrodinger s equation . </S>",
    "<S> von neumann formulated these two laws in the language of hilbert space as prob(p ) = trace(pd ) and dt = td respectively , where p is a projection , d and d are ( von neumann ) density matrices , and t is a unitary transformation . </S>",
    "<S> we ll see that if we regard link states as density matrices , the algebraic forms of these two core laws occur as completely general theorems about links . </S>",
    "<S> when we extend probability theory by allowing cases to count negatively , we find that the hilbert space framework of quantum mechanics proper emerges from the assumption that all d s are symmetrical in rows and columns . on the other hand , </S>",
    "<S> markovian systems emerge when we assume that one of every linked variable pair has a uniform probability distribution . by representing quantum and markovian structure in this way </S>",
    "<S> , we see clearly both how they differ , and also how they can coexist in natural harmony with each other , as they must in quantum measurement , which we ll examine in some detail . looking beyond quantum mechanics , </S>",
    "<S> we see how both structures have their special places in a much larger continuum of formal systems that we have yet to look for in nature .    </S>",
    "<S> 6.0 in 8.6 in    -0.25truein 0.30truein 0.30truein = 1.5pc    slac  pub-7890 + august 1998 +    * process , system , causality , and quantum mechanics * + _ * a psychoanalysis of animal faith * _    tom etter + 112 blackburn avenue + menlo park , california 940252704 + and + h. pierre noyes + stanford linear accelerator center + stanford university , stanford , ca 94309 +    submitted to _ international journal of theoretical physics . </S>",
    "<S> _    * introduction : counting sheep *    once upon a time there was a sheep farmer who had ten small barns , in each of which he kept five sheep . when asked how many sheep he had altogether , he replied  many \" , for people in those days counted on their fingers , and no one had ever thought of counting beyond ten .    every morning </S>",
    "<S> he would drive his sheep over the hill and through the woods to their pasture , where they assembled in five fields , ten sheep to a field . </S>",
    "<S> the farmer , who was of a reflective bent , saw here a curious and beautiful law of nature :  ten barns each with five sheep , and then five fields each with ten sheep ! \" </S>",
    "<S> unfortunately this law did not always hold , and when the wolves howled on the hill at night , it failed quite often . </S>",
    "<S> the farmer had an explanation for this :  the howling of the wolves greatly upsets my sheep , and the laws of nature , like the laws of man , are often disobeyed when agitated spirits prevail \" . </S>",
    "<S> the farmer realized that to make his law universal he would have to modify it thus :  when tranquillity reigns , ten of five turn into five of ten . \"    </S>",
    "<S> we today who know arithmetic would say that the farmer s law , though true enough in his particular situation , is nt a very good law by scientific standards . </S>",
    "<S> it needs to be  </S>",
    "<S> factored \" into two laws , the first being the simple and very general law that xy = yx and the second a more complicated and specialized law having to do with sheep and wolves . </S>",
    "<S> the farmer was indeed aware that xy = yx , at least in the case of 5 and 10 , but what he could not see is that the essential condition for xy to be yx has nothing to do with sheep or wolves or tranquillity but is simply that the total number of sheep remain constant . </S>",
    "<S> one reason he could nt see this is that he lacked any conception of the total number of his sheep ; that s because in those days there were no numbers beyond ten , just  many \" .    </S>",
    "<S> there are three morals to this tale . </S>",
    "<S> the first is that it s not enough just to ask whether a law is right or wrong  we should also ask whether it gets to the point . </S>",
    "<S> the second is more subtle : if the point escapes us , maybe it s because we lack the raw materials of thought needed to even conceive of it . </S>",
    "<S> the third is not subtle at all : learn to count !    </S>",
    "<S> we have learned to count beyond ten sheep and even beyond three dimensions , but we still are under a very stifling conceptual limitation in not being able to count beyond the two types of phenomena that we call _ classical _ and _ quantum_. this paper will set these two among many more . </S>",
    "<S> it will do this by teaching us some new ways to count cases , such as how to keep counting when the count goes below zero ! </S>",
    "<S> this will provide us with the raw materials for thought we need to clearly see some crucial points that quantum philosophy has so far missed , notably the significance , or rather the insignificance , of the wave function , and the essentially acausal nature of quantum processes .    </S>",
    "<S> quantum mechanics has revealed many puzzling patterns in nature , perhaps the most puzzling being the epr correlation . </S>",
    "<S> the explanations we hear for this phenomenon all too often resemble the farmer s spirit of tranquillity . </S>",
    "<S> we ll see that we can  factor \" a simple piece of probability theory out of physics that makes sense of things like epr in much the same way that xy = yx makes sense of the farmer s sheep counts . </S>",
    "<S> what s basically new here is that quantum phenomena in general can be represented as simple large number phenomena whose laws belong to the arithmetic of case counting .    </S>",
    "<S> but what about non - locality ? </S>",
    "<S> the collapse of the wave front ? </S>",
    "<S> quantum measurement ? </S>",
    "<S> how does the present paper fit in with the more familiar ways of interpreting the formalism of quantum mechanics ? </S>",
    "<S> let me briefly address this question .    </S>",
    "<S> it was recognized quite early that quantum mechanics bears the earmarks of a purely statistical theory . </S>",
    "<S> the schrodinger equation looks very much like the equation governing a markov process , and we actually get the schrodinger equation if we multiply the generator of a self - adjoint continuous markov chain by i. now markov processes belong entirely to the theory of probability  there s no physics in them at all . </S>",
    "<S> could it be that quantum mechanics does for mechanics what statistical mechanics did for the theory of heat ? </S>",
    "<S> can mechanics , and space and time along with it , be reduced to the statistical behavior of something simpler and more fundamental , or perhaps even to theorems in the bare science of probability itself ?    </S>",
    "<S> two obstacles have stood in the way of such a simplification .    </S>",
    "<S> the first is that probabilities ca nt be imaginary . </S>",
    "<S> it turns out that we can define imaginary probabilities in terms of negative probabilities , but that would nt seem to be of much help  have you ever counted fewer than 0 cases ? </S>",
    "<S> but then again , come to think of it , have you ever seen a pile of gold bricks with fewer than 0 bricks in it ? and yet the mathematics of negative piles of gold bricks has become indispensable for keeping accounts , especially government accounts . </S>",
    "<S> so why not  </S>",
    "<S> keep accounts \" with negative probabilities ? </S>",
    "<S> this first obstacle does nt look like it ought to stop us in our tracks for long , and if it were the only obstacle , it probably would nt .    </S>",
    "<S> the second has proved more obstinate . </S>",
    "<S> we saw that the schrodinger and markov equations have the same form , but we must next ask , do they govern the same quantities ? the answer appears to be no . </S>",
    "<S> the numbers that in the markov equation are probabilities , are the square roots of probabilities in the schrodinger equation . </S>",
    "<S> this little disparity has for sixty years kept alive the notion of the  wave function \" whose  amplitude \" squared is probability .    and this little disparity , this seeming technicality , has been the logjam that has kept quantum philosophy circling in the same stagnant pool of inadequate ideas for the last sixty years . </S>",
    "<S> the present paper aims to break up that logjam .    </S>",
    "<S> let s assume we have overcome obstacle 1 and can now work within an extended theory of probability where cases count negatively as well as positively . as we ll see , most of the math in this extended probability theory is the same as in the all - positive theory . </S>",
    "<S> now suppose we write down the schrodinger equation as a formal markov chain , in which the differential transition matrix operates on a state vector . </S>",
    "<S> the numbers in this vector are the amplitudes of the  wave function \" , which we would now like to think of as probabilities . </S>",
    "<S> the well - known problem with this construction is that the basic rule for quantum probability , the born rule , says that these amplitudes must be squared to give the probabilities that are actually observed in quantum measurement .    </S>",
    "<S> however , and now we are coming to the key idea , this chain of transition matrices we call a markov chain is only one of many ways to represent the joint probability distribution that is the markov process itself . </S>",
    "<S> another is by a chain of joint probability matrices linked by conditioning events of the form x = y . </S>",
    "<S> we define the state s of such a link as the probability matrix on x and y with the link removed . a state </S>",
    "<S> is called quantum if the distributions on x and y are identical , and a chain is called quantum if its states are quantum and its matrices are unitary . now here is the punch - line : in a quantum chain , the unlinked probabilities on x and y behave like quantum amplitudes , while the linked probabilities , which are the diagonal entries of s , are the squares of these amplitudes , and hence behave like the probabilities they are supposed to be . </S>",
    "<S> there is no wave function , only probabilities , positive and negative .    </S>",
    "<S> the link method embeds quantum mechanics in the mathematics of markov processes in such a way that quantum amplitudes are represented by unlinked probabilities and quantum measurement probabilities by linked probabilities . as we ll see , the same embedding maps classical markov chains into isomorphic representations of themselves . in linked chains of any kind , linked probabilities </S>",
    "<S> are quadratic in unlinked probabilities , but in classical chains one of the factors is constant and factors out , so probability is in effect linear in amplitude , i.e. the classical  state vector \" is the usual probability vector . </S>",
    "<S> if this sounds a bit cryptic , do nt worry , it will all be spelled out in detail .    </S>",
    "<S> most discussions of the meaning of quantum mechanics these days seem to be about the problem of the  collapse of the wave function . \" in link theory this problem simply vanishes , since there is no wave function to collapse . </S>",
    "<S> imagine if the eighteenth century caloric were still hanging around as the official theory of heat : we d be chronically plagued by ever more complicated theories explaining the collapse of the  caloric field \" when you measure an atom s energy . </S>",
    "<S> what a relief to get away from the spell of such nonsense !    </S>",
    "<S> this large - number explanation of quantum mechanics raises two basic questions : large numbers of what ? and must we buy it ?    </S>",
    "<S> the answer to the first question is implicit in the above discussion , but needs to be said simply : the things we count large numbers of are cases . </S>",
    "<S> simple arithmetic reveals that the core quantum laws , in a generalized form , are features of any probabilistic system whatsoever . </S>",
    "<S> von neumann s formulation of the born probability rule prob(p ) = trace(ps ) holds at every connection between the parts of such a system , and the dynamical rule st = ts governs every part that is connected at two places .    </S>",
    "<S> we brought up caloric to draw a parallel between our present situation and the situation in physics when it was discovered that the laws governing heat could be interpreted as statistical laws of atomic motion . </S>",
    "<S> however , there is a big difference . in the case of heat , </S>",
    "<S> the statistical theory sat on top of the newtonian theory of motion , whereas in our case there is no underlying empirical theory at all . </S>",
    "<S> probability theory is just the arithmetic of case counting , so the generalized quantum laws are like xy = yx in that their truth is assured , the only empirical issue being where and when they apply .    </S>",
    "<S> the answer to the second question is no , we do nt have to . </S>",
    "<S> however , the same can be said about the arithmetical explanation of five fields with ten sheep each . </S>",
    "<S> it s logically possible that when true tranquillity reigns , the gods always make sure that every field contains ten sheep ( presumably the age of true tranquillity is long since past ) . </S>",
    "<S> it s also logically possible that the non - local  guide wave \" explanation of quantum phenomena is the right one . with both sheep and quantum , </S>",
    "<S> the arithmetical explanation makes so much more sense that it would be most malicious of the gods to reject it just to save our old habits of thought .    </S>",
    "<S> we ll see that there is another reason to prefer the arithmetical explanation , which is that , as our discussion of markov processes suggests , it also applies to classical things like computers . </S>",
    "<S> this at last enables us to make sense of quantum measurement , which has always been a great mystery . </S>",
    "<S> quantum and classical now stand revealed as two  shapes \" made of the same stuff , so there is nothing more mysterious about their both being parts of the same process than there is about round wheels and square windows both being parts of the same car . </S>",
    "<S> the radical path also leads to a good kantian solution of hume s problem , which is that of finding causality in the order of succession , and we ll see that the choice between acausal and causal / classical thinking is to some extent a choice of analytical method , like the choice between polar and rectilinear coordinates .    but then comes the big question : what about the other shapes ? the ones other than quantum or classical that we have never before imagined , and therefore never thought to look for in nature ? </S>",
    "<S> we ll briefly touch on the big question , but it calls for a much bigger answer than we can give here , or now . </S>"
  ]
}