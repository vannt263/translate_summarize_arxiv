{
  "article_text": [
    "in this paper we aim to develop a general , model free , method for analyzing current status data using machine learning techniques .",
    "in particular , we propose a support vector machine ( svm ) learning method for estimation of the failure time expectation for current status data .",
    "svm was originally introduced by vapnik in the 1990 s and is firmly related to statistical learning theory @xcite .",
    "the choice of svms for current status data is motivated by the fact that svms can be implemented easily , have fast training speed , produce decision functions that have a strong generalization ability and can guarantee convergence to the optimal solution , under some weak assumptions @xcite .",
    "current status data is a data format where the failure time @xmath0 is restricted to knowledge of whether or not @xmath0 exceeds a random monitoring time @xmath1 .",
    "this data format is quite common and includes examples from various fields .",
    "@xcite mention a few examples including : studying the distribution of the age of a child at weaning given observation points ; when conducting a partner study of hiv infection over a number of clinic visits ; and when a tumor under investigation is occult and an animal is sacrificed at a certain time point in order to determine presence or absence of the tumor . for instance , in the last example of carcinogenicity testing , @xmath0 is the time from exposure to a carcinogen and until the presence of a tumor , and @xmath1 is the time point at which the animal is sacrificed in order to determine presence or absence of the tumor .",
    "clearly , it is difficult to estimate the failure time distribution since we can not observe the failure time @xmath0 .",
    "these examples illustrate the importance of this topic and the need to find advanced tools for analyzing such data .",
    "we present a support vector machine framework for current status data .",
    "we propose a learning method , denoted by csd - svm , for estimation of the failure time expectation .",
    "we investigate the theoretical properties of the csd - svm , and in particular , prove consistency for a large family of probability measures . in order to estimate the conditional expectation we use a modified version of the quadratic loss . using the methodology of @xcite",
    ", we construct a data dependent version of the quadratic loss .",
    "since the failure time @xmath0 is not observed , our data dependent loss function is based on the censoring time @xmath1 and on the current status indicator .",
    "finally , in order to obtain a csd - svm decision function for current status data , we minimize a regularized version of the empirical risk with respect to this data - dependent loss .",
    "there are several approaches for analyzing current status data .",
    "traditional methods include parametric models where the underlying distribution of the survival time is assumed to be known ( such as weibull , gamma , and other distributions with non - negative support ) .",
    "other approaches include semiparametric models , such as the cox proportional hazard model , and the accelerated failure time ( aft ) model ( see , for example , * ? ? ?",
    "in the cox model , the hazard function is assumed to be proportional to the exponent of a linear combination of the covariates . in the aft model",
    ", the log of the failure time is assumed to be a linear function of the covariates .",
    "several works including @xcite , @xcite and others have suggested the cox proportinal hazard model for current status data , where the cox model can be represented as a generalized linear model with a log - log link function .",
    "other works including @xcite discussed the use of the aft model for current status data and suggested different algorithms for estimating the model parameters . needless to say that both parametric and semiparametric models demand stringent assumptions on the distribution of interest which can be restrictive .",
    "for this reason , additional estimation methods are needed .    over the past two decades ,",
    "some learning algorithms for censored data have been proposed ( such as neural networks and splitting trees ) , but mostly with no theoretical justification .",
    "additionally , most of these algorithms can not be applied to current status data but only to other , more common , censored data formats . recently , several works suggested the use of svms for survival data .",
    "@xcite suggested the use of svms for survival analysis , and formulated the task as a ranking problem .",
    "shortly after , @xcite suggested the use of svms for regression problems with censored data ; this was done by asymmetrically modifying the @xmath2-insensitive loss function .",
    "both examples were empirically tested but lacked theoretical justification .",
    "@xcite proposed an empirical quantile risk estimator , which can also be applied to right censoring , and studied the estimator s performance .",
    "@xcite studied an svm framework for right censored data and proved that the algorithm converges to the optimal solution .",
    "@xcite suggested two svm - based formulations for classification problems with survival data .",
    "these examples illustrate that initial steps in this direction have already been taken . however , as far as we know , the only svm - based work that can also be applied to current status data is by @xcite which has a more computational and less theoretic nature .",
    "the authors studied the use of svm for regression problems with interval censoring and , using simulations , showed that the method is comparable to other missing data tools and performs significantly well when the majority of the training data is censored .",
    "the contribution of this work includes the development of an svm framework for current status data , the study of the theoretical properties of the csd - svm , and the development of new oracle inequalities for censored data .",
    "these inequalities , together with finite sample bounds , allow us to prove consistency and to compute learning rates .",
    "the paper is organized as follows . in section [ sec : preliminaries ] we describe the formal setting of current status data and discuss the choice of the quadratic loss for estimating the conditional expectation . in section [ sec : support - vector - machines ] we present the proposed csd - svm and its corresponding data - dependent loss function . section [ sec : theoretical - results ] contains the main theoretical results , including finite sample bounds , consistency proofs and learning rates . in section [ sec :",
    "estimation - of - the ] we illustrate the estimation procedure and show that the solution has a closed form .",
    "section [ sec : simulation - study ] contains the simulations .",
    "concluding remarks are presented in section [ sec : concluding - remarks ] .",
    "the lengthier proofs appear in appendix [ sec : appendix ] .",
    "the matlab code for both the algorithm and for the simulations can be found in the [ sub : supplementary - material ] .",
    "in this section we present the notations used throughout the paper .",
    "first we describe the data setting and then we discuss briefly loss functions and risks .",
    "assume that the data consists of @xmath3 i.i.d.random triplets @xmath4 .",
    "the random vector @xmath5 is a vector of covariates that takes its values in a compact set @xmath6 .",
    "the failure - time @xmath0 is non - negative , the random variable @xmath1 is the censoring time , the indicator @xmath7 is the current status indicator at time @xmath1 , and is contained in the interval @xmath8\\equiv\\mathcal{y}$ ] for some constant @xmath9 . for example , in carcinogenicity testing , an animal is sacrificed at a certain time point in order to determine presence or absence of the tumor . in this example",
    ", @xmath0 is the time from exposure to a carcinogen and until the presence of a tumor , @xmath1 is the time point at which the animal is sacrificed , and @xmath10 is the current status indicator at time @xmath1 ( indicating whether the tumor has developed before the censoring time , or not ) .",
    "we now move to discuss a few definitions of loss functions and risks , following @xcite .",
    "let@xmath11 be a measurable space and @xmath12 be a closed subset .",
    "then a loss function is any measurable function @xmath13 from @xmath14 to @xmath15 .",
    "let @xmath16 be a loss function and @xmath17 be a probability measure on @xmath18 .",
    "for a measurable function @xmath19 , the @xmath13-risk of @xmath20 is defined by @xmath21=\\int_{z\\times y}l\\left(z , y , f(z)\\right)dp(z , y)$ ] .",
    "a function @xmath20 that achieves the minimum @xmath13-risk is called a _",
    "bayes decision function _ and is denoted by @xmath22 , and the minimal @xmath13-risk is called the _ bayes risk _ and is denoted by @xmath23 .",
    "finally , the empirical @xmath13-risk is defined by @xmath24 .",
    "for example , it is well known ( see , for example , * ? ? ? * ) that the conditional expectation is the bayes decision function with respect to the quadratic loss .",
    "let @xmath25 be a reproducing kernel hilbert space ( rkhs ) of functions from @xmath26 to @xmath27 , where an rkhs is a function space that can be characterized by some kernel function @xmath28 . by definition ,",
    "if @xmath29 is a universal kernel , then @xmath25 is dense in the space of continuous functions on @xmath26 , @xmath30 ( see , for example , ( * ? ? ?",
    "* definition 4.52 ) ) .",
    "let us fix such an rkhs @xmath25 and denote its norm by @xmath31 and let @xmath32 be some sequence of regularization constants .",
    "an svm decision function for uncensored data is defined by : @xmath33 we recall that current status data consists of @xmath3 independent and identically - distributed random triplets @xmath4 .",
    "let @xmath34 and @xmath35 be the cumulative distribution functions of the failure time and censoring , respectively , given the covariates @xmath36 .",
    "let @xmath37 be the density of @xmath35 . for current status data ,",
    "we introduce the following identity between risks , following @xcite .",
    "we extend this notion and incorporate loss functions and covariates in the following identity .",
    "let @xmath38 be a loss function differentiable in the first variable .",
    "let @xmath39 be the derivative of @xmath13 with respect to the first variable .",
    "we would like to find the minimizer of @xmath40 over a set @xmath25 of functions @xmath20 .",
    "note that @xmath41\\\\ = & e_{z}\\left[\\int_{0}^{\\tau}\\ell(t , f(z))(1-f(t|z))dt-\\left.l(t , f(z))(1-f(t|z))\\right|_{0}^{\\tau}\\right]\\\\ = & e_{z}\\left[\\int_{0}^{\\tau}\\ell(t , f(z))(1-f(t|z))dt\\right]+e[l(0,f(z))]\\,,\\end{aligned}\\ ] ] where the equality before last follows from integration by parts .",
    "note also that @xmath42 and thus @xmath43= & e_{z , t}\\left[e_{c}\\left[\\left.\\frac{\\mathbf{{1}}\\{t > c\\}\\ell(c , f(z))}{g(c|z)}\\right|z = z , t = t\\right]\\right]\\\\ = & e_{z , t}\\left[\\int_{0}^{\\tau}\\frac{\\mathbf{{1}}\\{t > c\\}\\ell(c , f(z))g(c|z)}{g(c|z)}dc\\right]\\\\ = & e_{z , t}\\left[\\int_{0}^{\\tau}\\mathbf{{1}}\\{t > c\\}\\ell(c",
    ", f(z))dc\\right]\\\\ = & e_{z}\\left[\\int_{0}^{\\tau}\\ell(c , f(z))\\int_{0}^{\\tau}\\mathbf{{1}}\\{t > c\\}df(t|z)dc\\right]\\\\ = & e_{z}\\left[\\int_{0}^{\\tau}\\ell(c , f(z))(1-f(c|z))dc\\right]\\,.\\end{aligned}\\ ] ] this shows that the risk can be represented as the sum of two terms @xmath44+e[l(0,f(z))]$ ] .",
    "hence , in order to estimate the minimizer of @xmath40 , one can minimize a regularized version of the empirical risk with respect to the data - dependent loss function @xmath45    note that this function need not be convex nor a loss function . for the quadratic loss function ,",
    "our data - dependent loss function becomes    @xmath46    note that this function is convex but not necessarily a loss function since it can obtain negative values . in order to ensure positivity we add a constant term that does not depend on @xmath20 , and",
    "so our loss becomes @xmath47 , where for a fixed dataset of length @xmath48 @xmath49 .",
    "note that this additional term will not effect the optimization ( since @xmath50 is just a shift by a constant of @xmath51 ) and thus will be neglected here after .    in order to implement this result into the svm framework",
    ", we propose to define the csd - svm decision function for current status data by    @xmath52.\\label{eq : csd - svm}\\ ] ]    note that if the censoring mechanism is not known , we can replace the density @xmath53 with its estimate @xmath54 ; in this case our loss function becomes @xmath55 and the svm decision function is @xmath56 $ ] ( note the use of @xmath54 instead of @xmath53 in the denominator ) .",
    "we note that for current status data , the assumption of some knowledge of the censoring distribution is reasonable , for example , when it is chosen by the researcher @xcite . in other cases ,",
    "the density can be estimated using either parametric or nonparametric density estimation techniques such as kernel estimates .",
    "it should be noted that the censoring variable itself is not censored and thus simple density estimation techniques can be used in order to estimate the density @xmath53 .",
    "in this section we prove consistency of the csd - svm learning method for a large family of probability measures and construct learning rates .",
    "we first assume that the censoring mechanism is known , and thus the true density of the censoring variable @xmath53 is known . using this assumption , and some additional conditions , we bound the difference between the risk of the csd - svm decision function and the bayes risk in order to form finite sample bounds .",
    "we use this result , together with oracle inequalities , to show that the csd - svm converges in probability to the bayes risk .",
    "that is , we demonstrate that for a very large family of probability measures , the csd - svm learning method is consistent .",
    "we then consider the case in which the censoring mechanism is not known and thus the density @xmath53 needs to be estimated .",
    "we estimate the density @xmath53 using nonparametric kernel density estimation and develop a novel finite sample bound .",
    "we use this bound to prove that the csd - svm is consistent even when the censoring distribution is not known .",
    "finally we construct learning rates for the csd - svm learning method for both @xmath53 known and unknown .",
    "[ def : normalized loss]let @xmath57 be the normalized quadratic loss , let @xmath58 be its derivative with respect to the first variable , and let @xmath59 be the data - dependent version of this loss .    for simplicity",
    ", we use the normalized version of the quadratic loss .",
    "since both @xmath13 and @xmath60 are convex functions with respect to @xmath61 , then for any compact set @xmath62\\subset\\mathbb{r}$ ] , both @xmath13 and @xmath60 are bounded and lipschitz continuous with constants @xmath63 and @xmath64 that depend on @xmath65 .",
    "[ rem : loss bounds]@xmath66 for all @xmath67 and @xmath68 for all @xmath69 and for some constant @xmath70 .",
    "we need the following assumptions :    1 .",
    "the censoring time @xmath1 is independent of the failure time @xmath0 given @xmath5.[as : a1 ] 2 .",
    "@xmath1 takes its values in the interval @xmath8 $ ] and @xmath71 , for some @xmath72 [ as : a2 ] .",
    "@xmath6 is compact [ as : a3 ] .",
    "@xmath25 is an rkhs of a continuous kernel @xmath29 with @xmath73 [ as : a4 ] .",
    "define the approximation error by @xmath74    define @xmath75 and @xmath76 , where @xmath77 is defined in remark  [ rem : loss bounds ] .",
    "in this section we develop finite sample bounds assuming that the censoring density @xmath53 is known .",
    "[ theorem 1 - g known]assume that ( a[as : a1])-(a[as : a4 ] ) hold . then for fixed @xmath78 , and @xmath79 , with probability not less than @xmath80    @xmath81    where @xmath82 is the covering number of the @xmath83 of @xmath84 with respect to supremum norm and where @xmath85 is the unit ball of @xmath25 ( for further details see @xcite ) .",
    "the proof of this theorem appears in appendix [ sub : proof - of - theorem 1 ] .",
    "we now move to discuss consistency of the csd - svm learning method . by definition",
    ", @xmath17-universal consistency means that for any @xmath86 ,    @xmath87    where @xmath88 is the bayes risk .",
    "universal consistency means that ( [ eq:3 ] ) holds for all probability measures @xmath17 on @xmath18 .",
    "however , in survival analysis we have the problem of identifiability and thus we will limit our discussion to probability measures that satisfy some identification conditions . let @xmath89 be the set of all probablity measures that satisfy assumptions ( a[as : a1])-(a[as : a2 ] ) .",
    "we say that a learning method is @xmath89-universal consistent when ( [ eq:3 ] ) holds for all probability measures @xmath90 .    in order to show @xmath89-universal consistency",
    ", we utilize the finite sample bounds of theorem  [ theorem 1 - g known ] .",
    "the following assumption is also needed for proving @xmath89-universal consistency :    1 .   [ as : a5 ] @xmath91 , for all probability measures @xmath17 on @xmath18    assumptio  ( a[as : a5 ] ) means that our rkhs @xmath25 is rich enough to include the bayes decision function .",
    "[ cor - consistency ] assume the setting of theorem  [ theorem 1 - g known ] and that assumptio  ( a[as : a5 ] ) holds .",
    "let @xmath92 be a sequence such that @xmath93 and @xmath94 choose @xmath95 for some @xmath96 then the csd - svm learning method is @xmath89-universal consistent .    in theorem  [ theorem 1 - g known ] we showed that @xmath97    with probability not greater than @xmath98 .",
    "choose @xmath99 from assumption  ( a[as : a5 ] ) together with lemma  5.15 of @xcite , @xmath100 converges to zero as @xmath3 converges to infinity .",
    "clearly @xmath101 finally , from the choice of @xmath102 , it follows that both @xmath103 and @xmath104 converge to 0 as @xmath105 .",
    "hence for every fixed @xmath106    @xmath107 with probability not less than 1-@xmath98 .",
    "the right hand side of this converges to 0 as @xmath105 , which implies consistency ( * ? ? ?",
    "* lemma 6.5 ) .",
    "since this holds for all probability measures @xmath90 , we obtain @xmath89-universal consistency .",
    "in this section we form finite sample bounds for the case in which the censoring density is not known and needs to be estimated .",
    "we assume that the density of the censoring variable is estimated using nonparametric kernel density estimation . in lemma  [ lemma on density ]",
    "we construct finite sample bounds on the differnce between the estimated density @xmath54 and the true density @xmath53 . in theorem",
    "[ theorem g unknown ] we utilize this bound to form finite sample bounds for the csd - svm learning method .",
    "[ kernel of order m]we say that @xmath108 ( not to be confused with the kernel function @xmath29 of the rkhs @xmath109 is a kernel of order @xmath110 , if the functions @xmath111 are integrable and satisfy @xmath112 and @xmath113 .",
    "[ holder class ] the hlder class @xmath114 of functions @xmath115 is the set of @xmath116 times differentiable functions whose derivative @xmath117 satisfies @xmath118 for some constant @xmath119 .",
    "[ lemma on density]let @xmath108 be a kernel function of order m satisfying @xmath120 and define @xmath121where @xmath122 is the bandwidth .",
    "suppose that the true density @xmath53 satisfies @xmath123 .",
    "let us also assume that @xmath124 belongs to the @xmath125 class @xmath114 .",
    "finally , assume that @xmath126 .",
    "then for any @xmath86 , @xmath127 where @xmath128 and @xmath129 are constants , and for some @xmath130 $ ] .    the proof of the lemma is based on ( * ? ? ?",
    "* propositions 1.1 and 1.2 ) together with basic concentration inequalities ; the proof can be found in appendix [ sub : proof - of - lemma 1 ] .",
    "we would like to choose @xmath122 that minimizes the sum of @xmath131 and @xmath132 .",
    "define @xmath133 . taking the deivative of @xmath134 with respect to @xmath122 and setting to zero yields :    @xmath135    @xmath136    where @xmath137 .",
    "it can be shown that the second derivative of @xmath134 is positive which guarentees that the zero of the derivative above is the minimizer . after substituting @xmath138 in @xmath134",
    ", we obtain that @xmath139    choosing @xmath86 such that @xmath140 and substituting @xmath138 , we obtain by lemma  [ lemma on density ] that @xmath141    we now move to construct finite sample bounds for the csd - svm learning method when @xmath53 is unknown using the above lemma .",
    "we assume that @xmath54 is the kernel density estimate of @xmath142 such that the conditions of lemma  [ lemma on density ] hold .",
    "[ theorem g unknown]assume that ( a[as : a1])-(a[as : a4 ] ) hold .",
    "assume the setting of lemma  [ lemma on density ] and that @xmath143 for some @xmath72 .",
    "choose @xmath144 such that @xmath145 then for fixed @xmath146 , we have with probability not less than @xmath147 that    @xmath148    where @xmath149 .",
    "the proof of the theorem appears in appendix [ sub : proof - of - theorem 2 ] .    using the above theorem",
    "we show that under some conditions , the csd - svm decision function converges in probability to the conditional expectation .",
    "[ consistency 2]let @xmath92 be a sequence such that @xmath93 and that @xmath94 choose @xmath95 for some @xmath96 assume the setting of theorem 3 , then the csd - svm learning method is consistent .",
    "the proof of the corollary is derived similarly to the proof of corollary [ cor - consistency ] ( consistency - case i ) .      in this section",
    "we derive learning rates for cases i and ii .",
    "[ def learning rate]a learning method is said to learn with rate @xmath150 $ ] that converges to zero if for all @xmath151 and all @xmath152 $ ] , @xmath153 , where @xmath154 and @xmath155 are constants such that @xmath156 and @xmath157 .    [ theorem learning rate]assume that ( a[as : a1])-(a[as : a4 ] ) hold .",
    "choose @xmath158 and assume that there exist constants @xmath159 such that @xmath160 .",
    "additionally , assume that there exist constants @xmath161 $ ] such that @xmath162 .",
    "choose @xmath92 = @xmath163 .",
    "then    \\(i ) if @xmath53 is known , the learning rate is given by @xmath164 .",
    "\\(ii ) if @xmath53 is not known and the setup of theorem  [ theorem g unknown ] holds , then the leraning rate is given by @xmath165 .",
    "the proof of the theorem appears in appendix [ sub : proof - of - theorem 3 ] .",
    "in this section we demonstrate how to compute the csd - svm decision function with respect to the quadratic loss .",
    "in addition we show that the solution has a closed form .",
    "since @xmath166 is convex , then for any rkhs @xmath25 over @xmath26 and for all @xmath167 , it follows that there exists a unique svm solution @xmath168 .",
    "in addition , by the representer theorem @xcite , there exists constants @xmath169 such that @xmath170 .",
    "hence the optimization problem reduces to estimation of the vector @xmath144 .",
    "a more general approach will also include an intercept term @xmath171 such that @xmath172 .",
    "let @xmath173 be the feature map that maps the input data into an rkhs @xmath25 such that @xmath174 .",
    "our goal is to find a function @xmath175 that is the solution of ( [ eq : csd - svm ] ) . from the representer theorem",
    ", there exists a unique svm decision function of the form @xmath176 .",
    "define for each @xmath177 the function @xmath178 by @xmath179 .",
    "then for @xmath180 , the optimization problem reduces to : @xmath181+\\frac{1}{2}\\|w\\|^{2}\\ ] ]    @xmath182 @xmath183    this is an optimization problem under equality constraints and hence we will use the method of lagrange multipliers .",
    "the lagrangian is given by    @xmath184+\\frac{1}{2}\\|w\\|^{2}+\\sum_{i=1}^n \\alpha_{i}\\left(c_{i}-<w,\\phi(z_{i})>-b - r_{i}\\right)\\ ] ]    minimizing the original problem @xmath185 yields the following conditions for optimality : @xmath186 @xmath187    @xmath188 since these are equality constraints in the dual formulation , we can substitute them into @xmath185 to obtain the dual problem @xmath189 . by the strong duality theorem ( * ? ? ? * theorem 6.2.4 ) , the solution of the dual problem is equivalent to the solution of the primal problem . @xmath190+\\frac{1}{2}\\sum_{i=1}^n",
    "\\sum_{j=1}^n \\alpha_{i}\\alpha_{j}k(z_{i},z_{j})\\\\ + & \\sum_{i=1}^n \\alpha_{i}\\left(c_{i}-\\sum_{j=1}^n \\alpha_{j}k(z_{i},z_{j})-b-\\left(\\frac{\\alpha_{i}}{c_{\\lambda}}+c_{i}-\\frac{(1-\\vardelta_{i})}{2\\hat{g}(c_{i}|z_{i})}\\right)\\right).\\end{aligned}\\ ] ] some calculations yield : @xmath191 subject to the constraint @xmath192 and where @xmath193 .",
    "this is a quadratic programming problem subject to equality constraints .",
    "its solution satisfies : @xmath194    note that if we do not require an intercept term , the solution is @xmath195 .",
    "it is interesting to note that this solution is equivalent to the solution attained by the representer theorem for differentiable loss functions : @xmath196 ( * ? ? ?",
    "* section 5.2 ) . in our case ,",
    "@xmath197 ; hence @xmath198 and since @xmath199 we see that @xmath200 i.e. , @xmath195 .",
    "in this section we test the csd - svm learning method on simulated data and compare its performance to current state of the art . we construct four different data - generating mechanisms , including one - dimensional and multi - dimentional settings . for each data type ,",
    "we compute the difference between the csd - svm decision function and the true expectation .",
    "we compare this result to results obtained by the cox model and by the aft model . as a reference , we compare all these methods to the bayes risk .    for each data setting , we considered two cases ; : ( i ) the censoring density @xmath53 is known ; and ( ii ) the censoring density is not known . for the second",
    "setting , the distribution of the censoring variable was estimated using nonparametric kernel density estimation with a normal kernel .",
    "the code was written in matlab , using the spider library . in order to fit the cox model to current status data",
    ", we downloaded the ` icsurv ' r package @xcite . in this package ,",
    "monotone splines are used to estimate the cumulative baseline hazard function , and the model parameters are then chosen via the em algorithm .",
    "we chose the most commonly used cubic splines . to choose the number and locations of the knots",
    ", we followed @xcite and @xcite who both suggested using a fixed small number of knots and thus we placed the knots evenly at the quantiles .",
    "for the aft model , we used the ` surv reg ' function in the ` survival ' r package @xcite . in order to call r through matlab",
    ", we installed the r package rscproxy @xcite , installed the statconndcom server , and download the matlab r - link toolbox @xcite . for the kernel of the rkhs @xmath25",
    ", we used both a linear kernel and a gaussian rbf kernel @xmath201 , where @xmath202 and @xmath203 were chosen using 5-fold cross - validation .",
    "the code for the algorithm and for the simulations is available for download ; a link to the code can be found in the [ sub : supplementary - material ] .",
    "we consider the following four failure time distributions , corresponding to the four different data - generating mechanisms : ( 1 ) weibull , ( 2 ) multi - weibull , ( 3 ) multi - log - normal , and ( 4 ) an additional example where the failure time expectation is triangle shaped .",
    "we present below the csd - svm risks for each case and compare them to risks obtained by other methods .",
    "the risks are based on 100 iterations per sample size .",
    "the bayes risk is also plotted as a reference .    in setting 1 ( weibull failure - time ) , the covariates @xmath5 are generated uniformly on @xmath204,$ ] the censoring variables @xmath1 is generated uniformly on @xmath8,$ ] and the failure time @xmath0 is generated from a weibull distribution with parameters @xmath205 .",
    "the failure time was then truncated at @xmath206 .    figure  [ fig : weibull - failure - time ] compares the results obtained by the csd - svm to results achieved by the cox model and by the aft model , for different sample sizes .",
    "it should be noted that both the ph and the aft assumption hold for the weibull failure time distribution . in particular , when the ph assumption holds , estimation based on the cox regression is consistent and efficient ; hence , when the ph assumption holds , we will use the cox regression as a benchmark .",
    "figure  [ fig : weibull - failure - time ] shows that when @xmath53 is known , even though the csd - svm does not use the ph assumption or the aft assumption , the results are comparable to those of the cox regression , and are better than the aft estimates , especially for larger sample sizes . however , when @xmath53 is not known , the cox model produces the smallest risks , but its superiority reduces as the sample size grows .",
    "this coincides with the fact that when @xmath53 is not known , the learning rate of the csd - svm is slower .",
    ".[fig : weibull - failure - time ] ]    in setting 2 ( multi - weibull failure - time ) , the covariates @xmath5 are generated uniformly on @xmath204^{10},$ ] and the censoring variable @xmath1 is generated uniformly on @xmath8 $ ] , as in setting 1 .",
    "the failure time @xmath0 is generated from a weibull distribution with parameters @xmath207 .",
    "the failure time was then truncated at @xmath208 . note that this model depends only on the first three variables . in figure",
    "[ fig : multi - weibull - failure - time ] , boxplots of risks are presented .",
    "figure  [ fig : multi - weibull - failure - time ] illustrates that the csd - svm with a linear kernel is superior to the other methods , for all sample sizes and for both the cases @xmath53 known and uknown .",
    "however , since the data may be sparse in the feature space , the csd - svm with an rbf kernel might require a larger sample size to converge .    .",
    "[ fig : multi - weibull - failure - time ] ]    in setting 3 ( multi - log - normal ) , the covariates @xmath5 are generated uniformly on @xmath204^{10},$ ] @xmath1 was generated as before and the failure time @xmath0 was generated from a log - normal distribution with parameters @xmath209 .",
    "the failure time was then truncated at @xmath210 . figure  [ fig : log - normal - failure - time ] presents the risks of the compared methods .",
    "this example illustrates that for small sample sizes , the csd - svm risks are significantly superior and converge quickly to the bayes risk .",
    "as the sample size grows , the aft also converges to the bayes risk whereas the cox estimates does not , as can be seen by the very high risks they produce .",
    "note that for the log - normal distribution , even though the aft assumption is correct , the csd - svm manages to produce better or equivalent results .    .",
    "[ fig : log - normal - failure - time ] ]    in setting 4 , we considered a non - smooth conditional expectation function in the shape of a triangle .",
    "the covariates @xmath5 are generated uniformly on @xmath204,$ ] @xmath1 is generated uniformly on @xmath8 $ ] , and @xmath0 was generated according to the following @xmath211 the failure time was then truncated at at @xmath212 .    in figure",
    "[ fig : triangle box i ] , the boxplots of risks are presented . as can be seen",
    ", the csd - svm with an rbf kernel is superior in both cases , for sufficently large sample sizes .    .",
    "[ fig : triangle box i ] ]    to illustrate the flexibility of the csd - svm , we also present a graphical representation of the true conditional expectation and its estimates , as a function of the covariates .",
    "figure  [ fig : triangle plot i ] compares the true expectation to the computed estimates for the case that @xmath53 is known ; these estimates are based on the first iteration . as can be seen , the csd - svm with an rbf kernel produces the most superior results .",
    "is known ) .",
    "the true expectation is the blue line .",
    "the following estimates are compared : the csd - svm with an rbf kernel , the csd - svm with a linear kernel , cox and aft for sample sizes @xmath213.[fig : triangle plot i ] ]    to summarize , figures [ fig : weibull - failure - time]-[fig : triangle plot i ] showed that the csd - svm is comparable to other known methods for estimating the failure time distribution with current status data , and in certain cases is even better .",
    "specifically , we found that the csd - svm with an appropriate kernel was superior in three out of the four examples , especially when the true density @xmath53 is known .",
    "it should be noted that even when the assumptions of the other models were true the csd - svm estimates were comparable .",
    "additionally , when these assumptions fail to hold , the csd - svm estimates were generally better .",
    "the main advantage of the proposed svm approach is that it does not assume any parametric form and thus may be superior , especially when the assumptions of other models fail to hold .",
    "additionally , it seems that the csd - svm can perform well in higher dimensions .",
    "we proposed an svm approach for estimation of the failure time expectation , studied its theoretical properties and presented a simulation study .",
    "we believe this work demonstrates an important approach in applying machine learning techniques to current status data",
    ". however , many open questions remain and many possible generalizations exist .",
    "first , note that we only studied the problem of estimating the failure time expectation and not other distribution related quantities .",
    "further work needs to be done in order to extend the svm approach to other estimation problems with current status data .",
    "second , we assumed that the censoring is independent of the failure time given the covariates and that the censoring density is positive given the covariates over the entire observed time range .",
    "it would be worthwhile to study the consequences of violation of some of these assumptions .",
    "third , it could be interesting to extend this work to other censored data formats such as interval censoring .",
    "we believe that further development and generalization of svm learning methods for different types of censored data is of great interest .",
    "since @xmath59 is convex , it implies that there exists a unique svm solution ( see * ? ? ? * section 5.1 ) .",
    "for all distributions @xmath214 on @xmath18 , we define the svm decision function by @xmath215 we note that for an rkhs @xmath25 of a continuous kernel @xmath29 with @xmath73 ,          recall that the unit ball of @xmath25 is denoted by @xmath85 and its closure by @xmath222 ; since@xmath223 we can write @xmath224 . since @xmath6 is compact , it implies that the @xmath225 @xmath222 of the unit ball @xmath85 is compact in @xmath226 ( see * ? ? ?",
    "* corollary 4.31 ) .",
    "@xmath243+\\frac{1}{n}\\sum_{i=1}^n [ l(0,h(z_{i}))]-\\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{(1-\\delta_{i})\\ell(c_{i},f(z_{i}))}{g(c_{i}|z_{i})}\\right]-\\frac{1}{n}\\sum_{i=1}^n [ l(0,f(z_{i}))]\\right|\\\\ \\leq & \\left|\\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{(1-\\delta_{i})\\ell(c_{i},h(z_{i}))}{g(c_{i}|z_{i})}\\right]-\\frac{1}{n}\\sum_{i=1}^n",
    "\\left[\\frac{(1-\\delta_{i})\\ell(c_{i},f(z_{i}))}{g(c_{i}|z_{i})}\\right]\\right|+\\left|\\frac{1}{n}\\sum_{i=1}^n [ l(0,h(z_{i}))]-\\frac{1}{n}\\sum_{i=1}^n [ l(0,f(z_{i}))]\\right|\\\\ \\equiv & c_{n,1}+c_{n,2},\\end{aligned}\\ ] ]    where @xmath244\\right|\\\\ = & \\left|\\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{(1-\\delta_{i})}{g(c_{i}|z_{i})}\\left(\\ell(c_{i},h(z_{i}))-\\ell(c_{i},f(z_{i}))\\right)\\right]\\right|\\\\ \\leq & \\left|\\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{1}{g(c_{i}|z_{i})}\\left(\\ell(c_{i},h(z_{i}))-\\ell(c_{i},f(z_{i}))\\right)\\right]\\right|\\\\ \\leq & \\frac{1}{2k}\\left|\\frac{1}{n}\\sum_{i=1}^n \\left[\\ell(c_{i},h(z_{i}))-\\ell(c_{i},f(z_{i}))\\right]\\right|\\leq\\frac{1}{2nk}\\sum_{i=1}^n |\\ell(c_{i},h(z_{i}))-\\ell(c_{i},f(z_{i}))|\\\\ \\leq & \\frac{1}{2nk}\\sum_{i=1}^n c_{l}|h(z_{i})-f(z_{i})|\\leq\\frac{1}{2nk}\\sum_{i=1}^n c_{l}\\varepsilon=\\frac{c_{l}\\varepsilon}{2k},\\end{aligned}\\ ] ]    and where @xmath245\\right|\\\\ \\leq & \\frac{1}{n}\\sum_{i=1}^n \\left|l(0,h(z_{i}))-l(0,f(z_{i}))\\right|\\\\ \\leq & \\frac{1}{n}\\sum_{i=1}^n c_{l}|h(z_{i})-f(z_{i})|\\leq\\frac{1}{n}\\sum_{i=1}^n \\left[c_{l}\\varepsilon\\right]=c_{l}\\varepsilon\\end{aligned}\\ ] ]                                  as in ( * ? ? ?",
    "* proposition 1.1 ) , define @xmath259 $ ]",
    ". then @xmath260 , for @xmath261 are i.i.d .",
    "random variables with zero mean and with variance : @xmath262&=e_{g}\\left[\\left(\\eta_{i}(c)\\right)^{2}\\right]=e_{g}\\left[\\left(k\\left(\\frac{c_{i}-c}{h}\\right)-e_{g}\\left[k\\left(\\frac{c_{i}-c}{h}\\right)\\right]\\right)^{2}\\right]\\leq e_{g}\\left[k^{2}\\left(\\frac{c_{i}-c}{h}\\right)\\right ]   \\\\ & =    \\int_{u}{k^{2}\\left(\\frac{u - c}{h}\\right)g(u)du}\\leq g_{max}\\int_{u}k^{2}\\left(\\frac{u - c}{h}\\right)du\\stackrel{}{=g_{max}\\int_{v}k^{2}\\left(v\\right)dv = c_{1}h}\\end{aligned}\\ ] ] where the equality before last follows from change of variables and where @xmath263 . thus @xmath264=\\frac{1}{nh^{2}}e_{g}\\left[\\eta_{1}^{2}(c)\\right]\\leq\\frac{c_{1}h}{nh^{2}}=\\frac{c_{1}}{nh}.$ ]    by the cauchyschwarz inequality we have that @xmath265\\right|\\right]\\leq\\sqrt{e\\left[\\left|\\hat{g}(c)-e\\left[\\hat{g}(c)\\right]\\right|^{2}\\right]}=\\sqrt{v(\\hat{g}(c))}.$ ] hence @xmath265\\right|\\right]\\leq\\sqrt{\\frac{c_{1}}{nh}}.$]therfore , by markov s inequality ,          @xmath269\\right|+\\frac{1}{n}\\sum_{i=1}^n \\left|e\\left[\\hat{g}(c_{i})\\right]-g(c_{i})\\right|>\\epsilon+c_{2}\\cdot h^{\\beta}\\right)\\\\ \\leq & pr\\left(\\frac{1}{n}\\sum_{i=1}^n \\left|\\hat{g}(c_{i})-e\\left[\\hat{g}(c_{i})\\right]\\right|+c_{2}\\cdot h^{\\beta}>\\epsilon+c_{2}\\cdot h^{\\beta}\\right)\\\\ = & pr\\left(\\frac{1}{n}\\sum_{i=1}^n \\left|\\hat{g}(c_{i})-e\\left[\\hat{g}(c_{i})\\right]\\right|>\\epsilon\\right)\\leq\\sqrt{\\frac{c_{1}}{nh\\epsilon^{2 } } } \\end{alignedat}\\ ] ] where @xmath122 is the bandwidth .",
    "note that the proof of this theorem is similar to the proof of of theorem  [ theorem 1 - g known ] and thus we will only discuss the parts of the proof where they differ . as in theorem  [",
    "theorem 1 - g known ] , equation [ eq : abc ] , @xmath270          @xmath274+\\frac{1}{n}\\sum_{i=1}^n [ l(0,v(z_{i}))]-\\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{(1-\\delta_{i})\\ell(c_{i},f(z_{i}))}{\\hat{g}(c_{i}|z_{i})}\\right]-\\frac{1}{n}\\sum_{i=1}^n [ l(0,f(z_{i}))]\\right|\\\\ \\leq & \\left|\\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{(1-\\delta_{i})\\ell(c_{i},v(z_{i}))}{\\hat{g}(c_{i}|z_{i})}\\right]-\\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{(1-\\delta_{i})\\ell(c_{i},f(z_{i}))}{\\hat{g}(c_{i}|z_{i})}\\right]\\right|+\\left|\\frac{1}{n}\\sum_{i=1}^n [ l(0,v(z_{i}))]-\\frac{1}{n}\\sum_{i=1}^n [ l(0,f(z_{i}))]\\right|\\\\ \\equiv & c_{n,1}+c_{n,2}\\end{aligned}\\ ] ]      recall that the loss @xmath235 is bounded by @xmath250 .",
    "define @xmath279 by @xmath280+\\frac{1}{n}\\sum_{i=1}^n [ l(0,v(z_{i}))].\\ ] ] in other words , @xmath279 is the empirical risk with the true censoring density function @xmath53 .",
    "@xmath284-\\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{(1-\\delta_{i})\\ell(c_{i},v(z_{i}))}{\\hat{g}(c_{i}|z_{i})}\\right]\\right|\\\\ = & \\left|\\frac{1}{n}\\sum_{i=1}^n \\left[(1-\\delta_{i})\\ell(c_{i},v(z_{i}))\\left(\\frac{1}{g(c_{i}|z_{i})}-\\frac{1}{\\hat{g}(c_{i}|z_{i})}\\right)\\right]\\right|\\\\ \\leq & \\frac{1}{n}\\sum_{i=1}^n \\left[\\left|\\ell(c_{i},v(z_{i}))\\left(\\frac{1}{g(c_{i}|z_{i})}-\\frac{1}{\\hat{g}(c_{i}|z_{i})}\\right)\\right|\\right]\\\\ = & \\frac{b_{1}}{n}\\sum_{i=1}^n \\left[\\left|\\frac{\\hat{g}(c_{i}|z_{i})-g(c_{i}|z_{i})}{g(c_{i}|z_{i})\\hat{g}(c_{i}|z_{i})}\\right|\\right]\\leq\\frac{b_{1}}{2k^{2}n}\\sum_{i=1}^n \\left[\\left|\\hat{g}(c_{i}|z_{i})-g(c_{i}|z_{i})\\right|\\right].\\end{aligned}\\ ] ]    note that these inequalities hold for all functions @xmath285 .",
    "we would like to bound the last expression using lemma  [ lemma on density ] . by equation [ eq : h ] ,",
    "let @xmath138 , choose @xmath144 such that @xmath286 and let @xmath287 , then by lemma  [ lemma on density ]    @xmath288>\\eta\\right)\\\\ = & pr\\left(\\frac{b_{1}}{2k^{2}n}\\sum_{i=1}^n \\left[\\left|\\hat{g}(c_{i}|z_{i})-g(c_{i}|z_{i})\\right|\\right]>\\frac{b_{1}\\left(\\alpha+c_{2}\\cdot h^{\\beta}\\right)}{2k^{2}}\\right)\\\\ = & pr(\\frac{1}{n}\\sum_{i=1}^n \\left[\\left|\\hat{g}(c_{i}|z_{i})-g(c_{i}|z_{i})\\right|\\right]>\\alpha+c_{2}\\cdot h^{\\beta})\\\\ \\leq & \\sqrt{\\frac{c_{1}}{nh\\alpha^{2}}}=e^{-\\theta}. \\end{alignedat}\\ ] ]                      @xmath297 with probability not less than @xmath80 . for any compact set @xmath62\\subset\\mathbb{r}$ ] , both @xmath13 and @xmath60 are bounded and lipschitz continuous with lipschitz constants @xmath298 and @xmath299 .",
    "hence ,              recall that @xmath75 and @xmath76 , where @xmath77 is some bound on the derivative of the loss . since @xmath158 , then @xmath306 , and therefor @xmath307 .",
    "earlier we defined @xmath308 such that @xmath309 .",
    "thus , @xmath310 @xmath311 where we define @xmath312 .",
    "@xmath313+\\frac{n}{\\sqrt{\\lambda}}\\sqrt{\\frac{2\\theta}{n}}\\\\ \\leq & \\left(\\frac{p}{2}\\right)^{\\frac{-p}{1+p}}\\frac{n}{\\sqrt{\\lambda}}\\left[\\sqrt{2}\\left(\\frac{2a}{n}\\right)^{\\frac{1}{2 + 2p}}+\\frac{mp}{2n}\\left(\\frac{2a}{n}\\right)^{\\frac{1}{2 + 2p}}\\right]+\\frac{n}{\\sqrt{\\lambda}}\\sqrt{\\frac{2\\theta}{n}}\\\\ \\leq & \\left(\\frac{p}{2}\\right)^{\\frac{-p}{1+p}}\\frac{n}{\\sqrt{\\lambda}}\\left[2\\left(\\frac{2a}{n}\\right)^{\\frac{1}{2 + 2p}}+\\frac{mp}{n}\\left(\\frac{2a}{n}\\right)^{\\frac{1}{2 + 2p}}\\right]+\\frac{n}{\\sqrt{\\lambda}}\\sqrt{\\frac{2\\theta}{n } } \\end{alignedat}\\ ] ]            we would like to choose a sequence @xmath92 that will minimize the bound in ( [ eq : learning rate bound ] ) .",
    "define @xmath323.$ ] differentiating @xmath324 with respect to @xmath325 and setting to zero yields :    @xmath326=0\\\\ \\leftrightarrow\\\\ c\\gamma\\lambda^{\\gamma-1}= & \\frac{1}{2}n\\lambda^{-\\frac{3}{2}}\\left[6\\left(\\frac{2a}{n}\\right)^{\\frac{1}{2 + 2p}}+\\sqrt{\\frac{2\\theta}{n}}\\right]\\\\ \\leftrightarrow\\lambda= & \\left(\\frac{1}{2c\\gamma}n\\left[6\\left(\\frac{2a}{n}\\right)^{\\frac{1}{2 + 2p}}+\\sqrt{\\frac{2\\theta}{n}}\\right]\\right)^{\\frac{1}{\\gamma+\\frac{1}{2}}}\\propto\\left(\\frac{1}{n}^{\\frac{1}{2 + 2p}}+\\left(\\frac{1}{n}\\right)^{\\frac{1}{2}}\\right)^{\\frac{2}{2\\gamma+1}}\\\\ \\rightarrow\\lambda\\propto & n^{-\\frac{1}{(1+p)(2\\gamma+1 ) } } \\end{alignedat}\\ ] ]          @xmath330\\\\ = & cn^{-\\frac{\\gamma}{(1+p)(2\\gamma+1)}}+n\\cdot6\\left(2a\\right)^{\\frac{1}{2 + 2p}}n^{-\\frac{\\gamma}{(1+p)(2\\gamma+1)}}+n\\left(2\\theta\\right)^{\\frac{1}{2}}n^{-\\frac{2\\gamma(1+p)+p}{2(1+p)(2\\gamma+1)}}\\\\ \\leq & cn^{-\\frac{\\gamma}{(1+p)(2\\gamma+1)}}+n\\cdot6\\left(2a\\right)^{\\frac{1}{2 + 2p}}n^{-\\frac{\\gamma}{(1+p)(2\\gamma+1)}}+n\\left(2\\theta\\right)^{\\frac{1}{2}}n^{-\\frac{\\gamma}{(1+p)(2\\gamma+1)}}\\\\ = & n^{-\\frac{\\gamma}{(1+p)(2\\gamma+1)}}\\left(c+n\\cdot6\\left(2a\\right)^{\\frac{1}{2 + 2p}}+n\\left(2\\theta\\right)^{\\frac{1}{2}}\\right)\\\\ \\leq & q(1+\\sqrt{\\theta})n^{-\\frac{\\gamma}{(1+p)(2\\gamma+1 ) } } \\end{alignedat}\\ ] ]            where @xmath334 and with probability not greater than @xmath335 .",
    "choose @xmath303 , @xmath336 , @xmath337 , and define @xmath338 , then as in ( [ eq : learning rate bound ] ) , a very similar calculation shows that @xmath339 + 2\\eta.\\end{alignedat}\\ ] ]          @xmath339 + 2\\eta\\\\ \\leq & c\\lambda^{\\gamma}+\\frac{n}{\\sqrt{\\lambda}}\\left[6\\left(\\frac{2a}{n}\\right)^{\\frac{1}{2 + 2p}}+\\sqrt{\\frac{2\\theta}{n}}\\right]+\\frac{4k^{2}\\left(e^{\\frac{2\\beta+1}{2\\beta}}\\left(c_{1}\\right)^{\\frac{1}{2}}\\left(2\\beta c_{2}\\right)^{\\frac{1}{2\\beta}}+c_{2}\\kappa^{\\beta}\\right)}{b_{1}n^{\\frac{\\beta}{2\\beta+1 } } } \\end{alignedat}\\ ] ] similarly to case i , choosing @xmath343 minimizes the last bound ( note that the choice of @xmath92 does not depend on @xmath341 ) .",
    "hence that the resulting learning rate is given by              i.  d. diamond , j.  w. mcdonald , and i.  h. shah .",
    "proportional hazards models for current status data : application to the study of differentials in age at weaning in pakistan .",
    "_ demography _ , 230 ( 4):0 607620 , 1986 .",
    "antonio eleuteri and azzam f.  g. taktak .",
    "support vector machines for survival regression . in elia biganzoli , alfredo vellido , federico ambrogi , and roberto tagliaferri , editors , _ computational intelligence methods for bioinformatics and biostatistics _",
    ", number 7548 in lecture notes in computer science , pages 176189 .",
    "springer berlin heidelberg , 2012 .",
    "n.  jewell and m.  van  der laan .",
    "current status data : review , recent developments and open problems . in n.",
    "balakrishnan and c.r .",
    "rao , editors , _ handbook of statistics , advances in survival analysis _ , number  23 , pages 625642 .",
    "elsevier , 2004 .",
    "f.m . khan and v.b .",
    "support vector regression for censored data ( svrc ) : a novel tool for survival analysis . in _",
    "eighth ieee international conference on data mining , 2008 .",
    "icdm 08 _ , pages 863868 , december 2008 .",
    "v.  van  belle , k.  pelckmans , j.a.k .",
    "suykens , and s  van  huffel . support vector machines for survival analysi . in _ proceedings of the third international conference on computational intelligence in medicine and healthcare ( cimed2007 )",
    "_ , plymouth ( uk ) , 2007 ."
  ],
  "abstract_text": [
    "<S> current status data is a data format where the time to event is restricted to knowledge of whether or not the failure time exceeds a random monitoring time . </S>",
    "<S> we develop a support vector machine learning method for current status data that estimates the failure time expectation as a function of the covariates . in order to obtain the support vector machine decision function , </S>",
    "<S> we minimize a regularized version of the empirical risk with respect to a data - dependent loss . </S>",
    "<S> we show that the decision function has a closed form . using finite sample bounds and novel oracle inequalities , we prove that the obtained decision function converges to the true conditional expectation for a large family of probability measures and study the associated learning rates </S>",
    "<S> . finally we present a simulation study that compares the performance of the proposed approach to current state of the art .    and </S>"
  ]
}