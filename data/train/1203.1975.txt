{
  "article_text": [
    "the analysis of data consisting of curves or other types of functions , rather than scalars or vectors , is increasingly common in statistics ( ramsay & silverman , 2005 ) .",
    "many problems in this area involve modeling curves as functions of other curves .",
    "for example , figure [ fig : sample_curves](a ) shows daily trajectories of oxides of nitrogen in the city of sacramento , california , for 52 summer days in the year 2005 , and figure fig : sample_curves(b ) shows the corresponding trajectories of ozone concentration . the goal is to predict ozone concentration from oxides of nitrogen .",
    "functional linear regression models are normally used for this type of problems ( ramsay & silverman , 2005 , ch .",
    "recent papers have studied different aspects of the functional linear regression model ( yao et al . , 2005 ; cai & hall , 2006 ; hall & horowitz , 2007 ; crambes et al . , 2009 ; james et al . , 2009 ) . however , a characteristic feature of functional data that has not been widely investigated in a regression context is phase variability .",
    "functional samples often present a few distinct features , such as peaks and valleys , which vary in amplitude and location from curve to curve , as it is clear in figure [ fig : sample_curves ] .",
    "functional linear regression is usually based on functional principal components , which are well suited for fitting amplitude variability but not for location or phase variability .",
    "it may take an inordinate number of principal components to account even for very basic phase - variability processes ( ramsay & silverman , 2005 , ch .",
    "a more efficient strategy is to model amplitude and phase variability separately : the former using traditional functional principal components and the latter using warping models .",
    "this approach is more efficient , because the combined model often provides a better fit with fewer parameters than the classical principal component decomposition .",
    "it is also more informative , because it provides direct information about the warping process , which classical principal components only do indirectly .",
    "several warping methods have been proposed over the years ( gervini & gasser , 2004 , 2005 ; james , 2007 ; kneip et al . , 2000 ; kneip & ramsay , 2008 ; liu & mller , 2004 ; ramsay & li , 1998 ; tang & mller , 2008 , 2009 ; wang & gasser , 1999 ) .",
    "common functional linear regression models inherit the problems of functional principal components in presence of phase variability .",
    "although a high - dimensional model based on a large number of principal components can provide a good fit to the data , the problem again is one of efficiency and interpretability , not just minimizing prediction error .",
    "it is usually hard to extract specific information about phase variability from a traditional functional regression model because the two sources of variability , phase and amplitude , are confounded in the model .",
    "the curves shown in figure [ fig : sample_curves ] , for example , show peaks that vary not only in amplitude but also in location .",
    "it is reasonable to hypothesize that a large peak in oxides of nitrogen will be followed by a large peak in ozone concentration , and also that an early peak in oxides of nitrogen will be followed by an early peak in ozone level , and vice - versa .",
    "perhaps there may also be an interaction between timing and amplitude of the peaks",
    ". a common functional linear regression model of sufficiently high dimension will be able to fit these data well from the point of view of prediction error , but will not provide clear answers to these questions . a regression model that explicitly incorporates a warping component and does not confound the two sources of variability",
    "will be more useful for this , and that is what we propose in this paper .",
    "consider a sample of functions @xmath0 , ",
    ", @xmath1 , where @xmath2 is the covariate and @xmath3 the response , with @xmath4 and @xmath5 , and @xmath6 and @xmath7 closed intervals in @xmath8 . the functions @xmath2 and @xmath3",
    "are usually not directly observable ; instead we observe discretizations of them , with added random noise , at time grids @xmath9 and @xmath10 .",
    "thus the observed data consist of vectors @xmath11 , with @xmath12 and @xmath13 with elements @xmath14we will assume that the measurement errors @xmath15 and @xmath16 are independent with @xmath17 and @xmath18 .    the kind of curves we have in mind for our model will present a relatively small number of peaks and valleys that systematically appear in all curves but vary in amplitude and location . then @xmath19 and @xmath20 can be thought of as compound processes @xmath21where @xmath22 and @xmath23 account for amplitude variability and @xmath24 and @xmath25 account for phase variability .",
    "the @xmath26s and the @xmath27s are monotone increasing warping functions with @xmath28 and @xmath29 .",
    "the aligned processes @xmath22 and @xmath23 follow principal - component decompositions @xmath30with @xmath31 and @xmath32 orthonormal functions in @xmath33 and @xmath34 , respectively , and @xmath35 and @xmath36 uncorrelated zero - mean random variables .",
    "a few comments about ( [ eq : decomp_x])([eq : kl - model - y ] ) are in order , because models ( [ eq : decomp_x ] ) and ( [ eq : decomp_y ] ) may seem unidentifiable and models ( [ eq : kl - model - x ] ) and ( [ eq : kl - model - y ] ) may seem too restrictive for finite @xmath37 and @xmath38 .",
    "these issues are extensively discussed in kneip & ramsay ( 2008 , sec .",
    "2.3 ) and in the supplementary material .",
    "proposition 1 in kneip & ramsay ( 2008 ) shows that if the @xmath39s have at most @xmath40 peaks and valleys and their derivatives @xmath41 have at most @xmath40 zeros , then @xmath42 admits the decomposition @xmath43 for some @xmath44 , where the @xmath45s are non - random basis functions , the @xmath46s are random coefficients , and the @xmath47s are warping functions . orthogonalizing the @xmath45s one obtains model ( [ eq : kl - model - x ] ) .",
    "then @xmath37 in ( [ eq : kl - model - x ] ) and @xmath38 in ( [ eq : kl - model - y ] ) need not be large if the number of features to be aligned is small .",
    "the identifiability of ( [ eq : decomp_x ] ) and ( [ eq : decomp_y ] ) given amplitude models ( [ eq : kl - model - x ] ) and ( [ eq : kl - model - y ] ) and given certain conditions on the warping family @xmath48 is shown in the supplementary material . if the summations in ( [ eq : kl - model - x ] ) and ( eq : kl - model - y ) were allowed to be infinite , then ( [ eq : decomp_x ] ) and ( [ eq : decomp_y ] ) would be unidentifiable .",
    "the practical effect of large @xmath37 and @xmath38 in ( [ eq : kl - model - x ] ) and ( [ eq : kl - model - y ] ) is that the sample curves tend to present a large and unequal number of features , and then it does not make sense to try to align them ; in such cases amplitude and phase variability essentially become indistinguishable .",
    "samples like that do occur in practice , but the methods we propose in this paper are not intended for those situations .    the warping functions @xmath24 and @xmath25 will be modelled as monotone hermite splines ( fritsch & carlson , 1980 ) .",
    "although other families are possible , such as integrated splines ( ramsay , 1988 ) , monotone splines ( ramsay & li , 1998 ) and constrained b - splines ( brumback & lindstrom , 2004 ) , monotone hermite splines are better suited for the regression approach proposed here .",
    "details about this family of warping functions are given in appendix [ app : hermite ] .",
    "we only mention here that , like other spline families , this is a finite - dimensional semiparametric family determined by a knot sequence chosen by the user .",
    "thus , the family @xmath24 will be determined by a knot sequence @xmath49 of strictly increasing points in @xmath6 , and each @xmath50 will be determined by a corresponding sequence @xmath51 of basis coefficients which satisfy @xmath52 for @xmath53 . similarly , the family @xmath25 will be determined by a knot sequence @xmath54 of strictly increasing points in @xmath7 and each @xmath55 will be determined by basis coefficients @xmath56 which satisfy @xmath57 for @xmath58 .",
    "the dual role of the @xmath51s and the @xmath56s as basis coefficients and as values of @xmath50 and @xmath59 at the knots is what makes hermite splines appealing .",
    "it is natural then to choose the knot sequences @xmath60 and @xmath61 to roughly correspond to the average location of the main features of the @xmath62s and the @xmath63s .",
    "like @xmath37 and @xmath38 in ( [ eq : kl - model - x ] ) and ( [ eq : kl - model - y ] ) , the dimensions @xmath64 and @xmath65 need not be large , since they will roughly correspond to the number of peaks and valleys of the @xmath39s and the @xmath63s , which will not be large for the type of applications we envision .    unlike landmark registration , where the @xmath51s and the @xmath56s are individually estimated curve by curve",
    ", we will treat the @xmath51s and the @xmath56s as latent random effects , so they will not be estimated directly .",
    "this is a big advantage in practice , since individual estimation of the @xmath51s and the @xmath56s is difficult when the number of curves is large or when the curves are sparsely sampled .",
    "a minor complication is that the @xmath51s and the @xmath56s are constrained to be monotone increasing in @xmath6 and @xmath66 , respectively , so for convenience we will work with their jupp transforms @xmath67 and @xmath68 instead , which are unconstrained vectors ; the jupp transform is defined in appendix app : hermite .    since the warping functions @xmath69 and @xmath70 are determined by the random effects @xmath67 and @xmath71 , and the amplitude functions @xmath72 and @xmath73",
    "are determined by the random effects @xmath74 and @xmath75 , we can specify an indirect regression model of the @xmath63s on the @xmath39s via the random effects : @xmath76 = \\left [   \\begin{array}{c } \\mathbf{0 } \\\\",
    "\\mathbf{\\theta } _ { y0}\\end{array}\\right ] + \\mathbf{a}\\left ( \\left [   \\begin{array}{c } \\mathbf{u}_{i } \\\\",
    "\\mathbf{\\theta } _ { xi}\\end{array}\\right ] -\\left [   \\begin{array}{c } \\mathbf{0 } \\\\",
    "\\mathbf{\\theta } _ { x0}\\end{array}\\right ] \\right ) + \\mathbf{e}_{i } ,   \\label{eq : reg - model}\\]]where @xmath77 is the @xmath78 regression matrix and @xmath79 is an error term , which we assume @xmath80 with @xmath81 diagonal . for interpretability we split @xmath77 into four blocks corresponding to @xmath74 , @xmath67 , @xmath75 and @xmath68 : @xmath82 , \\]]with @xmath83 , @xmath84 , @xmath85 and @xmath86 . then ( [ eq : kl - model - x ] ) , ( [ eq : kl - model - y ] ) and ( [ eq : reg - model ] ) imply @xmath87where @xmath88 , @xmath89 , @xmath90 and @xmath91 .",
    "thus , for example , @xmath92 implies that @xmath93 and then the amplitude variability of the responses is unrelated to the time variability of the covariates ; similarly , @xmath94 implies that @xmath95 and then the time variability of the responses is unrelated to the amplitude variability of the covariates .",
    "models ( [ eq : kl - model - x ] ) and ( [ eq : kl - model - y ] ) depend on functional parameters that need to be estimated : the mean functions @xmath96 and @xmath97 and the principal components @xmath98 and @xmath99 .",
    "we will do that via b - splines .",
    "let @xmath100 be a b - spline basis in @xmath33 and @xmath101 a b - spline basis in @xmath34 .",
    "let @xmath102 , @xmath103 , @xmath104 and @xmath105 , for @xmath106 , @xmath107 , @xmath108 and @xmath109 .",
    "the orthogonality restrictions on the @xmath110s and the @xmath111s can be expressed as @xmath112 and @xmath113 , where @xmath114\\in \\mathbb{r}^{q_{1}\\times p_{1}}$ ] , @xmath115\\in \\mathbb{r}^{q_{2}\\times p_{2}}$ ] , @xmath116 and @xmath117 .",
    "if the curves @xmath118 and @xmath119 were observed on dense time grids and individual smoothing were possible , the spline coefficients and the rest of the model parameters could be estimated by least squares . however , we are more interested in applications where the trajectories are not densely sampled",
    ". then we will treat @xmath74 , @xmath75 , @xmath67 and @xmath68 as latent variables and estimate the model parameters by maximum likelihood .",
    "we assume @xmath120 is jointly multivariate normal of dimension @xmath121 , with mean and covariance given by @xmath122 , \\ \\",
    "\\mathbf{\\sigma } _ { w}=\\left [   \\begin{array}{cc } \\mathbf{\\lambda } & \\mathbf{\\sigma } _ { u\\theta _ { x } } \\\\   \\mathbf{\\sigma } _ { u\\theta _",
    "{ x}}^{t } & \\mathbf{\\sigma } _ { \\theta _ { x}}\\end{array}\\right ] , \\]]where @xmath123 the jupp transform of the knot vector @xmath60 and @xmath124 .",
    "this and model ( [ eq : reg - model ] ) imply that @xmath125 is multivariate normal of dimension @xmath126 with mean and covariance given by @xmath127 , \\ \\",
    "\\mathbf{\\sigma } _ { z}=\\mathbf{a\\sigma } _ { w}\\mathbf{a}^{t}+\\mathbf{\\sigma } _",
    "{ e},\\]]where @xmath128 is the jupp transform of the knot vector @xmath61 .",
    "thus @xmath129 with @xmath130 , where @xmath131 $ ] and @xmath132 the @xmath133 upper - left diagonal block of @xmath81 . since @xmath134 has to be diagonal by model ( [ eq : kl - model - y ] ) , and @xmath81 was assumed diagonal , it follows that @xmath135 must be diagonal , which imposes an additional restriction on the parameters .    to summarize , the parameters of this model are : the regression matrix @xmath77 , the residual covariance matrix @xmath81 , the covariance matrix @xmath136 of the explanatory random effects @xmath137 , the spline coefficients @xmath138 , @xmath139 , @xmath140 and @xmath141 of the functional parameters , and the variances @xmath142 and @xmath143 of the random noise in ( [ eq : raw_data_x ] ) and ( [ eq : raw_data_y ] ) .",
    "the derivation of the likelihood function and the em algorithm to compute these estimators are discussed in appendix [ app : likelihood ] and in the supplementary material .",
    "in addition to the model parameters there are meta - parameters that need to be chosen by the user , such as the dimension and knot placement of the b - spline bases for the functional parameters .",
    "this can be done either subjectively or by cross - validation .",
    "since the method ` borrows strength ' across curves , it is possible to use a larger number of knots than would be practical for single - curve smoothing .",
    "the other meta - parameters that need to be specified are the number of components in models ( [ eq : kl - model - x ] ) and ( [ eq : kl - model - y ] ) , @xmath37 and @xmath38 , and the warping dimensions @xmath64 and @xmath65 .",
    "as already discussed , these quantities should roughly correspond to the number of salient features of the @xmath39s and the @xmath63s .",
    "in addition to parameter estimation , it is usually of interest to predict a response curve for a given covariate curve .",
    "this can be done in a straightforward way .",
    "given a covariate data vector @xmath144 , obtained by discretizing a covariate curve @xmath145 on some time grid , the predictors @xmath146 and @xmath147 of the response random effects are given by @xmath148 and @xmath149 , which under model ( [ eq : reg - model ] ) come down to @xmath150 and @xmath151 . with @xmath146 and @xmath147 we compute @xmath152 and @xmath153 respectively , and then @xmath154 .",
    "consider now the asymptotic distribution of @xmath155 when the number of curves @xmath156 goes to infinity . for simplicity",
    ", we will assume that the time grids are equal for all individuals , which makes the raw data vectors @xmath11 independent and identically distributed .",
    "we will also assume that the functional parameters belong to the spline space used for estimation , whose dimension is held fixed .",
    "the asymptotic analysis is not entirely straightforward due to the parameter constraints .",
    "for this reason we will use the results of geyer ( 1994 ) . since we are only interested in the marginal asymptotic distribution of @xmath155 and not in the asymptotic covariance between @xmath155 and the rest of the parameters , we can assume without loss of generality that @xmath81 , @xmath138 , @xmath139 , @xmath140 , @xmath141 , @xmath142 and @xmath143 are fixed and known , because this assumption does not alter the asymptotic covariance matrix of @xmath155 .",
    "however , in principle we can not assume that @xmath136 is fixed and known because @xmath136 is part of the condition that @xmath135  be diagonal .",
    "so we will derive the joint asymptotic distribution of @xmath155 and @xmath157 , even though we are only interested in the marginal distribution of @xmath155 .",
    "the parameter of interest is then , in vector form , @xmath158 ,   \\label{eq : theta}\\]]where @xmath159 denotes the @xmath160 of the lower - triangular part of @xmath136 , including the diagonal .",
    "the dimension of @xmath161 is then @xmath162 .",
    "the restriction that @xmath163 be diagonal can be expressed as a system of @xmath164 constraints of the form @xmath165 , where @xmath166 and @xmath167 is the @xmath168th row of @xmath77 .",
    "the functions @xmath169 can be stacked together into a single vector - valued function @xmath170 , and the constrained parameter space can be expressed as @xmath171 .",
    "the additional condition that @xmath172 be positive definite does not alter the asymptotic distribution of the estimator because @xmath136 lies in the interior of this space , not on the border .",
    "let @xmath173 be the true value of the parameter @xmath161 .",
    "since @xmath174 is continuously differentiable , the tangent cone of @xmath175 at @xmath173 is @xmath176 , where @xmath177 is the differential ( rockafellar & wets , 1998 , ch .",
    "the asymptotic distribution of the constrained estimator @xmath178 is simple in this case : it is just the usual asymptotic normal distribution of an unconstrained maximum likelihood estimator , projected on @xmath179 .",
    "specifically , let @xmath180and @xmath181 ,   \\label{eq : u}\\]]where @xmath182 is the duplication matrix that satisfies @xmath183 ( magnus & neudecker , 1999 , ch .  3 ) .",
    "it is shown in the supplementary material that @xmath184 is the likelihood score function @xmath185 at @xmath186 .",
    "let @xmath187 , which is an @xmath188 matrix of rank @xmath189 with rows @xmath190,\\]]where @xmath79 is the @xmath168th canonical vector in @xmath191 .",
    "let @xmath192 be an orthogonal @xmath193 matrix of rank @xmath194 such that @xmath195 , which can be computed for instance via the singular value decomposition of the orthogonal projector @xmath196 ; this matrix is not unique but theorem [ theo : asymp ] below is invariant under the choice of @xmath192 .",
    "[ theo : asymp]under the above conditions , the asymptotic distribution of @xmath197 is @xmath198 where @xmath199 .",
    "matrix @xmath200 in theorem [ theo : asymp ] is fisher s information matrix for this model and can be estimated by @xmath201where the ` hat '  in @xmath202 denotes that the true parameters in ( eq : u ) are replaced by their estimators .",
    "the proof of theorem theo : asymp is given in the appendix .",
    "the assumption that the time grids were equal for all individuals was a simplification to make the data vectors @xmath203 , and consequently the likelihood scores ( [ eq : u ] ) , identically distributed . in many applications ,",
    "however , this will not be the case and the time grids will be unequal , giving @xmath204 and @xmath13 which are still independent but not identically distributed due to the different dimensions .",
    "usually this does not affect the final asymptotic result as long as ( eq : v ) does not become degenerate , as shown for instance by pollard ( 1990 , ch .",
    "11 ) in the context of regression with non - random covariates .",
    "although the fisher information matrix @xmath200 as such does not exists , ( eq : m ) and ( [ eq : n ] ) and consequently ( [ eq : u ] ) and ( [ eq : v ] ) can still be computed with @xmath203s of unequal dimensions .",
    "the statement of theorem [ theo : asymp ] should then be re - expressed as @xmath205 in distribution .",
    "to study the finite - sample accuracy of the proposed estimators we simulated data from the following models :    * model 1 : a one - dimensional amplitude and warping model , with @xmath206 , @xmath207 , @xmath208 and @xmath209 , for @xmath210 and @xmath211 in @xmath212 $ ] , where @xmath213 denotes the @xmath214 density function .",
    "the warping functions followed hermite spline models with knots @xmath215 and @xmath216 .",
    "thus , although @xmath96 and @xmath217 have two peaks , phase and amplitude variability are concentrated on the main peak .",
    "the regression matrix @xmath77 was the identity matrix , so there was no relationship between covariate phase variability and response amplitude variability , or vice versa , in this model .",
    "the other parameters were @xmath218 , @xmath219 , and @xmath220 .",
    "* model 2 : same as model 1 but with a non - diagonal @xmath77 ; specifically , @xmath221 and @xmath222 , so there was a relationship between covariate phase variability and response amplitude variability , and vice versa , in this model .",
    "* model 3 : a two - dimensional amplitude and warping model , with @xmath223 , @xmath97 , @xmath224 and @xmath225 as in model 1 , @xmath226 the function @xmath227 orthogonalized with @xmath228 , and @xmath225 the function @xmath229 orthogonalized with @xmath225 .",
    "the warping functions followed hermite spline models with knots @xmath230 and @xmath231 .",
    "this model , then , has amplitude and phase variability at both peaks of @xmath223 and @xmath97 .",
    "the regression matrix @xmath77 was the identity , and the other parameters were @xmath232 , @xmath233 , and @xmath234 . *",
    "model 4 : same as model 3 but with a non - diagonal regression matrix @xmath77 , with blocks @xmath235 and @xmath236 . * model 5 : a one - dimensional amplitude model like model 1 but with warping functions that do not follow a regression model and do not belong to the hermite - spline family ; they belonged to a generic b - spline family with monotone increasing coefficients , which produces monotone increasing functions ( brumback & lindstrom , 2004 ) . specifically , if @xmath237 are cubic b - splines with 7 equally - spaced knots in @xmath238 and @xmath239 is such that @xmath240 , the identity , then we generated @xmath241 and took @xmath242 , with @xmath243 and @xmath244 the coefficients of @xmath245 sorted in increasing order .",
    "the inverse warping functions of the responses , the @xmath246s , were generated in an analogous way and were independent of the @xmath247s .",
    "* model 6 : a two - dimensional amplitude model like model 3 with a non - hermite warping model like model 5 .",
    "two sample sizes , @xmath248 and @xmath249 , were considered for each model .",
    "each scenario was replicated 500 times . in all cases",
    "the time grids @xmath250 and @xmath251 were random and irregular , with @xmath252 and @xmath253 uniformly distributed between 10 and 20 , and independent of one another , and @xmath254 and @xmath255 uniformly distributed on @xmath212 $ ] .    for each sample we computed the proposed warped functional regression estimator using cubic b - splines with 10 equally spaced knots for the functional parameters , with the number of principal components @xmath37 and @xmath38 equal to the true model quantities , that is , @xmath256 for models 1 , 2 and 5 , and @xmath257 for models 3 , 4 and 6 .",
    "the specification of the warping functions , although always in a hermite - spline family , varied from model to model . for models 1 and 2 we used the same family used for estimation . for models 3 and 4 ,",
    "however , we used hermite - spline families with single knots at @xmath258 and @xmath259 , so as to study the behavior of the estimator when the number of warping knots is underspecified . for model 5 we used hermite splines with knots at @xmath230 and @xmath231 , and for model 6 we used hermite splines with knots at @xmath258 and @xmath260 ; this allows us to study the advantages of doing some kind of warping as opposed to not doing any warping at all , since the true warping processes of models 5 and 6 do not follow a regression model and do not belong to the hermite spline family .    for comparison we also computed ordinary functional regression estimators based on principal components , as in e.g.  mller et al .",
    "( 2008 ) , with the difference that the principal components were computed by maximum likelihood via b - spline models , as in james et al .",
    "( 2000 ) , rather than by kernel smoothing .    as measures of performance we computed bias and root mean squared errors of @xmath261 , @xmath262 , @xmath263 , @xmath264 and @xmath265 .",
    "we defined as ` bias ' of @xmath266 the quantity @xmath267^{2}\\mathrm{ds})^{1/2}$ ] and as ` root mean squared error ' the quantity @xmath268\\mathrm{ds})^{1/2}$ ] . for @xmath263 and @xmath261 the definitions were analogous , with double integrals for the latter .",
    "for the principal component estimators , which have undefined signs , we actually computed the bias and root mean squared errors of the bivariate functions @xmath269 and @xmath270 , which are sign - invariant .",
    "these are reported in tables [ tab : simulations_1_50 ] and tab : simulations_1_100 ; for @xmath266 and @xmath271 the quantities have been multiplied by 10 to eliminate leading zeros .",
    ".simulation results . bias and root mean squared errors of warped functional regression ( w ) and ordinary functional regression ( o ) for sample size @xmath248 .",
    "[ cols=\"^,^,^,^,^,^,^,^,^,^ \" , ]     table [ tab : simulations_3 ] shows the results .",
    "there are two aspects of the asymptotics that we are trying to assess : the adequacy of the normal approximation and the adequacy of the variance estimators .",
    "the first aspect can be best assessed using the true variance in the test statistics , so the variance estimation error is not a confounding factor . in this",
    "regard we see in table [ tab : simulations_3 ] that the asymptotic approximation is good even for @xmath248 , both for the global @xmath272-test and for the marginal @xmath273-tests . in the more realistic cases where the variance is estimated",
    ", we see that bootstrap variance estimators generally work better than the asymptotic - variance formula ; although both underestimate the true variances , bootstrap tends to underestimate them less , especially for random time grids .",
    "ground - level ozone is an air pollutant known to cause serious health problems . unlike other pollutants",
    ", ozone is not emitted directly into the air but is a result of complex chemical reactions in the atmosphere that include , among other factors , volatile organic compounds and oxides of nitrogen .",
    "oxides of nitrogen are emitted by combustion engines , power plants and other industrial sources .",
    "the modeling of ground - level ozone formation has been an active topic of air - quality studies for many years .    in this article",
    "we will use data from the california environmental protection agency online database .",
    "hourly concentration of pollutants at many locations in california are available for the years 19802009 .",
    "we will analyze trajectories of oxides of nitrogen ( nox ) and ozone ( o3 ) in the city of sacramento ( site 3011 in the database ) in the summer of 2005 .",
    "we omit weekends and holidays because nox and o3 levels are substantially lower and follow different patterns .",
    "we also removed some outlying trajectories , so the final sample consisted of 52 days between june 6 and august 26 , shown in figure [ fig : sample_curves ] .",
    "both nox and o3 trajectories follow simple regular patterns .",
    "nox curves tend to peak around 7 am , and o3 curves around 2 pm .",
    "therefore we fitted warped regression models with single warping knots , trying several values of @xmath274 and @xmath275 around 7 am and 2 pm respectively .",
    "the results were similar in all cases ; the estimators reported here correspond to @xmath276 and @xmath277 . as basis functions we used cubic b - splines with 7 equally spaced knots , one knot every 3 hours",
    "; we also tried 10 knots but the results were not substantially different .",
    "three warped regression models were fitted : _ ( i ) _ a model with _ _  _ _ one principal component for @xmath278 and one for @xmath279 , _ ( ii ) _ a model with _ _  _ _ two principal components for @xmath278 and one for @xmath279 , and _",
    "( iii ) _ a model with one principal component for @xmath278 and two for @xmath279 .",
    "the log - likelihood values were 44.44 , 45.21 and 52.04 , respectively .",
    "the second model did not seem to represent much of an improvement over the first one , so we discarded it . for models _",
    "( i ) _ and _ ( iii ) _ the estimated regression coefficients and the bootstrap standard deviations , based on 200 resamples , were @xmath280 , \\ \\",
    "\\mathrm{std}(\\mathbf{\\hat{a})}=\\left [   \\begin{array}{cc } 0.07 & 0.02 \\\\   0.08 & 0.06\\end{array}\\right ] , \\\\",
    "\\mathbf{\\hat{a } } & = & \\left [   \\begin{array}{cc } 0.36 & 0.12 \\\\   0.01 & 0.02 \\\\   0.18 & 0.54\\end{array}\\right ] , \\ \\",
    "\\mathrm{std}(\\mathbf{\\hat{a})}=\\left [   \\begin{array}{cc } 0.08 & 0.06 \\\\   0.04 & 0.10 \\\\   0.06 & 0.11\\end{array}\\right ] .\\end{aligned}\\ ] ]    for model ( iii ) the coefficients of the second principal component of the response , @xmath281 and @xmath282 , are not significant , while for model ( i ) all coefficients are significant even allowing for underestimation of the standard deviations , with the possible exception of @xmath281 which is a borderline case . for this reason we prefer",
    "( i ) as our final model . to interpret the principal components , figure [ fig : wfr_fit](a )",
    "shows @xmath266 and @xmath283 for some constant @xmath284 , and figure [ fig : wfr_fit](b ) shows @xmath271 and @xmath285 for another constant @xmath286 .",
    "both principal components are shape components : curves with positive scores tend to have sharper features than the mean while curves with negative scores tend to have flatter features than the mean .",
    "the fact that the diagonal coefficients of @xmath155 are positive indicates that the component scores @xmath287 and @xmath288 are positively correlated , as figure [ fig : wfr_fit](c ) shows , and the warping landmarks @xmath289 and @xmath290 , which can roughly be interpreted as peak locations , are also positively correlated , as figure [ fig : wfr_fit](f ) shows .",
    "amplitude and warping factors are also positively cross - correlated , since the off - diagonal elements of @xmath155 are also positive .",
    "in particular @xmath291 is highly significant , so late nox peaks tend to be associated with high peaks of o3 and vice - versa , as figure [ fig : wfr_fit](d ) shows .",
    "an ordinary functional regression fit is shown in figure [ fig : flr_fit ] ; the plot shows @xmath266 , @xmath271 , @xmath292 and @xmath293 for a three - component model , or overall dimension 9 .",
    "a two - component model , of overall dimension 4 and thus comparable to the warped regression model , would correspond to the upper four panels of figure [ fig : flr_fit ] .",
    "time variability in the explanatory curves is explained by the second @xmath278-component ( figure fig : flr_fit(c ) ) , but phase variability in the response curves is not accounted for until the third component ( figure [ fig : flr_fit](f ) ) , so it really takes a 9-dimensional ordinary regression model to explain the phase - variability features that a 4-dimensional warped model would explain . and",
    "the predominantly time - related principal components , figure fig : flr_fit(c , f ) , are also associated with some kinds of amplitude variability .",
    "likewise , principal components that are predominantly amplitude - related , like the first @xmath278-component , figure [ fig : flr_fit](a ) , are somewhat influenced by time variability .",
    "this blurring of the components is avoided by warped functional regression , which neatly separates the sources of variability and offers not only a more easily interpretable model but also a lower - dimensional one .",
    "this research was partially supported by a grant from the national science foundation .",
    "supplementary material available online includes a more thorough discussion of model identifiability , the derivation of the em algorithm for estimation , detailed derivation of formulae involved in the asymptotic distribution of the estimator , and a detailed treatment of monotone hermite splines .",
    "in this section we explain how the warping functions @xmath50 are constructed ; the @xmath59s are constructed in a similar way .",
    "let @xmath294 $ ] and @xmath295 be a sequence of @xmath296 knots in @xmath6 .",
    "define the basis functions @xmath297 and @xmath298 as follows : let @xmath299 and @xmath300 ; then @xmath301@xmath302for @xmath303 , @xmath304@xmath305@xmath306for @xmath303 , and @xmath307the function @xmath308where @xmath309 and @xmath310 , is a differentiable piecewise - cubic function that satisfies @xmath311 and @xmath312 for @xmath303 .",
    "thus the @xmath313s play the dual role of basis coefficients and values of @xmath314 at the knots . for ( [ eq : w_lin_exp ] ) to be strictly monotone increasing",
    "the @xmath315s must satisfy certain necessary and sufficient conditions given in fritsch & carlson ( 1980 ) . for situations like ours where no particular values of the @xmath315s are specified ,",
    "fritsch & carlson provide a simple algorithm to compute , from given @xmath313s , values of the @xmath315s that satisfy the monotonicity constraints .",
    "this algorithm is given in the supplementary material . since the algorithm is deterministic , the @xmath315s are functions of the @xmath313s and then ( [ eq : w_lin_exp ] ) is entirely parameterized by @xmath316 , thus forming an @xmath296-dimensional space .",
    "the jupp transform ( jupp , 1978 ) is defined as @xmath317with inverse given by @xmath318note that for any @xmath296-dimensional unconstrained vector @xmath319 the inverse jupp transform yields a vector @xmath320 of strictly increasing knots in @xmath321 . in particular , for @xmath322 the corresponding @xmath320 is a sequence of @xmath296 equally spaced knots in @xmath321 .      under the distributional assumptions in section [ sec :",
    "estimation ] , the likelihood function is derived as follows . the joint density function of the data vectors @xmath203 and the latent random effects @xmath323 can be factorized as @xmath324since @xmath325 depends on @xmath137 only through @xmath326 , according to ( [ eq : reg - model ] ) .",
    "clearly @xmath327 and @xmath328 .",
    "the conditional distributions @xmath329 and @xmath330 are derived as follows .",
    "given @xmath120 and @xmath125 , the values of @xmath67 and @xmath68 determine the warping functions @xmath50 and @xmath59 and consequently two warped time grids @xmath331 , @xmath332 , and @xmath333 , @xmath334 .",
    "let @xmath335 and @xmath336 be the b - spline bases evaluated at the warped time grids , that is @xmath337 and @xmath338 .",
    "then , in view of model specifications ( [ eq : raw_data_x])([eq : kl - model - y ] ) we have @xmath339 and @xmath340 .",
    "the maximum likelihood estimators maximize @xmath341but the integrals in ( [ eq : loglik ] ) do not have closed forms so we use the em algorithm to find the optimum , treating the random effects @xmath323 as missing data .",
    "most of the updating equations of the em algorithm are easy to derive but the restrictions on the parameters @xmath140 , @xmath141 , and @xmath77 pose some difficulties .",
    "this is discussed in detail in the supplementary material .",
    "this proof is a direct application of theorem 4.4 of geyer ( 1994 ) ; note that theorem 5.2 of geyer ( 1994 ) , which pertains to consistent local minimizers instead of global minimizers , can also be applied because our @xmath179 satisfies the stronger condition of being clarke - regular ( rockafellar & wets , 1998 , ch .",
    "following geyer s notation , let @xmath342 and @xmath343",
    ". then @xmath344 and @xmath345 .",
    "assumption a of geyer ( 1994 ) is that @xmath346with @xmath347 positive definite .",
    "this is satisfied in our case because @xmath348 and @xmath349 . to see that the latter is positive definite",
    ", note that for @xmath161 as in ( [ eq : theta ] ) we have @xmath350where @xmath351 , then @xmath352\\geq 0 $ ] and it is equal to zero only if @xmath353 with probability one , which can only happen if @xmath354 .",
    "assumption b of geyer , in our case , is that @xmath355for some @xmath356 such that the remainder @xmath357 is stochastically equicontinuous .",
    "this is satisfied by @xmath358 ; the fact that @xmath357 is stochastically equicontinuous follows from pollard ( 1984 , pp .",
    "150152 ) .",
    "clearly @xmath356 satisfies a central limit theorem with asymptotic covariance matrix @xmath77 that in this case is equal to @xmath200 , so assumption c of geyer is also satisfied .",
    "then theorem 4.4 of geyer can be applied .",
    "it states that the asymptotic distribution of @xmath197 is the same as the distribution of @xmath359 , the minimizer of @xmath360over @xmath361 , where @xmath362 .    in our case @xmath359",
    "can be obtained in closed form , due to the simplicity of @xmath179 .",
    "concretely , @xmath179 is the space of @xmath363s such that @xmath364 .",
    "let @xmath365 $ ] be a @xmath366 orthogonal matrix whose first @xmath189 columns @xmath367 span the space generated by the rows of @xmath368 and whose last @xmath194 columns @xmath192 are orthogonal to the rows of @xmath368 .",
    "then @xmath369 @xmath179 if and only if @xmath370 with @xmath371 ; that is , @xmath372 with @xmath373 the subvector containing the last @xmath194 coordinates of @xmath374 .",
    "then for @xmath369 @xmath179 we can write @xmath375which is clearly minimized by @xmath376 .",
    "therefore @xmath377 , and since @xmath378 , the result of the theorem follows .",
    "ash , r.b . & gardner , m.f . ( 1975 ) .",
    "_ topics in stochastic processes_. new york : academic press ."
  ],
  "abstract_text": [
    "<S> a characteristic feature of functional data is the presence of phase variability in addition to amplitude variability . </S>",
    "<S> existing functional regression methods do not handle time variability in an explicit and efficient way . in this paper </S>",
    "<S> we introduce a functional regression method that incorporates time warping as an intrinsic part of the model . </S>",
    "<S> the method achieves good predictive power in a parsimonious way and allows unified statistical inference about phase and amplitude components . </S>",
    "<S> the asymptotic distribution of the estimators is derived and the finite - sample properties are studied by simulation . </S>",
    "<S> an example of application involving ground - level ozone trajectories is presented .    _ </S>",
    "<S> key words : _ functional data analysis ; random - effect models ; registration ; spline smoothing ; time warping . </S>"
  ]
}