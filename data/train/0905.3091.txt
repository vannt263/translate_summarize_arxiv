{
  "article_text": [
    "in this paper we develop and analyze new methodology for inference about the mean of a stochastic process from data that consists of independent realizations of a stochastic process observed at discrete times , where each observation is contaminated by an additive error term .",
    "formally , let @xmath0 be a stochastic process with mean function @xmath1\\ ] ] and covariance function @xmath2 for all @xmath3 .",
    "we denote the zero mean process @xmath4 by @xmath5 .",
    "we observe @xmath6 at times @xmath7 , for @xmath8 , @xmath9 , that are of the form @xmath10 where @xmath11 , with mean @xmath12 , are random independent realizations of the process @xmath13 .",
    "we assume that @xmath14 are independent across @xmath15 and @xmath16 with zero mean and variance @xmath17 = \\sigma_{\\eps}^2 $ ] .",
    "+ in this paper we propose new methods for estimating and constructing confidence bands for @xmath18 . although the estimation of @xmath18 received considerable attention over the last decade , the theoretical study of data adaptive estimators in model ( [ model ] ) is still open to investigation .",
    "in contrast with the abundance of methods for estimating @xmath18 , methods for constructing confidence bands for @xmath18 are very limited .",
    "this motivates our twofold contribution to the existing literature : ( 1 ) we construct computationally efficient and fully data - driven estimators and confidence bands for @xmath18 , without making distributional assumptions on the process @xmath5 or smoothness assumptions on @xmath18 ; ( 2 ) we assess the quality of our data adaptive estimates theoretically and prove that both the estimators and the confidence bands adapt to the unknown regularity of @xmath18 .",
    "moreover , we show that our bands are , asymptotically in @xmath19 , uniform in @xmath18 .",
    "+ in what follows we review the existing results in the literature and provide further motivation for our procedure .",
    "the problem of estimating @xmath18 from data generated from ( [ model ] ) has been considered by a large number of authors , starting with ramsay and silverman ( 2002 , 2005 ) and rupert , wand and carroll ( 2003 ) .",
    "the existing methods are either based on kernel smoothers [ see , e.g. , zhang and chen ( 2007 ) , yao ( 2007 ) , benko , hrdle and kneip ( 2009 ) ] , penalized splines [ see , e.g. , ramsay and silverman ( 2005 ) ] , free - knot splines [ see , e.g. , gervini ( 2006 ) ] , or ridge - type least squares estimates , [ see , e.g. , rice and silverman ( 1991 ) ] .",
    "all resulting estimates depend on tuning parameters that are method specific",
    ". theoretical properties of these estimates of @xmath18 are still emerging , and have only been established for non - adaptive choices of the respective tuning parameters , that is choices that require prior knowledge of the smoothness of @xmath18 , [ see , e.g. zhang and chen ( 2007 ) and gervini ( 2006 ) ] .",
    "although guidelines for data - driven choices of these parameters are offered in all these works , the theoretical properties of the resulting estimates are still open to investigation . in contrast , we suggest in section 2 below a computationally simple method based on thresholded least squares estimators .",
    "our method does not require any specification of the regularity of @xmath12 or @xmath13 prior to estimation .",
    "we show via oracle inequalities that our estimators adapt to this unknown regularity . + whereas the estimation of the mean @xmath12 of the process @xmath13 is well understood , modulo the technical and possibly computational issues raised above , the construction of uniform confidence intervals for @xmath18 has not been investigated in this context and in general the construction of confidence bands for @xmath18 in model ( [ model ] ) seems to have received little attention .",
    "zhang and chen ( 2007 ) and yao ( 2007 ) construct kernel - type estimators and show that they are asymptotically normal with mean @xmath12 and variance @xmath20 .",
    "although not addressed implicitly in these works , one can use these results to build confidence bands .",
    "this construction would require the estimation of @xmath21 , using for instance the large body of work on the estimation of the covariance operator , based on the karhunen - loeve decomposition of the process @xmath13 and the subsequent estimation the the functional eigenfunctions and eigenvalues , see , e.g. , mller ( 2005 ) , benko , hrdle and kneip ( 2009 ) , who also comment on the possible instability of these estimates and offer refined methods for improved performance .",
    "we offer an alternative method in section 2.5 below .",
    "our procedure is computationally simple , avoids direct estimation of the covariance matrix @xmath22 , @xmath23 , and leads to adaptive bands that are uniform in the parameter @xmath18 .",
    "+ the rest of the paper is organized as follows . in section 2.2 below we discuss thresholded least squares estimators in the functional data setting .",
    "our emphasis is on hard threshold estimators , but we also discuss briefly the closely related soft threshold estimators . in section 2.3",
    "we establish oracle inequalities for the fit of the estimators which show that the estimates adapt to the unknown sparsity of the mean @xmath18 .",
    "the sparsity of @xmath18 is relative to a given approximating basis . in section 2.4 we suggest cross - validation for choosing the basis from a library of bases . since each basis induces an estimator , our procedure can be regarded as one of selecting an estimator from a given list .",
    "we then establish an oracle inequality for the selected estimator , that shows that the selected estimator performs , essentially , as well as the best estimator from the list , in terms of mean squared error relative to the unknown @xmath18 . in section 2.5",
    "we give the construction of the confidence bands and prove that they have the desired coverage probability .",
    "section 3 contains a comprehensive simulation study that strongly supports the theoretical merits of the method and indicates that our method compares favorably with existing methods .",
    "the net merit of the proposed method is very visible when the variance of the random noise @xmath24 is at the same level as that of the stochastic process @xmath5 and we discuss this in detail in sections 3.2 and 3.3 .",
    "all the proofs are collected in the appendix .",
    "as explained in the introduction , the aim of this paper is ( a ) to estimate the mean @xmath12 of the process @xmath13 and ( b ) to construct confidence bands for the mean @xmath12 .",
    "our approach is based on thresholded least squares estimates obtained relative to bases @xmath25 that are orthonormal in @xmath26 , where @xmath27 is the empirical measure that puts mass @xmath28 at each @xmath29 .",
    "thus , our bases satisfy @xmath30 for @xmath31 .",
    "examples include the fourier , local trigonometric and haar bases .",
    "+ since @xmath25 is orthonormal in @xmath26 , each @xmath32 has the decomposition @xmath33 with @xmath34 and @xmath35 for ease of notation we suppress the dependence on @xmath36 in @xmath37 and @xmath38 . under model ( [ model ] ) the random variables @xmath39 , for each @xmath40 , are independent and identically distributed with mean zero and variance @xmath41 = \\frac{1}{m^2 } \\sum_{j=1}^m \\sum_{j'=1}^m \\gamma ( t_j , t_{j ' } ) \\phi_k(t_j ) \\phi_k(t_{j ' } ) ; \\end{aligned}\\ ] ] in the special case where @xmath42 for all @xmath43 , the variances @xmath44 reduce to @xmath45 for @xmath46 .",
    "the coefficients @xmath37 determine the target vector @xmath47 via the formula @xmath48 for each @xmath16 .",
    "we motivate below our proposed methods for inference on @xmath47 .",
    "+      our procedure falls between two of the currently used strategies : averaging estimated individual trajectories and applying various smoothing methods to the entire data set .",
    "our initial estimator of @xmath49 is a least squares estimator , which can be viewed as an average ( over @xmath19 ) of weighted values of the @xmath6 s .",
    "our final estimator will be a truncated version of the least squares estimator , with data dependent truncation levels determined from the entire data set .",
    "we describe our procedure below .",
    "the least squares estimator based on all observations of @xmath50 , for @xmath37 defined by ( [ mu ] ) , is the vector @xmath51 that minimizes @xmath52 over @xmath53 . using the orthonormality property of the basis",
    "the estimators @xmath54 of @xmath37 are given by @xmath55 the sample average ( over @xmath19 ) of @xmath56 , which in turn are the least squares estimators of @xmath57 based on the observations @xmath6 from the @xmath15-th curve only .",
    "using again the orthonormality property of the basis used for this fit , the estimators @xmath58 of @xmath59 are given by @xmath60 recalling that each @xmath6 follows model ( [ model ] ) , and using the representations ( [ xi ] )  ( [ aik ] ) , we can further write @xmath56 as @xmath61 since @xmath14 and @xmath38 have mean zero and are independent across @xmath15 and @xmath16 we obtain , for every @xmath40 , that @xmath62=\\mu_k \\ \\ \\mbox{and }   \\ \\ \\text{var}(\\wh\\mu_{i , k})=\\sigma_k^2 + \\frac{\\sigma_\\eps^2 } { m}.\\ ] ] similarly , we find that @xmath63 with @xmath64=\\mu_k \\ \\",
    "\\mbox{and }   \\ \\",
    "\\text{var}(\\wh\\mu_{k})=\\frac{\\sigma_k^2}{n } + \\frac{\\sigma_\\eps^2 } { mn}.\\ ] ] the initial ( unbiased ) estimator @xmath65 based on the least squares estimates @xmath54 of the mean function @xmath49 is simply @xmath66 and its variance may be unnecessarily inflated by the presence of , possibly many , very small estimates @xmath67 .",
    "this can be remedied by truncating the coefficients at a level that takes into account both the variability of the measurement errors @xmath68 and the variability of the stochastic process @xmath5 ; this is the essential difference between truncated estimators based on data generated as in ( [ model ] ) and their counterpart based only on independent data in a standard nonparametric regression setting .",
    "we will use the truncation level @xmath69 given below and we will justify it theoretically and practically in the next sections .",
    "let @xmath70 be the quantile corresponding to a @xmath71 random variable and @xmath72 is a given number .",
    "define @xmath73 for some small @xmath74 ( that is set to zero in practice ) that depends on @xmath75 notice that @xmath76 a consistent and unbiased estimator of @xmath77 since the @xmath58 , @xmath8 are i.i.d . for fixed @xmath78 .",
    "+ we will focus on hard threshold estimators of the coefficients @xmath37 and function @xmath18 .",
    "they are , respectively @xmath79    we will also consider , for completeness , the soft threshold estimators : @xmath80 the two estimators are closely related as the coefficients @xmath81 and @xmath82 differ by at most @xmath69 since @xmath83      [ [ oracle - inequalities - for - the - estimators - of - the - mean - of - a - stochastic - process ] ] oracle inequalities for the estimators of the mean of a stochastic process ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    to discuss the quality of the estimates given above relative to the mean @xmath18 , we first investigate their properties relative to a truncated version of @xmath18 and obtain the desired results as a consequence .",
    "we motivate the truncation of @xmath18 below .",
    "consider the theoretical truncation level @xmath84 where @xmath70 is the quantile corresponding to a @xmath71 random variable and @xmath72 is a given number .",
    "notice that this is the population counterpart of the data based levels given in ( [ trunc ] ) above .",
    "define , for each @xmath78 , @xmath85 and we write @xmath86 for a truncated version of @xmath18 . when the truncation @xmath87 retains the main features of @xmath18 , it can be considered as the new target for inference .",
    "this is the approach we take in the sequel . as an illustrative example , we consider the mean function [ gaussian_mix ] @xmath88 shown in black in figures [ sparsity](a ) and [ sparsity](c ) .",
    "we project @xmath89 onto linear subspaces in @xmath90 generated by the fourier and haar basis functions , respectively , evaluated at @xmath91 equally distant points in @xmath92 $ ] .",
    "many of the projection coefficients @xmath93 are close to zero for both bases .",
    "we consider the truncation level @xmath94 given above with @xmath95 , @xmath96 , @xmath97 and for @xmath98 given by ( [ var ] ) above corresponding to the brownian bridge process with covariance function @xmath99 .",
    "+      figures [ sparsity](b ) and [ sparsity](d ) show @xmath100 versus their index , for the fourier and haar basis respectively . the reconstruction @xmath101 is shown in red in figures [ sparsity](a ) and [ sparsity](c ) , and is very close to @xmath12 in both cases .",
    "notice that only 11 coefficients are needed for this good reconstruction of @xmath18 via the fourier basis , versus 92 via the haar basis , as we reconstruct a differentiable function with differentiable and non - differentiable basis functions , respectively .",
    "following standard terminology in non - parametric estimation , we refer to the fact that @xmath18 can be reconstructed well via a smaller subset of the given collection of the basis functions by saying that @xmath18 has a sparse representation relative to that basis .",
    "+ in what follows we show that the thresholded estimates introduced in the section above adapt to the sparsity of @xmath18 . of course",
    ", since @xmath18 is unknown , so is its sparsity relative to a given basis .",
    "nevertheless , we show that our estimators adapt to this unknown sparsity in terms of their fit , and refer to these results as oracle inequalities .",
    "the type of oracle inequalities that we establish below illustrate that the fit of our estimators depends only on the estimation errors induced by the estimates of the non - zero coefficients of a sparse representation of @xmath18 within a given basis . as our example indicates , and as is the case in any non - parametric estimation problem , the overall quality of our estimator will further depend on the choice of the basis used for estimation .",
    "we will therefore complement the construction of our estimator with a basis selection step .",
    "we begin by stating our results for a given basis .",
    "+ for both hard and soft threshold estimators we obtain estimation bounds on the fit at the observation points @xmath29 .",
    "we formulate our results in terms of the empirical supremum norm @xmath102 and @xmath103 norm @xmath104 defined below . for any real function @xmath105 ,",
    "let @xmath106    all theorems and results of this article require that @xmath107<\\infty$ ] , for all @xmath108 , and that @xmath109<\\infty$ ] .",
    "the next three theorems are proved for a given basis and the desired probability @xmath110 .",
    "all estimates are based on the threshold level @xmath111 given in ( [ trunc ] ) , for a user specified value of @xmath110 .",
    "the following result establishes oracle inequalities for the hard - threshold estimators .",
    "define @xmath112 which differs by @xmath113 from @xmath94 defined above in ( [ theolevel ] ) , for a quantity @xmath114 that is arbitrarily close to zero ; this is needed for purely technical reasons , and for all practical purposes @xmath115 and @xmath94 can be considered the same .",
    "theorems [ thm1 ] and [ thm2 ] yield immediately results on the performance of these estimators relative to the untruncated @xmath18 .",
    "in particular , the following inequality holds with probability at least @xmath123 as @xmath124 : @xmath125 for both estimators @xmath126 and @xmath127 .",
    "this follows directly from the above results and the triangle inequality .",
    "the first term @xmath128 can be viewed as the approximation error or bias term , whereas the second term represents the estimation error or standard deviation term .",
    "the bias term is unavoidable , and its size depends on the basis choice .",
    "it suggests the need for an adaptive method , that would select the basis that is best suited for the unknown underlying mean function @xmath18 .",
    "we discuss this in section 2.4 below .",
    "+ theorems [ thm1 ] and [ thm2 ] are novel type of oracle inequalities for thresholded estimators , as they guarantee the  in probability \" , rather than  on average \" , performance of the estimator , at any probability level of interest @xmath72 .",
    "these properties hold for our estimates , as they are constructed relative to variable threshold levels that depend on @xmath110 . to the best of our knowledge ,",
    "such results are new in the functional data context .",
    "they are also new in the general non - parametric settings , where a more traditional way to state the oracle properties of the estimators is in terms of the expected mean squared error , see , for instance , donoho and johnstone ( 1995 , 1998 ) , wasserman ( 2006 ) , tsybakov ( 2009 ) and the references therein . for completeness",
    ", we also give an assessment of our estimates in terms of the expected mean squared error in theorem [ thm3 ] below , which restates theorem [ thm1 ] in terms of expected values .",
    "to avoid technical clutter , we consider the toy estimator @xmath129 in lieu of @xmath130 .",
    "recall the notation @xmath131      theorem [ thm3 ] shows that the expected mean squared error of our estimator also adapts to the unknown sparsity of @xmath18 , as indicated by the first term in either inequality ( [ gen ] ) or ( [ normal ] ) .",
    "the second term in these inequalities is essentially an average of the quantities that constitute the first term .",
    "this is more evident from the closed form expression ( [ normal ] ) , and shows that this second term is negligible relative to the first one , especially for small values of @xmath110 .",
    "the results of the previous section make it clear that the basis choice influences both the bias and the variance of our estimates ; the type of basis one uses for the fit can be regarded as the tuning parameter of our estimation procedure .",
    "we give below a data adaptive procedure of selection and show in theorem [ ds ] below that the estimator based on the selected basis behaves essentially as if the best basis for approximating the unknown @xmath18 was known in advance .",
    "+ we select the basis via a cross - validation ( data - splitting ) technique , by randomly dividing the @xmath19 discretized curves @xmath136 in two equally sized groups . the first sample @xmath137 is used for constructing various estimates , say @xmath138 , @xmath139 , based on various bases , choices of @xmath110 and thresholding methods ( hard and soft ) .",
    "the second sample ( hold - out or validation sample ) @xmath140 is used to select the optimal estimate @xmath141 that minimizes the empirical risk @xmath142 over @xmath143 . here",
    "@xmath144 is the index set for the curves that are set aside to evaluate the estimators @xmath138 and @xmath145 is its cardinality .",
    "theorem [ ds ] requires that the process @xmath5 and the random error @xmath24 have moments strictly larger than 2 , which is still a very mild assumption .",
    "+ the last term in the right hand side of ( [ sel ] ) is of order @xmath152 , making the sum of the last two terms of order @xmath152 .",
    "this can be regarded as the price to pay for using a data adaptive procedure to select the appropriate basis .",
    "the factor 2 multiplying the right hand side of ( [ sel ] ) can be reduced to @xmath153 at the cost of increasing the last two terms on the right by a factor proportional to @xmath154 , for @xmath155 arbitrarily close to zero . to avoid notational clutter we opted for using the constant 2 .",
    "therefore , theorem [ ds ] shows that the basis selection process yields an estimate that is essentially as good as the best estimate on the list , in terms of expected squared error . since which is best can not be known in advance , as @xmath18 is unknown , the result of theorem [ ds ] can also be regarded as an oracle inequality .      in this section",
    "we will construct confidence bands for @xmath18 that are uniform over the parameter space .",
    "we begin with the confidence band based on a hard threshold estimator given below . set @xmath156 and notice that it differs by @xmath114 from @xmath69 given in ( [ trunc ] ) above .",
    "this is again needed for technical reasons , as in practice @xmath114 can be set to zero .",
    "[ thm5 ] ( 1 ) for all @xmath116 , @xmath117 and @xmath74 @xmath157 contains @xmath158 with probability at least @xmath123 , as @xmath124 .",
    "+ ( 2 ) moreover , if all non - zero coefficients @xmath37 exceed @xmath159 , the band can be made smaller by a factor 3 : @xmath160 contains @xmath161 with probability at least @xmath123 , as @xmath124 .",
    "michal benko , wolfgang hrdle and alois kneip .",
    "common functional principal components . _",
    "annals of statistics _ 37(1 ) , 1 34 , 2009 .",
    "+ david donoho and iain johnstone .",
    "adapting to the unknown smoothness via wavelet shrinkage .",
    "_ journal of the american statistical association _ 90 , 12001224,1995 .",
    "+ david donoho and iain johnstone .",
    "minimax estimation via wavelet shrinkage .",
    "_ annals of statistics _ 26 , 789921 , 1998 .",
    "+ christopher genovese and larry wasserman .",
    "adaptive confidence bands . _",
    "annals of statististics _",
    "36(2 ) , 875905 , 2008 . + daniel gervini .",
    "free - knot spline smoothing for functional data .",
    "_ journal of the royal statistical society , series b _",
    "68(4 ) , 671687 , 2006 . +",
    "hans - georg mller . functional modelling and classification of longitudinal data .",
    "_ scandinavian journal of statistics _ 32 , 223240 , 2005 .",
    "+ hans - georg mller , rituparna sen and ulrich stadtmller .",
    "functional data analysis for volatility .",
    "_ manuscript _ , 2006 .",
    "+ ` r ` development core team ( 2008 ) . `",
    "r ` : _ a language and environment for statistical computing . _ ` r ` foundation for statistical computing , vienna , austria .",
    "isbn 3 - 900051 - 07 - 0 , url http://www.r-project.org .",
    "+ james ramsay and bernard silverman .",
    "_ functional data analysis _ , 2@xmath302 edition .",
    "springer , new york , 2005 .",
    "+ james ramsay and bernard silverman .",
    "_ applied functional data analysis . _",
    "springer , new york , 2002 .",
    "+ john rice and bernard silverman . estimating the mean and covariance structure nonparametrically when the data are curves .",
    "_ journal of the royal statistical society , series b _",
    "53(1 ) , 233243 , 1991 .",
    "+ david ruppert , simon sheather and matthew wand .",
    "an effective bandwidth selector for local least squares regression",
    ". _ journal of the american statistical association _",
    "90(432 ) , 1257 - 1270 , 1995 . + david ruppert , matthew wand and raymond carroll .",
    "_ semiparametric regression . _ cambridge university press , cambridge 2003 .",
    "+ burkhart seifert , michael brockmann , joachim engel and theo gasser .",
    "fast algorithms for nonparametric curve estimation .",
    "_ journal of computational and graphical statistics _",
    "3(2 ) , 192213 , 1994 . + alexandre b. tsybakov . _",
    "introduction to nonparametric estimation . _ springer , new york , 2009 .",
    "+ larry wasserman .",
    "_ all of nonparametric statistics .",
    "_ springer , new york , 2006 .",
    "+ marten wegkamp .",
    "model selection in nonparametric regression .",
    "_ annals of statistics",
    "_ 31(1 ) , 252273 , 2003 .",
    "+ fang yao .",
    "asymptotic distributions of nonparametric regression estimators for longitudinal and functional data .",
    "_ journal of multivariate analysis _ 98 , 4056 , 2007 .",
    "+ fang yao , hans - georg mller and jane - ling wang .",
    "functional data analysis for sparse longitudinal data .",
    "_ journal of the american statistical association _",
    "100(740 ) , 577590 , 2005 .",
    "+ jin - ting zhang and jianwei chen . statistical inferences for functional data .",
    "_ annals of statistics",
    "_ 35(3 ) , 10521079 , 2007 ."
  ],
  "abstract_text": [
    "<S> this paper proposes and analyzes fully data driven methods for inference about the mean function of a stochastic process from a sample of independent trajectories of the process , observed at discrete time points and corrupted by additive random error . </S>",
    "<S> the proposed method uses thresholded least squares estimators relative to an approximating function basis . </S>",
    "<S> the variable threshold levels are estimated from the data and the basis is chosen via cross - validation from a library of bases . </S>",
    "<S> the resulting estimates adapt to the unknown sparsity of the mean function relative to the selected approximating basis , both in terms of the mean squared error and supremum norm . </S>",
    "<S> these results are based on novel oracle inequalities . </S>",
    "<S> in addition , uniform confidence bands for the mean function of the process are constructed . </S>",
    "<S> the bands also adapt to the unknown regularity of the mean function , are easy to compute , and do not require explicit estimation of the covariance operator of the process . </S>",
    "<S> the simulation study that complements the theoretical results shows that the new method performs very well in practice , and is robust against large variations introduced by the random error terms . </S>",
    "<S> + keywords : stochastic processes ; nonparametric mean estimation ; thresholded estimators ; functional data ; oracle inequalities ; adaptive inference ; uniform confidence bands .    </S>",
    "<S> = 1 </S>"
  ]
}