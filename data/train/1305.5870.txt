{
  "article_text": [
    "that we are interested in an unknown @xmath7-by-@xmath2 matrix @xmath17 , thought to be either exactly or approximately of low rank , but we only observe a single noisy @xmath18-by-@xmath19 matrix @xmath20 , obeying @xmath21 .",
    "the noise matrix @xmath22 has independent , identically distributed , zero - mean entries .",
    "the matrix @xmath17 is a ( non - random ) parameter , and we wish to estimate it with some bound on the mean squared error ( mse ) .",
    "the default estimation technique for our task is _ truncated svd _ ( tsvd ) @xcite : write @xmath23 for the singular value decomposition of the data matrix @xmath20 , where @xmath24 and @xmath25 , @xmath26 are the left and right singular vectors of @xmath20 corresponding to the singular value @xmath27 .",
    "the tsvd estimator is @xmath28 where @xmath29 , assumed known , and @xmath30 .",
    "being the best approximation of rank @xmath12 to the data in the least squares sense @xcite , and therefore the maximum likelihood estimator when @xmath22 has gaussian entries , the tsvd is arguably as ubiquitous in science and engineering as linear regression @xcite .",
    "when the true rank @xmath31 of the signal @xmath17 is unknown , one might try to form an estimate @xmath32 and then apply the tsvd @xmath33 .",
    "extensive literature has formed on methods to estimate @xmath31 : we point to the early @xcite ( in factor analysis and principal component analysis ) , the recent @xcite ( in our setting of singular value decomposition ) , and reference therein .",
    "it is instructive to think about rank estimation ( using any method ) , followed by tsvd , simply as _ hard thresholding _ of the data singular values , where only components @xmath34 for which @xmath27 passes a specified threshold , are included in @xmath35 .",
    "let @xmath36 denote the hard thresholding nonlinearity , and consider singular value hard thresholding ( svht ) @xmath37 in words , @xmath38 sets to @xmath1 any data singular value below @xmath39 .",
    "matrix denoisers explicitly or implicitly based on hard thresholding of singular values have been proposed by many authors , including @xcite . as a common example of implicit svht denoising ,",
    "consider the standard practice of estimating @xmath31 by plotting the singular values of @xmath20 in decreasing order , and looking for a `` large gap '' or `` elbow '' ( figure [ hist : fig ] , left panel ) .",
    "when @xmath17 is exactly or approximately low - rank and the entries of @xmath22 are white noise of zero mean and unit variance , the empirical distribution of the singular values of the @xmath7-by-@xmath2 matrix @xmath21 forms a quarter - circle bulk whose edge lies approximately at @xmath40 , with @xmath41 @xcite .",
    "only data singular values that are larger than the bulk edge are noticeable in the empirical distribution ( figure [ hist : fig ] , right plot ) .",
    "since the singular value plot `` elbow '' is located at the bulk edge , the popular method of tsvd at the `` elbow '' is an approximation of _ bulk - edge hard thresholding _ , @xmath42 .     sampled from the model @xmath43 , where @xmath44 , @xmath45 and @xmath46 elsewhere . in matlab",
    ", a sample can be generated using the command y = svd(diag([[1.7 2.5 ] zeros(1,98)])+randn(100)/sqrt(100 ) ) .",
    "left : the singular values of @xmath20 plotted in decreasing order , known in principal components analysis as a scree plot .",
    "right : the singular values of @xmath20 in a histogram with @xmath47 bins . note the bulk edge approximately at @xmath48 , and note that the location of the top two singular values of @xmath20 is approximately @xmath49 ( @xmath50 ) , in agreement with lemma [ y - asy : lem ] . , width=355 ]      let us measure the denoising performance of a denoiser @xmath35 at a signal",
    "matrix @xmath17 using mean square error ( mse ) , @xmath51 the tsvd is an optimal rank-@xmath31 approximation of the data matrix @xmath20 , in mse .",
    "but this does not necessarily mean that it is a good , or even reasonable , estimator to the signal matrix @xmath17 , which we wish to recover .",
    "we may wonder :    * _ question 1 .",
    "_ assume that @xmath52 is unknown but small .",
    "is there a singular value threshold @xmath39 so that svht @xmath38 successfully adapts to unknown rank and unknown noise level , and performs as well as tsvd would , had we known the true @xmath52 ?    as we will see , it is convenient to represent the threshold as @xmath53 , where @xmath0 is a parameter typically between @xmath54 and @xmath55 .",
    "recently , s. chatterjee @xcite proposed that one could have a single universal choice of @xmath0 ; that in a setting more general , but similar , to our setting , any @xmath56 would give near - optimal mse , in a qualitative sense ; and he specifically proposed @xmath57 , namely @xmath58 as a universal choice for svht , regardless of the shape @xmath59 of the matrix , and regardless of the underlying signal matrix @xmath17 or its rank . while the rule of @xcite was originally intended to be ` fairly good ' across many situations not reducible to the low - rank matrix in i.i.d noise model considered here , @xmath60 is a specific proposal , which prompts the following question :    * _ question 2 .",
    "_ is there really a single threshold parameter @xmath61 that provides good performance guarantees for mse ?",
    "is that value @xmath62 ?",
    "is it really independent of @xmath7 and @xmath2 ?",
    "finally , note that singular value hard thresholding is just one strategy for matrix denoising .",
    "it is not a - priori clear whether the whole idea of only ` keeping ' or ` killing ' empirical singular values based on their size makes sense .",
    "could there exist a shrinkage rule @xmath63 , that more smoothly transitions from ` killing ' to ` keeping ' , which leads to a much better denoising scheme ?",
    "we may wonder :    * _ question 3 .",
    "_ how does optimally tuned svht compare with the performance of the _ best possible shrinkage _ of singular values , at least in the worst - case mse sense ?",
    "our main results imply that , in a certain asymptotic framework , there are simple and convincing answers to these questions .",
    "following perry @xcite and shabalin and nobel @xcite , we adopt an asymptotic framework where the matrix grows while keeping the nonzero singular values of @xmath17 fixed , and the signal - to - noise ratio of those singular values stays constant with increasing @xmath2 .    in this asymptotic framework , for a low - rank @xmath2-by-@xmath2 matrix observed in white noise of level @xmath3 , @xmath64 is the optimal location for the hard thresholding of singular values . for a non - square @xmath7-by-@xmath2 matrix with @xmath65 ,",
    "the optimal location is @xmath66 where @xmath41 . the value @xmath67 is the _ optimal hard threshold coefficient _ for known @xmath3 .",
    "it is given by formula below and tabulated for convenience in table [ lambdastar : tab ] .",
    "( note added in proof : we found that p. perry s phd thesis @xcite proposes a threshold which can be shown to be equivalent to . )",
    "our central observation is as follows .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ when a data singular value @xmath27 is too small , then its associated singular vectors @xmath68 are so noisy that the component @xmath69 should not included in @xmath35 . in our asymptotic framework , which models large , low - rank matrices observed in white noise , the cutoff below which @xmath27 is _ too small _ is exactly @xmath70 ( for square matrices ) . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    * _ answer to question 1 : optimal svht dominates tsvd . _",
    "optimally tuned svht @xmath71 is _ always _ at least as good as tsvd @xmath72 , in terms of amse ( theorem [ inadmis - tsvd : thm ] ) . unlike @xmath72",
    ", the optimal svht @xmath71 does not require knowledge of @xmath29 .",
    "in other words , it adapts to unknown low rank while giving uniformly equal or better performance . for square matrices ,",
    "the tsvd provides a guarantee on worst - case amse that is @xmath73 times the guarantee provided by @xmath71 ( table [ mmx - square - boundedrank : tab ] ) . *",
    "_ answer to question 2 : optimal svht dominates every other choice of hard threshold .",
    "_ in terms of amse , optimally tuned svht @xmath71 is _ always _ at least as good as svht @xmath38 at any other fixed threshold @xmath74 ( theorem [ inadmis : thm ] ) .",
    "it is the asymptotically minimax svht denoiser , over matrices of small bounded rank ( theorems [ mmx - square : thm ] and [ mmx - nonsquare : thm ] ) and over matrices of small bounded nuclear norm ( theorem [ mmx - l1:thm ] ) .",
    "in particular , the parameter @xmath75 is noticeably worse . for square matrices",
    ", @xmath60 provides a guarantee for worst - case amse that is @xmath76 times the guarantee provided by @xmath71 ( table [ mmx - square - boundedrank : tab ] ) . * _ answer to question 3 .",
    "optimal svht compares adequately to the optimal shrinker .",
    "_ optimally tuned svht @xmath71 provides a guarantee on worst - case asymptotic mse that is @xmath77 times ( for square matrices ) the best possible guarantee achievable by _ any _ shrinkage of data singular values ( table [ mmx - square - boundedrank : tab ] ) .",
    "these are all rigorous results , within a specific asymptotic framework , which prescribes a certain scaling of the noise level , the matrix size , and the signal - to - noise ratio as @xmath2 grows .",
    "but does amse predict actual mse in finite - sized problems ? in section [ msevsamse : subsec ] we show finite-@xmath2 simulations demonstrating the effectiveness of these results even at rather small problem sizes . in high signal - to - noise , all denoisers considered here",
    "perform roughly the same , and in particular the classical tsvd is a valid choice in that regime .",
    "however , in low and moderate snr , the performance gain of optimally tuned svht is substantial , and can offer @xmath78 decrease in amse .      for a low - rank @xmath2-by-@xmath2 matrix observed in white noise of _ unknown _ level , one can use the data to obtain an approximation of the optimal location @xmath79 . define @xmath80 where @xmath6 is the median singular value of the data matrix @xmath20 .",
    "the notation @xmath81 is meant to emphasize that this is not a fixed threshold chosen a - priori , but rather a data - dependent threshold . for a non - square @xmath7-by-@xmath2 matrix with @xmath65 , the approximate optimal location when @xmath3 is unknown is @xmath82",
    "the optimal hard threshold coefficient for unknown @xmath3 , denoted by @xmath83 , is not available as an analytic formula , but can easily be evaluated numerically .",
    "we provide a matlab script for this purpose @xcite ; the underlying derivation appears in section [ sigmahat : subsec ] below .",
    "some values of @xmath83 are provided in table [ lambdastar - hat : tab ] . when a high - precision value of @xmath83 can not be computed",
    ", one can use the approximation @xmath84 the optimal svht for unknown noise level , @xmath85 , is very simple to implement and does not require any tuning parameters .",
    "the denoised matrix @xmath86 can be computed using just a few code lines in a high - level scripting language .",
    "for example , in matlab :    ....     beta = size(y,1 ) / size(y,2 ) ;     omega = 0.56*beta^3 - 0.95*beta^2 + ...          1.82*beta + 1.43 ;     [ u d v ] = svd(y ) ;     y = diag(y ) ;     y ( y < ( omega * median(y ) ) = 0 ;     xhat = u * diag(y ) * v ' ; ....    here we have used the approximation .",
    "we recommend , whenever possible , to use a function omega(beta ) , such as the one we provide in the code supplement @xcite , to compute the coefficient @xmath83 to high precision .",
    "in our asymptotic framework , @xmath79 and @xmath81 enjoy exactly the same optimality properties .",
    "this means that @xmath85 adapts to unknown low rank _ and _ to unknown noise level .",
    "empirical evidence suggest that their performance for finite @xmath2 is similar . as a result ,",
    "the answers we provide above hold for the threshold @xmath81 when the noise level is unknown , just as they hold for the threshold @xmath79 when the noise level is known .",
    "column vectors are denoted by boldface lowercase letters , such as @xmath87 , their transpose is @xmath88 and their @xmath89-th coordinate is @xmath90 .",
    "the euclidean inner product and norm on vectors are denoted by @xmath91 and @xmath92 , respectively .",
    "matrices are denotes by uppercase letters , such as @xmath17 , its transpose is @xmath93 and their @xmath94-th entry is @xmath95 .",
    "@xmath96 denotes the space of real @xmath7-by-@xmath2 matrices , @xmath97 denotes the hilbert-schmidt inner product , and @xmath98 denotes the corresponding frobenius norm on @xmath96 . for simplicity",
    "we only consider @xmath99 .",
    "we denote matrix denoisers , or estimators , by @xmath100 . the symbols @xmath101 and",
    "@xmath102 denote almost sure convergence and equality of a.s .",
    "limits , respectively .      with the exception of tsvd ,",
    "when @xmath3 is known , all the denoisers we discuss operate by shrinkage of data singular values , namely are of the form @xmath103 where @xmath20 is given by and @xmath63 is some univariate shrinkage rule . as we will see , in the general model @xmath21 , the noise level in the singular values of @xmath20 is @xmath104 . instead of specifying a different shrinkage rule that depends on the matrix size @xmath2 , we calibrate our shrinkage rules to the `` natural '' model @xmath105 . in this convention ,",
    "shrinkage rules stay the same for every value of @xmath2 , and we conveniently abuse notation by writing @xmath35 as in for any @xmath100 , keeping @xmath7 and @xmath2 implicit . to apply any denoiser @xmath35 below to data from the general model @xmath21 , use the denoiser @xmath106 for example , to apply the svht @xmath107 to data sampled from the model @xmath21 , use @xmath108 , with @xmath109 throughout the text",
    ", we use @xmath110 to denote svht calibrated for noise level @xmath111 and @xmath112 to denote svht calibrated for a specific general model @xmath21 .    to translate the amse of any denoiser @xmath35 , calibrated for noise level @xmath111 , to an approximate mse of the corresponding denoiser @xmath113 , calibrated for a model @xmath21",
    ", we use the identity    lcl + & & n^2|| ( x/())+z/)-x/ ( ) ||_f^2 .",
    "below , we spell out this translation of amse where appropriate .      in this paper , we consider a sequence of increasingly larger denoising problems @xmath114 , with @xmath115 , satisfying the following assumptions :    1 .   _ invariant white noise : _ the entries of @xmath116 are i.i.d samples from a distribution with zero mean , unit variance and finite fourth moment . to simplify the formal statement of our results , we assume that this distribution is _ orthogonally invariant _ in the sense that @xmath116 follows the same distribution as @xmath117 , for every orthogonal @xmath118 and @xmath119 .",
    "this is the case , for example , when the entries of @xmath116 are gaussian . in section [ general_noise : sec ]",
    "we revisit this restriction and discuss general ( not necessarily invariant ) white noise .",
    "fixed signal column span @xmath120 : _ let the rank @xmath121 be fixed and choose a vector @xmath122 with coordinates @xmath123 such that @xmath124 .",
    "assume that for all @xmath2 , @xmath125 is an arbitrary and nonzero signal singular values @xmath126 are shared by all matrices @xmath127 , the signal left and right singular vectors @xmath128 and @xmath129 are unknown and arbitrary . ] singular value decomposition of @xmath127 , where @xmath130 and @xmath131 .",
    "_ asymptotic aspect ratio @xmath132 : _ the sequence @xmath133 is such that @xmath134 . to simplify our formulas , we assume that @xmath135 .",
    "let @xmath35 be any singular value shrinkage denoiser calibrated , as discussed above , for noise level @xmath111 .",
    "define the asymptotic mse ( amse ) of an @xmath35 at a signal @xmath136 by the ( almost sure ) limit . ]",
    "@xmath137    adopting the asymptotic framework above , we seek singular value thresholding rules @xmath110 that minimize the amse @xmath138 .",
    "as we will see , in this framework there are simple , satisfying answers to the questions posed in the introduction .",
    "define the _ optimal hard threshold for singular values _ for @xmath2-by-@xmath2 square matrices by @xmath139 more generally , define the optimal threshold for @xmath18-by-@xmath19 matrices with @xmath140 by @xmath141 some values of @xmath67 are provided in table [ lambdastar : tab ] .    .some optimal hard threshold coefficients @xmath67 from . for @xmath7-by-@xmath2 matrix in known noise level @xmath3 ( with @xmath142 ) ,",
    "the optimal svht denoiser @xmath71 sets to zero all data singular values below the threshold @xmath143 . [ cols=\">,<,>,<\",options=\"header \" , ]",
    "figure [ lambda : fig ] shows the optimal threshold @xmath67 over @xmath132 .",
    "the edge of the quarter circle bulk @xmath144 , the hard threshold that best emulates tsvd in our setting , is shown for comparison . in the null case @xmath145 , the largest data singular value is located asymptotically exactly at the bulk edge , @xmath146 .",
    "it might seem that just above the bulk edge is a natural place to set a threshold , since anything smaller could be the product of a pure noise situation .",
    "however , for @xmath147 , the optimal hard threshold @xmath148 is 15 - 20% larger than the bulk edge ; as @xmath149 , it grows about 40% larger . inspecting the proof of theorem [ inadmis : thm ] and particularly the expression for amse of svht ( lemma [ amse : lem ] )",
    ", one finds the reason : one component of the amse is due to the angle between the signal singular vectors and the data singular vectors .",
    "this angle converges to a nonzero value as @xmath150 ( given explicitly in lemma [ inner - asy : lem ] ) which grows as snr decreases . when some data singular value @xmath27 is too close to the bulk , its corresponding singular vectors are too badly rotated , and the rank - one matrix @xmath151 it contributes to the denoiser hurts the amse more than it helps .",
    "for example , for square matrices @xmath152 , this situation is most acute when the signal singular value is just barely larger than @xmath153 , causing the corresponding data singular value @xmath27 to be just barely larger than the bulk edge , which for square matrices is located at @xmath48 .",
    "a svht denoiser thresholding just above the bulk edge would include the component @xmath154 , incurring an amse about 5 times larger than the amse incurred by excluding @xmath27 from the reconstruction .",
    "the optimal threshold @xmath67 keeps such singular values out of the picture ; this is why it is necessarily larger than the bulk edge .",
    "the precise value of @xmath67 is the precise point at which it becomes advantageous to include the rank - one contribution of a singular value @xmath27 in the reconstruction .     from against @xmath132 .",
    "also shown are bulk - edge @xmath144 , which is the hard threshold corresponding to tsvd in our setting , and the usvt threshold @xmath62 from @xcite .",
    ", width=326 ]      as mentioned in the introduction , s. chatterjee has recently discussed svht in a broad class of situations @xcite . translating his much broader discussion to the confines of the present context , he observed that _ any _ @xmath56 can serve as a _ universal hard threshold for singular values _ ( usvt ) , offering fairly good performance regardless of the matrix shape @xmath59 and the underlying signal matrix @xmath17 .",
    "the author makes the specific recommendation @xmath57 and writes :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  the algorithm manages to cut off the singular values at the ` correct ' level , depending on the structure of the unknown parameter matrix .",
    "the adaptiveness of the usvt threshold is somewhat similar in spirit to that of the sureshrink algorithm of donoho and johnstone .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    keeping in mind that the scope of @xcite is much broader than the one considered here , we would like to evaluate this proposal , in the setting of low rank matrix in white noise , and specifically in our asymptotic framework .",
    "figure [ lambda : fig ] includes the value @xmath62 : indeed , this threshold is larger than the bulk edge , for any @xmath156 , so chatterjee s @xmath155 rule asymptotically set to zero all singular values which _",
    "could _ arise due to an underlying noise - only situation .",
    "when @xmath157 , the @xmath155 rule sometimes `` kills '' singular values that the optimal threshold deems good enough for keeping , and when @xmath158 , the @xmath155 rule sometimes `` keeps '' singular values that did in fact arise from signal , but are so close to the bulk that the optimal threshold declares them unusable .    for @xmath152 ,",
    "the guarantee on worst - case amse obtained by using @xmath57 over matrices of rank @xmath12 is about @xmath159 , roughly 140% larger than the guarantee obtained by using the minimax threshold @xmath160 ( see figure [ amse : fig ] ) . for square matrices ,",
    "the regret for preferring usvt to optimally - tuned svht can be substantial : in low snr ( @xmath161 ) , using the threshold @xmath57 incurs roughly twice the amse of the minimax threshold @xmath9 .",
    "we note that unlike the optimally tuned svht @xmath162 , the usvt @xmath155 does not take into account the shape factor @xmath132 , namely the ratio of number of rows to number of columns of the matrix in question .",
    "a comparison of worst - case amse between the fixed threshold choice @xmath57 and the optimal hard threshold @xmath163 is shown in figure [ sourav - mmx - lf : fig ] .",
    "the two curves intersect at @xmath164 , where the optimal threshold is approximately @xmath62 .",
    ", for two choices of hard threshold : @xmath57 from @xcite , and optimal threshold @xmath61 from .",
    ", width=326 ]    one might argue that @xcite proposed @xmath62 based on its mse performance over classes of matrices bounded in nuclear norm .",
    "but also for that purpose , @xmath62 is noticeably outperformed by @xmath148 .",
    "arguing as in theorem [ mmx - l1:thm ] we obtain , in the square case : @xmath165 the coefficient @xmath166 is about 110% larger than the best coefficient achievable by svht , namely @xmath167 in .",
    "one should keep in mind that usvt is applicable for a wide range of noise models , e.g. in stochastic block models .",
    "@xcite is the first , to the best of out knowledge , to suggest that a matrix denoising procedure as simple as svht could have universal optimality properties . in our asymptotic framework of low - rank matrices in white noise , the 2.02 threshold performs fairly well in amse , except for very small values of @xmath132 ( figure [ amse : fig ] ) ; but one often gets a substantial amse improvement by switching to the rule we recommend . since our recommendation dominates in amse , there is no downside to making this switch ",
    "i.e. there is no configuration of signal singular values @xmath168 which could make one regret this switch .",
    "setting additional notation required in the proofs , let    @xmath169 be a sequence of signal matrices in our asymptotic framework , so that @xmath170 ( resp .",
    "@xmath171 ) is the left ( resp .",
    "right ) singular vector corresponding to the singular value @xmath172 , namely , @xmath89-th column of @xmath128 ( resp .",
    "@xmath129 ) in .",
    "similarly , let @xmath173 be a corresponding sequence of observed matrices in our framework , and write @xmath174 so that @xmath175 ( resp .",
    "@xmath176 ) is the left ( resp .",
    "right ) singular vector corresponding to the singular value @xmath177 .",
    "( note that @xmath178 and @xmath179 are unknown , arbitrary , non - random vectors . )",
    "our main results depend on lemma [ amse : lem ] , a formula for the amse of svht .",
    "this formula in turn depends on lemma [ y - asy : lem ] and lemma [ inner - asy : lem ] .",
    "both follow from recent key results due to @xcite .",
    "[ y - asy : lem ] * asymptotic data singular values . * for @xmath180 , @xmath181    [ inner - asy : lem ] * asymptotic angle between signal and data singular vectors .",
    "* let @xmath182 and assume that @xmath172 has degeneracy @xmath183 , namely , there are exactly @xmath183 entries of @xmath136 equal to @xmath172 . if @xmath184 , we have @xmath185 and , a slightly different formula , @xmath186 if however @xmath187 , then we have @xmath188    to appeal to these results , we need to show that our asymptotic framework satisfies the assumptions of @xcite . by @xcite the limiting law of the singular values of @xmath189 is the quarter - circle density @xmath190}}(x)\\,;\\end{aligned}\\ ] ] by @xcite , @xmath191 ; by @xcite , @xmath192 .",
    "this satisfies assumptions 2.1 , 2.2 and 2.3 of @xcite , respectively",
    ". formulas , and , as seen in ( * ? ? ?",
    "* example 3.1 ) , depend only on the shape of the limiting distribution and not on any gaussian assumptions .    using lemma [ y - asy : lem ] and lemma [ inner - asy : lem ] , we can calculate the amse of the hard thresholding estimator @xmath110 , for given threshold @xmath0 , at a matrix of specific aspect ratio @xmath132 and signal singular values @xmath136 :    [ amse : lem ] * amse of singular value hard thresholding . * fix @xmath121 and @xmath122 .",
    "let @xmath193 and @xmath194 be matrix sequences in our asymptotic framework , and let @xmath195 .",
    "then @xmath196 where    lcl [ m : eq ] + & &    ( x+)(x + ) - ( x^2- ) & x x _ * ( ) + x^2 & x < x _ * ( )    and @xmath197 is given by eq . .",
    "figure [ amse : fig ] shows the amse of lemma [ amse : lem ] , in square case @xmath152 and nonsquare cases @xmath198 , @xmath199 and @xmath200 .    by definition ,",
    "@xmath201 where @xmath202 .",
    "observe that    llcl [ finite - dif : eq ] + & & & @xmath203 = + & & & @xmath204 + + & & & @xmath205 -2@xmath206= + & & & _ i=1^m_n _ h(y_n , i;)^2 + _ i=1^rx_i^2 - + & & & 2_i , j=1^rx_i _ h(y_n , j ; ) @xmath207 = + & & & _ i = r+1^m_n _ h(y_n , i;)^2 + _ i=1^r .    since @xmath208",
    ", the leftmost term above converges almost surely to zero . when @xmath209 , by lemma [ y - asy : lem ] and lemma [ inner - asy : lem ] , only the term @xmath172 survives and eq .",
    "assume now that @xmath184 .",
    "we now consider the a.s . limiting value of each of the remaining terms in . for the term @xmath210 , by lemma [ y - asy : lem ] , for @xmath211 we have    lcl + & &    ( x_i+)(x_i + ) & ( x_i+)(x_i + ) ^2 + 0 & ( x_i+)(x_i + ) < ^2",
    "turning to the rightmost term of , by lemma [ inner - asy : lem ] , for @xmath212 we find that it equals    lcl + & &    & x_i = x_j + 0 & x_ix_j    where @xmath213 .",
    "furthermore , since for all @xmath214 and @xmath215 we have @xmath216 , we find that for @xmath211 ,    lcl + & & _ j=1^r(y_n , j ; ) @xmath217 @xmath218 .    for the rightmost term of we conclude that    lcl + & & _ 1jr : x_j = x_i _ n = + & &    & ( x_i+)(x_i + ) ^2 + 0 & ( x_i+)(x_i + ) < ^2    ,    where we have used lemma [ y - asy : lem ] again . collecting the terms ,",
    "we find for the limiting value of that @xmath219 where @xmath220 is given by as required .",
    "for the tsvd , the same argument gives :    [ amse - tsvd : lem ] * amse of tsvd .",
    "* fix @xmath121 and @xmath122 .",
    "let @xmath193 and @xmath194 be matrix sequences in our asymptotic framework , and let @xmath195 .",
    "then @xmath221 where    lcl [ m_tsvd : eq ] + & &    ( x+)(x + ) - ( x^2- ) & x ^1/4 + ( 1+)^2 + x^2 & x ^1/4    .",
    "we now to turn to prove our main results .",
    "let @xmath222 where @xmath67 is defined in and @xmath197 is defined in",
    ". then @xmath223 it follows that for all @xmath224 and @xmath195 ,    lcl + & = & \\ { x^2 , ( x+)(x+ ) - ( x^2- ) } + & & m(_,x )    and the theorem follows from eq . .",
    "figure [ lf : fig ] provides a visual explanation of this proof for the square ( @xmath152 ) case .      for @xmath225 , by lemma [ amse : lem ] and lemma",
    "[ amse - tsvd : lem ] we have @xmath226 for @xmath227 , by lemma [ amse - tsvd : lem ] and theorem [ inadmis : thm ] we have @xmath228      theorem [ mmx - square : thm ] is a special case of theorem [ mmx - nonsquare : thm ] . by , it is enough to consider the univariate function @xmath229 defined in .",
    "the theorem follows from lemma [ amse : lem ] using the following simple observation .",
    "let @xmath156 and @xmath230 .",
    "denote by @xmath197 the unique positive solution to the equation @xmath231 .",
    "let @xmath61 be the unique solution to the equation in @xmath0 @xmath232 then for the function @xmath220 defined in , we have @xmath233    note that the least favorable situation occurs when @xmath234 , and that @xmath197 is precisely the value of @xmath168 for which the corresponding limiting data singular value satisfies @xmath235 . in other words ,",
    "the least favorable situation occurs when the data singular values all coincide with each other and with the chosen hard threshold .",
    "let @xmath236 denote the empirical cumulative distribution function ( cdf ) of the squared singular values of @xmath237 .",
    "write @xmath238 where @xmath239 is a functional which takes as argument the cdf and delivers the median of that cdf . under our asymptotic framework ,",
    "almost surely , @xmath236 converges weakly to a limiting distribution , @xmath240 , the cdf of the marenko - pastur distribution with shape parameter @xmath132 @xcite .",
    "this distribution has a positive density throughout its support , in particular at its median .",
    "the median functional is continuous for weak convergence at @xmath241 , and hence , almost surely , @xmath242 it follows that , @xmath243",
    "our results were formally stated for a sequence of models of the form @xmath21 , where @xmath17 is a non - random matrix to be estimated , and the entries of @xmath22 are i.i.d samples from a distribution that is orthogonally invariant ( in the sense that the matrix @xmath22 follows the same distribution as @xmath244 , for any orthogonal @xmath245 and @xmath119 ) . while gaussian noise is orthogonally invariant , many common distributions , which one could consider to model white observation noise , are not .",
    "one attractive feature of the discussion on optimal choice of singular value hard threshold , presented above , is that the amse @xmath246 only depends on the signal matrix @xmath17 through its rank , or more specifically , through its nonzero singular values @xmath136 . if the distribution of @xmath22 is not orthogonally invariant , mse ( or amse ) losses this property and depends on properties of @xmath17 other than its rank .",
    "this point is discussed extensively in @xcite .    in general white noise , which is not necessarily orthogonally invariant",
    ", one can still allow mse to depend on @xmath17 only through its singular values by placing a prior distribution on @xmath17 and shifting to a model where it is a random , instead of a fixed , matrix .",
    "specifically , consider an alternative asymptotic framework to the one in section [ framework : subsec ] , in which the sequence denoising problems @xmath114 satisfies the following assumptions :    1 .",
    "_ general white noise : _ the entries of @xmath116 are i.i.d samples from a distribution with zero mean , unit variance and finite fourth moment .",
    "2 .   _ fixed signal column span and uniformly distributed signal singular vectors : _ let the rank @xmath121 be fixed and choose a vector @xmath122 with coordinates @xmath123 .",
    "assume that for all @xmath2 , @xmath125 is a singular value decomposition of @xmath127 , where @xmath128 and @xmath129 are uniformly distributed random orthogonal matrices .",
    "formally , @xmath128 and @xmath129 are sampled from the haar distribution on the @xmath7-by-@xmath7 and @xmath2-by-@xmath2 orthogonal group , respectively .",
    "asymptotic aspect ratio @xmath132 : _ the sequence @xmath133 is such that @xmath134 .",
    "the second assumption above implies that @xmath127 a `` generic '' choice of matrix with nonzero singular values @xmath136 , or equivalently , a generic choice of coordinate systems in which the linear operator corresponding to @xmath17 is expressed .",
    "the results of @xcite , which we have used , hold in this case as well .",
    "it follows that lemma [ amse : lem ] and lemma [ amse - tsvd : lem ] , and consequently all our main results , hold under this alternative framework .",
    "in short , in general white noise , all our results hold if one is willing to only specify the signal singular values , rather than the signal matrix , and consider a `` generic '' signal matrix with these singular values .",
    "we have calculated the exact optimal threshold @xmath79 in a certain asymptotic framework .",
    "the practical significance of our results hinges on the validity of the amse as an approximation to mse , for values of @xmath247 and error distributions encountered in practice .",
    "this in turn depends on the simultaneous convergence of three terms :    * convergence of the top data singular values @xmath177 ( @xmath248 ) to the limit in lemma [ y - asy : lem ] , * convergence of the angle between the top data singular vectors @xmath249 and their respective signal singular vectors to the limit in lemma [ inner - asy : lem ] , and * convergence of the rest of the data singular values @xmath177 ( @xmath250 to the interval @xmath251 $ ] .",
    "analysis of each of these terms for specific error distributions is beyond our current scope .",
    "figure [ 4a : fig ] contains a few sample comparisons of amse and empirical mse we have performed .",
    "the matrix sizes and number of monte carlo draws are small enough to demonstrate that amse is a reasonable approximation even for relatively small low - rank matrices . as convergence of the empirical spectrum to its limit",
    "is known to depend on moments of the underlying distributions , we include results for different error distributions .",
    "amse is found to be a useful proxy to mse even in small matrix sizes .",
    "amse of svht was found to be inaccurate when : ( i ) the rank fraction is nontrivial ( e.g @xmath252 , @xmath253 shown at the bottom of figure [ 4a : fig ] ) ; ( ii ) the threshold @xmath0 is very close to the approximate bulk edge @xmath254 . in case ( i ) ,",
    "interaction effects between singular values , which are ignored in our asymptotic framework , start to have non - negligible effect . in case ( ii ) , where the discontinuity of the svht nonlinearity is placed close to the bulk edge , the distribution of the largest `` non - signal '' singular value @xmath255 , which is known in some cases to be asymptotically a tracy - widom distribution @xcite , becomes important .",
    "indeed , some data singular values from the bulk manage to pass the threshold @xmath0 and cause their singular vectors to be included in the estimator @xmath110 .",
    "our derivation of amse assumed however that no such singular vectors are included in @xmath110 , since @xmath256 .",
    "note however that the main recommendation of this paper is that one should _ not _ threshold at or near he bulk edge , as explained in detail above .",
    "therefore , from a practical perspective , the inaccuracy of amse for svht with @xmath0 near the bulk edge is slightly irrelevant .",
    "the asymptotic framework considered here is perhaps the simplest nontrivial model for matrix denoising .",
    "it allows one to calculate , in amse , basically any quantity of interest , for any denoiser of interest .",
    "the fundamental elements of matrix denoising in white noise , which underly more complicated models , are present yet understandable and quantifiable .",
    "for example , the amse of any denoiser based on singular value shrinkage contains a component due to noise contamination in the data singular vectors , and this component determines a fundamental lower bound on amse .    we conjecture that results calculated in this model , which are not attached to a specific assumption on rank ( e.g , the constants in table [ mmx - square - l1:tab ] , which determine the minimax amse over nuclear norm balls ) remain essentially correct in more complicated models .",
    "the decision - theoretic landscape as it appears through the naive prism of our asymptotic framework is extremely simple : there is a unique admissible hard thresholding rule , and moreover a unique admissible shrinkage rule , for singular values .",
    "this is of course quite different from the situation encountered , for example , in estimating normal means .",
    "the reason is the extreme simplicity of our model .",
    "for example , we have replaced the data singular values , which are random for finite matrix size , with their almost sure limits , and in effect neglected their random fluctuations around these limits .",
    "these fluctuations are now well understood ( see for example @xcite ) .",
    "we have ignored this structure . however , including these second - order terms in the asymptotic distributions is only likely to achieve second - order improvements in mse over our suggested optimal truncation threshold .",
    "in the code supplement @xcite we offer a matlab software library that includes :    1 .   a function that calculates the optimal shrinkage coefficient in known or unknown noise level .",
    "scripts that generate each of the figures in this paper .",
    "3 .   a script that generates figures similar to figure [ 4a : fig ] , which compare amse to mse in various situations .",
    "the authors would like to thank andrea montanari for pointing to the work of shabalin and nobel , drew nobel and sourav chatterjee for helpful discussions , art owen for pointing to the work of perry , and the anonymous referees for their useful suggestions .",
    "this work was partially supported by nsf dms 0906812 ( arra ) .",
    "mg was partially supported by a william r. and sara hart kimball stanford graduate fellowship .",
    "d.  l. donoho and m.  gavish , `` code supplement to ` the optimal hard threshold for singular values is @xmath9 ' , '' http://purl.stanford.edu/vg705qn9070 , 2014 , accessed 27 march 2014 .",
    "[ online ] .",
    "available : http://purl.stanford.edu/vg705qn9070        o.  alter , p.  brown , and d.  botstein , `` singular value decomposition for genome - wide expression data processing and modeling , '' _ proceedings of the national academy of sciences _ , vol .  97 , no .  18 , pp . 1010110106 , aug . 2000 .",
    "t.  d. lagerlund , f.  w. sharbrough , and n.  e. busacker , `` spatial filtering of multichannel electroencephalographic recordings through principal component analysis by singular value decomposition . ''",
    "_ journal of clinical neurophysiology : official publication of the american electroencephalographic society _ , vol .",
    "14 , no .  1 ,",
    "7382 , jan . 1997 .    a.  l. price , n.  j. patterson , r.  m. plenge , m.  e. weinblatt , n.  a. shadick , and d.  reich , `` principal components analysis corrects for stratification in genome - wide association studies . '' _ nature genetics",
    "_ , vol .",
    "38 , no .  8 , pp .",
    "9049 , aug .",
    "2006 .",
    "s.  wold , `` cross - validatory estimation of the number of components in factor and principal components components models , '' _ technometrics _ , vol .",
    "20 , no .  4 , pp .",
    "397405 , 1978 .",
    "[ online ] .",
    "available : http://www.tandfonline.com/doi/pdf/10.1080/00401706.1978.10489693      a.  b. owen and p.  o. perry , `` bi - cross - validation of the svd and the nonnegative matrix factorization , '' _ the annals of applied statistics _",
    ", vol .  3 , no .  2 , pp .",
    "564594 , jun . 2009 .",
    "[ online ] .",
    "available : http://projecteuclid.org/euclid.aoas/1245676186      d.  achlioptas and f.  mcsherry , `` fast computation of low rank matrix approximations , '' in _ proceedings of the thirty - third annual acm symposium on theory of computing _ , 2001 , pp .",
    "[ online ] .",
    "available : http://dl.acm.org/citation.cfm?id=380858    y.  azar , a.  fiat , a.  r. karlin , f.  mcsherry , and j.  saia , `` spectral analysis of data , '' in _ proceedings of the thirty - third annual acm symposium on theory of computing _ , 2001 , pp .",
    "[ online ] .",
    "available : http://dl.acm.org/citation.cfm?id=380859    p.  j. bickel and e.  levina , `` covariance regularization by thresholding , '' _ the annals of statistics _",
    "36 , no .  6 , pp .",
    "25772604 , dec .",
    "[ online ] .",
    "available : http://projecteuclid.org/euclid.aos/1231165180        r.  h. keshavan , a.  montanari , and s.  oh , `` matrix completion from a few entries , '' _ ieee transcations on information theory _",
    "56 , no .  6 , pp.2980 - 2998 , 2010 .",
    "[ online ] .",
    "available : http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5466511            d.  l. donoho , m.  gavish and i.  m. johnstone , `` optimal shrinkage of eigenvalues in the spiked covariance model , '' _ stanford university statistics department technical report 2013 - 10 _ , 2013 .",
    "[ online ] .",
    "available : http://arxiv.org/abs/1311.0851    f.  benaych - georges and r.  r. nadakuditi , `` the singular values and vectors of low rank perturbations of large rectangular random matrices , '' _ journal of multivariate analysis _",
    "120135 , oct . 2012 .",
    "y.  q. yin , z.  d. bai , and p.  r. krishnaiah , `` on the limit of the largest eigenvalue of the large dimensional sample covariance matrix , '' _ probability theory and related fields _ , vol .",
    "78 , pp . 509-521 , 1988 .",
    "cai , e.  j. cands , and z.  shen , `` a singular value thresholding algorithm for matrix completion , '' _ siam journal on optimization _ , vol .",
    "20 , no .  4 , p. 1956 , 2008 .",
    "[ online ] .",
    "available : http://arxiv.org/abs/0810.3286    e.  j. cands , c.  a. sing - long , and j.  d. trzasko , `` unbiased risk estimates for singular value thresholding and spectral estimators , '' _ ieee transactions on signal processing _ , vol .",
    "61 , no .",
    "19 , pp . 46434657 , 2012 .",
    "d.  l. donoho and m.  gavish , `` minimax risk of matrix denoising by singular value thresholding , '' _ stanford university statistics department technical report 2013 - 03 _ , 2013 .",
    "[ online ] .",
    "available : http://arxiv.org/abs/1304.2085    s.  kritchman and b.  nadler , `` non - parametric detection of the number of signals : hypothesis testing and random matrix theory , '' _ signal processing , ieee transactions _ , vol .",
    "57 , no .",
    "10 , pp . 39303941 , 2009 .",
    "z.  bai and j .- f .",
    "yao , `` central limit theorems for eigenvalues in a spiked population model , '' _ annales de linstitut henri poincare ( b ) probability and statistics _",
    "44 , no .  3 , pp .",
    "447474 , jun . 2008 .",
    "matan gavish received the dual b.sc .",
    "degree in mathematics and physics from tel aviv university ( tau ) in 2006 and the m.sc .",
    "degree in mathematics from the hebrew university of jerusalem in 2008 .",
    "he is currently a doctoral student in statistics at stanford university , in collaboration with the yale university program in applied mathematics .",
    "his research interests include applied harmonic analysis , high - dimensional statistics and computing .",
    "he was in the adi lautman interdisciplinary program for outstanding students at tau from 2002 to 2006 and held a william r. and sara hart kimball stanford graduate fellowship from 2009 to 2012 .",
    "david l. donoho is a professor at stanford university .",
    "his research interests include computational harmonic analysis , high - dimensional geometry , and mathematical statistics .",
    "dr . donoho received the ph.d .",
    "degree in statistics from harvard university , and holds honorary degrees from university of chicago and ecole polytechnique federale de lausanne .",
    "he is a member of the american academy of arts and sciences and the us national academy of sciences , and a foreign associate of the french acadmie des sciences ."
  ],
  "abstract_text": [
    "<S> we consider recovery of low - rank matrices from noisy data by hard thresholding of singular values , in which empirical singular values below a threshold @xmath0 are set to @xmath1 . </S>",
    "<S> we study the asymptotic mse ( amse ) in a framework where the matrix size is large compared to the rank of the matrix to be recovered , and the signal - to - noise ratio of the low - rank piece stays constant . </S>",
    "<S> the amse - optimal choice of hard threshold , in the case of @xmath2-by-@xmath2 matrix in white noise of level @xmath3 , is simply @xmath4 when @xmath3 is known , or simply @xmath5 when @xmath3 is unknown , where @xmath6 is the median empirical singular value . for nonsquare @xmath7 by @xmath2 matrices with @xmath8 the thresholding coefficients @xmath9 and @xmath10 </S>",
    "<S> are replaced with different provided constants that depend on @xmath11 . </S>",
    "<S> asymptotically , this thresholding rule adapts to unknown rank and unknown noise level in an optimal manner : it is _ always _ </S>",
    "<S> better than hard thresholding at any other value , and is _ always _ </S>",
    "<S> better than ideal truncated svd ( tsvd ) , which truncates at the true rank of the low - rank matrix we are trying to recover . </S>",
    "<S> hard thresholding at the recommended value to recover an @xmath2-by-@xmath2 matrix of rank @xmath12 guarantees an amse at most @xmath13 . in comparison , </S>",
    "<S> the guarantees provided by tsvd , optimally tuned singular value soft thresholding and the best guarantee achievable by any shrinkage of the data singular values are @xmath14 , @xmath15 , and @xmath16 , respectively . </S>",
    "<S> the recommended value for hard threshold also offers , among hard thresholds , the best possible amse guarantees for recovering matrices with bounded nuclear norm . </S>",
    "<S> empirical evidence suggests that performance improvement over tsvd and other popular shrinkage rules can be substantial , for different noise distributions , even in relatively small @xmath2 .    singular values shrinkage , optimal threshold , low - rank matrix denoising , unique admissible , scree plot elbow truncation , quarter circle law , bulk edge . </S>"
  ]
}