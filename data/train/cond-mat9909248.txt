{
  "article_text": [
    "in this paper , we consider the problem of learning binary classification tasks from examples with neural networks .",
    "the network s architecture and the neurons weights are determined based on a training set of examples or patterns @xmath0 , composed of @xmath1 input vectors @xmath2 in @xmath3-dimensional space and their corresponding classes @xmath4 .",
    "the latter are the targets to be learned .",
    "hereafter , we call @xmath5 the _ size _ of the training set . one interesting property that characterizes a neural network is its storage capacity , which is the size @xmath6 of the largest training set with arbitrary targets the network is able to learn ( with probability @xmath7 ) .",
    "the perceptron , a single neuron connected to its inputs through @xmath3 weights , performs linear separations and has a storage capacity @xmath8  @xcite .",
    "it is possible to increase the storage capacity of neural networks by considering more complicated architectures , like those with one hidden layer of @xmath9 units .",
    "such monolayer perceptrons map each input vector @xmath10 to a binary @xmath9-dimensional internal representation determined by the outputs of @xmath9 perceptrons , which in this context are also called hidden units .",
    "the overall network s output to an input pattern is a boolean function of the corresponding internal representation .",
    "this function may be learned by an output perceptron , but then the internal representations of the training set must be linearly separable . in order",
    "to get rid of this constraint , networks implementing particular functions of the hidden states have been investigated . among these ,",
    "the committee machine , whose output is the class of the majority of the hidden units , and the parity machine , whose output is the product of the @xmath9 components of the internal representation , have deserved particular attention  @xcite .    learning consists of adapting the number of hidden perceptrons and their weights in order that the outputs of the network to the training examples match the corresponding targets .",
    "the main problem is that the internal representations are unknown . besides the chir algorithm  @xcite , that determines the internal representations through a random process involving learning faithful sets of internal representations with",
    "@xmath9 fixed , most learning algorithms build the internal representations through a deterministic incremental procedure that determines @xmath9 by construction . in the latter case ,",
    "the hidden perceptrons are trained one after the other with targets that differ from one algorithm to another , until the correct classification is achieved .",
    "the first incremental procedure has been proposed by gallant  @xcite .",
    "many other authors developed further this idea , like mzard and nadal with the tiling algorithm  @xcite , rujn and marchand with the sequential learning algorithm  @xcite and biehl and opper with the tilinglike learning algorithm  @xcite .",
    "other variations have been proposed  @xcite .",
    "it has been argued that these incremental procedures may require a number of hidden units much larger than the number actually needed by a network making use of its full storage capacity . in the following",
    "we distinguish thus the algorithm s capacity , defined as the size of the largest training set ( with arbitrary targets ) learnable with the algorithm , from the capacity of the network with the same architecture .",
    "clearly the former can not be larger than the latter .",
    "an upper bound for the storage capacity of the parity machine with @xmath9 hidden perceptrons has been obtained by mitchinson and durbin  @xcite through a geometric approach : @xmath11 .",
    "recent replica calculation results , obtained in the limit of a large number of hidden perceptrons ( @xmath12 )  @xcite , strongly suggest that this upper bound may effectively be reached .",
    "however , the learning problem remains : is there a learning algorithm whose capacity saturates this bound ?",
    "this question was addressed recently in  @xcite within the same statistical mechanics framework as the present work . in spite of a thorough analysis",
    ", no clear - cut conclusion could be drawn in the asymptotic regime of large @xmath9 , because of a lack of precision in the numerical integration of the corresponding equations .    in this paper , we determine analytically the storage capacity of a parity machine built with the tilinglike learning algorithm ( tla ) .",
    "our results present strong evidence showing that the storage capacity of the obtained network is close to the upper bound , at least within the replica - symmetry approximation .",
    "the paper is organized as follows : in section [ sec : tla ] , we describe the tla . the conditions necessary for the tla to converge impose strong constraints on the cost function used to train the hidden perceptrons .",
    "these are discussed in section [ sec : convcond ] . despite intensive research in this field ,",
    "no analytic results on the learning properties of the perceptron with threshold , in the asymptotic limit @xmath13 needed here , exist .",
    "these are deduced in section [ sec : percept ] for the gardner cost function with vanishing and finite margin , within the replica - symmetry ( rs ) approximation .",
    "as this approximation is known to provide only a lower bound to the perceptron s actual training error  @xcite , we also determined an upper bound through a generalization of the kuhn - tucker ( kt ) cavity method proposed by gerl and krey  @xcite .",
    "the general expression for the number of hidden perceptrons generated by the tla in the limit @xmath13 is deduced in section [ sec : k(alpha ) ] .",
    "our main result is that the number of hidden units needed by the tla to converge grows proportionally to @xmath14 in the large @xmath15 limit , where @xmath16 in the rs approximation and @xmath17 within the kt cavity method , provided that the hidden perceptrons learn through the minimization of their training errors .",
    "our results are discussed and compared both to the mitchinson and durbin bound  @xcite and to the numerical results obtained by west and saad  @xcite .",
    "the general conclusion is left to section [ sec : concl ] .",
    "in the following , we describe the tilinglike learning algorithm ( tla ) considered in the following because of its simplicity .",
    "the tla needs hidden perceptrons with a threshold to generate the parity machine .",
    "the classification performed by a perceptron is a linear separation defined by a hyperplane in the @xmath3-dimensional input space , of normal vector @xmath18 ( @xmath19 ) and distance to the origin @xmath20 .",
    "the @xmath3 components of @xmath18 are the perceptron s weights and @xmath20 is its threshold .",
    "an example @xmath10 is classified as follows :    @xmath21    as already pointed out in  @xcite the threshold is useful in the case of unbalanced training sets , containing more examples of one class than of the other . as we will see in the following ,",
    "this is the case for the successive perceptrons included by the tla .    in the first learning step of the algorithm ,",
    "the parameters @xmath22 and @xmath23 of a perceptron are adapted in order to obtain the lowest possible number of training errors .",
    "this is usually done through the minimization of a cost function :    @xmath24    where the potential @xmath25 is a function of @xmath26 , the stability of the example @xmath27 :    @xmath28    the stability is positive if and only if the example is correctly classified .",
    "its absolute value is the distance of the example to the separating hyperplane .    in principle",
    ", there is some freedom in the choice of the potential @xmath29 . as it has to penalize training errors , it has to be a decreasing function of @xmath30 .",
    "considering as cost function the number of training errors corresponds to the particular choice @xmath31 , where @xmath32 is the heaviside function .",
    "other potentials , that do not minimize the number of training errors but possess interesting learning or algorithmic properties may be chosen .",
    "examples are @xmath33 where @xmath34 is a fixed positive margin chosen _ a priori_. the case @xmath35 corresponds to the so - called gardner potential  @xcite which reduces to the error counting function for @xmath36 .",
    "the potential defined by @xmath37 corresponds to the perceptron learning algorithm  @xcite and @xmath38 to the adatron  @xcite .    after learning , the training error of the first perceptron",
    "is :    @xmath39    where @xmath40 , the class given by the perceptron to the example @xmath41 , depends through equation  ( [ classe ] ) on the parameters @xmath42 and @xmath43 that minimize the cost function .",
    "if the training error is zero , the learning procedure stops .",
    "then , the class associated to the patterns by the parity machine is just the class given by the first perceptron .",
    "otherwise , another perceptron is included and trained with the aim of separating the correctly learned examples from the wrongly learned ones .",
    "the corresponding training set @xmath44 contains the same input examples as @xmath0 with new targets @xmath45 defined as follows :",
    "@xmath46 if the example @xmath41 is correctly classified by the previous perceptron and @xmath47 if not",
    ". these targets may be expressed as @xmath48 .",
    "notice that a fraction @xmath49 of patterns have targets @xmath50 , and a fraction @xmath51 have targets @xmath52 .",
    "since we expect the training error @xmath51 to be smaller than @xmath53 , the probability of targets @xmath52 is smaller than that of targets @xmath50 .",
    "the successive perceptrons need a threshold to learn such biased training sets .",
    "otherwise , the tilinglike construction can not converge .    the parameters @xmath54 and @xmath55 of the second perceptron are learned with the training set @xmath56 , minimizing the same cost function as the first one . the same procedure , in which the perceptron @xmath57 learns the training set @xmath58 , has to be iterated until @xmath59",
    "then , the product @xmath60 of the classes @xmath61 given by the hidden perceptrons to an example @xmath41 corresponds to the target @xmath62  @xcite , as @xmath63 .",
    "thus , the tla constructs a parity machine with @xmath9 hidden units .",
    "it has been shown that if the examples are binary  @xcite , or real - valued vectors in general position  @xcite , there is a solution that satisfies the tla construction with the property that @xmath64 is a succession of decreasing integer numbers .",
    "thus , a finite @xmath65 exists for which @xmath59 .    in the following , we are interested in the _ typical _ number @xmath9 of hidden perceptrons necessary for the tla to learn a training set of size @xmath15 .",
    "this is obtained in the thermodynamic limit where @xmath3 and @xmath66 diverge keeping @xmath67 constant . in this limit , @xmath9 is expected to be independent of the particular set of training patterns , and to depend only on @xmath15 . however , as @xmath68 , it is not possible to argue that @xmath64 is a succession of strictly decreasing numbers in order to guarantee the convergence of the tla in a finite number of steps ( i.e. of hidden units ) . in particular , the solution in which a single example is correctly learned at each step , used by the convergence proofs  @xcite at finite @xmath3 , leads to @xmath69 . in order to obtain a _ finite _",
    "number @xmath70 in the thermodynamic limit , each perceptron has to learn at least a number of examples of the order of @xmath3 .",
    "this imposes some general conditions on the learning algorithm used to train the perceptrons .",
    "it is worth to point out that the conditions for convergence with finite @xmath9 in the thermodynamic limit do not guarantee the convergence for _ all _ the possible training sets of size @xmath15 .",
    "this is due to the probabilistic nature of the statistical physics results , which predict the average behaviour .",
    "the results may not be correct for subsets of zero measure in the space of training sets , and in particular for the worst case .    as described before ,",
    "the training set @xmath71 used to train the perceptron @xmath72 contains a fraction @xmath73 of patterns with targets @xmath50 , and a fraction @xmath74 of patterns with targets @xmath52 .",
    "these targets are slightly correlated , as they are determined by the training errors of the preceding perceptron .",
    "however , it has been shown that these correlations are weak  @xcite .",
    "we neglect them in the limit @xmath75 considered in the following .",
    "thus , we consider that the targets to be learned by the successive perceptrons are i.i.d",
    ". random variables , and have a probability @xmath76 to be @xmath50 and @xmath74 to be @xmath52 .",
    "as this neglects the constraints imposed by the correlations on the minimization of the training error , we expect that the assumption of uncorrelated targets underestimate the perceptrons training errors .",
    "it follows that our estimation of the number @xmath70 of perceptrons necessary to construct the parity machine is a lower bound to the actual value .",
    "consider a perceptron learning a training set of size @xmath15 with targets given by the following biased probability distribution :    @xmath77    if @xmath78 is the perceptron s training error , i.e. the fraction of wrongly learned examples , there is a simple relationship between the training errors @xmath74 and @xmath79 of two successive hidden perceptrons :    @xmath80    since the bias in the probability of the targets of perceptron @xmath72 is due to the training error of the preceding unit .",
    "the successive training errors @xmath79 must decrease monotonically with @xmath72 and eventually vanish for a finite @xmath9 . otherwise the tla does not converge .",
    "taking equation  ( [ relationet ] ) into account , this imposes that :    @xmath81    condition  ( [ condition ] ) restricts the possible potentials in the cost function  ( [ cost ] ) . for example , in the following section we show that the perceptron and the adatron potentials  @xcite do not satisfy the condition  ( [ condition ] ) for all @xmath15 when @xmath82 .",
    "the stopping condition of the tla imposes that there is a finite value of @xmath9 such that :    @xmath83    this in turn imposes that for all @xmath15 , there always exists @xmath84 such that @xmath85 .",
    "thus , the stopping condition  ( [ stop ] ) imposes that the inverse function @xmath86 diverges as @xmath87 .",
    "in fact , @xmath86 is the storage capacity of a perceptron learning targets drawn with the biased probability  ( [ prob ] ) ( in the literature , the bias is usually defined as @xmath88 ) .",
    "actually , the divergence of @xmath86 occurs whenever the potential @xmath29 vanishes for @xmath89 and is strictly positive for @xmath90 .",
    "this is the case for the gardner potential with @xmath91 , for which @xmath92  @xcite .",
    "however , even if the perceptron has been extensively studied , very few results exist for the case of training sets with biased distributions of targets  @xcite .",
    "in particular , the asymptotic behaviour of the learning curves @xmath78 as a function of @xmath15 is unknown",
    ". these are deduced in the next section .",
    "the reader not interested in these intermediate calculations may skip them and go straight to section [ sec : k(alpha ) ] . only the results displayed by equations  ( [ eqet ] ) , ( [ eqetkappa ] ) , ( [ eqetkt ] ) and ( [ eqetktkappa ] ) are used to determine the asymptotic behaviour of the tla .",
    "in order to learn such training sets with biased distributions of targets , the perceptron must have a threshold , as the separating hyperplanes that minimize the training error do not contain the origin . here",
    "we present new analytic results , mainly in the asymptotic regime @xmath93 , for the gardner cost function defined by the potential :    @xmath94    for @xmath95 , the corresponding cost function is the number of training errors . for @xmath96 ,",
    "the cost function is the number of examples with stability  ( [ stab ] ) smaller than @xmath97 .",
    "the section is divided in two parts . in the first one we derive results within the replica - symmetry ( rs ) approximation , which is known to underestimate the training error . in the second part",
    "we obtain upper bounds for the training error , using a cavity method .",
    "we briefly recall the main steps of the replica calculation , that follows the same lines as  @xcite .",
    "as we are interested in the properties of the minimum of the cost function , a temperature @xmath98 is introduced and the cost function is considered as an energy . the corresponding partition function writes :    @xmath99    where the components of @xmath18 are the weights , and @xmath20 is the perceptron s threshold .",
    "@xmath100 is a training set of size @xmath15 .",
    "the input vectors @xmath41 are drawn from a gaussian distribution with zero mean and unit variance in all the directions .",
    "the targets have the biased distribution ( [ prob ] ) .    following gardner s approach ,",
    "the patterns of the training set are considered as frozen disordered variables .",
    "the replica trick allows to calculate the mean free energy in the thermodynamic limit ( @xmath101 , @xmath68 and @xmath15 constant ) averaged over all possible training sets , as follows :    @xmath102    where the bar stands for the mean over the training sets with same size @xmath15 .",
    "thus , the free energy is obtained through the averaging of a partition function of @xmath103 replicas of the original system .",
    "hereafter we assume replica symmetry ( rs ) , i.e. that the replicas are equivalent under permutation .",
    "however , it is well known that replica symmetry breaks down when the training error is finite  @xcite .",
    "calculations including one step of replica symmetry breaking have shown that the training error obtained within the rs approximation is a lower bound for the actual one  @xcite .    assuming that the weights have a uniform prior probability over the surface of the @xmath3-dimensional sphere of unitary radius , and the threshold a uniform distribution over the real axis between @xmath104 and @xmath105 , the free energy within the rs approximation writes :    @xmath106    where the function @xmath107 is :    @xmath108    with @xmath109 the function that minimizes : @xmath110 .",
    "@xmath111 is the usual order parameter in replica calculations ( @xmath112 with @xmath113 and @xmath114 the directions corresponding to two different replicas ) .",
    "the parameters @xmath111 and @xmath20 are solutions of the following extremum conditions :    @xmath115    the training error @xmath116 may be easily deduced by integration of the distribution of stabilities over the negative values  @xcite , yielding :    @xmath117    equations ( [ partition ] ) to ( [ foncet ] ) are valid for any potential @xmath29 in ( [ cost ] ) . in the following , we concentrate specifically on the gardner potential  ( [ potential ] ) .",
    "the function @xmath118 that minimizes @xmath119 for a given @xmath97 is :    @xmath120    introducing ( [ lambda ] ) into ( [ foncg ] ) , we deduce @xmath121 .",
    "the conditions ( [ foncextr ] ) allow to determine the equations for @xmath111 and @xmath20 :    @xmath122    where @xmath123 .",
    "the distribution of stabilities of the training patterns is @xmath124 with :    @xmath125    @xmath126 presents a two band structure with a gap between @xmath127 and @xmath128 .",
    "notice that only if @xmath129 the lower band corresponds to wrongly classified patterns . if @xmath130 , then @xmath131 may become positive for sufficiently small values of @xmath111 . in that case , the training error is only a fraction of the patterns lying in the lower band . taking this into account ,",
    "the training error @xmath78 is  :    @xmath132}^{+ \\infty } dy + \\varepsilon \\int_{\\max [ \\theta ,   \\sqrt{2c } - \\kappa + \\theta]}^{+\\infty } dy,\\ ] ]    we derive separately the asymptotic properties for @xmath95 and for @xmath133 , for reasons that will become clear in the following .",
    "we consider first the case @xmath95 .",
    "the band of positive stabilities starts at @xmath134 so that the gap , of width @xmath135 , lies strictly in the region of negative stabilities .",
    "as we expect that the gap vanishes for @xmath13 , we look for solutions of the extremum equations with @xmath136 and @xmath137 ( notice that @xmath20 is negative for @xmath82 ) with the product @xmath138 finite . introducing these assumptions into ( [ eqtheta1 ] ) , we determine @xmath139 as a function of @xmath140 :    @xmath141    the relation between @xmath142 and @xmath140 follows from ( [ eqc1 ] ) and ( [ afonc ] ) :    @xmath143    @xmath140 and @xmath20 are increasing functions of @xmath139 as expected . for a symmetric distribution of targets ( @xmath144 )",
    "then @xmath145 corresponding to a vanishing threshold .",
    "conversely , if all the targets are @xmath50 ( @xmath146 ) , the threshold diverges to @xmath147 . for finite @xmath82 , the absolute value of the threshold is an increasing function of @xmath15 . from equation ( [ thetafonc ] ) we obtain the development @xmath148 .",
    "notice that neglecting @xmath149 with respect to @xmath150 is an approximation only valid for large enough @xmath15 ( @xmath151 ) .",
    "as was already pointed out in  @xcite , this behaviour can not be deduced by solving the equations ( [ eqc1 ] ) and ( [ eqtheta1 ] ) numerically .",
    "the training error @xmath152  ( [ et1 ] ) with @xmath95 in the limit @xmath13 is then :    @xmath153    using equations  ( [ thetafonc ] ) and ( [ etfonc ] ) , we deduce :    @xmath154    where @xmath155 is the inverse function of @xmath156 given by ( [ afonc ] ) .    consider now the gardner potential with finite @xmath97 .",
    "although , a solution of equations ( [ eqc1 ] ) and ( [ eqtheta1 ] ) under the assumption that @xmath157 with finite @xmath20 in the limit @xmath13 exists , it does _ not _ correspond to the correct extremum of @xmath107  ( [ foncg ] ) .",
    "it is however worth to examine it . the corresponding value of @xmath20 as a function of @xmath139 and @xmath97 follows form ( [ eqtheta1 ] ) , and the relation between @xmath158 and @xmath111 from ( [ eqc1 ] ) .",
    "we find :    @xmath159    as @xmath160 , the training error given by equation ( [ et1 ] ) writes :    @xmath161    and is larger than @xmath139 for any finite @xmath20 .",
    "notice that this ( incorrect ) solution does not satisfy the condition ( [ condition ] ) necessary for the tla to converge .",
    "in fact , the correct training error corresponds to a solution with finite gap ( @xmath162 ) and a diverging threshold ( @xmath163 ) in the large @xmath15 limit . defining @xmath164 , and keeping only the leading terms , equations  ( [ eqc1 ] ) , ( [ eqtheta1 ] ) and ( [ et1 ] ) for @xmath130 give :    @xmath165    the neglected terms are of the order @xmath166 , which are only negligible if @xmath97 is finite .",
    "the prefactor @xmath167 in  ( [ eqetkappa ] ) , that diverges when @xmath168 , reflects the existence of the different behaviours for vanishing and for finite @xmath97 .",
    "this second solution only exists for bounded potentials .",
    "the perceptron and the adatron potentials diverge for @xmath169 , and the corresponding training errors become larger than @xmath139 in the large @xmath15 limit .",
    "thus , if these learning algorithms were used to train the hidden perceptrons , the tla would not converge .",
    "although the case of unbiased targets ( i.e. @xmath144 ) is not essential for our study , we include here the corresponding analytic results for the sake of completeness . in this case , the free energy @xmath107  ( [ foncg ] ) is invariant with respect to the threshold symmetry @xmath170 .",
    "thus , @xmath171 is a trivial extremum of @xmath107 .",
    "however , as already discussed by west and saad in  @xcite , two new solutions breaking the threshold symmetry appear above a given training set size @xmath172 .",
    "the analytical expression of @xmath172 may be deduced under the assumption that the two different solutions appear continuously at @xmath172 , as in usual second order phase transitions , through a series expansion of the free energy in powers of @xmath20 :    @xmath173    due to the symmetry , the odd derivatives with respect to @xmath20 vanish .",
    "the condition :    @xmath174    defines @xmath175 at the transition .",
    "the size @xmath172 satisfies :    @xmath176    and the two new solutions that appear for @xmath177 correspond to a threshold @xmath178 .",
    "notice that the usual stability criterion for second order phase transitions , @xmath179 , can not be directly applied here because we have two order parameters .",
    "taking into account the leading corrections to @xmath111 , proportional to @xmath180 , it is straightforward to verify that the solutions with finite threshold are stable .",
    "in order to circumvent the rs approximation , we determine the training error @xmath152 using the kuhn - tucker ( kt ) cavity method proposed by gerl and krey  @xcite , that we generalize here to the case of a perceptron with a threshold learning a training set with a biased probability of targets given by  ( [ prob ] ) .",
    "contrary to the rs solution , this cavity method has been shown to overestimate the training error  @xcite . consequently",
    ", the results allow us to deduce an upper bound for the number of perceptrons needed by the tilinglike procedure to converge .",
    "the kt cavity method allows to determine the properties of the perceptron by analyzing self - consistently its response to the introduction of a new pattern into the training set .",
    "it is particularly adapted to study the properties of the gardner potential  ( [ potential ] ) because it is based on the fact that the weights minimizing the corresponding cost function are a ( conveniently normalized ) linear combination of the patterns with stability @xmath97 , which are called _ support vectors_.    let us assume that the perceptron has learned the training set and that the value of the cost function is @xmath181 .",
    "this is the number of examples with stability smaller than the margin @xmath97 .",
    "the support vectors belong to the subset of @xmath182 remaining examples that do not contribute to the cost .",
    "the perceptron s weights may be expressed as follows :    @xmath183    with @xmath184 for @xmath185 , and @xmath186 for @xmath187 .",
    "these are the so - called kuhn - tucker conditions . defining @xmath186 for examples with @xmath188 ,",
    "the normalization of the weights imposes :    @xmath189    as usual with cavity methods , we introduce a new example @xmath190 with target @xmath191 , drawn respectively with the same probability densities as the other inputs and targets in the training set . before any modification ,",
    "as the pattern @xmath192 is uncorrelated with the direction @xmath18 and its components are assumed to have a gaussian distribution , its projection onto @xmath18 has a gaussian probability .",
    "therefore , the joint probability distribution of the target @xmath191 and the stability @xmath193 before learning is :    @xmath194    where @xmath195 is defined by  ( [ prob ] ) .",
    "we assume a single ground state and we calculate the necessary adjustments of the weights @xmath18 in order to obtain self - consistent equations for the cost function as a function of @xmath15 .    if @xmath196 , no learning is needed , as the new example does not contribute to the cost .",
    "if @xmath197 , two different situations may occur .",
    "either the distance of the new example to the hyperplane is too large and the perceptron is unable to learn it , or the example is close enough and can be learned .",
    "the natural strategy to minimize the cost function is to include the new example in the subset of support vectors only if @xmath198 , where @xmath175 is a positive quantity which has to be determined self - consistently . otherwise , the weights are not modified and the new example is left in the subset of examples contributing to the cost .",
    "we are left with the problem of determining the perturbation on the weights such that examples with @xmath199 become support vectors after learning . as a first step ,",
    "this can be obtained by taking @xmath200 .",
    "however , this modifies the stabilities of the other support vectors .",
    "the coefficients @xmath184 ( @xmath201 ) must be corrected by a small amount to compensate for this perturbation .",
    "this correction in turn modifies the stability of the new example @xmath192 , and @xmath202 has to be corrected . after a full summation of the contributions , gerl and krey  @xcite",
    "have shown that the correct value of @xmath202 is :    @xmath203    where @xmath204 is the probability that @xmath184 .",
    "this probability is determined assuming that the new example is equivalent to the others :    @xmath205    having specified the learning procedure , we are able to determine @xmath175 and @xmath181 self - consistently .",
    "first of all , the normalization of the weights given by equation  ( [ norm ] ) , may be written as follows :    @xmath206    with @xmath202 given by ( [ eq_a0 ] ) for @xmath207 and @xmath208 elsewhere . combining equations  ( [ eq_pamu ] ) and ( [ eqnorm ] )",
    ", we obtain :    @xmath209    this equation , which determines @xmath175 for a fixed threshold @xmath20 , is slightly different from the rs result  ( [ eqc1 ] ) . the cost function @xmath181 is determined assuming that it remains unchanged ( to order @xmath210 ) upon learning the new example .",
    "thus , the cost per example writes :    @xmath211    notice that when @xmath212 , @xmath213  ( [ eqet ] ) represents the fraction of training errors @xmath78 and is similar to  ( [ et1 ] ) .",
    "the threshold @xmath20 may be optimized in order to minimize the cost function :    @xmath214    in the following , we solve  ( [ eq2c ] ) and  ( [ eq_theta ] ) in the large @xmath15 limit .",
    "first of all , we consider the case @xmath95 . in this case , @xmath213 ( equation  ( [ eqet ] ) ) is the training error @xmath215 .",
    "as for the rs calculation , we may assume @xmath216 and @xmath138 finite .",
    "we obtain the following equations :    @xmath217    these results differ from those obtained with the rs calculation ( equations ( [ afonc ] ) , ( [ thetafonc ] ) and ( [ eqet ] ) ) .    in the case of finite margin @xmath97 ,",
    "the pertinent assumptions in the large @xmath15 limit are @xmath218 with @xmath219 and @xmath163 . with these , here again @xmath213  ( [ eqet ] ) is the training error , and we get :    @xmath220    it is worth to point out that even within the kt cavity method , the training error satisfies the convergence conditions  ( [ condition ] ) and  ( [ stop ] ) .",
    "the main conclusion of this section is that the tla converges provided that the hidden perceptrons are trained through the minimization of a cost function with a bounded potential .",
    "the gardner potential  ( [ potential ] ) satisfies this constraint .",
    "the asymptotic behaviours of the training error in the large @xmath15 limit , calculated for @xmath95 and @xmath133 using two different approaches are used in the following sections to characterize the storage capacity of the constructive algorithm .",
    "we assume that the probability distribution of the targets @xmath62 in the training set is symmetric , given by ( [ prob ] ) with @xmath221 , so that the training error of the first perceptron is @xmath222 .",
    "considering iteratively the relationship between the training errors of two consecutive perceptrons  ( [ relationet ] ) yields :    @xmath223    where @xmath224 stands for @xmath116 , the symbol @xmath225 for the composition of functions and @xmath9 is the number of perceptrons necessary for convergence of the tla algorithm .",
    "the evolution of the training errors of the successive perceptrons is schematically represented on figure [ epsit ] for an arbitrary function @xmath226 , where the tilinglike algorithm is shown to converge in six steps , i.e. @xmath227 .",
    "we are interested in the limit of large training set sizes ( @xmath13 ) . in this limit ,",
    "the training error @xmath116 is close to @xmath139 :    @xmath228    with @xmath229 a function that vanishes in the limit @xmath230 .",
    "notice that those cost functions that do not satisfy condition ( [ condition ] ) for all @xmath15 are useless in this limit , since the error reduction at each step @xmath231 vanishes at some finite value of @xmath15 . for larger values of @xmath15",
    "it becomes positive , and the tla does not converge . in the preceding section we showed that the gardner potential both with vanishing and finite margin @xmath97 has @xmath232 ( see equations ( [ eqet ] ) and ( [ eqetkappa ] ) ) and satisfies condition ( [ condition ] ) .    as @xmath229 vanishes in the limit @xmath13 , we can guess that the number @xmath70 diverges . in this limit",
    "we can introduce the continuum approximation , replacing @xmath233 by the real - valued variable @xmath234 .",
    "then , the error reduction at each step is given by :    @xmath235    after integration of both sides of the equation @xmath236 at constant @xmath15 , from @xmath144 and @xmath237 to @xmath146 and @xmath238 , we obtain :    @xmath239    equation  ( [ kalpha ] ) gives the asymptotic behaviour of the number of hidden perceptrons necessary for the tilinglike algorithm to converge in the limit @xmath230 .",
    "it depends on the cost function used to train the perceptrons through @xmath116 .",
    "the storage capacity @xmath240 of the tla is then obtained through the inversion of @xmath70 .",
    "hereafter we consider the case where the hidden perceptrons are trained with the gardner cost function , using the results of the preceding section .",
    "we determine first the number of hidden units obtained when the perceptrons minimize the number of training errors , that is , the gardner cost function with @xmath95 . inserting into  ( [ kalpha ] ) ,",
    "the result  ( [ eqet ] ) obtained within the rs approximation , we obtain :    @xmath241    where @xmath155 is given by  ( [ afonc ] ) . from this result , we deduce the storage capacity in the limit of a large number of hidden perceptrons :    @xmath242    surprisingly , the capacity of the tla scales with @xmath9 like the upper bound for the parity machine with the same number of hidden units , and only the prefactor is overestimated .    using the result ( [ eqetkt ] ) obtained with the kt cavity method , that overestimates the perceptron s training error",
    ", we get :    @xmath243    where @xmath244 is defined in ( [ eqetkt ] ) and @xmath155 is given by ( [ eqakt ] ) .",
    "the corresponding storage capacity is :    @xmath245    we find that @xmath246 as expected .",
    "the behaviour of the storage capacity , obtained with the kuhn - tucker cavity method is linear in @xmath9 .",
    "this suggests that including replica symmetry breaking in the replica calculation may modify the @xmath247 behaviour to one proportional to @xmath248 with @xmath249 .",
    "however , as the actual training error of the perceptrons seems closer to the rs solution than to the kuhn - tucker cavity result  @xcite , we expect @xmath250 to be close to @xmath7 .    in the following we consider the parity machine obtained when",
    "the perceptrons are trained using the gardner cost function with a finite margin @xmath97 .",
    "we get :    @xmath251    after inversion of ( [ krskt ] ) , the capacities deduced within the two approximations are :    @xmath252    respectively . here",
    "again , the behaviours of @xmath70 and @xmath253 obtained with the rs approximation and with the kuhn - tucker cavity method differ . in both cases ,",
    "the value of @xmath97 only affects the prefactor but not the scaling with @xmath15 or @xmath9 .",
    "consistently , the prefactor of @xmath6 diverges for @xmath254 , where the expressions ( [ krskt ] ) and ( [ alpharskt ] ) have to be replaced by ( [ alphars ] ) and ( [ alphakt ] ) respectively , as the functional dependence of the storage capacity with @xmath9 is different for @xmath95 .    imposing a finite margin dramatically decreases the capacity of the tla .",
    "more precisely , the exponents @xmath250 of the logarithmic factor differ , depending on the approximations ( rs and kt cavity method ) , in both @xmath97-regimes ( @xmath255 , @xmath256 , @xmath257 and @xmath258 ) .    it is interesting to compare the exponents determined analytically within the rs approximation , to those obtained by west and saad  @xcite through a numerical iteration over the successive perceptrons training errors . for @xmath95 , they obtain @xmath250 close to @xmath7 ( @xmath259 and @xmath260 , and @xmath261 and @xmath262 , for @xmath263 and @xmath264 respectively ( table 3 in  @xcite ) ) in very good agreement with our result @xmath265 . in the case of finite @xmath97 , west and saad",
    "find that the exponent decreases with increasing @xmath97 ( figure 13 left in  @xcite ) . our result ( [ alpharskt ] )",
    "shows that the exponent does not depend on @xmath97 , only the prefactor does .",
    "the dependence found numerically is probably due to higher order corrections , that behave like @xmath266 .",
    "these terms , which are less and less negligible when approaching @xmath95 , hinder the determination of the power - law exponent in the asymptotic regime @xmath230 .",
    "remarkably , the rs and kt exponents @xmath267 and @xmath268 provide correct upper and lower bounds for the exponent obtained numerically within the one - step replica symmetry breaking approximation ( figure 13 right in  @xcite ) .",
    "we determined _ analytically _ the typical number of hidden units needed by a simple constructive procedure , the tilinglike learning algorithm proposed in  @xcite , to build a parity machine .",
    "the number of hidden units depends strongly on the asymptotic properties of the learning algorithm used to train them .",
    "we showed that the cost function minimized by the hidden perceptrons has to be bounded .",
    "this rules out , in particular , the perceptron or the adatron learning algorithms , as with these the training error can not decrease beyond a finite value that depends on the training set size and on the bias of the target s distribution .",
    "this is so because the hidden perceptrons have to learn highly biased output distributions . in the asymptotic regime ,",
    "large thresholds are needed to minimize the training error as , loosely speaking , such solutions allow to classify correctly most patterns of the majority class . in such solutions , a non - negligible fraction of patterns",
    "have large negative stabilities .",
    "if the cost function is unbounded for @xmath169 , it favours solutions with small thresholds , which have large training errors . with bounded potentials , like",
    "the counting functions used in the gardner cost function , solutions with large thresholds exist .",
    "we deduced the properties of a perceptron with threshold , learning targets drawn with a biased distribution , trained with the gardner cost function with and without margin .",
    "in particular , solutions such that the training error is smaller than the bias always exist . this is a condition necessary for the tla to converge .",
    "the asymptotic behaviour of the learning curves @xmath226 was determined through a replica calculation assuming replica symmetry , and also using the kuhn - tucker cavity method .",
    "the former approximation underestimates the training error , while the latter overestimates it .",
    "the main results are the expressions  ( [ eqet ] ) , ( [ eqetkappa ] ) , ( [ eqetkt ] ) and ( [ eqetktkappa ] ) relating the training error of the perceptron @xmath116 to the bias @xmath139 of the target distribution .",
    "closer inspection of equations ( [ eqet ] ) and ( [ eqetkappa ] ) shows that the error reduction @xmath269 at large @xmath15 is larger if @xmath270 than for @xmath271 .",
    "these results allow us to find analytically the number of units @xmath70 needed by the constructive procedure to converge in the large @xmath15 limit . as expected , the smallest @xmath70 is obtained when the hidden perceptrons minimize their training errors , which corresponds to the gardner cost function with @xmath95 .",
    "nevertheless , it is worth to study also the case with @xmath130 , which is interesting in noisy applications .",
    "the storage capacity @xmath240 of the tla is obtained through the inversion of @xmath70 .",
    "our results have been obtained under the simplifying assumption that the targets the successive perceptrons have to learn are uncorrelated .",
    "this hypothesis has been shown to be a good approximation  @xcite in the limit of large training sets considered here .    in the limit of large @xmath9",
    "we find @xmath272 within the rs approximation .",
    "it is interesting to compare this algorithm - dependent storage capacity to the storage capacity of a parity machine with the same number of hidden perceptrons .",
    "the latter is independent of the learning algorithm .",
    "geometric arguments  @xcite and a replica calculation where the permutation symmetry among hidden units has to be broken  @xcite , both lead to @xmath273 .",
    "it is surprising that , although we disregarded the correlations between perceptrons and assumed replica - symmetry , which both lead to an overestimation of the storage capacity , we find the same leading behaviour . only the prefactor is overestimated .",
    "in fact , the permutation symmetry only arises when the perceptrons are trained simultaneously .",
    "as it is absent in the case of the incremental construction , the consequence of the rs approximation is less dramatic than in  @xcite .",
    "as the kuhn - tucker cavity method provides an upper bound to the perceptron s training error , it allows to determine a lower bound for the tla storage capacity .",
    "this bound scales linearly with the number of hidden units , suggesting that a calculation including full replica symmetry - breaking may change the power - law of the logarithmic factor .",
    "we expect that @xmath274 with @xmath249 .",
    "it is a pleasure to thank k. y. michael wong for clarifying comments about the kt cavity method ."
  ],
  "abstract_text": [
    "<S> upper and lower bounds for the typical storage capacity of a constructive algorithm , the tilinglike learning algorithm for the parity machine [ m. biehl and m. opper , phys . </S>",
    "<S> rev . </S>",
    "<S> a * 44 * 6888 ( 1991 ) ] , are determined in the asymptotic limit of large training set sizes . </S>",
    "<S> the properties of a perceptron with threshold , learning a training set of patterns having a biased distribution of targets , needed as an intermediate step in the capacity calculation , are determined analytically . </S>",
    "<S> the lower bound for the capacity , determined with a cavity method , is proportional to the number of hidden units . </S>",
    "<S> the upper bound , obtained with the hypothesis of replica symmetry , is close to the one predicted by mitchinson and durbin [ biol . </S>",
    "<S> cyber . * 60 * 345 ( 1989 ) ] . </S>"
  ]
}