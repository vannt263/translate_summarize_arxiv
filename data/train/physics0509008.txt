{
  "article_text": [
    "in particle physics as well as other branches of science , fitting theoretical models to data is a crucial end stage to the performance of experiments .",
    "minimizing the @xmath0 between theory and experiment is perhaps the most commonly used form of fitting , with data binned in histograms .",
    "such fits yield not only the fitted parameters and errors on the fitted parameters but also a measure of the goodness of fit .",
    "another common fitting method is the maximum likelihood method which can be performed on binned and unbinned data to obtain the best values of theoretical parameters . in the case of unbinned likelihood",
    "fitting , there is currently no measure of the goodness of fit . in this paper , we propose a solution to the problem , which by its nature works generally for both binned and unbinned likelihood fits .",
    "a general theory of goodness of fit in likelihood fits results .      in what follows",
    ", we will denote by the vector @xmath1 , the theoretical parameters ( @xmath1 for `` signal '' ) and the vector @xmath2 , the experimentally measured quantities or `` configurations '' . for simplicity",
    ", we will illustrate the method where both @xmath1 and @xmath2 are one dimensional , though either or both can be multi - dimensional in practice .",
    "we thus define the theoretical model by the conditional probability density @xmath3 , defined as the probability of observing @xmath2 given a value of @xmath1 .",
    "the theoretical probability function obeys the normalization condition @xmath4 then an unbinned maximum likelihood fit to data is obtained by maximizing the likelihood  @xcite , @xmath5 where the likelihood is evaluated at the @xmath6 observed data points @xmath7 .",
    "such a fit will determine the maximum likelihood value @xmath8 of the theoretical parameters , but will not tell us how good the fit is .",
    "the goodness of fit variable must be invariant under a change of variable @xmath10 . the value of the likelihood @xmath9 at the maximum likelihood point does not furnish a goodness of fit , since the likelihood is not invariant under change of variable .",
    "this can be seen by observing that one can transform the variable set @xmath2 to a variable set @xmath11 such that @xmath12 is uniformly distributed between 0 and 1 . in one dimension ,",
    "this is trivially done by the transformation function @xmath13 such that    @xmath14    the variable @xmath2 ranges from @xmath15 to @xmath16 and the probability function @xmath17 normalizes to unity in this range .",
    "this implies that @xmath11 ranges from 0 to 1 .",
    "such a transformation is known as a hypercube transformation , in multi - dimensions .",
    "the transformed probability distribution in the variable @xmath11 is unity in this interval as can be seen by examining the jacobian of the transformation @xmath18 @xmath19 other datasets will yield different values of likelihood in the variable space @xmath2 when the likelihood is computed with the original function @xmath17 .",
    "however , in hypercube space , the value of the likelihood is unity regardless of the dataset @xmath20 , thus the likelihood @xmath9 can not furnish a goodness of fit by itself , since neither the likelihood , nor ratios of likelihoods computed using the same distribution @xmath17 is invariant under variable transformations .",
    "the fundamental reason for this non - invariance is that only a single distribution , namely , @xmath17 is being used to compute the goodness of fit .    to illustrate further",
    ", we use a concrete example of fitting a dataset using the maximum likelihood method as shown in figure  [ exps](a ) .",
    "the fitting is done in the range @xmath21 , where @xmath22 and @xmath23 .",
    "the fitting function is @xmath24 which normalizes to unity in the range @xmath21 .",
    "the fitted dataset is shown as a full histogram .",
    "the dashed histogram shows a dataset that is a poor fit to the data and will produce a smaller value of @xmath9 when fitted as a function of @xmath2 .",
    "figure  [ exps](b ) shows the same data in the hypercube space where the fitted function is flat as per the transformation given in equation  [ hyp ] .",
    "both the datasets will produce a value of unity for @xmath9 in this space implying an equally good fit in either case , which is obviously false .",
    "this clearly demonstrates that the likelihood by itself can not provide a goodness of fit variable .     in this space which implies that @xmath9 can not be used as a goodness of fit variable .",
    "[ exps ] ]     in this space which implies that @xmath9 can not be used as a goodness of fit variable .",
    "it is interesting to note that while using @xmath0 as the goodness of fit technique for binned histograms , we use two distribution functions , namely the theoretical curve and the data . by binning the data , we are in effect estimating the probability density function of the data as the second distribution , in addition to the theoretical distribution specified by the theoretical curve . in likelihood language",
    "we define the probability density function ( @xmath25 ) of the data as @xmath26 where @xmath27 is the number of times the experiment is repeated that results in the observable @xmath2 .",
    "the function @xmath28 obeys the normalization condition @xmath29 when one is using binned likelihoods , the @xmath25 of the data would be estimated by binning the events in a histogram and normalizing the sum of contents of all bins to unity . in the unbinned case",
    ", we will describe below a technique  @xcite on estimating @xmath28 using probability density estimators ( @xmath30 ) .    we can now define a likelihood ratio @xmath31 such that @xmath32 where we have used the notation @xmath33 to denote the dataset @xmath7 .",
    "since the @xmath6 events @xmath34 are independent , the probability of obtaining the dataset @xmath33 is given by @xmath35 the quantity @xmath36 we name the `` data likelihood '' of the dataset @xmath33 and the quantity @xmath37 as the `` theory likelihood '' of the dataset @xmath33 .",
    "we note that the `` data likelihood '' @xmath36 may also be thought of as the probability density of the `` @xmath38 '' @xmath33 which obeys the normalization condition @xmath39 let us now note that @xmath40 is invariant under a general variable transformation ( not restricted to hypercube transformation ) @xmath41 , since @xmath42 and the jacobian of the transformation @xmath43 cancels in the numerator and denominator in the ratio .",
    "this is an extremely important property of the likelihood ratio @xmath40 that qualifies it to be a goodness of fit variable .",
    "later , we will show that the binned likelihood ratio asymptotically approaches a @xmath0 distribution as the number of events @xmath44 , further motivating this choice .",
    "since the denominator @xmath45 is independent of the theoretical parameters @xmath1 , both the likelihood ratio and the likelihood maximize at the same point @xmath8 .",
    "the likelihood ratios for two different data sets @xmath46 and @xmath33 can be combined by multiplication as per    @xmath47    this rule follows from the definition of @xmath40 in equation  [ lrd ] . in practice , we will use the negative log - likelihood ratio @xmath48 as the goodness of fit variable and minimize it .",
    "the multiplication rule of equation  [ mult ] results in an addition rule for @xmath49 .",
    "the problem of finding the distribution of @xmath49 for a good fit then reduces to finding the distribution of @xmath49 in hyper - cube space for a variable that is uniformly distributed between zero and one , as in figure  [ exps](b ) .",
    "this is because @xmath50 is invariant under the transformation of variable .",
    "so all goodness of fit problems using likelihood ratios can be reduced to finding the distribution of @xmath49 for a variable that is uniformly distributed in hypercube space .",
    "the neyman - pearson lemma  @xcite states that if one is trying to choose between two hypotheses @xmath51 and @xmath52 , then the cut on the likelihood ratio @xmath53 will have the optimum power in differentiating between the hypotheses @xmath51 and @xmath52 , where @xmath54 is a constant adjusted to obtain the desired purity in favor of hypothesis @xmath51 . notice that this likelihood ratio is between the likelihood computed for two different hypotheses @xmath51 and @xmath52 .",
    "our likelihood ratio differs fundamentally from this in that the denominator we use @xmath55 is the `` data likelihood '' that is computed from the distribution of the data and is not tied to any hypothesis as such .",
    "the method of maximum likelihood fits the shape of the theoretical distribution to the data distribution .",
    "the theoretical model obeys the normalization condition in equation  [ thenorm ] and the likelihood is evaluated at the number of observed data events @xmath6 .",
    "there is no explicit mention of the theoretically expected number of events , which we denote by @xmath56 .",
    "later we will show how to incorporate a goodness of fit in the absolute normalization by making use of the binomial distribution and its limiting cases the poisson and the normal distributions",
    ". we will begin by obtaining goodness of fit formulae for the case where we bin the data and fit the theoretical shape to the experimental distribution .",
    "when one bins data in histograms and fits the theory shape to the data , one can fit by using either maximum likelihood or by minimizing @xmath0 . in either case",
    ", the goodness of fit is usually evaluated using @xmath0 .",
    "we now illustrate how the likelihood ratio defined in section  [ lrat ] can be used to obtain a goodness of fit after the maximum likelihood fitting is done . in order to evaluate the likelihood ratio",
    ", one needs to evaluate the theory likelihood and the data likelihood for each value of @xmath57 . for the binned histogram",
    ", we make the approximation of assuming that both these quantities are constant for all values of @xmath57 in a given bin and evaluating each at the bin center .",
    "let there be @xmath58 bins and let the @xmath59 bin contain @xmath60 entries .",
    "the probability of obtaining the histogram is given by the multinomial distribution @xmath61      the factor @xmath62 denotes the number of ways @xmath6 events can be partitioned to form the observed histogram , which we term the degeneracy @xmath63 of the histogram .",
    "each of the @xmath64 histograms is identical to each other and possesses the same goodness of fit .",
    "we can then evaluate the goodness of fit for any one of the @xmath64 degenerate histograms , the likelihood for which is given by    @xmath65    and the likelihood ratio can be written as @xmath66 the value of @xmath67 raised to the power @xmath60 in equation  [ lrb ] results from the fact that there are @xmath60 configurations @xmath57 in the @xmath59 bin and we are multiplying a constant ratio ( at the bin center ) over @xmath60 configurations . if @xmath68 is the bin width for the @xmath59 bin , then the data likelihood can be approximated by @xmath69 this obeys the normalization condition @xmath70 the theoretical likelihood can be integrated over the bin to yield @xmath71 this obeys the normalization condition @xmath72 then the likelihood ratio can be written @xmath73 where @xmath74 is the theoretically expected number of events in the @xmath59 bin obeying the normalization condition @xmath75 , as per equation  [ tnorm ] . this likelihood ratio may be used to obtain a maximum likelihood fit as well as to obtain a goodness of fit . note that the likelihood ratio is well - behaved even for empty bins where @xmath76 , since @xmath77 is unity for such cases .",
    "note that the negative log - likelihood ratio @xmath49 resulting from equation  [ lrat2 ] yields @xmath78 which is the same result as derived by baker and cousins  @xcite for the multinomial case where normalization is preserved between theory and experiment .",
    "we have derived the result using very different arguments ( than baker and cousins ) for the denominator of the likelihood ratio , namely it is the value of the @xmath79 at the bin center as a result of the general theory developed here .",
    "if we are reluctant to work out ( for reasons of computing speed ) the integral in equation  [ tint ] for each bin at each step of the fitting process , then we can approximate it by the bin center values @xmath80 this then obeys the normalization equation  [ tnorm ] and the expression in equation  [ lrat3 ] for @xmath49 can be used generally .",
    "let the difference between @xmath60 , the observed number of events and @xmath81 the theoretical number of events be denoted by @xmath82",
    ". then @xmath83 , by virtue of the normalization conditions .",
    "then the binned negative log likelihood ratio @xmath49 can be written @xmath84 this can be expanded in powers of @xmath85 as @xmath86 as @xmath87 , the individual bin contents become normally distributed about their expected value @xmath81 with variance @xmath88 for @xmath89 .",
    "this is true for all cases ( named the _ null hypothesis _ ) where the data and theory fit each other",
    ". then we can write @xmath90 and @xmath91 for large @xmath6 , @xmath92 and the higher order terms may be neglected yielding @xmath93 this is an example of the likelihood ratio theorem  @xcite .",
    "the expected value of the @xmath49 can then be written @xmath94 where @xmath95 are the @xmath96 moments of the normal distribution about the mean . since the normal distribution is symmetric about the mean , all the odd moments ( @xmath97 ) are zero .",
    "the even moments of the normal distribution ( for integer @xmath98 ) are given by the formula @xmath99 this yields @xmath100 all the remaining terms tend to zero as @xmath101 as @xmath102 leading to @xmath103 the number of degrees of freedom for @xmath49 would be @xmath104 , due to the normalization condition @xmath105 .      as we have pointed out",
    ", maximum likelihood fitting only fits the shape of the theoretical distribution to the experimental data .",
    "this is due to the normalization condition of equation  [ thenorm ] .",
    "however , if we employ a binomial distribution and define the first bin as containing the number of observed events @xmath6 with theoretical expectation of @xmath56 events , and the second bin to contain the number of unobserved events in @xmath27 tries , then one can employ the formula in equation  [ lrat2 ] with @xmath106 to obtain the likelihood ratio .",
    "@xmath107 we now take the poissonian limit of @xmath108 with @xmath56 and @xmath6 finite and the above likelihood ratio becomes @xmath109 where we have employed the relations @xmath110 and @xmath111 as @xmath108 .",
    "equation  [ poiss ] provides the goodness of fit likelihood ratio for all poissonian problems where @xmath56 events are expected and @xmath6 are observed .",
    "we can now multiply this poissonian @xmath112 with equation  [ lrat2 ] to produce the likelihood ratio for a general binned likelihood problem where the normalization for theory and experiment vary .",
    "@xmath113 where we have defined @xmath114 and @xmath115 . with this redefinition , we obtain the @xmath49 for the multinomial with theoretical normalization differing from the experimental one as @xmath116 this is same as the `` poissonian result '' of baker and cousins  @xcite again derived using very different arguments for the denominator of the likelihood ratio .",
    "the poissonian result is useful when @xmath56 and @xmath6 are relatively small numbers ( @xmath117 ) .",
    "when we have larger number of events , then the gaussian approximation is more relevant .",
    "we have already shown that ( equation  [ lambchi ] ) that in a multinomial , the negative log likelihood ratio can be approximated by @xmath118 we apply this to the binomial with @xmath106 , @xmath119 , and @xmath120 and @xmath121 .",
    "then @xmath122 where @xmath123 is the probability of an event appearing in the first bin and @xmath124 and @xmath125 is the variance of the bin contents of the first bin .",
    "we now let @xmath108 , @xmath44 and @xmath126 . in this case",
    ", the variance can be approximated by @xmath6 and we have the gaussian case with @xmath127 .",
    "this @xmath49 can be added to the one resulting from the maximum likelihood shape fitting to get an overall goodness of fit .",
    "we must emphasize once again that the method of maximum likelihood always fits theoretical shapes to experimental data .",
    "we have been able to circumvent this restriction by using the device of the binomial distribution where the observed events @xmath6 are in the first bin and the total number of events in the distribution @xmath27 refer to the `` number of tries '' and the second bin consists of the @xmath128 events that failed to appear in the experiment .",
    "the binomial distribution is special in this regard since once we specify the properties of the first bin , the second bin is completely specified and anti - correlated with the first bin .",
    "the number of tries is unknown , but we set it to infinity in two different limits as discussed resulting in the poisson and the gaussian likelihood ratios .",
    "the most commonly used method for goodness of fit is the @xmath0 test of karl pearson , which is used even when the quantities being fitted are not events but measurements with error bars .",
    "we show here that the @xmath0 measure is also twice the negative logarithm of a gaussian likelihood @xmath129 rather than the negative logarithm of a gaussian likelihood , as is the popular misconception .",
    "consider a binned histogram where the contents in the @xmath59 bin is noted by @xmath130 and the theoretical expectation of this bin is @xmath131 .",
    "the standard error of the observed variable @xmath130 is known to be @xmath132 .",
    "then , one can write @xmath133 this leads to @xmath134 from the above expression , people are mistakenly led to conclude that @xmath0 is equivalent to twice the negative log - likelihood .",
    "this ignores the term @xmath135 in the above equation , which varies from bin to bin . in order to work out the likelihood ratio",
    ", we need to estimate the data density @xmath136 at each measurement .",
    "the data points are distributed as a gaussian with standard deviation @xmath132 .",
    "the best estimate of the mean of the gaussian from the data alone is @xmath130 .",
    "this leads to @xmath137 yielding the likelihood ratio @xmath138 the overall likelihood ratio is given by @xmath139 leading to @xmath140 i.e. @xmath0 is equal to twice the negative log - likelihood ratio and not the negative log - likelihood!.",
    "very often the data are not plentiful enough to bin adequately and it is more efficient to perform an unbinned likelihood fit .",
    "presently , a goodness of fit method does not exist for unbinned likelihood fits . using the formalism developed above",
    ", we present a solution . after the unbinned likelihood fit",
    "is performed by maximizing the likelihood in equation  [ like ] one needs to work out the _ data likelihood _ @xmath45 in order to evaluate the likelihood ratio and the goodness of fit .",
    "we employ the technique of probability density estimators @xmath141 , also known as kernel density estimators  @xcite @xmath142 to do this .",
    "the @xmath25 @xmath28 is approximated by @xmath143 where a kernel function @xmath144 is centered around each data point @xmath57 , is so defined that it normalizes to unity .",
    "the choice of the kernel function can vary depending on the problem .",
    "a popular kernel is the gaussian defined in the multi - dimensional case as @xmath145 where @xmath146 is the error matrix of the data defined as @xmath147 and the @xmath148 implies average over the @xmath6 events , and @xmath149 is the number of dimensions .",
    "the hessian matrix @xmath150 is defined as the inverse of @xmath146 and the repeated indices imply summing over .",
    "the parameter @xmath151 is a `` smoothing parameter '' , which has@xcite a suggested optimal value @xmath152 , that satisfies the asymptotic condition @xmath153 the parameter @xmath151 will depend on the local number density and will have to be adjusted as a function of the local density to obtain good representation of the data by the @xmath30 .",
    "our proposal for the goodness of fit in unbinned likelihood fits is thus the likelihood ratio @xmath154 evaluated at the maximum likelihood point @xmath8 .",
    "we consider a simple one - dimensional case where the data is an exponential distribution , say decay times of a radioactive isotope .",
    "the theoretical prediction is given by @xmath155 we have chosen an exponential with @xmath156 for this example . the gaussian kernel for the @xmath30",
    "would be given by @xmath157 where the variance @xmath158 of the exponential is numerically equal to @xmath1 . to begin with",
    ", we chose a constant value for the smoothing parameter , which for 1000 events generated is calculated to be 0.125 . figure  [ genev ] shows the generated events , the theoretical curve @xmath3 and the @xmath30 curve @xmath159 normalized to the number of events .",
    "the @xmath30 fails to reproduce the data near the origin due to the boundary effect , whereby the gaussian probabilities for events close to the origin spill over to negative values of @xmath2 .",
    "this lost probability would be compensated by events on the exponential distribution with negative @xmath2 if they existed . in our case , this presents a drawback for the @xmath30 method , which we will remedy later in the paper using @xmath30 definitions on the hypercube and periodic boundary conditions . for the time being , we will confine our example to values of @xmath160 to avoid the boundary effect .",
    "figure  [ genev1 ] shows the `` data '' with 1000 events generated as an exponential in the fiducial range @xmath161 .",
    "superimposed on it is a gaussian of 500 events .",
    "more events in the exponential are generated in the interval @xmath162 to avoid the boundary effect at the fiducial boundary at c=1.0 .",
    "since the number density varies significantly , we have had to introduce a method of iteratively determining the smoothing factor as a function of @xmath2 .",
    "the expression @xmath163 clearly is meant to give a smoothing factor that decreases slowly with increased statistics @xmath6 .",
    "it is expected to be true on average over the whole distribution .",
    "however , the exponential distribution under consideration has event densities that vary by orders of magnitude as a function of the time variable @xmath2 . in order to obtain a function @xmath164 that takes into account this variation , we first work out a @xmath30 with constant @xmath151 and then use the number densities obtained thus  @xcite to obtain @xmath164 as per the equation",
    "@xmath165 the equation is motivated by the consideration that a uniform distribution of events between @xmath15 and @xmath16 has a @xmath166 whereas the real @xmath25 is approximated by @xmath30 .",
    "the function @xmath164 thus obtained is used to work out a better @xmath167 .",
    "this process is iterated three times to give the best smoothing function .",
    "we generate @xmath6=1000 events in the fiducial interval .",
    "if now we were to superimpose a gaussian with 500 events centered at @xmath2=2.0 and width=0.2 on the data , the @xmath30 estimator will follow the data as shown in figure  [ genev1 ] .",
    "this shows that the @xmath30 estimator we have is adequate to reproduce the data , once the smoothing parameter is made to vary with the number density appropriately . the smoothing function @xmath164 for the events in figure  [ genev1 ] is shown in figure  [ hc ] .",
    "it can be seen that the value of @xmath151 increases for regions of low statistics and decreases for regions of high statistics . superimposed is the constant smoothing factor obtained by the equation @xmath168 , with @xmath6 being the total number of events generated , including those outside the fiducial volume .     as a function of @xmath2 for the example shown in figure  [ genev1 ] .",
    "the variation of the smoothing parameter is obtained iteratively as explained in the text .",
    "the flat curve is a smoothing factor resulting from the formula @xmath169 .",
    "[ hc],width=384 ]      the negative log - likelihood ratio @xmath170 at the maximum likelihood point now provides a measure of the goodness of fit . in order to use this effectively",
    ", one needs an analytic theory of the sampling distribution of this ratio .",
    "this is difficult to arrive at , since this distribution is sensitive to the smoothing function used .",
    "if adequate smoothing is absent in the tail of the exponential , larger and broader sampling distributions of @xmath50 will result .",
    "one can however determine the distribution of @xmath49 empirically , by generating the events distributed according to the theoretical model many times and determining @xmath49 at the maximum likelihood point for each such distribution .",
    "the solid histogram in figure  [ fitlike ] shows the distribution of @xmath50 for 500 such fits .",
    "this has a mean of 2.8 and an @xmath171 of 1.8 .",
    "the dotted histogram shows the corresponding value of @xmath49 for the constant value of smoothing factor shown in figure  [ hc ] .",
    "this distribution is clearly broader ( @xmath171=2.63 ) with a higher mean(=9.1 ) and thus has less discrimination power in judging the goodness of fit than the solid curve .     at the maximum likelihood point for 500 distributions , using the iterative smoothing function mechanism .",
    "the dashed curve shows the corresponding distribution in the case of a constant smoothing function .",
    "[ fitlike],width=384 ]       generated as an exponential with decay constant @xmath1=1.0 with a superimposed gaussian of 500 events centered at @xmath2=2.0 and width=0.2 .",
    "the @xmath30 estimator is the ( solid ) histogram with no errors .",
    "[ genev1 ] ]    we now vary the number of events in the gaussian and obtain the value of the negative log likelihood ratio @xmath49 as a function of the strength of the gaussian .",
    "table  [ tab1 ] summarizes the results .",
    "the number of standard deviations the unbinned likelihood fit is from what is expected is determined empirically by plotting the value of @xmath49 for a large number of fits where no gaussian is superimposed ( i.e. the null hypothesis ) and determining the mean and @xmath172 of this distribution and using these to estimate the number of @xmath158 s the observed @xmath49 is from the null case .",
    "table  [ tab1 ] also gives the results of a binned fit on the same `` data '' .",
    "it can be seen that the unbinned fit gives a @xmath173 discrimination when the number of gaussian events is 85 , where as the binned fit gives a @xmath174 of 42/39 for the same case .    .",
    "goodness of fit results from unbinned likelihood and binned likelihood fits for various data samples .",
    "the negative values for the number of standard deviations in some of the examples is due to statistical fluctuation .",
    "[ tab1 ] [ cols=\"^,^,^,^ \" , ]      the observed data is a bad fit to the model .",
    "we have managed not only obtain a goodness of fit for the problem ( made extreme by the sparsity of data ) , but also to show that the method gives reliable results for a variety of smoothing parameters .",
    "the method is also robust with respect to the data size @xmath6 .",
    "we see that as we increase the smoothing parameter to 0.4 , we begin to increase the chance of fitting .",
    "when @xmath175 , everything will fit .",
    "a smoothing parameter of @xmath176 or @xmath177 gives reliable results .",
    "the probability to exceed the observed @xmath49 is estimated from the histograms with 1000 experiments .",
    "we can improve the accuracy of this by running more monte carlo statistics .",
    "r.  a.  fisher,``on the mathematical foundations of theoretical statistics '' , _ philos .",
    "london ser . a _ * 222 * , 309 - 368(1922 ) ; + r.  a.  fisher,``theory of statistical estimation '' , _ proc .",
    "cambridge philos .",
    "* 22 * , 700 - 725 ( 1925 ) .",
    "e.  parzen , `` on estimation of a probability density function and mode '' _ ann.math.statis . _  * 32 * , 1065 - 1072 ( 1962 ) . j.  neyman and e.  s.  pearson , biometrika 20a ( 1928 ) 263 .",
    "see also s. brandt , `` statistical and computational methods in data analysis '' , springer , new york , ( 1997 ) for a proof .",
    "s.  baker , r.  d.  cousins , nucl .",
    "instrum . meth .",
    "a221 ( 1984 ) .",
    "see p.  g.  hoel , `` introduction to mathematical statistics '' , @xmath178 ed .",
    ", wiley , new york 1971 , p211 for a derivation of the likelihood ratio theorem .",
    "d.  scott .",
    "_ multivariate density estimation_.  john wiley & sons , 1992 .",
    "+ m.  wand and m.  jones ,  _ kernel smoothing_. chapman & hall , 1995 .  an essay towards solving a problem in the doctrine of chances \" , rev .",
    "thomas  bayes , biometrika,*45 * 293 - 315 ( reprint of 1763 ) ( 1958 ) .",
    "empirically we found that a power of the order of -0.6 is needed to provide sufficiently large smoothing factors for large values of time .",
    "we can in principle optimize this smoothing function further , but have not done so .",
    "objective bayesianism may first have been proposed by pierre - simon , marquis de laplace , `` theorie analytique des probabilite '' ( 1812 ) .",
    "+ see also e.  t.  jaynes,``probability theory : the logic of science '' , for a more modern treatment of objective bayesianism .",
    "bruno de finetti , `` theory of probability , a critical introductory treatment '' , john wiley & sons,(1974 ) for a description of hierarchical bayesian formalism , see bradley p. carlin and thomas a. louis , `` bayes and empirical bayes methods for data analysis '' , chapman  &  hall / crc , page 19 . c.  n.  morris , `` parametric empirical bayes inference : theory and applications '' , j.amer.statist.assoc.,*78*,47-65 .",
    "all bayesians make this substitution .",
    "see for example de finetti  @xcite p. 142 .",
    "r.  a.  fisher,``the fiducial argument in statistical inference '' , annals of eugenics , 6:391 - 398(1935 ) .",
    "the author is grateful to bruce knuteson for posing this problem ."
  ],
  "abstract_text": [
    "<S> maximum likelihood fits to data can be performed using binned data and unbinned data . </S>",
    "<S> the likelihood fits in either case produce only the fitted quantities but not the goodness of fit . with binned data , one can obtain a measure of the goodness of fit by using the @xmath0 method , after the maximum likelihood fitting is performed . with unbinned data , currently , the fitted parameters are obtained but no measure of goodness of fit is available . </S>",
    "<S> this remains , to date , an unsolved problem in statistics . by considering the transformation properties of likelihood functions with respect to change of variable , </S>",
    "<S> we conclude that the likelihood ratio of the theoretically predicted probability density to that of _ the data density _ is invariant under change of variable and provides the goodness of fit . </S>",
    "<S> we show how to apply this likelihood ratio for binned as well as unbinned likelihoods and show that even the @xmath0 test is a special case of this general theory . in order to calculate errors in the fitted quantities </S>",
    "<S> , we need to solve the problem of inverse probabilities . </S>",
    "<S> we use bayes theorem to do this , using the data density obtained in the goodness of fit . </S>",
    "<S> this permits one to invert the probabilities without the use of a bayesian prior . </S>",
    "<S> the resulting statistics is consistent with frequentist ideas . </S>"
  ]
}