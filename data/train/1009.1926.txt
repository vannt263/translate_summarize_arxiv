{
  "article_text": [
    "suppose the linear regression model is used to relate @xmath2 to the @xmath3 potential predictors @xmath4 , @xmath5 where the subscript @xmath6 refers to the full model @xmath7 . in the model , @xmath8 is an unknown intercept parameter , @xmath9 is an @xmath10 vector of ones , @xmath11 is an @xmath12 design matrix , and @xmath13 is a @xmath14 vector of unknown regression coefficients . in the error term of",
    ", @xmath15 is an unknown scalar and @xmath16 has a spherically symmetric ( ss ) distribution with the density @xmath17 , @xmath18=\\bm{0}_n$ ] and @xmath19=\\bm{i}_n$ ] .",
    "we assume that the columns of @xmath20 have been standardized so that for @xmath21 , @xmath22 and @xmath23 .",
    "we shall be particularly interested in the variable selection problem where we would like to select an unknown subset of the effective predictors .",
    "it will be convenient throughout to index each of these @xmath24 possible subset choices by the vector @xmath25 where @xmath26 or @xmath27 .",
    "we use @xmath28 to denote the size of the @xmath29th subset .",
    "the problem then becomes that of selecting a submodel of @xmath30 in , @xmath31 is the @xmath32 matrix whose columns correspond to the @xmath29th subset of @xmath33 , @xmath34 is a @xmath35 vector of unknown regression coefficients .",
    "let @xmath36 denote the submodel given by .",
    "we assume the error term @xmath37 has the ss density @xmath38 for all @xmath29 , with @xmath39=\\bm{0}_n$ ] and @xmath40=\\bm{i}_n$ ] .",
    "further @xmath41 is an unknown scalar in the error term .",
    "we note that , in most earlier studies , the error terms in linear models have been assumed to have a gaussian distribution , e.g.  as in @xcite and @xcite .    in this paper ,",
    "we assume that @xmath42 ( the so called classical setup ) and @xmath43 are linearly independent , which implies that @xmath44 we also assume in much of the paper that the null model @xmath45 ( @xmath46 or @xmath47 ) is not a possible model , that is , the number of possible models is @xmath48 , rather than , @xmath24 , although in section [ sec : marginal ] , we indicate an alternative development which allows all @xmath24 possible models .",
    "a bayesian approach to this problem entails the specification of prior distributions on the models @xmath49 , and on the parameters @xmath50 of each model . for each such specification , of key interest",
    "is the posterior probability of @xmath36 given @xmath51 , @xmath52 where @xmath53 is assumed as mentioned in the above ( although see remark [ rem : null ] ) . in",
    ", @xmath54 is the marginal density under @xmath36 and @xmath55 is the bayes factor for comparing each of @xmath36 to the full model @xmath7 which is defined as @xmath56 where @xmath57 is the marginal density under the full model . in bayesian model selection ,",
    "@xmath58 is typically selected as the best model .    in this paper ,",
    "the main focus is on @xmath59 , not @xmath60 .",
    "hence our main aim is to propose and study specifications for the prior distribution of the parameters for each submodel @xmath36 . in particular , the joint density we consider for @xmath61 has the form @xmath62 for @xmath63 and a non - random hyper - parameter @xmath64 with @xmath65 .",
    "since the term including @xmath66 in the prior above , @xmath67 for @xmath68 , is known as a sub - harmonic function , that is , @xmath69 we call the prior given by a sub - harmonic prior",
    ". we will show that such priors lead to robust bayes factors , in the sense that each bayes factor does not depend on the form of the underlying error distribution .",
    "we also show that the resulting procedure has model selection consistency and is also coherent .",
    "the organization of this paper is as follows . in section [ sec : prior ] , we give details of the prior distribution . in section [ sec : marginal ] , we show that the bayes factor with respect to the above prior is given by @xmath70 where @xmath71 for @xmath72 . in",
    ", @xmath73 is the bayes factor for standard gaussian errors and @xmath74 and @xmath75 are the coefficient of determination under the submodel @xmath36 and the full model @xmath7 , respectively . from",
    ", the bayes factor does not depend on the same ss sampling density .",
    "hence , even when there is no specific information about the form of the error distribution of each model ( other than spherical symmetry ) , it is not necessary to specify the exact form of the sampling density .",
    "it suffices to assume it is gaussian .",
    "as far as we know , in the area of bayesian variable selection with shrinkage priors or zellner s @xmath0-priors , the sampling density has been assumed to be gaussian and this kind of robustness result has not yet been studied .",
    "originally analogous robustness results were derived by @xcite , @xcite and others in the problem of estimating regression coefficients with the stein effect and it was this type of result that lead us to search for a similar phenomenon in the context of model selection .",
    "note that we use the term `` robustness '' in this sense of distributional robustness over the class of ss error distributions .",
    "we specifically are not using the term to indicate a high breakdown point .",
    "the use of the term `` robustness '' in our sense is however common ( if somewhat misleading ) in the context of insensitivity to the error distribution in the context of shrinkage literature . in section [ sec : laplace ] , by use of the laplace approximation , we approximate the bayes factor given by as @xmath76 @xmath77}\\ ] ] as @xmath78 where @xmath79 and @xmath80}$ ] is the bic based alternative for standard gaussian errors @xmath81}= \\left\\{\\frac{(1-r_{{\\gamma}}^2)^{-n}n^{-q_{{\\gamma}}}}{(1-r_f^2)^{-n}n^{-p}}\\right\\}^{1/2}.\\ ] ] ( see , e.g.  @xcite , chapter 7 . ) since @xmath82 does not depend on @xmath83 , is asymptotically equivalent to bic with a simple @xmath84 rational correction function depending upon @xmath64 as well as the @xmath85-squares and the numbers of predictors .",
    "actually this is a special case of theorem [ thm : laplace ] in which several bayes factors under gaussian errors which have been proposed in earlier studies , are shown to have similar asymptotic approximations .",
    "while the main theme in this paper is to develop the relationship under sub - harmonic priors , we believe that this asymptotic equivalence is another noteworthy contribution , in particular , from the computational point of view . in section [ sec : consistency ] , we show that our bayes factor has model selection consistency uniformly over the class of ss error distributions , as @xmath86 and @xmath3 is fixed .",
    "it also follows from these results that several model selection methods recently studied in the literature for gaussian errors have model selection consistency for the entire class of ss error distributions .",
    "we provide illustrations of the method and comparisons with other methods using both simulated and real data in section [ sec : ex ] .",
    "we give concluding remarks in section [ sec : cr ] .",
    "the appendix presents some of the more technical proofs .",
    "in this section , for each submodel , we give a prior joint density of a form @xmath87 where @xmath88 .",
    "the natural choice of priors for location ( @xmath8 ) and scale ( @xmath89 ) are @xmath90 and @xmath91 in and , the superscript @xmath92 means that the prior density is improper . since and",
    "have invariance to location and scale transformation , respectively , they are considered by many as non - informative objective priors .",
    "the problem is that they are improper and hence determined only up to an arbitrary multiplicative constant . in this paper",
    ", the use of improper priors is justified through sequences of proper priors approaching the target improper priors and : @xmath93 where @xmath94 and @xmath95 where @xmath96 .",
    "see the beginning of section [ sec : marginal ] for details of the justification .",
    "next we give a sequence of proper conditional priors on @xmath34 given @xmath89 , which approach an improper conditional prior on @xmath34 given @xmath89 : @xmath97 where @xmath64 and @xmath98 are non - random positive parameters .",
    "further @xmath99}(g),\\ ] ] and @xmath100 denotes the @xmath101-variate gaussian density with mean vector @xmath102 and covariance matrix @xmath103 .",
    "the prior clearly has a hierarchical structure and it can be interpreted a scale mixture of zellner s @xmath0-priors .",
    "similar priors have been considered by @xcite and @xcite under the gaussian linear regression setup .",
    "see sub - section [ sec : g - prior ] below for a review of priors on @xmath0 . for any fixed @xmath104 and @xmath105",
    ", the prior @xmath106 is clearly a proper probability density , that is , @xmath107 when @xmath108 and @xmath109 , the limit of a variant of @xmath106 is given by @xmath110 in summary , the proper prior joint density under @xmath36 , which we will use in this paper , is given by @xmath111 with @xmath112 , which clearly satisfies @xmath113 for any fixed @xmath114 and any @xmath105 . in section [ sec : marginal ] , we will use the improper joint density of @xmath8 , @xmath34 and @xmath89 given by @xmath115 where @xmath109 and @xmath116 hence , in a certain sense , the larger @xmath64 is ( as long as @xmath117 ) , the more objective the joint prior @xmath118 is .    in this presentation of the improper joint density @xmath119 , two facts ,    1 .",
    "@xmath120 and @xmath89 are separable , [ key1 ] 2 .",
    "the part depending on @xmath89 is given by the power function @xmath121 , [ key2 ]    will be the key for calculating the marginal density in the next section .",
    "see also remark [ rem : separable ] .",
    "if , in the above joint prior on @xmath122 , we make the change of variables , @xmath123 , the joint prior of @xmath124 becomes @xmath125 as noted in section [ sec : intro ] , the part depending on @xmath66 , @xmath126 for @xmath127 , is known as a sub - harmonic function , that is , @xmath128      as noted above , the prior given by is a scale mixture of zellner s @xmath0-priors .",
    "actually the original zellner s @xmath0-priors were used for the gaussian linear regression setup and historically the hyperparameter @xmath0 has been a priori fixed or somehow estimated . the first paper to effectively use a prior on @xmath0 was @xcite ; they stated things in terms of multivariate cauchy densities , which can always be expressed as a mixture of @xmath0-priors . here",
    "we review the prior on @xmath0 , the second stage of @xmath0-priors for gaussian linear regression .",
    "we hope that it makes the positioning of our prior on @xmath0 , , which is for ss errors , clearer .    as a generalization of @xmath129 given by , consider the prior on @xmath0 , @xmath130}(g)}{v_g(\\{\\nu , k\\};h_g)},\\ ] ] ( so @xmath131 ) . in",
    ", @xmath132 , @xmath133 , @xmath104 and @xmath134 note that it is improper at @xmath135 when @xmath136 .",
    "as we will see in section [ sec : marginal ] and [ sec : laplace ] , bayes factors are not well - defined when the prior on @xmath0 is improper at @xmath135 and that is why @xmath137 is assumed .    when @xmath138 and @xmath139 , without truncating by @xmath98 the prior of @xmath0 is proper under the support @xmath140 . in this case ,",
    "the normalizing constant becomes @xmath141 further even when @xmath142 ( non - negative @xmath64 implies impropriety at @xmath143 ) , the bayes factor under @xmath36 is well - defined as shown in section [ sec : marginal ] . as in and",
    ", we are considering the ( improper ) case where @xmath109 and @xmath144 . @xcite",
    "considered the ( proper ) case where @xmath145 and @xmath146 .",
    "@xcite and @xcite considered the ( improper ) case where @xmath147 and @xmath148 .",
    "the noteworthy difference between @xmath138 and @xmath144 is that the ( proper or improper ) prior on @xmath34 given @xmath89 , @xmath149 is not separable for any @xmath138 . as will be seen in section [ sec : marginal ] , the separability of the prior is the key for calculating the marginal density for ss error models other than gaussian .    here",
    "we discuss objectivity ( or at least non - subjectivity ) of the prior in terms of hyper - parameters of the prior on @xmath0 .",
    "consider the ( proper or improper ) prior on @xmath34 given @xmath89 , which is given by . in order to obtain the asymptotic behavior of the density as @xmath150",
    ", we appeal to the tauberian theorem for the laplace transform ( see @xcite ) . since @xmath151 for any @xmath152 as @xmath153 , @xmath154 when @xmath155 .",
    "hence the asymptotic order of is the same as .",
    "the larger @xmath156 is , the more objective the prior @xmath157 is .",
    "in this section we derive the marginal density under each submodel and the bayes factor for comparing each @xmath158 to the full model @xmath7 . the marginal density of @xmath51 under @xmath159 ,",
    "is given by @xmath160 where the proper joint prior @xmath161 is given by .",
    "in , @xmath114 does not depend on the submodel , but is the same in all models . in the following , instead of @xmath162 directly , we consider the limit of the product of @xmath162 and @xmath163 , @xmath164 which is the marginal density of @xmath51 with respect to the improper prior @xmath118 given by : @xmath165 the second equality in follows from the monotone convergence theorem . since @xmath163 does not depend on @xmath61 if we choose the same @xmath64 in all submodels , the bayes factor @xmath166 approaches @xmath167 as @xmath168 .",
    "hence the use of the improper joint prior is justified as long as @xmath167 is well - defined . as remarked in section [ sec : intro ] , the null - model is not a possible model . since there is no @xmath169 and hence no @xmath98 , @xmath170 is not well - defined . and thus @xmath171 is assumed . however , see remark [ rem : null ] for an alternative development which allows @xmath45 as a possible model .",
    "let @xmath172 be the marginal density under @xmath36 with standard gaussian errors @xmath173 . before proceeding with the entire calculation of the marginal density , @xmath174 ,",
    "when @xmath175 has a general ss distribution , we will provide a relationship between @xmath174 and @xmath176 as follows .",
    "[ relationship - g ] let @xmath64 be between @xmath177 and @xmath178 .",
    "assume the existence of @xmath179 $ ] .",
    "then @xmath180}{e[\\|{\\bm{\\epsilon}}_g \\|^\\nu ] } m_{{\\gamma}}^g(\\bm{y}|\\nu).\\ ] ]    see appendix .",
    "hence @xmath181 depends on the error distribution @xmath182 only through the @xmath64-th moment of @xmath183 , @xmath184 $ ] .",
    "[ rem : separable ] this relationship of lemma [ relationship - g ] remains true under more general separable priors @xmath185 , which satisfies k[key1 ] and k[key2 ] .",
    "unfortunately we could not find any other priors , which lead to analytically tractable bayes factors under gaussian errors , except for our sub - harmonic priors @xmath186 given by .",
    "it follows that for any such separable prior , the distributional robustness of the bayesian variable selection procedure we develop for our sub - harmonic prior will carry over .",
    "that is , whether mcmc or another computational method is used , it suffices to assume that the error distribution is gaussian ( with the same error variance ) for all submodels , but the procedure will be simultaneously valid for all ss error distributions .",
    "our prior has the advantage of analytic tractability , and as we show below , good performance .",
    "we will make use of the following result which may be founded in @xcite .",
    "[ m - g ] let @xmath109 .",
    "then @xmath187 where @xmath74 is the coefficient of determination under the submodel @xmath36 .",
    "see equation ( 5 ) of @xcite . combining lemmas [ relationship - g ] and [ m - g ] , we have the main result of this paper .",
    "[ thm : main - bf ] assume the full model @xmath7 and the submodel @xmath61 are given by and , respectively .",
    "also assume their error terms , @xmath188 and @xmath175 have the same ss distribution with mean zero and the identity covariance matrix .",
    "let @xmath189 .",
    "assume that the proper joint prior densities of @xmath190 and @xmath191 are given by and assume also @xmath192<\\infty$ ] .",
    "then , for @xmath193 , the limit of the bayes factor for comparing each of @xmath158 to the full model @xmath194 is given by @xmath195 where @xmath196    @xcite considered bayesian variable selection under gaussian errors",
    ". they proposed bayes factors with a simple analytic form under generalized ridge - type priors .",
    "the results heavily depend on special features of gaussian distributions and hence the extension or generalization of @xcite to the general ss case , may not be possible , or may not lead to analytically tractable procedures which are distributionally robust to ss error distributions .",
    "[ rem : null ] expression again shows why the null model is not allowed as a possibility . for the null model @xmath45 , @xmath197",
    "so the numerator of is infinite , and hence so would be @xmath198",
    ". this situation may be avoided at a slight cost in complexity and in interpretability of the expressions .",
    "the required alteration in the prior distributions ( proper and improper ) is to treat the intercept parameter @xmath8 as another @xmath169 , ( and not give it a `` uniform '' prior ) .",
    "this results in replacing the improper prior in by @xmath199 where @xmath200 and @xmath201 .",
    "similarly the marginal distribution in and the bayes factor given by are replaced by @xmath202 and @xmath203 where @xmath204 , ( the `` coefficient of determination '' of the model @xmath36 relative to the @xmath177-intercept model ) .",
    "hence with the substitution @xmath205 , @xmath206 , @xmath207 , @xmath208 , all expressions and results in the paper remains valid and the result ( corollary [ cor : consistency ] ) on model selection consistency in section [ sec : consistency ] holds also for the null model . clearly @xmath209 is somewhat unusual , but if model selection consistency under the null - model is desirable , we can use @xmath210 . actually , under the gaussian regression setup , @xcite and @xcite recommend use of the bayes factor as a function of @xmath209 , which just substitutes @xmath211 with @xmath212 in @xmath210 given by .",
    "[ rem : coherent ] a collection of bayes factors is called coherent if @xmath213 for all @xmath214 and @xmath215 ( see , e.g.  @xcite ) . by ,",
    "the bayes factors corresponding to our sub - harmonic priors are coherent ( with the exception of those involving the null model @xmath45 ) , which is why we require @xmath216 .",
    "also with the adaptation of the alternative specification given in remark [ rem : null ] , coherence holds for all bayes factors including those involving the null model @xmath45 .    as in @xcite and @xcite ,",
    "the posterior probability of any model is an expression of the form given by @xmath217 in our development , we choose the full model @xmath7 as the base model rather than the null model @xmath45 employing the encompassing approach of @xcite . without employing the adaptation of our prior described in remark [ rem : null ] , the choice of the full model @xmath7 as the base model , as opposed to the null model @xmath45 , is forced on us since @xmath216 .",
    "@xcite argue that the null model is the superior choice as a base model in their setup ( which also involves @xmath0-priors or mixture thereof ) due to incoherence of the bayes factors if @xmath7 is chosen as the base model ( in their setup ) .",
    "this incoherence arises in the setup of @xcite because prior distribution on the full model @xmath7 depends on each nested alternative @xmath218 .",
    "this incoherence is not a problem in our setup since the choice of prior for each submodel depends only on the submodel , and we have taken care that all relevant ( conditional ) posteriors are well defined . as noted above , by our bayes factors",
    "are coherent .",
    "in fact our development ( aside from eliminating the null model from the consideration ) is very close in spirit to the null - based bayes factors approach in @xcite .",
    "[ rem : stein - effect ] by theorem [ thm : main - bf ] , even when there is no specific information about the error distribution of each model ( other than spherical symmetry ) , but we assume they are all the same , it is not necessary to specify the exact form of the sampling density .",
    "it suffices to assume they are all gaussian .",
    "as far as we know , in the area of bayesian variable selection with shrinkage priors , the sampling density has been assumed to be gaussian and this kind of robustness result has not yet been studied .",
    "analogous robustness results have been derived by @xcite , @xcite , @xcite and others in the problem of estimating regression coefficients with the stein effect .",
    "bic ( @xcite ) is a popular criterion for model selection .",
    "see e.g.  @xcite chapter 7 .",
    "we will show in this subsection that bic has a similar distributional robustness property to the above bayes model selection procedure . in section [ sec : laplace ]",
    ", we will develop laplace approximations to our bayes factors which relate them to bic . in section [ sec : consistency ]",
    "we will show that both the bic and our bayes model selection procedures are consistent for the entire class of ss models .",
    "bic for the model @xmath36 is defined as @xmath219}_{\\gamma}=-2\\ln \\left\\ {   \\max_{\\alpha,{\\bm{\\beta}}_{{\\gamma}},\\eta_{{\\gamma}}}\\eta_{{\\gamma}}^{n/2 } f\\left ( \\eta_{{\\gamma}}\\|\\bm{y}-\\alpha \\bm{1}_n -\\bm{x}_{{\\gamma } } { \\bm{\\beta}}_{{\\gamma } } \\|^2 \\right ) n^{-q_{{\\gamma}}/2}\\right\\},\\ ] ] and is derived by eliminating @xmath84 terms from the approximate marginal densities .",
    "here we denote @xmath220}_{\\gamma}/2).\\ ] ] in general , the maximization with respect to unknown parameters in is not always tractable .",
    "however when @xmath37 has a unimodal ss distribution , the maximization is achieved by @xmath221 , @xmath222 , and @xmath223 where @xmath224 is the sole solution of @xmath225 hence @xmath226 may be expressed as @xmath227 where @xmath228 is @xmath229 with gaussian errors , specifically @xmath230 ( since @xmath231 is given by @xmath83 ) . clearly and correspond to and , respectively .",
    "hence we have the following result .",
    "[ thm : bicrobust ] assume the full model @xmath7 and the submodel @xmath61 are given by and , respectively . also assume their error terms , @xmath188 and @xmath175 have a unimodal ss distribution with the mean zero and the identity covariance matrix .",
    "then the bayes factor based on bic for comparing each of @xmath158 to the full model @xmath194 is given by @xmath232}=\\frac{m_{{\\gamma}}(\\bm{y}|\\mathrm{bic})}{m_f(\\bm{y}|\\mathrm{bic } ) } = { \\mathrm{bf}}^g{_{\\gamma : f}}{[\\mathrm{bic}]}\\ ] ] where @xmath80}$ ] is the bic based bayes factor under gaussian errors , @xmath233}= \\left\\{\\frac{(1-r_{{\\gamma}}^2)^{-n}n^{-q_{{\\gamma}}}}{(1-r_f^2)^{-n}n^{-p}}\\right\\}^{1/2}.\\ ] ]    obviously corresponds to . by theorem [ thm : bicrobust ] , the bayes factor based on bic is also independent of the error distribution provided each distribution is unimodal and is the same for all models ( c.f .",
    "theorem [ thm : main - bf ] ) .",
    "note that @xmath80}$ ] is well defined if @xmath234 .",
    "[ rem : generalization ] in an earlier version of this paper we developed the results in the more general context wherein the ss distribution of @xmath235 could depend on @xmath29 , i.e.  it could be different for each submodel @xmath61 .",
    "so @xmath236 as well .",
    "all of the above results can be developed for the more general case .",
    "the only essential changes are that in theorem [ thm : main - bf ] becomes @xmath237}{e[\\|{\\bm{\\epsilon}}_f \\|^\\nu ] } { \\mathrm{bf}}^g{_{\\gamma : f}}(\\nu)\\ ] ] and that in theorem [ thm : bicrobust ] becomes @xmath238}=\\frac{c^{-n/2}_{{\\gamma } } f_{{\\gamma}}(c_{{\\gamma}})}{c^{-n/2}_f f_f(c_f ) } { \\mathrm{bf}}^g{_{\\gamma : f}}{[\\mathrm{bic}]}\\ ] ] where @xmath239 and @xmath240 are the corresponding solution of respectively .",
    "we investigated the ranges of these `` correction terms '' in and respectively when @xmath175 and @xmath188 have possibly different ss @xmath241-distributions with at least @xmath242-degrees of freedom ( so that the variances exist ) , both analytically and numerically .",
    "we found that @xmath243 was independent of @xmath83 and reasonably stable for all @xmath64 in range @xmath244 but that stability was greater for @xmath64 close to @xmath177 . this trade - off between stability ( favoring @xmath245 ) and objectivity",
    "( favoring larger @xmath64 ) led us initially to prefer the midpoint of the allowable values in @xmath244 , namely @xmath246 as the default choice .",
    "however the examples presented in section [ sec : ex ] indicate that the performance of the method seems insensitive to the choice of @xmath64 in the range of @xmath244 .",
    "the correction factor for @xmath247}$ ] on the other hand depends on @xmath83 and is considerably less stable to changes in the distributions of @xmath175 than @xmath243 .",
    "it is interesting to note in connection with the above that the correction term @xmath179/e[\\|{\\bm{\\epsilon}}_f \\|^\\nu]$ ] for @xmath243 approaches @xmath27 as @xmath248 .",
    "hence choices of @xmath64 close to @xmath177 are essentially completely robust to choice of ss error distribution for the submodels .",
    "note also that if we force @xmath249 for all submodels such that @xmath250 , then the allowable range of @xmath64 is @xmath251 and hence @xmath252 becomes a possible choice . in this case , again , the correction term @xmath179/e[\\|{\\bm{\\epsilon}}_f \\|^\\nu]=1 $ ] regardless of the choice of error distributions , since we have assumed the variance of each component is @xmath27 .",
    "hence , again , in this case , the bayes factor is completely robust to choice of ss error distribution .",
    "additionally , the case @xmath252 corresponds to the harmonic prior @xmath253 where @xmath123 and @xmath254 .",
    "it is well - known that the harmonic prior plays important roles in estimation problems with the stein effect .",
    "see @xcite for the detail .",
    "it is interesting to observe the additional advantage of the harmonic prior in the model choice problem .",
    "see section [ sec : cr ] for some additional discussion of such priors .",
    "in section [ sec : marginal ] , we saw that the bayes factor @xmath255 under ss errors is equal to @xmath73 , which is the bayes factor under gaussian errors . in this section",
    ", we consider the so - called laplace approximation of some bayes factors under gaussian errors .",
    "we will approximate not only the function @xmath73 but also bayes factors with respect to more general priors where the prior on @xmath0 is ; @xmath256 when the same prior on @xmath0 is used for @xmath158 and @xmath194 , improper choices of @xmath64 ( @xmath257 ) as well as proper choices of @xmath64 ( @xmath258 ) are valid for use . under gaussian errors ,",
    "the bayes factor for comparing each of @xmath158 to @xmath194 is well - defined as @xmath259 \\\\ & = \\frac{\\int_0^{\\infty } g^{\\nu/2 - 1}(1+g^{-1})^{-k/2}(1+g)^{\\frac{n - q_{{\\gamma}}-1}{2 } } \\{g(1-r_{{\\gamma}}^2)+1 \\}^{-\\frac{n-1}{2}}\\ , dg } { \\int_0^{\\infty } g^{\\nu/2 - 1}(1+g^{-1})^{-a/2}(1+g)^{\\frac{n - p-1}{2 } } \\{g(1-r_f^2)+1 \\}^{-\\frac{n-1}{2}}\\ , dg } \\end{split}\\ ] ] where @xmath132 , @xmath260 .",
    "first we provide a summary of laplace approximations to the integral based on @xcite . for integrals of the form @xmath261 we make the use of the fully exponential laplace approximation , based on expanding a smooth unimodal function @xmath262 in a taylor series expansion about @xmath263 , the mode of @xmath262 .",
    "the laplace approximation is given by @xmath264 where @xmath265 in the following , we will use the symbol @xmath266 ( @xmath86 ) if @xmath267 hence the approximation given by is written as @xmath268    the next result gives approximations of the bayes factor in terms of the bayes factor based on bic given in .",
    "[ thm : laplace ] let the prior be given by .",
    "assume that @xmath269 does not depend on @xmath83 .    1 .",
    "assume @xmath270 , and @xmath271 .",
    "then @xmath272 where @xmath273 .",
    "[ thm : laplace : part1 ] 2 .",
    "assume that @xmath260 and that @xmath269 does not depend on @xmath36 .",
    "assume also @xmath274 .",
    "then @xmath275 \\approx \\widetilde{{\\mathrm{bf}}}^g{_{\\gamma : f}}(\\nu)$ ] where @xmath276}\\ ] ] and @xmath277}$ ] is the @xmath278 based alternative under gaussian errors.[thm : laplace : part2 ]    see appendix .",
    "clearly the function @xmath279 does not depend on @xmath83 and hence theorem [ thm : laplace ] shows that @xmath275 $ ] is asymptotically equivalent to bic with a simple @xmath84 correction function depending @xmath64 as well as @xmath280 and the @xmath85-squares .",
    "although several fully bayes factors for the variable selection problem have been proposed in the literature , the relationship between the approximate bayes factors and naive bic has not been shown to the authors knowledge . in this sense , while the main contributions in this paper are given in section [ sec : marginal ] , theorem [ thm : laplace ] may be a practically useful contribution because of the simplicity of the approximate bays factor .    in this section ,",
    "we have considered general bayes factors under gaussian errors .",
    "remember that @xmath243 the bayes factor w.r.t .",
    "sub - harmonic priors , under ss errors , is equal to @xmath281 $ ] for @xmath282 . under gaussian errors , @xcite recommended the use of @xmath283 $ ] with @xmath284 .",
    "@xcite and @xcite recommended the use of @xmath285 $ ] . by theorem [ thm : laplace ] , these bayes factors may be approximated as follows .",
    "[ cor : approxbf ] @xmath286 \\approx \\widetilde{{\\mathrm{bf}}}^g{_{\\gamma : f}}(\\nu )   \\mbox { for   } 0<\\nu<1 . \\quad \\mbox{sub - harmonic prior}\\\\ &    { \\mathrm{bf}}^g{_{\\gamma : f}}[0,2 ] \\approx \\widetilde{{\\mathrm{bf}}}^g{_{\\gamma : f}}(0 ) .",
    "\\quad \\mbox{a version of \\cite{guo - speckman-2009}}\\\\ &    { \\mathrm{bf}}^g{_{\\gamma : f}}[\\nu,2-\\nu ] \\approx \\widetilde{{\\mathrm{bf}}}^g{_{\\gamma : f}}(\\nu )   \\mbox { for   } -2<\\nu<0 . \\quad \\mbox{\\cite{liang - etal-2008 } } \\end{split}\\ ] ]    in section [ sec : ex ] , we will see how approximate bayes factors @xmath287 work numerically and how sensitive they are to to the choice of @xmath64 .",
    "in this section , we consider model selection consistency in the case where @xmath3 is fixed and as @xmath83 approaches infinity .",
    "let @xmath288 be the true model , @xmath289 consistency for model choice is defined as @xmath290 where plim denotes convergence in probability and the probability distribution is the sampling distribution under the true model @xmath288 .",
    "we will show that bayes factors considered in the previous sections has a model selection consistency under generally ss errors .",
    "the consistency property is clearly equivalent to @xmath291 for model selection consistency , we make the following assumptions ;    1 .   @xmath292 is bounded in probability from below and from above ,",
    "that is , for any @xmath293 and any positive integer @xmath83 , there exists an @xmath294 such that @xmath295 [ as1 ] 2 .",
    "the limit of the correlation matrix of @xmath33 , @xmath296 , exists and is positive definite .",
    "[ as2 ]    a[as1 ] seems more general than necessary .",
    "it appears that , by the law of large numbers , @xmath297 ought to converge to @xmath27 in probability , but this is not necessarily true if the error distribution is not gaussian . in the case of a scale mixture of gaussians , @xmath297 approaches , in law , a random variable @xmath298 which has the distribution of the mixing variable of the variance .",
    "even when the error distribution is not a scale mixture of gaussians , a[as1 ] appears to be a reasonable and minimal assumption .",
    "a[as2 ] is the standard assumption which also appears in @xcite and @xcite . under these mild assumptions",
    ", we have following preliminary results for proving the consistency .",
    "[ lem : prel ] assume a[as1 ] and a[as2 ] .    1 .",
    "[ 1:lem : prel ] assume @xmath193 .",
    "for any @xmath299 and any positive integer @xmath83 , there exists a @xmath300 such that @xmath301 2 .",
    "[ 2:lem : prel ] let @xmath302",
    ". then @xmath303 .",
    "further for any @xmath299 and any positive integer @xmath83 , there exists a @xmath304 such that @xmath305 3 .",
    "[ 3:lem : prel ] let @xmath306 .",
    "then for any @xmath299 and any positive integer @xmath83 , there exists a @xmath307 such that @xmath308    see appendix .",
    "first we give a consistency result on bic .",
    "[ thm : bicconsistency ] assume a[as1 ] and a[as2 ] .",
    "the bayes factor based on bic under gaussian errors @xmath81}= \\left\\{\\frac{(1-r_{{\\gamma}}^2)^{-n}n^{-q_{{\\gamma}}}}{(1-r_f^2)^{-n}n^{-p}}\\right\\}^{1/2}\\ ] ] is consistent for model selection under ss errors ( including @xmath234 ) .",
    "we show that @xmath309}}{{\\mathrm{bf}}^g_{t : f}{[\\mathrm{bic}]}}= { \\mathop{\\mathrm{plim}}}_{n \\to \\infty } \\left\\ { n^{q_t - q_{{\\gamma } } }   \\left(\\frac{1-r^2_t}{1-r^2_{{\\gamma}}}\\right)^{n } \\right\\}^{1/2}=0.\\ ] ] consider the following two situations :    1 .",
    "@xmath302 : by part [ 2:lem : prel ] of lemma [ lem : prel ] , @xmath310 is bounded in probability . since @xmath311 , is satisfied .",
    "@xmath306 : by part [ 3:lem : prel ] of lemma [ lem : prel ] , @xmath312 is strictly less than @xmath27 in probability .",
    "hence @xmath313 converges to zero in probability exponentially fast with respect to @xmath83 .",
    "therefore , no matter what value @xmath314 takes , is satisfied .",
    "these complete the proof .",
    "note that in theorem [ thm : bicconsistency ] we do not exclude the null model @xmath45 and hence bic has model selection consistency even when the null model is true .",
    "when we consider consistency of the bayes factors treated in the previous sections , @xmath255 , @xmath315 $ ] , @xmath316 , we have to exclude the null model @xmath45 , but they all still have model selection consistency among non - null models .",
    "[ cor : consistency ] assume a[as1 ] and a[as2 ] .",
    "@xmath269 is assumed independent of @xmath83 and @xmath61 .",
    "assume also @xmath45 is excluded from possible models",
    ". then    1 .",
    "@xmath287 for @xmath317 is consistent for model selection under ss errors .",
    "@xmath315 $ ] for @xmath318 is consistent for model selection under ss errors .",
    "@xmath243 for @xmath282 is consistent for model selection under ss errors .    by part [ 1:lem : prel ] of lemma [ lem : prel ] , when @xmath319 , both @xmath320 and @xmath321 are positive and strictly less than 1 with probability 1 .",
    "hence both @xmath322 and @xmath323 where @xmath79 are positive and bounded from above with probability 1 provided @xmath324 and @xmath64 is independent of @xmath83 and @xmath61 .",
    "( on the other hand , since @xmath325 , @xmath326 is not defined . ) as in theorem [ thm : laplace ] , @xmath327}.\\ ] ] hence consistency of @xmath287 follows from consistency of bic .",
    "further as @xmath78 , we have @xmath328 \\approx \\widetilde{{\\mathrm{bf}}}^g{_{\\gamma : f}}(\\nu)\\ ] ] by theorem [ thm : laplace ] provided @xmath318 and @xmath269 are independent of @xmath83 and @xmath61",
    ". hence consistency of @xmath315 $ ] follows from consistency of @xmath287 .",
    "remember that @xmath243 , the bayes factor w.r.t .",
    "sub - harmonic priors , under ss errors , is equal to @xmath281 $ ] for @xmath282 .",
    "hence consistency of @xmath243 follows from consistency of @xmath315 $ ] .",
    "@xcite established model selection consistency for @xmath329 and @xmath146 for gaussian errors .",
    "corollary [ cor : consistency ] in conjunction with theorem [ thm : laplace ] extends their result to the entire class of ss distributions for a broader class of @xmath64 and @xmath330 . also as noted in remark [ rem : null ]",
    ", @xmath331 results in model selection consistency for all models including the null model for all ss distributions .",
    "additionally a development along the lines of theorem [ thm : laplace ] allows an analogous bic based approximation to @xmath331 .",
    "this extension also allows an extension of model selection consistency to all ss error distributions for the method of @xcite based on @xmath332 .",
    "further as noted above , an alternative robust sub - harmonic prior based method , the model selection consistency for these @xmath332 based methods also apply for the null model .",
    "it should be emphasized in each of the above cases that is is the bayes factor method developed for the gaussian case that is shown to have model selection consistency for the entire class of ss error distributions .",
    "these gaussian based bayes factors , however , are not bayes factors for error distributions which are not gaussian , the sole exception being our robust bayes factors which are based on separable priors in the sense described earlier , and which are simultaneously bayes factors relative to the same prior .",
    "the issue of model selection consistency in our setup , is somewhat complicated by the wide choice of possible error distributions .",
    "if all errors are normally distributed , then under our assumptions a[as2 ] on the design matrix @xmath20 ,",
    "imply that each @xmath74 approaches a constant , and that @xmath333 .",
    "if on the other hand , all models are variance mixtures of gaussians with mixture variance distributed as a positive random variable @xmath298 , then @xmath334 a random variable , and @xmath335 also approaches a random variable which is bounded above and below in probability provided that @xmath298 is similarly bounded .    in general philosophical terms , it might be better to assume that the sequence of error terms @xmath336 are exchangeable for all @xmath83 . by de finetti s theorem",
    ", this would imply that the error terms all have a variance mixture of normal distributions .",
    "we have chosen a slightly weaker requirement on the sequence of error distributions , namely , that @xmath337 remains bounded above and below in probability , which extracts the necessary limiting behavior of the error terms to ensure consistency of model selection .",
    "interestingly , although we attain model selection consistency with these assumptions , it is not necessarily true that @xmath338 is consistently estimated by @xmath337 .",
    "in this section , we provide illustrations of the method using both simulated and real data . in each example",
    ", we compare several different versions of the laplace approximated bayes factors @xmath287 and @xmath80}$ ] .",
    "the values of @xmath64 are @xmath339 .",
    "these choices correspond to our default choice , @xmath340 and @xmath341 which also satisfies our robustness condition @xmath282 .",
    "the choice @xmath147 approximates @xmath342 $ ] of @xcite and @xmath343 and @xmath344 approximates two choices of @xcite as presented in corollary [ cor : approxbf ] .",
    "we compare numerical performance of our with bic in a small simulation study .",
    "we generated 16 possible correlated predictors ( @xmath345 ) as follows : @xmath346 here `` cor '' denotes the correlation of two gaussian random variables .",
    "also @xmath347 , @xmath348 , @xmath349 , @xmath350 , @xmath351 , @xmath352 are assumed to be independent . after generating pseudo random @xmath353 , we centered and scaled them as noted in section [ sec : intro ] .",
    "we set @xmath354 and consider 4 cases where the true predictors are    @xmath355 : :    @xmath356 @xmath357 : :    @xmath358 @xmath359 : :       @xmath360 @xmath361 : :     @xmath362    ( where @xmath363 denotes the number of true predictors ) and the true model is given by @xmath364 with @xmath365 .",
    "tables [ normal ] and [ multi - t ] show how often the true model ranks first and how often it is in the top 3 among @xmath366 candidates when the number of replicates is @xmath367 .",
    "the error distributions are gaussian ( table [ normal ] ) and multivariate-@xmath241 with 3 degrees of freedom ( table [ multi - t ] ) . for the case of normally distributed errors ( table [ normal ] )",
    ", the bayes factor methods performed well and stably for @xmath368 and @xmath369 and did reasonably well for @xmath370 for the smaller true models ( @xmath371 ) .",
    "bic seemed , generally , to have a preference for larger models , and performed much less well than the bayes factor method for @xmath368 and @xmath369 for models of smaller size ( @xmath372 ) for @xmath370 , bic did substantially better than bf for the largest model ( @xmath373 ) and somewhat better for @xmath374 .",
    "performance of @xmath287 seemed relatively insensitive to the choice of @xmath64 .",
    "when @xmath375 , the choice of @xmath64 makes little difference .",
    "but when @xmath376 , positive @xmath377 seems to perform better especially for larger @xmath378 .",
    "interestingly , for the case of a multivariate-@xmath241 error distribution with @xmath242 degrees of freedom ( the minimum so that a variance exists ) , the numerical results were quite similar to those in the normal case for both @xmath287 and bic , both quantitatively and qualitatively .",
    "one possible aspect of the relative insensitivity of the results to choice of @xmath64 in heavy tailed case is the extension of model selection consistency for the entire class of ss errors to a broad class mixture of @xmath0-prior based methods given by corollary [ cor : consistency ] .",
    ".frequency of the true model ( gaussian error ) [ cols=\"^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ table : uscrime ]",
    "bayesian model selection for linear regression models with gaussian errors has been popular area of study for some time .",
    "there is also a substantial literature devoted to studying the extension of stein - type shrinkage estimators from models with gaussian errors to those with general ss errors . in particular",
    ", it has long been observed that certain shrinkage estimators which improve over the least squares ( ls ) estimator for gaussian models also improve over the ls estimator simultaneously for all ss error models ( see for example , @xcite ) . @xcite and @xcite found , in addition , that certain separable priors ( in the sense described in section [ sec : prior ] ) leads to generalized bayes shrinkage estimators that do not depend on the form of the underlying ss distribution and that also simultaneously improve on the ls estimator , sometimes dramatically so .",
    "the original aim of this research was to see if similar separable priors could be found that have this distributional robustness property in the variable selection problem ( and , of course , also to perform well ) .",
    "the generalized bayes priors developed in sections [ sec : prior ] and [ sec : marginal ] turned out to satisfy our requirements and also to be closely related to other so called @xmath0-priors ( or mixtures of @xmath0-priors ) in the literature ( see e.g.  @xcite ) .",
    "our original development required that the null model be excluded from the class of possible models , however we observed ( see remark [ rem : null ] ) that a slightly altered version which treats the intercept term the same as all other regression coefficients ( as opposed to giving it a uniform prior ) , allows the null model to be included as well .",
    "this alternative development gives bayes factor which depend on the @xmath379 relative to the null model where all coefficients including @xmath8 is @xmath177 .",
    "this dependence on @xmath379 is related to the model selection procedures of @xcite .",
    "our prior distribution on the regression parameters is sub - harmonic for each ( non - null ) model , and as such , leads to admissible estimators under quadratic loss in each of the non - null models when the variance is known ( see @xcite ) regardless of the ss error distribution .",
    "also if @xmath380 and @xmath252 the generalized bayes estimator for each submodel is minimax and dominates the james - stein estimator ( see @xcite ) . hence although we require @xmath381 , we nevertheless expect good performance of the corresponding bayes estimators .",
    "the expression of our bayes factors , e.g.  , are relatively simple involving the ratio of two @xmath27-dimensional integrals . to further simplify calculations we investigated laplace approximations to our bayes factors , and more generally , to a collection of bayes factors arising from mixtures of @xmath0-priors that have recently appealed ( see @xcite ) .",
    "we show in section 4 that in each case the bayes factor can be approximated as the bayes factor for the gaussian model based on bic times a simple rational function depending @xmath101 , @xmath64 and the @xmath1 of the models .    using these laplace approximations we are able to establish model selection consistency of our robust procedure for the entire class of ss distributions and to extend the model consistency results of several earlier papers for the gaussian case to the entire class of ss distributions .",
    "a small simulation study and an analysis of the hald data ( see @xcite ) and the us crime data ( see @xcite ) indicates that our method performs well .",
    "it gives results consistent with the results of the cited papers for the real data sets and performs comparably and sometimes better than several of the mixture of @xmath0-prior methods .",
    "under the submodel @xmath36 , the conditional marginal density of @xmath51 with respect to improper prior @xmath121 given @xmath8 and @xmath34 is @xmath382}{e[\\|{\\bm{\\epsilon}}_g\\|^\\nu ] } \\int_0^\\infty   \\eta_{{\\gamma}}^{n/2}f_g\\left(\\eta_{{\\gamma}}\\|\\bm{y}-\\alpha \\bm{1}_n -\\bm{x}_{{\\gamma}}{\\bm{\\beta}}_{{\\gamma}}\\|^2\\right ) \\eta_{{\\gamma}}^{\\nu/2 - 1 } d\\eta_{{\\gamma } } \\end{split}\\ ] ] where @xmath383 , provided @xmath384<\\infty.\\ ] ] therefore , we have @xmath385 } { e[\\|{\\bm{\\epsilon}}_g\\|^\\nu ] } m^g_{{\\gamma}}(\\bm{y}|\\nu).\\ ] ]        for the submodel @xmath36 , @xmath405 is given by @xmath406 with @xmath407 . the numerator and denominator are rewritten as @xmath408 where @xmath409 and similarly @xmath410 since @xmath411 , @xmath412 is bounded as @xmath413 where @xmath414 and @xmath415 . in , we have the following .    * since @xmath416=\\bm{0}$ ] and @xmath417=\\bm{i}_n$ ] , @xmath418=\\bm{0}$ ] and @xmath419 therefore @xmath420 approaches @xmath177 in probability .",
    "* when @xmath421 , @xmath422 is a zero matrix . when @xmath306 , @xmath423 in probability can be proved as .",
    "* by the assumption a[as2 ] , @xmath424 is positive - definite for any @xmath83 and hence @xmath425 * @xmath426 converges to @xmath27 in probability .",
    "* by the assumption a[as1 ] on @xmath427 , @xmath428 is also bounded in probability from below and from above .      since @xmath430 for @xmath421 and using , @xmath431 is given by @xmath432 .",
    "further we easily have @xmath433 note @xmath434 is distributed as @xmath435 where @xmath436 and @xmath437 are independent .",
    "hence @xmath438 since @xmath439 in probability .",
    "therefore @xmath440 is bounded in probability from above and hence the theorem follows .",
    "@xmath441 is written as @xmath442 clearly @xmath443 in probability . also since @xmath306 , @xmath444 for any @xmath83 .",
    "further as @xmath445 in probability , @xmath441 is strictly smaller than @xmath27 in probability .",
    "denote the left - hand side of by @xmath386 .",
    "when approximating @xmath386 , make the change of variables @xmath387 .",
    "see @xcite for details . with this transformation ,",
    "the integral becomes @xmath388 where the extra @xmath389 comes from the jacobian of the transformation of variables .",
    "denote the logarithm of the integrand function in by @xmath262 .",
    "we have @xmath390 where @xmath391 . since @xmath271 and @xmath392 , the equation @xmath393 has the only one positive root @xmath394 .",
    "it clearly satisfies @xmath395 hence we have @xmath396 similarly , as in , we have @xmath397 and @xmath398 therefore we have @xmath399 as @xmath86 . hence the part [ thm : laplace : part1 ] of the theorem follows ."
  ],
  "abstract_text": [
    "<S> this paper studies bayesian variable selection in linear models with general spherically symmetric error distributions . </S>",
    "<S> we propose sub - harmonic priors which arise as a class of mixtures of zellner s @xmath0-priors for which the bayes factors are independent of the underlying error distribution . </S>",
    "<S> as long as it is spherically symmetric . </S>",
    "<S> because of this invariance to spherically symmetric error distribution , we refer to our method as a robust bayesian variable selection method . </S>",
    "<S> we demonstrate that our bayes factors have model selection consistency and are coherent . </S>",
    "<S> we also develop laplace approximations to bayes factors for a number of recently studied mixtures of @xmath0-priors that have appeared in the literature ( including our own ) for gaussian errors . </S>",
    "<S> these approximations , in each case , are given by the gaussian bayes factor based on bic times a simple rational function of the prior s hyper - parameters and the @xmath1 s for the respective models . </S>",
    "<S> we also extend model selection consistency for several @xmath0-prior based bayes factor methods for gaussian errors to the entire class of spherically symmetric error distributions . a simulation study and </S>",
    "<S> an analysis of two real data sets indicates good performance of our robust bayes factors relative to bic and to other mixture of @xmath0-prior based methods . </S>"
  ]
}