{
  "article_text": [
    "the last few years have seen considerable growth in the use of graphics processing units ( gpus ) to accelerate general purpose applications .",
    "today , the opencl standard  @xcite is used to express applications in which computationally intensive segments , or _",
    "kernels _ are launched for execution on the gpu .",
    "however , to realize the performance benefits of gpus , programmers must _ optimize _ their kernels to better exploit the underlying gpu architecture  @xcite .",
    "one important optimization is the use of local memory  a small user - managed on - chip storage , which can improve performance by an order of magnitude",
    ". however , the optimization is not always beneficial , depending on 1 ) the amount of data reuse and the degree of memory non - coalescing in the kernel , and 2 ) the instruction overhead and the amount of drop in parallelism the optimization introduces . the extent to which these factors influence the optimization s benefit is often not clear .",
    "there is no simple heuristic for deciding whether or not the optimization should be applied .",
    "we explore the use of machine learning to auto - tune the local memory optimization .",
    "we build a model that predicts the benefit of caching a region of an array in local memory , based on the performance of a set of training kernels with and without the optimization .",
    "we then apply this model to a new kernel to decide if the optimization should be applied .",
    "a unique aspect of our work is the use of many synthetically generated kernels for model training .",
    "we believe that machine learning , particularly on a high - dimensional feature space , demands a large training set which is difficult to assemble from real - world benchmarks .",
    "the use of synthetic benchmarks allows us to build a robust model fully exploiting the power of machine learning .",
    "these synthetic kernels capture common data access patterns in the domains of dense linear algebra and structured grids .",
    "performance data of a large number of synthetic kernel instances ( with and without using local memory ) on an nvidia gpu shows that the optimization brings a wide range of kernel speedup ( from @xmath0 to @xmath1 ) .",
    "a _ random forest _  @xcite model trained on",
    "a random 10% of the data can predict if the optimization is beneficial , on the remaining data , with nearly 95% accuracy .",
    "this model also achieves an average of nearly 95% accuracy on eight real - world kernels .    to our best knowledge ,",
    "we believe that this is the first work that : 1 ) builds an accurate model using machine learning to auto - tune the local memory optimization , and 2 ) uses a large number of synthetic benchmarks for model training .",
    "local memory is a software - managed cache shared by all workitems  within a workgroup . despite being small ( in kilo - bytes )",
    ", its use can significantly improve kernel performance , because it has close - to - register access latency  @xcite .",
    "local memory improves performance for two reasons .",
    "first , caching data in local memory exploits data locality in the kernel and can reduce the number of transactions reaching the gpu dram .",
    "data locality in a kernel ( executed by many threads ) can be classified as _ temporal _ or _ spatial _ , and _ intra - thread _ or _",
    "inter - thread_. local memory is commonly used to exploit all four categories of data locality except intra - thread temporal locality , where the gpu compilers are able to put the data in thread - private registers .",
    "the benefit of using local memory increases with the amount of data reuse .    even in the absence of data reuse ,",
    "local memory can improve kernel performance by transforming non - coalesced memory accesses into coalesced ones  @xcite .",
    "a common scenario is when each workitem  performs some sort of row - wise reduction , forcing workitems  to access a _",
    "column _ of a 2d array at the same time .",
    "this results in totally non - coalesced accesses that come with a high performance penalty .",
    "these non - coalesced accesses can be eliminated by first copying a batch of the columns to the local memory in a coalesced manner .",
    "workitems access data from the local memory , still one column at a time , but with no performance penalty .",
    "this is because , unlike global memory , local memory does not suffer from non - coalescing .",
    "the copying of an array region from global memory to local memory is performed _",
    "cooperatively _ by all workitems  in a workgroup .",
    "the region is divided into a sequence of row segments , each having a width of a single dram transaction and is aligned to the transaction boundary  @xcite .",
    "these segments are cyclically distributed among _",
    "warps _ in the workgroup .",
    "elements in each segment are accessed by workitems  in the designated warp in a fully coalesced manner , resulting in a single dram transaction .",
    "overall , all global memory accesses made during the copying process are fully coalesced .",
    "while the use of local memory reduces the number of dram transactions , it may not always improve the performance of the kernel as a whole .",
    "first , the optimization introduces the overhead of copying array regions from the global memory to the local memory .",
    "second , it may reduce the level of parallelism , i.e. , the number of threads that can concurrently execute on a gpu multi - processor , due to additional resource usage . a reduction in parallelism",
    "can hurt kernel performance in a holistic manner , potentially exposing latencies of _ all _ memory accesses .",
    "the extent to which this happens is a function of how many memory accesses exist in the kernel as well as the amount of computation in the kernel that may help _ hide _ this performance penalty .",
    "therefore , the performance impact of using local memory depends on various kernel characteristics :    * amount of data reuse of the array region copied to the local memory * degree of memory non - coalescing of array accesses * usage of registers and local memory by the optimization , which can reduce parallelism * memory accesses and computation in the unoptimized kernel , which influences the performance impact of any parallelism drop    in order to assess the performance impact of these factors , we synthetically generate a large number of kernels with varying values of the characteristics listed above , and evaluated eight real - world benchmarks with varying launch configurations and other kernel parameters ( section  [ sec : eval ] ) . for each kernel",
    ", we empirically determine the kernel speedup of the local memory optimization , as the ratio of the execution time of the original kernel over that of the optimized kernel .",
    "figure  [ fig : speedup - histograms ] shows the histograms of the resulting speedup values for both synthetic and real - world kernels .",
    "it confirms that the use of local memory is not always beneficial and its performance impact is non - trivial to determine .",
    "given a kernel ( with its launch configuration ) containing accesses to an array that may exhibit data reuse or memory non - coalescing , our framework decides if the local memory optimization would improve kernel performance , by caching the smallest array region that covers these accesses in the local memory .",
    "the framework consists of two phases , as shown in figure  [ fig : mlat - framework - overview ] . in the first phase ( the left part of the figure )",
    ", a model is built using a machine learning algorithm , based on performance data of a set of _ training opencl kernels _ with and without the local memory optimization .",
    "the model correlates 1 ) characteristics ( or _ features _ ) extracted from a kernel , e.g. , the degree of data reuse and the presence of memory non - coalescing and 2 ) optimization configuration , i.e. , the shape of the array region to be cache , with 3 ) the benefit of the optimization , e.g. , kernel speedup .    in the second phase ( the right part of figure  [ fig : mlat - framework - overview ] ) , given a new ( non - training ) kernel along with the candidate array accesses , we extract kernel features as required by the model , determine the shape of the array region to be cached in local memory , and apply the model to predict if the optimization is beneficial .    since a large number of training kernels is required for machine learning to work well , we opt to synthetically generate them instead of using real - world kernels . in the rest of the section",
    ", we discuss the design of the synthetic kernels and the machine learning model .",
    "we design the synthetic kernels in the form of a single kernel template with a number of compile - time and run - time parameters .",
    "these parameters are `` knobs '' to alter kernel characteristics that may influence the benefit of using local memory ( section  [ sec : opt - perf - impact ] ) .",
    "the kernel template is shown in figure  [ fig : syn - kernel - template ] .",
    "it processes data from a 2d input array ` in ` and writes the result to a 2d output array ` out ` .",
    "we call the amount of work that produces an output array element a _ work unit _ , shown between lines 14 and 33 .",
    "it contains two nested loops followed by a code segment , which we call _ epilogue_. the loop nest encloses one or more accesses to array ` in ` , interleaved with computation ( fused - multiply - add operations ) and accesses to an auxiliary input array ` in2 ` , a third kernel parameter .",
    "the epilogue also contains computation and accesses to ` in2 ` , and ends with a write to array ` out ` .",
    "accesses to array ` in ` are those to be possibly cached in local memory , so we also refer to array ` in ` as the _ target array_.    accesses to array ` in ` in the inner loop body are centered around a _ home coordinate _ ` ( idx_o , idx_i ) ` , with different constant offsets in each dimension : ` co_1 ` , ... , ` co_k ` , ` ci_1 ` , ... , ` ci_k ` ( lines 25 and 27 ) .",
    "the home coordinate is a linear function of the current work unit coordinate ` ( wu_x , wu_y ) ` and the loop iterators ` i ` and ` j ` , as specified by ` fo ` and ` fi ` at lines 23 and 24 .",
    "the 2d grid of work units is distributed across a 2d space of workgroups  in a blocked manner , and is further distributed across a 2d space of workitems  in a cyclic manner .",
    "the geometry of array ` out ` and the launch configuration collectively determine the number of work units each workitem  processes ( ` num_wus_x ` and ` num_wus_y ` at lines 12 and 13 .",
    "the current work unit coordinate ` ( wu_x , wu_y ) ` is computed based on work group i d ` ( wg_x , wg_y ) ` , work item i d ` ( wi_x , wi_y ) ` and work unit i d ` ( iter_x , iter_y ) ` , at line 15 .",
    "the kernel template provides the flexibility to vary those characteristics that may affect the benefit of caching ` in ` data in the local memory .",
    "first , the data access pattern to the target array ` in ` is configurable .",
    "it is collectively defined by 1 ) the function tuple ( ` fo ` , ` fi ` ) that determines the _ home access pattern _ , and 2 ) the offsets ` co ` s and ` ci ` s that determine the _ stencil pattern _ ( within each workitem ) .",
    "we design 7 function tuples , shown in figure  [ fig : home - access - pattern - visual ] , that correspond to regular access patterns with potentially different degrees of data reuse and memory non - coalescing . in each diagram ,",
    "an arrow indicates the order of home coordinates ( of array ` in ` ) a workitem  accesses as it goes through the iterations of loop ` i ` and ` j ` ( line 21 and 22 of figure  [ fig : syn - kernel - template ] ) .",
    "each arrow is associated with a label that indicates what workitems  of a workgroup  make such accesses .",
    "for example , ` wi(1 , * ) ` refers to all workitems  with ` wi_x = 1 ` .",
    "combined with the stencil pattern , this label reflects the amount of data reuse .",
    "the entire grey region shown in each diagram , extended with apron regions that cover neighbouring accesses , corresponds to the region of ` in ` to be cached in the local memory , reflecting the amount of resources consumed .",
    "we use three common stencil patterns : rectangular , diamond and star , shown in figure  [ fig : syn - kernel - neighbourhood - patterns ] .",
    "the target array ` in ` is padded to ensure no out - of - bound accesses .",
    "in addition to the data access pattern , the amount of computation and memory accesses in the inner loop body and the epilogue are also configurable .",
    "they collectively model _ contextual _ kernel computation and memory accesses that may affect the optimization s benefit with a drop in parallelism .",
    "further , they enable variation of kernel register usage .    table  [ tab : syn - kernel - params ] summarizes the list of 13 parameters the kernel template provides .",
    "run - time parameters are shown in italics ; the rest are compile - time parameters .    [ cols=\"<,<,<\",options=\"header \" , ]     we collect the execution time of all kernel instances ( both synthetic and real ) on nvidia tesla m2090 with 6 gb of memory , housed in a system with an intel xeon e5 - 2620 cpu and 64 gb of memory , running cuda 5.0 on centos 6.4 .",
    "we measure the execution time of the kernel only .",
    "figure  [ fig : speedup - hist - syn ] showed the distribution of synthetic kernel speedup values brought by the local memory optimization .",
    "figure  [ fig : speedup - hist - transpose]-  [ fig : speedup - hist - mri - gridding ] showed the distribution of real - world kernel speedup values .",
    "we make two observations from the figures .",
    "first , the use of local memory is not always beneficial for both synthetic and real - world kernels .",
    "second , the speedup distributions have different shapes across the synthetic and real - world kernels .",
    "this demonstrates the need for auto - tuning the use of local memory .",
    "we train a model using the performance data of randomly selected 560k synthetic kernel instances ( 10% of the total ) .",
    "we build the model using random forest ( rf )  @xcite , from weka 3.7.10  @xcite , configured with 20 trees ( of unlimited depth ) and 4 attributes per tree node .",
    "we evaluate the model s accuracy by applying it to the remaining synthetic kernel instances and all real - world kernel instances .",
    "we use two accuracy metrics .",
    "the first is _ count - based accuracy _ , defined as the percentage of kernel instances where the decision of whether or not to use local memory , predicted by the model , matches the _ oracle _ decision based on actual kernel performance data .",
    "effectively this metric assigns to each kernel instance a score of 1 when the model predicts correctly and 0 otherwise , and computes the average across all kernel instances .",
    "note that , when the model mis - predicts , the metric does not take into account the performance loss it incurs . hence we introduce the second metric , _ penalty - weighted accuracy _ , which extends the first metric by assigning a score equal to the performance ratio ( a value between 0 and 1 ) , instead of 0 , when the model mis - predicts .",
    "effectively this metric measures the percentage of kernel performance achieved using the model - predicted decision , over that achieved by the oracle decision , averaged across all kernel instances .",
    "figure  [ fig : model - accuracy - syn - training ] shows the model accuracy , with both metrics , for synthetic and real - world kernels . for penalty - weighted accuracy , we also show the range ( min - max ) of per - kernel - instance scores using error bars .",
    "overall , the trained model achieves 86% count - based accuracy and nearly 95% penalty - weighted accuracy on the remaining synthetic kernel instances .",
    "however , a 30% minimum score indicates that the model does mis - predict on a small percentage of kernel instances with high performance penalty . for real - world benchmarks , the model is able to achieve nearly 95% penalty - weighted accuracy , although the count - based accuracy drops noticeably for sad , tpacf and mri - gridding .",
    "this shows that the model trained with a large number of synthetic kernels can achieves high accuracy consistently across a variety of kernels .",
    "[ fig : model - accuracy - syn - training ]",
    "there has been growing interest in the use of machine learning to auto - tune the performance of gpu applications  @xcite .",
    "for example , magni et al .",
    "@xcite explore the use of neural network for auto - tuning thread coarsening , and grewe et al .  @xcite use a decision tree to decide if an opencl kernel should be executed on the cpu or the gpu .",
    "in contrast , we focus on auto - tuning the use of local memory .",
    "there is work that explored auto - tuning of the use of local memory , but focused on the use of analytical modeling and empirical search  @xcite .",
    "in contrast , we build machine learning models , which have the potential to be more accurate than analytical approaches .    finally , there is a large body of work that treats auto - tuning for platforms other than gpus , including multi - cores  @xcite and single - core processors  @xcite .",
    "in contrast , we focus on gpus .",
    "in this paper we described a machine learning model for use in automatic performance tuning of local memory usage on gpus .",
    "we have shown that the optimization is not always beneficial , depending on application characteristics .",
    "we train a random forest model with a large number of synthetic benchmarks and predict whether local memory should be used or not for a single array access in both synthetic and real benchmarks .",
    "we have shown that the penalty - weighted prediction accuracy is nearly 95% .",
    "this work can be extended in various directions .",
    "first , the use of this model in practice demands a compiler framework that automatically applies the local memory optimization and extracts kernel features .",
    "second , the model can be extended to predict the _ usage _ of local memory when multiple arrays compete for local memory resources .",
    "third , the prediction accuracy can be evaluated for a larger set of real - world benchmarks .",
    "fourth , the quality of the machine learning model using synthetic benchmarks , as opposed to real benchmarks , can be evaluated .",
    "finally , other machine learning models ( e.g. , support vector machines ) can be evaluated ."
  ],
  "abstract_text": [
    "<S> the use of local memory is important to improve the performance of opencl programs . however , its use may not always benefit performance , depending on various application characteristics , and there is no simple heuristic for deciding when to use it . </S>",
    "<S> we develop a machine learning model to decide if the optimization is beneficial or not . </S>",
    "<S> we train the model with millions of synthetic benchmarks and show that it can predict if the optimization should be applied for a single array , in both synthetic and real benchmarks , with high accuracy . </S>"
  ]
}