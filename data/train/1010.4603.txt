{
  "article_text": [
    "magnetic recording channels are typically modeled as binary - input intersymbol interference ( isi ) channels , also called partial response channels @xcite .",
    "an implicit assumption made in these channel models is that the data are correctly written on the disk and that errors occur only during the readback process .",
    "while this is a realistic assumption in conventional recording on continuous media , it is questionable in the context of certain advanced recording technologies , such as bit - patterned media ( bpm ) recording , that may be required to achieve higher storage densities . in this paper , we will examine some of the underlying causes of errors in the recording process , particularly in bpm recording , and then propose a new probabilistic write channel model that captures some of the data dependence of these write errors .",
    "thus , the input to this write channel model is the data sequence to be recorded and the channel output is the `` noisy '' sequence that actually gets stored on the medium .",
    "this leads to a description of the full data recording and readback process as a cascade of an imperfect write channel and a noisy ( partial response ) readback channel .",
    "@xcite proposed a model for the bpm recording channel in which the write channel was a binary symmetric channel ( bsc ) , and the readback channel was a linear , intersymbol - interference ( isi ) channel with additive noise .",
    "they proposed and evaluated detection methods for this channel , and they investigate achievable information rates for such a system . in @xcite , we considered an idealized cascaded channel model in which the write channel was again a bsc and the readback channel was a memoryless , binary - input , additive white gaussian noise ( awgn ) channel .",
    "we studied theoretical properties of this channel , as well as the decodable regions of ldpc codes under a number of decoding algorthms . in this paper , we focus on a new write channel model , and we determine and compare bounds on several relevant information - theoretic limits : capacity , symmetric information rate ( sir ) , information rate with first - order markov inputs ( markov-@xmath0 rate ) , and zero - error capacity .    the remainder of the paper is organized as follows . in section [ sec_bpmr ] , we start with a brief description of the write process in bit - patterned media recording , highlighting the main factors leading to write errors .",
    "we review some useful mathematical notation in section [ ssec_not ] , and then present the new probabilistic , data - dependent write channel model in section [ ssec_model ] .",
    "we discuss the types of write errors that can occur  including substitution - like errors and insertion - deletion errors  and explain their connection to the channel state process that underlies the model .",
    "two classes of channel state processes are introduced : a bernoulli state process ( section [ ssec_suberr ] ) and a binary markov state process ( section [ ssec_iderr ] ) . in section [ ssec_tdmr ]",
    "we make the observation that , although proposed as a model for bpm recording , the markov state channel model is also relevant to high - density magnetic recording using conventional granular media . in section [ sec_suberr ]",
    "we give bounds on the capacity , the sir , and the markov-@xmath0 rate of the bernoulli state channel .",
    "section [ sec_insdel ] gives similar bounds on the capacity for the binary markov state channel .",
    "the sir is numerically computed for both of the channels considered in section [ sec_chnmod ] . in section [ sec_kid ] , we introduce a generalization of binary markov state channels , namely the @xmath1-ary markov state channels . for one such generalized channel",
    ", we numerically estimate the sir and derive bounds on the channel capacity .",
    "finally , in section [ sec_zec ] , we explore the zero - error capacity of the proposed class of write channel models . section [ sec_conc ] provides concluding remarks .",
    "a conventional magnetic recording medium is a continuous film of magnetic grains that coats the surface of the disk substrate .",
    "each grain is an atomic magnetic unit that assumes one of two possible magnetic states .",
    "a group of grains together form a bit - cell , an entity that stores one bit of information . therefore , as the areal information density is increased , the number of grains forming a bit - cell reduces .",
    "one of the problems with high - density magnetic recording in conventional media is the super - paramagnetic effect , wherein the magnetic states of individual grains change due to the influence of neighbouring grains or due to changes in temperature . when the areal information density is increased to point where there are only a few grains per bit , such uncontrolled changes in the magnetic states of grains are detrimental to reliable information storage .",
    "bit - patterned media recording ( bpmr ) proposes to get around this problem by making use of patterned magnetic islands separated by non - magnetic material @xcite .",
    "however , this new structure of the magnetic medium introduces technical challenges not seen in recording on conventional media .",
    "an immediate requirement of this media structure is near - perfect synchronization of the write process to ensure that the write head is positioned correctly over the magnetic islands on this disk , i.e. , to ensure that the head is positioned within the so - called writing window zone of the islands @xcite . assuming that this write synchronization is achieved through the use of timing synchronization patterns",
    ", there is a possiblity of frequency and/or phase mismatch leading to incorrect writing .",
    "furthermore , even without a timing mismatch , imperfections in the configuration of the patterned magnetic islands may cause writing errors .",
    "finally , as in conventional magnetic recording , the switching field distribution of magnetic grains may contribute to errors in the write process .",
    "we will refer to write errors induced by any of these mechanisms as written - in errors",
    ".    another important feature of bpmr is the geometry of the writing process .",
    "along the down - track direction , the span of influence of the magnetic write field is typically larger than the spacing between the islands .",
    "therefore , at any given time , the write head influences multiple adjacent islands .",
    "so , in the process of recording a bit on a specified island , the bit value is also recorded on a certain number of subsequent islands , with these islands themselves being overwritten in the future by subsequent bits .",
    "[ fig_bpm ] gives an illustration of this idealized write process .    .",
    "depicted here are snapshots of the top view of the disk and write mechanism .",
    "initially , all islands , represented here using gray squares , have unknown magnetic states represented by @xmath2 , which could be @xmath3 or @xmath0 . at the first time",
    "instant , the write head , shown here using a dashed rectangle , is positioned over the first four islands . in this position ,",
    "the first island is written with the data bit @xmath3 .",
    "the three subsequent islands are also written with the first data bit @xmath3 , before being overwritten with their own data .",
    "after each bit is written , the head is moved over to the next island along the down - track direction . at each position of the write head , the corresponding `` current '' island",
    "is highlighted with a black square boundary . ]",
    "we refer to the number of subsequent islands influenced by the write head as the _ writing span _",
    ", @xmath4 .",
    "we will assume throughout this paper that @xmath5 , which implies that write process has memory and , as a consequence , the write channel is data - dependent .",
    "we will denote random variables using capital letters @xmath6 , @xmath7 , @xmath8 ; random vectors as @xmath9 when @xmath10 , a null variable when @xmath11 or when @xmath12 is non - positive .",
    "infinite - dimensional random vectors are considered to be discrete - time random processes and are denoted using calligraphic letters @xmath13 , @xmath14 , @xmath15 .",
    "the alphabets over which random processes are defined are denoted by @xmath16 , @xmath17 , @xmath18 .",
    "unless otherwise stated , we will denote the channel input variable , process and alphabet using the letters @xmath6 , @xmath13 and @xmath16 respectively .",
    "similarly , @xmath7 , @xmath14 , @xmath17 will correspond to the channel output , and @xmath8 , @xmath15 , @xmath18 to the channel state ( @xmath18 is not to be confused with the set of integers @xmath19 ) . for a real number @xmath20 $ ] , we define @xmath21 .",
    "we assume that the input , output and the channel state alphabet is the binary set @xmath22 .",
    "the write channel model can be written as @xmath23 where the state process @xmath15 is independent of the inputs and the outputs , and @xmath24 and @xmath25 represent addition and multiplication in @xmath26 , respectively .",
    "the channel state @xmath27 is not to be confused with the magnetic state of the island .",
    "it is natural to assume that the channel input and output alphabets are binary since the ( intended and actual ) magnetic states of the islands can be one of two possible states .",
    "the channel state @xmath27 is a random variable that represents a failure in writing due to one of the conditions mentioned in the previous section , i.e. , when @xmath28 the write head can be assumed to have failed in writing the intended bit .",
    "however , this does not necessarily imply a written - in error because there would be no error if the bit to be written is the same as the existing magnetic state of the island .",
    "this is captured by the term @xmath29 that is multiplied with @xmath27 in .",
    "this `` noise '' term @xmath30 is justified because we assume that the timing mismatch or the irregularity of island patterns can cause the write head to be positioned , in the worst case , on the island immediately following the correct island .",
    "we will continue with this assumption until section [ sec_kid ] , where we construct a more generial write channel model . also note that when the first bit is written late , i.e. when @xmath31 , we have @xmath32 which represents the pre - existing magnetic state of the first island .",
    "we will assume that @xmath33 is equally likely to be a @xmath3 or a @xmath0 .",
    "similarly , when the last bit is written late , the last island has a bit in error if the last and the penultimate bits are different . in either case , @xmath34 magnetic islands are read and their contents are interpreted as the @xmath34 data bits so that there is no blocklength mismatch .",
    "based on the @xmath27 sequence , the channel in appears to produce different types of errors . when the @xmath27 sequence consists of isolated ones , the channel appears to produce substitution errors",
    "this is illustrated in the fig .",
    "[ fig_subeg ] .     gives the channel state for each magnetic island .",
    "in this example , @xmath28 for the third and the sixth islands .",
    "the sequence @xmath35 gives the intended magnetization of the islands , i.e. , the data to be written .",
    "taking into account the channel states @xmath27 , the sequence @xmath36 shows the resulting island magnetizations . note that substitution - like errors occur only when @xmath28 , as was the case with the sixth island here , highlighted with a box around the island in the @xmath36 sequence",
    "however , not every @xmath28 results in an error , as illustrated by the third island . ]    such substitution - like errors can occur when the write head misses islands at random and independently of its success in writing on previous islands . noting that when @xmath37 , @xmath38 so that the output reproduces the input exactly , and when @xmath28 , @xmath39 so that the output reproduces the input with a delay of one time instant , we can see that another way to write the relation in is @xmath40 now , when the @xmath27 sequence consists of long runs of ones , the channel appears to produce paired insertion - deletion errors , with insertions accompanying @xmath41 channel state transitions and deletions accompanying @xmath42 channel state transitions , as shown in fig .",
    "[ fig_ideg ] .",
    ", the channel inputs are transformed as shown by the arrows between the @xmath35 and @xmath36 sequences with the resulting magnetic states of the islands shown in the sequence @xmath36 .",
    "the inserted bit in the @xmath36 sequence accompanying the channel state transition @xmath43 is shown with a box around the corresponding island .",
    "the deleted bit from the @xmath35 sequence accompanying the channel state transition @xmath44 is shown with a dotted box around the island . ]",
    "these insertion - deletion errors can happen as a result of timing synchronization errors , wherein there is a frequency mismatch between the islands and the write head ; or as a result of a group of islands being separated farther than usual or having larger switching fields .",
    "the channel in and is completely defined by specifying the statistics of the @xmath15 process . we will consider the channel under two different statistical assumptions on the @xmath15 process in the following , and show how this difference in statistics changes the typical behaviour of the channel .      when the channel state process @xmath15 is an i.i.d .",
    "bernoulli@xmath45 process , i.e. , @xmath46 , we will call the channel the bernoulli state channel . in this case , the channel is completely specified by the parameter @xmath47 .",
    "we shall henceforth denote this channel by @xmath48 .",
    "typically , for small values of the parameter @xmath47 we can expect this channel to produce errors resembling substitution errors ( see fig .",
    "[ fig_subeg ] ) .",
    "when the channel state process @xmath15 is a first - order binary markov process denotes the alphabet size over which the process is defined and the subscript denotes the memory in the process .",
    "the two arguments give the transition probabilities between the two states . ]",
    "@xmath49 , i.e. , @xmath50 , @xmath51 and @xmath52 , we will refer to the channel as the binary markov state channel and denote it by @xmath53 . as noted earlier",
    ", we can expect such a channel to typically produce paired insertion - deletion errors ( see fig .",
    "[ fig_ideg ] ) .",
    "hence , the parameters @xmath54 and @xmath55 can be thought of as insertion and deletion probabilities , respectively , of the channel .",
    "we note here that this channel differs from the most general insertion - deletion channel in two ways .",
    "first , the inserted bit is always the same as the last written bit . whereas this seems to be a serious limitation of the model in comparison with the general insertion channel model , it is to be noted that in most practical insertion - deletion channels  channels with synchronization errors , or sticky channels @xcite  the inserted bit is usually the one",
    "last written ( or transmitted ) , rather than being a random bit .",
    "second , the insertions and deletions are paired so that one never sees two consecutive insertions or deletions . while this seems to limit the scope of the channel , a straightforward extension to channels with a finite maximum number of consecutive insertions or deletions is possible .",
    "however , the estimation of the channel characteristics becomes more complex as the maximum number of consecutive insertions increases ( see section [ sec_kid ] ) .",
    "nevertheless , the binary markov state channel gives us good insight into more general insertion - deletion channels .",
    "moreover , unlike much of the previous work in the literature wherein channels cause only insertion @xcite errors or only deletion @xcite errors , this model considers both insertions and deletions in the same setting . among works that handle both insertions and deletions ,",
    "the set up in this paper resembles the duplication - deletion channel of @xcite more than it does the generic insertion - deletion channel of @xcite .",
    "however , the channel model considered here differs from those in the aforementioned papers in that the insertion - deletion process has memory , i.e. the insertions and deletions are not i.i.d .      the channel model in can also be used to describe high - density magnetic recording on conventional granular magnetic media@xcite .",
    "although media granularity is typically considered as a two - dimensional phenomenon , we consider granularity only in one - dimension  along the down - track direction , and assume adjacent tracks to be independent .",
    "this simplification allows us to establish lower bounds for performance over the two - dimensional channel as proposed in @xcite .",
    "media granularity in one - dimension results in written - in errors as follows . at storage densities of the order of @xmath0 bit per grain , the variation in grain size plays an important role in deciding the reliability of data storage . in this case ,",
    "bit - cells are at most as large as individual grains .",
    "we assume that the grains are all @xmath0 or @xmath56 bit - cells in size , and that the magnetic state of each grain is decided by the last bit written on them .",
    "this is depicted in fig .",
    "[ fig_tdmr ] .",
    "s in the first row , corresponding to the magnetic states read from these cells , which could be @xmath3 s or @xmath0 s . also shown in shades of gray",
    "are the magnetic grains : grains that comprise one bit - cell are shown in light gray and those that comprise two bit - cells are in a darker shade of gray .",
    "the two bit - cells comprising a grain of size @xmath56 always have the same magnetic state . as in the case of bpmr , the write head spans multiple bit - cells , as shown by the dashed rectangles . the `` current '' bit - cell at each time instant is shown with a black square .",
    "]    grains that are @xmath56 bit - cells large will result in written - in errors if the two bits written on them are different . fig .",
    "[ fig_tdmeg ] gives an illustration of the written - in errors in this scenario .",
    "process represents the grain pattern with @xmath27 being @xmath0 when the corresponding bit - cell is the first bit - cell in a grain that comprises two bit - cells .",
    "the @xmath35 sequence shows the data to be written .",
    "the arrows between the @xmath35 and @xmath36 sequences show the transformation of information according to the grain pattern , resulting in the sequence @xmath36 being stored .",
    "the written - in errors are shown with a box around the corresponding bit - cell . ]    comparing figures [ fig_ideg ] and [ fig_tdmeg ] , it is easy to see that the channel model in this case can be written as @xmath57 using a simple time - reversal argument , it can be seen that this is exactly the channel in .",
    "however , in this case , the @xmath27 sequence can not have two consecutive ones , i.e. , it satisfies the @xmath58 run - length constraint ( * ? ? ?",
    "4 ) . we choose to model this as the @xmath59 channel since any realization of the @xmath60 process meets the run - length requirement . in @xcite ,",
    "the authors consider this channel and derive upper bounds for the achievable rates over the channel .",
    "our focus here will be on obtaining bounds on the achievable information rates for the general channel in in the context of the two channel state processes defined in section [ ssec_model ] .",
    "the channel space for the bernoulli state channel defined in the previous section is parameterized by @xmath61 $ ] .",
    "since the channel has memory , its capacity is given as @xmath62 all the entropy and mutual information terms calculated are in bits , so that the capacity is always measured in bits per channel use .",
    "we will denote by @xmath63 the capacity of the bernoulli state channel with parameter @xmath47 .",
    "[ prop_chnsym ] @xmath64 .",
    "let us denote by @xmath65 the mutual information between the vectors @xmath66 and @xmath67 when the channel parameter is @xmath47 .",
    "note that the channel output can be simultaneously written as @xmath68 and @xmath69 where @xmath70 and @xmath71 .",
    "thus @xmath72 for all but a vanishing fraction of indices @xmath73 , as @xmath74 .",
    "therefore , @xmath75 @xmath76 the channel space can therefore be reduced to the interval @xmath77 $ ] . further note that the same symmetry argument holds for not just the rate - maximizing input distribution , but for all input distributions .",
    "the capacity of the bernoulli state channel is upper bounded by the achievable rate for a genie - aided decoder , i.e. , one with the @xmath15 process realization known . given the realization of the @xmath15 process , the inserted bits and the positions of the deleted bits are known so that the bernoulli state channel is equivalent to a correlated erasure channel with average erasure rate @xmath78 .",
    "the resulting erasure channel is a correlated channel since , erasures being dependent on @xmath42 transitions in the @xmath15 process , two consecutive bits can not be erased .",
    "therefore , @xmath79 since the capacity of a correlated erasure channel is the same as that of a memoryless erasure channel with the same erasure probability .",
    "we call this upper bound , @xmath80 , the genie - erasure capacity of the channel @xmath48 .    consider @xmath81 where @xmath82 is the binary entropy function @xcite .",
    "the equality labelled @xmath83 follows from the definition of @xmath36 in and the fact that @xmath46 .",
    "since the information - rate - maximizing input distribution is unknown , we will now derive lower bounds to the capacity by making certain assumptions about the statistics of the input process @xmath13 .",
    "we will first assume that the input process @xmath13 is an i.i.d .",
    "@xmath84 process . with this assumption , the maximum achievable information rate , called the i.i.d .",
    "capacity , denoted @xmath85 , gives a lower bound to the capacity .",
    "[ prop_iidipsym ] @xmath86 .",
    "let @xmath66 and @xmath67 be the input and output respectively of the channel @xmath48 .",
    "define @xmath87 and @xmath88 .",
    "when @xmath89 , @xmath90 .",
    "further since @xmath91 and @xmath92 are bijections , we have @xmath93 clearly , @xmath94 and @xmath95 also satisfy the relation in and consequently @xmath96 from this and the fact that @xmath97 we have the desired result .    as a consequence of propositions [ prop_chnsym ] and [ prop_iidipsym ]",
    ", we have @xmath98    from proposition [ prop_iidipsym ] and the fact that @xmath85 is concave in @xmath99 , we immediately have the following .    [ cor_sir ] @xmath100}c_{iid}(\\alpha , { { \\mathsf}{p } } ) = c_{iid}(\\frac{1}{2 } , { { \\mathsf}{p}}){\\ } \\forall{\\ } { { \\mathsf}{p}}\\in [ 0 , 1].\\ ] ]    when the binary input is i.i.d . with a uniform distribution ( sometimes abbreviated as @xmath101 ) , the corresponding rate",
    "is called the symmetric information rate ( sir ) and is denoted @xmath102 .",
    "the sir is of interest because it can be achieved by a random linear coset code @xcite .",
    "we have from @xmath103    we can lower bound the sir by disregarding the data - dependence of the noise in the channel .",
    "this gives a channel equivalent to a bsc with crossover probability @xmath104 so that @xmath105 further lower bounds can be obtained by conditioning the entropy of the output as follows : @xmath106 where we have used the fact that @xmath36 depends only on @xmath107 and @xmath35 , and given @xmath107 , @xmath36 is independent of @xmath108 . continuing as above",
    ", we can obtain a tighter lower bound @xmath109 a straightforward upper bound for the sir , implied by is @xmath110 which follows because the entropy rate for a binary process @xmath111 . note that this bound is achieved when @xmath14 is the i.i.d .",
    "@xmath112 process .",
    "again , starting from , we can obtain upper bounds for the sir by removing conditioning from the entropy of the output , as shown below : @xmath113 as with the lower bounds , we can find a tighter upper bound for the entropy rate @xmath114 as follows @xmath115    fig .",
    "[ fig_capbounds ] plots the lower and the upper bounds for sir discussed above .",
    "note that the tighter lower and upper bounds  @xmath116 in and @xmath117 in  almost coincide for @xmath118 ( and from symmetry , for @xmath119 ) . in this range , therefore , where the bounds themselves are greater than 0.5 , they approximate the sir fairly accurately . the process @xmath14 is ergodic , as all channel states can be reached within a finite number of steps at any time with strictly positive probability @xcite , and it converges to a stationary process . as a result , from the asymptotic equipartition property @xcite , we have the entropy rate @xmath120 which can be numerically evaluated through a forward pass of the bcjr algorithm @xcite , @xcite . by using long enough sequences",
    "@xmath66 and @xmath67 in the computation , the sir can be obtained with an accuracy of @xmath121 .",
    "this is shown as the `` sir '' curve in fig .",
    "[ fig_capbounds ] , from which we conclude that the upper bound @xmath117 in is a good approximation for the sir .      to explore the loss in the achievable rate due to independent uniformly distributed input",
    ", we let the source have memory .",
    "we consider a first - order binary markov input process @xmath13 . taking a cue from the input symmetry of the bernoulli state channel",
    ", we consider a symmetric binary markov process @xmath13 with @xmath122 .",
    "we denote this by @xmath123 , meaning that @xmath13 is a binary ( alphabet of size @xmath56 ) markov source with memory @xmath0 and transition parameter @xmath124 .",
    "starting from , we can arrive at lower and upper bounds for the _ symmetric markov-@xmath0 rate _ ( m1r ) , @xmath125 , which we define as the maximum rate of information transfer when @xmath123 .",
    "the lower bounds analogous to those for the sir are @xmath126 which is the same as @xmath127 in , @xmath128 and @xmath129 the trivial upper bound analogous to @xmath130 in is @xmath131 the upper bounds corresponding to @xmath132 and @xmath133 are @xmath134 and @xmath135 fig .",
    "[ fig_bsscap ] shows the contours of the bounds for @xmath136 in and .",
    "( only the tighter bounds , @xmath137 and @xmath138 are shown . )        as was the case for i.u.d .",
    "input , the tighter bounds and have almost coinciding contours for a wide range of parameters @xmath139 . unlike the case of i.i.d .",
    "inputs , the rate - maximizing input parameter @xmath140 is not easily obtained .",
    "a close estimate can be obtained by maximizing the bounds obtained above .",
    "these are shown ( dotted lines ) in fig .",
    "[ fig_bsscap ] . since the optimal @xmath124 values for the tighter lower and upper bounds @xmath137 and @xmath138 almost coincide",
    ", we can say that @xmath140 is monotonically decreasing in @xmath47 in the interval @xmath141 $ ] with @xmath142 and @xmath143 .    fig .",
    "[ fig_iidbsscap ] compares the sir ( solid line representing @xmath116 in , and dashed line representing @xmath117 in ) and the m1r ( solid line for @xmath144 in , dashed line for @xmath145 in ) over the range of @xmath47 values .",
    "it is clear that considerable gains in reliable information transfer rate are possible by using an input with memory . in particular , note that whereas there is a range of @xmath47 values for which the sir is smaller than @xmath146 , the m1r is strictly larger than @xmath147 .",
    "it is clear that for a sequence of input markov processes of increasing orders , the achievable rates are non - decreasing .",
    "the algorithm suggested in @xcite can be employed to optimize the input markov process to maximize the rate .",
    "the channel space for the binary markov state channel defined in section [ ssec_iderr ] is @xmath148 ^ 2 $ ] .",
    "note that the channel space is ordered , i.e. , the first parameter is the @xmath41 transition ( insertion ) probability and the second the @xmath42 transition ( deletion ) probability . as in the case of the bernoulli state channel ,",
    "the capacity of the binary markov state channel , denoted @xmath149 , is given as @xmath150 we shall assume that the channel has converged to the stationary distribution .",
    "this means that when @xmath54 ( resp . , @xmath55 )",
    "is zero , the channel is the noise - free channel ( resp . , noise - free channel with a delay ) and hence @xmath151 ( resp . , @xmath152 ) .",
    "we first establish the following symmetry property of the binary markov state channel .",
    "[ prop_1idsym ] @xmath153 .",
    "we know that if @xmath154 is a markov process , then so is @xmath155 @xcite .",
    "furthermore , since the channel is assumed to have converged to the stationary distribution , the conditional distributions @xmath156 and @xmath157 are identical . however , note that whereas a transition @xmath158 is an insertion , the transition @xmath159 is a deletion .",
    "this implies that @xmath160 , where @xmath70 , for all but a vanishing fraction of indices @xmath161 , as @xmath74 .",
    "therefore , @xmath162 the channel space can therefore be reduced to the region @xmath163 , { { \\mathsf}{p}}_{{\\mathsf}{d}}\\in [ 0 , { { \\mathsf}{p}}_{{\\mathsf}{i}}]$ ] . as was the case for the @xmath48 channel , we have the same symmetry for any fixed input distribution .    as a consequence of proposition  [ prop_1idsym ]",
    ", we can assume an unordered pair @xmath164 parameterizing the channel space .",
    "as for the bernoulli state channel , we can define the genie - erasure capacity @xmath165 of the binary markov state channel @xmath53 . in this case , the resulting channel is a correlated erasure channel with an average erasure probability @xmath166 , so that @xmath167     for the @xmath53 channel . ]    as a result of the memory in the @xmath15 process , it is considerably harder than it was for the @xmath48 channel to obtain closed - form expressions for lower bounds on the capacity of @xmath53 by computing information rates for known input distributions .",
    "however , the @xmath53 channel is still an ergodic channel , and the sir @xmath168 can be obtained numerically .",
    "[ fig_siraid ] shows the contours of the sir for the @xmath53 channel .",
    "note that the symmetry proved in proposition [ prop_1idsym ] is evident .     and the genie - erasure capacity @xmath169 for the symmetric binary markov state channel @xmath170 . ]    in fig .",
    "[ fig_insdelsir ] , we show the sir for the case @xmath171 , which we call the symmetric binary markov state channel @xmath170 .",
    "the values of the sir when @xmath172 and @xmath173 are easily explained . when @xmath172 , the channel is noiseless .",
    "when @xmath174 , the channel deterministically flips between the identity and the delayed channel so that every odd bit is repeated twice , and every even bit is lost , and the maximum achievable information transfer rate is @xmath175 bit per channel use .",
    "also shown is the genie - erasure capacity in , which in this case becomes , @xmath176 interestingly enough , when @xmath174 , the sir satisfies @xmath177 , so that @xmath178 .",
    "we also include in fig .",
    "[ fig_oneinfsir ] the sir obtained for the channel @xmath59 , as well as the corresponding genie - erasure capacity , @xmath179 .     and the genie - erasure capacity @xmath180 for the channel @xmath59 with the @xmath15 process satisfying the @xmath58 constraint . ]",
    "we now consider a generalization of the binary markov state channel as described in . here , we allow @xmath181 and let @xmath15 be a first - order markov process whose transition probabilities satisfy @xmath182 for @xmath183 .",
    "further , we have @xmath184 and @xmath185 we denote this by @xmath186 .",
    "note that when @xmath187 , this model gives the binary markov state channel considered earlier",
    ". we will hence be interested in the @xmath1-ary markov state channel where @xmath188 , which we denote by @xmath189 .",
    "note that this channel now generalizes the binary markov state channel in the sense that it allows up to @xmath190 consecutive insertions or deletions .",
    "we further assume that the parameters @xmath191 are chosen such that the process @xmath15 is aperiodic and irreducible so that @xmath14 is ergodic .",
    "the channel space is given by @xmath163 , { { \\mathsf}{p}}_{{\\mathsf}{d}}\\in [ 0 , 1 - { { \\mathsf}{p}}_{{\\mathsf}{i}}]$ ] . the channel symmetry argument of proposition [ prop_1idsym ] holds in this case also , so that @xmath192 where @xmath193 is the capacity of @xmath1-ary markov state channel @xmath189 .",
    "hence , the channel space can be further reduced to @xmath163 , { { \\mathsf}{p}}_{{\\mathsf}{d}}\\in [ 0 , \\min\\{{{\\mathsf}{p}}_{{\\mathsf}{i } } , 1 - { { \\mathsf}{p}}_{{\\mathsf}{i}}\\}]$ ] . as was the case for the binary markov state channel , we have @xmath194 $ ] .",
    "the genie - erasure capacity of the @xmath189 channel is given by the capacity of a correlated erasure channel with an average erasure rate @xmath195 where @xmath196 is the steady state probability of @xmath197 , so that @xmath198 for the symmetric @xmath1-ary markov state channel , we have @xmath199 for @xmath200 $ ] because in this case , @xmath201 .",
    "note that @xmath202 reduces to the capacity of a bec with erasure probability @xmath203 , @xmath204 , when @xmath205 .",
    "the sir of the @xmath189 channel can be obtained numerically as in the case of the @xmath53 channel .",
    "however , the computational complexity of the bcjr algorithm @xcite used to estimate the sir increases roughly exponentially in the size @xmath1 of the alphabet of the process @xmath15 , and hence the evaluation of the sir for @xmath206 may require considerable computing resources .",
    "we carried out the calculation for the case when @xmath207 , and the estimated sir is shown in fig .",
    "[ fig_2insdelsir ] .     for the symmetric @xmath208-ary markov state channel , along with the genie - erasure capacity @xmath209 . also shown is the lower bound on the zero - error capacity @xmath210 from corollary [ cor_zeck ] . ]",
    "the zero - error capacity @xmath211 of a discrete - memoryless channel was introduced by shannon @xcite , from which it is readily seen that a noisy discrete - output binary - input memoryless channel has a zero - error capacity of @xmath3 , i.e. , no information can be transmitted over such a channel with zero error .",
    "however , we will now show that the zero - error capacity of the noisy discrete - output binary - input channel in is strictly positive .",
    "we will denote the generic channel in for all binary @xmath15 processes by @xmath212 .",
    "[ prop_zec ] @xmath213 .",
    "consider the @xmath56-repetition code .",
    "the channel input process @xmath13 and the message process @xmath214 satisfy the relationship @xmath215 for @xmath216 . since @xmath217 , @xmath218 so that discarding the @xmath219s gives us @xmath214 exactly , thereby achieving zero error at a rate @xmath175 . thus , @xmath220 .",
    "it is obvious that the capacity of a channel is an upper bound on the zero - error capacity .",
    "from , we can see that @xmath221 , and the coding scheme above achieves a rate @xmath146 for any realization of @xmath15 .",
    "thus , @xmath178 . therefore , as long as the channel admits the infinitely long alternating sequence @xmath222 , we have @xmath213 .",
    "note that both the bernoulli state channel @xmath48 and the binary markov state channel @xmath53 can generate the infinite sequence @xmath222 , albeit with vanishing probability .",
    "it is also worth noting here that this infinite sequence satisfies the @xmath58 constraint ( see section [ ssec_tdmr ] ) .",
    "thus , unlike binary - input discrete memoryless channels , the binary - input discrete channel in allows a non - zero information rate even under the severe requirement of zero - error . from this result and fig .",
    "[ fig_iidbsscap ] , it is clear that even under the milder condition of asymptotically vanishing error probability , random linear coset coding achieves a lower rate than the @xmath56-repetition code , which guarantees zero errors , for the bernoulli state channel over a range of @xmath47 values .",
    "referring to fig .",
    "[ fig_siraid ] , the same can be said for the binary markov state channels .",
    "however , by using a first - order markov input ( cf .",
    "[ fig_iidbsscap ] ) , higher rates than that of the zero - error achieving scheme are achievable for all bernoulli state channels , although only with asymptotically vanishing error probability .",
    "the zero - error capacity of the @xmath189 channel , denoted by @xmath223 , satisfies the following bounds .",
    "[ cor_zeck ] @xmath224 .    repeating every bit @xmath1 times achieves zero error , since every @xmath225 bit is always correct for any realization of the @xmath15 process .",
    "hence , @xmath226 .",
    "the smallest upper bound for the capacity is also an upper bound for the zero - error capacity .",
    "thus , @xmath227 observe that when @xmath187 , the minimum genie - erasure capacity occurs at @xmath228 and at this point the upper bound @xmath229 is same as the lower bound @xmath230 which was used in the proof of proposition [ prop_zec ] .",
    "note that the bounds proposed above are loose asymptotically , i.e. , @xmath231 and @xmath232 as @xmath205 .",
    "we proposed a new write channel model for bit - patterned media recording that reflects the data dependence of write synchronization errors .",
    "this model generates both substitution - like errors and insertion - deletion errors whose statistics are determined by an underlying channel state process . for bernoulli and markov channel state models , we studied information - theoretic limits imposed by the channel , computing bounds and numerical estimates for the maximum achievable information rate under different assumptions on the channel input statistics .",
    "we then generalized the markov channel state model to allow a channel state space of size @xmath206 and computed the sir numerically for the case @xmath233 .",
    "finally , we showed that the rate-@xmath175 2-repetition code achieves the zero - error capacity over the new write channel when the channel state space is binary .",
    "bounds on the zero - error capacity of the general @xmath1-ary markov channel state model were also presented . in future work , we plan to combine the new write channel model in cascade with the partial - response readback channel model and investigate the achievable rates , as was done for the i.i.d .",
    "insertion - deletion channel in @xcite .",
    "furthermore , the channel model considered here can be readily represented using a factor graph and hence the construction of sparse graph - based codes with iterative decoding techniques seems possible .",
    "a.  r. iyengar , p.  h. siegel , and j.  k. wolf , `` ldpc codes for the cascaded bsc - bawgn channel , '' in _ proc .",
    "47th annual allerton conf . on communication , control and computing _ , sep .",
    "30 - oct . 2 , 2009 , pp . 620627 .",
    "b.  livshitz , a.  inomata , h.  bertram , and v.  lomakin , `` semi - analytical approach for analysis of ber in conventional and staggered bit patterned media , '' _ ieee trans .",
    "_ , vol .  45 , no .  10 , pp .",
    "35193522 , oct .",
    "2009 .",
    "j.  hu , t.  duman , m.  erden , and a.  kavcic , `` achievable information rates for channels with insertions , deletions , and intersymbol interference with i.i.d .",
    "inputs , '' _ ieee trans .",
    "_ , vol .",
    "58 , no .  4 , pp .",
    "1102 1111 , apr . 2010 .    r.  wood , m.  williams , a.  kavcic , and j.  miles , `` the feasibility of magnetic recording at 10 terabits per square inch on conventional media , '' _ ieee trans . magn . _ , vol .  45 , no .  2 , pp .",
    "917923 , feb .",
    "2009 .",
    "p.  vontobel , a.  kavcic , d.  arnold , and h .- a .",
    "loeliger , `` a generalization of the blahut - arimoto algorithm to finite - state channels , '' _ ieee trans .",
    "inf . theory _ ,",
    "54 , no .  5 , pp .",
    "1887 1918 , may 2008 .",
    "j.  b. soriaga , h.  d. pfister , and p.  h. siegel , `` determining and approaching achievable rates of binary intersymbol interference channels using multistage decoding , '' _ ieee trans .",
    "inf . theory _",
    "53 , no .  4 , pp . 14161429 , apr . 2007 ."
  ],
  "abstract_text": [
    "<S> we propose a new write channel model for bit - patterned media recording that reflects the data dependence of write synchronization errors . </S>",
    "<S> it is shown that this model accommodates both substitution - like errors and insertion - deletion errors whose statistics are determined by an underlying channel state process . </S>",
    "<S> we study information theoretic properties of the write channel model , including the capacity , symmetric information rate , markov-@xmath0 rate and the zero - error capacity .    </S>",
    "<S> bit - patterned media , high - density magnetic recording , channel capacity , symmetric information rate , markov-@xmath0 rate , zero - error capacity . </S>"
  ]
}