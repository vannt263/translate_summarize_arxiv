{
  "article_text": [
    "model selection is an important problem in many areas including machine learning .",
    "if a proper model is not selected , any effort for parameter estimation or prediction of the algorithm s outcome is hopeless .",
    "given a set of candidate models , the goal of model selection is to select the model that best approximates the observed data and captures its underlying regularities .",
    "model selection criteria are defined such that they strike a balance between the _ goodness - of - fit ( gof ) _ , and the _ generalizability _ or _",
    "complexity _ of the models .",
    "goodness - of - fit measures how well a model capture the regularity in the data .",
    "generalizability / complexity is the assessment of the performance of the model on unseen data or how accurately the model fits / predicts the future data .",
    "models with higher complexity than necessary can suffer from overfitting and poor generalization , while models that are too simple will underfit and have low gof  @xcite .",
    "cross - validation  @xcite , bootstrapping  @xcite , akaike information criterion ( aic )  @xcite , and bayesian information criterion ( bic )  @xcite , are well known examples of traditional model selection . in re - sampling methods such as cross - validation and bootstraping , the generalization error of the model",
    "is estimated using monte carlo simulation .",
    "in contrast with re - sampling methods , the model selection methods like aic and bic do not require validation to compute the model error , and are computationally efficient . in these procedures",
    "an _ information criterion _ is defined such that the generalization error is estimated by penalizing the model s error on observed data .",
    "a large number of information criteria have been introduced with different motivations that lead to different theoretical properties .",
    "for instance , the tighter penalization parameter in bic favors simpler models , while aic works better when the dataset has a very large sample size .",
    "kernel methods are strong , computationally efficient analytical tools that are capable of working on high dimensional data with arbitrarily complex structure .",
    "they have been successfully applied in wide range of applications such as classification , and regression . in kernel methods ,",
    "the data are mapped from their original space to a higher dimensional feature space , the reproducing kernel hilbert space ( rkhs ) .",
    "the idea behind this mapping is to transform the nonlinear relationships between data points in the original space into an easy - to - compute linear learning problem in the feature space .",
    "for example , in kernel regression the response variable is described as a linear combination of the embedded data .",
    "any algorithm that can be represented through dot products has a kernel evaluation .",
    "this operation , called kernelization , makes it possible to transform traditional , already proven , model selection methods into stronger , corresponding kernel methods .",
    "the literature on kernel methods has , however , mostly focused on kernel selection and on tuning the kernel parameters , but only limited work being done on kernel - based model selection  @xcite . in this study , we investigate a kernel - based information criterion for ridge regression models . in kernel ridge regression ( krr ) , tuning the ridge parameters to find the most predictive subspace with respect to the data at hand and the unseen data is the goal of the kernel model selection criterion .    in classical model selection methods the performance of the model selection criterion is evaluated theoretically by providing a consistency proof where the sample size tends to infinity and empirically through simulated studies for finite sample sizes .",
    "other methods investigate a probabilistic upper bound of the generalization error  @xcite .",
    "proving the consistency properties of the model selection in _ kernel model selection _ is challenging .",
    "the proof procedure of the classical methods does not work here .",
    "some reasons for that are : the size of the model to evaluate problems such as under / overfitting  @xcite is not apparent ( for @xmath1 data points of dimension @xmath2 , the kernel is @xmath3 , which is independent of @xmath2 ) and asymptotic probabilities of generalization error or estimators are hard to compute in rkhs .",
    "researchers have kernelized the traditional model selection criteria and shown the success of their kernel model selection empirically .",
    "kobayashi and komaki @xcite extracted the kernel - based regularization information criterion ( kric ) using an eigenvalue equation to set the regularization parameters in kernel logistic regression and support vector machines ( svm ) .",
    "rosipal et al .",
    "@xcite developed covariance information criterion ( cic ) for model selection in kernel principal component analysis , because of its outperformed results compared to aic and bic in orthogonal linear regression .",
    "demyanov et al .",
    "@xcite , provided alternative way of calculating the likelihood function in akaike information criterion ( aic ,  @xcite and bayesian information criterion ( bic ,  @xcite ) , and used it for parameter selection in svms using the gaussian kernel .    as pointed out by van emden  @xcite , a desirable model is the one with the fewest dependent variables . thus defining a complexity term that measures the interdependency of model parameters enables one to select the most desirable model . in this study , we define a novel variable - wise variance and obtain a complexity measure as the additive combination of kernels defined on model parameters . formalizing the complexity term in this way effectively captures the interdependency of each parameter of the model .",
    "we call this novel method _ kernel - based information criterion ( kic)_.    model selection criterion in gaussian process regression ( gpr ;  @xcite ) , and kernel - based information complexity ( icomp ;  @xcite ) resemble kic in using a covariance - based complexity measure .",
    "however , the methods differ because these complexity measures capture the interdependency between the data points rather than the model parameters .",
    "although we can not establish the consistency properties of kic theoretically , we empirically evaluate the efficiency of kic both on synthetic and real datasets obtaining state - of - the - art results compared to leave - one - out - cross - validation ( loocv ) , kernel - based icomp , and maximum log marginal likelihood in gpr .",
    "the paper is organized as follows . in section  [ sec : krr ] , we give an overview of kernel ridge regression .",
    "kic is described in detail in section  [ sec : kic ] .",
    "section  [ sec : om ] is provides a brief explanation of the methods to which kic is compared , and in section  [ sec : exp ] we evaluate the performance of kic through sets of experiments .",
    "in regression analysis , the regression model of the form : @xmath4 where @xmath5 can be either a linear or non - linear function .    in linear regression we have , @xmath6 , where @xmath7 is an observation vector ( response variable ) of size @xmath8 , @xmath9 is a full rank data matrix of independent variables of size @xmath10 , and @xmath11 , is an unknown vector of regression parameters , where @xmath12 denotes the transposition .",
    "we also assume that the error ( noise ) vector @xmath13 is an @xmath1-dimensional vector whose elements are drawn i.i.d , @xmath14 , where @xmath15 is an @xmath1-dimensional identity matrix and @xmath16 is an unknown variance .",
    "the regression coefficients minimize the squared errors , @xmath17 , between estimated function @xmath18 , and target function @xmath5 .",
    "when @xmath19 , the problem is ill - posed , so that some kind of regularization , such as tikhanov regularization ( ridge regression ) is required , and the coefficients minimize the following optimization problem @xmath20 where @xmath21 is the regularization parameter .",
    "the estimated regression coefficients in ridge regression @xmath22 are : @xmath23 in _ kernel _ ridge regression ( krr ) , the data matrix @xmath9 is non - linearly transformed in rkhs using a feature map @xmath24 .",
    "the estimated regression coefficients based on @xmath25 are : @xmath26 where @xmath27 is the kernel matrix .",
    "equation  [ eq : theta ] does not obtain an explicit expression for @xmath28 because of @xmath24 ( the kernel trick enables one to avoid explicitly defining @xmath25 that could be numerically intractable if computed in rkhs , if known ) , thus a ridge estimator is used ( e.g. @xcite ) that excludes @xmath24 : @xmath29 using @xmath30 in the calculation of krr is similar to regularizing the regression function instead of the regression coefficients , where the objective function is : @xmath31 and @xmath32 denotes the relevant rkhs . for @xmath33 , and @xmath34",
    "we have : @xmath35 where @xmath36 is the kernel function , and @xmath37 .",
    "the main contribution of this study is to introduce a new kernel - based information criterion ( kic ) for the model selection in kernel - based regression . according to equation   kic balances between the goodness - of - fit and the complexity of the model .",
    "gof is defined using a log - likelihood - based function ( we maximize penalized log likelihood ) and the complexity measure is a function based on the covariance function of the parameters of the model . in the next subsections we elaborate on these terms .",
    "the definition of van emden @xcite for the complexity measure of a random vector is based on the interactions among random variables in the corresponding covariance matrix .",
    "a desirable model is the one with the fewest dependent variables .",
    "this reduces the information entropy and yields lower complexity . in this paper",
    "we focus on this definition of the complexity measures .    considering a @xmath2-variate normal distribution @xmath38 , the complexity of a covariance matrix , @xmath39 , is given by the shannon s entropy @xcite , @xmath40 where @xmath41 , @xmath42 are the marginal and the joint entropy , and",
    "@xmath43 is the @xmath44 diagonal element of @xmath39 .",
    "@xmath45 if and only if the covariates are independent .",
    "the complexity measure in equation   changes with orthonormal transformations because it is dependent on the coordinates of the random variable vectors @xmath46 @xcite .",
    "to overcome these drawbacks , bozodgan and haughton @xcite introduced icomp information criterion with a complexity measure based on the maximal covariance complexity , which is an upper bound on the complexity measure in equation  : @xmath47 this complexity measure is proportional to the estimated arithmetic ( @xmath48 ) and geometric mean ( @xmath49 ) of the eigenvalues of the covariance matrix .",
    "larger values of @xmath50 , indicates higher dependency between random variables , and vice versa .",
    "zhang @xcite introduced a kernel form of this complexity measure @xmath50 , that is computed on kernel - based covariance of the ridge estimator : @xmath51 the complexity measure in gaussian process regression ( gpr ; @xcite ) is defined as @xmath52 , a concept from the joint entropy @xmath42 ( as shown in equation  [ eq : complexity ] ) .",
    "in contrast to icomp and gpr , the complexity measure in kic is defined using the hilbert - schmidt ( hs ) norm of the covariance matrix , @xmath53 . minimizing this complexity measure obtains a model with more independent variables .    in the next sections ,",
    "we explain in detail how to define the needed variable - wise variance in the complexity measure , and the computation of the complexity measure .",
    "+      in kernel - based model selection methods such as icomp , and gpr , the complexity measure is defined on a covariance matrix that is of size @xmath54 for @xmath9 of size @xmath10 .",
    "the idea behind this measure is to compute the interdependency between the model parameters , which independent of the number of the model parameters @xmath2 . in the other words ,",
    "the concept of the size of the model is hidden because of the definition of a kernel . to have a complexity measure that depends on @xmath2",
    ", we introduce variable - wise variance using an additive combination of kernels for each parameter of the model .",
    "let @xmath55 be the parameter vector of the kernel ridge regression : @xmath56 where @xmath57 and @xmath58 , and @xmath59 the solution of krr is given by @xmath60 .",
    "the quantity @xmath61 = \\sigma^2 \\operatorname{tr}[k(k+\\alpha i)^{-2 } ] $ ] can be interpreted as the sum of variances for the component - wise parameter vectors , if the following sum of component - wise kernel is introduced : @xmath62 where @xmath63 and @xmath64 denote the j - th component of vectors @xmath65 and @xmath66 . with this sum kernel",
    ", the function @xmath67 can be written as : @xmath68 where @xmath69 is a function in @xmath70 , the rkhs defined by @xmath71 .",
    "the parameter @xmath28 in this case is given by @xmath72 where @xmath73 , and thus @xmath69 in equation  [ eq : g ] is equal to @xmath74 .",
    "let @xmath75 be the conditional covariance of @xmath74 or @xmath69 given @xmath76 .",
    "we have @xmath77,\\end{aligned}\\ ] ] where @xmath78 be the gram matrix with @xmath71 . since @xmath79 , we have @xmath80 = \\operatorname{tr}[\\sigma_{\\theta}].\\end{aligned}\\ ] ]    formalizing the complexity term with variable - wise variance effectively captures the interdependency of each parameter of the model ( measures the significance of the contribution by the variables ) explicitly .",
    "+      gretton et al .",
    "@xcite introduced a kernel - based independence measure , namely the hilbert - schmidt independence criterion ( hsic ) , which is explained here .",
    "suppose @xmath81 , and @xmath82 are random vectors with feature maps @xmath83 , and @xmath84 , where @xmath85 , and @xmath86 are rkhss .",
    "the cross - covariance operator corresponding to the joint probability distribution @xmath87 is a linear operator , @xmath88 such that : @xmath89,\\end{aligned}\\ ] ] where @xmath90 denotes the tensor product , @xmath91= e[k(\\cdot , x)]$ ] , and @xmath92=e[k(\\cdot , y)]$ ] , for @xmath93 , and associated kernel function @xmath36 .",
    "the hsic measure for separable rkhs @xmath85 , and @xmath86 is the squared hs - norm of the cross - covariance operator and is denoted as : @xmath94\\end{aligned}\\ ] ] * theorem 1 . *",
    "assume @xmath95 , and @xmath96 are compact , for all @xmath97 , and @xmath98 , @xmath99 , and @xmath100 , @xmath101 if and only if @xmath102 , and @xmath7 are independent ( theorem 4 in @xcite ) . + by computing the hsic on covariance matrix associated with model s parameters",
    "@xmath103 we can measure the independence between the parameters . since @xmath104 is a symmetric positive semi - definite matrix , @xmath105 , and",
    "the trace of the hs norm of the covariance matrix is equal to : @xmath106 = \\sum_{j=1}^p v_j^2\\nonumber\\\\ ~~&= \\sigma^4 \\operatorname{tr}[k(k+\\alpha i)^{-2 } k(k+\\alpha i)^{-2}]\\end{aligned}\\ ] ]      kic is defined as : @xmath107 where @xmath108 is the complexity term based on equation  [ eq : hs ] .",
    "the normalization by @xmath109 obtains a complexity measure that is robust to changes in variance ( similar to icomp criterion ) .",
    "the minimum kic defines the best model . ] .",
    "the penalized log - likelihood ( pll ) in krr for normally distribution data is defined by : @xmath110 the unknown parameters @xmath22 , and @xmath111 are calculated by minimizing the kic objective function . @xmath112 we also investigated the effect of using @xmath113 $ ] , and @xmath61 $ ] as complexity terms .",
    "the empirical results reported in subsection  [ subsec : realdata ] on real datasets , and compared with kic .",
    "we denote these information criteria as :    @xmath114,\\end{aligned}\\ ] ]    @xmath115.\\end{aligned}\\ ] ]    in both kic_1 , and kic_2 , similar to kic , @xmath116 , while because the complexity term is dependent on @xmath16 , @xmath111 for kic_1 is : @xmath117=0.\\end{aligned}\\ ] ] if we denote @xmath118 , @xmath111 is the solution of a quadratic optimization problem , @xmath119 , where @xmath120 . in the case of kic_2 , the @xmath111 is the real root of the following cubic problem : @xmath121 where @xmath122 $ ] .",
    "we compared kic with loocv @xcite , kernel - based icomp @xcite , and maximum log of marginal likelihood in gpr ( abbreviated as gpr ) @xcite to find the optimal ridge regressors .",
    "the reason to compare kic with icomp and gpr is that in all of these methods the complexity measure computes the interdependency of model parameters as a function of covariance matrix in different ways .",
    "loocv is a standard and commonly used methods for model selection .",
    "* loocv : * re - sampling model selection methods like cross - validation is time consuming @xcite . for instance , the leave - one - out - cross - validation ( loocv ) has the computational cost of @xmath123 the number of parameter combinations ( @xmath124 is the processing time of the model selection algorithm @xmath125 ) for @xmath126 training samples . to have cross - validation methods with faster processing time ,",
    "the closed form formula for the risk estimators of the algorithm under special conditions are provided .",
    "we consider the kernel - based closed form of loocv for linear regression introduced by @xcite : @xmath127^{-1}[i - h]y\\|_2 ^ 2}{n}\\end{aligned}\\ ] ] where @xmath128 is the hat matrix .    *",
    "maximizing the log of marginal likelihood ( gpr ) * is a kernel - based regression method . for a given training set @xmath129 , and @xmath130 ,",
    "a multivariate gaussian distribution is defined on any function @xmath5 such that , @xmath131 , where @xmath39 is a kernel .",
    "marginal likelihood is used as the model selection criterion in gpr , since it balances between the lack - of - fit and complexity of a model . maximizing the log of marginal likelihood obtains the optimal parameters for model selection .",
    "the log of marginal likelihood is denoted as : @xmath132 where @xmath133 denotes the model s fit , @xmath134 , denotes the complexity , and @xmath135 is a normalization constant . without loss of generality in this paper gpr means the model selection criterion is used in gpr .",
    "* icomp : * the kernel - based icomp introduced in @xcite is an information criterion to select the models and is defined as @xmath136 , where @xmath50 , and @xmath39 elaborated in equations  [ eq : cicomp ] , and  [ eq : sigmaicomp ] .",
    "in this section we evaluate the performance of kic on synthetic , and real datasets , and compare with competing model selection methods .",
    "kic was first evaluated on the problem of approximating @xmath137 from a set of 100 points sampled at regular intervals in @xmath138 $ ] . to evaluate robustness to noise , normal random noise",
    "was added to the @xmath139 function at two noise - to - signal ( nsr ) ratios : @xmath140 , and @xmath141 .",
    "figure  [ sinc ] shows the sinc function and the perturbed datasets .",
    "the following experiments were conducted : ( 1 ) shows how kic balances between gof and complexity , ( 2 ) shows how kic and mse on training sets change when the sample size and the level of noise in the data change ( 3 ) investigates the effect of using different kernels , and ( 4 ) evaluates the consistency of kic in parameter selection .",
    "all experiments were run 100 times using randomly generated datasets , and corresponding test sets of size 1000 .    * experiment 1 . * the effect of @xmath21 on complexity , lack - of - fit and kic values was measured by setting @xmath142 , with krr models being generated using a gaussian kernel with different standard deviations , @xmath143 , computed over the 100 data points .",
    "the results are shown in figure  [ co_la_kic ] . the model generated with @xmath144 overfits , because it is overly complex , while @xmath145 gives a simpler model that underfits .",
    "as the ridge parameter @xmath21 increases , the model complexity decreases while the goodness - of - fit is adversely affected .",
    "kic balances between these two terms , which yields a criterion to select a model that has good generalization , as well as goodness of fit to the data .",
    "* experiment 2 . * the influence of training sample size was investigated by comparing sample sizes , @xmath1 , of 50 , and 100 , for a total of four sets of experiments : ( @xmath146 ) : ( @xmath147 ) , ( @xmath148 ) , ( @xmath149 ) , ( @xmath150 ) .",
    "the gaussian kernel was used with @xmath151 . the kic value and mean squared error ( mse , @xmath152 ) , for different @xmath153 @xmath154 is shown in figure  [ kic - mse ] .",
    "the data with nsr=@xmath141 has larger mse values , and larger error bars , and consequently larger kic values compared to data with nsr=@xmath140 . in both cases , kic and mse change with similar profiles with respect to @xmath21 .",
    "the noise and the sample size have no effect on kic for selecting the best model ( parameter @xmath21 ) .",
    "* experiment 3 .",
    "* the effect of using a gaussian kernel , @xmath155 , versus the cauchy kernel , @xmath156 , was investigated , where @xmath157 , and @xmath158 in the computation of the kernel - based model selection criteria icomp , kic , gpr , and loocv . the results are reported in figures  [ gaussian kernel ] and  [ cauchy kernel ] .",
    "the graphs show box plots with markers at @xmath159 , and @xmath160 of the empirical distributions of mse values . as expected , the mse of all methods is larger when nsr is high , @xmath161 , and smaller for the larger of the two training sets ( 100 samples ) .",
    "loocv , icomp , and kic performed comparably , and better than gpr using a gaussian kernel for data with nsr @xmath162 . in the other cases , the best results ( smallest mse ) was achieved by kic .",
    "all methods have smaller mse values using the gaussian kernel versus the cauchy kernel .",
    "gpr with the cauchy kernel obtains results comparable with kic , but with a standard deviation close to zero .",
    "* experiment 4 .",
    "* we assessed the consistency of selecting / tuning the parameters of the models in comparison with loocv .",
    "we considered four experiment of sample size , @xmath163 , and nsr @xmath164 .",
    "the parameters to tune or select are @xmath165 @xmath166 , and @xmath167 for the gaussian kernel .",
    "the frequency of selecting the parameters are shown in figure  [ loocv ] for loocv , and in figure  [ kic_frequency ] for kic .",
    "the more concentrated frequency shows the more consistent selecting criterion .",
    "the diagrams show that kic is more consistent in selecting the parameters rather than loocv .",
    "loocv is also sensitive to sample size .",
    "it provides a more consistent result for benchmarks with @xmath168 samples .",
    "+      we used three benchmarks selected from the delve datasets ( www.cs.toronto.edu/~delve/data ) : ( 1 ) abalone dataset ( 4177 instances , 7 dimensions ) , ( 2 ) kin - family of datasets ( 4 datasets ; 8192 instances , 8 dimensions ) , and ( 3 ) puma - family of datasets ( 4 datasets ; 8192 instances , 8 dimensions ) .    for the abalone dataset , the task is to estimate the age of abalones .",
    "we used normalized attributes in range [ 0,1 ] .",
    "the experiment is repeated 100 times to obtain the confidence interval . in each trial",
    "100 samples were selected randomly as the training set and the remaining 4077 samples as the test set .",
    "the kin - family and puma - family datasets are realistic simulations of a robot arm taking into consideration combinations of attributes such as whether the arm movement is nonlinear ( n ) or fairly linear ( f ) , and whether the level of noise ( unpredictability ) in the data is : medium ( m ) , or high ( h ) .",
    "the kin - family includes : kin-8fm , kin-8fh , kin-8 nm , kin-8nh datasets , and the puma - family contains : puma-8fm , puma-8fh , puma-8 nm , and puma-8nh datasets .    in the kin - family of datasets , having the angular positions of an 8-link robot arm , the distance of the end effector of the robot arm from a starting position is predicted . the angular position of a link of the robot arm",
    "is predicted given the angular positions , angular velocities , and the torques of the links .",
    "we compared kic_1 ( [ eq : kic1 ] ) , kic_2 ( [ eq : kic2 ] ) , and kic with loocv , icomp , and gpr on the three datasets .",
    "the results are shown as box - plots in figures  [ abalone ] , [ kin - family ] , and  [ puma - family ] for abalone , kin - family , and puma - family datasets , respectively .",
    "the best results across all three datasets were achieved using kic , and the second best results were for loocv .    for the abalone dataset , comparable results were achieved for kic and loocv , that are better than icomp , and the smallest mse value obtained by sgpr .",
    "kic_1 , and kic_2 had similar mse values , which are larger than for the other methods .    for the kin - family datasets , except for kin-8fm , kic gets better results than gpr , icomp , and loocv .",
    "kic_1 , and kic_2 obtain better results than gpr , and loocv for kin-8fm , and kin-8 nm , which are datasets with medium level of noise , but larger mse value for datasets with high noise ( kin-8fh , and kin-8nh ) .    for the puma - family datasets ,",
    "kic got the best results on all datasets except for on puma-8 nm , where the smallest mse was achieved by loocv .",
    "the result of kic is comparable to icomp and better than gpr for puma-8 nm dataset .",
    "for puma-8fm , puma-8fh , and puma-8nh , although the median of mse for loocv and gpr are comparable to kic , kic has a more significant mse ( smaller interquartile in the box bots ) .",
    "the median mse value for kic_1 , and kic_2 are closer to the median mse values of the other methods on puma-8fm , and puma-8 nm , where the noise level is moderate compared to puma-8fh , and puma-8nh , where the noise level is high . the sensitivity of kic_1 , and kic_2 to noise is due to the existence of variance in their formula .",
    "kic_2 has a larger interquartile of mse than kic_1 in datasets with high noise , which highlights the effect of @xmath109 in its formula ( equation  [ eq : kic2 ] ) rather than @xmath16 in equation  .",
    "we introduced a novel kernel - based information criterion ( kic ) for model selection in regression analysis . the complexity measure in kic",
    "is defined on a variable - wise variance which explicitly computes the interdependency of each parameter involved in the model ; whereas in methods such as kernel - based icomp and gpr , this interdependency is defined on a covariance matrix , which obscures the true contribution of the model parameters .",
    "we provided empirical evidence showing how kic outperforms loocv ( with kernel - based closed form formula of the estimator ) , kernel - based icomp , and gpr , on both artificial data and real benchmark datasets : abalon , kin family , and puma family . in these experiments ,",
    "kic efficiently balances the goodness of fit and complexity of the model , is robust to noise ( although for higher noise we have larger confidence interval as expected ) and sample size , is consistent in tuning / selecting the ridge and kernel parameters , and has significantly smaller or comparable mean squared values with respect to competing methods , while yielding stronger regressors .",
    "the effect of using different kernels was also investigated since the definition of a proper kernel plays an important role in kernel methods .",
    "kic had superior performance using different kernels and for the proper one obtains smaller mse .",
    "this work was funded by fnsnf grants ( p1tip2_148352 , pbtip2_140015 ) .",
    "we want to thank arthur gretton , and zoltn szab for the fruitful discussions ."
  ],
  "abstract_text": [
    "<S> this paper introduces kernel - based information criterion ( kic ) for model selection in regression analysis . the novel kernel - based complexity measure in kic </S>",
    "<S> efficiently computes the interdependency between parameters of the model using a variable - wise variance and yields selection of better , more robust regressors . </S>",
    "<S> experimental results show superior performance on both simulated and real data sets compared to leave - one - out cross - validation ( loocv ) , kernel - based information complexity ( icomp ) , and maximum log of marginal likelihood in gaussian process regression ( gpr ) . </S>"
  ]
}