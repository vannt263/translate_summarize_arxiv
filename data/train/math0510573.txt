{
  "article_text": [
    "in many applied settings , dealing with a very large data set , it is important to reduce the size of data in order to make an inference about the features of data set , in a timely manner .",
    "assume that the data set is represented by an @xmath0 matrix @xmath8 .",
    "it is important to find an approximation @xmath9 of a specified rank @xmath3 to @xmath1 , where @xmath3 is much smaller than @xmath4 and @xmath5 .    here",
    "are several motivation to obtain such @xmath2 .",
    "first , the storage space needed for @xmath2 is @xmath10 , which is much smaller than the storage space @xmath11 needed for @xmath1 . indeed",
    ", @xmath2 can be represented as @xmath12 where @xmath13 and @xmath14 are @xmath3 column vectors with @xmath4 and @xmath5 coordinates respectively .",
    "we store the vectors @xmath15 , which need the storage @xmath10 , and compute the entries of @xmath2 , when needed , using the above expression of @xmath2 .",
    "the second most common application is clustering algorithms as in @xcite , @xcite , @xcite , @xcite and @xcite .",
    "assume that our data represents @xmath5 points and each point has @xmath4 coordinates .",
    "that is each point represented by a column of the matrix @xmath1 .",
    "we want to cluster the points in such a way that the distance between points in the same cluster is much smaller than the distance between any two points from different clusters .",
    "one way to do this is to project all the point on the @xmath3 main orthonormal directions encoded by the first @xmath3 left singular vectors of @xmath1 . then cluster using either @xmath3 one dimensional subspaces or the whole @xmath3 dimensional subspace .",
    "the @xmath3-rank approximation @xmath2 gives the approximation to this @xmath3 dimensional subspace and the approximation to the first @xmath3 singular vectors .",
    "another way to cluster is to use the projective clustering .",
    "it is known that a fast svd , i.e. fast @xmath3-rank approximation , is a main tool in this area @xcite and @xcite .",
    "the third application is in dna microarrays , in particular in data imputation @xcite .",
    "let @xmath1 be the gene expression matrix .",
    "the @xmath4 rows of the matrix @xmath1 are indexed by the genes , while the @xmath5 columns are indexed by the number of experiments .",
    "it is known that the effective rank of @xmath1 is @xmath3 , is usually much less then @xmath5 . the svd decomposition is the main ingredient of the fraa , ( _ fixed rank approximation algorithm _ ) , which successfully implemented in @xcite to impute the corrupted entries of @xmath1 .",
    "the fast @xmath3- approximation algorithm suggested in this paper , combined with the clustering of similar genes , can be implemented to improve the fraa algorithm .",
    "the best approximation @xmath2 , which minimizes the frobenius norm @xmath16 , is given by the _ singular value decomposition _ ( svd ) of @xmath1 .",
    "however svd decomposition needs @xmath17 time computation , which is often too prohibitive .",
    "one way to find a _ fast _",
    "@xmath3-rank approximation is to choose at random @xmath18 columns or rows of @xmath1 and obtain from them @xmath3-rank approximations of @xmath1 .",
    "this is basically the spirit of the algorithm suggested in @xcite .",
    "we call this algorithm the fkv algorithm . assuming a statistical model for the distribution of the entries of @xmath1 the authors give some error bounds on their @xmath3-rank approximation",
    ". the weak point of fkv algorithm is its inability to improve iteratively fkv approximation by incorporating additional parts of @xmath1 .",
    "in fact in the recent paper @xcite , which uses fkv random algorithm , this point is mentioned specifically in the end of the paper :  ... , it would be interesting to design an algorithm that improves this approximation by accessing @xmath1 ( or parts of @xmath1 ) again . \"",
    "the aim of this paper is to provide a sampling framework for iterative updates of k - rank approximations of @xmath1 , by reading iteratively additional columns or rows of @xmath1 , which improves _ for sure _ the approximation @xmath2 each time it is updated .",
    "the quality of the approximation of @xmath2 is given by the frobenius norm @xmath19 , which is a nondecreasing sequence under these iterations .",
    "the rate of increase of the norms @xmath19 can be used as a stopping rule for terminating the algorithm .",
    "also the updating algorithm of @xmath3-rank approximation gives approximation to the first @xmath3 singular values of @xmath1 , and the approximations to the first @xmath3 left and right singular vectors of @xmath1 .",
    "assuming that the number of columns or rows which are read is @xmath20 , the complexity of our algorithm is @xmath7 .",
    "the intensive part of the computations is devoted to obtain the spectral decompositions of @xmath21 real symmetric matrices , where the computation for each decomposition is of order @xmath22 .",
    "the @xmath7 part of the algorithm is due to the multiplications of @xmath3 column vectors , which approximate the @xmath3 left or right singular eigenvectors of @xmath1 , by @xmath23 or @xmath1 respectively .",
    "this part of the algorithm can be parallelized , which will speed significantly the algorithm suggested here .",
    "the simulations that we performed show that we need a small number of iterations , ( around 5 ) , to get a very good approximation to the best @xmath3-rank approximation of @xmath1 .",
    "we believe that this algorithm will have many applications in data mining , data storage and data analysis .",
    "in this section , we recall some basic facts about svd , which are embedded in our algorithm , see @xcite .",
    "let @xmath24 be the linear space of real column vectors with @xmath4 coordinates , the linear space of real @xmath25 matrices , the subset of @xmath26 real valued matrices whose @xmath27 columms is an orthonormal system and the subspace of @xmath28 real symmetric matrices .",
    "for @xmath29 we let @xmath30 if @xmath31 is nonnegative definite and @xmath32 if @xmath31 is positive definite .",
    "denote by @xmath33 the matrix with the diagonal entry @xmath34 on the @xmath35 position for @xmath36 and all other entries are equal to zero .",
    "let @xmath37 be the rank of @xmath1 .",
    "then the svd decomposition of @xmath1 is given as @xmath38 where @xmath39 , and @xmath40 .",
    "let @xmath41 and @xmath42 denote @xmath43 orthonormal columns of @xmath44 and @xmath45 respectively .",
    "then @xmath38 can be written as @xmath46 .",
    "the vectors @xmath47 are called the _ left _ and _ right singular vectors _ of @xmath1 respectively , which correspond to the singular value @xmath48 . the left and the right singular vectors",
    "can be computed from the right and the left singular vectors by the formulas : @xmath49 equivalently , @xmath50 are all the positive eigenvalues of the nonnegative definite symmetric matrices @xmath51 , with the corresponding orthonormal eigenvectors @xmath41 and @xmath42 .",
    "denote by @xmath52 the frobenius ( @xmath53 ) norm of @xmath1 .",
    "it is the euclidean norm of @xmath1 viewed as a vector with @xmath54 coordinates .",
    "each term @xmath55 in svd decomposition of @xmath1 is a rank one matrix with @xmath56 .",
    "let @xmath57 denote the set of @xmath58 matrices of at most rank @xmath3 ( @xmath59 ) .",
    "then for each @xmath3 , @xmath60 , the svd of @xmath1 gives the solution to the following approximation problem :    @xmath61    if @xmath62 then @xmath63 is the unique solution to the above minima problem . for the purposes of this paper , it will be convenient to assume that @xmath64 for any @xmath65 .",
    "given @xmath66 $ ] then @xmath67 , @xmath68 and @xmath69 are characterized by the following maximal characterization :    [ maxchar ] let @xmath8 and @xmath70 $ ] .",
    "let @xmath13 and @xmath14 be two sets of orthonormal vectors in @xmath71 and @xmath72 respectively .",
    "then @xmath73 equality in the first or second inequality occurs if @xmath74 or @xmath75 , contains @xmath3 linearly independent left or right singular vectors of @xmath1 , corresponding the the @xmath3 maximal singular values of @xmath1 respectively .",
    "the above characterization follows from the maximal , ( ky fan characterization ) , of the sum of the first biggest eigenvalues of a real symmetric matrix :    [ kyfanchar ] let @xmath76 be a real @xmath77 symmetric matrix .",
    "let @xmath78 be the eigenvalues @xmath31 arranged in a decreasing order and listed with their multiplicities .",
    "let @xmath79 be an orthonormal set of the corresponding eigenvectors of @xmath31 : @xmath80 .",
    "let @xmath81 $ ] be an integer .",
    "then for any orthonormal set @xmath82 @xmath83 equality holds if and only if the subspace @xmath74 contains @xmath3 linearly independent eigenvectors of @xmath31 corresponding to the eigenvalues @xmath84 .",
    "see for example @xcite for proofs and the references .",
    "note that @xmath85 is a system of orthonormal vectors in @xmath71 if and only if the @xmath86 matrix @xmath87 is in @xmath88 .    to obtain theorem [ maxchar ] from theorem [ kyfanchar ]",
    "we let @xmath89 , ( or @xmath90 ) , and @xmath31 to be equal to @xmath91 , ( or @xmath92 ) . in ( [ maxchar1 ] )",
    "we emphasized the complexity of the computations of the left - hand side of the inequalities .",
    "see also @xcite for applications of theorem [ kyfanchar ] to data imputation in dna microarrays .",
    "in the rest of the paper we give the version of our results to @xmath3-orthornormal systems @xmath85 .",
    "similar results holds for @xmath3-orthonormal systems @xmath93 .",
    "[ minchar ] let @xmath94 and @xmath70 $ ] be an integer .",
    "then for any @xmath3-orthornormal system @xmath85 the following equality holds : @xmath95 in particular the best @xmath3-rank approximation of @xmath1 is given by @xmath96 , where @xmath97 is an orthonormal sets of the left singular vectors of @xmath1 corresponding to @xmath98 .",
    "the next theorem is the key theorem for updating the @xmath3-rank approximation for    @xmath99 , for some @xmath100 .",
    "[ updatel ] let @xmath85 , be an orthonormal system in @xmath71 .",
    "let @xmath101 , be a given set in @xmath71 .",
    "perform the modified gram - schmidt algorithm on @xmath102 , to obtain an orthonormal set @xmath103 , where @xmath104 .",
    "assume that @xmath105 , i.e. @xmath106 . form @xmath77 real symmetric matrix @xmath107 , and assume that @xmath108 are the @xmath3-largest eigenvalues of @xmath31 with the corresponding @xmath3-orthonormal vectors @xmath109 .",
    "let @xmath110 and define @xmath3-orthonomal vectors @xmath111 as follows : @xmath112 then @xmath113 furthermore @xmath114    we now explain the essence of theorem [ updatel ] .",
    "view @xmath13 as approximation to the first @xmath3-left singular vectors of @xmath1 , and @xmath115 as the @xmath3-rank approximation to @xmath1 .",
    "hence @xmath116 is an approximation to @xmath117 of @xmath1 for @xmath118 .",
    "read additional vectors @xmath101 such that at least one of this vectors is not in the subspace spanned by @xmath13 .",
    "let @xmath119 be the subspace spanned by @xmath13 and @xmath120 .",
    "hence @xmath121 is the orthonormal basis of @xmath119 obtained from the vectors @xmath122 , using the modified gram - schmidt algorithm .",
    "( the modified gram - schmidt algorithm used to ensure the numerical stability . ) note that @xmath123 , and in general one has that @xmath124 .",
    "consider the @xmath77 nonnegative definite matrix @xmath125 .",
    "find its first @xmath3 eigenvectors to obtain @xmath126 using ( [ xtildef ] ) .",
    "then @xmath127 is the best approximation of @xmath1 by matrix @xmath2 of rank @xmath3 at most , whose columns are in the subspace @xmath119 . in particular , the approximation @xmath128 is better than the previous approximation @xmath115 , which is equivalent to the ( [ update2 ] ) .",
    "@xmath129    * outline of proof of theorem [ updatel ] .",
    "* let @xmath130 .",
    "let @xmath131 be the standard orthonormal basis in @xmath132 .",
    "use the definition of @xmath1 and ky fan characterization of the sum of the maximal @xmath3 eigenvalues of symmetric @xmath31 to deduce @xmath133 let @xmath134 .",
    "then @xmath135 .",
    "hence the @xmath136 are the singular values of @xmath128 and @xmath137 are the right singular vectors of @xmath128 .",
    "thus @xmath138 , where @xmath139 is the left singular vector of @xmath128 corresponding to the singular value @xmath140 for @xmath141 .",
    "a straightforward calculation shows that @xmath142 for @xmath118 .",
    "hence ( [ update2 ] ) holds .",
    "furthermore we also deduced the equalities in ( [ lambdeq ] ) for @xmath118 .",
    "one starts the algorithm by choosing the first @xmath3-rank approximation to @xmath1 as follows .",
    "let @xmath144 be the @xmath5 columns of @xmath1 .",
    "choose @xmath3 integers @xmath145 .",
    "let @xmath146 be the orthonormal set obtained from @xmath147 using the modified gram - schmidt algorithm .",
    "set @xmath148 in general @xmath149 , but in some cases if @xmath13 are linearly dependent , @xmath150 .",
    "assume for simplicity of the exposition that @xmath149 .",
    "then @xmath151 is of the form ( [ bexpres ] ) where @xmath152 .",
    "now update iteratively @xmath3-rank approximation of @xmath153 of @xmath1 to @xmath154 , using theorem [ updatel ] , by letting @xmath155 , for some @xmath156 integers @xmath157 .",
    "that is , we choose another @xmath156 sets of columns of @xmath1 , preferably that were not chosen before , and update the @xmath3-rank approximation using the algorithm suggested by theorem [ updatel ] to obtain an improved @xmath3-rank approximation @xmath154 of @xmath1 .",
    "furthermore one can use the @xmath3-rank approximation @xmath154 from the above algorithm to approximate the first @xmath3-singular values @xmath98 , and the left and the right singular vectors @xmath68 and @xmath69 as follows .",
    "first , the square roots @xmath158 of the matrix @xmath31 are approximations for @xmath98 .",
    "second , the vectors @xmath159 approximate @xmath68 . let @xmath160 .",
    "then @xmath161 for @xmath118 .",
    "third , the renormalized @xmath162 which are given as @xmath163 approximate the right singular eigenvectors @xmath164 for @xmath118 .",
    "@xmath165 @xmath165 we now explain briefly the main steps of our algorithm . we read the dimensions @xmath166 of the data matrix @xmath1 .",
    "we set @xmath167 as the maximal number of iterations we are going to execute to find the @xmath3-rank approximation of @xmath1 .",
    "we read the entries of the data matrix @xmath1 , and finally the small parameter @xmath168 .",
    "we choose the @xmath3-rank approximation @xmath151 using ( [ binit ] ) .",
    "assume that @xmath153 is the current @xmath3-rank approximation to @xmath1 .",
    "next we choose additional @xmath156 columns of @xmath1 and update @xmath153 to @xmath154 using theorem [ updatel ] as explained in the previous section .",
    "recall that @xmath169 .",
    "if the relative improvement in @xmath170 is less than @xmath171 , i.e. @xmath172 , we are satisfied with the approximation @xmath154 and finish our algorithm .",
    "if this does not happen then our algorithm stops after the @xmath167 iteration .",
    "we assume here that @xmath173 and we consider our algorithm applied to the randomly selected columns of @xmath1 .",
    "( otherwise we apply our algorithm to the rows of @xmath1 . )",
    "the first step of our algorithm is to find an orthonormal @xmath174 basis of the subspace spanned by @xmath3 randomly chosen columns of @xmath1 .",
    "the modified gram - schmidt algorithm needs @xmath175 flops assumed the worst , ( generic ) , case @xmath149 .",
    "this step needs @xmath176 flops @xcite .",
    "let @xmath177 for @xmath118 . the computations of @xmath14 needs @xmath178 operations and can be parallelized .",
    "@xmath179 and does not have to be computed .    to find @xmath180 we choose @xmath156 columns @xmath120 of @xmath1 at random",
    ". then we perform the modified gram - schmidt algorithm on @xmath102 to find an orthonormal system @xmath181 , with @xmath182 $ ] .",
    "this step requires @xmath183 flops .",
    "let @xmath184 for @xmath185 .",
    "this step needs at most @xmath186 flops and can be parallelized .",
    "next we have the following two choices .",
    "first possibility is to find the svd of @xmath187 $ ] .",
    "this needs at most @xmath188 flops .",
    "second possibility is to compute the matrix @xmath189 given in theorem [ updatel ] .",
    "this needs @xmath190 flops .",
    "then we compute the spectral decomposition of @xmath191 which needs @xmath192 flops .",
    "if @xmath193 is much smaller than @xmath5 , it seems to us that the second method is more efficient .",
    "the first @xmath3 right singular vectors of @xmath194 are identical to the first @xmath3 eigenvectors of @xmath31 .",
    "use these @xmath3 vectors , as explained in theorem [ updatel ] , to update @xmath13 and @xmath14 .",
    "this needs @xmath195 flops .    to update @xmath153 to @xmath154 for @xmath196 require the same number of flops as to compute @xmath180",
    "hence , the most intensive part of the computation lies in the computation of the spectral decomposition @xmath197 , which is @xmath198 . assuming that @xmath20",
    ", we deduce that the intensive part of the computation is of order @xmath22 .",
    "the simulations that we performed show that we need a small number of iterations , ( around 5 ) , to get a very good approximation to the best @xmath3-rank approximation of @xmath1 .",
    "hence our algorithm is expected to converge in @xmath7 steps .",
    "to assess the performance of our k - rank approximation algorithm we conducted different simulation on synthetic data and images .",
    "we also implemented our algorithm for three different sampling methods , uniform sampling with replacement , uniform sampling without replacement and weighted sampling for images according to weight of each row in gradient image . to show our algorithm guarantee convergence to the optimum ( deterministic ) k - rank approximation we applied the algorithm on a randomly generated data matrix of @xmath199 with rank @xmath200 . to measure the approximation error we defined the relative error of the approximation as , @xmath201 , where @xmath202 is the original data matrix and @xmath154 is k - rank approximation to @xmath1 .",
    "figure 1 shows convergence of the relative error to the optimum relative error , @xmath203 where @xmath204 is the optimum true k - rank approximation to matrix @xmath1 , as a function of iteration parameter @xmath205 for @xmath206 on the synthetic data matrix . in each iteration",
    "we randomly picked @xmath207 rows of the data matrix with and without replacement . for comparison",
    "the optimum relative error has been pointed out on the axis . as we expected",
    "the convergence property is faster when the rows are sampled without replacement .",
    "figure 2 shows the same convergence result for the real images of cameraman and liftingbody ( from image processing toolbox of matlab ) .",
    "figure 3 shows the plots of the relative error versus total number of sampled rows in each method of sampling for the cameraman and liftingbody images .",
    "it can be seen that for these images our algorithm return very reasonable relative error only by using portion of the data .",
    "needless to say that the importance of this iterative algorithm can be observed when it is applied to the big matrices of data where the svd of the data matrix is often impossible to compute . in this case",
    "the simple randomized svd methods @xcite may not be fast , since it needs to sample large enough number of rows to give the acceptable error bound on the approximation . to highlight this feature of our algorithm we applied our algorithm on a synthetic full rank data matrix of @xmath208 .",
    "we chose the parameter @xmath209 such that the relative error to be less than 2 times of optimum relative error .",
    "we observed that the speed up of our algorithm , which is the rate of the time needed by deterministic svd to compute 100-rank approximation to the time needed by our algorithm to compute @xmath154 , is 42 in our system .",
    "the result for three more data sets with different values of @xmath3 has also been illustrated in table 1 .",
    "due to system limitation and the comparison issue we were not able to show the speed up for bigger matrices where deterministic svd ( svd in matlab ) may fail to compute the svd of the matrix .",
    "however , our algorithm can always compute the k - rank approximation of the matrix as long as @xmath193 is not too big since the algorithm in each iteration deals with small matrices .",
    ".comparison of relative error and speed up of our algorithm with optimum @xmath3-rank approximation algorithm [ cols=\"^,<,<,<,<\",options=\"header \" , ]",
    "we have proposed a novel approach for fast computing of @xmath3-rank approximation of a given @xmath0 data matrix , using monte - carlo method by choosing at random @xmath156 columns or rows of @xmath1 .",
    "the advantage of our algorithm is that we guarantee that every iteration improves the quality of our approximation .",
    "we applied our algorithm on synthetic data matrices as well as images and the result confirms the convergence of the relative error of the approximation to the optimum relative error . to highlight the important feature of this algorithm we applied this method on a big matrix of randomly generated data and we observed for the reasonable level of relative error the algorithm is also much faster than optimum k - rank approximation using deterministic svd which may also fail to compute the svd for big matrices .",
    "we believe that this algorithm will have many applications in data mining , data storage and data analysis where dealing with high dimensional data is a major problem .",
    "aggrawal , c.m .",
    "procopiuc , j.l .",
    "wolf , p.s .",
    "yu and j.s .",
    "park , fast algorithms for projected clustering , _ proc . of acm sigmod intl . conf .",
    "management of data _ 1999 , 61 - 72 .",
    "r. agrawal , j.gerhrke , d.gunopulos , and p. raghavan , automatic subspace clustering of high dimensional data for data mining applications , _ proc .",
    "acm sigmod conf . on management of data _ , 1998 , 94 - 105 .",
    "p. drineas , a. frieze , r. kannan , s. vempala and v. vinay , clustering large graphs via the singular value decomposition , _ journal of machine learning _ , 56 ( 2004 ) , 9 - 33 .",
    "m. ester , h .-",
    "krieger , j. sander and x.xu , a density - based algortihm for discovering clusters in large spatial databases with nose , _ proc .",
    "knowledge discovery and data mining _ , 1996 , 226 - 231 .",
    "s. friedland , inverse eigenvalue problems , _ linear algebra appl .",
    "_ 17 ( 1977 ) , 15 - 51",
    ". s.  friedland , a.  niknejad and l.  chihara , a simultaneous reconstruction of missing data in dna microarrays , _ linear algebra appl .",
    "_ , to appear .",
    "a. frieze , r. kannan and s. vempala , fast monte - carlo alogrithms for finding low rank approximations , _ proceedings of the 39th annual symposium on foundation of computer science _ , 1998 .",
    "golub and c.f .",
    "van loan , _ matrix computation _",
    ", john hopkins univ . press , 3rd ed . , 1996 .",
    "procopiuc , p.k .",
    "agarwal , m. jones and t.m .",
    "murali , a monte carlo algorithm for fast projective clustering , _ proc . of acm sigmod intl .",
    "management of data _ 2002 ."
  ],
  "abstract_text": [
    "<S> in many applications , it is of interest to approximate data , given by @xmath0 matrix @xmath1 , by a matrix @xmath2 of at most rank @xmath3 , which is much smaller than @xmath4 and @xmath5 . </S>",
    "<S> the best approximation is given by singular value decomposition , which is too time consuming for very large @xmath4 and @xmath5 .    </S>",
    "<S> we present here a monte carlo algorithm for iteratively computing a @xmath3-rank approximation to the data consisting of @xmath0 matrix @xmath1 . </S>",
    "<S> each iteration involves the reading of @xmath6 of columns or rows of @xmath1 . </S>",
    "<S> the complexity of our algorithm is @xmath7 . </S>",
    "<S> our algorithm , distinguished from other known algorithms , guarantees that each iteration is a better @xmath3-rank approximation than the previous iteration . </S>",
    "<S> we believe that this algorithm will have many applications in data mining , data storage and data analysis . </S>"
  ]
}