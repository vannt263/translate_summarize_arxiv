{
  "article_text": [
    "short pieces of texts reach us every day through the use of social media such as twitter , newspaper headlines , and texting . especially on social media , millions of such short texts",
    "are sent every day , and it quickly becomes a daunting task to find similar messages among them , which is at the core of applications such as event detection ( @xcite ) , news recommendation ( @xcite ) , etc .    in this paper",
    "we address the issue of finding an effective vector representation for a very short text fragment . by effective we mean that the representation should grasp most of the semantic information in that fragment . for this",
    "we use semantic word embeddings to represent individual words , and we learn how to weigh every word in the text through the use of tf - idf ( term frequency - inverse document frequency ) information to arrive at an overall representation of the fragment",
    ".    these representations will be evaluated through a semantic similarity task .",
    "it is therefore important to point out that textual similarity can be achieved on different levels . at the most strict level , the similarity measure between two texts",
    "is often defined as being ( near ) paraphrases . in",
    "a more relaxed setting one is interested in topic- and subject - related texts .",
    "for example , if a sentence is about the release of a new star wars episode and another about darth vader , they will be dissimilar in the most strict sense , although they share the same underlying subject .",
    "in this paper we focus on the broader concept of topic - based semantic similarity , as this is often applicable in the already mentioned use cases of event detection and recommendation .",
    "our main contributions are threefold .",
    "first , we construct a technique to calculate effective text representations by weighing word embeddings , for both fixed- and variable - length texts .",
    "second , we devise a novel median - based loss function to be used in the context of minibatch learning to mitigate the negative effect of outliers .",
    "finally we create a dataset of semantically related and non - related pairs of text from both wikipedia and twitter , on which the proposed techniques are evaluated .",
    "we will show that our technique outperforms most of the baselines in a semantic similarity task .",
    "we will also demonstrate that our technique is independent of the word embeddings being used , so that the technique is directly applicable and thus does not require additional model training when used in different contexts , in contrast to most state - of - the art techniques .    in the next section ,",
    "we start with a summary of the related work , and our own methodology will be devised in section [ sec : methodology ] .",
    "next we explain how data is collected in section [ sec : datacollection ] , after which we discuss our experimental results in section [ sec : experiments ] .",
    "in this work we use so - called word embeddings as a basic building block to construct text representations .",
    "such an embedding is a distributed vector representation of a single word in a fixed - dimensional semantic space , as opposed to term tf - idf vectors , in which a word is represented by a one - hot vector ( @xcite ) .",
    "a word s term frequency ( tf ) is the number of times the word occurs in the considered document , and a word s document frequency ( df ) is the number of documents in the considered corpus that contain that word .",
    "its ( smoothed ) inverse document frequency ( idf ) is defined as : @xmath0 in which @xmath1 is the number of documents in the corpus ( @xcite ) .",
    "a tf - idf - based similarity measure is based on exact word overlap .",
    "as texts become smaller in length , however , the probability of having words in common decreases . furthermore",
    ", these measures ignore synonyms and any semantic relatedness between different words , and are prone to negative effects of homonyms .    instead of relying on exact word overlap",
    ", one can incorporate semantic information into the similarity process .",
    "latent semantic indexing ( lsi ) and latent dirichlet allocation ( lda ) are two examples , in which every word is projected into a semantic ( topic ) space ( @xcite ) . at test time",
    ", inference is performed to obtain a semantic vector for a particular sentence .",
    "both training and inference of standard lsi and lda , however , are computationally expensive on large vocabularies .",
    "although lsi and lda have been used with success in the past , skip - gram models have been shown to outperform them in various tasks ( @xcite ) . in skip - gram ,",
    "part of google s word2vec toolkitavailable at code.google.com/archive/p/word2vec ] , distributed word embeddings are learned through a neural network architecture to predict its surrounding words in a fixed window .",
    "once the word embeddings are obtained , we have to combine them into a useful sentence representation .",
    "one possibility is to use an multilayer perceptron ( mlp ) with the whole sentence as an input , or a 1d convolutional neural network ( @xcite ) .",
    "such an approach , however , requires either an input of fixed length or aggregation operations  such as dynamic k - max pooling ( @xcite )  to arrive at a sentence representation that has the same dimensionality for every input .",
    "recurrent neural networks ( rnns ) and variants can overcome the problem of fixed dimensionality or aggregation , since one can feed word after word in the system and in the end arrive at a text representation ( @xcite ) .",
    "the recently introduced skip - thought vectors , heavily inspired on skip - gram , combine the learning of word embeddings with the learning of a useful sentence representation using an rnn encoder and decoder ( @xcite ) .",
    "rnn - based methods present a lot of advantages over mlps and convolutional networks , but still retraining is required when using different types of embeddings .",
    "paragraph2vec is another method , inspired by the skip - gram algorithm , to derive sentence vectors ( @xcite ) .",
    "the technique requires the user to train vectors for frequently occurring word groups .",
    "the method , however , is not usable in a streaming or on - the - fly fashion , since it requires retraining for unseen word groups at test time . aggregating word embeddings through a mean , max , min ",
    "function is still one of the most easy and widely used techniques to derive sentence embeddings , often in combination with an mlp or convolutional network ( @xcite ) . on one hand ,",
    "the word order is lost , which can be important in e.g.  paraphrase identification . on the other hand ,",
    "the methods are simple , out - of - the - box and do not require a fixed length input .",
    "related to the concepts of semantic similarity and weighted embedding aggregation , there is extensive literature .",
    "@xcite  calculate a similarity metric between documents based on the travel distance of word embeddings from one document to another one .",
    "we on the other hand will derive vectors for the documents themselves .",
    "@xcite  learn semantic features for every sentence in the dataset based on a saliency weighted network for which the bm25 algorithm is used .",
    "however , the features are being learned for every sentence prior to test time , and therefore not applicable in a real - time streaming context .",
    "finally , @xcite  calculate a cosine similarity matrix between the words of two sentences that are sorted based on their idf value , which they use as a feature vector for an mlp .",
    "their approach is similar to our work in the sense that the authors use idf information to rescale term contribution .",
    "their primary goal , however , is calculating semantic similarity instead of learning a sentence representation .",
    "in fact , the authors totally discard the original word embeddings and only use the calculated cosine similarity features .",
    "the core principle of our methodology is to assign a weight to each word in a short text .",
    "these weights are determined based on the idf value of the individual words in that text .",
    "the idea is that important words ",
    "i.e.  words that are needed to determine most of the text s semantics  usually have higher idf values than less important words , such as articles and auxiliaries   indeed , the latter appear more frequently in various different texts , while words with a high idf value mostly occur in similar contexts .",
    "the final goal is to combine the weighted words into a semantically effective , single text representation .    to achieve this goal",
    ", we will model the problem of finding a suitable text representation as a semantic similarity task between couples of short texts . in order to classify such couples of text fragments into",
    "either semantically related pairs or non - related pairs , the vector representations of these two texts are directly compared . in this paper",
    "we use a simple threshold function on the distance between the two text representations , as we want related pairs to lie close to each other in their representation space , and non - related pairs to lie far apart : @xmath2 in this expression @xmath3 and @xmath4 are two short text vector representations of dimensionality @xmath5 , @xmath6 is a vector distance function of choice ( e.g.  cosine distance , euclidean distance  ) , @xmath7 is a threshold , and @xmath8 is the binary prediction of semantic relatedness .      as mentioned before",
    ", we will assign a weight to each word in a text according to that word s idf value . to learn these weights , we devise a model that is visualised in figure [ fig : example_1 ] . in the learning scheme",
    ", we use related and non - related couples of text as input data .",
    "first , the words in every text are sorted from high to low idf values .",
    "original word order is therefore discarded , as is the case in usual standard aggregation operations .",
    "after that , every embedding vector for each of the sorted words is multiplied with a weight that can be learned .",
    "finally , the weighted vectors are averaged to arrive at a single text representation .    in more detail , consider a dataset @xmath9 consisting of couples of short texts .",
    "an arbitrary couple is denoted by @xmath10 , and the two texts of @xmath10 by @xmath11 and @xmath12 .",
    "we indicate the vector representation of word @xmath13 in text @xmath14 by @xmath15 .",
    "all word vectors have the same dimensionality @xmath5 .",
    "each text @xmath11 also has an associated length @xmath16 , i.e.  the number of words in @xmath11 . for now , in this section , we assume that @xmath17 , a notion we will relax in section [ sec : variablelength ] .",
    "the final goal of our methodology is to arrive at a vector representation for both texts in @xmath10 , denoted by @xmath18 and @xmath19 . denoting the sorted texts as @xmath20 and @xmath21",
    ", we arrive at a vector representation @xmath18 and @xmath19 through the following equation : @xmath22 in which @xmath23 are the weights to be learned . as such ,",
    "we create a weighted sum of the individual embeddings , the weights of which are only related to the rank order according to the idf values of all words in the fragment .",
    "( ) at ( 0,0 ) .,title=\"fig : \" ] ;    ( bo ) at ( 2.0,2.75 ) @xmath24 ; ( b8 ) at ( 6.2,2.75 ) @xmath25 ;    \\(d ) at ( 4.19,2.75 ) @xmath26 ;    \\(s ) at ( 4.16,1.88 ) @xmath27 ;    \\(n ) at ( 4.36,1.30 ) @xmath28 ;    ( bo ) at ( 0.6,4.0 ) @xmath5 ; ( bo ) at ( 4.15,0.35 ) @xmath5 ;    the model we construct through this procedure is related to the siamese neural network with parameter sharing from the early nineties ( @xcite ) .",
    "the learning procedure of such models is as follows .",
    "we first calculate the vector representations for both texts in a particular couple through equation , both using the same weights , after which we compare the two vector representations through a loss function @xmath29 that we wish to minimize .",
    "after that , the weights are updated through a gradient descent procedure using a learning rate @xmath30 : @xmath31      ( ) at ( 0,0 ) ; ( bo ) at ( 0.1,0.0 ) @xmath32 ; ( b3 ) at ( 6.95,0.0 ) @xmath33 ; ( 0.1,0.0 ) ",
    "node[below=3pt ] normalized distance ( 6.95,0.0 ) ; ( 0.0 , 0.1 ) ",
    "node[left=3pt ] ( 0.0 , 5.3 ) ; ( lo ) at ( 0.0,0.1 ) @xmath32 ; ( l1 ) at ( 0.0,5.3 ) @xmath34 ;    as pointed out in the beginning of this section , we want to have semantically related texts to lie close to each other in the representation space , and non - related texts to lie far apart from each other .",
    "we can visually inspect the distribution of the distances between every couple in the dataset .",
    "in fact , we calculate two distributions , one for the related pairs and one for the non - related pairs",
    ". two examples of such distributions , created using twitter data and an average word embedding text representation , are shown in figure [ fig : mean_distribution ] .",
    "related pairs tend to lie closer to each other than non - related pairs .",
    "there is however a considerable overlap between the two distributions .",
    "this means that making binary decisions on similarity based on a well - chosen distance threshold will lead to a large error .",
    "the goal is to reduce this error , and thus to minimize the overlap between the two distributions . directly minimizing",
    "the overlap is difficult , since it requires the overlap as a function of the model weights .",
    "we will instead describe two different loss functions as an approximation to this problem .",
    "the first loss function is related to the contrastive loss function regularly used in siamese neural architectures ( @xcite ) .",
    "we define the quantity @xmath35 as follows : @xmath36 the loss function , which we will conveniently call the contrastive - based loss , is then given by : @xmath37 in which @xmath6 is a vector distance function of choice , as before .",
    "note that , when trying to minimize this loss , related pairs will get pushed to each other in the representation space , while non - related pairs will get dragged apart .",
    "this loss function , however , has two main problems .",
    "first , there is an imbalance between the loss for related pairs and non - related pairs , in which the latter can get an arbitrarily negative loss , while the related pairs loss can not be pushed below zero . to solve this",
    ", we could add a maximum possible distance  which is e.g.  1 in the case of cosine distance  but this can not be generalized to arbitrary distance functions .",
    "second , this loss function can skew distance distributions , so that minimizing overlap between distributions is not guaranteed .",
    "in fact , the overlap can even increase while minimizing this loss .",
    "this happens , for example , when the distance between some of the related pairs can be drastically reduced while other related pairs get dragged farther apart , and vice versa for the non - related pairs .",
    "the loss function as such allows this to happen , since it can focus on data points that are easier to shift towards or away from each other and it can ignore what happens to the other data points .",
    "as long as the contribution of these shifts to the overall loss remains dominant , the loss will diminish , although the predictions according to equation will be worse . despite these problems we still consider this loss function due to its simplicity .",
    "the derivative with respect to weight @xmath38 is given by : @xmath39    in a second loss function we try to mitigate the loss imbalance and potential skewing of the distributions caused by the contrastive - based loss function .",
    "for this purpose we will use the median , as it is a very robust statistic insensitive to outliers . as we need multiple data points to calculate the median , this loss function",
    "can only be used in the context of minibatch gradient descent , in which the number of positive and negative examples in each minibatch is balanced . in practice",
    "we consider a minibatch @xmath40 of @xmath41 randomly sampled data points , in which there are exactly @xmath42 related pairs and @xmath42 non - related pairs .",
    "we consider the couple of texts @xmath43 as the median couple if @xmath44 is the median of all distances between the couples in @xmath45 : @xmath46 since minibatch @xmath45 is randomly sampled , we can consider @xmath47 as an approximation to the optimal split point between related and non - related pairs , in the sense of threshold @xmath7 in equation .",
    "we thus consider all related pairs with a distance larger than @xmath47 , and all non - related pairs with a distance smaller than @xmath47 to be classified incorrectly .",
    "since minimizing a 0 - 1 loss is np - hard , we use a scaled cross - entropy function in our loss , which we will call the median - based loss : @xmath48,\\end{aligned}\\ ] ] in which @xmath49 is a hyperparameter .",
    "the derivative with respect to weight @xmath38 is given by the following expression ( in which @xmath50 is the sigmoid function ) : @xmath51      the method described thus far is only applicable to short texts of a fixed length , which is limiting . in this section",
    "we will extend our technique to texts of a variable , but given maximum length .",
    "for this purpose we have devised multiple approaches , of which we will elaborate the one that performed best in the experiments .",
    "suppose that all texts have a fixed maximum length of @xmath52 .",
    "in the learning procedure we will learn a total of @xmath52 weights with the techniques described earlier .",
    "to find the weights for a text with length @xmath53 we will use subsampling and linear interpolation .",
    "that is , for an arbitrary text @xmath54 we first find the sequence of real - valued indices @xmath55 through subsampling : @xmath56 then , in the second step , we calculate the new weights @xmath57 for the words in @xmath54 through linear interpolation , in which @xmath58 is arbitrarily small : @xmath59 in this , @xmath60 and @xmath61 are resp .  the ceil and floor functions .",
    "finally , equation needs to be updated with these new weights instead of @xmath38 : @xmath62 calculating the derivative of the new weights with respect to the original weights is straightforward .",
    "to train the weights for the individual embeddings and to conduct experiments , we collect data from different sources .",
    "first we gather textual pairs from wikipedia which we will use as a base dataset to finetune our methodology .",
    "we will also use this dataset to test our hypotheses and perform initial experiments .",
    "the second dataset will consist of twitter message pairs , which we will use to show that our method can be used in a practical streaming context .",
    "we will perform our initial experiments in a base setting using english wikipedia articles .",
    "the most important benefit of using wikipedia is that there is a lot of well structured data .",
    "it is therefore a good starting point to collect a ground truth to finetune our methodology .",
    "we use the english wikipedia dump of march 4th 2015 , and we remove its markup and punctuation .",
    "we convert all letters to lower case and every number is replaced by a single character ` 0 ' ( zero ) .",
    "next we construct related pairs of texts which both have the same , fixed length @xmath63 .",
    "to do this , we take a wikipedia article and we extract @xmath63 consecutive words out of a paragraph .",
    "then we skip two words , after which we extract the next @xmath63 consecutive words , as long as they remain in the same paragraph . to extract non - related text pairs",
    "we follow the same procedure , but we make sure that the two texts are from different articles , which we choose at random .",
    "this approach is closely related to the data collection practice used in ( @xcite ) .",
    "we want to emphasize again that our vision of semantic similarity is one of topic - based similarity instead of paraphrase - similarity , as discussed in the introduction .",
    "this notion is reflected in our data collection .",
    "we extract a total of 4.9 million related pairs and 4.9 million non - related pairs , each for fixed - length texts of 20 words long .",
    "we also extract 4.9 million related and non - related pairs of which the texts varies in length between 10 and 30 words .",
    "all datasets are divided into a train set of 1.5 million pairs , a test set of 1.5 million pairs and a validation set of 1.9 million pairs .",
    "twitter is a very different kind of medium than wikipedia .",
    "wikipedia articles are written in a formal register and mostly consist of linguistically correct sentences .",
    "twitter messages , on the other hand , count no more than 140 characters and are full of spelling errors , abbreviations and slang .",
    "we propose that two tweets are semantically related if they are generated by the same event . as in ( @xcite ) , we require that such an event is represented by one or more hashtags . since twitter posts and associated events are very noisy by nature , we restrict ourselves to tweets by 100 english news agencies .",
    "we manually created this list through inspection of their twitter accounts ; the list is available through our github page , see section [ sec : conclusion ] .",
    "we gathered 341 949 tweets from all news agencies through the twitter rest api at the end of august 2015 .",
    "we convert all words to lowercase , replace all numbers by the single character ` 0 ' and remove non - informative hashtags such as _ # breaking _ , _ # update _ and _ # news_. to generate related pairs out of these tweets , we consider four simple heuristic rules :    1",
    ".   the number of words in each tweet , different from hashtags , mentions or urls , should be at least 5 .",
    "the jaccard similarity between the set of hashtags in both tweets should be at least 0.5 .",
    "3 .   the tweets should be sent no more than 15 minutes from each other .",
    "the jaccard similarity between the set of words in both tweets should be less than 0.5 .",
    "we add the last rule in order to have sufficient word dissimilarity between the pairs , as tweets that mostly contain the same words are too easy to relate . to generate non - related pairs ,",
    "we remove rule 3 and rule 2 is changed : the jaccard similarity between sets of hashtags should now be zero . using these heuristics",
    ", we generate a train set of 15 000 pairs , a validation set of 20 000 pairs and a test set of 13 645 pairs , of which we remove all overlapping hashtags .",
    "we manually label 200 generated pairs and non - pairs , and we achieve an error rate of 28% . due to the used heuristics and the linguistic nature of tweets in general , the ground truth can be considered very noisy ; achieving an error rate lower than around 28% on this dataset will therefore be difficult , and the gain would not lead to a better model of the human notion of similarity anyway .",
    "in this section we discuss the results of several experiments on all aspects of our methodology given the data we collected .",
    "first we will discuss some results using the wikipedia dataset , after which we also take a look at the twitter dataset .",
    "we will use two performance metrics in our evaluation .",
    "the first is the optimal split error , i.e.  we classify all pairs according to equation  after determining the optimal split point @xmath7  and we determine the classification error rate .",
    "a second performance metric is the jensen - shannon ( js ) divergence .",
    "this is a symmetric distance measure between two probability distributions , related to the  well - known , but asymmetric  kl divergence .",
    "we will use it to capture the difference between the related and non - related pairs distributions , as shown in figure [ fig : mean_distribution ] .",
    "the bigger the js divergence , the greater the difference between the two distributions .    in our experiments",
    "we will use google s _ word2vec _ software to calculate word embeddings .",
    "we choose skip - gram with negative sampling as the learning algorithm , using a context window of five words and 400 dimensions .",
    "we feed an entire cleaned english wikipedia dump of march 4th 2015 to the algorithm , after which we arrive at a total vocabulary size of 2.2 million words .",
    "since we also need document frequencies , we calculate these for each of the vocabulary words using the same wikipedia dump .    in previous work we showed that using an euclidean distance function leads to a much better separation between related and non - related pairs than the more often used cosine distance ,",
    "so we will also use the euclidean distance throughout our experiments here ( @xcite ) .",
    "calculating the gradient of this distance function  which is used in equation  is straightforward .    to obtain the results hereafter , we use the following procedure .",
    "we use the train set to train the weights @xmath38 in equation .",
    "the validation set is used to determine the optimal split point @xmath7 in equation .",
    "finally predictions and evaluations are made on the test set . in the next two subsections",
    "we discuss the results on the wikipedia and twitter datasets .",
    "( ) at ( 0,0 ) ;    ( bo ) at ( 0.1,0.0 ) @xmath33 ; ( b1 ) at ( 1.75,0.0 ) @xmath64 ; ( b2 ) at ( 3.5,0.0 ) @xmath65 ; ( b3 ) at ( 5.2,0.0 ) @xmath66 ; ( 0.1,0.0 ) ",
    "node[below=12pt ] weight index @xmath13 ( 6.95,0.0 ) ; ( 0.0 , 0.1 )  node[left=3pt ] ( 0.0 , 5.3 ) ; ( lo ) at ( 0.0,0.1 ) @xmath32 ; ( l1 ) at ( 0.0,5.3 ) @xmath33 ;    [ cols=\"<,^,^,^,^,^,^ \" , ]",
    "we devised an effective method to derive vector representations for very short fragments of text . for this purpose we learned to weigh word embeddings based on their idf value , using both a contrastive - based loss function and a novel median - based loss function that can effectively mitigate the effect of outliers .",
    "our method is applicable to texts of a fixed length , but can easily be extended to texts of a variable length through subsampling and linear interpolation of the learned weights .",
    "our method can be applied out - of - the - box , that is , there is no need to retrain the model when using different types of word embeddings .",
    "we showed that our method outperforms widely - used baselines that naively combine word embeddings into a text representation , using both toy wikipedia and real - word twitter data .",
    "all code for this paper is readily available on our github page github.com/cedricdeboom/representationlearning .",
    "this work is soon to be published in pattern recognition letters .",
    "cedric de boom is funded by a ph.d .",
    "grant of the flanders research foundation ( fwo ) .",
    "steven van canneyt is funded by a ph.d .",
    "grant of the agency for innovation by science and technology in flanders ( iwt ) .",
    "we acknowledge nvidia for its generous hardware support .",
    "29 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , , . , .",
    ", , a. , in : .",
    ", , , . , .",
    ", , , , , . , .",
    ", , . . , ,",
    ", , . , . , ,"
  ],
  "abstract_text": [
    "<S> short text messages such as tweets are very noisy and sparse in their use of vocabulary . </S>",
    "<S> traditional textual representations , such as tf - idf , have difficulty grasping the semantic meaning of such texts , which is important in applications such as event detection , opinion mining , news recommendation , etc . </S>",
    "<S> we constructed a method based on semantic word embeddings and frequency information to arrive at low - dimensional representations for short texts designed to capture semantic similarity . </S>",
    "<S> for this purpose we designed a weight - based model and a learning procedure based on a novel median - based loss function . </S>",
    "<S> this paper discusses the details of our model and the optimization methods , together with the experimental results on both wikipedia and twitter data . </S>",
    "<S> we find that our method outperforms the baseline approaches in the experiments , and that it generalizes well on different word embeddings without retraining . </S>",
    "<S> our method is therefore capable of retaining most of the semantic information in the text , and is applicable out - of - the - box .    68p20,68t50,68t01 information storage and retrieval , natural language processing , artificial intelligence </S>"
  ]
}