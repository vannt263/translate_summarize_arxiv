{
  "article_text": [
    "the celebrated paper by slepian and wolf @xcite has ignited a long  lasting , intensive research activity on separate source coding and joint decoding of correlated sources , during the last four decades .",
    "besides its extensions in many directions , some of the more recent studies have been devoted to further refinements of performance analysis , such as exponential bounds on the decoding error probability . in particular , gallager @xcite derived a lower bound on the achievable random coding error exponent pertaining to random binning ( henceforth , random binning exponent ) , using a technique that is very similar to that of his derivation of the ordinary random coding error exponent ( * ? ? ?",
    "* sections 5.55.6 ) .",
    "this random binning exponent was later shown by csiszr , krner and marton @xcite , @xcite to be universally achievable .",
    "the work of csiszr and krner @xcite is about a universally achievable error exponent using linear codes as well as a non ",
    "universal , expurgated exponent which is improved at high rates .",
    "more recently , csiszr @xcite and oohama and han @xcite have derived error exponents for the more general setting of coded side information . for large rates at one of the encoders , kelly and wagner @xcite improved upon these results , but they did not consider the general case .    since slepian ",
    "wolf decoding is essentially an instance of channel decoding , we find it natural to examine its performance also in the framework of generalized channel decoders , that is , decoders with an erasure / list option .",
    "accordingly , this paper is about the analysis of random binning exponents associated with generalized decoders .",
    "it should be pointed out that error exponents for list decoders of the slepian  wolf encoders were already analyzed in @xcite , but in that work , it was assumed that the list size is fixed ( independent of the block length ) and deterministic . in this paper , on the hand , we analyze achievable trade - offs between random binning exponents associated with erasure / list decoders in the framework similar to that of forney @xcite .",
    "this means , among other things , that the erasure and list options are treated jointly , on the same footing , using an optimum decision rule of a common form , and that in the list option , the list size is a random variable whose typical value might be exponentially large in the block length .",
    "the erasure option allows the decoder not to decode when the confidence level is not satisfactory .",
    "it can be motivated , for example , by the possibility of generating a rate - less slepian  wolf code ( see also @xcite ) , provided that there is at least some minimum amount of feedback .",
    "we analyze random binning error exponents associated with erasure / list slepian ",
    "wolf decoding using two different methods and then compare the resulting bounds .",
    "the first method follows the well known techniques of gallager @xcite and forney @xcite , whereas the second method is based on a technique of distance enumeration , or more generally , on type class enumeration .",
    "this method has already been used in previous work ( see ( * ? ? ?",
    "* chapters 67 ) and references therein ) and proved useful in obtaining bounds on error exponents which are always at least as tight ( and in many cases , strictly tighter ) than those obtained in the traditional methods of the information theory literature .",
    "this technique is rooted in the statistical mechanics of certain models of disordered magnetic materials . while in the case of ordinary random coding , the parallel statistical",
    " mechanical model is the random energy model ( rem ) ( * ? ? ?",
    "* chapters  56 ) , ( * ? ? ?",
    "* chapters 67 ) , here , since random binning is considered , the parallel statistical ",
    "mechanical model is slightly different , but related .",
    "we will refer to this model as the _ random dilution model _ ( rdm ) for reasons that will become apparent in the sequel .",
    "as mentioned in the previous paragraph , the type class enumeration method is guaranteed to yield an exponent function which is at least as tight as that of the classical method .",
    "but it is also demonstrated that for certain combinations of coding rates and thresholds of the erasure / list decoder , the exponent of the type class enumeration method is strictly tighter than that of the ordinary method .",
    "in fact , the gap between them ( i.e. , their ratio ) can be arbitrarily large , and even strictly infinite .",
    "in other words , for a small enough threshold ( pertaining to list decoding ) , the former exponent can be infinite while the latter is finite .    while the above described study is carried out for fixed  rate slepian ",
    "wolf encoding , we also demonstrate how variable  rate encoding ( with a certain structure ) can strictly improve on the random binning exponents .",
    "this is shown in the context of the exponents derived using the forney / gallager method , but a similar generalization can be carried out using the other method .",
    "the outline of the paper is as follows . in section 2 ,",
    "we provide notation conventions and define the objectives of the paper more formally . in section 3 , we derive the random binning exponents using the forney / gallager method , and in section 4 , we extend this analysis to allow variable rate coding . finally , in section 5 , after a short background on the relevant statistical  mechanical model ( subsection 5.1 ) , we use the type class enumeration technique , first in the binary case ( subsection 5.2 ) , then compare the resulting exponents to those of section 3 ( subsection 5.3 ) , and finally , generalize the analysis to a general pair of correlated finite alphabet memoryless sources ( subsection 5.4 ) .",
    "throughout the paper , random variables will be denoted by capital letters , specific values they may take will be denoted by the corresponding lower case letters , and their alphabets will be denoted by calligraphic letters .",
    "random vectors and their realizations will be denoted , respectively , by capital letters and the corresponding lower case letters , both in the bold face font .",
    "their alphabets will be superscripted by their dimensions .",
    "for example , the random vector @xmath0 , ( @xmath1  positive integer ) may take a specific vector value @xmath2 in @xmath3 , the @xmath1th order cartesian power of @xmath4 , which is the alphabet of each component of this vector .",
    "for a given vector @xmath5 , let @xmath6 denote the empirical distribution , that is , the vector @xmath7 , where @xmath8 is the relative frequency of the letter @xmath9 in the vector @xmath5 .",
    "let @xmath10 denote its type class of @xmath5 , namely , the set @xmath11 .",
    "the empirical entropy associated with @xmath5 , denoted @xmath12 , is the entropy associated with the empirical distribution @xmath6 .",
    "similarly , for a pair of vectors @xmath13 , the empirical joint distribution @xmath14 is the matrix @xmath15 of relative frequencies of symbol pairs @xmath16 .",
    "the conditional type class @xmath17 is the set @xmath18 .",
    "the empirical conditional entropy of @xmath5 given @xmath19 , denoted @xmath20 , is the conditional entropy of @xmath21 given @xmath22 , associated with the joint empirical distribution @xmath23 .",
    "the expectation operator will be denoted by @xmath24 .",
    "logarithms and exponents will be understood to be taken to the natural base unless specified otherwise .",
    "the indicator function will be denoted by @xmath25 .",
    "the notation function @xmath26_+$ ] will be defined as @xmath27 . for two positive sequences , @xmath28 and @xmath29",
    ", the notation @xmath30 will mean asymptotic equivalence in the exponential scale , that is , @xmath31 .",
    "similarly , @xmath32 will mean @xmath33 , and so on .",
    "let @xmath34 be @xmath1 independent copies of a random vector @xmath35 , distributed according to a given probability mass function @xmath36 , where @xmath9 and @xmath37 take on values in finite alphabets , @xmath4 and @xmath38 , respectively .",
    "the source vector @xmath2 , which is a generic realization of @xmath0 , is compressed at the encoder by random binning , that is , each @xmath1tuple @xmath39 is randomly and independently assigned to one out of @xmath40 bins , where @xmath41 is the coding rate in nats per symbol . given a realization of the random partitioning into bins ( revealed to both the encoder and the decoder ) ,",
    "let @xmath42 denote the encoding function , i.e. , @xmath43 is the encoder output . accordingly , the inverse image of @xmath44 , defined as @xmath45 , is the bin of all source vectors mapped by the encoder into @xmath44 . the decoder has access to @xmath44 and to @xmath46 , which is a realization of @xmath47 , namely , the side information at the decoder .",
    "following @xcite , we consider a decoder with an erasure / list option , defined as follows .",
    "let @xmath48 denote the probability of the event @xmath49 and let @xmath50 be a given real valued parameter .",
    "the decoding rule is as follows . for every @xmath51 ,",
    "if @xmath52 then @xmath53 is referred to as a _",
    "candidate_. if there are no candidates , an erasure is declared , namely , the decoder acts in its erasure mode .",
    "if there is exactly one candidate , @xmath53 , then this is the estimate that the decoder produces , just like in ordinary decoding .",
    "finally , if there is more than one candidate , then the decoder operates in the list mode and it outputs the list of all candidates . obviously , for @xmath54 , the list can contain at most one candidate .",
    "the list may contain two candidates or more only for sufficiently small negative values of @xmath50 .",
    "forney @xcite used the neymann  pearson lemma , in an analogous channel coding setting , to show that the above rule simultaneously gives rise to : ( i ) an optimum trade - off between the probability of erasure and the probability of decoding error , in the erasure mode , and ( ii ) an optimum trade - off between the probability of list error and the expected number of incorrect candidates on the list , in the list mode .",
    "our goal , in this paper , is to assess the exponential rates associated with these trade - offs .",
    "similarly as in @xcite , we define the event @xmath55 as the event that the correct source vector @xmath5 is not a candidate , that is , @xmath56 we next derive a lower bound on the exponential rate @xmath57 of the average probability of @xmath55 , where the averaging is with respect to ( w.r.t . )",
    "the ensemble of random binnings .",
    "the other exponent , @xmath58 ( of decoding error in the erasure option , or the expected list size in list option ) will then be given by @xmath59 , similarly as in @xcite .",
    "we now have the following chain of inequalities for any @xmath60 : @xmath61}{p({\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$ } } ) } > 1\\right\\}\\nonumber\\\\ & \\le&\\sum_{{\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}}}p({\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}})\\left[\\frac{e^{nt}\\sum_{{\\mbox{\\boldmath $ x$}}'\\ne{\\mbox{\\boldmath $ x$ } } } p({\\mbox{\\boldmath $ x$}}',{\\mbox{\\boldmath $ y$}}){{\\cal i}}[f({\\mbox{\\boldmath $ x$}}')=f({\\mbox{\\boldmath $ x$}})]}{p({\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}})}\\right]^s\\nonumber\\\\ & = & e^{nst}\\sum_{{\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}}}p^{1-s}({\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}})\\left[\\sum_{{\\mbox{\\boldmath $ x$}}'\\ne{\\mbox{\\boldmath $ x$ } } } p({\\mbox{\\boldmath $ x$}}',{\\mbox{\\boldmath $ y$}}){{\\cal i}}[f({\\mbox{\\boldmath $ x$}}')=f({\\mbox{\\boldmath $ x$}})]\\right]^s.\\end{aligned}\\ ] ] now , let @xmath62 be another parameter .",
    "then , @xmath63\\right]^{s/\\rho}\\right)^\\rho\\\\ & \\le&e^{nst}\\sum_{{\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}}}p^{1-s}({\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}})\\left(\\sum_{{\\mbox{\\boldmath $ x$}}'\\ne{\\mbox{\\boldmath $ x$ } } } p^{s/\\rho}({\\mbox{\\boldmath $ x$}}',{\\mbox{\\boldmath $ y$}}){{\\cal i}}[f({\\mbox{\\boldmath $ x$}}')=f({\\mbox{\\boldmath $ x$}})]\\right)^\\rho.\\end{aligned}\\ ] ] where we have used the inequality @xmath64 for @xmath65 $ ] .",
    "taking now the expectation w.r.t .",
    "the randomness of the binning , and assuming that @xmath66 , we get @xmath67\\right)^\\rho\\right\\}\\\\ & \\le&e^{nst}\\sum_{{\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}}}p^{1-s}({\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}})\\left(\\sum_{{\\mbox{\\boldmath $ x$}}'\\ne{\\mbox{\\boldmath $ x$ } } } p^{s/\\rho}({\\mbox{\\boldmath $ x$}}',{\\mbox{\\boldmath $ y$}}){\\mbox{\\boldmath $ e$}}\\{{{\\cal i}}[f({\\mbox{\\boldmath $ x$}}')=f({\\mbox{\\boldmath $ x$}})]\\}\\right)^\\rho\\\\ & = & e^{nst}\\sum_{{\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}}}p^{1-s}({\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}})\\left(\\sum_{{\\mbox{\\boldmath $ x$}}'\\ne{\\mbox{\\boldmath $ x$ } } } p^{s/\\rho}({\\mbox{\\boldmath $ x$}}',{\\mbox{\\boldmath $ y$}})e^{-nr}\\right)^\\rho\\\\ & = & e^{-n(\\rho r - st)}\\sum_{{\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}}}p^{1-s}({\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}})\\left(\\sum_{{\\mbox{\\boldmath $ x$}}'\\ne{\\mbox{\\boldmath $ x$ } } } p^{s/\\rho}({\\mbox{\\boldmath $ x$}}',{\\mbox{\\boldmath $ y$}})\\right)^\\rho\\\\ & = & e^{-n(\\rho r - st)}\\sum_{{\\mbox{\\boldmath $ y$}}}p({\\mbox{\\boldmath $ y$}})\\sum_{{\\mbox{\\boldmath $ x$}}}p^{1-s}({\\mbox{\\boldmath $ x$}}|{\\mbox{\\boldmath $ y$}})\\left(\\sum_{{\\mbox{\\boldmath $ x$}}'\\ne{\\mbox{\\boldmath $ x$ } } } p^{s/\\rho}({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}})\\right)^\\rho\\\\ & \\le&e^{-n(\\rho r - st)}\\left[\\sum_{y\\in{{\\cal y}}}p(y)\\sum_{x\\in{{\\cal x}}}p^{1-s}(x|y)\\left(\\sum_{x'\\in{{\\cal x } } } p^{s/\\rho}(x'|y)\\right)^\\rho\\right]^n.\\end{aligned}\\ ] ] thus , after optimization over @xmath68 and @xmath69 , subject to the constraints @xmath70 , we obtain @xmath71 where @xmath72\\ ] ] with @xmath73.\\ ] ] a few elementary properties of the function @xmath57 are the following .    1 .",
    "@xmath57 is jointly convex in both arguments .",
    "this follows directly from the fact that it is given by the supremum over a family of affine functions in @xmath74 .",
    "clearly , @xmath57 is increasing in @xmath41 and decreasing in @xmath50 .",
    "2 .   at @xmath75 ,",
    "the optimum @xmath69 is @xmath76 , similarly as in @xcite and @xcite .",
    "thus , as observed in @xcite , here too , the case @xmath75 is essentially equivalent ( in terms of error exponents ) to ordinary decoding , although operationally , there still might be erasures in this case .",
    "3 .   for a given @xmath50 , the infimum of @xmath41 such that @xmath77 is @xmath78 which is a concave increasing function . at @xmath75 ,",
    "@xmath79 4 .   for a given @xmath41 , the supremum of @xmath50 such that @xmath77 is @xmath80 which is a convex increasing function , the inverse of @xmath81 .",
    "additional properties can be found similarly as in @xcite , but we will not delve into them here .",
    "a possible extension of the above error exponent analysis allows variable rate coding . in this section ,",
    "we demonstrate how the flexibility of variable  rate coding can improve the error exponents .",
    "consider an encoder that first sends a relatively short header that encodes the type class of @xmath5 ( using a logarithmic number of bits ) , and then a description of @xmath5 within its type class , using a random bin @xmath43 in the range @xmath82 - 1\\}$ ] , where @xmath83 depends on @xmath5 only via the type class of @xmath5 .",
    "the bin @xmath44 for every @xmath5 in its type class is selected independently at random with a uniform probability distribution @xmath84 .",
    "the average coding rate would be , of course , @xmath85 ( neglecting the rate of the header ) .",
    "for example , consider an additive rate function in an arbitrary manner , are still manageable , but require the method of types . ] @xmath86 .",
    "thus , @xmath87 . extending the above error exponent analysis , one readily obtains whenever @xmath88 and @xmath89 elsewhere , thus @xmath90 everywhere . ]",
    "@xmath91,\\ ] ] where @xmath92 and where @xmath93 is defined as @xmath94.\\ ] ] it is interesting to find the optimum rate assignment @xmath95 that maximizes the exponent .",
    "consider , for example , the case where @xmath41 and @xmath50 are such that @xmath57 is achieved by @xmath96 .",
    "then , @xmath97 where @xmath98 our task now is to minimize @xmath99 subject to the constraints @xmath100 and @xmath101 for all @xmath102 , which is a standard convex program . for simplicity ,",
    "let us first ignore the constraints @xmath103 , @xmath102 , and assume that the parameters of the problem are such that the resulting solution will satisfy these positivity constraints anyway .",
    "then , @xmath104 where @xmath105 is determined by the average rate constraint , that is @xmath106,\\end{aligned}\\ ] ] where @xmath107 thus , @xmath108 we see that fixed ",
    "rate coding is optimum only if @xmath109 happens to be proportional to @xmath110 , namely , @xmath111 ( which is the case , for example , when @xmath112 ) . upon substituting @xmath113 back into the objective function ,",
    "we obtain @xmath114\\}\\\\ & = & \\sum_{x\\in{{\\cal x}}}p(x)e^{-\\lambda } = e^{-\\lambda},\\end{aligned}\\ ] ] and so , @xmath115\\\\ & = & r+d(p\\|q)-\\ln\\left[\\sum_{y\\in{{\\cal y}}}p(y)\\sum_{x\\in{{\\cal x}}}p^{1-s}(x|y)\\sum_{x'\\in{{\\cal x}}}p^s(x'|y)\\right]\\\\ & = & e_0(1,s)+d(p\\|q).\\end{aligned}\\ ] ] the term @xmath116 then represents the improvement we have obtained upon passing from fixed  rate coding to variable  rate coding with an additive rate function .",
    "this is true for a given @xmath69 .",
    "however , after re  optimizing the bound over @xmath69 , the improvement can be even larger .",
    "when @xmath117 $ ] are not all positive , the optimum solution is given by @xmath118_+\\ ] ] where @xmath119 is the ( unique ) solution to the equation @xmath120_+=r.\\ ] ] for @xmath121 , the optimization over @xmath122 is less trivial , but it can still be carried out at least numerically .",
    "this subsection can be skipped without essential loss of continuity , however , we believe that before getting into the detailed technical derivation , it would be instructive to give a brief review of the statistical  mechanical models that are at the basis of the type class enumeration method .    in ordinary random coding ( as opposed to random binning ) , the derivations of bounds on the error probability ( especially in the methods of gallager and forney ) are frequently associated with expressions of the form @xmath123 , where @xmath124 is ( randomly selected ) codebook and @xmath125 is some parameter . as explained in ( * ? ? ?",
    "6 ) , this can be viewed , from the statistical  mechanical perspective , as a partition function @xmath126 where @xmath127 plays the role of inverse temperature and where the energy function ( hamiltonian ) is @xmath128 .",
    "since the codewords are selected independently at random , then for a given @xmath19 , the energies @xmath129 are i.i.d",
    ".  random variables .",
    "this is , in principle , nothing but the _ random energy model _ ( rem ) , a well known model in statistical mechanics of disordered magnetic materials ( spin glasses ) , which exhibits a phase transition : below a certain critical temperature ( @xmath130 )",
    ", the system freezes in the sense that the partition function is exponentially dominated by a subexponential number of configurations at the ground  state energy ( zero thermodynamical entropy ) .",
    "this phase is called the _ frozen phase _ or the _ glassy phase_. the other phase , @xmath131 , is called the _ paramagnetic phase _ ( see more details in ( * ? ? ?",
    "* chap.5 ) ) .",
    "accordingly , the resulting exponential error bounds associated with random coding ` inherit ' this phase transition ( see @xcite and references therein ) .    in random binning the situation is somewhat different . as we have seen in section 3 , here the bound involves an expression like @xmath132 $ ] .",
    "the source vectors @xmath133 that participate in the summation are now deterministic , but the random ingredient is the function @xmath134 . the analogous statistical  mechanical model is then encoded into the partition function @xmath135 where @xmath136 are i.i.d .  binary random variables , taking on values in @xmath137 , where @xmath138 . in other words",
    ", @xmath139 is a randomly diluted version of the full partition function @xmath140 , where each configuration @xmath5 ` survives ' with probability @xmath141 or is discarded with probability @xmath142 .",
    "accordingly , we refer to this model as the _ random dilution model _ ( rdm ) . to the best of our knowledge",
    ", such a model has not been used in statistical mechanics thus far , but it can be analyzed in the very same fashion , and it is easy to see that it also exhibits a glassy phase transition ( depending on @xmath41 ) .",
    "in fact , the rdm can be considered as a variant of the rem , where the configurational energies are @xmath143 , where @xmath144 with probability @xmath141 and @xmath145 with probability @xmath142 .",
    "thus , @xmath146 can be thought of as disordered potential function , associated with long  range interactions , with infinite spikes that forbid access to certain points in the configuration space .",
    "let us return to the fixed  rate regime .",
    "it is instructive to begin from the relatively simple special case where @xmath147 and @xmath148 are correlated binary symmetric sources ( bss s ) , that is , @xmath149 we begin similarly as in section 3 : our starting point is the same bound as in the last line of eq .",
    "( [ beginning ] ) , specialized to the binary case considered here , where we also take the ensemble average : @xmath150\\right]^s\\right\\}\\\\ & = & e^{nst}\\sum_{{\\mbox{\\boldmath $ y$}}}p({\\mbox{\\boldmath $ y$}})\\left[\\sum_{{\\mbox{\\boldmath $ x$}}}p^{1-s}({\\mbox{\\boldmath $ x$}}|{\\mbox{\\boldmath $ y$}})\\right ] \\cdot{\\mbox{\\boldmath $ e$}}\\left\\{\\left[\\sum_{{\\mbox{\\boldmath $ x$}}'\\ne{\\mbox{\\boldmath $ x$}}}p({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}}){{\\cal i}}[f({\\mbox{\\boldmath $ x$}}')=f({\\mbox{\\boldmath $ x$}})]\\right]^s\\right\\}\\\\ & = & e^{nst}\\sum_{{\\mbox{\\boldmath $ y$}}}2^{-n}\\left[p^{1-s}+(1-p)^{1-s}\\right]^n \\cdot{\\mbox{\\boldmath $ e$}}\\left\\{\\left[\\sum_{{\\mbox{\\boldmath $ x$}}'\\ne{\\mbox{\\boldmath $ x$}}}p({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}}){{\\cal i}}[f({\\mbox{\\boldmath $ x$}}')=f({\\mbox{\\boldmath $ x$}})]\\right]^s\\right\\}\\\\ & = & e^{nst}\\left[p^{1-s}+(1-p)^{1-s}\\right]^n \\cdot{\\mbox{\\boldmath $ e$}}\\left\\{\\left[\\sum_{{\\mbox{\\boldmath $ x$}}'\\ne{\\mbox{\\boldmath $ x$}}}p({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}}){{\\cal i}}[f({\\mbox{\\boldmath $ x$}}')=f({\\mbox{\\boldmath $ x$}})]\\right]^s\\right\\}\\end{aligned}\\ ] ] where the last step is justified by the fact that the expectation term is independent of @xmath19 , as will be seen shortly .",
    "now , @xmath151\\right]^s\\right\\ } & { \\stackrel{\\cdot } { = } } & \\sum_{{{\\cal t}}({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}})}p^s({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}}){\\mbox{\\boldmath $ e$}}\\{n^s({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}})\\}\\\\ & = & ( 1-p)^{ns}\\sum_{\\delta}\\left(\\frac{p}{1-p}\\right)^{ns\\delta}{\\mbox{\\boldmath $ e$}}\\{n^s({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}})\\}\\end{aligned}\\ ] ] where @xmath152 is the normalized hamming distance , the summation is over the set @xmath153 , and @xmath154 $ ] . now , @xmath155 is the sum of @xmath156 i.i.d .  binary random variables @xmath157\\}$ ] with @xmath158",
    "thus , similarly as in ( * ? ? ?",
    "6.3 ) @xmath159\\ } & h(\\delta)\\ge r\\\\ \\exp\\{n[h(\\delta)-r]\\ } & h(\\delta ) < r \\end{array}\\right.\\\\ & = & \\exp\\{n(s[h(\\delta)-r]-(1-s)[r - h(\\delta)]_+)\\}\\end{aligned}\\ ] ] and so @xmath160^n(1-p)^{ns}\\sum_\\delta \\left(\\frac{p}{1-p}\\right)^{ns\\delta}\\times\\nonumber\\\\ & & \\exp\\{n(s[h(\\delta)-r]-(1-s)[r - h(\\delta)]_+)\\}\\\\ & { \\stackrel{\\cdot } { = } } & e^{nst}\\left[p^{1-s}+(1-p)^{1-s}\\right]^n(1-p)^{ns}e^{-nl(r , s)}\\end{aligned}\\ ] ] where @xmath161 with @xmath162+(1-s)[r - h(\\delta)]_+.\\ ] ] standard optimization of @xmath163 gives the following result ( see appendix a for the details ) . define the sets ( see also fig .",
    "[ swepd ] ) @xmath164 then , @xmath165 & ( s , r)\\in c\\cup f\\cup g\\\\ sh^{-1}(r)\\ln\\frac{1-p}{p } & ( s , r)\\in",
    "b\\\\ sp_s\\ln\\frac{1-p}{p}+r - h(p_s ) & ( s , r)\\in a\\cup d\\cup e \\end{array}\\right.\\ ] ] finally , the exponent of @xmath166 is lower bounded by @xmath167-st\\right\\}.\\ ] ] equivalently , @xmath168 can be presented as follows : @xmath169 where @xmath170 & ( s , r)\\in c\\cup f\\cup g\\\\ s[r - t+d(h^{-1}(r)\\|p)]-\\ln[p^{1-s}+(1-p)^{1-s } ] & ( s , r)\\in b\\\\ r - st-\\ln[p^s+(1-p)^s]-\\ln[p^{1-s}+(1-p)^{1-s } ] & ( s , r)\\in a\\cup d\\cup e \\end{array}\\right.\\ ] ]    fig .",
    "1 depicts a phase diagram of the function @xmath171 .",
    "this function inherits phase transitions associated with the analogous statistical  mechanical model  the rdm .",
    "the strip defined by @xmath60 and @xmath172 is divided into seven regions , labeled by the letters a  g as defined above .",
    "there are three main phases that are separated by solid lines , which differ in terms of the expression of @xmath171 .",
    "the phase @xmath173 is the phase where typical realiztions of the random binning ensemble dominate the partition function ( that is , conditional type classes of size less than @xmath174 contain no matching bin , whereas conditional type classes of larger size have an exponentially typical number of bin matches ) , phase @xmath175 is the glassy phase , and phase @xmath176 is the phase where the conditional small type classes dominate the partition function ( unlike in phase @xmath173 ) . a secondary partition into sub ",
    "phases ( dashed lines ) correspond to different shapes of the objective function @xmath163 . in regions",
    "@xmath177 , @xmath175 , @xmath178 ( @xmath179 ) , the derivative of the objective function has a positive jump at @xmath180 , and the minimizer is smaller than @xmath181 , equal to @xmath181 , and larger than @xmath181 , respectively . in regions",
    "@xmath182 , @xmath183 , @xmath184 and @xmath185 ( @xmath186 ) , the derivative of @xmath163 w.r.t .",
    "@xmath152 has a negative jump at @xmath180 , in regions @xmath183 and @xmath184 , this jump is from a positive derivative to a negative derivative , meaning that @xmath180 is a ( non  smooth ) local maximum and there are two local minima , one at @xmath187 and one at @xmath188 . in region @xmath183 , the local minimum at @xmath189 is smaller than the local minimum at @xmath190 and in region @xmath184 it is vice versa . in region",
    "@xmath185 there is only one local minimum at @xmath190 and in region @xmath182 there is only one local minimum at @xmath189 .      the expression of @xmath168 should be compared with @xmath57 specialized to the double bss considered in subsection 5.2 , i.e. , @xmath191-\\rho\\ln[p^{s/\\rho}+(1-p)^{s/\\rho}]-st\\right\\}.\\ ] ] obviously , @xmath192 since derivation of @xmath168 is guaranteed to be exponentially tight starting from ( [ beginning ] ) , in contrast to the derivation of @xmath57 , which is associated with jensen s inequality , as well as the inequality @xmath193 , @xmath194 , following @xcite .    to show an extreme situation of a strict inequality , @xmath195 , consider the case where @xmath196 and @xmath197 < 0 $ ] ( a list option ) .",
    "then , @xmath198^s\\right)\\right]-\\right.\\nonumber\\\\ & & \\left.\\ln\\left[p^{1-s}\\left(1+\\left[\\frac{1-p}{p}\\right]^{1-s}\\right)\\right]\\right\\}\\\\ & = & \\lim_{s\\to\\infty}\\left\\{r - st - s\\ln(1-p)- \\ln\\left(1+\\left[\\frac{p}{1-p}\\right]^s\\right)-(1-s)\\ln p-\\right.\\nonumber\\\\ & & \\left.\\ln\\left(1+\\left[\\frac{p}{1-p}\\right]^{s-1}\\right)\\right\\}\\\\ & = & \\lim_{s\\to\\infty}\\left\\{r - st - s\\ln(1-p ) -(1-s)\\ln p\\right\\}\\\\ & = & \\ln\\frac{1}{p}+r+\\lim_{s\\to\\infty}s\\left[\\ln\\frac{p}{1-p}-t\\right]\\\\ & = & \\infty.\\end{aligned}\\ ] ] on the other hand , in this case , @xmath199\\}\\\\ & = & r+|t| < \\infty.\\end{aligned}\\ ] ]    another situation , where it is relatively easy to calculate the exponents is the limit of very weak correlation between the bss s @xmath21 and @xmath22 ( in analogy to the notion of a very noisy channel ( * ? ? ?",
    "* , example 3 ) ) .",
    "let @xmath200 for @xmath201 . in this case , a second order taylor series expansion of the relevant functions ( see appendix b for the details ) yields ,",
    "for @xmath202 and @xmath203 , with @xmath204 being fixed : @xmath205 whereas @xmath206\\epsilon^2.\\ ] ] now , observe that the upper bound on @xmath57 is affine in @xmath207 , whereas the lower bound on @xmath168 is quadratic in @xmath207 , thus the ratio @xmath208 can be made arbitrarily large for any sufficiently large @xmath204 .    in both examples , we took advantage of the fact that the range of optimization of @xmath69 for @xmath168 includes all the positive reals , whereas for @xmath57 , it is limited to the interval @xmath209 $ ] due to the combination of using of jensen s inequality ( which requires @xmath66 ) and the inequality @xmath193 ( which requires @xmath210 ) .",
    "note that the second example is not a special case the first one , because in the first example , for @xmath200 , @xmath211=o(\\epsilon)$ ] , whereas in the second example , @xmath212 .      in this subsection",
    ", we use the type class enumeration method for general finite alphabet sources @xmath213 and @xmath22 . consider the expression @xmath214\\right]^s\\right\\}\\ ] ] that appears upon taking the expectation over the last line of ( [ beginning ] ) .",
    "then , we have @xmath215\\right]^s\\right\\}\\\\ & = & p^s({\\mbox{\\boldmath $ y$}}){\\mbox{\\boldmath $ e$}}\\left\\{\\left[\\sum_{{\\mbox{\\boldmath $ x$}}'\\ne{\\mbox{\\boldmath $ x$ } } } p({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}}){{\\cal i}}[f({\\mbox{\\boldmath $ x$}}')=f({\\mbox{\\boldmath $ x$}})]\\right]^s\\right\\}\\\\ & \\le&p^s({\\mbox{\\boldmath $ y$}})\\sum_{{{\\cal",
    "t}}({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}})}p^s({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}}){\\mbox{\\boldmath $ e$}}\\left\\{\\left[\\sum_{\\tilde{{\\mbox{\\boldmath $ x$}}}\\in { { \\cal t}}({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$ } } ) } { { \\cal i}}[f(\\tilde{{\\mbox{\\boldmath",
    "$ x$}}})=f({\\mbox{\\boldmath $ x$}})]\\right]^s\\right\\}\\\\ & { \\stackrel{\\delta } { = } } & p^s({\\mbox{\\boldmath $ y$}})\\sum_{{{\\cal t}}({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}})}p^s({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}}){\\mbox{\\boldmath $ e$}}\\left\\{n^s({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ x$}},{\\mbox{\\boldmath $ y$}})\\right\\}\\end{aligned}\\ ] ] where @xmath155 is the ( random ) number of @xmath216 in @xmath217 which belong to the same bin as @xmath5 .",
    "now , @xmath218\\ } & \\hat{h}_{{\\mbox{\\boldmath $ x$}}'{\\mbox{\\boldmath $ y$}}}(x|y)>r\\\\ \\exp\\{n[\\hat{h}_{{\\mbox{\\boldmath $ x$}}'{\\mbox{\\boldmath $ y$}}}(x|y)-r]\\ } & \\hat{h}_{{\\mbox{\\boldmath $ x$}}'{\\mbox{\\boldmath $ y$}}}(x|y)\\le r\\end{array}\\right.\\\\ & = & \\exp\\{n(s[\\hat{h}_{{\\mbox{\\boldmath $ x$}}'{\\mbox{\\boldmath $ y$}}}(x|y)-r]-(1-s)[r-\\hat{h}_{{\\mbox{\\boldmath $ x$}}'{\\mbox{\\boldmath $ y$}}}(x|y)]_+)\\},\\end{aligned}\\ ] ] thus , @xmath215\\right]^s\\right\\}\\\\ & { \\stackrel{\\cdot } { = } } & p^s({\\mbox{\\boldmath $ y$}})\\sum_{{{\\cal t}}({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}})}p^s({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}})\\exp\\{n(s[\\hat{h}_{{\\mbox{\\boldmath $ x$}}'{\\mbox{\\boldmath $ y$}}}(x|y)- r]-(1-s)[r-\\hat{h}_{{\\mbox{\\boldmath $ x$}}'{\\mbox{\\boldmath $ y$}}}(x|y)]_+)\\}\\\\ & = & p^s({\\mbox{\\boldmath",
    "$ y$}})\\sum_{{{\\cal t}}({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}})}p^s({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}})\\exp\\{n(s[\\hat{h}_{{\\mbox{\\boldmath $ x$}}'{\\mbox{\\boldmath $ y$}}}(x|y)- r]-(1-s)[r-\\hat{h}_{{\\mbox{\\boldmath $ x$}}'{\\mbox{\\boldmath $ y$}}}(x|y)]_+)\\}\\\\ & = & p^s({\\mbox{\\boldmath $ y$}})\\sum_{{{\\cal t}}({\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}})}\\exp\\{-n(s[d(\\hat{p}_{{\\mbox{\\boldmath $ x$}}'|{\\mbox{\\boldmath $ y$}}}\\|p_{x|y}|\\hat{p}_{{\\mbox{\\boldmath $ y$}}})+r]+ ( 1-s)[r-\\hat{h}_{{\\mbox{\\boldmath $ x$}}'{\\mbox{\\boldmath $ y$}}}(x|y)]_+)\\}\\\\ & { \\stackrel{\\cdot } { = } } & p^s({\\mbox{\\boldmath $ y$}})\\exp\\left\\{-n\\min_{p_{x'|y}}(s[d(p_{x'|y}\\|p_{x|y}|\\hat{p}_{{\\mbox{\\boldmath $ y$}}})+r]+ ( 1-s)[r - h(x'|y)]_+)\\right\\}\\\\ & { \\stackrel{\\delta } { = } } & p^s({\\mbox{\\boldmath $ y$}})e^{-nl(\\hat{p}_{{\\mbox{\\boldmath $ y$}}},r , s)},\\end{aligned}\\ ] ] where @xmath219 is the empirical conditional distribution of a random variable @xmath220 given @xmath22 induced by @xmath221 , and @xmath222 is defined as @xmath223 consequently , @xmath224 where @xmath225-st.\\ ] ] finally , @xmath226",
    "* calculation of @xmath171 . *",
    "let @xmath227 $ ] .",
    "consider first the case @xmath228 $ ] , where @xmath229 . in this case , the minimizer @xmath230 that achieves @xmath171 is given by @xmath231 here , for @xmath232 , the derivative of the objective function vanishes only at @xmath233 , where the term @xmath234_+$ ] vanishes . on the other hand , for @xmath235 ,",
    "the derivative vanishes only at @xmath236 , where the term @xmath234_+$ ] is active . in the intermediate range , the derivative jumps from a negative value to a positive value at @xmath180 discontinuously , hence it is a minimum .",
    "thus , for @xmath237 , we have : @xmath165 & r < h(p)\\\\ sh^{-1}(r)\\ln\\frac{1-p}{p } & h(p)\\le r < h(p_s)\\\\ sp_s\\ln\\frac{1-p}{p}+r - h(p_s ) & r\\ge h(p_s)\\end{array}\\right.\\ ] ] for @xmath238 , @xmath239 . and so @xmath240 .",
    "here , for @xmath241 , which means also @xmath232 , the derivative vanishes only at @xmath242 . on the other hand , for @xmath243 ,",
    "the derivative vanishes only at @xmath244 . in the intermediate range , @xmath245 ,",
    "the derivative vanishes both at @xmath190 and @xmath189 , so the minimum is the smaller between the two .",
    "namely , it is @xmath246 if @xmath247+(1-s)[r - h(p_s)]_+\\le sp\\ln\\frac{1-p}{p}+s[r - h(p)]+(1-s)[r - h(p)]_+\\ ] ] or equivalently , @xmath248,\\ ] ] and it is @xmath249 otherwise .",
    "the choice between the two depends on @xmath41 .",
    "let @xmath250+sh(p_s)-h(p)}{s-1 } = -\\frac{\\ln[p^s+(1-p)^s]}{s-1}\\ ] ] then , for @xmath251 , @xmath165 & r < r(s)\\\\ sp_s\\ln\\frac{1-p}{p}+r - h(p_s ) & r\\ge r(s)\\end{array}\\right.\\ ] ]",
    "* calculations of error exponents for very weakly correlated bss s . * for @xmath200",
    ", we have , to the second order in @xmath252 , @xmath253 . consider the range of rates @xmath254 .",
    "a second order taylor series expansion of @xmath255 $ ] around @xmath256 ( for fixed @xmath257 ) gives @xmath258 and so , @xmath259+(s-\\rho)\\left(\\ln",
    "2- \\frac{2s\\epsilon^2}{\\rho}\\right)\\\\ & = & 4s\\epsilon^2 - 2s^2\\left(1+\\frac{1}{\\rho}\\right)\\epsilon^2-\\rho\\ln 2.\\end{aligned}\\ ] ] now , @xmath260.\\ ] ] we will find it convenient to present @xmath261 , where @xmath262 $ ] , and so , from here on , the rate is parametrized by @xmath263 . the maximization over @xmath62 , for a given @xmath69 , is readily found to give @xmath264 on substituting @xmath265 , we get @xmath266\\\\ & = & \\max_{0\\le s\\le 1}\\left[s(4\\epsilon^2-t)-s|\\epsilon|\\sqrt{2(\\ln 2-r)}-2s^2\\epsilon^2 - 2s|\\epsilon|\\sqrt{\\frac{\\ln 2-r}{2}}\\right]\\\\ & = & \\max_{0\\le s\\le 1}\\{s[4\\epsilon^2-t-2|\\epsilon|\\sqrt{2(\\ln 2-r)}]-2s^2\\epsilon^2\\}\\\\ & = & \\max_{0\\le s\\le 1}\\{s[4\\epsilon^2(1-\\theta)-t ] -2s^2\\epsilon^2\\}\\end{aligned}\\ ] ] where the inequality is because when we maximized over @xmath68 , we have ignored the constraint @xmath66 .",
    "next , let @xmath267 for @xmath268 , then @xmath269 and so , @xmath270 on the other hand , @xmath271\\\\ & = & \\sup_{s\\ge 1}[s(4\\epsilon^2-t)-4s^2\\epsilon^2]+r-\\ln 2\\\\ & = & \\sup_{s\\ge 1}[s(4\\epsilon^2-t)-4s^2\\epsilon^2]-2\\theta^2\\epsilon^2\\\\ & = & \\frac{(4\\epsilon^2-t)^2}{16\\epsilon^2}-2\\theta^2\\epsilon^2\\\\ & \\ge&\\frac{[(\\tau+4)\\epsilon^2]^2}{16\\epsilon^2}-2\\epsilon^2\\\\ & = & \\left[\\frac{\\tau(\\tau+8)}{16}-1\\right]\\epsilon^2.\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> we analyze random coding error exponents associated with erasure / list slepian  </S>",
    "<S> wolf decoding using two different methods and then compare the resulting bounds . </S>",
    "<S> the first method follows the well known techniques of gallager and forney and the second method is based on a technique of distance enumeration , or more generally , type class enumeration , which is rooted in the statistical mechanics of a disordered system that is related to the random energy model ( rem ) . </S>",
    "<S> the second method is guaranteed to yield exponent functions which are at least as tight as those of the first method , and it is demonstrated that for certain combinations of coding rates and thresholds , the bounds of the second method are strictly tighter than those of the first method , by an arbitrarily large factor . </S>",
    "<S> in fact , the second method may even yield an infinite exponent at regions where the first method gives finite values . </S>",
    "<S> we also discuss the option of variable  rate slepian  </S>",
    "<S> wolf encoding and demonstrate how it can improve on the resulting exponents . </S>",
    "<S> + * index terms : * slepian  wolf coding , error exponents , erasure / list decoding , phase transitions .    </S>",
    "<S> department of electrical engineering + technion - israel institute of technology + technion city , haifa 32000 , israel + e  mail : merhav@ee.technion.ac.il + </S>"
  ]
}