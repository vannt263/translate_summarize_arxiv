{
  "article_text": [
    "in recent years , decision support and data mining systems have became very important for analyzing very large databases . yet these systems have limited functionality and lack of some important features .",
    "one of the very important features that these systems lack is the ability to detect and report interesting finding ( e.g. , exceptions ) .",
    "moreover , these systems can not directly be applied on hidden web databases in a time where these databases could have a rich and valuable content that interests various third parties .    in this paper , we develop a novel technique for discovering @xmath0 of aggregate queries , e.g. , avg , sum , over @xmath1 hidden web databases .",
    "* change detection : * monitor and report changes in databases is very important for decision support and data mining systems .",
    "these systems benefit from executing aggregate queries ( e.g. , sum , avg ) on very large databases to detect and report interesting finding , which can often be very expensive and resource intensive .",
    "many systems use pre - computation of aggregates to improve response time .",
    "although they do not calculate all aggregates ( seen as view selection problem ) and relay on a predetermined number of queries .",
    "an examples of such systems is the online analytical processing ( olap ) .",
    "* hidden web databases : * the idea of hidden web databases is that they are @xmath2 behind restrictive search interfaces .",
    "these databases are not accessible through the traditional search engines , and the only way to access them is by submitting desired values for one or more attributes ( to form a conjunctive search query ) and get a small number ( bounded by a constant @xmath3 which can be 50 or 100 ) of tuples that match the specified query .",
    "this type of databases is very common on the internet , and some examples of such databases include amazon.com , yahoo !",
    "autos , ebay.com , etc .",
    "* problem motivation : * hidden web databases contain a rich content that interests various third parties , such as government agencies , and academic and commercial sectors .",
    "these parties could benefit from the ability to monitor a wide variety of aggregate queries and report interesting finding , since these aggregates are the most common type of queries in decision support systems .",
    "for example , durring black friday / cyber monday of 2014 , amazon announced a huge discount on their new unlocked amazon fire smartphone from $ 499 to $ 199 ( a $ 250 price drop ) .",
    "this announcement was resulted by a sharp drop in the average price of the same smartphone on the other websites ( i.e. , the average price on ebay for all amazon fire smartphones droped from $ 310 to $ 258 ) .",
    "more generally , when the avg price for a certain product that listed on a sale database increases rapidly , this may indicate an increase in demand for this product .",
    "similarly , when the avg salary offered on job database that require a certain skill increases quickly , this may indicate an expansion of the corresponding market .",
    "* challenges : * discovering exceptions of aggregate queries on hidden web databases has two critical challenges :    the challenge of hidden web databases in general ; and    the challenge of dynamic aggregate estimation .    1 .",
    "[ itm : challenge of hidden web databases ] challenge of hidden web databases : most of real - world hidden web databases do not directly support aggregate queries through their web interfaces .",
    "the only way to answer such aggregates is by combining multiple search queries .",
    "a problem with this solution is that most real - world web databases apply limitation to the number of search queries one can issue through per - ip and per - developer ( i.e. , ebay limits api calls to 5,000 per day ) .",
    "prior work @xcite discussed such way to estimate aggregate queries for static databases ( i.e. , assuming databases do not change overtime ) .",
    "this is unreasonable assumption , since in reality , most real - world web databases are frequently updated .",
    "an improved technique has been introduced by @xcite to solve the limitation problem over dynamic databases .",
    "however , their solution only consider one single aggregate at a time . to discover exceptions over a dynamic database ,",
    "we need to monitor a large group of aggregates , which make solving the limitation problem more challenging .",
    "challenge of dynamic aggregate estimation : prior work ( e.g. , @xcite ) has introduced techniques to overcome ( [ itm : challenge of hidden web databases ] ) up to a certain level and estimates aggregates over dynamic hidden web databases",
    ". their existing algorithm applies a random walk technique and distributes the query budget available for reissuing ( i.e. , updating ) previous drill downs , and the rest for initiating new drill downs to track and estimate various types of aggregates .",
    "this technique wastes a lot of queries by producing independent samples , such that each sample can only be used to estimate one candidate once ( e.g. , no information is retained / reused to other aggregate queries ) .",
    "in contrast , applying traditional decision support systems to this problem requires a fully access to the database itself , something that is often not applicable for hidden web databases .",
    "even if we assume that we somehow have a full access to the database , finding exceptions of aggregates constantly requires formidable resources .",
    "previous studies @xcite proposed using precomputed samples of the data ( e.g. , uniform random sampling ) to reduce response time while giving acceptable answers . yet , selecting inappropriate samples for arbitrary aggregates would lead to large estimation errors . therefore , selecting the right samples to give the right estimations and find exceptions while minimizing the query cost becomes a very complicated but important problem .",
    "the problem we consider in this paper is how to track and discover exceptions of aggregates over dynamic hidden web databases .      in this paper , we restrict our discussion to categorical data .",
    "consider a database @xmath4 with @xmath5 tuples @xmath6 , and @xmath7 attributes @xmath8 , ... ,",
    "@xmath9 , each of which has a discrete domain .",
    "the domain of @xmath10 is denoted by @xmath11 for each @xmath12 @xmath13 @xmath14 $ ] , where @xmath15 represents the cardinality of @xmath11 , i.e. , the number of possible values of @xmath10 ( domain values are always known ) .",
    "we assume no duplicate tuple exists in @xmath4 , and each tuple @xmath16 can be represented by a d - dimension vector @xmath17 .",
    "we assume a prototypical interface where users can query the database by specifying values for a subset of attributes .",
    "thus a search query @xmath18 is of the form :    select @xmath19 from @xmath4 where @xmath20 , where @xmath21 is a value from @xmath22 , and @xmath23 is the result of @xmath18 .",
    "let @xmath23 be the set of tuples in @xmath4 that satisfies @xmath18 .",
    "as it is common with most web interfaces , we shall assume that the query interface is restricted to only return @xmath3 tuples , where @xmath24 is a predetermined small constant ( such as 100 or 500 ) .",
    "thus , @xmath23 will be entirely returned iff the number of tuples returned is less than @xmath3 . if the query is too broad ( i.e. , number of tuples returned is more than @xmath3 ) , only the @xmath25 tuples in @xmath23 will be selected according to a ranking function , and returned as the query result . note that repeating the same query @xmath23 may not retrieve new tuples , i.e. , the same @xmath3 tuples may always be returned .",
    "along with @xmath25 tuples , we also assume that the interface returns the total number of tuples satisfying @xmath23 .",
    "let @xmath26 be the total number of tuples returned by the interface , such that @xmath26 can determine the status of @xmath18 ; we say @xmath23 is an overflow when @xmath27 , i.e. , that not all tuples satisfying @xmath18 can be returned . at the other extreme , when @xmath18 is too specific such that @xmath28 , we have an underflow , i.e. , no tuples returned .",
    "if @xmath29 then we have a valid query .    for the purpose of this paper , we assume that a restrictive interface does not allow users to scroll through the complete answer of @xmath23 when @xmath18 overflows .",
    "instead , users must pose new queries by reformulating some of the search conditions .",
    "this is a reasonable assumption since many real - world @xmath25 interfaces ( e.g. , google ) limit page turns ( e.g. , 100 at the time this paper was written ) .",
    "aggregate query @xmath18 for a target attribute @xmath30 with the selection condition @xmath31 takes the form of :    select @xmath32 from @xmath4 where @xmath31    where @xmath33 is an aggregate , such as @xmath34 , @xmath35 , @xmath36 , @xmath37 , and @xmath38 .",
    "the selection condition @xmath31 is a conjunction of @xmath39 , where @xmath40 is a value from @xmath41 .",
    "for example , assume we have two attributes @xmath8 and @xmath42 , both are boolean attributes .",
    "@xmath31 could be a conjunction of @xmath43 and @xmath44 , and we donate the result of this aggregate query as @xmath45 .",
    "now let us say you want to know the average price of the htc cellphones with a five - inch screen .",
    "the aggregate query takes the form of :    select @xmath46 from @xmath47 where @xmath48 and @xmath49      consider an aggregate query , based on its values with fixed time interval at @xmath50 .",
    "we define an aggregate query as an exception by three factors .",
    "firstly , the aggregate which is always over only a few of tuples is unnecessary to consider .",
    "for example , if only one tuple satisfies the selection condition , the change of itself will be the change of the aggregate .",
    "therefore , this aggregate wo nt be as meaningful as others with a lot of tuples , so we set a @xmath51 @xmath52 @xmath53 to filter the small size aggregates .",
    "thus , if @xmath54 , we do not treat its aggregate as a potential exception . secondly ,",
    "if the aggregate is far away from what it was before , we consider it as a potential exception . to measure how",
    "far away \" this aggregate is from what it was before , we compute the distribution of @xmath45 based on historical data",
    ". then we can have an interval @xmath55 , the average of @xmath45 @xmath56 , and the most of @xmath45 belong to @xmath55 .",
    "if @xmath57 is out of @xmath55 , it is a low probability event .",
    "thus , we continue to treat it as a potential exception .",
    "to define the low probability event , we set a @xmath58 @xmath52 @xmath59 , such that a low probability event has a chance of occurrence of less than or equal @xmath59 ( or more than or equal @xmath60 ) .",
    "thirdly , the aggregate change percentage ( increment / decrement ) of last day comparing with previous days must be big enough .",
    "thus , we set a @xmath61 @xmath52 @xmath62 , such that an aggregate with a change percentage for last day is an exception when the change percentage is more than or equal @xmath62 .",
    "once an aggregate meets all these three factors , we treat it as an exception .",
    "we define an aggregate as an exception at @xmath63 when it satisfies all conditions below :    \\(1 ) @xmath64 ;    \\(2 ) @xmath65 or @xmath66 ;    \\(3 ) @xmath67 ;    where @xmath68 is the change percentage , @xmath69 is the cumulative distribution function of @xmath70 , and @xmath70 is a random variable such that @xmath71 in which @xmath72 is the average of @xmath73 and @xmath74 is their variance @xmath75 . here , we assume that @xmath45 is in normal distribution form .",
    "an example as follow : assume we want to monitor the average price of all cellphones , where @xmath76 , @xmath77 , and @xmath78 .",
    "if the average price of the motorola cellphones with 4-inch screen drops suddenly , based on the historical data , the current avg(@xmath79 ) drops @xmath80 or more and in the left side of the normal distribution graph .",
    "select @xmath82 where @xmath83 = motorola & @xmath84 = 4-inch \" is returned as an exception .",
    "a straightforward approach is to crawl the entire database @xmath4 , then find exceptions .",
    "the benefit of this solution is that @xmath85 exceptions can be found .",
    "however , this is unreasonable solution for this type of problem since you do nt only need to crawl @xmath4 in @xmath63 but also for all time interval @xmath86 .",
    "moreover , because of query limitation forced on hidden web databases , we can not enumerate all the selection conditions to check whether each of them is an exception or not .",
    "a more reasonable approach is to take a sample to estimate the @xmath45 .",
    "there is a deep research for answering this question for estimation with selection condition .",
    "but in this problem , estimating each selection condition separately wastes a lot of queries , because a sample can not only be for one selection condition , i.e. , if we already know @xmath23 , where @xmath87 @xmath88 @xmath44 , @xmath23 is a sample for both @xmath89 and @xmath44 .",
    "for example , if we have the full list of the motorola cellphones with 4-inch screen already , these cellphones can be a sample to estimate average price for both motorola cellphones and 4-inch screen cellphones .",
    "hence , we take a sample of the dataset first and then make different estimations based on it .",
    "now we introduce two concepts that are essential in our solution , query - pool and apriori .",
    "* query - pool : * a query - pool @xmath90 is introduced to guide the process of identifying @xmath91 queries .",
    "a query @xmath18 is a candidate when its result @xmath92 , i.e. , the number of returned tuples is bigger than support threshold .",
    "our purposed algorithms use @xmath90 as part of their solutions to identify candidates .",
    "once all candidates are identified and in @xmath90 , we answer aggregate queries and find exceptions .",
    "* apriori algorithm:*[apriori ] apriori algorithm @xcite is a famous and influential algorithm for mining frequent itemsets for boolean association rules .",
    "one of its functionality is to determine the frequent itemsets , i.e. , the sets of item which has minimum support .",
    "apriori extends frequent subsets by one item at a time in a ",
    "bottom up \" approach ( a step known as candidate generation ) and tests the groups of candidates against the data .",
    "similarly , the query candidates generation in our query - pool uses the same approach , where predicates are extended one attribute at a time until no further successful extensions are found .",
    "recall that , as mentioned in * query - pool * , a query @xmath18 is a candidate when @xmath92 .",
    "for example , assume we have two attributes @xmath8 and @xmath42 , both are boolean , and @xmath93 .",
    "we have eight queries @xmath94 , @xmath95 , @xmath96 , ... , @xmath97 .",
    "a query @xmath98 for @xmath99 $ ] is a candidate when @xmath100 .",
    "the baseline algorithm consists of two phases .",
    "the first one is generating the query - pool @xmath90 using apriori algorithm discussed in section [ apriori ] .",
    "once this phase is done , we have all candidates in @xmath90 .",
    "the second phase is to find all exceptions from the query - pool .",
    "this can be done using the random walk approach to sample hidden web databases , which is proposed in @xcite .",
    "the idea of random walk is centered on @xmath101 @xmath102 over a query tree .",
    "the root level of query tree is select @xmath19 from @xmath4 , where the query tree organizes queries from broad to specific from top to bottom . for each @xmath101 @xmath103 ,",
    "the query appends a random conjunctive constraint to the selection condition until a valid query is reached .    to imagine the random walk process , assume a specific ordering of all binary attributes each time , e.g. @xmath104 $ ] .",
    "the random walk starts from the root by issuing the query @xmath18 = select * from @xmath4 .",
    "each time @xmath18 overflows , we expand it by adding the next attribute and assign a random predicate to it one at a time .",
    "for example , when @xmath18 overflows for the first time , we expand it by assigning a random selected predicate to @xmath8 ( either  @xmath87 \" or  @xmath43 \" ) .",
    "this process leads @xmath18 to be either a valid or underflowing query .",
    "if @xmath18 is valid , then we randomly choose the returned tuple .",
    "otherwise , we restart the random walk process .    consider figure 1 , which shows a database @xmath4 with three attributes and three tuples , and a complete binary tree with 4 @xmath105 levels .",
    "the @xmath12-@xmath106 level @xmath107 represents attribute @xmath10 and the leaves represent possible tuples .",
    "each node that falls in a level @xmath108 has two edges labelled as 0 and 1 respectively .",
    "the leaves at level @xmath7 + 1 represent all possible assignments of values to the attributes .",
    "the combination of any assignment is unique such that there is no other leave node with the same path . not all leaves necessary represent existing tuples as some leaves may be empty , which is very common in a real - world database . when applying the random walk , we start from the top node of the tree @xmath8",
    "( i.e. , the first attribute ) and randomly we pick a value either 0 or 1 ( i.e. , represents the edge ) with equal probability . every time we pick",
    "an attribute we check whether it is an overflow or not .",
    "if it is an overflow , we pick a random value for the next attribute , in this case it is @xmath42 , and check again for its validity .",
    "we repeat this and assign attributes in order until we reach a valid or underflow query .",
    "if the query is underflow we start the process from the beginning , otherwise we have a valid query returning @xmath109 tuples .",
    "we then we pick one of the @xmath110 tuples with probability of @xmath111.note that the access probability of the tuple that gets picked is therefore @xmath112 = @xmath113 .",
    "then , the tuple is accepted with probability @xmath114 where @xmath115 where c is a scale factor that boosts the selection probabilities to make the algorithm efficient .",
    "for categorical dataset , the only difference is that we have @xmath15 choices at @xmath12-@xmath106 level .",
    "and the access probability is @xmath116 .    for each candidate in @xmath90",
    ", we issue queries based on the approach above to get samples at @xmath50 . but instead of starting from select @xmath19 from @xmath4 as the root , the root of each query candidate is the query candidate itself , i.e. , if @xmath117 is select @xmath19 from @xmath4 where @xmath87 & @xmath44 , the root for this query tree is @xmath87 & @xmath44 . based on our definition in section [ hidden database ]",
    ", the candidate starts with extremely broad ( and thus overflowing ) query and the drill - down process narrows the query down by adding randomly selected predicates , until a valid query is reached .",
    "once we have a valid query , we select our sample .",
    "@xcite describes the random walk process in more details .",
    "when selecting a sample , we also compute the probability @xmath118 of this sample being selected in a drill - down .",
    "same sample from different candidates could have different @xmath118 based on the number of drill - down levels performed before selecting the sample . to illustrate this , consider two candidates @xmath119 and @xmath120 , such that @xmath119 is @xmath121 and @xmath120 is @xmath122 .",
    "if both @xmath119 and @xmath120 select the same sample @xmath123 , their probabilities will be different , @xmath124 and @xmath125 respectively ( i.e. , @xmath119 has three drill - down levels while @xmath120 has two drill - down levels ) .",
    "once we select the sample and calculate its probability , we estimate the average price for the candidate and judge whether @xmath45 is an exception or not by the definition in section [ exception ] .",
    "thus , the algorithm is as follow :    @xmath5 is total number of candidates in @xmath126 .",
    "@xmath127 is a candidate .",
    "@xmath128 is set of samples corresponding to @xmath127 .",
    "however , a sample which satisfies the selected conditions of several candidates in @xmath90 at the same time can only be used to estimate one of the candidates once by this algorithm .",
    "for example , the sample @xmath129 above can be used for both candidates @xmath119 and @xmath120 , but by the baseline algorithm , it is only used once . when the intersection of two candidates is very big , this is obviously not a good idea . to estimate the aggregates of all candidates over time",
    ", the baseline algorithm treats every candidate on @xmath130 , @xmath131 , @xmath132 , @xmath63 separately .",
    "the estimations are independent with each others",
    ". although it is simple , there are two obvious disadvantages .",
    "firstly , it wastes numerous queries because no information is retained / reused from time to time .",
    "secondly , when there are two candidates having an intersection with each other , it is a waste that the samples in the intersection are only for one estimation as the naive algorithm . in subsections [ stratified - detector ] and [ udometer ] ,",
    "we introduce our stratified - detector and udometer algorithms to address these two problems based on the state - of - art techniques separately .        given a query - pool @xmath90 , stratified - detector improves the selectivity of samples for this query - pool .",
    "the @xmath133 @xmath134 technique reduces query cost while minimizing estimation error for a given query - pool , which is proposed in @xcite .",
    "the idea of stratified sampling is to generalize uniform sampling by partitioning the population into multiple strata and samples are selected uniformly from each stratum .",
    "the more ? important ?",
    "the strata is , the more contribution it has on picking samples .",
    "* stratified samplingexample : * let us consider a database @xmath4 that has one attribute @xmath135 and four tuples , such that @xmath136 = \\{100 } , @xmath137 = \\{150 } , @xmath138 = \\{200 } , and @xmath136 = \\{250}. let us have the aggregate query @xmath139 = select count(@xmath19 ) from @xmath4 where @xmath140 .",
    "let p(@xmath139 ) defines the population of @xmath139 on @xmath4 as a set of size @xmath141 that contains the value of the aggregated column selected by @xmath139 , or 0 otherwise .",
    "we have p(@xmath139 ) = \\{0 , 0 , 1 , 1}. since p(@xmath139 ) has a mix of 1 s and 0 s , it is a nonzero variance , which makes uniform sampling a poor choice for this problem . to have a zero variance , we better partition @xmath4 into two starta \\{t1 , t2 } and \\{t3 , t4 } , such that p(@xmath139 ) contains now two starta \\{0 , 0 } and \\{1 , 1 } , with both have zero variance .",
    "nonetheless , if we also have @xmath142 = select count(@xmath19 ) from @xmath4 where @xmath143 , then p@xmath142 = \\{0 , 1 , 1 , 1 } ( different than p(@xmath139 ) ) , and each query will have its own population of @xmath4 . so the challenge is to adapt stratified sampling so that it works well for all queries in query - pool .    in general , stratified sampling partitions @xmath4 into @xmath144 strata with @xmath145 tuples ( where @xmath146 ) with @xmath147 tuples uniformly sampled from each stratum ( where @xmath148 ) .",
    "the stratified sampling proposed in @xcite discusses how can we be apply stratified sampling effectively in databases .",
    "it consists of three steps ( 1 ) _ stratification _ , to determine the number of strata @xmath144 to partition @xmath4 into and the number tuples from @xmath4 for each stratum , ( 2 ) _ allocation _ , to determine how to divide @xmath149 into @xmath150 across @xmath144 strata , ( 3 ) _ sampling _ , apply random walk to sample @xmath151 tuples from stratum @xmath152 .",
    "when applying the steps above into our problem , we divide the query - pool into starta , such that the number of these starta @xmath153 where for any starta @xmath154 , each @xmath117 selects either @xmath85 tuples in @xmath152 or none .",
    "the number of starta @xmath144 depends on both @xmath4 and @xmath90 ( generally , the total number is upper - bounded by @xmath155 ) . after identifying starta",
    ", stratified sampling algorithm picks samples .",
    "note that the @xmath144 strata must be non - overlapping and mutually exclusive .",
    "however , the above technique is not suitable when the size of query - pool is big ( i.e. , number of starta is large ) .",
    "instead , when the space of one candidate @xmath120 is the subset of another @xmath119 , stratified - detector takes samples separately from @xmath120 and @xmath156 , then merge these two together by stratified sampling to estimate @xmath119 .",
    "here we can get the estimations for both @xmath120 and @xmath119 .",
    "note by applying this technique , we also need to use the probability of @xmath120 to calculate @xmath119 .",
    "further more , if we have another candidate @xmath157 , which @xmath119 is a subset of @xmath157 , we can directly take sample from @xmath158 , and then get the estimation of @xmath157 by the sample of @xmath158 , and the stratified sample of @xmath119 we got previously .",
    "moreover , when a candidate @xmath119 contains more than one longer candidate , we consider the longer candidate with the biggest subspace in term of size ( i.e. , the longer candidate with biggest count ) .",
    "for example , if @xmath119 contains three longer candidates @xmath159 , @xmath160 , and @xmath161 with count of 1000 , 2000 , and 3000 respectively , we use @xmath161 to estimate @xmath119 .    to illustrate this stratified sampling ,",
    "let us reconsider @xmath119 and @xmath120 , such that @xmath119 is a two conjunction candidate and @xmath120 is a three conjunction candidate .",
    "note that @xmath120 exists in @xmath119 .",
    "thus , the average price of @xmath119 is    @xmath162    where @xmath163 is the average price of @xmath164 and @xmath165 is the average price of @xmath120 . if we select the samples @xmath166 ( level 2 ) and @xmath167 ( level 3 ) to calculate @xmath165 , then + par @xmath168    note that @xmath169 and @xmath170 .",
    "similarly , we can calculate @xmath163 where the only difference is that the @xmath171 becomes @xmath172 ( i.e. , @xmath173 ) .",
    "algorithm [ alg : stratified - detector ] depicts the pseudocode .",
    "@xmath174   is   empty   in @xmath90    @xmath175 is the set of candidates with length @xmath3 workload .",
    "@xmath127 is a candidate .",
    "the details of the stratified sampling is :    for the current candidate @xmath176 , take the @xmath177 that contains the most tuples as @xmath178 . assuming that we have samples already for @xmath179 ,",
    "then take samples from the remaining subspace @xmath180 and then combing them with the sample in @xmath177 .",
    "and then we can have an estimation of the average of the target attribute , @xmath181 .",
    "@xmath182 is the count of tuples that satisfying the selected condition of the candidate @xmath176 .",
    "after we finished the loop of @xmath183 , we can have estimations for all the candidates in @xmath90 .",
    "after repeated on @xmath184 , because we have the estimations for all the candidates on @xmath185 , we can find out which ones are exceptions based on the definition .",
    "however , when the variance of the samples is big , we can not have a good result .",
    "the reason is , when we randomly pick samples for a candidate on @xmath184 , with the big variance , we can not distinguish whether a dramatic change is caused by the sampling or the data itself .",
    "a straightforward solution is to take more than one sample for each candidate in each time interval in order to reduce the variance ( if the change is caused by sampling ) .",
    "the more samples we collect , the more accurate estimations we get . of course , this solution is not practical neither reasonable since we have a limited number of queries to issue .    to overcome this problem",
    ", we introduce our udometer algorithm discussed in the next subsection .",
    "even though most of the real - world databases are dynamic ( updated at arbitrary time ) , yet many of these databases do not see frequent updates . and",
    "the fewer changes happen to the database , the few changes the sample will see .",
    "this means that the random walk technique discussed in section [ baseline ] could lead to a significant waste of queries after a period of time since no information is retained / reused",
    ". not only that , but the saving of query cost can be directly translated to more accurate aggregate estimations . in particular , since updating a drill - down may consume fewer queries , the remaining query budget ( after updating all previous drill downs ) can be used to initiate new drill downs , increasing the number of drill downs and thereby reducing the estimation error .    to understand how saving query cost reduces estimation error , consider the example mentioned in section [ baseline ] ( figure [ fig : random walk ] ) .",
    "note that each drill down has a unique sequence number of the leaf - level node corresponding to it , which can be uniquely identified this drill down .",
    "we notify this as a @xmath186 , such that at each time interval @xmath187 where @xmath188 , we have a signature set @xmath189 where each @xmath190 defines one drill - down performed .",
    "now , to collect any sample ( from figure 1 ) at @xmath130 , it requires at least 4 queries , i.e. , from root to any leaf node ( a tuple ) .",
    "if no change happens to the database at @xmath131 , we can reach the same leaf node from @xmath130 in only 1 query ( by issuing the valid query from @xmath130 ) .",
    "this means we can save 3 out of 4 queries .",
    "however , we still need to issue one more query over leaf node s parent to determine if it is still the top non - overflowing query .",
    "this reduces the number of drill downs that can be updated at each interval time to at most half of the query budget .",
    "moreover , the estimation produced in different time interval are no longer independent of each other due to the reuse of the same signature set ( of drill - downs ) .",
    "udometer addresses the problem above by reissuing the same sample set from @xmath130 for all time interval @xmath187 where @xmath188 .",
    "although , udometer does not reduce the query cost for generating the query - pool ( as apriori still does it ) . yet , it still minimizes the total query cost by reducing the query cost of the random walk process ( along with reducing the estimation error ) . as for minimizing the total query",
    "cost even further for a given query - pool of aggregate queries , we still need to minimize the query cost of generating candidates .    when applying udometer , we divide the problem into two parts based in the time interval : ( 1 ) day one ( i.e. , @xmath130 ) , ( 2 ) day two and beyond @xmath191 . for day one (",
    "i.e. , @xmath130 ) , we apply the stratified - detector as is ( algorithm [ alg : stratified - detector ] ) .",
    "once finished , we will have a sample set for @xmath130 .",
    "now for day two and beyond ( i.e. , @xmath192 ) , we reissue the same sample set from @xmath130 .",
    "note by doing so , each query - pool @xmath90 in @xmath192 is a subspace of the query - pool @xmath193 ( from day one @xmath130 ) .",
    "although the number of candidates that is considered by the udometer may be less than the actual number of candidates for a specific day , the accuracy of our estimations should be higher , which lead to more accurate exceptions .",
    "algorithm [ alg : udometer ] provides the pseudocode for udometer .",
    "@xmath174   is   empty   in @xmath90    reissuing the same set of samples decreases the variance of the samples and lead to more accurate estimation .",
    "udometer is now able to detect the overall trend of the average price of most candidates ( whether its an increment or decrement ) . yet",
    ", detecting the overall trend is sometimes not enough to detect the exception itself .",
    "two of the main reasons that an exception occurs for a candidate are ( 1 ) sudden drop / rise in price for sizable number of tuples , and ( 2 ) insertion or deletion of new and existing tuples .",
    "when we consider a subspace @xmath178 with the most tuples to estimate @xmath194 , these new or deleted tuples may not have a great impact on @xmath194 to be an exception because of the ratio size of @xmath178 to @xmath194 .",
    "our priority - udometer , which we introduce in the next subsection can handle both events .      in the previous sections",
    ", we mentioned that when we applied the stratified - detector , we divided the target candidate @xmath194 into two subspaces @xmath178 and @xmath195 , in which the count of @xmath178 is the biggest among the candidates which are the subspaces of @xmath194 .",
    "it is a straightforward idea because we only need to take a new sample in the smallest space @xmath195 .",
    "however , while this method to divide the space is good to estimate the aggregates like average and sum , it can not achieve a good result for difference operator , like the change of average .",
    "for example , we have two candidates @xmath178 and @xmath196 , @xmath178 is the bigger subspace but the average is almost kept constant , while @xmath196 is slightly smaller than @xmath178 , but the average changes dramatically on @xmath197 , which leads @xmath194 to be an exception . in this situation ,",
    "@xmath196 and @xmath198 is obviously a better partition . on the contrary , if @xmath196 is very small , although the average changes dramatically on @xmath197 , it has little impact on @xmath194 , @xmath196 and @xmath198 can not be a good partition .",
    "thus , we purpose a score function @xmath199 to decide which one is the best partition .    the purpose of the score function is to compare the impact of different candidates which are the subspace of the target @xmath194 .",
    "@xmath194 can be impacted by two factors , the size of the subspace @xmath178 , and how much the aggregate changed . the score function on @xmath197 we propose is @xmath200 and we compute the score of each candidate which is a subspace of target @xmath194",
    ", then choose the one with the biggest score as @xmath178 .",
    "* dataset : * we tested our algorithm over a real world ( categorical ) web database ebay.com , specifically cell phones and smartphones category , to which we have full offline access .",
    "the dataset , which we crawled , is for the period from oct 21st , 2015 to nov 20th , 2015 and contains 2,393,361 tuples and 8 attributes .",
    "all tuples that have been crawled offer a `` fixed price '' option while the attribute domain sizes ranges from 4 to 82 .",
    "* challenges : * recall from section [ intro ] that one of the main challenges hidden web databases is the query budget available daily .",
    "even though we have a complete access to the dataset , we still applied same number of query budget provided by ebay.com per - ip and per - developer .",
    "the default query budget per day is 5000 .",
    "however , because of query limitation , our algorithms first use the query budget available for one day as a bootstrapping to generate the query - pool before starting the algorithms the following day .    * query - pool : * examining all candidates in a query - pool is very challenging because of the limitation of query budget .",
    "bigger query - pool means less query budget we can spend per candidate .",
    "we tested our four algorithms under 5 different sizes of query - pool ( i.e. , with supported threshold @xmath53 @xmath201 1000 , 2000 , 3000 , 4000 , and 5000 ) . based on our dataset",
    ", this means that we sometimes can only issue two query budget per - candidate , while other times we have the luxury of spending sixteen query budget per - candidate .    * releasing @xmath62 : * when we tested the four algorithms , we released the percentage threshold from 15% to 12% .",
    "this is because in some settings , a lot of real exceptions in the dataset have the exact of 15% change that make them hard to detect without releasing @xmath62 .",
    "* algorithms evaluated : * we tested four algorithms presented in this paper : baseline , stratified - detector , udometer , and priority - udometer .",
    "all these four algorithms share the same database - controlled parameter of query budget per day and the parameters @xmath53 , @xmath59 , and @xmath62 that are presented in subsection [ exception ] that define exceptions .",
    "* performance measures : * for estimation accuracy , we measure the percent error ( i.e. , @xmath202 for an estimator @xmath203 of aggregates @xmath204 ) .",
    "we also measure the precision ( i.e. , @xmath205 for an estimator @xmath203 of aggregates @xmath204 ) .",
    "* recall and precision * we start comparing the performance of all four algorithms for the recall under the default setting per - day query budget of 5000 . figure [ fig : recallk ] shows the recall of the four algorithms when @xmath53 = 5000 and 0.01 @xmath206 0.99 .",
    "as shown , both priority - udometer and udometer improve their recall when @xmath3 is larger .",
    "however , when @xmath3 becomes very large both algorithms reach almost 100% recall , and this is because both algorithms can almost download the entire tuples corresponding to its candidate . on the other hand ,",
    "the stratified - detector and baseline do not show any improvement . in figure",
    "[ fig : recalls ] , all algorithms achieve a higher recall with bigger supported threshold .",
    "this is because when the support threshold is large , there are less candidates in the query - pool and each candidate has larger space and sample size . figures [ fig : precisionk ] and [ fig : precisions ] depicts a performance comparison between the four algorithms in terms of precision . in figure",
    "[ fig : precisionk ] , the impact of @xmath3 for both priority - udometer and udometer is small .",
    "and with larger @xmath53 the gap between these two algorithms is small .",
    "the precision for these two algorithms is not affected by the drill - downs .",
    "this is because when support threshold is large , each candidate has bigger sample size to spend . on the other hand ,",
    "the gap between these two algorithms and the other two ( stratified - sampler and baseline ) is very big .",
    "figure [ fig : recallp ] shows that the change threshold does not have a big impact on the recall . and with larger @xmath3 and @xmath53 , there is barely a gap between priority - udometer and udometer .",
    "so the change threshold nearly does not lead to any difference between these two algorithms . as for stratified - detector and baseline",
    ", their recall seems to improve when @xmath59 is large .",
    "however , and as shown in figure [ fig : precisionp ] , the precision for both algorithms seems to improve when @xmath59 is smaller .",
    "in contrast , figures [ fig : recallqb ] and [ fig : precisionqb ] reflect how query limitation has big impact on both recall and precision .",
    "the recall and drops from 95% for larger query budget to close to 55% in worst case for priority - udometer ( or 35% for udometer ) .",
    "moreover , the precision also drops from 95% to 40% for both algorithms . when @xmath207 we can only spend two query budget per - candidate to estimate its average price and judge whether it is an exception or not .",
    "thus , we get low recall and precision . overall , the priority - udometer have higher recall and precision than udometer .",
    "* score function vs. highest count : * figure [ fig : recallscorefk ] , and [ fig : recallscorefs ] depict the recall measurement of @xmath128 that used in priority - udometer comparing with taking highest count in udometer when we choose subspace .",
    "these figures take into consideration only the candidates that contain longer conjunction ( i.e. , candidates with subspaces ) .",
    "no matter what the @xmath3 or support threshold are , the score function always give better prediction comparing with taking just the highest count .",
    "thus , priority - udometer is more accurate for candidates with subspaces .",
    "moreover , figures [ fig : precisionscorefk ] and [ fig : precisionscorefs ] compare the precision under both @xmath3 and @xmath53 .",
    "similar to the recall , the priority - udometer in general gives more accurate precision .",
    "finally , figure [ fig : trend ] depicts how priority - udometer detects the overall trend for candidates , even when it incorrectly judges whether a candidate is an exception or not .",
    "the figure shows how the average price of phones with gray color drops sufficiently on nov 20th to be an exception ( i.e. , which priority - udometer detected correctly ) .",
    "* crawling and extraction for hidden databases : * there has been a number of prior work in crawling and extracting hidden databases content .",
    "this crawling requires understanding of query interfaces which was extensively studied ( e.g. , @xcite ) . even though crawling the entire database ( e.g. , @xcite ) could lead us to find all exceptions , it still unreasonable solution for this particular problem as we not only need to crawl the database one time but also for all time interval ( e.g. , every day ! ) .",
    "moreover , because of query limitations forced on hidden databases , we can not enumerate all the selection conditions to check whether each of them is an exception or not .",
    "* aggregate estimations over hidden web databases : * aggregate estimations over hidden web databases has been investigated over time for both static and dynamic databases .",
    "both types use efficient techniques to obtain random samples from hidden web databases described in @xcite . unlike this paper ,",
    "all prior work focuses on answering estimation with one select condition .",
    "but in this problem , estimating each select condition separately waste a lot of queries , where a sample could be used for one or more select conditions",
    ".    * approximate answers for aggregate queries : * decision support applications and data mining applications execute aggregate queries over very large databases to obtain important and useful information .",
    "existing work ( e.g. , @xcite ) introduced an efficient approach using a stratified sampling technique to optimize scalability and resources when providing approximate answers for a given workload of queries .",
    "unlike their workload where tuples have probability of occurrence given as an input , our workload has no knowledge about the distribution function specified by the workload .",
    "moreover , the workloads we consider do not have to be fixed nor similar and the size of these workloads could be very large , which make their optimal stratification technique unsuitable for this particular problem . other work ( e.g. , @xcite )",
    "focuses only on a specific type of query ( i.e. , join - queries ) for approximate query answering .    * data discovery and exploration : * detecting anomalies on very large data , such as data cubes and data warehouses , is very important to detect problem areas or new opportunities .",
    "most of existing work ( e.g. , @xcite ) focuses on traditional multidimensional databases where a full access to these databases is a must .",
    "other work ( e.g. , @xcite ) discusses detecting meaningful changes over hierarchically structured data , such as nested object data , while @xcite presents a single operator on olap products to summarize reasons for drops or increases observed at an aggregated level .",
    "moreover , detecting changes for large scale data using sampling has been discussed ( e.g. , @xcite ) .",
    "in this paper , we have developed a novel technique for tracking and discovering exceptions over dynamic hidden web databases , which change over time through its restrictive web search interface . in general , our technique consists of two main phases : ( 1 ) generating a query - pool that contains all query candidates , and ( 2 ) finding all exceptions ( in terms of sudden changes of aggregates ) from this query - pool .",
    "we developed a stratified sampling technique along with query reissuing to guide the process of finding exceptions .",
    "we presented a comprehensive set of experiments that demonstrate the superiority of our approach over the baseline solutions on real - world datasets .",
    "agrawal , r. and srikant , r. ( 1994 ) .",
    "fast algorithms for mining association rules in large databases . _",
    "proceedings of the 20th international conference on very large data bases ( vldb94 ) _ ( p./pp .",
    "478499 ) , september , : morgan kaufmann .",
    "dasgupta , a. , das , g. and mannila , h. ( 2007 ) . a random walk approach to sampling hidden databases . _ proceedings of the 2007 acm sigmod international conference on management of data _",
    "629640 ) , new york , ny , usa : acm .",
    "isbn : 978 - 1 - 59593 - 686 - 8 liu , w. , thirumuruganathan , s. , zhang , n. and das , g. ( 2014 )",
    ". aggregate estimation over dynamic hidden web databases .. pvldb , 7 , 1107 - 1118 .",
    "chaudhuri , s. , das , g. and narasayya , v. r. ( 2007 ) .",
    "optimized stratified sampling for approximate query processing .. _ acm trans .",
    "database syst .",
    "_ , 32 , 9 .",
    "zhang , z. , he , b. and chang , k. c .- c .",
    "understanding web query interfaces : best - effort parsing with hidden syntax",
    ". _ proceedings of the 2004 acm sigmod international conference on management of data ( sigmod 2004 ) _ ( p./pp .",
    "107 - 118 ) , .",
    "kabisch , t. , dragut , e. c. , yu , c. t. and leser , u. ( 2009 ) . a hierarchical approach to model web query interfaces for web source integration .. _ pvldb _ , 2 , 325 - 336 .",
    "raghavan , s. and molina , h. g. ( 2001 ) . crawling the hidden web .",
    "_ proceedings of the 27th international conference on very large databases ( vldb 2001 ) _ ( p./pp . 129 - 138 ) , .",
    "dasgupta , a. , jin , x. , jewell , b. , zhang , n. and das , g. ( 2010 ) .",
    "unbiased estimation of size and other aggregates over hidden web databases .. in a. k. elmagarmid and d. agrawal ( eds . ) , _ sigmod conference _ ( p./pp . 855 - 866 ) ,",
    "isbn : 978 - 1 - 4503 - 0032 - 2 chaudhuri , s. , das , g. and narasayya , v. r. ( 2001 ) . a robust , optimization - based approach for approximate answering of aggregate queries .. in s. mehrotra and t. k. sellis ( eds . ) , _",
    "sigmod conference _",
    "295 - 306 ) , : acm .",
    "isbn : 1 - 58113 - 332 - 4 hellerstein , j. m. , haas , p. j. and wang , h. j. ( 1997 ) .",
    "online aggregation .. in j. peckham ( ed . ) , _ acmsigmod international conference on management of data _",
    "171 - 182 ) , may , tucson : acm press .",
    "vitter , j. s. and wang , m. ( 1999 ) .",
    "approximate computation of multidimensional aggregates of sparse data using wavelets .. in a. delis , c. faloutsos and s. ghandeharizadeh ( eds . ) , _ sigmod conference _",
    "193 - 204 ) , : acm press .",
    "isbn : 1 - 58113 - 084 - 8 sarawagi , s. , agrawal , r. and megiddo , n. ( 1998 ) .",
    "discovery - driven exploration of olap data cubes .. in h .- j .",
    "schek , f. saltor , i. ramos and g. alonso ( eds . ) , _ edbt _",
    "168 - 182 ) , : springer .",
    "isbn : 3 - 540 - 64264 - 1 agarwal , d. , barman , d. , gunopulos , d. , young , n. e. , korn , f. and srivastava , d. ( 2007 ) .",
    "efficient and effective explanation of change in hierarchical summaries .. in p. berkhin , r. caruana and x. wu ( eds . ) , _ kdd _ ( p./pp .",
    "6 - 15 ) , : acm .",
    "isbn : 978 - 1 - 59593 - 609 - 7 ramaswamy , s. , rastogi , r. and shim , k. ( 2000 ) .",
    "efficient algorithms for mining outliers from large data sets .",
    "_ sigmod 00 : proceedings of the 2000 acm sigmod international conference on management of data _",
    "427438 ) , new york , ny , usa : acm .",
    "isbn : 1 - 58113 - 217 - 4 chawathe , s. s. and garcia - molina , h. ( 1997 ) .",
    "meaningful change detection in structured data .",
    "26 - 37 ) , may , tuscon , arizona acharya , s. , gibbons , p. b. , poosala , v. and ramaswamy , s. ( 1999 ) .",
    "join synopses for approximate query answering .. in a. delis , c. faloutsos and s. ghandeharizadeh ( eds . ) , _ sigmod conference _ ( p./pp . 275 - 286 ) , : acm press .",
    "isbn : 1 - 58113 - 084 - 8 sarawagi , s. ( 1999 ) . explaining differences in multidimensional aggregates .. in m. p. atkinson , m. e. orlowska , p. valduriez , s. b. zdonik and m. l. brodie ( eds . ) , _ vldb _ ( p./pp .",
    "42 - 53 ) , : morgan kaufmann .",
    "isbn : 1 - 55860 - 615 - 7 cho , j. and ntoulas , a. ( 2002 ) .",
    "effective change detection using sampling .",
    "_ proceedings of the 28th international conference on very large databases ( vldb 2002 ) _",
    "514525 ) , ."
  ],
  "abstract_text": [
    "<S> nowadays , many web databases  hidden \" behind their restrictive search interfaces ( e.g. , amazon , ebay ) contain rich and valuable information that is of significant interests to various third parties . </S>",
    "<S> recent studies have demonstrated the possibility of estimating / tracking certain aggregate queries over dynamic hidden web databases . </S>",
    "<S> nonetheless , tracking all possible aggregate query answers to report interesting findings ( i.e. , exceptions ) , while still adhering to the stringent query - count limitations enforced by many hidden web databases providers , is very challenging . in this paper </S>",
    "<S> , we develop a novel technique for tracking and discovering exceptions ( in terms of sudden changes of aggregates ) over dynamic hidden web databases . </S>",
    "<S> extensive real - world experiments demonstrate the superiority of our proposed algorithms over baseline solutions .    </S>",
    "<S> = 1 </S>"
  ]
}