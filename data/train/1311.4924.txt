{
  "article_text": [
    "classical compressed sensing ( cs ) theory assumes that the representation matrix ( dictionary ) and sampling ( measurement ) matrix are known exactly in advance @xcite @xcite @xcite .",
    "however , some uncertainty or possible inaccuracy can affect them in many applications .",
    "for example , in the sparse representation of the signal , the assumed basis typically corresponds to a gridding of the parameter space , e. g. , a discrete fourier transformation ( dft ) grid @xcite .",
    "but in reality no physical field is exactly sparse in the dft basis .",
    "no matter how finely the parameter space is gridded , the signal may not lie perfectly on the sampling points .",
    "this leads to mismatch between the assumed and the actual bases , which results in the uncertainty in the representation matrix .",
    "the sampling of the analogue signal s circuit noise and other non - linear effects can induce uncertainty in the sampling matrices @xcite .",
    "the classical sparse signal model did not consider these uncertainties ; and the corresponding sparse signal recovery methods can suffer performance degeneration because of signal model mismatch .",
    "some papers have addressed related problems recently .",
    "@xcite , @xcite , @xcite and @xcite analyzed signal recovery by basis pursuit ( bp ) and greedy algorithms with perturbations in either measurement matrix or representation matrix . to deal with the performance degeneration ,",
    "several methods were proposed @xcite-@xcite . instead of a fixed basis , @xcite used a tree - structured dictionary of bases and the best bases were estimated with an iteratively processed recovery of the signal . @xcite and",
    "@xcite relaxed the distortionless constraint to allow entry - wise sampling error by a series of large inequality zoom operations .",
    "similarly , @xcite and @xcite generalized the approximate message passing ( amp ) algorithm to hold the sampling matrix uncertainty with several parameters to be tuned or predefined .",
    "@xcite proposed a way only for the structured sensing matrix perturbation . in @xcite @xcite ,",
    "two non - convex methods were proposed to deal with uncertainty in data in the sparse linear regression problem .",
    "the non - convexity requires knowledge of the @xmath0 norm of the unknown sparse signal in order to maintain bounded iterates , which is not available in many applications .",
    "@xcite introduced a sparsely regularized total least - squares ( srtls ) method to deal with the uncertainty in the representation matrix .",
    "but its solver needs a number of iterations between the sparse signal estimation and the uncertainty matrix estimation , which implies a large computational burden . in summary ,",
    "previous publications have not fully analyzed the resulting total uncertainty from both sampling and representation uncertainties .",
    "furthermore , no algorithm of low computational complexity exists for sparse signal recovery in the presence of either sampling uncertainty or representation uncertainty .    in this paper",
    ", we generalize the sparse signal model containing both measurement and representation errors .",
    "based on the generalized sparse signal model and possible statistical prior knowledge about the measurement and representation errors , a new data fitting constraint is deduced with stochastic uncertainty .",
    "we combine it with the @xmath1 norm minimization based sparsity - inducing constraint , and obtain an optimization model for robust sparse signal recovery .",
    "two approaches are used to solve the optimization problem .",
    "one relaxes the @xmath2 norm to the @xmath3 norm to obtain a convex programming problem ; and the other one takes a greedy algorithm approach . for convex programming , we give a sufficient condition for successful recovery ; and for the greedy algorithm , we prove it can be solved by regular greedy algorithms with transformations on sensing matrix and measurements .",
    "numerical results show the performance of the proposed method with both simulated data and real - life ecg signals .    the rest of the paper is organized as follows .",
    "section [ sec2 ] gives the generalized sparse signal model . in section [ sec3 ] , the corresponding optimization model for robust sparse signal recovery",
    "is deduced . in section [ sec4 ] , both convex relaxation and a greedy algorithm are used to solve the optimization model .",
    "section [ sec5 ] demonstrates the performance of the proposed method by numerical experiments .",
    "finally , section [ sec6 ] presents the conclusions of this work .",
    "in cs , instead of acquiring the signal @xmath4 directly according to the nyquist sampling , a measurement matrix @xmath5 is used to sample the signal with @xmath6 , which can be formulated as : @xmath7 where the obtained vector @xmath8 contains the sub - nyquist - sampled random measurements .",
    "sparsity widely exists in many natural and man - made signals .",
    "it means that many of the representative coefficients are close to or equal to zero , when the signal is represented in a dictionary @xmath9 .",
    "it can be formulated as :    @xmath10    where @xmath11 is the representative vector with most of its entries are zero .",
    "when most of the entries are not strictly zero but trivial , or only a few of the entries are significant , strictly we should call the vector is compressible , but sometimes we say it be sparse too .",
    "the number of nonzero or significant entries are _",
    "k_.    combining ( [ eq2 measurement model ] ) and ( [ eq2 sparse model ] ) , we can get :    @xmath12    where @xmath13 where * a * is called sensing matrix .",
    "based on the standard sparse signal model ( [ eq2 cs model ] ) , classical cs proves that the signal can be successfully recovered by sparse signal recovery methods @xcite .    to further consider the errors in the data , an additive noise term is included into the signal model as : @xmath14 where @xmath15 is the additive white gaussian noise ( awgn ) with zero mean and covariance matrix @xmath16 @xcite",
    "however , in many practical scenarios , uncertainty in the sampling matrix exists .",
    "when sampling the analogue signals , uncertainty can result from various types of non - ideal effects , such as aliasing , aperture effect , jitter and deviation from the precise sample timing intervals , noise , and other non - linear effects . after sampling , uncertainty can also be introduced by an inconsistent channel effect , channels coupling effect , and so on . here",
    "we can model the sampling matrix with uncertainty as : @xmath17 where @xmath18 is the uncertainty - free sampling matrix which is known in advance or can be estimated by training data , and @xmath19 is the sampling matrix error .",
    "the exact information about @xmath19 can not be available .",
    "we can approximately treat it as a random gaussian variable matrix or some deterministic unknown variable matrix @xcite @xcite .",
    "there is uncertainty in the representation matrix ( dictionary ) too .",
    "it can result from the quantification of the representation matrix , such as the gridding of the parameter space of dictionary , the mismatch between the assumed dictionary for sparsity and the actual dictionary in which the signal is sparse , and so on .",
    "similarly we model the representation matrix with uncertainty as : @xmath20 where @xmath21 is the uncertainty - free representation matrix which is known in advance or can be estimated by training data , and @xmath22 is the representation matrix error .",
    "we can approximately treat it as a random gaussian variable matrix , or a random variable matrix in uniform distribution or some deterministic unknown variable matrix @xcite .",
    "to take the errors in both sampling and representation into consideration , we can reformulate ( [ eq2 a model ] ) as :    @xmath23    where    @xmath24    @xmath25    * e * is the sensing matrix error . as can be seen in ( [ eq2 generalized a model 2 ] )",
    ", the correlation between measurement error @xmath19 and representation error @xmath22 affects the estimation of * e * mainly by the term @xmath26 .",
    "based on the discussed model above , we can set up the sparse signal model with sampling and representation uncertainties and the additive noise .",
    "the generalized sparse signal model can be formulated as :    @xmath27",
    "given the measurement vector * y * and the matrix @xmath28 , we need to recover the sparse representative vector @xmath29 . in cs , to find the sparsest signal that yields the measurements , we can solve the sparse least squares problem :    @xmath30    where @xmath31 is the @xmath32 norm which counts the number of the nonzero entries of the vector @xmath29 , and it encourages sparse distribution in @xmath33 .",
    "it should be noted that the @xmath32 norm is not a full - fledged norm . solving ( [ eq3 l0 opt ] ) is np - hard .",
    "the basis pursuit denoising ( bpdn ) uses the @xmath0 norm to replace the @xmath32 norm to make it convex and relaxes the data fitting constraint to deal with the additive noise , where the the @xmath0 norm of the vector @xmath29 is defined as @xmath34 .",
    "bpdn can recover compressible signal with additive noise .",
    "however , it can not allow the multiplicative error as in ( [ eq2 generalized a model ] ) which is caused by sampling and representation uncertainties .",
    "in fact , the relaxed data fitting constraint of bpdn matches the sparse signal model with additive noise but does not match the generalized sparse signal model with sampling and representation uncertainties ( [ eq2 generalized cs model ] ) . the performance degradation of bpdn has been investigated in @xcite @xcite @xcite .    to explain why classical @xmath35 pseudo norm and @xmath36 norm based optimization methods could lead to incorrect solution with large error",
    ", we give an example to illustrate the situation in the presence of sampling and dictionary uncertainty , as shown in fig .",
    "[ fig : contour1 ] . the designed data fitting constraint @xmath37 is @xmath38 ( i.e. @xmath39\\left [ { \\begin{array}{*{20}{c } } { { \\theta _ 1}}\\\\ { { \\theta _ 2 } } \\end{array } } \\right ] $ ] ) where @xmath40^t } $ ] . because of multiplicative noise , the real data constraint in practice is @xmath41 ( i.e. @xmath42\\left [ { \\begin{array}{*{20}{c } } { { \\theta _ 1}}\\\\ { { \\theta _ 2 } } \\end{array } } \\right]$ ] ) . in fig . [",
    "fig : l0 ball ] and fig .",
    "[ fig : l1 ball ] , we can see that the tangent points of the minimized @xmath32 and @xmath0 balls with the observed line are on the coordinate axes , which means the corresponding solutions are sparse .",
    "but they are far away from the ones of the minimized @xmath32 and @xmath0 balls with the original line which are the true solutions , which means that the error of the solutions are very large and they are not robust .",
    "0.25   balls which are tangent to the accurate line and the line which has error on slope ( multiplicative noise ) : correct solution ( red point of intersection ): @xmath43^t } $ ] ; incorrect solution ( blue point of intersection ) : @xmath44^t } $ ] ; ( b ) .",
    "the contour map of minimized @xmath0 balls which are tangent to the accurate line and the line which has error on slope ( multiplicative noise ) : correct solution ( red point of intersection ) : @xmath43^t } $ ] , incorrect solution ( blue point of intersection ) : @xmath44^t } $ ] .,title=\"fig : \" ]    0.25   balls which are tangent to the accurate line and the line which has error on slope ( multiplicative noise ) : correct solution ( red point of intersection ): @xmath43^t } $ ] ; incorrect solution ( blue point of intersection ) : @xmath44^t } $ ] ; ( b ) .",
    "the contour map of minimized @xmath0 balls which are tangent to the accurate line and the line which has error on slope ( multiplicative noise ) : correct solution ( red point of intersection ) : @xmath43^t } $ ] , incorrect solution ( blue point of intersection ) : @xmath44^t } $ ] .,title=\"fig : \" ]      to robustly recover this generalized sparse signal , a new data fitting constraint , other than the one @xmath45 of bpdn , should be deduced .",
    "we assume the uncertainty term * e * in ( [ eq2 generalized a model ] ) is a random variable matrix , and * p * is the covariance matrix @xmath46 , which is positive - semidefinite and symmetric .",
    "we refer to the stochastic robust approximation @xcite .",
    "@xmath47 denotes the assumed measurement vector obtained by the signal model ( [ eq2 generalized cs model ] ) , i. e. @xmath48 , and part of its parameters ( @xmath49 and * p * ) are known in advance ; and @xmath50 denotes the measurement vector obtained in practice without any knowledge of the signal model .",
    "we use the signal model to fit the practical measurements _ * y*_. we try to fit the obtained measurement vector with the generalized sparse signal model ( [ eq2 generalized cs model ] ) , and the expected value of the data fitting error can be formulated as : @xmath51.\\\\ ] ] incorporating the generalized sparse signal model ( [ eq2 generalized a model ] ) , we can get @xmath52    = { \\rm e}\\left [ { { { \\left ( { { \\bf{y } } - { { \\bf{y}}_0 } } \\right)}^t}\\left ( { { \\bf{y } } - { { \\bf{y}}_0 } } \\right ) } \\right ] \\\\    = { \\rm e}\\left [ { { { \\left [ { \\left ( { \\overline { \\bf{a } }   + { \\bf{e } } } \\right){\\bf{\\theta } } + { \\bf{n } } - { \\bf{y } } } \\right]}^t}\\left [ { \\left ( { \\overline { \\bf{a } }   + { \\bf{e } } } \\right){\\bf{\\theta } } + { \\bf{n } } - { \\bf{y } } } \\right ] } \\right ] \\\\",
    "%   = { \\rm e}\\left [ { \\left [ { { { \\left ( { \\overline { \\bf{a } } { \\bf{\\theta } } - { \\bf{y } } } \\right)}^t } + { { \\left ( { { \\bf{e\\theta } } + { \\bf{n } } } \\right)}^t } } \\right]\\left [ { \\left ( { \\overline { \\bf{a } } { \\bf{\\theta } } - { \\bf{y } } } \\right ) + \\left ( { { \\bf{e\\theta } } + { \\bf{n } } } \\right ) } \\right ] } \\right ]",
    "\\\\    = { \\rm e}\\left [ { \\left\\| { \\overline { \\bf{a } } { \\bf{\\theta } } - { \\bf{y } } } \\right\\|_2 ^ 2 + { { \\left ( { \\overline { \\bf{a } } { \\bf{\\theta } } - { \\bf{y } } } \\right)}^t}\\left ( { { \\bf{e\\theta } } + { \\bf{n } } } \\right ) } \\right . \\\\",
    "{ + { { \\left ( { { \\bf{e\\theta } } + { \\bf{n } } } \\right)}^t}\\left ( { \\overline { \\bf{a } } { \\bf{\\theta } } - { \\bf{y } } } \\right ) + \\left\\| { { \\bf{e\\theta } } + { \\bf{n } } } \\right\\|_2 ^ 2 } \\right ] . \\\\   \\end{array}\\ ] ]    here we assume that all the entries in * n * are i.i.d gaussian with @xmath53 , and * n * is independent from * e*. thus we can get :    @xmath54\\\\   = \\left\\| { \\overline { \\bf{a } } { \\bf{\\theta } } - { \\bf{y } } } \\right\\|_2 ^ 2 + { { \\bf{\\theta } } ^t}{\\bf{p\\theta } } + { m\\sigma ^2 } \\end{array}.\\\\ ] ]    bounding this data fitting error expectation with a parameter @xmath55 would give a new constraint which matches the generalized sparse signal model ( [ eq2 generalized a model ] ) as :    @xmath56    combining ( [ eq3.1 distortionless constraint ] ) with the @xmath32 norm minimization yields the optimization model for recovering a generalized sparse signal with sampling and representation matrix uncertainties : @xmath57    it can be further generalized to :    @xmath58    where @xmath59 and @xmath60 are nonnegative parameters balancing the constraints , which can be tuned using cross validation , regularization path following etc .",
    "the proposed optimization ( [ eq3 rl0 opt ] ) is called robust @xmath32 ( rl0 ) optimization because it has robustness against the measurement and representation matrix uncertainties .",
    "one of its equivalent forms is : @xmath61 where @xmath62 and @xmath63 are parameters too .",
    "we assume that the covariance matrix * p * is _ a priori _ known in the rl0 optimization . on one hand , we can model * p * on the basis of an analysis of the cs setup .",
    "for example , we can make a possible assumption that the sampling matrix error is gaussian as done in @xcite ; or , when the dictionary error is caused by finite gridding of the parameters ( dictionary error corresponding to the quantization of the sparse vector ) we can assume a uniform distribution of the gridding parameter , etc . on the other hand , we can estimate * p * as addressed in the errors - in - variables modeling literature , see e.g. @xcite@xcite@xcite@xcite . as far as we know , the best way to estimate p is via replicated observations @xcite@xcite@xcite . assuming that independent repeated measurements are available for each variable",
    "observed with error , this type of replications provides enough information about the error covariance matrix to derive a consistent unbiased estimate of * p*. a simple way to calculate * p * exists provided we can assume that the entries of the error matrix @xmath64 $ ] are i.i.d with mean zero and unknown variance @xmath65 .",
    "the estimated covariance matrix is then @xmath66 . a consistent estimate of @xmath65 is provided by the squared minimal singular value of @xmath67 $ ] @xcite . to address more general error distributions",
    ", we refer to the extended errors - in - variables modeling literature , see e.g. @xcite @xcite . using these consistent estimates instead of the true covariance matrix does not change the consistency properties of the parameter estimators for linear errors - in - variables models , of which the generalized sparse signal model ( [ eq2 generalized cs model ] ) is a special case @xcite @xcite .",
    "the newly proposed optimization model finds the sparsest solution among all possible solutions satisfying ( [ eq3.1 distortionless constraint ] ) .",
    "it can be formulated in a more generalized form as ( [ eq3 rl0 opt ] ) which uses regularization parameters to balance different constraints .",
    "^t } $ ] , incorrect solution ( blue point of intersection ) : @xmath68^t } $ ] .,scaledwidth=50.0% ]",
    "if we further assume the random elements of * e * are uncorrelated , * p * can be a diagonal matrix .",
    "if we assume the entries in the multiplicative uncertainty matrix * e * are uncorrelated random variables with the variances @xmath69 , ( [ eq3 rl0 opt ] ) can be simplified as @xmath70 where @xmath71 when we further assume the multiplicative uncertainties are equal with the same variances , i.e. @xmath72 , ( [ eq4.1 rl0 opt simplified ] ) is further simplified in another form of the elastic net @xcite : @xmath73 its performance for cs was evaluated in @xcite recently .",
    "similarly to the classical sparse signal recovery methods , several kinds of methods can solve the optimization model ( [ eq3 rl0 opt ] ) . in this section , convex relaxation and a greedy algorithm",
    "are used to solve it .",
    "a natural way relaxes the @xmath32 norm into the @xmath0 norm in ( [ eq3 rl0 opt ] ) , which achieves a convex optimization model : @xmath74 this newly formed one is called convex robust @xmath0 ( cr - l1 ) optimization .",
    "another equivalent formulation , which is also the convex relaxation of ( [ eq3 rl0 opt2 ] ) , is : @xmath75 several approaches exist to solve the cr - l1 optimization , such as interior - point methods , subgradient methods , splitting bregman algorithm , etc .",
    "the convergence can be guaranteed because of its convexity .    to explain why the proposed cr - l1 optimization ( [ eq4.1 crl1 opt ] ) is robust to multiplicative noise , we use the same example as fig .",
    "[ fig : contour1 ] .",
    "thus the covariance matrix is @xmath76 $ ] .",
    "one example of the simplified cr - l1 optimization is : @xmath77 to combine the ellipsoid constraint s robustness to multiplicative noise and @xmath0 ball constraint s sparsity , we use the mixed ball in fig . [",
    "fig : mixed ball ] .",
    "the tangent point of the minimized mixed ball ( @xmath0 ball + ellipsoid ) with the observed line is quite near the one of the minimized @xmath0 ball with the original line , and they are near the coordinate axes too .",
    "the additional quadratic term induces a slight compressibility loss but brings robustness to multiplicative noise in terms of better accuracy .",
    "therefore , we can see that the proposed mixed ball can achieve a robust compressible solution .    for analysis convenience , we assume there is no additive noise .",
    "therefore , one equivalent form of ( [ eq3 rl0 opt2 ] ) is : @xmath78    similarly , one equivalent form of ( [ eq4.1 crl1 opt ] ) , which is also the convex relaxation of ( [ eq4.1 simplified rl0 opt2 ] ) , is : @xmath79    [ theorem : condition ] assuming there is no additive noise , the cr - l1 optimization ( [ eq4.1 simplified crl1 opt2 ] ) can solve rl0 optimization ( [ eq4.1 simplified rl0 opt2 ] ) provided that @xmath80 where @xmath81 is a constant independent of the dimensions ; @xmath82 is the expectation of the compatible matrix norm of the @xmath83 vector norm .",
    "assuming the optimal solutions of the simplified rl0 optimization and cr - l1 optimization are : @xmath84 and @xmath85 the solutions of ( [ eq4.1 simplified rl0 opt : alpha ] ) can solve ( [ eq4.1 simplified crl1 opt : beta ] ) , if @xmath86 recalling @xmath87 is the kernel ( null space ) of @xmath88 .",
    "( [ eq4.1 condition ] ) means that in all the possible solutions of @xmath89 , @xmath90 also achieves the smallest value of @xmath91 .",
    "let @xmath92 be the support set @xmath93 and @xmath94 where @xmath95^t } $ ] , i.e. @xmath92 is the support of the nonzero entries of @xmath96 ; and @xmath97 is the support of the zero entries of @xmath96 .",
    "then , @xmath98 where @xmath99 keeps its entries corresponding to the support @xmath92 and let the others be zero ; and @xmath100 keeps its entries corresponding to the support @xmath101 and let the others be zero .",
    "furthermore , we have @xmath102 and @xmath103    combining ( [ eq4.1 condition : l1 - 1 ] ) and ( [ eq4.1 condition : ellipsoid-2 ] ) results in : @xmath104    from ( [ eq4.1 condition - all ] ) , we can see that ( [ eq4.1 condition ] ) holds provided that @xmath105 .",
    "in general we have @xmath106 .",
    "however , if the elements of @xmath107 are sampled i.i.d . from gaussian process with zero mean and unit variance , with high probability",
    ", we have @xmath108 where @xmath81 is a constant @xcite @xcite .",
    "when @xmath109 is gaussian , with high probability , we have ( [ eq4.1 condition ] ) holds if @xmath110 is satisfied .",
    "obviously we have @xmath111 .",
    "( [ eq4.1 exact condition_final ] ) can be met if ( [ eq4.1 condition_final ] ) holds .",
    "therefore , theorem [ theorem : condition ] is proved .    the similar sufficient condition for convex relaxation of ( [ eq4.1 elastic net ] )",
    "can be obtained if we let @xmath112 in ( [ eq4.1 condition_final ] ) .",
    "furthermore , if @xmath113 which means no noise in the model , the sufficient condition for standard bp for cs can be obtained , and the resulted condition agrees with previous conclusions too @xcite @xcite @xcite . in order to suppress the multiplicative noise , the proposed new method has a larger lower bound on the required number of measurements than that of the standard bp with no multiplicative noise .",
    "the requirement of additional measurements is the price paid for multiplicative noise suppression .",
    "the introduction of the @xmath83 norm constraint will not only enhance robustness , but also smooth the @xmath0 norm based penalty function , as can be seen in fig .",
    "[ fig : mixed ball ] @xcite . the convergence of the sub - gradient algorithm would be accelerated .      to reduce the computational complexity ,",
    "greedy algorithms can be used to solve the rl0 optimization model .",
    "in contrast to the classical omp which greedily chooses the atoms giving the minimum @xmath114 , we update by choosing the ones to minimize    @xmath115    to find the minimum with different values of @xmath116 , we can let @xmath117 \\\\",
    "=   - 2{{\\mathbf{y}}^t}{\\mathbf{\\overline { a } } } + 2{{\\mathbf{\\theta } } ^t}\\left ( { { { \\mathbf{\\overline { a}}}^t}{\\mathbf{\\overline { a } } } + { \\mathbf{p } } } \\right ) \\\\     = 0 , \\\\",
    "\\end{gathered }      \\label{eq:4.2:differential operation}\\ ] ] which results in a new equation : @xmath118 where @xmath119 @xmath120 therefore , in greedy algorithms , we can find one or several atoms which give the minimum residual for each iteration .",
    "i.e. we use the new `` sensing matrix '' * b * and `` measurements '' * z * instead of @xmath121 and * y*. with these transformations of sensing matrix and measurement , we can use all the greedy algorithms for cs as before @xcite .",
    "we should note that the new sensing matrix * b * should not be fully random but partly random .",
    "the component * p * is deterministic and may prevail when the multiplicative error is strong and its corresponding covariance matrix has a bad cs performance , such as a large coherence @xcite , a large restricted isometry constant ( ric ) @xcite",
    ". it can require a stricter condition for successful recovery .",
    "a generalized omp ( orthogonal matching pursuit ) , which is also called ommp ( orthogonal multi - matching pursuit ) , is used to realize the robust greedy algorithm @xcite @xcite .",
    "it is in the sense that multiple indices are identified in each iteration . when the number of identified indices is @xmath122 , ommp is equivalent to omp",
    ". the proposed robust orthogonal multiple matching pursuit ( rommp ) algorithm is summarized in algorithm [ alg1 ] .",
    "the algorithm can be stopped when the residual is smaller than a threshold @xmath123 which is proportional to the standard deviation of its additive gaussian noise @xcite .",
    "@xmath124 * input : * @xmath125 , @xmath126 in ( [ eq:4.2:b ] ) , @xmath127 in ( [ eq:4.2:z ] ) and @xmath128    @xmath124 * output : * @xmath129    @xmath124 initial iteration : @xmath130    @xmath124 initial support : @xmath131    @xmath124 initial residual : @xmath132    @xmath124 @xmath133",
    "in the numerical experiments with simulated data , the length of the sparse signal @xmath29 is _ n _ = 200 .",
    "it contains only a few nonzero entries . the number of nonzero entries of the sparse signal _ k _ = 10 .",
    "the locations of the nonzero entries vary randomly .",
    "it is normalized by its @xmath83 norm .",
    "the signal is sparse with respect to the canonical basis of the euclidean space , i. e. @xmath134 ; and the sampling matrix @xmath135 is gaussian distributed .",
    "the matrix @xmath28 are be generated by the signal model ( [ eq2 generalized a model 2 ] ) and ( [ eq2 generalized cs model ] ) , where @xmath136 and @xmath137 are generated by sampling a white gaussian distribution with zero mean , @xmath138 is an identity matrix , and @xmath139 is generated by sampling a uniform distribution with zero mean . to make the expression of the signal - to - multiplicative - noise ratio in the signal model convenient ,",
    "every column of @xmath140 and @xmath141 is normalized by its @xmath83 norm , but a uncertainty parameter @xmath142 is employed to weight the multiplicative noise matrix , i.e. @xmath143 . the standard deviation of the awgn * n * is @xmath144 .",
    "all the parameters in all three methods are chosen to give the best performance based on advanced searching .",
    "here @xmath145 , @xmath146 , @xmath59 and @xmath60 are chosen to achieve the best accuracy performance . getting these optimal values for the parameters",
    "is not straightforward . similarly to parameter estimation in bpdn",
    ", cross validation may be used resulting in additional computation burden .",
    "the number of iterations for srtls is 20 .",
    "before the number of iterations reach 20 , the error does not vary much . the matrix * p * is chosen as the sampled covariance matrix @xmath147 , where _ l _ is the number of monte carlo simulations , which is chosen to be _",
    "= 500 .    to quantify the performance of signal recovery",
    ", the estimation error is calculated via the normalized mean l - b error : @xmath148 and the mean coherence : @xmath149 where @xmath150 and @xmath151 are the real and estimated signals in the _ l_-th experiment , and they are normalized by their @xmath152 norms ; @xmath153 indicates different criteria for the evaluation of the estimation performance ; when _ b _ = 1 , we call @xmath154 the normalized mean l1 error ; and when _ b _ = 2 , @xmath155 is called the normalized mean l2 error .    fig .",
    "[ figure1 ] - fig .",
    "[ figure3 ] demonstrate the signal reconstruction performance with the normalized mean l1 and l2 errors , and mean coherence .",
    "[ figure1 ] gives the normalized mean l1 and l2 errors and mean coherence with the number of measurements ranging from _ m _ = 10 to 200 , when the uncertainty parameter @xmath156 ; fig .",
    "[ figure3 ] gives the normalized mean l1 and l2 errors and mean coherence with the uncertainty parameter ranging from @xmath157 to @xmath158 , when the number of measurements _ m _ = 100 .",
    "0.48 .,title=\"fig:\",width=340,height=188 ]       0.48 .,title=\"fig:\",width=340,height=188 ]       0.48 .,title=\"fig:\",width=340,height=188 ]    0.48        0.48        0.48     from fig .",
    "[ figure1 ] , we can see that the cr - l1 optimization outperforms or at least share the similar performance with the bpdn and srtls with all the possible number of measurements in terms of the normalized mean l1 and l2 errors and mean coherence , especially when the number of measurements is large .",
    "[ figure3 ] also shows that the cr - l1 optimization performs the best or achieves similar performance , and the performance improvement is especially obvious when the uncertainties are strong .      to test the proposed method for real - life data , we use ecg data which is obtained from the physiobank database @xcite @xcite @xcite .",
    "mobile ecg monitoring is one of the most popular applications in cs of ecg signals . in this application",
    ", the computational complexity should be as low as possible .",
    "therefore , in this group of numerical experiments , the greedy algorithms , i.e. omp , ommp with @xmath159 , and the proposed rommp with @xmath159 , are compared .",
    "the measurement matrix is the gaussian matrix .",
    "the utilized representation matrix is given by the orthogonal daubechies wavelets ( db 10 ) with the decomposition level 5 which is one of the most popular wavelet families for ecg compression @xcite .",
    "the ecg data has 15 channels with 37888 samples for each channel . in each channel ,",
    "the data are divided into 37 segments , i.e. the length of the signal in each reconstruction is _",
    "= 1024 . the sensing matrix error",
    "is generated similarly to that in section [ sec5.1 ] , but only the dictionary uncertainty matrix is set to be 0 .",
    "in addition , the standard deviation of the awgn * n * is @xmath160 .",
    "[ figecg ] shows part of an ecg signal and its estimates from sub - samples by omp , ommp and rommp when the number of the compressive measurements is _ m _ = 410 and @xmath161 .",
    "we can see that the signal reconstructed by rommp is less noisy than the ones reconstructed by the omp and ommp .    in fig .",
    "[ figure20 ] , the normalized mean l1 and l2 errors and mean coherence for the reconstruction of ecg signal from the noisy compressive measurements with different number of measurements is shown when the uncertainty parameter is @xmath156 ; and fig .",
    "[ figure21 ] shows the normalized mean l1 and l2 errors and mean coherence for the reconstruction of the ecg signal from the noisy compressive measurements with different uncertainty parameters when the number of measurements are 512 .",
    "it can be seen that omp and ommp have almost the same reconstruction accuracy .",
    "however , rommp improves the accuracy much better over various number of measurements and uncertainty degrees compared to omp and ommp .",
    "0.48        0.48        0.48     0.48        0.48        0.48     0.48        0.48     to compare the computational complexity , fig .",
    "[ figure22 ] shows the mean number of iterations at various percentages of additive and multiplicative noise and various number of compressive measurements .",
    "we can see that omp needs the largest number of iterations .",
    "the proposed rommp needs less iterations than ommp . considering that the computational complexity of each iteration of omp , ommp , and rommp is almost the same",
    ", we can conclude that the rommp has the least computational complexity .",
    "in this paper , we discuss the sampling and representation uncertainties in cs . a generalized sparse signal model considering both multiplicative noise and additive noise is given . based on this model , a new optimization model for robust recovery of the generalized sparse signal",
    "is deduced by a stochastic analysis . both convex relaxation and a greedy algorithm",
    "are used to solve the optimization model .",
    "sufficient conditions for successful recovery are analyzed .",
    "numerical experiments show that the proposed rl0 optimization based algorithms are in general superior to the previous ones .",
    "e.  j. cands , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ information theory , ieee transactions on _ , vol .",
    "52 , no .  2 ,",
    "489509 , 2006 .",
    "d.  h. chae , p.  sadeghi , and r.  a. kennedy , `` effects of basis - mismatch in compressive sampling of continuous sinusoidal signals , '' in _ future computer and communication ( icfcc ) , 2010 2nd international conference on _ , vol .",
    "2.1em plus 0.5em minus 0.4emieee , 2010 , pp . v2739 .",
    "j.  t. parker , v.  cevher , and p.  schniter , `` compressive sensing under matrix uncertainties : an approximate message passing approach , '' in _ signals , systems and computers ( asilomar ) , 2011 conference record of the forty fifth asilomar conference on_.1em plus 0.5em minus 0.4emieee , 2011 , pp .",
    "804808 .",
    "f.  krzakala , m.  mzard , and l.  zdeborov , `` compressed sensing under matrix uncertainty : optimum thresholds and robust approximate message passing , '' in _ acoustics , speech and signal processing ( icassp ) , 2013 ieee international conference on_.1em plus 0.5em minus 0.4em ieee , 2013 , pp . 55195523 .",
    "z.  yang , c.  zhang , and l.  xie , `` robustly stable signal recovery in compressed sensing with structured matrix perturbation , '' _ signal processing , ieee transactions on _ , vol .",
    "60 , no .  9 ,",
    "pp . 46584671 , 2012 .",
    "a.  l. goldberger , l.  a. amaral , l.  glass , j.  m. hausdorff , p.  c. ivanov , r.  g. mark , j.  e. mietus , g.  b. moody , c .- k .",
    "peng , and h.  e. stanley , `` physiobank , physiotoolkit , and physionet components of a new research resource for complex physiologic signals , '' _ circulation _ , vol . 101 , no .",
    "e215e220 , 2000 .",
    "h.  mamaghanian , n.  khaled , d.  atienza , and p.  vandergheynst , `` compressed sensing for real - time energy - efficient ecg compression on wireless body sensor nodes , '' _ biomedical engineering , ieee transactions on _ , vol .",
    "58 , no .  9 , pp . 24562466 , 2011 .",
    "y.  liu , m.  de  vos , i.  gligorijevic , v.  matic , y.  li , and s.  van  huffel , `` multi - structural signal recovery for biomedical compressive sensing , '' _ ieee transactions on biomedical engineering _ , vol .  60 , no .  10 , pp .",
    "27942805 , 2013 ."
  ],
  "abstract_text": [
    "<S> compressed sensing ( cs ) shows that a signal having a sparse or compressible representation can be recovered from a small set of linear measurements . in classical cs theory , </S>",
    "<S> the sampling matrix and representation matrix are assumed to be known exactly in advance . </S>",
    "<S> however , uncertainties exist due to sampling distortion , finite grids of the parameter space of dictionary , etc . in this paper , we take a generalized sparse signal model , which simultaneously considers the sampling and representation matrix uncertainties . based on the new signal model , a new optimization model for robust sparse signal reconstruction is proposed . this optimization model can be deduced with stochastic robust approximation analysis . </S>",
    "<S> both convex relaxation and greedy algorithms are used to solve the optimization problem . for the convex relaxation method </S>",
    "<S> , a sufficient condition for recovery by convex relaxation is given ; for the greedy algorithm , it is realized by the introduction of a pre - processing of the sensing matrix and the measurements . in numerical experiments , both simulated data and real - life ecg data based results </S>",
    "<S> show that the proposed method has a better performance than the current methods .    </S>",
    "<S> shell : bare demo of ieeetran.cls for journals    compressed sensing , robust sparse signal recovery , sampling uncertainty , dictionary uncertainty . </S>"
  ]
}