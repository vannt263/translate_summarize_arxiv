{
  "article_text": [
    "generalization is one of the basic notions in machine learning . yet , in the existing literature , usually only the indicators of generalization quality like the mean square error over the test samples are presented , without a more detailed study of the characteristics of the generalization functions produced by different learning machines .    in this paper , a special kind of generalization is analyzed , on the example of classic feedforward neural networks with linear weight functions . in the discussed generalization type ,",
    "generalized samples exist which are distant to any training samples .",
    "the distance of two samples is defined as the distance @xmath0 between the independent variables of the samples , in the input space of a feedforward learning machine @xmath1 .",
    "for example , let the sample @xmath2 be @xmath3 where the independent variables are @xmath4 and @xmath5 , and the dependent variable is @xmath6 .",
    "then , the discussed distance @xmath0 between two samples @xmath7 and @xmath8 might be defined as the euclidean distance between the points in the input space of @xmath1 , whose coordinates are the independent variables @xmath9 and @xmath10 .",
    "if a generalized sample @xmath11 is distant from any training samples , it means that there are different groups of training samples , that might be expected to compete in generalizing @xmath11 .",
    "input_space.latex    let us discuss examples of the distant and , conversely , close samples .",
    "[ fig : input_space ] illustrates an input space of a feedforward learning machine .",
    "let the learning machine has two inputs @xmath12 and @xmath13 .",
    "let there be some samples in the space , whose independent variables @xmath14 determine the respective position in the input space , and which have a dependent variable @xmath6 .",
    "let the training samples have the values of @xmath6 equal to either @xmath15 or @xmath16 , and let us call these samples ` 0 ' or ` 1 ' samples , respectively .",
    "let there be also two generalized samples absent in the training set , whose dependent variables are unknown , and thus their @xmath6 values are denoted by @xmath17 and @xmath18 . the sample with @xmath19 ,",
    "let us call it @xmath20 , can be regarded as a close one  it is near only to a cluster of ` 1 ' samples , and it is likely that the user of the learning machine expects that the dependent variable of the sample should be estimated to a value that is close to @xmath16 . let the sample with @xmath21 be called @xmath22 .",
    "at least three obvious ways of generalization of @xmath22 can be thought of :    * in the surrounding of @xmath22 , there are some ` 0 ' samples and some ` 1 ' samples in an approximate balance , thus , the dependent variable of @xmath22 should be equal to about @xmath23 . *",
    "all samples ` 1 ' create together a single horizontal stripe  shaped feature , and @xmath22 is inside the feature .",
    "additionally ,  0 s create two horizontal stripe  shaped features and @xmath22 is outside each one .",
    "thus , the dependent variable of @xmath22 should be equal to about @xmath16 . *",
    "the closest training sample to @xmath22 is ` 0 ' , so , the dependent variable of @xmath22 should be equal to about @xmath15 .",
    "thus , groups of samples of different type were discerned around @xmath22 , that can compete in generalizing of @xmath22 .",
    "the sample @xmath22 is thus regarded as a distant sample .",
    "it will be shown , that such alternate ways of generalization , in the case of the feedforward neural networks , may sometimes produce a random and spurious generalization .",
    "that is , the problem of long distance generalization may sometimes be solved well by the neural network , but in some other cases the network may give quite unexpected results , being the artifacts revealing an internal structure of the learning machine rather than a likely estimation hypothesis .",
    "the performance of support vector machines will be presented as well , to show the generalization differences that exist between different types of learning machines .",
    "in a feedforward neural network ( fnn ) , the combination function in a neuron of the mcculloch type @xcite is a linear combination of the input values of the neuron . to obtain",
    "the output value of the neuron , the value of the combination function is non - linearly transformed , typically using a sigmoidal or hyperbolic tangent activation function .",
    "it means that the neuron acts the same for arguments that create hyperplanes in the space of the domain of the neuron .",
    "for example , there is a hyperplane @xmath24 , for which the output value of the neuron is constant and equal to @xmath25 .",
    "the partial derivatives of the neuron function against each of the inputs of the neuron are constant for @xmath24 as well .",
    "it might be said , thus , that a trained neuron transfers the properties of some samples , that it learned during the training process , over infinitely large regions in the input space of the neuron , because hyperplanes are infinite .",
    "the infinity of the transfer might make fnns good for distant generalizations , as it will be further shown in tests . on the other hand ,",
    "though , the infinite transfer may sometimes produce wrong results , because a training sample @xmath26 may influence on the generalization of some sample @xmath27 even if these samples are very distant from each other .",
    "but , intuitively , samples that are very far from each other might have nothing in common .",
    "let us discuss a real process of training a fnn with two kinds of data  the first one , @xmath28 , deliberately constructed to simplify the distant generalization , and the second one , @xmath29 , constructed to make the generalization complex to solve by the fnn .    0.3 in    [ cols=\"^,^,^,^,^ \" ,",
    "]     let us compare the fnns to svms @xcite .",
    "svms give very different results for both sets .",
    "example results are illustrated in fig .",
    "[ fig : svm - results ] .",
    "the particular example used @xmath30svc @xcite trained using libsvm @xcite . in the particular examples ,",
    "svms solved the problem of distant generalization in a different way than the tested fnns in the case of both the set @xmath28 and the set @xmath29 .",
    "the svms were able to produce a generalization with minimal artifacts if their learning coefficients allowed for a proper fitting to the training data , as seen in fig .",
    "[ fig : svm - results](c ) and ( f ) .",
    "the svms have a large test error for both sets , though , as they did not fuse @xmath31 into a single set of parallel bars .",
    "thus , fnns have a smaller test error for @xmath28 , because they could fuse the features @xmath31 , and both fnns and svms have a relatively large test error for @xmath29 , but for different reasons .",
    "the distant generalization may work quite differently for different training sets and for different learning machines .",
    "in particular , the resulting generalizing functions may contain artifacts , related to the internal structure of the learning machine .",
    "for example , the classic fnns with linear combination functions and hyperbolic tangent activation functions may introduce substantial random artifacts to the generalizing functions . in some applications where the stability of the results is important ,",
    "usage of such fnns might thus be discouraged .",
    "but , conversely , the tested fnns , thanks to the structure of neurons , can be capable of generalizing by extending and fusing together elongated features that exist in the training set .",
    "krogh , a. and hertz , j.  a. ( 1992 ) .",
    "a simple weight decay can improve generalization . in moody , j.  e. , hanson , s.  j. , and lippmann , r.  p. , editors , _ advances in neural information processing systems _ , volume  4 , pages 950957 .",
    "morgan kaufmann publishers , inc ."
  ],
  "abstract_text": [
    "<S> this paper discusses the notion of generalization of training samples over long distances in the input space of a feedforward neural network . </S>",
    "<S> such a generalization might occur in various ways , that differ in how great the contribution of different training features should be .    </S>",
    "<S> the structure of a neuron in a feedforward neural network is analyzed and it is concluded , that the actual performance of the discussed generalization in such neural networks may be problematic  while such neural networks might be capable for such a distant generalization , a random and spurious generalization may occur as well .    to illustrate the differences in generalizing of the same function by different learning machines , </S>",
    "<S> results given by the support vector machines are also presented .    </S>",
    "<S> * keywords : supervised learning , generalization , feedforward neural network , support vector machine * </S>"
  ]
}