{
  "article_text": [
    "]    given users ratings of movies or products , how can we model a user s preferences for different types of items and recommend other items that the user will like ?",
    "this problem , often referred to as the netflix problem , has generated a flurry of research in collaborative filtering , with a variety of proposed matrix factorization models and inference methods .",
    "top recommendation systems have used thousands of factors per item and per user , as was the case in the winning submissions in the netflix prize @xcite .",
    "recent state - of - the - art methods have relied on learning even larger , more complex factorization models , often taking nontrivial combinations of multiple submodels @xcite .",
    "such complex models use large amounts of memory , are increasingly difficult to interpret , and are often difficult integrate into larger systems .",
    "our approach is drastically different from previous collaborative filtering research .",
    "rather than start with the assumptions of a matrix factorization model , we make _ co - clustering _",
    "effective for high quality matrix completion and approximation .",
    "co - clustering has been well studied @xcite but was not previously competitive in large behavior modeling and matrix completion problems . to achieve state of the art results",
    ", we use an _ additive model of simple co - clusterings _ that we call stencils , rather than building a large single co - clustering .",
    "the result is a model that is conceptually simple , has a small parameter space , has interpretable structure , and achieves the best published accuracy for matrix completion on netflix , as seen in figure [ fig : netflix_test_crownjewel ] .    using a linear combination of co - clusterings corresponds to a rather different interpretation of user preferences and movie properties .",
    "matrix factorization assumes that a movie preference is based on a weighted sum of preferences for different genres , with the movie properties being represented in vectorial form .",
    "for instance , if a user likes comedies but not romantic movies , then a romantic comedy may have a predicted neutral 3-star rating .",
    "co - clustering on the other hand assumes there exists some `` correct '' partitioning of movies ( and users ) .",
    "for instance , a user might be part of a group that likes all comedies but does not like romantic movies .",
    "correspondingly , all romantic comedies might be aggregated into a cluster , possibly partitioned further into pg-13 rated , or r - rated romantic comedies .",
    "this quickly leads to a combinatorial explosion .    by taking a linear combination of co - clusterings we benefit from both perspectives",
    ": there is no single correct partitioning of movies and users ; however , we can use the membership in several independent groups to encode the _ factorial _ nature of attributes without incurring the cost of a necessarily high - dimensional model of matrix factorization .",
    "for instance , a movie may be \\{funny , sad , thoughtful } , it might have a certain age rating , it might be an \\{action , romantic , thriller , documentary , family } movie , it might be shot in a certain visual style , and by a certain group of actors . by taking linear combinations of co - clusterings we can take these attributes into account .",
    "the mathematical challenge that motivated this work is that , in order to encode a rank-@xmath0 matrix by a factorization , we need @xmath0 numbers per row ( and column ) respectively . with linear combinations of stencils , on the other hand ,",
    "we only need @xmath1 bits per row ( and column ) plus @xmath2 floating point numbers regardless of the size of the matrix .",
    "we denote by a stencil a small @xmath3 template of a matrix and its mapping to the row and column vectors respectively .",
    "this is best understood by the example below : assume that we have two simple stencils containing @xmath4 and @xmath5 co - clusterings .",
    "their linear combination yields a rather nontrivial @xmath6 matrix of rank @xmath7 .",
    "in contrast , classic co - clustering would require a @xmath8 partitioning to match this structure .",
    "when we have @xmath9 stencils of size @xmath3 , this requires a @xmath10 partitioning .    by",
    "design our model has a parameter space that is an order of magnitude smaller than competing methods , requiring only @xmath11 bits per user and per movie and @xmath12 floating point numbers , where @xmath0 is generally quite small .",
    "this is computationally advantageous of course , but also demonstrates that our modeling assumptions better match real world structure of human decision making .",
    "finding succinct models for binary matrices , e.g.  by minimizing the minimum description ( mdl ) , has been the focus of significant research and valuable results in the data mining community @xcite .",
    "that said , these models are quite different . to the best of our knowledge ,",
    "ours is the first work aimed at finding a parsimonious model for general ( real - valued ) matrix completion and approximation .",
    "our paper makes a number of contributions to the problem of finding sparse representations of matrices .",
    "* we present accams , an iterative @xmath0-means style algorithm that minimizes the approximation error by backfitting the residuals of previous approximations . *",
    "we provide linear approximation rates exploiting the geometry of rows and columns of rating matrix via bounds on the metric entropy of banach spaces .",
    "* we present a generative bayesian non - parametric model and devise a collapsed gibbs sampling algorithm , baccams , for efficient inference .",
    "* experiments confirm the efficacy of our approach , offering the best published results for matrix completion on netflix , an interpretable hierarchy of content , and succinct matrix approximations for ratings , image , and network data .",
    "we believe that these contributions offer a promising new direction for behavior modeling and matrix approximation",
    ".    * outline . *",
    "we begin by discussing related work from recommendation systems , non - parametric bayesian models , co - clustering , and minimum description length .",
    "we subsequently introduce the simple @xmath0-means style co - clustering and its approximation properties in section [ sec : matapp ] .",
    "subsequently , in section [ sec : single ] we define our bayesian co - clustering model and collapsed gibbs sampler for a single stencil .",
    "in section  [ sec : many ] we extend our bayesian model to multiple stencils .",
    "section  [ sec : experiments ] reports our experimental results and we conclude with a discussion of future directions for the work .",
    "* recommender systems . * probably the closest to our work is the variety of research on behavior modeling and recommendation .",
    "matrix factorization approaches , such as koren s",
    "svd++ @xcite , have enjoyed great success in recommender systems .",
    "recent models such as dfc @xcite and llorma @xcite have focused on using ensembles of factorizations to exploit local structure .",
    "more closely related to our model are bayesian non - parametric approaches . for instance , @xcite use the indian buffet process ( ibp ) for recommendation . in doing",
    "so they assume that each user ( and movie ) has certain preferential binary attributes .",
    "it can be seen as an extreme case of accamswhere the cluster size @xmath13 , while using a somewhat different strategy to handle cluster assignment and overall similarity within a cluster . following a similar intuition as accamsbut different perspective and focus",
    ", @xcite extended the ibp to handle @xmath14 for link prediction tasks on binary graphs .",
    "our work differs in its focus on general , real - valued matrices , its application of co - clustering , and its significantly simpler parameterization .",
    "a co - clustering approach to recommendation was proposed by @xcite .",
    "this model uses co - clustering to allow for sharing of strength within each group .",
    "however , it does not overcome the rank-@xmath0 problem , i.e.  while clustering reduces intra - cluster variance and improves generalization , it does not increase the rank beyond what a simple factorization model is capable of doing . finally , @xcite proposed a factorization model based on a dirichlet process over users and columns .",
    "all these models are closely related to the mixed - membership stochastic blockmodels of @xcite",
    ".    * co - clustering .",
    "* it was was originally used primarily for understanding the clustering of rows and columns of a matrix rather than for matrix approximation or completion @xcite .",
    "this formulation was well suited for biological tasks , but it computationally evolved to cover a wider variety of objectives @xcite .",
    "@xcite defined a soft co - clustering objective akin to a factorization model .",
    "recent work has defined a bayesian model for co - clustering focused on matrix modeling @xcite .",
    "@xcite focuses on exploiting co - clustering ensembles , but do so by finding a single consensus co - clustering . as far as we know",
    ", ours is the first work to use an additive combination of co - clusterings .    * matrix approximation .",
    "* there exists a large body of work on matrix approximation in the theoretical computer science community .",
    "they focus mainly on efficient low - rank approximations , e.g.  by projection or by interpolation",
    ". examples of the projection based strategy are @xcite .",
    "essentially one aims to find a general low - rank approximation of the matrix , as is common in most recommender models .",
    "a more parsimonious strategy is to seek _ interpolative _ decompositions .",
    "there one aims to approximate columns of a matrix by a linear combination of a subset of other columns @xcite .",
    "nonetheless this requires us to store at least one , possibly more scaling coefficients per column .",
    "also note the focus on column interpolations  this can easily be extended to row and column interpolations , simply by first performing a row interpolation and then interpolating the columns . to the best of our knowledge ,",
    "the problem of approximating matrices with piecewise constant block matrices as we propose here is not the focus of research in tcs .",
    "* succinct modeling . *",
    "the data mining community has focused on finding succinct models of data , often directly optimizing the model size described by the minimum description language ( mdl ) principle @xcite .",
    "finding effective ways to compress real world data allows for better modeling and understanding of the datasets .",
    "this approach has led to valuable results in pattern and item - set mining @xcite as well as graph summarization @xcite .",
    "however , these approaches typically focus on modeling databases of discrete items rather than real - valued datasets with missing values .",
    "before delving into the details of bayesian non - parametrics we begin with an optimization view of accams .",
    "key to our model is the notion of a stencil , an extremely easy to represent block - wise constant rank-@xmath0 matrix .",
    "a stencil @xmath15 is a matrix @xmath16 with the property that @xmath17 for a template and discrete index vectors and @xmath18 respectively .",
    "given a matrix @xmath19 it is now our goal to find a stencil @xmath20 such that the approximation error @xmath21 is small while simultaneously the cost for storing @xmath22 is small . in the context of the example above",
    ", the @xmath6 matrix is given by the sum of two stencils , one of size @xmath4 and one of size @xmath23 .",
    "this already indicates that we may require more than one stencil for efficient approximation . in general",
    ", our model will be such as to solve @xmath24 that is , we would like to find an additive model of @xmath9 stencils that minimizes the approximation error to @xmath25 .",
    "such an expansion affords efficient compression using a trivial codebook , as can be seen below .",
    "the stencil @xmath15 can be + stored at @xmath26 element - wise accuracy at no more cost than @xmath27    this follows directly from the construction . storing the vector @xmath28 costs at most @xmath29 bits if we assume a uniform code ( this also holds for @xmath30 ) .",
    "when storing @xmath31 approximately , we must not quantize at a level of the approximation error or higher .",
    "hence , a simplistic means of encoding the dynamic range requires @xmath32 bit , which is used on a per - element basis in @xmath31 .",
    "note that considerably better codes _ may _ exist whenever the entropy of @xmath28 is less than @xmath33 ( likewise for @xmath30 ) .",
    "nonetheless , even the crude @xmath33 bound is already much better than what can be accomplished by a low - rank factorization .",
    "obviously , given @xmath25 , it is our goal to _ find _ such stencils @xmath20 with good approximation properties .",
    "unfortunately , finding linear combinations of co - clusterings is as hard as co - clustering : assume that we are given all but one stencil of the optimal solution . in this case our problem reduces to co - clustering as its subproblem , which is np - hard .",
    "we consider a simple iterative procedure in which stencils are computed one at a time , using the residuals as input .",
    "the inner loop consists of a simple algorithm that is reminiscent of @xmath0-means clustering .",
    "it proceeds in two stages . without loss of generality",
    "we assume that we have more rows than columns , i.e.  @xmath34 with @xmath35 .",
    "* row clustering .",
    "* we first perform @xmath0-means clustering of the rows .",
    "that is , we aim to find an approximation of @xmath25 that replaces all rows by a small subset thereof .",
    "note that this is more stringent than the interpolative approximations of matrices which only require us to find a set of rows which will form a ( possibly sparse ) basis for all other rows .",
    "draw @xmath36 rows from @xmath25 at random without replacement and copy them to @xmath37 .",
    "@xmath38 and @xmath39 @xmath40 @xmath41 @xmath42 @xmath43 @xmath44 and @xmath45 @xmath46 clusters @xmath47 , ids @xmath28 , counts @xmath48    algorithm  [ alg : rowcluster ] is essentially @xmath0-means clustering on the rows of @xmath25 .",
    "once we have this , we now cluster the columns of the new matrix in an analogous manner .",
    "the only difference is that the approximation needs to be particularly good for row clusters that occur frequently .",
    "consequently the approximation measure @xmath49 is replaced by the mahalanobis distance .",
    "that is , the only substantial difference to algorithm  [ alg : rowcluster ] is that now we use the assignment @xmath50 where @xmath51 is the matrix obtained by stacking @xmath52 and @xmath53 is the diagonal matrix of counts , i.e.  @xmath54 .",
    "* missing entries . * in many cases , however , @xmath25 itself is incomplete .",
    "this can be addressed quite easily by replacing the assignment @xmath55 by @xmath56 where we used @xmath57 as a shorthand for the existing entries in @xmath25 . in finding a good cluster for the row",
    "@xmath58 we restrict ourselves to the coordinates in @xmath59 where @xmath60 exists .    likewise , for the purpose of obtaining the column clusters , we now need to keep track for each coordinate in @xmath61 how many elements in @xmath25 contributed to it .",
    "correspondingly denote by @xmath62 the number of entries mapped into coordinate @xmath63 .",
    "then the assignment for column clusters is obtained via @xmath64 and likewise the averages are now per - coordinate according to the counts for both @xmath65 and @xmath66 .",
    "* backfitting . *",
    "the outcome of row and column clustering is a stencil @xmath15 consisting of the clusters obtained by first row and then column clustering and of the assignment vectors @xmath28 and @xmath30 once the process is complete",
    ". it may be desirable to alternate between row and column clustering for further refinement .",
    "since each step can only reduce the objective function further and the state space of @xmath67 is discrete , convergence to a local minimum is assured , with the same caveat on solution quality as in @xmath0-means clustering .",
    "the last step is to take the residual @xmath21 and use it as the starting point of a new approximation round .",
    "matrix @xmath25 , clusters @xmath68 , max stencils @xmath9 @xmath69 @xmath70 @xmath71 @xmath72 and back up @xmath73    essentially the last stage is used to ensure that the stencil has minimum approximation error given the partitioning .",
    "this procedure is repeatedly invoked on the residuals to minimize the loss .",
    "the result is an additive model of co - clusterings .",
    "a key question is how well any given matrix @xmath25 can be approximated by an appropriate stencil .",
    "for the sake of simplicity we limit ourselves to the case where all entries of the matrix are observed .",
    "we use covering numbers and spectral properties of @xmath25 to obtain approximation guarantees .",
    "denote by @xmath74 a banach space .",
    "then for any given set @xmath75 the covering number @xmath76 is given by the set of points @xmath77 such that for any @xmath78 there exists some @xmath79 with @xmath80 .",
    "of particular interest for us are covering numbers @xmath81 of unit balls and their functional inverses @xmath82 .",
    "the latter are referred to as entropy number and they quantify the approximation error incurred by using a cover of @xmath83 elements ( * ? ? ?",
    "* chapter 8) .",
    "a key tool for computing entropy numbers of scaling operators is the theorem of gordon , knig and schtt @xcite , relating entropy numbers to singular values .",
    "[ th : svd ] denote by @xmath53 a diagonal scaling operator with @xmath84 with scaling coefficients @xmath85 for all @xmath86 .",
    "then for all @xmath87 the entropy number @xmath88 is bounded via @xmath89    this means that if we have a matrix with rapidly decaying singular values , we only need to focus on the leading largest ones in order to approximate all elements in the space efficiently . here",
    "the tradeoff between dimensionality and accuracy is obtained by using the harmonic mean .",
    "[ cor : ball ] the covering number of a ball @xmath74 of radius @xmath90 in @xmath91 is bounded by @xmath92    this follows directly from using the linear operator @xmath93 for @xmath94 . here",
    "the scaling operator has eigenvalues @xmath95 for all @xmath96 and @xmath97 for @xmath98 .",
    "the maximum in is always @xmath99 .",
    "the following theorem states that we can approximate @xmath25 up to a multiplicative constant at any step , provided that we pick a large enough clustering .",
    "it also means that we get linear convergence , i.e.convergence in @xmath100 steps to @xmath101 error , since the bound can be applied iteratively .",
    "denote by @xmath102 the singular values of @xmath25 . then using @xmath48 clusters for rows and columns respectively",
    "the matrix @xmath25 can be approximated with error at most @xmath103 here @xmath104 is given by theorem  [ th : svd ] and corollary  [ cor : ball ] respectively .    using the singular value decomposition of @xmath25 into @xmath105 we can factorize @xmath106 where @xmath107 and @xmath108 . by construction ,",
    "the singular values of @xmath109 and @xmath110 are @xmath111 .",
    "we now cluster the rows of @xmath109 and @xmath110 independently to obtain an approximation of @xmath25 .    for @xmath109 we know that its rows can be approximated by @xmath48 balls with error @xmath112 as per theorem  [ th : svd ] .",
    "also note that its row vectors are contained in the image of the unit ball under @xmath109  if they were not , project them onto the unit ball and the approximation error can not increase since the targets are within the unit ball , too .",
    "hence the @xmath104-cover of the latter also provides an approximation of the row - vectors of @xmath109 by @xmath113 with accuracy @xmath104 , where @xmath113 contains at most @xmath48 distinct rows .",
    "the same holds for the matrix @xmath110 , as approximated by @xmath114 .",
    "hence we have @xmath115 this provides a _ pointwise _ approximation guarantee.if we only have a bound on the rank and on @xmath116 , this yields @xmath117 moreover , since each row in @xmath109 and @xmath110 respectively will be approximated with residual bounded by @xmath104 we can bound @xmath118 and @xmath119 respectively .",
    "this yields a bound on the matrix norm of the residual via @xmath120 this bounds the matrix norm of the residual .",
    "note that the above is an _ existence _",
    "proof rather than a constructive prescription .",
    "however , by using the fact that set cover is a submodular problem @xcite , it follows that given @xmath48 , we are able to obtain a near - optimal cover , thus leading to a _ constructive _ algorithm . note , however , that the main purpose of the above analysis is to obtain theoretical upper bounds on the rate of convergence . in practice , the results can be considerably better , as we show in section  [ sec : experiments ] .",
    "in the same manner as many risk minimization problems ( e.g.  penalized logistic regression ) have a bayesian counterpart ( gaussian process classification ) @xcite , we now devise a bayesian counterpart to accams , which we will refer to as baccams .",
    "we begin with the single stencil case in section  [ sec : single ] and extend it to many stencils in section  [ sec : many ] .",
    "; ( gammatau ) edge[thick ] ( tau ) ( tau ) edge[thick ] ( s ) ( alpha ) edge[thick ] ( c ) ( beta ) edge[thick ] ( d ) ( c ) edge[thick ] ( r ) ( d ) edge[thick ] ( r ) ( s ) edge[thick ] ( r ) ( sigma ) edge[thick ] ( r ) ( gamma ) edge[thick ] ( sigma ) ;    background ( usercluster ) [ plate , fit=(c ) ]   + for @xmath121 ; ( moviecluster ) [ plate , fit=(d ) ]   + for @xmath122 ; ( stencil ) [ plate , fit=(s ) ]   + for @xmath123 ; ( ratings ) [ plate , fit=(r ) ]   + for @xmath124 ; ( stencils ) [ plate , fit=(usercluster ) ( moviecluster ) ( tau ) ( stencil ) ]   + for all @xmath48 ;      we begin with a simplistic model of co - clustering .",
    "it serves as the basic template for single - matrix inference .",
    "all subsequent steps use the same idea . in a nutshell",
    ", we assume that each user @xmath121 belongs to a particular cluster @xmath125 drawn from a chinese restaurant process @xmath126 .",
    "likewise , we assume that each movie @xmath122 belongs to some cluster @xmath127 drawn analogously from @xmath128 .",
    "the scores of the matrix @xmath129 are obtained from a stencil @xmath130 with additive noise @xmath131 .",
    "the stencil values @xmath132 themselves are drawn from a normal distribution @xmath133 . in turn , the variances @xmath134 and @xmath135 are obtained via a conjugate prior , i.e.  the inverse gamma distribution .    this is an extremely simple model similar to @xcite , akin to a decision stump .",
    "the rationale for picking such a primitive model is that we will be combining linear combinations thereof to obtain a very flexible tool .",
    "the model is shown in figure [ fig : multi_stencil_model ] .",
    "for @xmath136 the formal definition is as follows :    @xmath137    recall that the inverse gamma distribution is given by @xmath138 consequently the joint probability distribution over all scores , given the variances is given by @xmath139 the idea is that each user and each movie are characterized by a simple cluster .",
    "since we chose all priors to be conjugate to the likelihood terms , it is possible to collapse out the choice of @xmath132 .",
    "this is particularly useful as it allows us to accelerate the sampler considerably  now we only sample over the discrete random variables @xmath140 indicating the cluster memberships for a particular user and movie . in other words , we obtain a closed form expression for @xmath141",
    ". moreover , @xmath142 and @xmath143 are both inverse gamma due to conjugacy , hence we can sample them efficiently after sampling @xmath31 .      in the following",
    "we discuss a partially collapsed gibbs sampler ( effectively we use a rao - blackwellization strategy when sampling cluster memberships ) for efficient inference .",
    "we begin with the part of sampling @xmath144 .",
    "chinese restaurant process : : :    it is well known that for exponential families the conjugate    distribution allows for collapsing by taking ratios between    normalization coefficients with and without the additional sufficient    statistics item .",
    "see e.g. ( * ? ? ?",
    "* appendix a ) for a detailed    derivation . denote by @xmath145 the number of users and    movies belonging to clusters @xmath86 and @xmath146    respectively .",
    "moreover , denote by @xmath83 and @xmath122    the total number of users and movies , and by @xmath147    the number of clusters . in this case",
    "we can express    @xmath148 an analogous expression is available for    @xmath149 and @xmath150 .",
    "note that the superscript    @xmath151 denotes that the @xmath86-th    observation is left out when computing the statistic .",
    "large values of    @xmath152 and @xmath153 encourage the formation    of larger numbers of clusters .",
    "the collapsed expressions will be    useful for gibbs sampling . integrating out @xmath31 : : :    for faster mixing we need to integrate out @xmath31 whenever we    resample @xmath28 and @xmath30 .",
    "as we shall see , this is    easily accomplished by keeping simple linear statistics of the    ratings . moreover , by integrating out @xmath31 we avoid the    problem of having to instantiate a new value whenever a new cluster is    added .    +    for a given block @xmath154 with associated    @xmath132 , the distribution of ratings is gaussian with    mean @xmath155 and with covariance matrix    @xmath156 ( due to the independence    of the variances and the additive nature of the normal distribution ) .",
    "here we use @xmath157 to denote the identity matrix and    @xmath158 to denote the vector of all @xmath158 .",
    "denote by    @xmath159 the number of rating pairs @xmath160    for which @xmath161 and @xmath162 .",
    "moreover , denote by    @xmath163 the vector of associated ratings .",
    "hence , the    likelihood of the cluster block @xmath154 , as observed in    @xmath163 is @xmath164 in computing the above expression we need    to compute the determinant of a diagonal matrix with rank-1 update ,    and the inverse of said matrix . for the former",
    ", we use the    matrix - determinant lemma , and for the latter , the    sherman - morrison - woodbury formula : @xmath165    this allows us to assess whether it is beneficial to assign a user    @xmath121 or a movie @xmath122 to a different or a new    cluster efficiently , since the only statistics involved in the    operation are sums of residuals and of their squares .",
    "this leads to a collapsed gibbs - sampling algorithm . at each step",
    "we check how likelihoods change by assigning a movie ( or user ) to another cluster .",
    "we denote by @xmath166 the new cluster count and by @xmath167 the new set of residuals .",
    "let @xmath168 be a constant offset , in log - space , that only depends on the additional ratings that are added to a cluster .",
    "in other words , it is _ independent _ of the cluster that the additional scores are assigned to . hence @xmath169 can be safely ignored .",
    "@xmath170 for a new cluster this can be simplified since there is no data , hence @xmath171 and @xmath172 $ ] .",
    "@xmath173 the above expression is fairly straightforward to compute : we only need to track @xmath159 , i.e.  the number of ratings assigned to a particular ( user cluster , movie cluster ) combination and @xmath174 , i.e.  the sum of the scores for this combination .      for the purpose of recommendation and for a subsequent combination of several matrices ,",
    "we need to infer variances and instantiate the scores @xmath132 . by checking",
    "we see that @xmath175 is given by a normal distribution with parameters @xmath176 note that the term @xmath177 plays the role of a classic shrinkage term just as in a james - stein estimator . to sample @xmath135 and @xmath134",
    "we use the inverse gamma distribution of .",
    "denote by @xmath178 the total number of observed values in @xmath25 . in this case",
    ", @xmath135 is drawn from an inverse gamma prior with parameters @xmath179 : @xmath180 analogously , we draw @xmath134 from an inverse gamma with parameters @xmath181 @xmath182 and @xmath36 denote the number of user and movie clusters .      with these inference equations",
    "we can implement an efficient sampler , as seen in algorithm [ alg : sampler ] .",
    "the key to efficient sampling is to cache the per - cluster sums of ratings @xmath174 .",
    "then reassigning a user ( or movie ) to a different ( or new ) cluster is just a matter of checking the amount of change that this would effect .",
    "hence each sampling pass costs @xmath183 operations .",
    "it is linear in the number of ratings and of partitions .",
    "row - index and column - index of data in @xmath25 sum of squares @xmath184 statistics for each partition @xmath185 for all movie clusters @xmath30 compute the incremental changes @xmath186 remove @xmath121 from their cluster @xmath187 remove @xmath121 from their cluster @xmath188 sample new user cluster @xmath125 using and .",
    "update statistics @xmath189 sample movie cluster assignments analogously .",
    "resample @xmath132 using and the statistics @xmath190 .",
    "resample @xmath135 and @xmath134 via the inverse gamma distribution using and .",
    "note that once @xmath191 and @xmath192 are available for all users ( or all movies ) , it is cheap to perform additional sampling sweeps at comparably low cost .",
    "it is therefore beneficial to iterate over all users ( or all movies ) more than once , in particular in the initial stages of the algorithm .",
    "also note that the algorithm can be used on datasets that are being streamed from disk , provided that an index and an inverted index of @xmath25 can be stored : we need to be able to traverse the data when ordered by users and when ordered by movies .",
    "it is thus compatible with solid state disks .",
    "if there was no penalty on the number of clusters it would be possible to approximate any matrix by a trivial model using as many clusters as we have rows and columns .",
    "any matrix @xmath19 has nonvanishing support in regardless of @xmath135 .",
    "since any partitionings of sets of size @xmath193 respectively have nonzero support , it follows that partitioning all rows and all columns into separate bins is possible .",
    "hence , we can assign a different mean @xmath194 to any entry @xmath129 .",
    "obviously , the crp prior on @xmath28 and @xmath30 makes this highly unlikely .",
    "on the other hand , we want to retain the ability to fit a richer set of matrices than what can be effectively covered by piecewise constant block matrices .",
    "we take linear combinations of matrices , as introduced in section  [ sec : matapp ] .",
    "as before , we enumerate the stencils by @xmath195 .",
    "correspondingly we now need to sample from a set of @xmath196 and @xmath134 _ per _ matrix .",
    "however , we keep the additive noise term @xmath197 unchanged .",
    "this is the model of figure [ fig : multi_stencil_model ] .",
    "the additivity of gaussians renders makes inference easy : @xmath198 note , though , that estimating @xmath199 jointly _ for all _ indices @xmath48 is not tractable since various clusterings @xmath200 overlap and intersect with each other , hence the joint normal distribution over all variables would be expensive to factorize .    residuals @xmath201 and @xmath202 compute partial residuals @xmath203 sample over @xmath204 using @xmath177 instead of @xmath25 update residuals with @xmath205    instead , we sample over one stencil at a time , as shown in algorithm [ alg : baccams ] .",
    "this algorithm only requires repeated passes through the dataset .",
    "moreover , it can be modified into a backfitting procedure by fitting one matrix at a time and then fixing the outcome .",
    "capacity control can be enforced by modifying @xmath152 and @xmath153 such that the probability of a new cluster decreases for larger @xmath48 , i.e.  by decreasing @xmath152 and @xmath153 . as a result following the analysis in the single stencil case , each sampling pass costs @xmath206 operations where @xmath9 is the number of stencils .",
    "it is linear in the number of ratings , in the number of partitions and in the number of stencils .",
    "we evaluate our method based on its ability to perform matrix completion , matrix approximation and to give interpretable results . here",
    "we describe our experimental setup and results on real world data , such as the netflix ratings .",
    "we implemented both accams , the @xmath0-means - based algorithm , as well as baccams , the bayesian model . unless specified otherwise , we run the rowclustering of algorithm  [ alg : rowcluster ] for up to @xmath207 iterations .",
    "our system can also iterate over the stencils multiple times , such that earlier stencils can be re - learned after we have learned later ones . in practice",
    ", we observe this only yields small gains in accuracy , hence we generally do not use it .",
    "we implemented baccamsusing gibbs sampling ( section  [ sec : many ] ) and used the @xmath0-means algorithm accamsfor the initialization of each stencil .",
    "following standard practice , we bound the range of @xmath208 by @xmath209 from above .",
    "this rejection sampler avoids pathological cases . for the sake of simplicity",
    ", we set @xmath210 to be the maximum number of clusters that can be generated in each stencil . when inferring the cluster assignments for a given stencil , we run three iterations of the sampler before proceeding to the next stencil .",
    "as common in mcmc algorithms , we use a burn - in period of at least 30 iterations ( each with three sub - iterations of sampling cluster assignments ) and then average the predictions over many draws .",
    "code for both accamsand baccamsis available at http://alexbeutel.com/accams .",
    "* netflix . *",
    "we run our algorithms on data from a variety of domains .",
    "our primary testing dataset is the ratings dataset from the netflix contest .",
    "the dataset contains 100 m ratings from 480k users and 17k movies .",
    "following standard practice for testing recommendation accuracy ,",
    "we average over three different random 90:10 splits for training and testing . * cmu face images . * to test how well we can approximate arbitrary matrices , we use image data from the cmu face images dataset .",
    "it contains black and white images of 20 different people , each in 32 different positions , for a total of 640 images .",
    "each image has @xmath211 pixel resolution ; we flatten this into a matrix of @xmath212 , i.e.  an image by pixel matrix .    * as peering graph . * to assess our model s ability to deal with graph data we consider the as graph .",
    "it contains information on the peering information of 13,580 nodes .",
    "it thus creates a binary matrix of size @xmath213 with 37k edges .",
    "since our algorithm is not designed to learn binary matrices , we treat the entries @xmath214 as real valued numbers .",
    "* parameters .",
    "* for all experiments , we set the hyperparameters in baccamsto @xmath215 , @xmath216 , @xmath217 , @xmath218 , and @xmath219 .",
    "depending on the task , we compare accamsagainst svd++ using the graphchi @xcite implementation , svd from matlab for full matrices , and previously reported state - of - the - art results .    * model complexity . *",
    "since our model is structurally quite different from factorization models , we compare them based on the number of bits in the model and prediction accuracy . for factorization models , we consider each factor to be a 32 bit float .",
    "hence the complexity of a rank @xmath110 svd++ model of @xmath83 users and @xmath122 movies is @xmath220 bits .    for accamswith @xmath9 stencils and @xmath3 co - clusters in each stencil , the cluster assignment for a given row or column is @xmath221 bits and each value in the stencil is a float . as such",
    ", the complexity of a model is @xmath222 bits .    in calculating the parameter space size for llorma",
    ", we make the very conservative estimate that each row and column is on average part of two factorizations , even though the model contains more than 30 factorizations that each row and column could be part of .",
    "since the primary motivation of our model is collaborative filtering we begin by discussing results on the classic netflix problem ; accuracy is measured in rmse . to avoid divergence we set @xmath223 .",
    "we then vary both the number of clusters @xmath0 and the number of stencils @xmath9 .    a summary of recent results as well as results using our method",
    "can be found in table [ tab : netflix ] .",
    "using graphchi we run svd++ on our data .",
    "we use the reported values from llorma @xcite and dfc @xcite , which were obtained using the same protocol as reported here .    as can be seen in table [",
    "tab : netflix ] , baccamsachieves the _ best _ published result .",
    "we achieve this while using a very different model that is significantly simpler both conceptually and in terms of parameter space size .",
    "we also did not use any of the temporal and contextual variants that many other models use to incorporate prior knowledge .    as shown in figure",
    "[ fig : netflix_test_crownjewel ] , we observe that per bit our model achieves much better accuracy at a fraction of the model size . in figure",
    "[ fig : matrix_approx](a ) we compare different configurations of our algorithm .",
    "as can be seen , classic co - clustering quickly overfits the training data and provides a less fine - grained ability to improve prediction accuracy than accams .",
    "since accamshas no regularization , it too overfits the training data . by using a bayesian model with baccams",
    ", we do not overfit the training data and thus can use more stencils for prediction , greatly improving the prediction accuracy .",
    ".baccamsachieves an accuracy for matrix completion on netflix better than or on - par with the best published results , while having a parameter space a fraction of the size of other methods .",
    "@xmath224 denotes the number of anchor points for llorma and sizes listed are the parameter space size .",
    "[ cols=\"<,<,<,<\",options=\"header \" , ]          aside from accams s success across matrix completion and approximation , it is valuable to understand how our method is working , particularly because of how different it is from previous models .",
    "first , because accamsuses backfitting , we expect that the first stencil captures the largest features , the second captures secondary ones , etc .",
    "this idea is backed up by our theoretical results in section [ sec : entropy ] , and we observe that this is working experimentally by the drop off in rmse for our matrix approximation results in figure [ fig : matrix_approx ] .",
    "we can visually observe this in the image approximation of the cmu faces .",
    "as can be seen in figure [ fig : faces ] , the first stencil captures general structures of the room and heads , and the second starts to fill in more fine grained details of the face .    the bayesian model , baccams , backfits in the first iteration of the sampler but ultimately resamples each stencil many times thus loosening these properties . in figure",
    "[ fig : analysis ] , we observe how the distribution of users and movies across clusters changes over iterations and number of stencils , based on our run of baccamswith @xmath225 stencils and a maximum of @xmath226 clusters per stencil . as we see in the plot of entropy , movies , across all 70 stencils ,",
    "are well distributed across the 10 possible clusters .",
    "users , however , are well distributed in the early stencils but then are only spread across a few clusters in later stencils .",
    "in addition , we notice that while the earlier clusters are stable , later stencils are much less stable with a high percentage of cluster assignments changing .",
    "both of these properties follow from the fact that most users rate very few movies . for most users",
    "only a few clusters are necessary to capture their observed preferences .",
    "movies , however , typically have more ratings and more latent information to infer .",
    "thus through all 70 stencils we learn useful clusterings , and our prediction accuracy improves through @xmath227 stencils .",
    "here we formulated a model of additive co - clustering .",
    "we presented both a @xmath0-means style algorithm , accams , as well as a generative bayesian non - parametric model with a collapsed gibbs sampler , baccams ; we obtained theoretical guarantees for matrix approximation through additive co - clustering ; and we showed that our method is concise and accurate on a diverse range of datasets , including achieving the best published accuracy on netflix .    given the novelty and initial success of the method , we believe that domain - specific variants of accams , such as for community detection and topic modeling , can and will lead to new models and improved results .",
    "in addition , given the modularity of our framework , it is easy to incorporate side information , such as explicit genre and actor data , in modeling rating data that should lead to improved accuracy and interpretability",
    ".    * acknowledgements .",
    "* we would like to thank christos faloutsos for his valuable feedback throughout the preparation of this paper .",
    "this research was supported by funds from google , a facebook fellowship , a national science foundation graduate research fellowship ( grant no .",
    "dge-1252522 ) , and the national science foundation under grant no .",
    "cns-1314632 and iis-1408924 .",
    "any opinions , findings , and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reflect the views of the national science foundation , or other funding parties .",
    "a.  banerjee , i.  dhillon , j.  ghosh , s.  merugu , and d.  s. modha .",
    "a generalized maximum entropy approach to bregman co - clustering and matrix approximation . in _ proceedings of the tenth acm sigkdd international conference on knowledge discovery and data mining _ , pages 509514 .",
    "acm , 2004 .",
    "i.  porteous , e.  bart , and m.  welling .",
    "multi - hdp : a non parametric bayesian model for tensor factorization . in d.  fox and c.p .",
    "gomes , editors , _ proceedings of the twenty - third aaai conference on artificial intelligence _ , pages 14871490 .",
    "aaai press , 2008 ."
  ],
  "abstract_text": [
    "<S> matrix completion and approximation are popular tools to capture a user s preferences for recommendation and to approximate missing data . instead of using low - rank factorization we take a drastically different approach , based on the simple insight that an additive model of co - clusterings allows one to approximate matrices efficiently . </S>",
    "<S> this allows us to build a concise model that , per bit of model learned , significantly beats all factorization approaches to matrix approximation . </S>",
    "<S> even more surprisingly , we find that summing over small co - clusterings is more effective in modeling matrices than classic co - clustering , which uses just one large partitioning of the matrix .    following occam s razor principle suggests that the simple structure induced by our model better captures the latent preferences and decision making processes present in the real world than classic co - clustering or matrix factorization . </S>",
    "<S> we provide an iterative minimization algorithm , a collapsed gibbs sampler , theoretical guarantees for matrix approximation , and excellent empirical evidence for the efficacy of our approach . </S>",
    "<S> we achieve state - of - the - art results on the netflix problem with a fraction of the model complexity . </S>"
  ]
}