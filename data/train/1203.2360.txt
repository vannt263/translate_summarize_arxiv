{
  "article_text": [
    "in the last decade , parallelism across the time @xcite , based on the decomposition of the time domain has been exploited to accelerate the simulation of systems governed by time dependent partial differential equations  @xcite . among others , the parareal algorithm  @xcite or multi - shooting schemes  @xcite have shown excellent results . in the framework of optimal control , this approach has been used to control parabolic systems  @xcite .    in this paper , we introduce a new approach to tackle such problems . the strategy we follow is based on the concept of target trajectory that has been introduced in the case of hyperbolic systems in  @xcite . because of the irreversibility of parabolic equations , a new definition of this trajectory is considered .",
    "it enables us to define at each end point of the time sub - domains relevant initial conditions and intermediate targets , so that the initial problem is split up into independent optimization problems .",
    "the paper is organized as follows : the optimal control problem is introduced in section  [ sec:2 ] and the parallelization setting is described in section  [ sec:3 ] .",
    "the properties of the cost functionals involved in the control problem are studied in section  [ sec:4 ] .",
    "the general structure of our algorithm is given in section  [ sec:5 ] and its convergences is proven in section  [ sec:6 ] . in section  [ sec:7 ] ,",
    "we propose a fully parallelized version of our algorithm .",
    "some numerical tests showing the efficiency of our approach are presented in section  [ sec:8 ] .    in the sequel",
    ", we consider the optimal control problem associated with the heat equation on a compact set @xmath0 and a time interval @xmath1 $ ] , with @xmath2 .",
    "we denote by @xmath3 the space norm associated with @xmath4 , and by @xmath5 the @xmath6-norm corresponding to a sub - domain @xmath7 . also , we use the notations @xmath8 ( resp .",
    "@xmath9 ) and @xmath10 ( resp .",
    "@xmath11 ) to represent the norm and the scalar product of the hilbert space @xmath12 ( resp .",
    "@xmath13 ) , with @xmath14 a sub - interval of @xmath1 $ ] ) . given a function @xmath15 defined on the time interval @xmath1 $ ]",
    ", we denote by @xmath16 the restriction of @xmath15 to @xmath14 .",
    "given @xmath17 , consider the optimal control problem defined by : @xmath18;l^2(\\omega_c))}j(v),\\ ] ] with @xmath19 where @xmath20 is a given state in @xmath4 .",
    "the state @xmath15 evolves from @xmath21 on @xmath1 $ ] according to @xmath22 in this equation , @xmath23 denotes the laplace operator , @xmath24 is the control term , applied on @xmath25 and @xmath26 is the natural injection from @xmath25 into @xmath0 .",
    "we assume dirichlet conditions for @xmath15 on the boundary of @xmath0 .",
    "+ the corresponding optimality system reads as    @xmath27\\times\\omega\\\\ y(0)&=&y_0 , \\end{array } \\right.\\ ] ]    @xmath28\\times\\omega\\\\ p(t ) & = &   y(t)-y_{target } , \\end{array } \\right.\\ ] ]    @xmath29    where @xmath30 is the adjoint operator of @xmath26 .",
    "+ note that for any @xmath17 , the functional @xmath31 is continuous , @xmath32-convex in @xmath33 and consequently the system ( [ os1][os3 ] ) has a unique solution by @xmath34 .",
    "we denote by @xmath35 , @xmath36 the associated state and adjoint state .",
    "in this section , we describe the relevant setting for a time parallelized resolution of the optimality system .",
    "+ consider @xmath37 and a subdivision of @xmath1 $ ] of the form : @xmath38=\\cup_{n=0}^{n-1}i_n,\\ ] ] with @xmath39 $ ] , @xmath40 .",
    "for the sake on simplicity , we assume here that the subdivision is uniform , i.e. for @xmath41 we assume that @xmath42 ; we denote @xmath43 .",
    "given a control @xmath24 and its corresponding state @xmath15 and adjoint state @xmath44 , we define the _ target trajectory _ by :    @xmath45\\times\\omega .   \\ ] ]    the trajectory @xmath46 is not governed by a partial differential equation , but reaches @xmath47 at time @xmath48 from ( [ os2]@xmath49 ) , hence its denomination .",
    "+ for @xmath50 , consider the sub - problems @xmath51 with @xmath52 where the function @xmath53 is defined by @xmath54 recall that this optimal control problem is parameterized by @xmath24 ( and @xmath15 and @xmath44 ) through the local target @xmath55 , we note that this sub - problem has the same structure as the original one , and is also strictly convex . the optimality system associated with this optimization problem is given by and the equations    @xmath56    @xmath57    we denote by @xmath58 its solution .",
    "the introduction of the target trajectory in the last section is motivated by the following result .",
    "denote by @xmath60 the target trajectory defined by with @xmath61 and @xmath62 and by @xmath63 the solutions of ( [ pos1][pos3 ] ) with @xmath61 and @xmath64 .",
    "one has : @xmath65    thanks to the uniqueness of the solution of the sub - problem , it is enough to show that @xmath66 satisfies the optimality system  ( [ pos1][pos3 ] ) .",
    "+ first , note that @xmath67 obviously satisfies   with @xmath68 .",
    "it directly follows from the definition of @xmath60 ( see  ) , that : @xmath69 so that @xmath70 satisfies  . finally , equation   is a consequence of  .",
    "the result follows .",
    "@xmath71    let @xmath72 denote the hessian operator associated with @xmath31 ; there exists a strong connection between the hessian operators @xmath72 and @xmath73 of @xmath31 and @xmath59 , as indicated in the next lemma .",
    "[ restric ] the hessian operator @xmath73 coincides with the restriction of @xmath72 to controls whose time supports are included in @xmath74 $ ] .",
    "first note that @xmath31 is quadratic so that @xmath72 is a constant operator . given an increase @xmath75;l^2(\\omega_c))$ ]",
    ", we have : @xmath76 where @xmath77 is the solution of @xmath78 \\times\\omega\\\\ \\delta y(0)&=&0 .",
    "\\end{array } \\right.\\ ] ] given @xmath79 , consider now an increase @xmath80 .",
    "one finds in the same way that : @xmath81 where @xmath82 is the solution of @xmath83 \\times\\omega\\\\ \\delta y_n(t_n)&=&0 .",
    "\\end{array } \\right.\\ ] ] suppose now that @xmath84 on @xmath85 $ ] , it is a simple matter to check that @xmath86 over @xmath87 $ ] .",
    "the restriction of @xmath77 on the interval @xmath74 $ ] thus satisfies @xmath88 and is consequently ( up to a time translation ) the solution of  .",
    "@xmath71    we end this section with an estimate on these hessian operators .    [ lemm : poinc ] given @xmath75;l^2(\\omega_c))$ ] , one has : @xmath89 where @xmath90 with @xmath91 the poincar s constant associated with @xmath4 .",
    "the proof of this result is standard and given in appendix for the sake of completeness .",
    "because of lemma  [ restric ] , the hessian operator @xmath73 also satisfies  .",
    "we are now in a position to propose a time parallelized procedure to solve ( [ os1][os3 ] ) . in what follows",
    "we describe the principal steps of a parallel algorithm named `` sitpoc '' ( serial intermediate targets for parallel optimal control ) .",
    "[ galalg ] consider an initial control @xmath92 and suppose that , at step @xmath93 one knows @xmath94 .",
    "the computation of @xmath95 is achieved as follows :    a.   [ step1 ] compute @xmath96 , @xmath97 and the associated target trajectory @xmath98 according to , and respectively .",
    "b.   [ step2 ] solve approximately the @xmath99 sub - problems in parallel .",
    "for @xmath50 , denote by @xmath100 the corresponding solutions and by @xmath101 the concatenation of @xmath102 .",
    "c.   [ step4 ] define @xmath95 by @xmath103 , where @xmath104 is defined to minimize @xmath105 .",
    "note that we do not explain in detail here the optimization step ( step [ step2 ] ) and rather present a general structure of our algorithm .",
    "because of the strictly convex setting , some steps of , e.g. , a gradient method or a small number of conjugate gradient method step can be used .",
    "the convergence of algorithm  [ galalg ] can be guaranteed under some assumptions . in what follows ,",
    "we denote by @xmath106 the gradient of @xmath31 .",
    "suppose that the sequence @xmath107 defined in algorithm  [ galalg ] satisfies , for all @xmath108 : @xmath109    @xmath110    and @xmath111 for a given @xmath112 .",
    "then @xmath107 converges linearly with a rate @xmath113 to the solution of  ( [ os1][os3 ] )    note that in the case   is not satisfied , there exists @xmath114 such that @xmath115 and the optimum is reached in a finite number of steps .",
    "define the shifted functional @xmath116 and note that because of the definition of @xmath34 , one has @xmath117 since @xmath31 is quadratic , for any @xmath118 @xmath119 and consequently @xmath120 so that @xmath121 combining and , one gets @xmath122 with @xmath123 .",
    "+ on the other hand , the variations in the functional between two iterations of our algorithm reads as @xmath124 combining this last inequality with , one finds that : @xmath125 since @xmath126 , we have : @xmath127 where @xmath128 .",
    "indeed follows from , from and from .",
    "it follows from the monotonic convergence of @xmath129 that the sequence @xmath94 is cauchy , thus its convergence .",
    "+ let us now study the convergence rate . define @xmath130 .",
    "summing   between @xmath93 and @xmath131 , we obtain : @xmath132 using again   and  , one finds that : @xmath133 note that this inequality implies that @xmath134 .",
    "define @xmath135 , we have @xmath136 .",
    "because of  : @xmath137 and the result follows .",
    "@xmath138 +   + we now give an example where hypothesis  ( [ eq1][eq2 ] ) are satisfied .",
    "[ locoptgrad ] assume that step  [ step2 ] of algorithm  [ galalg ] is achieved using only one step of a locally optimal step gradient method and that at step @xmath93 , the algorithm is initialized with @xmath139 , then  ( [ eq1][eq2 ] ) are satisfied hence the algorithm converges to the solution of  ( [ os1][os3 ] ) .",
    "because of the assumptions , the optimization step ( step .  [ step2 ] ) reads : @xmath140 since the functionals @xmath59 are quadratic , one has : @xmath141 a first consequence of these equalities is that : @xmath142 moreover lemmas  [ restric ] and  [ lemm : poinc ] imply : @xmath143 one can also obtain similar estimates of @xmath104 . in this view , note first that since the only iteration which is considered uses as directions of descent @xmath144 .",
    "then : @xmath145 using  , one deduces : @xmath146 this preliminary results will now be used to prove the theorem .",
    "the proof of  , follows from  : @xmath147 this last estimate is a consequence of  .",
    "it remains to prove  .",
    "we have : @xmath148 and the result follows .    @xmath138",
    "the method we have presented with algorithm  [ galalg ] requires in step  [ step1 ] two sequential resolutions of the evolution equation   on the whole interval @xmath1 $ ] , which does not fit with the parallel setting . in this section , we make use of the parareal algorithm to parallelize the corresponding computations .",
    "let us first recall the main features of the parareal algorithm .",
    "we consider the example of equation  . in order to solve in parallel an evolution equation , for the parareal scheme  @xcite we introduce intermediate initial conditions at times @xmath149 that are updated",
    "suppose that these values @xmath150 are known at step @xmath93 .",
    "denote by @xmath151 and @xmath152 coarse and fine solutions of   at time @xmath153 with @xmath154 as initial value .",
    "the update is done according to the following iteration : @xmath155 we use this procedure in step  [ step1 ] of algorithm  [ galalg ] .",
    "the idea we follow consists in merging the two procedures , i.e. doing one parareal iteration at each iteration of our algorithm .",
    "we now give details on the resulting procedure .",
    "since the evolution equations depend on the control , we replace the notations @xmath151 and @xmath152 by @xmath156 and @xmath157 respectively .",
    "as we need backward solvers to compute @xmath44 , see  , we also introduce @xmath158 and @xmath159 to denote coarse and fine solutions of   at time @xmath160 with @xmath161 as `` initial '' value ( given at time @xmath153 ) .",
    "note that these bakward solvers @xmath162 ( resp : @xmath163 ) do not depend on the control .",
    "+ we describe in the following the principal steps of an enhanced version of the sitpoc algorithm which we give the name `` pitpoc '' as parareal intermediate targets for optimal control .    [ paralg ] denote by @xmath164 .",
    "consider a control @xmath165 , initial values @xmath166 ( through forward scheme @xmath167 ) , final values @xmath168 ( through backward scheme @xmath169 .",
    "+ suppose that , at step @xmath93 one knows @xmath94 , @xmath170 and @xmath171 .",
    "the computation of @xmath95 , @xmath172 and @xmath173 is achieved as follows :    a.   build the target trajectory @xmath174 according to a definition similar to  : @xmath175 b.   [ step2para ] solve approximately the @xmath99 sub - problems in parallel . for @xmath50 ,",
    "denote by @xmath100 the corresponding solutions .",
    "c.   define @xmath101 as the concatenation of the sequence @xmath102 .",
    "d.   [ step1para ] compute @xmath176 , @xmath173 by : @xmath177 e.   [ step4para ] define @xmath95 and @xmath172 @xmath178 where @xmath104 is defined to minimize @xmath179 f.   @xmath180 and return to i.",
    "in this section , we test the efficiency of our method and show how robust the approach is .",
    "we consider two independent parts describing numerical results of the selected algorithm .",
    "we consider a 2d example , where @xmath181\\times [ 0,1]$ ] and @xmath182\\times[\\frac{1}{3},\\frac{2}{3}]$ ] .",
    "the parameters related to our control problem are @xmath183 , @xmath184 and @xmath185 .",
    "the time interval is discretized using a uniform step @xmath186 , and an implicit - euler solver is used to approximate the solution of equations  ( [ os1][os2 ] ) . for the space discretization , we use @xmath187 finite elements .",
    "our implementation makes use of the freeware ` f`reefem  @xcite and the parallelization is achieved thanks to the message passing interface library .",
    "the independent optimization procedures required in step  [ step2 ] are simply carried out using one step of an optimal gradient method .      in this section , step ii of algorithm  [ galalg ] and",
    "algorithm  [ paralg ] are achieved by using one step of an optimal step gradient method .",
    "we first test our algorithm by varying the number of sub - intervals .",
    "the evolution of the cost functional values are plotted with respect to the number of iteration ( figure  [ itersitpit ] ) , the number of matrix multiplication ( figure  [ multsitpit ] ) and the number of wall - clock time of computation ( figure  [ timesitpit ] ) .",
    "we first note that algorithm 4 actually acts as a preconditioner , since it improves the convergence rate of the optimization process .",
    "the introduceion of the intermediates targets allows to accelerate the decrease of the functional values , as shown in figure  [ multsitpit ] ( left ) .",
    "note that this property holds mostly for small numbers of sub - intervals , and disapears when dealing with large subdivisions .",
    "this feature is lost when considering algorithm  [ paralg ] , whose convergence does not significantly depend on the number of sub - intervals that is considered , see figure  [ multsitpit ] ( right ) .    on the contrary ,",
    "algorithm  [ paralg ] achieves a good acceleration when considering the number of mutliplications involved in the computations .",
    "the corresponding results are shown in figure  [ multsitpit ] , where the parallel operations have been counted only once .",
    "we see that algorithm is close to the full efficiency , since the number of multiplications required to obtain a given value for the cost functional is roughly proportional to @xmath188 .",
    "we finally consider the wall - clock time required to carry out our algorithms . as",
    "the main part of the operations involved in the computation consists in matrix multiplications , the results we present in figure 3 are close to the ones of figure 2 .",
    "we now vary the number of steps of the gradient method used in step ii of our algorithm .",
    "the results are presented in figure 4 .",
    "subdivisions of @xmath189 and @xmath190 intervals are considered . in both cases , we see that an increase in the number of gradient steps improves the preconditionning feature of our algorithm .",
    "however , we also observe that this strategy saturates for large numbers of gradient steps which probably reveals that the sub - problems considered in step ii are practically solved after 5 sub - iterations .",
    "more results can be found in @xcite .",
    "subdivision ( right ) : variation of the number of ( lower / local)inner - iterations @xmath191.,width=238,height=226 ]     subdivision ( right ) : variation of the number of ( lower / local)inner - iterations @xmath191.,width=238,height=226 ]",
    "for the sake of completeness , we recall here the proof of lemma  [ lemm : poinc ] .",
    "+ because of   and thanks to young s inequality , one has for all @xmath192 $ ] and all @xmath193 : @xmath194 where @xmath195 denotes the gradient with respect to the space variable . as @xmath77 is supposed to satisfies dirichlet conditions , one can apply poincar s inequality to obtain : @xmath196 for a given @xmath197 . combining this last estimate with  ,",
    "one gets : @xmath198 now , setting @xmath199 gives : @xmath200 since @xmath201 , the result follows with the fact that @xmath202 .",
    "k. burrage , parallel and sequential methods for ordinary differential equations , numerical mathematics and scientific computation , oxford science publications , the clarendon press , oxford university press , new york , 1995 ."
  ],
  "abstract_text": [
    "<S> in this paper , we present a method that enables solving in parallel the euler - lagrange system associated with the optimal control of a parabolic equation . </S>",
    "<S> our approach is based on an iterative update of a sequence of intermediate targets that gives rise to independent sub - problems that can be solved in parallel . </S>",
    "<S> this method can be coupled with the parareal in time algorithm . </S>",
    "<S> numerical experiments show the efficiency of our method . </S>"
  ]
}