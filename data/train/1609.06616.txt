{
  "article_text": [
    "methods have been developed to efficiently obtain representations of words in @xmath0 that capture subtle semantics across the dimensions of the vectors ( collobert and weston , 2008 ) .",
    "for instance , after sufficient training , relationships encoded in difference vectors can be uncovered with vector arithmetic : vec(``king '' ) - vec(``man '' ) + vec(``woman '' ) returns a vector close to vec(``queen '' ) ( mikolov et al .",
    "2013a ) .",
    "applying this powerful notion of distributed continuous vector space representations of words , we embed representations of institutions and the words from their law and policy documents into shared semantic space .",
    "we can then combine positively and negatively weighted word and government vectors into the same query , enabling complex , targeted and subtle similarity computations .",
    "for instance , which government branch is more characterized by `` validity and truth , '' or `` long - term government career '' ?",
    "we apply this method , gov2vec , to a unique corpus of supreme court opinions , presidential actions , and official summaries of congressional bills .",
    "the model discerns meaningful differences between house , senate , president and court vectors .",
    "we also learn more fine - grained institutional representations : individual presidents and congresses ( 2-year terms ) .",
    "the method implicitly learns important latent relationships between these government actors that was not provided during training .",
    "for instance , their temporal ordering was learned from only their text .",
    "the resulting vectors are used to explore differences between actors with respect to policy topics .",
    "a common method for learning vector representations of words is to use a neural network to predict a target word with the mean of its context words vectors , obtain the gradient with back - propagation of the prediction errors , and update vectors in the direction of higher probability of observing the correct target word ( bengio et al .  2003 ; mikolov et al .",
    "after iterating over many word contexts , words with similar meaning are embedded in similar locations in vector space as a by - product of the prediction task ( mikolov et al .",
    "le and mikolov ( 2014 ) extend this word2vec method to learn representations of documents . for predictions of target words ,",
    "a vector unique to the document is concatenated with context word vectors and subsequently updated .",
    "similarly , we embed institutions and their words into a shared vector space by averaging a vector unique to an institution with context word vectors when predicting that institution s words and , with back - propagation and stochastic gradient descent , update representations for institutions and the words ( which are shared across all institutions ) .        there are two hyper - parameters for the algorithm that can strongly affect results , but suitable values are unknown .",
    "we use a tree of parzen estimators search algorithm ( bergstra et al .",
    "2013 ) to sample from parameter space and save all models estimated .",
    "subsequent analyses are conducted across all models , propagating our uncertainty in hyper - parameters . due to stochasticity in training and the uncertainty in the hyper - parameter values , patterns robust across the ensemble are more likely to reflect useful regularities than individual models .",
    "gov2vec can be applied to more fine - grained categories than entire government branches . in this context , there are often relationships between word sources , e.g. obama after bush , that we can incorporate into the learning process . during training ,",
    "we alternate between updating govvecs based on their use in the prediction of words in their policy corpus and their use in the prediction of other word sources located nearby in time .",
    "we model temporal institutional relationships , but any known relationships between entities , e.g.  ranking congresses by number of republicans , could also be incorporated into the structured gov2vec training process ( fig .  1 ) .    after training",
    ", we extract @xmath1 parameters , where @xmath2 is the number of unique words , @xmath3 is the number of word sources , and @xmath4 the vector dimensionality , which varies across the @xmath5 models ( we set @xmath6 ) .",
    "we then investigate the most cosine similar words to particular vector combinations , @xmath7 , where @xmath8 , @xmath9 is one of @xmath10 wordvecs or govvecs of interest , @xmath11 are the @xmath12 most frequent words in the vocabulary of @xmath2 words ( @xmath13 to exclude rare words during analysis ) excluding the @xmath10 query words , @xmath14 is _ 1 _ or _",
    "-1 _ for whether we re positively or negatively weighting @xmath9 .",
    "we repeat similarity queries over all @xmath5 models , retain words with @xmath15 cosine similarity , and rank the word results based on their frequency and mean cosine similarity across the ensemble .",
    "we also measure the similarity of wordvec combinations to each govvec and the similarities between govvecs to validate that the process learns useful embeddings that capture expected relationships .",
    "we created a unique corpus of 59 years of all u.s .",
    "supreme court opinions ( 1937 - 1975 , 1991 - 2010 ) , 227 years of all u.s . presidential memorandum , determinations , and proclamations , and executive orders ( 1789 - 2015 ) , and 42 years of official summaries of all bills introduced in the u.s .",
    "congress ( 1973 - 2014 ) .",
    "we used official summaries rather than full bill text because full texts are only available from 1993 and summaries are available from 1973 .",
    "we scraped all presidential memorandum ( 1,465 ) , determinations ( 801 ) , executive orders ( 5,634 ) , and proclamations ( 7,544 ) from the http://www.presidency.ucsb.edu/[american presidency project website ] .",
    "the sunlight foundation downloaded https://github.com/unitedstates/congress/wiki[official bill summaries ] from the u.s .",
    "government publishing office ( gpo ) , which we downloaded .",
    "we downloaded supreme court decisions issued 19371975 ( vol .",
    "300 - 422 ) from the https://www.gpo.gov/fdsys/bulkdata/scd/1937[gpo ] , and the pdfs of decisions issued 19912010 ( vol .",
    "502 - 561 ) from the http://www.supremecourt.gov/opinions/boundvolumes.aspx[supreme court ] .",
    "we removed html artifacts , whitespace , http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop[stop words ] , words occurring only once , numbers , and punctuation , and converted to lower - case .",
    "we tested whether our learned vectors captured meaningful differences between branches .",
    "fig .  2 displays similarities between these queries and the branches , which reflect _ a priori _ known differences .",
    "gov2vec has unique capabilities that summary statistics , e.g.  word frequency , lack : it can compute similarities between any source and word as long as the word occurs at least in one source , whereas word counting can not provide meaningful similarities when a word never occurs in a source s corpus .",
    "most importantly , gov2vec can combine complex combinations of positively and negatively weighted vectors in a similarity query .",
    "we learned representations for individual presidents and congresses by using vectors for these higher resolution word sources in the word prediction task . to investigate if the representations capture important latent relationships between institutions , we compared the cosine similarities between the congresses over time ( 93rd113th ) and the corresponding sitting presidents ( nixon  obama ) to the bill veto rate .",
    "we expected that a lower veto rate would be reflected in more similar vectors , and , indeed , the congress - president similarity and veto rate are negatively correlated ( spearman s @xmath16 computed on raw veto rates and similarities : -0.74 ; see also fig .  3 ) .",
    "as a third validation , we learn vectors from only text and project them into two dimensions with principal components analysis . from fig .  4 it s evident that temporal and institutional relationships were implicitly learned .",
    "one dimension ( top - to - bottom ) almost perfectly rank orders presidents and congresses by time , and another dimension ( side - to - side ) separates the president from congress .",
    "5 ( top ) asks : how does obama and the 113th house differ in addressing climate change and how does this vary across environmental and economic contexts ?",
    "the most frequent word across the ensemble ( out of words with @xmath17 similarity to the query ) for the obama - economic quadrant is `` unprecedented . ''",
    "`` greenhouse '' and `` ghg '' are more frequent across models and have a higher mean similarity for obama - environmental than 113th house - environmental .    fig .",
    "5 ( bottom ) asks : how does the house address war from `` oil '' and `` terror '' perspectives and how does this change after the 2001 terrorist attack . compared to the 106th , both the oil and terrorism panels in the 107th ( when 9 - 11 occurred ) have words more cosine similar to the query ( further to the right ) suggesting that the 107th house was closer to the topic of war , and the content changes to primarily strong verbs such as instructs , directs , requires , urges , and empowers .",
    "@xmath18 , where _ _ g__=@xmath19 , @xmath20 , @xmath21 , @xmath22 , _ _",
    "c__=@xmath23 , @xmath24 , @xmath25 , @xmath26 .",
    "the bottom panel is the war policy query for the u.s .",
    "house of representatives before and after the 9 - 11 terrorist attacks : wv ( ) , _ _",
    "g__=@xmath27 , _ _ c__=@xmath28 .",
    "the exact query used to create each quadrant is provided at the bottom of the quadrant.,title=\"fig:\",width=316 ]   @xmath18 , where _ _ g__=@xmath19 , @xmath20 , @xmath21 , @xmath22 , _ _",
    "c__=@xmath23 , @xmath24 , @xmath25 , @xmath26 .",
    "the bottom panel is the war policy query for the u.s .",
    "house of representatives before and after the 9 - 11 terrorist attacks : wv ( ) , _ _ g__=@xmath27 , _",
    "_ c__=@xmath28 .",
    "the exact query used to create each quadrant is provided at the bottom of the quadrant.,title=\"fig:\",width=316 ]",
    "political scientists model text to understand political processes ( grimmer 2010 ; roberts et al .",
    "2014 ) ; however , most of this work focuses on variants of topic models ( blei et al .",
    "djuric et al .  (",
    "2015 ) apply a learning procedure similar to structured gov2vec to streaming documents to learn representations of documents that are similar to those nearby in time . structured gov2vec applies this joint hierarchical learning process ( using entities to predict words _ and _ other entities ) to non - textual entities .",
    "kim et al .  ( 2014 ) and kulkarni et al .  ( 2015 ) train neural language models for each year of a time ordered corpora to detect changes in words . instead of learning models for distinct times , we learn a global model with embeddings for time - dependent entities that can be included in queries to analyze change .",
    "kiros et al .",
    "( 2014 ) learn embeddings for text attributes by treating them as gating units to a word embedding tensor .",
    "their process is more computationally intensive than ours .",
    "we learned vector representations of text meta - data on a novel data set of legal texts that includes case , statutory , and administrative law .",
    "the representations effectively encoded important relationships between institutional actors that were not explicitly provided during training .",
    "finally , we demonstrated fine - grained investigations of policy differences between actors based on vector arithmetic .",
    "more generally , the method can be applied to measuring similarity between any entities producing text , and used for recommendations , e.g.  what s the closest _ think - tank vector _ to the _ non - profit vector _ representation of the sierra club ?",
    "methodologically , our next goal is to explore where training on non - textual relations , i.e. structural gov2vec , is beneficial .",
    "it seems to help stabilize representations when exploiting temporal relations , but political relations may prove to be even more useful . substantively",
    ", our goal is to learn a large collection of vectors representing government actors at different resolutions and within different contexts to address a range of targeted policy queries .",
    "once we learn these representations , researchers could efficiently search for differences in law and policy across time , government branch , and political party .",
    "we thank the anonymous reviewers for helpful suggestions .",
    "bengio , yoshua , rjean ducharme , pascal vincent , and christian janvin .",
    "2003 . a neural probabilistic language model . _ j. mach .",
    "res . _ 3 ( march ) : 113755 .",
    "bergstra , james s. , daniel yamins , and david cox .",
    "2013 . making a science of model search : hyperparameter optimization in hundreds of dimensions for vision architectures . in _ proceedings of the 30th international conference on machine learning _ , 11523",
    "collobert , ronan and jason weston .",
    "2008 . a unified architecture for natural language processing : deep neural networks with multitask learning . in _ proceedings of the 25th international conference on machine",
    "learning_. 160167 .",
    "djuric , nemanja , hao wu , vladan radosavljevic , mihajlo grbovic , and narayan bhamidipati",
    "hierarchical neural language models for joint representation of streaming documents and their content . in _ proceedings of the 24th international conference on world wide web _ , 24855 .",
    "new york , ny , usa : acm . grimmer , justin . 2010 . `` a bayesian hierarchical topic model for political texts : measuring expressed agendas in senate press releases . '' _ political analysis _ 18 ( 1 ) : 135 .",
    "kim , yoon , yi - i .",
    "chiu , kentaro hanaki , darshan hegde , and slav petrov .",
    "temporal analysis of language through neural language models . in _ proceedings of the acl 2014 workshop on language technologies and computational social science _ , 6165 .",
    "association for computational linguistics .",
    "kiros , ryan , richard zemel , and ruslan r salakhutdinov .",
    "2014 . a multiplicative model for learning distributed text - based attribute representations .",
    "in _ advances in neural information processing systems 27 _ , edited by z. ghahramani , m. welling , c. cortes , n. d. lawrence , and k. q. weinberger , 234856 .",
    "curran associates , inc .",
    "kulkarni , vivek , rami al - rfou , bryan perozzi , and steven skiena . 2015 . statistically significant detection of linguistic change . in _ proceedings of the 24th international conference on world wide web _",
    "new york , ny , usa : acm .",
    "le , quoc , and tomas mikolov .",
    "2014 . distributed representations of sentences and documents . in",
    "_ proceedings of the 31st international conference on machine learning _ , 118896 .",
    "mikolov , tomas , ilya sutskever , kai chen , greg s corrado , and jeff dean .",
    "2013b . distributed representations of words and phrases and their compositionality . in _ advances in neural information processing systems",
    "26 _ , edited by c. j. c. burges , l. bottou , m. welling , z. ghahramani , and k. q. weinberger , 31119 .",
    "curran associates , inc .",
    "roberts , margaret e. , brandon m. stewart , dustin tingley , christopher lucas , jetson leder - luis , shana kushner gadarian , bethany albertson , and david g. rand .",
    "`` structural topic models for open - ended survey responses . ''",
    "_ american journal of political science _ 58 ( 4 ) : 106482 ."
  ],
  "abstract_text": [
    "<S> we compare policy differences across institutions by embedding representations of the entire legal corpus of each institution and the vocabulary shared across all corpora into a continuous vector space . </S>",
    "<S> we apply our method , gov2vec , to supreme court opinions , presidential actions , and official summaries of congressional bills . </S>",
    "<S> the model discerns meaningful differences between government branches . </S>",
    "<S> we also learn representations for more fine - grained word sources : individual presidents and ( 2-year ) congresses . </S>",
    "<S> the similarities between learned representations of congresses over time and sitting presidents are negatively correlated with the bill veto rate , and the temporal ordering of presidents and congresses was implicitly learned from only text . with the resulting vectors we answer questions such as : how does obama and the 113th house differ in addressing climate change and how does this vary from environmental or economic perspectives ? </S>",
    "<S> our work illustrates vector - arithmetic - based investigations of complex relationships between word sources based on their texts . we are extending this to create a more comprehensive legal semantic map </S>"
  ]
}