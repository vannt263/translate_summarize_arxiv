{
  "article_text": [
    "this paper describes learning for algorithms solving classes of the logic minimization problem minsat .",
    "each class is defined by a propositional formula and costs that apply when  values are assigned to the variables .",
    "the instances of the class are derived from the formula by fixing or deleting variables and deleting clauses .",
    "such classes arise in expert systems or logic modules  for example , for natural language processing , medical diagnosis , or traffic control .",
    "learning is done once a compiler has constructed a solution algorithm for a given class .",
    "the learning step applies the solution algorithm to relatively few instances of the class , analyses each case where the algorithm does not find a solution quickly , and then modifies the underlying formula of the class so that future runs avoid such poor performance .",
    "the modifications consist of the addition and deletion of clauses .",
    "the added clauses are of two types : clauses that are always valid , and clauses that are valid only when the solution algorithm has already found a satisfying solution with total cost below some threshold value .",
    "clauses are deleted when they are dominated by learned clauses .",
    "later , we call the added clauses _ lemmas _ , in agreement with the terminology of learning for the satisfiability problem sat of propositional logic .    the learning step has been implemented in an existing compiler .",
    "test results have shown a worst - case time reduction for a given class by a factor ranging from a not - so - useful @xmath0 to a desirable @xmath1 .",
    "total time for the learning step has ranged from @xmath2sec to almost @xmath3hr , with the majority of classes requiring less than @xmath4min .",
    "while a learning time of @xmath3hr is long , even that time may be acceptable if an application demands that worst - case solution times are below a critical bound that one is unable to satisfy by other means .",
    "we define the problems sat and minsat .",
    "an instance of sat is a propositional logic formula @xmath5 in conjunctive normal form ( cnf ) .",
    "thus , @xmath5 is a conjunction of _ clauses_. in turn , each clause is a disjunction of _ literals _ , which are instances of possibly negated variables .",
    "a literal is _ negative _ ( resp . _ positive _ ) when it is an instance of a negated ( resp .",
    "nonnegated ) variable .",
    "the sat problem demands that one either determines @xmath5 to be unsatisfiable  that is , there do not exist  for the variables so that @xmath5 evaluates to or produces a satisfying solution .",
    "an instance of minsat consists of a cnf formula @xmath5 and , for each variable @xmath6 of @xmath5 , a pair @xmath7 of rational numbers .",
    "the number @xmath8 ( resp .",
    "@xmath9 ) is the cost incurred when @xmath6 takes on the value ( resp . ) .",
    "the minsat problem demands that one either determines @xmath5 to be unsatisfiable or produces a satisfying solution whose total cost @xmath10 is minimum .",
    "it is easy to see that the optimality of a solution is not affected if one subtracts a constant from both @xmath8 and @xmath9 , or if one replaces a variable @xmath6 by @xmath11 and @xmath12 by @xmath13 , and for @xmath13 defines the cost @xmath14 of  to be @xmath9 and the cost @xmath15 of  to be @xmath8 .",
    "hence , we may suppose that @xmath16 and @xmath17 .",
    "due to that reduction , we may represent any minsat instance by a pair @xmath18 , where @xmath19 is a nonnegative rational vector of costs that apply when variables take on the value .",
    "a _ subinstance _ of @xmath5 or @xmath18 is derived from @xmath5 or @xmath18 by fixing some variables of @xmath5 to .",
    "a _ lemma _ obtained by learning is a cnf clause .",
    "the _ length _ of the lemma or of a cnf clause is the number of literals of the clause . due to this definition",
    ", we can use qualitative terms such as _ short _ or _ long _ in connection with lemmas or clauses .",
    "we have two special forms of cnf formulas .",
    "a cnf formula has _ restricted hidden horn form _ if complementing of the literals of some variables whose cost of  is @xmath20 can turn the formula into one where each clause has at most one positive literal .",
    "a cnf formula has _",
    "network form _ if complementing of the literals of some variables , followed by complementing of the literals of some clauses , can turn the given formula into one where at least one of the following two conditions is satisfied .",
    "the first condition demands that each clause has at most two literals ; in the case of two literals , exactly one must be negative .",
    "these cnf formulas with the network property are special cases of 2sat .",
    "the second condition requires that each variable occurs in at most two clauses ; in the case of two clauses , exactly one of the two literals must be negative .",
    "consider , for example , the cnf formula in network form with the following clauses : @xmath21 after complementing the literals of the first clause , we obtain the cnf formula @xmath22 where each variable occurs in at most two clauses .",
    "each of the variables @xmath23 , @xmath24 , @xmath25 , @xmath26 , which occurs in two clauses , occurs exactly once negatively .",
    "presence of hidden horn or network form can be tested in linear time .",
    "any minsat instance @xmath18 whose @xmath5 has restricted hidden horn form ( resp .",
    "network form ) can be solved in linear ( resp .",
    "low - order polynomial ) time and thus very fast @xcite .",
    "the typical class  of minsat that is treated here consists of a minsat instance @xmath18 , plus all instances that may be derived from that instance by fixing some variables of @xmath5 to and deleting the clauses that become satisfied by these values , and by outright removal of some variables and clauses . in the typical application ,",
    "the candidate variables and clauses that possibly will be removed are known a priori , and their removal is readily handled by the process of fixing variables , as follows .",
    "first , the removal of a candidate variable @xmath6 can be modeled by the introduction of additional variables @xmath27 , @xmath28 , and @xmath29 , where @xmath30 represents presence of @xmath6 , and where @xmath31 represents removal of @xmath6 . for the new variables , the cost of  and is equal to @xmath20 .",
    "each occurrence of @xmath6 in @xmath5 is replaced by @xmath28 , each occurrence of @xmath12 in @xmath5 is replaced by @xmath29 , and cnf clauses equivalent to @xmath32 and @xmath33 are added to @xmath5 .",
    "second , the removal of a candidate clause can be effected by the addition of a variable @xmath34 to the clause so that @xmath35 causes the clause to be satisfied , while @xmath36 , by itself , does not . hence , it suffices that we consider the class consisting of all instances derived from the given minsat instance @xmath18 by fixing of variables and deletion of satisfied clauses  that is ,  consists of @xmath18 and its subinstances .",
    "classes  of minsat arise from applications where one must find a least - cost satisfying solution for a minsat instance @xmath18 when some variables take on specified values .",
    "uses in expert systems or logic modules abound .",
    "for example in diagnosis , we can use costs to search for minimal sets of defects , or we can realize priorities and penalties . for examples and references",
    "see eiter and gottlob  .",
    "other examples , like natural language processing and traffic control , produce minsat classes of the specified type .",
    "the problem of finding minimal models can be solved by a minsat instance if a -cost of @xmath2 is assigned to each variable .",
    "ben - eliyahu and dechter  , for example , investigate two classes of the minimal model problem and present an effective algorithm for each of the two classes .",
    "much work has been done on learning in sat algorithms .",
    "early references are dechter   and prosser  .",
    "they enhance backtracking search algorithms for csp by a learning process as follows . whenever an assignment of values to variables violates a constraint , the reason for the violation is determined and added to the csp instance as a lemma which becomes a constraint of the csp instance .",
    "the same ideas are used by effective sat algorithms such as grasp @xcite , sato3 @xcite , or relsat(4 ) @xcite .",
    "since the required space for learned lemmas can be exponential , marques - silva and sakallah   and zhang   keep only clauses of bounded length .",
    "the sat solver relsat(4 ) not only keeps short clauses , but also retains long clauses temporarily ; see bayardo and schrag   for details . algorithm learn - sat by richards and richards   for csp assigns values to the variables incrementally so that no constraint is violated .",
    "if such an assignment can not be extended without violating a constraint , a lemma that invalidates the current partial assignment is added to the csp instance , and learn - sat tries to find another assignment .",
    "van gelder and okushi   use lemmas to prune refutation trees in the sat solver modoc .",
    "learning is also used in the sat algorithm satz @xcite , where short clauses are computed by resolution before the solution process begins . in the preprocessing step of marques",
    "- silva  , lemmas of at most length 2 are inferred from small subsets of the cnf clauses with length 2 and 3 in the given sat instance .",
    "variables . in this more general",
    "setting , a different learning technique makes a - priori predictions about an instance to select and tune a sat algorithm .",
    " nuallin , de rijke , and van benthem   apply a prediction based on bayesian methods .",
    "their systematic backtracking search procedure derives criteria for restart strategies .",
    "the backtracking search of lagoudakis and littman   uses a learning technique that selects a branching rule at each node in the search tree .",
    "some compilation techniques that are applied to classes of sat instances try to obtain computationally more attractive logic formulations that preserve equivalence ; see , for example , del val  . kautz and selman",
    "compute tractable formulations that approximate the original sat instance .",
    "for further references on compilation techniques , see the survey of cadoli and donini  .",
    "the logic minimization problem minsat so far has not attracted much attention .",
    "most work treats the special case of finding minimal models where all costs for  are @xmath2 and all costs for  are @xmath20 .",
    "ben - eliyahu and dechter  , for example , characterize formulas that have an efficient algorithm for computing minimal models .",
    "liberatore   describes an algorithm for the problem of finding minimal models of cnf formulas and for its extension minsat based on backtracking search . in experiments",
    ", he investigates in which cases the problem is hard and in which cases it is easy .",
    "a compiler for minsat is described in truemper   and is implemented in the leibniz system  .",
    "the compiler obtains a solution algorithm for a given  and determines an upper bound on the solution time for the members of the class .",
    "we need a basic understanding of the compiler since the learning step uses some information produced by that process .",
    "the compiler employs several decompositions that break up the instance @xmath18 defining a class  into any number of components , each of which is a cnf formula plus applicable costs . for the subinstances of each component",
    ", the compiler determines a solution algorithm that is used as subroutine in the overall solution algorithm for the instances of . for a given instance of , the overall algorithm invokes the subroutines any number of times , each time solving some subinstance of a component .    for the description of the typical subroutine , temporarily let @xmath18 denote one component .",
    "we obtain a _ partial instance _ by deleting from the clauses of @xmath5 all literals arising from a specified set of variables and by reducing @xmath19 accordingly .",
    "the compiler partitions the variables of @xmath18 into two sets that induce two partial instances @xmath37 and @xmath38 .",
    "the partition is so done that the partial instance @xmath37 has one of two properties and , subject to that condition , has as many variables as possible .",
    "the properties are restricted hidden horn form and network form , defined in section  2 .",
    "each of the two properties is maintained under the deletion of variables or clauses , and permits fast solution of any instance derived from @xmath37 by deletion of variables and clauses .",
    "let @xmath39 be the set of variables of @xmath38 . still consider @xmath18 to be just one component .",
    "the solution algorithm for @xmath18 consists of two parts : an enumerative subroutine that chooses values for the variables of @xmath39 , and the fast subroutine for @xmath37 .",
    "specifically , the enumerative subroutine implicitly tries out all possible  values for the variables of @xmath39 , evaluates which clauses of @xmath5 become satisfied by these values , and uses the fast subroutine for @xmath37 to find a least - cost solution for the remaining clauses or to decide that no such solution exists .",
    "the growth of the search tree is controlled by the moms ( maximum occurrences in minimum size clauses ) heuristic , which selects the next variable on which to branch .",
    "we modified a version of the heuristic described in bhm  .",
    "the original selection rule of bhm   is part of a purely enumerative algorithm for sat .",
    "it aims at fixing variables in such a sequence that each branch of the search tree quickly reaches provable unsatisfiability .",
    "the rule achieves this goal very well by , roughly speaking , fixing variables that occur in a maximum number of the currently shortest clauses .",
    "computational results achieved by bhm and speckenmeyer   with the rule are excellent . we have found a modified version of the rule to be just as effective for the case at hand , where @xmath18 has been partitioned into @xmath37 and @xmath38 .",
    "details of the rule are as follows .",
    "let @xmath40 be the cnf formula that results by resolving all unit clauses in @xmath5 .",
    "we want to find a variable in @xmath39 with maximum occurrences in minimum clauses .",
    "the variable should also satisfy at least one clause in @xmath41 so that @xmath41 might become satisfiable in a succeeding step . for each variable @xmath42 and",
    "not yet fixed in a previous step , define vectors @xmath43 and @xmath44 as follows .",
    "the @xmath45th entry @xmath46 of @xmath43 is the number of times the literal @xmath6 occurs in a clause of @xmath40 of length @xmath45 that contains at most one literal of a variable not in @xmath39 .",
    "the vector @xmath44 records in analogous fashion the occurrences of the literal @xmath12 .",
    "we combine the vectors so that the resulting vector @xmath47 has large values @xmath48 if @xmath49 and @xmath46 are large and if the difference between @xmath49 and @xmath46 is small .",
    "we set @xmath50 for a suitable constant @xmath51 .",
    "experiments have shown that @xmath52 is a good choice @xcite .",
    "therefore , we use @xmath53 .",
    "let @xmath54 be such that @xmath55 is the lexicographically largest vector of the @xmath47 vectors .",
    "the variable @xmath54 is to be fixed next . in order to take advantage of learned lemmas ,",
    "see section 5 , we want to obtain a satisfying assignment with low total cost at an early stage .",
    "thus , we assign to @xmath54 the value , if @xmath56 , and if a satisfying solution has not yet been found or the cost of  for @xmath6 is 0 .",
    "otherwise , assign to @xmath54 the value . throughout the paper , we refer to the modified rule as _ bhm s",
    "rule_.    the efficiency of the overall algorithm depends on how many times subinstances of the various components must be solved and how fast the subroutines solve those instances .",
    "the first factor depends on the decomposition and is not addressed here .",
    "the second factor can be influenced by learning for each component .",
    "the learning process treats one component at a time , using just the clauses of the component that are clauses of @xmath5 and that have no variable in common with any other component . due to these restrictions , the learned lemmas when added to the component and to @xmath5",
    "do not invalidate the decomposition .",
    "this means that , for the purposes of this section , we only need to consider the case of a component @xmath18 that has not been decomposed , and where learning for the class is to be accomplished .",
    "the learning is done in two steps . in the first step ,",
    "we ignore the costs , treat  as a class of sat problems , and learn lemmas for @xmath5 . in the second step ,",
    "we learn lemmas that are cost dependent .",
    "we use the two - step approach since learning from the sat cases tends to make learning easier for the minsat cases .",
    "in fact , for some situations , learning from the minsat cases without prior learning from the sat cases requires a huge computational effort that makes the learning process impractical .",
    "in addition , the first step can be applied to classes originally defined as sat cases as well .",
    "we describe the first step , which ignores the cost vector @xmath19 and considers  to consist of sat instances derived from @xmath5 .",
    "since bhm s rule depends on the currently shortest clauses , a different but equivalent cnf formula may lead to a different selection of variables .",
    "for example , if the cnf formula @xmath5 contains the clauses @xmath57 and @xmath58 , but does not contain the implied clause @xmath59 , then bhm s rule sees only the two explicit clauses and not the implied one , and therefore may not detect that fixing @xmath6 is an attractive choice .",
    "the learning process is designed to discover lemmas that represent such useful implied clauses , which then guide bhm s rule toward good choices .",
    "note that we want lemmas that are useful not just for solving the single sat instance @xmath5 , but that are useful for solving all instances of the class derived from @xmath5 . for the moment",
    "let us ignore that aspect and see how we can learn just to solve the instance @xmath5 more efficiently .",
    "for this , we apply to @xmath5 the algorithm derived by the compiler for @xmath60 and @xmath41 as described above , using bhm s rule to select the variables of @xmath39 for enumeration .",
    "if @xmath5 is unsatisfiable , then , mathematically speaking , only one lemma , which is the empty clause , needs to be learned ; in applications , that situation signals a formulation error .",
    "so let us assume that @xmath5 is found to be satisfiable .",
    "when the algorithm stops , the search tree has been pruned to a path @xmath61 whose end node has led to a satisfying solution .",
    "starting at the root node of @xmath61 , let us number the nodes @xmath62 , for some @xmath63 .",
    "suppose at node @xmath45 the variable @xmath64 was fixed to value @xmath65 .",
    "at that node , one of two cases applies .",
    "either the algorithm fixed @xmath64 to @xmath65 without trying the opposite value @xmath66 first , or the algorithm first tried the opposite value @xmath66 , discovered unsatisfiability , and then assigned @xmath65 .",
    "the latter case implies that @xmath67 , @xmath68 , @xmath69 , @xmath70 produce unsatisfiability .",
    "hence , we may add to @xmath5 a lemma that rules out that assignment .",
    "for example , if @xmath71 , @xmath72 , and @xmath73 produce unsatisfiability , then the lemma is @xmath74 . at this point , we begin a time - consuming process that is acceptable for learning in a compiler but would not be reasonable at run time .",
    "that is , we sharpen the lemma by removing from the lemma the literals corresponding to @xmath75 one at a time . for each such removal",
    ", we check whether the reduced clause @xmath76 is still a logic consequence of @xmath5 .",
    "we test this by solving the sat instance @xmath77 .",
    "if @xmath77 is unsatisfiable , that is @xmath78 is a tautology , the clause @xmath76 is a valid lemma . otherwise , we add the previously removed literal again to @xmath76 .",
    "we continue to remove literals from the resulting clause .",
    "when that effort stops , we have a minimal lemma , that is , a lemma that becomes invalid if any literal is removed .",
    "we want these lemmas to steer bhm s rule so that good choices are made at or near the root of the search trees .",
    "since bhm s rule selects variables based on short clauses , the desired effect can only be achieved by short lemmas .",
    "thus , we discard all minimal lemmas of length greater than some constant .",
    "> from our experiments , we determined that constant to be 3 .",
    "that is , we only retain the minimal lemmas of length 1 , 2 , or 3 and add them to @xmath5 .",
    "observe that a learned lemma contains only variables of @xmath39 and thus does not violate the special property of @xmath41 .",
    "up to this point , we have considered learning of lemmas that help the solution algorithm to solve @xmath5 .",
    "we extend this now to instances of different from @xmath5 .",
    "any such instance is derived from @xmath5 by fixing some variables .",
    "correspondingly , we start the enumerative search by first fixing these variables and then proceeding as before . effectively , the search tree begins with a path @xmath79 representing the initial fixing instead of just with the root node .",
    "the algorithm either finds a satisfying solution , or it stops and declares @xmath5 to be unsatisfiable . in the first case",
    ", we determine minimal lemmas , if possible , discard the minimal lemmas that are too long , and adjoin the remaining ones to @xmath5 . due to the path @xmath79 , a lemma added to @xmath5 may involve variables of @xmath41 and thus may destroy the special property of @xmath41 . nevertheless , these lemmas do not have to be discarded . a lemma that destroys",
    "the special property of @xmath41 is a logical consequence of @xmath5 .",
    "hence , it can be ignored , whenever the satisfiability of @xmath5 is tested , and thus whenever , the instance @xmath41 is solved . for details , see remshagen  .",
    "we have completed the discussion of learning lemmas for sat and turn to the second step of the learning process . here",
    "we learn cost - dependent lemmas for the minsat instance @xmath18 and for all instances derived from @xmath18 by fixing some variables to  values in all possible ways .",
    "the solution algorithm for minsat not only prunes unsatisfiable assignments as in the sat case , but also eliminates assignments resulting in nonoptimal total costs .",
    "learning is possible for both cases , as follows . at some point",
    ", the solution algorithm for minsat finds a fixing of variables that eventually turns out to be part of an optimal solution .",
    "say , @xmath23 , @xmath24 ,  , @xmath80 fixed to @xmath81 , @xmath82 , ",
    ", @xmath83 induce an optimal solution with total cost @xmath84 .",
    "when the algorithm terminates , we know the following for each @xmath85 : the fixing of @xmath23 , @xmath24 ,  , @xmath86 to the values @xmath81 , @xmath82 ,  , @xmath87 , @xmath88 results into unsatisfiability , or that fixing can be extended to a solution that at best has total cost @xmath89 . the first case is treated exactly as before .",
    "that is , we define lemma @xmath90 where , for @xmath91 , @xmath92 ( resp .",
    "@xmath93 ) if @xmath94 ( resp .",
    "@xmath95 ) and where @xmath96 ( resp .",
    "@xmath97 ) if @xmath98  ( resp .",
    "@xmath99 ) . in the second case",
    ", we define the same lemma @xmath76 and combine it with @xmath100 to the pair @xmath101 . in the solution algorithm , the clause @xmath76 of @xmath101 is activated if we have already a solution with total cost not exceeding @xmath100 .",
    "otherwise , the clause is ignored . in both cases ,",
    "we do not use @xmath76 directly , but reduce it to a minimal lemma . in the first case ,",
    "the reduction is the same as for sat . in the second case , @xmath76 is reduced by the following process : except for @xmath102 , process the literals @xmath103 of @xmath76 one by one and in decreasing order of indices . using decreasing order of indices ,",
    "favors the retention of literals whose corresponding variable has been selected first .",
    "these variables are generally more likely to be selected early .",
    "thus , the new lemma will more likely be used to prune nonoptimal solutions .",
    "derive @xmath104 from @xmath76 by removing @xmath103 , find an optimal solution for the minsat instance @xmath105 , and permanently remove @xmath103 from @xmath76 if the total cost of that solution is not less than @xmath100 . using the final @xmath76 , the pair @xmath101 is then inserted into to the formula . as before , we retain only pairs @xmath101 where @xmath76 has at most length @xmath106 .",
    "we want to add minimal lemmas to @xmath5 that improve the effectiveness of bhm s rule when that rule , unassisted by lemmas , would perform badly .",
    "moreover , we want to achieve this across the full range of instances of .",
    "a simple idea to achieve this goal is as follows .",
    "we select an instance of  and solve it .",
    "if the enumerative effort is large , we determine minimal lemmas as described earlier and add them to @xmath5 .",
    "we repeat this process for other instances until we get a fast solution time no matter which instance of  is selected .",
    "how much learning might be required ?",
    "we do not have a complete answer for that question .",
    "one can show that , if one could achieve that goal reliably by learning from a number of instances that is bounded by a polynomial in the size of @xmath5 , then @xmath107 for the polynomial hierarchy ; see remshagen  . for details of that hierarchy ,",
    "see , for example , chap .",
    "17 of papadimitriou  .",
    "this negative result makes it unlikely that in general we can learn enough from a polynomial subset of . on the other hand",
    ", we may be able to carry out such learning for specific classes . in the next section",
    ", we demonstrate experimentally that this is indeed possible for nontrivial classes , provided the instances of  to which the learning process is applied are selected according to a certain rule . in the remainder of this section , we develop that rule .",
    "it is based on the reasonable argument that one should focus on instances of  that are difficult prior to learning , in the hope that the learned lemmas not only help in the solution of the difficult cases , but also do not worsen the performance for the easy cases .",
    "we begin with a conceptual process for the selection of difficult cases .",
    "we say `` conceptual '' since the process is computationally inefficient and later is replaced by a more effective scheme . for @xmath108 , let @xmath109 be the subset of  where each instance is obtained by fixing @xmath45 arbitrarily selected variables in @xmath5 .",
    "let @xmath110 be the average time required to solve an instance of @xmath109 .",
    "( a method to compute the average time will be discussed shortly . )",
    "since the algorithm produced by the compiler solves instances of  very rapidly if all or almost all variables of @xmath39 have been fixed , the values @xmath110 are small when @xmath45 is close to or equal to @xmath111 .",
    "correspondingly , there is no need to learn lemmas from those easy instances . on the other hand , large @xmath110 values point to sets @xmath109 with instances where learning of lemmas would be useful .",
    "accordingly , the conceptual process is as follows . for @xmath108 , we randomly select a certain number of instances ,",
    "say @xmath112 , from @xmath109 , solve them , and estimate @xmath110 by the average @xmath113 of the solution times .",
    "when the @xmath113 values are plotted , they produce a curve that typically starts high and gradually decreases , or that rises quickly , reaches a plateau , and then gradually decreases . in both cases , we stop the computation when consecutive @xmath113 values become consistently small .",
    "let @xmath114 be the index of the largest @xmath113 . in case of a tie ,",
    "pick @xmath114 as the largest index satisfying the condition . by experimentation we found that significant learning took place when we used all @xmath109 with @xmath115 .",
    "in contrast , learning from any @xmath109 with @xmath116 did not improve performance .",
    "appealing as the conceptual selection process may seem , it suffers from a serious shortcoming .",
    "computational effort for obtaining the index @xmath114 is large , yet nothing of that effort is utilized for learning lemmas save for the termination criterion based on @xmath114 .",
    "indeed , in initial tests , sometimes more than @xmath117% of the computational effort for learning was spent on the determination of @xmath114 .",
    "we eliminate such waste as follows . while learning lemmas , we determine indirectly when the index @xmath114 has been exceeded by estimating the index where learning stops to improve performance .",
    "whenever the solution algorithm solves an instance during learning , it records the required time .",
    "let @xmath118 be the largest solution time for all processed instances derived by fixing of @xmath45 variables .",
    "as soon as an index @xmath45 is reached where @xmath118 exceeds @xmath119 , we know that learning no longer improves performance .",
    "accordingly , we estimate that @xmath45 is @xmath120 of the conceptual process and terminate learning .",
    "we have tested whether the estimate is correct .",
    "it turned out that , except for cases of early termination ",
    "see next paragraph ",
    "the termination decisions made via @xmath120 of the conceptual process and the largest solution time @xmath118 were identical .",
    "there are two cases in which the learning process for minsat stops early . in the first case ,",
    "learning is stopped since the worst - case time bound becomes so low that further improvement is not needed . in our implementation , we determine a new decomposition of the cnf formula whenever new lemmas are added .",
    "if the resulting set @xmath39 of each component contains at most five variables , then each component can be solved in polynomial time , and learning terminates . in the second case ,",
    "learning is stopped since the number of learned clauses becomes too large and processing of those clauses becomes too time - consuming .",
    "we terminate when the total number of clauses and lemmas has become triple the number of clauses of the original cnf formula .",
    "indeed , the overhead of processing a significantly increased cnf formula can become so large that the solution algorithm is slowed down even though fewer backtracking steps are needed .",
    "we have added the learning process for sat and minsat classes to logic programming software @xcite that is based on truemper  .",
    "the computational results described below were obtained on a sun ultrasparc iii ( 333mhz ) workstation .",
    "epsf    let @xmath113 ( resp .",
    "@xmath121 ) be the average time estimate for @xmath109 before ( resp .",
    "after ) the learning process .",
    "let us call the curve of the plotted @xmath113 ( resp .",
    "@xmath121 ) the _ performance curve before learning _ ( resp .",
    "_ performance curve after learning _ ) .",
    "when  contains difficult - to - solve instances , the performance curve before learning typically starts high and gradually decreases , or rises quickly , reaches a plateau , and then gradually decreases . in the ideal situation , learning eliminates the high portion of that curve so that the values of the performance curve after learning are uniformly small .",
    "we illustrate this notion using a sat instance called sat200 - 4_0 with 200 variables and 800 clauses .",
    "each clause contains exactly three literals .",
    "the corresponding class  consists of all instances derived from sat200 - 4_0 by fixing of some variables . learning increases the number of clauses to 1600 .",
    "figure  1 shows the performance curves of sat200 - 4_0 before and after learning . before learning",
    ", @xmath122 produces the peak @xmath123sec . after learning ,",
    "the high portion of the curve is eliminated , and the curve has values that are uniformly close to @xmath20 . even more desirable than a uniform learning of the average solution times of the @xmath109 is reduction of the worst - case run time for each @xmath109 so that the solution time of each instance becomes uniformly small . for our purposes",
    ", it suffices that we estimate the worst - case run time for @xmath109 using the highest run time of the 100 instances that are randomly selected for each @xmath109 when the @xmath113 and @xmath121 are calculated . in the case of sat200 - 4_0 , the high values of the worst - case run times before learning , which range up to @xmath124sec ,",
    "are uniformly reduced to values not exceeding @xmath125sec .",
    ".test instances [ cols= \" < , > , > , > , > \" , ]     no improvement took place for ochem .",
    "the reason is that ochem has a large solution space so that the satisfiability problem is very easy .",
    "the problem becomes difficult only when it is solved as a minimization problem as originally defined .",
    "there are large subclasses of minsat having the same characteristics .",
    "for example , the cnf formula of the set covering problem consists only of positive literals , that is , each variable is monotone with preferred value .",
    "thus , any sat instance derived from a set covering problem is trivial .",
    "however , set covering becomes -hard if the number of  assignments has to be minimized .",
    "because of the monotonicity of the variables , no new minimal clauses exist , and hence learning of new minimal lemmas is not possible .",
    "we discuss the results for the entire minsat learning process .",
    "for all problems except ochem , the learning process terminates early since either a low worst - case time bound is determined , or the number of clauses becomes too large .",
    "table  3 displays the computational results .",
    "the interpretation is as for table  2 .",
    "the times for the learning process range from @xmath2sec to almost @xmath3hrs .",
    "the majority of the cases requires less than @xmath4min . to evaluate the effect of the learning",
    ", we apply the same evaluation criteria as for table  2 .",
    "that is , we look at the problems that have worst time before learning greater than @xmath126sec .",
    "the problems are sat100 - 4_3 , sat200 - 4_3 , sat100 - 4_0 , sat200 - 4_0 , jnh201 , par8 - 3-c , par16 - 1-c , bw_large.a , and ochem .",
    "no instance guarantees a solution time below @xmath127sec before learning . after learning ,",
    "the classes derived from sat100 - 4_3 , par8 - 3-c , par16 - 1-c , and bw_large.a , obtain a guaranteed low time bound .",
    "for bw_large.a , the time bound is @xmath128sec .",
    "for the other instances , the time bounds do not exceed @xmath129sec .",
    "the reduction factor of these problems for the worst time ranges from @xmath130 to @xmath1 .",
    "the reduction factors of sat200 - 4_3 and sat200 - 4_0 are @xmath131 and @xmath132 .",
    "if the learning process is already terminated after the first step , the worst time of sat200 - 4_0 is reduced by a factor of @xmath133 and the worst time of sat200 - 4_3 by a factor of @xmath134 .",
    "thus , the pairs @xmath135 inserted in the second step halve the run time at most , and the overall improvement is primarily due to the lemmas inserted in the first learning step .",
    "the strong effect of the lemmas inserted in the first step declines for instance sat100 - 4_0 .",
    "learning of lemmas results in a reduction factor of @xmath136 , further learning of lemmas and pairs in the second step improves the worst - case time by another factor of @xmath106 .",
    "the worst - case times before learning show that jnh201 and ochem are much more difficult as optimization problems than as satisfiability problems .",
    "these problems have a very large solution space , and hence the computational effort to prune nonoptimal solutions by tree search increases .",
    "the effect of the first learning step is small for jnh201 and is zero for ochem .",
    "the first learning step for jnh201 reduces the worst - case time from @xmath137sec to @xmath138sec .",
    "the entire learning process achieves worst - case time @xmath139sec by insertion of pairs @xmath135 .",
    "that is a reduction factor of @xmath140 .",
    "for instance ochem , no lemmas are inserted in the first learning step , and thus only pairs @xmath135 cause the speedup . however , the improvement for ochem is not significant .",
    "the worst - case time decreases only from @xmath141sec to @xmath142sec , which is a reduction factor of @xmath0 . enforcing learning of more lemmas and pairs @xmath135 for ochem reduced the worst - case time further to @xmath143sec , which gives a reduction factor of @xmath144 .",
    "compared with lemmas , the disadvantage of pairs @xmath135 is that during execution of tree search only those pairs @xmath135 are enforced whose cost value @xmath145 does not exceed the cost of the currently best solution .",
    "one may alleviate that problem by computing heuristically a starting solution with low cost @xmath146 .",
    "then , all pairs @xmath135 with @xmath147 can be activated before tree search starts .",
    "we used the heuristic of the leibniz system , which uses linear programming and rounding of fractional values , to determine a good solution .",
    "that solution speeded up tree search , but due to the computational effort for the heuristic , the total solution time was only slightly reduced .",
    "we have introduced a solution algorithm for classes of minsat instances .",
    "the solution algorithm is based on backtracking search .",
    "the search takes place for a subset of the variables only .",
    "the subset containing the remaining variables induces a cnf formula that can be solved efficiently .",
    "a compiler is used to determine the partition of the variables into the two subsets .",
    "in addition , a learning process within the compiler determines lemmas .",
    "lemmas are logical consequences of the given cnf formula or clauses that prune nonoptimal satisfying truth assignments .",
    "the number and kind of learned lemmas is crucial for effective learning .",
    "the learning process computes useful lemmas and measures current execution times within the compiler to terminate before the number of learned lemmas becomes too large .",
    "the compiler does not need human interaction or manual setting of parameters . in most test cases ,",
    "the learned lemmas improve the solution process significantly .",
    "this research was supported in part by the office of naval research under grant n00014 - 93 - 1 - 0096 .",
    "del val , a. ( 1994 ) .",
    "tractable databases : how to make propositional unit resolution complete through compilation .",
    "_ proceedings of fourth international conference on principles of knowledge representation and reasoning ( kr-94 ) _ , 551561 .",
    "marques - silva , j. p. ( 2000 ) .",
    "algebraic simplification techniques for propositional satisfiability .",
    "_ proceedings of the 6th international conference on principles and practice of constraint programming _ , 537542 .",
    "trick , m. a. ( 1996 ) .",
    "second dimacs challenge test problems . in d.",
    "s. johnson and m. a. trick ( eds . ) , _ cliques , coloring and satisfiability : second dimacs implementation challenge _ ,",
    "volume 26 of dimacs series in discrete mathematics and computer science , american mathematical society , pp ."
  ],
  "abstract_text": [
    "<S> this paper describes learning in a compiler for algorithms solving classes of the logic minimization problem minsat , where the underlying propositional formula is in conjunctive normal form ( cnf ) and where costs are associated with the  values of the variables . </S>",
    "<S> each class consists of all instances that may be derived from a given propositional formula and costs for  values by fixing or deleting variables , and by deleting clauses . </S>",
    "<S> the learning step begins once the compiler has constructed a solution algorithm for a given class . </S>",
    "<S> the step applies that algorithm to comparatively few instances of the class , analyses the performance of the algorithm on these instances , and modifies the underlying propositional formula , with the goal that the algorithm will perform much better on all instances of the class . </S>"
  ]
}