{
  "article_text": [
    "inferring macroscopic properties of physical systems from their microscopic description is an ongoing work in many disciplines of physics , like condensed matter , ultra cold atoms or quantum chromo dynamics .",
    "the most drastic changes in the macroscopic properties of a physical system occur at phase transitions , which often involve a symmetry breaking process .",
    "the theory of such phase transitions was formulated by landau as a phenomenological model @xcite and later devised from microscopic principles using the renormalization group @xcite .",
    "one can identify phases by knowledge of an order parameter which is zero in the disordered phase and nonzero in the ordered phase .",
    "whereas in many known models the order parameter can be determined by symmetry considerations of the underlying hamiltonian , there are states of matter where such a parameter can only be defined in a complicated non - local way @xcite .",
    "these systems include topological states like topological insulators , quantum spin hall states @xcite or quantum spin liquids @xcite .",
    "therefore , we need to develop new methods to identify parameters capable of describing phase transitions .    such methods might be borrowed from machine learning . since the 1990s this field has undergone major changes with the development of more powerful computers and artificial neural networks .",
    "it has been shown that such neural networks can approximate every function under mild assumptions @xcite .",
    "they quickly found applications in image classification , speech recognition , natural language understanding and predicting from high - dimensional data .",
    "furthermore , they began to outperform other algorithms on these tasks @xcite .    in the last years physicists started to employ machine learning techniques .",
    "most of the tasks were tackled by supervised learning algorithms or with the help of reinforcement learning @xcite . supervised",
    "learning means one is given labeled training data from which the algorithm learns to assign labels to data points . after successful training",
    "it can then predict the labels of previously unseen data with high accuracy .",
    "in addition to supervised learning , there are unsupervised learning algorithms which can find structure in unlabeled data .",
    "they can also classify data into clusters , which are however unlabelled .",
    "it is already possible to employ unsupervised learning techniques to reproduce monte - carlo - sampled states of the ising model @xcite .",
    "phase transitions were found in an unsupervised manner using principal component analysis @xcite .",
    "we employ more powerful machine learning algorithms and transition to methods that can handle nonlinear data .",
    "a first nonlinear extension is kernel principal component analysis @xcite .",
    "the first versions of autoencoders have been around for decades @xcite and were primarily used for dimensional reduction of data before feeding it to a machine learning algorithm .",
    "they are created from an encoding artificial neural network , which outputs a latent representation of the input data , and a decoding neural network that tries to accurately reconstruct the input data from its latent representation .",
    "very shallow versions of autoencoders can reproduce the results of principal component analysis @xcite .    in 2013 ,",
    "variational autoencoders have been developed as one of the most successful unsupervised learning algorithms @xcite .",
    "in contrast to traditional autoencoders , variational autoencoders impose restrictions on the distribution of latent variables .",
    "they have shown promising results in encoding and reconstructing data in the field of computer vision .    in this work",
    "we use unsupervised learning to determine phase transitions without any information about the microscopic theory or the order parameter .",
    "we transition from principal component analysis to variational autoencoders , and finally test how the latter handles different physical models .",
    "our algorithms are able to find a low dimensional latent representation of the physical system which coincides with the correct order parameter .",
    "the decoder network reconstructs the encoded configuration from its latent representation .",
    "we find that the reconstruction is more accurate in the ordered phase , which suggests the use of the reconstruction error as a universal identifier for phase transitions .",
    "whereas for physicists this work is a promising way to find order parameters of systems where they are hard to identify , computer scientists and machine learning researchers might find an interpretation of the latent parameters .",
    "the ising model is one of the most - studied and well - understood models in physics .",
    "whereas the one - dimensional ising model does not possess a phase transition , the two - dimensional model does .",
    "the hamiltonian of the ising model on the square lattice with vanishing external magnetic @xmath0 field reads @xmath1 with uniform interaction strength @xmath2 and discrete spins @xmath3 on each site @xmath4 .",
    "the notation @xmath5 indicates a summation over nearest neighbors . a spin configuration @xmath6 is a fixed assignment of a spin to each lattice site",
    ", @xmath7 denotes the set of all possible configurations @xmath8 .",
    "we set the boltzmann constant @xmath9 and the interaction strength @xmath10 for the ferromagnetic case and @xmath11 for the antiferromagnetic case .",
    "a spin configuration @xmath8 can be expressed in matrix form as @xmath12 lars onsager solved the two dimensional ising model in 1944 @xcite .",
    "he showed that the critical temperature is @xmath13 .",
    "for the purpose of this work , we assume a square lattice with length @xmath14 such that @xmath15 , and periodic boundary conditions .",
    "we sample the ising model using a monte - carlo algorithm @xcite at temperatures @xmath16 $ ] to generate @xmath17 samples in the ferromagnetic case and @xmath18 samples in the antiferromagnetic case .",
    "the ising model obeys a discrete @xmath19-symmetry , which is spontaneously broken below @xmath20 .",
    "the magnetization of a spin sample is defined as @xmath21 the partition function @xmath22 allows us to define the corresponding order parameter .",
    "it is the expectation value of the absolute value of the magnetization at fixed temperature @xmath23 similarly , with the help of the matrix @xmath24 , we define the order parameter , as the expectation value of the staggered magnetization .",
    "the latter is calculated from an element - wise product with a matrix form of the spin configurations @xmath25      the mermin - wagner - hohenberg theorem @xcite prohibits continuous phase transitions in @xmath26 dimensions at finite temperature when all interactions are sufficiently short - ranged .",
    "hence , we choose the xy model in three dimensions as a model to probe the ability of a variational autoencoder to classify phases of models with continuous symmetries .",
    "the hamiltonian of the xy model reads    @xmath27    with spins on the one - sphere @xmath28 .",
    "employing @xmath10 , the transition temperature of this model is @xmath29 @xcite using a cubic lattice with @xmath30 , such that @xmath31 , we perform monte - carlo simulations to create 10000 independent sample spin configurations in the temperature range of @xmath32 $ ] .",
    "the order parameter is defined analogously to the ising model magnetization , but with the @xmath33-norm of a magnetization consisting of two components .",
    "_ principal component analysis",
    "_ @xcite is an orthogonal linear transformation of the data to an ordered set of variables , sorted by their variance .",
    "the first variable , which has the largest variance , is called the first principal component , and so on . the linear function @xmath34 , which maps a collection of spin samples @xmath35 to its first principal component , is defined as @xmath36 \\ , \\label{eq : pca}\\end{aligned}\\ ] ] where @xmath37 is the vector of mean values of each spin averaged over the whole dataset .",
    "further principal components are obtained by subtracting the already calculated principal components and repeating .",
    "_ kernel principal component analysis _",
    "@xcite projects the data into a kernel space in which the principal component analysis is then performed . in this work",
    "the nonlinearity is induced by a radial basis functions kernel .    _",
    "traditional neural network - based autoencoders _ consist of two artificial neural networks stacked on top of each other .",
    "the encoder network is responsible for encoding the input data into some latent variables .",
    "the decoder network is used to decode these parameters in order to return an accurate recreation of the input data , shown in .",
    "the parameters of this algorithm are trained by performing gradient descent updates in order to minimize the reconstruction loss ( reconstruction error ) between input data and output data .",
    "_ variational autoencoders _ are a modern version of autoencoders which impose additional constraints on the encoded representations , see latent variables in .",
    "these constraints transform the autoencoder to an algorithm that learns a latent variable model for its input data . whereas the neural networks of traditional autoencoders learn an arbitrary function to encode and decode the input data , variational autoencoders learn the parameters of a probability distribution modeling the data . after learning the probability distribution , one can sample parameters from it and then let the encoder network generate samples closely resembling the training data .    to achieve this",
    ", variational autoencoders employ the assumption that one can sample the input data from a unit gaussian distribution of latent parameters .",
    "the weights of the model are trained by simultaneously optimizing two loss functions , a reconstruction loss and the kullback - leibler divergence between the learned latent distribution and a prior unit gaussian .    in this work",
    "we use autoencoders and variational autoencoders @xcite with one fully connected hidden layer in the encoder as well as one fully connected hidden layer in the decoder , each consisting of 256 neurons .",
    "the number of latent variables is chosen to match the model from which we sample the input data .",
    "the activation functions of the intermediate layers are rectified linear units .",
    "the activation function of the final layer is a _ sigmoid _ in order to predict probabilities of spin @xmath38 or @xmath39 in the ising model , or _",
    "tanh _ for predicting continuous values of spin components in the xy model .",
    "we do not employ any @xmath40 or dropout regularization .",
    "however , we tune the relative weight of the two loss functions of the variational autoencoder to fit the problem at hand .",
    "the kullback - leibler divergence of the variational autoencoder can be regarded as reguarization of the traditional autoencoder . in our autoencoder",
    "the reconstruction loss is the cross - entropy loss between the input and output probability of discrete spins , as in the ising model .",
    "the reconstruction loss is the mean - squared - error between the input and the output data of continuous spin variables in the xy model .    to understand why a variational autoencoder can be a suitable choice for the task of classifying phases , we recall what happens during training .",
    "the weights of the autoencoder learn two things : on the one hand , they learn to encode the similarities of all samples to allow for an efficient reconstruction . on the other hand , they learn a latent distribution of the parameters which encode the most information possible to distinguish between different input samples .",
    "let us translate these considerations to the physics of phase transitions .",
    "if all the training samples are in the unordered phase , the autoencoder learns the common structure of all samples .",
    "the autoencoder fails to learn any random entropy fluctuations , which are averaged out over all data points .",
    "however , in the ordered phase there exists a common order in samples belonging into the same phase .",
    "this common order translates to a nonzero latent parameter , which encodes correlations on each input sample .",
    "it turns out that in our cases this parameter is the order parameter corresponding to the broken symmetry .",
    "it is not necessary to find a perfect linear transformation between the order parameter and the latent parameter as it is the case in .",
    "a one - to - one correspondence is sufficient , such that one is able to define a function that maps these parameters onto each other and captures all discontinuities of the derivatives of the order parameter .",
    "we point out similarities between principal component analysis and autoencoders .",
    "although both methods seem very different , they both share common characteristics .",
    "principal component analysis is a dimensionality reduction method which finds the linear projections of the data that maximizes the variance .",
    "reconstructing the input data from its principal components minimizes the mean squared reconstruction error .",
    "although the principal components do not need to follow a gaussian distribution , principal components have the highest mutual agreement with the data if it emerges from a gaussian prior .",
    "moreover , a single layer autoencoder with linear activation functions closely resembles principal component analysis @xcite .",
    "principal component analysis is much easier to apply and in general uses less parameters than autoencoders .",
    "however , it scales very badly to a large dataset .",
    "autoencoders based on convolutional layers can reduce the number of parameters . in extreme cases",
    "this number can be even less than the parameters of principal component analysis .",
    "furthermore , such autoencoders can promote locality of features in the data .",
    "+     +     +     +     +     pixels , from the latent parameter .",
    "the brightness indicates the probability of the spin to be up ( white : @xmath41 , black : @xmath42 ) . the first row is a reconstruction of sample configurations from the ferromagnetic ising model .",
    "the second row corresponds to the antiferromagnetic ising model .",
    "the third row is the prediction from the af latent parameter , where each second spin is multiplied by @xmath43 , to show that the second row indeed predicts an antiferromagnetic state.,title=\"fig:\",scaledwidth=50.0% ] +     +",
    "the four different algorithms can be applied to the ising model to determine the role of the first principal components or the latent parameters .",
    "shows a clear correlation between these parameters and the magnetization for all four methods .",
    "however , the traditional autoencoder is inaccurate ; this fact leads us to enhancing traditional autoencoders to variational autoencoders .",
    "the principal component methods show the most accurate results , slightly better than the variational autoencoder .",
    "this is to be expected , since the former are modeled by fewer parameters .    in the following results section , we concentrate on the variational autoencoder as the most advanced algorithm for unsupervised learning .    to begin with ,",
    "we choose the number of latent parameters in the variational autoencoder to be one . after training for 50 epochs and a saturation of the training loss ,",
    "we visualize the results in . on the left",
    ", we see a close linear correlation between the latent parameter and the magnetization . in the middle",
    "we see a histogram of encoded spin configurations into their latent parameter .",
    "the model learned to classify the configurations into three clusters .",
    "having identified the latent parameter to be a close approximation to the magnetization @xmath44 allows us to interpret the properties of the clusters .",
    "the right and left clusters in the middle image correspond to an average magnetization of @xmath45 , while the middle cluster corresponds to the magnetization @xmath46 . employing a different viewpoint , from we",
    "conclude that the parameter which holds the most information on how to distinguish ising spin samples is the order parameter . on the right panel , the average of the magnetization , the latent parameter and the reconstruction loss are shown as a function of the temperature .",
    "a sudden change in the magnetization at @xmath47 defines the phase transition between paramegnetism and ferromagnetism . even without knowing this order parameter",
    ", we can now use the results of the autoencoder to infer the position of the phase transition . as an approximate order parameter",
    ", the average absolute value of latent parameter also shows a steep change at @xmath20 .",
    "the averaged reconstruction loss also changes drastically at @xmath20 during a phase transition .",
    "while the latent parameter is different for each physical model , the reconstruction loss can be used as a universal parameter to identify phase transitions . to summarize , without any knowledge of the ising model and its order parameter , but sample configurations",
    ", we can find a good estimation for its order parameter and the occurrence of a phase transition .",
    "it is a priori not clear how to determine the number of latent neurons in the creation of the neural network of the autoencoder . due to the lack of theoretical groundwork ,",
    "we find the optimal number by experimenting .",
    "if we expand the number of latent dimensions by one , see , the results of our analysis only change slightly .",
    "the second parameter contains a lot less information compared to the first , since it stays very close to zero .",
    "hence , for the ising model , one parameter is sufficient to store most of the information of the latent representation .    while the ferromagnetic ising model serves as an ideal starting ground , in the next step we are interested in models where different sites in the samples contribute in a different manner to the order parameter .",
    "we do this in order to show that our model is even sensitive to structure on the smallest scales . for the magnetization in the ferromagnetic ising model ,",
    "all spins contribute with the same weight .",
    "in contrast , in the antiferromagnetic ising model , neighboring spins contribute with opposite weight to the order parameter .",
    "again the variational autoencoder manages to capture the traditional order parameter .",
    "the staggered magnetization is strongly correlated with the latent parameter , see .",
    "the three clusters in the latent representation make it possible to interpret different phases .",
    "furthermore , we see that all three averaged quantities - the magnetization , the latent parameter and the reconstruction loss - can serve as indicators of a phase transition .    demonstrates the reconstruction from the latent parameter . in the first row",
    "we see the reconstruction from samples of the ferromagnetic ising model , the latent parameter encodes the whole spin order in the ordered phase .",
    "reconstructions from the antiferromagnetic ising model are shown in the second and third row .",
    "since the reconstructions clearly show an antiferromagnetic phase , we infer that the autoencoder encodes the spin samples even to the most microscopic level .      in the xy model",
    "we examine the capabilities of a variational autoencoder to encode models with continuous symmetries . in models like the ising model , where discrete symmetries are present , the autoencoder only needs to learn a discrete set , which is often finite , of possible representations of the symmetry broken phase .",
    "if a continuous symmetry is broken , there are infinitely many possibilities of how the ordered phase can be realized .",
    "hence , in this section we test the ability of the autoencoder to embed all these different realizations into latent variables .",
    "the variational autoencoder handles this model equally well as the ising model .",
    "we find that two latent parameters model the phase transition best . the latent representation in the middle of shows the distribution of various states around a central cluster .",
    "the radial symmetry in this distribution leads to the assumption that a sensible order parameter is constructed from the @xmath33-norm of the latent parameter vector . in",
    ", one sees the correlation between the magnetization and the absolute value of the latent parameter vector . averaging the samples for the same temperature hints to the facts that the latent parameter and the reconstruction loss can serve as an indicator for the phase transition .",
    "we have shown that it is possible to observe phase transitions using unsupervised learning .",
    "we compared different unsupervised learning algorithms ranging from principal component analysis to variational autoencoders and thereby motivated the need for the upgrade of the traditional autoencoder to a variational autoencoder .",
    "the weights and latent parameters of the variational autoencoder approach are able to store information about microscopic and macroscopic properties of the underlying systems .",
    "the most distinguished latent parameters coincide with the known order parameters .",
    "furthermore , we have established the reconstruction loss as a new universal indicator for phase transitions . we have expanded the toolbox of unsupervised learning algorithms in physics by powerful methods , most notably the variational autoencoder , which can handle nonlinear features in the data and scale very well to huge datasets . using these techniques",
    ", we expect to predict unseen phases or uncover unknown order parameters , e.g. in quantum spin liquids .",
    "we hope to develop deep convolutional autoencoders which have a reduced number of parameters compared to fully connected autoencoders and can also promote locality in feature selection . furthermore , since there exists a connection between deep neural networks and renormalization group @xcite , it may be helpful to employ deep convolutional autoencoders to further expose this connection .    _ acknowledgments _ we would like to thank timo milbich , bjrn ommer , michael scherer , manuel scherzer and christof wetterich for useful discussions .",
    "we thank shirin nkongolo for proofreading the manuscript .",
    "s.w . acknowledges support by the heidelberg graduate school of fundamental physics .",
    "stefano curtarolo , dane morgan , kristin persson , john rodgers , and gerbrand ceder .",
    "predicting crystal structures with data mining of quantum calculations . _ physical review letters _ , 910 ( 13 ) ,",
    "doi : 10.1103/physrevlett.91.135503 .",
    "matthias rupp , alexandre tkatchenko , klaus - robert mller , and o.  anatole von lilienfeld .",
    "fast and accurate modeling of molecular atomization energies with machine learning .",
    "_ physical review letters _ , 1080 ( 5 ) , jan 2012 .",
    "doi : 10.1103/physrevlett.108.058301 .",
    "zhenwei li , james  r. kermode , and alessandro  de vita .",
    "molecular dynamics with on - the - fly machine learning of quantum - mechanical forces .",
    "_ physical review letters _",
    ", 1140 ( 9 ) , mar 2015 .",
    "doi : 10.1103/physrevlett.114.096405 .",
    "erin ledell , prabhat , dmitry  yu .",
    "zubarev , brian austin , and william  a. lester",
    ". classification of nodal pockets in many - electron wave functions via machine learning .",
    "_ journal of mathematical chemistry _ , 500 ( 7):0 20432050 , may 2012 .",
    "doi : 10.1007/s10910 - 012 - 0019 - 5 .",
    "g.  pilania , j.  e. gubernatis , and t.  lookman .",
    "structure classification and melting temperature prediction in octet ab solids via machine learning .",
    "_ physical review b _ , 910 ( 21 ) , jun 2015 .",
    "doi : 10.1103/physrevb.91.214302 .",
    "yousef saad , da  gao , thanh ngo , scotty bobbitt , james  r. chelikowsky , and wanda andreoni .",
    "data mining for materials : computational experiments withabcompounds . _",
    "physical review b _ , 850 ( 10 ) , mar 2012 .",
    "doi : 10.1103/physrevb.85.104104 .",
    "o.  s. ovchinnikov , s.  jesse , p.  bintacchit , s.  trolier - mckinstry , and s.  v. kalinin .",
    "disorder identification in hysteresis data : recognition analysis of the random - bondrandom - field ising model .",
    "_ physical review letters _ , 1030 ( 15 ) , oct 2009 .",
    "doi : 10.1103/physrevlett.103.157203 .",
    "louis - franois arsenault , alejandro lopez - bezanilla , o.  anatole von lilienfeld , and andrew  j. millis .",
    "machine learning for many - body physics : the case of the anderson impurity model . _",
    "physical review b _ , 900 ( 15 ) , oct 2014 .",
    "doi : 10.1103/physrevb.90.155136 .",
    "john  c. snyder , matthias rupp , katja hansen , klaus - robert mller , and kieron burke .",
    "finding density functionals with machine learning . _ physical review letters _ , 1080 ( 25 ) , jun 2012 .",
    "doi : 10.1103/physrevlett.108.253002 .",
    "geoffroy hautier , christopher  c. fischer , anubhav jain , tim mueller , and gerbrand ceder .",
    "finding nature s missing ternary oxide compounds using machine learning and density functional theory . _",
    "chemistry of materials _ , 220 ( 12):0 37623767 , jun 2010 .",
    "doi : 10.1021/cm100795d .",
    "pierre baldi and kurt hornik .",
    "neural networks and principal component analysis : learning from examples without local minima . _ neural networks _ , 20 ( 1):0 5358 , jan 1989 .",
    "doi : 10.1016/0893 - 6080(89)90014 - 2 .",
    "n.  d. mermin and h.  wagner .",
    "absence of ferromagnetism or antiferromagnetism in one- or two - dimensional isotropic heisenberg models .",
    "_ physical review letters _ , 170 ( 22):0 11331136 , nov 1966 .",
    "doi : 10.1103/physrevlett.17.1133 .",
    "aloysius  p. gottlob and martin hasenbusch .",
    "critical behaviour of the 3d xy - model : a monte carlo study .",
    "_ physica a : statistical mechanics and its applications _",
    ", 2010 ( 4):0 593613 , dec 1993 .",
    "doi : 10.1016/0378 - 4371(93)90131-m ."
  ],
  "abstract_text": [
    "<S> we employ unsupervised machine learning techniques to learn latent parameters which best describe states of the two - dimensional ising model and the three - dimensional xy model . </S>",
    "<S> these methods range from principal component analysis to artificial neural network based variational autoencoders . </S>",
    "<S> the states are sampled using a monte - carlo simulation above and below the critical temperature . </S>",
    "<S> we find that the predicted latent parameters correspond to the known order parameters . </S>",
    "<S> the latent representation of the states of the models in question are clustered , which makes it possible to identify phases without prior knowledge of their existence or the underlying hamiltonian . </S>",
    "<S> furthermore , we find that the reconstruction loss function can be used as a universal identifier for phase transitions . </S>"
  ]
}