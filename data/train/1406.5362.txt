{
  "article_text": [
    "it is a long lasting dream of humanity to build a machine that predicts the future . for long time intervals ,",
    "this is likely going to stay a dream .",
    "but for shorter time spans this is not such an unreasonable goal .",
    "humans , in fact , can predict rather reliably how , , a video will continue over the next few seconds , at least when nothing extraordinary happens . in this work we aim at making a first step towards giving computers similar abilities .",
    "we study the situation of a time - varying probability distribution from which sample sets at different time points are observed .",
    "our main result is a method for learning an operator that captures the dynamics of the time - varying data distribution .",
    "it relies on two recent techniques : the embedding of probability distributions into a reproducing kernel hilbert space , and vector - valued regression . by extrapolating the learned dynamics into the future",
    ", we obtain an estimate of the future distribution .",
    "this estimate can be used to solve practical tasks , for example , learn a classifier that is adapted to the data distribution at a future time step , without having access to data from this situation already .",
    "one can also use the estimate to create a new sample set , which then can serve as a drop - in replacement for an actual sample set from the future .",
    "we first formally define the problem setting of _ predicting the future of a time - varying probability distribution_. let @xmath0 be a data domain , and let @xmath1 for @xmath2 be a time - varying data distribution over @xmath3 . at a fixed point of time , @xmath4",
    ", we assume that we have access to sequences of sets , @xmath5 , for @xmath6 , that are sampled from the respective distributions , @xmath7 .",
    "our goal is to construct a distribution , @xmath8 , that is as close as possible to the so far unobserved @xmath9 , it provides an estimate of the data distribution one step into the future .",
    "optionally , we are also interested in obtaining a set , @xmath10 , of samples that are distributed approximated according to the unknown @xmath9 .",
    "our main contribution is a regression - based method that tackles the above problem for the case when the distribution @xmath11 evolves smoothly ( sections  [ sec : edd ] and  [ sec : extension ] ) .",
    "we evaluate this method experimentally in section  [ sec : experiments ] .",
    "subsequently , we show how the ability to extrapolate the distribution dynamics can be exploited to improve the accuracy of a classifier in a domain adaptation setting without observed data from the test time distribution ( section  [ sec : pda ] ) .",
    "we propose a method for _ extrapolating the distribution dynamics ( edd ) _ that consist of four steps :    1 .",
    "represent each sample set as a vector in a hilbert space , 2 .",
    "learn an operator that reflects the dynamics between the vectors , 3 .",
    "apply the operator to the last vector in the sequence , thereby extrapolating the dynamics by one step , 4 .",
    "( optionally ) create a new sample set for the extrapolated distribution .    in the following",
    "we discuss the details of each step .",
    "see figure  [ fig : schematic ] for a schematic illustration .",
    "[ [ a - rkhs - embedding . ] ] a ) rkhs embedding .",
    "+ + + + + + + + + + + + + + + + + +    in order to allow the handling of arbitrary real data , we would like to avoid making any domain - specific assumptions , such as that the samples correspond to objects in a video , or parametric assumptions , such as gaussianity of the underlying distributions .",
    "we achieve this by working in the framework of _ reproducing kernel hilbert space ( rkhs ) embeddings of probability distributions _  @xcite . in this section",
    "we provide the most important definitions ; for a comprehensive introduction see  @xcite .",
    "let @xmath12 denote the set of all probability distributions on @xmath0 with respect to some underlying @xmath13-algebra .",
    "let @xmath14 be a positive definite kernel function with induced rkhs @xmath15 and feature map @xmath16 that fulfills @xmath17 for all @xmath3 .",
    "the _ kernel mean embedding _",
    ", @xmath18 , associated with @xmath19 is defined by @xmath20    since we assume @xmath19 ( and therefore @xmath15 ) fixed in this work , we also refer to @xmath21 as `` the '' rkhs embedding of @xmath22 .",
    "we denote by @xmath23 the image of @xmath12 under @xmath24 , the set of vectors that correspond to embedded probability distributions . for _ characteristic _ kernels , such as the gaussian ,",
    "the kernel mean map is an _ bijection _ between @xmath12 and @xmath23 , so no information is lost by the embedding operation  @xcite . in the rest of this section , we will use the term distribution to refer to objects either in @xmath12 or in @xmath25 , when it is clear from the context which ones we mean .",
    "a useful property of the kernel mean map is that it allows us to express the operation of taking expected values by an inner product using the identity @xmath26 for any @xmath27 and @xmath28 .    for a set @xmath29 of samples from @xmath22",
    ", @xmath30 is called the _ empirical ( kernel mean ) embedding _ of @xmath31 .",
    "it is known that under mild conditions on @xmath15 , the empirical embedding , @xmath32 , converges with high probability to the true embedding , @xmath21 , at a rate of @xmath33  @xcite .",
    "the first step of edd consists of forming the embeddings , @xmath34 of the observed sample sets , @xmath35 .",
    "note that for many interesting kernels the vectors @xmath36 can not be computed explicitly , because the kernel feature map , @xmath37 , is unknown or would require infinite memory to be represented .",
    "however , as we will see later and as it is typical for kernel methods  @xcite , explicit knowledge of the embedding vectors is also not required .",
    "it is sufficient that we are able to compute their inner products with other vectors , and this can be done via evaluations of the kernel function .",
    "[ [ b - learning - the - dynamics . ] ] b ) learning the dynamics .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + +    we use _ vector - valued regression _  @xcite to learn a model of the process how the ( embedded ) distribution evolves from one time step to the next .",
    "vector - valued regression generalizes classical scalar - valued regression to the situation , in which the inputs and outputs are vectors , the learning of an operator .",
    "again , we start by providing a summary of this technique , here following the description in  @xcite .    as basis set in which we search for a suitable operator , we define a space , @xmath38 , of linear operators on @xmath15 in the following way .",
    "let @xmath39 be the space of all bounded linear operators from @xmath15 to @xmath15 , and let @xmath40 be the _ nonnegative @xmath39-valued kernel _",
    "defined by @xmath41 for any @xmath42 , where @xmath43 is the identity operator on @xmath15 .",
    "then @xmath44 can be shown to be the reproducing kernel of an operator - valued rkhs , @xmath45 , which contains at least the span of all rank-1 operators , @xmath46 , for all @xmath42 , with @xmath47 .",
    "the inner product between such operators is @xmath48 for any @xmath49 , and the inner product of all other operators in @xmath38 can be derived from this by linearity and completeness .    as second step of eddwe",
    "solve a vector - valued regression in order to learn a predictive model of the dynamics of the distribution .",
    "for this we assume that the changes of the distributions between time steps can be approximated by an autoregressive process , @xmath50 , for some operator @xmath51 , such that the @xmath52 for @xmath6 are independent zero - mean random variables . to learn the operator we solve the following least - squares functional with regularization constant @xmath53 : @xmath54 with coefficient matrix @xmath55 , where @xmath56 is the kernel matrix with entries",
    "@xmath57 , and @xmath58 is the identity matrix of the same size , see  @xcite for the derivation .",
    "recently , it has been shown that the above regression on distributions is consistent under certain technical conditions  @xcite . consequently , if @xmath59 , then the estimated operator , @xmath60 , will converge to the true operator , @xmath61 , when the number of sample sets and the number of samples per set tend to infinity .",
    "[ [ c - extrapolating - the - evolution . ] ] c ) extrapolating the evolution .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the third step of edd is to extrapolate the dynamics of the distribution by one time step . with the results of a ) and b ) , all necessary components for this are available : we simply apply the learned operator , @xmath60 to the last observed distribution @xmath62 .",
    "the result is a prediction , @xmath63 , that approximates the unknown target , @xmath64 . from equation   we see that @xmath65 can be written as a weighted linear combination of the observed distributions , @xmath66 for @xmath67 .",
    "the coefficients , @xmath68 , can be computed from the original sample sets by means of only kernel evaluations , because @xmath69 .",
    "the values of @xmath68 can be positive or negative , so @xmath65 is not just an interpolation between previous values , but potentially an extrapolation .",
    "in particular , it can lie outside of the convex hull of the observed distributions .",
    "at the same time , the estimate @xmath70 is guaranteed to lie in the subspace spanned by @xmath71 , for which we have sample sets available .",
    "therefore , so we can compute expected values with respect to @xmath65 by forming a suitably weighted linear combinations of the target function at the original data points . for any @xmath28",
    ", we have @xmath72 where the last identity is due to the fact that @xmath15 is the rkhs of @xmath19 , which has @xmath37 is its feature map , so @xmath73 for all @xmath3 and @xmath28 .",
    "we use the symbol @xmath74 instead of @xmath75 to indicate that @xmath76 does not necessarily correspond to the operation of computing an expected value , because @xmath65 might not have a pre - image in the space of probability distributions .",
    "the following lemma shows that @xmath70 can , nevertheless , act as a reliable proxy for @xmath64 :    [ lem : errorbound ] let @xmath77 and @xmath78 , for some @xmath79 , @xmath80 and @xmath81 .",
    "then the following inequality holds for all @xmath28 with @xmath82 , @xmath83    the proof is elementary , using the properties of the inner product and of the rkhs embedding .",
    "lemma  [ lem : errorbound ] quantifies how well @xmath65 can serve as a drop - in replacement of @xmath64 .",
    "the introduced error will be small , if all three terms on the right hand side are small .",
    "for the first term , we know that this is the case when the number of samples in @xmath84 is large enough , since @xmath85 is a constant , and we know that the empirical distribution , @xmath62 , converges to the true distribution , @xmath86 . similarly , the second terms becomes small in the limit of many samples set and many samples per set , because we know that the estimated operator , @xmath60 , converges to the operator of the true dynamics , @xmath61 , in this case .",
    "consequently , eddwill provide a good estimate of the next distribution time step , given enough data and if our assumptions about the distribution evolution are fulfilled ( @xmath87 is small ) .",
    "[ [ d - generating - a - sample - set - by - herding . ] ] d ) generating a sample set by herding .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    equation   suggests a way for associating a set of weighted samples with @xmath65 : @xmath88 where @xmath89 indicates not multiplication but that the sample @xmath90 appears with a weight @xmath91 . as we will show in section  [ sec : pda ] , this representation is sufficient for many purposes , in particular for learning a maximum - margin classifier . in other situations ,",
    "however , one might prefer a representation of @xmath65 by uniformly weighted samples , a set @xmath92 such that @xmath93 . to obtain such a set we propose using the rkhs variant of _ herding _",
    "@xcite , a deterministic procedure for approximating a probability distribution by a set of samples . for any embedded distribution , @xmath94 ,",
    "herding constructs a sequence of samples , @xmath95 by the following rules , @xmath96 herding can be understood as an iterative greedy optimization procedure for finding examples @xmath97 that minimize @xmath98  @xcite .",
    "this interpretation shows that the target vector , @xmath99 , is not restricted to be an embedded distribution , so herding can be applied to arbitrary vectors in @xmath15 . doing so for @xmath65 yields a set @xmath100 that can act as a drop - in replacement for an actual training set @xmath101 . however , it depends on the concrete task whether it is possible to compute @xmath102 in practice , because it requires solving multiple pre - image problems  , which is not always computationally tractable .",
    "a second interesting aspect of herding is that for any @xmath103 , the herding approximation always has a pre - image in @xmath12 ( the empirical distribution defined by @xmath97 ) .",
    "therefore , herding can also be interpreted as an approximate projection from @xmath15 to @xmath23 .    in algorithm  [ alg : edd ]",
    "we provide pseudo - code for edd .",
    "it also shows that despite its mathematical derivation , the actual algorithms is easy to implement and execute .",
    "kernel function @xmath14 sets @xmath104 for @xmath6 regularization parameter @xmath53 @xmath105 @xmath106-matrix with entries    @xmath107    @xmath108 @xmath109-vector with entries    @xmath110    @xmath111    weighted sample set    @xmath112    ' '' ''    optional herding step :    output size @xmath113 @xmath114    @xmath115 $ ] sample set @xmath116      our above description of edd , in particular equation  , treats all given samples sets as equally important . in practice",
    ", this might not be desirable , and one might want to put more emphasis on some terms in the regression than on others .",
    "this effect can be achieved by introducing a weight , @xmath117 , for each of the summands of the least - squares problems  .",
    "typical choices are @xmath118 , for a constant @xmath119 , which expresses a belief that more recent observations are more trustworthy than earlier ones , or @xmath120 , which encodes that the mean embedding of a sample set is more reliable if the set contains more samples .    as in ordinary least squares regression ,",
    "per - term weights impact the coefficient matrix @xmath121 , and thereby the concrete expressions for @xmath68 .",
    "however , they do not change the overall structure of @xmath70 as a weighted combination of the observed data , so herding and predsvm ( see section  [ sec : pda ] ) training remain possible without structural modifications .",
    "to our knowledge , the problem of extrapolating a time - varying probability distribution from a set of samples has not been studied in the literature before . however",
    ", a large of body work exists that studies related problems or uses related techniques .",
    "the prediction of future states of a dynamical system or time - variant probability distribution is a classical application of probabilistic state space models , such as _ kalman filters _",
    "@xcite , and _ particle filters _  @xcite .",
    "these techniques aim at modeling the probability of a time - dependent system jointly over all time steps .",
    "this requires observed data in the form of time series , trajectories of moving particles .",
    "edd , on the other hand , learns only the transitions between the marginal distribution at one point of time to the marginal distribution at the next point of time . for this , independent sample sets from different time points are sufficient .",
    "the difference between both approaches become apparent , , by looking at a system of homogeneously distributed particles that rotate around a center . a joint model",
    "would learn the circular orbits , while edd would learn the identity map , since the data distributions are the same at any time .    in the literature of _ rkhs embeddings",
    "_ , a line of work related to edd is the learning of _ conditional distributions _ by means of covariance operators , which has also be interpreted as a vector - valued regression task  @xcite . given a current distribution and such a conditional model , one could infer the marginal distribution of the next time step  @xcite .",
    "again , the difference to edd lies in the nature of the modeled distribution and the training data required for this . to learn conditional distributions",
    ", the training data must consist of pairs of data points at two subsequent time points ( essentially a minimal trajectory ) , while in the scenario we consider correspondences between samples at different time points are not available and often would not even make sense .",
    "for example , in section  [ sec : experiments ] we apply edd to images of car models from different decades .",
    "correspondence between the actual cars depicted in such images do not exist .",
    "a different line of work aims at predicting the future motion of specific objects , such as people or cars , from videos  @xcite .",
    "these are model - based approaches that target specifically the situation of learning trajectories of objects in videos . as such",
    ", they can make precise predictions about possible locations of objects at future times .",
    "they are not applicable to the generic situation we are interested , however , in which the given data are separate sample sets and the goal is to predict the future behavior of the underlying probability distribution , not of individual objects .",
    "we report on experiments on synthetic and real data in order to highlight the working methodology of edd , and to show that extrapolating the distribution dynamics is possible for real data and useful for practical tasks .",
    "[ [ experiment-1-synthetic - data . ] ] experiment 1 : synthetic data .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    first , we perform experiments on synthetic data for which we know the true data distribution and dynamics , in order to highlight the working methodology of edd . in each case",
    ", we use sample sets of size @xmath122 and we use a regularization constant of @xmath123 .",
    "where possible we additionally analytically look at the limit case @xmath124 , @xmath125 . for the rkhs embedding we use a gaussian kernel with unit variance .",
    "first , we set @xmath126 , a mixture of gaussians distribution with mixture coefficients that vary over time as @xmath127 .",
    "figure  [ fig : gaussians ] ( top ) illustrates the results : trained on the first six samples sets ( blue lines ) , the prediction by edd ( orange ) match almost perfectly the seventh ( dashed ) , with or without herding . in order to interpret this result",
    ", we first observe that due the form of the distributions @xmath11 it is not surprising that @xmath64 could be expressed as linear combination of the @xmath128 , provided we allow for negative coefficients . what the result shows , however , is that edd is indeed able to find the right coefficients from the sample sets , indicating that the use of an autoregressive model is justified in this case",
    ".    predicting the next time step of a distribution can be expected to be harder if not only the values of the density change between time steps but also the support .",
    "we test this by setting @xmath129 , a gaussian with shifting location of the mean , which we call the _ translation _ setting .",
    "figure  [ fig : gaussians ] ( bottom left ) illustrates the last three steps of the total nine observed steps of the dynamics ( blue ) and its extrapolation ( orange ) with and without herding .",
    "one can see that edd indeed is able to extrapolate the distribution to a new region of the input space : the mode of the predicted @xmath65 and @xmath130 lie right of the mode of all inputs .",
    "however , the prediction quality is not as good as in the mixture setting , indicating that this is in fact a harder task .",
    "finally , we study a situation where it is not clear on first sight whether the underlying dynamics has a linear model : a sequence of gaussians with decreasing variances , @xmath131 , which we call the _ concentration _ setting .",
    "the last three steps of the nine observed steps of the dynamics and its extrapolation with and without herding are illustrated in figure  [ fig : gaussians ] ( bottom right ) in blue and orange , respectively .",
    "one can see that despite the likely nonlinearity , edd is able to predict a distribution that is more concentrated ( has lower variance ) than any of the inputs . in this case , we also observe that the predicted distribution function has negative values , and that herding removes those .    as a quantitative evaluation we report in tables  [ tab : resultshs ] and  [ tab : resultskl ] how well the predicted distributions correspond to the ground truth ones as measured by the hilbert space ( hs ) distance and the kullback - leibler divergences , respectively .",
    "the latter is only possible for edd after herding , when the prediction is a proper probability distribution ( non - negative and normalized ) . besides edd",
    ", we include the baseline of reusing the last observed sample set as a proxy for the next one . to quantify how much of the observed distance is due to the prediction step and",
    "how much is due to an unavoidable sampling error , we also report the values for a sample set @xmath101 of the same size from the true distribution @xmath9 .",
    "the results confirm that , given sufficiently many samples of the earlier tasks , edd is indeed able to successfully predict the dynamics of the distribution . the predicted distribution @xmath65 is closer to the true distribution @xmath64 than the most similar observed distribution , @xmath62 . for _ translation _ and _ concentration _ ,",
    "the analytic results show that even for @xmath124 the difference is non - zero , suggesting that the true dynamics are not exactly linear in the rkhs .",
    "however , the residual is small compared to the measured quantities .    [ [ experiment-2-real - world - data . ] ] experiment 2 : real world data . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in a second set of experiments , we test edd s suitability for real data by applying to video sequences from  @xcite .",
    "the dataset consists of 1121 video sequences of six semantic categories , _ birthday , parade , picnic , show , sports _ , and _ wedding _ , from two sources , _",
    "kodak _ and _ youtube_. each video is represented by a collection of spatio - temporal interest points ( stips ) with 162-dimensional feature vectors .    for each video , except six that were less than one second long",
    ", we split the stips into groups by creating segments of 10 frames each .",
    "different segments have different numbers of samples , because the stips were detected based on the response of an interest operator .",
    "different video also show a strong diversity in this characteristics : the number of per stips per segment varies between 1 and 550 , and the number of segments per video varies between 3 and 837 .    as experimental setup",
    ", we use all segments of a movie except the last one as input sets for edd , and we measure the distance between the predicted next distribution and the actual last segment .",
    "table  [ tab : video ] shows the results split by data source and category for two choices of kernels : the _ rbf-@xmath132 kernel _ , @xmath133 for @xmath134 , and the _ histogram intersection kernel _ , @xmath135 , both for @xmath136 .",
    "for each data source and category we report the average and standard error of the hilbert - space distance between distribution . as baselines",
    ", we compare against re - using the last observed segment , not extrapolating , and against the distribution obtained from merging all segments , the global video distribution .",
    "one can see that the predictions by edd are closer to the true evolution of the videos than both baselines in all cases but two , in which it is tied with using the last observation .",
    "the improvement is statistically significant ( bold print ) to a 0.05 level according to wilcoxon signed rank test with multi - test correction , except for some cases with only few sequences .",
    "we are convinced that being able to extrapolate a time - varying probability distribution into the future will be useful for numerous practical applications . as an example",
    ", we look at one specific problem : learning a classifier under distribution drift , when for training the classifier data from the time steps @xmath6 is available , but by the time the classifier is applied to its target data , the distribution has moved on to time @xmath137 . a natural choice to tackle",
    "this situation would be by using _",
    "domain adaptation _",
    "techniques  @xcite . however , those typically require that at least unlabeled data from the target distribution is available , which in practice might not be the case .",
    "for example , in an online prediction setting , such as spam filtering , predictions need to be made on the fly .",
    "one can not simply stop , collect data from the new data distribution , and retrain the classifiers .",
    "instead , we show how eddcan be used to train a maximum margin classifier for data distributed according to @xmath9 , with only data from @xmath138 to @xmath139 available .",
    "we call this setup _ predictive domain adaptation ( pda)_.    let @xmath140 for @xmath6 , be a sequence of labeled training sets , where @xmath141 is the input space , images , and @xmath142 is the set of class labels . for any kernel @xmath143 on @xmath141 , we form a joint kernel , @xmath144 on @xmath145 and we apply edd for @xmath146 . the result is an estimate of the next time step of the joint probability distribution , @xmath147 , as a vector , @xmath65 , or in form of a weighted sample set , @xmath148 .    to see how this allows us to learn a better adapted classifier , we first look at the situation of classification with 0/1-loss function , @xmath149 . if a correctly distributed training @xmath101 of size @xmath150 were available , one would aim for minimizing the regularized risk functional @xmath151 where @xmath152 is a regularization parameter and @xmath153 is any feature map , not necessarily the one induced by @xmath154 . to do so numerically , one would bound the loss by a convex surrogate , such as the hinge loss , @xmath155 , which make the overall optimization problem convex and therefore efficiently solvable .    in the pda situation",
    ", we do not have a training set @xmath101 , but we do have a prediction @xmath148 provided by edd in the form of equation  .",
    "therefore , instead of the empirical average in  , we can form a predicted empirical average using the weighted samples in @xmath148 .",
    "this leads to the _ predicted regularized risk functional _ , @xmath156 that we would like to minimize .    [",
    "cols=\"^,^,^ \" , ]",
    "in this work , we have introduced the task of predicting the future evolution of a time - varying probability distribution .",
    "we described a method that , given a sequence of observed samples set , extrapolates the distribution dynamics by one step .",
    "its main components are two recent techniques from machine learning : the embeddings of probability distributions into a hilbert space , and vector - valued regression .",
    "furthermore , we showed how the predicted distribution obtained from the extrapolation can be used to learn a classifier for a data distribution from which no training examples is available , not even unlabeled ones .",
    "our experiments on synthetic and real data gave insight into the working methodology of edd and showed that it is to some extend possible to predict the next state of a time - varying distribution from sample sets of earlier time steps , and that this can be useful for learning better classifiers .",
    "one shortcoming of our method is that currently it is restricted to equally spaced time steps and that the extrapolation is only by a single time unit .",
    "we plan to extend our framework to more flexible situations , including distributions with a continuous time parameterization ."
  ],
  "abstract_text": [
    "<S> we study the problem of predicting the future , though only in the probabilistic sense of estimating a future state of a time - varying probability distribution . </S>",
    "<S> this is not only an interesting academic problem , but solving this extrapolation problem also has many practical application , for training classifiers that have to operate under time - varying conditions .    </S>",
    "<S> our main contribution is a method for predicting the next step of the time - varing distribution from a given sequence of sample sets from earlier time steps . for this </S>",
    "<S> we rely on two recent machine learning techniques : embedding probability distributions into a reproducing kernel hilbert space , and learning operators by vector - valued regression .    </S>",
    "<S> we illustrate the working principles and the practical usefulness of our method by experiments on synthetic and real data . </S>",
    "<S> we also highlight an exemplary application : training a classifier in a domain adaptation setting without having access to examples from the test time distribution at training time . </S>"
  ]
}