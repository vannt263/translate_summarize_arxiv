{
  "article_text": [
    "a sparse system is defined when impulse response contains only a small fraction of large coefficients compared to its ambient dimension .",
    "sparse systems widely exist in many applications , such as digital tv transmission channel @xcite and the echo path @xcite .",
    "generally , they can be further classified into two categories : exact sparse system ( ess ) and near sparse system ( nss ) .",
    "if most coefficients of the impulse response are exactly zero , it is defined as an exact sparse system ( fig .",
    "[ sparsec ] .",
    "a ) ; instead , if most of the coefficients are close ( not equal ) to zero , it is a near sparse system ( fig .  [ sparsec ] .",
    "b and c ) . otherwise , a system is non - sparse if its most taps have large values ( fig .",
    "[ sparsec ] .",
    "d ) . for the simplicity of theoretical analysis ,",
    "sparse systems are usually simplified into exact sparse . however , in real applications most systems are near sparse due to the ineradicable white noise .",
    "therefore , it is necessary to investigate on near sparse system modeling and identification .    among many adaptive filtering algorithms for system identification , least mean square ( lms ) algorithm @xcite , which was proposed by widrow and hoff in 60s of the past century , is the most attractive one for its simplicity , robustness and low computation cost .",
    "however , without utilizing the sparse characteristic , it shows no advantage on sparse system identification . in the past few decades",
    ", some modified lms algorithms for sparse systems are proposed .",
    "m - max normalized lms ( mmax - nlms ) @xcite and sequential partial update lms ( s - lms ) @xcite reduces the computational complexity and steady - state misalignment by partially updating the filter coefficients . proportionate lms ( plms ) @xcite and its improved ones such as ipnlms@xcite and iipnlms@xcite accelerate the convergence rate by updating each coefficient iteratively with different step size proportional to the magnitude of filter coefficient .",
    "stochastic tap - normalized lms ( st - nlms ) @xcite improves the performance on specific sparse system identification where large coefficients appear in clusters .",
    "it locates and tracks the non - zero coefficients by adjusting the filter length dynamically . however , its convergence performance largely depends on the span of clusters .",
    "if the span is too long or the system has multiple clusters , it shows no advantage compared with standard lms algorithm .     and @xmath1 respectively .",
    "( d ) is a non - sparse system generated by gaussian distribution .",
    ", width=384 ]    more recently , inspired by the research of cs reconstruction problem @xcite , a class of novel adaptive algorithms for sparse system identification have emerged based on the @xmath2 ( @xmath3 ) norm constraint @xcite .",
    "especially , zero - point attraction lms ( za - lms ) algorithm @xcite significantly improves the performance on exact sparse system identification by introducing a @xmath0 norm constraint on the cost function of standard lms , which exerts the same zero - point attraction force on all coefficients .",
    "however , for near sparse systems identification , the zero - point attractor can be a double - edged sword .",
    "though it increases the convergence rate because by the @xmath0 norm constraint , it also produces larger steady - state misalignment as it forces all coefficients to exact zero .",
    "thus , it possesses less advantage against standard lms algorithm when the system is near sparse . in this paper",
    ", firstly generalized gaussian distribution ( ggd)@xcite is introduced to model the near sparse system .",
    "then two improvements on the za - lms algorithm is proposed .",
    "above all , by adding a window on the @xmath0 norm constraint , the steady - state misalignment is reduced without increasing the computational complexity .",
    "furthermore , the zero - point attractor is weighted to adjust the zero - point attraction by utilizing the estimation error . by combining the two improvements , the dynamic windowing za - lms ( dwza - lms ) algorithm is proposed which shows improved performance on the near sparse system identification .",
    "the rest of the paper is organized as follows : in section ii , za - lms algorithm based on @xmath0 norm constraint is reviewed , and the near sparse system is modeled .",
    "the new algorithm is proposed in section iii . in section iv",
    ", the mean square convergence performance of dwza - lms is analyzed .",
    "the performances of the new algorithm and other improved lms algorithms for sparse system identification are compared by simulation in section v , where the effectiveness of our analysis is verified as well .",
    "finally , section vi concludes the paper .",
    "let @xmath4 be a sample of the desired output signal @xmath5 where @xmath6^{\\rm t}$ ] is the unknown system with memory length @xmath7 , @xmath8^{\\rm t}$ ] denotes the input vector , and @xmath9 is the observation noise assumed to be independent of @xmath10 .",
    "the estimation error @xmath11 between desired and output signal is defined as @xmath12 where @xmath13 are the filter coefficients and @xmath14^{\\rm t}$ ] .",
    "thus the cost function of za - lms is @xmath15 where @xmath16 denotes the @xmath0 norm of the filter coefficients .",
    "parameter @xmath17 is the factor balancing the new penalty and the estimation error . by minimizing ( [ za cost function ] )",
    ", the za - lms algorithm updates its coefficients by @xmath18,\\label{2}\\ ] ] where @xmath19 is the step size , @xmath20 is the zero - point attraction controller , and sgn[@xmath21 is the component - wise sign function .    by observing ( [ 2 ] )",
    ", the recursion of filter coefficients for sparse system can be summarized as @xmath22 where @xmath23 denotes the gradient correction function , and @xmath24 stands for zero - point attractor which is caused by the @xmath0 norm penalty .",
    "for each iteration , the zero - point attractor forces the filter taps to decrease a little when it is positive , or otherwise to increase a little when it is negative .",
    "exact sparse system is appropriate for theoretical analysis , however , most physical systems are near sparse with widely existing white noise in real life , thus their modeling is of more significant importance .",
    "generalized gaussian distribution ( ggd ) is one of the most prominent and widely used sparse distributions .",
    "for example , in multimedia communications such as image and speech coding , ggd is usually found to best fit the coefficients of the discrete sine and cosine transforms , the walsh - hadamard transform , and the wavelet transform @xcite . in ultra wide bandwidth",
    "( uwb ) systems , it has recently been found to fit the multiuser interference better @xcite .",
    "these findings lead to applications of ggd in video coding , speech recognition , blind signal separation and uwb receiver design @xcite .",
    "therefore ggd is utilized to model the near sparse system in this study .",
    "it is a class of symmetry distribution with the gaussian and laplacian distribution as the special cases , with delta and uniformity distribution as limit .",
    "the probability density function of ggd is    @xmath25,\\label{4}\\ ] ]    where @xmath26 , @xmath27 denotes the gamma function , @xmath19 and @xmath28 are called the mean and variance of ggd , respectively .",
    "besides , @xmath29 determines the decay rate of the density function and can be used to denote the sparsity of the system ( see as fig .",
    "[ sparsec ] .",
    "b , c ; please notice that the system sparsity decreases as @xmath29 increases ) . especially , ggd is gaussian distribution when @xmath30 , and it turns into laplacian distribution when @xmath31 . by integrating @xmath32 ,",
    "the distribution function is @xmath33}{2\\gamma(1/\\beta)}.\\label{5}\\ ] ] where @xmath34 is called the lower incomplete function .",
    "the za - lms algorithm improves the performance by exerting an @xmath0 norm penalty forcing small coefficients to zero iteratively .",
    "it is very effective for the exact sparse system identification .",
    "however , for the near sparse system with small white noise on all coefficients , most of the small coefficients are forced to zero .",
    "thus za - lms usually degrades with large steady - state misalignment , showing no improvement compared with standard lms .",
    "the situation is more clearly illustrated in fig .",
    "[ drawback ] , where the exact sparse sparse system is generated with 8 nonzero large coefficients with tap length of 100 , and the near sparse system is the same with the former except that a power of @xmath35 white noise is added on all coefficients .",
    "the step sizes for all algorithms are set as @xmath36 .",
    "the optimal parameter @xmath37 is derived theoretically from ( 36 ) of @xcite by minimizing the steady - state msd .        from fig .",
    "[ drawback ] , it can be seen that by choosing the theoretically optimal parameter @xmath37 the performance of za - lms is much better than standard lms for exact sparse system .",
    "however , the performance of za - lms severely degrades on near sparse system identification . as stated above",
    ", the main reason is that the strong zero - point attraction forces near zero coefficients to zero that results in the large steady - state misalignment .",
    "though empirically decreasing @xmath37 may alleviate this problem , in that case , za - lms will degrade to standard lms showing no improvement , as shown in fig .",
    "[ drawback ] .",
    "thus to improve the performance of near sparse system identification , a new za - lms based algorithm is put forward , the main modifications are as follows .",
    "[ zaterm ]    ( 90,44 ) ( 0,24)(1,0)40 ( 20,4)(0,1)40 ( 40,22)@xmath38 ( 17,21)@xmath39 ( 22,42)@xmath40 ( 18.5,0)(a ) ( 20,14)(1,0)18 ( 20,34)(-1,0)18 ( 20,14)(0,1)20 ( 45,24)(1,0)40 ( 65,4)(0,1)40 ( 85,22)@xmath38 ( 67,42)@xmath41 ( 62,21)@xmath39 ( 66,24.5)@xmath42 ( 74,24.5)@xmath43 ( 63.5,0)(b ) ( 66,14)(1,0)8 ( 66,14)(0,1)10 ( 74,14)(0,1)10 ( 64,34)(-1,0)8 ( 64,34)(0,-1)10 ( 56,34)(0,-1)10      as the attracting range of za - lms algorithm reaches infinity , all coefficients of the sparse system are attracted to zero point .",
    "however , the identical attraction on both large and small ones will lead to increase the computational complexity and steady - state misalignment .",
    "thus , the first improvement lies in the constraint of zero - point attracting range . as shown in fig .",
    "3 , the new zero - point attraction , which attracts the coefficients only in a certain range , is proposed by adding a window on the original zero - point attracting .",
    "the new recursion of coefficients of the proposed windowing za - lms ( wza - lms ) is @xmath44,\\label{wzalms}\\ ] ] where @xmath45 $ ] is the component - wise partial sign function , defined as @xmath46=-f_{\\rm wza}(t)=\\left\\ { \\begin{array}{ll } { \\rm sgn}(t ) & \\mbox{$a<|t|\\le b$;}\\\\ 0 & \\mbox{elsewhere.}\\label{new sgn } \\end{array } \\right.\\ ] ] where @xmath42 and @xmath43 are both positive constant which denotes the lower and upper threshold of the attraction range , respectively .    from fig .",
    "3 , it can be concluded that za - lms is the special case of wza - lms when @xmath42 reaches 0 and @xmath43 approaches infinity , respectively .",
    "besides , by investigating ( [ 2 ] ) , ( [ wzalms ] ) and ( [ new sgn ] ) , it can be seen that the computational complexity of the two algorithms is approximately the same . and by adopting the new zero - point attractor and properly setting the threshold , the coefficients , whether too small or too large , will not be attracted any more .",
    "thus , the steady - state misalignment is significant reduced especially for near sparse system .",
    "as mentioned above , the sparse constraint should be relaxed in order to reduce the steady - state misalignment when the updating procedure reaches the steady - state .",
    "inspired by the idea of variable step size methods of standard lms algorithm@xcite , the magnitude of estimation error , which denotes the depth of convergence , is introduced here to adjust the force of zero - point attraction dynamically .",
    "that is , @xmath47 at the beginning of iterations , large estimation error increases zero - point attraction force which also accelerates the convergence .",
    "when the algorithm is approaching the steady - state , the error decreases to a minor value accordingly .",
    "thus the influence of zero - point attraction force on small coefficients is reduced that produce smaller steady - state misalignment . by implementation of this improvement on za - lms algorithm ,",
    "the algorithm is named as dynamic za - lms ( dza - lms ) .",
    "finally , by combining the two improvements , the final dynamic windowing za - lms ( dwza - lms ) algorithm can be drew .",
    "the new recursion of filter coefficients is as follows , @xmath48\\quad\\forall 0\\leq i < l.\\label{6}\\end{aligned}\\ ] ]    in addition , the new method can also improve the performance of za - nlms , which is known for its robustness .",
    "the recursion of dwza - nlms is @xmath49\\right\\ } \\quad\\forall 0\\leq i < l.\\label{7}\\ ] ] where @xmath50 is the regularization parameter .",
    "the mean square convergence analysis of dwza - lms algorithm is carried out in this section .",
    "the analysis is based on the following assumptions .    1 .",
    "the input signal @xmath10 is i.i.d zero - mean gaussian .",
    "the observation noise @xmath9 is zero - mean white .",
    "the tap - input vectors @xmath51 and the desired response @xmath4 follow the common independence assumption @xcite , which are generally used for performance analysis of lms algorithm .",
    "2 .   the unknown near sparse filter tap follows ggd .",
    "as stated in section ii , this assumption is made because ggd is the suitable sparse distribution for near sparse system modeling .",
    "the steady state adaptive filter tap @xmath52 follows the same distribution with @xmath53 ( @xmath54 ) .",
    "this is a reasonable assumption in that the error between the coefficients of the identified and the unknown real systems are very small when the algorithm converges .    under these assumptions , the mean square convergence condition and the steady - state mse of dwza - lms algorithm",
    "are derived .",
    "also , the choice of parameters is discussed in the end of this section .",
    "first of all , the misalignment vector is defined as @xmath55 and auto - covariance matrix of @xmath56 as @xmath57      combining ( [ 1 ] ) , ( [ 3 ] ) , ( [ 6 ] ) and ( [ 8 ] ) , one derives @xmath58 where @xmath59 , @xmath60 $ ] , and @xmath61 \\quad\\forall 0\\leq i < l.\\label{11}\\ ] ] by utilizing the independence assumption@xcite , and substituting ( [ 10 ] ) into ( [ 9 ] ) yields @xmath62 where @xmath63 is an @xmath64 unit matrix , @xmath65 and @xmath66 denote the power of input signal and observation noise , respectively . by utilizing the property",
    "that the fourth - order moment of a gaussian variable is three times the variance square , one obtains @xmath67 where @xmath68 $ ] . with ( [ 8 ] )",
    ", one has @xmath69 combining ( [ 12 ] ) , ( [ 13 ] ) and ( [ 14 ] ) , one derives @xmath70 by taking trace on both sides of ( [ 15 ] ) , it can be concluded that the adaptive filter is stable if and only if @xmath71 which is simplified to @xmath72    this implies that the proposed dwza - lms algorithm has the same stability condition for the mean square convergence as the za - lms and standard lms algorithm@xcite .      in this subsection ,",
    "the steady - state mean square error ( mse ) of dwza - lms algorithm is analyzed . by definition",
    ", mse is @xmath73 where @xmath74 , then ( [ 18 ] ) can be rewritten as @xmath75 thus , our work is to estimate @xmath76 . in ( [ 15 ] ) , let @xmath77 approach infinity , by observing the @xmath78th ( @xmath79 ) element @xmath80 of the matrix @xmath81 , one obtains @xmath82 with reference to ( [ 11 ] ) , it is obvious that @xmath83 to derive @xmath84 , by multiplying @xmath85 on the right of each item of ( [ 10 ] ) and taking the expectation value on both sides as well as letting @xmath77 approach infinity it yields @xmath86 thus , when @xmath87 or @xmath88 ( @xmath79 ) , it has @xmath89 when @xmath90 ( @xmath79 ) , it has @xmath91.\\label{24}\\ ] ]    according to assumption ( 3 ) , and combining ( [ 5 ] ) , we have @xmath92\\nonumber\\\\ & = & \\left\\{\\theta\\left[1/\\beta,(\\frac{|b|}{\\lambda})^\\beta\\right]-\\theta\\left[1/\\beta,(\\frac{|a|}{\\lambda})^\\beta\\right]\\right\\ } /\\gamma(1/\\beta),\\end{aligned}\\ ] ] where @xmath93 denotes the probability that the coefficients of adaptive filter will be attracted . on the other hand , the probability that they will not be attracted is @xmath94 . by combining ( [ 23 ] ) and ( [ 24 ] ) and summing up all the diagonal items of matrix @xmath95 , it yields @xmath96}{\\mu\\sigma_x^2\\left[2-\\mu\\sigma_x^2(l+2)\\right]},\\label{25}\\ ] ] combining ( [ 19 ] ) and ( [ 25 ] ) , finally one has @xmath97+l\\mu^2\\sigma_x^2\\sigma_v^2}{2\\mu-\\mu^2\\sigma_x^2(l+2)-\\rho^2{\\rm p}_al\\left({\\displaystyle\\frac{2}{\\mu\\sigma_x^2}}-1\\right)}.\\label{mse}\\ ] ] if @xmath98 , equation ( [ mse ] ) is the same with mse of standard lms algorithm @xcite , @xmath99      the performance of the proposed algorithm is largely affected by the balancing parameter @xmath37 and the thresholds @xmath42 and @xmath43 .",
    "according to ( [ za cost function ] ) and ( [ 2 ] ) , it can be seen that the parameter @xmath37 determines the importance of the @xmath0 norm and the intensity of zero - point attraction . in a certain range , a larger @xmath37 , which indicates stronger attraction intensity ,",
    "will improve the convergence performance by forcing small coefficients toward zero with fewer iterations .",
    "however , according to ( [ mse ] ) , a larger @xmath37 also results in a larger steady - state misalignment .",
    "so the parameter @xmath37 can balance the tradeoff between adaptation speed and quality .",
    "moreover , the optimal parameter @xmath37 empirically satisfies @xmath100 . by analyzing steady - state mse in ( [ mse ] ) under such circumstance",
    ", it can be seen that @xmath101 according to ( [ compare ] ) , the influence of the last term in the denominator of ( [ mse ] ) can be ignored , which means that the steady - state mse of the proposed algorithm is approximately the same with standard lms for near sparse systems identification .",
    "the same conclusion can also be drawn from ( [ 6 ] ) intuitively : when the adaptation reaches steady - state , the small @xmath11 renders the value of @xmath102 trivial compared to @xmath19 , letting the relaxation of zero - point attraction constraint . on the other hand , with large @xmath11 in the beginning and the process of adaptation which indicates larger zero - point attraction force",
    ", the zero - point attractor adjusts the small taps more effectively than za - lms , forcing them to zero with fewer iterations , which accelerate the convergence rate significantly .    the thresholds @xmath42 and @xmath43 determine the zero - point attraction range together .",
    "the parameter @xmath42 is set to avoid forcing all small coefficients to exact zero , it is suggested to be set as the mean amplitude of those near zero coefficients of the real system .",
    "specifically , for exact sparse systems , as most coefficients of the unknown system are exactly zero except some large ones , accordingly @xmath103 is set to force most small coefficients to exact zero . for exact sparse systems contaminated by small gaussian white noise",
    ", @xmath42 should be set as the standard deviation of the noise .",
    "for near sparse systems generated by ggd , as the mean amplitude of the small coefficient is hard to derive , we empirically choose @xmath42 for the proposed algorithm . as a small sparsity indicator @xmath29 in ggd usually means smaller mean amplitude of the small coefficient , we choose smaller @xmath42 when @xmath29 is smaller . according to the simulations ,",
    "@xmath42 is chosen in the range @xmath104 to @xmath105 for ggd with @xmath29 varying from @xmath106 to @xmath107 .",
    "the parameter @xmath43 is chosen to reduce the unnecessary attraction of large coefficients in za - lms , therefore , empirically any constant @xmath43 , which is much larger than the deviation of small coefficients and much smaller than infinity , should be appropriate .",
    "various simulations demonstrate that the parameter @xmath43 can be set as a constant around 1 for most near sparse systems .",
    "this choice of @xmath43 is quite standard for most applications .",
    "in this section , first we demonstrate the convergence performance of our proposed algorithm on two near sparse systems and a exact sparse system in experiment 1 - 4 , respectively .",
    "second , experiment 5 - 7 are designed to verify the derivation and discussion in section iv . besides the proposed algorithm , standard nlms , za - lms , ipnlms@xcite and iipnlms@xcite are also simulated for comparison . to be noticed , the normalized variants of za - lms and the proposed algorithm are adopted to guarantee a fair comparison in all experiments except the fourth , where dwza - lms is simulated to verify the theoretical analysis result .    the first experiment is to test the convergence and tracking performance of the proposed algorithm on near sparse system driven by gaussian white signal and correlated input , respectively .",
    "the unknown system is generated by ggd which has been shown in fig .",
    "[ sparsec ] .",
    "b with filter length @xmath108 , it is initialized randomly with @xmath109 and @xmath110 .",
    "for the white and correlated input , the system is regenerated following the same distribution after @xmath111 and @xmath112 iterations , respectively .",
    "for the white input , the signal @xmath10 is generated by white gaussian noise with power @xmath113 . for the correlated input , the signal is generated by white gaussian noise @xmath114 driving a first - order auto - regressive ( ar ) filter , @xmath115 , and @xmath10 is normalized . besides , the power of observation noise is @xmath116 for both input .",
    "the five algorithms are simulated 100 times respectively with parameter @xmath117 in both cases .",
    "the other parameters are as follows    * ipnlms and iipnlms with white input : @xmath118 , @xmath119 , @xmath120 , @xmath121 , @xmath122 ; * ipnlms and iipnlms with correlated input : @xmath118 , @xmath123 , @xmath120 , @xmath121 , @xmath122 ; * za - nlms and dwza - nlms with white input : @xmath124 , @xmath125 , @xmath126 , @xmath127 .",
    "* za - nlms and dwza - nlms with correlated input : @xmath124 , @xmath128 , @xmath126 , @xmath127 .",
    ".comparison of computational complexity of ipnlms , iipnlms and dwza - nlms [ cols=\"^,^,^,^,^ \" , ]     where @xmath129 $ ] denotes the fraction of coefficients in the attracting range of dwza - nlms .",
    "+    all the parameters are particularly selected to keep their steady - state error in the same level .",
    "the msd of these algorithms for both white and correlated input are shown in fig .",
    "[ experiment_1_a ] and fig .",
    "[ experiment_1_b ] , respectively .",
    "the simulation results show that all algorithms converge more slowly in the color input driven scenario than in the white noise driven case .",
    "however , the ranks or their relative performances are similar and the proposed algorithm reaches the steady state first with both white and correlated input . on the other hand , the performance of za - nlms degenerate to standard nlms as the system is near sparse .",
    "furthermore , the computational complexity of the proposed algorithm is also smaller compared with improved pnlms algorithms ( table [ computational complexity ] ) . besides , when the system is changed abruptly , the proposed algorithm also reaches the steady - state first in both cases .",
    "the second experiment is to demonstrate the proposed algorithm on near - sparse systems other than ggd .",
    "the near - sparse system with 100 taps is generated in the following manner .",
    "first , 8 large coefficients following gaussian distribution @xmath130 are generated , where all their tap positions follow uniform distribution .",
    "second , white gaussian noise with variance @xmath131 is added to all taps , enforcing the system to be near - sparse .",
    "the signal @xmath10 is generated by white gaussian noise with power @xmath113 .",
    "five algorithms , the same as in experiment 1 , are simulated 100 times respectively with parameter @xmath117 .",
    "the parameters are set to the same values as in the white input case of experiment 1 except @xmath132 for the proposed algorithm . from fig .",
    "[ experiment_5 ] , we can conclude that the proposed algorithm reaches the steady - state first in such near sparse system .",
    "the third experiment demonstrates the effectiveness of the proposed modification on za - lms algorithm on near sparse system .",
    "the system and the signal are generated in the same way as in experiment 2 .",
    "the proposed dza - nlms , dwza - nlms are compared with za - nlms for the system with 100 simulations .",
    "the step length for all algorithms are set as @xmath117 .",
    "we particularly choose parameters @xmath133 and @xmath134 for dza - nlms and za - nlms to ensure their steady - state mean square error in the same level .",
    "we set @xmath135 for dwza - nlms algorithm for a fair comparison with dza - nlms .",
    "the parameter @xmath42 and @xmath43 in dwza - nlms are chosen as @xmath136 and @xmath127 , respectively .        from fig .",
    "[ experiment_7 ] , we can see that with the dynamic zero - point attractor dza - nlms convergences faster than za - nlms . by adding another window constraint on the zero - point attractor , the dwza - nlms",
    "not only preserves the property of fast convergence of dza - nlms , but also shows smaller steady - state mean square error than both za - nlms and dza - nlms .",
    "the fourth experiment shows that the proposed improvement is still effective on the exact sparse system identification .",
    "the unknown system is shown in fig .",
    "[ sparsec ] .",
    "a , where filter length @xmath108 . besides , 8 large coefficients is uniformly distributed and generated by gaussian distribution @xmath130 , and all other tap coefficients are exactly zero .",
    "the input signal is generated by white gaussian noise with power @xmath113 , and the power of observation noise is @xmath116 .",
    "the proposed algorithm is compared with za - nlms and standard nlms , where each algorithm is simulated 100 times with 2000 iterations .",
    "the step size @xmath19 is set to @xmath137 for nlms , and @xmath117 for both the za - nlms and the proposed algortihms .",
    "the parameter @xmath138 is set to @xmath139 and @xmath140 .",
    "all the parameters are chosen to make sure that the steady - state error is the same for comparison . according to fig .",
    "[ experiment_4 ] , it can be seen that the convergence performance is also improved compared with za - nlms via the proposed method on exact sparse system , thus the proposed improvement on the algorithms are robust .        the fifth experiment is to test the sensitivity to sparsity of the proposed algorithm .",
    "all conditions are the same with the first experiment except the sparsity .",
    "the parameter @xmath29 is selected as @xmath141 , and @xmath142 , respectively . besides , we also compared our algorithm when the system is non - sparse which is generated by gaussian distribution . for each @xmath29 , both the proposed algorithm and nlms are simulated @xmath143 times with @xmath111 iterations .",
    "the step size @xmath19 is @xmath144 for both algorithms , and @xmath145 , @xmath146 , @xmath127 for the proposed algorithm .",
    "the simulated msd curves are shown in fig .",
    "[ experiment_3 ] .",
    "the steady - state msd remains approximately the same for the proposed algorithm with varying parameter @xmath29 which denotes the sparsity of systems , meanwhile the convergence rate decreases as the sparsity decreases .",
    "for the non - sparse case , our algorithm degenerates and shows similar behavior with standard nlms .",
    "however , for each @xmath29 the proposed algorithm is never slower than standard nlms . it should be noticed that nlms is independent on the system sparsity and behaves similar when @xmath29 varies .        the sixth experiment is to test the steady - state mse with different parameters .",
    "the coefficients of unknown system follows ggd with filter length @xmath108 , the sparsity and variance are chosen as @xmath110 and @xmath147 , respectively .",
    "the input is generated by white gaussian noise with normalized power @xmath113 , and the power of observation noise is @xmath35 . under such circumstance , the steady - state msd is tested . here @xmath146 and @xmath127 are set for each simulation .",
    "the step size @xmath19 is varied from 0 to @xmath148 for given @xmath149 . and",
    "@xmath37 is changed from 0 to @xmath150 for given @xmath151 .",
    "[ experiment_2_mu ] and fig .",
    "[ experiment_2_rho ] show that the analytical results accord with the simulated ones of different parameters for variable values . specifically , in fig .",
    "[ experiment_2_mu ] , the steady - state mse goes up as the step size increases , whose trend is the same with standard lms .",
    "[ experiment_2_rho ] shows that the analytical steady - state mse matches with simulated one as the parameter @xmath37 gets larger , which verifies the result in section v.        , width=384 ]    the seventh experiment is designed to test the behavior of the proposed algorithm with respect to different parameters of @xmath42 and @xmath43 .",
    "all conditions of the proposed algorithm are the same with the second experiment except the parameters @xmath42 and @xmath43 .",
    "first , we set @xmath127 and vary @xmath42 as @xmath152 , and @xmath153 .",
    "second , we set @xmath154 and vary @xmath43 as @xmath155 , and @xmath156 . from fig .",
    "[ experiment_6_a ] , we can see that the optimal @xmath42 is chosen as the variance of the small coefficients .",
    "smaller @xmath42 will result in larger misalignment , and larger @xmath42 will cause slow convergence . from fig .",
    "[ experiment_6_b ] , we conclude that @xmath157 shows the best performance .",
    "either too large or too small @xmath43 will result in slower convergence .",
    "in order to improve the performance of za - lms for near sparse system identification , an improved algorithm , dwza - lms algorithm , is proposed in this paper by adding a window to the zero - point attractor in za - lms algorithm and utilizing the magnitude of estimation error to weight the zero - point attractor .",
    "such improvement can adjust the zero - point attraction force dynamically to accelerate the convergence rate with no computational complexity increased .",
    "in addition , the mean square convergence condition , steady - state mse and parameter selection of the proposed algorithm are theoretically analyzed .",
    "finally , computer simulations demonstrate the improvement of the proposed algorithm and effectiveness of the analysis .",
    "j. jin , y. gu , s. mei , a stochastic gradient approach on compressive sensing signal reconstruction based on adaptive filtering framework , _ ieee journal of selected topics in signal processing _",
    ", vol .  4 ,  pp.409 - 420 , apr .",
    "2010 .",
    "k. sharifi and a. leon - garcia , estimation of shape parameter for generalized gaussian distributions in subband decompositions of video , _ ieee trans . on circuits syst .",
    "video technol .",
    "_ , vol . 5 , pp .",
    "52 - 56 , 1995 ."
  ],
  "abstract_text": [
    "<S> the newly proposed @xmath0 norm constraint zero - point attraction least mean square algorithm ( za - lms ) demonstrates excellent performance on exact sparse system identification . </S>",
    "<S> however , za - lms has less advantage against standard lms when the system is near sparse . </S>",
    "<S> thus , in this paper , firstly the near sparse system modeling by generalized gaussian distribution is recommended , where the sparsity is defined accordingly . </S>",
    "<S> secondly , two modifications to the za - lms algorithm have been made . </S>",
    "<S> the @xmath0 norm penalty is replaced by a partial @xmath0 norm in the cost function , enhancing robustness without increasing the computational complexity . </S>",
    "<S> moreover , the zero - point attraction item is weighted by the magnitude of estimation error which adjusts the zero - point attraction force dynamically . by combining the two improvements , dynamic windowing za - lms ( dwza - lms ) algorithm </S>",
    "<S> is further proposed , which shows better performance on near sparse system identification . </S>",
    "<S> in addition , the mean square performance of dwza - lms algorithm is analyzed . </S>",
    "<S> finally , computer simulations demonstrate the effectiveness of the proposed algorithm and verify the result of theoretical analysis .    </S>",
    "<S> * keywords : * lms , sparse system identification , zero - point attraction , za - lms , generalized gaussian distribution </S>"
  ]
}