{
  "article_text": [
    "a parallel program can be represented as a directed acyclic graph ( dag ) , where nodes correspond to tasks ( or subtasks ) and arrows represent control or communication between tasks .",
    "leiserson  @xcite characterizes the performance of parallel programs by the elapsed time @xmath1 to execute all the nodes in a dag ( e.g. , fig .",
    "[ fig : dag ] ) , and the time @xmath0 to execute the _ critical path_.    in project management , a critical path is the sequence of project network activities ( e.g. , a pert chart ) which add up to the _ longest _ overall duration .",
    "it determines the shortest possible time to complete the project .",
    "any delay of an activity on the critical path directly impacts the planned project completion date ( i.e. there is no float on the critical path ) .",
    "a project can have more than one critical path .",
    "the time to execute a program on @xmath2 processors is @xmath3 and the speedup metric is : @xmath4 with computational efficiency : @xmath5",
    "i.e. , the average amount of speedup per processor .",
    "leiserson  @xcite claims there are two _ lower _ bounds on parallel performance for fig .",
    "[ fig : dag ] : @xmath6 @xmath7 is the reduced execution time attained by partitioning the work ( equally ) across @xmath2 processors .",
    "clearly , @xmath3 can not be less than the time it takes to execute a @xmath2-th of the work  the meaning of ( [ eqn : linear ] ) .",
    "similarly , @xmath3 can not be less than the time it takes to execute the critical path , even if there are an infinite number of physical processors  the meaning of ( [ eqn : ceiling ] ) .    substituting ( [ eqn : linear ] ) into ( [ eqn : speedup ] ) : @xmath8 which corresponds to ideal _ linear _ speedup . in reality",
    ", we expect the speedup to be generally sublinear : @xmath9 under certain special circumstances speedup may exhibit _ superlinear _ performance : @xmath10 leiserson excludes ( [ eqn : superlin ] ) on the basis of ( [ eqn : linear ] ) .",
    "he also states that because of ( [ eqn : ceiling ] ) , the _ maximum _ possible speedup is given by : @xmath11 he calls ( [ eqn : parallelism ] ) the `` parallelism '' and it corresponds to the average amount of work - per - node along the critical path .",
    "but what do these bounds really mean ?",
    "consider an example based on fig .",
    "[ fig : dag ] .",
    "following leiserson , let s assume for simplicity that each node in the dag takes just 1 unit of time to execute .",
    "then , the total time to execute the entire dag on a single processor is @xmath12 time steps .",
    "similarly , the contains nine nodes , so @xmath13 and from ( [ eqn : parallelism ] ) : @xmath14 hence , the possible speedup is 2 .    note , however , that this maximum speedup ( [ eqn : parallelism ] ) is not the same as the more familiar amdahl bound  @xcite : @xmath15    equation ( [ eqn : amdasymp ] ) is the asymptotic form of the amdahl speedup function  @xcite : @xmath16 in the limit of an infinite number of processors @xmath17 . in fig .",
    "[ fig : dag ] , the _ serial fraction _ ( @xmath18 ) corresponds to 4 single nodes out of 18 total nodes and therefore : @xmath19 which is numerically greater than the `` maximum '' in ( [ eqn : para2 ] ) .",
    "how can we reconcile these various algorithmic speedup metrics ?",
    "leiserson s @xmath20 is identical to the  @xcite defined as : @xmath21 where @xmath22 is the total amount of work ( expressed in cpu - seconds , for example ) and @xmath23 is the total parallel execution time .      1 .",
    "start at the top of the dag 2 .   at each level where there are nodes ,",
    "draw a horizontal line through them 3 .   on each horizontal row",
    "calculate the time - node product for each node 4 .",
    "sum all the time - node products on each row to get @xmath22    which can be written symbolically as : @xmath24 for fig .",
    "[ fig : dag ] we obtain : @xmath25 or @xmath26 .",
    "the value of @xmath23 can be obtained be simply adding together all the time factors in the products of ( [ eqn : work ] ) , i.e. , @xmath27 , since @xmath28 and there are nine terms . applying ( [ eqn : avgpara ] ) : @xmath29 , which is identical to ( [ eqn : para2 ] ) .",
    "thus , @xmath30 .",
    "@xmath31    this is consistent with leiserson s definition of @xmath20 as the average amount of work - per - node along the critical path .",
    "see section  [ sec : bounds ] .",
    "other examples of calculating average parallelism are presented in ref .",
    "@xcite .          in fig .",
    "[ fig : dag ] , such node compression is equivalent to having all 18 nodes positioned on the same horizontal row .",
    "since it is not possible to squash the dag any flatter , the best possible speedup corresponds to distributing those 18 nodes simultaneously onto @xmath32 processors .",
    "this bound is identical to ideal linear speedup ( [ eqn : linspeed ] ) , i.e. , @xmath33 .",
    "@xmath31      superlinear speedup ( [ eqn : superlin ] ) corresponds to an efficiency @xmath35 .",
    "one way this might be observed is to run the work in fig .",
    "[ fig : dag ] successively on @xmath36 processors .",
    "the speedup for small-@xmath2 would be inferior to that for large-@xmath2 , so the scaling would appear to become better than linear",
    ". however , this apparent improvement is just an artifact of choosing the wrong baseline to establish linearity in the first place ."
  ],
  "abstract_text": [
    "<S> a parallel program can be represented as a directed acyclic graph . </S>",
    "<S> an important performance bound is the time @xmath0 to execute the critical path through the graph . </S>",
    "<S> we show how this performance metric is related to amdahl speedup and the degree of average parallelism . </S>",
    "<S> these bounds formally exclude superlinear performance . </S>"
  ]
}