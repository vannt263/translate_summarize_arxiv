{
  "article_text": [
    "in many statistical inference problems , the task is to detect , from given data , a global structure such as low - rank structure or clustering .",
    "the task is usually hard to solve since modern datasets usually have a large dimensionality .",
    "when the dataset can be represented as a matrix , spectral methods are popular as it gives a natural way to reduce the dimensionality of data using eigenvectors or singular vectors . in the point - of - view of inference",
    ", data can be seen as measurements to the underlying structure .",
    "thus more data gives more precise information about the underlying structure .",
    "however in many situations when we do not have enough measurements , i.e. the data matrix is sparse , standard spectral methods usually have localization problems thus do not work well .",
    "one example is the community detection in sparse networks , where the task is to partition nodes into groups such that there are many edges connecting nodes within the same group and comparatively few edges connecting nodes in different groups .",
    "it is well known that when the graph has a large connectivity @xmath0 , simply using the first few eigenvectors of the adjacency matrix @xmath1 ( with @xmath2 denoting an edge between node @xmath3 and node @xmath4,and @xmath5 otherwise ) gives a good result . in this case , like that of a sufficiently dense erds - rnyi ( er ) random graph with average degree @xmath0 , the spectral density follows wigner s semicircle rule , @xmath6 , and there is a gap between the edge of bulk of eigenvalues and the informative eigenvalue that represents the underlying community structure .",
    "however when the network is large and sparse , the spectral density of the adjacency matrix deviates from the semicircle , the informative eigenvalue is hidden in the bulk of eigenvalues , as displayed in fig .",
    "[ fig : spectrum ] left .",
    "its eigenvectors associated with largest eigenvalues ( which are roughly proportional to @xmath7 for er random graphs ) are localized on the large - degree nodes , thus reveal only local structures about large degrees rather than the underlying global structure .",
    "other standard matrices for spectral clustering @xcite , e.g. laplacian , random walk matrix , normalized laplacian , all have localization problems but on different local structures such as dangling trees .",
    "another example is the matrix completion problem which asks to infer missing entries of matrix @xmath8 with rank @xmath9 from only few observed entries .",
    "a popular method for this problem is based on the singular value decomposition ( svd ) of the data matrix",
    ". however it is well known that when the matrix is sparse , svd - based method performs very poorly , because the singular vectors corresponding to the largest singular values are localized , i.e. highly concentrated on high - weight column or row indices .    a simple way to ease the pain of localization induced by high degree or weight is trimming @xcite which sets to zero columns or rows with a large degree or weight .",
    "however trimming throws away part of the information , thus does not work all the way down to the theoretical limit in the community detection problem  @xcite . it also performs worse than other methods in matrix completion problem  @xcite .    in recent years , many methods have been proposed for the sparsity - problem .",
    "one kind of methods use new linear operators related to the belief propagation and bethe free energy , such as the non - backtracking matrix @xcite and bethe hessian @xcite .",
    "another kind of methods add to the data matrix or its variance a rank - one regularization matrix  @xcite .",
    "these methods are quite successful in some inference problems in the sparse regime .",
    "however in our understanding none of them works in a general way to solve the localization problem .",
    "for instance , the non - backtracking matrix and the bethe hessian work very well when the graph has a locally - tree - like structure , but they have again the localization problems when the system has short loops or sub - structures like triangles and cliques . moreover its performance is sensitive to the noise in the data  @xcite .",
    "rank - one regularizations have been used for a long time in practice , the most famous example is the `` teleportation '' term in the google matrix .",
    "however there is no satisfactory way to determine the optimal amount of regularization in general .",
    "moreover , analogous to the non - backtracking matrix and bethe hessian , the rank - one regularization approach is also sensitive to the noise , as we will show in the paper .",
    "the main contribution of this paper is to illustrate how to solve the localization problem of spectral methods for general inference problems in sparse regime and with noise , by learning a proper regularization that is specific for the given data matrix from its localized eigenvectors . in the following text",
    "we will first discuss in sec .",
    "[ sec : reg ] that all three methods for community detection in sparse graphs can be put into the framework of regularization .",
    "thus the drawbacks of existing methods can be seen as improper choices of regularizations . in sec .",
    "[ sec : our ] we investigate how to choose a good regularization that is dedicated for the given data , rather than taking a fixed - form regularization as in the existing approaches .",
    "we use matrix perturbation analysis to illustrate how the regularization works in penalizing the localized eigenvectors , and making the informative eigenvectors that correlate with the global structure float to the top positions in spectrum . in sec .",
    "[ sec : num ] we use extensive numerical experiments to validate our approach on several well - studied inference problems , including the community detection in sparse graphs , clustering from sparse pairwise entries , rank estimation and matrix completion from few entries .    [ fig : spectrum ]   nodes , average degree @xmath10 , @xmath11 groups and @xmath12 .",
    "red arrows point to eigenvalues out of the bulk .",
    ", title=\"fig : \" ]   nodes , average degree @xmath10 , @xmath11 groups and @xmath12 .",
    "red arrows point to eigenvalues out of the bulk .",
    ", title=\"fig : \" ]",
    "we see that the above three methods for the community detection problem in sparse graphs , i.e. trimming , non - backtracking / bethe hessian , and rank - one regularizations , can be understood as doing different ways of regularizations . in this framework",
    ", we consider a regularized matrix @xmath13 here matrix @xmath14 is the data matrix or its ( symmetric ) variance , such as @xmath15 with @xmath16 denoting the diagonal matrix of degrees , and matrix @xmath17 is a regularization matrix .",
    "the rank - one regularization approaches  @xcite fall naturally into this framework as they set @xmath18 to be a rank - one matrix , @xmath19 , with @xmath20 being a tunable parameter controlling strength of regularizations .",
    "it is also easy to see that in the trimming , @xmath14 is set to be the adjacency matrix and @xmath17 contains entries to remove columns or rows with high degrees from @xmath21 .    for spectral algorithms using the non - backtracking matrix , its relation to form eq .",
    "is not straightforward .",
    "however we can link them using the theory of graph zeta function  @xcite which says that an eigenvalue @xmath22 of the non - backtracking operator satisfies the following quadratic eigenvalue equation , @xmath23=0,\\ ] ] where @xmath24 is the identity matrix .",
    "it indicates that a particular vector @xmath25 that is related to the eigenvector of the non - backtracking matrix satisfies @xmath26 .",
    "thus spectral clustering algorithm using the non - backtracking matrix is equivalent to the spectral clustering algorithm using matrix with form in eq .  , while @xmath27 , @xmath28 , and @xmath22 acting as a parameter .",
    "we note here that the parameter does not necessarily be an eigenevalue of the non - backtracking matrix .",
    "actually a range of parameters work well in practice , like those estimated from the spin - glass transition of the system @xcite .",
    "so we have related different approaches of resolving localizations of spectral algorithm in sparse graphs into the framework of regularization .",
    "although this relation is in the context of community detection in networks , we think it is a general point - of - view , when the data matrix has a general form rather than a @xmath29 matrix .    as we have argued in the introduction , above three ways of regularization work from case to case and",
    "have different problems , especially when system has noise .",
    "it means that in the framework of regularizations , the effective regularization matrix @xmath17 added by these methods do not work in a general way and is not robust . in our understanding ,",
    "the problem arises from the fact that in all these methods , the form of regularization is _ fixed _ for all kinds of data , regardless of different reasons for the localization .",
    "thus one way to solve the problem would be looking for the regularizations that are specific for the given data , as a feature . in the following section",
    "we will introduce our method explicitly addressing how to learn such regularizations from localized eigenvectors of the data matrix .",
    "the reason that the informative eigenvectors are hidden in the bulk is that some random eigenvectors have large eigenvalues , due to the localization which represent the local structures of the system . in the complementary side ,",
    "if these eigenvectors are not localized , they are supposed to have smaller eigenvalues than the informative ones which reveal the global structures of the graph .",
    "this is the main assumption that our idea is based on .    in this work",
    "we use the _ inverse participation ratio _ ( ipr ) , @xmath30 , to quantify the amount of localization of a ( normalized ) eigenvector @xmath25 .",
    "ipr has been used frequently in physics , for example for distinguishing the extended state from the localized state when applied on the wave function  @xcite .",
    "it is easy to check that @xmath31 ranges from @xmath32 for vector @xmath33 to @xmath34 for vector @xmath35 .",
    "that is , a larger @xmath31 indicates more localization in vector @xmath25 .",
    "our idea is to create a matrix @xmath36 with similar structures to @xmath21 , but with non - localized leading eigenvectors .",
    "we call the resulting matrix _ x - laplacian _ , and define it as @xmath37 , where matrix @xmath21 is the data matrix ( or its variant ) , and @xmath38 is learned using the procedure detailed below :    1",
    ".   set @xmath38 to be all - zero matrix .",
    "2 .   find set of eigenvectors @xmath39 associated with the first @xmath40 largest eigenvalues ( in algebra ) of @xmath36 .",
    "3 .   identify the eigenvector @xmath25 that has the largest inverse participation ratio among the @xmath40 eigenvectors in @xmath41 .",
    "that is , find @xmath42 4 .",
    "if @xmath43 , return @xmath37 ; otherwise , @xmath44 , then go to step 2 .",
    "we can see that the regularization matrix @xmath38 is a diagonal matrix , its diagonal entries are learned gradually from the most localized vector among the first several eigenvectors .",
    "the effect of @xmath38 is to penalize the localized eigenvectors , by suppressing down the eigenvalues associated with the localized eigenvectors .",
    "the learning will continue until all @xmath40 leading eigenvectors are delocalized , thus are supposed to correlate with the global structure rather than the local structures . as an example , we show the effect of @xmath38 to the spectrum in fig .  [ fig : spectrum ] . in the left panel , we plot the spectrum of the adjacency matrix ( i.e. before learning @xmath38 ) and the x - laplacian ( i.e. after learning @xmath38 ) of a sparse network generated by the stochastic block model with @xmath11 groups . for the adjacency matrix in the left panel ,",
    "localized eigenvectors have large eigenvalues and contribute a tail to the semicircle , covering the informative eigenvalue , leaving only one eigenvalue , which corresponds to the eigenvector that essentially sorts vertices according to their degree , out of the bulk .",
    "the spectral density of x - laplacian is shown in the right panel of fig .",
    "[ fig : spectrum ] .",
    "we can see that the right corner of the continues part of the spectral density appearing in the spectrum of the adjacency matrix , is missing here .",
    "this is because due to the effect of @xmath38 , the eigenvalues that are associated with localized eigenvectors in the adjacency matrix are pushed into the bulk , maintaining a gap between the edge of bulk and the informative eigenvalue ( being pointed by the left red arrow in the figure ) .",
    "the key procedure of the algorithm is the learning part in step @xmath45 , which updates diagonal terms of matrix @xmath38 using the most localized eigenvector @xmath25 . throughout the paper , by default we use learning rate @xmath46 and threshold @xmath47 . as @xmath48 and @xmath49",
    ", we can treat the learned entries in each step , @xmath50 , as a perturbation to matrix @xmath36 . after applying this perturbation",
    ", we anticipate that an eigenvalue of @xmath51 changes from @xmath52 to @xmath53 , and an eigenvector changes from @xmath54 to @xmath55 .",
    "if we assume that matrix @xmath36 is not ill - conditioned , and the first few eigenvectors that we care about are distinct , then we have @xmath56 .",
    "derivation of the above expression is straightforward , but for the completeness we put the derivations in the appendices . in our algorithm",
    ", @xmath50 is a diagonal matrix with entries @xmath57 with @xmath25 denoting the identified eigenvector who has the largest inverse participation ratio , so last equation can be written as @xmath58 . for the identified vector @xmath25 ,",
    "we further have @xmath59 it means the eigenvalue of the identified eigenvector with inverse participation ratio @xmath31 is decreased by amount @xmath60 . that is , _ the more localized the eigenvector is , the larger penalty on its eigenvalue_.    in addition to the penalty to the localized eigenvalues , we see that the leading eigenvectors are delocalizing during learning .",
    "we have analyzed the change of eigenvectors after the perturbation given by the identified vector @xmath25 , and obtained ( see appendices for the derivations ) the change of an eigenvector @xmath61 as a function of all the other eigenvalues and eigenvectors , @xmath62 .",
    "then the inverse participation ratio of the new vector @xmath55 can be written as + @xmath63 as eigenvectors @xmath54 and @xmath64 are orthogonal to each other , the term @xmath65 can be seen as a signal term and the last term can be seen as a cross - talk noise with zero mean .",
    "we see that the cross - talk noise has a small variance , and empirically its effect can be neglected . for the leading eigenvector corresponding to the largest eigenvalue @xmath66 , it is straightforward to see that the signal term is strictly positive",
    ". thus if the learning is slow enough , the perturbation will always decrease the inverse participation ratio of the leading eigenvector .",
    "this is essentially an argument for convergence of the algorithm . for other top eigenvectors , i.e. the second and third eigenvectors and",
    "so on , though @xmath67 is not strictly positive , there are much more positive terms than negative terms in the sum , thus the signal should be positive with a high probability . thus one can conclude that the process of learning @xmath38 makes first few eigenvectors de - localizing .",
    "an example illustrating the process of the learning is shown in fig .",
    "[ fig : v2v3 ] where we plot the second eigenvector vs. the third eigenvector , at several times steps during the learning , for a network generated by the stochastic block model with @xmath68 groups .",
    "we see that at @xmath69 , i.e. without learning , both eigenvectors are localized , with a large range of distribution in entries .",
    "the color of eigenvectors encodes the group membership in the planted partition .",
    "we see that at @xmath69 three colors are mixed together indicating that two eigenvectors are not correlated with the planted partition . at @xmath70 three colors begin to separate , and range of entry distribution become smaller , indicating that the localization is lighter . at @xmath71 , three colors are more separated , the partition obtained by applying k - means algorithm using these vectors successfully recovers @xmath72 of the group memberships",
    ". moreover we can see that the range of entries of eigenvectors shrink to @xmath73 $ ] , giving a small inverse participation ratio .     the second eigenvector @xmath74 compared with the third eigenvector @xmath75 of @xmath36 for a network at three steps with @xmath76 and @xmath77 during learning .",
    "the network has @xmath78 nodes , @xmath68 groups , average degree @xmath10 , @xmath79 , three colors represent group labels in the planted partition .",
    ", title=\"fig : \" ]   the second eigenvector @xmath74 compared with the third eigenvector @xmath75 of @xmath36 for a network at three steps with @xmath76 and @xmath77 during learning .",
    "the network has @xmath78 nodes , @xmath68 groups , average degree @xmath10 , @xmath79 , three colors represent group labels in the planted partition .",
    ", title=\"fig : \" ]   the second eigenvector @xmath74 compared with the third eigenvector @xmath75 of @xmath36 for a network at three steps with @xmath76 and @xmath77 during learning .",
    "the network has @xmath78 nodes , @xmath68 groups , average degree @xmath10 , @xmath79 , three colors represent group labels in the planted partition .",
    ", title=\"fig : \" ]",
    "in this section we validate our approach with experiments on several inference problems , i.e. community detection problems , clustering from sparse pairwise entries , rank estimation and matrix completion from a few entries .",
    "we will compare performance of spectral algorithms using the x - laplacian with recently proposed state - of - the - art spectral methods in the sparse regime .",
    "first we use synthetic networks generated by the stochastic block model  @xcite , and its variant with noise  @xcite .",
    "the standard stochastic block model ( sbm ) , also called the planted partition model , is a popular model to generate ensemble of networks with community structure .",
    "there are @xmath40 groups of nodes and a planted partition @xmath80 .",
    "edges are generated independently according to a @xmath81 matrix @xmath82 . without loss of generality here",
    "we discuss the commonly studied case where the @xmath40 groups have equal size and where @xmath82 has only two distinct entries , @xmath83 if @xmath84 and @xmath85 if @xmath86 . given the average degree of the graph , there is a so - called detectability transition @xmath87  @xcite , beyond which point it is not possible to obtain any information about the planted partition .",
    "it is also known spectral algorithms based on the non - backtracking matrix succeed all the way down to the transition  @xcite .",
    "this transition was recently established rigorously in the case of @xmath11  @xcite .",
    "comparisons of spectral methods using different matrices are shown in fig .",
    "[ fig : community_detection ] left . from the figure",
    "we see that the x - laplacian works as well as the non - backtracking matrix , down to the detectability transition . while the direct use of the adjacency matrix , i.e. @xmath36 before learning , does not work well when @xmath88 exceeds about @xmath89 .    in the right panel of fig .",
    "[ fig : community_detection ] , each network is generated by the stochastic block model with the same parameter as in the left panel , but with @xmath90 extra cliques , each of which contains @xmath90 randomly selected nodes .",
    "theses cliques do not carry information about the planted partition , hence act as noise to the system .",
    "in addition to the non - backtracking matrix , x - laplacian , and the adjacency matrix , we put into comparison the results obtained using other classic and newly proposed matrices , including bethe hessian  @xcite , normalized laplacian  ( n. laplacian ) @xmath91 , and regularized and normalized laplacian  ( r.n .",
    "laplacian ) @xmath92 , with a optimized regularization @xmath20 ( we have scanned the whole range of @xmath20 , and chosen an optimal one that gives the largest overlap , i.e. fraction of correctly reconstructed labels , in most of cases ) . from the figure we see that with the noise added , only x - laplacian works down to the original transition ( of sbm without cliques ) .",
    "all other matrices fail in detecting the community structure with @xmath93 .",
    "we have tested other kinds of noisy models , including the noisy stochastic block model , as proposed in  @xcite .",
    "our results show that the x - laplacian works well ( see appendices ) while all other spectral methods do not work at all on this dataset  @xcite . moreover , in addition to the classic stochastic block model , we have extensively evaluated our method on networks generated by the degree - corrected stochastic block model  @xcite , and the stochastic block model with extensive triangles .",
    "we basically obtained qualitatively results as in fig .  [ fig : community_detection ] that the x - laplacian works as well as the state - of - the - art spectral methods for the dataset . the figures and detailed results",
    "can be found at the appendices .",
    "we have also tested real - world networks with an expert division , and found that although the expert division is usually easy to detect by directly using the adjacency matrix , the x - laplacian significantly improves the accuracy of detection .",
    "for example on the political blogs network  @xcite , spectral clustering using the adjacency matrix gives @xmath94 mis - classified labels among totally @xmath95 labels , while the x - laplacian gives only @xmath96 mis - classified labels .",
    "consider the problem of grouping @xmath97 items into clusters based on the similarity matrix @xmath98 , where @xmath99 is the pairwise similarity between items @xmath3 and @xmath4 .",
    "here we consider not using all pairwise similarities , but only @xmath100 random samples of them .",
    "in other words , the similarity graph which encodes the information of the global clustering structure is sparse , rather than the complete graph .",
    "there are many motivations for choosing such sparse observations , for example in some cases all measurements are simply not available or even can not be stored .    in this section",
    "we use the generative model recently proposed in @xcite , since there is a theoretical limit that can be used to evaluate algorithms . without loss of generality , we consider the problem with only @xmath11 clusters .",
    "the model in @xcite first assigns items hidden clusters @xmath101 , then generates similarity between a randomly sampled pairs of items according to probability distribution , @xmath102 and @xmath103 , associated with membership of two items .",
    "there is a theoretical limit @xmath104 satisfying @xmath105 that with @xmath106 no algorithm could obtain any partial information of the planted clusters ; while with @xmath107 some algorithms , e.g. spectral clustering using the bethe hessian  @xcite , achieve partial recovery of the planted clusters .",
    "similar to the community detection in sparse graphs , spectral algorithms directly using the eigenvectors of a similarity matrix @xmath108 does not work well , due to the localization of eigenvectors induced by the sparsity . to evaluate whether our method , the x - laplacian , solves the localization problem , and how it works compared with the bethe hessian , in fig .",
    "[ fig : clustering ] we plot the performance ( in overlap , the fraction of correctly reconstructed group labels ) of three algorithms on the same set of similarity matrices .",
    "for all the datasets there are two groups with distributions @xmath109 and @xmath110 being gaussian with unit variance and mean @xmath111 and @xmath112 respectively . in the left panel of fig .",
    "[ fig : clustering ] the topology of pairwise entries is random graph , bethe hessian works down to the theoretical limit , while directly using of the measurement matrix gives a poor performance .",
    "we can also see that x - laplacian has fixed the localization problem of directly using of the measurement matrix , and works almost as good as the bethe - hessian .",
    "we note that the bethe hessian needs to know the parameters ( i.e. parameters of distributions @xmath102 and @xmath103 ) , while the x - laplacian does not use them at all .    in the right panel of fig .",
    "[ fig : clustering ] , on top of the er random graph topology , we add some noisy local structures by randomly selecting @xmath113 nodes and connecting neighbors of each selected node to each other .",
    "the weights for the local pairwise were set to @xmath34 , so that the noisy structures do not contain information about the underlying clustering .",
    "we can see that bethe hessian is influenced by noisy local structures and fails to work , while x - laplacian solves the localization problems induced by sparsity , and is robust to the noise .",
    "we have also tested other kinds of noise by adding cliques , or hubs , and obtained similar results ( see appendices ) .",
    "randomly selected nodes with all their neighbors connected .",
    "each point in the figure is averaged over @xmath113 realizations of size @xmath114 . [",
    "fig : clustering ] , title=\"fig : \" ]   randomly selected nodes with all their neighbors connected .",
    "each point in the figure is averaged over @xmath113 realizations of size @xmath114 .",
    "[ fig : clustering ] , title=\"fig : \" ]      the last problem we consider in this paper for evaluating the x - laplacian is completion of a low rank matrix from few entries .",
    "this problem has many applications including the famous collaborative filtering .",
    "a problem that is closely related to it is the rank estimation from revealed entries . indeed estimating rank of the matrix is usually the first step before actually doing the matrix completion .",
    "the problem is defined as follows : let @xmath115 , where @xmath116 and @xmath117 are chosen uniformly at random and @xmath118 is the ground - true rank",
    ". only few , say @xmath119 , entries of matrix @xmath120 are revealed .",
    "that is we are given a matrix @xmath121 who contains only subset of @xmath122 , with other elements being zero .",
    "many algorithms have been proposed for matrix completion , including nuclear norm minimization  @xcite and methods based on the singular value decomposition  @xcite etc . trimming which sets to zero all rows and columns with a large revealed entries ,",
    "is usually introduced to control the localizations of singular vectors and to estimate the rank using the gap of singular values  @xcite .",
    "analogous to the community detection problem , trimming is not supposed to work optimally when matrix @xmath21 is sparse .",
    "indeed in  @xcite authors reported that their approach based on the bethe hessian outperforms trimming+svd when the topology of revealed entries is a sparse random graph .",
    "moreover , authors in  @xcite show that the number of negative eigenvalues of the bethe hessian gives a more accurate estimate of the rank of @xmath21 than that based on trimming+svd .",
    "however , we see that if the topology is not locally - tree - like but with some noise , for example with some additional cliques , both trimming of the data matrix and bethe hessian perform much worse , reporting a wrong rank , and giving a large reconstruction error , as illustrated in fig .",
    "[ fig : rank ] . in the left panel of the figure we plot the eigenvalues of the bethe hessian , and singular values of trimmed matrix @xmath21 with true rank @xmath123 .",
    "we can see that both of them are continuously distributed : there is no clear gap in singular values of trimmed @xmath21 , and bethe hessian has lots of negative eigenvalues . in this case since matrix @xmath21 could be a non - squared matrix , we need to define the x - laplacian as @xmath124 .",
    "the eigenvalues of @xmath36 are also plotted in fig .",
    "[ fig : rank ] where one can see clearly that there is a gap between the second largest eigenvalue and the third one .",
    "thus the correct rank can be estimated using the value minimizing consecutive eigenvalues , as suggested in @xcite . after estimating the rank of the matrix , matrix completion",
    "is done by using a local optimization algorithm @xcite starting from initial matrices , that obtained using first @xmath125 singular vectors of trimming+svd , first @xmath125 eigenvectors of bethe hessian and x - laplacian with estimated rank @xmath125 respectively .",
    "the results are shown in fig .",
    "[ fig : rank ] right where we plot the probability that obtained root mean square error ( rmse ) is smaller than @xmath126 as a function of average number of revealed entries per row @xmath0 , for the er random - graph topology plus noise represented by several cliques .",
    "we can see that x - laplacian outperforms bethe hessian and trimming+svd with @xmath127 . moreover , when @xmath128 , for all instances , only x - laplacian gives an accurate completion for all instances .    .",
    "their entries are gaussian random variables with mean zero and unit variance , so the rank of the original matrix is @xmath129 .",
    "the topology of revealed observations are random graphs with average degree @xmath130 plus @xmath90 random cliques of size @xmath113 . _",
    "( right : ) _ fraction of samples that rmse is smaller than @xmath126 , among @xmath131 samples of rank-@xmath132 data matrix @xmath133 of size @xmath134 , with the entries of @xmath41 and @xmath135 drawn from a gaussian distribution of mean @xmath136 and unit variance .",
    "the topology of revealed entries is the random graph with varying average degree @xmath0 plus @xmath90 size-20 cliques .",
    "[ fig : rank ] , title=\"fig : \" ] .",
    "their entries are gaussian random variables with mean zero and unit variance , so the rank of the original matrix is @xmath129 .",
    "the topology of revealed observations are random graphs with average degree @xmath130 plus @xmath90 random cliques of size @xmath113 .",
    "_ ( right : ) _ fraction of samples that rmse is smaller than @xmath126 , among @xmath131 samples of rank-@xmath132 data matrix @xmath133 of size @xmath134 , with the entries of @xmath41 and @xmath135 drawn from a gaussian distribution of mean @xmath136 and unit variance .",
    "the topology of revealed entries is the random graph with varying average degree @xmath0 plus @xmath90 size-20 cliques .",
    "[ fig : rank ] , title=\"fig : \" ]",
    "we have presented the x - laplacian , a general approach for detecting latent global structure in a given data matrix .",
    "it is completely a data - driven approach that learns different forms of regularization for different data , to solve the problem of localization of eigenvectors or singular vectors .",
    "the mechanics for de - localizing of eigenvectors during learning of regularizations has been illustrated using the matrix perturbation analysis .",
    "we have validated our method using extensive numerical experiments , and shown that it outperforms state - of - the - art algorithms on various inference problems in the sparse regime and with noise .    in this paper",
    "we discuss the x - laplacian using directly the data matrix @xmath21 , but the this is not the only choice .",
    "actually we have tested approaches using various variants of a , such as @xmath137 , and found they work as well .",
    "we have also tried learning regularizations for the bethe hessian , and found it succeeds in repairing bethe hessian when bethe hessian has localization problem .",
    "these indicate that our scheme of regularization - learning is a general spectral approach for hard inference problems",
    ".    10    l.  a. adamic and n.  glance .",
    "the political blogosphere and the 2004 us election : divided they blog . in _ proceedings of the 3rd international workshop on link discovery _ , pages 3643 .",
    "acm , 2005 .",
    "a.  a. amini , a.  chen , p.  j. bickel , e.  levina , et  al .",
    "pseudo - likelihood methods for community detection in large sparse networks . , 41(4):20972122 , 2013 .",
    "r.  bell and p.  dean .",
    "atomic vibrations in vitreous silica . , 50:5561 , 1970 .",
    "cai , e.  j. cands , and z.  shen .",
    "a singular value thresholding algorithm for matrix completion .",
    ", 20(4):19561982 , 2010 .",
    "e.  j. cands and b.  recht .",
    "exact matrix completion via convex optimization .",
    ", 9(6):717772 , 2009 .",
    "a.  coja - oghlan .",
    "graph partitioning via adaptive spectral techniques .",
    ", 19:227284 , 3 2010 .",
    "a.  decelle , f.  krzakala , c.  moore , and l.  zdeborov .",
    "asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications .",
    ", 84:066106 , dec 2011 .",
    "zeta functions of finite graphs and representations of p - adic groups . , 15:211280 , 1989 .",
    "p.  w. holland , k.  b. laskey , and s.  leinhardt .",
    "stochastic blockmodels : first steps . , 5(2):109137 , 1983 .",
    "a.  javanmard , a.  montanari , and f.  ricci - tersenghi .",
    "phase transitions in semidefinite relaxations . , 113(16):e2218 , 2016 .",
    "a.  joseph and b.  yu . impact of regularization on spectral clustering . , 2013 .",
    "b.  karrer and m.  e.  j. newman .",
    "stochastic blockmodels and community structure in networks . , 83:016107 , jan 2011 .",
    "r.  h. keshavan , a.  montanari , and s.  oh . low - rank matrix completion with noisy observations : a quantitative comparison . in _ communication , control , and computing , 2009 .",
    "allerton 2009 .",
    "47th annual allerton conference on _ , pages 12161222 .",
    "ieee , 2009 .",
    "r.  h. keshavan , s.  oh , and a.  montanari . matrix completion from a few entries . in _ information theory , 2009 .",
    "isit 2009 .",
    "ieee international symposium on _ , pages 324328 .",
    "ieee , 2009 .",
    "f.  krzakala , c.  moore , e.  mossel , j.  neeman , a.  sly , l.  zdeborov , and p.  zhang .",
    "spectral redemption in clustering sparse networks .",
    ", 110(52):2093520940 , 2013 .    c.  m. le , e.  levina , and r.  vershynin .",
    "sparse random graphs : regularization and concentration of the laplacian . , 2015 .",
    "c.  m. le and r.  vershynin .",
    "concentration and regularization of random graphs .",
    ", 2015 .",
    "j.  lei , a.  rinaldo , et  al .",
    "consistency of spectral clustering in stochastic block models .",
    ", 43(1):215237 , 2014 .",
    "u.  v. luxburg , m.  belkin , o.  bousquet , and pertinence .",
    "a tutorial on spectral clustering . , 2007 .",
    "l.  massouli .",
    "community detection thresholds and the weak ramanujan property . in _ proceedings of the 46th annual acm symposium on theory of computing _ , pages 694703 .",
    "acm , 2014 .",
    "e.  mossel , j.  neeman , and a.  sly .",
    "stochastic block models and reconstruction . , 2012 .",
    "a.  y. ng , m.  i. jordan , y.  weiss , et  al . on spectral",
    "clustering : analysis and an algorithm .",
    ", 2:849856 , 2002 .",
    "t.  qin and k.  rohe .",
    "regularized spectral clustering under the degree - corrected stochastic blockmodel . in _ advances in neural information processing systems _ , pages 31203128 , 2013 .",
    "a.  saade , f.  krzakala , and l.  zdeborov .",
    "spectral clustering of graphs with the bethe hessian . in _ advances in neural information processing systems _ ,",
    "pages 406414 , 2014 .",
    "a.  saade , f.  krzakala , and l.  zdeborov .",
    "matrix completion from fewer entries : spectral detectability and rank estimation . in c.",
    "cortes , n.  lawrence , d.  lee , m.  sugiyama , and r.  garnett , editors , _ advances in neural information processing systems 28 _ , pages 12611269 .",
    "curran associates , inc . , 2015 .",
    "a.  saade , m.  lelarge , f.  krzakala , and l.  zdeborov .",
    "clustering from sparse pairwise measurements . , 2016 .",
    "s.g.johnson . the nlopt nonlinear - optimization package , 2014 .",
    "p.  zhang .",
    "triangular stochastic block model . ,",
    "after applying the perturbation , we anticipate that an eigenvalue of @xmath36 changes from @xmath52 to @xmath53 , and an eigenvector changes from @xmath54 to @xmath55 .",
    "if we assume that matrix @xmath36 is not ill - conditioned , and the first few eigenvectors that we care about are distinct , then we have @xmath138 by making use of @xmath139 and keeping only first order terms , we have @xmath140 since @xmath36 is a real symmetric matrix , we can represent @xmath61 as a weighted sum of eigenvectors of @xmath36 , as @xmath141 where @xmath142 is the coefficient and @xmath64 is jth eigenvector of @xmath36 .",
    "insert last equation into eq .  , we have @xmath143 which evaluates to @xmath144 multiplying @xmath145 to both sides of last equation results to @xmath146 notice that in the last equation @xmath147 and the second term in the left hand side and the first term in the right hand side cancel each other , thus we have @xmath148 in our algorithm , @xmath50 is a diagonal matrix with entries @xmath149 where @xmath150 denotes the @xmath3th element of the selected eigenvector @xmath25 who has the largest inverse participation ratio",
    ". thus the shift of an eigenvalue @xmath151 associated with eigenvector @xmath64 ( which is different from @xmath25 ) is then @xmath152 for the selected vector @xmath25 , the change of its eigenvalue is @xmath153 that is , the amount of decreasing of eigenvalue associated with the selected vector is proportional to its inverse participation ratio .",
    "in addition to the shift of eigenvalues , we can also derive the change of eigenvectors after perturbation .",
    "multiplying transpose of an eigenvector @xmath64 to both sides of eq .",
    "results to @xmath154 which evaluates to @xmath155 where we can find that @xmath156 given that the perturbation is @xmath57 , we have an expression for the change of an eigenvector @xmath157 notice that the inverse participate ratio of the new vector @xmath55 is @xmath158 expand above equation to the first order of @xmath159 , we have @xmath160      in fig .  [",
    "fig : process1 ] we plot the evolution of eigenvalues , overlap and the inverse participation ratio ( ipr ) for the second , third and forth eigenvectors during learning of the x - laplacian for a network generated by the stochastic block model .",
    "the network has a community structure with @xmath132 groups , however the first three eigenvectors of the adjacency matrix are localized ( see left panel at @xmath69 ) and do not reveal the underlying community structure ( see the right panel at @xmath161 small .",
    "we can also see from the left panel that the ipr of them are decreasing as @xmath161 increases during learning . from the middle panel of the figure",
    ", we see that all the @xmath132 eigenvalues are decreasing , while the spectral gap @xmath162 is increasing during learning .",
    "it is interesting to see that at @xmath70 , there is a exchange of positions of the third eigenvector and the forth eigenvector .",
    "this gives a bump of the ipr , as well as an increase of accuracy of detection ( characterized by overlap ) at @xmath70 .",
    "[ fig : process1 ] , overlap ( the fraction of correctly reconstructed labels ) and first three eigenvalues ( @xmath163 ) as a function of learning steps @xmath161 for a network generated by the stochastic block model with @xmath78 , @xmath68 groups , average degree @xmath10 , @xmath79 and learning rate @xmath164 .",
    "the overlap is the fraction of successfully reconstructed labels , maximized over group permutations.,title=\"fig : \" ] , overlap ( the fraction of correctly reconstructed labels ) and first three eigenvalues ( @xmath163 ) as a function of learning steps @xmath161 for a network generated by the stochastic block model with @xmath78 , @xmath68 groups , average degree @xmath10 , @xmath79 and learning rate @xmath164 .",
    "the overlap is the fraction of successfully reconstructed labels , maximized over group permutations.,title=\"fig : \" ] , overlap ( the fraction of correctly reconstructed labels ) and first three eigenvalues ( @xmath163 ) as a function of learning steps @xmath161 for a network generated by the stochastic block model with @xmath78 , @xmath68 groups , average degree @xmath10 , @xmath79 and learning rate @xmath164 .",
    "the overlap is the fraction of successfully reconstructed labels , maximized over group permutations.,title=\"fig : \" ]      here we compare the performance of the x - laplacian with other state - of - art spectral algorithms on variants of the stochastic block model , namely the degree - corrected stochastic block model  @xcite and the triangular stochastic block model  @xcite which is the stochastic block model with triangles .",
    "it is known that in the stochastic block model , there is a detectability transition at @xmath165 where @xmath104 is the excess average degree @xmath166 and the spectral clustering algorithm based on the non - backtracking matrix achieves this threshold . in the left panel of  fig.[fig : community_detection ] we compare the performance ( evaluated using the overlap , fraction of correctly reconstructed labels ) of spectral algorithms using the adjacency matrix , the non - backtracking matrix and the x - laplacian on networks generated by the degree corrected stochastic block model with a power - law degree distribution with exponent @xmath167 .",
    "as the figure shows , our approach works even better than the algorithm using the non - backtracking matrix , this is because when the networks size ( @xmath114 ) is not large enough , the long tails of degree distribution creates short loops in the network , downgrading the performance of the algorithm using the non - backtracking matrix which is supposed to work optimally in the locally - tree like networks .    for the triangular stochastic block model , due to the presence of triangles , the non - backtracking matrix suffers from short loops and does not work well . in this case",
    "the generalized non - backtracking matrix , which runs on a factor graph with both edges and triangles treated as function nodes , works down to the transition  @xcite . in the right panel of fig .",
    "[ fig : community_detection ] we compare the performance of the spectral algorithm using the adjacency matrix , generalized non - backtracking matrix and the x - laplacian , and we can see that x - laplacian works as well as the generalized non - backtracking matrix and down to the transition .    it has been reported in  @xcite that on the perturbed stochastic block model , spectral algorithms including the one using bethe hessian do fail in detecting the community structures , while other method , e.g. semi - definite programming , works well .",
    "in the perturbed stochastic block model , after a network is generated by the stochastic block model , neighbors of some randomly selected nodes are connected to each other acting as noise to the underlying community structure , in fig .",
    "[ fig : sbm_noise ] we numerically examined the performance of x - laplacian using @xmath137 , i.e. @xmath168 , on networks generated by perturbed stochastic block model   @xcite , with exactly the same network size and parameters as in @xcite ( see fig .",
    "@xmath169 in their appendices ) , with parameter @xmath170 and @xmath171 denoting expected number of edges per node connecting nodes in the same group and in different groups , respectively . by comparing fig .",
    "[ fig : sbm_noise ] with fig .",
    "@xmath169 in appendices of @xcite we can see from x - laplacian works similarly to the semi - definite programming while bethe - hessian based method does not work at all .",
    "( _ left _ ) ; triangular stochastic block model ( _ right _ ) with average degree @xmath10 and @xmath172 which means half of edges belong to triangles rather than single edges  [ unpublished ] .",
    "all networks has @xmath173 nodes and @xmath11 groups . in @xmath38-axis",
    ", @xmath174 controls the hardness of the problem . each data point",
    "is averaged over @xmath113 realizations .",
    "[ fig : community_detection ] , title=\"fig : \" ]    ( _ left _ ) ; triangular stochastic block model ( _ right _ ) with average degree @xmath10 and @xmath172 which means half of edges belong to triangles rather than single edges  [ unpublished ] . all networks has @xmath173 nodes and @xmath11 groups . in @xmath38-axis",
    ", @xmath174 controls the hardness of the problem . each data point",
    "is averaged over @xmath113 realizations .",
    "[ fig : community_detection ] , title=\"fig : \" ]     x - laplacian using @xmath137 on networks generated by the perturbed stochastic block model @xcite , with exactly the same network size and parameters as in fig.@xmath169 of appendices in @xcite . @xmath170 and @xmath171 denote average degree connecting nodes in the same group and different groups respectively , and @xmath175 is the fraction of selected noisy nodes . each point",
    "is averaged over @xmath113 instances . ]",
    "[ [ additional - numerical - evaluations - on - spectral - clustering - using - pairwise - similarity - measurements ] ] additional numerical evaluations on spectral clustering using pairwise similarity measurements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    in this section we compare the performance of spectral algorithms using the data matrix , the bethe hessian and the x - laplacian , on the model recently proposed in  @xcite , which generates pairwise measurements between two groups of nodes from different probability distributions .",
    "two distributions @xmath109 and @xmath110 are chosen to be gaussian with unit variance and mean @xmath111 and @xmath112 respectively . on top of the network",
    "we add two different kinds of noise , i.e. cliques and hubs to the random graph topology . and",
    "from figures we can see that the results are qualitatively similar to right panel of fig .",
    "@xmath45 in the main text where x - laplacian outperforms both bethe hessian and x - laplacian in reconstructing the planted partition .",
    "together with @xmath90 size-@xmath113 cliques .",
    "( _ right _ ) : panel the topologies are random graphs with average degree @xmath0 together with @xmath90 hubs whose degrees are @xmath96 .",
    "each point in the figure is averaged over @xmath113 realizations of data set of size @xmath114 .",
    "[ fig : clustering ] , title=\"fig : \" ]   together with @xmath90 size-@xmath113 cliques .",
    "( _ right _ ) : panel the topologies are random graphs with average degree @xmath0 together with @xmath90 hubs whose degrees are @xmath96 .",
    "each point in the figure is averaged over @xmath113 realizations of data set of size @xmath114 .",
    "[ fig : clustering ] , title=\"fig : \" ]"
  ],
  "abstract_text": [
    "<S> spectral methods are popular in detecting global structures in the given data that can be represented as a matrix . </S>",
    "<S> however when the data matrix is sparse or noisy , classic spectral methods usually fail to work , due to localization of eigenvectors ( or singular vectors ) induced by the sparsity or noise . in this work , </S>",
    "<S> we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors . </S>",
    "<S> using matrix perturbation analysis , we demonstrate that the learned regularizations suppress down the eigenvalues associated with localized eigenvectors and enable us to recover the informative eigenvectors representing the global structure . </S>",
    "<S> we show applications of our method in several inference problems : community detection in networks , clustering from pairwise similarities , rank estimation and matrix completion problems . using extensive experiments , </S>",
    "<S> we illustrate that our method solves the localization problem and works down to the theoretical detectability limits in different kinds of synthetic data . </S>",
    "<S> this is in contrast with existing spectral algorithms based on data matrix , non - backtracking matrix , laplacians and those with rank - one regularizations , which perform poorly in the sparse case with noise . </S>"
  ]
}