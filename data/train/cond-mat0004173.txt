{
  "article_text": [
    "the perceptron which was first analyzed with statistical mechanics techniques in the seminal paper of gardner @xcite is by now a well - known and standard model in theoretical studies and practical applications in connection with learning and generalization @xcite .",
    "a number of extensions of the perceptron model have been formulated , including many - state and graded - response perceptrons ( e.g. , @xcite ) . here",
    "we present some new extensions allowing for so - called coloured or ashkin - teller type neurons , i.e. , different types of binary neurons at each site possibly having different functions .",
    "the idea of looking at such a model is based upon our recent work on ashkin - teller recurrent neural networks @xcite .",
    "there we showed that for this model with two types of binary neurons interacting through a four - neuron term and equipped with a hebb learning rule , both the thermodynamic and dynamic properties suggest that such a model can be more efficient than a sum of two hopfield models .",
    "for example , the quality of pattern retrieval is enhanced through a larger overlap at higher temperatures and the maximal capacity is increased . for more details and an underlying neurobiological motivation for the introduction of different types of neurons",
    "we refer to @xcite .    in the light of these results",
    "an interesting question is whether such a coloured perceptron can still be more efficient than the standard perceptron . in other words",
    ", can it have a larger maximal capacity than the one of a standard perceptron , which is known @xcite to be @xmath0 ( for random uncorrelated patterns ) .",
    "it has been suggested that this number is characteristic for all binary networks independent of the multiplicity of the neuron interactions .",
    "thereby , the capacity is defined as the thermodynamic limit of the ratio of the total number of bits per ( input ) neuron to be stored and the total number of couplings per ( output ) neuron @xcite .",
    "we remark that `` input '' and `` output '' refer specifically to the perceptron case .    in the sequel",
    "the maximal capacity of coloured perceptron models is studied using the gardner approach @xcite .",
    "first - step replica - symmetry - breaking effects are evaluated and the analytic results are compared with extensive numerical simulations using various learning algorithms .    the rest of this paper is organized as follows . in sec .",
    "ii we introduce two ashkin - teller type perceptron models .",
    "section iii contains the replica theory and determines the maximal capacity by calculating the available volume in the space of couplings both in the replica - symmetric ( sec .",
    "iiia ) and the first - step replica - symmetry - breaking approximation ( sec .",
    "section iv describes the results of numerical simulations with algorithms obtained by generalizing various algorithms for simple perceptrons . in sec .",
    "v we present our conclusions . finally , two appendices contain some technical details of the derivations .",
    "let us first formulate the coloured perceptron models .",
    "we consider @xmath1 input patterns @xmath2 consisting out of two different types of patterns @xmath3 and @xmath4 , and a corresponding set of outputs @xmath5 which are determined by @xmath6 where @xmath7 ( @xmath8 ) are the local fields acting on the patterns @xmath9 , @xmath10 and their product @xmath11 respectively @xmath12 both types of input patterns and their corresponding outputs are supposed to be independent identically distributed random variables ( iidrv ) taking the values @xmath13 or @xmath14 with probability @xmath15 .",
    "the set of three equations ( [ e.s1])-([e.s3 ] ) defines a mapping of the inputs @xmath16 onto the corresponding outputs @xmath17 .",
    "we call it model i. we remark that the specific form of the equations ( [ e.s1])-([e.s3 ] ) is related to the transition probabilities for a spin - flip in the dynamics @xcite . a second model , denoted by ii , is defined by considering only the two equations ( [ e.s1 ] ) and ( [ e.s2 ] ) . when @xmath18 and @xmath19 then the relations ( [ e.s1])-([e.s2 ] ) are satisfied by two ( out of the four possible ) values of the output @xmath20 , otherwise model ii gives the same output as model i. in other words , due to the presence of the @xmath21 and @xmath22 in the gain functions , model ii contains more freedom and , strictly speaking , it is not a mapping",
    ".    the sequential dynamics of these two models has been studied in the case of low loading with the hebb rule and shown to lead to the same equilibrium behaviour @xcite . however , this is not guaranteed here since we are concerned with optimal couplings maximizing the loading capacity .",
    "at this point we remark that when all @xmath23 are equal to zero we find back two independent standard binary perceptron models . in the sequel",
    "we take the couplings to satisfy the spherical constraint @xmath24 .",
    "the coloured perceptron is trained to store correctly @xmath25 patterns with @xmath26 the loading capacity .",
    "the factor @xmath27 follows naturally from the definition of capacity given in the introduction .",
    "a pattern is stored correctly when the so - called aligning field @xcite is bigger than a certain constant @xmath28 whereby the latter indicates the stability .",
    "it is a measure for the size of the basin of attraction of that pattern .",
    "specifically we require that @xmath29 with @xmath30 denoting the configurations in the space of interactions . for @xmath31",
    "all patterns that satisfy equations ( [ e.s1])-([e.s3 ] ) also satisfy ( [ eq : afieldxi])-([eq : afieldxieta ] ) .",
    "we remark that for model  ii the last inequality is superfluous .    ' '' ''    ' '' ''    the aim is then to determine the maximal value of the loading @xmath26 for which couplings satisfying ( [ eq : afieldxi])-([eq : afieldxieta ] ) can still be found .",
    "in particular , the question whether this model can be more efficient than the existing two - state models is relevant .    following refs .",
    "@xcite we formulate the problem as an energy minimization in the space of couplings with the formal energy function defined as @xmath32\\ , .",
    "\\label{eq : energyi}\\end{aligned}\\ ] ] we remark that for model  ii the third @xmath33-factor is absent .",
    "the quantity above counts the number of weakly embedded patterns , i.e. , the patterns with stability less than @xmath34 .",
    "therefore , the minimal energy gives the minimal number of patterns that are stored incorrectly .",
    "this number is zero below a maximal storage capacity @xmath35 .",
    "the basic quantity to start from is the partition function @xmath36 \\rangle_{\\{j\\}}\\\\                        \\langle ... \\rangle_{\\{j\\ } }       & = & \\int \\prod_i dj_i       \\prod _ r \\delta \\left(\\sum_i ( j_i^{(r)})^2 - n \\right) ... ~ ,    \\end{aligned}\\ ] ] with @xmath37 the inverse temperature . as usual",
    "it is @xmath38 which is assumed to be a self - averaging extensive quantity @xcite .",
    "the related free energy per site @xmath39 is equal , in the limit @xmath40 , to @xmath41\\rangle_{\\{j\\ } } } { nz(\\beta ) } \\ , ,      \\label{eq : flimit}\\ ] ] which is the minimal fraction of wrong patterns ( recall eq .",
    "( [ eq : energyi ] ) ) .    in order to perform the average over the disorder in the input patterns @xmath42 and the corresponding outputs @xmath43 we employ the replica method .",
    "the calculations proceed in a standard way although the technical details are much more complex . introducing the order parameters @xmath44 , with @xmath8 and @xmath45 we write following @xcite    @xmath46          \\int\\prod_{r ' } { \\rm d}x^{r'\\gamma }             \\exp\\left\\{\\sum_{r ' } ix^{r'\\gamma}\\lambda^{r'\\gamma }                     \\right.\\right.\\right .                \\nonumber\\\\       & - & \\frac{1}{2}\\left[\\left(x^{1\\gamma}+x^{3\\gamma}\\right)^2                  + \\left(x^{2\\gamma}+x^{3\\gamma}\\right)^2              + \\left(x^{1\\gamma}+x^{2\\gamma}\\right)^2\\right ]                 \\nonumber\\\\           & -&\\sum_{\\tau>\\gamma}\\left[\\left(x^{1\\gamma}+x^{3\\gamma}\\right )          \\left(x^{1\\tau}+x^{3\\tau}\\right)q_{\\gamma\\tau}^{(1 ) }",
    "+ \\left(x^{2\\gamma}+x^{3\\gamma}\\right )            \\left(x^{2\\tau}+x^{3\\tau}\\right)q_{\\gamma\\tau}^{(2)}\\right .",
    "+ \\left.\\left.\\left.\\left.\\left(x^{1\\gamma}+x^{2\\gamma}\\right )          \\left(x^{1\\tau}+x^{2\\tau}\\right)q_{\\gamma\\tau}^{(3 ) }        \\right ]        \\rule{0cm}{0.5cm}\\right\\ }        \\rule{0cm}{0.6cm}\\right )        \\rule{0cm}{0.7cm}\\right\\}\\nonumber",
    "\\\\        g_1&=&\\ln\\left\\{\\int\\prod_{r,\\gamma}\\left({\\rm d } j^{(r)\\gamma}\\right )     \\exp\\left[i\\sum_{r,\\gamma}\\epsilon_\\gamma^r((j^{(r)\\gamma})^2 - 1)-     i\\sum_{r,\\gamma,\\tau >",
    "\\gamma}\\phi_{\\gamma\\tau}^r j^{(r)\\gamma}j^{(r)\\tau }     \\right]\\right\\},\\nonumber\\end{aligned}\\ ] ]    2 where @xmath47 denotes the average over the patterns , @xmath48 for model  i and @xmath49 for model  ii . because of the latter we remark that for model  ii the formula for @xmath50 can be simplified : the integrals with respect to @xmath51 and @xmath52 are not present and thus @xmath52 , @xmath53 and @xmath51 have to be set to zero .",
    "because of this simplification we only outline explicitly the calculations for model  ii in the sequel .",
    "the corresponding formulas for model  i can be found in appendix  b.      we continue by making the replica - symmetric ( rs ) anzatz @xmath54 .",
    "moreover , for convenience , we set @xmath55 . the latter is justified for model  i because of the symmetry present in this model .",
    "furthermore , since we are going to take all @xmath56 in the gardner - derrida analysis anyway , we keep this equality also for model  ii .",
    "taking then the limits @xmath40 , @xmath57 and @xmath58 we arrive , in the case of model  ii , at @xmath59 with @xmath60 where @xmath61 , @xmath62 is a modified gaussian measure , @xmath63 and @xmath64 takes those values that minimize @xmath65 , the available    ' '' ''    ' '' ''    volume in the space of couplings . for the corresponding expression in the case of model",
    "i we refer to appendix  b.    taking @xmath66 and supposing that the maximal capacity , @xmath67 , is signaled by the gardner - like criterion @xmath68 we obtain @xmath69 this maximal capacity as a function of @xmath70 is shown for both models in figs .",
    "[ res1 ] and [ res2 ] as a full line . for model",
    "i we obtain , e.g. , @xmath71 , a value that is smaller than the gardner capacity for the simple perceptron . for model  ii however , we get the interesting result that @xmath72 .",
    "it is straightforward to show geometrically that learning almost antiparallel patterns , i.e. , patterns satisfying @xmath73 results in a splitting of the space of couplings into disconnected regions .",
    "this suggests that rs is broken and , consequently , the results for @xmath74 found in sec .",
    "iiia are only upperbounds for the true capacity .",
    "therefore , we want to improve the rs results by applying the first step of parisi s replica - symmetry - breaking ( rsb ) scheme ( e.g. , @xcite ) .",
    "so , we assume that the @xmath75 in equation ( [ zpn ] ) have the following matrix block structure @xmath76 where @xmath77 is the size of the matrix @xmath75 , @xmath78 is the number of diagonal blocks and int(@xmath79 ) denotes the integer part of @xmath79 .    for model",
    "ii we take @xmath80 reflecting the symmetry of this model . for model",
    "i we repeat that all @xmath81 s can be taken equal .",
    "we then consider the limits @xmath82 and @xmath58 in such a way that @xmath83 , with @xmath84 , remains finite . after a tedious calculation",
    "we arrive at the following expression for the rsb1 maximal capacity for model  ii    @xmath85    2 with @xmath86 and d@xmath87 a gaussian measure .",
    "the explicit form of the function @xmath88 can be found in appendix  a. an analogous form for model  i is written down in appendix  b.    the results are presented in figs .",
    "[ res1 ] and [ res2 ] as full lines .",
    "as expected they lie below the rs results confirming the breaking of rs , e.g. , @xmath89 for model  i and @xmath90 for model  ii .",
    "we remark that the breaking for model  ii is stronger than for model  i , the reason being that model  ii allows more freedom as explained in the introduction .",
    "finally , on the basis of results in the literature for the simple perceptron @xcite , @xcite we expect that the rsb1 results are very close to the exact ones .",
    "this is further examined by performing numerical simulations as described in the following section .",
    "the idea of these simulations is to train the network with a certain learning algorithm in order to learn as many random patterns as possible .",
    "the main technical difficulties are to find an efficient algorithm and prove its convergence .",
    "we have tried to generalize various algorithms proposed for simple perceptrons @xcite .",
    "the most effective ones appeared to be some particular generalization of the adaptive gardner algorithm @xcite and the adatron algorithm @xcite . in the sequel",
    "we only report on the results obtained with these two algorithms .",
    "we remark that we have chosen @xmath91 in all simulations .",
    "one of the algorithms that has demonstrated its efficiency and for which convergence has been shown in the case of the standard perceptron is given in ref .",
    "it is an adaptive version of the original algorithm proposed by gardner @xcite . using heuristic arguments presented in @xcite we have constructed for the coloured perceptron model  ii the following analogous learning rule @xmath92          \\,.\\end{aligned}\\ ] ]    ' '' ''    ' '' ''    the form of the algorithm for model",
    "i is a bit different and given in appendix  b. this algorithm should be carried out sequentially over the patterns and sequentially or parallel over the couplings as long as one of the arguments of the @xmath33 functions is positive .",
    "it appears to have the characteristics of the most efficient , non - linear algorithm discussed in @xcite .    using this learning rule",
    "we have trained networks of sizes @xmath93 sites ( depending on the value of @xmath70 ) in order to store perfectly as many randomly chosen patterns as possible . for each value of @xmath70",
    "we have calculated the maximal capacity for different @xmath94 and extrapolated the results to @xmath95 .",
    "results for a given value of @xmath70 and @xmath94 are averages over 1000 samples . as shown in figs .",
    "[ res1 ] and [ res2 ] this algorithm performs especially well for small values of @xmath70 for both the models i and ii .",
    "the second algorithm we report on is the adatron algorithm @xcite which works in a different way . instead of searching the maximal capacity for a given stability it tries to find the maximal stability for a given capacity .",
    "the derivation of this algorithm and a proof of its convergence are based upon the assumption that the problem can be formulated as a quadratic optimization with linear constraints @xcite .",
    "such a formulation can not be given for the coloured perceptron model , because the three different types of couplings have to be normalized independently and because the stability conditions ( [ eq : afieldxi])-([eq : afieldeta ] ) are more complex .",
    "hence , a straightforward generalization similar to the one for the potts model @xcite is not possible .",
    "below we describe a learning rule that tries to incorporate the ideas of the adatron approach .",
    "we assume that the couplings can be written in the form ( cfr.,@xcite and references therein ) @xmath96 where @xmath97 ( @xmath8 ) are the so called embedding strengths of pattern @xmath98 .",
    "then , in the case of model  ii the couplings are updated by modifying @xmath97 with the following increments @xmath99 this is done sequentially over the patterns .",
    "we remark that again the algorithm for model  i is somewhat different ( see appendix  b ) . for each value of the capacity we have considered system sizes @xmath100 and extrapolated the results to @xmath95 .",
    "the best results were obtained for a learning rate @xmath101 .",
    "results for each size are averages over 1000 samples . for small values of the capacity",
    "the algorithm gives better results , both in the case of models i and ii than the first algorithm we have discussed , as shown in figs .",
    "[ res1 ] and [ res2 ] . for larger values of the capacity , however , it performs worse .",
    "the results for the adatron algorithm are displayed only in the region where they are better than the results for the gardner algorithm .",
    "we remark that the numerical simulations with the different algorithms give different results and that we have not shown their convergence analytically such that , in principle , the values for @xmath102 obtained here are lower bounds .    looking at figs .",
    "[ res1 ] and [ res2 ] in more detail we see that for the whole range of @xmath70 the values of the maximal capacity in model ii are larger than those of a standard binary perceptron . for @xmath103 ,",
    "e.g. , the simulations give @xmath104 , which is bigger than the maximal capacity of the binary perceptron model @xcite and the binary many - neuron interaction model @xcite , both of which have @xmath0 . for model",
    "i the maximal capacity at @xmath103 found by simulations is @xmath105 .",
    "in this work we have calculated the maximal capacity per number of couplings for two coloured perceptron models . compared with the standard perceptron these models have two neuronal variables per site and a local field that contains higher order neuron terms .",
    "the    ' '' ''    ' '' ''     +   + method used is a generalization of the gardner approach and both the rs and rsb1 results have been discussed .",
    "we expect that the latter give very close upperbounds for the exact values .",
    "extensive numerical simulations have been performed for finite systems and extrapolated to @xmath95 . the adaptive gardner algorithm and the adatron algorithm",
    "give the best , but different results .",
    "hence , the results of the simulations can be considered only as lower bounds for the exact maximal capacity .",
    "additional work looking for improved algorithms would be welcome .",
    "comparing both the rsb1 results and the results from numerical simulations we conclude that they are in good agreement . for bigger values of @xmath70",
    "they even completely coincide . for model",
    "i we find that at @xmath103 the maximal capacity satisfies @xmath106 .",
    "this suggests that it is equal to the maximal capacity of the @xmath107-potts perceptron , i.e. , @xmath108 ( after appropriate rescaling of the latter @xcite ) .",
    "this would parallel the situation for hebb learning @xcite . for model",
    "ii we have for @xmath103 that @xmath109 , which is larger than the maximal capacity of the standard binary perceptron .",
    "this is due to the fact that model  ii is not a strict mapping such that it allows for more freedom in the determination of the couplings .",
    "the authors would like to thank m.  bouten and j. van mourik for critical discussions .",
    "the function @xmath88 in formula ( [ alpharsb1 ] ) reads    @xmath110                      \\nonumber\\\\   & + & \\frac{1}{2c_1}e^{\\varepsilon_2 }         \\int_{-\\infty}^{\\frac{c1}{c}(u_1-\\delta_2 ) }       { \\rm d}s\\left[1+{\\rm erf}\\left(\\sqrt{\\frac{3r}{2c^2 } }          \\left(-\\frac{x_2}{\\sqrt{3r}}+\\delta_2          + \\frac{c}{c_1}s\\right)\\right)\\right ]                     \\nonumber\\\\    & + & \\frac{1}{2c_2}e^{\\phi_2 }        \\int_{-\\infty}^{-\\frac{c2}{c}(u_1-\\gamma_2 ) }       { \\rm d}s\\left[1+{\\rm erf}\\left(\\sqrt{\\frac{3r}{2c^2 } }          \\left(\\frac{x_2}{\\sqrt{3r}}-\\gamma_2        + \\frac{c}{c_2}s\\right)\\right)\\right ]                         \\nonumber\\\\    & + & \\frac{1}{2c_2}e^{\\phi_3}\\int_{-\\infty}^{-\\frac{c2}{c}(u_1-\\gamma_3 ) }       { \\rm d}s\\left[1+{\\rm erf}\\left(\\sqrt{\\frac{3r}{2c^2 } }         \\left(-\\frac{x_3}{\\sqrt{3r}}-\\gamma_3        + \\frac{c}{c_2}s\\right)\\right)\\right ]                           \\nonumber\\\\     & + & \\frac{1}{2c'}e^{d_1}\\int_{-\\infty}^{-\\frac{u_1}{c ' } }       { \\rm d}s\\left[{\\rm erf}\\left(\\sqrt{\\frac{3r}{2 } }          \\left(\\frac{x_3}{\\sqrt{3r}}-b_1-\\frac{1}{c'}s\\right)\\right )       + { \\rm erf}\\left(\\sqrt{\\frac{3r}{2}}\\left(-\\frac{x_2}{\\sqrt{3r}}-b_1-            \\frac{1}{c'}s\\right)\\right)\\right ]                        \\nonumber\\\\     & + & \\frac{1}{2}\\int_{-\\infty}^{u_1 }       { \\rm d}s\\left[{\\rm erf}\\left(\\sqrt{\\frac{3r}{2 } }        \\left(\\frac{x_2}{\\sqrt{3r}}-s\\right)\\right )       + { \\rm erf}\\left(-\\sqrt{\\frac{3r}{2}}\\left(\\frac{x_3}{\\sqrt{3r } }                + s\\right)\\right)\\right]\\nonumber\\end{aligned}\\ ] ]    2 with d@xmath111 a gaussian measure and @xmath112    ' '' ''    ' '' ''    @xmath113",
    "for model i the calculations are very similar . some resulting expressions , however , have a somewhat different structure . for completeness we write down these expressions here .                    finally , the learning algorithms for model  i differ in the way that the couplings @xmath122 and @xmath123 are updated .",
    "we have for the adaptive gardner algorithm @xmath124                 \\nonumber\\\\    j_i^{(2)}&\\rightarrow&j_i^{(2)}+\\eta_0^{\\mu}\\eta_i^{\\mu}\\frac{1}{2 }              \\left[\\left(\\kappa_\\eta-\\lambda_\\eta^\\mu\\right)\\right .",
    "\\theta\\left(\\kappa_\\eta-\\lambda_\\eta^\\mu\\right )               \\nonumber\\\\                  & + & \\left(\\kappa_{\\xi\\eta}-\\lambda_{\\xi\\eta}^\\mu\\right)\\left .",
    "\\theta\\left(\\kappa_{\\xi\\eta}-\\lambda_{\\xi\\eta}^\\mu\\right)\\right ]        \\nonumber\\end{aligned}\\ ] ] instead of ( [ j1per ] ) and ( [ j2per ] ) and for the adatron algorithm we take @xmath125 instead of ( [ x1ada ] ) and ( [ x2ada ] ) .    99 e. gardner j. , phys .",
    "a * 21 * , 257 ( 1988 ) .",
    "j. hertz , a. krogh and r. g. palmer , _ introduction to the theory of neural computation _",
    "( addison - wesley , redwood city 1991 ) .",
    "b. mller , j. reinhardt and m. t. strickland , _ neural networks : an introduction _",
    "( springer , berlin 1995 ) .",
    "m. opper and w. kinzel , _ models of neural networks iii _ ed .",
    "e. domany , j. l. van hemmen and k. schulten , 151 ( springer , new - york 1996 ) .",
    "d. saad , ed._on - line learning in neural networks _",
    "( cambridge university press 1998 ) .",
    "j. p. nadal and a. rau , j. phys .",
    "i france * 1 * , 1109 ( 1991 ) . f. gerl and u. krey , j. phys .",
    "a * 27 * , 7353 ( 1994 ) .",
    "d. a. khring , j. phys .",
    "france * 51 * , 145 ( 1990 ) .",
    "s. mertens , h. m. khler and s. bs , j. phys .",
    "a * 24 * , 4941 ( 1991 ) .",
    "d. boll , p. dupont and j. van mourik , europhys . lett . * 15 * , 893 ( 1991 ) .",
    "d. boll , r. khn and j. van mourik , j. phys .",
    "a * 26 * , 3149 ( 1993 ) .",
    "d. boll and p. kozowski , j. phys .",
    "a * 31 * , 6319 ( 1998 ) .",
    "d. boll and p. kozowski , j. phys .",
    "a * 32 * , 8577 ( 1999 ) .",
    "e. gardner and b. derrida , j. phys . a * 21 * , 271 ( 1988 ) .",
    "w. whyte and d. sherrington , j. phys .",
    "a * 29 * , 3063 ( 1996 ) .",
    "d. boll and r. erichsen , jr phys .",
    "e * 59 * , 3386 ( 1999 ) .",
    "w. krauth and m. mzard , j. phys .",
    "a * 20 * , l745 ( 1987 ) .",
    "l. f. abbott and t. b. kepler , j. phys .",
    "a * 22 * , l711 ( 1989 ) .",
    "j. k. anlauf and m. biehl , europhys",
    ". lett . * 10 * , 687 ( 1989 ) .",
    "j. imhoff , j. phys .",
    "a * 28 * , 2173 ( 1995 ) .",
    "m. mzard , g. parisi and m. a. virasoro , _ spin glass theory and beyond _",
    "( world scientific , singapore 1987 ) .",
    "p. kozowski , phd thesis k.u.leuven , belgium , in preparation ."
  ],
  "abstract_text": [
    "<S> ashkin - teller type perceptron models are introduced . </S>",
    "<S> their maximal capacity per number of couplings is calculated within a first - step replica - symmetry - breaking gardner approach . </S>",
    "<S> the results are compared with extensive numerical simulations using several algorithms .    2 </S>"
  ]
}