{
  "article_text": [
    "the _ nearest neighbor _ problem ( ) is a fundamental geometric problem which has major applications in many areas such as databases , computer vision , pattern recognition , information retrieval , and many others . given a set @xmath9 of @xmath0 points in a @xmath10-dimensional space , the goal is to build a data - structure , such that given a query point @xmath3 , the algorithm can report the closest point in @xmath9 to the query @xmath3 .",
    "a particularly interesting and well - studied instance is when the points live in a @xmath10-dimensional real vector space @xmath11 . efficient",
    "exact and approximate algorithms are known for this problem .",
    "( in the _ approximate nearest neighbor _ ( ) problem , the algorithm is allowed to report a point whose distance to the query is larger by at most a factor of @xmath12 , than the real distance to the nearest point . )",
    "see @xcite , the surveys @xcite , @xcite and @xcite , and references therein ( of course , this list is by no means exhaustive ) .",
    "one of the state of the art algorithms for , based on locality sensitive hashing , finds the @xmath12-with query time @xmath13 , and preprocessing / space @xmath14 in @xmath15 , where @xmath16 @xcite , where @xmath17 hides a constant that is polynomial in @xmath18 . for the @xmath19 norm , this improves to @xmath20 @xcite .    despite the ubiquity of nearest neighbor methods ,",
    "the vast majority of current algorithms suffer from significant limitations when applied to data sets with corrupt , noisy , irrelevant or incomplete data .",
    "this is unfortunate since in the real world , rarely one can acquire data without some noise embedded in it .",
    "this could be because the data is based on real world measurements , which are inherently noisy , or the data describe complicated entities and properties that might be irrelevant for the task at hand .    in this paper",
    ", we address this issue by formulating and solving a variant of the nearest neighbor problem that allows for some data coordinates to be arbitrarily corrupt . given a parameter @xmath2 ,",
    "the * _ @xmath2-robust nearest neighbor _ * for a query point @xmath3 , is a point @xmath4 whose distance to the query point is minimized ignoring `` the optimal '' set of @xmath2-coordinates ( the term ` robust ' is used as an analogy to _ robust pca _",
    "that is , the @xmath2 coordinates are chosen so that deleting these coordinates , from both @xmath5 and @xmath3 minimizes the distance between them . in other words ,",
    "the problem is to solve the problem in a different space ( which is definitely not a metric ) , where the distance between any two points is computed ignoring the worst @xmath2 coordinates . to the best of our knowledge ,",
    "this is the first paper considering this formulation of the robust problem .",
    "this problem has natural applications in various fields such as computer vision , information retrieval , etc . in these applications",
    ", the value of some of the coordinates ( either in the dataset points or the query point ) might be either corrupted , unknown , or simply irrelevant . in computer vision ,",
    "examples include image de - noising where some percent of the pixels are corrupted , or image retrieval under partial occlusion ( e.g. see @xcite ) , where some part of the query or the dataset image has been occluded",
    ". in these applications there exists a perfect match for the query after we ignore some dimensions . also , in medical data and recommender systems , due to incomplete data @xcite , not all the features ( coordinates ) are known for all the people / recommendations ( points ) , and moreover , the set of known values differ for each point .",
    "hence , the goal is to find the perfect match for the query ignoring some of those features .    for the binary hypercube , under the hamming distance",
    ", the @xmath2-robust nearest neighbor problem is equivalent to the _ near neighbor _ problem .",
    "the near neighbor problem is the decision version of the search , where a radius @xmath6 is given to the data structure in advance , and the goal is to report a point that is within distance @xmath6 of the query point .",
    "indeed , there exists a point @xmath5 within distance @xmath6 of the query point @xmath3 if and only if @xmath6 coordinates can be ignored such that the distance between @xmath5 and @xmath3 is zero .",
    "[ [ budgeted - version . ] ] budgeted version . + + + + + + + + + + + + + + + + +    we also consider the weighted generalization of the problem where the amount of uncertainty varies for each feature . in this model ,",
    "each coordinate is assigned a weight @xmath21 in advance , which tries to capture the certainty level about the value of the coordinate ( @xmath22 indicates that the value of the coordinate is correct and @xmath23 indicates that it can not be trusted ) .",
    "the goal is to ignore a set of coordinates @xmath24 of total weight at most @xmath25 , and find a point @xmath4 , such that the distance of the query to the point @xmath5 is minimized ignoring the coordinates in @xmath24 .",
    "surprisingly , even computing the distance between two points under this measure is np - complete(it is almost an instance of ) .",
    "we present the following new results :    reduction from robust to .",
    "we present a general reduction from the robust problem to the `` standard '' problem .",
    "this results in a bi - criterion constant factor approximation , with sublinear query time , for the @xmath2-robust nearest neighbor problem .    for @xmath15-norm",
    "the result can be stated as follows .",
    "if there exists a point @xmath26 whose distance to the query point @xmath3 is at most @xmath6 by ignoring @xmath2 coordinates , the new algorithm would report a point @xmath5 whose distance to the query point is at most @xmath27 , ignoring @xmath28 coordinates .",
    "the query algorithm performs @xmath29 queries in @xmath30-data structures , where @xmath31 is a prespecified parameter .    in , we present the above result in the somewhat more general settings of the @xmath32 norm",
    "the algorithm reports a point whose distance is within @xmath33 after ignoring @xmath34 coordinates while performing @xmath35 of @xmath36-queries .",
    "we modify the new algorithm to report a point whose distance to the query point is within @xmath37 by ignoring @xmath38 coordinates while performing @xmath39 queries ( specifically , @xmath40- ) . for the sake of simplicity of exposition , we present this extension only in the @xmath15 norm .",
    "see for details .    budgeted version . in",
    ", we generalize our algorithm for the weighted case of the problem .",
    "if there exists a point within distance @xmath6 of the query point by ignoring a set of coordinates of weight at most @xmath25 , then our algorithm would report a point whose distance to the query is at most @xmath7 by ignoring a set of coordinates of weight at most @xmath41 .",
    "again , for the sake of simplicity of exposition , we present this extension only in the @xmath15 norm .    data sensitive lsh queries .",
    "it is a well known phenomenon in proximity search ( e.g. see andoni _",
    "et  al . _",
    "* section 4.5 ) ) , that many data - structures perform dramatically better than their theoretical analysis . not only that , but also they find the real nearest neighbor early in the search process , and then spend quite a bit of time on proving that this point is indeed a good .",
    "it is natural then to ask whether one can modify proximity search data - structures to take an advantage of such a behavior .",
    "that is , if the query is easy , the data - structure should answer quickly ( and maybe even provide the exact nearest neighbor in such a case ) , but if the instance is hard , then the data - structure works significantly harder to answer the query .    as an application of our sampling approach",
    ", we show a data - sensitive algorithm for for the binary hypercube case under the hamming distance .",
    "the new algorithm solves the approximate near neighbor problem , in time @xmath42 , where @xmath43 is the smallest value with @xmath44 where @xmath45 is the distance of the query @xmath3 to the @xmath46thpoint @xmath47 , and @xmath6 is the distance being tested .",
    "we also get that such queries works quickly on low dimensional data , see for details .",
    "the new algorithms are clean and should be practical .",
    "moreover , our results for the @xmath2-robust hold for a wide range of the parameter @xmath2 , from @xmath41 to @xmath48 .",
    "there has been a large body of research focused on adapting widely used methods for high - dimensional data processing to make them applicable to corrupt or irrelevant data .",
    "for example , robust pca @xcite is an adaptation of the pca algorithm that handles a limited amount of adversarial errors in the input matrix .",
    "although similar in spirit , those approaches follow a different technical development than the one in this paper .    also , similar approaches to robustness has been used in theoretical works . in the work of indyk on @xmath32 sketching @xcite , the distance between two points @xmath49 and @xmath50 , is defined to be the median of @xmath51 .",
    "thus , it is required to compute the @xmath52 norm of their distance , but only over the smallest @xmath53 coordinates .",
    "finally , several generalizations of the problem have been considered in the literature .",
    "two related generalizations are the nearest @xmath2-flat search and the approximate nearest @xmath2-flat . in the former , the dataset is a set of @xmath2-flats ( @xmath2-dimensional affine subspace ) instead of simple points but the query is still a point ( see @xcite for example ) . in the latter",
    "however , the dataset consists of a set of points but the query is now a @xmath2-flat ( see for example @xcite ) .",
    "we note that our problem can not be solved using these variations ( at least naively ) since the set of coordinates that are being ignored in our problem are not specified in advance and varies for each query .",
    "this would mean that @xmath54 different subspaces are to be considered for each point . in our settings",
    ", both @xmath10 and @xmath2 can be quite large , and the new data - structures have polynomial dependency in both parameters .    [ [ data - sensitive- . ] ] data sensitive .",
    "+ + + + + + + + + + + + + + + +    the fast query time for low dimensional data was demonstrated before for an scheme ( * ? ? ?",
    "* appendix a ) ( in our case , this is an easy consequence of our data - structure ) .",
    "similarly , optimizing the parameters of the construction to the hardness of the data / queries was done before ( * ? ? ?",
    "* section 4.3.1 )  our result however does this on the fly for the query , depending on the query itself , instead of doing this fine tuning of the whole data - structure in advance for all queries .      by definition of the problem",
    ", we can not directly apply johnson - lindenstrauss lemma to reduce the dimensions ( in the @xmath19 norm case ) .",
    "intuitively , dimension reduction has the reverse effect of what we want  it spreads the mass of a coordinate `` uniformly '' in the projection s coordinates  thus contaminating all projected coordinates with noise from the `` bad '' coordinates .",
    "the basic idea of our approach is to generate a set of random projections , such that all of these random projections map far points to far points ( from the query point ) , and at least one of them projects a close point to a close point .",
    "thus , doing queries in each of these projected point sets , generates a set of candidate points , one of them is the desired robust .",
    "our basic approach is based on a simple sampling scheme , similar to the clarkson - shor technique @xcite and @xcite .",
    "the projection matrices we use are _ probing _ matrices .",
    "every row contains a single non - zero entry , thus every row copies one original coordinate , and potentially scales it up by some constant .",
    "[ [ a - sketch - of - the - technique ] ] a sketch of the technique : + + + + + + + + + + + + + + + + + + + + + + + + + +    consider the case where we allow to drop @xmath2 coordinates .",
    "for a given query point @xmath3 , it has a robust nearest neighbor @xmath55 , such that there is a set @xmath24 of @xmath2 `` bad '' coordinates , such that the distance between @xmath3 and @xmath26 is minimum if we ignore the @xmath2 coordinates of @xmath24 ( and this is minimum among all such choices ) .",
    "we generate a projection matrix by picking the @xmath46thcoordinate to be present with probability @xmath56 , where @xmath57 is some constant , for @xmath58 . clearly , the probability that such a projection matrix avoids picking the @xmath2 bad coordinates is @xmath59 . in particular ,",
    "if we repeat this process @xmath60 times , where @xmath61 is some constant , then the resulting projection avoids picking any bad coordinate with probability @xmath62 . on the other hand , imagine a `` bad '' point @xmath4 , such that one has to remove , say , @xmath63 coordinates before the distance of the point to the query @xmath3 is closer than the robust @xmath26 ( when ignoring only @xmath2 coordinates ) .",
    "furthermore , imagine the case where picking any of these coordinates is fatal  the value in each one of these bad coordinates is so large , that choosing any of these bad coordinates results in this bad point being mapped to a far away point .",
    "then , the probability that the projection fails to select any of these bad coordinates is going to be roughly @xmath64 namely , somewhat informally , with decent probability all bad points get mapped to faraway points , and the near point gets mapped to a nearby point .",
    "thus , with probability roughly @xmath65 , doing a regular query on the projected points , would return the desired .",
    "as such , repeating this embedding @xmath66 times , and returning the best encountered would return the desired robust with high probability .",
    "[ [ the - good - the - bad - and - the - truncated . ] ] the good , the bad , and the truncated .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    ultimately , our technique works by probing the coordinates , trying to detect the `` hidden '' mass of the distance of a bad point from the query .",
    "the mass of such a distance might be concentrated in few coordinates ( say , a point has @xmath67 coordinates with huge value in them , but all other coordinates are equal to the query point )  such a point is arguably still relatively good , since ignoring slightly more than the threshold @xmath2 coordinates results in a point that is pretty close by .    on the other hand , a point where one has to ignore a large number of coordinates ( say @xmath68 ) before it becomes reasonably close to the query point is clearly bad in the sense of robustness . as such",
    ", our data - structure would classify points , where one has to ignore slightly more than @xmath2 coordinates to get a small distance , as being close .    to capture this intuition ,",
    "we want to bound the influence a single coordinate has on the overall distance between the query and a point .",
    "to this end , if the robust nearest neighbor distance , to @xmath3 when ignoring @xmath2 coordinates , is @xmath6 , then we consider capping the contribution of every coordinate , in the distance computation , by a certain value , roughly , @xmath69 . under this _ truncation _ , our data - structure returns a point that is @xmath7 away from the query point , where @xmath6 is the distance to the @xmath2-robust point .",
    "thus , our algorithm can be viewed as a bicriterion approximation algorithm - it returns a point where one might have to ignore slightly more coordinates than @xmath2 , but the resulting point is constant approximation to the nearest - neighbor when ignoring @xmath2 points .",
    "in particular , a point that is still bad after such an aggressive truncation , is amenable to the above random probing . by carefully analyzing the variance of the resulting projections for such points",
    ", we can prove that such points would be rejected by the data - structure on the projected points .",
    "[ [ budgeted - version.-1 ] ] budgeted version",
    ". + + + + + + + + + + + + + + + + +    to solve the budgeted version of the problem we use a similar technique to importance sampling .",
    "if the weight of a coordinate is @xmath70 , then in the projection matrix , we sample the @xmath46thcoordinate with probability @xmath71 and scale it by a factor of @xmath72 .",
    "this would ensure that the set of  bad \" coordinates are not sampled with probability @xmath73 .",
    "again we repeat it @xmath60 times to get the desired bounds .    [",
    "[ data - sensitive-.-1 ] ] data sensitive .",
    "+ + + + + + + + + + + + + + + +    the idea behind the data - sensitive , is that can be interpreted as an estimator of the local density of the point set . in particular , if the data set is sparse near the query point , not only the data - structure would hit the nearest - neighbor point quickly ( assuming we are working in the right resolution ) , but furthermore , the density estimation would tell us that this event happened . as such , we can do the regular exponential search  start with an insensitive scheme ( that is fast ) , and go on to use more sensitive s , till the density estimation part tells us that we are done .",
    "of course , if all fails , the last data - structure used is essentially the old scheme .",
    "[ def : tail ] for a point @xmath74 , let @xmath75 be a permutation of @xmath76 , such that @xmath77 for a parameter @xmath78 , the * _ @xmath46-tail _ * of @xmath5 is the point",
    "@xmath79%          = %          \\bigl ( 0 , \\ldots , 0 , |\\pntc_{\\pi(i+1 ) } | , \\allowbreak          |\\pntc_{\\pi(i+2 ) } | , \\ldots , |\\pntc_{\\pi(d ) } | \\bigr ) .",
    "$ ] note , that given @xmath74 and @xmath46 , computing @xmath80 can be done in @xmath48 time , by median selection .",
    "thus , given two points @xmath81 , their distance ( in the @xmath32-norm ) , ignoring the @xmath2 worst coordinates ( which we believe to be noise ) , is @xmath82 . here , we are interested in computing the nearest neighbor when one is allowed to ignore @xmath2 coordinates . formally , we have the following :    [ def : k : fold : n : n ] for parameters @xmath83 , @xmath2 , a set of points @xmath84 , and a query point @xmath85 , the * _ @xmath2-robust nearest - neighbor _ * to @xmath3 in @xmath9 is @xmath86 which can be computed , naively , in @xmath87 time .",
    "[ def : r : m : c ] for a point @xmath74 and a set of coordinates @xmath88 , we define @xmath89 to be a point in @xmath90 which is obtained from @xmath5 by deleting the coordinates that are in @xmath91 .",
    "[ def : set : proj ] consider a sequence @xmath92 of @xmath93 , not necessarily distinct , integers @xmath94 , where @xmath95 .",
    "for a point @xmath96 , its * _ projection _ * by @xmath92 , denoted by @xmath97 , is the point @xmath98 .",
    "similarly , the _ projection _ of a point set @xmath84 by @xmath92 is the point set @xmath99 . given a weight vector @xmath100",
    "the * _ weighted projection _ * by a sequence @xmath92 of a point @xmath5 is @xmath101 .",
    "note , that can interpret @xmath92 as matrix with dimensions @xmath102 , where the @xmath103throw is zero everywhere except for having @xmath104 at the @xmath105thcoordinate ( or @xmath25 in the unweighted case ) , for @xmath106 .",
    "this is a restricted type of a projection matrix .",
    "[ def : t : splay ] given a probability @xmath107 , a natural way to create a projection , as defined above , is to include the @xmath46thcoordinate , for @xmath58 , with probability @xmath108 .",
    "let @xmath109 denote the distribution of such projections",
    ".    given two sequences @xmath110 and @xmath111 , let @xmath112 denote the _ concatenated _ sequence @xmath113 let @xmath114 denote the distribution resulting from concatenating @xmath115 such independent sequences sampled from @xmath109 .",
    "( i.e. , we get a random projection matrix , which is the result of concatenating @xmath116 independent projections . )",
    "observe that for a point @xmath74 and a projection @xmath117 , the projected point @xmath118 might be higher dimensional than the original point @xmath5 as it might contain repeated coordinates of the original point .",
    "[ rem : compress ] consider a projection @xmath117 that was generated by the above process ( for either the weighted or unweighted case ) .",
    "note , that since we do not care about the order of the projected coordinates , one can encode @xmath119 by counting for each coordinate @xmath120 , how many time it is being projected . as such , even if the range dimension of @xmath119 is larger than @xmath10 , one can compute the projection of a point in @xmath48 time .",
    "one can also compute the distance between two such projected points in @xmath48 times .",
    "[ sec : sec : lp : case ] in this section , we present an algorithm for approximating the @xmath2-robust nearest neighbor under the @xmath32-norm , where @xmath83 is some prespecified fixed constant ( say @xmath25 or @xmath30 ) . as usual in such applications , we approximate the @xmath83thpower of the @xmath32-norm , which is a sum of @xmath83thpowers of the coordinates .      [",
    "sec : sec : lp : alg ]    [ [ input . ] ] input .",
    "+ + + + + +    the input is a set @xmath9 of @xmath0 points in @xmath11 , and a parameter @xmath2 .",
    "furthermore , we assume we have access to a data - structure that answer ( regular ) @xmath121-queries efficiently , where @xmath122 is a quality parameter associated with these data - structures .    [ [ preprocessing . ] ] preprocessing .",
    "+ + + + + + + + + + + + + +    let @xmath123 be three constants to be specified shortly , such that @xmath124 .",
    "we set @xmath125 , @xmath126 , and @xmath127 .",
    "we randomly and independently pick @xmath128 sequences @xmath129 next , the algorithm computes the point sets @xmath130 , for @xmath131 , and preprocesses each one of them for @xmath121-approximate nearest - neighbor queries for the @xmath32-norm ( @xmath132 ) , using a standard data - structure for that supports this .",
    "let @xmath133 denote the resulting data - structure for @xmath134 , for @xmath131 ( for example we can use the data structure of indyk and motwani @xcite for the @xmath135 cases ) .",
    "[ [ query . ] ] query .",
    "+ + + + + +    given a query point @xmath85 , for @xmath136 , the algorithm computes the point @xmath137 , and its @xmath138 in @xmath134 using the data - structure @xmath133 .",
    "each computed point @xmath139 corresponds to an original point @xmath47 .",
    "the algorithm returns the @xmath2-robust nearest neighbor to @xmath3 ( under the @xmath32-norm ) among @xmath140 via direct calculation .",
    "[ def : truncate ] for a point @xmath96 , and a threshold @xmath141 , let @xmath142 be the * _ @xmath143-truncated _ * point , where @xmath144 , for @xmath145 . in words",
    ", we max out every coordinate of @xmath5 by the threshold @xmath143 .",
    "as such , the _ @xmath143-truncated @xmath32-norm _ of @xmath146 is @xmath147    for parameters @xmath143 and @xmath6 , a point @xmath5 is * _ @xmath148-light _ * if @xmath149 . similarly , for a parameter @xmath150 , a point is * _ @xmath151-heavy _ * if @xmath152 .",
    "intuitively a light point can have only a few large coordinates .",
    "the following lemma shows that being light implies a small tail .",
    "[ lemma : lp : light ] for a number @xmath6 , if a point @xmath74 is @xmath153-light , then @xmath154 .",
    "let @xmath155 , and let @xmath50 be the number of truncated coordinates in @xmath156 .",
    "if @xmath157 then @xmath158 { y \\threshold^\\p } %          > %          \\sqrt[\\p ] { k \\threshold^\\p } %          = %          k^{1/\\p } \\threshold%          = %          \\rr ,      $ ] which is a contradiction . as such , all the non - zero coordinates of @xmath159 are present in @xmath156 , and we have that @xmath160      [ lemma : basic : exp : var ] let @xmath161 be a point in @xmath11 , and consider a random @xmath162 , see .",
    "we have that @xmath163 } } = { { { \\tau}}}{\\left\\| { { { { \\bm{x } } } } } \\right\\|_{{{{\\rho}}}}}^{{{{\\rho}}}}$ ] , and @xmath164    let @xmath165 be a random variable that is @xmath166 with probability @xmath108 and @xmath167 otherwise .",
    "for @xmath168 , we have that @xmath169 as for the variance , we have @xmath170 as such , we have @xmath171    [ lemma : lp : t : heavy ] let @xmath5 be a point in @xmath11 , and let @xmath141 be a number .",
    "we have that @xmath172 .",
    "consider the @xmath143-truncated point @xmath173 , see .",
    "each coordinate of @xmath174 is smaller than @xmath143 , and thus @xmath175    [ lemma : lp : heavy : block ] consider a sequence @xmath176 . if @xmath5 is a @xmath177-heavy point and @xmath178 , then @xmath179 where @xmath155 .",
    "consider the @xmath143-truncated point @xmath173 .",
    "since @xmath5 is @xmath180-heavy , we have that @xmath181 .",
    "now , setting @xmath182 , and using , we have @xmath183 and @xmath184 by .",
    "now , we have that @xmath185 as such , by chebyshev s inequality , and since @xmath125 , if @xmath178 , we have @xmath186 } }       \\leq       { \\mathop{\\mathbf{pr}}{\\mleft [ { { \\left| { \\bigl.z - \\mu } \\right| } \\geq        \\frac{\\mu/2}{\\sigma } \\sigma } \\mright ] } }       \\leq       { \\mleft({\\frac{\\sigma}{\\mu/2}}\\mright)}^2                   \\leq             \\frac { { { { { \\tau}}}(1-{{{\\tau } } } )   { { { \\psi}}}^{{{\\rho}}}{\\left\\| { { { { \\bm{v } } } } } \\right\\|_{{{{\\rho}}}}}^{{{{\\rho } } } }             } } { { { { \\tau}}}^2 { \\left\\| { { { { \\bm{v } } } } } \\right\\|_{{{{\\rho}}}}}^{2{{{\\rho}}}}/4 }             \\leq                                                 { \\frac { 4 { { { \\psi}}}^{{{\\rho}}}}{{{{\\tau}}}{\\left\\| { { { { \\bm{v } } } } } \\right\\|_{{{{\\rho}}}}}^{{{{\\rho } } } } } }             \\leq             4 { \\frac { { { { \\alpha}}}k { { { r}}}^{{{{\\rho } } } } } { k { { { r}}}^{{{{\\rho } } } } } }             \\leq             \\frac{1}{2}.      \\end{aligned}\\ ] ]    [ lemma : success ] let @xmath5 be a prespecified point .",
    "the probability that a sequence @xmath119 sampled from @xmath114 does not sample any of the @xmath2 heaviest coordinates of @xmath5 is @xmath187 ( for the simplicity of exposition , in the following , we use this rougher estimate ) .",
    "let @xmath188 be the set of @xmath2 indices of the coordinates of @xmath5 that are largest ( in absolute value ) .",
    "the probability that @xmath189 does not contain any of these coordinates is @xmath190 , and overall this probability is @xmath191 now , we have @xmath192 since , for any integer @xmath193 , we have @xmath194 , and @xmath195 , for @xmath196 .    [",
    "lemma : lp : light : good ] consider a point @xmath5 such that @xmath197 ( see ) .",
    "conditioned on the event of , we have that @xmath198 } } \\leq 1/2 $ ] , where @xmath117 .    by for @xmath199",
    ", we have @xmath200 } } = { { { t}}}{{{\\tau}}}{\\left\\| { \\smash { { { { { { { \\bm{x}}}}^{}_{\\bbslash k } } } } }      } \\right\\|_{{{{\\rho}}}}}^{{{\\rho}}}\\leq { { { t}}}{{{\\tau}}}{{{r}}}^{{{\\rho}}}$ ] .",
    "the desired probability is @xmath201 } } \\leq 1/2 $ ] , which holds by markov s inequality .",
    "[ lemma : lp : heavy : far ] let @xmath178 . if @xmath5 is a @xmath177-heavy point , then @xmath202    let @xmath155 and @xmath173 , and for all @xmath46 , let @xmath203 .",
    "by , with probability at least half , we have that @xmath204 in particular , let @xmath205 , and observe that @xmath206 thus , we have that @xmath207 now set @xmath208 and note that @xmath209 .",
    "now , by hoeffding s inequality , we have that @xmath210 } }       & \\leq         { \\mathop{\\mathbf{pr}}{\\mleft [ { \\bigl .",
    "z \\leq { { { { u } } } } } \\mright ] } }         \\leq         { \\mathop{\\mathbf{pr}}{\\mleft [ { \\bigl .",
    "{ \\left| { z - \\mu } \\right| } \\geq \\mu - { { { { u } } } } } \\mright ] } }         \\leq         2 \\exp{\\mleft ( { - \\frac{2(\\mu - { { { { u}}}})^2 } { { { { t}}}{\\mleft ( { { { { \\tau}}}{{{r}}}^{{{{\\rho}}}}/ 2}\\mright)}^2}}\\mright ) }       \\\\ &           \\leq           2 \\exp{\\mleft ( { - \\frac{8 ( \\mu/2)^2 } { { { { t}}}{{{\\tau}}}^2 { { { r}}}^{2{{{\\rho}}}}}}\\mright ) }           \\leq           2 \\exp{\\mleft ( { - \\frac{8 ( { { { t}}}{{{\\tau}}}{{{r}}}^{{{{\\rho}}}}/ 8)^{2 } } { { { { t}}}{{{\\tau}}}^2 { { { r}}}^{2{{{\\rho}}}}}}\\mright ) }           =           2 \\exp{\\mleft ( { - \\frac { { { { \\beta}}}\\ln n } { 8 } } \\mright ) }           \\leq           \\frac{2}{n^{{{{\\beta}}}/8}}.      \\end{aligned}\\ ] ]      [ lemma : lp : heavy : tail ] let @xmath124 be a parameter .",
    "one can build the data - structure described in with the following guarantees . for a query point @xmath85 ,",
    "let @xmath55 be its @xmath2-robust nearest neighbor in @xmath9 under the @xmath32 norm , and let @xmath211 .",
    "then , with high probability , the query algorithm returns a point @xmath212 , such that @xmath213 is a @xmath214-light .",
    "the data - structure performs @xmath215 of @xmath121-queries under @xmath32-norm .",
    "we start with the painful tedium of binding the parameters . for the bad probability , bounded by , to be smaller than @xmath216",
    ", we set @xmath217 . for the good probability @xmath218 of to be larger than @xmath219 , implies @xmath220 , thus requiring @xmath221 .",
    "namely , we set @xmath222 . finally , requires @xmath223 let @xmath224 and let @xmath225 .    for a query point @xmath3 ,",
    "let @xmath26 be its @xmath2-robust , and let @xmath188 be the set of @xmath2 largest coordinates in @xmath226 .",
    "let @xmath227 denote the event of sampling a projection @xmath228 that does not contain any of the coordinates of @xmath188 . by , with probability @xmath229",
    ", the event @xmath227 happens for the data - structure @xmath133 , for any @xmath46 .",
    "as such , since the number of such data - structures built is @xmath230 we have that , by chernoff inequality , with high probability , that there are at least @xmath231 such data structures , say @xmath232 .",
    "consider such a data - structure @xmath133 .",
    "the idea is now to ignore the coordinates of @xmath188 all together , and in particular , for a point @xmath74 , let @xmath233 be the point where the @xmath2 coordinates of @xmath188 are removed ( as defined in ) . since by assumption @xmath234 , by , with probability at least half ,",
    "the distance of @xmath235 from @xmath236 is at most @xmath237 .",
    "since there are @xmath238 such data - structures , we know that , with high probability , in one of them , say @xmath239 , this holds . by , any point @xmath240 ( of @xmath9 ) , that is @xmath241-heavy , would be in distance at least @xmath242 in the projection @xmath243 from the projected @xmath3 . since @xmath239 is a @xmath121-data - structure under the @xmath244 norm , we conclude that no such point can be returned , because the distance from @xmath3 to @xmath26 in this data - structure is smaller than @xmath245 . note that since for the reported point @xmath174 , the point @xmath246 can not be @xmath247-heavy , and that the coordinates in @xmath188 can contribute at most @xmath248 . we conclude that the point @xmath174 can not be @xmath249-heavy .",
    "thus , the data - structure returns the desired point with high probability .",
    "as for the query performance , the data - structure performs @xmath128 queries in @xmath121-data - structures .",
    "this lemma would translate to the following theorem using .",
    "[ theo : l : p ] let @xmath84 be a set of @xmath0 points with the underlying distance being the @xmath32 metric , and @xmath250 , @xmath124 , and @xmath132 be parameters .",
    "one can build a data - structure for answering the @xmath2-robust queries on @xmath9 , with the following guarantees :    preprocessing time / space is equal to the space / time needed to store @xmath251 data - structures for performing @xmath121-queries under the @xmath32 metric , for a set of @xmath0 points in @xmath252 dimensions .    the query time is dominated by the time it takes to perform @xmath121-queries in the @xmath253 data - structures .    for a query point @xmath3 , the data - structure returns , with high probability , a point @xmath212 , such that if one ignores @xmath254 coordinates , then the @xmath32 distance between @xmath3 and @xmath174 is at most @xmath255 where @xmath6 is the distance of the nearest neighbor to @xmath3 when ignoring @xmath2 coordinates .",
    "( formally , @xmath213 is @xmath256-light . )",
    "setting @xmath257 , the algorithm would report a point @xmath174 using @xmath258-data - structures , such that if one ignores @xmath259 coordinates , the @xmath32 distance between @xmath3 and @xmath174 is at most @xmath260 .",
    "formally , @xmath213 is @xmath261 -light .",
    "[ sec : sec : budgeted ]      in this section , we consider the budgeted version of the problem for @xmath15-norm . here",
    ", a coordinate @xmath46 has a cost @xmath262 of ignoring it , and we have a budget of @xmath25 , of picking the coordinates to ignore ( note that since we can safely remove all coordinates of cost @xmath263 , we can assume that @xmath264 ) .",
    "formally , we have a vector of * _ costs _ * @xmath265 , where the @xmath46thcoordinate , @xmath266 , is the cost of ignoring this coordinate .",
    "intuitively , the cost of a coordinate shows how much we are certain that the value of the coordinate is correct .",
    "the set of * _ admissible projections _ * , is @xmath267 given two points @xmath268 , their * _ admissible distance _ * is @xmath269 where we interpret @xmath91 as a projection ( see ) .    the problem is to find for a query point @xmath3 and a set of points @xmath9 , both in @xmath11 , the _ robust nearest - neighbor distance _ to @xmath3 ; that is , @xmath270 the point in @xmath9 realizing this distance is the * _ robust nearest - neighbor _ * to @xmath3 , denoted by @xmath271 the unweighted version can be interpreted as solving the problem for the case where all the coordinates have uniform cost @xmath272 .",
    "[ def : good : bad ] if @xmath26 is the nearest - neighbor to @xmath3 under the above measure , then the set of _ good _ coordinates is @xmath273 and the set of * _ bad _ * coordinates is @xmath274    in what follows , we modify the algorithm for the unweighted case and analyze its performance for the budgeted case .",
    "interestingly , the problem is significantly harder .      for two points , computing",
    "their distance is a special instance of .",
    "the problem is np - hard(which is well known ) , as testified by the following lemma .",
    "[ lemma:2:points ] given two points @xmath81 , and a cost vector @xmath275 , computing @xmath276 is np - complete , where @xmath277 is the set of admissible projections for @xmath275 ( see ) .",
    "this is well known , and we provide the proof for the sake of completeness .",
    "consider an instance of with integer numbers @xmath278 .",
    "let @xmath279 , and consider the point @xmath280 , and set the cost vector to be @xmath281 .",
    "observe that @xmath282 .",
    "in particular , there is a point in robust distance at most @xmath25 from the origin , with the total cost of the omitted coordinates being @xmath25 @xmath283 the given instance of has a solution .",
    "indeed , consider the set of coordinates @xmath91 realizing @xmath284 .",
    "let @xmath285 , and observe that the cost of the omitted coordinates @xmath286 is at most @xmath25 ( by the definition of the admissible set @xmath277 ) .",
    "in particular , we have @xmath287 and @xmath288 . as such",
    ", the minimum possible value of @xmath93 is @xmath25 , and if it is @xmath25 , then @xmath289 , and @xmath91 and @xmath24 realize the desired partition .",
    "adapting the standard for subset - sum for this problem , readily gives the following .",
    "[ lemma : i - aamkp ] given points @xmath81 , and a cost vector @xmath290^d$ ] , one can compute a set @xmath291 , such that @xmath292 the running time of this algorithm is @xmath293 .",
    "given a vector @xmath290^d$ ] , consider generating a sequence @xmath92 of integers @xmath294 , by picking the number @xmath120 , into the sequence , with probability @xmath266 .",
    "we interpret this sequence , as in , as a projection , except that we further scale the @xmath103thcoordinate , by a factor of @xmath295 , for @xmath296 .",
    "namely , we project the @xmath46thcoordinate with probability @xmath297 , and if so , we scale it up by a factor of @xmath298 , for @xmath58 ( naturally , coordinates with @xmath299 would never be picked , and thus would never be scaled ) .",
    "let @xmath300 denote this distribution of weighted sequences ( maybe a more natural interpolation is that this is a distribution of projection matrices ) .",
    "[ observation : norm:1 ] let @xmath301^d$ ] be a cost vector with non zero entries .",
    "for any point @xmath74 , and a random @xmath302 , we have that @xmath303      the input is a point set @xmath9 of @xmath0 points in @xmath11 . let @xmath304 be the vector of costs , and @xmath123 be three constants to be specified shortly , such that @xmath124 .",
    "let @xmath126 , and @xmath127 .",
    "[ [ preprocessing.-1 ] ] preprocessing .",
    "+ + + + + + + + + + + + + +    we use the same algorithm as before .",
    "we sample @xmath128 sequences @xmath305 then we embed the point set using these projections , setting @xmath130 , for @xmath131 .",
    "next , we preprocess the point set @xmath134 for @xmath30-queries under the @xmath15-norm , and let @xmath133 be the resulting data - structure , for @xmath136 .    [ [ answering - a - query . ] ] answering a query .",
    "+ + + + + + + + + + + + + + + + + +    given a query point @xmath3 , the algorithm performs a @xmath30-query for @xmath236 in @xmath133 , this corresponds to some original point @xmath47 , for @xmath131 .",
    "the algorithm then @xmath12-approximate the distance @xmath306 , for @xmath131 , using the algorithm of , and returns the point realizing the minimum distance as the desired .",
    "[ lemma : budget : success ] for a query point @xmath3 , let @xmath307 be the bad coordinates of @xmath3 ( thus @xmath308 ) .",
    "then the probability that @xmath309 misses all the coordinates of @xmath24 is at least @xmath310 .",
    "let @xmath311 be the bad coordinates in @xmath24 .",
    "we have that @xmath312 . as such ,",
    "the probability that @xmath313 fails to sample a coordinate in @xmath24 is @xmath314 as @xmath315 , and @xmath316 , for @xmath196 . as such",
    ", the probability that a sequence @xmath317 avoids @xmath24 is at least @xmath318 .",
    "let @xmath319 and let @xmath320 be the distance from @xmath3 to its nearest neighbor only considering the coordinates in @xmath321 .",
    "[ lemma : e : v : w ] for a point @xmath74 , and a random @xmath309 , we have that @xmath322 and @xmath323    the claim on the expectation follows readily from . as for the variance ,",
    "let @xmath46 be a coordinate that has non - zero cost , and let @xmath165 be a random variable that is @xmath324 with probability @xmath325 , and @xmath167 otherwise .",
    "we have that @xmath326 as such for @xmath327 , we have that @xmath328 and thus the lemma follows .    as before , we want to avoid giving too much weight to a single coordinate which might have a huge ( noisy ) value in it . as such",
    ", we truncate coordinates that are too large . here",
    ", things become somewhat more subtle , as we have to take into account the probability of a coordinate to be picked .    for a cost vector @xmath329 , a positive number @xmath330 , and a point @xmath74 , let @xmath331 be the * _ truncated _ * point , where @xmath332 for @xmath58 .",
    "the truncation seems a bit strange on the first look , but note that a coordinate @xmath46 that has a cost @xmath266 approaching @xmath167 , is going to be truncated to zero by the above .",
    "furthermore , it ensures that a `` heavy '' point would have a relatively small variance in the norm under the projections we use , as testified by the following easy lemma .",
    "[ lemma : stupid ] consider a point @xmath74 , and a random @xmath333 .",
    "for , @xmath334 , consider the random variable @xmath335 .",
    "we have that @xmath336 @xmath337 and @xmath338    the bound on the expectation is by definition . as for the variance , by and ,",
    "we have @xmath339    let @xmath340}}$ ] , and @xmath341}}$ ] . by chebyshev s inequality , we have that @xmath342 } }       & \\geq         1 - { \\mathop{\\mathbf{pr}}{\\mleft [ { { \\left| { \\bigl.{\\left\\| { { { { { \\mathpzc{m}}}}}{{{\\bm{x } } } } ' } \\right\\|_{1 } } - \\mu } \\right| } \\geq          \\frac{\\mu/2}{\\sigma } \\sigma } \\mright ] } }         \\geq         1 - { \\mleft({\\frac{\\sigma}{\\mu/2}}\\mright)}^2         =         1 - 4 \\frac{{\\mathop{\\mathbf{v}}{\\mleft [ { x } \\mright]}}}{\\mu^2 } .",
    "\\end{aligned}\\ ] ] by the above , we have that @xmath343    [ def : light : heavy ] for a value @xmath344 , a point @xmath5 is * _ @xmath345-light _ * ( resp .  * _ heavy _ * ) if @xmath346 ( resp .",
    "@xmath347 ) , where @xmath348 .",
    "[ lemma : stupid2 ] consider a point @xmath74 that is @xmath349-heavy , for @xmath350 , and a random @xmath309 .",
    "then , we have that @xmath351    let @xmath352 .",
    "let @xmath353 , and let @xmath354 be an indicator variable that is one if @xmath355 . we have , by , that @xmath356 } }        & =         { \\mathop{\\mathbf{pr}}{\\mleft [ { \\bigl .",
    "x_i \\geq \\frac{{{{r}}}}{2 } } \\mright ] } }         \\geq          { \\mathop{\\mathbf{pr}}{\\mleft [ { \\bigl .   { \\left\\| { { { { { \\mathpzc{m}}}}}_i { { { \\bm{x } } } } ' } \\right\\|_{1 } } \\geq \\frac{{{{r}}}}{2 }          } \\mright ] } }         \\geq         { \\mathop{\\mathbf{pr}}{\\mleft [ { \\bigl .   { \\left\\| { { { { { \\mathpzc{m}}}}}_i { { { \\bm{x } } } } ' } \\right\\|_{1 } } \\geq          \\frac{{\\mathop{\\mathbf{e}}{\\mleft [ { { \\left\\| { { { { { \\mathpzc{m}}}}}_i { { { \\bm{x } } } } ' } \\right\\|_{1 } } } \\mright]}}}{2 } } \\mright ] } }         \\geq         1 - \\frac{4 { { { r } } } } { { \\left\\| { { { { \\bm{x } } } } ' } \\right\\|_{1 } } }       \\\\ &           =           1 - \\frac{4 { { { r } } } } { { { { r } } } }           \\geq           \\frac{1}{2 } ,      \\end{aligned}\\ ] ] as @xmath357 . by chernoff inequality",
    ", we have that @xmath358 since @xmath126 .    in particular , we have that @xmath359 and as such @xmath360 } }        \\geq       { \\mathop{\\mathbf{pr}}{\\mleft [ { \\sum\\nolimits_{i=1}^{{{t}}}y_i \\geq { { { t}}}/4 } \\mright ] } }       \\geq       1 - { 1}/{n^{\\beta/2}}.      \\end{aligned}\\ ] ]      [ theo : main : budgeted ] let @xmath9 be a point set in @xmath11 , let @xmath290^d$ ] be the cost of the coordinates , and let @xmath361 be a parameter .",
    "one can build a data - structure , such that given a query point @xmath3 , it can report a robust approximate nearest - neighbor under the costs of @xmath275 . formally , if @xmath362 is the robust nearest - neighbor ( see ) when one is allowed to drop coordinates of total cost @xmath25 , and its distance to this point is @xmath363 ( see ) , then the data - structure returns a point @xmath5 , such that @xmath364 is @xmath365-light ( see ) .",
    "the data - structure has the following guarantees :    the preprocessing time and space is @xmath366 , where @xmath367 is the preprocessing time and space needed to build a single data - structure for answering ( standard ) @xmath30-queries in the @xmath15-norm for @xmath0 points in @xmath10 dimensions .",
    "the query time is @xmath368 , where @xmath369 is the query time of answering @xmath30-queries in the above data - structures .",
    "the proof is similar to in the unweighted case .",
    "we set @xmath370 , @xmath371 , and @xmath372 . by the same arguments as the unweighted case , and using , , and markov s inequality , with high probability ,",
    "there exists a data - structure @xmath133 that does not sample any of the bad coordinates @xmath373 , and that @xmath374 . by and union bound , for all the points @xmath5 such that @xmath375 ( see ) is @xmath376-heavy",
    ", we have @xmath377 .",
    "thus by no such point would be retrieved by @xmath133 .",
    "note that since for the reported point @xmath5 , we have that @xmath375 is @xmath376-light , and that @xmath378 the point @xmath5 is @xmath379-light .",
    "using implies an additional @xmath12 blowup in the computed distance , implying the result .    under the conditions and notations of , for the query point @xmath3 and its returned point @xmath5",
    ", there exists a set of coordinates @xmath88 of cost at most @xmath41 ( i.e. , @xmath380 such that @xmath381 .",
    "that is , we can remove a set of coordinates of cost at most @xmath41 such that the distance of the reported point @xmath5 from the query @xmath3 is at most @xmath7 .",
    "let @xmath382 and by @xmath174 is @xmath383-light for some constant @xmath384 .",
    "let @xmath385 and let @xmath91 be the set of coordinates being truncated ( i.e. , all @xmath46 such that @xmath386 ) .",
    "clearly , the weight of the coordinates not being truncated is at most @xmath387 .",
    "also for the coordinates in the set @xmath91 , we have that @xmath388 . therefore , @xmath389 assuming that @xmath390 , and noting that @xmath391 .",
    "[ sec : d : s : l ]    given a set of @xmath0 points @xmath9 and a radius parameter @xmath6 , in the approximate near neighbor problem , one has to build a data - structure , such that for any given query point @xmath3 , if there exists a point in @xmath9 that is within distance @xmath6 of @xmath3 , it reports a point from @xmath9 which is within distance @xmath37 of the query point .",
    "in what follows we present a data structure based on our sampling technique whose performance depends on the relative distances of the query from all the points in the data - set .",
    "[ [ input.-1 ] ] input .",
    "+ + + + + +    the input is a set @xmath9 of @xmath0 points in the hamming space @xmath392 , a radius parameter @xmath6 , and an approximation parameter @xmath393 .    in the spirit of",
    ", one can generate the projections in this case directly . specifically , for any value of @xmath46 , consider a random projection @xmath394 two points @xmath395 * _ collide _ * under @xmath119 , if @xmath396 , for @xmath397 .",
    "since we care only about collisions ( and not distances ) for the projected points , we only care what subset of the coordinates are being copied by this projection .",
    "that is , we can interpret this projection as being the projection @xmath398 , which can be sampled directly from @xmath109 , where @xmath399 .",
    "as such , computing @xmath119 and storing it takes @xmath48 time .",
    "furthermore , for a point @xmath400 , computing @xmath118 takes @xmath48 time , for any projection @xmath401 .",
    "[ [ preprocessing.-2 ] ] preprocessing .",
    "+ + + + + + + + + + + + + +    for @xmath402 , let @xmath403 where @xmath404 is a sufficiently large constant . here",
    ", @xmath405 is the _ collision probability function _ of two points at distance @xmath93 under projection @xmath406 , and @xmath407 is the number times one has to repeat an experiment with success probability @xmath408 till it succeeds with high probability . let @xmath409 .    for @xmath410 ,",
    "compute a set @xmath411 of projections .",
    "for each projection @xmath412 , we compute the set @xmath413 and store it in a hash table dedicated to the projection @xmath92 . thus , given a query point @xmath414 ,",
    "the set of points _ colliding _ with @xmath3 is the set @xmath415 stored as a linked list , with a single entry in the hash table of @xmath92 .",
    "given @xmath3 , one can extract , using the hash table , in @xmath416 time , the list representing @xmath417 .",
    "more importantly , in @xmath48 time , one can retrieve the size of this list ; that is , the number @xmath418 . for any @xmath419 ,",
    "let @xmath420 denote the constructed data - structure .",
    "[ [ query.-1 ] ] query .",
    "+ + + + + +    given a query point @xmath414 , the algorithm starts with @xmath421 , and computes , the number of points colliding with it in @xmath422 .",
    "formally , this is the number @xmath423 if @xmath424 , the algorithm increases @xmath46 , and continues to the next iteration , where @xmath425 is any constant strictly larger than @xmath426 .",
    "otherwise , @xmath427 and the algorithm extracts from the @xmath407 hash tables ( for the projections of @xmath422 ) the lists of these @xmath165 points , scans them , and returns the closest point encountered in these lists .    the only remaining situation is when the algorithm had reached the last data - structure for @xmath428 without success .",
    "the algorithm then extracts the collision lists as before , and it scans the lists , stopping as soon as a point of distance @xmath429 had been encountered . in this case",
    ", the scanning has to be somewhat more careful ",
    "the algorithm breaks the set of projections of @xmath428 into @xmath430 blocks @xmath431 , each containing @xmath432 projections , see .",
    "the algorithm computes the total size of the collision lists for each block , separately , and sort the blocks in increasing order by the number of their collisions .",
    "the algorithm now scans the collision lists of the blocks in this order , with the same stopping condition as above .",
    "there are various modifications one can do to the above algorithm to improve its performance in practice .",
    "for example , when the algorithm retrieves the length of a collision list , it can also retrieve some random element in this list , and compute its distance to @xmath3 , and if this distance is smaller than @xmath433 , the algorithm can terminate and return this point as the desired approximate near - neighbor . however , the advantage of the variant presented above , is that there are many scenarios where it would return the _ exact _ nearest - neighbor .",
    "see below for details .        the expected number of collisions with the query point @xmath3 , for a single @xmath412 , is @xmath434 } }   =   \\sum_{{{{\\bm{x}}}}\\in { \\ensuremath{{{p}}}\\xspace } } { { { \\mathsf{f}}}_{i}}{\\mleft({\\bigl .",
    "{ \\mathrm{d}_h{\\mleft({{{{\\bm{q } } } } , { { { \\bm{x}}}}}\\mright)}}}\\mright ) }    \\leq   \\sum_{{{{\\bm{x}}}}\\in { \\ensuremath{{{p}}}\\xspace } } \\exp { \\mleft ( { \\bigl . - { \\mathrm{d}_h{\\mleft({{{{\\bm{q } } } } , { { { \\bm{x}}}}}\\mright ) } }",
    "i /{{{r}}}}\\mright)}.    { \\label{equation : coll}}\\end{aligned}\\ ] ] this quantity can be interpreted as a convolution over the point set .",
    "observe that as @xmath435 is a monotonically decreasing function of @xmath46 ( for a fixed @xmath93 ) , we have that @xmath436 .",
    "the expected number of collisions with @xmath3 , for all the projections of @xmath422 , is @xmath437 } }   =   { { { \\mathsf{s}}}{\\mleft({i}\\mright ) } }   { { { \\gamma}}_{i}}.      { \\label{equation : collisions}}\\ ] ] if we were to be naive , and just scan the lists in the @xmath46thlevel , the query time would be @xmath438 . as such , if @xmath439 , then we are `` happy '' since the query time is small . of course , a priori it is not clear whether @xmath165 ( or , more specifically , @xmath440 ) is small .",
    "intuitively , the higher the value @xmath46 is , the stronger the data - structure `` pushes '' points away from the query point .",
    "if we are lucky , and the nearest neighbor point is close , and the other points are far , then we would need to push very little , to get @xmath165 which is relatively small , and get a fast query time .",
    "the standard analysis works according to the worst case scenario , where one ends up in the last layer @xmath428 .",
    "[ example : low : dim ] the quantity @xmath440 depends on how the data looks like near the query . for example , assume that locally near the query point , the point set looks like a uniform low dimensional point set . specifically , assume that the number of points in distance @xmath93 from the query is bounded by @xmath441 , where @xmath2 is some small constant and @xmath6 is the distance of the nearest - neighbor to @xmath3 .",
    "we then have that @xmath442 by setting @xmath443 , we have @xmath444 therefore , the algorithm would stop in expectation after @xmath8 rounds .",
    "namely , if the data near the query point locally behaves like a low dimensional uniform set , then the expected query time is going to be @xmath445 , where the constant depends on the data - dimension @xmath2 .",
    "[ lemma : fast ] if there exists a point within distance @xmath6 of the query point , then the algorithm would compute , with high probability , a point @xmath5 which is within distance @xmath433 of the query point .",
    "let @xmath55 be the nearest neighbor to the query @xmath3 .",
    "for any data - structure @xmath420 , the probability that @xmath26 does not collide with @xmath3 is at most @xmath446 since the algorithm ultimately stops in one of these data - structures , and scans all the points colliding with the query point , this implies that the algorithm , with high probability , returns a point that is in distance @xmath447 .",
    "an interesting consequence of is that if the data - structure stops before it arrives to @xmath448 , then it returns the _ exact _ nearest - neighbor  since the data - structure accepts approximation only in the last level . stating it somewhat differently , only if the data - structure gets overwhelmed with collisions it returns an approximate answer .",
    "[ rem : m : large ] one can duplicate the coordinates @xmath449 times , such that the original distance @xmath6 becomes @xmath450 .",
    "in particular , this can be simulated on the data - structure directly without effecting the performance . as such , in the following , it is safe to assume that @xmath6 is a sufficiently large  say larger than @xmath451 .",
    "[ lemma : n : i ] for any @xmath452 , we have @xmath453 .",
    "we have that @xmath454 since for any positive integer @xmath193 , we have @xmath194 . as such , since we can assume that @xmath455 , we have that @xmath456 now , we have @xmath457    [ lemma : worst : case ] for a query point @xmath3 , the worst case query time is @xmath458 , with high probability .",
    "the worst query time is realized when the data - structure scans the points colliding under the functions of @xmath428 .",
    "we partition @xmath9 into two point sets :    the close points are @xmath459 , and    the far points are @xmath460 .",
    "any collision with a point of @xmath461 during the scan terminates the algorithm execution , and is thus a _ good _ collision .",
    "collision is when the colliding point belongs to @xmath462 .",
    "let @xmath463 be the partition of the projections of @xmath428 into blocks used by the algorithm .",
    "for any @xmath103 , we have @xmath464 since @xmath465 and by .",
    "such a block , has probability of @xmath466 to not have the nearest - neighbor to @xmath3 ( i.e. , @xmath26 ) in its collision lists .",
    "if this event happens , we refer to the block as being _",
    "useless_.    for a block @xmath467 , let @xmath468 be the total size of the collision lists of @xmath3 for the projections of @xmath467 when ignoring good collisions altogether .",
    "we have that @xmath469 } }       & \\leq         { \\left| { b_j } \\right| } \\cdot { \\left| { { { \\ensuremath{{{p}}}\\xspace } _ { > } } } \\right| } \\cdot           { { { \\mathsf{f}}}_{{{{\\mathcal{n}}}}}}{\\mleft({\\bigl .",
    "( 1+{{{\\varepsilon } } } ) { { { r}}}}\\mright ) }         \\leq         { \\left| { b_j } \\right| }   n   \\exp{\\mleft ( { - ( 1+{{{\\varepsilon } } } ) { { { r}}}{{{\\mathcal{n}}}}/{{{r}}}}\\mright ) }          =               \\\\ &                  { \\left| { b_j } \\right|}n \\exp{\\mleft ( { - ( 1+{{{\\varepsilon } } } ) { { { \\mathcal{n}}}}}\\mright ) }           =           { \\left| { b_j } \\right| }           =           o{\\mleft({n^{1/(1+{{{\\varepsilon}}})}}\\mright ) } ,           \\end{aligned}\\ ] ] since @xmath465 .",
    "in particular , the @xmath103thblock is _ heavy _ , if @xmath470 .",
    "the probability for a block to be heavy , is @xmath471 , by markov s inequality .",
    "in particular , the probability that a block is heavy or useless , is at most @xmath472 . as such , with high probability , there is a light and useful block .",
    "since the algorithm scans the blocks by their collision lists size , it follows that with high probability , the algorithm scans only light blocks before it stops the scanning , which is caused by getting to a point that belongs to @xmath461 . as such , the query time of the algorithm is @xmath473 .",
    "next , we analyze the data - dependent running time of the algorithm .",
    "let @xmath474 , for @xmath475 , where @xmath476 .",
    "let @xmath43 be the smallest value such that @xmath477 .",
    "then , the expected query time of the algorithm is @xmath478 .",
    "the above condition implies that @xmath479 , for any @xmath480 . by , for @xmath480 , we have that @xmath481 thus , by markov s inequality , with probability at least @xmath482 , we have that @xmath483 , and the algorithm would terminate in this iteration . as such , let @xmath468 be an indicator variable that is one of the algorithm reached the @xmath103thiteration .",
    "however , for that to happen , the algorithm has to fail in iterations @xmath484 . as such",
    ", we have that @xmath485    the @xmath103thiteration of the algorithm , if it happens , takes @xmath486 time , and as such , the overall expected running time is proportional to @xmath487 namely , the expected running time is bounded by @xmath488 using the bound @xmath489 from , and since @xmath490 .",
    "[ theo : l : s : h : sensitive ] given a set @xmath491 of @xmath0 points , and parameters @xmath361 and @xmath6 , one can preprocess the point set , in @xmath492 time and space , such that given a query point @xmath414 , one can decide ( approximately ) if @xmath493 , in @xmath494 expected time , where @xmath495 is the hamming distance .",
    "formally , the data - structure returns , either :    `` @xmath496 '' , and the data - structure returns a witness @xmath4 , such that @xmath497 .",
    "this is the result returned if @xmath498 .",
    "`` @xmath499 '' , and this is the result returned if @xmath499 .    the data - structure is allowed to return either answer if @xmath500 .",
    "the query returns the correct answer , with high probability .    furthermore ,",
    "if the query is `` easy '' , the data - structure would return the _ exact _ nearest neighbor . specifically ,",
    "if @xmath493 , and there exists @xmath501 , such that @xmath502 , then the data - structure would return the exact nearest - neighbor in @xmath503 expected time .",
    "[ rem : l : dim ] if the data is @xmath2 dimensional , in the sense of having bounded growth ( see ) , then the above data - structure solves approximate in @xmath445 time , where the constant hidden in the @xmath504 depends ( exponentially ) on the data dimension @xmath2 .",
    "this result is known , see datar _",
    "et  al . _",
    "* appendix a ) . however , our data - structure is more general , as it handles this case with no modification , while the data - structure of datar _",
    "et  al._is specialized for this case .",
    "[ rem : f : tune ] fine tuning the scheme to the hardness of the given data is not a new idea . in particular ,",
    "et  al . _",
    "* section 4.3.1 ) suggest fine tuning the construction parameters for the set of queries , to optimize the overall query time .",
    "contrast this with the new data - structure of , which , conceptually , adapts the parameters on the fly during the query process , depending on how hard the query is .",
    "ultimately , our data - structure is a prisoner of our underlying technique of sampling coordinates .",
    "thus , the main challenge is to come up with a different approach that does not necessarily rely on such an idea . in particular",
    ", our current technique does not work well for points that are sparse , and have only few non - zero coordinates .",
    "we believe that this problem should provide fertile ground for further research .    [ [ acknowledgments . ] ] acknowledgments .",
    "+ + + + + + + + + + + + + + + +    the authors thank piotr indyk for insightful discussions about the problem and also for the helpful comments on the presentation of this paper .",
    "the authors also thank jen gong , stefanie jegelka , and amin sadeghi for useful discussions on the applications of this problem .",
    "clmw11    n.  ailon and b.  chazelle .",
    "approximate nearest neighbors and the fast johnson - lindenstrauss transform . in _ proc .",
    "38th annu .",
    "acm sympos .",
    "theory comput .",
    "_ ( stoc ) _",
    "_ , pages 557563 , 2006 .",
    "[ ] .    a.  andoni , m.  datar , n.  immorlica , p.  indyk , and v.  s. mirrokni .",
    "locality - sensitive hashing using stable distribution . in t.",
    "darrell , p.  indyk , and g.  shakhnarovich , editors , _ nearest - neighbor methods in learning and vision : theory and practice _ , pages 6172 .",
    "mit press , 2006 .",
    "a.  andoni and p.  indyk . near - optimal hashing algorithms for approximate nearest neighbor in high dimensions . , 51(1):117122 , 2008 .",
    "http://dx.doi.org/10.1145/1327452.1327494 [ ] .",
    "a.  andoni , p.  indyk , r.  krauthgamer , and h.  l. nguyen .",
    "approximate line nearest neighbor in high dimensions . in _ proc .",
    "20th acm - siam sympos",
    ". discrete algs . _",
    "( soda ) _ _ , pages 293301 , 2009 .",
    "http://dx.doi.org/10.1137/1.9781611973068.33 [ ] .",
    "a.  andoni , p.  indyk , h.  l. nguyen , and i.  razenshteyn . beyond locality - sensitive hashing . in _ proc .",
    "25th acm - siam sympos",
    ". discrete algs . _",
    "( soda ) _ _ , pages 10181028 , 2014 .",
    "http://dx.doi.org/10.1137/1.9781611973402.76 [ ] .",
    "s.  arya , d.  m. mount , n.  s. netanyahu , r.  silverman , and a.  y. wu .",
    "an optimal algorithm for approximate nearest neighbor searching in fixed dimensions .",
    ", 45(6):891923 , 1998 .",
    "url : http://www.cs.umd.edu/~mount/papers/dist.pdf , http://dx.doi.org/10.1145/293347.293348 [ ] .",
    "a.  andoni and i.  razenshteyn .",
    "optimal data - dependent hashing for approximate near neighbors . in _ proc .",
    "47th annu .",
    "acm sympos .",
    "theory comput .",
    "_ ( stoc ) _",
    "_ , pages 793801 , 2015 .",
    "f.  cismondi , a.  s. fialho , s.  m. vieira , s.  r. reti , j.  m.  c. sousa , and s.  n. finkelstein .",
    "missing data in medical databases : impute , delete or classify ?",
    ", 58(1):6372 , 2013 .",
    "http://dx.doi.org/10.1016/j.artmed.2013.01.003 [ ] .",
    "e.  j. cands , x.  li , y.  ma , and j.  wright .",
    "robust principal component analysis ?",
    ", 58(3):11 , 2011 .",
    "http://dx.doi.org/10.1145/1970392.1970395 [ ] .",
    "a.  chakrabarti and o.  regev .",
    "an optimal randomized cell probe lower bound for approximate nearest neighbor searching .",
    ", 39(5):19191940 , february 2010 . http://dx.doi.org/10.1137/080729955 [ ]",
    ".    k.  l. clarkson and p.  w. shor .",
    "applications of random sampling in computational geometry , ii . , 4:387421 , 1989 .",
    "http://dx.doi.org/10.1007/bf02187740 [ ] .",
    "m.  datar , n.  immorlica , p.  indyk , and v.  s. mirrokni .",
    "locality - sensitive hashing scheme based on @xmath505-stable distributions . in _ proc .",
    "20th annu .",
    "( socg ) _ _ , pages 253262 , 2004 .",
    "s.  har - peled . a replacement for voronoi diagrams of near linear size .",
    "in _ proc .",
    "42nd annu .",
    "ieee sympos . found .",
    "( focs ) _ _ , pages 94103 , 2001 .",
    "url : http://sarielhp.org/p/01/avoronoi , http://dx.doi.org/10.1109/sfcs.2001.959884 [ ] .",
    "j.  hays and a.  a. efros .",
    "scene completion using millions of photographs .",
    ", 26(3):4 , 2007 .",
    "http://dx.doi.org/10.1145/1276377.1276382 [ ] .",
    "s.  har - peled , p.  indyk , and r.  motwani .",
    "approximate nearest neighbors : towards removing the curse of dimensionality .",
    ", 8:321350 , 2012 .",
    "special issue in honor of rajeev motwani .",
    "[ ] .    p.  indyk and r.  motwani .",
    "approximate nearest neighbors : towards removing the curse of dimensionality . in _ proc .",
    "30th annu .",
    "acm sympos .",
    "theory comput .",
    "_ ( stoc ) _",
    "_ , pages 604613 , 1998 .",
    "[ ] .    p.  indyk .",
    "nearest neighbors in high - dimensional spaces . in j.",
    "e. goodman and j.  orourke , editors , _ handbook of discrete and computational geometry _ , chapter  39 , pages 877892 .",
    "crc press llc , 2nd edition , 2004 .",
    "http://dx.doi.org/10.1201/9781420035315.ch39 [ ] .",
    "p.  indyk .",
    "stable distributions , pseudorandom generators , embeddings , and data stream computation .",
    ", 53(3):307323 , 2006 .",
    "http://dx.doi.org/10.1145/1147954.1147955 [ ] .",
    "m.  t. islam .",
    "approximation algorithms for minimum knapsack problem .",
    "master s thesis , dept .",
    "math & comp .",
    "sci . , 2009 .",
    "https://www.uleth.ca/dspace/handle/10133/1304 .",
    "r.  krauthgamer and j.  r. lee .",
    "navigating nets : simple algorithms for proximity search . in _ proc .",
    "15th acm - siam sympos",
    ". discrete algs . _",
    "( soda ) _ _ , pages 798807 , philadelphia , pa , usa , 2004 . society for industrial and applied mathematics .",
    "j.  kleinberg .",
    "two algorithms for nearest - neighbor search in high dimensions . in _ proc .",
    "29th annu .",
    "acm sympos .",
    "theory comput .",
    "_ ( stoc ) _",
    "_ , pages 599608 , 1997 .",
    "e.  kushilevitz , r.  ostrovsky , and y.  rabani .",
    "efficient search for approximate nearest neighbor in high dimensional spaces .",
    ", 2(30):457474 , 2000 .",
    "url : http://epubs.siam.org/sam-bin/dbq/article/34717 .",
    "s.  mahabadi .",
    "approximate nearest line search in high dimensions . in _ proc .",
    "26th acm - siam sympos .",
    "discrete algs .",
    "_ ( soda ) _",
    "_ , pages 337354 , 2015 .",
    "w.  mulzer , h.  l. nguyn , p.  seiferth , and y.  stein .",
    "approximate @xmath2-flat nearest neighbor search . in _ proc .",
    "47th annu .",
    "acm sympos .",
    "theory comput .",
    "_ ( stoc ) _",
    "_ , pages 783792 , 2015 . http://dx.doi.org/10.1145/2746539.2746559 [ ] .",
    "r.  panigrahy .",
    "entropy based nearest neighbor search in high dimensions . in _ proc .",
    "17th acm - siam sympos .",
    "discrete algs . _",
    "( soda ) _ _ , pages 11861195 , 2006 .",
    "h.  samet . .",
    "the morgan kaufmann series in computer graphics and geometric modeling .",
    "morgan kaufmann publishers inc . , 2005 .",
    "g.  shakhnarovich , t.  darrell , and p.  indyk . .",
    "the mit press , 2006 .",
    "j.  a.  c. sterne , i.  r. white , j.  b. carlin , m.  spratt , p.  royston , m.  g. kenward , a.  m. wood , and j.  r. carpenter .",
    "multiple imputation for missing data in epidemiological and clinical research : potential and pitfalls .",
    ", 338:b2393 , 2009 .",
    "url : http://dx.doi.org/10.1136/bmj.b2393 .",
    "b.  j. wells , k.  m. chagin , a.  s. nowacki , and m.  w. kattan .",
    "strategies for handling missing data in electronic health record derived data . , 1(3 ) , 2013 .",
    "http://dx.doi.org/10.13063/2327-9214.1035 [ ] .",
    "[ apnd : eps : approx ] the tail of a point in has to be quite heavy , for the data - structure to reject it as an",
    ". it is thus natural to ask if one can do better , that is  classify a far point as far , even if the threshold for being far is much smaller ( i.e. , ultimately a factor of @xmath506 ) . maybe surprisingly , this can be done , but it requires that such a far point would be quite dense , and we show how to do so here . for the sake of simplicity of exposition",
    "the result of this section is provided only under the @xmath15 norm .",
    "the algorithm is the same as the one presented in , except that for the given parameter @xmath507 we use @xmath508-data - structures .",
    "we will specify it more precisely at the end of this section .",
    "also the total number of data - structures is @xmath509          let @xmath513 be a parameter and consider the @xmath143-truncated point @xmath173 .",
    "since @xmath5 is @xmath514-heavy , we have that @xmath515 .",
    "now , we have @xmath516 } }       =       { { { \\tau}}}{\\left\\| { { { { \\bm{v } } } } } \\right\\|_{1 } }        \\geq ( 1+{{{\\varepsilon } } } ) { { { \\tau}}}{{{r}}}\\qquad\\text{and}\\qquad       \\sigma^2       =        { \\mathop{\\mathbf{v}}{\\mleft [ { \\bigl . { \\left\\| { { { { { \\mathpzc{m}}}}}{{{\\bm{v } } } } } \\right\\|_{1 } } } \\mright ] } }       =       { { { \\tau}}}(1-{{{\\tau } } } ) { \\left\\| { { { { \\bm{v } } } } } \\right\\|_{2}}^2 .                 \\end{aligned}\\ ] ] now , by chebyshev s inequality , we have that @xmath517 } }         \\geq         { \\mathop{\\mathbf{pr}}{\\mleft [ { \\bigl . { \\left\\| { { { { { \\mathpzc{m}}}}}{{{\\bm{v } } } } } \\right\\|_{1 } } \\geq ( 1+{{{\\varepsilon}}}/4 ) { { { \\tau}}}{{{r } } } } \\mright ] } }         \\geq         { \\mathop{\\mathbf{pr}}{\\mleft [ { \\bigl . { \\left\\| { { { { { \\mathpzc{m}}}}}{{{\\bm{v } } } } } \\right\\|_{1 } } \\geq \\frac{(1+{{{\\varepsilon } } } ) { { { \\tau}}}{{{r}}}}{1+{{{\\varepsilon}}}/2 } } \\mright ] } }       \\\\ &           \\geq           { \\mathop{\\mathbf{pr}}{\\mleft [ { { \\left\\| { { { { { \\mathpzc{m}}}}}{{{\\bm{v } } } } } \\right\\|_{1 } } \\geq \\frac{\\mu}{1+{{{\\varepsilon}}}/2 } } \\mright ] } }           =           { \\mathop{\\mathbf{pr}}{\\mleft [ { { \\left\\| { { { { { \\mathpzc{m}}}}}{{{\\bm{v } } } } } \\right\\|_{1 } } - \\mu \\geq            { \\mleft({\\frac{1}{1+{{{\\varepsilon}}}/2 } -1 } \\mright ) } \\mu } \\mright ] } }       \\\\ &           =           { \\mathop{\\mathbf{pr}}{\\mleft [ { - { \\left\\| { { { { { \\mathpzc{m}}}}}{{{\\bm{v } } } } } \\right\\|_{1 } } + \\mu \\leq            { \\frac{{{{\\varepsilon}}}/2}{1+{{{\\varepsilon}}}/2 } \\mu } } \\mright ] } }           =           1- { \\mathop{\\mathbf{pr}}{\\mleft [ { - { \\left\\| { { { { { \\mathpzc{m}}}}}{{{\\bm{v } } } } } \\right\\|_{1 } } + \\mu \\geq            { \\frac{{{{\\varepsilon}}}}{2+{{{\\varepsilon } } } } \\mu } } \\mright ] } }       \\\\ &           \\geq           1 - { \\mathop{\\mathbf{pr}}{\\mleft [ { { \\hspace{0.6pt}}{\\left| { \\bigl.{\\left\\| { { { { { \\mathpzc{m}}}}}{{{\\bm{v } } } } } \\right\\|_{1 } } - \\mu } \\right| }            \\geq \\frac{{{{\\varepsilon}}}}{2+{{{\\varepsilon } } } } \\cdot \\frac { \\mu}{\\sigma } \\cdot            \\sigma } \\mright ] } }           \\geq           1 - { \\mleft({\\frac{2+{{{\\varepsilon}}}}{{{{\\varepsilon}}}}}\\mright)}^2 { \\mleft({\\frac{\\sigma}{\\mu}}\\mright)}^2       \\\\ &           =           1 - \\frac{9}{{{{\\varepsilon}}}^2 } \\cdot \\frac { { { { { \\tau}}}(1-{{{\\tau } } } )            { \\left\\| { { { { \\bm{v } } } } } \\right\\|_{2}}^2           } } { { { { \\tau}}}^2 { \\left\\| { { { { \\bm{v } } } } } \\right\\|_{1}}^2 }           \\geq           1 - \\frac{9}{{{{\\varepsilon}}}^2}\\frac { { ( 1-{{{\\tau } } } ) \\xi                          } } { { { { \\tau}}}(1+{{{\\varepsilon } } } ) }           \\geq           1 - \\frac { 9\\xi } { { { { \\varepsilon}}}^2{{{\\tau } } } } ,     \\end{aligned}\\ ] ] by . now by setting @xmath518 , this probability would be at least @xmath519 .",
    "[ lemma : light : good : eps ] consider a point @xmath5 such that @xmath520 .",
    "conditioned on the event of , we have that @xmath521 } } \\geq 1- \\frac{1}{1+{{{\\varepsilon}}}/32 } \\geq { { { \\varepsilon}}}/33 $ ] , where @xmath117 .      for all @xmath46 , let @xmath527 . by , with probability at least @xmath528",
    ", we have that @xmath529 in particular , let @xmath530 .",
    "we have that @xmath531 } }       =       { \\mathop{\\mathbf{e}}{\\mleft [ { \\sum_{i=1}^{{{t}}}z_i } \\mright ] } }       \\geq       ( 1+{{{\\varepsilon}}}/4){{{t}}}{{{\\tau}}}{{{r}}}(1-{{{\\varepsilon}}}/10 ) \\geq        ( 1+{{{\\varepsilon}}}/8){{{t}}}{{{\\tau}}}{{{r}}}.      \\end{aligned}\\ ] ] now , by hoeffding s inequality , we have that @xmath532 } }       & \\leq         { \\mathop{\\mathbf{pr}}{\\mleft [ { \\bigl .",
    "z \\leq { { { { u } } } } } \\mright ] } }         \\leq         { \\mathop{\\mathbf{pr}}{\\mleft [ { \\bigl .",
    "{ \\left| { z - \\mu } \\right| } \\geq \\frac{{{{\\varepsilon}}}}{16 } { { { t}}}{{{\\tau}}}{{{r } } } } \\mright ] } }         \\leq         2 \\exp{\\mleft ( { - \\frac{2({{{t}}}{{{\\tau}}}{{{r}}}{{{\\varepsilon}}}/16)^2 } { { { { t}}}{\\mleft ( { ( 1+{{{\\varepsilon}}}/4){{{\\tau}}}{{{r}}}}\\mright)}^2}}\\mright ) }       \\\\ &           \\leq           2 \\exp{\\mleft ( { - \\frac{t{{{\\varepsilon}}}^2}{16 ^ 2}}\\mright ) }           \\leq           \\frac{2}{n^{{{{\\beta}}}{{{\\varepsilon}}}^2/256}}.      \\end{aligned}\\ ] ]    [ lemma : heavy : tail : eps ] let @xmath533 be two parameters",
    ". for a query point @xmath85 , let @xmath534 be its @xmath2-fold nearest neighbor in @xmath9 , and let @xmath535 .",
    "then , with high probability , the algorithm returns a point @xmath212 , such that @xmath213 is a @xmath536-light , where @xmath537 .",
    "the data - structure performs @xmath538 of @xmath539- queries .",
    "as before , we set @xmath540 and @xmath222 . also , by conditions of we have @xmath541 .",
    "also , let @xmath542 .",
    "let @xmath188 be the set of @xmath2 largest coordinates in @xmath543 . by similar arguments as in",
    ", there exists @xmath544 data structures say @xmath232 such that @xmath228 does not contain any of the coordinates of @xmath188 .",
    "since by assumption @xmath545 , and by , with probability at least @xmath546 the distance of @xmath547 from @xmath236 is at most @xmath548 . since there are @xmath549 such structures , we know that , with high probability , for one of them , say @xmath239 , this holds . by , any point @xmath240 ( of @xmath9 ) that is @xmath550-heavy , would be in distance at least @xmath551 in the projection @xmath243 from the projected @xmath3 , and since @xmath239 is a @xmath539-data - structure under the @xmath15 norm , we conclude that no such point can be returned",
    ". note that since for the reported point @xmath174 , the point @xmath246 can not be @xmath552-heavy , and the coordinates in @xmath188 can contribute at most @xmath553 , the point @xmath174 can not be @xmath554-heavy .",
    "thus , the data - structure returns the desired point with high probability ."
  ],
  "abstract_text": [
    "<S> we introduce a new variant of the nearest neighbor search problem , which allows for some coordinates of the dataset to be arbitrarily corrupted or unknown . formally , given a dataset of @xmath0 points @xmath1 in high - dimensions , and a parameter @xmath2 , the goal is to preprocess the dataset , such that given a query point @xmath3 , one can compute quickly a point @xmath4 , such that the distance of the query to the point @xmath5 is minimized , when ignoring the `` optimal '' @xmath2 coordinates . </S>",
    "<S> note , that the coordinates being ignored are a function of both the query point and the point returned .    </S>",
    "<S> we present a general reduction from this problem to answering queries , which is similar in spirit to ( locality sensitive hashing ) @xcite . </S>",
    "<S> specifically , we give a sampling technique which achieves a bi - criterion approximation for this problem . </S>",
    "<S> if the distance to the nearest neighbor after ignoring @xmath2 coordinates is @xmath6 , the data - structure returns a point that is within a distance of @xmath7 after ignoring @xmath8 coordinates . </S>",
    "<S> we also present other applications and further extensions and refinements of the above result .    </S>",
    "<S> the new data - structures are simple and ( arguably ) elegant , and should be practical  specifically , all bounds are polynomial in all relevant parameters ( including the dimension of the space , and the robustness parameter @xmath2 ) . </S>"
  ]
}