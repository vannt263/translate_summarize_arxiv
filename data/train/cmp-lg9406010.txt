{
  "article_text": [
    "when automated part of speech tagging was initially explored @xcite , people manually engineered rules for tagging , sometimes with the aid of a corpus . as large corpora became available , it became clear that simple markov - model based stochastic taggers that were automatically trained could achieve high rates of tagging accuracy @xcite .",
    "markov - model based taggers assign a sentence the tag sequence that maximizes @xmath0 .",
    "these probabilities can be estimated directly from a manually tagged corpus .",
    "stochastic taggers have a number of advantages over the manually built taggers , including obviating the need for laborious manual rule construction , and possibly capturing useful information that may not have been noticed by the human engineer . however , stochastic taggers have the disadvantage that linguistic information is only captured indirectly , in large tables of statistics .",
    "almost all recent work in developing automatically trained part of speech taggers has been on further exploring markov - model based tagging @xcite .    in @xcite",
    ", a trainable rule - based tagger is described that achieves performance comparable to that of stochastic taggers .",
    "training this tagger is fully automated , but unlike trainable stochastic taggers , linguistic information is encoded directly in a set of simple non - stochastic rules . in this paper , we describe some extensions to this rule - based tagger .",
    "these include a rule - based approach to : lexicalizing the tagger , tagging unknown words , and assigning the k - best tags to a word .",
    "all of these extensions , as well as the original tagger , are based upon a learning paradigm called transformation - based error - driven learning .",
    "this learning paradigm has shown promise in a number of other areas of natural language processing , and we hope that the extensions to transformation - based learning described in this paper can carry over to other domains of application as well .",
    "transformation - based error - driven learning has been applied to a number of natural language problems , including part of speech tagging , prepositional phrase attachment disambiguation , and syntactic parsing @xcite .",
    "a similar approach is being explored for machine translation @xcite .",
    "figure [ ded - learn ] illustrates the learning process .",
    "first , unannotated text is passed through the initial - state annotator .",
    "the initial - state annotator can range in complexity from assigning random structure to assigning the output of a sophisticated manually created annotator .",
    "once text has been passed through the initial - state annotator , it is then compared to the _ truth _ , and transformations are learned that can be applied to the output of the initial state annotator to make it better resemble the _",
    "truth_.    in all of the applications described in this paper , the following greedy search is applied : at each iteration of learning , the transformation is found whose application results in the _ highest score _ ; that transformation is then added to the ordered transformation list and the training corpus is updated by applying the learned transformation . to define a specific application of transformation - based learning",
    ", one must specify the following : ( 1 ) the initial state annotator , ( 2 ) the space of transformations the learner is allowed to examine , and ( 3 ) the scoring function for comparing the corpus to the _ truth _ and choosing a transformation .",
    "once an ordered list of transformations is learned , new text can be annotated by first applying the initial state annotator to it and then applying each of the learned transformations , in order .",
    "the original transformation - based tagger @xcite works as follows .",
    "the initial state annotator assigns each word its most likely tag as indicated in the training corpus .",
    "the most likely tag for unknown words is guessed based on a number of features , such as whether the word is capitalized , and what the last three letters of the word are .",
    "the allowable transformation templates are :    change tag * a * to tag * b * when :    1 .   the preceding ( following ) word is tagged _",
    "z_. 2 .   the word two before ( after ) is tagged _ z_. 3 .",
    "one of the two preceding ( following ) words is tagged _",
    "one of the three preceding ( following ) words is tagged _",
    "the preceding word is tagged _ z _ and the following word is tagged _",
    "the preceding ( following ) word is tagged _ z _ and the word two before ( after ) is tagged _",
    "w_.    where _ a , b , z _ and _ w _ are variables over the set of parts of speech .",
    "to learn a transformation , the learner in essence applies every possible transformation , counts the number of tagging errors after that transformation is applied , and chooses that transformation resulting in the greatest error reduction .",
    "learning stops when no transformations can be found whose application reduces errors beyond some prespecified threshold .",
    "an example of a transformation that was learned is : change the tagging of a word from * noun * to * verb * if the previous word is tagged as a * modal*. once the system is trained , a new sentence is tagged by applying the initial state annotator and then applying each transformation , in turn , to the sentence .",
    "no relationships between words are directly captured in stochastic taggers . in the markov model , state transition probabilities ( @xmath1 )",
    "express the likelihood of a tag immediately following @xmath2 other tags , and emit probabilities ( @xmath3 ) express the likelihood of a word given a tag .",
    "many useful relationships , such as that between a word and the previous word , or between a tag and the following word , are not directly captured by markov - model based taggers .",
    "the same is true of the earlier transformation - based tagger , where transformation templates did not make reference to words .    to remedy this problem ,",
    "the transformation - based tagger was extended by adding contextual transformations that could make reference to words as well as part of speech tags .",
    "the transformation templates that were added are :    change tag * a * to tag * b * when :    1 .",
    "the preceding ( following ) word is _",
    "the word two before ( after ) is _",
    "one of the two preceding ( following ) words is _",
    "the current word is _ w _ and the preceding ( following ) word is _ x_. 5 .",
    "the current word is _ w _ and the preceding ( following ) word is tagged _",
    "z_.    where w and x are variables over all words in the training corpus , and z is a variable over all parts of speech .",
    "below we list two lexicalized transformations that were learned :    change the tag : + ( 12 ) from * preposition * to * adverb * if the word two positions to the right is * as*. + ( 16 ) from * non-3rd person singular present verb * to * base form verb * if one of the previous two words is * nt*. +    the penn treebank tagging style manual specifies that in the collocation _ as  as _ , the first _ as _ is tagged as an adverb and the second is tagged as a preposition . since _ as _ is most frequently tagged as a preposition in the training corpus , the initial state tagger will mistag the phrase _ as tall as _ as :    as/*preposition * tall / adjective as / preposition    the first lexicalized transformation corrects this mistagging .",
    "note that a stochastic tagger trained on our training set would not correctly tag the first occurrence of _",
    "although adverbs are more likely than prepositions to follow some verb form tags , the fact that @xmath4 is much greater than @xmath5 , and @xmath6 is much greater than @xmath7 lead to _ as _ being incorrectly tagged as a preposition by a stochastic tagger .",
    "a trigram tagger will correctly tag this collocation in some instances , due to the fact that @xmath8 is greater than @xmath9 , but the outcome will be highly dependent upon the context in which this collocation appears .",
    "the second transformation arises from the fact that when a verb appears in a context such as _ we do nt _ _ _ _ or _ we did nt usually _ _ _ _ , the verb is in base form .",
    "a stochastic trigram tagger would have to capture this linguistic information indirectly from frequency counts of all trigrams of the form :    * = adverb = present_verb + * adverb base_verb + adverb * present_verb + adverb * base_verb +    and from the fact that @xmath10 is fairly high .    in @xcite ,",
    "results are given when training and testing a markov - model based tagger on the penn treebank tagged wall street journal corpus .",
    "they cite results making the closed vocabulary assumption that all possible tags for all words in the test set are known . when training contextual probabilities on 1 million words , an accuracy of 96.7%",
    "was achieved .",
    "accuracy dropped to 96.3% when contextual probabilities were trained on 64,000 words .",
    "we trained the transformation - based tagger on 600,000 words from the same corpus , making the same closed vocabulary assumption , and achieved an accuracy of 97.2% on a separate 150,000 word test set .",
    "the transformation - based learner achieved better performance , despite the fact that contextual information was captured in only 267 simple nonstochastic rules , as opposed to 10,000 contextual probabilities that were learned by the stochastic tagger . to see whether lexicalized transformations were contributing to the accuracy rate , we ran the exact same test using the tagger trained using the earlier transformation template set , which contained no transformations making reference to words .",
    "accuracy of that tagger was 96.9% .",
    "disallowing lexicalized transformations resulted in an 11% increase in the error rate .",
    "these results are summarized in table [ nounkwds ] .",
    ".comparison of tagging accuracy with no unknown words [ cols=\"^,^,^,^ \" , ]",
    "in this paper , we have described a number of extensions to previous work in rule - based part of speech tagging , including the ability to make use of lexical relationships previously unused in tagging , a new method for tagging unknown words , and a way to increase accuracy by returning more than one tag per word in some instances .",
    "we have demonstrated that the rule - based approach obtains competitive performance with stochastic taggers on tagging both unknown and known words .",
    "the rule - based tagger captures linguistic information in a small number of simple non - stochastic rules , as opposed to large numbers of lexical and contextual probabilities .",
    "recently , we have begun to explore the possibility of extending these techniques to other problems , including learning pronunciation networks for speech recognition and learning mappings between sentences and semantic representations ."
  ],
  "abstract_text": [
    "<S> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ most recent research in trainable part of speech taggers has explored stochastic tagging . </S>",
    "<S> while these taggers obtain high accuracy , linguistic information is captured indirectly , typically in tens of thousands of lexical and contextual probabilities . in @xcite </S>",
    "<S> , a trainable rule - based tagger was described that obtained performance comparable to that of stochastic taggers , but captured relevant linguistic information in a small number of simple non - stochastic rules . in this paper </S>",
    "<S> , we describe a number of extensions to this rule - based tagger . </S>",
    "<S> first , we describe a method for expressing lexical relations in tagging that stochastic taggers are currently unable to express . </S>",
    "<S> next , we show a rule - based approach to tagging unknown words . </S>",
    "<S> finally , we show how the tagger can be extended into a k - best tagger , where multiple tags can be assigned to words in some cases of uncertainty . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ </S>"
  ]
}