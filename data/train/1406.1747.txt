{
  "article_text": [
    "functions depending on a large number of variables play nowadays a crucial role in many areas , including parametric and stochastic pde s , bioinformatics , financial mathematics , data analysis and learning theory .",
    "together with an extensive computational power being used in these applications , results on basic numerical aspects of these functions became crucial .",
    "unfortunately , multivariate problems suffer often from the _ curse of dimension _ , i.e. the minimal number of operations necessary to achieve ( an approximation of ) a solution grows exponentially with the underlying dimension of the problem .",
    "although this effect was observed many times in the literature , we refer to @xcite for probably the most impressive result of this kind - namely that even the uniform approximation of infinitely - differentiable functions is intractable in high dimensions .    in the area of _ information based complexity _ it was possible to achieve a number of positive results on tractability of multivariate problems by posing an additional ( structural ) assumption on the functions under study .",
    "the best studied concepts in this area include tensor product constructions and different concepts of anisotropy and weights .",
    "we refer to the series of monographs @xcite for an extensive treatment of these and related problems .",
    "we pursue the direction initiated by cohen , daubechies , devore , kerkyacharian and picard in @xcite and further developed in a series of recent papers @xcite .",
    "this line of study is devoted to _ ridge functions _ , which are multivariate function @xmath3 taking the form @xmath0 for some univariate function @xmath4 and a non - zero vector @xmath5 we refer also to @xcite for a related approach .",
    "functions of this type are by no means new in mathematics .",
    "they appear for example very often in statistics in the frame of the so - called _ single index models_. they play also an important role in approximation theory , where their simple structure motivated the question if a general function could be well approximated by sums of ridge functions . the pioneering work in this field",
    "is @xcite , where the term `` ridge function '' was first introduced , and also @xcite , where the fundamentality of ridge functions was investigated .",
    "ridge functions appeared also in mathematical analysis of neural networks @xcite and as the basic building blocks of _ ridgelets _ of cands and donoho @xcite . a survey on approximation by ( sums of ) ridge functions",
    "was given in @xcite .",
    "the biggest difference between our setting and the usual approach of statistical learning and data analysis is that we suppose that the sampling points of @xmath3 can be freely chosen , and are not given in advance .",
    "this happens , for instance , if sampling of the unknown function at a point is realized by a ( costly ) pde solver .",
    "most of the techniques applied so far in recovery of ridge functions are based on the simple formula @xmath6 one way , how to use is to approximate the gradient of @xmath3 at a point with non - vanishing @xmath7 . by , it is then co - linear with @xmath8 .",
    "once @xmath8 is recovered , one can use any one - dimensional sampling method to approximate @xmath4 .    another way to approximate @xmath8",
    "is inspired by the technique of _ compressed sensing _",
    "taking directional derivatives of @xmath3 at @xmath9 results into @xmath10 i.e. it gives an access to the scalar product of @xmath8 with a chosen vector @xmath11 .",
    "if we assume , that most of the coordinates of @xmath8 are zero ( or at least very small ) and choose the directions @xmath12 at random , one can recover @xmath8 effectively by the algorithms of sparse recovery .",
    "our aim is to fill some gaps left so far in the analysis done in @xcite .",
    "although the possibility of extending the analysis also to functions defined on other domains than the unit ball was mentioned already in @xcite , no steps in this direction were done there .",
    "we study in detail ridge functions defined on the unit cube @xmath1^d$ ] .",
    "the crucial component of our analysis is the use of the sign of a vector @xmath13 , which is defined componentwise .",
    "although the mapping @xmath14 is obviously not continuous , the mapping ( for @xmath15 fixed ) @xmath16 is continuous at @xmath8 ( and takes the value @xmath17 there ) .",
    "this observation allows to imitate the approach of @xcite and to adapt it to this setting .",
    "let us remark , that all our approximation schemes recover first an approximation of the vector @xmath18 .",
    "afterwards , the problem becomes essentially one - dimensional and a good approximation of @xmath3 by a limited number of sampling points can then be recovered by many classical methods , i.e. by spline approximation .",
    "we will therefore concentrate on an effective recovery of an approximation of @xmath8 and the approximation of @xmath3 will be given only implicitly .",
    "another topic only briefly discussed in @xcite was the recovery of ridge functions from noisy measurements , which is an important step for every possible application of the methods so far .",
    "furthermore , our analysis as well as the approach of @xcite or even the classical results of @xcite are based on approximation of first ( or higher ) order derivatives by differences , which poses naturally the question on numerical stability of the presented algorithms .",
    "we present an algorithm based on the dantzig selector of @xcite , which allows for recovery of a ridge function also in this setting .",
    "it turns out , that in the case of a small step size @xmath19 , the first order differences can not be evaluated with high enough precision . on the other hand , for a large step size @xmath20 the first order differences do not approximate the first order derivatives well enough .",
    "typically , there is therefore an @xmath19 , for which an optimal degree of approximation is achieved .",
    "next thing we discuss is the robustness of the methods developed .",
    "we show that ( without much additional effort ) it can be applied also for uniform recovery of translated radial functions @xmath21 , which are constant along co - centered spheres instead of parallel hyperplanes .",
    "similarly to the model of ridge functions , both the center @xmath18 and the univariate function @xmath4 are unknown .    finally , we close the paper with few numerical simulations of the algorithms presented .",
    "they highlight the surprising fact , that their accuracy _ improves _ with increasing dimension .",
    "this is essentially based on the use of _ concentration of measure _ phenomenon in the underlying theory and goes in line with similar observations made in the area of compressed sensing .",
    "the paper is structured as follows .",
    "section 2 collects some necessary notation and certain basic facts on sparse recovery from the area of _ compressed sensing_. section 3 extends the analysis of @xcite to the setting of ridge functions defined on the unit cube .",
    "section 4 treats the recovery of ridge functions defined on the unit ball from noisy measurements .",
    "section 5 studies the translated radial functions @xmath2 and section 6 closes with numerical examples .",
    "in this section we collect some notation and give an overview of results from the area of compressed sensing , which we shall need later on .      for a given vector @xmath22 and @xmath23",
    "we define @xmath24 where @xmath25 denotes the cardinality of the set @xmath26 .",
    "this notation is further complemented by putting for @xmath27 @xmath28 where @xmath29 , @xmath30 denotes the non - increasing rearrangement of the absolute entries of @xmath9 , i.e. @xmath31 and @xmath32 for some permutation @xmath33 and all @xmath34    it is a very well known fact , that @xmath35 is a norm for @xmath36 and a quasi - norm if @xmath37 also @xmath38 is a quasi - norm for every @xmath27 . if @xmath39 , the space @xmath40 is a hilbert space with the usual inner product given by @xmath41",
    "if @xmath42 is a natural number , then a vector @xmath22 is called _",
    "@xmath43-sparse _ if it contains at most @xmath43 nonzero entries , i.e. @xmath44 . the set of all @xmath43-sparse vectors is denoted by @xmath45 finally , the best @xmath43-term approximation of a vector @xmath9 describes , how well can @xmath9 be approximated by @xmath43-sparse vectors .",
    "best @xmath43-term approximation _ of a given vector @xmath22 with respect to the @xmath46-norm is given by @xmath47      next we recall some basic concepts and results from compressed sensing which we will use later .",
    "compressed sensing emerged in @xcite as a method of recovery of sparse vectors @xmath9 from a small set of linear measurements @xmath48 since then , a vast literature on the subject appeared , concentrating on various aspects of the theory , and its applications .",
    "as it is not our aim to develop the theory of compressed sensing , but rather to use it in approximation theory , we shall restrict ourselves to the most important facts needed later on .",
    "we refer to @xcite for recent overviews of the field and more references .",
    "we focus on the recovery of vectors from noisy measurements , i.e. we want to recover the vectors @xmath22 from @xmath49 linear measurements of the form @xmath50 where @xmath51 is the measurement matrix and the noise is a composition of two factors , namely of the deterministic noise @xmath52 and the random noise @xmath53 .",
    "typically , we will assume , that @xmath54 is small ( with respect to some @xmath55 norm ) and that the components of @xmath56 are generated independently according to a gaussian distribution with small variance .",
    "obviously , some conditions have to be posed on @xmath57 , so that the recovery of @xmath9 from the measurements @xmath58 given by is possible .",
    "the most usual one in the theory of compressed sensing is that the matrix @xmath57 satisfies the _ restricted isometry property_.    the matrix @xmath51 satisfies the _ restricted isometry property _ ( rip ) of order @xmath59 if there exists a constant @xmath60 such that @xmath61 holds for all @xmath43-sparse vectors @xmath62 .",
    "the smallest constant @xmath63 for which this inequality holds is called the restricted isometry constant and we will denote it by @xmath64 .    in general it",
    "is very hard to show that a given matrix satisfies this rip or not .",
    "this is in particular the main reason why we will use random matrices , since it turns out that those matrices satisfy the rip with overwhelming high probability .",
    "we present a version of such a statement , which comes from @xcite .",
    "[ rip ] for every @xmath60 there exist constants @xmath65 depending on @xmath63 such that the random matrix @xmath51 with entries generated independently as @xmath66 satisfies the rip of order @xmath43 for each @xmath67 with rip constant @xmath68 with probability at least @xmath69    a matrix @xmath57 generated by is called _ normalized bernoulli matrix_. for the sake of simplicity , we work with bernoulli sensing matrices , but note that most of the statements presented below remain true for other classes of random matrices , c.f .",
    "* section 5 ) .",
    "next we present several recovery results for our starting problem .",
    "the first result of this kind deals with the case of exact measurements ( i.e. @xmath70 ) and uses the so called @xmath46-minimizer , cf .",
    "* theorem 4.3 ) .",
    "[ rip - reconstruction ] let @xmath51 satisfy the rip of order @xmath71 with constant @xmath72 .",
    "let @xmath22 and let us denote @xmath73 .",
    "finally , let @xmath74 be the solution of the minimization problem @xmath75 then it holds @xmath76 with constant @xmath77 depending only on @xmath63 .",
    "this theorem implies that @xmath43-sparse vectors are recovered exactly by the @xmath46-minimizer in the noise - free setting , since @xmath78 holds for every @xmath62 . to deal with the deterministic noise @xmath54",
    ", we shall need some more information about the geometrical properties of bernoulli matrices .",
    "in particular , we will make use of theorem 3.5 and theorem 4.1 of @xcite , cf . also @xcite .",
    "[ surjective - deterministic - noise ] let @xmath51 be a normalized bernoulli matrix and let @xmath79 .",
    "let @xmath80 , where @xmath81    1 .",
    "then there exists an absolute constant @xmath82 such that with probability at least @xmath83 for every @xmath84 there is an @xmath22 , such that @xmath85 and @xmath86 .",
    "2 .   let @xmath87 and let @xmath88 and @xmath89 be the constants from theorem [ rip ] .",
    "then there exists an absolute constant @xmath90 and a constant @xmath91 depending on @xmath63 such that , with probability at least @xmath92 , for each @xmath84 there exists a vector @xmath22 with @xmath85 , @xmath93 and @xmath94 .",
    "we will use those two theorems to handle the deterministic noise @xmath54 .",
    "further we need a similar result to handle the random noise @xmath56 , therefore we recall the _ dantzig selector _ from @xcite .    for a matrix @xmath51 and constants @xmath95 the _ dantzig selector _",
    "@xmath96 of an input vector @xmath97 is defined as the solution of the minimization problem @xmath98    [ rem : setting ] in what follows we shall use several parameters as the description of the typical frame of compressed sensing .",
    "first , we take @xmath99 to be natural numbers and denote by @xmath51 the normalized bernoulli matrix .",
    "we put @xmath100 and denote by @xmath88 and @xmath89 the constants appearing in theorem [ rip ] .",
    "next , we assume that the natural numbers @xmath101 satisfy @xmath102 hence , by theorem [ rip ] , @xmath57 has ( with high probability ) the rip of order @xmath103 with a constant at most 1/6 .",
    "now we can use theorem 1.3 of @xcite to handle the random noise @xmath56 .",
    "[ dantzig ]",
    "let @xmath104 be natural numbers with and let @xmath51 be a normalized bernoulli matrix .",
    "let @xmath105 for @xmath22 with @xmath106 , @xmath107 , and @xmath53 with independent entries @xmath108 . then there exists a constant @xmath109 such that the dantzig selector ( with @xmath110 ) satisfies @xmath111 with high probability .    combining theorem [ surjective - deterministic - noise ] and theorem [ dantzig ] we get the following new result .",
    "[ random+deterministic ] let @xmath104 be natural numbers with and let @xmath51 be a normalized bernoulli matrix . for @xmath22 and @xmath112 with @xmath113 , @xmath114 , @xmath115 and @xmath108 for some constants",
    "@xmath116 let @xmath117 then there exist constants @xmath118 such that the dantzig selector @xmath119 ( with @xmath110 ) applied to @xmath58 satisfies the estimate @xmath120 with high probability , where @xmath121 .",
    "it follows from the assumptions that @xmath122 .",
    "then we use theorem [ surjective - deterministic - noise ] ( ii ) to find a vector @xmath123 , such that @xmath124    further we apply the triangle inequality for the @xmath125 quasinorm ( see , for instance , lemma 2.7 in @xcite ) to get @xmath126 finally , applying theorem [ dantzig ] ( with @xmath127 ) we get @xmath128 which finishes the proof .",
    "in this section we consider uniform approximation of ridge functions of the form @xmath129^d\\to{\\mathbb{r}},~x\\mapsto g(\\langle a , x\\rangle).\\end{aligned}\\ ] ] we assume that both the _ ridge vector _ @xmath18 and the univariate function @xmath4 ( also called _ ridge profile _ ) are unknown .    first , we note that the problem is invariant with respect to scaling .",
    "suppose that @xmath3 is a ridge function with representation @xmath0 .",
    "then for any scalar @xmath130 we put @xmath131 and @xmath132 to get another representation of @xmath3 in the form of , namely @xmath133 thus we can pose a scaling condition on @xmath8 without any loss of generality . furthermore ,",
    "if @xmath134 , we can switch from @xmath8 to @xmath135 , and obtain a ridge representation of @xmath3 with @xmath136    in @xcite , the scaling condition @xmath137 was assumed .",
    "this fitted together with both the scalar product structure used in the definition of @xmath3 , as well as with the geometry of the domain of @xmath3 used in @xcite , namely the euclidean unit ball .",
    "it is easy to observe , that it will be more convenient for us to work with the @xmath138-norm of @xmath8 .",
    "indeed , let us consider that the ridge profile @xmath139 is known , i.e. that we have @xmath140 for some ( unknown ) @xmath18 , and let us assume , that we have an @xmath46-approximation @xmath141 of @xmath8 with @xmath142 . then hlder s inequality gives us @xmath143^d}\\lvert \\langle a-\\hat a , x\\rangle\\rvert      \\leq\\sup_{x\\in [ -1,1]^d}\\|a-\\hat a\\|_{l_1^d}\\|x\\|_{l^d_\\infty }      \\le { \\varepsilon}.\\end{aligned}\\ ] ]    in what follows we shall therefore assume that @xmath144 and that @xmath4 is a univariate function defined on @xmath145.$ ] we further assume that @xmath4 and @xmath146 are lipschitz continuous with constants @xmath147 , i.e. @xmath148 holds for all @xmath149 $ ] .",
    "finally , we assume that @xmath150 as it is known , cf .",
    "@xcite , that approximation of ridge functions may be intractable if this condition is left out .",
    "in this part we evolve an approximation scheme for ridge functions with an arbitrary ridge vector @xmath18 , merely assuming the right normalization .",
    "after this we consider the same problem with an additional sparsity condition on @xmath8 , where we will use results from compressed sensing to reduce the number of samples .",
    "motivated by the formula for @xmath151 @xmath152 we set for a small constant @xmath19 and @xmath153 @xmath154 where @xmath155 are the usual canonical basis vectors of @xmath156 as expected , it turns out that @xmath157 is a good approximation of @xmath158 as the mean value theorem gives @xmath159 for some @xmath160 . and",
    "for the @xmath138-approximation we obtain @xmath161 thus @xmath162 is a good approximation to @xmath163 and since we want an approximation to @xmath8 and we know that @xmath8 is @xmath46-normalized we set @xmath164 now we have to estimate the difference between @xmath8 and @xmath141 , therefore we will use a variant of lemma 3.4 of @xcite .",
    "[ stability - subspaces ] let @xmath22 with @xmath165 , @xmath166 and @xmath167 .",
    "then it holds @xmath168    this lemma is a direct consequence of the triangle inequality .",
    "first we obtain @xmath169 and therefore @xmath170 which proves the claim .",
    "we only used the triangle inequality to prove the previous lemma .",
    "thus the lemma remains true for any norm on @xmath171 .    applying this lemma to our case it holds with and the assumption @xmath172",
    "although we now know that @xmath141 is a good approximation of @xmath8 , it is still not clear how to define the uniform approximation @xmath173 of @xmath3 . the naive approach ( used with success in @xcite for ridge functions defined on the euclidean unit ball ) is to sample @xmath3 along @xmath141 , i.e. to put @xmath174 , and then define @xmath175 .",
    "but when trying to estimate @xmath176 , we would need to ensure that @xmath177 is close to 1 .",
    "this was indeed the case in @xcite , where an estimate on @xmath178 was obtained , but it is not true any more in our setting of @xmath138 approximation .",
    "on the other hand , because of the normalization of @xmath8 , we have @xmath179 where we defined the _ sign _ of a vector @xmath22 entrywise , i.e. @xmath180 note that this function is discontinuous , hence @xmath181 and @xmath182 can be far from each other , even if the difference @xmath183 is small .",
    "nevertheless their scalar product with @xmath8 is nearly the same as hlder s inequality gives @xmath184 thus we define @xmath185\\to{\\mathbb{r}},~t\\mapsto f\\big(t\\cdot{\\mathrm{sign}}(\\hat a)\\big)\\end{aligned}\\ ] ] and @xmath186 let us summarize our approximation algorithm as follows .",
    ".2 cm .2 cm    we formulate the approximation properties of algorithm a as the following theorem .",
    "[ thm : alga ] let @xmath187^d\\to{\\mathbb{r}}$ ] be a ridge function with @xmath0 for some @xmath18 with and a differentiable function @xmath188\\to { \\mathbb{r}}$ ] with - .",
    "for @xmath19 we construct the function @xmath173 as described in algorithm a. then @xmath189 where the last inequality only holds if @xmath190 is positive .",
    "first , we show that @xmath191 where the last inequality only holds if @xmath190 is positive .",
    "due to , we only have to show the last inequality of .",
    "since @xmath146 is lipschitz continuous with lipschitz constant @xmath192 we have for any @xmath193 $ ] @xmath194 and therefore @xmath195 for some @xmath196 $ ] , it follows by the triangle inequality and @xmath197 which proves and the second inequality in .    to prove the first inequality in , we use and to show that @xmath198 is a good uniform approximation of @xmath4 on @xmath1 $ ] .",
    "we obtain @xmath199 for each @xmath200 $ ] . finally , we combine this estimate with the definition of @xmath173 as given in and arrive at @xmath201    [ rem:3.4 ]    1 .",
    "the estimate depends heavily on the value of @xmath202 .",
    "especially , the approximation becomes difficult , when this value gets smaller and becomes void if @xmath203 this is a very well known aspect of approximation of ridge functions , which was studied in a great detail in @xcite .",
    "we refer also to a slightly weaker condition used in @xcite .",
    "if @xmath204 is small , the following improvement of becomes of interest .",
    "first , we observe that can be improved to @xmath205 , which results into @xmath206 finally , this allows to improve to @xmath207      in this subsection we assume that the ridge vector @xmath18 is not only @xmath138-normalized , but satisfies also some sparsity condition , i.e. most of the entries of @xmath8 are zero or at least very small .",
    "we will use techniques of compressed sensing to address the approximation of the ridge vector @xmath8 , afterwards we obtain an approximation of @xmath3 in the same way as before .",
    "let @xmath51 be a normalized bernoulli matrix and let @xmath12 be its rows .",
    "taking their scalar product with the quantities in , we obtain @xmath208    we use again first order differences as an approximation of the directional derivatives in , i.e. we set @xmath209 as in the previous section the mean value theorem gives the existence of some @xmath210 with @xmath211 such that @xmath212 in this sense , we expect @xmath213 to be a good approximation of @xmath214 and @xmath215 to be a good approximation of @xmath216 hence , we recover @xmath162 through @xmath217-minimization . from this point on",
    ", we may continue as before .",
    "let us summarize this procedure as the algorithm b. .2 cm .2 cm    [ thm : algb ] let @xmath187^d\\to{\\mathbb{r}}$ ] be a ridge function with @xmath0 for some vector @xmath18 with and some differentiable function @xmath188\\to { \\mathbb{r}}$ ] with .",
    "let @xmath218 and @xmath19 be fixed . then there exist some constants @xmath219 , such that for every positive integer @xmath43 with @xmath220 the function @xmath173 constructed in algorithm b satisfies @xmath221 where @xmath222 with probability at least @xmath223 provided the denominator in the expression for @xmath224 is positive .",
    "the first inequality in follows again by combined with .    setting @xmath225 and @xmath226 we get @xmath227",
    "therefore we obtain @xmath228 for @xmath52 with @xmath229 and , similarly , @xmath230 and @xmath231 .",
    "hence , by using theorem [ surjective - deterministic - noise ] there exists some vector @xmath123 with @xmath232 and @xmath233 where we used @xmath234 and @xmath218 for the last inequality .",
    "take some @xmath235 fixed , e.g. @xmath236 , and apply theorem [ rip - reconstruction ] to @xmath237 .",
    "this gives us @xmath238 finally , by setting @xmath239 and @xmath240 , lemma [ stability - subspaces ] provides @xmath241 from this point on we can proceed as in the proof of theorem [ thm : alga ] .",
    "we can again estimate the @xmath46-norm of @xmath162 from below .",
    "we get @xmath242 thus we can replace the norm @xmath243 with this expression ( if it is positive ) to get @xmath244 for some constant @xmath245 depending only on @xmath246 .",
    "[ rem : algb ]     1 .   in particular ,",
    "if @xmath8 is @xmath43-sparse , we get @xmath247 and , therefore , @xmath248 2 .",
    "the constant @xmath249 can be chosen to be @xmath250 with @xmath77 being the constant from theorem [ rip - reconstruction ] .",
    "if the sparsity level of @xmath8 is @xmath251 , the condition @xmath220 implies @xmath252 .",
    "thus , in this case we need @xmath253 measurements to reconstruct the vector @xmath8 .",
    "in this section we study another aspect of recovery of ridge functions , which was hardly discussed up to now in the literature .",
    "we consider ridge functions defined on the unit ball as in @xcite but we assume that the measurements are affected by random noise .",
    "in addition , we suppose that the vector @xmath8 satisfies a compressibility condition .    to be more precise , we consider ridge functions @xmath254 we assume , that the ridge vector @xmath18 is @xmath255-normalized @xmath256 and compressible in the following sense , @xmath257 furthermore , we assume that the ridge profile is a differentiable function @xmath188\\to{\\mathbb{r}}$ ] with . we shall use again the setting of remark [ rem : setting ] .",
    "let @xmath79 and let @xmath51 be a normalized bernoulli matrix with rows @xmath258 . by theorem [ rip ] it is ensured that @xmath57 satisfies the rip of order @xmath71 with @xmath259 with high probability for every positive integer @xmath43 with @xmath260 , where the constant @xmath89 is the constant from theorem [ rip ] .",
    "but in contrary to , we now assume that the evaluation of @xmath3 is perturbed by noise . to make the presentation technically simpler",
    ", we shall assume that the value @xmath261 is given precisely ( i.e. without noise ) .",
    "this can be achieved ( with high precision ) by resampling the value @xmath261 several times , and applying hoeffding s inequality .",
    "hence , we set for @xmath262 and a small constant @xmath19 @xmath263 we assume that the random noise @xmath264 has independent components @xmath265 . since @xmath266",
    "are independent , it is well known that @xmath267 are also independent . as in the case with exact measurements",
    "the mean value theorem gives us @xmath268 for some real @xmath210 with @xmath269 , hence @xmath270 to recover the vector @xmath8 from these measurements let us first define the deterministic noise @xmath52 by @xmath271 i.e. @xmath272    we then recover @xmath141 with the help of dantzig selector instead of @xmath273-minimization . finally , for the construction of @xmath198 and @xmath173 , we can use the direct approach of @xcite , which is given by @xmath274    let us summarize this procedure as the following algorithm .",
    ".2 cm .2 cm",
    "let @xmath275 be a ridge function @xmath0 with , , - .",
    "furthermore , let @xmath276 and let @xmath101 be positive integers with .",
    "let @xmath265 be independent .",
    "then there is a constant @xmath277 , such that the function @xmath173 defined by algorithm c satisfies with high probability @xmath278 where @xmath279 for some constants @xmath118 .",
    "the second inequality in only holds if the denominator is positive .    to prove this theorem",
    ", we apply theorem [ random+deterministic ] to .",
    "therefore we need to estimate the norm of @xmath52 , defined by .",
    "we obtain @xmath280 ^ 2      \\leq\\sum\\limits_{j=1}^m\\left(c_1h\\langle a,{\\varphi}_j\\rangle^2\\right)^2\\\\      & \\leq c_1 ^ 2h^2\\sum\\limits_{j=1}^m\\left(\\|a\\|_{l_1^d}\\|{\\varphi}_j\\|_{l_\\infty^d}\\right)^4      \\leq \\frac{c_1 ^ 2h^2r^4}{m}\\end{aligned}\\ ] ] and similarly we can show @xmath281 we can now apply theorem [ random+deterministic ] with @xmath282 to get @xmath283 with @xmath284 and some constants @xmath118 . and since we know that @xmath8 is @xmath255-normalized we set @xmath285 applying lemma [ stability - subspaces ] we get @xmath286 where the last inequality only holds if the denominator is positive .",
    "this proves the second inequality in .    the proof of the first part of proceeds as in @xcite .",
    "first we define an approximation @xmath198 to @xmath4 @xmath287 this is indeed a good approximation to @xmath4 as for any @xmath200 $ ] we get @xmath288 with this approximation @xmath198 to @xmath4 we define the function @xmath173 by @xmath289 it remains to show that @xmath173 is a good approximation to @xmath3 . with the help of and",
    "we obtain @xmath290 for all @xmath291",
    "the methods we presented so far , as well as the methods of @xcite , were developed in the ( quite restrictive ) frame of ridge functions . as an example of a possible extension of these algorithms",
    ", we consider the class of translated radial functions , i.e. functions of the form @xmath292 for some fixed @xmath255-normalized vector @xmath18 @xmath293 and a function @xmath294\\to{\\mathbb{r}}$ ] .",
    "hence , @xmath3 is constant on the spheres centered in @xmath8 or , equivalently , it is a radial function translated by @xmath8 .",
    "typically , we shall again assume that @xmath4 and @xmath146 are lipschitz continuous with constants @xmath295 and @xmath192 , respectively .",
    "the idea to recover those functions is similar to the case of ridge functions .",
    "first we recover the center @xmath8 and then we define approximations @xmath198 to @xmath4 and @xmath173 to @xmath3 .",
    "+ for a small constant @xmath19 and fixed vectors @xmath296 we set @xmath297 where @xmath155 are again the canonical basis vectors of @xmath171 . with help of the mean value theorem we can express this as @xmath298 for some real @xmath299 between @xmath300 and @xmath301 .",
    "the nominator can be simplified by @xmath302 let us choose @xmath303 to get @xmath304 for some @xmath299 between @xmath305 and @xmath306 .",
    "next let us note that @xmath299 is very close to @xmath307 : @xmath308 finally we obtain that @xmath141 is a good approximation to @xmath309 , since @xmath310 thus @xmath162 is almost a multiple of @xmath8 .",
    "again , we need to assume that the derivative of @xmath146 is non - trivial in some sense . due to the construction ,",
    "we replace by the condition @xmath311 then the @xmath255-normalized vector @xmath312 approximates @xmath8 , possibly up to a sign . choosing any vector @xmath313 orthogonal to @xmath141",
    ", we can identify the sign by sampling along @xmath314 afterwards , the correct sign might be assigned to @xmath315 we will therefore restrict ourselves to the case @xmath316    once an approximation of @xmath8 was recovered , it is again easy to define an approximation of @xmath4 , and finally of @xmath3 .",
    "we summarize this procedure as the following algorithm .",
    ".2 cm .2 cm the performance of algorithm d is estimated by the following theorem .",
    "[ thm : sing ] let @xmath275 , @xmath294\\to{\\mathbb{r}}$ ] and @xmath18 be such that @xmath2 and @xmath8 and and @xmath4 satisfy , , and . then @xmath317 if @xmath318 is positive .",
    "first , we estimate the difference between @xmath8 and @xmath141 . by and @xmath319",
    "hence @xmath320 therefore , if the right hand side of is positive , we get @xmath321 now we apply lemma [ stability - subspaces ] to obtain @xmath322 given the approximation @xmath141 to @xmath8 we define an approximation @xmath198 to @xmath4 by @xmath323\\to{\\mathbb{r}},~t\\mapsto f(\\hat a(1-\\sqrt{t})).\\end{aligned}\\ ] ] essentially , @xmath198 is the restriction of @xmath3 onto @xmath324 .",
    "using we obtain the estimate @xmath325 for all @xmath326 $ ] .",
    "next we define @xmath327 with and we get the final estimate @xmath328    [ rem : thm : sing ] ( extensions of theorem [ thm : sing ] )    1 .",
    "we assumed in theorem [ thm : sing ] , that the function @xmath4 and its derivative @xmath146 are both lipschitz . if we assume this property only on the interval @xmath329 , we can still recover at least .",
    "this applies even to the case , when @xmath4 ( and also its derivative ) are unbounded near the origin . in that case , we can still approximate the position of the singularity , although uniform approximation of @xmath3 is out of reach .",
    "2 .   as in the approximation scheme for ridge functions we can use techniques from compressed sensing to recover @xmath3 if @xmath8 is compressible . to be more precise , if @xmath8 satisfies",
    "@xmath330 and @xmath51 is a normalized bernoulli matrix with rows @xmath331 we define @xmath332 as @xmath3 is defined only on the unit ball @xmath333 and @xmath334 , we must always have at least @xmath335 to ensure that @xmath336 to allow for comparison with the non - compressible case just discussed in theorem [ thm : sing ] , we denote @xmath337 which leads to @xmath338 + by defining the deterministic noise @xmath52 @xmath339 we can show with similar calculations as before that @xmath340 using the @xmath341 minimizer of @xcite we put @xmath342 with @xmath343 given by the right hand side of .",
    "we then get the estimate , cf .",
    "* theorem 4.22 ) or ( * ? ? ?",
    "* theorem 1.6 ) , @xmath344 with two universal constants @xmath345 here again @xmath346 .",
    "lemma [ stability - subspaces ] ( with @xmath255 instead of @xmath46 ) gives for @xmath347 @xmath348 finally , using @xmath349 we get @xmath350 if @xmath351 + this gives a replacement of , the rest of the proof of theorem [ thm : sing ] then applies without further modifications .",
    "3 .   once we have this approximation scheme using techniques from compressed sensing , we can easily extend it to an approximation scheme with noisy measurements .",
    "we assume again that @xmath215 from is corrupted by noise @xmath352 , where the components of @xmath353 are again independent @xmath354 distributed random variables .",
    "formula is then replaced by @xmath355 and dantzig selector can be applied .",
    "in this section we investigate the performance of the algorithms presented so far in several model situations .",
    "the results shed a new light on some of the aspects , which we did not discuss in detail , especially on the size of the constants used in previous theorems .",
    "all the approximation schemes started by looking for a good approximation @xmath141 of the unknown direction @xmath8 and , consequently , the quality of the uniform approximation of @xmath3 by @xmath173 was then bounded by the corresponding distance between @xmath141 and @xmath8 . in",
    "what follows , we will therefore discuss only the approximation error between @xmath8 and @xmath141 .",
    "we start with algorithm a , i.e. with approximation of a ridge function @xmath0 defined on the cube @xmath1^d$ ] with @xmath358 we have considered different dimensions ( @xmath359 ) . as the algorithm a does not make any use of sparsity of @xmath8 , it is reasonable to assume , that all its coordinates are equally likely to be non - zero .",
    "the entries of @xmath8 were therefore always independently normally distributed ( i.e. @xmath360 ) , afterwards @xmath8 got @xmath46-normalized according to .",
    "figure [ fig : approx - a ] shows the ( average ) approximation error @xmath361 in dependence of the step size @xmath19 for two different profiles @xmath356 and @xmath357 .",
    "note that the @xmath58-axis scales logarithmically .",
    "let us give some remarks on figure [ fig : approx - a ] .",
    "* the approximation improves rapidly with growing dimension .",
    "this is given by considering the non - sparse ridge vectors @xmath8 and by the concentration of measure phenomenon as described also in remark [ rem:3.4 ] .",
    "* smaller step size @xmath20 implies also better quality of approximation , but already reasonable sizes of @xmath20 ( i.e. @xmath362 ) imply relatively very small errors . *",
    "finally , the second derivative of the first profile at zero vanishes , were it is non - zero for the second profile .",
    "therefore , the first order differences approximate the first order derivative less accurately in that case , leading to larger ( but still surprisingly small ) approximation errors .      the left part of figure [ fig : approx - b ] shows the dependence of the number of the sampling points @xmath363 on the dimension @xmath364 and sparsity @xmath43 , cf .",
    ", when using algorithm b. we fixed the ridge profile @xmath357 , the sparsity @xmath365 and the step size @xmath366 and constructed an @xmath43-sparse random vector @xmath8 by matlab command sprandn , followed by the @xmath138-normalization . for each integer @xmath364 between @xmath367 and @xmath368 and for each integer @xmath363 between @xmath369 and @xmath370 , we then run the algorithm b 120 times and the average approximation error @xmath361 corresponds afterwards to the shade of grey of the point with coordinates @xmath364 and @xmath363 . in accordance with the theory of compressed sensing ( and with remark [ rem : algb ] ) , we observe that the number of measurements needs to grow only logarithmically in the dimension @xmath364 to guarantee good approximation with high probability .",
    "the right part of figure [ fig : approx - b ] then shows the average value of @xmath361 for the same profile and sparsity for three different pairs of @xmath371 .",
    "we observe , that especially for large dimensions even extremely small number of measurements guarantees already reasonable approximation errors .       with noisy measurements according to algorithm c ( left ) and a modification of algorithm a ( right ) .",
    "note , that only the @xmath58-axis of the left plot is logarithmic.,title=\"fig:\",width=226 ]   with noisy measurements according to algorithm c ( left ) and a modification of algorithm a ( right ) .",
    "note , that only the @xmath58-axis of the left plot is logarithmic.,title=\"fig:\",width=226 ]    figure [ fig : approx - c ] studies the performance of the recovery of the ridge vector @xmath8 from noisy measurements as described in algorithm c. we fixed the parameters @xmath372 , @xmath373 , and @xmath365 , the ridge profile @xmath357 and four different noise levels @xmath374 .",
    "we have used the @xmath217-magic implementation of dantzig selector , available at the web page of justin romberg at http://users.ece.gatech.edu/~justin / l1magic/. as the noise level gets amplified by the factor @xmath375 , when taking the first order differences , cf . , it is not surprising that the recovery fails completely for small values of @xmath20 . on the other hand , for large values of @xmath20 ,",
    "the correspondence between first order differences and first order derivatives gets weaker and the quality of approximation deteriorates as well .",
    "this effect is clearly visible from and , numerically , in the left part of figure [ fig : approx - c ] , where there is an optimal @xmath20 for the recovery of @xmath8 . strictly speaking",
    ", the functions considered in section [ stability - ridge ] were defined only on the unit ball @xmath333 , so that the value of @xmath20 in figure [ fig : approx - c ] should be limited to be smaller than @xmath376 .",
    "we have decided to include also larger values @xmath20 to exhibit the optimal @xmath20 , although for our profile and our parameters it lies outside of this interval .    although not discussed before , it is quite straightforward to modify the non - probabilistic algorithm a also to the case of noisy measurements .",
    "essentially , the gradient @xmath377 is then approximated by the first - order differences , this time corrupted by noise .",
    "we applied this approach to the profile @xmath357 and parameters just described with the results plotted in the right part of figure [ fig : approx - c ] .",
    "we observe that the approximation errors get much larger , demonstrating once again the success of dantzig selector .",
    "according to algorithm d with sparsity ( left ) and with noisy measurements ( right).,title=\"fig:\",width=230,height=226 ] according to algorithm d with sparsity ( left ) and with noisy measurements ( right).,title=\"fig:\",width=230,height=226 ]    in figure [ fig : approx - d ] we considered the approximation of the pole @xmath8 of a shifted radial function @xmath3 with @xmath2 and @xmath378 . on the left plot , we fixed the sparsity @xmath365 and considered three values of @xmath379 and @xmath380 .",
    "the number of measurements was then @xmath381 , or @xmath382 , respectively .",
    "finally , we run the modification of algorithm d described in remark [ rem : thm : sing ] and plot the average approximation error @xmath383 against the step size @xmath20 .",
    "the right hand plot of figure [ fig : approx - d ] shows the noise - aware modification of algorithm d described also in remark [ rem : thm : sing ] .",
    "99 r. baraniuk , m. davenport , r. devore , m. wakin , a simple proof of the restricted isometry property for random matrices , constr .",
    "approx . 28",
    "( 2008 ) 253263 .",
    "h. boche , r. calderbank , g. kutyniok , j. vybral , a survey of compressed sensing , applied and numerical harmonic analysis , birkhuser , boston , to appear .",
    "buhmann , a. pinkus , identifying linear combinations of ridge functions , adv . in appl .",
    "22 ( 1999 ) 103118 .",
    "e. cands , harmonic analysis of neural networks , appl .",
    "comput . harmon .",
    "anal . 6 ( 1999 )",
    "e. cands , the restricted isometry property and its implications for compressed sensing , compte rendus de lacademie des sciences , paris , serie i 346 ( 2008 ) 589592 .",
    "e. cands , d.l .",
    "donoho , ridgelets : a key to higher - dimensional intermittency ?",
    "ser . a math .",
    "357 ( 1999 ) 24952509 .",
    "e. cands , t. tao , decoding by linear programming , ieee trans .",
    "theory 51 ( 2005 ) 42034215 .",
    "e. cands , t. tao , the dantzig selector : statistical estimation when @xmath384 is much larger than @xmath385 , ann .",
    "stat . 35 ( 2007 ) 23132351 .",
    "e. cands , j. romberg , t. tao , robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , ieee trans .",
    "theory 52 ( 2006 ) 489509 .",
    "a. cohen , w. dahmen , r. devore , compressed sensing and best @xmath386-term approximation , j. amer .",
    "( 2009 ) 211231 .",
    "a. cohen , i. daubechies , r. devore , g. kerkyacharian , d. picard , capturing ridge functions in high dimensions from point queries , constr .",
    "35 ( 2012 ) 225243 .",
    "davenport , m.f .",
    "duarte , y.c .",
    "eldar , g. kutyniok , introduction to compressed sensing .",
    "compressed sensing , 164 , cambridge univ .",
    "press , cambridge , ( 2012 ) r. devore , g. petrova , p. wojtaszczyk , instance - optimality in probability with an @xmath217-minimization decoder , appl .",
    "( 2009 ) 275288 .",
    "r. devore , g. petrova , p. wojtaszczyk , approximation of functions of few variables in high dimensions , constr .",
    "33 ( 2011 ) 125143 .",
    "donoho , compressed sensing , ieee trans .",
    "inform . theory 52 ( 2006 )",
    "m. fornasier , k. schnass , j. vybral , learning functions of few arbitrary linear parameters in high dimensions , found .",
    "( 2012 ) 229262 .",
    "m. fornasier , h. rauhut , compressive sensing , in : scherzer , otmar ( ed . )",
    "handbook of mathematical methods in imaging , springer , pp .",
    "s. foucart , h. rauhut , a mathematical introduction to compressive sensing , applied and numerical harmonic analysis , birkhuser , boston , 2013 .",
    "t. hemant , v. cevher , active learning of multi - index function models , in advances in neural information processing systems 25 ( 2012 ) 14751483 , available at http://books.nips.cc/papers/files/nips25/nips2012_0701.pdf v. ya .",
    "lin , a. pinkus , fundamentality of ridge functions , j. approx .",
    "theory 75 ( 1993 ) 295311 .",
    "litvak , a. pajor , m. rudelson , n. tomczak - jaegermann , smallest singular value of random matrices and geometry of random polytopes , adv .",
    "( 2005 ) 491523 .",
    "logan , l.a .",
    "shepp , optimal reconstruction of a function from its projections , duke math .  j.  42 ( 1975 ) 645659 .",
    "s. mayer , t. ullrich , j. vybral , entropy and sampling numbers of classes of ridge functions , submitted , available at http://arxiv.org/abs/1311.2005 .",
    "e.  novak , h.  woniakowski , approximation of infinitely differentiable multivariate functions is intractable , j.  compl .  25 ( 2009 ) 398404 .",
    "e. novak , h. woniakowski , tractability of multivariate problems , volume i : linear information .",
    "ems tracts in mathematics , vol . 6 , eur .",
    "house , zrich , 2008 .",
    "e. novak , h. woniakowski , tractability of multivariate problems , volume ii : standard information for functionals .",
    "ems tracts in mathematics , vol .",
    "house , zrich , 2010 .",
    "e. novak , h. woniakowski , tractability of multivariate problems , volume iii : standard information for operators .",
    "ems tracts in mathematics , vol .",
    "house , zrich , 2012 .",
    "a. pinkus , approximating by ridge functions , surface fitting and multiresolution methods ( 1997 ) 279292 .",
    "a. pinkus , approximation theory of the mlp model in neural networks , acta numerica 8 ( 1999 ) 143195 .",
    "k. schnass , j. vybral , compressed learning of high - dimensional sparse functions , in : ieee int .",
    "conf . on acoustics , speech and signal processing ( icassp ) ( 2011 ) 39243927 .",
    "p. wojtaszczyk , complexity of approximation of functions of few variables in high dimensions , j. compl . 27",
    "( 2011 ) 141150 ."
  ],
  "abstract_text": [
    "<S> we present effective algorithms for uniform approximation of multivariate functions satisfying some prescribed inner structure . </S>",
    "<S> we extend in several directions the analysis of recovery of ridge functions @xmath0 as performed earlier by one of the authors and his coauthors . </S>",
    "<S> we consider ridge functions defined on the unit cube @xmath1^d$ ] as well as recovery of ridge functions defined on the unit ball from noisy measurements . </S>",
    "<S> we conclude with the study of functions of the type @xmath2 .    </S>",
    "<S> ridge functions ; high - dimensional function approximation ; noisy measurements ; compressed sensing ; dantzig selector 65d15 , 41a25 </S>"
  ]
}