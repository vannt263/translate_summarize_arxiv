{
  "article_text": [
    "several earth - based interferometric experiments for the detection of gravitational waves ( gw ) are currently under development , and expected to reach the data - taking stage in the near future . on a longer time",
    "scale , space - based experiments are foreseen @xcite .",
    "these experiments will search , among other , for gw generated by inspiralling compact binary - star systems .",
    "the expected functional form of the signal produced by a coalescing system is known to good approximation @xcite , so matched filtering is an effective strategy to extract gw signals from the noise background .",
    "matched filtering is basically obtained by projecting the experimental output ( signal plus noise ) onto the expected theoretical signal , and is best done in fourier space , using fast fourier transform ( fft ) techniques ( see later for more details ) .",
    "the functional form of the expected signal depends however on the physical parameters ( e.g. , masses , angular momenta , eccentricity ) of the inspiralling system .",
    "it is necessary to match the experimental output to a set of expected signals ( so - called templates ) corresponding to points in the parameter space that cover the physical region of interest and are close enough ( under some appropriate metric ) to ensure sufficient overlap with any expected gw event .",
    "the number of needed templates for e.g. the virgo experiment is of order of @xmath0 , so the corresponding computational cost is huge by current standards .",
    "a nice requirement is the possibility of real - time analysis of the experimental data , which means that the available computational power is enough to process experimental data at the rate at which they are produced , so a prompt `` trigger '' of a gw event is possible @xcite .",
    "matched filtering to a ( large ) set of templates is an obvious candidate for parallel processing of the simplest form , e.g. , data farming with all elements of the farm performing the same computation ( single program multiple data ( spmd ) processing ) .",
    "indeed , the experimental data stream is sent to all processors in the farm , each element performing the matching procedures for a subset of the physical templates .",
    "massively parallel specialized spmd architectures , with peak processing power of the order of 1 tflops have been developed by several groups to fulfill the computational requirements of lattice gauge theories ( lgt ) @xcite . in this paper",
    "we want to analyze the performance of one such system ( the apemille system @xcite ) for matched filtering of gw signals .",
    "this paper is not a proposal to use ape - like systems in an actual experimental ( the relative merits of different computer systems in a large experiment have so many facets that they can only be assessed by those directly working on it ) .",
    "rather , the potential usefulness of our work lies in the following : given the fast pace of development in the computer industry , an experiment will try to delay the commissioning of a production system to as late a point in time as possible , since huge gains in price and/or price / performance can be expected .",
    "this means that very large computing capabilities will not be available for much needed early tests and simulations .",
    "ape systems might provide an answer to this problem .",
    "the focus of this paper is the measurement of the performance of ape systems for matched filtering .",
    "some parts of the paper have however a more general scope and refer to general parallelization criteria for the problem at hand .",
    "this paper is structured as follows : section 2 briefly reviews the formalism of matched filtering .",
    "section 3 evaluates the associated computational cost in general terms and discusses some strategies to minimize this quantity .",
    "section 4 discusses the features of the ape systems relevant for the problem , while section 5 presents a procedure for allocation of templates to processors suitable for ape and general enough to adapt to other computer systems .",
    "section 6 presents the result of actual performance measurements made on ape , while section 7 contains our concluding remarks .",
    "in this section we briefly summarize the mathematical formalism recently developed to analyze matched filtering of gw signals from coalescing binaries .",
    "we closely follow the notation presented in @xcite .",
    "we call @xmath1 the interferometer output , which is the sum of the signal @xmath2 and the noise @xmath3 , while @xmath4 is a template .",
    "@xmath3 is characterized by its one - sided spectral density : @xmath5 = \\frac{1}{2}\\   \\delta(f_{1 } - f_{2})\\ s_{n}(|f_{1}|)\\ ] ] where @xmath6 $ ] means ensemble expectation value , tilde ( @xmath7 ) stands for fourier transformed functions and asterisk ( @xmath8 ) for complex conjugation .    for the sake of definiteness , we consider in the following templates computed to second post - newtonian expansion .",
    "they depend , in principle , on several parameters : the coalescing phase @xmath9 and coalescing time @xmath10 , and the parameters corresponding to the physical characteristics of the system , called intrinsic parameters and globally referred to by the vector @xmath11 .",
    "a template is precisely identified by @xmath12 .",
    "it is believed that the most relevant intrinsic parameters are the masses of the binary systems , so as a first approximation it is usual to neglect all intrinsic parameters except masses . in this approximation",
    ", @xmath11 is a vector of two components .    in a matched filter the signal to noise ratio ( snr )",
    "is usually defined by @xmath13 where @xmath14 is a particular inner product defined as : @xmath15    it can be shown that @xmath16 , so ( [ s / n ] ) simplify to @xmath17 , if normalized templates are used @xcite .    filtering a signal means to look for local maxima of the signal to noise ratio , in terms of its continuous parameters .",
    "the maximization over the phase @xmath9 can be done analytically ( it can be seen that the maximum value is obtained computing two inner product as in ( [ prod ] ) on two real templates with opposite phases and then summing their square values @xcite ) .",
    "maximization over @xmath10 instead is achieved at low computational cost calculating the cross correlations by the fft algorithm .",
    "maximizations over the intrinsic parameters are not possible analytically .",
    "for this reason the normal procedure consists in a discretizations of templates in the space of the intrinsic parameters .",
    "the obvious question concerns the number of templates needed to cover the whole parameter space .",
    "a differential geometrical approach has been developed recently @xcite .",
    "one introduces a new function , the _ match _",
    "@xmath18 , which is the product of two templates with different intrinsic parameters , where a maximization is assumed over @xmath10 and @xmath9 :    @xmath19    the match between two templates with near equal parameters may be taylor expanded @xmath20 suggesting the definition of a metric @xmath21 @xmath22    in the limit of close template spacing we have an analytical function able to measure the distance between templates in the intrinsic parameter space .",
    "the metric @xmath23 depends on the intrinsic parameters so the real volume covered by a template varies locally .",
    "this effect can be reduced writing the templates in terms of some new variables for which the metric is more regular .",
    "one suitable choice is the following : @xmath24 @xmath25 where @xmath26 is the total mass of the binary system , @xmath27 the reduced mass , and @xmath28 an arbitrary frequency .",
    "this change of variables makes the metric tensor components constant at the first post - newtonian order , so only small @xmath11-dependent contributions are present at the second order approximation .",
    "it is now possible to simply estimate the total number of templates necessary to recover the signal at a given level of accuracy .",
    "we calculate the volume covered by a single template in the parameter space in term of a minimal value for the match , the so called _ minimal match _ @xmath29 which states a minimal requirement on signal recovering capabilities .",
    "for example , if we simply use a face centered hyper - cubic lattice , we can write the maximum covering volume with : @xmath30 where @xmath31 is the dimension of the parameter space ( 2 in our example ) .    an approximate estimation of the total template number , applicable when @xmath32 is very large @xcite ,",
    "is given taking the ratio between the total volume of the physically relevant parameter space and the volume covered by a template placed in the center of a lattice tile @xmath33 using ( [ numtot ] ) we estimate that in the range from @xmath34 to @xmath35 solar masses the total template number is roughly @xmath36 for ligo and @xmath37 for virgo ( see section 5 for the additional assumptions involved in this calculation ) .",
    "a last remark we want to make is that the minimal match requirement also determines a threshold value for the signal ( and templates ) sampling frequency .",
    "this frequency can be simply estimated and will be take into account later on in our computational estimates .",
    "in this section we present some observations about a general strategy to compute correlations . here",
    "we consider an _ ideal _",
    "case in which most computer - related issues are neglected .",
    "we also limit our treatment only to the _ stored templates strategy _ , where templates are pre - calculated , then fourier transformed and prepared to be processed and finally stored in memory .",
    "this ideal case is not unrealistic , given the pace at which actual memory sizes increase in real computers .",
    "the quantity to be evaluated on every template is given by    @xmath38    where @xmath39 is the fourier transform of a complex template @xmath1 .",
    "+ at present the best way of compute @xmath40 uses a fft algorithm , reducing the number of needed operations from @xmath41 to @xmath42 .",
    "the fft algorithm assumes input periodicity , while in our case signal and templates are not repeated data .",
    "the usual trick to overcome this problem @xcite consists in _ padding _ with a certain number of zeros the tail of the templates to be processed .",
    "assume that the template has @xmath43 points .",
    "we pad it so its total length become @xmath44 , and then compute the correlation by using the padded template and @xmath44 signal points .",
    "the resulting correlations are only valid in their first @xmath45 points , all remaining points being affected by the periodicity assumption implied in the fft technique .",
    "we define padding - ratio the quantity @xmath46 .",
    "the result obtained in this way covers a time - period of length @xmath47 , where @xmath48 is the sampling frequency of the experimental signal .",
    "the last @xmath43 data - points will have to be re - analyzed in a successive analysis .",
    "the computing power necessary for an on - line analysis of templates of given @xmath43 and @xmath44 ( floating point operations per second ) is given by :    @xmath49    @xmath50 and @xmath51 are constants , usually of the same order , depending on the specific algorithm used .",
    "in this paper we use a simple - minded fft algorithm for power of two length vectors that involves @xmath52 and @xmath53 for the whole analysis .",
    "although more general and efficient algorithms exist , our choice does not influences strongly the following observations and final results .",
    "one interesting question concerns the optimal padding that minimizes computing requirements . if one disregards the fact that ( [ costab ] ) holds only for @xmath54 values that are powers of 2 , the answer is given by fig.[minimum ] , where the minimum in @xmath44 of eq .",
    "14 is plotted as a function of @xmath43 , for @xmath55 .",
    "the behavior is very close to a logarithmic function in @xmath43 , so computing costs depend very weakly on @xmath43 .",
    "this result is obtained for an optimal choice of @xmath54 , as discussed above . as shown in fig.[best ] , the optimal value for @xmath44 grows with @xmath43 , implying in principle very large memory requests . in practice",
    "however ( see again fig.[best ] ) a value of @xmath56 is very close to the optimal case for reasonable values of @xmath43 .",
    "this finally means that deviations from the optimal padding length do not produce drastic consequences on the computing power needed to perform the analysis , and that @xmath57 can be easily adjusted to a suitable power of two .",
    "the ape family of massively parallel processor has been developed in order to satisfy the number crunching requirements of lattice gauge theories ( lgt)@xcite . machines of the present ape generation ( apemille ) are installed at several sites , delivering an overall peak processing power of about 2 tflops .",
    "the largest sites have typically 1000 processing nodes ( i.e. , 520 gflops ) @xcite . sustained performance on production - grade lgt codes is about 45 % of peak performance .",
    "a new ape generation ( apenext ) is under development , and expected to reach the physics - production stage in early 2004 .",
    "@xmath58 peak performance installations are being considered .",
    "apemille systems are based on a building block containing 8 processing nodes ( processor and memory ) running in single instruction multiple data ( simd ) mode .",
    "each processor is optimized for floating point arithmetics and has a peak performance of 500 mflops in ieee single precision mode .",
    "the processors are logically assembled as the sites of a @xmath59 mesh , with data links connecting the edges .",
    "this arrangement is called a `` cluster '' or a `` cube '' .",
    "large apemille systems are based on a larger 3-dimensional mesh of processor , based on replicas of the above - described building block .",
    "the resulting mesh has a full set of first neighbor communication links . in a typical lgt application",
    "the whole system works in lock - step mode as a single simd system .",
    "more important for the present application , each cube is able to operate independently , running its own program under the control of a linux - based personal - computer acting as a host .",
    "there is one host machine every 4 cubes .",
    "a set of up to 32 cubes ( i.e. , 256 nodes ) and the corresponding 8 host machines is a fully independent unit housed in a standard - size mechanical enclosure .",
    "each cube has access to networked disks with a bandwidth of about 4 mbyte / sec . in some apemille installations ,",
    "disks have been mounted directly on the host pcs .",
    "in this case , bandwidth increases approximately by a factor 4 .",
    "the next generation ape system ( apenext ) is , for the purposes of the present discussion , just a faster version of the same architecture .",
    "the only ( welcome ) architectural difference is the fact that the basic logical building block ( capable of independent operation ) is now just one processing node .",
    "a large apemille system can be seen as a large farm of processors , whose basic element is a simd machine of dimension 8 .",
    "a better way to look at the simd cluster in our case follows the paradigm of vector computing : the simd cluster applies the input signal to a vector of 8 templates and produces a vector of 8 correlations . in a variation of the same method",
    ", the same template could be present on all nodes of the simd cluster , and correlations at 8 staggered time points could be computed .",
    "since the number of correlations is of the order of @xmath0 , each element of a large farm ( say @xmath60 simd clusters ) takes responsibility for several hundreds or thousands of templates .",
    "this is good news , since ape processors can exploit vector processing within the node to reach high efficiency ( we just recall here for reader interested in architectural details that vector processing effectively helps to hide memory access latencies ) .",
    "we have written an ape code performing all the steps needed for matched filtering on a pre - calculated ( and pre - fft transformed ) set ( vector ) of @xmath61 templates each of length @xmath62 , and measured its performance on an ape cluster .",
    "an analysis of the details of the apemille processor suggest to model the computation time @xmath63 as    @xmath64    @xmath65 is related to the complexity of the computation , that we model as @xmath66 , following eq.[costab ] and introducing one more parameter ( @xmath67 ) covering machine effects .",
    "@xmath68 is a measure of the processor efficiency as a function of the vector length @xmath61 , that we normalize to @xmath69 .",
    "taking into account that the computation is memory - bandwidth limited ( as opposed to processing - power limited ) , we adopt the following functional form for @xmath68 :    @xmath70    measured and fitted values for @xmath65 and @xmath68 are shown in fig.[f_n ] and fig.[g_k ] respectively .",
    "apemille efficiencies are smooth functions of @xmath44 and @xmath61 .",
    "a rather good value of @xmath71 , including all computational overheads , is possible when large sets of templates ( @xmath72 ) are used .",
    "a general templates allocation strategy on real computers has to take into account the limited size in memory and the available computing power available . here",
    "we present some quantitative aspects of memory and cpu usage involved in our analysis , then we give our allocation criteria for the optimal template number manageable by a single processor .",
    "this discussion focuses on criteria that are appropriate for the ape family of processors .",
    "the focus is to exploit vectorization as much as possible and to find ways to reduce input - output bandwidth requirements , so our discussion can be applied to a larger class of processors .",
    "we start from memory .",
    "each processor has @xmath61 stored templates of similar length @xmath73 .",
    "( in the apemille case , the term processor must be understood to refer to the basic cluster of 8 processing element ) .",
    "vector processing of all the templates requires that they are all padded to the same @xmath54 , so we need @xmath61 arrays of @xmath44 complex words , and matching space for the final correlation results .",
    "there are two basic memory allocation strategies : we may assign different sets of @xmath61 templates to each element in a basic 8 processor cluster , and have all of them compute the corresponding correlations for the same time stretch @xmath74 , so each cluster computes @xmath75 correlations .",
    "alternatively , we may assign the same set of templates to all processing elements and have each of them compute correlations for different time intervals . with this choice @xmath61 correlations",
    "are computed for a longer time stretch @xmath76 .",
    "the best choice between these two nearly equivalent cases is based on bandwidth constraints . in apemille",
    ", data items reaching the cluster can be delivered to just one element , or broadcast to all of them . in the latter case , bandwidth is effectively multiplied by a large factor ( @xmath77 ) , so there is an advantage if large data blocks must be broadcast to the complete cluster .",
    "we will use quantitatively these observations later on in this section .",
    "we now consider processing power .",
    "the real - time requirement stipulates that each processor cluster completes processing all its templates within an elapsed time @xmath74 ( or @xmath76 ) .",
    "as shown later on , for several realistic templates sizes , the processing time @xmath78 is much shorter that the elapsed time for the @xmath61 value allowed by memory constraints .",
    "we may therefore try to use the same cluster for a different set of templates .",
    "this may become inefficient since loading a large data base ( the new set of templates ) may be a lengthy procedure .",
    "this cost may be reduced by using the same templates several times ( corresponding to longer elapsed times ) before loading a new set of templates .",
    "we disregard the overhead associated to the output of the computer correlations , that can be made very small taking into account the gaussian character of the noise ( e.g. a @xmath79-cut could reduce the number of the output correlations to the order of @xmath80 ) .",
    "more interestingly a cross correlation among closely spaced templates could be performed on line packing more densily the available information .",
    "we would like to optimize among these conflicting requirements .",
    "let us consider the total compute time both for different sets of templates ( case 1 ) or the same set of templates ( case 2 ) on each cluster element .",
    "we have    * case 1 : we want to compute @xmath81 sets of @xmath82 correlations each on templates of length @xmath54 , corresponding to the same time interval .",
    "we compute correlations on @xmath83 adjoining time intervals before switching to a new set of templates .",
    "the computation time can be modeled as @xmath84 + where b is the cluster input - output bandwidth ( measured in words per unit time ) .",
    "the first term in ( [ cost1 ] ) is the time required to load the templates on all processors , the second term is the time needed to broadcast @xmath54 signal points to all cluster elements while the third term refers to the actual computation , to be performed @xmath85 times .",
    "templates , correlations and input data must fit inside the memory , implying that @xmath86 , where @xmath87 is the available memory on each node ( measured in units of complex words ) .",
    "also , the computation must complete in a time interval @xmath88 .",
    "in ( [ cost1 ] ) we assume that all data - points are loaded once .",
    "this reduces input - output time but reserves a large fraction of memory space to data - points ( as opposed to templates ) . alternatively ( case 1b )",
    ", we may load a smaller set of data - points every time we start a new computation .",
    "the corresponding compute time becomes + @xmath89 + while the memory constraint changes to @xmath90 . for any physical template of length @xmath73",
    ", we must maximize @xmath91 in terms of @xmath81 , @xmath61 , @xmath83 and @xmath54 satisfying all constraints .",
    "* case 2 : the procedure discussed above can be applied also in this case .",
    "the corresponding processing time is given by @xmath92 this equation differs from ( [ cost1 ] ) since we now broadcast templates while we load different data - points to each processing elements .",
    "the memory constraint is the same as in case 1 , while the maximum allowed processing time is @xmath93 .",
    "case 2b ( multiple data loads ) is also easily computed as @xmath94 in case 2 , we are interested in optimizing @xmath95 in terms of the same parameters as in the previous case .",
    "there is one free parameter in the optimization process ( @xmath83 ) .",
    "if we increase @xmath83 we reduce the relative cost associated with template loading , but increase the latency associated to the computation .",
    "we arbitrarily decide to keep @xmath83 small enough so the latency for any @xmath73 is not longer that a fixed amount of time @xmath96 .",
    "we choose @xmath96 as the time length of the longest template contained in the set .",
    "this choice may be useful also for data - organization purposes : every @xmath96 time interval all correlations corresponding to templates of all lengths are made available .",
    "the result of the optimization process are given in table 1 for apemille and table 2 for apenext .",
    "results depend weakly on the allocation procedure discussed above , and are largely dominated by the sustained processing power .",
    "bandwidth limitations are neatly dealt with : if we increase the available bandwidth by a factor four ( e.g. , using local disks ) the number of templates handled by each cluster increases by less than 10% . with our choice of parameters case 1b",
    "is the preferred one for almost all template lengths .",
    "+    .number of templates handled by each apemille processor cluster , as a function of the template length @xmath73 .",
    "parameters are ( see the text for definitions ) @xmath97 , @xmath98 , @xmath99 .",
    "numbers in bold flag the best case , while @xmath100 mark cases where allocation can not be performed due to memory limits . [ cols=\"<,>,>,>,>\",options=\"header \" , ]     first , we show in fig.[ideal ] the total computational cost to compute the correlation for binary systems whose masses are in a range of @xmath101 to @xmath102 solar masses , as a function of @xmath101 , under the assumption of optimal padding .",
    "we use the parameters listed in tables [ noise ] and [ cuts ] .",
    "the computational cost roughly follows a ( fitted ) power - law behavior , with exponent of the order of @xmath103 .",
    "this behavior can be easily guessed , taking advantage of the fact that the computational load of each template depends very weakly on its length , and that the @xmath104 depends weakly on the @xmath105 variables . under these assumptions",
    "the computational cost scales up to log - corrections as the area of the region in @xmath105 space corresponding to a given interval of allowed star - masses .",
    "the latter can be easily shown by power counting to behave as @xmath106 .",
    "+    the large difference in computational cost between the two experiment , clearly noticeable in fig.[ideal ] , derives , although in a complex way , from the different noise spectra and from the correspondingly different frequency cuts .",
    "we now specialize the discussion to ape systems .",
    "we proceed establishing a mass interval , then generating its template distribution .",
    "we `` stretch '' template lengths to the nearest power of two larger than the actual length ( a slightly pessimistic assumption ) .",
    "finally we divide each group of templates of equal length by the corresponding number of templates handled by one processor cluster ( the bold numbers in tab.1 and tab.2 ) , and sum all the resulting quotients . the final result",
    "represent the number of ape processor needed to satisfy the real time requirement on the given mass interval .",
    "the computational cost of this matching filter analysis is particularly sensible to the lower mass limit because of the increasing template length and of the irregular behavior of the metric tensor @xmath107 in that region of the parameter space .",
    "for this reason it is useful to plot the number of processor versus the lower mass limit . the number of nodes ( one cluster consist of 8 nodes ) for a mass interval from @xmath101 to @xmath108 is plotted in fig . [ proc ] , where we use noise spectra relevant for ligo and virgo .",
    "this complete our analysis .",
    "in this paper we have developed a reliable estimate of the computational costs for real - time matched filters for gw search from binary star systems , in a massively parallel processing environment .",
    "we have analyzed some criteria to optimally allocate the processing load to a farm of processors .",
    "we have written a code performing the analysis on an ape system and we have measured its performances .",
    "our result is that available ( apemille ) systems are able to satisfy the requirements of a real - time analysis of the complexity corresponding to the ligo experiment in the mass range between 0.25 and 10 @xmath109 .    the virgo experiment ( with its lower and wider noise curve ) has substantially larger computing requirements that can not be fulfilled by an apemille system in the same mass range .",
    "the new ape generation , expected to be available in early 2004 , partially closes this performance gap .",
    "we thank f. vetrano for reading our manuscript .",
    "t. giorgino and f. toschi wrote the fft code for apemille .",
    "this work was partially supported by neuricam spa , through a doctoral grant program with the university of ferrara .      for a review ,",
    "see for instance : a.rudiger a.brillet k.danzmann a.giazotto and j.hough c.r.acad.sci .",
    "paris t.2 , series iv , 1331 ( 2001 ) , and `` proceedings of the 4th e.amaldi conference , perth(2001 ) '' , in class . quantum gravity , 19 ( 7 ) ( 2002 ) and references therein .",
    "l.blanchet , t.damour , b.r.iyer , c.m.will and a.g.wiseman , phys.rev.lett .",
    "@xmath110 3515 ( 1995 ) , see also l.blanchet c.r.acad.sci .",
    "paris , series iv 2 , 1343 ( 2001 ) .",
    "b.allen _ et al .",
    "_ phys.rev.lett .",
    "@xmath111 1498 ( 1999 ) .",
    "n. h. christ , nucl .",
    "b ( proc . suppl . ) @xmath112 , 111 ( 2000 ) .",
    "r. tripiccione , parallel computing @xmath113 , 1297 ( 1999 ) .",
    "c.cutler and .e.flanagan , phys.rev.d @xmath114 , 2658 ( 1994 ) .",
    "b.j.owen , phys.rev.d @xmath115 , 6749 ( 1996 ) . b.j.owen and b.s.sathyaprakash , phys.rev.d @xmath114 , 2002 ( 1999 ) .",
    "et al . _ , numerical recipes in c _ the art of scientific computing _ , cambridge university press .",
    "p.canitrot , l.milano , a.vicer _ computational costs for coalescing binaries detection in virgo using matched filters _ , vir - not - pis-1390 - 149 , issue 1 , 5/5/2000 .",
    "a.vicer _ optimal detection of burst events in gravitational wave interferometric observatories _ arxiv : gr - qc/0112013 , phys.rev.d in press . see table ii and reference [ 43 ] therein .",
    "a.bartoloni _ et al . _",
    "nucl.phys.b ( proc.suppl ) 106 - 107 1043 ( 2002 ) ."
  ],
  "abstract_text": [
    "<S> in this paper we discuss some computational problems associated to matched filtering of experimental signals from gravitational wave interferometric detectors in a parallel - processing environment . </S>",
    "<S> we then specialize our discussion to the use of the apemille and apenext processors for this task . </S>",
    "<S> finally , we accurately estimate the performance of an apemille system on a computational load appropriate for the ligo and virgo experiments , and extrapolate our results to apenext .    ,    ,    ,    ,    gw interferometric detectors ; coalescing binaries ; parallel computing </S>"
  ]
}