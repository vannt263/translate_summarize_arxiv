{
  "article_text": [
    "wikipedia is the largest source of open and collaboratively curated knowledge source in the world",
    ". introduced in 2001 , it has evolved to be a very useful repository for entities , events , concepts etc .",
    "entities and event pages are often created and collaboratively edited creating a knowledge source which is both authentic and recent . as a result , this invaluable resource has found application in information extraction and knowledge base construction , e.g.yago  @xcite and dbpedia  @xcite , text categorization  @xcite , entity disambiguation  @xcite , and entity ranking  @xcite .    owing to events being increasingly documented in online media , existing entities in wikipedia continuously evolve and new entities are added .",
    "moreover , online news has seen a lot of growth of late , and records important events reasonably quickly . consequently , a high proportion of the entity pages in wikipedia ( pages devoted to named entities like persons , organizations and locations ) have news articles as references , a factor which suggests that news is an authoritative source for important facts ( we do a detailed analysis of the density in section  [ sec : news - dyn ] ) .",
    "automatic knowledge base construction tasks can rely on news as a source or an indicator to add or update entities .",
    "first , news could be a primary source for addition of emerging entities  @xcite .",
    "secondly , knowledge bases which harvest this resource need to periodically refresh their contents .",
    "they constantly deal with the natural trade - off between the cost of maintenance of a fresh and consistent state with the loss of useful information .",
    "for newsworthy entities and events , understanding this delay in appearing in wikipedia would suitably help knowledge bases to improve their maintenance or characterize the information loss .",
    "in this work , we study how fast wikipedia reacts to these real world events captured by news collections .",
    "we carry out our study on the wikipedia revision history and the new york times news corpus for the overlapping years between 2001 and 2007 .",
    "we extract entities from the news articles and link them to the version of the entity page which is closest in time to the publication time of the article . in other words , we _ align _ the news collection to the wikipedia versions using entities as proxies .",
    "next , we define _ lag _ as the time difference between when an entity or event was reported in news and the first time it appeared in wikipedia .",
    "this aligned resource allows us to carry out several studies which shed light on the evolution of entities and events and on how they are captured in wikipedia .",
    "specifically , we try to answer the following questions :    * _ what fraction of external references in entity pages are news articles ? _ * _ how much does wikipedia lag behind news articles ?",
    "how has this lag evolved over time ? _ * _ which categories or classes of entities in news lead or lag wikipedia ? _ * _ how do events reported by news articles lag with the wikipedia event pages ? _",
    "we perform our alignment studies on the entire english wikipedia revision history and the new york times collection ( as the news corpora ) .",
    "we also consider wikipedia s current events portal as a repository for high quality manually created resource for events .",
    "some of the highlights of our study are :    * approximately 20% of all external references in entity pages are news articles .",
    "* entity lag follows a distinct normal distribution and show that wikipedia has been catching up on news ever since it was introduced .",
    "* unlike entities , events are quickly reflected in wikipedia as soon as they are reported in news .",
    "* events are responsible for creation of emergent entities , with 12% of the entities mentioned in event pages being created after the creation of the event page .",
    "the rest of this paper unfolds as follows . in section  [ sec : alignment ]",
    "we introduce the experimental setup and the generated data .",
    "section  [ sec : news - dyn ] provides an overview of the news dynamics in wikipedia and serves as a motivation to our study , while in section  [ sec : entity - lag ] and [ sec : event - lag ] we provide thorough analysis and results on entity and event lag . in section  [ sec : related - work ] we review related literature relevant to our work . finally in section  [ sec : conclusion ] we conclude the findings in our analysis between wikipedia and news corpora such as nyt .",
    "the related work and state of the art with respect to our work can be classified into the following three parts :    * wikipedia studies * goes into similar directions with our analysis .",
    "kittur et al .",
    "@xcite analyses the collaborator structure of wikipedia .",
    "they further classify the collaborators into five different classes based on the number of revisions .",
    "furthermore , they measure the population growth of the collaborators falling into the five different classes . in their paper the authors conclude an interesting observation of the shift of how content is mostly provided by collaborators with lower number of edits , due to the increased fraction of such users in the wikipedia community structure .",
    "this , however , does not correlate with any decline of the content provided by collaborators with high number of edits , hence , is accounted to the higher fraction of low edit users .",
    "in contrast to the work from kittur et al . , we have a different focus in our analysis , namely that of entity and event lag in wikipedia , without any distinction of the wikipedia community structure . in @xcite",
    "the authors analyze several aspects of wikipedia s editors .",
    "they conclude that the number of edits is decreasing .",
    "another slightly related work  @xcite analyzes the number of research papers about wikipedia , here too they conclude that the number has been decreasing , however , papers that use wikpedia s data has seen an increase .",
    "closely related work is done by keegan et al .",
    "@xcite . their work , similarly to ours , focuses on the dynamics of wikipedia s coverage of real world entities . in  @xcite ,",
    "the authors consider emerging events like the thoku catastrophe . in the case of such high dynamic events ,",
    "it is found out that for localized wikipedias ( e.g. japanese ) , the corresponding event appears only after six minutes after the event , whereas in the english wikipedia , it appears in less than an hour .",
    "furthermore , they analyze the co - authorship of such articles in wikipedia .",
    "it is concluded that within wikipedia there are sub - communities that edit articles of the same topic . as a continuation of their work , in  @xcite the social network structure of wikipedia collaborators",
    "is analyzed .",
    "the analysis is based on four main hypotheses that are based on two main set of attributes , article and editor attributes , respectively .",
    "the first hypothesis validates the fact that for breaking news articles attract more editors .",
    "the second hypothesis validates the co - authorship of articles in wikipedia from collaborators that are categorized into three main classes : _ experienced , apprentice , non - expert_. significant collaborations between the three classes of collaborators is found only on _ contemporary _ articles ( articles are divided into _ breaking , contemporary , historical _ ) between _ apprentice _ and _ experienced _ collaborators .",
    "the third hypothesis , analyzes the editor attributes and implies that experienced editors will edit more articles than others .",
    "the third hypothesis leads to the fourth and last hypothesis .",
    "it analyzes the fact that experienced editors are more likely to contribute to similar types of articles rather than to dissimilar .",
    "strong correlation is found for editors belonging to the _ apprentice _ class and for most of the article types .",
    "in contrast to our analysis the work by keegan et al . has as a main focus modeling the network structure of editors and how this reflects on the dynamics of wikipedia and contemporary and emergent entities and events .",
    "on the other hand , in our analysis we focus on larger real world news corpora which inherently represent emerging entities and events .",
    "in addition , we also distinguish the lag for different entity types . as a last diverging point in our work , is the analysis of how entities are co - created and its impact on the entity lag .",
    "* entity interlinking * tries to detect links between entities withing a knowledge base . the work done by nunes et al .",
    "@xcite uses social network theory measures , such as katz index to find links between entities .",
    "this is related to our work since we analyze the co - referencing of entities within wikipedia , their collaborator structure and interlinking with events in the wikipedia s event portal .",
    "such attributes of entities are used to analyze their implications on the entity lag in wikipedia against news corpora .    *",
    "first story detection * typically deals with event onset identification from a stream of text . in  @xcite ,",
    "osborne et al . , analyze twitter data for first story detection .",
    "wikipedia in this case is used through its entity / event page views to filter tweets that do not represent events .",
    "the two sources of information are considered as streams which later on are mapped , by simply checking the spikes of page views for a certain entity / event in a tweet . in our case , the focus is at modeling between two sources of information , wikipedia and nyt corpus , rather than its usage for story detection .",
    "in this section we introduce the experimental setup . to carry out our experiments we first align the two collections , wikipedia and nyt corpus .",
    "the resulting dataset is referred to as the _ news - wiki aligned collection _ or simply the _ aligned collection_. the detailed descriptions of the datasets in our experimental setup are given below :    * * wikipedia * the _ english wikipedia revision history _",
    "@xcite , whose uncompressed raw data amounts to tbytes , contains the full edit history of the english wikipedia from january  2001 to december  2013 .",
    "we consider all versions of encyclopedia articles including versions that were marked as the result of a minor edit ( e.g. , the correction of spelling errors etc . ) .",
    "* * news * the _ new york times annotated corpus _",
    "@xcite comprises more than 1.8 million articles from the new york times published between 1987 and 2007 .",
    "every article has an associated publication time and we refer to this as the time of the article . since wikipedia was released in the year 2001 and our nyt corpora is valid until 2007 we consider sub - collections from both corpora in the time period between the years 2001 and 2007 . * * taxonomy * the taxonomy from the _ yago2 ontology _  @xcite which combines the clean taxonomy of wordnet with the richness of the wikipedia category system , assigning the entities to more than 350,000 classes .",
    "we also use dbpedia(resource of type ` dbpedia - owl : event ` ) to collect event pages along with their creation times .      before delving into detail in the lag analysis",
    ", it is necessary to introduce the entity and event notions .",
    "[ def : entity ] we define an entity as something which has a canonical ( i.e. , uniquely identifiable ) representation in wikipedia . in other words an entity represents a real world concept , e.g. ` people , organization , location ` , which might be mentioned in multiple forms in text .",
    "we refer to the wikpedia page dedicated to a given entity as an entity page .",
    "[ def : event ] it is defined as a real - world event that has a wikipedia article , e.g. ` u.s elections 2004 ` . the wikipedia article dedicated to the event",
    "is referred to as the event page .",
    "we now explain in details the experimental setup .",
    "as mentioned before entities are mentioned in text , in this case news , in multiple forms and this sometimes gives rise to the problem of ambiguity , i.e. , a given mention potentially refers to more than one entity .",
    "one way to resolve such ambiguities , is resolved through the task of _ entity linking_. _ entity linking _ maps such mentions of ambiguous names onto canonical entities registered in a knowledge base like dbpedia or yago .",
    "for this task we use _ tagme ! _",
    "@xcite .    to maintain high accuracy of the disambiguated entities , we filter out entities with a threshold below 0.3 ( values above 0.3 represent high disambiguation scores ) .",
    "additionally we manually evaluate a random sample of 1000 pairs of disambiguated entities and the corresponding text snippet in the news article .",
    "the evaluation took into account whether the disambiguated entity correctly represents the entity in the text snippet .",
    "the resulting accuracy of the tagme !",
    "tool after filtering entities , across the different entity types was on average above 0.9 .",
    "after filtering the number of disambiguated entities falls to 506,151 from 722,888 , with a drop of 30% in the number of entities .",
    "we analyze in total 1.8 million nyt articles , resulting in approximately 506,151 distinct entities .",
    "figure  [ fig : entity_year_dist ] shows the distribution of extracted entities for the years 20012007 , alongside the number of entities appearing in wikipedia .",
    "the final set of entities for our experimental analysis comprises of a collection of 180,478 entities that appear only for the years 2001 - 2007 .",
    "furthermore , the articles are linked to the corresponding state of an entity for the specific year it appears in a nyt articles .",
    "for this purpose we make use of the jwpl  @xcite , where given an entity name and a time reference , entity revisions can be retrieved .",
    "to start off we want to investigate how news impacts wikipedia by studying such news references in entity pages .",
    "an _ entity page _ refers to a wikipedia article dedicated to an entity . since knowledge bases are reasonable repositories of entities , we compile our set of entities from dbpedia .",
    "entity pages , typically contain references to qualify the stated facts therein .",
    "these references are broadly classified into the following sources  ` web , news , book , report and journal , etc . ` by wikipedia .",
    "we first study the distribution of news references(of type ` news ` ) in entity pages across multiple _ entity categories _ and the corresponding entity sections .",
    "for this experiment , we first crawled all news articles referenced in the entity pages that are still online .",
    "this resulted in a dataset of @xmath0 available news articles out of @xmath1 news references .",
    "we define the news reference density ( nrd ) of an entity page , as the fraction of news references over all references of all types in the page .",
    "similarly reference densities of other citation types are defined .",
    "we observe that , as expected , most of the references are from the ` web ` .",
    "however , the second most dominant type of reference are news references constituting 20% of overall references .",
    "the nrd varies across entity categories as shown in figure  [ fig : cite_density ] . while the category",
    "` officeholders ` ( mostly politicians ) has a high news density , on the other hand ` bands ` have high density for web references .",
    "the nrd in most cases is stable across years for the different entity categories as shown in figure  [ fig : domain_year_cite_density ] .",
    "however , there are slight variations on the reference density for specialized categories and the corresponding reference types , e.g. category ` legalcase ` and ` court ` reference types .    taking into account the organization of wikipedia entity pages into section",
    ", we analyze the distribution of news densities across sections in an wikipedia entities .",
    "we observe that sections in entity pages vary considerably across categories with only some of the sections being common among categories , e.g. ` _ _ early life _ _ ' and ` _ _ career _ _ ' .",
    "when we look at the partial contribution of the sections to the page news reference density , we observe that while ` _ _ early life and career _ _ ' in ` politicians ` have highest nrd contribution of 64% , the section ` _ _ sports team _ _ ' in ` athletes ` has the highest contribution of 19% .",
    "for the concept entity we refer to the definition in section  [ sec : alignment ] .",
    "an entity can have multiple ways in which it can be mentioned in text .",
    "the task of resolving these mentions to the actual entities is a field of _ entity disambiguation _ , _ record linkage _ and _ entity",
    "linking_. we utilize the output of such a linking task to identify entities in our target news corpus and link them to their corresponding entity pages ( see section  [ subsec : setup ] )",
    ".    however , many of the entity pages were created at different points in time .",
    "this can be attributed to two factors : _ inherent popularity of the entity _ , and _ evolution of authorship _ of entity pages in wikipedia .",
    "one explanation is that entities appearing in authoritative news sources like nyt reflect their popularity .",
    "figure  [ fig : cumm_dist ] shows the average entity mention distribution ( in nyt ) across years before the first appearance of an entity in wikipedia .",
    "this follows the assumption that an increase of entity mentions in news sources will eventually result in the creation of an entity in wikipedia . from figure",
    "[ fig : cumm_dist ] it is obvious that shortly before the entity creation in wikipedia , the entity is mentioned most in news .",
    "the second factor , is that wikipedia s authorship has increased with an ever growing number of editors , hence establishing itself as a independent source of information  @xcite , thus entities can be created from what is deemed as important by the editors in wikipedia .    to measure the time span between the entity mention and its creation time in wikipedia",
    "we define the _ entity lag _ below .",
    "we define this delay of the first appearance of an entity page relative to the first appearance of the entity mention in a news article as _ entity lag _ or simply _ lag _ @xmath2 .",
    "@xmath3 , where @xmath4 is the time when the first version of entity page of @xmath5 was authored and @xmath6 is the publication time of the first mention of @xmath5 in news .",
    "we now proceed to answer the first question of how the creation of entities in wikipedia lag their mentions in news .",
    "we denote the entities which have an absolute lag of less than a month as _ low lag entities _ , the ones with lag less than a year as _ medium - lag entities _ and the rest with a lag more than a year as _ high - lag entities_. figure  [ fig : hist - med - lag - dist - months ] shows the distribution of lag in months for a period of six years .",
    "second , we see that in the first year of wikipedia the average lag was high with a majority of entities in wikipedia lagging behind news .",
    "however , quite distinctly , the lag re - distributes towards a means of zero in the course of time into a gaussian or normal distribution .",
    "we also see that the absolute number of entities with a lag of zeros go up , and the standard deviation reduces . the lag distribution through the years shifts to a normal distribution , with most of the entities centered around the mean , which in our case is zero . because wikipedia only started after 2001",
    ", we also consider the entities which were _ emergent _ in news after 2001 ( denoted by the red histogram ) .",
    "an entity is considered as an _ emergent entity _",
    "( ee ) if its first mention in nyt is after the time when wikipedia was released , i.e. , january 2001 .",
    "we observe that the emergent entities , much like the existing entities , have the same distribution .",
    "since news articles are rich in political news and their coverage , we observe that emergent political topics and entities show low lag .",
    "an example is ` freedom fries ` which came into prominence in 2003 as a political euphemism for the actual french fries . on the other hand works of fiction like ` the lost city ` typically exhibit high lag .",
    "similar to the non - emergent entities the lag distribution for _ emergent entities _ is normal . in table",
    "[ tbl : sig_test ] we test for normality the lag distributions for the non - emergent ( nee ) and emergent entities ( ee ) .",
    "we use the shapiro - wilk test for a _",
    "p - value _ @xmath7 , in which case the hypothesis that the distribution is normal is rejected , otherwise for greater _ p - values _ the hypothesis is accepted .    based on the computed distributions",
    ", we can already provide a rough estimate of the fraction of _ ` newsworthy ' _ entities , which could be missed given a maintenance period .",
    "services that periodically update their entity repositories would lose around half of the entities if their update periods is greater than a month than if they update daily .",
    "however , there is not much gain in improving this maintenance period from 10 months to 9 months .    .",
    "[ cols=\"<,<,<\",options=\"header \" , ]",
    "we now turn to studying lag in real world events as documented by news articles . similar to the definition of entities , we define events as those which have a canonical representation via an _ event page _ in wikipedia . also similar to entities , there might be more than one articles which refer to the same event . we define lag as the publication time difference between the first news article which reports the event and the wikipedia event page . events reported in the news can be as a reaction to an event in the past , or a build up to an upcoming event .",
    "we do not make a difference in both these cases and treat the first news article reporting the event as the inception of the event .          in our final experiment",
    "we study how events influence the creation of entity pages in wikipedia . for this experiment we considered all event pages in dbpedia with their publication time ( resource of type ` dbpedia - owl : event ` ) . unlike the previous experiments we do not rely on the nyt corpora and hence can consider the entire wikipedia revision history .",
    "the notion of the publication is synonymous to our earlier notion , i.e. , the first time the event page was introduced in wikipedia .",
    "we then extracted all the entities in the event page which are explicitly linked ( i.e. linked to a valid wikipedia entity page ) in the most current version of the event page .",
    "next , we compared the publication times of the entities mentioned in the events page and the event publication time . to this effect",
    ", we make a simplistic assumption about the entities mentioned in the event page : _ entities created after the event page are created because of this event_.    based on this assumption , we define _ emerging entity density _ of an event page as the fraction of entities which were created after the event page .",
    "we refer to such entities as emerging entities ( note that this is different from the emergent entities present in the previous section ) .    as an example , consider the event page of the `` _ _ charlie hebdo shootings _ _ '' which was created in 7th january , 2015 .",
    "the entity `` _ _ corinne rey _ _ '' or `` _ _ coco _ _ '' who is mentioned in the event page became popular after the event and subsequently had an entity page created five days later on 12th january .",
    "the emerging entity density ( eed ) evolution from 2001 - 2010 is presented in figure  [ fig : eed - yearly ] where the y - axis represents the average emerging entity density of event pages in a given year .",
    "we have a total of 14,604 events with 179,981 entities with the exception of events from the last few years owing to the lack of event data in dbpedia for this period .",
    "we see that in the early years the eed of event pages was very high , sometimes above 80% , meaning most of the entities mentioned in the event pages were emerging .",
    "understandably , this declines every year resembling the phenomena of diminishing returns . however , we still see a high percentage of emerging entities in the recent event pages which point to the fact that event pages are great repositories of upcoming and emerging entities missing in the knowledge bases .",
    "we also observe that the curve , although decreasing , tends to stabilize in the recent years around 13% . finally , we look at the categories of emerging entities in figure  [ fig : stacked_person ] to find that people comprise the majority of the emergent entities consistently over the years . on the other hand , organizations were emergent between 2001 - 2005 but their eed contribution to event pages",
    "has been decreasing from 2006 onwards .",
    "wikipedia is an invaluable resource documenting entities and events and is used as a important input source for constructing knowledge bases .",
    "news articles on the other hand , we find are routinely cited in wikipedia , suggesting that they are high - quality and authoritative sources of facts about entities and events . in this work ,",
    "we attempt to understand how newsworthy entities and events flow into wikipedia by defining lag as the inter - appearance time in news and wikipedia .",
    "we use seven years of overlapping news and the wikipedia revision history to analyze how lag is distributed and how it has evolved over time .",
    "we see that the lag distribution is interestingly a normal distribution .    the implications of this study is manifold .",
    "first , it shows the promise of news collections as a resource for mining emerging entities .",
    "the normal distribution of the entity lag shows that almost 50% of the entities before occurring in wikipedia are already mentioned in news .",
    "hoffart et .",
    "al in  @xcite have initial attempts for discovering emergent entities in news and web streams .",
    "secondly , news is an invaluable resource for mining facts about entities and relations between entities . our experiments on news reference density show that a high proportion of the facts and relations about entities are qualified with a news reference .",
    "additionally our category- and section - wise analysis shows what kind of aspects of which entity - type can be found in news .",
    "secondly , entity and event repositories relying on wikipedia can now quantify the degree of loss or re - calibrate their update frequencies based on the lag distribution we provide .",
    "additionally , they can also optimize emergent entity coverage of entities by focusing on event pages . in the earlier years , wikipedia used to lag more than news in terms of entities , while this slowly converges to a normal distribution over the years .",
    "we also observe that the lag for events is far lower than entity pages , which means they get reported far quickly .",
    "thirdly , we also discover that event pages are nice containers for emergent entities with around 12% of the entities from event pages being emergent pages .",
    "there have been studies  @xcite which have reported the low growth rate in wikipedia .",
    "we attest their finding by showing that event pages in wikipedia , which contributed to a high number of entities , have a low emerging - entity density in the recent years .",
    "however , this low density might be because wikipedia has eventually achieved a steady state and yields diminishing returns for new entities .",
    "one of the limitations of the study is that new only consider the new york times collection which might be biased towards in news coverage and hence in the entity coverage .",
    "we hope that , given the size and international nature of nyt , the results might still be representative of the overall effect of news over wikipedia .",
    "this work was funded by the erc advanced grant alexandria under the grant number 339233 .",
    "johannes hoffart , yasemin altun , and gerhard weikum . discovering emerging entities with ambiguous names . in _",
    "23rd international world wide web conference , www 14 , seoul , republic of korea , april 7 - 11 , 2014 _ , pages 385396 , 2014 .",
    "johannes hoffart , fabian  m. suchanek , klaus berberich , edwin lewis - kelham , gerard de  melo , and gerhard weikum .",
    "yago2 : exploring and querying world knowledge in time , space , context , and many languages . in _ proceedings of the 20th international conference companion on world wide web _ , www 11 , pages 229232 , new york , ny , usa , 2011 .",
    "johannes hoffart , mohamed  amir yosef , ilaria bordino , hagen frstenau , manfred pinkal , marc spaniol , bilyana taneva , stefan thater , and gerhard weikum .",
    "robust disambiguation of named entities in text . in _ proceedings of the conference on empirical methods in natural language processing _ , emnlp 11 , stroudsburg , pa , usa , 2011 .",
    "association for computational linguistics .",
    "meiqun hu , ee - peng lim , aixin sun , hady  wirawan lauw , and ba - quy vuong . measuring article quality in wikipedia : models and evaluation . in _ proceedings of the sixteenth acm conference on conference on information and knowledge management _ , cikm 07 , new york , ny , usa , 2007 .",
    "rianne kaptein , pavel serdyukov , arjen de  vries , and jaap kamps .",
    "entity ranking using wikipedia as a pivot . in _ proceedings of the 19th acm international conference on information and knowledge management _ , cikm 10 , new york , ny , usa , 2010 .",
    "brian keegan , darren gergle , and noshir contractor .",
    "hot off the wiki : dynamics , practices , and structures in wikipedia s coverage of the tohoku catastrophes . in _ proceedings of the 7th international symposium on wikis and open collaboration _ , wikisym 11 , pages 105113 , new york , ny , usa , 2011 .",
    "brian keegan , darren gergle , and noshir contractor .",
    "do editors or articles drive collaboration ? : multilevel statistical network analysis of wikipedia coauthorship . in",
    "_ proceedings of the acm 2012 conference on computer supported cooperative work _ , cscw 12 , new york , ny , usa , 2012 .",
    "bernardo  pereira nunes , stefan dietze , marco  antonio casanova , ricardo kawase , besnik fetahu , and wolfgang nejdl .",
    "combining a co - occurrence - based and a semantic measure for entity linking . in _ eswc _ , 2013 .",
    "fabian  m. suchanek , gjergji kasneci , and gerhard weikum .",
    "yago : a core of semantic knowledge . in _ proceedings of the 16th international conference on world wide web _ , www 07 , new york , ny , usa , 2007 .",
    "bongwon suh , gregorio convertino , ed  h. chi , and peter pirolli .",
    "the singularity is not near : slowing growth of wikipedia . in _ proceedings of the 5th international symposium on wikis and open collaboration _ , wikisym 09 , pages 8:18:10 , new york , ny , usa , 2009 .",
    "pu  wang and carlotta domeniconi .",
    "building semantic kernels for text classification using wikipedia . in _ proceedings of the 14th acm sigkdd international conference on knowledge discovery and data mining _ , kdd 08 , new york , ny , usa , 2008 .",
    "torsten zesch , christof mller , and iryna gurevych .",
    "extracting lexical semantic knowledge from wikipedia and wiktionary . in _ proceedings of the international conference on language resources and evaluation , lrec 2008",
    ", 26 may - 1 june 2008 , marrakech , morocco _ , 2008 ."
  ],
  "abstract_text": [
    "<S> wikipedia , rich in entities and events , is an invaluable resource for various knowledge harvesting , extraction and mining tasks . </S>",
    "<S> numerous resources like dbpedia , yago and other knowledge bases are based on extracting entity and event based knowledge from it . </S>",
    "<S> online news , on the other hand , is an authoritative and rich source for emerging entities , events and facts relating to existing entities . in this work </S>",
    "<S> , we study the creation of entities in wikipedia with respect to news by studying how entity and event based information flows from news to wikipedia .    </S>",
    "<S> we analyze the lag of wikipedia ( based on the revision history of the english wikipedia ) with 20 years of _ the new york times _ dataset ( nyt ) . </S>",
    "<S> we model and analyze the lag of entities and events , namely their first appearance in wikipedia and in nyt , respectively . in our extensive experimental analysis , we find that almost 20% of the external references in entity pages are news articles encoding the importance of news to wikipedia . </S>",
    "<S> second , we observe that the entity - based lag follows a normal distribution with a high standard deviation , whereas the lag for news - based events is typically very low . finally , we find that events are responsible for creation of emergent entities with as many as 12% of the entities mentioned in the event page are created after the creation of the event page .    </S>",
    "<S> [ news and wikipedia dynamics ] </S>"
  ]
}