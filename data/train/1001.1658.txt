{
  "article_text": [
    "the network coding techniques for information transmission in networks introduced in @xcite have attracted significant interest in the literature , both because of posing theoretically interesting questions , as well as because of potential impact in applications .",
    "the first fundamental result proved in network coding , and perhaps still the most useful from a practical point of view today , is that , using linear network coding @xcite , one can achieve rates up to the common min - cut value when multicasting to @xmath0 receivers .",
    "in general this may require operations over a field of size approximately @xmath1 , which translates to communication using packets of length @xmath2 bits @xcite .",
    "however , this result assumes that the receivers know perfectly the operations that the network nodes perform . in large",
    "dynamically changing networks , collecting network information comes at a cost , as it consumes bandwidth that could instead have been used for information transfer . in practical networks , where such deterministic knowledge is not sustainable , the most popular approach is to perform randomized network coding @xcite and to append coding vectors at the headers of the packets to keep track of the linear combinations of the source packets they contain ( see , _ e.g.",
    "_ , @xcite ) .",
    "the coding vectors have an overhead of @xmath3 bits , where @xmath4 is the total number of packets to be linearly combined .",
    "this results in a loss of information rate that can be significant with respect to the min - cut value .",
    "in particular , in wireless networks such as sensor networks where communication is restricted to short packet lengths , the coding vector overhead can be a significant fraction of the overall packet length @xcite .",
    "use of coding vectors is akin to use of training symbols to learn the transformation induced by a network .",
    "a different approach is to assume a non - coherent scenario for communication , as proposed in @xcite , where neither the source(s ) nor the receiver(s ) have any knowledge of the network topology or the network nodes operations .",
    "non - coherent communication allows for creating end - to - end systems completely oblivious to the network state .",
    "several natural questions arise considering this non - coherent framework : ( i ) what are the fundamental limits on the rates that can be achieved in a network where the intermediate node operations are unknown , ( ii ) how can they be achieved , and ( iii ) how do they compare to the coherent case .    in this work",
    "we address such questions for two different cases .",
    "first , we consider the scenario where a single source aims to transmit information to one or multiple receiver(s ) over a network under the non - coherence assumption using fixed packet length . because network nodes only perform linear operations , the overall network behavior from the source(s ) to a receiver",
    "can be represented as a matrix multiplication of the sent source packets .",
    "we consider operation in time - slots , and assume that the channel transfer matrices are distributed uniformly at random and i.i.d . over different time - slots . under this probabilistic model",
    ", we characterize the asymptotic capacity behavior of the introduced channel and show that using _ subspace coding _ we can achieve the optimal performance .",
    "we extend our model for the case of multiple sources and characterize the asymptotic behavior of the optimal rate region for the case of two sources .",
    "we believe that this result can be extended to the case of more than two sources using the same method that is applied in  [ sec : proofmultisource ] .",
    "for the multi - source case we prove as well that encoding information using subspaces is sufficient to achieve the optimal rate region .",
    "the idea of non - coherent modeling for randomized network coding was first proposed in the seminal work by koetter and kschischang in @xcite . in that work",
    ", the authors focused on algebraic subspace code constructions over a grassmannian .",
    "independently and in parallel to our work in @xcite , montanari _ et al . _",
    "@xcite introduced a different probabilistic model to capture the end - to - end functionality of non - coherent network coding operation , with a focus on the case of error correction capabilities .",
    "their model does not examine subsequent time slots , but instead , allows the packets block length ( in this paper terminology ; packet length @xmath5 ) to increases to infinity , with the result that the overhead of coding vectors becomes negligible , very fast .",
    "silva _ et al .",
    "_ @xcite independently and subsequent to our works in @xcite and @xcite , also considered a probabilistic model for non - coherent network coding , which is an extension of the model introduced in @xcite over multiple time - slots . in their model the transfer matrix is constrained to be square as well as full rank .",
    "this is in contrast to our model , where the transfer matrix can have arbitrary dimensions , and the elements of the transfer matrix are chosen uniformly at random , with the result that the transfer matrix itself may not have full rank ( this becomes more pronounced for small matrices ) .",
    "moreover , we extend our work to multiple source multicast , which corresponds to a virtual non - coherent multiple access channel ( mac ) .",
    "our results coincide for the case of a single source , when the packet length and the finite field of operations are allowed to grow sufficiently large .",
    "another difference is that the work in @xcite focuses on additive error with constant dimensions ; in contrast , we focus on packet erasures .",
    "an interpretation of our results is that it is the finite field analog of the grassmannian packing result for non - coherent mimo channels as studied in the well known work in @xcite .",
    "in particular , we show that for the non - coherent model over finite fields , the capacity critically depends on the relationship between the `` coherence time '' ( or packet length @xmath5 in our model ) and the min - cut of the network .",
    "in fact the number of active subspace dimensions depend on this relationship ; departing from the non - coherent mimo analogy of @xcite .",
    "the paper is organized as follows .",
    "we define our notation and channel model in ",
    "[ sec : channelmodel - notations ] ; we state and discuss our main results in  [ sec : mainresults ] ; we prove the capacity results for the single and multiple sources in sections ",
    "[ sec : thechanlcap - snglsrc ] and  [ sec : proofmultisource ] respectively ; and conclude the paper in  [ sec : conclusion ] .",
    "all the missing proofs for lemmas , theorems , and etc . , are given in appendix  [ sec : apndx1 ] unless otherwise stated .",
    "we here introduce the notation and definitions we use in the following sections .",
    "let @xmath6 be a power of a prime .",
    "in this paper , all vectors and matrices have elements in a finite field @xmath7 .",
    "we use @xmath8 to denote the set of all @xmath9 matrices over @xmath7 , and @xmath10 to denote the set of all row vectors of length @xmath5 . the set @xmath11 forms a @xmath5-dimensional vector space over the field @xmath7 .    throughout the paper , we use capital letters , _",
    "e.g. _ , @xmath12 , to denote random objects , including random variables , random matrices , or random subspaces , and corresponding lower - case letters , _",
    "e.g. _ , @xmath13 to denote their realizations .",
    "for example , we denote by @xmath14 a `` random subspace '' which takes as values the subspaces in a vector space according to some distribution , and by @xmath15 a specific realization .",
    "also , bold capital letters , _",
    "e.g. _ , @xmath16 , are reserved for deterministic matrices and bold lower - case letters , _",
    "e.g. _ , @xmath17 , are used for deterministic vectors .    for subspaces @xmath18 and @xmath19",
    ", @xmath20 denotes that @xmath18 is a subspace of @xmath19 .",
    "recall that for two subspaces @xmath18 and @xmath19 , @xmath21 is the intersection of these subspaces which itself is a subspace .",
    "we use @xmath22 to denote the smallest subspace that contains both @xmath18 and @xmath19 , namely , @xmath23 it is well known that @xmath24    for a set of vectors @xmath25 we denote their linear span by @xmath26 . for a matrix @xmath27 , @xmath28 is the subspace spanned by the rows of @xmath27 and @xmath29 is the subspace spanned by the columns of @xmath27 .",
    "we then have @xmath30 .",
    "we use the calligraphic symbols , _",
    "i.e. _ , @xmath31 or @xmath32 to denote a set of matrices .",
    "to denote a set of subspaces we use the same calligraphic symbols but with a `` @xmath33 '' , _",
    "i.e. _ , @xmath34 or @xmath35 .",
    "we use the symbols `` @xmath36 '' and `` @xmath37 '' to denote the element - wise inequality between vectors and matrices of the same size .    for two real valued functions @xmath38 and @xmath39 of @xmath13 , we use @xmath40 to denote that '' is used for multi - variate functions .",
    "however , since in this work the growing variable is always @xmath41 , the field size , we will not repeat it for sake of brevity . ]",
    "@xmath42 note that the definition of `` @xmath43 '' is different from the more standard definition which is we also use a similar definition for @xmath44 to denote that @xmath45 where @xmath46 is a constant .",
    "we use the big-@xmath47 notation which is defined as follows .",
    "let @xmath38 and @xmath39 be two functions defined on some subset of the real numbers .",
    "we write @xmath48 if there exists a positive real number @xmath49 and a real number @xmath50 such that @xmath51 for the little @xmath52 notation we use the following definition .",
    "we write @xmath53 if for all @xmath54 there exists a real number @xmath50 such that @xmath55 we use also the big-@xmath56 notation which is defined as follows .",
    "we write @xmath57 if we have @xmath58 .",
    "finally , we use the big-@xmath59 notation to denote that a function is bounded both above and below by another function asymptotically .",
    "formally , we write @xmath60 if and only if we have @xmath61 and @xmath62 .",
    "[ grassmannian and gaussian coefficient @xcite ] the grassmannian @xmath63 is the set of all @xmath64-dimensional subspaces of the @xmath5-dimensional space over a finite field @xmath7 , namely , @xmath65 the cardinality of @xmath63 is the gaussian coefficient , namely , @xmath66    we define @xmath67 to be the set ( sphere ) of all subspaces of dimension at most @xmath68 in the @xmath5-dimensional space @xmath11 , namely @xmath69 } \\mathrm{gr}(t , d)_q = \\{\\pi\\sqsubseteq { \\mathbb{f}}_q^t : \\dim(\\pi)\\leq \\min[m , t]\\}.\\end{aligned}\\ ] ] the cardinality of @xmath67 equals @xmath70 } |\\mathrm{gr}(t , d)_q|.\\ ] ]    [ def : psi ] we denote by @xmath71 the number of different @xmath72 matrices with elements from a field @xmath7 , such that their rows span a specific subspace @xmath73 of dimension @xmath74 $ ] .    for simplicity , in the rest of the paper we will drop the subscript @xmath41 in the previous definitions whenever it is obvious from the context .",
    "we here state some preliminary lemmas related to the definitions introduced in  [ subsec : notations ] .",
    "existing bounds in the literature allow to approximate the gaussian number , for example , we have from ( * ? ? ? * lemma",
    "4 ) that ( * ? ? ?",
    "* section  iii ) @xmath75 using definition  [ eq : gaussiannumber ] and we have lemma  [ lem : gauss_approximation ] .",
    "[ lem : gauss_approximation ] for large @xmath41 we can approximate the gaussian number as follows @xmath76    [ lem : psi_value ] for @xmath77 given in definition  [ def : psi ] , we have that @xcite @xmath78 _ i.e. _ , it does not depend on @xmath5 .    since @xmath77",
    "does not depend on @xmath5 , and only depends on @xmath79 through its dimension , as a shorthand notation we will also use @xmath80 instead of @xmath77 , where @xmath81 .    using lemma  [ lem : psi_value ]",
    "the following lower and upper bounds are straightforward @xmath82 which imply lemma  [ lem : psi_approximation ] ( see also @xcite ) .",
    "[ lem : psi_approximation ] for large values of @xmath41 the following approximation holds @xmath83    it is also worthwhile to mention that @xmath84 is the number of @xmath85 matrices of rank @xmath64 .",
    "we can count all the @xmath72 matrices through the following lemma  [ lem : psi_recursive ] , ( also see @xcite , and ( * ? ? ? *",
    "corollary  5 ) ) .",
    "[ lem : psi_recursive ] for every @xmath86 and @xmath87 we can write @xmath88 } \\psi(n , d ) { { t \\brack d } } = q^{nt},\\ ] ] where @xmath89 .",
    "we consider a network where nodes perform random linear network coding over a finite field @xmath7 .",
    "we are interested in the maximum information rate at which a single ( or multiple ) source(s ) can successfully communicate over such a network when neither the transmitter nor the receiver(s ) have any channel state information ( csi ) . for simplicity , we will present the channel model and our analysis for the case of a single receiver ; the extension to multiple receivers ( with the same channel parameters ) is straightforward , as we also discuss in the results section .",
    "we assume that time is slotted and the channel is block time - varying .",
    "for the single source communication , at time slot @xmath90 , the receiver observes @xmath91=g[t]x[t],\\ ] ] where @xmath92\\in{\\mathbb{f}}_q^{m\\times t}$ ] , @xmath93\\in{\\mathbb{f}}_q^{n\\times m}$ ] , and @xmath94\\in{\\mathbb{f}}_q^{n\\times t}$ ] . at each time - slot ,",
    "the receiver receives @xmath95 packets of length @xmath5 ( captured by the rows of matrix @xmath94 $ ] ) that are random linear combinations of the @xmath68 packets injected by the source ( captured by the rows of matrix @xmath92 $ ] ) . in our model",
    ", the packet length @xmath5 can be interpreted as the coherence time of the channel , during which the transfer matrix remains constant .",
    "each element of the transfer matrix @xmath93 $ ] is chosen uniformly at random from @xmath96 , changes independently from time slot to time slot , and is unknown to both the source and the receiver . in other words ,",
    "the channel transfer matrix is chosen uniformly at random from all possible matrices in @xmath97 and has i.i.d .",
    "distribution over different blocks . in general",
    ", the topology of the network may impose some constraints on the transfer matrix @xmath93 $ ] ( for example , some entries might be zero , see @xcite )",
    ". however , we believe that this is a reasonable general model , especially for large - scale dynamically - changing networks where apart from random coefficients there exist many other sources of randomness . formally , we define the non - coherent matrix channel as follows .",
    "[ def : matrix_channel_p2p ] this is defined to be the matrix channel @xmath98 described by ( [ eq : channel_model_p2p ] ) with the assumption that @xmath93 $ ] is i.i.d . and",
    "uniformly distributed over all matrices @xmath97 .",
    "it is a discrete memoryless channel with input alphabet @xmath99 and output alphabet @xmath100 .",
    "the capacity of the channel @xmath101 is given by @xmath102 where @xmath103 is the input distribution . to achieve the capacity a coding scheme may employ the channel given in ( [ eq : channel_model_p2p ] ) multiple times , and a codeword is a sequence of input matrices from @xmath104 . for a coding strategy that induces an input distribution @xmath103 ,",
    "the achievable rate is @xmath105    now we define a non - coherent subspace channel @xmath106 which takes as an input a subspace and outputs another subspace . then , in theorem  [ thm : channel_equivalence_p2p ] we will show that the two channels @xmath101 and @xmath106 are equivalent from the point of view of calculating the mutual information between their inputs and their outputs .",
    "[ def : subspace_channel_p2p ] this is defined to be the channel @xmath107 with input alphabet @xmath108 and output alphabet @xmath109 and transition probability @xmath110 where @xmath111 and @xmath112 are the input and output variables of the channel @xmath106 .",
    "the capacity of the channel @xmath106 is given by @xmath113 where @xmath114 is the input distribution defined over the set of subspaces @xmath115 .",
    "we next consider a multiple sources scenario , and the multiple access channel corresponding to ( [ eq : channel_model_p2p ] ) . in this case",
    ", we have @xmath116=\\sum_{i=1}^{n_s } g_i[t]x_i[t],\\ ] ] where @xmath117 is the number of sources , and each source @xmath118 inserts @xmath119 packets to the network .",
    "thus , @xmath120\\in{\\mathbb{f}}_q^{m_i\\times t}$ ] , @xmath121\\in{\\mathbb{f}}_q^{n\\times m_i}$ ] and @xmath94\\in{\\mathbb{f}}_q^{n\\times t}$ ] .",
    "we can also collect all @xmath121 $ ] in an @xmath122 matrix @xmath123 $ ] and all @xmath120 $ ] in an @xmath124 matrix @xmath125 $ ] as following @xmath126 = \\left[\\begin{array}{c } x_1[t]\\\\ \\vdots\\\\ x_{n_s}[t]\\\\ \\end{array } \\right],\\quad\\text{and}\\quad   g_{\\textsl{mac}}[t ] = \\left[\\begin{array}{ccc } g_1[t ] & \\cdots & g_{n_s}[t ] \\end{array}\\right],\\ ] ] so we can rewrite as @xmath127=g_{\\textsl{mac}}[t ] x_{\\textsl{mac}}[t].\\ ] ] each source @xmath118 then controls @xmath119 rows of the matrix @xmath125 $ ] .",
    "again we assume that each entry of the matrices @xmath121 $ ] is chosen i.i.d . and uniformly at random from the field @xmath7 for all source nodes and all time instances .",
    "[ def : matrix_channel_mac ] this is defined to be the channel described in , with the assumption that @xmath121 $ ] , @xmath128 , are i.i.d . and",
    "uniformly distributed over all matrices @xmath129 , @xmath128 .",
    "it forms a discrete memoryless mac with input alphabets @xmath130 , @xmath128 , and output alphabet @xmath100 .",
    "it is well known @xcite that the rate region of any multiple access channel including @xmath131 is given by the closure of the convex hull of the rate vectors satisfying @xmath132 for some product distribution @xmath133 .",
    "note that @xmath134 where @xmath135 is the transmission rate of the @xmath118th source , @xmath136 and @xmath137 is the complement set of @xmath138 .    as before ,",
    "we define a non - coherent subspace version sources is straightforward .",
    "] of the matrix multiple access channel and in theorem  [ thm : channel_equivalence_mac ] we show that from the point of view of rate region these two channels are equivalent .",
    "[ def : subspace_channel_mac ] this is defined to be the channel @xmath139 with input alphabets @xmath140 , @xmath141 , output alphabet @xmath109 and transition probability @xmath142 where @xmath143 and @xmath144 are the input and @xmath112 is the output variables of the channel @xmath145 .",
    "our main results , theorem  [ thm : main_result_single_src ] and theorem  [ thm : main_result_single_src_dist ] , characterize the capacity for non - coherent network coding for the model given in .",
    "we show that the capacity is achieved through subspace coding , where the information is communicated from the source to the receivers through the choice of subspaces .",
    "formally , we have the following results .",
    "[ thm : channel_equivalence_p2p ] the matrix channel @xmath98 defined in definition  [ def : matrix_channel_p2p ] and the subspace channel @xmath146 defined in definition  [ def : subspace_channel_p2p ] are equivalent in terms of evaluating the mutual information between the input and output .",
    "more precisely , for every input distribution for the channel @xmath106 there is an input distribution for the channel @xmath101 such that @xmath147 and vice versa . as a result , these channels have the same capacity @xmath148 .    for the proof of theorem  [ thm : channel_equivalence_p2p ] refer to appendix  [ sec : apndx1 ] and for more discussion refer to  [ subsec : equiv - matrixchannel - subspacechannel ] .",
    "[ thm : main_result_single_src ] for the channel @xmath98 defined in definition  [ def : matrix_channel_p2p ] , the capacity is given by @xmath149 where @xmath150 $ ] , and @xmath151 tends to zero as @xmath41 grows .    theorem  [ thm : main_result_single_src ] is proved in  [ subsec : capacity - upperlowerbound ] .",
    "the result of theorem  [ thm : main_result_single_src ] is for large alphabet regime regime .",
    "we have included that proof in ",
    "[ subsec : capacity - upperlowerbound ] .",
    "our original proof was based partially on the proof now given for theorem  [ thm : main_result_single_src_dist ] . ] .",
    "the following result , theorem  [ thm : main_result_single_src_dist ] , is valid for a finite field size , and therefore is a non - asymptotic result .",
    "[ thm : main_result_single_src_dist ] consider the channel @xmath98 defined in definition  [ def : matrix_channel_p2p ] .",
    "there exists a finite number  @xmath152 such that for @xmath153 the optimal input distribution is nonzero only for matrices of rank in the set @xmath154,\\ldots,\\min\\left[m , n , t\\right ] \\right\\}.\\ ] ] moreover , for all values of @xmath41 the optimal input distribution is uniform over all matrices @xmath12 of the same rank , and the total probability allocated to transmitting matrices of rank @xmath118 equals @xmath155 } } } = 2^{-c_{\\textsl{m } } } q^{i(t - i ) } \\left[1+o(1)\\right],\\quad \\forall i\\in\\mathcal{a}.\\ ] ]    the proof of theorem  [ thm : main_result_single_src_dist ] is presented in ",
    "[ subsec : optsolution_singlesrc_genapproach ] and  [ subsec : optsolution_singlesrc_large_q ] , and uses standard techniques from convex optimization , as well as large field size approximations . note that , the same coding scheme at the source simultaneously achieves the capacity for all receivers with the same channel parameters ( _ i.e. _ , values of @xmath95 , @xmath68 and @xmath5 ) .",
    "that is , each receiver is able to successfully decode .",
    "the result of theorem  [ thm : main_result_single_src_dist ] for the active set of input dimensions is not asymptotic in @xmath41 .",
    "however , it is not easy to analytically find the minimum value of @xmath152 such that the theorem statement holds for all @xmath153 .",
    "theorem  [ thm : main_result_single_src_extnd ] demonstrates how we can analytically characterize @xmath152 given in theorem  [ thm : main_result_single_src_dist ] for the case @xmath156 $ ] .",
    "the proof of theorem  [ thm : main_result_single_src_extnd ] is presented in  [ sec : proof_main_result_single_src_extnd ] .",
    "[ thm : main_result_single_src_extnd ] if @xmath156 $ ] , then the capacity of @xmath101 for @xmath157 is given by @xmath158 where @xmath159 is the indicator function and @xmath152 is the minimum field size that satisfies the set of inequalities @xmath160 and @xmath161 where @xmath162 $ ] and @xmath163 } \\psi(n , d_y ) { { l \\brack d_y } } q^{-nl } \\log_2\\left ( \\frac{{{t \\brack d_y}}}{{{i^ * \\brack d_y } } } \\right )   -\\min[n , l](t - i^*).\\ ] ] the capacity is achieved by sending matrices @xmath12 such that their rows span different @xmath164-dimensional subspaces .    moreover ,",
    "asymptotically in @xmath5 , we can show that @xmath165 is sufficient for the case @xmath166 and @xmath167 is sufficient if @xmath168 .    theorems  [ thm : main_result_single_src ]  and  [ thm : main_result_single_src_dist ] state that the capacity behaves as @xmath169 , for sufficiently large @xmath41 .",
    "however , numerical simulations indicate a very fast convergence to this value as @xmath41 increases .",
    "[ fig : numericalcapacity_p2p ] depicts the capacity for small values of @xmath41 , calculated using the differential evolution toolbox for matlab @xcite .",
    "this shows that the result is relevant at much lower field size than dictated by the formalism of the statement of theorems  [ thm : main_result_single_src ]  and  [ thm : main_result_single_src_dist ] .     and @xmath170 , @xmath171 .",
    "the dotted line depicts @xmath172 .",
    ", width=480 ]    from theorem [ thm : main_result_single_src_dist ] , we can derive the following guidelines for non - coherent network code design .",
    "the optimal input distribution uses subspaces of a single dimension equal to @xmath173 $ ] for @xmath174+n$ ] .",
    "as @xmath5 reduces , the set of used subspaces gradually increases , by activating one by one smaller and smaller dimensional subspaces , until , for @xmath175 , all subspaces are used with equal probability since there are different number of subspaces of each dimension . ] .",
    "[ fig : active ] pictorially depicts this gradual inclusion of subspaces .",
    "this behavior is different from the result of  @xcite where all the subspaces up to dimension equal to the min - cut appeared in the optimal input distribution .",
    "this difference is due to the different channel model used in our work and in @xcite .    ,",
    "as it is shown in theorem  [ thm : main_result_single_src_dist ] there exist three different regimes . ]      for a given and fixed packet length @xmath5 , the optimal value of @xmath68 and @xmath95 equals @xmath177 ( optimality is in the sense of minimum requirement in order to obtain the maximum capacity for this @xmath5 ) .",
    "for fixed @xmath5 and @xmath68 , the optimal value of @xmath95 equals @xmath178 $ ] . for fixed @xmath5 and @xmath95 ,",
    "the optimal value of @xmath68 equals @xmath179 $ ] .",
    ".[tabl_1 ] information loss from using coding vectors when @xmath180 .",
    "[ cols=\"^,^,^\",options=\"header \" , ]      one of the aims of this work was to find the regimes in which the using of coding vectors @xcite is far from optimal .",
    "table  [ tabl_1 ] summarizes this difference .",
    "as we see from the table  [ tabl_1 ] subspace coding does not offer benefits as compared to the coding vectors approach for large field size .",
    "table  [ tabl_1 ] is calculated as follows .",
    "the achievable rate @xmath181 using coding vectors equals @xmath182 } } } k(t - k)\\log_2{q},\\ ] ] where @xmath183 is the number of packets in each generation , _",
    "i.e. _ , each packet includes a coding vector of length @xmath184 and @xmath185 information symbols .",
    "equivalently , we assume that we use @xmath184 of the @xmath68 possible input packets .",
    "the matrix @xmath186 is the @xmath187 sub - matrix of @xmath188 that is applied over the input packets . to calculate @xmath181 , we know that @xmath189 } } } = \\prod_{i=0}^{k-1 } ( 1-q^{-k+i})=1-q^{-1}+o(q^{-2})$ ] .",
    "assume we choose @xmath190 we have @xmath191 , where @xmath192 $ ] .",
    "for the capacity @xmath193 we use the large @xmath41-regime as considered in theorem  [ thm : main_result_single_src ] for the case @xmath194 and the finite @xmath41-regime of theorem  [ thm : main_result_single_src_extnd ] for the case @xmath195 .      after the error free single source scenario ,",
    "we consider packet erasure networks , and calculate an upper and lower bound on the capacity for this case .",
    "the work in @xcite , which is the closest to ours , did not consider erasures but instead constant - dimension additive errors . in practice ,",
    "depending on the application , either of the models might be more suitable : for example , if network coding is deployed at an application layer , then , unless there exist malicious attackers , packet erasures are typically used to abstract both the underlying physical channel errors , as well as packet dropped at queues or lost due to expired timers .",
    "we model the erasures in the network as an end - to - end phenomenon which randomly erases packets according to some probability distribution . formally , we rewrite the channel defined in as @xmath196=e[t]g[t]x[t],\\ ] ] where @xmath197 is assumed to be a squre chanel matrix and @xmath198 is a diagonal random matrix whose elements on its diagonal are either @xmath199 or @xmath200 .",
    "we also assume that @xmath41 is large , and as a result the transfer matrix is full rank with high probability .",
    "moreover , we consider the case where @xmath201 , i.e. the matrix @xmath12 is a fat matrix . recall that we can think of the rows of this matrix as packets send by the source , and the rows of the @xmath202 matrix as packets received at the destination .",
    "note that in equation ( [ eq : channel_model_p2p_erasure ] ) all of the erasure events are captured by the erasure matrix @xmath203 $ ] .",
    "moreover , the erasure pattern is important only up to determining the number of packets that the destination receives , since the transfer matrix @xmath93 $ ] is unknown and distributed uniformly at random over all full rank matrices .",
    "thus , we model the number of received packets ( number of non - zero elements on the diagonal of @xmath203 $ ] ) as a random variable @xmath204 which takes values in @xmath205 according to some distribution that depends on the packet erasures in the network . in this case the capacity is @xmath206 we can then use our previous result , theorem  [ thm : main_result_single_src ] , to find an upper and lower bound for the capacity @xmath207 when we have packet erasure in the network , as the following theorem  [ thm : p2p_erasure ] describes .",
    "[ thm : p2p_erasure ] let the number of received packets at the destination be a random variable @xmath204 defined over the set of integers @xmath205 .",
    "also , assume that @xmath208 .",
    "then for large @xmath41 , we have the following upper and lower bound for the capacity @xmath207 , @xmath209 where @xmath210 } } } $ ] and @xmath211 } } } $ ]",
    ".    for the proof of theorem  [ thm : p2p_erasure ] and more discussion refer to appendix  [ sec : apndx2 ] .",
    "note that because we do not necessarily employ full - rank matrices @xmath12 , it is possible that although some packets are erased at the destination , the received packets still span a matrix of the same rank as @xmath12 ; thus erasing packets is not equivalent to erasing dimensions .      in several practical applications , such as sensor networks , data sources are not necessarily co - located .",
    "we thus extend our work to the case where multiple not co - located sources transmit information to a common receiver . in particular",
    ", we consider the non - coherent mac introduced in definition  [ def : matrix_channel_mac ] , and characterize the capacity region of this network for the case of two sources with @xmath212 and @xmath213 input packets and packet length @xmath214 .",
    "we believe that this technique can be extended to more than two sources .    to find the rate region of the matrix multiple access channel @xmath131 , we first show that the two channels @xmath131 and @xmath145 are equivalent , as stated in theorem  [ thm : channel_equivalence_mac ] .",
    "we then find the rate region of the subspace multiple access channel @xmath145 which is stated in theorem  [ thm : main_result_multpl_src ] .",
    "to avoid repetition , we state theorem  [ thm : channel_equivalence_mac ] without a proof because its proof is very similar to that of theorem  [ thm : channel_equivalence_p2p ] .",
    "[ thm : channel_equivalence_mac ] the matrix mac @xmath131 defined in definition  [ def : matrix_channel_mac ] is equivalent to the subspace mac @xmath145 defined in definition  [ def : subspace_channel_mac ] in the sense that the optimal rate region for these two channels is the same .",
    "[ thm : main_result_multpl_src ] for @xmath215 , the asymptotic ( in the field size @xmath41 ) capacity region of the mac @xmath131 introduced in definition  [ def : matrix_channel_mac ] is given by @xmath216 where @xmath217 @xmath218 and @xmath219 ,   0\\leq d_1+d_2 \\leq \\min[n , m_1+m_2]\\}.\\end{aligned}\\ ] ]    we note that the rate region forms a polytopes that has the following number of corner points ( see corollary  [ cor : numcornerpointsmac ] in  [ sec : proofmultisource ] ) @xmath220 + \\min\\left[m_2 , ( n - m_1)^+\\right ] + 2 - { \\mathds{1}}_{\\{n\\ge m_1+m_2\\}}.\\ ] ] the rate region @xmath221 is shown in fig .",
    "[ fig : mac_region ] for a particular choice of parameters .",
    "the proof of this theorem is provided in  [ sec : proofmultisource ] .",
    "we first derive an outer bound by deriving two other bounds : a cooperative bound and a coloring bound . for",
    "the coloring bound , we utilize a combinatorial approach to bound the number of _ distinguishable _ symbol pairs that can be transmitted from the sources to the receiver .",
    "we then show that a simple scheme that uses coding vectors achieves the outer bound .",
    "we thus conclude that , for the case of two sources when @xmath215 , use of coding vectors is ( asymptotically ) optimal .     for parameters @xmath222 , @xmath223 , @xmath176 , @xmath224 .",
    ", width=432 ]",
    "in this section we will prove theorem  [ thm : main_result_single_src ] , theorem  [ thm : main_result_single_src_dist ] , and theorem  [ thm : main_result_single_src_extnd ] .      for convenience",
    "let us rewrite the channel again . ]",
    "@xmath225 to find the capacity of the above channel we need to maximize the mutual information between the input and the output of the channel with respect to the input distribution @xmath103 . since the rows of @xmath188 are chosen independently of each other , assuming that a matrix @xmath226 has been transmitted",
    ", we can think of the rows of the received matrix @xmath202 as chosen independently from each other , among all the possible vectors in the row span of @xmath13 .",
    "the independence of rows of @xmath202 allows us to write the conditional probability of @xmath202 given @xmath12 , referred to as the channel transition probability , as follows @xmath227 where @xmath228 , and @xmath229 .",
    "the mutual information @xmath230 between @xmath12 and @xmath202 is a function of @xmath103 and @xmath231 that can be expressed as @xmath232 it is clear from ( [ eq : p2p_channel_transfer_prob_1 ] ) that @xmath233 for all @xmath234 such that @xmath235 which reveals symmetry for the channel @xmath101 .",
    "we exploit this symmetry to show that @xmath148 as it is stated in theorem  [ thm : channel_equivalence_p2p ] and proved in appendix  [ sec : apndx1 ] .",
    "the proof of theorem  [ thm : channel_equivalence_p2p ] determines how we can map an input distribution of @xmath106 to an input distribution for @xmath101 that achieves the same mutual information .",
    "the input distribution @xmath103 should be chosen such that we have @xmath236 .",
    "one simple way to do this is to put all the probability mass of @xmath237 on one matrix @xmath13 such that @xmath238 .      here , we state the proof of theorem  [ thm : main_result_single_src ] by giving upper and lower bounds for the capacity that differ in @xmath151 bits , which vanishes as @xmath239 .",
    "let @xmath240 denote the capacity of the channel @xmath101 .",
    "let @xmath241 denote the capacity of the channel @xmath242 where @xmath243 is a full - rank matrix chosen uniformly at random among all the full - rank matrices in @xmath97 .",
    "then , we have the following lemma .    [",
    "lem : cap_singlesrc_upperlower ] we can bound @xmath240 from above and below as follows @xmath244 where @xmath245 $ ] .",
    "let @xmath246 denote a generic random matrix chosen uniformly at random and independently from any other variable .",
    "similarly , let @xmath247 denote a generic _ full - rank _ matrix chosen uniformly at random among all such full - rank matrices and independent from any other variable .",
    "( note that each new instance of such a matrix in the same equation denotes a different random variable which is independent from the other random variables . )    since the channel @xmath248 is statistically equivalent to the channel @xmath249 , we have , by the data processing inequality , that @xmath250 .    using the same argument , since the channel @xmath251 is equivalent to the channel @xmath252 if @xmath253 , and is equivalent to the channel @xmath254 if @xmath255 we have @xmath256 .    to obtain the lower bound we proceed as follows .",
    "let us choose @xmath257\\overline{x}$ ] and @xmath258 y$ ] , where @xmath251 .",
    "then we can write @xmath259 u_{n\\times m } { i_h \\brack 0}\\overline{x } = u_{h\\times h}\\overline{x},\\ ] ] where @xmath260 is the upper left @xmath261 sub - matirx of @xmath262 .",
    "thus , again the data processing inequality implies that @xmath263 .",
    "[ lem : cap_singlesrc_upper ] for @xmath240 we have @xmath264 where @xmath265 $ ] .    by lemma  [ lem : cap_singlesrc_upperlower ]",
    "we have @xmath266 where @xmath267 follows from ( * ? ? ?",
    "* corollary  2 ) and @xmath268 follows from lemma  [ lem : gauss_approximation ] .",
    "[ lem : cap_singlesrc_lower ] for @xmath240 we have @xmath269 where @xmath265 $ ] .    for",
    "every subspace @xmath270 , let @xmath271 be a matrix in reduced row echelon form such that @xmath272 .",
    "choose @xmath273\\times \\mathrm{rref}(\\pi_x)\\in{\\mathbb{f}}_q^{m\\times t}$ ] , where @xmath111 is chosen uniformly at random from @xmath274 .",
    "define the random variable @xmath275 .",
    "note that @xmath276 when @xmath277 .",
    "thus , we have @xmath278 and @xmath279 .",
    "then , it follows that @xmath280 } } } i(\\pi_x;\\pi_y|q=1)\\\\ & \\ge { { \\ensuremath{\\mathbb{p}\\left [ q=1 \\right ] } } } i^*(t - i^*)\\log_2{q},\\end{aligned}\\ ] ] where @xmath267 is due to lemma  [ lem : cap_singlesrc_upperlower ] , @xmath268 follows follows from theorem  [ thm : channel_equivalence_p2p ] , and @xmath281 holds since @xmath282 is a deterministic function of @xmath112 .",
    "now , note that we can write @xmath283 } } } & = { { \\ensuremath{\\mathbb{p}\\left [ \\mathrm{rank}(u_{h\\times h}x)=i^ * \\right ] } } } \\\\ & = { { \\ensuremath{\\mathbb{p}\\left [ \\mathrm{rank}\\left(u_{h\\times h } \\left[\\begin{smallmatrix } i_{i^ * } \\\\ \\\\ 0 \\end{smallmatrix}\\right ] \\right)=i^ * \\right ] } } } \\\\ & = { { \\ensuremath{\\mathbb{p}\\left [ \\mathrm{rank}(u_{h\\times i^*})=i^ * \\right ] } } } \\\\ & \\ge 1-\\frac{i^*}{q^{k - i^*+1}}\\\\ & \\ge 1-\\frac{i^*}{q},\\end{aligned}\\ ] ] and thus we obtain the desired result .",
    "combining lemma  [ lem : cap_singlesrc_upper ] and lemma  [ lem : cap_singlesrc_lower ] recovers theorem  [ thm : main_result_single_src ] .",
    "generally , we are interested in finding the capacity and input distribution of @xmath101 exactly .",
    "it is shown in theorem  [ thm : channel_equivalence_p2p ] that instead of the channel @xmath101 we can focus on the channel @xmath106 .",
    "thus , we are interested in optimizing the following quantity @xmath284 remember that @xmath108 and @xmath109 .    the following lemma states that the optimal solution for the channel @xmath106 should be uniform over all subspaces with the same dimension , as it is intuitively expected from the symmetry of the channel .",
    "[ lem : uniform_dist ] the input distribution that maximizes @xmath285 for @xmath106 is the one which is uniform over all subspaces having the same dimension .",
    "lemma [ lem : uniform_dist ] shows that the optimal input distribution can be expressed as @xmath286 } } } = \\frac{\\alpha_{d_x}}{{{t \\brack d_x}}},\\ ] ] where @xmath287 , @xmath288 } } } $ ] , and we have @xmath289 } \\alpha_{d_x}=1 $ ] .",
    "we can then simplify @xmath285 as stated in the following lemma .",
    "[ lem : p2p_mutualinfo_subspace_final ] assuming an optimal input probability distribution of the form in ( [ eq : optimal_dist_form ] ) , the mutual information @xmath285 can be simplified to @xmath290 } \\alpha_{d_x } nd_x   \\log_2{q } \\nonumber\\\\ & - \\sum_{d_x=0}^{\\min[m , t ] } \\alpha_{d_x } q^{-nd_x } \\sum_{d_y=0}^{\\min[n , d_x ] } \\psi(n , d_y){{d_x \\brack d_y } } \\log_2(f(d_y)),\\end{aligned}\\ ] ] where @xmath291 } { { d_x \\brack d_y } } q^{-nd_x } \\alpha_{d_x}.\\ ] ]    lemmas  [ lem : uniform_dist ]  and  [ lem : p2p_mutualinfo_subspace_final ] show that the problem of finding the optimal input distribution for the channel @xmath106 is reduced to finding the optimal choice for @xmath292 $ ] .",
    "we know that the mutual information is a concave function with respect to @xmath114 s .",
    "observation  [ obs : lin_trans_preserve_concavity ] implies that because is a linear transformation from @xmath114 s to @xmath293 s , as a result the mutual information @xmath285 is also concave with respect to @xmath293 s @xcite .",
    "[ obs : lin_trans_preserve_concavity ] let @xmath294 be a concave function and let @xmath295 be a linear transform from @xmath296 to @xmath297 .",
    "then @xmath298 is also a concave function .    using observation  [ obs : lin_trans_preserve_concavity ] , we know that the mutual information is a concave function with respect to @xmath293 s .",
    "this allows us to use the kuhn - tucker theorem @xcite to solve the convex optimization problem . according to this theorem , the set of probabilities @xmath299 , @xmath300 $ ] ,",
    "maximize the mutual information if and only if there exists some constant @xmath301 such that @xmath302 where @xmath303 } \\alpha^*_i=1 $ ] , @xmath304 $ ] , and @xmath305 is the vector of the optimum input probabilities of choosing subspaces of certain dimension , @xmath306 } \\end{array}\\right]^\\mathrm{t}.\\ ] ]    [ lem : i_derivative_simplified ] by taking the partial derivative of the mutual information given in with respect to @xmath307 , we have @xmath308 } \\psi(n , d_y ) { { k \\brack d_y } } q^{-nk } \\log_2\\left(f(d_y ) \\right ) -\\log_2{e}.\\ ] ]    multiplying both sides of ( [ eq : i_derivative_simplified ] ) by @xmath307 and summing over @xmath184 we get @xmath309 } \\alpha_k i'_k.\\end{aligned}\\ ] ] by choosing the optimal values @xmath310 for @xmath304 $ ] , the rhs becomes @xmath301 , and the mutual information increases to @xmath311 .",
    "so we may write @xmath312 .      in this subsection , we focus on large size fields , @xmath313 .",
    "this assumption allows us to use some approximations to simplify the conditions in . assuming large @xmath41 we can rewrite as follows @xmath314 } \\left(1+o(q^{-1})\\right ) q^{-(n - d_y)(k - d_y ) } \\log_2\\left(f(d_y ) \\right ) , \\end{aligned}\\ ] ] where we have used lemma  [ lem : gauss_approximation ] and lemma  [ lem : psi_approximation ] . using similar approximations , @xmath315 defined in",
    "can be approximated as @xmath316 } q^{-(n - d_y)d_x } \\alpha_{d_x } \\right ) . \\ ] ] then we have the following result , lemma  [ lem : dominterm_derivativei ] .",
    "[ lem : dominterm_derivativei ] the dominating term in the summation in ( [ eq : i_derivative_midl1 ] ) is the one obtained for @xmath317 $ ] .    from the proof of lemma  [ lem : dominterm_derivativei ] written in appendix  [ sec :",
    "apndx1 ] , we can also see that the remaining terms in the summation of are of order @xmath151 , so we can write @xmath318-nk]\\log_2{q } + \\underbrace{o(1)}_{\\epsilon_q(k ) } -\\log_2{e } -\\log_2\\left(\\sum_{d_x=\\min[n , k]}^{\\min[m , t ] } q^{-[n-\\min[n , k]]d_x } \\alpha_{d_x } \\right).\\end{aligned}\\ ] ]    assuming that the expression inside the @xmath319 function in ( [ eq : i_derivative_approx ] ) is not zero for every @xmath320 $ ] , we can rewrite the kuhn - tucker conditions as @xmath321}^{\\min[m , t ] } q^{-[n-\\min[n , k]]d_x } \\alpha_{d_x } \\ge   2^{-c_{\\textsl{s}}+o(1 ) } q^{[t\\min[n , k]-nk]},\\ ] ] where the inequality holds with equality for all @xmath184 with @xmath322 .",
    "let @xmath323 $ ] and define the @xmath324 matrix @xmath16 with elements @xmath325j }   & \\min[n , i ] \\leq",
    "j \\leq \\delta,\\\\ 0 & \\textrm{otherwise}. \\end{array } \\right.\\end{aligned}\\ ] ]    we also define the column vector @xmath326 with elements @xmath327-ni]}$ ] for @xmath328 .",
    "note that for convenience the indices of matrix @xmath16 and vector @xmath326 start from @xmath200 .",
    "using these definitions , we are able to rewrite the kuhn - tucker conditions in the matrix form as @xmath329 in the following , we consider two cases for @xmath330 and @xmath331 , and find @xmath305 for each of them , separately .    * first case : @xmath330 . * in this case we can explicitly write the matrix @xmath16 and vector @xmath326 as @xmath332,\\ ] ] and @xmath333^\\mathrm{t}.\\ ] ]    the fact that the expression inside the @xmath334 function in ( [ eq : i_derivative_approx ] ) is non - zero for @xmath335 , forces @xmath336 to be positive . thus the last row of the matrix inequality in should be satisfied as an equality .",
    "therefore , @xmath337    now we use induction to show that the optimal solution has the form @xmath338 where we will determine @xmath339 later .",
    "let us fix @xmath340 and assume that @xmath341 for @xmath342 .",
    "then for @xmath343 we can write @xmath344 or equivalently @xmath345.\\end{aligned}\\ ] ] we can use induction for one step more to show that @xmath346 is of the desired form if the previous expression is satisfied with equality .",
    "this is true if we have @xmath347 , or equivalently ( assuming large @xmath41 ) if we have @xmath348 .",
    "so we can conclude that we should have @xmath349 .",
    "it can be easily verified that for @xmath350 the kuhn - tucker equation for @xmath351 satisfies the strict inequality so @xmath352 for @xmath353 $ ] .",
    "the above argument results in a solution of the following form for the case @xmath330",
    "@xmath354\\le i\\le\\delta,\\\\ 0 & : & 0\\le i < \\min\\left[(t - n)^+,\\delta\\right ] .",
    "\\end{array}\\right.\\ ] ]    * second case : @xmath331 . *",
    "we now write matrix @xmath16 and vector @xmath326 as @xmath355,\\end{aligned}\\ ] ] and @xmath356^\\mathrm{t}.\\ ] ] the last @xmath357 rows of @xmath16 are the same while @xmath358 is decreasing with @xmath118 for @xmath359 .",
    "thus , the last @xmath360 inequalities are strict and therefore , @xmath361    the remaining equations can simply be reduced to the first case .",
    "define @xmath362,\\ ] ] and @xmath363^\\mathrm{t}.\\ ] ] the remaining conditions in this case can be written as @xmath364 which is exactly similar to , for @xmath365 .",
    "therefore , the optimal solution for the first case will also satisfy these conditions , _",
    "i.e. _ , @xmath366 with @xmath367 $ ] . summarizing and , we can obtain the optimal solution for this regime , as @xmath368 where @xmath367 $ ] .",
    "this completes the proof of theorem  [ thm : main_result_single_src_dist ] . by normalizing @xmath351 to @xmath199",
    "we can also obtain an alternative proof to theorem  [ thm : main_result_single_src ] .",
    "* discussion : * to characterize the exact value of @xmath152 one have to consider the exact form of the set of equations given in ( for each @xmath340 ) which are as follows , @xmath369}\\right].\\end{aligned}\\ ] ] although it is hard to find @xmath152 exactly , it is possible to show that there exists finite @xmath152 such that result of theorem  [ thm : main_result_single_src_dist ] holds for .",
    "this can be done by solving above equations assuming that @xmath370 is zero for every @xmath184 ( assuming @xmath313 ) .",
    "then , it can be observed that the rhs of are either greater or less than zero .",
    "now by assuming finite but large enough @xmath41 and considering the exact form of we have some small perturbations that can not change the sign of rhs of so we are done .",
    "let @xmath370 denotes the error term in .",
    "we can easily write the exact expression for @xmath370 which is as follows @xmath371 } \\alpha_{d_x } \\frac{{{d_x \\brack d_y}}}{{{t \\brack d_y } } } q^{-nd_x } \\right ) \\nonumber\\\\ & + \\log_2\\left ( \\sum_{d_x = r_k}^{\\min[m , t ] } q^{r_k(d_x - r_k)-nd_x } \\alpha_{d_x } \\right ) - r_k(t - r_k)\\log_2{q},\\end{aligned}\\ ] ] where @xmath372 $ ] .",
    "we consider the case where @xmath156 $ ] so theorem  [ thm : main_result_single_src_dist ] implies that for the optimal input distribution we have @xmath373 where @xmath162 $ ] and @xmath153 .",
    "then we can simplify @xmath370 more and write @xmath374 where we also use lemma  [ lem : psi_recursive ] in the above simplification .    to find @xmath152 , the minimum value of @xmath41 that the result of theorem  [ thm : main_result_single_src_extnd ] is valid for",
    ", we should consider the exact form of and check that the rhs of is less than or equal to zero for @xmath375 .",
    "so from for every @xmath375 we may write @xmath376}\\right]\\le 0,\\end{aligned}\\ ] ] or equivalently @xmath377 using a similar argument we should have also @xmath378    from for the capacity @xmath311 we have @xmath379 .",
    "evaluating at @xmath190 we have @xmath380 which results in the capacity stated in the assertion of theorem  [ thm : main_result_single_src_extnd ] .",
    "* discussion : * we derive a sufficient condition on the minimum size of @xmath41 to satisfy the set of conditions stated in and . using this sufficient condition",
    "we explore the behavior of @xmath152 as @xmath5 increases .    for @xmath381 we can write @xmath382-\\min[n , k]+1 ) } \\left ( 2 + ( r_k-1)(t - i^*)\\log_2{q } \\right ) \\nonumber\\\\ & \\stackrel{(b)}{\\le } ( 8 + 8r_k ) + \\left ( 4r_k(r_k-1)(t - i^ * ) \\frac{\\log_2{q}}{q^{(\\max[n , k]-\\min[n , k]+1 ) } } \\right),\\end{aligned}\\ ] ] where @xmath267 follows from and , and in @xmath268 we use the fact that @xmath381 .",
    "then for @xmath190 we can write @xmath383 where @xmath267 follows from and .",
    "let us consider two cases .",
    "first , we assume that @xmath166 so @xmath384 . to find a sufficient condition for @xmath152 we have to only consider conditions given in . using and and assuming that @xmath385 we should have @xmath386 , or equivalently @xmath387 .    for the second case we have @xmath168 which means @xmath388 . here , using a similar argument to the one given above for the first case we can show that conditions give some constant @xmath152 as @xmath385 .",
    "however , the conditions give a sufficient condition for @xmath152 which grows as @xmath385 .",
    "now , using , , and and assuming that @xmath385 , a sufficient condition for @xmath152 would be @xmath389 . for large @xmath5 for the sufficient condition we have @xmath390 .",
    "the goal of this section is to characterize @xmath391 , the set of all achievable rate pairs @xmath392 for two user communication over the multiple access channel @xmath393 described in definition  [ def : matrix_channel_mac ] .",
    "more precisely , we will show that @xmath394 . in order to do this ,",
    "we first formulate a mathematical model for this channel .",
    "then , we present an achievability scheme , to show that @xmath221 is achievable , _ i.e. ,",
    "_ @xmath395 . in the next subsection",
    "we prove the optimality of this scheme and show that @xmath396 .",
    "the proof of the converse part of the theorem is based on two outer bounds , namely , a cooperative bound and a coloring bound . for the coloring bound , we utilize a combinatorial argument to bound the number of _ distinguishable _ symbol pairs that can be transmitted from the two sources to the destination .",
    "this bound allows us to restrict the _ effective _ input alphabets of the sources to subsets of the original alphabets , with significantly smaller size .",
    "we can then easily bound the capacity region of the network using the restricted input alphabet .",
    "our first result , stated in theorem  [ thm : channel_equivalence_mac ] , is that the multiple access matrix channel described in definition  [ def : matrix_channel_mac ] is equivalent to the `` subspace '' channel @xmath145 described in definition  [ def : subspace_channel_mac ] , that has subspaces as inputs and outputs .",
    "so to characterize the optimal rate region of @xmath131 , we can focus on finding the optimal rate region of @xmath145 . we will use this equivalence in the rest of this section .    we know from @xcite that the rate region of the multiple access channel @xmath145 is given by the closure of the convex hull of the rate vectors satisfying @xmath399 for some product distribution @xmath400",
    ". note that @xmath134 , where @xmath135 is the transmission rate of the @xmath118th source , @xmath401 and @xmath137 is the complement set of @xmath138 .      in this subsection",
    "we illustrate a simple achievability scheme for the corner points of the rate region defined in theorem  [ thm : main_result_multpl_src ] .",
    "the remaining points in the rate region can be achieved using time - sharing .    for given @xmath402 , define the following subspace code - books @xmath403 , \\mathbf{u}_1\\in{\\mathbb{f}}_q^{d_1\\times ( t - d_1-d_2 ) } \\bigg\\ } \\end{aligned}\\ ] ] and @xmath404 , \\mathbf{u}_2\\in{\\mathbb{f}}_q^{d_2\\times ( t - d_1-d_2 ) } \\bigg\\}.\\end{aligned}\\ ] ] if we transmit messages from these code - books , we have @xmath405,\\end{aligned}\\ ] ] where @xmath406 captures the first @xmath407 columns of @xmath408 .",
    "therefore , decoding at the receiver would be just recovering of @xmath409 and @xmath410 given @xmath411 , @xmath412 , and @xmath413 .",
    "since @xmath414 , the matrix @xmath415 $ ] is full - rank with high probability , and therefore the decoder is able to decode @xmath409 and @xmath410 .",
    "note that the achievability scheme uses effectively the coding vectors approach @xcite .",
    "this indicates that for @xmath416 $ ] and @xmath41 large enough , the subspace coding and the coding vectors approach achieve the same rate .      in the following we will present an outer bound for @xmath391 , the admissible rate region of the non - coherent two - user multiple access channel @xmath131 .",
    "recall that by theorem  [ thm : channel_equivalence_mac ] we can focus on the subspace channel @xmath145 .",
    "we first show in proposition  [ pre : outerboundcoop ] that @xmath417 , a cooperative outer - bound",
    ". then proposition  [ pre : outerboundcolering ] demonstrates that @xmath418 , a coloring outer - bound .",
    "finally we show that @xmath419 , yielding the desired outer - bound @xmath420 which matches the achievability of  [ sec : multiple_src_achv ] .",
    "the first outer bound , called cooperating outer bound , is simply obtained by letting the two transmitters cooperate to transmit their messages to the receiver , i.e. we assume they form a super - source . applying theorem  [ thm : main_result_single_src ] for the non - coherent scenario for the single super - source , the one who controls the packets of both transmitters , we have the following proposition .",
    "the rest of this section is dedicated to deriving the second outer bound which is denoted by @xmath425 .",
    "this bound is based on an argument on the number of messages per channel use that each user can reliably communicate over the multiple access channel .",
    "let @xmath426 be an achievable rate pair for which there exists an encoding and decoding scheme with block length @xmath204 and small error probability .",
    "one can follow the usual converse proof of the multiple access channel from @xcite to show that @xmath427 for each time instance @xmath90 , denote by @xmath428 , the projection of the code - book used by user @xmath118 to its @xmath90-th element . for a single source scenario ,",
    "we have shown in  [ sec : thechanlcap - snglsrc ] that we can use the set @xmath429 as our input alphabet for all time slots , and have the receiver successfully decode the sent messages , and hence , the user can communicate @xmath430 distinct messages . for the multi - source case , @xmath428 is more restricted .",
    "the main reason for this is that the transition probability of the multiple access channel @xmath431 is of the form @xmath432 .",
    "that is , if @xmath433 and @xmath434 satisfy @xmath435 , then @xmath436 , and hence the receiver can not distinguish between the two pairs .",
    "in the following we will discuss this indistinguishability in detail , and derive the maximum number of distinguishable pairs which can be conveyed through the channel . in order to do so , we start with some useful definitions and lemmas .",
    "it turns out that the cardinality of the set @xmath438 depends on @xmath18 only through its dimension , @xmath442 .",
    "therefore , we denote this number by @xmath443 , which is characterized in the following lemma .",
    "[ lem : a ] the cardinality of the set @xmath448 only depends on the dimensions of the two subspaces and their intersection , @xmath442 , @xmath449 , and @xmath450 .",
    "moreover , it can be asymptotically characterized by @xmath451 .      for a fixed time instance @xmath90 , and corresponding subsets @xmath456 and @xmath457",
    ", we can construct a table with @xmath458 rows and @xmath459 columns , each row ( column ) corresponding to one subspace @xmath18 ( @xmath19 ) in @xmath456 ( @xmath457 ) . in the following ,",
    "we define an equivalence relation for the cells of this table .",
    "it is clear that the coloring definition above exactly matches with that of indistinguishability we discussed before .",
    "more precisely , two pairs of subspaces @xmath463 and @xmath464 are distinguishable if and only if their corresponding cells in the table have different colors .",
    "the following theorem upper bounds the cardinality of the subspace sets based on this fact .",
    "[ thm : outerboundcolering ] for each pair of uniquely distinguishable sets @xmath465 defined on the input alphabet @xmath466 for the multiple access channel @xmath145 , there exist integer numbers @xmath467 such that latexmath:[\\ ] ]                        m.  jafari  siavoshani , s.  mohajer , c.  fragouli , and s.  diggavi , `` on the capacity of non - coherent network coding '' , _ ieee international symposium on information theory _ ,",
    "seoul , korea , pp .",
    "273277 , jun . 2009 .",
    "l.  keller , m.  jafari  siavoshani , c.  fragouli , k.  argyraki , and s.  diggavi , `` joint identity - message coding for sensor networks , '' _ ieee journal on selected areas in communications _ , vol .",
    "28 , no .  7 , pp .  10831093 , sep .",
    "see also _ proc .",
    "infocom _ , pp .",
    "21772185 , 2009 .",
    "l.  zheng and d.  n.  c.  tse , `` communication on the grassmannian manifold : a geometric approach to the non - coherent multiple - antenna channel , '' _ ieee transaction on information theory _ ,",
    "48 , pp .  359383 , feb .",
    "p.  sattari , a.  markopoulou , c.  fragouli , `` multiple source multiple destination topology inference using network coding , '' _ the workshop on network coding , theory and applications _ , lausanne , jun .",
    "2009 .",
    "m.  gadouleau and z.  yan , `` on the decoder error probability of bounded rank - distance decoders for maximum rank distance codes , '' _ ieee transactions on information theory _ , vol .",
    "54 , no .  7 , pp .  32023206 , jul",
    ". 2008 .",
    "m.  gadouleau and z.  yan , `` packing and covering properties of subspace codes for error control in random linear network coding , '' _ ieee transactions on information theory _ , vol .",
    "56 , no .  5 , pp .",
    "20972108 , may 2010 .",
    "mahdi jafari siavoshani received the bachelor degree in communication systems with a minor in applied physics at sharif university of technology , tehran , iran , in 2005 .",
    "he was awarded an excellency scholarship from epfl , switzerland , to study a master degree in communication system finished in 2007 .",
    "he is currently a phd student at the same university .",
    "his research interests include network coding , coding and information theory , wireless communications , and signal processing .",
    "soheil mohajer received the b.s .",
    "degree in electrical engineering from the sharif university of technology , tehran , iran , in 2004 , and the m.s .",
    "degrees in communication systems from ecole polytechnique fdrale de lausanne ( epfl ) , lausanne , switzerland , in 2005 .",
    "he completed his ph.d .",
    "at epfl in september 2010 , and since october 2010 is a post - doctoral researcher at princeton university .",
    "his fields of interests are multiuser information theory , network coding theory , and wireless communication .",
    "christina fragouli is a tenure - track assistant professor in the school of computer and communication sciences , epfl , switzerland .",
    "she received the b.s .",
    "degree in electrical engineering from the national technical university of athens , athens , greece , in 1996 , and the m.sc . and ph.d .",
    "degrees in electrical engineering from the university of california , los angeles , in 1998 and 2000 , respectively .",
    "she has worked at the information sciences center , at&t shannon labs , florham park new jersey , and the national university of athens .",
    "she also visited bell laboratories , murray hill , nj , and dimacs , rutgers university . from 2006 to 2007 , she was an fns assistant professor in the school of computer and communication sciences , epfl , switzerland .",
    "she served as an editor for ieee communications letters .",
    "she is currently serving as an editor for ieee transactions on information theory , ieee transactions on communications , elsevier computer communications and ieee transactions on mobile computing .",
    "she was the technical co - chair for the 2009 network coding symposium in lausanne and has served on program commmittees of several conferences .",
    "she received the fulbright fellowship for her graduate studies , the outstanding ph.d .",
    "student award 2000 - 2001 , ucla , electrical engineering department , the zonta award 2008 in switzerland , and the young investigator erc starting grant in 2009 .",
    "her research interests are in network information flow theory and algorithms , network coding , and connections between communications and computer science .",
    "suhas n. diggavi received the b. tech .",
    "degree in electrical engineering from the indian institute of technology , delhi , india , and the ph.d .",
    "degree in electrical engineering from stanford university , stanford , ca , in 1998 .    after completing his ph.d .",
    ", he was a principal member technical staff in the information sciences center , at&t shannon laboratories , florham park , nj . after that he was on the faculty at the school of computer and communication sciences , epfl , where he directed the laboratory for information and communication systems ( licos ) .",
    "he is currently a professor , in the department of electrical engineering , at the university of california , los angeles .",
    "his research interests include wireless communications networks , information theory , network data compression and network algorithms .",
    "he is a recipient of the 2006 ieee donald fink prize paper award , 2005 ieee vehicular technology conference best paper award and the okawa foundation research award .",
    "he is currently an editor for acm / ieee transactions on networking and ieee transactions on information theory .",
    "he has 8 issued patents ."
  ],
  "abstract_text": [
    "<S> we consider the problem of multicasting information from a source to a set of receivers over a network where intermediate network nodes perform randomized network coding operations on the source packets . </S>",
    "<S> we propose a channel model for the non - coherent network coding introduced by koetter and kschischang in @xcite , that captures the essence of such a network operation , and calculate the capacity as a function of network parameters . </S>",
    "<S> we prove that use of subspace coding is optimal , and show that , in some cases , the capacity - achieving distribution uses subspaces of several dimensions , where the employed dimensions depend on the packet length . </S>",
    "<S> this model and the results also allow us to give guidelines on when subspace coding is beneficial for the proposed model and by how much , in comparison to a coding vector approach , from a capacity viewpoint . </S>",
    "<S> we extend our results to the case of multiple source multicast that creates a virtual multiple access channel .    </S>",
    "<S> * keywords *    network coding , non - coherent communication , subspace coding , channel capacity , multi - source multicast , randomized network coding . </S>"
  ]
}