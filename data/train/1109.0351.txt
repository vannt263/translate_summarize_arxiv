{
  "article_text": [
    "the directed information @xmath0 between two random @xmath1-sequences @xmath2 and @xmath3 is a natural generalization of shannon s mutual information to random objects obeying causal relations",
    ". introduced by massey  @xcite , this notion has been shown to arise as the canonical answer to a variety of problems with causally dependent components .",
    "for example , it plays a pivotal role in characterizing the capacity @xmath4 of a communication channel with feedback .",
    "massey  @xcite showed that the feedback capacity is upper bounded as @xmath5 where @xmath6 and @xmath7 ; see also kramer  @xcite that streamlines the notion of directed information by causal conditioning .",
    "the upper bound in   is tight for certain classes of ergodic channels , such as general nonanticipatory channels satisfying certain regularity conditions  @xcite , channels with finite input memory and ergodic noise  @xcite , and indecomposable finite - state channels  @xcite , paving the road to a computable characterization of feedback capacity ; see  @xcite for examples .",
    "directed information and its variants also characterize ( via multiletter expressions ) the capacity for two - way channels  @xcite , multiple access channels with feedback  @xcite , broadcast channels with feedback @xcite , and compound channels with feedback @xcite , as well as the rate ",
    "distortion function with feedforward @xcite . in another context ,",
    "directed information captures the difference in growth rates of wealth in horse race gambling due to _ causal _ side information  @xcite .",
    "this provides a natural interpretation of @xmath0 as the amount of information about @xmath8 causally provided by @xmath9 on the fly .",
    "similar interpretations for directed information can be drawn for other problems in science and engineering  @xcite .",
    "this paper is dedicated to extending the mathematical notion of directed information to continuous - time random processes and to establishing results that demonstrate the operational significance of this notion in estimation and communication .",
    "our contributions include the following :    * we introduce the notion of directed information in continuous time . given a pair of continuous - time processes in a time interval and its partition consisting of @xmath1 subintervals , we first consider the ( discrete - time ) directed information for the two sequences of length @xmath1 whose components are the sample paths on the respective subintervals .",
    "the resulting quantity depends on the specific partition of the time interval .",
    "we define directed information in continuous time by taking the infimum over all finite time partitions .",
    "thus , in contrast to mutual information in continuous time which can be defined as a _",
    "supremum _ of mutual information over finite `` space '' partitions  ( * ? ? ?",
    "2.5 ) , ( * ? ? ?",
    "3.5 ) , inherent to our notion of directed information is a similar supremum followed by an _",
    "infimum _ over time partitions .",
    "we explain why this definition is natural by showing that the continuous - time directed information inherits key properties of its discrete - time origin and by establishing new properties that are meaningful in continuous time . *",
    "we show that this notion of directed information arises in extending classical relationships between information and estimation in continuous time ",
    "duncan s theorem @xcite that relates the minimum mean squared error ( mmse ) in causal estimation of a target signal based on an observation through an additive white gaussian noise channel to the _ mutual information _ between the target signal and the observation , and its counterpart for the poisson channel  to the scenarios in which the channel input process can causally depend on the channel output process , whereby corresponding relationships now hold between _ directed information _ and estimation .",
    "* we illustrate these relationships between directed information and estimation by characterizing the directed information rate and the feedback capacity of a continuous - time poisson channel with inputs constrained to be constant between events at the channel output .",
    "* we establish the fundamental role of continuous - time directed information in characterizing the feedback capacity of a large class of continuous - time channels .",
    "in particular , we show that for channels where the output is a function of the input and some stationary ergodic `` noise '' process , the continuous - time directed information characterizes the feedback capacity of the channel .    the remainder of the paper is organized as follows .",
    "section [ sec : definition of directed information in continuous time ] is devoted to the definition of directed information and related quantities in continuous time , which is followed by a presentation of key properties of continuous - time directed information in section [ sec : properties of the directed information in continuous time ] . in section [ sec : directed information and causal estimation ] , we establish the generalizations of duncan s theorem and its poisson counterpart that accommodate the presence of feedback . in section",
    "[ sec : poisson feedback example ] , we apply the relationship between the causal estimation error and directed information for the poisson channel to compute the directed information rate between the input and the output of this channel in a scenario that involves feedback . in section [ sec : communication over continuous - time channels with feedback ] , we study a general feedback communication problem in which our notion of directed information in continuous time emerges naturally in the characterization of the feedback capacity .",
    "section [ sec : concluding remarks ] concludes the paper with a few remarks .",
    "let @xmath10 and @xmath11 be two probability measures on the same space and @xmath12 be the radon  nikodym derivative of @xmath10 with respect to @xmath11 .",
    "the relative entropy between @xmath10 and @xmath11 is defined as @xmath13 for jointly distributed random objects @xmath14 and @xmath15 , the mutual information between them is defined as @xmath16 where @xmath17 denotes the product distribution under which @xmath14 and @xmath15 are independent but maintain their respective marginal distributions . as an alternative , the mutual information is defined  ( * ? ? ?",
    "2.5 ) as @xmath18;[v]),\\ ] ] where the supremum is over all finite quantizations of @xmath14 and @xmath15 . that the two notions coincide has been established in , e.g. , @xcite , ( * ? ? ?",
    "we write @xmath19 instead of @xmath20 when we wish to emphasize the dependence on the joint distribution @xmath21 .    for a jointly distributed random triple @xmath22 with components in arbitrary measurable spaces ,",
    "we define the conditional mutual information between @xmath14 and @xmath15 given @xmath23 as @xmath24;[v ] | w )   , \\ ] ] where the supremum is over all finite quantizations of @xmath14 and @xmath15 .",
    "this quantity , due to wyner  @xcite , is always well defined and satisfies all the basic properties of conditional mutual information for discrete and continuous random variables , in particular :    1 .",
    "_ nonnegativity : _ @xmath25 with equality",
    "iff @xmath26 form a markov chain ( that is , @xmath14 and @xmath15 are conditionally independent given @xmath23 ) .",
    "chain rule : _",
    "data processing inequality : _ if @xmath28 form a markov chain , then @xmath29 with equality iff @xmath30 .    the definition in coincides with dobrushin s more restrictive definition @xcite @xmath31 where @xmath32 is a regular version of the conditional probability law of @xmath33 given @xmath34 ( cf .",
    "* ch .  6 ) ) if it exists .",
    "let @xmath35 be a pair of random @xmath1-sequences .",
    "the directed information from @xmath9 to @xmath8 is defined as @xmath36 note that , unlike mutual information , directed information is asymmetric in its arguments , i.e. , @xmath37 in general .",
    "let us now develop the notion of directed information between two continuous - time stochastic processes on the time interval @xmath38 . for a continuous - time process",
    "@xmath39 , let @xmath40 denote the process in the time interval @xmath41 .",
    "let @xmath42 denote a vector with components satisfying @xmath43 let @xmath44 denote the sequence of length @xmath1 resulting from `` chopping up '' the continuous - time signal @xmath45 into consecutive segments as @xmath46 note that each component of the sequence is a continuous - time stochastic process . for a pair of jointly distributed stochastic processes @xmath47 , define @xmath48 where on the right side of is the directed information between two sequences of length @xmath1 defined in  ; and in we note that the conditional mutual information terms , defined as in ( [ eq : conditional mutinf defined ] ) , are between two continuous - time processes , conditioned on a third .",
    "we extend this definition to @xmath49 , where @xmath15 is a random object jointly distributed with @xmath50 , in the obvious way , namely @xmath51    we define @xmath52 to be the set of all finite partitions of the time interval @xmath53 .",
    "the quantity @xmath54 is monotone in @xmath55 in the following sense :    [ claim : monotonicity of it ] let @xmath55 and @xmath56 be partitions in @xmath57 . if @xmath56 is a refinement of @xmath55 , i.e. , @xmath58 , then @xmath59 .",
    "it suffices to prove the claim assuming @xmath55 as in and that @xmath56 is the @xmath60-dimensional vector with components @xmath61 for such @xmath55 and @xmath56 , we have from @xmath62 \\\\ & \\qquad = i ( y_{t_{i-1}}^{t_i } ; x_0^{t_i } | y_0^{t_{i-1 } }     )   - \\bigl [ i ( y_{t_{i-1}}^{t ' } ; x_0^{t ' } | y_0^{t_{i-1 } }     ) + i ( y_{t'}^{t_i } ; x_0^{t_i } |     y_0^{t_{i-1 } } , y_{t_{i-1}}^{t ' }     ) \\bigr ] \\\\ & \\qquad=   i ( x_0^{t ' } , x_{t'}^{t_i } ; y_{t_{i-1}}^{t ' } , y_{t'}^{t_i }                     | y_0^{t_{i-1 } } )          - \\bigl [ i ( y_{t_{i-1}}^{t ' } ; x_0^{t ' } | y_0^{t_{i-1 } } )          + i ( y_{t'}^{t_i } ; x_0^{t ' } , x_{t'}^{t_i }                     | y_0^{t_{i-1 } } , y_{t_{i-1}}^{t ' } ) \\bigr ] \\\\ & \\qquad = i ( x_0^{t ' } , x_{t'}^{t_i } ; y_{t_{i-1}}^{t ' } , y_{t'}^{t_i } | y_0^{t_{i-1 } }     ) - i ( x_0^{t ' }   x_{t'}^{t_i } \\to y_{t_{i-1}}^{t ' } , y_{t'}^{t_i } | y_0^{t_{i-1 } }     ) \\\\ & \\qquad \\ge 0,\\end{aligned}\\ ] ] where the last inequality follows since directed information ( between two sequences of length 2 in this case ) is upper bounded by the mutual information ( * ? ? ?",
    "* th .  2 ) .",
    "the following definition is now natural :    [ def : definition of directed info in cont time ] let @xmath47 be a pair of stochastic processes .",
    "the _ directed information _ from @xmath45 to @xmath63 is defined as @xmath64 if @xmath15 is another random object jointly distributed with @xmath50 we define the conditional directed information @xmath65 as @xmath66    note that the definitions and conventions preceding definition [ def : definition of directed info in cont time ] imply that the directed information @xmath67 is a nonnegative extended real number ( i.e. , as an element of @xmath68 $ ] ) .",
    "it is also worth noting , by recalling ( [ eq : mutinf defined alt ] ) , that each of the conditional mutual information terms in ( [ eq : mut inf summands in definition of ieps ] ) , and hence the sum , is a supremum over `` space '' partitions of the stochastic process in the corresponding time intervals .",
    "thus the directed information in ( [ eq : directed info in continuous time defined ] ) is an infimum over time partitions of a supremum over space partitions .",
    "also note that @xmath69 where the infimum is over all partitions in @xmath57 with subinterval lengths uniformly bounded by @xmath70 .",
    "indeed , for any @xmath70 and any partition @xmath71 , have @xmath72 , since a refinement of the time interval does not increase the directed information as seen in proposition [ claim : monotonicity of it ] . by the arbitrariness of @xmath71",
    ", this implies @xmath73 which in turn implies @xmath74 by the arbitrariness of @xmath75 .",
    "since the reverse inequality @xmath76 is immediate from the definition of @xmath67 , we have .",
    "as is clear from its definition in , the discrete - time directed information satisfies @xmath77 a continuous - time analogue would be that , for small @xmath78 , @xmath79 thus , if our proposed notion of directed information in continuous time is to be a natural extension of that in discrete time , one might expect the approximate relation to hold in some sense . toward a precise statement ,",
    "denote @xmath80 whenever the limit exists .",
    "assuming @xmath81 exists , let @xmath82 and note that is equivalent to @xmath83    [ claim : dir inf in terms of its differential ] fix @xmath84 .",
    "suppose that @xmath81 is continuous at @xmath85 and that the convergence in is uniform in the interval @xmath86 for some @xmath87 .",
    "then @xmath88    note that proposition [ claim : dir inf in terms of its differential ] formalizes by implying that the left and right hand sides of , when normalized by @xmath89 , coincide in the limit of small @xmath89 .",
    "note first that the stipulated uniform convergence in implies the existence of @xmath87 and a monotone function @xmath90 such that @xmath91 and @xmath92 fix now @xmath93 and consider @xmath94 , \\label{eq : directed inf plus eps}\\end{aligned}\\ ] ] where the equality in follows since the infimum over all partitions does not change by restricting to partitions that have an interval up to time @xmath85 and from time @xmath85 and the last equality follows by the definition of the function @xmath95 in .",
    "now , @xmath96 & \\le     \\inf_{{\\mathbf{t}}\\in { \\mathcal{t}}(t , t+ { \\varepsilon } ) }   \\sum_{i = 1}^n ( t_i - t_{i-1 } ) \\cdot     \\biggl[\\,\\sup_{t ' \\in [ t , t+ { \\varepsilon } ) } i_{t ' } + f ( { \\varepsilon } )    \\biggr ]     \\label{eq : exp of int with eta terms } \\\\ & =    { \\varepsilon}\\biggl[\\ , \\sup_{t ' \\in [ t , t+ { \\varepsilon } ) } i_{t ' } + f ( { \\varepsilon } ) \\biggr ] ,     \\label{eq : upper bound on the diff}\\end{aligned}\\ ] ] where the inequality in is due to and the monotonicity of @xmath97 , which implies @xmath98 , as @xmath99 is the length of a subinterval in @xmath100 . bounding the @xmath95 terms in from the other direction , we similarly obtain @xmath101 \\ge        { \\varepsilon}\\biggl[\\ , \\inf_{t ' \\in [ t , t+ { \\varepsilon } ) } i_{t ' } - f ( { \\varepsilon } )    \\biggr ] .",
    "\\label{eq : lower bound on the diff}\\ ] ] combining , , and yields @xmath102 the continuity of @xmath81 at @xmath85 implies @xmath103 and thus , taking the limit @xmath104 in and applying finally yields @xmath105 which completes the proof of proposition [ claim : dir inf in terms of its differential ] .    beyond the intuitive appeal of proposition [ claim : dir inf in terms of its differential ] in formalizing , it also provides a useful formula for computing directed information .",
    "indeed , the integral version of is @xmath106 as the following example illustrates , evaluating the right hand side of ( via the definition of @xmath81 in ) can be simpler than tackling the left hand side directly via definition [ def : definition of directed info in cont time ] .",
    "let @xmath107 be a standard brownian motion and @xmath108 be independent of @xmath107 .",
    "let @xmath109 for all @xmath85 and @xmath110 .",
    "letting @xmath111 denote the mutual information between a gaussian random variable of variance @xmath10 and its corrupted version by an independent gaussian noise of variance @xmath112 , we have for every @xmath113 @xmath114 with such an explicit expression for @xmath115 , @xmath81 can be obtained directly from its definition : @xmath116 we can now compute the directed information by applying proposition  [ claim : dir inf in terms of its differential ] : @xmath117 note that in this example @xmath118 and thus , by , we have @xmath119 .",
    "this equality between mutual information and directed information holds in more general situations , as elaborated in the next section .",
    "the directed information we have just defined is between two processes on @xmath38 .",
    "we extend this definition to processes of different durations by zero - padding at the beginning of the shorter process .",
    "for instance , @xmath120 where @xmath121 denotes a process on @xmath38 formed by concatenating a process that is equal to the constant @xmath122 for the time interval @xmath123 and then the process @xmath124 .",
    "define now @xmath125 and @xmath126 finally , define the directed information @xmath127 by @xmath128 when the limit exists , or equivalently , when @xmath129 . as we shall see below ( in the last part of proposition [ prop : 2 properties of directed info ] ) , @xmath127 is guaranteed to exist whenever @xmath130 .",
    "the following proposition collects some properties of directed information in continuous time :    [ prop : 2 properties of directed info ] let @xmath131 be a pair of jointly distributed stochastic processes . then :    1 .",
    "monotonicity : @xmath132 is monotone nondecreasing in @xmath133 .",
    "2 .   invariance to time dilation : for @xmath134 , if @xmath135 and @xmath136 , then @xmath137 . more generally , if @xmath138 is monotone strictly increasing and continuous , and @xmath139 , then @xmath140 3 .   coincidence of directed information and mutual information : if the markov relation @xmath141 holds for all @xmath142 , then @xmath143 4 .",
    "equivalence between discrete time and piecewise constancy in continuous time : let @xmath144 be a pair of jointly distributed @xmath1-tuples and suppose @xmath145 satisfy  .",
    "let the pair @xmath50 be defined as the piecewise - constant process satisfying @xmath146 for @xmath147",
    ". then @xmath148 5 .",
    "conservation law : for any @xmath149 we have @xmath150 further , if @xmath151 then @xmath152 exists and @xmath153    1 .",
    "the first , second , and fourth parts in the proposition present properties that are known to hold for mutual information ( when all the directed information expressions in those items are replaced by the corresponding mutual information ) , which follow immediately from the data processing inequality and the invariance of mutual information to one - to - one transformations of its arguments . that these properties hold also for directed information is not as obvious in view of the fact that directed information is , in general , not invariant to one - to - one transformations nor does it satisfy the data processing inequality in its second argument .",
    "2 .   the third part of the proposition is a natural analogue of the fact that @xmath154 whenever @xmath155 form a markov chain for all @xmath156 .",
    "it covers , in particular , any scenario where @xmath45 and @xmath63 are the input and output of any channel of the form @xmath157 , where the process @xmath158 ( which can be thought of as the internal channel noise ) is independent of the channel input process @xmath45 . to see this",
    ", note that in this case we have @xmath159 for all @xmath160 , implying @xmath141 since @xmath161 is determined by the pair @xmath162 .",
    "3 .   particularizing even further , we obtain @xmath163 whenever @xmath63 is the outcome of corrupting @xmath45 with additive noise , i.e. , @xmath164 , where @xmath45 and @xmath158 are independent .",
    "4 .   the fifth part of the proposition can be considered the continuous - time analogue of the discrete - time conservation law @xcite @xmath165 it is consistent with , and in fact generalizes , the third part .",
    "indeed , if the markov relation @xmath141 holds for all @xmath166 then our definition of directed information is readily seen to imply that @xmath167 for all @xmath78 and therefore that @xmath168 exists and equals zero .",
    "thus in this case reduces to .",
    "the first part of the proposition follows immediately from the definition of directed information in continuous time ( definition  [ def : definition of directed info in cont time ] ) and from the fact that , in discrete time , @xmath169 for @xmath170 . the second part follows from definition  [ def : definition of directed info in cont time ] upon noting that , under a dilation @xmath138 as stipulated , due to the invariance of mutual information to one - to - one transformations of its arguments , for any partition @xmath55 of @xmath38 , @xmath171 where @xmath172 is shorthand for @xmath173 .",
    "thus @xmath174 where and   follow from definition [ def : definition of directed info in cont time ] , follows from , and is due to the strict monotonicity and continuity of @xmath138 which implies that @xmath175    moving to the proof of the third part , assume that the markov relation @xmath141 holds for all @xmath166 and fix @xmath42 as in",
    ". then @xmath176 where follows since @xmath177 for each @xmath178 , and is due to the chain rule for mutual information .",
    "the proof of the third part of the proposition now follows from the arbitrariness of @xmath55 .    to prove the fourth part ,",
    "consider first the case @xmath179 . in this case @xmath180 and @xmath181 for all @xmath113 .",
    "it is an immediate consequence of the definition of directed information that @xmath182 and therefore that @xmath183 for all @xmath55 .",
    "consequently @xmath184 , which establishes the case @xmath179 . for the general case @xmath185 ,",
    "note first that it is immediate from the definition of @xmath186 and from the construction of @xmath187 based on @xmath188 in that for @xmath189 consisting of the time epochs in we have @xmath190 .",
    "thus @xmath191 .",
    "we now argue that @xmath192 for any partition @xmath193 . by proposition  [ claim :",
    "monotonicity of it ] , it suffices to establish with equality assuming @xmath193 is a refinement of the particular @xmath55 just discussed , that is , @xmath193 is of the form @xmath194 then , @xmath195 where follows by applying a similar argument as in the case @xmath179 .    moving to the proof of the fifth part of the proposition , fix @xmath42 as in with @xmath196 . applying the discrete - time conservation law , we have @xmath197 and consequently , for any @xmath75 , @xmath198 \\label{eq : in dirminus by refinement prop } \\\\",
    "& \\qquad= i ( x_0^t ; y_0^t ) , \\label{eq : in dirminus by refinement prop eps pos}\\end{aligned}\\ ] ] where the equality in follows since due to its definition in , @xmath199 does not decrease by refining the time interval @xmath55 in the @xmath200 interval ; the equality in follows from the refinement property in proposition  [ claim : monotonicity of it ] , which implies that for arbitrary processes @xmath201 and partitions @xmath55 and @xmath202 there exists a third partition @xmath203 ( which will be a refinement of both ) such that @xmath204 and the equality in follows since holds for any @xmath205 with @xmath206 .",
    "hence , @xmath207 \\label{eq : property 5 step a }   \\\\ & = \\lim_{{\\varepsilon}\\to 0^+ }   \\inf _ { \\ { { \\mathbf{t } } : t_1 = \\delta ,   \\max_{i \\ge 2 } t_i - t_{i-1 } \\le { \\varepsilon}\\ } } i_{{\\mathbf{t } } } (   x_0^t \\to y_0^t ) + \\lim_{{\\varepsilon}\\to 0^+ } \\inf _ { \\ { { \\mathbf{t } } :   \\max_{i } t_i - t_{i-1 } \\le { \\varepsilon}\\ } } i_{{\\mathbf{t } } } ( y_0^{t-\\delta } \\to x_0^t )   \\label{eq : frist exp w 2 limits } \\\\ & =   \\lim_{{\\varepsilon}\\to 0^+ }   \\inf _ { \\ { { \\mathbf{t } } : t_1 = \\delta ,   \\max_{i \\ge 2 } t_i - t_{i-1 } \\le { \\varepsilon}\\ } } \\biggl [   i(x_0^{\\delta } ; y_0^{\\delta } ) +   \\sum_{i=2}^{n } i ( y_{t_{i-1}}^{t_i } ; x_0^{t_i } | y_0^{t_{i-1 } }     ) \\biggr ] +   i ( y_0^{t-\\delta } \\to x_0^t ) \\label{eq : property 5 step b } \\\\ & = i(x_0^{\\delta } ; y_0^{\\delta } ) + \\lim_{{\\varepsilon}\\to 0^+ }   \\inf _ { \\ { { \\mathbf{t } } : t_1 = \\delta ,   \\max_{i \\ge 2 } t_i - t_{i-1 } \\le { \\varepsilon}\\ } }       \\sum_{i=2}^{n } i ( y_{t_{i-1}}^{t_i } ; x_0^{t_i } | y_0^{t_{i-1 } } ) +   i",
    "( y_0^{t-\\delta } \\to x_0^t )   \\\\ & = i(x_0^{\\delta } ; y_0^{\\delta } ) + i ( x_0^t \\to   y_\\delta^t | y_0^{\\delta } ) +   i ( y_0^{t-\\delta } \\to x_0^t ) , \\label{eq : property 5 step c}\\end{aligned}\\ ] ] where the equality in   follows by taking the limit @xmath208 from both sides of ; the equality in   follows by writing out @xmath209 explicitly for @xmath55 with @xmath210 and using to equate the second limit in with @xmath211 ; and the equality in   follows by applying on the conditional distribution of the pair @xmath212 given @xmath213 . we have thus proven or , equivalently , the identity @xmath214 toward the proof of ( [ eq : conservation law under continuity ] ) , for @xmath71 and @xmath215 let @xmath216 denote the refinement of @xmath55 obtained by adding an additional point at @xmath89",
    ". then @xmath217 where the first inequality follows since @xmath216 is a refinement of @xmath55 , the equality by writing out the sum that defines @xmath218 and isolating its first term , and the second inequality by the infimum over partitions inherent in the definition of @xmath219 .",
    "the arbitrariness of @xmath215 in ( [ eq : ub interms of arbit delta ] ) implies @xmath220 which , by the arbitrariness of @xmath71 , implies @xmath221 on the other hand , for any @xmath78 , we clearly have @xmath222 as the right hand side , by its definition , is an infimum over all partitions in @xmath57 , while the left hand side corresponds to an infimum over the subset consisting only of those partitions with @xmath210 . by the arbitrariness of @xmath89 in ( [ eq : lb on i plus delta ] ) we obtain @xmath223 which , when combined with ( [ eq : limsup part of revision ] ) , finally implies @xmath224 existence of the limit in ( [ eq : existence of the limit ] ) , when combined with ( [ eq : conservation law in cont time ] ) and the added assumption @xmath151 , implies existence of the limit @xmath225 and that @xmath226 thus completing the proof .",
    "in @xcite , duncan discovered the following fundamental relationship between the minimum mean squared error ( mmse ) in causal estimation of a target signal corrupted by an additive white gaussian noise ( awgn ) in continuous time and the mutual information between the clean and noise - corrupted signals :    [ th : duncan68 ] let @xmath45 be a signal of finite average power @xmath227 dt < \\infty$ ] , independent of a standard brownian motion @xmath228 .",
    "let @xmath63 satisfy @xmath110 .",
    "then @xmath229)^2 \\bigr ] dt = i (   x_0^t ; y_0^t ) .\\ ] ]    a remarkable aspect of duncan s theorem is that the relationship holds regardless of the distribution of @xmath45 . among its ramifications is the invariance of the causal mmse to the flow of time , or more generally , to any reordering of time @xcite . it should also be mentioned that , although this exact relationship holds in continuous - time , approximate versions that hold in discrete - time can be derived from it , as is done in ( * ? ? ?",
    "* theorem 9 ) .",
    "a key stipulation in duncan s theorem is the independence between the noise - free signal @xmath45 and the channel noise @xmath230 , which excludes scenarios in which the evolution of @xmath231 is affected by the channel noise , as is often the case in signal processing ( e.g. , target tracking ) and communication ( e.g. , in the presence of feedback ) .",
    "indeed , the identity does not hold in the absence of such a stipulation .    as an extreme example , consider the case where the channel input is simply the channel output with some delay , i.e. , @xmath232 for some @xmath75 ( and @xmath233 for @xmath234 ) . in this case",
    "the causal mmse on the left side of is clearly @xmath122 , while the mutual information on its right side is infinite . on the other hand , in this case the directed information @xmath235 , as can be seen by noting that @xmath236 for all @xmath55 satisfying @xmath237 ( since for such @xmath55 , @xmath238 is determined by @xmath239 for all @xmath240 ) .",
    "the third remark following proposition [ prop : 2 properties of directed info ] implies that theorem [ th : duncan68 ] could be equivalently stated with @xmath241 on the right side of replaced by @xmath242 .",
    "furthermore , such a modified identity would be valid in the extreme example in  .",
    "this is no coincidence and is a consequence of the result that follows , which generalizes duncan s theorem . to state it",
    "formally we assume a probability space @xmath243 with an associated filtration @xmath244 satisfying the `` usual conditions '' ( right - continuous and @xmath245 contains all the @xmath10-negligible events in @xmath246 , cf . ,",
    "e.g. , ( * ? ? ?",
    "* definition 2.25 ) ) .",
    "recall also that when the standard brownian motion is adapted to @xmath244 then , by definition , it is implied that , for any @xmath247 , @xmath248 is independent of @xmath249 ( rather than merely of @xmath250 , cf . ,",
    "e.g. , ( * ? ? ?",
    "* definition 1.1 ) ) .",
    "[ th : our generalization of duncan s theorem ] let @xmath251 be adapted to the filtration @xmath252 , where @xmath45 is a signal of finite average power @xmath227 dt < \\infty$ ] and @xmath253 is a standard brownian motion .",
    "let @xmath63 be the output of the awgn channel whose input is @xmath45 and whose noise is driven by @xmath253 , i.e. , @xmath254 suppose that the regularity assumptions of proposition [ claim : dir inf in terms of its differential ] are satisfied for all @xmath84 .",
    "then @xmath255)^2 \\bigr ] dt = i (   x_0^t \\to y_0^t ) .\\ ] ]    note that unlike in theorem [ th : duncan68 ] , where the channel input process is independent of the channel noise process , in theorem [ th : our generalization of duncan s theorem ] no such stipulation exists and thus the setting in the latter accommodates the presence of feedback . furthermore , since @xmath67 is not invariant to the direction of the flow of time in general , theorem [ th : our generalization of duncan s theorem ] implies , as should be expected , that neither is the causal mmse for processes evolving in the generality afforded by the theorem .    that theorem [ th : duncan68 ]",
    "can be extended to accommodate the presence of feedback has been established for a communication theoretic framework by kadota , zakai , and ziv  @xcite . indeed , in communication over the awgn channel where @xmath256 is the waveform associated with message @xmath257 , in the absence of feedback the markov relation @xmath258 implies that @xmath259 on the right hand side of , when applying theorem [ th : duncan68 ] in this restricted communication framework , can be equivalently written as @xmath260 .",
    "the main result of @xcite is that this relationship between the causal estimation error and @xmath260 persists in the presence of feedback , i.e. , that @xmath261)^2 \\bigr ] dt = i (   m ; y_0^t ) \\ ] ] with or without feedback , even though , in the presence of feedback , one no longer has @xmath262 and therefore ( [ eq : duncan relation original ] ) is no longer true . the combination of theorem [ th : our generalization of duncan s theorem ] with the main result of @xcite ( namely , with ( [ eq : duncan relation original again ] ) ) thus implies that in communication over the awgn channel , with or without feedback , we have @xmath263 .",
    "this equality holds well beyond the gaussian channel , as is elaborated in section [ sec : communication over continuous - time channels with feedback ] .",
    "evidently , theorem [ th : our generalization of duncan s theorem ] can be considered an extension of the kadota ",
    "zakai  ziv result as it holds in settings more general than communication , where there is no message but merely a signal observed through additive white gaussian noise , adapted to a general filtration .    theorem [ th : our generalization of duncan s theorem ] is a direct consequence of proposition [ claim : dir inf in terms of its differential ] and the following lemma .",
    "[ lemma : relative ent with fb for gaussian ] let @xmath10 and @xmath11 be two probability laws governing @xmath50 , under which and the stipulations of theorem [ th : our generalization of duncan s theorem ] are satisfied",
    ". then @xmath264)^2 - ( x_t - e_p[x_t|y_0^t])^2   dt   \\biggr ]   .\\ ] ]    lemma [ lemma : relative ent with fb for gaussian ] was implicit in @xcite .",
    "it follows from the second part of ( * ? ? ?",
    "* theorem 2 ) , put together with the exposition in ( * ? ? ?",
    "* subsection iv - d ) ( cf . , in particular , equations ( 148 ) through ( 161 ) therein ) .",
    "consider @xmath265)^2 - ( x_s - x_s)^2     ds \\,\\bigg|\\ , y_0^t , x_t^{t+\\delta } \\biggr ] d p_{y_0^t , x_t^{t+\\delta } } ( y_0^t , x_t^{t+\\delta } ) \\label{eq : duncan proof step a } \\\\ & =   \\frac{1}{2 } \\int_t^{t + \\delta } e \\bigl[(x_s - e[x_s | y_0^s])^2 \\bigr ] ds,\\end{aligned}\\ ] ] where the equality in   follows by applying to the integrand in as follows : replacing the time interval @xmath38 by @xmath266 , substituting @xmath10 by the law of @xmath267 conditioned on @xmath268 ( note that @xmath269 is deterministic at @xmath270 under this law ) , and substituting @xmath11 by the law of @xmath267 conditioned on @xmath271 .",
    "the last step is obtained by switching between the integral @xmath272 and @xmath273 and then using the definition of conditional expectation .",
    "the switch between the integrals is possible due to fubini s theorem and the fact that the signal has finite average power @xmath227 dt < \\infty$ ] .",
    "it follows that @xmath81 defined in exists and is given by @xmath274)^2 \\bigr],\\ ] ] which completes the proof by an appeal to proposition [ claim : dir inf in terms of its differential ] .",
    "consider the function @xmath275 $ ] given by @xmath276 that this function is natural for quantifying the loss when estimating nonnegative quantities is implied in ( * ? ? ?",
    "* section 2 ) , where some of its basic properties are exposed . among them is that conditional expectation is the optimal estimator not only under the squared error loss but also under @xmath277 , i.e. , for any nonnegative random variable @xmath278 jointly distributed with @xmath279 , @xmath280 = e \\left [ \\ell ( x , e(x|y ) ) \\right ] , \\ ] ] where the minimum is over all ( measurable ) maps from the domain of @xmath279 into @xmath281 . with this loss function , the analogue of duncan s theorem for the case of doubly stochastic poisson process ( i.e. , the intensity is a random process ) can be stated as :    [ th : liptsershiryaev2001 ] let @xmath63 be a doubly stochastic poisson process and @xmath45 be its intensity process ( i.e. , conditioned on @xmath45 , @xmath63 is a nonhomogenous poisson process with rate function @xmath45 ) satisfying @xmath282 . then @xmath283 ) ] dt = i (   x_0^t ; y_0^t ) .\\ ] ]    we remark that for @xmath284 , one has @xmath285 ) \\bigr ] = e \\bigl [ \\ell   ( x_t ,",
    "e[x_t | y_0^t ] ) \\bigr ] , \\ ] ] and thus ( [ eq : duncan analogue relation original for poisson ] ) can equivalently be expressed as @xmath286 ) \\bigr ] dt = i (   x_0^t ; y_0^t ) , \\ ] ] as was done in @xcite and other classical references .",
    "but it was not until @xcite that the left hand side was established as the minimum mean causal estimation error under an explicitly identified loss function , thus completing the analogy with duncan s theorem .",
    "the condition stipulated in the third item of proposition [ prop : 2 properties of directed info ] is readily seen to hold when @xmath63 is a doubly stochastic poisson process and @xmath45 is its intensity process .",
    "thus , the above theorem could equivalently be stated with directed information rather than mutual information on the right hand side of .",
    "indeed , with continuous - time directed information replacing mutual information , this relationship remains true in much wider generality , as the next theorem shows .",
    "in the statement of the theorem , we use the notions of a point process and its predictable intensity , as developed in detail in , e.g. , ( * ? ? ? * chapter ii ) .",
    "[ th : our generalization of duncan s theorem for poisson ] let @xmath287 be a point process and @xmath231 be its @xmath288-predictable intensity , where @xmath288 is the @xmath289-field @xmath290 generated by @xmath161 .",
    "suppose that @xmath291 , and that the assumptions of proposition [ claim : dir inf in terms of its differential ] are satisfied for all @xmath84",
    ". then @xmath292 ) ] dt    = i (   x_0^t \\to y_0^t ) .\\ ] ]    paralleling the proof of theorem [ th : our generalization of duncan s theorem ] , the proof of theorem [ th : our generalization of duncan s theorem for poisson ] is a direct application of proposition [ claim : dir inf in terms of its differential ] and the following :    [ lemma : relative ent with fb for poisson ] let @xmath10 and @xmath11 be two probability laws governing @xmath50 under the setting and stipulations of theorem [ th : our generalization of duncan s theorem for poisson ]",
    ". then @xmath293 ) -   \\ell ( x_t , e_p[x_t|y_0^t ] )   dt   \\right ]   .\\ ] ]    lemma [ lemma : relative ent with fb for poisson ] is implicit in @xcite , following directly from ( * ? ? ?",
    "* theorem 4.4 ) and the discussion in ( * ? ? ?",
    "* subsection 7.5 ) . equipped with it , the proof of theorem [ th :",
    "our generalization of duncan s theorem for poisson ] follows similarly as that of theorem [ th : our generalization of duncan s theorem ] , the role of being played here by ( [ eq : main relevant result from atarweissman2011 ] ) .",
    "the poisson channel ( e.g. , @xcite ) is a channel where the input at time @xmath85 , @xmath231 , determines the intensity of the doubly stochastic poisson process @xmath287 occurring at the output of the channel .",
    "a poisson channel with feedback refers to the case where the input signal @xmath231 may depend on the previous observation of the output @xmath294 .    in this section",
    "we consider a special case of poisson channel with feedback .",
    "let @xmath295 and @xmath296 be the input and output processes of the continuous - time poisson channel with feedback , where each time an event occurs at the channel output , the channel input changes to a new value , drawn according to the distribution of a positive random variable @xmath278 , independently of the channel input and output up to that point in time .",
    "the channel input remains fixed at that value until the occurrence of the next event at the channel output , and so on . throughout this section ,",
    "the shorthand `` poisson channel with feedback '' will refer to this scenario , with its implied channel input process .",
    "the poisson channel we use here is similar to the well - known poisson channel model ( e.g. , @xcite ) with one difference that the intensity of the poisson channel changes according to the input @xmath278 only when there is an event at the output of the channel .",
    "note that the channel description given here uniquely determines the joint distribution of the channel input and output processes .    in the first part of this section , we derive , using theorem [ th : our generalization of duncan s theorem for poisson ] , a formula for the directed information rate of this poisson channel with feedback . in the second part ,",
    "we demonstrate the use of this formula by computing and plotting the directed information rate for a special case in which the intensity alphabet is of size 2 .      for jointly distributed processes @xmath297",
    "define the directed information rate @xmath298 by @xmath299 when the limit exists .",
    "[ proposition : directed info in poisson channel example ] assume that @xmath278 is finite - valued with probability mass function ( pmf ) @xmath300 .",
    "the directed information rate between the input and output processes of the poisson channel with feedback @xmath298 exists and is given by @xmath301},\\ ] ] where , in @xmath302 on the right hand side , @xmath303 , i.e. , the conditional density of @xmath279 given @xmath304 is @xmath305 .",
    "the key component in the proof of the proposition is the use of theorem  [ th : our generalization of duncan s theorem for poisson ] for directed information in continuous time as a causal mean estimation error .",
    "an intuition for the expression in ( [ eq : directed info expression for our poisson example ] ) can be obtained by considering rate per unit cost @xcite , i.e. , @xmath306 $ ] , where @xmath307 is the cost of the input . in our case ,",
    "the `` cost '' of @xmath278 is proportional to the average duration of time until the channel can be used again , i.e. , @xmath308 .",
    "finally , we remark that the assumption of discreteness of @xmath278 in proposition [ proposition : directed info in poisson channel example ] is made for simplicity of the proof , though the result carries over to more generally distributed @xmath278 .    to prove proposition  [ proposition : directed info in poisson channel example ] ,",
    "let us first collect the following observations :    [ lemma : observations collection for our prop ] let @xmath309 and @xmath303 .",
    "define @xmath310 = \\frac{\\sum_{x } x e^{-t x } p_x(x)}{\\sum_{x } e^{-t x } p_x(x ) } , \\quad t \\ge 0.\\ ] ] then the following statement holds .    1 .",
    "the marginal distribution of @xmath231 is @xmath311 and consequently @xmath312 = \\frac{e [ \\log x]}{e [ 1/x]}.\\ ] ] 2 .",
    "let @xmath313 denote the time of occurrence of the last ( most recent ) event at the channel output prior to time @xmath122 and define @xmath314 .",
    "the density of @xmath315 is @xmath316 } , \\quad t \\ge 0.\\ ] ] 3 .   for @xmath315 distributed as in , @xmath317 = \\frac{1 - h(y)}{e[1/x]}.\\ ] ]    for the first part of the lemma , note that @xmath231 is an ergodic continuous - time markov chain and thus @xmath318 is equal to the fraction of time that @xmath231 spends in state @xmath319 which is proportional to @xmath320 , accounting for , which , in turn , yields @xmath321 = \\sum_x \\frac{(1/x)p_x(x)}{\\sum_{x ' } ( 1/x')p_x(x ' ) } x \\log x = \\frac { \\sum_x p_x(x )   \\log x}{\\sum_{x ' } ( 1/x ' ) p_x(x ' ) } = \\frac{e [ \\log x]}{e [ 1/x]},\\ ] ] accounting for .    to prove the second part of the lemma ,",
    "observe that    1 .",
    "the interarrival times of the process @xmath322 are independent and identically distributed ( i.i.d . )",
    "copies of a random variable @xmath279 ; 2 .",
    "@xmath279 has a density @xmath323 3 .",
    "the probability density of the length of the interarrival interval of the @xmath322 process around @xmath122 is proportional to @xmath324 ; and 4 .   given the length of the interarrival interval around @xmath122 is @xmath325 , its left point is uniformly distributed on @xmath326 $ ] .    letting @xmath327 ( \\cdot ) $ ]",
    "denote the density of a random variable uniformly distributed on @xmath328 $ ] , it follows that the density of @xmath315 is @xmath329(t )    dy \\\\ & = \\int_t^\\infty \\frac{f_y(y ) \\cdot y}{\\int_0^\\infty f_y(y ' ) \\cdot y ' dy ' } \\frac{1}{y }    dy \\\\ & = \\label{eq : f tau step c } \\frac {   \\sum_x p_x(x ) x \\int_t^\\infty e^{-x y } dy } { \\sum_x p_x(x ) x \\int_0^\\infty e^{-x y ' } \\cdot y ' dy ' } \\\\ & = \\frac {   \\sum_x p_x(x ) x \\frac{e^{-tx}}{x } } { \\sum_x p_x(x ) x \\frac{1}{x^2 } } \\\\ & = \\frac {   \\sum_x p_x(x )   e^{-tx } } { e[1/x]},\\end{aligned}\\ ] ] where follows by combining observations ( c ) and ( d ) , and follows by substituting from .",
    "we have thus proven the second part of the lemma .    to establish the third part ,",
    "let @xmath330 denote the cumulative distribution function of @xmath279 and consider @xmath331 & =    \\int_0^\\infty   f_{\\tau } ( t ) g(t ) \\log g(t ) \\\\ & = \\label{eq : cdf of y step a } \\int_0^\\infty \\frac {   \\sum_x p_x(x )   e^{-tx } } { e[1/x ] } \\frac{\\sum_{x } x e^{-t x } p_x(x)}{\\sum_{x } e^{-t x } p_x(x ) } \\log \\frac{\\sum_{x } x e^{-t x } p_x(x)}{\\sum_{x } e^{-t x } p_x(x ) } dt   \\\\ & = \\frac{1}{e[1/x ] } \\int_0^\\infty \\sum_{x } x e^{-t x } p_x(x ) \\log \\frac{\\sum_{x } x e^{-t x } p_x(x)}{\\sum_{x } e^{-t x } p_x(x ) } dt    \\\\ & = \\label{eq : cdf of y step b } \\frac{1}{e[1/x ] } \\int_0^\\infty f_y(t ) \\log \\frac{f_y(t)}{1-f_y(t ) } dt \\\\ & = \\frac{1}{e[1/x ] } \\biggl ( \\int_0^\\infty f_y(t ) \\log \\frac{1}{1-f_y(t ) } dt   - h(y ) \\biggr ) \\\\ & =   \\frac{1}{e[1/x ] } \\biggl ( \\int_0 ^ 1   \\log \\frac{1}{1- u } du   - h(y ) \\biggr ) \\\\ & = \\frac{1}{e[1/x ] } ( 1   - h(y ) ) , \\end{aligned}\\ ] ] where follows by substituting from the second part of the lemma and follows by substituting from and noting that @xmath332 we have thus established the third and last part of the lemma .",
    "we have @xmath333 \\log e[x_t | y_0^t ] \\bigr ] dt   \\\\ & = \\label{eq : poisson step b } e \\bigl [ x_0 \\log x_0 - e[x_0 | y_{-\\infty}^0 ] \\log e[x_0 | y_{-\\infty}^0 ] \\bigr ]   \\\\ & = \\frac{e [ \\log x]}{e [ 1/x ] } -   e \\bigl [   e[x_0 | y_{-\\infty}^0 ] \\log e[x_0 | y_{-\\infty}^0 ] \\bigr ] , \\label{eq : first part eq in pf of prop}\\end{aligned}\\ ] ] where follows from the relation between directed information and causal estimation in ; follows from the stationarity and martingale convergence .",
    "specifically , by martingale convergence @xmath334 \\rightarrow e[x_0| y_{-\\infty}^0]$ ] as @xmath335 a.s .  and thus @xmath336 \\log e[x_t | y_0^t ] \\bigr]$ ] , which by stationarity is equal to @xmath337 \\log e[x_0| y_{-t}^0 ] \\bigr ] $ ] , converges to @xmath338 \\log e[x_0 | y_{-\\infty}^0 ] \\bigr ] $ ] by the bounded convergence theorem ( recall that @xmath339 is finite - valued ) ; and follows from the first part of lemma [ lemma : observations collection for our prop ] .",
    "now , recalling the definition of the function @xmath340 in we note that @xmath341 = g ( - \\ell ( y_{-\\infty}^0 ) ) .\\ ] ] thus @xmath342 \\log e[x_0 | y_{-\\infty}^0 ] \\bigr ] & = \\label{eq : e with g step a } e \\bigl [ e[x_0 | \\ell ( y_{-\\infty}^0 ) ] \\log e[x_0 | \\ell ( y_{-\\infty}^0 ) ] \\bigr ] \\\\ & = \\label{eq : e with g step b } e \\bigl [ g ( - \\ell ( y_{-\\infty}^0 ) ) \\log g ( - \\ell ( y_{-\\infty}^0 ) ) \\bigr ]   \\\\ & = e [ g ( \\tau ) \\log g ( \\tau ) ] \\\\ & = \\frac{1 - h(y)}{e[1/x ] } , \\label{eq : 2nd part eq in prop}\\end{aligned}\\ ] ] where follows from the markov relation @xmath343 , follows from , and from the last part of lemma [ lemma : observations collection for our prop ] . thus @xmath344}{e [ 1/x ] } \\\\ & = \\label{eq : poisson proof last step b } \\frac{h(y ) - h(y|x)}{e [ 1/x ] } \\\\ & = \\frac{i(x;y)}{e [ 1/x ] } , \\end{aligned}\\ ] ] where follows by combining with , and follows by noting that @xmath345.\\ ] ] this completes the proof of proposition  [ proposition : directed info in poisson channel example ] .      fig .",
    "[ f_poisson1 ] depicts the directed information rate @xmath346 for the case where @xmath278 takes only two values @xmath347 and @xmath348 .",
    "we have used numerical evaluation of @xmath302 in the right hand side of to compute the directed information rate .",
    "the figure shows the influence of @xmath349 on the directed information rate where @xmath350 and @xmath351 .",
    "as expected , the maximum is achieved when there is higher probability that the encoder output will be the higher rate @xmath348 , which would imply more channel uses per unit time , but not much higher as otherwise the input value will be close to deterministic .",
    "[ ] [ ] [ 1 ] [ ] [ ] [ 1 ] [ ] [ ] [ 1 ] [ ] [ ] [ 1 ] [ ] [ ] [ 1][][][1 ] [ ] [ ] [ 1]@xmath352 [ ] [ ] [ 1 ] [ ] [ ] [ 1 ] [ ] [ ] [ 1]@xmath353 [ ] [ ] [ 1 ]    , the pmf of the input to the channel .",
    "the input to the channel is one of two possible values @xmath350 and @xmath351 , and it is the intensity of the poisson process at the output of the channel until the next event .",
    ", width=264 ]    fig .",
    "[ f_poisson_lambda2 ] depicts the maximal value ( optimized w.r.t .",
    "@xmath354 ) of the directed information rate when @xmath347 is fixed and is equal to 1 and @xmath348 varies .",
    "this value is the capacity of the poisson channel with feedback , when the inputs are restricted to one of the two values @xmath347 or @xmath348 .    [ ] [ ] [ 1]@xmath348 [ ] [ ] [ 1]@xmath355 [ ] [ ] [ 1 ]    , when @xmath347 is fixed and is equal to 1 and @xmath348 varies.,width=264 ]    when @xmath356 the capacity is obviously zero since any use of @xmath357 as input will cause the channel not to change any further .",
    "it is also obviously zero at @xmath358 since in this case @xmath359 , so there is only one possible input to the channel . as @xmath348 increases , the capacity of the channel increases without bound since , for @xmath360 , the channel effectively operates as a noise - free binary channel , where one symbol `` costs '' an average duration of @xmath361 while the other a vanishing average duration .",
    "thus the limiting capacity with increasing @xmath348 is equal to @xmath362 .",
    "one can consider a discrete - time memoryless channel , where the input @xmath278 is discrete ( @xmath347 or @xmath348 ) and the output @xmath279 is distributed according to @xmath363 .",
    "consider now a random cost @xmath364 , where @xmath279 is the output of the channel .",
    "using the result from @xcite we obtain that the capacity per unit cost of the discreet memoryless channel is @xmath365 } = \\max_{p(x)}\\frac{i(x;y)}{e[1/x]},\\ ] ] where the equality follows since @xmath366=e[e[y|x]]=e[1/x]$ ] . finally , we note that the capacity of the poisson channel in the example above is the capacity per unit cost of the discrete memoryless channel .",
    "thus , by proposition [ proposition : directed info in poisson channel example ] we can conclude that the continuous - time directed information rate characterizes the capacity of the poisson channel with feedback . in the next section",
    "we will see that the continuous - time directed information rate characterizes the capacity of a large family of continuous - time channels .",
    "we first review the definition of a block - ergodic process as given by berger @xcite .",
    "let @xmath367 denote a continuous - time process @xmath368 drawn from a space @xmath369 according to the probability measure @xmath370 .",
    "for @xmath371 , let @xmath372 be a @xmath85-shift transformation , i.e. , @xmath373 .",
    "a measurable set @xmath374 is _",
    "@xmath85-invariant _ if it does not change under the @xmath85-shift transformation , i.e. , @xmath375 .",
    "a continuous - time process @xmath376 is _",
    "@xmath315-ergodic _ if every measurable @xmath315-invariant set of processes has either probability 1 or 0 , i.e. , for any @xmath315-invariant set @xmath377 , in other words , @xmath378 .",
    "the definition of @xmath315-ergodicity means that if we take the process @xmath379 and slice it into time - blocks of length @xmath315 , then the new discrete - time process @xmath380 is ergodic . a continuous - time process @xmath376 is _ block - ergodic _ if it is @xmath315-ergodic for every @xmath381 .",
    "berger @xcite showed that weak mixing ( therefore also strong mixing ) implies block ergodicity .",
    "[ ] [ ] [ 1 ] [ ] [ ] [ 1]message [ ] [ ] [ 1 ] message estimate [ ] [ ] [ 1]@xmath382 [ ] [ ] [ 1]@xmath231 [ ] [ ] [ 1]delay @xmath383 [ ] [ ] [ 1]@xmath384 [ ] [ ] [ 1]@xmath287 [ ] [ ] [ 1]@xmath385 [ ] [ ] [ 1]@xmath386 [ ] [ ] [ 1]encoder [ ] [ ] [ 1]@xmath387[][][1]channel [ ] [ ] [ 1]@xmath388[][][1]decoder     and channel of the form @xmath389 where @xmath390 is a block ergodic process.,width=415 ]    now let us describe the communication model of our interest ( see fig . [ f_channel_con ] ) and show that the continuous - time directed information characterizes the capacity . consider a continuous - time channel that is specified by    * the channel input and output alphabets @xmath369 and @xmath391 , respectively , that are not necessarily finite , and * the channel output at time @xmath85 @xmath392 corresponding to the channel input @xmath231 at time @xmath85 , where @xmath393 is a stationary ergodic noise process on an alphabet @xmath394 and @xmath395 is a given measurable function .",
    "a @xmath396 code with delay @xmath397 for the channel consists of    * a message set @xmath398 , * an encoder that assigns a symbol @xmath399 to each message @xmath400 and past received output signal @xmath401 for @xmath113 , where @xmath402 is measurable , and * a decoder that assigns a message estimate @xmath403 to each received output signal @xmath404 , where @xmath405 is measurable .",
    "we assume that the message @xmath257 is uniformly distributed on @xmath406 and independent of the noise process @xmath407 .    by the definition of the channel in , the definition of the encoding function in ( [ e_encoding ] ) , and the independence of @xmath257 and @xmath407 , it follows that for any @xmath78 and any @xmath408 , @xmath409 form a markov chain .",
    "this is analogous to the assumption in the discrete case that @xmath410 ; the analogy is exact when we convert a discrete time channel to a continuous time channel with constant piecewise process between the time samples .",
    "furthermore , for any @xmath408 , @xmath78 , and @xmath411 , @xmath412 form a markov chian .",
    "this is analogous to the assumption in the discrete case that whenever there is feedback of delay @xmath413 , @xmath414 .",
    "similar communication settings with feedback in continuous time were studied by kadota , zakai , and ziv @xcite for continuous - time memoryless channels , where it is shown that feedback does not increase the capacity , and by ihara  @xcite for the gaussian case .",
    "our main result in this section is showing that the operational capacity , defined below , can be characterized by the information capacity , which is the maximum of directed information from the channel input process to the output process .",
    "next we define an achievable rate , the operational feedback capacity , and the information feedback capacity for our setting .",
    "a rate @xmath415 is said to be _ achievable with feedback delay @xmath383 _ if for each @xmath416 there exists a family of @xmath417 codes such that @xmath418    let @xmath419 be the _ ( operational ) feedback capacity _ with delay @xmath383 , and let the _ ( operational ) feedback capacity _",
    "be @xmath420    from the monotonicity of @xmath421 in @xmath383 we have @xmath422 .",
    "this definition coincides with the feedback capacity definition of continuous time channels given in @xcite , where there also was assumed a positive but arbitrary small delay in the feedback capacity .",
    "let @xmath423 be the information feedback capacity defined as @xmath424 where the supremum in is over @xmath425 , which is the set of all channel input processes of the form @xmath426 some family of measurable functions @xmath427 , and some process @xmath428 which is independent of the channel noise process @xmath429 ( appearing in ) and has a finite cardinality that may depend on @xmath416 .",
    "the limit in is shown to exist in lemma [ l_superadditivty ] using the superadditivity property .",
    "we now characterize @xmath421 in terms of @xmath423 for the class of channels defined in .",
    "[ t_capacity ] for the channel defined in , @xmath430    since @xmath423 is a decreasing function in @xmath383 , may be written as @xmath431 , and the limit exists because of the monotonicity . since the function is monotonic then @xmath432 with a possible exception of the points of @xmath383 of a set of measure zero @xcite .",
    "therefore @xmath433 for any @xmath434 except of a set of points of measure zero .",
    "furthermore and imply that @xmath435 , hence we also have @xmath436 .    before proving the theorem",
    "we show that the limits in exist .",
    "[ l_superadditivty ] the term @xmath437 is superadditive , namely , @xmath438 and therefore the limit in exists and is equal to @xmath439    to prove lemma [ l_superadditivty ] we use the following result :    [ l_di_inequality ] let @xmath440 be a pair of discrete - time processes such that markov relation @xmath441 holds for @xmath442 .",
    "then @xmath443    the result is a consequence of the identity ( * ? ? ? * eq .",
    "( 11))@xmath444    consider @xmath445 where follows from the identity given in , and follows from the markov chain assumption in the lemma .",
    "first note that we do not increase the term @xmath446 by restricting the time - partition @xmath55 to have an interval starting at point @xmath447 .",
    "now fix three time - partitions : @xmath448 in @xmath449 , @xmath450 in @xmath451 , and @xmath55 in @xmath452 such that @xmath55 is a concatenation @xmath448 and @xmath450 . for @xmath453 and @xmath454 , fix the input functions of the form of and fix the arguments @xmath455 and @xmath456 which corresponds to @xmath453 and @xmath454 , respectively .",
    "the construction is such that the random processes @xmath455 and @xmath456 are independent of each other .",
    "let @xmath457 be a concatenation of @xmath453 and @xmath454 . applying lemma [ l_di_inequality ] on the discrete - time process @xmath440 , where @xmath458 for @xmath459 we obtain that for any fixed @xmath448 , @xmath450 , @xmath453 , @xmath454 , @xmath455 , and @xmath456 as described above , we have @xmath460 note that the markov condition @xmath461 indeed holds because of the construction of @xmath457 . furthermore , because of the stationarity of the noise implies . finally , using fekete s lemma ( * ? ? ?",
    "2.6 ) and the superadditivity in implies the existence of the limit in .",
    "the proof of theorem  [ t_capacity ] consists of two parts : the proof of the converse , i.e. , , and the proof of achievability , i.e. , .",
    "fix an encoding scheme @xmath462 with rate @xmath415 and probability of decoding error , @xmath463 .",
    "in addition , fix a partition @xmath55 of length @xmath1 such that @xmath464 for any @xmath465 $ ] and let @xmath466 .",
    "consider @xmath467where the equality in   follows since the message is distributed uniformly , the inequality in   follows from fano s inequality , where @xmath468 , the equality in   follows from the fact that @xmath469 is a deterministic function of @xmath257 and @xmath239 , the equality in   follows from the assumption that @xmath464 , the equality in   follows from , and the equality in   follows from .",
    "hence , we obtained that for every @xmath55 @xmath470 since the number of codewords is finite , we may consider the input signal of the form @xmath471 with @xmath472 , where the cardinality of @xmath473 is bounded , i.e. , @xmath474 for any given @xmath416 ( the bound may depend on @xmath416 ) , independently of the partition @xmath55 .",
    "furthermore , @xmath475 finally , for any @xmath415 that is achievable there exists a sequence of codes such that @xmath476 , hence @xmath477 and we have established .",
    "note that as a byproduct of the sequence of equalities  , we conclude that for the communication system depicted in fig .",
    "[ f_channel_con ] , @xmath478 the only assumptions that we used to prove  is that the encoders uses a strictly causal feedback of the form given in and that the channel satisfies the benign assumption given in .",
    "this might be a valuable result by itself that provides a good intuition why directed information characterizes the capacity of a continuous - time channel .",
    "furthermore , the interpretations of the measure @xmath479 , for instance , as given in @xcite , should also hold for directed information and vice versa .    for the proof of achievability we will use the following result for discrete - time channels .",
    "[ l_ach ] consider the discrete - time channel , where the input @xmath480 at time @xmath240 has a finite alphabet , i.e. , @xmath481 , and the output @xmath482 at time @xmath240 has an arbitrary alphabet @xmath483 .",
    "we assume that the relation between the input and the output is given by @xmath484 where the noise process @xmath485 is stationary and ergodic with an arbitrary alphabet @xmath486 .",
    "then , any rate @xmath415 is achievable for this channel if @xmath487 where the joint distribution of @xmath488 is induced by the input distribution @xmath489 , the stationary distribution of @xmath490 , and .",
    "fix the pmf @xmath489 that attains the maximum in .",
    "since @xmath491 can be approximated arbitrarily close by a finite partition of @xmath279 @xcite , assume without loss of generality that @xmath391 is finite .",
    "the proof uses the random codebook generation and joint typicality decoding in ( * ? ? ?",
    "* ch .  3 ) .",
    "randomly and independently generate @xmath492 codewords @xmath493 , @xmath494 , each according to @xmath495 .",
    "the decoder finds the unique @xmath496 such that @xmath497 is jointly typical .",
    "( for the definition and properties of joint typicality , refer to @xcite , ( * ? ? ?",
    "* ch .  2 ) .",
    ") now , assuming that @xmath498 is sent , the decoder makes an error only if @xmath499 is not typical or @xmath500 is typical for some @xmath501 . by the packing lemma ( ( * ? ? ?",
    "3 ) ) , the probability of the second event tends to zero as @xmath502 if @xmath503 . to bound the probability of the first event , recall from ( *",
    "10.3.1 ) that if @xmath504 is i.i.d .",
    "and @xmath505 is stationary ergodic , independent of @xmath504 , then the pair @xmath506 is jointly stationary ergodic .",
    "consequently , from the definition of the channel in , @xmath507 is jointly stationary ergodic .",
    "thus , by birkhoff s ergodic theorem , the probability that @xmath508 is not typical tends to zero as @xmath502 .",
    "therefore , any rate @xmath503 is achievable .",
    "the proof of achievability is based on the lemma above and the definition of directed information for continuous time .",
    "it is essential to divide into small time - interval as well as increasing the feedback delay by a small but positive value @xmath509 .",
    "let @xmath510 , where @xmath509 .",
    "in addition , let @xmath511 be such that @xmath512 for all @xmath513 .",
    "let @xmath514 be of the form @xmath515 where the cardinality of @xmath428 is bounded .",
    "then we show that any rate @xmath516 is achievable .",
    "assume that the communication is over the time interval @xmath517 $ ] , where @xmath416 is fixed and @xmath1 may be chosen to be as large as needed .",
    "partition the time interval @xmath517 $ ] into @xmath1 subintervals of length @xmath416 and in each subinterval @xmath518 , which we index by @xmath519 , fix the relation @xmath520 note that this coding scheme is possible with feedback delay @xmath383 since @xmath521 .",
    "this follows from the assumption that @xmath512 and @xmath522 .",
    "now , let us define a discrete - time channel where the input at time @xmath523 is @xmath524 ( which has an alphabet @xmath525 $ ] ) , the output at time @xmath523 is the vector @xmath526 and the noise at time @xmath523 is @xmath527 . note that since @xmath528 is a stationary and block - ergodic the noise process @xmath529 is stationary and ergodic . furthermore the relation @xmath530 holds and the alphabet of @xmath531 is finite .",
    "hence by lemma [ l_ach ] , any rate @xmath532 is achievable . now using the definition of the discrete - time channel and the properties of directed information",
    ", we obtain @xmath533 where the equality in follows from the definition of the discrete - time channel and the equality in follows from the same sequence of equalities as in . since holds for any @xmath55 such that @xmath512 we conclude that @xmath534 finally , by the definition of directed information and by the fact that holds for any @xmath416 we have established .",
    "we have introduced and developed a notion of directed information between continuous - time stochastic processes .",
    "it emerges naturally in the characterization of the fundamental limit on reliable communication for a wide class of continuous - time channels with feedback , quite analogously to the discrete - time setting .",
    "it also arises in estimation theoretic relations as the replacement for mutual information when extending the scope to the presence of feedback .",
    "in particular , with continuous - time directed information replacing mutual information , duncan s theorem generalizes to estimation problems in which the evolution of the target signal is affected by the past channel noise . an analogous relationship based on the directed information holds for the poisson channel .",
    "we have illustrated the use of the latter in an explicit computation of the directed information rate between the input and output of a poisson channel where the input intensity changes only when there is an event at the channel output .",
    "one important direction for future exploration is to use the `` multiletter '' characterization of capacity developed here to compute or approximate the feedback capacity of interesting continuous - time channels .",
    "the authors thank the associate editor and the anonymous reviewers for their careful reading of the original manuscript and many valuable comments that helped improve the presentation .",
    "r.  dabora and a.  j. goldsmith , `` capacity theorems for discrete , finite - state broadcast channels with feedback and unidirectional receiver cooperation , '' _ ieee trans .",
    "_ , vol .",
    "56 , pp . 59585983 , december 2010 .",
    "h.  h. permuter , y.  h. kim , and t.  weissman , `` interpretations of directed information in portfolio theory , data compression , and hypothesis testing , '' _ ieee trans .",
    "inf . theory _",
    "57 , no .  6 , pp .",
    "32483259 , 2011 ."
  ],
  "abstract_text": [
    "<S> a notion of directed information between two continuous - time processes is proposed . </S>",
    "<S> a key component in the definition is taking an infimum over all possible partitions of the time interval , which plays a role no less significant than the supremum over `` space '' partitions inherent in the definition of mutual information . </S>",
    "<S> properties and operational interpretations in estimation and communication are then established for the proposed notion of directed information . </S>",
    "<S> for the continuous - time additive white gaussian noise channel , it is shown that duncan s classical relationship between causal estimation error and mutual information continues to hold in the presence of feedback upon replacing mutual information by directed information . </S>",
    "<S> a parallel result is established for the poisson channel . </S>",
    "<S> the utility of this relationship is demonstrated in computing the directed information rate between the input and output processes of a continuous - time poisson channel with feedback , where the channel input process is constrained to be constant between events at the channel output . </S>",
    "<S> finally , the capacity of a wide class of continuous - time channels with feedback is established via directed information , characterizing the fundamental limit on reliable communication .    </S>",
    "<S> causal estimation , conditional mutual information , continuous time , directed information , duncan s theorem , feedback capacity , gaussian channel , poisson channel , time partition . </S>"
  ]
}