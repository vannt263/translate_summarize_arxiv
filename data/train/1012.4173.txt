{
  "article_text": [
    "in order to fix ideas , it is useful to give an explicit `` neural network '' interpretation to the theory that will be developed .",
    "the model will consist of 2 layers of nodes .",
    "the input layer has a `` pattern of activity '' that represents the components of the input vector @xmath0 , and the output layer has a pattern of activity that is the collection of activities of each output node .",
    "the activities in the output layer depend only on the activities in the input layer .",
    "if an input vector @xmath0 is presented to this network , then each output node `` fires '' discretely at a rate that corresponds to its activity .",
    "after @xmath1 nodes have fired the probabilistic description of the relationship between the input and output of the network is given by @xmath2 , where @xmath3 is the location in the output layer ( assumed to be on a rectangular lattice of size @xmath4 )  of the @xmath5 node that fires .",
    "in this paper it will be assumed that the order in which the @xmath1 nodes fire is not observed , in which case @xmath2 is a sum of probabilities over all @xmath6 permutations of @xmath7 , which is a symmetric function of the @xmath3 , by construction .",
    "the theory that is introduced in section [ subsect : probencodedecode ] concerns the special case @xmath8 . in the @xmath8 case",
    "the probabilistic description @xmath9 is proportional to the firing rate of node @xmath10 in response to input @xmath0 .",
    "when @xmath11 there is an indirect relationship between the probabilistic description @xmath2 and the firing rate of node @xmath10 , which is given by the marginal probability @xmath12 it is important to maintain this distinction between events that are observed ( i.e. @xmath7 given @xmath0 ) and the probabilistic description of the events that are observed ( i.e. @xmath2 ) .",
    "the only possible exception is in the @xmath13 limit , where @xmath2 has all of its probability concentrated in the vicinity of those @xmath7 that are consistent with the observed long - term average firing rate of each node .",
    "it is essential to consider the @xmath11 case to obtain the results that are described in this paper .",
    "a theory of self - organising networks based on an analysis of a probabilistic encoder / decoder was presented in @xcite .",
    "it deals with the @xmath8 case referred to in section [ subsect : nninterp ] .",
    "the objective function that needs to be minimised in order to optimise a network in this theory is the euclidean distortion @xmath14 defined as @xmath15 where @xmath0 is an input vector , @xmath10 is a coded version of @xmath0 ( a vector index on a @xmath16-dimensional rectangular lattice of size @xmath4 ) , @xmath17 is a reconstructed version of @xmath0 from @xmath10 , @xmath18 is the probability density of input vectors , @xmath19 is a probabilistic encoder , and @xmath20 is a probabilistic decoder which is specified by bayes theorem as @xmath21 @xmath14 can be rearranged into the form @xcite @xmath22 where the reference vectors @xmath23 are defined as @xmath24 although equation [ eq : objective ] is symmetric with respect to interchanging the encoder and decoder , equation [ eq : objectivesimplified ] is not .",
    "this is because bayes theorem has made explicit the dependence of @xmath25 on @xmath19 . from a neural network viewpoint @xmath19",
    "describes the feed - forward transformation from the input layer to the output layer , and @xmath23 describes the feed - back transformation that is implied from the output layer to the input layer .",
    "the feed - back transformation is necessary to implement the objective function that has been chosen here .",
    "minimisation of @xmath14 with respect to all free parameters leads to an optimal encoder / decoder . in equation",
    "[ eq : objectivesimplified ] the @xmath19 are the only free parameters , because @xmath23 is fixed by equation [ eq : refvect ] .",
    "however , in practice , both @xmath19 and @xmath23 may be treated as free parameters @xcite , because @xmath23 satisfy equation [ eq : refvect ] at stationary points of @xmath14 with respect to variation of @xmath23 .",
    "the probabilistic encoder / decoder requires an explicit functional form for the posterior probability @xmath19 .",
    "a convenient expression is @xmath26 where @xmath27 can be regarded as a node `` activity '' , and @xmath28 .",
    "any non - negative function can be used for @xmath29 , such as a sigmoid ( which satisfies @xmath30 ) @xmath31 where @xmath32 and @xmath33 are a weight vector and bias , respectively .",
    "a drawback to the use of equation [ eq : postprob ] is that it does not permit it to scale well to input vectors that have a large dimensionality .",
    "this problem arises from the restricted functional form allowed for @xmath29 .",
    "a solution was presented in @xcite @xmath34 where @xmath35 , and @xmath36 is a set of lattice points that are deemed to be `` in the neighbourhood of '' the lattice point @xmath10 , and @xmath37 is the inverse neighbourhood defined as the set of lattice points that have lattice point @xmath10 in their neighbourhood .",
    "this expression for @xmath19 satisfies @xmath38 ( see appendix [ app : postprobpmdnorm ] ) .",
    "it is convenient to define @xmath39 which is another posterior probability , by construction .",
    "it includes the effect of the output nodes that are in the neighbourhood of node @xmath40 only .",
    "@xmath41 is thus a localised posterior probability derived from a localised subset of the node activities .",
    "this allows equation [ eq : postprobpmd ] to be written as @xmath42 , so @xmath19 is the average of the posterior probabilities at node @xmath10 arising from each of the localised subsets that happens to include node @xmath10 .",
    "the model may be extended to the case where @xmath1 output nodes fire .",
    "@xmath19 is then replaced by @xmath43 , which is the probability that @xmath44 are the first @xmath1 nodes to fire ( in that order ) . with this modification",
    ", @xmath14 becomes @xmath45 where the reference vectors @xmath46 are defined as @xmath47 the dependence of @xmath43 and @xmath46 on @xmath1 output node locations complicates this result .",
    "assume that @xmath43 is a symmetric function of its @xmath44 arguments , which corresponds to ignoring the order in which the first @xmath1 nodes choose to fire ( i.e. @xmath43 is a sum over all permutations of @xmath44 ) .",
    "for simplicity , assume that the nodes fire independently so that @xmath48 ( see appendix [ app : upperbound ] for the general case where @xmath49 does not factorise ) .",
    "@xmath14 may be shown to satisfy the inequality @xmath50 ( see appendix [ app : upperbound ] ) , where @xmath51 @xmath52 and @xmath53 are both non - negative .",
    "@xmath54 as @xmath13 , and @xmath55 when @xmath56 , so the @xmath52 term is the sole contribution to the upper bound when @xmath56 , and the @xmath53 term provides the dominant contribution as @xmath13 .",
    "the difference between the @xmath52 and the @xmath53 terms is the location of the @xmath57 average : in the @xmath53 term it averages a vector quantity , whereas in the @xmath52 term it averages a euclidean distance .",
    "the @xmath53 term will therefore exhibit interference effects , whereas the @xmath52 term will not .",
    "the model may be further extended to the case where the probability that a node fires is a weighted average of the underlying probabilities that the nodes in its vicinity fire .",
    "thus @xmath19 becomes @xmath58 where @xmath59 is the conditional probability that node @xmath10 fires given that node @xmath40 would have liked to fire . in a sense , @xmath59 describes a `` leakage '' of probability from node @xmath40 that onto node @xmath10 .",
    "@xmath59 then plays the role of a soft `` neighbourhood function '' for node @xmath40 .",
    "this expression for @xmath19 can be used wherever a plain @xmath19 has been used before .",
    "the main purpose of introducing leakage is to encourage neighbouring nodes to perform a similar function .",
    "this occurs because the effect of leakage is to soften the posterior probability @xmath19 , and thus reduce the ability to reconstruct @xmath0 accurately from knowledge of @xmath10 , which thus increases the average euclidean distortion @xmath14 . to reduce the damage that leakage causes",
    ", the optimisation must ensure that nodes that leak probability onto each other have similar properties , so that it does not matter much that they leak .",
    "the focus of this paper is on minimisation of the upper bound @xmath60 ( see equation [ eq : upperboundpieces ] )  to @xmath14 in the multiple firing model , using a scalable posterior probability @xmath19 ( see equation [ eq : postprobpmd ] ) , with the effect of activity leakage @xmath59 taken into account ( see equation [ eq : leakage ] ) .",
    "gathering all of these pieces together yields @xmath61 where @xmath62 .    in order to ensure that the model is truly scalable",
    ", it is necessary to restrict the dimensionality of the reference vectors . in equation [ eq : objectivemodel ] @xmath63 , which is not acceptable in a scalable network . in practice",
    ", it will be assumed any properties of node @xmath10 that are vectors in input space will be limited to occupy an `` input window '' of restricted size that is centred on node @xmath10 .",
    "this restriction applies to the node reference vector @xmath23 , which prevents @xmath60 from being fully minimised , because @xmath23 is allowed to move only in a subspace of the full - dimensional input space .",
    "however , useful results can nevertheless be obtained , so this restriction is acceptable .",
    "optimisation is achieved by minimising @xmath60 with respect to its free parameters .",
    "thus the derivatives with respect to @xmath23 are given by @xmath64 and the variations with respect to @xmath29 are given by @xmath65 the functions @xmath66 , @xmath67 , @xmath68 , and @xmath69 are derived in appendix [ app : derivatives ] . inserting a sigmoidal function @xmath70 then yields the derivatives with respect to @xmath32 and @xmath33 as @xmath71 because all of the properties of node @xmath10 that are vectors in input space ( i.e. @xmath23 and @xmath32 )  are assumed to be restricted to an input window centred on node @xmath10 , the eventual result of evaluating the right hand sides of the above equations must be similarly restricted to the same input window .",
    "the expressions for @xmath52 and @xmath53 , and especially their derivatives , are fairly complicated , so an intuitive interpretation will now be presented .",
    "when @xmath60 is stationary with respect to variations of @xmath23 it may be written as ( see appendix [ app : d1d2refvect ] ) . @xmath72",
    "the @xmath73 and @xmath74 factors do not appear in this expression because @xmath19 is normalised to sum to unity . the first term ( which derives from @xmath52 )",
    "is an incoherent sum ( i.e. a sum of euclidean distances ) , whereas the second term ( which derives from @xmath53 ) is a coherent sum ( i.e. a sum of vectors ) .",
    "the first term contributes for all values of @xmath1 , whereas the second term contributes only for @xmath75 , and dominates for @xmath76 . in order to minimise the first term the @xmath77 like to be as large as possible for those nodes that have a large @xmath19 .",
    "since @xmath23 is the centroid of the probability density @xmath78 , this implies that node @xmath10 prefers to encode a region of input space that is as far as possible from the origin .",
    "this is a consequence of using a euclidean distortion measure @xmath79 , which has the dimensions of @xmath80 , in the original definition of the distortion in equation [ eq : objective ] . in order to minimise the second term the superposition of @xmath23 weighted by",
    "the @xmath19  likes to have as large a euclidean norm as possible .",
    "thus the nodes co - operate amongst themselves to ensure that the nodes that have a large @xmath19 also have a large @xmath81 .",
    "the purpose of this section is to work through a case study in order to demonstrate the various properties that emerge when @xmath60 is minimised .",
    "it convenient to begin by ignoring the effects of leakage @xmath82 , and to concentrate on a simple ( non - scaling ) version of the posterior probability model ( as in equation [ eq : postprob ] ) @xmath83 , where the @xmath84 are threshold functions of @xmath0@xmath85 it is also convenient to imagine that a hypothetical infinite - sized training set is available , so it may be described by a probability density @xmath18 .",
    "this is a `` frequentist '' , rather than a `` bayesian '' , use of the @xmath18 notation , but the distinction is not important in the context of this paper .",
    "assume that @xmath86 is drawn from a training set , that has 2 statistically independent subspaces , so that @xmath87 furthermore , assume that @xmath88 and @xmath89 each have the form @xmath90 i.e. @xmath91 is a loop ( parameterised by a phase angle @xmath92 )  of probability density that sits in @xmath93-space . in order to make it easy to deduce the optimum reference vectors ,",
    "choose @xmath94 so that the following 2 conditions are satisfied for @xmath95@xmath96 this type of training set can be visualised topologically .",
    "each training vector @xmath97 consists of 2 subvectors , each of which is parameterised by a phase angle , and which therefore lives in a subspace that has the topology of a circle , which is denoted as @xmath98 . because of the independence assumption in equation [ eq : independentpdf ] , the pair @xmath97 lives on the surface of a 2-torus , which is denoted as @xmath99 .",
    "the minimisation of @xmath60 thus reduces to finding the optimum way of designing an encoder / decoder for input vectors that live on a 2-torus , with the proviso that their probability density is uniform ( this follows from equation [ eq : parametricpdf ] and equation [ eq : parametricpdfconstraint ] ) . in order to derive the reference vectors",
    "@xmath23 , the solution(s ) of the stationarity condition @xmath100 must be computed .",
    "the stationarity condition reduces to ( see appendix [ app : d1d2refvect ] ) @xmath101    it is useful to use the simple diagrammatic notation shown in figure [ fig : s1s1 ] .",
    "[ fig : s1s1 ] topology with a threshold @xmath102 superimposed.,title=\"fig:\",width=377 ]    each circle in figure [ fig : s1s1 ] represents one of the @xmath98 subspaces , so the two circles together represent the product @xmath99 .",
    "the constraints in equation [ eq : parametricpdfconstraint ] are represented by each circle being centred on the origin of its subspace ( @xmath103 is constant ) , and the probability density around each circle being constant ( @xmath104 is constant ) .",
    "a single threshold function @xmath84 is represented by a chord cutting through each circle ( with 0 and 1 indicating on which side of the chord the threshold is triggered ) .",
    "the @xmath93 that lie above threshold in each subspace are highlighted . both @xmath105 and",
    "@xmath106 must lie above threshold in order to ensure @xmath107 , i.e. they must both lie within regions that are highlighted in figure [ fig : s1s1 ] . in this case",
    "node @xmath10 will be said to be `` attached '' to both subspace 1 and subspace 2 .",
    "a special case arises when the chord in one of the subspaces ( say it is @xmath106)does not intersect the circle at all , and the circle lies on the side of the chord where the threshold is triggered . in this case",
    "@xmath84 does not depend on @xmath106 , so that @xmath108 , in which case node @xmath10 will be said to be `` attached '' to subspace 1 but `` detached '' from subspace 2 .",
    "the typical ways in which a node becomes attached to the 2-torus are shown in figure [ fig : torus ] .",
    "[ fig : torus ] topology as a torus with the effect of 3 different types of threshold @xmath102 shown.,title=\"fig:\",width=377 ]    in figure [ fig : torus](a ) the node is attached to one of the @xmath98 subspaces and detached from the other . in figure",
    "[ fig : torus](b ) the attached and detached subspaces are interchanged with respect to figure [ fig : torus](a ) . in figure",
    "[ fig : torus](c ) the node is attached to both subspaces .",
    "consider the configuration of threshold functions shown in figure [ fig : attachone ] .",
    "this is equivalent to all of the nodes being attached to loops to cover the 2-torus , with a typical node being as shown in figure [ fig : torus](a ) ( or , equivalently , figure [ fig : torus](b ) ) .",
    "[ fig : attachone ]    when @xmath60 is minimised , it is assumed that the 4 nodes are symmetrically disposed in subspace 1 , as shown .",
    "each is triggered if and only if @xmath105 lies within its quadrant , and one such quadrant is highlighted in figure [ fig : attachone ] .",
    "this implies that only 1 node is triggered at a time .",
    "the assumed form of the threshold functions implies @xmath109 , so equation [ eq : stationaryrefvect ] reduces to @xmath110 whence @xmath111      consider the configuration of threshold functions shown in figure [ fig : attachboth ] .",
    "this is equivalent to all of the nodes being attached to patches to cover the 2-torus , with a typical node being as shown in figure [ fig : torus](c ) .",
    "[ fig : attachboth ]    in this case , when @xmath60 is minimised , it is assumed that each subspace is split into 2 halves .",
    "this requires a total of 4 nodes , each of which is triggered if , and only if , both @xmath105 and @xmath106 lie on the corresponding half - circles .",
    "this implies that only 1 node is triggered at a time .",
    "the assumed form of the threshold functions implies that the stationarity condition becomes @xmath112 whence @xmath113      consider the configuration of threshold functions shown in figure [ fig : attacheither ] .",
    "this is equivalent to half of the nodes being attached to loops to cover the 2-torus , with a typical node being as shown in figure [ fig : torus](a ) .",
    "the other half of the nodes would then be attached in an analogous way , but as shown in figure [ fig : torus](b ) .",
    "thus the 2-torus is covered twice over .",
    "[ fig : attacheither ]    in this case , when @xmath60 is minimised , it is assumed that each subspace is split into 2 halves .",
    "this requires a total of 4 nodes , each of which is triggered if @xmath105 ( or @xmath106 ) lies on the half - circle in the subspace to which the node is attached .",
    "thus exactly 2 nodes @xmath114 and @xmath115 are triggered at a time , so that @xmath116 for simplicity , assume that node @xmath117 is attached to subspace 1 , then @xmath118 and the stationarity condition becomes @xmath119 this may be simplified to yield @xmath120 write the 2 subspaces separately ( remember that node @xmath117 is assumed to be attached to subspace 1 ) @xmath121 if this result is simultaneously solved with the analogous result for node @xmath117 attached to subspace 2 , then the @xmath122 terms vanish to yield @xmath123      consider the left hand side of figure [ fig : attachone ] for the case of @xmath73 nodes , when the @xmath73 threshold functions form a regular @xmath73-ogon .",
    "@xmath124 then denotes the part of the circle that is associated with node @xmath117 , whose radius of gyration squared is given by ( assuming that the circle has unit radius ) @xmath125 gather the results for @xmath126 in equations [ eq : refvectone ] ( referred to as type 1 ) , [ eq : refvectboth ] ( referred to as type 2 ) , and [ eq : refvecteither ] ( referred to as type 3 ) together and insert them into @xmath60 in equation [ eq : d1d2refvect ] to obtain ( see appendix [ app : d1d2compare ] )    @xmath127    in figure [ fig : plotn1 ] the 3 solutions are plotted for the case @xmath8 .",
    "[ fig : plotn1 ] for @xmath8 for each of the 3 types of optimum.,title=\"fig:\",width=377 ]    for @xmath8 the type 3 solution is never optimal , the type 1 solution is optimal for @xmath128 , and the type 2 solution is optimal for @xmath129 this behaviour is intuitively sensible , because a larger number of nodes is required to cover a 2-torus as shown in figure [ fig : torus](c ) than as shown in figure [ fig : torus](a ) ( or figure [ fig : torus](b ) ) .    in figure [",
    "fig : plotn2 ] the 3 solutions are plotted for the case @xmath130 .    [",
    "fig : plotn2 ] for @xmath130 for each of the 3 types of optimum.,title=\"fig:\",width=377 ]    for @xmath130 the type 1 solution is optimal for @xmath131 , and the type 2 solution is optimal for large @xmath132 , but there is now an intermediate region @xmath133 ( type 1 and type 3 have an equal @xmath60 at @xmath134 ) where the @xmath1-dependence of the type 3 solution has now made it optimal .",
    "again , this behaviour is intuitively reasonable , because the type 3 solution requires at least 2 observations in order to be able to yield a small euclidean resonstruction error in each of the 2 subspaces , i.e. for @xmath130 the 2 nodes that fire must be attached to different subspaces .",
    "note that in the type 3 solution the nodes that fire are not guaranteed to be attached to different subspaces . in the type 3 solution",
    "there is a probability @xmath135 that @xmath136 ( where @xmath137 ) nodes are attached to subspace @xmath138 , so the trend is for the type 3 solution to become more favoured as @xmath1 is increased .    in figure",
    "[ fig : plotnasymptotic ] the 3 solutions are plotted for the case @xmath13 .",
    "[ fig : plotnasymptotic ] for @xmath13 for each of the 3 types of optimum.,title=\"fig:\",width=377 ]    for @xmath139 the type 2 solution is never optimal , the type 1 solution is optimal for @xmath140 , and the type 3 solution is optimal for @xmath141 .",
    "the type 2 solution approaches the type 3 solution from below asymptotically as @xmath142 . in figure",
    "[ fig : phasediagram ] a phase diagram is given which shows how the relative stability of the 3 types of solution for different @xmath73 and @xmath1 , where the type 3 solution is seen to be optimal over a large part of the @xmath143 plane .",
    "[ fig : phasediagram ]    thus the most interesting , and commonly occurring , solution is the one in which half the nodes are attached to one subspace and half to the other subspace ( i.e. solution type 3 ) .",
    "although this result has been derived using the non - scaling version of the posterior probability model @xmath9 ( as in equation [ eq : postprob ] ) , it may also be used for scaling posterior probabilities ( as given in equation [ eq : postprobpmd ] ) in certain limiting cases , and also for cases where the effect of leakage @xmath82 is small .",
    "the effect of leakage will not be analysed in detail here . however , its effect may readily be discussed phenomenologically , because the optimisation acts to minimise the damaging effect of leakage on the posterior probability by ensuring that the properties of nodes that are connected by leakage are similar .",
    "this has the most dramatic effect on the type 3 solution , where the way in which the nodes are partitioned into 2 halves must be very carefully chosen in order to minimise the damage due to leakage .",
    "if the leakage is presumed to be a local function , so that @xmath144 , which is a localised `` blob''-shaped function , then the properties of adjacent node are similar ( after optimisation ) . since nodes that are attached to 2 different subspaces necessarily have very different properties , whereas nodes that are attached to the same subspace can have similar properties , it follows that the nodes must split into 2 continguous halves , where nodes @xmath145 are attached to subspace 1 and nodes @xmath146 are attached to subspace 2 , or vice versa . the effect of leakage is thereby minimised , with the worst effect occurring at the boundary between the 2 halves of nodes .",
    "the above analysis has focussed on the non - scaling version of the posterior probability , in which all @xmath73 nodes act together as a unit .",
    "the more general scaling case where the @xmath73 nodes are split up by the effect of the neighbourhood function @xmath147 will not be analysed in detail , because many of its properties are essentially the same as in the non - scaling case . for simplicity",
    "assume that the neighbourhood function @xmath147 is a `` top - hat '' with width @xmath148 ( an odd integer ) centred on @xmath117 .",
    "impose periodic boundary conditions so that the inverse neighbourhood function @xmath149 is also a top - hat , @xmath150 . in this case",
    "an optimum solution in the non - scaling case ( with @xmath151 )  can be directly related to a corresponding optimum solution in the scaling case by simply repeating the node properties periodically every @xmath148 nodes . strictly speaking",
    ", higher order periodicities can also occur in the scaling case ( and can be favoured under certain conditions ) , where the period is @xmath152 ( @xmath153 is an integer ) , but these will not be discussed here .",
    "the effect of the periodic replication of node properties is interesting . the type 3 solution ( with leakage and with @xmath154 splits the nodes into 2 halves , where nodes @xmath155 are attached to subspace 1 and nodes @xmath156 are attached to subspace 2 , or vice versa .",
    "when this is replicated periodically every @xmath148 nodes it produces an alternating structure of node properties , where @xmath157 nodes are attached to subspace 1 , then the next @xmath157 nodes are attached to subspace 2 , and thenthe next @xmath157 nodes are attached to subspace 1 , and so on .",
    "this behaviour is reminiscent of the so - called `` dominance stripes '' that are observed in the mammalian visual cortex .",
    "the purpose of this section is to demonstrate the emergence of the dominance stripes in numerical simulations .",
    "the main body of the software is concerned with evaluating the derivatives of @xmath60 , and the main difficulty is choosing an appropriate form for the leakage ( this has not yet been automated ) .",
    "the parameters that are required for a simulation are as follows :    1 .",
    "@xmath158 : size of 2d rectangular array of nodes .",
    "@xmath160 : size of 2d rectangular input window for each node ( odd integers ) .",
    "ensure that the input window is not too many input data `` correlation areas '' in size , otherwise dominance stripes may not emerge .",
    "dominance stripes require that the correlation _ within _ an input window are substantially stronger than the correlations _ between _ input windows that are attached to different subspaces .",
    "@xmath161 : size of 2d rectangular neighbourhood window for each node ( odd integers ) .",
    "the neighbourhood function @xmath162 is a rectangular top - hat centred on @xmath163 .",
    "the size of the neighbourhood window has to lie within a limited range to ensure that dominance stripes are produced .",
    "this corresponds to ensuring that @xmath73 lies in the type 3 region of the phase diagram in figure [ fig : phasediagram ] .",
    "it is also preferable for the size of the neighbourhood window to be substantially smaller than the input window , otherwise different parts of a neighbourhood window will see different parts of the input data , which will make the network behaviour more difficult to interpret .",
    "4 .   @xmath164 : size of 2d rectangular leakage window for each node ( odd integers ) . for simplicity the leakage @xmath82",
    "is assumed to be given by @xmath165 , where @xmath166 is a `` top - hat '' function of @xmath167 which covers a rectangular region of size @xmath164 centred on @xmath168**. * * the size of the leakage window must be large enough to correlate the parameters of adjacent nodes , but not so large that it enforces such strong correlations between the node parameters that it destroys dominance stripes . 5 .",
    "@xmath169 : additive noise level used to corrupt each member of the training set",
    "@xmath170 : wavenumber of sinusoids used in the training set . in describing",
    "the training sets the index @xmath117 will be used to denote position in input space , thus position @xmath117 in input space lies directly `` under '' node @xmath117 of the network . in 1d simulations",
    "each training vector is a sinusoid of the form @xmath171 , where @xmath172 is a random phase angle , and @xmath173 is a random number sampled uniformly from the interval @xmath174 $ ] - this generates an @xmath98 topology training set ( i.e. parameterised by 1 random angle ) . in 2d simulations",
    "each training vector is a sinusoid of the form @xmath175 , where the additional angle @xmath176 is a random azimuthal orientation for the sine wave - this generates an @xmath99 topology training set ( i.e. parameterised by 2 independent random angles ) .",
    "note that @xmath177 and @xmath178 must be an integer multiple of @xmath179 in order to ensure that the probability density around the @xmath98 subspace generated by @xmath172 has uniform density ( in effect , the @xmath98 then becomes a circular lissajous figure , which therefore has uniform probability density , unlike non - circular lissajous figures ) , and thus to ensure that there are no artefacts induced by the periodicity of the training data that might mimic the effect of dominance stripes .",
    "if @xmath177 and @xmath178 are much greater than @xmath179 then it is not necessary to fix them to be integer multiples of @xmath179 - because the fluctuations in the probability density are then negligible .",
    "note that this restriction on the value of @xmath170 would not have been necessary had complex exponentials been used rather than sinusoids .",
    "@xmath180 : number of subspaces .",
    "this fixes the number of statistically independent subspaces in the training set .",
    "when @xmath181 the training set is generated exactly as above .",
    "when @xmath182 the training set is split up as follows .",
    "the 1d case has even @xmath117 in one subspace , and odd @xmath117 in the other subspace , thus successive components of each training vector alternate between the 2 subspaces .",
    "the 2d case has even @xmath183 in one subspace , and odd @xmath183 in the other subspace , thus each training vector is split up into a chessboard pattern of interlocking subspaces .",
    "this strategy readily generalises for @xmath184 , although this is not used here . within each subspace",
    "the training vector is generated as above , and the subspaces are generated so that they are statistically independent .",
    "@xmath185 : update parameter used in gradient descent .",
    "this is used to update parameters thus @xmath186 there are 3 internally generated update parameter , which control the update of the 3 different types of parameter , i.e. the biases , the weights , and the reference vectors .",
    "this is necessary because these parameters all have different dimensionalities , and by inspection of equation [ equpdateprescription ] the dimensionality of an update parameter is the dimensionality of the parameter it updates ( squared ) divided by the dimensionality of the euclidean distortion .",
    "these 3 internal parameters are automatically adjusted to ensure that the average change in absolute value of each of the 3 types of parameter is equal to @xmath185 times the typical diameter of the region of parameter space populated by the parameters .",
    "this adjustment is made anew as each training vector is presented .",
    "the size of @xmath185 determines the `` memory time '' of the node parameters .",
    "this memory time determines the effective number of training vectors that the nodes are being optimised against , and thus must be sufficiently long ( i.e. @xmath185 sufficiently small ) that if @xmath187 it is possible to discern that the subspaces are indeed statistically independent .",
    "this is crucially important , for dominance stripes can not be obtained if the subspaces are not sufficiently statistically independent .",
    "so @xmath185 must be small , which unfortunately leads to correspondingly long training times .",
    "the training set is globally translated and scaled so that the components of all of its training vectors lie in the interval @xmath188 $ ] .",
    "there are 3 parameter types to initialise .",
    "the weights were all initialised to random numbers sampled from a uniform distribution in the interval @xmath189 $ ] , whereas the biasses and the reference vector components were all initialised to 0 .",
    "because the 2d simulations took a very long time to run , they were periodically interrupted and the state of all the variables written to an output file .",
    "the simulation could then be continued by reading this output file in again and simply continuing where the simulation left off .",
    "alternatively , some of the variables might have their values changed before continuing .",
    "in particular , the random number generator could thus be manipulated to simulate the effect of a finite sized training set ( i.e. use the _ same _ random number seed at the start of each part of the simulation ) , or an infinite - sized training set ( i.e. use a _ different _ random number seed at the start of each part of the simulation ) .",
    "the size of the @xmath185 parameter could also thus be manipulated should a large value be required initially , and reduced to a small value later on , as required in order to guarantee that when @xmath75 the input subspaces are seen to be statistically independent , and dominance stripes may emerge .",
    "there are many ways to choose the boundary conditions . in the numerical simulations",
    "periodic boundary conditions will be avoided , because they can lead to artefacts in which the node parameters become topologically trapped .",
    "for instance , in a 2d simulation , periodic boundary conditions imply that the nodes sit on a 2-torus .",
    "leakage implies that the node parameter values are similar for adjacent nodes , which limits the freedom for the parameters to adjust their values on the surface of the 2-torus .",
    "for instance , any acceptable set of parameters that sits on the 2-torus can be converted into another acceptable set by mapping the 2-torus to itself , so that each of its @xmath98 `` coils up '' an integer number of times onto itself . such a multiply wrapped parameter configuration",
    "is topologically trapped , and can not be perturbed to its original form .",
    "this problem does not arise with non - periodic boundary conditions .",
    "there are several different problems that arise at the boundaries of the array of nodes :    1 .",
    "the neighbourhood function @xmath162 can not be assumed to be a rectangular top - hat centred on @xmath163 .",
    "instead , it will simply be truncated so that it does not fall off the edge of array of nodes , i.e. @xmath190 for those @xmath163 that lie outside the array .",
    "2 .   the leakage function @xmath191 will be similarly truncated . however , in this case @xmath191 must normalise to unity when summed over @xmath163 , so the effect of the truncation must be compensated by scaling the remaining elements of @xmath191 .",
    "the input window for each node implies that the input array must be larger than the node array in order that the input windows never fall off the edge of the input array .",
    "the most important result is the emergence of dominance stripes . for @xmath130",
    "there are thus 2 numbers that need to be displayed for each node : the `` degree of attachment '' to subspace 1 , and similarly for subspace 2 .",
    "there are many ways to measure degree of attachment , for instance the probability density @xmath25 gives a direct measurement of how strongly node @xmath10 depends on the input vector @xmath0 , so its `` width '' or `` volume '' in each of the subspaces could be used to measure degree of attachment . however , in the simulations presented here ( i.e. sinusoidal training vectors ) the degree of attachment is measured as the average of the absolute values of the components of the reference vector in the subspace concerned .",
    "this measure tends to zero for complete detachment . for 1d simulations",
    "2 dominance plots can be overlaid to show the dominance of subspaces 1 and 2 for each node . for 2d simulations it is simplest to present only 1 of these plots as a 2d array of grey - scale pixels , where the grey level indicates the dominance of subspace 1 ( or , alternatively , subspace 2 ) .",
    "the parameter values used were : @xmath192 , @xmath193 , @xmath194 , @xmath195 , @xmath196 , @xmath197 , @xmath182 , @xmath198 , @xmath199 .",
    "this value of @xmath170 implies @xmath200 , so @xmath170 is approximately an integer multiple of @xmath179 , as required for an artefact - free simulation . in figure",
    "[ fig : dominance1d ] a plot of the 2 dominance curves obtained after 3200 training updates is shown .",
    "[ fig : dominance1d ]    this dominance plot clearly shows alternating regions where subspace 1 dominates and subspace 2 dominates .",
    "the width of the neighbourhood function is 21 , which is the same the period of the variations in the dominance plots , i.e. within each set of adjacent 21 nodes half the nodes are attached to subspace 1 and half to subspace 2 .",
    "there are boundary effects , but these are unimportant .",
    "the normalisation of the expression for @xmath19 in equation [ eq : postprobpmd ] may be demonstrated as follows : @xmath201 in the first step the order of the @xmath10 and the @xmath40 summations is interchanged using @xmath202 , in the second step the numerator and denominator of the summand cancel out .",
    "it is possible to simplify equation [ eq : objectivemultifire ] by using the following identity @xmath203 note that this holds for all choices of @xmath204 .",
    "this allows the euclidean distance to be expanded thus @xmath205 each term of this expansion can be inserted into equation [ eq : objectivemultifire ] to yield @xmath206 @xmath43 has been assumed to be a symmetric function of @xmath44 in the first two results , and the definition of @xmath46 in equation [ eq : refvectmultifire ] has been used to obtain the third result .",
    "these results allow @xmath14 in equation [ eq : objectivemultifire ] to be expanded as @xmath207 , where @xmath208 by noting that @xmath209 , an upper bound for @xmath14 in the form @xmath50 follows immediately from these results .",
    "note that @xmath210 whereas @xmath53 can have either sign . in the special case where @xmath211 ( i.e. @xmath212 and @xmath213 are independent of each other given that @xmath214 is known )",
    "@xmath53 reduces to @xmath215 which is manifestly positive .",
    "this is the form of @xmath53 that is used throughout this paper .",
    "@xmath52 and @xmath53 are as given in equation [ eq : upperboundpieces ] , i.e. it is assumed that @xmath216 and @xmath19 has the scalable form given in equation [ eq : postprobpmd ] . define a compact matrix notation as follows @xmath217 using this matrix notation , the functions @xmath66 , @xmath67 , @xmath68 , and @xmath69 may be defined as @xmath218 the variation of @xmath41 is then given by @xmath219 in order to rearrange the expressions to ensure that only a single dummy index is required at every stage of evaluation of the sums it will be necessary to use the result @xmath220      the derivative is given by @xmath222 use matrix notation to write this as @xmath223 finally remove the explicit summations to obtain the required result @xmath224      the derivative is given by @xmath226 use matrix notation to write this as @xmath227 finally remove the explicit summations to obtain the required result @xmath228      the differential is given by @xmath230 use matrix notation to write this as @xmath231 reorder the summations to obtain @xmath232 relabel the indices and evaluate the sum over the kronecker delta to obtain @xmath233 finally remove the explicit summations to obtain the required result @xmath234      the differential is given by @xmath236 use matrix notation to write this as @xmath237 reorder the summations to obtain @xmath238 relabel the indices and evaluate the sum over the kronecker delta to obtain @xmath239 finally remove the explicit summations to obtain the required result @xmath240",
    "from equation [ eq : upperboundpieces ] @xmath60 can be written as @xmath242 where the constant terms do not depend on @xmath23 . however , from equation [ eq : upperboundpieces ] the derivative @xmath243 can be written as @xmath244 using bayes theorem the stationarity condition @xmath100 yields a matrix equation for the @xmath23@xmath245 which may then be used to replace all instances of @xmath0 in equation [ eq : upperboundrefvectpieces ] .",
    "this yields the result @xmath246",
    "in order to compare the value of @xmath60 that is obtained when different types of supposedly optimum configurations of the threshold functions @xmath84 are tried , the @xmath23 that solves @xmath100 ( see appendix [ app : d1d2refvect ] )  must be inserted into the expression for @xmath60 . in the following derivations",
    "the constant term is omitted , and the definition @xmath247 ( see equation [ eq : radiussquared ] ) has been used .            [ [ type-3-optimumhalf - the - nodes - are - attached - one - subspace - and - half - are - attached - to - the - other ] ] type 3 optimum :  half the nodes are attached one subspace and half are attached to the other ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
  ],
  "abstract_text": [
    "<S> this paper shows how a folded markov chain network can be applied to the problem of processing data from multiple sensors , with an emphasis on the special case of 2 sensors . </S>",
    "<S> it is necessary to design the network so that it can transform a high dimensional input vector into a posterior probability , for which purpose the partitioned mixture distribution network is ideally suited . </S>",
    "<S> the underlying theory is presented in detail , and a simple numerical simulation is given that shows the emergence of ocular dominance stripes . </S>"
  ]
}