{
  "article_text": [
    "the field of channel coding was started with shannon s famous theorem proposed in 1948 @xcite , which shows that the channel capacity upper bounds the amount of information that can be reliably transmitted over a noisy communication channel .",
    "after this result , seeking for practical coding schemes that could approach channel capacity became a central objective for researchers . on the way from theory to practice , many coding schemes are proposed .",
    "different types of codes emerge in improving the performance , giving consideration to the trade - off between coding complexity and error decay rate .",
    "the history of channel coding traces back to the era of algebraic coding , including the well - known hamming codes @xcite , golay codes @xcite , reed - muller codes @xcite@xcite , reed - solomon codes @xcite , lattice codes @xcite , and others @xcite .",
    "however , despite enabling significant advances in code design and construction , algebraic coding did not turn out to be the most promising means to approach the shannon limit .",
    "the next era of probabilistic coding considered approaches that involved optimizing code performance as a function of coding complexity .",
    "this line of development included convolutional codes @xcite , and concatenated codes @xcite at earlier times , as well as turbo codes @xcite and low - density parity - check ( ldpc ) codes @xcite@xcite afterwards .",
    "recently , polar codes @xcite have been proved to achieve shannon limit of binary - input symmetric channels with low encoding and decoding complexity . in another recent study @xcite@xcite , new types of rateless codes , viz .",
    "spinal codes , are proposed to achieve the channel capacity .",
    "another well - studied ( and practically valuable ) research direction in information theory is the problem of compression of continuous - valued sources .",
    "given the increased importance of voice , video and other multimedia , all of which are typically `` analog '' in nature , the value associated with low - complexity algorithms to compress continuous - valued data is likely to remain significant in the years to come . for discrete - valued `` finite - alphabet '' problems , the associated coding theorem @xcite and practically - meaningful coding schemes are well known .",
    "trellis based quantizers @xcite are the first to achieve the rate distortion trade - off , but with encoding complexity scaling exponentially with the constraint length .",
    "later , matsunaga and yamamoto @xcite show that a low density parity check ( ldpc ) ensemble , under suitable conditions on the ensemble structure , can achieve the rate distortion bound using an optimal decoder .",
    "@xcite shows that low density generator matrix ( ldgm ) codes , as the dual of ldpc codes , with suitably irregular degree distributions , empirically perform close to the shannon rate - distortion bound with message - passing algorithms .",
    "more recently , polar codes are shown to be the first provably rate distortion limit achieving codes with low complexity @xcite . in the case of analog sources , although both practical coding schemes as well as theoretical analysis are very heavily studied , a very limited literature exists that connects the theory with low - complexity codes .",
    "the most relevant literature in this context is on lattice compression and its low - density constructions @xcite . yet ,",
    "this literature is also limited in scope and application .",
    "the problem of coding over analog noise channels is highly non - trivial in general . to this end , a method of modulation is commonly utilized to map discrete inputs to analog signals for transmission through the physical channel @xcite . in this paper , we focus on designing and coding over such mappings .",
    "in particular , we propose a new coding scheme for general analog channels with moderate coding complexity based on an expansion technique , where channel noise is perfectly or approximately represented by a set of independent discrete random variables ( see fig .",
    "[ fig : expansion_framework ] ) . via this representation",
    ", the problem of coding over an analog noise channel is reduced to that of coding over parallel discrete channels .",
    "we focus on additive exponential noise ( aen ) , and we show that the shannon limit , i.e. , the capacity , is achievable for aen channel in the high snr regime . more precisely , for any given @xmath0 , it is shown that the gap to capacity is at most @xmath1 when at least @xmath2 number of levels are utilized in the coding scheme together with embedded binary codes .",
    "generalizing results to @xmath3-ary alphabets , we show that this gap can be reduced more .",
    "the main advantage of the proposed method lies on its complexity inheritance property , where the encoding and decoding complexity of the proposed schemes follow that of the embedded capacity achieving codes designed for discrete channels , such as polar codes and spinal codes .     to @xmath4 .",
    "channel noise @xmath5 is considered as its binary expansion @xmath6 , and similar expansions are adopted to channel input and output .",
    "carries exist between neighboring levels . ]    in the second part of this paper , we present an expansion coding scheme for compressing of analog sources .",
    "this is a dual problem to the channel coding case , and we utilize a similar approach where we consider expanding exponential sources into binary sequences , and coding over the resulting set of parallel discrete sources . we show that this scheme s performance can get very close to the rate distortion limit in the low distortion regime ( i.e. , the regime of interest in practice ) .",
    "more precisely , the gap between the rate of the proposed scheme and the theoretical limit is shown to be within a constant gap ( @xmath7 ) for any distortion level @xmath8 when at least @xmath9 number of levels are utilized in the coding scheme ( where , @xmath10 is the mean of the underlying exponential source ) . moreover",
    ", this expansion coding scheme can be generalized to laplacian sources ( two - sided symmetric exponential distribution ) , where the sign bit is considered separately and encoded perfectly to overcome the difficulty of source value being negative .",
    "the rest of paper is organized as follows .",
    "related work is investigated and summarized in section  [ sec : related_work ] .",
    "the expansion coding scheme for channel coding is detailed and evaluated in section  [ sec : channel_coding ] . the expansion source coding framework and its application to exponential sources",
    "are demonstrated in section  [ sec : source_coding ] . finally , the paper is concluded in section  [ sec : conclusion ] .",
    "multilevel coding is a general coding method designed for analog noise channels with a flavor of expansion @xcite . in particular ,",
    "a lattice partition chain @xmath11 is utilized to represent the channel input , and , together with a shaping technique , the reconstructed codeword is transmitted to the channel .",
    "it has been shown that optimal lattices achieving shannon limit exist . however , the encoding and decoding complexity for such codes is high , in general . in the sense of representing the channel input , our scheme is coincident with multilevel coding by choosing @xmath12 ,  , @xmath13 , for some @xmath14 , where coding of each level is over @xmath3-ary finite field ( see fig .  [",
    "fig : multilevel_framework ] ) .",
    "the difference in the proposed method is that besides representing the channel input in this way , we also `` expand '' the channel noise , such that the coding problem for each level is more suitable to solve by adopting existing discrete coding schemes with moderate coding complexity .",
    "moreover , by adapting the underlying codes to channel - dependent variables , such as carries , the shannon limit is shown to be achievable by expansion coding with moderate number of expanded levels .    , only channel input is expressed by multi - levels , but not the channel noise . ]",
    "the deterministic model , proposed in @xcite , is another framework to study analog noise channel coding problems , where the basic idea is to construct an approximate channel for which the transmitted signals are assumed to be noiseless above a certain noise level .",
    "this approach has proved to be very effective in analyzing the capacity of networks . in particular , it has been shown that this framework perfectly characterizes degrees of freedom of point - to - point awgn channels , as well as some multi - user channels . in this sense ,",
    "our expansion coding scheme can be seen as a generalization of these deterministic approaches . here , the effective noise in the channel is carefully calculated and the system takes advantage of coding over the noisy levels at any snr .",
    "this generalized channel approximation approach can be useful in reducing the large gaps reported in the previous works , because the noise approximation in our work is much closer to the actual distribution as compared to that of the deterministic model ( see fig .",
    "[ fig : expansion_deterministic_compare ] ) .",
    "there have been many attempts to utilize discrete codes for analog channels ( beyond simple modulation methods ) .",
    "for example , after the introduction of polar codes , considerable attention has been directed towards utilizing their low complexity property for analog channel coding .",
    "a very straightforward approach is to use the central limit theorem , which says that certain combinations of i.i.d .",
    "discrete random variables converge to a gaussian distribution .",
    "as reported in @xcite and @xcite , the capacity of awgn channel can be achieved by coding over large number of bscs , however , the convergence rate is linear which limits its application in practice . to this end",
    ", @xcite proposes a mac based scheme to improve the convergence rate to exponential , at the expense of having a much larger field size . a newly published result in @xcite attempts to combine polar codes with multilevel coding",
    ", however many aspects of this optimization of polar - coded modulation still remain open . along the direction of this research",
    ", we also try to utilize capacity achieving discrete codes to approximately achieve the capacity of analog channels .",
    "the additive exponential noise ( aen ) channel is of particular interest as it models worst - case noise given a mean and a non - negativity constraint on noise @xcite .",
    "in addition , the aen model naturally arises in non - coherent communication settings , and in optical communication scenarios .",
    "( we refer to @xcite and @xcite for an extensive discussion on the aen channel . )",
    "verd derived the optimal input distribution and the capacity of the aen channel in @xcite .",
    "martinez , on the other hand , proposed the pulse energy modulation scheme , which can be seen as a generalization of amplitude modulation for the gaussian channels . in this scheme ,",
    "the constellation symbols are chosen as @xmath15 , for @xmath16 with a constant @xmath17 , and it is shown that the information rates obtained from this constellation can achieve an energy ( snr ) loss of @xmath18 db ( with the best choice of @xmath19 ) compared to the capacity in the high snr regime . another constellation technique for this coded modulation approach is recently considered in @xcite , where log constellations are designed such that the real line is divided into ( @xmath20 ) equally probable intervals .",
    "@xmath21 of the centroids of these intervals are chosen as constellation points , and , by a numerical computation of the mutual information , it is shown that these constellations can achieve within a @xmath22 db snr gap in the high snr regime .",
    "our approach , which achieves arbitrarily close to the capacity of the channel , outperforms these previously proposed modulation techniques .    in the domains of image compression and speech coding , laplacian and exponential distributions",
    "are widely adopted as natural models of correlation between pixels and amplitude of voice @xcite .",
    "exponential distribution is also fundamental in characterizing continuous - time markov processes @xcite .",
    "although the rate distortion functions for both have been known for decades , there is still a gap between theory and existing low - complexity coding schemes .",
    "the proposed schemes , primarily for the medium to high distortion regime , include the classical scalar and vector quantization schemes @xcite , and markov chain monte carlo ( mcmc ) based approach in @xcite .",
    "however , the understanding of low - complexity coding schemes , especially for the low - distortion regime , remains limited . to this end",
    ", our expansion source coding scheme aims to approach the rate distortion limit with practical encoding and decoding complexity . by expanding the sources into independent levels , and using the decomposition property of exponential distribution ,",
    "the problem has been remarkably reduced to a set of simpler subproblems , compression for discrete sources .",
    "in general , expansion channel coding is a scheme of reducing the problem of coding over an analog channel to coding over a set of discrete channels . in particular , we consider the additive noise channel given by @xmath23 where @xmath24 are channel inputs with alphabet @xmath25 ( possibly having channel input requirements , such as certain moment constraints ) ; @xmath26 are channel outputs ; @xmath27 are additive noises independently and identically distributed with a continuous probability density function ; @xmath28 is block length . we represent the inputs as @xmath29 .",
    "( similar notation is used for other variables throughout the sequel . )    when communicating , the transmitter conveys one of the messages , @xmath30 , which is uniformly distributed in @xmath31 ; and it does so by mapping the message to the channel input using encoding function @xmath32 such that @xmath33 .",
    "the decoder uses the decoding function @xmath34 to map its channel observations to an estimate of the message .",
    "specifically , @xmath35 , where the estimate is denoted by @xmath36 .",
    "a rate @xmath37 is said to be achievable , if the average probability of error defined by @xmath38 can be made arbitrarily small for large @xmath28 .",
    "the capacity of this channel is denoted by @xmath39 , which is the maximum achievable rate @xmath37 , and its corresponding optimal input distribution is denoted as @xmath40 .",
    "our proposed coding scheme is based on the idea that by `` expanding '' the channel noise ( i.e. , representing it by its @xmath3-ary expansion ) , an approximate channel can be constructed , and proper coding schemes can be adopted to each level in this representation . if the approximation is close enough , then the coding schemes that are optimal for each level can be translated to an effective one for the original channel .",
    "more formally , consider the original noise @xmath41 and its approximation @xmath42 , which is defined by the truncated @xmath3-ary expansion of @xmath41 .",
    "for this moment , we simply take @xmath43 ( i.e. , considering binary expansion ) , and leave the general case for later discussion .",
    "@xmath44 where @xmath45 represents the sign of @xmath41 , taking a value from @xmath46 ; @xmath47 s are mutually independent bernoulli random variables . by similarly expanding the channel input ,",
    "we convert the problem of coding over analog channels to coding over a set of binary discrete channels .",
    "this mapping is highly advantageous , as capacity achieving discrete codes can be adopted for coding over the constructed binary channels .",
    "assume the input distributions for sign channel and discrete channel at @xmath48 are represented by @xmath49 and @xmath50 correspondingly , then an achievable rate ( via random coding ) for the approximated channel is given by @xmath51 where @xmath52 by adopting the same coding scheme over the original channel , one can achieve a rate given by @xmath53 the following result provides a theoretical basis for expansion coding .",
    "( here , @xmath54 denotes convergence in distribution . )",
    "[ thm : channel_expansion_coding ] if @xmath55 and @xmath56 , as @xmath57 , where @xmath58 , i.e. , the optimal input distribution for the original channel , then @xmath59 .",
    "the proof of this theorem follows from the continuity property of mutual information . in other words , if the approximate channel is close to the original one , and the distribution of the input is close to the optimal input distribution , then the expansion coding scheme will achieve the capacity of the channel under consideration .      in this section",
    ", we consider an example where expansion channel coding can achieve the capacity of the target channel .",
    "the particular channel considered is an additive exponential noise ( aen ) channel , where the channel noise @xmath27 in is independently and identically distributed according to an exponential density with mean @xmath60 , i.e. , omitting the index @xmath61 , noise has the following density : @xmath62 where @xmath63 for @xmath64 and @xmath65 otherwise .",
    "moreover , channel input @xmath24 in is restricted to be non - negative and satisfies the mean constraint @xmath66\\leq e_{\\mathsf{x}}.\\label{equ : aen_input_constraint}\\ ] ]    the capacity of aen channel is given by @xcite , @xmath67 where @xmath68 , and the capacity achieving input distribution is given by @xmath69 where @xmath70 if and only if @xmath71 . here ,",
    "the optimal input distribution is not exponentially distributed , but a mixture of an exponential distribution with a delta function .",
    "however , we observe that in the high snr regime , the optimal distribution gets closer to an exponential distribution with mean @xmath72 , since the weight of delta function approaches to @xmath73 as snr tends to infinity .",
    "the basis of the proposed coding scheme is the expansion of analog random variables to discrete ones , and the exponential distribution emerges as a first candidate due to its decomposition property .",
    "we show the following lemma , which allows us to have independent bernoulli random variables in the binary expansion of an exponential random variable .",
    "[ lem : exponential_expansion ] let @xmath74 s be independent bernoulli random variables with parameters given by @xmath75 , i.e. , @xmath76 , and consider the random variable defined by @xmath77 then , the random variable @xmath78 is exponentially distributed with mean @xmath79 , i.e. , its pdf is given by @xmath80 if and only if the choice of @xmath75 is given by @xmath81    see appendix  [ app : exponential_expansion_proof ] .",
    "this lemma reveals that one can reconstruct exponential random variable from a set of independent bernoulli random variables perfectly .",
    "[ fig : exponential_recovery ] illustrates that the distribution of recovered random variable from expanded levels ( obtained from the statistics of @xmath82 independent samples ) is a good approximation of original exponential distribution .    ) .",
    "* @xmath82 samples are generated from the expansion form of discrete random variables , where expansion levels are truncated from @xmath83 to @xmath84 . ]    a set of typical numerical values of @xmath75s for @xmath85 is shown in fig .",
    "[ fig : exponential_expansion_parameter ] .",
    "it is evident that @xmath75 approaches to @xmath73 for the `` higher '' levels and approaches @xmath86 for what we refer to as `` lower '' levels .",
    "hence , the primary non - trivial levels for which coding is meaningful are the so - called `` middle '' ones , which provides the basis for truncating the number of levels to a finite value without a significant loss in performance .     with @xmath85 .",
    "* x - axis is the level index for binary expansion ( e.g. , value @xmath87 means the weight of corresponding level is @xmath88 ) , and y - axis shows the corresponding probability of taking value @xmath89 at each level , i.e. , @xmath75 . ]",
    "we consider the binary expansion of the channel noise @xmath90 where @xmath91 are i.i.d .",
    "bernoulli random variables with parameters @xmath92 by lemma [ lem : exponential_expansion ] , @xmath93 as @xmath94 . in this sense , we approximate the exponentially distributed noise perfectly by a set of discrete bernoulli distributed noises . similarly , we also expand channel input and output as in the following , @xmath95 where @xmath96 and @xmath97 are also bernoulli random variables with parameters @xmath98 and @xmath99 correspondingly . here , the channel input is chosen as zero for levels @xmath100 . noting that the summation in the original channel is a sum over real numbers",
    ", we do not have a binary symmetry channel ( bsc ) at each level ( from @xmath50s to @xmath101s ) .",
    "if we could replace the real sum by modulo-@xmath102 sum such that at each level @xmath48 we have an independent coding problem , then any capacity achieving bsc code can be utilized over this channel .",
    "( here , instead of directly using the capacity achieving input distribution of each level , we can use its combination with the method of gallager  @xcite to achieve a rate corresponding to the one obtained by the mutual information @xmath103 evaluated with an input distribution bernoulli with parameter @xmath104 .",
    "this helps to approximate the optimal input distribution of the original channel . )",
    "however , due to the addition over real numbers , carries exist between neighboring levels , which further implies that the levels are not independent .",
    "every level , except for the lowest one , is impacted by carry from lower levels . in order to alleviate this issue ,",
    "two schemes are proposed in the following to ensure independent operation of the levels . in these models of coding over independent parallel channels ,",
    "the total achievable rate is the summation of individual achievable rates over all levels .      denoting the carry seen at level @xmath48 as @xmath105 , which is also a bernoulli random variable with parameter @xmath106",
    ", the remaining channels can be represented with the following , @xmath107 where the effective noise , @xmath108 , is a bernoulli random variable obtained by the convolution of the actual noise and the carry , i.e. , @xmath109 here , the carry probability is given by the following recursion relationship :    * for level @xmath110 , @xmath111 * for level @xmath112 , @xmath113    using capacity achieving codes for bsc , e.g. , polar codes or spinal codes , combined with the gallager s method , expansion coding achieves the following rate by considering carries as noise .",
    "[ thm : aen_expansion_coding_schemei ] expansion coding , considering carries as noise , achieves the rate for aen channel given by @xmath114,\\label{equ : aen_achievable_rate_schemei}\\ ] ] for any @xmath115 , where @xmath116 $ ] is chosen to satisfy constraint , i.e. , @xmath117=\\frac{1}{n } \\sum\\limits_{i=1}^n\\sum\\limits_{l =- l_1}^{l_2}2^l \\mathbb{e}[\\mathsf{x}_{i , l } ]   = \\sum\\limits_{l =- l_1}^{l_2 } 2^{l } p_l \\leq e_{\\mathsf{x}}.\\nonumber\\ ] ]      in this scheme , let us consider decoding starting from the lowest level @xmath110 .",
    "the receiver will obtain the correct @xmath118 for @xmath119 by using powerful discrete coding at this level . as the receiver has the knowledge of @xmath120",
    ", it can determine the correct noise sequence @xmath121 for @xmath119 . with this knowledge ,",
    "the receiver can directly obtain each @xmath122 for @xmath119 , which is the carry from level @xmath110 to level @xmath123 . this way , by iterating to higher levels , the receiver can recursively subtract the impact of carry bits .",
    "therefore , when there is no decoding error at each level , the effective channel that the receiver observes is given by @xmath124 for @xmath125 .",
    "we remark that with this decoding strategy , the effective channels will no longer be a set of independent parallel channels , as decoding in one level affects the channels at higher levels . however , if the utilized coding method is strong enough ( e.g. , if the error probability decays to @xmath73 exponentially with @xmath28 ) , then decoding error due to carry bits can be made insignificant by increasing @xmath28 for a given number of levels .",
    "we state the rate resulting from this approach in the following theorem .",
    "[ thm : aen_expansion_coding_schemeii ] expansion coding , by decoding the carries , achieves the rate for aen channel given by @xmath126,\\label{equ : aen_achievable_rate_schemeii}\\ ] ] for any @xmath115 , where @xmath116 $ ] is chosen to satisfy constraint , i.e. , @xmath117=\\frac{1}{n } \\sum\\limits_{i=1}^n\\sum\\limits_{l =- l_1}^{l_2}2^l \\mathbb{e}[\\mathsf{x}_{i , l } ]   = \\sum\\limits_{l =- l_1}^{l_2 } 2^{l } p_l \\leq e_{\\mathsf{x}}.\\nonumber\\ ] ]    compared to the previous case , the optimization problem is simpler here as the rate expression is simply the sum of the rates obtained from a set of parallel channels .",
    "optimizing for these two theoretical achievable rates require choosing proper values for @xmath104 .",
    "note that , the optimization problems given by theorem   [ thm : aen_expansion_coding_schemei ] and [ thm : aen_expansion_coding_schemeii ] are not easy to solve in general . here , instead of searching for the optimal solutions directly , we utilize the information from the optimal input distribution of the original channel . recall that the distribution in can be approximated by an exponential distribution with mean @xmath72 at high snr .",
    "hence , one can simply choose @xmath104 from the binary expansion of the exponential distribution with mean @xmath72 as an achievable scheme , i.e. , @xmath127    we now show that this proposed scheme achieves the capacity of aen channel in the high snr regime for a sufficiently high number of levels .",
    "for this purpose , we first characterize the asymptotic behavior of entropy at each level for @xmath128 and @xmath129 correspondingly , where the later one is closely related to carries .",
    "[ lem : aen_entropy_bound ] the entropy of noise seen at level @xmath48 , @xmath130 , is bounded by @xmath131 where @xmath132 .",
    "see appendix  [ app : aen_entropy_bound ] .",
    "[ lem : aen_equivalent_entropy_bound ] the entropy of equivalent noise at level @xmath48 , @xmath133 , is bounded by @xmath134 where @xmath135 .",
    "see appendix  [ app : aen_equivalent_entropy_bound ] .",
    "the intuitions behind these lemmas are given by the example scenario in fig .",
    "[ fig : aen_expanded_level ] , which shows that the bounds on noise tails are both exponential .",
    "now , we state the main result indicating the capacity gap of expansion coding scheme over aen channel .",
    "[ thm : aen_achivable_rate_main_result ] for any positive constant @xmath0 , if    * @xmath136 ; * @xmath137 ; * _ @xmath138 _ , where _ @xmath139 _ ,    then , with the choice of @xmath104 as ,    1 .   considering carries as noise , the achievable rate given by satisfies _ @xmath140 _ where @xmath17 is a constant independent of _",
    "@xmath141 _ and @xmath142 ; 2 .",
    "decoding carries , the achievable rate given by satisfies _ @xmath143 _    the proof of this theorem",
    "is based on the observation that the sequence of @xmath104 is a left - shifted version of @xmath128 at high snr regime . as limited by power constraint ,",
    "the number of levels shifted is at most @xmath144 , which further implies the rate we gain is roughly @xmath144 as well , when carries are decoded .",
    "if considering carries as noise , then there is apparent gap between the two version of noises , which leads to a constant gap for achievable rate .",
    "[ fig : aen_expanded_level ] helps to illustrate key steps of the intuition , and a detailed proof with precise calculations is given in appendix  [ app : aen_achivable_rate_main_result ] .    , @xmath128 , @xmath145 , @xmath129 , @xmath146 and",
    "rates at each level are shown .",
    "in this example , @xmath147 and @xmath148 , which further implies @xmath104 is a left - shifted version of @xmath128 by @xmath149 levels .",
    "the coding scheme with @xmath150 and @xmath151 covers the significant portion of the rate obtained by using all of the parallel channels . ]    by lemma  [ lem : exponential_expansion ] , @xmath152 , and combined with the argument in theorem  [ thm : channel_expansion_coding ] , we have @xmath153 as @xmath57 .",
    "hence , the coding scheme also works well for the original aen channel .",
    "more precisely , expansion coding scheme achieves the capacity of aen channel at high snr region using moderately large number of expansion levels .",
    "we calculate the rates obtained from the two schemes above ( @xmath154 as and @xmath155 as ) with input probability distribution given by ( [ equ : aen_input_parameter ] ) .",
    "numerical results are given in fig .",
    "[ fig : aen_achievable_rate ] .",
    "it is evident from the figure ( and also from the analysis given in theorem  [ thm : aen_achivable_rate_main_result ] ) that the proposed technique of decoding carries , when implemented with sufficiently large number of levels , achieves channel capacity at high snr regime .",
    "another point is that neither of the two schemes works well in low snr regime , which mainly results from the fact that input approximation is only perfect for sufficiently high snr .",
    "nevertheless , the scheme ( the rate obtained by decoding carries ) performs close to optimal in the moderate snr regime as well .    : the rate obtained by considering carries as noise . @xmath155 : the rate obtained by decoding carry at each level .",
    "solid lines represent adopting enough number of levels as indicated in theorem  [ thm : aen_achivable_rate_main_result ] , while dashed lines represent only adopting constant number of levels ( not scaling with snr ) . ]      in the previous section , only binary expansion was considered .",
    "generalization to @xmath3-ary expansion with @xmath156 is discussed here .",
    "note that this change does not impact the expansion coding framework , and the only difference lies in that each level after expansion should be modeled as a @xmath3-ary discrete memoryless channel . for this , we need to characterize the @xmath3-ary expansion of exponential distribution .",
    "mathematically , the parameters of expanded levels for an exponential random variable @xmath78 with parameter @xmath157 can be calculated as follows : @xmath158\\nonumber\\\\          & = \\frac{\\left(1-e^{-\\lambda q^l}\\right)e^{-\\lambda q^l s}}{1-e^{-\\lambda q^{l+1}}},\\nonumber\\end{aligned}\\ ] ] where @xmath159 and @xmath160 .",
    "based on this result , consider channel input and noise expansions as @xmath161 and @xmath162 then , the achievable rate by decoding carries ( note that in @xmath3-ary expansion case , carries are still bernoulli distributed ) can be expressed as @xmath163,\\label{equ : aen_achivable_rate_baseq}\\ ] ] where @xmath164 and @xmath165 denote the distribution of expanded random variables at level @xmath48 for input and noise respectively ; @xmath166 represents for the vector convolution .",
    "when implemented with enough number of levels in coding , the achievable rates given by achieves the capacity of aen channel for any @xmath156 .",
    "more precisely , as shown in the numerical result in fig .",
    "[ fig : aen_achievable_rate_baseq ] , expansion coding with larger @xmath3 can achieve a higher rate ( although this enhancement becomes limited when @xmath3 gets greater than @xmath84 ) .",
    "this property of the coding scheme can be utilized to trade - off number of levels ( @xmath167 ) and the alphabet size ( @xmath3 ) to achieve a certain rate at a given @xmath168 .",
    "-ary expansion .",
    "* the achievable rates using @xmath3-ary expansion coding by decoding carries are illustrated . ]",
    "expansion source coding is a scheme of reducing the problem of compressing analog sources to compressing a set of discrete sources . in particular , consider an i.i.d .",
    "source @xmath169 .",
    "a @xmath170-rate distortion code consists of an encoding function @xmath171 , where @xmath172 , and a decoding function @xmath173 , which together map @xmath174 to an estimate @xmath175 .",
    "then , the rate and distortion pair @xmath176 is said to be achievable if there exists a sequence of @xmath170-rate distortion codes with @xmath177\\leq d$ ] for a given distortion measure of interest @xmath178 .",
    "the rate distortion function @xmath179 is the infimum of such rates , and by shannon s theorem @xcite , we have : @xmath180\\leq d}i(\\mathsf{x};\\tilde{\\mathsf{x}}),\\nonumber\\ ] ] where the optimal conditional distribution is given by @xmath181 .",
    "the expansion source coding scheme proposed here is based on the observation that by expanding the original analog source into a set of independent discrete random variables , proper source coding schemes could be adopted for every expanded level .",
    "if this approximation in expansion is close enough , then the overall distortion obtained from expansion coding scheme is also close to the optimal distortion .",
    "more formally , consider the original analog source @xmath182 and its approximation @xmath183 given by ( omitting index @xmath61 ) @xmath184 where @xmath49 represents the sign of @xmath183 and takes values from @xmath46 , and @xmath50 is the expanded bernoulli random variable at level @xmath48 . similarly ,",
    "if we expand the estimate by @xmath185 where @xmath186 represents the sign of @xmath187 , random variable taking values from @xmath46 , and @xmath188 is independent bernoulli random variable at level @xmath48 after expansion .    here , we reduce the original problem to a set of source coding subproblems over levels @xmath189 to @xmath4 .",
    "similar to the channel coding case analyzed above , if @xmath190 , and @xmath191 , as @xmath192 , @xmath193 , then the achieved rate distortion pair approximates the original one .",
    "note that , in general , the decomposition may not be sufficiently close for most of the sources , and the distribution for the estimate may not be sufficiently approximated .",
    "these situations add more distortion and result in a gap from the theoretical limit .      in this section , a particular lossy compression example is introduced to illustrate the effectiveness of expansion source coding .",
    "consider an i.i.d .",
    "exponential source sequence @xmath194 , i.e. , omitting index @xmath61 , each variable has a pdf given by @xmath195 where @xmath79 is the mean of @xmath182 . distortion measure of concern is the `` one - sided error distortion '' given by @xmath196 where @xmath197 indicates comparison of vectors element - wise ( each element should be greater than the other ) .",
    "this setup is equivalent to the one in @xcite , where another distortion measure is considered .",
    "[ lem : expsc_rate_distortion ] the rate distortion function for an exponential source with the one - sided error distortion is given by @xmath198 moreover , the optimal conditional distribution to achieve the limit is given by @xmath199    proof is given in @xcite , and it is based on the observation that among the ensemble of all probability density functions with positive support set and mean constraint , exponential distribution maximizes the differential entropy . by designing a test channel from @xmath200 to @xmath182 , with additive noise distributed as exponential with parameter @xmath201 , both",
    "the infimum mutual information and optimal conditional distribution can be characterized .",
    "details can be found in appendix  [ app : expsc_rate_distortion ] .",
    "using lemma [ lem : exponential_expansion ] , we reconstruct the exponential distribution by a set of discrete bernoulli random variables . in particular , the expansion of exponential source over levels ranging from @xmath189 to @xmath4 can be expressed as @xmath202 where @xmath96 are bernoulli random variables with parameter @xmath203 this expansion perfectly approximates exponential source by letting @xmath57 .",
    "consider a similar expansion of the source estimate , i.e. , @xmath204 where @xmath205 is the resulting bernoulli random variable with parameter @xmath206 .",
    "utilizing the expansion method , the original problem of coding for a continuous source can be translated to a problem of coding for a set of independent binary sources .",
    "this transformation , although seemingly obvious , is valuable as one can utilize powerful coding schemes over discrete sources to achieve rate distortion limits with low complexity . in particular , we design two schemes for the binary source coding problem at each level .      in order to guarantee @xmath207",
    ", we formulate each level as a binary source coding problem under the binary one - sided distortion constraint : @xmath208 . denoting the distortion at level @xmath48 as @xmath209 , an asymmetric test channel ( z - channel ) from @xmath210 to @xmath211",
    "can be constructed , where @xmath212 based on this , we have @xmath213 , and the achievable rate at a single level @xmath48 is given by @xmath214 due to the decomposability property as stated previously , the coding scheme considered is over a set of parallel discrete levels indexed by @xmath215 .",
    "thus , by adopting rate distortion limit achieving codes over each level , expansion coding scheme readily achieves the following result .",
    "[ thm : expsc_achievable_rate_z ] for an exponential source , expansion coding achieves the rate distortion pair given by _",
    "@xmath216 _ for any @xmath115 , and @xmath217 $ ] for @xmath218 , where @xmath104 is given by .",
    "see appendix  [ app : expsc_achievable_rate_z ] .",
    "note that , the last two terms in are a result of the truncation and vanish in the limit of large number of levels . in later parts of this section , we characterize the number of levels required in order to bound the resulting distortion within a constant gap .",
    "note that it is not necessary to make sure @xmath219 for every @xmath48 to guarantee @xmath207 . to this end",
    ", we introduce successive coding scheme , where encoding and decoding start from the highest level @xmath4 to the lowest . at a certain level , if all higher levels are encoded as equal to the source , then we must model this level as binary source coding with the one - sided distortion .",
    "otherwise , we formulate this level as binary source coding with the symmetric distortion ( see figure  [ fig : expsc_successive_coding ] for an illustration of this successive coding scheme ) .",
    "in particular , for the later case , the distortion of concern is hamming distortion , i.e. @xmath220 . denoting the equivalent distortion at level @xmath48 as @xmath209 , i.e. @xmath221=d_l$ ] , then the symmetric test channel from @xmath222 to @xmath50 is modeled as @xmath223 hence , the achievable rate at level @xmath48 is given by @xmath224 based on these , we have the following achievable result :    [ thm : expsc_achievable_rate_x ] for an exponential source , applying successive coding , expansion coding achieves the rate distortion pairs _",
    "@xmath225,\\label{equ : expsc_achievable_rate_r2}\\\\ d_2&=\\sum_{l =- l_1}^{l_2}2^ld_l+2^{-l_2 + 1}/\\lambda^2 + 2^{-l_1 - 1},\\label{equ : expsc_achievable_rate_d2}\\end{aligned}\\ ] ] _ for any @xmath115 , and @xmath217 $ ] for @xmath218 . here ,",
    "@xmath104 is given by , and the values of @xmath226 are determined by :    * for @xmath227 , @xmath228 * for @xmath229 , @xmath230    see appendix  [ app : expsc_achievable_rate_x ] .    in this sense ,",
    "the achievable pairs in both theorems are given by optimization problems over a set of parameters @xmath231 .",
    "however , the problems are not convex , so an effective theoretical analysis may not be performed here for the optimal solution .",
    "but , by a heuristic choice of @xmath209 , we can still get a good performance .",
    "inspired by the fact that the optimal scheme models noise as exponential with parameter @xmath201 in the test channel , we design @xmath209 as the expansion parameter from this distribution , i.e. , we consider @xmath232    we note that higher levels get higher priority and lower distortion with this choice , which is consistent with the intuition .",
    "this choice of @xmath209 may not guarantee any optimality , although simulation results imply that this can be an approximately optimal solution . in the following ,",
    "we show that the proposed expansion coding scheme achieves within a constant gap to the rate distortion function ( at each distortion value ) .",
    "[ thm : expsc_main_result ] for any @xmath233 $ ] , there exists a constant @xmath234 ( independent of @xmath157 and @xmath8 ) , such that if    * @xmath235 ; * @xmath236 ,    then , with a choice of @xmath209 as in , the achievable rates obtained from expansion coding schemes are both within @xmath237 bits gap to shannon rate distortion function , i.e. , @xmath238    see appendix  [ app : expsc_main_result ] .    we remark that the requirement for highest level is much more restricted than channel coding case .",
    "the reason is that number of levels should be large enough to approximate both rate and distortion . from the proof in appendix  [ app : expsc_main_result ]",
    ", it is evident that @xmath239 is enough to bound the rate , however , another @xmath240 is required to approximate the distortion closely .",
    "( if only the relative distortion is considered , these extra levels may not be essential . )",
    "expansion source coding can be also applied to other source statistics .",
    "for instance , for laplacian ( two - sided symmetric exponential ) sources , the proposed coding scheme can still approximately approach to the shannon rate distortion limit with a small constant gap @xcite , where the sign bit of laplacian is considered separately and encoded perfectly with @xmath89 bit , and each expanded level is encoded with hamming distortion , for low distortion regime .",
    "numerical results showing achievable rates along with the rate distortion limit are given in fig .",
    "[ fig : expsc_achievable_rate ] .",
    "it is evident that both forms of expansion coding perform within a constant gap of the limit over the whole distortion region , which outperforms existing scalar quantization technique , especially in the low distortion regime . since samples are independent , the simulations for vector quantization are expected to be close to scalar quantization and omitted in this result .",
    "theorem  [ thm : expsc_main_result ] shows that this gap is bounded by a constant . here , numerical results show that the gap is not necessarily as wide as predicted by the analysis . especially for the low distortion region ,",
    "the gap is numerically found to correspond to @xmath241 bits and @xmath242 bits for each coding scheme respectively .    ) with one - sided error distortion .",
    "* @xmath179 ( red - solid ) is rate distortion limit ; @xmath243 ( purple ) is given by theorem  [ thm : expsc_achievable_rate_z ] ; @xmath244 ( blue ) is given by theorem  [ thm : expsc_achievable_rate_x ] .",
    "linear and non - linear scalar quantization methods are simulated for comparison . ]",
    "in this paper , the method of expansion coding is proposed to construct good codes for analog channel coding and source coding . with a perfect or approximate decomposition of channel noise and original sources , we consider coding over independent parallel representations , thus providing a foundation for reducing the original problems to a set of parallel simpler subproblems .",
    "in particular , via expansion channel coding , we consider coding over @xmath3-ary channels for each expansion level .",
    "this approximation of the original channel together with capacity achieving codes for each level ( to reliably transmit messages over each channel constructed ) and gallager s method ( to achieve desired communication rates for each channel ) allow for constructing near - capacity achieving codes for the original channel .",
    "similarly , we utilize expansion source coding to adopt discrete source codes that achieve the rate distortion limit on each level after expansion , and design codes achieving near - optimal performance .",
    "theoretical analysis and numerical results are provided to detail performance guarantees of the proposed expansion coding scheme .",
    "one significant benefit from expansion coding is coding complexity . as indicated in theoretical analysis , approximately @xmath245 and @xmath246 number of levels are sufficient for the channel coding and source coding schemes respectively .",
    "thus , by choosing `` good '' low complexity optimal codes within each level ( such as polar codes @xcite ) , the overall complexity of the coding scheme can be made small for the original continuous - valued channel coding and source coding problems .",
    "although the discussion in this paper focuses on aen channels as well as exponential sources , expansion coding scheme is a more general framework and its applications are not limited to such scenarios . towards this end , any channel noise and any source with decomposable distribution",
    "could fit into the range of expansion coding .",
    "moreover , the idea of expansion could also be generalized to network information theory , where it can play a role similar to deterministic models @xcite .",
    "the expanded channels are not completely deterministic in our case ; they possess different noise levels , which may enable the construction of precise models for network analysis .",
    "10 [ 1]#1 url@samestyle [ 2]#2 [ 2]l@#1=l@#1#2    c.  e. shannon , `` a mathematical theory of communication , '' _ bell system technical journal _ , vol .  27 , no .  3 , pp .",
    "379423 , jul .",
    "r.  w. hamming , `` error detecting and error correcting codes , '' _ bell system technical journal _ , vol .",
    "29 , no .  2 , pp . 147160 , apr . 1950 .",
    "m.  j.  e. golay , `` notes on digital coding , '' _ proceeding of the institute of radio engineers _ , vol .  37 , no .  6 , pp . 657657 , jun .",
    "d.  e. muller , `` application of boolean algebra to switching circuit design and to error detection , '' _ ire transactions on electronic computers _",
    ", vol .  3 , no .  3 , pp .",
    "612 , sep .",
    "1954 .    i.  s. reed , `` a class of multiple - error - correcting codes and the decoding scheme , '' _ ire transactions on information theory _ ,",
    "vol .  4 , no .  4 , pp .",
    "3849 , sep .",
    "i.  s. reed and g.  solomon , `` polynomial codes over certain finite fields , '' _ journal of the society for industrial & applied mathematics _ ,",
    "vol .  8 , no .  2 , pp .",
    "300304 , jun . 1960 .",
    "j.  h. conway and n.  j.  a. sloane , _ sphere packings , lattices and groups_.1em plus 0.5em minus 0.4emnew york :  springer , 1988 .",
    "f.  j. mcwilliams and n.  j.  a. sloane , _ the theory for error - correcting codes_.1em plus 0.5em minus 0.4emnorth - holland , 1983 .    j.  g.  d.  forney , `` convolutional codes i : algebraic structure , '' _ ieee transactions on information theory _ ,",
    "16 , no .  6 , pp .",
    "720738 , nov .",
    " , _ concatenated codes_.1em plus 0.5em minus 0.4em cambridge : mit press , 1966 .",
    "c.  berrou and a.  glavieux , `` near optimum error correcting coding and decoding : turbo - codes , '' _ ieee transactions on communications _ , vol .",
    "44 , no .",
    "10641070 , oct .",
    "d.  j.  c. mackay and r.  m. neal , `` good codes based on very sparse matrices , '' in _ cryptography and coding_.1em plus 0.5em minus 0.4em springer , 1995 , pp .",
    "100111 .",
    "m.  sipser and d.  a. spielman , `` expander codes , '' _ ieee transactions on information theory _ , vol .",
    "42 , no .  6 , pp .",
    "17101722 , nov .",
    "e.  arikan , `` channel polarization : a method for constructing capacity - achieving codes for symmetric binary - input memoryless channels , '' _ ieee transactions on information theory _ ,",
    "55 , no .  7 ,",
    "pp . 30513073 , jul .",
    "j.  perry , h.  balakrishnan , and d.  shah , `` rateless spinal codes , '' in _ proc . of the 10th acm workshop on hot topics in networks ( hotnets 2011 ) _ , new york city , new york , usa , nov . 2011 , pp .",
    "h.  balakrishnan , p.  iannucci , d.  shah , and j.  perry , `` de - randomizing shannon : the design and analysis of a capacity - achieving rateless code , '' _ arxiv:1206.0418 _ , jun . 2012 .",
    "t.  m. cover and j.  a. thomas , _ elements of information theory_.1em plus 0.5em minus 0.4emjohn wiley & sons , 1991 .",
    "a.  j. viterbi and j.  k. omura , `` trellis encoding of memoryless discrete - time sources with a fidelity criterion , '' _ ieee transactions on information theory _",
    "20 , no .  3 , pp .",
    "325332 , may 1974 .",
    "y.  matsunaga and h.  yamamoto , `` a coding theorem for lossy data compression by ldpc codes , '' _ ieee transactions on information theory _",
    "49 , no .  9 , pp . 22252229 , sep .",
    "2003 .    m.  j. wainwright , e.  maneva , and e.  martinian , `` lossy source compression using low - density generator matrix codes : analysis and algorithms , '' _ ieee transactions on information theory _",
    "56 , no .  3 , pp .",
    "13511368 , mar .",
    "s.  b. korada and r.  l. urbanke , `` polar codes are optimal for lossy source coding , '' _ ieee transactions on information theory _ , vol .  56 , no .  4 , pp . 17511768 , apr .",
    "r.  zamir , `` lattices are everywhere , '' in _ proc .",
    "2009 information theory and applications workshop ( ita 2009 ) _ , san diego , california , usa , feb .",
    "2009 , pp . 392421 .",
    "j.  d.  j.  costello and j.  g.  d.  forney , `` channel coding : the road to channel capacity , '' _ proceedings of the ieee _ , vol .",
    "95 , no .  6 , pp . 11501177 , jun .",
    "2007 .    j.  g.  d.  forney , m.  d. trott , and s.  y. chung , `` sphere - bound - achieving coset codes and multilevel coset codes , '' _ ieee transactions on information theory _",
    "46 , no .  3 , pp .",
    "820850 , may 2000 .",
    "a.  avestimehr , s.  diggavi , and d.  tse , `` wireless network information flow : a deterministic approach , '' _ ieee transactions on information theory _",
    "57 , no .  4 , pp . 18721905 , apr . 2011 .",
    "e.  abbe and a.  barron , `` polar coding schemes for the awgn channel , '' in _ proc .",
    "2011 ieee international symposium on information theory ( isit 2011 ) _ , st .  petersburg , russia , jul .",
    "2011 , pp . 194198 .",
    "e.  abbe and e.  telatar , `` polar codes for the @xmath247-user multiple access channel , '' _ ieee transactions on information theory _ , vol .",
    "58 , no .  8 , pp . 54375448 , aug .",
    "m.  seidl , a.  schenk , c.  stierstorfer , and j.  b. huber , `` polar - coded modulation , '' _ ieee transactions on communications _ , vol .",
    "61 , no .",
    "41084119 , oct .",
    "s.  verd , `` the exponential distribution in information theory , '' _ problems of information transmission _ ,",
    "32 , no .  1 ,",
    "100111 , jan .",
    "a.  martinez , `` communication by energy modulation : the additive exponential noise channel , '' _ ieee transactions on information theory _ , vol .",
    "57 , no .  6 , pp . 33333351 , jun .",
    "s.  y. le  goff , `` capacity - approaching signal constellations for the additive exponential noise channel , '' _ ieee wireless communications letters _ , vol .  1 , no .  4 , pp .",
    "320323 , aug .",
    "r.  g. gallager , _ information theory and reliable communication_.1em plus 0.5em minus 0.4emjohn wiley & sons , 1968 .",
    "r.  m. gray and d.  l. neuhoff , `` quantization , '' _ ieee transactions on information theory _ , vol .",
    "44 , no .  6 , pp . 23252383 ,",
    "oct . 1998 .",
    "d.  baron and t.  weissman , `` an mcmc approach to universal lossy compression of analog sources , '' _ ieee transactions on signal processing _ ,",
    "60 , no .",
    "52305240 , oct .",
    "h.  si , o.  o. koyluoglu , and s.  vishwanath , `` lossy compression of exponential and laplacian sources using expansion coding , '' in _ proc .",
    "2014 ieee international symposium on information theory ( isit 2014 ) _ , honolulu , hawaii , usa , jul .",
    "2014 , pp . 30523056 .",
    "g.  marsaglia , `` random variables with independent binary digits , '' _ the annals of mathematical statistics _ ,",
    "42 , no .  6 , pp .",
    "19221929 , dec . 1971 .",
    "the `` if '' part follows by extending the one given in @xcite , which considers the expansion of a truncated exponential random variable . we show the result by calculating the moment generating function of @xmath78 . using the assumption that @xmath248 are mutually independent , we have @xmath249                  = \\prod_{l=-\\infty}^{\\infty}\\mathbb{e}\\left[e^{t2^l { \\mathsf{b}}_l}\\right].\\nonumber\\end{aligned}\\ ] ] noting that @xmath74 are bernoulli random variables , we have @xmath250= \\frac{e^{t2^l}}{1+e^{\\lambda 2^l}}+\\left(1-\\frac{1}{1+e^{\\lambda 2^l}}\\right ) = \\frac{1+e^{(t-\\lambda ) 2^l}}{1+e^{-\\lambda 2^l}}.\\nonumber\\end{aligned}\\ ] ] then , using the fact that for any constant @xmath251 , @xmath252 we can obtain the following for @xmath253 , @xmath254 = \\lim_{n\\rightarrow\\infty}\\prod_{l=0}^{n}\\frac{1+e^{(t-\\lambda ) 2^l}}{1+e^{-\\lambda 2^l } } = \\frac{1-e^{-\\lambda}}{1-e^{t-\\lambda}}.\\label{equ : exponential_expansion_proof_part1}\\end{aligned}\\ ] ] similarly , for the negative part , we have @xmath255 which further implies that @xmath256 = \\lim_{n\\rightarrow\\infty}\\frac{1-e^{t-\\lambda } } { 1-e^{(t-\\lambda)2^{-n}}}\\frac{1-e^{-\\lambda2^{-n}}}{1-e^{-\\lambda } } = \\frac{\\lambda(1-e^{t-\\lambda})}{(\\lambda - t)(1-e^{-\\lambda})}. \\label{equ : exponential_expansion_proof_part2}\\end{aligned}\\ ] ]    thus , finally for any @xmath253 , combining equations   and  , we get @xmath257 the observation that this is the moment generation function for an exponentially distributed random variable with parameter @xmath157 concludes the proof .    the independence relationships between levels in `` only if '' part can be simply verified using memoryless property of the exponential distribution . here",
    ", we just need to show the parameter for bernoulli random variable at each level .",
    "observe that for any @xmath258 , @xmath259 using cdf of exponential distribution , we obtain @xmath260 using this in , we have @xmath261      from , and @xmath135 , we have @xmath262 by definition of entropy , we obtain @xmath263    when @xmath264 , we obtain a lower bound as @xmath265 where @xmath266 is due to @xmath267 and @xmath268 .    on the other hand , when @xmath269 , we have @xmath270 where    * is from @xmath271 and @xmath272 ; * is from @xmath273 for any @xmath274 ; * is from @xmath275 for any @xmath276 ; * is from @xmath277 and @xmath278 for any @xmath48 and @xmath279 .      by definition ,",
    "@xmath280 , so its behavior is closely related to carries .",
    "note that for any @xmath48 , we have @xmath281 where the last inequality holds due to @xmath282 and @xmath283 .",
    "then , for @xmath284 , we have @xmath285 where the first inequality holds due to monotonicity of entropy , and the last inequality is due to in lemma  [ lem : aen_entropy_bound ] . for the @xmath286 part , we need to characterize carries first .",
    "we have the following assertion : @xmath287 and the proof is based on the following induction analysis . for @xmath288 ,",
    "this is simply true , because @xmath289 for any @xmath48 .",
    "assume is true for level @xmath290 , then at the @xmath291 level , we have @xmath292 where    * is due to @xmath293 and @xmath294 ; * is due to the assumption for level @xmath48 ; * is due to the fact that @xmath295>2/(1+e^{2^{{l-\\eta}+1}})$ ] holds for any @xmath286 . to this end , the assertion also holds for level @xmath296 , and this completes the proof of .    using",
    ", we obtain that for any @xmath286 @xmath297 where the last inequality holds due to @xmath298 for any @xmath286 .",
    "finally , we obtain @xmath299 where    * is from and the monotonicity of entropy ; * is from @xmath300 for any @xmath301 ; * is from @xmath302 for any @xmath286 .    from the proof ,",
    "the information we used for @xmath104 is that @xmath293 , so this bound holds uniformly for any snr .",
    "we first prove that @xmath155 achieves capacity .",
    "denote @xmath303 and @xmath304 .",
    "then , we have an important observation that @xmath305 which shows that channel input is a shifted version of noise with respect to expansion levels ( see fig .",
    "[ fig : aen_expanded_level ] for intuition ) . based on this , we have @xmath306\\nonumber\\\\              & \\stackrel{(a)}{\\geq}\\sum_{l =- l_1}^{l_2}\\left [ h(p_l)-h(q_l)\\right]\\nonumber\\\\              & \\stackrel{(b)}{=}\\sum_{l =- l_1}^{l_2}\\left [ h(q_{l+\\eta-\\xi})-h(q_l)\\right]\\nonumber\\\\              & = \\sum_{l =-",
    "l_1+\\eta-\\xi}^{l_2+\\eta-\\xi}h(q_l ) -\\sum_{l =- l_1}^{l_2}h(q_l)\\nonumber\\\\              & = \\sum_{l =- l_1+\\eta-\\xi}^{-l_1 - 1}h(q_l ) -\\sum_{l = l_2+\\eta-\\xi+1}^{l_2}h(q_l)\\nonumber\\\\              & \\stackrel{(c)}{\\geq}\\sum_{l =- l_1+\\eta-\\xi}^{-l_1 - 1}\\left[1 - 2^{l-\\eta}\\log e \\right ] -\\sum_{l = l_2+\\eta-\\xi+1}^{l_2}2^{\\eta - l}3\\log e\\nonumber\\\\              & \\stackrel{(d)}{\\geq}(\\xi-\\eta)-2^{-l_1-\\eta}\\log e -2^{-l_2+\\xi}3\\log e \\nonumber\\\\              & \\stackrel{(e)}{\\geq}\\log \\left(\\frac{e_{\\mathsf{x}}}{e_{\\mathsf{z}}}\\right)-\\epsilon\\log e-3\\epsilon\\log e\\epsilon\\nonumber\\\\              & \\stackrel{(f)}{\\geq}\\log \\left(1+\\frac{e_{\\mathsf{x}}}{e_{\\mathsf{z}}}\\right)-\\frac{e_{\\mathsf{z}}}{e_{\\mathsf{x}}}\\log e-4\\epsilon\\log e\\nonumber\\\\              & \\stackrel{(g)}{\\geq}\\log \\left(1+\\frac{e_{\\mathsf{x}}}{e_{\\mathsf{z}}}\\right)-5\\epsilon\\log e,\\label{equ : aen_rate_gap_proof2}\\end{aligned}\\ ] ] where    * is due to @xmath307 , and monotonicity of entropy ; * follows from ; * follows from and in lemma  [ lem : aen_entropy_bound ] ; * holds as @xmath308 and @xmath309 * is due to the assumptions that @xmath310 , and @xmath311 ; * is due to the fact that @xmath312 as @xmath313 for any @xmath314 ; * is due to the assumption that @xmath315 .",
    "next , we show the result for @xmath154 . observe that @xmath316\\nonumber\\\\              & \\stackrel{(h)}{\\geq } \\sum_{l =- l_1}^{l_2}\\left [ h(p_l\\otimes q_l)-h(\\tilde{q}_l ) \\right]\\nonumber\\\\              & = \\sum_{l =- l_1}^{l_2}\\left [ h(p_l\\otimes q_l)-h(q_l ) \\right]+\\sum_{l =- l_1}^{l_2}\\left [ h(q_l)-h(\\tilde{q}_l ) \\right]\\nonumber\\\\              & = \\hat{r}_2-\\sum_{l =- l_1}^{l_2}\\left [ h(\\tilde{q}_l)-h(q_l ) \\right]\\nonumber\\\\              & \\stackrel{(i)}{\\geq}\\hat{r}_2-\\sum_{-l_1}^{\\eta}\\left[1-\\left(1 - 2^{l-\\eta}\\log e \\right)\\right]-\\sum_{\\eta+1}^{l_2}\\left[6(l-\\eta)2^{-l+\\eta}\\log e-0\\right]\\nonumber\\\\              & = \\hat{r}_2-\\sum_{-l_1}^{\\eta } 2^{l-\\eta } \\log",
    "e -\\sum_{\\eta+1}^{l_2}6(l-\\eta)2^{-l+\\eta}\\log e\\nonumber\\\\              & \\stackrel{(j)}{\\geq}\\hat{r}_2- 2\\log e -12\\log e\\nonumber\\\\              & \\stackrel{(k)}{\\geq}\\log \\left(1+\\frac{e_{\\mathsf{x}}}{e_{\\mathsf{z}}}\\right)-5\\epsilon\\log e-14\\log e,\\nonumber\\end{aligned}\\ ] ] where    * is due to @xmath317 , which further implies @xmath318 ; * follows from and , together with the fact that @xmath319 and @xmath320 for any @xmath48 ; * follows from the observations that @xmath321 and @xmath322 * is due to .",
    "thus , choosing @xmath323 completes the proof .",
    "note that , in the course of providing these upper bounds , the actual gap might be enlarged .",
    "the actual value of the gap is much smaller ( e.g. , as shown in fig .",
    "[ fig : aen_achievable_rate ] , numerical result for the capacity gap is around @xmath324 bits ) .",
    "note that the maximum entropy theorem implies that the distribution maximizing differential entropy over all probability densities @xmath325 on support set @xmath326 satisfying @xmath327 is exponential distribution with parameter @xmath157 . based on this result , in order to satisfy @xmath328\\leq d$ ] , where @xmath329 for @xmath330 , we have to restrict @xmath331 with probability @xmath89 . to this end",
    ", we have @xmath332)\\nonumber\\\\              & \\geq \\log(e/\\lambda)-\\log(ed)\\nonumber\\\\              & = -\\log ( \\lambda d).\\nonumber\\end{aligned}\\ ] ]    to achieve this bound , we need @xmath333 to be exponentially distributed and independent with @xmath200 as well . accordingly , we can consider a test channel from @xmath200 to @xmath182 with additive noise @xmath334 distributed as exponential with parameter @xmath201 , which gives the conditional distribution given by ( [ equ : expsc_optimal_conditional_distribution ] ) .",
    "due to decomposability of exponential distribution , the levels after expansion are independent , hence , the achievable rate in this theorem is obtained by additions of individual rates . on the other hand , for the calculation of distortion , we have @xmath335\\nonumber\\\\ & = \\mathbb{e}\\left[\\sum_{l=-\\infty}^{\\infty}2^l\\mathsf{x}_l-\\sum_{l =- l_1}^{l_2}2^l\\tilde{\\mathsf{x}}_l\\right]\\nonumber\\\\      & \\overset{(a)}{=}\\sum_{l =- l_1}^{l_2}2^ld_l + \\sum_{l = l_2 + 1}^{\\infty } 2^lp_l+\\sum_{l=-\\infty}^{-l_1 - 1}2^lp_l\\nonumber\\\\      & \\overset{(b)}{\\leq } \\sum_{l =- l_1}^{l_2}2^ld_l + \\sum_{l = l_2 + 1}^{\\infty } 2^{-l+1}/\\lambda^2 + \\sum_{l=-\\infty}^{-l_1 - 1}2^{l-1}\\nonumber\\\\      & \\overset{(c)}{= } \\sum_{l =- l_1}^{l_2}2^ld_l + 2^{-l_2 + 1}/\\lambda^2 + 2^{-l_1 - 1},\\nonumber\\end{aligned}\\ ] ] where    * follows from @xmath213 ; * follows from @xmath336 and @xmath293 for any @xmath48 ; * follows from @xmath337",
    "by the design of coding scheme , if all higher levels are decoded as equivalence , then they must be encoded with one - sided distortion . recall that for z - channel , we have @xmath338 hence , due to independence of expanded levels , @xmath339 then , at each level , the achievable rate is @xmath340 with probability @xmath226 and is @xmath341 otherwise . from this",
    ", we obtain the expression of @xmath342 given by the theorem . on the other hand , since in both cases we have @xmath213 , the form of distortion remains the same .",
    "denote @xmath343 , and @xmath344 . then , from @xmath345 , we have @xmath346 by noting that @xmath104 and @xmath209 are both expanded parameters from exponential distribution , we have @xmath347 hence , @xmath104 is shifted version of @xmath209 ( analog to the channel coding case ) , i.e. , @xmath348 using this relationship , we obtain @xmath349 & \\overset{(a)}{=}\\sum_{l =- l_1}^{l_2}h(p_l)-\\sum_{l =- l_1}^{l_2}h(p_{l+\\gamma+\\xi})\\nonumber\\\\ & = \\sum_{l =- l_1}^{l_2}h(p_l)-\\sum_{l =- l_1+\\gamma+\\xi}^{l_2+\\gamma+\\xi}h(p_{l})\\nonumber\\\\ & \\overset{(b)}{=}\\sum_{l =- l_1}^{-l_1+\\gamma+\\xi-1}h(p_l)-\\sum_{l = l_2 + 1}^{l_2+\\gamma+\\xi}h(p_{l})\\nonumber\\\\ & \\overset{(c)}{\\leq } \\gamma+\\xi,\\label{equ : expsc_main_proof3}\\end{aligned}\\ ] ] where      from the expression of @xmath351 , we have @xmath352\\nonumber\\\\ & = \\sum_{l =- l_1}^{l_2}\\left[h(p_l)-h(d_l)\\right ] + \\sum_{l =- l_1}^{l_2 } \\left[h(d_l)-(1-p_l+d_l)h\\left(\\frac{d_l}{1-p_l+d_l}\\right)\\right]\\nonumber\\\\ & \\leq \\gamma+\\xi + \\sum_{l =- l_1}^{l_2 } \\left[h(d_l)-(1-p_l+d_l)h\\left(\\frac{d_l}{1-p_l+d_l}\\right)\\right],\\label{equ : expsc_main_proof4}\\end{aligned}\\ ] ] where is used to obtain the last inequality . it remains to bound @xmath353 for this , two cases are considered :    1 .   for @xmath354 , @xmath209 and @xmath104",
    "are close and both tend to @xmath86 .",
    "more precisely , we have @xmath355\\log e \\nonumber\\\\                  & = 2^{l+\\xi}\\log e   , \\label{equ : expsc_main_proof5 }      \\end{aligned}\\ ] ] where * follows from the fact that @xmath356 is a decreasing function over @xmath357 $ ] , hence , @xmath358 ; * follows from the observation that @xmath359 for any @xmath360 $ ] ; * follows from the fact that @xmath361 and @xmath362 where the first inequality is due to @xmath363 for any @xmath364 $ ] ( @xmath365 due to @xmath354 ) , and the last inequality holds for any @xmath48 .",
    "2 .   on the other hand , for @xmath366",
    ", @xmath209 tends to @xmath73 , so as @xmath367 and @xmath368 get close .",
    "more precisely , we have @xmath369 where * follows from the fact @xmath370 ; * follows from the observation that @xmath371 for any @xmath360 $ ] ; * follows from the fact that @xmath372 where the second inequality holds from @xmath373 for any @xmath374 ( @xmath375 due to @xmath366 ) .",
    "* follows from @xmath179 is convex such that for any @xmath382 and @xmath383 , @xmath384 where @xmath385 is the derivative , and setting @xmath386 , @xmath387 completes the proof of this step ; * follows from ; * follows from theorem assumptions that @xmath235 and @xmath388 ."
  ],
  "abstract_text": [
    "<S> a general method of coding over expansion is proposed , which allows one to reduce the highly non - trivial problems of coding over analog channels and compressing analog sources to a set of much simpler subproblems , coding over discrete channels and compressing discrete sources . </S>",
    "<S> more specifically , the focus of this paper is on the additive exponential noise ( aen ) channel , and lossy compression of exponential sources . taking advantage of the essential decomposable property of these channels ( sources ) </S>",
    "<S> , the proposed expansion method allows for mapping of these problems to coding over parallel channels ( respectively , sources ) , where each level is modeled as an independent coding problem over discrete alphabets . </S>",
    "<S> any feasible solution to the resulting optimization problem after expansion corresponds to an achievable scheme of the original problem . utilizing this mapping , </S>",
    "<S> even for the cases where the optimal solutions are difficult to characterize , it is shown that the expansion coding scheme still performs well with appropriate choices of parameters . </S>",
    "<S> more specifically , theoretical analysis and numerical results reveal that expansion coding achieves the capacity of aen channel in the high snr regime . </S>",
    "<S> it is also shown that for lossy compression , the achievable rate distortion pair by expansion coding approaches to the shannon limit in the low distortion region . </S>",
    "<S> remarkably , by using capacity - achieving codes with low encoding and decoding complexity that are originally designed for discrete alphabets , for instance polar codes , the proposed expansion coding scheme allows for designing low - complexity analog channel and source codes . </S>"
  ]
}