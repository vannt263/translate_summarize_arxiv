{
  "article_text": [
    "empirical studies have shown that learning multiple tasks ( mtl ) simultaneously often provides superior predictive performance relative to learning each tasks @xcite .",
    "this observation also has solid theoretical foundations @xcite , especially when the training sample size is small for each task",
    ". one popular mtl method especially for high - dimensional data is multi - task feature learning ( mtfl ) , which uses the group lasso penalty to ensure that all tasks select a common set of features @xcite .",
    "mtfl has found great success in many real - world applications including but not limited to : breast cancer classification @xcite , disease progression prediction @xcite , gene data analysis @xcite , and neural semantic basis discovery @xcite .",
    "a major issue in mtfl  that is of great practical importance  is to develop efficient solvers @xcite .",
    "however , it remains challenging to apply the mtfl models to large - scale problems .",
    "the idea of _ screening _ has been shown to be very effective in scaling the data and improving the efficiency of many popular sparse models , e.g. , lasso @xcite , nonnegative lasso @xcite , group lasso @xcite , mixed - norm regression @xcite , @xmath0-regularized logistic regression @xcite , sparse - group lasso @xcite , support vector machine ( svm ) @xcite , and least absolute deviations ( lad ) @xcite . essentially , screening aims to quickly identify the zero components in the solution vectors such that the corresponding features  called _ inactive features _",
    "( e.g. , lasso)or data samples  called _ non - support vectors _",
    "( e.g. , svm)can be removed from the optimization .",
    "therefore , the size of the data matrix and the number of variables to be computed can be significantly reduced , which may lead to substantial savings in the computational cost and memory usage _ without sacrificing accuracy_. compared to the solvers without screening , the speedup gained by the screening methods can be several orders of magnitude .    however , we note that all the existing screening are only applicable to sparse models with a _ single _ data matrix .",
    "therefore , motivated by the challenges posed by large - scale data and the promising performance of existing screening methods , we propose a novel framework for developing effective and efficient screening rules for a popular mtfl model via the * * d**ual * * p**rojection onto * * c**onvex sets ( dpc ) .",
    "the framework of dpc extends the state - of - the - art screening rule , called edpp @xcite , for the lasso problem @xcite  that assumes a _ single _ data matrix  to a popular mtfl model  that involves _ multiple _ data matrices across different tasks . to the best of our knowledge",
    ", dpc is the _ first _ screening rule that is applicable to sparse models with _ multiple _",
    "data matrices .",
    "the dpc screening rule detects the inactive features by maximizing a convex function over a convex set containing the dual optimal solution , which is a nonconvex problem . to find the region containing the dual optimal solution",
    ", we show that the corresponding dual problem can be formulated as a _ projection _ problem  which admits many desirable geometric properties  by utilizing the of the _ inner product_. then , by a carefully chosen parameterization of the constraint set , we transform the nonconvex problem to a quadratic programming problem over one quadratic constraint ( qp1qc ) @xcite , which can be solved for the global optimum efficiently .",
    "experiments on both synthetic and real data sets indicate that the speedup gained by dpc can be orders of magnitude .",
    "moreover , shows better performance as the feature dimension increases , which makes it a very competitive candidate for the applications of very high - dimensional data .",
    "we organize the rest of this paper as follows . in section [ section : basics ] , we briefly review some basics of a popular mtfl model .",
    "then , we derive the dual problem in section [ section : mtfl_dual ] . based on an indepth analysis of the geometric properties of the dual problem and the dual feasible set , we present the proposed dpc screening rule in section [ section : dpc ] . in section",
    "[ section : experiments ] , we evaluate the dpc rule on both synthetic and real data sets .",
    "we conclude this paper in section [ section : conclusion ] .",
    "please refer to the supplement for proofs not included in the main text .",
    "* notation * : denote the @xmath1 norm by @xmath2 . for @xmath3 , let its @xmath4 component be @xmath5 , and the diagonal matrix with the entries of @xmath6 on the main diagonal be @xmath7 . for a set of positive integers @xmath8 , we denote the @xmath9 subvector of @xmath10 by @xmath11 such that @xmath12 , where @xmath13 for @xmath14 . for vectors",
    "@xmath15 , we use @xmath16 and @xmath17 interchangeably to denote the inner product . for a matrix @xmath18 ,",
    "let @xmath19 , @xmath20 , and @xmath21 be its @xmath4 row , @xmath22 column and @xmath23 entry , respectively .",
    "we define the @xmath24-norm of @xmath25 by @xmath26 . for two matrices @xmath27",
    ", we define their inner product by @xmath28 .",
    "let @xmath29 be the identity matrix .",
    "for a convex function @xmath30 , let @xmath31 be its subdifferential .",
    "for a vector @xmath32 and a convex set @xmath33 , the _ projection operator _ is :    @xmath34",
    "in this section , we briefly review some basics of a popular mtfl model and mention several equivalent formulations .",
    "suppose that we have @xmath35 learning tasks @xmath36 , where @xmath37 is the data matrix of the @xmath9 task with @xmath38 samples and @xmath39 features , and @xmath40 is the corresponding response vector . a widely used mtfl model @xcite takes the form of @xmath41 where @xmath42 is the weight vector of the @xmath9 task and @xmath43 . because the @xmath44-norm induces on the rows of @xmath45 , the weight vectors across all tasks share the same sparse pattern .",
    "we note that the model in ( [ prob : mtfl ] ) is equivalent to several other popular mtfl models .",
    "the first example introduces a positive weight parameter @xmath46 for @xmath14 to each term in the loss function : @xmath47 which reduces to ( [ prob : mtfl ] ) by setting @xmath48 and @xmath49 .",
    "the second example introduces another regularizer to ( [ prob : mtfl ] ) : @xmath50 where @xmath51 is a positive parameter and @xmath52 is the frobenius norm .",
    "let @xmath53 be the identity matrix and @xmath54 be the @xmath39-dimensional vector with all zero entries . by letting @xmath55",
    "we can also simplify the above mtfl model to ( [ prob : mtfl ] ) .    in this paper , we focus on developing the dpc screening rule for the mtfl model in ( [ prob : mtfl ] ) .",
    "in this section , we show that we can formulate the dual problem of the mtfl model in ( [ prob : mtfl ] ) as a projection problem by utilizing the bilinearity of the inner product .",
    "we first introduce a new set of variables : @xmath56 then , the mtfl model in ( [ prob : mtfl ] ) can be written as @xmath57 let @xmath58 be the vector of lagrangian multipliers . then , the lagrangian of ( [ prob : mtfl ] ) is @xmath59 to get the dual problem , we need to minimize @xmath60 over @xmath45 and @xmath61 .",
    "we can see that @xmath62 for notational convenience , let @xmath63 thus , to minimize @xmath64 with respect to @xmath45 , it is to minimize @xmath65 , i.e. , @xmath66 by the _ bilinearity of the inner product _ , we can decouple @xmath65 into a set of independent subproblems . indeed , we can rewrite the second term of @xmath65 as @xmath67 where @xmath68 . eq .",
    "( [ eqn : ipc ] ) expresses @xmath69 by the sum of the inner products of the corresponding columns . by the bilinearity of the inner product , we can also express @xmath70 by the sum of the inner products of the corresponding rows : @xmath71 denote the @xmath22 column of @xmath72 by @xmath73 .",
    "we can see that @xmath74 moreover , as @xmath75 , eqs .",
    "( [ eqn : ipr ] ) implies that : @xmath76 where @xmath77 .",
    "thus , to minimize @xmath65 , we can minimize each @xmath78 .",
    "the subdifferential counterpart of the fermat s rule @xcite , i.e. , @xmath79 , : @xmath80 where @xmath81 is the minimizer of @xmath82 .",
    "we note that eq .",
    "( [ eqn : mtfl_minf2_fermat ] ) implies @xmath83 .",
    "if this is not the case , then @xmath84 is not lower bounded ( see the supplements for discussions ) , i.e. , @xmath85 .",
    "thus , by eqs .",
    "( [ eqn : mtfl_minf1 ] ) and ( [ eqn : mtfl_minf2_fermat ] ) , the dual function is @xmath86 maximizing @xmath87 yields the dual problem of ( [ prob : mtfl ] ) as follows : @xmath88 it is evident that the problem in ( [ prob : mtfl_dual_max ] ) is equivalent to @xmath89 in view of ( [ prob : mtfl_dual ] ) , it is indeed a projection problem .",
    "let @xmath90 be the feasible set of ( [ prob : mtfl_dual ] ) .",
    "then , the optimal solution of ( [ prob : mtfl_dual ] ) , denoted by @xmath91 , is the projection of @xmath92 onto @xmath90 , namely , @xmath93",
    "in this section , we present the proposed dpc screening rule for the mtfl model in ( [ prob : mtfl ] ) . inspired by the karush - kuhn - tucker ( kkt ) conditions @xcite , in section [ subsection : mtfl_guideline_dpc ] , we first present the general guidelines .",
    "the most challenging part lies in two folds : 1 ) we need to estimate the dual optimal solution as accurately as possible ; 2 ) we need to solve a nonconvex optimization problem . in section [",
    "subsection : estimation ] , we give an accurate estimation of the dual optimal solution based on the geometric properties of the projection operators .",
    "then , in section [ subsection : nonconvex ] , we show that we can efficiently solve for the global optimum to the nonconvex problem .",
    "we present the dpc rule for the mtfl model ( [ prob : mtfl ] ) in section [ subsection : dpc ] .",
    "we present the general guidelines to develop screening rules for the mtfl model ( [ prob : mtfl ] ) via the kkt conditions .",
    "let @xmath94 be the optimal solution ( [ prob : mtfl ] ) . by eqs .",
    "( [ eqn : auxiliary_variable ] ) , ( [ eqn : mtfl_minf1 ] ) and ( [ eqn : mtfl_minf2_fermat ] ) , the kkt conditions are : @xmath95,\\hspace{1mm}{\\rm if}\\,({\\bf w}^{\\ell})^*(\\lambda)=0 ,      \\end{dcases}\\ell=1,\\ldots , d.\\end{aligned}\\ ] ] where @xmath96 is the @xmath97 row of @xmath98 , and @xmath99 for @xmath100 , eq .",
    "( [ eqn : mtfl_kkt2 ] ) yields @xmath101 the rule in ( [ rule ] ) provides a method to identify the rows in @xmath98 that have _ only _ zero entries .",
    "however , ( [ rule ] ) is not applicable to real applications , as it assumes knowledge of @xmath91 , and solving the dual problem ( [ prob : mtfl_dual ] ) could be as expensive as solving the primal problem ( [ prob : mtfl ] ) .",
    "inspired by safe @xcite , we can first estimate a set @xmath102 that contains @xmath91 , and then relax ( [ rule ] ) as follows : @xmath103 therefore , to develop a screening rule for the mtfl model in ( [ prob : mtfl ] ) , ( [ rule * ] ) implies that : 1 ) we need to estimate a region @xmath102that turns out to be a ball ( please refer to section [ subsection : estimation])containing @xmath91 ; 2 ) we need to solve the maximization problem  that out to be nonconvex ( please refer to section [ subsection : nonconvex])on the left hand side of ( [ rule * ] ) .      based on the geometric properties of the dual problem ( [ prob : mtfl_dual ] ) that is a projection problem , we first derive the closed form solutions of the primal and dual problems for values of @xmath104 in section [ subsubsection : mtfl_closed_form_solution ] , and then give an accurate estimation of @xmath91 for the general cases in section [ subsubsection : mtfl_estimation_general ] .      the primal and dual optimal solutions @xmath98 and @xmath91 are generally unknown .",
    "however , when the value of @xmath104 is sufficiently large , we expect that @xmath105 , and @xmath106 by eq .",
    "( [ eqn : mtfl_kkt1 ] ) .",
    "the following theorem confirms this .",
    "[ thm : mtfl_primal_dual_closed_form ] for the mtfl model in ( [ prob : mtfl ] ) , let @xmath107 then , the following statements are equivalent :    @xmath108 .    [ remark : mtfl_lambda_range ] theorem indicates that : both the primal and optimal solutions of the mtfl model admit closed form solutions for @xmath109 .",
    "thus , we will focus on the cases with @xmath110 in the rest of this paper .",
    "theorem [ thm : mtfl_primal_dual_closed_form ] gives a closed form solution of @xmath91 for @xmath109 .",
    "therefore , we can estimate @xmath91 with @xmath111 in terms of a known @xmath112 .",
    "specifically , we can simply set @xmath113 and utilize the result @xmath114 . to make this paper self - contained , we first review some geometric properties of projection operators .",
    "[ thm : normal_cone ] let @xmath33 be a nonempty closed convex set . then , for any point @xmath115 , we have @xmath116 where @xmath117 is called the normal cone to @xmath33 at @xmath118 .    another useful property of the projection operator in estimating @xmath91 is the so - called",
    "_ firmly nonexpansiveness_.    [ thm : firmly_nonexpansive ] let @xmath33 be a nonempty closed convex subset of a hilbert space @xmath119 .",
    "the projection operator with respect to @xmath33 is firmly nonexpansive , namely , for any @xmath120 , @xmath121    the firmly nonexpansiveness of projection operators leads to the following useful result .",
    "[ corollary : convex_set_projection ] let @xmath33 be a nonempty closed convex subset of a hilbert space @xmath119 and @xmath122 . for any @xmath123",
    ", we have : @xmath124 .",
    "@xmath125 .",
    "part of indicates that : if a closed convex set @xmath33 contains the origin , then , for any point @xmath126 , the norm of its projection with respect to @xmath33 is upper bounded by the norm of @xmath127 .",
    "the second part is a useful consequence of the first part and plays a crucial role in the estimation of the dual optimal solution ( see theorem ) .",
    "we are now ready to present an accurate estimation of the dual optimal solution @xmath91 .",
    "[ thm : mtfl_estimation ] for the mtfl model in , suppose that @xmath112 is known with @xmath128 $ ] .",
    "let @xmath129 be given by for @xmath100 , and @xmath130 for any @xmath131 , we define @xmath132 then , the following holds :    @xmath133 ,    @xmath134 ,    @xmath135 ,    @xmath136 .",
    "consider theorem [ thm : mtfl_estimation ] .",
    "part 1 characterizes @xmath91 via the normal cone .",
    "parts 2 and 3 illustrate key geometric identities that lead to the accurate estimation of @xmath91 in part 4 ( see supplement for details ) .",
    "the estimation of the dual optimal solution in dpc and edpp that is for lasso  are both based on the geometric properties of the projection operators .",
    "thus , the formulas of the estimation in theorem are similar to that of . however , we note that the estimations in dpc and edpp are determined by the completely different geometric structures of the corresponding dual feasible sets .",
    "problem implies that the dual feasible set of the mtfl model is much more than that of lasso  which is a polytope the intersection of a set of closed half spaces .",
    "therefore , the estimation of the dual optimal solution in dpc is much more challenging than that of edpp , e.g. , we need to find a vector in the normal cone to the dual feasible set at @xmath137 see @xmath138 .    for notational convenience , let @xmath139 theorem [ thm : mtfl_estimation ] implies that @xmath91 lies in the ball : @xmath140      in this section",
    ", we solve the optimization problem in ( [ rule * ] ) with @xmath102 given by @xmath141 [ see eq .",
    "( [ eqn : mtfl_estimation_ball ] ) ] , namely , @xmath142 although @xmath143 and @xmath141 are convex , problem ( [ prob : nonconvex_ball ] ) is nonconvex , as it is a maximization problem .",
    "however , we can efficiently solve for the _ global _ optimal solutions to ( [ prob : nonconvex_ball ] ) by transforming it to a qp1pc via a parametrization of the constraint set .",
    "we first cite the following result .",
    "[ thm : qp1pc ] let @xmath144 be a symmetric matrix and @xmath145 be a positive definite matrix .",
    "consider @xmath146 where @xmath147 .",
    "then , @xmath148 minimizes @xmath149 over the constraint set if and only if there exists @xmath150that is unique  such that @xmath151 is positive semidefinite , @xmath152    we are now ready to solve for @xmath153 .",
    "[ thm : nonconvex ] let @xmath154 and @xmath148 be the optimal solution of problem with @xmath155 , @xmath156 , @xmath157 namely , there exists a @xmath150 such that @xmath158 and @xmath148 solve and .",
    "let @xmath159 then , the following hold :    .",
    "@xmath158 is unique , and @xmath160 .    .",
    "we define @xmath161 by @xmath162 then , we have @xmath163    .",
    "let @xmath164 . then",
    ", we have @xmath165    .",
    "the maximum value of problem is given by @xmath166    we first transform problem ( [ prob : nonconvex_ball ] ) to a qp1pc by a parameterization of @xmath167 : @xmath168 where @xmath169 .",
    "we define @xmath170 thus , problem ( [ prob : nonconvex_ball ] ) becomes @xmath171 by the cauchy - schwartz inequality , for a fixed @xmath172 , we have @xmath173 let @xmath174 .",
    "we can see that @xmath175 thus , problem ( [ prob : nonconvex_ball ] ) becomes @xmath176 therefore , to solve ( [ prob : nonconvex_ball ] ) , it suffices to solve problem ( [ prob : qp1pc1 ] ) with @xmath177 , @xmath145 , @xmath144 , and @xmath178 as in the theorem .",
    "the statement follows immediately from theorem [ thm : qp1pc ] .    to develop the dpc rule , ( [ rule * ] )",
    "implies that we only need the maximum value of problem .",
    "thus , theorem [ thm : qp1pc ] does not show the global optimal solutions .",
    "however , in view of the proof , we can easily compute the global optimal solutions in terms of @xmath158 and @xmath148 .    * computing @xmath158 and @xmath148 * consider theorem . if @xmath179 and @xmath180 for @xmath181 , then @xmath158 and @xmath148 admit closed form solutions . otherwise , @xmath158 is strictly larger than @xmath182 , which implies that @xmath183 is positive definite and invertible .",
    "if this is the case , we apply newton s method to find @xmath158 as follows .",
    "let @xmath184 because @xmath185 is strictly increasing on @xmath186 , @xmath158 is the root of @xmath185 on @xmath186 .",
    "let @xmath187 .",
    "then , the @xmath188 iteration of newton s method to solve @xmath189 is : @xmath190 as pointed out by , newton s method is very efficient to find @xmath158 as @xmath191 is almost linear on @xmath186 .",
    "our experiments indicates that five iterations usually leads to an accuracy higher than @xmath192 .      as implied by [ rule * ] , we present the proposed screening rule , dpc , for the mtfl model ( [ prob : mtfl ] ) in the following theorem .",
    "[ thm : dpc ] for the mtfl model , suppose that @xmath112 is known with @xmath128 $ ] .",
    "then , we have @xmath193 where @xmath153 is given by theorem .    in real applications ,",
    "the optimal parameter value of @xmath104 is generally unknown .",
    "commonly used approaches to determine an appropriate value of @xmath104 , such as cross validation and stability selection , need to solve the mtfl model over a grid of tuning parameter values @xmath194 , which is very time consuming .",
    "inspired by the ideas of strong rule @xcite and safe @xcite , we develop the sequential version of dpc .",
    "specifically , suppose that the optimal solution @xmath195 is known .",
    "then , we apply dpc to identify the inactive features of mtfl model ( [ prob : mtfl ] ) at @xmath196 via @xmath195 .",
    "we repeat this process until all @xmath197 , @xmath198 are computed .",
    "[ corollary : dpcs ] for the mtfl model , suppose that we are given a sequence of parameter values @xmath199 .",
    "then , for any @xmath200 , if @xmath197 is known , we have @xmath201 where @xmath153 is given by theorem .",
    "we omit the proof of corollary [ corollary : dpcs ] since it is a direct application of theorem [ thm : dpc ] .",
    "we evaluate dpc on both synthetic and real data sets . to measure the performance of dpc , we report the _ rejection ratio _ , namely , the ratio of the number of inactive features identified by dpc to the actual number of inactive features . we also report the _ speedup _ , i.e. , the ratio of the running time of solver without screening to the running time of solver with dpc .",
    "the solver is from the slep package @xcite . for each data",
    "set , we solve the mtfl model in ( [ prob : mtfl ] ) along a sequence of @xmath202 tuning parameter values of @xmath104 spaced on the logarithmic scale of @xmath203 from @xmath204 to @xmath205 .",
    "we only evaluate dpc since no existing screening rule is applicable for the mtfl model in ( [ prob : mtfl ] ) .",
    "we perform experiments on two synthetic data sets , called synthetic 1 and synthetic 2 , that are commonly used in the literature @xcite . both synthetic 1 and synthetic 2 have @xmath206 tasks .",
    "each task contains @xmath206 samples . for @xmath207 ,",
    "the true model is @xmath208 for synthetic 1 , the entries of each data matrix @xmath209 are i.i.d .",
    "standard gaussian with pairwise correlation zero , i.e. , @xmath210 . for synthetic 2 ,",
    "the entries of each data matrix @xmath209 are drawn from i.i.d .",
    "standard gaussian with pairwise correlation @xmath211 , i.e. , @xmath212 . to construct @xmath213 ,",
    "we first randomly select @xmath214 of the features .",
    "then , the corresponding components of @xmath213 are populated from a standard gaussian , and the remaining ones are set to @xmath215 . for both synthetic 1 and synthetic 2 , we set the feature dimension to @xmath216 , @xmath217 , and @xmath218 , . for each setting , we run @xmath219 trials and report the average performance in fig .  [",
    "fig : rej_ratio_synthetic ] and table [ table : dpc_runtime ] .",
    "[ fig : rej_ratio_synthetic ] shows the rejection ratios of dpc on synthetic 1 and synthetic 2 .",
    "for all the six settings , the rejection ratios of dpc are higher than @xmath220 , even for small parameter values .",
    "this demonstrates one of the advantages of dpc , as previous empirical studies @xcite indicate that the capability of screening rules in identifying inactive features usually decreases as the parameter value decreases",
    ". moreover , fig .",
    "[ fig : rej_ratio_synthetic ] also shows that as the feature dimension increases , the rejection ratios of dpc become higher  that is very close to @xmath221 .",
    "this implies that the potential capability of dpc in identifying the inactive features on high - dimensional data sets would be even more significant .",
    "table [ table : dpc_runtime ] presents the running time of the solver with and without .",
    "the speedup is very significant , which is up to @xmath222 times .",
    "take 1 for example .",
    "when the feature dimension is @xmath218 , the solver without dpc takes about @xmath223 hours to solve problem ( [ prob : mtfl ] ) at @xmath202 paramater values .",
    "in contrast , combined with , the solver only takes less than one hour to solve the same @xmath202 problems  which leads to a speedup about @xmath222 times .",
    "table [ table : dpc_runtime ] also shows that the computational cost of dpc is very low  which is negligible compared to that of the solver without screening .",
    "moreover , as the rejection ratios of dpc increases with feature dimension growth ( see fig .  [",
    "fig : rej_ratio_synthetic ] ) , table [ table : dpc_runtime ] shows that the speedup by dpc increases as well .",
    "we perform experiments on three real data sets : 1 ) the text data set @xcite ; 2 ) the animal data set @xcite ; 3 ) the alzheimer s disease neuroimaging initiative ( adni ) data set ( http://adni.loni.usc.edu/ ) .",
    "* the animal data set * the data set consists of @xmath224 images of @xmath206 animals classes . by following the experiment settings in @xcite ,",
    "we choose @xmath219 animal classes in the data set : antelope , grizzly - bear , killer - whale , beaver , dalmatian , persiancat , horse , german- shepherd , blue - whale , siamese - cat , skunk , ox , tiger , hippopotamus , leopard , moose , spidermonkey , humpback - whale , , and gorilla .",
    "we construct @xmath219 tasks , where each of them is a classification task of one type of animal against all the others . for the @xmath9 task , we first select @xmath225 samples from the @xmath9 class as the positive samples ; and then we randomly select @xmath225 samples from all the other classes as the negative samples .",
    "we make use of all the seven sets of features kindly provided by @xcite : color histogram features , local self - similarity features , pyramidhog ( phog ) features , sift features , colorsift features , surf features , and decaf features .",
    "thus , each image is represented by a @xmath226-dimensional vectors .",
    "hence , the data matrix @xmath72 of the @xmath227 task is of @xmath228 , where @xmath229 .    * the tdt2 data set * the original data set contains @xmath230 documents of @xmath225 categories .",
    "each document is represented by a @xmath231-dimensional vector .",
    "similar to the animal data set , we construct @xmath225 tasks , each of which is a classification task of one category against all the others @xcite . also , for the @xmath9 task , we first select @xmath206 samples from the @xmath9 category as the positive samples , and then we randomly select @xmath206 samples from all the other categories as the negative samples .",
    "moreover , we remove the features that have only zero entries , thus leaving us @xmath232 features .",
    "hence , the data matrix @xmath72 of the @xmath227 task is of @xmath233 , where @xmath234 .",
    "l c|c|c|c|c| & & solver & dpc & dpc+solver & * speedup * +   + [ -2.5ex ] & & 405.75 & 0.7 & 28.12 & * 14.43 * + & & 913.70 & 1.36 & 37.02 & * 24.68 * + & & 2441.57 & 3.50 & 42.08 & * 58.03 * + & & 406.85 & 0.70 & 29.28 & * 13.89 * + & & 906.09 & 1.37 & 36.66 & * 24.72 * + & & 2435.38 & 3.46 & 44.78 & * 54.39 * + & 15036 & 311.71 & 0.47 & 16.36 & * 19.05 * + & 24262 & 958.66 & 1.87 & 44.11 & * 21.74 * + & 504095 & 9625.58 & 21.13 & 35.34 & * 272.37 * +    * the adni data set * the data set consists of @xmath235 patients with @xmath236 single nucleotide polymorphisms ( snps ) , and the volume of @xmath237 brain regions for each patient .",
    "we first randomly select @xmath219 brain regions .",
    "then , for each region , we randomly select @xmath206 patients , and utilize the corresponding snps data as the data matrix and the volumes of that brain region as the response .",
    "thus , we have @xmath219 tasks , each of which is a regression task .",
    "the data matrix @xmath72 of the @xmath9 task is of @xmath238 , where @xmath229 .",
    "[ fig : rej_ratio_real ] shows the rejection ratios of dpc  that are above @xmath220on the aforementioned three real data sets .",
    "in particular , the rejection ratios of dpc on the adni data set are higher than @xmath239 at the @xmath202 parameter values .",
    "table [ table : dpc_runtime ] shows that the resulting speedup is very significant  that is up to @xmath240 times .",
    "we note that the feature dimension of the adni data set is more than _ half million_. without screening , table [ table : dpc_runtime ] shows that the solver takes about _ seven days _ ( approximately _ one week _ ) to compute the mtfl model ( [ prob : mtfl ] ) at @xmath202 parameter values",
    ". however , integrated with the screening rule , the solver computes the @xmath202 solutions in about _ half an hour_. the experiments again that dpc provides better performance ( in terms of rejection ratios and speedup ) for higher dimensional data sets .",
    "in this paper , we propose a novel screening method for the mtfl model in ( [ prob : mtfl ] ) , called dpc .",
    "the dpc screening rule is based on an indepth analysis of the geometric properties of the dual problem and the dual feasible set . to the best of our knowledge",
    ", dpc is the first screening rule that is applicable to sparse models with multiple data matrices .",
    "dpc is _ safe _ in the sense that the identified features by dpc are guaranteed to have zero coefficients in the solution vectors across all tasks .",
    "experiments on synthetic and real data sets demonstrate that dpc is very effective in identifying the inactive features , which leads to a substantial savings in computational cost and memory usage _ without sacrificing accuracy_. moreover , dpc is more effective as the feature dimension increases , which makes dpc a very competitive candidate for the applications of very high - dimensional data .",
    "we plan to extend dpc to more general mtfl models , e.g. , the mtfl models with multiple regularizers .",
    "although eq .  (",
    "[ eqn : mtfl_minf2_fermat ] ) implies that @xmath241 , this might not be the case .",
    "thus , we need to consider the following two cases .    1 .",
    "( [ eqn : mtfl_minf2_fermat ] ) holds , we can see that @xmath242 and thus @xmath243 therefore , we have @xmath244 2 .   if eq .",
    "( [ eqn : mtfl_minf2_fermat ] ) does not hold , i.e. , @xmath245 , we would have @xmath246 and thus @xmath247 to see this , we define @xmath248 and thus @xmath249 then , we have @xmath250 because @xmath245 , the above equation yields @xmath251    the above discussion implies that @xmath252",
    "for notational convenience , let        ( @xmath255 ) suppose that 2 holds .",
    "( [ eqn : mtfl_kkt1 ] ) implies that @xmath256 for @xmath257 .",
    "denote the objective function of the mtfl model ( [ prob : mtfl ] ) by @xmath65 .",
    "we claim that @xmath98 must be zero . to see this ,",
    "let @xmath258 be another optimal solution of ( [ prob : mtfl ] ) and thus @xmath259 for @xmath14 .",
    "however , it is evident that @xmath260 .",
    "this leads to a contradiction .",
    "thus , the optimal solution @xmath98 is zero and we have proved @xmath261 .",
    "the converse direction , i.e. , @xmath262 is a direct consequence of eq .",
    "( [ eqn : mtfl_kkt1 ] ) .",
    "( @xmath263 ) it is evident that 1 holds if and only if @xmath92 is a feasible solution of problem ( [ prob : mtfl_dual ] ) , namely , all constraints in ( [ prob : mtfl_dual ] ) holds at @xmath92 . by plugging @xmath92 into the constraints in ( [ prob : mtfl_dual ] ) , we can see that the feasibility of @xmath92 is equivalent to 4 .",
    "thus , we can see that 1 is equivalent to 4 .",
    "this completes the proof .                1 .   for @xmath110 , theorem",
    "[ thm : mtfl_primal_dual_closed_form ] implies that @xmath276 .",
    "thus , the statement holds for @xmath110 by theorem [ thm : normal_cone ] and eq .",
    "( [ eqn : theta*_proj ] ) [ let @xmath277 and @xmath278",
    ". + to show the statement holds at @xmath279 , theorem [ thm : normal_cone ] indicates that we need to show @xmath280 because @xmath281 is convex , we have @xcite @xmath282 note that , @xmath129 is the constraint function of the dual problem in ( [ prob : mtfl_dual ] ) .",
    "thus , for any dual feasible solution @xmath283 , it is evident that @xmath284 .",
    "moreover , eq .  ( [ eqn : mtfl_lambdamx ] ) implies that @xmath285 .",
    "therefore , the left hand of the inequality ( [ ineqn : g*_convex ] ) must be non - positive , which yields inequality ( [ ineqn : gradient_g*_normal_cone ] ) .",
    "thus , the statement holds .",
    "a direct application of part 2 of corollary [ corollary : convex_set_projection ] yields @xmath286 when @xmath113 , by noting that @xmath287 , we have @xmath288 thus , the statement holds .",
    "3 .   by eq .",
    "( [ eqn : mtfl_r ] ) , we have @xmath289 by eqs .",
    "( [ eqn : mtfl_n ] ) and ( [ eqn : theta*_proj ] ) , the second term on the right hand side of eq .",
    "( [ eqn : ip_rn ] ) is nonnegative for all @xmath128 $ ] .",
    "+ the fact that @xmath290 yields @xmath291 thus , the first term on the right hand side of eq .",
    "( [ eqn : ip_rn ] ) is nonnegative for @xmath113 . for @xmath292 ,",
    "part 2 of corollary [ corollary : convex_set_projection ] , eqs .",
    "( [ eqn : theta*_proj ] ) and ( [ eqn : mtfl_n ] ) imply that @xmath293 thus , the first term on the right hand side of eq .",
    "( [ eqn : ip_rn ] ) is nonnegative for @xmath292 . + as a result , the inner product @xmath294 is nonnegative .",
    "we define @xmath295 part 1 of lemma [ lemma : normal_cone_projection ] implies that @xmath296 the nonexpansiveness of the projection operators yields [ let @xmath297 and @xmath298 and plug them into ( [ ineqn : nonexpansive ] ) ] @xmath299 by eqs .",
    "( [ eqn : theta*_proj ] ) , ( [ eqn : projection_ray ] ) and ( [ eqn : mtfl_r ] ) , the above inequality reduces to @xmath300 let us consider @xmath301 because @xmath302 is a quadratic function of @xmath303 , we can see that @xmath304 + because of part 3 , we have @xmath305 plugging eqs .",
    "( [ eqn : mtfl_min_rt ] ) and ( [ eqn : mtfl_argmin_rt ] ) into ( [ ineqn : mtfl_estimation1 ] ) yields the statement , which completes the proof .",
    "1 .   to show part 1 , we only need to set @xmath264 and @xmath265 , and then plug them into the inequality ( [ ineqn : nonexpansive ] ) [ note that @xmath266 since @xmath122 ] .",
    "part 1 implies that @xmath268 .",
    "thus , we have @xmath269 which is equivalent to the statement in part 2 ."
  ],
  "abstract_text": [
    "<S> multi - task feature learning ( mtfl ) is a powerful technique in boosting the predictive performance by learning multiple related classification / regression / clustering tasks . </S>",
    "<S> however , solving the mtfl problem challenging when the feature dimension is large . in this paper </S>",
    "<S> , we propose a novel screening rule  that is based on the * * d**ual * * p**rojection onto * * c**onvex sets ( dpc)to identify the _ </S>",
    "<S> inactive features_that have zero in the solution vectors across tasks . </S>",
    "<S> one of the appealing features of dpc is that : it is _ safe _ in the sense that the detected inactive features are guaranteed to have coefficients in the solution vectors across all tasks . </S>",
    "<S> thus , by removing the inactive features from the training phase , we may have substantial savings in the computational cost and memory usage _ without sacrificing accuracy_. to the best of our knowledge , it is the _ first _ screening rule that is applicable to sparse models with _ multiple </S>",
    "<S> _ matrices . </S>",
    "<S> a key challenge in deriving dpc is to solve a nonconvex problem . </S>",
    "<S> we show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set . </S>",
    "<S> moreover , dpc has very low cost and can be with any existing solvers . </S>",
    "<S> we have evaluated the proposed dpc rule on both synthetic and real data sets . </S>",
    "<S> the indicate that is very effective in identifying the inactive features  especially for high dimensional data  which leads to a speedup up to several orders of magnitude . </S>"
  ]
}