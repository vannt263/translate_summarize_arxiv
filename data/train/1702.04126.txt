{
  "article_text": [
    "for any @xmath2 , fix any @xmath3^s$ ] and consider probabilities @xmath4 associated with components of @xmath5 .",
    "let the vector @xmath6 of probabilities itself be random and dirichlet - distributed with parameters @xmath7 .",
    "let @xmath8 be a vector of @xmath9 independent samples drawn from the associated categorical distribution over components of @xmath5 .",
    "note that the components of @xmath10 are conditionally independent , conditioned on @xmath6 , but are not unconditionally independent .",
    "conditioned on @xmath6 , the mean of each @xmath11 is @xmath12 .",
    "let @xmath13^n$ ] and @xmath14 for each @xmath15 .",
    "then , the distribution of @xmath6 conditioned on @xmath16 is dirichlet with parameters @xmath17 .",
    "let @xmath18 be distributed @xmath19 , ( \\alpha^\\top { \\bf 1})^{-1}\\right)$ ] .",
    "let @xmath20 be a vector of @xmath21 independent samples distributed according to @xmath22 .",
    "the distribution of @xmath18 conditioned on @xmath23 is @xmath24 , where @xmath25    in this paper , we establish that = 0mu = 0mu = 0mu @xmath26 , where @xmath27 denotes second - order stochastic dominance . in other words , conditioned on identical outcomes ,",
    "the posterior mean of the categorical distribution second - order - stochastically dominates the posterior mean of the gaussian distribution .",
    "this result extends earlier work relating variances of posterior means under gaussian and dirichlet models @xcite .",
    "our result provides a dominance relation that applies to all moments .",
    "our interest in this result stems from its significance in the area of _ reinforcement learning _",
    "@xcite , where we have used it to establish a notion of stochastic optimism achieved by particular reinforcement algorithms that generate randomized value functions to explore in an efficient manner @xcite .",
    "this paper presents the result and its proof in a form that will be cited by our work on reinforcement learning and that will be accessible to researchers more broadly .",
    "in this section we will review several notions of partial orderings for real - valued random variables .",
    "all random variables we define will be with respect to the probability space @xmath28 .",
    "[ def : fsd ] let @xmath10 and @xmath29 be real - valued random variables .",
    "we say that @xmath10 is ( first order ) stochastically dominant for @xmath29 if for all @xmath30 , @xmath31 we write @xmath32 for this relationship .",
    "first order stochastic dominance defines a partial ordering between random variables but it also quite a blunt notion of dominance that will be insufficient for our purposes .",
    "consider @xmath33 and @xmath34 with @xmath35 .",
    "these random variables can not be related in terms of fsd . however , in the context of gambling we might imagine that the return from @xmath10 is in some sense preferable to @xmath29 , since they have the same mean but @xmath10 is somehow less risky . our next definition formalizes this notion .",
    "[ def : ssd ] let @xmath10 and @xmath29 be real - valued random variables .",
    "we say that @xmath10 is second order stochastically dominant for @xmath29 if for all @xmath36 convex and non - decreasing , @xmath37 \\ge { \\mathds{e}}[u(y)].\\ ] ] we write @xmath38 for this relationship .",
    "[ prop : ssd equiv ] let @xmath10 and @xmath29 be real - valued random variables with finite expectation .",
    "the following are equivalent :    1 .",
    "@xmath38 2 .",
    "for any @xmath36 concave and increasing @xmath39 \\ge { \\mathds{e}}[u(y)]$ ] 3 .   for any @xmath40 , @xmath41 .",
    "4 .   @xmath42 for @xmath43 and @xmath44 = 0 $ ] for all values @xmath45 .",
    "this follows from a simple integration by parts @xcite .",
    "second order stochastic dominance @xmath38 ensures that @xmath46 \\ge { \\mathds{e}}[y]$ ] .",
    "it also establishes that for _ any _ convex loss @xmath47 that @xmath10 is less `` spread out '' than @xmath29 in the sense @xmath48 ) ] \\le { \\mathds{e}}[l(y-{\\mathds{e}}[y])]$ ] .",
    "motivated by this equivalence , we introduce another related dominance condition .",
    "[ def : single crossing ] let @xmath10 and @xmath29 be real - valued random variables with cdfs @xmath49 and finite expectation .",
    "we say that @xmath10 single - crossing dominates @xmath29 if and only if @xmath46 \\ge { \\mathds{e}}[y]$ ] and there a crossing point @xmath50 such that : @xmath51 we write @xmath52 for this relationship .",
    "single crossing dominance is actually a stronger condition than ssd , as we show in proposition [ prop : relate scd ssd ] .",
    "in general the reverse implication is not true , as we demonstrate in example [ ex : ssd not sc ] .    [",
    "prop : relate scd ssd ] let @xmath10 and @xmath29 be real - valued random variables with finite expectation then @xmath53    suppose @xmath52 with single crossing point @xmath54 .",
    "let @xmath55 .",
    "by @xmath52 we know @xmath56 for all @xmath57 and that @xmath58 is decreasing for all @xmath59 .",
    "now we consider the limit @xmath60 - { \\mathds{e}}[y ] \\ge 0 $ ] .",
    "hence @xmath61 for all @xmath40 , which shows that @xmath62 by proposition [ prop : ssd equiv ] .",
    "[ ex : ssd not sc ] consider @xmath63 and let @xmath64 where @xmath65)$ ] and independent of @xmath29 . by proposition",
    "[ prop : ssd equiv ] @xmath38 , however @xmath10 is not single crossing dominant for @xmath29 .",
    "we display the cdfs of these variables in figure [ fig : double cross ] , they are not single crossing . in particular",
    "the ordering of @xmath49 switches at least three points @xmath66 .",
    "the main technical result in this paper comes in theorem [ thm : dir_norm ] , which we prove in section [ sec : proof thm ] .",
    "[ thm : dir_norm ] let @xmath67 for @xmath3^s$ ] fixed and @xmath68 with @xmath69 and @xmath70 .",
    "let @xmath71 with @xmath72 , then @xmath46 = { \\mathds{e}}[y]$ ] and @xmath38",
    ".    at first glance , theorem [ thm : dir_norm ] may seem quite arcane , it provides an ordering between two paired families of gaussian and dirichlet distributions in terms of ssd .",
    "the reason this result is so useful is that , given matched prior distributions , the resultant posteriors for the gaussian and dirichlet models will remain ordered in this way _ for any _ observation data .",
    "the condition @xmath70 is technical but does not pose significant difficulties so long as at the posterior is updated with at least two observations .",
    "we present this result as corollary [ cor : dir_norm ] .",
    "[ cor : dir_norm ] let @xmath67 for @xmath3^s$ ] fixed and @xmath68 with @xmath69 .",
    "let @xmath71 with @xmath72 .",
    "let @xmath73 be the data from @xmath9 i.i.d .",
    "samples from the categorical distribution @xmath6 and values @xmath5 .",
    "let @xmath74 be the posterior distribution for @xmath75 and @xmath76 be the posterior distribution for @xmath77 but updating according to a mis - specified likelihood as if the observations were @xmath78 .",
    "then , for all datasets @xmath73 such that @xmath79 we can guarantee that @xmath80 .",
    "this result is a consequence of theorem [ thm : dir_norm ] together with algebraic relations for the conjugate updates of @xmath74 and @xmath76 given any data @xmath73 .",
    "we write @xmath81 for the number of observations from each category @xmath5 in the dataset @xmath73 with @xmath82",
    ". then we can write the posterior distribution @xmath83 for @xmath84 and @xmath85 .    in a a similar way we can compute the posterior distribution of @xmath86 where we update with an misspecified likelihood as if the data were @xmath78 .",
    "once again we can use a conjugate form for the update @xmath87 explicitly , @xmath88 @xmath89 we conclude by application of theorem [ thm : dir_norm ] on the updated posterior parameters @xmath90 .",
    "the complete proof of theorem [ thm : dir_norm ] is long but the essential argument is simple .",
    "we outline the main arguments below and fill in the details in sections [ app : beta dir ] and [ app : gauss beta ] .",
    "first , we consider an auxilliary random variable @xmath91 with @xmath92 and @xmath93 . in lemma",
    "[ lem : dir beta ] we show that @xmath94 .",
    "next , we show that this auxilliary beta @xmath74 is single crossing dominant for the approximating gaussian posterior , @xmath95 . therefore , by proposition [ prop : relate scd ssd ] @xmath38 .",
    "the main difficulty in this proof comes in establishing @xmath95 .",
    "to do this we use a laborious calculus argument together with repeated applications of the mean value theorem .",
    "our proof requires separate upper and lower bounds for different regions of @xmath90 and @xmath96 , but no real insight beyond that .",
    "we believe that there should be a much more enlightened and elegant method to obtain these results .",
    "we begin our proof of theorem [ thm : dir_norm ] with an intermediate comparison of the dirichlet distribution to a matched beta posterior .",
    "we first state a more basic result that we will use on gamma distributions .    [",
    "le : gamma ] let @xmath97 and @xmath98 be independent random variables .",
    "then the conditional expectations @xmath99 = \\frac{k_1}{k_1 + k_2 } ( \\gamma_1+\\gamma_2)$ ] and @xmath100 = \\frac{k_2}{k_1 + k_2 } ( \\gamma_1+\\gamma_2).$ ]    [ lem : dir beta ] let @xmath101 for the random variable @xmath68 and constants @xmath102 and @xmath103 . without loss of generality ,",
    "assume @xmath104 .",
    "let @xmath105 and @xmath106 .",
    "then , there exists a random variable @xmath107 such that , for @xmath108 , @xmath109 = { \\mathds{e}}[x]$ ] and so @xmath94 .",
    "let @xmath110 , with @xmath111 independent , and let @xmath112 , so that @xmath113 let @xmath114 and @xmath115 so that @xmath116 define independent random variables @xmath117 and @xmath118 so that @xmath119    take @xmath120 and @xmath121 to be independent , and couple these variables with @xmath122 so that @xmath123 note that @xmath124 and @xmath125 .",
    "let @xmath126 and @xmath127 , so that @xmath128 and @xmath129 couple these variables so that @xmath130 and @xmath131 we can now say @xmath132 & = & { \\mathds{e}}[(1- \\tilde{p } ) v_1 + \\tilde{p } v_d | x ] = { \\mathds{e}}\\left[\\frac{v_1 \\overline{\\gamma}^0}{\\overline{\\gamma } } + \\frac{v_d \\overline{\\gamma}^1}{\\overline{\\gamma } } \\big| x\\right ] \\\\ & = & { \\mathds{e}}\\left[{\\mathds{e}}\\left[\\frac{v_1 \\overline{\\gamma}^0 + v_d \\overline{\\gamma}^1}{\\overline{\\gamma } } \\big| \\gamma , x\\right ] \\big| x \\right ] = { \\mathds{e}}\\left[\\frac{v_1 { \\mathds{e}}[\\overline{\\gamma}^0 | \\gamma ] + v_d { \\mathds{e}}[\\overline{\\gamma}^1 | \\gamma]}{\\overline{\\gamma } } \\big| x \\right ] \\\\ & = & { \\mathds{e}}\\left[\\frac{v_1 \\sum_{i=1}^d { \\mathds{e}}[\\gamma^0_i | \\gamma_i ] + v_d \\sum_{i=1}^dxp[\\gamma^1_i | \\gamma_i]}{\\overline{\\gamma } } \\big| x \\right ] \\\\ & \\stackrel{\\text{(a)}}{= } & { \\mathds{e}}\\left[\\frac{v_1 \\sum_{i=1}^d \\gamma_i \\alpha_i^0 / \\alpha_i + v_d \\sum_{i=1}^d \\gamma_i \\alpha_i^1/\\alpha_i}{\\overline{\\gamma } } \\big| x \\right ] \\\\ & = & { \\mathds{e}}\\left[\\frac{v_1 \\sum_{i=1}^d \\gamma_i ( v_i - v_1 ) + v_d \\sum_{i=1}^d \\gamma_i ( v_d - v_i)}{\\overline{\\gamma } ( v_d - v_1 ) } \\big| x \\right ] \\\\ & = & { \\mathds{e}}\\left[\\frac{\\sum_{i=1}^d \\gamma_i   v_i}{\\overline{\\gamma } } \\big| x \\right ] = { \\mathds{e}}\\left[\\sum_{i=1}^d p_i   v_i \\big| x \\right ] = x,\\end{aligned}\\ ] ] where ( a ) follows from lemma [ le : gamma ] .",
    "therefore , @xmath74 is a mean - preserving spread of @xmath10 and so by proposition [ prop : ssd equiv ] , @xmath94 .",
    "we complete the proof of theorem [ thm : dir_norm ] by showing that this auxilliary beta random variable @xmath74 defined in lemma [ lem : dir beta ] is second order stochastic dominant for the gaussian posterior @xmath29 .",
    "[ lem : gauss beta ] let @xmath133 for any @xmath134 and @xmath135 .",
    "then , @xmath95 ( and by proposition [ prop : relate scd ssd ] this implies @xmath136 ) whenever @xmath137 .",
    "we want to prove that the cdfs cross at most once on @xmath138 . by the mean value theorem @xcite ,",
    "it is sufficient to prove that the pdfs cross at most twice on the same interval .",
    "we lament that the proof as it stands is so laborious , but our attempts at a more elegant solution has so far been unsuccessful .",
    "the remainder of this appendix is devoted to proving this `` double - crossing '' property via manipulation of the pdfs for different values of @xmath139 .",
    "we write @xmath140 for the density of the normal @xmath29 and @xmath141 for the density of the beta @xmath74 respectively . we know that at the boundary @xmath142 and @xmath143 where the @xmath144 represents the left and right limits respectively .",
    "as these densities are positive over the interval , we can consider the log pdfs @xmath145 @xmath146 the function @xmath147 is injective and increasing ; if we can show that @xmath148 has at most two solutions on the interval we will be done .",
    "instead we will attempt to prove an even stronger condition , that @xmath149 has at most one solution in the interval .",
    "this sufficient condition may be easier to deal with since we can ignore the distributional normalizing constants .",
    "@xmath150    finally we consider an even stronger condition , if @xmath151 has no solution then @xmath152 must be monotone over the region and so it can have at most one root .",
    "@xmath153 with these definitions now let us define : @xmath154 our goal now is to show that @xmath155 does not have any solutions for @xmath156 $ ] .",
    "once again , we will look at the derivatives and analyze them for different values of @xmath157 .",
    "@xmath158 @xmath159    our proof will proceed by considering specific ranges for the values of @xmath157 and use different calculus arguments for each of these regions . by symmetry in the problem",
    ", we only need to prove the result for @xmath160 . within this section of possible parameter values",
    "we will need to subdivide the quadrant into three proof regions .",
    "@xmath161 , @xmath162 and @xmath163 .",
    "these regions completely cover all @xmath137 and hence suffice to complete the proof of lemma [ lem : gauss beta ] .",
    "can be verified individually , in this case the pdfs do not intersect at any point . ]      in this region we will show that @xmath165 has no solutions .",
    "we write @xmath166 and @xmath167 as before .",
    "@xmath168 @xmath169 @xmath170 we note that @xmath171 and so @xmath172 is a concave function . if we can show that the maximum of @xmath173 lies below @xmath174 then we know that there can be no roots .",
    "we now attempt to solve @xmath175 : @xmath176 where here we write @xmath177 .",
    "we ignore the case @xmath178 as a trivial special case .",
    "we write @xmath179 and evaluate the function @xmath173 at its minimum @xmath180 . @xmath181",
    "therefore the lemma holds for all @xmath182      in the case of @xmath184 we know that @xmath185 is a convex function on @xmath138 .",
    "if we solve @xmath186 and @xmath187 then we prove our statement .",
    "we will write @xmath188 for convenience .",
    "first we solve @xmath189 in terms of @xmath190 , @xmath191 we can now evaluate the function @xmath192 at its minimum @xmath180 . @xmath193 as long as @xmath194 we have shown that the cdfs are single crossing .",
    "we note that for all @xmath195 @xmath196 this completes the proof for @xmath197 .",
    "our argument for this final region is no different than before , although it is slightly more involved .",
    "the key additional difficulty is that it in this region is not enough to only look at the derivatives of the log likelihoods ; we need to use some bound on the normalizing constants to get our bounds .    in @xmath199 , we know that @xmath200 so we will make use of an upper bound to the normalizing constant of the beta distribution , the beta function . @xmath201",
    "the intuition is that , because in @xmath199 the value of @xmath202 is relatively small , this approximation will not be too bad .",
    "therefore , we can explicitly bound the log likelihood of the beta distribution : @xmath203    we now repeat a familiar argument based upon explicit calculus .",
    "we want to find two points @xmath204 for which @xmath205 .",
    "since @xmath184 we know that @xmath192 is convex and so for all @xmath206 $ ] then @xmath207 .",
    "we define the gap of the beta over the maximum of the normal log likelihood , @xmath208 if we can show the gap is positive then it must mean there are no crossings over the region @xmath209 $ ] .",
    "this is because @xmath210 is concave and therefore totally above the maximum of @xmath211 over the whole region @xmath209 $ ] .",
    "consider any @xmath212 ; we know from the ordering of the tails of the cdf that if there is more than one root in this segment then there must be at least three crossings .",
    "if there are three crossings , then the second derivative of their difference @xmath192 must have at least one root on this region .",
    "however we know that @xmath192 is convex , so if we can show that @xmath213 this can not be possible .",
    "we use a similar argument for @xmath214 $ ] and complete this proof via laborious calculus .",
    "we remind the reader of the definition in , @xmath215 .",
    "for ease of notation we will write @xmath188 .",
    "we note that : @xmath216 @xmath217 and we solve for @xmath218 .",
    "this means that @xmath219 and clearly @xmath220 .",
    "now , if we can show that , for all possible values of @xmath221 in this region @xmath222 , our proof will be complete .    to make the dependence on @xmath223 more clear we write @xmath224 below    = 1mu = 1mu = 1mu @xmath225 @xmath226",
    "we will demonstrate that @xmath227 for all of the values in our region @xmath228 .",
    "@xmath229 similarly , @xmath230 therefore , for any @xmath231 this means that @xmath232 .",
    "therefore this expression @xmath233 is maximized over @xmath234 for @xmath235 .",
    "we can evaluate this expression explicitly : @xmath236    this provides a monotonicity result which states that both @xmath237 are minimized at at the largest possible @xmath238 for any given @xmath234 over our region .",
    "we will now write @xmath239 . if we can show that @xmath240 for all @xmath241 and @xmath242 we will be done with our proof .",
    "we will perform a similar argument to show that @xmath243 is monotone increasing for all @xmath241 .",
    "@xmath244 note that the function @xmath245 is increasing in @xmath234 for @xmath241 .",
    "we can conservatively bound @xmath173 from below noting @xmath246 in our region .",
    "@xmath247 we can use calculus to say that : @xmath248 this expression is monotone decreasing in @xmath234 and with a limit @xmath249 .",
    "therefore @xmath250 for all @xmath234",
    ". we can explicitly evaluate this numerically and @xmath251 so we are done . the final piece of this proof involves a similar argument for @xmath252 .",
    "= 1mu = 1mu = 1mu @xmath253 once again we can see that @xmath254 is monotone increasing @xmath255    we complete the argument by noting @xmath256 .",
    "this concludes our proof of the pdf double crossing in region @xmath199 .",
    "the results of sections [ sec : r1 ] , [ sec : r2 ] and [ sec : r3 ] together prove lemma [ lem : gauss beta ] . by proposition [ prop : ssd equiv ] ,",
    "lemmas [ lem : dir beta ] and [ lem : gauss beta ] together complete the proof of theorem [ thm : dir_norm ] .",
    "this work was generously supported by a research grant from boeing , a marketing research award from adobe , and stanford graduate fellowships , courtesy of paccar ."
  ],
  "abstract_text": [
    "<S> we consider the problem of sequential learning from categorical observations bounded in @xmath0 $ ] . </S>",
    "<S> we establish an ordering between the dirichlet posterior over categorical outcomes and a gaussian posterior under observations with @xmath1 noise . </S>",
    "<S> we establish that , conditioned upon identical data with at least two observations , the posterior mean of the categorical distribution will always second - order stochastically dominate the posterior mean of the gaussian distribution . </S>",
    "<S> these results provide a useful tool for the analysis of sequential learning under categorical outcomes . </S>"
  ]
}