{
  "article_text": [
    "this concerns approximate frequentist , bayesian , and objective bayesian inferences for a widely applied two - level normal hierarchical model . at , for @xmath14 ,",
    "unbiased estimates @xmath15 are observed with means @xmath0 and with known variance @xmath16 . in practice",
    "the @xmath17 usually are unequal , perhaps with @xmath18 and @xmath19 known or accurately estimated .",
    "thus @xmath20,\\quad i = 1 , \\ldots , k. \\label{eqlevel1}\\ ] ] in practice each level-1 value @xmath15 here represents a  sufficient statistic or a summary unbiased estimate based on the @xmath21 observations taken from the @xmath11th of the @xmath8 units ( e.g. , a hospital , a small area , or a teaching unit ) .",
    "level-2 specifies a normal model for the random effects @xmath0 , each with its own @xmath22-dimensional predictor variables @xmath23 so that for @xmath24 and an unknown variance @xmath25 , @xmath26,\\quad i=1 , \\ldots , k. \\label{eqlevel2}\\ ] ] the case @xmath27 corresponds to @xmath24 fully known and then it may be convenient to set @xmath28 and @xmath29 , wlog . if @xmath30 , @xmath31 as a known @xmath32 matrix , assumed to have full rank @xmath22 .",
    "the marginal distribution of @xmath33 , given  @xmath24 and @xmath5 , and the conditional distribution of @xmath0 follow from the above , so that @xmath34,\\quad i=1,\\ldots , k , \\label{eqmarginaly } \\\\",
    "\\label{eqcondtheta}\\hspace*{24pt}\\theta_i | y_i , \\beta , a & \\stackrel{\\mathit{ind}}{\\sim } & n[(1-b_i ) y_i + b_i \\mu_i , v_i ( 1-b_i ) ] , \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\hspace*{99pt } { } i=1 , \\ldots , k,\\end{aligned}\\ ] ] where @xmath35 , and @xmath36 is a `` shrinkage factor . ''",
    "when @xmath30 , the vector @xmath24 is assumed throughout to follow lebesgue s flat prior on @xmath37 , so @xmath38 using this flat prior density for @xmath24 is equivalent to restricted maximum likelihood ( reml ) . when @xmath39 is proper , the posterior distribution for this prior is proper ( it integrates finitely ) if @xmath40 .",
    "when @xmath39 is improper , a larger @xmath8 is needed , with @xmath41 sufficing for the main distributions @xmath39 of interest here .",
    "when @xmath42 , as assumed initially , or when @xmath43 and with @xmath24 integrated out , we can focus on the main issue of dealing with the ( nuisance ) variance component @xmath44 and how to make inferences about the shrinkages @xmath4 .",
    "widely used programs like hlm , ml3 and sas use mle / reml methods to fit this model , while software for fully bayesian inferences is available via bugs and mlwin ( @xcite ) . maximum likelihood and reml",
    "obtain an estimate @xmath45 that maximizes the likelihood function of @xmath5 ( or marginal likelihood in the reml case ) .",
    "asymptotically ( @xmath8 large ) , maximum likelihood provides optimal estimates of @xmath5 , leading to convergence of estimates via frequentist and bayesian approaches .",
    "however , the standard errors assigned by mle and reml methods to the random effect estimates and the corresponding interval estimates can lead to confidence intervals with much smaller than their nominal confidences , even asymptotically .",
    "this happens with mle and reml methods not only because @xmath5 can be underestimated so that shrinkages are overestimated , but also because these procedures do not account for the fact that @xmath5 has been estimated .",
    "maximum likelihood and reml estimates of @xmath5 not infrequently produce @xmath46 , in which case shrinkage mles are @xmath47 .",
    "examples occur in every field , as for the 8 schools data ( @xcite ) , and in small area estimation ( @xcite ) .",
    "then , per typical usage , the variance estimates may be taken to be @xmath48 when @xmath42 , leading to zero - width or overly narrow confidence intervals of @xmath0 . as will be seen in section  [ seccoverageprobrisk ] , even when @xmath49 and this situation is avoided , overfitting via mle and reml",
    "can be considerable and nominal @xmath50 confidence intervals for @xmath0 might have true coverages in the 5080% range .",
    "the procedures developed here to fit the two - level model above offer computational ease comparable to maximum likelihood and reml methods , being based on differentiating the ( adjusted ) likelihood function twice .",
    "when @xmath8 is small or moderate , however , the adjustment provides much better standard errors and interval coverages .",
    "`` better '' coverage is meant in the level-2 frequentist sense of averaging over the data and the level-2 model ( 2 ) , for all  fixed  @xmath24 , @xmath5 , as illustrated in the equal variances case of figure [ figcoveragetheta ] , section  [ seccoverageprobrisk ] .",
    "central to this development is the adm procedure , `` adjustment for density maximization '' ( @xcite ) , albeit not then with the adm label .",
    "adm can be used with any pearson family ( normal , gamma , inverted gamma , beta , @xmath51 , @xmath52 or ) to approximate another distribution with a one - dimensional density .",
    "one merely multiplies the density by an adjustment which is determined by the pearson family , and then makes the argmax function produce the mean , not the mode , of the pearson distribution . as seen in ( [ eqcondtheta ] ) , posterior means and variances of the random effects are linear functions of the shrinkage factors @xmath4 , not of @xmath5 , so it is desirable to estimate the posterior mean of @xmath4 , and not the mode of @xmath4 or the mean of @xmath5 .",
    "shrinkage factor distributions are skewed and lie in [ 0,1 ] , both of which make a beta distribution approximate better than a normal .",
    "fitting beta distributions via adm is described in section  [ subsecadmapproxbeta ] . estimating shrinkage factors via adm",
    "will be seen to reduce to maximizing the posterior density of  @xmath5 ( or the marginalized density , if necessary ) , after having multiplied this density by @xmath5 .",
    "this adjustment has several benefits , which include prevention of estimating @xmath5 as 0 , and overestimating @xmath5 by just enough to account for the convex dependence of @xmath4 on @xmath5 .",
    "adm methods have been used successfully before to improve inferences of random effects in other multilevel models , as in @xcite for a poisson multilevel model .",
    "the main procedure here approximates a formal posterior distribution stemming from the flat prior @xmath53 on @xmath7 in ( 5 ) .",
    "this flat prior on @xmath5 , in conjunction with ( [ eqlevel2 ] ) , induces stein s harmonic prior ( shp ) ( [ eqpriorgoodapproxproportional ] ) on the random effects ( @xcite ) and a minimax admissible estimator .",
    "( stein s prior on @xmath54 for @xmath55 , @xmath56 , is harmonic except at the origin , so it actually is `` superharmonic . ''",
    "the shorter term `` harmonic '' is used here for simplicity of discourse . )",
    "the adm approximations are seen in section 3 to approximate closely the exact posterior means and variances of the random effects .",
    "buttressed with the examples of sections  [ secapproxaccuracy ] and [ seccoverageprobrisk ] , our assessments show , by frequency standards , so for all fixed hyperparameters @xmath7 and @xmath24 , that the adm - shp combination outperforms commonly used mle and reml procedures for estimating the random effects ( [ eqlevel1])([eqlevel3flat ] ) .",
    "the adm approximations of section  [ subsecadmeqvar ] apply to any smooth prior density @xmath39 , including the scale - invariant prior densities @xmath39 on @xmath5 @xmath57 these receive some specific attention , but our frequency evaluations are limited to the special choice in  ( [ eqsuperharmonica ] ) of @xmath58 for which @xmath59 .",
    "stein s harmonic prior not only produces safe frequency procedures for squared - error point estimation , but the posterior variances of @xmath0 are large enough to serve as a basis for confidence intervals centered at the posterior means ( @xcite ; morris @xcite ; @xcite ) .",
    "hierarchically , the uniform formal prior @xmath60 is suggested by the fact that the renowned james  stein estimator is the posterior mean , exactly , if this flat prior is extended ( inappropriately ) to @xmath61 ( @xcite , @xcite ) .",
    "section  [ secadm ] starts with the `` equal variances case , '' stein s setting ( @xcite ) for which @xmath62 .",
    "although equal variances are unusual in practice , this situation provides a rich and meaningful structure that has been studied widely because of its relative simplicity for mathematical investigation . among other advantages , when @xmath63 and the unknown means @xmath64 must be estimated , the equal variances situation allows easy recovery of risks and coverage probabilities merely by translating these quantities from the simpler ( @xmath65)-dimensional situation when shrinkages are toward known means @xmath66 .",
    "also with equal variances , adm approximations to bayes rules are easily developed for the range of scale - invariant priors ( [ eqsuperharmonica ] ) , merely by solving a quadratic equation for @xmath5 .",
    "section  [ secadm ] continues by extending these adm rules for the `` unequal variance case '' ( the variances @xmath16 differ , as is common in practice ) .",
    "section 2.8 introduces a new , more general approximation for the posterior means and variances , which allows any @xmath67 so that shrinkages can be toward an estimated regression . with computational and programming methods similar to those of reml , noticeably more accurate procedures emerge .",
    "section  [ secapproxaccuracy ] examines how well adm methods approximate the exact bayes rule .",
    "these approximations are good for small values of @xmath8 and they become exact as @xmath68 .",
    "even the data analyst who insists on exact computations can find such approximations useful because of increased speed , even if only for doing preliminary analyses .    for the case",
    "@xmath58 when @xmath5 is flat , section  [ seccoverageprobrisk ] evaluates the resulting adm - shp procedure s performance in repeated sampling for relative mean squared errors and for interval coverages . in the equal variances case , and in the unequal variance examples considered ,",
    "nominal coverages are achieved or exceeded for any @xmath69 .",
    "mle and reml procedures can not do this .",
    "this section starts by examining the inadequacy of mle methods as a basis for inferences about shrinkage factors @xmath4 and random effects , and why the adm approach for shrinkage constants should be better . for most of this section @xmath27 , the dimension of @xmath24 ,",
    "so that @xmath24 and all @xmath70 are assumed known .",
    "thus , the only unknown level-2 ( nuisance ) parameter is @xmath5 , the between groups variance that governs the shrinkage factors @xmath71 . with @xmath27 , ( 3 ) and ( 4 ) simplify slightly to @xmath72 let @xmath73 independently .",
    "@xmath74 is a ( minimal , if all @xmath16 differ ) sufficient statistic for @xmath75 .",
    "then @xmath76 for @xmath77 are independent unbiased estimates of @xmath5 with@xmath78 .",
    "one could average these  @xmath79 , weighted by the reciprocal of these variances to estimate @xmath5 , iteratively until convergence , with a negative estimate of @xmath5 reset to 0 .",
    "this produces @xmath80 , the mle of @xmath5 ( @xcite ) .        in the equal variances case , @xmath81 is complete and sufficient for @xmath5 , @xmath82",
    ". then @xmath83 is unbiased for @xmath5 .",
    "of course , @xmath84 can be negative , and @xmath85 , where the equal shrinkages are @xmath86 . because @xmath8 exceeds the median of @xmath87 , @xmath88 if @xmath89 is near 1 so that @xmath5 is near to zero .",
    "this inequality holds for any @xmath8 if @xmath90 , in which case @xmath91 and @xmath92 more often than not .",
    "this issue of @xmath80 being zero or quite small has received theoretical attention at least since @xcite , and has been recognized for some time in practice ( @xcite ) , because its occurrence is not rare .",
    "still , the problem has yet to be sufficiently recognized so as to be avoided in practice , and avoided in widely used software .",
    "when @xmath27 the likelihood function is proportional to @xmath93 \\\\[-8pt ] \\nonumber & & { } \\cdot \\exp\\biggl\\ { - \\frac{1}{2 } \\sum_{i=1}^{k } s_{i } / ( v_{i } + a ) \\biggr\\ } .\\end{aligned}\\ ] ] this is positive at @xmath94 and decreasing near 0 if the @xmath95 s are small enough to make the exponential term be nearly constant .",
    "then 0 is a local maximum and if @xmath92 fisher s information can not be used to assess the variance of the mle .",
    "when @xmath80 = 0 , the mle of @xmath96 also is zero . an unwary data analyst who uses this for",
    "the width of a confidence interval would assert that @xmath97 with arbitrarily high confidence .",
    "the left panel of figure  [ figexmleboundary ] illustrates a case when the logarithm of the posterior density of @xmath5 , equivalently the log - likelihood @xmath98 since @xmath5 has a  flat prior , can not use fisher s observed information to estimate the variance of @xmath5 since @xmath99 , there is no stationary point , and the second derivative is not negative .",
    "the situation for these data is much improved by using adm to arrive at the adjusted log - likelihood in the middle and right panels of figure  [ figexmleboundary ] .",
    "mle methods , viewed from a bayesian ( posterior probability ) perspective , amount to finding the posterior mode of a parameter s distribution and its variance ( reciprocal of observed information ) when the parameter has a flat prior distribution .",
    "normal distributions are used to approximate the mle s distribution based on two derivatives of the log - likelihood . that works well when the likelihood is approximately normal , for example , with large samples , but it works poorly when likelihoods are quite non - normal",
    ", as can happen when estimating shrinkage factors .",
    "@xcite , on approximating posterior distributions , showed how to fit any prespecified pearson family ( normal , gamma , f , beta , t , etc . ) to a density ( but also a likelihood function ) by calculating two derivatives of the `` adjusted '' ( posterior ) density function .",
    "the adjustment , multiplying by the quadratic or linear function that generates the particular pearson family , makes the maximizer approximate the mean of the parameter , and not its mode .",
    "for a nearly symmetric bell - shaped distribution or likelihood , the normal is the best pearson approximation , the adjustment is a constant .",
    "then the mode agrees with the mean and the mle is the adm . for skewed likelihoods ,",
    "the statistician may be able to choose a better approximating pearson family , for example , the beta family for shrinkage factors .",
    "the following factors compare the adm and its  fitting process , perhaps starting with a flat prior on  @xmath5 , with that of the mle .    1 .",
    "an adm fit is accomplished via a  complexity level comparable to the mle , that is , both require two derivatives .",
    "2 .   normality .",
    "if a normal distribution is chosen for the matching pearson family , the adm approach agrees exactly with the mle , and the variances in both cases are estimated by using fisher s observed information .",
    "3 .   asymptotics .",
    "no matter which pearson distribution is chosen , adm provides the same asymptotic inferences ( for large @xmath8 ) as the mle .",
    "this holds because each pearson family has an asymptotic normal limit .",
    "4 .   linear expectations . while various transformations of a parameter can be considered for the mle , adm targets the mean .",
    "for example , shrinkage factors @xmath4 enter linearly in ( [ eqcondtheta ] ) , so we approximate their means and variances , not @xmath5 or some other function of @xmath4 .",
    "the adm procedure could be termed alm ( adjustment for likelihood maximization ) , to parallel with mle language .",
    "alm and mle both work best when a version of the parameter is chosen to represent vague prior information , giving a relatively flat prior .",
    "we will see that adm - shp amounts to maximizing not the likelihood of  @xmath5 , as the mle does , but the likelihood after adjustment via multiplication by @xmath5 .",
    "@xcite proposed using `` adjusted maximum likelihood estimator '' that is identical to adm if @xmath27 .",
    "they showed its advantages in small area estimation for estimating shrinkages and for constructing parametric bootstrap prediction intervals .",
    "multivariate adm ?",
    "adjustments for density maximization agree with the mle for approximations via the multivariate normal .",
    "the paucity of non - normal multivariate pearson families restricts adm s extensions of the mle to univariate parameters .",
    "however , hybrid extensions are possible , and here we use a multivariate normal to approximate the @xmath22-dimensional vector @xmath24 and a  beta distribution for a shrinkage factor .    given a prior distribution on @xmath75 , say @xmath100 ( proper or not ) , and still with @xmath27 , knowledge of @xmath101\\quad \\mbox{and}\\quad v_{i } \\equiv\\operatorname{var}_{\\mbox{\\tiny{$\\pi$ } } } ( b_{i } | y ) \\label{eqpostmomentsb}\\ ] ] enables computation of two moments of @xmath0 , which with @xmath42 ( @xmath64 known ) are @xmath102 & = & ( 1 - \\hat{b}_{i } ) y_{i } + \\hat{b}_{i } \\mu_{i } , \\label{eqpostmeantheta } \\\\",
    "\\operatorname{var } ( \\theta| y ) & = & v_{i } ( 1 - \\hat{b}_{i } ) + v_{i } ( y_{i } - \\mu_{i } ) ^2 .",
    "\\label{eqpostvartheta}\\end{aligned}\\ ] ]    the second variance component in ( [ eqpostvartheta ] ) often is not represented in mle applications , understating variances and encouraging overconfidence .",
    "each of the following issues can cause overassessment of the information in the data .",
    "this perfect storm can have serious consequences when @xmath8 is small or moderate .",
    "1 .   nonlinearity .",
    "the posterior means and variances of the random effects are linear in @xmath103 , not in @xmath5 .",
    "@xmath104 is a convex function of @xmath5 , even if @xmath45 were unbiased for @xmath5 , one sees , jensen s inequality , which states that @xmath105 ) > e [ b_{i } | \\mathbf{y } ] $ ] , indicates that the plug - in shrinkage estimate would be biased too large .",
    "this is why the james  stein estimator that shrinks according to @xmath106 , uses the @xmath107 in its numerator , and not @xmath8 ( as in the mle ) , and leads to smaller mean squared errors than when using mle shrinkages @xmath108 .",
    "2 .   boundary limits .",
    "normal approximations to @xmath103 put positive probability outside the boundaries of the interval @xmath109 $ ] .",
    "3 .   boundary pileup . while @xmath110 is guaranteed , even in the equal variances case @xmath111 is possible .",
    "the mle can not exceed 1 , but @xmath112 with positive probability .",
    "this pileup happens despite there being no prior distribution on @xmath5 , other than @xmath113 with certainty , that can allow @xmath114 = 1 $ ] for any observation @xmath115 .",
    "4 .   skewness .",
    "@xmath116 tends to be right - skewed , substantially when the modal value of @xmath5 is small .",
    "alternatively , choose a fixed @xmath11 and replace @xmath5 by @xmath103 in the likelihood by substituting @xmath117 in @xmath118 .",
    "the resulting likelihood function of @xmath103 will be left - skewed .",
    "approximating such a skewed likelihood by a symmetric ( normal ) distribution overstates the magnitude of @xmath103 .",
    "a beta density better approximates an asymmetric likelihood .",
    "5 .   zero variances .",
    "the mle approach assesses@xmath119 as being @xmath120 .",
    "when@xmath99 , this approach in effect attributes perfect certainty to @xmath94 and that @xmath97 .",
    "variance components . estimating the varianceof  @xmath0 by plugging into @xmath121",
    "overlooks the variance component @xmath122 which would account for the uncertainty in @xmath5 when estimating @xmath123 . ignoring the term @xmath124 amounts to setting @xmath125 .",
    "all six of these biases produces overconfidence .",
    "the unknown variance @xmath5 is underestimated , shrinkage  @xmath103 is overestimated , and @xmath126 is underestimated .",
    "the applications here require approximating the means and variances of the shrinkage factors @xmath103 , @xmath110 .",
    "beta distributions are constrained to , so are the obvious approximating pearson distribution .",
    "consider an exact beta distribution for @xmath89 with @xmath127 and density @xmath128 maximizing over @xmath89 gives @xmath129 , the mode ( if  @xmath130 ) , not the mean .",
    "the `` adjustment '' for the beta distribution maximizes the product @xmath131 , giving @xmath132 , the mean of the @xmath133 distribution . maximizing a beta density after multiplying by @xmath134 produces the mean , not the mode .",
    "now let @xmath135 this is a concave function , maximized uniquely at a  point interior to ( 0,1 ) .",
    "we have at @xmath136 .",
    "then @xmath137 thus , given @xmath138 and @xmath139 allows one to recover  @xmath140 and @xmath141 via @xmath142 and @xmath143 .",
    "if @xmath144 is a beta@xmath145 density , exactly , then @xmath146 if a density @xmath147 is not exactly beta but it lies near to a beta density , the adm approach proceeds similarly , based on two derivatives of log@xmath148 , and approximates @xmath149 = \\int_{0}^{1 } b f(b ) \\,db$ ] by @xmath138 , the maximizer of this adjusted density .",
    "the variance @xmath150 is approximated by ( [ eqmomentsofadjustmentbeta ] ) , starting with @xmath151 that is , adm for a beta approximation first finds @xmath152 .",
    "then it determines @xmath153 and uses that to approximate @xmath154 by @xmath155 this beta distribution approximation to a density on [ 0,1 ] is exact if the original density is a beta exactly , and it will be a good approximation if the match is close .",
    "its asymptotic accuracy can be evaluated favorably ( @xcite , with discussion ) .",
    "it is useful when fitting shrinkages @xmath156 to re - express the results just outlined in terms of @xmath5 , or equivalently in terms of its logarithm @xmath157 log@xmath158 , being sure to include the jacobian in the posterior density .",
    "instead of using derivatives of @xmath159 , the `` invariant information '' will be calculated , defined by @xmath160 the derivative @xmath161 , which gives @xmath162 as @xmath163 , we have @xmath164 .",
    "@xmath165thus , if @xmath147 is ( nearly ) a beta density @xmath166 , then @xmath167 = \\frac{a_{1}}{a_{1}+a_{0 } } = \\hat{b}$ ] with @xmath168 , and the ( approximate ) variance is @xmath169 \\\\[-8pt ] \\nonumber & = & \\frac { ( \\hat{b } ( 1- \\hat{b } ) ) ^2 } { \\operatorname{inv.info } + \\hat{b } ( 1- \\hat{b } ) } .\\end{aligned}\\ ] ] use of this invariant information is especially valuable because of the identity @xmath170 \\\\[-8pt ] \\nonumber & = & - \\frac { d^2 \\ell ( b ( a(\\alpha ) ) ) } { d \\alpha^2 } .\\end{aligned}\\ ] ] this follows from @xmath171 with @xmath172 .",
    "the invariant information is the negative second derivative with respect to @xmath173 of @xmath174 , being the log density written as a function of @xmath173 : @xmath175 thus , inv.info agrees with fisher s observed information , but only if the parameter is @xmath172 .",
    "now return to the normal model with @xmath27 and likelihood function @xmath176 .",
    "suppose @xmath177 has a prior density @xmath39 , not necessarily proper , and consider the shrinkage coefficient for component @xmath11 , @xmath178 , @xmath179 .",
    "the posterior density for @xmath103 , given @xmath115 , is proportional to @xmath180 , where @xmath181 and @xmath182 .",
    "then@xmath183 is proportional to the density of @xmath103 . to apply adm ,",
    "define @xmath184 still thinking of @xmath5 as a function of @xmath103 , @xmath185    the following theorem summarizes what has just been demonstrated about the adm approximation by a beta distribution for @xmath186 , starting with a posterior density on @xmath5 that is proportional to @xmath187 .",
    "[ adm - theorem1approx ] given a prior density @xmath39 and a  likelihood function @xmath176 , the adm procedure for a  beta distribution approximates the first two posterior moments of @xmath103 as @xmath188 = \\hat{b}_{i } = \\frac{v_{i}}{v_{i}+\\hat{a } } , \\label{eqebgivenyisvovervplushata}\\ ] ] where @xmath189 , @xmath190 , and    @xmath191    with @xmath192 .",
    "neither @xmath45 nor the invariant information depends on @xmath11 or on @xmath16 .",
    "admissible rules , which are bayes and extended bayes rules ( per the `` fundamental theorem of decision theory '' ) , can provide good frequency properties if they are based on priors that let the data speak .",
    "one way to do that restricts to scale invariant improper priors @xmath193 , @xmath194 . as discussed earlier , given @xmath8 , these priors with @xmath195 ( @xmath196 , but not too small ) produce estimators of  @xmath197",
    "whose posterior means are minimax estimators for squared - error loss in the equal variance setting , so that for all vectors @xmath54 ( fixed ) , @xmath198 @xmath199 .",
    "\\label{eqsteinsense}\\ ] ] the choice @xmath200 , so @xmath201 , puts essentially all mass at @xmath5 nearly 0 , making @xmath202 with certainty , no matter what the data say",
    ". this choice must be avoided , but sometimes it is not .",
    "as @xmath203 increases , shrinkages @xmath204 decrease . for @xmath58 and for some smaller values ,",
    "down to @xmath205 , minimax and admissible estimators result .",
    "our preference @xmath206 is equivalent to stein s harmonic prior , that is , for @xmath207 , @xmath208 , the ( improper ) measure on @xmath54 is seen to be @xmath56 .",
    "this is the density of @xmath54 if , independently for @xmath209 , @xmath210 and @xmath211 , as seen from @xmath212 this prior with @xmath58 , that is , @xmath213 , is strongly suggested in the equal variance case by  the fact that the james  stein shrinkage constant @xmath214 is precisely the posterior mean @xmath215 $ ] if  @xmath216 .",
    "lopping off the impossible part where @xmath217 leads to @xmath218 ( @xcite ) . that the james  stein estimator is asymptotically optimal for large @xmath219 further suggests its use , that is , choosing @xmath58 . still in the equal variances case ,",
    "some values of @xmath220 , for example @xmath221 , shrink harder , which lowers the summed mean squared error if @xmath222 is suspected not to be large .",
    "experience with this flat prior on @xmath5 has borne out its good frequency properties in a variety of situations , also including for unequal variances .",
    "supporting evidence is given in sections 3 and 4 .",
    "the exact posterior means and variances of @xmath223 for @xmath58 , @xmath5 being uniform ( @xcite ) , are as follows .",
    "denote @xmath224 , so @xmath225 when @xmath27 .",
    "if @xmath63 , the dimension of @xmath24 , then the one can shrink toward the @xmath22-dimensional fitted subspace determined by @xmath226 .",
    "in the ( @xmath65)-dimensional space orthogonal to the range of @xmath227 , shrinkage is toward the @xmath228-vector .",
    "we therefore can focus on that @xmath229 subspace with @xmath27 and  @xmath8 replacing @xmath229 ( or think of shrinkage as toward a known , fixed vector @xmath230 as here ) . now with @xmath231 , let @xmath232 , and let @xmath233 .",
    "the james  stein estimate is @xmath234 .",
    "let @xmath235 be the moment generating function of a @xmath236 distribution at @xmath237 , a confluent hypergeometric function ( @xcite ) , @xmath238\\,db^m \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & = & \\gamma(m+1)t^{-m}\\exp(t)p(\\chi^2_{2m}\\le2t).\\end{aligned}\\ ] ] then ( @xcite ) , @xmath239\\nonumber\\\\ & = & \\frac { m}{t}\\bigl(1 - 1/m_m(t)\\bigr ) \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & = & \\frac{(k - r-2 ) v } { s_{+}}\\\\ & & { } \\cdot \\frac { p(\\chi ^2_{2m+2 } \\leq s_{+ } / v ) } { p ( \\chi^2_{2 m } \\leq s_{+}/v ) } , \\nonumber\\\\ \\label{eqvarbexact } v_{\\mathrm{exact}}&\\equiv & { \\operatorname{var}}(b|s ) \\nonumber\\\\ & = & \\frac{1}{m } \\hat { b}_{\\mathrm{exact}}^2 - ( \\hat{b}_{\\mathrm{js } } - \\hat{b}_{\\mathrm{exact}})\\\\ & & \\hspace*{28pt}\\qquad\\cdot\\biggl(1-\\frac{m+1}{m } \\hat{b}_{\\mathrm{exact } } \\biggr).\\nonumber\\end{aligned}\\ ] ] with @xmath42 , it follows that @xmath240 \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & = & ( 1-\\hat{b}_{\\mathrm{exact } } ) y_i + \\hat{b}_{\\mathrm{exact } } \\mu_{i},\\\\ \\label{eqvarthetahatexact } s_{\\mathrm{exact } , i}^2 & \\equiv & { \\operatorname{var}}(\\theta_i | \\mathbf{y } ) \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & = & v ( 1 - \\hat{b}_{\\mathrm{exact } } ) + v_{\\mathrm{exact } } ( y_i - \\mu_{i})^2.\\end{aligned}\\ ] ] the elegance of these formulas for the equal variances case is striking .",
    "unfortunately , this disappears in the unequal variances case that invariably arises in practice , which motivates the search for relatively simple alternatives to exact calculations .",
    "maximum likelihood estimates have optimal asymptotic properties , but the small and moderate sample sizes ( @xmath8 ) that arise in hierarchical modeling applications may be too small for the mle to perform well . the mode of @xmath5 , or more relevantly of @xmath89 , may be quite inadequate approximations to the posterior mean that corresponds to a flat prior that makes the likelihood agree with the posterior density .",
    "figure  [ figexmleboundary ] provides a simple example for equal variances , scaled for a sample size @xmath241 with shrinkage toward zero ( @xmath27 ) and a sufficient statistic @xmath242 .",
    "@xmath242 is the mode of a @xmath243 distribution , and also is the largest value of @xmath244 that makes the james  stein shrinkage estimate @xmath245 .",
    "likelihood graphs like this are not uncommon in practice , even when unequal variances occur .",
    "the right - most panels , which have made an adjustment to the likelihood , make it possible for two derivatives to capture the distribution , whereas there is no hope of this with the unadjusted left panel .",
    "figure  [ figbvst ] plots estimated shrinkages @xmath138 against @xmath246 , for values of @xmath247 , each panel showing three different estimation methods : the exact shrinkage estimate for the flat harmonic prior @xmath58 , shp ( solid curve ) ; the adm approximation to the same prior ( dotted ) ; and the mle @xmath248 .",
    "the mle shrinks much more heavily than the other two methods when @xmath237 ( or @xmath249 ) is small .",
    "the adm shrinkage curves are fairly close to the exactly computed expected shrinkage in each case , but are slightly more conservative .    when @xmath24 is unknown so that @xmath63 , the marginal distribution of @xmath5 is gotten by integrating @xmath24 out of the joint posterior density of @xmath24 and @xmath5 ( which is done in the next section , and extended to unequal variances ) .",
    "the marginal density is neatly written in this equal variances case in terms of the sum of squared residuals , @xmath250 and @xmath251 as @xmath252 \\\\[-10pt ] \\nonumber & & { } \\cdot\\exp \\biggl\\ { - \\frac{s_+}{2(v+a ) } \\biggr\\ } \\pi(a).\\end{aligned}\\ ] ] for @xmath253 , the logarithm of the adjusted density ( multiplying by @xmath5 ) is @xmath254 \\\\[-8pt ] \\nonumber & & { } - \\frac{tv}{v+a},\\end{aligned}\\ ] ] @xmath255 . with no covariates , @xmath27 ,",
    "this equation continues to hold with @xmath256 .",
    "now , @xmath257 the numerator of ( [ eqconvexqdtahat ] ) is a convex quadratic function of @xmath5 ( with @xmath258 ) which is negative at @xmath94 .",
    "it therefore has two real roots , one negative and unacceptable .",
    "the positive root is the adm estimator @xmath45 .",
    "then , @xmath259 \\\\[-8pt ] \\nonumber & = & \\frac{2(m - c+1)}{t+m+1+\\sqrt{(t - m-1)^2 + 4ct}}.\\end{aligned}\\ ] ] note that @xmath138 is monotone decreasing in @xmath237 and that @xmath138 reaches its maximum , @xmath260 at @xmath261 .",
    "shrinkage is bounded away from @xmath262 if @xmath263 , for example , if @xmath58 and @xmath27 the maximum shrinkage is @xmath264 .",
    "these shrinkages @xmath138 decrease as @xmath203 increases and as @xmath265 in ( [ eqsolutionbhat ] ) , @xmath266 .",
    "of course @xmath267 is not allowed because then the posterior guarantees @xmath262 shrinkage , no matter what the data say .",
    "define @xmath172 and @xmath268 .",
    "then for any @xmath203 , the invariant information@xmath269inv.info satisfies @xmath270 \\\\[-8pt ] \\nonumber & = & m(1 - \\hat{b})^2 + \\hat{b}^2 + ( 1-c)(1 - 2\\hat{b}).\\end{aligned}\\ ] ] matching the first and second derivatives of the two densities ( i.e. , of the adjusted density and of a @xmath271 density ) gives @xmath272 and this beta distribution has variance @xmath273 \\\\[-8pt ] \\nonumber & = & \\frac{\\hat{b}^2 ( 1-\\hat{b})^2}{m ( 1 - \\hat{b})^2 + ( 1-c ) + ( 2c-1 ) \\hat{b}}.\\end{aligned}\\ ] ] when @xmath58 , the adm approximations in this equal variances case to the posterior moments of @xmath274 are @xmath275     versus its own @xmath138 from two different methods .",
    "the solid line is from the exact method , that is , formulas  ( [ eqbhatexact ] ) and  ( [ eqvarbexact ] ) , and the dotted line is from the approximate method , formulas  ( [ eqbhatapprox ] ) and ( [ eqvarbhatapprox ] ) . ]    for the shp case @xmath58 in figure  [ figbvst ] , @xmath138 is plotted as a function of @xmath237 , showing that the adm estimate of @xmath89 shrinks slightly less than the exactly computed @xmath89 , while it matches exactly at @xmath276 , and asymptotes to the exact value for large @xmath237 .",
    "the mle produces much larger shrinkages .",
    "figure  [ figbvsvhat ] , as in figure  [ figbvst ] , also shows graphs for the shp ( @xmath58 ) and with curves for @xmath277 ( e.g. , if @xmath27 , then for @xmath247 ) .",
    "it reveals that the adm approximation to @xmath278 corresponds well with the exact posterior variance of a shrinkage factor , each as a function of its own shrinkage @xmath138 . in both cases the shrinkage @xmath138 decreases monotonically as the sufficient statistic @xmath249 rises .",
    "figure  [ figbvsvhat ] shows adm s excellent adm approximation of the exact variance , and that it becomes exact as @xmath237 nears 0 ( where maximal shrinkage in both cases is for @xmath138 = @xmath279 ) .    for any @xmath67 in this equal variance case , the preceding estimates of the shrinkages and of their variances provide the following estimates of the means and the variances of the random effects @xmath0 in terms of the adm approximations to the posterior moments @xmath138 and @xmath280 : @xmath281 \\\\[-8pt ] \\nonumber & = & ( 1-\\hat{b } ) y_i + \\hat{b } x_i ' \\hat\\beta , \\label{eqthetahatapprox } \\\\",
    "\\label{eqvarthetahatapprox } s_{i}^2 & \\equiv&\\widehat{{\\operatorname{var}}}(\\theta_i | y ) \\nonumber\\\\ & = & v(1-\\hat{b } ) + v\\bigl(x'_i ( x'x)^{-1 } x_i\\bigr)\\hat{b } \\\\ & & { } + v ( y_i - x'_i\\hat{\\beta})^2.\\nonumber\\end{aligned}\\ ] ] note that @xmath282 depends on @xmath11 by increasing proportionally to the squared residual , as one would expect because mis - estimation of @xmath89 hardly matters when @xmath283 is small .",
    "these results are seen most easily by using a least squares regression predictor in the @xmath22-dimensional range space of @xmath227 , and shrinking to @xmath228 in the ( @xmath229)-dimensional orthogonal subspace .",
    "the extension to the unequal variance case , which is next , is more complicated .",
    "an adm approach to fitting our general model starts by integrating out the @xmath284 to get , in matrix notation , @xmath285 where @xmath286 is a @xmath8-by-@xmath8 diagonal matrix . with @xmath24 having a flat prior on @xmath287 , standard calculations with ( [ eqgenmodygivenbetaa ] ) lead to @xmath288 with",
    "@xmath5 known , @xmath289 is at once both the posterior mean and the weighted least squares estimate of @xmath24 .",
    "the full distribution , given @xmath5 , is @xmath290    the objective is to make inferences about the vector @xmath291 with conditional distribution @xmath292 \\\\[-8pt ] \\nonumber & & \\hspace*{58pt}(i - b_a)v\\bigr).\\end{aligned}\\ ] ] this is ( [ eqcondtheta ] ) in matrix notation , with @xmath293 the  identity matrix , @xmath294 and @xmath295 integrating out @xmath24 , with help from ( [ eqgenmodbetahat ] ) , it follows that @xmath296 where in ( [ eqgenmoddistrthetaintegrateoutbeta ] ) @xmath297 is a @xmath298 projection matrix of rank  @xmath22 , @xmath299    when @xmath5 has prior density element @xmath300 , the posterior density of @xmath5 , given @xmath301 , follows : @xmath302 the logarithm of this adjusted posterior density , with @xmath303 , is @xmath304 \\\\[-8pt ] \\nonumber & & { } -\\frac{1}{2}\\log    & & { } -\\frac{1}{2}(y - x\\hat\\beta_a)^{\\prime } d_{v+a}^{-1}(y - x\\hat\\beta_a).\\nonumber\\end{aligned}\\ ] ] denote @xmath305 , set @xmath306 , and define @xmath307 then the adm approximation , with @xmath308 , is @xmath309 with approximate mean @xmath310 and variance @xmath311 both moments depending on the prior @xmath39 . maximizing @xmath312 and determining its second derivative at @xmath313 , the negative of the invariant information , can be done by numerical methods , by newton s method ( which requires matrix derivatives ) , or by other means that include an em technique available in @xcite .    given @xmath314 and the values @xmath315 , one could insert @xmath314 into ( [ eqgenmoddistrthetaintegrateoutbeta ] ) to estimate both posterior moments of the @xmath0 .",
    "however , that underestimates the variance and makes no use of the @xmath316 , so we proceed as follows , leading to a main theorem .",
    "define @xmath317 as @xmath289 evaluated at @xmath314 and @xmath318 then from ( [ eqgenmoddistrthetaintegrateoutbeta ] ) , and approximating @xmath289 by @xmath317 , @xmath319    to minimize complications in making our final approximations to @xmath320 and @xmath321 , we neglect variations of @xmath289 in ( [ eqgenmodbetahat ] ) and @xmath297 in ( [ eqgenmodpa ] ) as @xmath5 varies around @xmath314 .",
    "this is exact in the equal variances case because both @xmath289 and @xmath297 do not depend on @xmath5 , and it will be nearly true if the @xmath322 differ only slightly . with unequal variances",
    "both @xmath289 and ( [ eqgenmodpa ] ) involve weights that depend on @xmath323 .",
    "if @xmath314 is near @xmath5 , as happens when @xmath8 is large , then @xmath324 is near 1 . with data",
    ", one can evaluate @xmath325 these variances may be acceptably small , and @xmath326 diminishes as @xmath327 as @xmath328    assume the model ( [ eqlevel1 ] ) , ( [ eqlevel2 ] ) , and the prior in ( [ eqlevel3flat ] ) .",
    "write @xmath329 and @xmath330 as the adm approximations to @xmath331 and to @xmath332 assume @xmath333 and @xmath334 . then for @xmath335 @xmath336 \\\\[-8pt ] \\nonumber & & { } + v_i(y_i-\\hat y_i)^2.\\end{aligned}\\ ] ] here @xmath337 is the @xmath11th diagonal term in @xmath338 .",
    "equation ( [ eqgenmodtheoremexp ] ) follows from ( [ eqgenmoddistrthetaintegrateoutbeta ] ) , ( [ eqgenmodexpthetaiay ] ) and @xmath339 , since @xmath340 now use eve s law ( total variation ) to get , from  ( [ eqgenmoddistrthetaintegrateoutbeta ] ) and ( [ eqgenmodvarexpthetaiay ] ) , @xmath341 \\\\[-8pt ] \\nonumber & & { } + v_i(y_i-\\hat { y}_i)^2,\\end{aligned}\\ ] ] which is ( [ eqgenmodtheoremvar ] ) .    in our experience ,",
    "these regression approximations when @xmath342 and @xmath58 especially , have been quite satisfactory .",
    "@xcite provides a basis for making more precise approximations to @xmath343 and to @xmath344 based on matrix and determinant derivatives . in the equal variance case , the theorem s two moments are exact provided exact formulas for@xmath345 and @xmath346 are used .",
    "however , normality of @xmath347 does not hold exactly for @xmath0 after averaging over @xmath348 , although that normal approximation is commonly made .",
    "figures  [ figbvst ] and [ figbvsvhat ] show in the equal variance setting that even for small samples like @xmath247 , the adm approximation of the first two exactly computed posterior moments of @xmath89 is quite good .",
    "our end goal , however , is verifying this leads to good approximations of the posterior means and variances of each random effect ( @xmath349 ) .",
    "first , in the equal variance situation with @xmath42 , we compare the weighted average of posterior mean squared error of the @xmath0 values via the adm approximation with this measure with the `` exact '' posterior mean .",
    "let us measure the difference of their mean squared errors , given the data @xmath301 , by computing @xmath350 for the adm approximation , with the expectation calculated exactly , when @xmath53 .",
    "now @xmath351 where the subscript @xmath352 denotes estimates done exactly ( see section  [ subsecexactmomentseqvar ] ) , with @xmath353 is given in ( [ eqvarthetahatexact ] ) . therefore @xmath354 measures how well the adm approximation works for random effects estimates , smaller values indicating better approximations .",
    "the highest ( worst ) ratio is @xmath355 which occurs for @xmath8 near 20 , and for 60% shrinkage .",
    "greater accuracy holds for @xmath356 and for @xmath357 .",
    "thus , in the equal variances setting , the conditional mean squared errors of the adm approximation and the exact estimator of @xmath358 never differ by more than @xmath355 .",
    "now , still with @xmath60 , consider the unequal variance case and adm s accuracy for approximating the exact bayes estimator of @xmath359 .",
    "the following example involves two groups of variances for the  @xmath15 values , and estimates the unknown mean vector @xmath360 in the second level ( so @xmath361 ) .",
    "five `` small '' variances are set at , and five `` large '' ones at @xmath362 .",
    "their  maximum - to - minimum variance ratio is a factor of  10 , and their harmonic mean is 1.0 ( for convenience only ) .",
    "shrinkages @xmath363 are toward the nine - dimensional subspace orthogonal to the unit vector .",
    "we calculated exact and adm means and variances of these shrinkages , which depend on the separate values of the two - dimensional statistic @xmath364 ( these two sums of squares are standardized by their respective  @xmath365 , each summed over its respective subgroup of size  5 , both centered on their common fitted grand mean ) .",
    "figure  [ figb1vstunequal ] concerns shrinkages for the first five components with small variances , @xmath366 , and figure  [ figb2vstunequal ] shows shrinkages for the five components with large variances , @xmath367 .",
    "the left panels of each figure show shrinkage factor patterns for three different rules : the mle ( dashed curve ) , the exactly computed shrinkage using the harmonic prior for which  @xmath5 has a flat density ( solid curve ) , and the adm approximations to that shrinkage factor ( dotted curve ) .",
    "these are graphed as a function of @xmath368 ( figure  [ figb1vstunequal ] ) and @xmath369 ( figure  [ figb2vstunequal ] ) with separate displays , each conditional on one of four different values of the opposite @xmath370 .",
    "both figures show that the mle has quite large shrinkages , just as for equal variances .",
    "the relationship between the adm approximation and the exactly computed expected shrinkage that the adm approximates is similar to what was seen in the equal variance case .",
    "the right - hand panels of each figure show good agreement between the adm variance approximation and the exactly computed variances @xmath330 when each is plotted against its own shrinkage .",
    "the maximum shrinkages for adm and the exact rule are limited to values @xmath371 , curtailing the horizontal axes for plots of @xmath372 .    to summarize for the prior @xmath60 , the adm approximations of exact shrinkage factors for posterior means and variances of shrinkage factors are slightly conservative , but generally are in good agreement with the exact values obtained in the equal variance case .",
    "similar results hold for the unequal variance case when variances @xmath16 differ by a factor of 10 and when @xmath373 .",
    "confidence interval coverage rates for @xmath0 are evaluated next for the two main procedures of section  2 , both based on assuming @xmath374 has a flat prior @xmath53 so that the posterior density is the likelihood function .",
    "one procedure , labeled `` exact '' here , evaluates the exactly computed posterior means and variances of @xmath0 , given @xmath301 , as in ( 34 ) and ( 35 ) for the equal variances case , and otherwise by numerical integration .",
    "it then assigns a normal distribution with these two moments to determine a posterior interval .",
    "the second approach uses normal distributions in the same way , but centered and scaled via the adm approximations of these two moments in ( 44 ) and ( 45 ) , or when @xmath67 and with unequal variances , as in ( 57 ) and ( 58 ) .",
    "normal distributions are not exact for  @xmath0 , since the actual distributions are skewed ( right - skewed for relatively large @xmath15 , and left - skewed for small @xmath15 ) .",
    "this matters less in repeated sampling evaluations that randomize over @xmath301 , making skewnesses average to zero for each  @xmath11 .        for all @xmath209",
    ", we seek two - tailed frequency coverage probabilities as a function of @xmath5 : @xmath375,\\end{aligned}\\ ] ] when the nominal coverage is @xmath50 , so @xmath376 .",
    "each procedure studied uses its own estimate @xmath377 of the conditional variance of @xmath0 .",
    "a related measure directly assesses how well each @xmath377 envelops the expected squared error , given @xmath5 , with values @xmath378 indicating that @xmath379 assigns sufficiently large intervals : @xmath380 details of the simulation are in @xcite , where rao  blackwellization increased the accuracy by evaluating some conditional normal distributions exactly , given @xmath5 and @xmath301 .",
    "that is , for ( [ eqrisk2 ] ) , @xmath381 \\\\ & & \\quad =   e \\biggl[\\phi\\biggl\\{\\frac{\\hat{\\theta}_i - ( 1-b_i ) y_i -b_i x'_i \\beta+ z^\\ast s_i}{\\sqrt{v_i ( 1-b_i)}}\\biggr\\}\\\\ & & \\qquad{}- \\phi\\biggl\\{\\frac{\\hat{\\theta}_i - ( 1-b_i ) y_i - b_i x'_i \\beta -z^\\ast s_i}{\\sqrt{v_i ( 1-b_i)}}\\biggr\\}\\big| a \\biggr].\\end{aligned}\\ ] ]          figure  [ figcoveragetheta ] plots the actual coverage probabilities for the three confidence interval procedures , each against the possible `` true '' @xmath89 values , for three equal variance procedures always with @xmath42 , and for @xmath382 ( @xmath383 ) , @xmath241 ( @xmath384 ) and @xmath385 ( @xmath386 ) . for each @xmath387 ,",
    "@xmath388 data sets were generated and the interval procedures for `` exact , '' its adm approximation , and the mle were evaluated and averaged to estimate the coverage probabilities .",
    "confidence intervals for the mle were determined simply by taking each variance to be the mle @xmath389 .",
    "these mle coverages are plotted with long dashes in figure  [ figcoveragetheta ] .",
    "when shrinkage @xmath89 is large , these mle intervals give poor coverages , ultimately dropping to just under @xmath390 , as shown in section 2 .",
    "the graph of figure  [ figcoveragetheta ] is redone in the first row of figure  [ figcoveragestdrisktheta ] , but without the mle . that allows an amplified scale that shows the slight differences in coverage rates between the `` exact '' rule and its adm - shp approximation .",
    "the adm - shp coverages meet or exceed @xmath391 for all @xmath5 ( within simulation error ) .",
    "the `` exact '' procedure s coverages can be slightly nonconservative , but its lowest coverage is at least @xmath392 ( when @xmath385 and @xmath393 ) for all @xmath8 shown . the adm - shp intervals achieve ( or exceed ) their nominal @xmath394 coverage rates by having slightly wider intervals than `` exact , '' due to adm s reduced shrinkage estimates and its larger variance estimates @xmath278 , as studied in section  3 . as @xmath89 increases",
    "both methods become quite conservative , with coverages well above @xmath394 .",
    "the bottom row of figure  [ figcoveragestdrisktheta ] plots the function  ( [ eqrisk2 ] ) against @xmath89 to compare the two different methods . values less than 1.0 indicate that the estimated variances @xmath377 average to as much as or more than the average mean square .",
    "this further suggests that the interval coverages will ( nearly ) provide the nominal coverage ( @xmath50 ) for all values of @xmath374 .",
    "we return to the unequal variances example of section 3 with @xmath241 , @xmath373 , @xmath395 , and @xmath396 . for this simulation ,",
    "@xmath397 data sets were generated for each of 50 values @xmath398 , where @xmath399 and @xmath400 is the harmonic mean of the @xmath16 .",
    "nominal @xmath401 confidence intervals for each @xmath0 were evaluated for each data set .",
    "the confidence rates and average calibrated losses ( [ eqrisk2 ] ) then were averaged over the simulated values .",
    "figure  [ figcoveragethetaunequal ] plots coverages of the adm - shp intervals and calibrated risk functions ( [ eqrisk2 ] ) for @xmath402 and for @xmath403 as @xmath404 varies",
    ". the upper left panel of figure  [ figcoveragethetaunequal ] plots the coverage probabilities against @xmath405 for the group of five with small variances @xmath406 , and the upper right for the remaining group of five with large variances @xmath407 . as @xmath405 increases and @xmath5 decreases , coverage rates generally increase .",
    "coverages achieve or exceed their nominal @xmath394 levels ( within simulation error ) , while for small @xmath5 and big  @xmath405 , coverages for the large variance group substantially exceed both their nominal rate and the coverages for the small variance group .",
    "the calibrated risks are less than 1.0 in figure  [ figcoveragethetaunequal ] which show that the intervals are wide enough to be conservative , although they may be excessively conservative for the large variance group .",
    "one remedy could be using the scale - invariant prior @xmath408 , which makes @xmath409 flat .",
    "coverages rates for the exact version of shp were not evaluated for this unequal variance case , and that can be time - consuming for repeated sampling .",
    "simple and fast computing , plus a procedure s transparency , are reasons for finding simple and accurate approximations .    .",
    "here @xmath410 , @xmath395 , @xmath411 . ]",
    "why might a bayesian or objective bayesian statistician who has settled on prior distribution @xmath39 on @xmath5 consider approximating with adm ?",
    "there are several reasons , beyond the general observation that any procedure used in an application is an approximation .    1 .",
    "speed of convergence is valuable with big data sets , especially if a procedure is to be used repeatedly for model selection and model checking .",
    "the approximations here avoid mcmc burn - ins .",
    "speed also makes it feasible to simulate many times , for example , for bootstrapping , or to check a procedure s operating characteristics .",
    "data analysts may need to obtain the same results each time a particular model is re - fit to the same data , which stochastic approximations do not do .",
    "3 .   mle methods always will play a central role in statistics . for the model of this paper , adm maintains the spirit of mle while making small sample improvements .",
    "4 .   using adm to help fit shrinkage factors extends to multilevel generalized linear models , for example , to fit a poisson model ( @xcite ) . in such more complicated non - normal models , mcmc and exact numerical integration may be more difficult or impossible , giving mle and adm a greater advantage of ease",
    ". then the frequency properties of adm can be checked with each data application by simulating or bootstrapping from the fitted multilevel model . however",
    ", that will not reveal how well adm approximates the exact bayes procedure .",
    "5 .   multiplying a likelihood by @xmath5 before maximizing combines neatly with em methods as used to find the mle of @xmath5 ( @xcite ) . with adm",
    ", em would avoid infinite loops that occur when the mle @xmath46 .",
    "data analysts always will need well - checked , prepackaged , documented , widely known and available procedures for fitting models .",
    "statistical software programmers should find it easy to program and adopt the adm - shp formulas , for example , the formulas of section 2.8 , in standard software .",
    "for example , adm could be an option in sas proc mixed along with mle and reml .",
    "barring prior information that @xmath5 is likely to be small , the adm - shp methods developed here for  making inferences , especially interval estimates , about the random effects in a two - level normal regression model will have better frequency performance over the entire range of @xmath7 than mle and reml methods .",
    "our derivation has benefited from viewing stein s harmonic prior shp on the random effects  @xmath0 as arising from a uniform mixture over @xmath5 of the level-2 normal distribution @xmath412 , that is , according to @xmath53 .    with this formal ( improper ) prior",
    ", the posterior density on @xmath5 agrees with the marginalized likelihood function @xmath10 .",
    "that justifies the term `` adjustment for likelihood maximization '' when `` alm '' is restricted to point estimation of a shrinkage factor .",
    "the results here go on to use the flat @xmath53 prior and conditional ( bayesian ) reasoning as a guide to accounting for variability of the shrinkage factors  @xmath4 and ultimately , of the random effects @xmath0 .",
    "adm approximates the exact bayes procedures with considerable accuracy , given that it retains the ( relative ) ease of mle / reml calculations , that is , by using two derivatives of the adjusted log - likelihoodlog(@xmath5 @xmath10 ) .",
    "of course the adjustment here more generally would adjust by using the multiplier @xmath39 if @xmath413 .",
    "while more testing is needed for unequal variances cases , the confidence intervals for random effects arising from the adm - shp combination here thus far have met or exceeded their nominal coverages if @xmath414 . still , the search should continue for priors on @xmath5 that will provide even better frequency interval coverages .",
    "the authors gratefully acknowledge funding for this project provided in part by nsf grant dms-97 - 05156 ,  and for many helpful suggestions made by the editors , the associate editor and a referee ."
  ],
  "abstract_text": [
    "<S> we develop and evaluate point and interval estimates for the random effects @xmath0 , having made observations @xmath1 , i = 1 , \\ldots , k$ ] that follow a two - level normal hierarchical model . fitting this model </S>",
    "<S> requires assessing the level-2 variance @xmath2 to estimate shrinkages @xmath3 toward a ( possibly estimated ) subspace , with @xmath4 as the target because the conditional means and variances of @xmath0 depend linearly on  @xmath4 , not on @xmath5 . </S>",
    "<S> adjustment for density maximization , adm , can do the fitting for any smooth prior on @xmath5 . like the mle , adm bases inferences on two derivatives , but adm can approximate with any pearson family , with beta distributions being appropriate because shrinkage factors satisfy @xmath6 .    </S>",
    "<S> our emphasis is on frequency properties , which leads to adopting a  uniform prior on @xmath7 , which then puts stein s harmonic prior ( shp ) on the  @xmath8 random effects . </S>",
    "<S> it is known for the `` equal variances case '' @xmath9 that formal bayes procedures for this prior produce admissible minimax estimates of the random effects , and that the posterior variances are large enough to provide confidence intervals that meet their nominal coverages . </S>",
    "<S> similar results are seen to hold for our approximating `` adm - shp '' procedure for equal variances and also for the unequal variances situations checked here .    for shrinkage coefficient estimation </S>",
    "<S> , the adm - shp procedure allows an alternative frequency interpretation . writing @xmath10 as the likelihood of  @xmath4 with @xmath11 fixed , adm - shp estimates @xmath4 as @xmath12 with @xmath13 . </S>",
    "<S> this justifies the term `` adjustment for likelihood maximization , '' alm .    . </S>"
  ]
}