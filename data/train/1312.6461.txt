{
  "article_text": [
    "in the backpropagation learning of a neural network , the initial weight parameters are crucial to its final estimates . since hidden parameters are put inside nonlinear activation functions , simultaneous learning of all parameters by backpropagation is accompanied by a non - convex optimization problem .",
    "when the machine starts from an initial point far from the goal , the learning curve easily gets stuck in local minima or lost in plateaus , and the machine fails to provide good performance .",
    "recently deep learning schemes draw tremendous attention for their overwhelming high performances for real world problems@xcite .",
    "deep learning schemes consist of two stages : _ pre - training _ and _ fine - tuning_. the pre - training stage plays an important role for the convergence of the following fine - tuning stage . in pre - training , the weight parameters are constructed layer by layer , by stacking unsupervised learning machines such as restricted boltzmann machines@xcite or denoising autoencoders@xcite . despite the brilliant progress in application fields , theoretical interpretation of the schemes is still an open question@xcite .    in this paper",
    "we introduce a new initialization / pre - training scheme which could avoid the non - convex optimization problem .",
    "the key concept is the probability distribution of weight parameters derived from murata s integral representation of neural networks@xcite .",
    "the distribution gives an intuitive idea what the parameters represent and contains information about where efficient parameters exist .",
    "sampling from this distribution , we can initialize weight parameters more efficiently than just sampling from a uniform distribution .",
    "in fact , for relatively simple or low dimensional problems , our method by itself attains a high accuracy solution without backpropagation .",
    "de freitas et al.@xcite also introduced a series of stochastic learning methods for neural networks based on the sequential monte carlo ( smc ) . in their methods",
    "the learning process is iterative and initial parameters are given by less informative distributions such as normal distributions .",
    "on the other hand we could draw the parameters from a _ data dependent _ distribution . furthermore , in smc , the number of hidden units must be determined before the learning , while it is determined naturally in our method .",
    "one of the most naive initialization heuristics is to draw samples uniformly from an interval @xmath0 $ ] .",
    "nguyen and widrow@xcite gave two fundamental points of view .",
    "first , since a typical activation function such as sigmoid and hyperbolic tangent is approximated as a linear function at its inflection point , one should initialize the hidden parameters in such a way that the inputs for each hidden unit are in the _ linear region_. second , since each hidden unit determines the slice of the _ fourier transformed _ input space , that is , each individual hidden unit responds _ selectively _ to only the inputs whose spatial frequency is in a particular band , one should initialize hidden parameters in such a way that the corresponding frequency bands cover the possible input frequencies .",
    "lecun et al.@xcite also emphasized the need to preset parameters in the linear region because parameters outside the linear region have small gradients and stray into more difficult nonlinear regions .",
    "they focused on the curvature of input vectors and proposed to use @xmath1 , where @xmath2 is the fan - in , or the dimensionality of input vectors .",
    "shimodaira@xcite proposed to initialize parameters such that corresponding _",
    "activation _ regions to cover whole the possible inputs .",
    "linear algebraic techniques are also employed .",
    "for example , shepanski@xcite used the pseudo inverse to determine the parameters of linear approximated neural networks , and yam and chow@xcite used the qr decomposition .",
    "integral transform viewpoints originated from more theoretical backgrounds than linear region viewpoints : the theoretical evaluation of the approximation power of neural networks . in the earliest stage , purely functional analysis methods were employed . in 1957",
    "kolmogorov@xcite showed that any multivariate continuous functions can be exactly represented by sums of compositions of _ different _ continuous functions of only one variable .",
    "inspired by the kolmogorov s theorem , hecht - nielsen@xcite and krkov@xcite applied the idea to neural networks , which are sums of compositions of the _ same _ sigmoid function . sprecher@xcite gave more constructive version of the proof and later implemented the improved proof as a learning algorithm of neural networks@xcite . in 1989 the universal approximation property of _ single _ layer neural networks has been investigated and the integral transform aspects emerged .",
    "carroll and dickinson@xcite introduced the radon transform and funahashi@xcite used the fourier analysis and the paley - weiner theory , whereas cybenko@xcite employed the hahn - banach and riesz representation theorems . in the following years , upper bounds of the approximation error were investigated@xcite .",
    "barron@xcite refined the jones result@xcite using the weighted fourier transform .",
    "krkov@xcite later developed the general theory of integral transforms .",
    "inspired by the barron s result , murata@xcite introduced a family of integral transforms defined by _ ridge functions _ , which are regarded as a hybrid of the radon and wavelet transforms .",
    "cands@xcite inherited murata s transforms and developed ridgelets , which was the beginning of the series of multiscale `` -lets '' analysis@xcite .",
    "those multiscale viewpoints also inherits the _ selective _ activation properties of neural networks .",
    "denoeux and lengell@xcite proposed to collect @xmath3 _ prototype _ vectors as initial hidden parameters .",
    "each prototype @xmath4 is drawn from its corresponding cluster @xmath5 , where the clusters @xmath6 are formed in a stereographically projected input space . in this manner",
    "each prototype @xmath4 comes to selectively respond to the input vectors @xmath7 which belongs to the cluster @xmath5 .",
    "this study is based on the integral transform viewpoint , and proposes a new way for practical implementation .",
    "although integral transforms have been well studied as theoretical integral representations of neural networks , practical implementations for training have been merely done .",
    "however integral representations have big advantage over linear region viewpoints in that they can give global directions how each neural units should behave , while the latter only give local directions .",
    "let @xmath8 be a neural network with a single hidden layer expressed as @xmath9 where the map @xmath10 is called the _ activation function _ ; @xmath11 and @xmath12 are called _ hidden parameters _ , and @xmath13 are _ output parameters_. with an ordinary _ sigmoid function _",
    "@xmath14 , the activation function @xmath10 is supposed to be the _ sigmoid pair _ in the form @xmath15 where @xmath16 normalizes the maximum value of @xmath10 to be one .",
    "we consider an _ oracle _",
    "distribution @xmath17 of hidden parameters .",
    "if such a distribution exists , we can sample and fix these hidden parameters according to @xmath17 first , and then we could fit the rest output parameters by ordinary linear regression .",
    "we call this two - stage framework as _ sampling regression ( sr ) learning_.    the candidates of @xmath17 could be some parametric distributions such as normal distributions or uniform distributions . in the following sections we derive a data dependent distribution from an integral representation of neural networks .      consider approximating a map @xmath18 with a neural network .",
    "murata@xcite defined an integral transform @xmath19 of @xmath20 with respect to a _ decomposing kernel _ @xmath21 as @xmath22 where @xmath23 is a normalizing constant .",
    "murata also showed that given the decomposing kernel @xmath21 , there exists the associating _ composing kernel _ @xmath24 such that for any @xmath25 , the inversion formula @xmath26 holds ( th.1 in @xcite ) where @xmath27 denotes the complex conjugate .",
    "the convergence factor @xmath28 is omitted when @xmath29 , which is attained when @xmath20 is compactly supported and @xmath30-hlder continuous with @xmath31 ( th.3 in @xcite ) , or compactly supported and bounded @xmath32-smooth ( cor.2 in @xcite ) .",
    "in particular one can set a composing kernel @xmath24 as a sigmoid pair @xmath10 given in eq.[eq : sigpair ] and the associating decomposing kernel as : @xmath33 where @xmath34 is a nonnegative @xmath35-smooth function whose support is in the interval @xmath36 $ ] .",
    "such a @xmath34 does exist and is known as a _mollifier_@xcite.the _ standard mollifier _",
    "@xmath37 is a well - known example .",
    "hereafter we assume @xmath24 is a sigmoid pair and @xmath21 is the corresponding derivative of the standard mollifier .",
    "we also assume that our target @xmath20 is a bounded and compactly supported @xmath38-smooth function .",
    "then the integral transform @xmath19 of @xmath20 is absolutely integrable and the inversion formula is reduced to the direct form @xmath39",
    ".    let @xmath40 be a probability distribution function over @xmath41 which is proportional to @xmath42 , and @xmath43 be satisfying @xmath44 for all @xmath45 . with this notations ,",
    "the inversion formula is rewritten as the expectation form with respect to @xmath46 , that is , @xmath47 the expression implies the finite sum @xmath48 converges to @xmath20 in mean square as @xmath49 , i.e. @xmath50 = f$ ] and @xmath51 < \\infty$ ] holds for any @xmath52 ( th.2 in @xcite ) . here",
    "@xmath53 is a neural network with @xmath54 hidden units , therefore we can regard the inversion formula as an _ integral representation _ of neural networks .",
    "now we attempt to make use of the integral transform @xmath55 as an oracle distribution @xmath17 of hidden parameters .",
    "although the distribution is given in the explicit form as we saw in the preceding section , further refinements are required for practical calculation .",
    "given a set @xmath56 of input and output pairs , @xmath57 is empirically approximated as @xmath58 with some constant @xmath59 which is hard to calculate exactly .",
    "in fact sampling algorithms such as the acceptance - rejection method@xcite and markov chain monte carlo method@xcite work with any unnormarized distribution because they only evaluate the ratio between probability values .",
    "note that the approximation converges to the exact @xmath57 in probability by the law of large numbers _ only _ when the input vectors are i.i.d .",
    "samples from a uniform distribution .    as a decomposing kernel @xmath21",
    "we make use of the @xmath60-th order derivative of the standard mollifier @xmath61 where @xmath62 if @xmath2 is even and @xmath63 otherwise .",
    "the @xmath60-th derivative @xmath64 of the mollifier takes the form @xmath65 where @xmath66 denotes a polynomial of @xmath67 which is calculated by the following recurrence formula : @xmath68 the higher order derivatives of a mollifier has more rapid oscillations in the neighbourhoods of both edges of its support .",
    "given a data set @xmath69 , our * sampling regression * method is summarized as below :    1 .   :",
    "calculate @xmath64 according to eq.[eq : deriv.mol ] , eq.[eq : p0 ] and eq.[eq : pk ] , where @xmath62 if @xmath2 is even and @xmath63 otherwise .",
    "then @xmath57 is calculated by eq.[eq : tab.approx ] with setting @xmath70 . as we noted above",
    ", one can choose arbitrary @xmath59 .",
    "draw @xmath52 samples @xmath71 from the probability distribution @xmath72 by acceptance - rejection method , where @xmath52 denotes the number of hidden ( sigmoid pair ) units .",
    "then we obtain the hidden parameters @xmath73 .",
    "let @xmath74 for all @xmath75 and @xmath76 .",
    "solve the system of linear equations @xmath77 with respect to @xmath78 .",
    "then we obtain the output parameters @xmath78 .",
    "generally @xmath42 is ill - shaped and sampling from the distribution is difficult .",
    "for example in fig.[fig : tab.of.sin ] left , samples drawn from @xmath42 of @xmath79 with @xmath80 $ ] is plotted .",
    "whereas in fig.[fig : tab.of.sin ] right , the same distribution is transformed to another @xmath81-coordinate system ( which is explained below ) .",
    "the support of the distribution is reshaped into a rectangular , which implies sampling from @xmath82 is easier than doing from @xmath42 .",
    "c     of @xmath83 case .",
    "red lines indicate the theoretical boundary @xmath84 of the support of @xmath42 . *",
    "left * : @xmath42 has a non - convex support , in which case sampling is inefficient . *",
    "right * : the same sample points are plotted in the coordinate transformed @xmath81-space .",
    "coordinate lines are deformed to lattice lines , and @xmath82 has a rectangular support.,width=226 ]     of @xmath83 case .",
    "red lines indicate the theoretical boundary @xmath84 of the support of @xmath42 . *",
    "left * : @xmath42 has a non - convex support , in which case sampling is inefficient .",
    "* right * : the same sample points are plotted in the coordinate transformed @xmath81-space .",
    "coordinate lines are deformed to lattice lines , and @xmath82 has a rectangular support.,width=226 ]    this ill - shapeness is formulated as following proposition .",
    "[ prop : support ] suppose the objective function @xmath85 has a compact support , then the support of its transform @xmath57 is in the region @xmath86 with @xmath87 .",
    "recall the support of @xmath21 is included in the interval @xmath36 $ ] , therefore for any @xmath88 and @xmath89 , @xmath90 implies @xmath91 .",
    "the latter condition is equivalently deformed to @xmath92 , which implies @xmath93 . by the compact support assumption of @xmath20 , taking",
    "the maximum with respect to @xmath89 leads to @xmath94 . by tracking back the inferences , for any @xmath95 and @xmath96 , @xmath97 since for any @xmath98",
    ", the integrand of @xmath57 is always zero , the integration domain of @xmath57 can be restricted into @xmath99",
    ". therefore by eq.[eq : prop1.4 ] , @xmath100 holds , which comes to the conclusion : @xmath101 .    in a relatively high dimensional input case ,",
    "sampling in the coordinate transformed @xmath81-space @xmath102 is more efficient than sampling in the @xmath103-space because the shape of the support of @xmath42 in the @xmath81-space is rectangular ( see , fig.[fig : tab.of.sin ] ) and therefore the proposal distribution is expected to reduce miss proposals , out of the support .    in case that the coordinate transform technique is not enough , it is worth sampling from each _ component _ distribution .",
    "namely , the empirically approximated @xmath42 is bounded above by a _ mixture distribution _ : @xmath104 where @xmath105 is a _ component distribution _ and @xmath106 is a _ mixing probabilities_.    in addition , an upper bound of @xmath21 is given by the form @xmath107 for some @xmath108 and @xmath109 .",
    "we conducted three sets of experiments comparing three types of learning methods :    * whole parameters are initialized by samples from a uniform distribution , and trained by ackropagation . *",
    "hidden parameters are initialized by ampling from @xmath42 ; and the rest output parameters are initialized by samples from a uniform distribution",
    ". then whole parameters are trained by ackropagation . *",
    "hidden parameters are determined by ampling from @xmath42 ; the rest output parameters are fitted by linear egression .    in order to compare the ability of the three methods , we conducted three experiments on three different problems : one - dimensional complicated curve regression , multidimensional boolean functions approximation and real world data classification .",
    "c    . * left * : sr ( solid black line ) by itself achieved the highest accuracy without the iterative learning , whereas sbp ( dashed red line ) converged to lower rmse than bp ( dotted green line ) .",
    "* right * : the original curve ( upper left ) has high frequencies around the origin . sr ( upper right ) followed such a dynamic variation of frequency better than other two methods .",
    "sbp ( lower left ) roughly approximated the curve with noise .",
    "bp ( lower right ) only fitted moderate part of the curve . , width=226 ]    . *",
    "left * : sr ( solid black line ) by itself achieved the highest accuracy without the iterative learning , whereas sbp ( dashed red line ) converged to lower rmse than bp ( dotted green line ) . *",
    "right * : the original curve ( upper left ) has high frequencies around the origin .",
    "sr ( upper right ) followed such a dynamic variation of frequency better than other two methods .",
    "sbp ( lower left ) roughly approximated the curve with noise .",
    "bp ( lower right ) only fitted moderate part of the curve .",
    ", width=226 ]    first we performed one - dimensional curve regression .",
    "the objective function is a two - sided _",
    "topologists s sine curve ( tsc ) _",
    "@xmath111 defined on the interval @xmath36 $ ] whose indefiniteness at zero is removed by defining @xmath112 .",
    "the tsc is such a complicated curve whose spatial frequency gets arbitrary high as @xmath7 tends to zero . for training",
    ", @xmath113 points were sampled from the domain @xmath36 $ ] in equidistant manner .",
    "the number of hidden parameters were fixed to @xmath114 in each model .",
    "note that relatively redundant quantity of parameters are needed for our sampling initialization scheme to obtain good parameters .",
    "the output function was set to linear and the batch learning was performed by bfgs quasi - newton method .",
    "uniformly random initialization parameters for bp and sbp were drawn from the interval @xmath36 $ ] .",
    "sampling from @xmath42 was performed by acceptance - rejection method .    in fig.[fig",
    ": tsc ] left , the root mean squared error ( rmse ) in training phase of three methods are shown .",
    "the solid black line corresponds to the result by sr , which by itself achieved the highest accuracy without iterative learnings .",
    "the dashed red line corresponds to the result by sbp , and it converged to lower rmse than that of bp depicted in the dotted green line . in fig.[fig : tsc ] right , fitting results of the three methods are shown . as we noted the original curve ( upper left ) has numerical instability around the origin , therefore it is difficult to fit the curve .",
    "sr ( upper right ) approximated the original curve well except around the origin , while other two methods , sbp ( lower left ) and bp ( lower right ) could just partly fit the original curve . in this experiment , we examined the flexibility of our method by fitting a complicated curve .",
    "the experimental result supports that the oracle distribution gave advantageous directions .",
    "r[1pt]6 cm     [ fig : logic ]    second we performed a binary problem with two - dimensional input and three - dimensional output .",
    "output vectors are composed of three logical functions : @xmath115 .",
    "therefore the total number of data is just four : @xmath116 .",
    "the number of hidden units were fixed to @xmath117 .",
    "the output function was set to sigmoid and the loss function was set to cross - entropy .",
    "uniformly random initialization parameters for bp and sbp were drawn from the interval @xmath36 $ ] .",
    "sampling from @xmath42 was performed by acceptance - rejection method . in fig.[fig : logic ] both the cross - entropy curves and classification error rates are depicted in thin and thick lines respectively .",
    "the solid black line corresponds to the results by sr , which achieved the perfectly correct answer from the beginning .",
    "the dashed red line corresponds to the results by sbp , which also attained the perfect solution faster than bp .",
    "the dotted green line corresponds to the results by bp , which cost @xmath114 iterations of learning to give the correct answer . in this experiment",
    "we have validated that the proposed method works well with multiclass classification problems .",
    "the quick convergence of sbp indicates that @xmath42 contains advantageous information on the training examples to the uniform distribution .",
    "r[1pt]6 cm        finally we examined a real classification problem using the mnist data set@xcite .",
    "the data set consists of @xmath118 training examples and @xmath119 test examples .",
    "each input vector is a @xmath120-level gray - scaled @xmath121-pixel image of a handwritten digit .",
    "the corresponding label is one of 10 digits .",
    "we implemented these labels as @xmath117-dimensional binary vectors whose components are chosen randomly with equivalent probability for one and zero .",
    "we used randomly sampled @xmath122 training examples for training and whole @xmath119 testing examples for testing .",
    "the number of hidden units were fixed to @xmath123 , which is the same size as used in the previous study of lecun et al.@xcite .",
    "note that @xmath52 sigmoid pairs corresponds to @xmath54 sigmoid units , therefore we used @xmath124 sigmoid pairs for sr and sbp , and @xmath123 sigmoid units for bp .",
    "the output function was set to sigmoid and the loss function was set to cross - entropy . in obedience to lecun et al.@xcite , input vectors were normalized and randomly initialized parameters for bp and sbp were drawn from uniform distribution with mean zero and standard deviation @xmath125 .",
    "direct sampling from @xmath42 is numerically difficult because the differential order of its decomposing kernel @xmath21 piles up as high as @xmath126-th order .",
    "we abandoned rigorous sampling and tried sampling from a _ mixture annealed _ distribution .",
    "as described in eq.[eq : mix ] , we regarded @xmath42 as a mixture of @xmath127 . by making use of the log boundary given by eq.[eq : log.bound ] , we numerically approximated @xmath128 from above @xmath129 and drew samples from an _ easier _ component distribution @xmath130 .",
    "details of the sampling technique is explained in [ supp : mix ] .",
    "the sampling procedure scales linearly with the dimensionality of the input space ( @xmath126 ) and the number of required hidden units ( @xmath124 ) respectively .",
    "in particular it scales constantly with the number of the training examples .",
    "the following linear regression was conducted by singular value demcomposition ( svd ) , which generally costs @xmath131 operations , assuming @xmath132 , for decomposing a @xmath133-matrix . in our case",
    "@xmath2 corresponds to the number of the training examples ( @xmath122 ) and @xmath134 corresponds to the number of hidden units ( @xmath123 ) . at last backpropagation learning",
    "was performed by stochastic gradient descent ( sgd ) with adaptive learning rates and diagonal approximated hessian@xcite .",
    "the experiment was performed in r@xcite on a xeon x5660 @xmath135ghz with @xmath136 gb memory .    in fig.[fig",
    ": mnist ] the classification error rates for test examples are depicted .",
    "the black real line corresponds to the results by sr , which marked the lowest error rate ( @xmath137 ) of the three at the beginning , and finished @xmath138 after @xmath139 iterations of sgd training .",
    "the training process was not monotonically decreasing in the early stage of training , it appears that the sr initialization overfitted to some extent .",
    "the red dashed line corresponds to the results by sbp , which marked the steepest error reduction in the first @xmath140 iterations of sgd training and finished @xmath141 .",
    "the green dotted line corresponds to the results by bp , which declined the slowest in the early stage of training and finished @xmath142 .    in tab.[tab : time ] the training time from initialization through sgd training is listed .",
    "the sampling step in sr ran faster than the following regression and sgd steps .",
    "in addition , the sampling time of sr and sbp was as fast as the sampling time of bp .",
    "as we expected , the regression step in sr , which scales linearly with the amount of the data , cost much more time than the sampling step did .",
    "the sgd step also cost , however each step cost around merely @xmath143 seconds , and it would be shorten if the initial parameters had better accuracy .",
    ".training times for mnist [ cols=\"<,^,^,^\",options=\"header \" , ]     [ tab : time ]    in this experiment , we confirmed that the proposed method still works for real world data with the aid of an annealed sampling technique .",
    "although sr showed an overfitting aspects , the fastest convergence of sbp supports that the oracle distribution gave meaningful parameters , and the annealed sampling technique could draw meaningful samples .",
    "hence the overfitting of sr possibly comes from regression step , which suggests the necessity for further blushing up of regression technique .",
    "in addition , our further experiments also indicated that when the number of hidden units increased to @xmath144 , the _ initial _ test error rate scored @xmath145 , which is smaller than the previously reported error rates @xmath146 by lecun et al.@xcite with @xmath123 hidden units .",
    "in this paper , we introduced a two - stage weight initialization method for backpropagation : sampling hidden parameters from the oracle distribution and fitting output parameters by ordinary linear regression .",
    "based on the integral representation of neural networks , we constructed our oracle distributions from given data in a nonparametric way .",
    "since the shapes of those distributions are not simple in high dimensional input cases , we also discussed some numerical techniques such as the coordinate transform and the mixture approximation of the oracle distributions .",
    "we performed three numerical experiments : complicated curve regression , boolean function approximation , and handwritten digit classification .",
    "those experiments show that our initialization method works well with backpropagation .",
    "in particular for the low dimensional problems , well - sampled parameters by themselves achieve good accuracy without any parameter updates by backpropagation .",
    "for the handwritten digit classification problem , the proposed method works better than random initialization .",
    "sampling learning methods inevitably come with redundant hidden units since drawing good samples usually requires a large quantity of trial .",
    "therefore the model shrinking algorithms such as pruning , sparse regression , dimension reduction and feature selection are naturally compatible to the proposed method .",
    "although plenty of integral transforms have been used for theoretical analysis of neural networks , numerical implementations , in particular sampling approaches are merely done .",
    "even theoretical calculations often lack practical applicability , for example a higher order of derivative in our case , each integral representation interprets different aspects of neural networks .",
    "further monte carlo discretization of other integral representations is an important future work .    in the deep learning context , it is said that the deep structure remedies the difficulty of a problem by multilayered superpositions of simple information transformations .",
    "we conjecture that the complexity of high dimensional oracle distributions can be decomposed into relatively simple distributions in each layer of the deep structure .",
    "therefore , extending our method to the multilayered structure is our important future work .",
    "the authors are grateful to hideitsu hino for his incisive comments on the paper .",
    "they also thank to mitsuhiro seki for having constructive discussions with them .",
    "sampling hidden parameter @xmath103 s from the oracle distribution @xmath17 demands a little ingenuity . in our experiments ,",
    "we have implemented two sampling procedures : a rigorous but naive , computationally inefficient way and an approximative / ad hoc but quick and well - performing way . although both work quickly and accurately in a low dimensional input problem , only the latter works in a high dimensional problem such as mnist .      given a decomposing kernel @xmath147 , we employed acceptance - rejection ( ar ) method directly on rigorous sampling from @xmath17 on a proposal distribution @xmath148 , we employed uniform distribution .",
    "we assume here that the support @xmath149 of proposal distribution @xmath148 has been adjusted to cover the _ mass _ of @xmath17 as tight as possible , and the infimum @xmath150 has been estimated .",
    "then our sampling procedure is conducted according to the following alg.[alg : naive ] .",
    "note that in a high dimensional case , the estimation accuracy of @xmath60 and the tightness of @xmath149 affects the sampling efficiency and accuracy materially .",
    "in fact , the expectation number of trial to obtain one sample by ar is @xmath60 times , which gets exponentially large as the dimensionality increases . since the support of the oracle distribution @xmath17 is not rectangular , sampling from",
    "coordinate transformed @xmath155 remedies the difficulty .",
    "in addition , the high order differentiation in the decomposing kernel @xmath21 cause numerical unstability .      in order",
    "to overcome the high dimensional sampling difficulty , we approximately regarded @xmath17 as a mixture distribution @xmath156 ( as described in eq.[eq : mix ] ) and conducted two - step sampling : first choose one component distribution @xmath157 according to the mixing probability @xmath106 , second draw a sample @xmath103 from chosen component distribution @xmath157 .",
    "sampling from @xmath158 holds another difficulty due to its high order differentiation in @xmath128 . according to its upper bound evaluation ( eq.[eq : log.bound ] ) ,",
    "a high order derivative @xmath159 has its almost all _ mass _ around both edge of its domain interval @xmath36 $ ] and almost no mass in the middle of the domain ( see fig.[fig : dmol ] left ) .",
    "hence we approximated , or _",
    "annealed _ , @xmath160 by a beta distribution , which could model extreme skewness of @xmath160 ( e.g. , @xmath161 ; see fig.[fig : dmol ] right ) .",
    "then we conducted further steps of sampling : first sample @xmath162 $ ] according to the annealing beta distribution , then sample @xmath163 and @xmath164 under the restriction @xmath165 .       of mollifier .",
    "* left * : @xmath166 has almost all mass , with high frequency , at both ends , and no mass in the middle of domain . *",
    "right * : the right half of @xmath167 is approximated by beta distribution @xmath168 ( red line ) .",
    "[ fig : dmol],width=226 ]     of mollifier .",
    "* left * : @xmath166 has almost all mass , with high frequency , at both ends , and no mass in the middle of domain . *",
    "right * : the right half of @xmath167 is approximated by beta distribution @xmath168 ( red line ) .",
    "[ fig : dmol],width=226 ]    obviously the mixture approximation gives rise to poor restriction and virtual indefiniteness of @xmath103 . since the rigorous computation establishes all relations between @xmath103 and all @xmath169 s , whereas the mixture approximation does just one relation between @xmath103 and one particular @xmath169 .",
    "we introduced two additional assumptions .",
    "first , @xmath163 is parallel to given @xmath169 .",
    "since @xmath163 always appears in the form @xmath170 , only the parallel component of @xmath163 could have any effect ( on one particular @xmath169 ) , hence we eliminated the extra freedom in the orthogonal component .",
    "second , the norm @xmath171 has similar scale to the distances @xmath172 between input vectors .",
    "since @xmath173 controls the spatial frequency of a hidden unit , it determines how broad the hidden unit covers the part of the input space .",
    "namely , @xmath173 controls which input vectors are selectively responded by the unit .",
    "therefore , in order to avoid such an isolation case that an unit responds for only one input vector , we assumed @xmath173 is no smaller than the distance between input vectors . in this procedure",
    "we set @xmath173 as a distance @xmath172 of randomly selected two input examples @xmath169 and @xmath174 .",
    "we denote this procedure simply by @xmath175 .",
    "once @xmath163 is fixed with these assumptions , @xmath164 is determined as @xmath176 .",
    "given shape parameters @xmath177 of the beta distribution @xmath178 , one cycle of our second sampling method is summarized as alg.[alg : mix ] .",
    "this method consists of no more expensive steps .",
    "it scales linearly with the dimensionality of the input space and the number of required sample parameters respectively .",
    "moreover , it does not depends on the size of the training data ."
  ],
  "abstract_text": [
    "<S> a new initialization method for hidden parameters in a neural network is proposed . </S>",
    "<S> derived from the integral representation of neural networks , a nonparametric probability distribution of hidden parameters is introduced . in this proposal , </S>",
    "<S> hidden parameters are initialized by samples drawn from this distribution , and output parameters are fitted by ordinary linear regression . </S>",
    "<S> numerical experiments show that backpropagation with proposed initialization converges faster than uniformly random initialization . </S>",
    "<S> also it is shown that the proposed method achieves enough accuracy by itself without backpropagation in some cases . </S>"
  ]
}