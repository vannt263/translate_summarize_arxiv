{
  "article_text": [
    "starting with a set of weighted items , we want to create a generic sample of a certain size that we can later use to estimate the total weight of arbitrary subsets .",
    "applied to internet traffic analysis , the items could be records summarizing the flows streaming by a router , with , say , a hundred records sampled each hour",
    ". a subset could be flow records of a worm attack whose signature is only determined after sampling has taken place .",
    "the samples taken in the past allow us to trace the history of the attack even though the worm was unknown at the time of sampling .",
    "estimation from the samples must be accurate even with heavy - tailed distributions where most of the weight is concentrated on a few heavy items .",
    "we want the sample to be weight sensitive , giving priority to heavy items . at the same time , we want sampling without replacement in order to avoid selecting heavy items multiple times . to fulfill these requirements we introduce priority sampling , which is the first weight sensitive sampling scheme without replacement that is suitable for estimating subset sums .",
    "testing priority sampling on internet traffic analysis , we found it to perform orders of magnitude better than previous schemes .",
    "_ priority sampling _ is a fundamental new technique to sample @xmath6 items from a stream of weighted items so as to later estimate arbitrary subset sums .",
    "the scheme is illustrated in figure  [ fig : sample ] .",
    "we consider a stream of items with positive weights @xmath15 .",
    "for each item @xmath16 , we generate an independent uniformly random @xmath17 , and a _ priority _ @xmath4 . assuming that all priorities are distinct",
    ", the _ priority sample @xmath5 of size @xmath18 _ consists of the @xmath6 items of highest priority . an associated _ threshold _",
    "@xmath7 is the @xmath19 priority .",
    "then @xmath20 .",
    "each sampled item @xmath21 gets a weight estimate @xmath22 . if @xmath23 , @xmath10",
    ". we will prove @xmath24}=w_i\\ ] ] by linearity of expectation , if we want to estimate the total weight of an arbitrary subset @xmath25=\\{0,1,\\ldots , n-1\\}$ ] , we just sum the corresponding weight estimates in the sample , that is , @xmath26}=\\sum_{i\\in i}w_i\\ ] ] ties between priorities happen with probability zero , and can be resolved arbitrarily .",
    "we resolve them in favor of earlier items .",
    "thus we view priority @xmath27 as higher than @xmath28 , denoted @xmath29 , if either @xmath30 or @xmath31 and @xmath32 . with any such resolution of ties ,",
    "priority sampling works even if some weights are zero .",
    "note that in the case of unit weights , @xmath7 is just the @xmath19 largest value @xmath33 , and then ( [ eq : subset ] ) simplifies to @xmath34}=n.\\ ] ] this unit case is a classic theorem in order statistics ( see e.g. ,  @xcite ) .",
    "we will now , with a few examples , illustrate how subsets could be selected .",
    "the basic point is that an item , besides the weight , has other associated information , and selection of an item may be based on all its associated information . to estimate the total weight of all selected items , we sum the weight estimates of all sampled items that would be selected .",
    "we note that the examples below could be based on any kind of sampling .",
    "what distinguishes priority sampling is the quality of the answers .",
    "[ [ internet - traffic - analysis ] ] internet traffic analysis + + + + + + + + + + + + + + + + + + + + + + + + +    our motivating application comes from internet traffic analysis .",
    "internet routers export information about transmissions of data passing through .",
    "these transmissions are called flows .",
    "a flow could be an ftp transfer of a file , an email , or some other collection of related data moving together .",
    "a flow record is exported with statistics such as summary information such as application type , source and destination ip addresses , and the number of packets and total bytes in the flow .",
    "we think of byte size as the weight .",
    "we want to sample flow records in such a way that we can answer questions like how many bytes of traffic came from a given customer or how much traffic was generated by a certain application .",
    "both of these questions ask what is the total weight of a certain selection of flows .",
    "if we knew in advance of measurement which selections were of interest , we could have a counter for each selection and increment these as flows passed by .",
    "the challenge here is that we must not be constrained to selections known in advance of the measurements .",
    "this would preclude exploratory studies , and would not allow a change in routine questions to be applied retroactively to the measurements .",
    "a killer example where the selection is not known in advance was the tracing of the _ internet slammer worm _ @xcite .",
    "it turned out to have a simple signature in the flow record ; namely as being udp traffic to port 1434 with a packet size of 404 bytes .",
    "once this signature was identified , the historical development of the worm could be determined by selecting records of flows matching this signature from a data base of sampled flow records .",
    "we note that data streaming algorithms have been developed that generalizes counters to provide answers to a range of selections such as , for example , range queries in a few dimensions @xcite .",
    "however , each such method is still restricted to a limited type of selection to be decided in advance of the measurements .",
    "[ [ external - information - in - the - selection ] ] external information in the selection + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in our next example , suppose wallmart saved samples of all their sales where each record contained information such as item , location , time , and price .",
    "based on sampled records , they might want to ask questions like how many days of rain does it take before we get a boom in the sale of rain gear . knowing this",
    "would allow them to tell how long the would need to order and disperse the gear if the weather report promissed a long period of rain .",
    "now , the weather information was not part of the sales records , but if they had a data base with historical weather information , they could look up each sampled sales record with rain gear , and check how many days it had rained at that location before the sale .",
    "the important lesson from this example is that selection can be based on external information not even imagined relevant at the time when measurements are made .",
    "such scenarios preclude any kind of streaming algorithm based on selections of limitated complexity , and shows the inherent relevance of sampling preserving full records for the perpose of arbitrary selections .",
    "what distinguishes priority sampling is how well it does in the common case of a heavy tailed weight distribution @xcite .",
    "the problem with _ uniform sampling _ is that it is likely to miss out on the small proportion of heavy items .",
    "an alternative is _ weighted sampling with replacement _ where each sample is chosen independently , each item being selected with probability proportional to its weight .",
    "the problem is that we are likely to get many duplicates of the heavy items , and hence provide less information on lighter items .",
    "a variant of weighted sampling with replacement for integer weights is to _ divide them into unit weights_. this way we can get at most @xmath1 samples of units from item @xmath2 .",
    "however , when weights are large compared with the number of samples , this is still very similar to the basic weighted sampling without replacement .",
    "the above observations suggest that we need _ weight - sensitive sampling without replacement_. for example , we can perform weighted sampling with replacement , but skip duplicates until we have the desired number of samples .",
    "the book @xcite mentions 50 such schemes , but none of these provides estimates of sums .",
    "the basic problem is that the probability that a given item is included in the sample is a complicated function of all the involved weights .",
    "clearly priority sampling acts without replacement . to see",
    "that it is weight - sensitive , suppose we have an item @xmath2 which is @xmath35 times smaller than an item @xmath36",
    ". then the probability that @xmath2 gets higher priority than @xmath36 is @xmath37 .",
    "more precisely , @xmath38=\\pr[w_i/\\alpha_i",
    "> w_j/\\alpha_j ] = \\pr[\\alpha_i<\\alpha_j / r]=\\int_0 ^ 1 \\alpha_j / r\\ ; d \\alpha_j=1/2r\\ ] ] priority sampling is thus weight - sensitive without replacement , and , as stated in ( [ eq : subset ] ) , it provides simple unbiased estimates of arbitrary sums .",
    "we will present tests of priority sampling on real internet data , and see that estimating subset sums needs orders of magnitude fewer samples than uniform sampling and weighted sampling without replacement .",
    "the rest of the paper is organized as follows . in  [ sec : unbias ] we present the proof that priority sampling provides unbiased estimators as stated in ( [ eq : unbias ] ) .",
    "in addition we will show how we can estimate the variance of our subset sum estimates .",
    "this relies on the striking property of priority sampling that we establish , namely , that with more than one sample , the covariance between different weight estimates is zero . in ",
    "[ sec : threshold ] we compare our new priority sampling with threshold sampling from @xcite , a scheme which is very closely related but does not provide a specified number of samples . in  [ sec : experiments ] , we present experiments with priority sampling on real data from the internet , demonstrating orders of magnitude gain in accuracy in estimation weight sums , as compared with uniform sampling and weighted sampling without replacement . in ",
    "[ sec : analysis ] , we analyze the performance of the different sampling schemes in some simple cases in order to gain further understanding of the experiments . in  [ sec : conj - opt ] , we conjecture an extremely strong near - optimality ; namely that for any weight sequence , there exists no specialized scheme for sampling @xmath6 items with unbiased weight estimators that gets smaller total variance than priority sampling with @xmath14 items .",
    "this conjecture was recently settled by szegedy @xcite.q in  [ sec : alg ] , we show how we can maintain a priority sample of size @xmath6 for a stream of weighted items , spending only constant time on each item as it comes by .",
    "we finish with some concluding remarks in  [ sec : conclusion ] .",
    "a preliminary version of parts of this work was published in a conference proceeding @xcite , including the basic announcement of the priority sampling scheme .",
    "our original proof of ( [ eq : unbias ] ) was based on the standard proofs for the known unit case  @xcite , but here we present a much simpler combinatorial proof and include an entirely new analysis of variance and covariance .",
    "the experiments reported here are all new , and so is most of the analysis of simple cases , as well as the conjecture concerning near - optimality .",
    "in this section , we will show that priority sampling yields unbiased estimates of subset sums as stated in ( [ eq : unbias ] ) .",
    "the proof is simpler and more combinatorial than the standard proofs for the known unit case  @xcite .",
    "we will also show how to form unbiased estimators of secondary weights .",
    "finally , we consider variance estimation .",
    "we show that there is no covariance between the weight estimates of different items , and that we can get unbiased estimates of the variance of any subset sum estimate .",
    "recall that we consider items with positive weights @xmath15 . for each item",
    "@xmath39 $ ] , we generate an independent uniformly distributed random number @xmath17 , and a priority @xmath4 .",
    "priority @xmath27 is higher than @xmath28 , denoted @xmath29 , if either @xmath30 , or @xmath31 and @xmath32 .",
    "a priority sample @xmath5 of size @xmath6 consists of the @xmath6 items of highest priority .",
    "the threshold @xmath7 is the @xmath40 highest priority .",
    "then @xmath41 .",
    "each @xmath21 gets a weight estimate @xmath42 . also ,",
    "for @xmath23 , we define @xmath10 .",
    "now ( [ eq : unbias ] ) states that @xmath43=w_i$ ] .    we will prove that ( [ eq : unbias ] ) holds for an item @xmath2 no matter which values the other @xmath44 take . fixing these values , we fix all the other priorities @xmath45 .",
    "let @xmath46 be the @xmath47 highest of these other priorities .",
    "we can now view @xmath46 as a fixed number .",
    "more formally , our analysis is conditioned on the event @xmath48 of @xmath46 being the @xmath47 highest among the priorities @xmath45 , and we will prove @xmath49=w_i.\\ ] ] proving ( [ eq : unbias - tau ] ) for any value of @xmath46 implies ( [ eq : unbias ] ) .",
    "the essential observation is as follows .",
    "[ lem : tau ] conditioned on @xmath48 , item @xmath2 is picked with probability @xmath50 , and if picked , @xmath51 .",
    "we pick @xmath17 uniformly at random , thus fixing @xmath4 . if @xmath52 , there are at least @xmath6 priorities higher than @xmath27 , so @xmath23 .",
    "conversely , if @xmath53 , then @xmath46 becomes the @xmath54th priority among all priorities , so @xmath55 , and then @xmath21 . finally , @xmath56}={\\pr\\!\\left[{{q_i\\succ\\tau'}}\\right]}={\\pr\\!\\left[{{\\alpha_i",
    "< w_i/\\tau'}}\\right ] } = \\min\\{1,w_i/\\tau'\\}\\ ] ]    from lemma  [ lem : tau ] , we get @xmath57&=&{\\pr\\!\\left[{{i\\in s|a(\\tau')}}\\right]}\\times { { \\textsf{e}}\\left[{{\\hat w_i|i\\in s\\wedge a(\\tau')}}\\right]}\\\\ & = & \\min\\{1,w_i/\\tau'\\}\\times\\max\\{w_i,\\tau'\\}\\\\ & = & w_i\\end{aligned}\\ ] ] the last equality follows by observing that both the @xmath58 and the @xmath59 take their first , respectively their second value , depending on whether or not @xmath60 .",
    "this completes the proof of ( [ eq : unbias - tau ] ) , hence of ( [ eq : unbias ] )      we note here that priority sampling , as defined above , works even in the presence of zero weights .",
    "first we note that @xmath61 while @xmath62 .",
    "it follows that zero weight items can only be sampled if all positive weight items have been sampled .",
    "moreover , if we do sample a zero weight item @xmath2 , we have @xmath63 , so @xmath64 , and then @xmath65 for all items @xmath36 .",
    "having noted that zero weight items do not cause problems , we will mostly ignore them .",
    "above we have assumed @xmath18 , but we note a natural view of a priority sample of everything , that is , with @xmath66 .",
    "we define an @xmath67 priority @xmath68 , as if we had an extra zero weight @xmath69 .",
    "then @xmath70 for all @xmath71 $ ] , so all items get sampled .",
    "moreover @xmath72 , so the weight estimate is equal to the original weight .",
    "suppose that each item @xmath2 has a secondary variable @xmath73 .",
    "we can then use ( [ eq : unbias ] ) to give unbiased estimators of corresponding secondary subset sums .",
    "more precisely , we set @xmath74 .",
    "that is @xmath75 if @xmath2 is sampled ; @xmath76 otherwise",
    ". then ( [ eq : unbias ] ) implies @xmath77=x_i$ ]",
    ".    an application could be to deal with negative and positive weights @xmath73 .",
    "we could define the priority weights as their absolute values , that is , @xmath78 , and use these non - negative weights in the priority sample",
    ".    another application could be if we had several different variables for each item . instead of making an independent priority sample for each variable",
    ", we could construct a compromise weight .",
    "for example , for each item , the weight could be a weighted sum of all the associated variables .",
    "we now provide a simple variance estimator @xmath79 and show that it is unbiased , that is , @xmath80}=\\var[\\hat w_i].\\ ] ] as in the proof of ( [ eq : unbias ] ) , we define @xmath48 to be the event that @xmath46 is the @xmath47 highest among the priorities @xmath45",
    ". we will prove @xmath81}={\\textsf{e}}[\\hat w_i^2|a(\\tau')]-w_i^2.\\ ] ] from lemma  [ lem : tau ] , we get @xmath82&=&{\\pr\\!\\left[{{i\\in s|a(\\tau')}}\\right]}\\times { { \\textsf{e}}\\left[{{\\hat v_i|i\\in s\\wedge a(\\tau')}}\\right]}\\\\ & = & \\min\\{1,w_i/\\tau'\\}\\times \\tau'\\max\\{0,\\tau'-w_i\\}\\\\ & = & \\max\\{0,w_i\\tau'-w_i^2\\}.\\\\\\end{aligned}\\ ] ] on the other hand , @xmath83&=&{\\pr\\!\\left[{{i\\in s|a(\\tau')}}\\right]}\\times { { \\textsf{e}}\\left[{{\\hat w_i^2|i\\in s\\wedge a(\\tau')}}\\right]}\\\\ & = & \\min\\{1,w_i/\\tau'\\}\\times \\max\\{w_i,\\tau'\\}^2\\\\ & = & \\max\\{w_i^2,w_i\\tau'\\}.\\\\\\end{aligned}\\ ] ] this establishes ( [ eq : var - est - tau ] ) and hence ( [ eq : var - est ] ) .      assuming that we sample more than one item , we will show that the covariance between our weight estimates is zero , that is , for @xmath84 and @xmath85 , @xmath86}=w_iw_j\\ ] ] if @xmath87 , we have @xmath88}=0 $ ] since we can not sample both @xmath2 and @xmath36 .",
    "note that ( [ eq : covar ] ) is somewhat counter - intuitive in that if we sample @xmath2 then this reduces the probability that we also sample @xmath36 . however , the assumption that @xmath2 is sampled affects the threshold @xmath7 and thereby the weight estimate @xmath13 and somehow , the different effects cancel out",
    ".    we will prove ( [ eq : covar ] ) via the following common generalization of ( [ eq : covar ] ) and ( [ eq : unbias ] ) holding for any @xmath89,\\ |i|\\leq k$ ] : @xmath90}=\\prod_{i\\in i } w_i\\ ] ] if @xmath91 , we have @xmath92}=0 $ ] since at most @xmath6 items are sampled with @xmath93 .",
    "the proof of ( [ eq : prod ] ) generalizes that of ( [ eq : unbias ] ) .",
    "inductively on the size of @xmath94 , we will prove that ( [ eq : prod ] ) holds no matter what values all the other @xmath95 , @xmath96 take .",
    "the equality is trivially true in the base case where @xmath97 and the products equals one .",
    "thus , for all @xmath96 , fix all @xmath98 and priorities @xmath99 . fix @xmath100 to be the @xmath101",
    "highest of these priorities @xmath102 .",
    "this priority exists because @xmath103 .",
    "next for @xmath104 , we pick @xmath3 and set @xmath4 .",
    "we can now have at most @xmath105 priorities below @xmath100 , so @xmath100 is at least as big as our new threshold @xmath7 .    consider the case that @xmath94 has a weight @xmath106 .",
    "fix @xmath107 arbitrarily .",
    "then @xmath108 , so item @xmath109 is sampled with @xmath110 . hence @xmath111}=w_h{{\\textsf{e}}\\left[{{\\prod_{i\\in",
    "i\\setminus\\{m\\}}\\hat w_i}}\\right]}$ ] .",
    "we have now fixed all @xmath95 , @xmath112 , and by induction , @xmath113}= \\prod_{i\\in i\\setminus\\{m\\ } } w_i$ ] .",
    "this completes the proof of ( [ eq : prod ] ) in the case that @xmath106 .",
    "next consider the case that all weights from @xmath94 are smaller than @xmath100 .",
    "let @xmath114 be the lowest priority from @xmath94 .",
    "if @xmath115 , then there are at least @xmath116 priorities higher than @xmath114 , so @xmath117 , and @xmath118 .",
    "thus , if @xmath115 , there is no contribution to @xmath119}$ ] .",
    "conversely , if @xmath115 , then all priorities from @xmath94 are bigger than @xmath100 . in this case",
    "there are exactly @xmath120 priorities higher than @xmath100 , so @xmath100 becomes our threshold @xmath7 . then each @xmath21 are sampled .",
    "since @xmath121 , we get @xmath122 .",
    "hence @xmath123 .",
    "since no weights in @xmath94 is higher than @xmath100 , the probability that all their priorities are bigger is @xmath124 .",
    "thus , the contribution to @xmath119}$ ] is @xmath125 .",
    "this completes the proof of ( [ eq : prod ] ) in the remaining case where @xmath126 .",
    "we can now use our variance estimator from  [ sec : item - var - est ] to estimate the variance over any subset . by ( [ eq : covar ] ) and ( [ eq : var - est ] ) we get an unbiased estimator of the variance of any subset sum estimate simply by summing the variance estimators from the subset , that is , if @xmath84 for any subset @xmath127 $ ]",
    ", @xmath128={{\\textsf{e}}\\left[{{\\sum_{i\\in s\\cap i}\\hat v_i}}\\right]}\\ ] ] in fact , ( [ eq : var - sum ] ) also holds if @xmath87 , but this is because @xmath129=\\infty$ ] for any non - empty subset @xmath94 .",
    "we shall return to this point later in  [ sec : singe - inf ] .",
    "it is instructive to compare our priority sampling scheme with threshold sampling from @xcite . in that approach each item",
    "is sampled independently , so we do not control the exact number of samples . before sampling , a fixed threshold @xmath130 is chosen .",
    "an item @xmath2 is sampled if @xmath131 , or with probability @xmath132 if @xmath133 .",
    "we denote the set of selected items by @xmath134 .    to see the relation to priority sampling , note that threshold sampling can be expressed in a manner similar to priority sampling as follows : generate a random number @xmath135 $ ] and sample item @xmath2 if @xmath136 .",
    "as in our new scheme , the sampled items get weight estimate @xmath137 whereas @xmath138 if @xmath139 . thus the only difference between priority sampling and the threshold sampling from @xcite is in the choice of the threshold . in threshold sampling ,",
    "the threshold is fixed independent of the random choices .",
    "thus the threshold determines only the expected number of independent samples , not the actual random number of samples .",
    "by contrast , in priority sampling , the threshold is picked depending on the random choices so as to get a fixed number of dependent samples .",
    "we note that it is far from obvious that such a threshold could be chosen without violating the unbiasedness of estimation .      in @xcite , the fixed threshold approach to independent sampling",
    "is proved to give an optimal trade - off between variance and sampling rate .",
    "more for an item @xmath2 with weight @xmath1 , we have to decide on a sampling probability @xmath140 .",
    "if @xmath2 is not picked , the weight estimate is zero , that is , @xmath141 . to get an unbiased estimator , if item @xmath2 is picked , it should have weight estimate @xmath142",
    ". then @xmath143}=w_i$ ] .",
    "generally , we want to sample few items , yet keep the variance low .",
    "this motivates an objective of the form @xmath144}\\ ] ] here @xmath145}={{\\textsf{e}}\\left[{{(\\hat w_i(p_i))^2}}\\right]}-w_i^2\\mbox { where } { { \\textsf{e}}\\left[{{(\\hat w_i(p_i))^2}}\\right]}=p_i(w_i / p_i)^2=w_i^2/p_i.\\ ] ] thus we want to @xmath146 for @xmath147 $ ] , the solution is to set @xmath148 . with @xmath149",
    "this is equivalent to the fixed threshold scheme .",
    "that is , for any choice of @xmath130 , the fixed threshold scheme picks the @xmath150 so as to @xmath151}.\\ ] ] summing over the whole stream of items , we @xmath152}\\left(p_i+(1/\\tau^{thr})^2 { \\var\\left[{{\\hat w_i(p_i)}}\\right]}\\right ) = \\sum_{i\\in[n]}p_i+1/(\\tau^{thr})^2{\\var\\left[{{\\sum_{i\\in[n]}\\hat w_i(p_i)}}\\right]}\\ ] ] we now constrain ourselves to getting an expected number @xmath6 of samples . to minimize the total variance",
    ", we just have to identify @xmath130 such that @xmath153}{p_i}=\\sum_{i\\in[n]}\\min\\{1,w_i/\\tau^{thr}\\}=k\\ ] ] with this value of @xmath130 , the fixed threshold scheme from @xcite minimizes the total variance subject to unbiased estimation and an expected number @xmath6 of samples .",
    "any other assignments of individual sampling probabilities @xmath140 will do worse .",
    "the quality of our new scheme is largely inherited from this fixed threshold scheme , but we have some extra variability due to the variability of the threshold .",
    "we tested priority sampling on 10 minutes of flows from an internet gateway router . for increasing sample sizes , we wanted to check our ability to estimate subset sums where each subset was defined by flows originated by certain applications such as ftp and web traffic .",
    "this illustrates how priority sampling can be used today in a backbone network .",
    "the basic flow statistics for the different applications is presented in table  [ tab : stats ] .",
    ".statistics on ten minutes of flows from an internet gateway router showing traffic from some different applications .",
    "note that nearly half the flows belong to applications not mentioned . [ cols=\"<,>,>,>,>,>,>,>\",options=\"header \" , ]     for priority sampling ( pri ) , uniform sampling without replacement ( u@xmath154r ) , and weighted sampling with replacement ( w@xmath155r ) , the number @xmath6 of samples is exact .    in threshold sampling ( thr ) ,",
    "the threshold determines only the expected number of samples . for each item @xmath2",
    ", we used the same priority @xmath4 for priority sampling and threshold sampling . in priority sampling , we picked exactly @xmath6 samples using the @xmath8 priority @xmath7 as a threshold . in threshold sampling , we computed the threshold @xmath130 giving an expected number @xmath6 of samples . thus , for a given @xmath6 , the only difference is in the choice of threshold .",
    "finally , figure  [ fig : distinct - samples ] tells the number of distinct samples as a percentage of the target . for priority sampling ( pri ) and uniform sampling ( u@xmath154r )",
    "we have no replacement , so we get exactly @xmath6 distinct samples , that is , 100% . with weighted sampling with replacements ( w@xmath155r ) the duplicates mean that we get less distinct samples . finally , with threshold sampling ( thr ) , all samples are distinct , but we only have an expected number @xmath6 of samples , hence the deviation from the target @xmath6 .      the quality of a sampling scheme is the number of samples it takes before the estimates converges towards the true value .",
    "first we compare our priority sampling ( pri ) scheme with the other schemes providing an exact number @xmath6 of samples , that is , with uniform sampling without replacement ( u@xmath154r ) and weighted sampling without replacement ( w@xmath155r ) . in figure",
    "[ fig : applications ] and  [ fig : matrix ] , we see that priority sampling provides very substantial gains in accuracy over the other schemes .",
    "when comparing the curves , there are two points to consider .",
    "one is how many samples it takes before we get one from a given application .",
    "this is the point at which we get our first non - zero estimates .",
    "second we consider how quickly we converge after this point .",
    "[ [ number - of - samples - needed - to - hit - an - application ] ] number of samples needed to hit an application + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    with uniform sampling , the number of samples expected before we get one from a given application is roughly the total number of flows divided by the number of application flows . in that regard ,",
    "ftp traffic is clearly the worst .    with weighted sampling without replacement ,",
    "the expected number is roughly the total traffic divided by the application traffic .",
    "the worst application here is dns traffic which was the best application for uniform sampling .",
    "priority sampling is like weighted sampling without replacement but it avoids making duplicates of dominant items .",
    "if the dominant items are outside the application , we waste at most one sample on each .",
    "the impact is clear for dns traffic where we get the first sample about 30 times earlier with priority sampling than we did with weighted sampling without replacement .",
    "a more direct illustration of the problem is found in figure  [ fig : distinct - samples ] where we see how the fraction of distinct samples drops in weighted sampling without replacement .",
    "[ [ convergence - after - first - hitting - an - application ] ] convergence after first hitting an application + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    after we have started getting samples from an application , uniform sampling may still have problems with convergence .",
    "this typically occurs if the weight distribution within the application is heavy - tailed . once again",
    ", ftp traffic is the worst application , this time because it has a dominant flow with more than 99% of its traffic .",
    "until this flow is sampled , we expect to underestimate .",
    "if it is sampled early , we will hugely overestimate , although this is unlikely .",
    "the typical heavy - tail behavior is that the estimate grows as we catch up with more and more dominant items .",
    "we see this phenomena both for ftp traffic and for all traffic combined .    with weighted sampling without replacement and with priority sampling ,",
    "we get quicker convergence as soon as we start having samples from an application .",
    "neither scheme has any problems with skewed weight distributions within the applications .",
    "for example , we see that weighted sampling without replacement starts slower than uniform on web traffic , yet it ends up converging faster .",
    "similarly , priority sampling starts slower than uniform on dns traffic , yet it converging faster .",
    "[ [ the - traffic - matrix ] ] the traffic matrix + + + + + + + + + + + + + + + + + +    figure  [ fig : matrix ] shows the average relative error over @xmath156 entries .",
    "we note first the poor performance of uniform sampling .",
    "in fact , it is only luck that the error with uniform is remains below 100% .",
    "this is because we miss the dominant items and get under - estimates that can never be by more than 100% .",
    "we could instead have gotten a dominant item early , leading to a huge over - estimate by far more than 100% .",
    "comparing priority sampling with weighted sampling without replacement the faster convergence of priority sampling is very clear .",
    "for example , priority sampling gets down around a 1% error with about 150 samples whereas weighted sampling with replacement needs about 3000 samples , and the weighted sampling falls further behind with smaller errors because it gets more and more duplicates .",
    "a reason to believe that priority sampling works very well for a fixed number of samples is its similarity with threshold sampling which for an expected number of independent samples minimized the total variance . in figure",
    "[ fig : applications ] and  [ fig : matrix ] we see that indeed priority sampling ( pri ) and threshold sampling ( thr ) are very close ; neither having a systematic advantage .",
    "hence , in our experiment , we see now loss in quality going from an expected number of samples ( thr ) to an exact number of samples ( pri ) . the variation in the actual number of samples with thr shown in figure  [ fig : distinct - samples ] .",
    "as we shall see below , there are certain boundary phenomena that makes priority sampling perform significantly worse than threshold sampling .",
    "in this section , we will compare the different sampling schemes on some simple cases where we can analyze the variance , so as to gain some intuition for what is going on .    generalizing notation from ",
    "[ sec : threshold ] , if @xmath157 is a weight and @xmath158 $ ] a sampling probability , we let @xmath159 denote the random variable that is @xmath160 with probability @xmath161 ; @xmath76 otherwise .",
    "then @xmath162}&=&w\\\\ { { \\textsf{e}}\\left[{{(\\hat w(p))^2}}\\right]}&=&p ( w / p)^2=w^2/p\\\\ { \\var\\left[{{\\hat w(p)}}\\right]}&=&w^2/p - w^2=w^2 \\frac{1-p}p\\end{aligned}\\ ] ] it is also convenient to define the function @xmath163 then , with fixed threshold @xmath130 , the variance for item @xmath2 is @xmath164&=&{\\var\\left[{{\\hat w_i(\\max\\{1,w_i/\\tau^{thr}\\})}}\\right]}\\\\ & = & w_i^2(1/\\max\\{1,w_i/\\tau^{thr}\\}-1)\\\\ & = & w_i\\max\\{0,\\tau - w_i)\\\\ & = & v(w_i,\\tau^{thr})\\end{aligned}\\ ] ] with our new priority sampling , the threshold changes , and the variance of item @xmath2 is @xmath165}=\\int_{\\tau'=0}^\\infty   f(\\tau')v(w_i,\\tau')\\,d\\tau'\\ ] ] where @xmath166 is the probability density function for @xmath46 to be the @xmath167 threshold amongst the items @xmath168 . with @xmath46",
    "thus defined , by lemma  [ lem : tau ] , item @xmath2 is picked if @xmath169 with @xmath170 ; @xmath76 otherwise .",
    "this imitates the fixed threshold scheme with @xmath171 .",
    "thus ( [ eq : freq ] ) follows from the previous calculation with a fixed threshold .    sometimes it is easier with a more direct calculation .",
    "summing over all @xmath168 , we integrate over choices of @xmath95 , multiply with the probability that @xmath99 is the @xmath6th highest priority from @xmath172\\setminus\\{i\\}$ ] , and multiply with the variance @xmath173 .",
    "that is , @xmath174}=\\sum_{j\\in[n]\\setminus\\{i\\}}\\int_0 ^ 1 { \\pr\\!\\left[{{|\\{h\\in [ n]\\setminus\\{i , j\\}|q_h\\succ q_j\\}|=k-1}}\\right]}v(w_i , q_j)\\ ; d\\alpha_j\\ ] ]      we will show that if we only make a single priority sample with @xmath87 , then the variance of any weight estimate is infinite . the proof is based on ( [ eq : var - expression ] ) .",
    "we assume @xmath175 . for",
    "a lower - bound , in the sum , we only need to consider one other item @xmath176 . also , when integrating over @xmath177 , we only consider very small values of @xmath177 .",
    "more precisely , define @xmath178 where @xmath179 is the sum of all weights . if @xmath180 , we have @xmath181 , and then @xmath182\\setminus\\{i , j\\}|q_h\\succ q_j\\}|=k-1}}\\right ] } & = & { \\pr\\!\\left[{{|\\{h\\in \\{2, ... ,n-1\\}|q_h\\succ q_1\\}|=0}}\\right]}\\\\ & > & 1-\\sum_{h\\in \\{2, ...",
    ",n-1\\}}{\\pr\\!\\left[{{q_h>2w}}\\right]}\\\\ & = & 1-\\sum_{h\\in \\{2, ...",
    ",n-1\\}}(w_h/2w)\\\\ & > & 1/2\\end{aligned}\\ ] ] moreover @xmath183 thus , by ( [ eq : var - expression ] ) , we have @xmath184}>\\int_0^{{\\varepsilon}}1/2\\cdot w_1/(2\\alpha_1)\\ ; d\\alpha_1 = \\infty\\ ] ] we note that none of the other sampling schemes considered can get infinite variance .",
    "next , we argue that the variance is bounded if we make at least two priority samples . again",
    ", we focus on the variance for item @xmath175 .",
    "also , it suffices to show that the integral in ( [ eq : var - expression ] ) is finite for each value of @xmath36 , that is , we want to show that @xmath185\\setminus\\{i , j\\}|q_h\\succ q_j\\}|=k-1}}\\right]}v(w_i , q_j)\\ ; d\\alpha_j\\ ] ] is bounded . now , for @xmath186 , @xmath182\\setminus\\{i , j\\}|q_h\\succ q_j\\}|=k-1}}\\right ] } & \\leq&{\\pr\\!\\left[{{|\\{h\\in [ n]\\setminus\\{i , j\\}|q_h\\succ q_j\\}|\\geq 1}}\\right]}\\\\ & \\leq&\\sum_{h\\in [ n]\\setminus\\{i , j\\}}{\\pr\\!\\left[{{q_h\\succ q_j}}\\right]}\\\\ & \\leq&\\sum_{h\\in [ n]\\setminus\\{i , j\\}}{\\pr\\!\\left[{{w_h/\\alpha_h > w_j/\\alpha_j}}\\right]}\\\\ & = & \\sum_{h\\in [ n]\\setminus\\{i , j\\}}\\min\\{1,w_h\\alpha_j / w_j\\}\\\\ & \\leq&\\sum_{h\\in [ n]\\setminus\\{i , j\\}}(w_h\\alpha_j / w_j)\\\\ & < & w\\alpha_j / w_j\\end{aligned}\\ ] ] moreover , @xmath187 so we get that @xmath188 hence @xmath189}=\\sum_{j\\in[n]\\setminus\\{i\\}}v_{i , j } < n\\,ww_i,\\ ] ] so indeed the variance is bounded . since the covariance is zero , it also follows that estimates of weights of subsets are bounded .",
    "thus we have proved    if we make a single priority sample , then all weight estimates have infinite variance . with",
    "more than one priority samples , all weight estimates are finite .",
    "by contrast , with all the other sampling schemes , the variance estimates are finite as soon as we make at least one sample",
    ".      we will now study identical unit weights , focusing on the first item @xmath175 .",
    "we will compute the exact variance for each of the sampling schemes considered .",
    "[ [ uniform - sampling - without - replacement ] ] uniform sampling without replacement + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for uniform sampling without replacement , item @xmath76 is picked with probability @xmath190 , hence with @xmath191}=\\frac{1-p_0^{u - r}}{p_0^{u - r}}=\\frac { n - k}k\\ ] ]    [ [ weighted - sampling - without - replacement ] ] weighted sampling without replacement + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for weighted sampling with replacement , item @xmath76 is picked with probability @xmath192 , hence with @xmath193}= \\frac{1-p_0^{w+r}}{p_0^{w+r}}=\\frac{(1 - 1/n)^k}{1-(1 - 1/n)^k}\\ ] ] for @xmath194 , the variance approaches @xmath195 from above .",
    "however , for @xmath66 , the variance approaches @xmath196 .",
    "[ [ fixed - threshold ] ] fixed threshold + + + + + + + + + + + + + + +    in the fixed threshold scheme from @xcite , we set @xmath197",
    ". then @xmath198}=v_0(\\tau^{thr})=w_1\\max\\{0,\\tau^{thr}-w_i\\ } = \\frac{n - k}k\\ ] ]    [ [ priority - sampling-1 ] ] priority sampling + + + + + + + + + + + + + + + + +    for priority sampling , we will evaluate ( [ eq : var - expression ] ) exactly .",
    "we use that @xmath199}={\\pr\\!\\left[{{\\alpha_h<\\alpha_1}}\\right]}=\\alpha_1\\ ] ] and @xmath200 so @xmath201}&=&\\sum_{j=1}^{n-1}\\int_0 ^ 1 { \\pr\\!\\left[{{|\\{h\\in \\{2, ... ,n-1\\}|q_h\\succ q_1\\}|=k-1}}\\right]}v_0(q_1)\\ ; d\\alpha_1\\\\ & = & ( n-1)\\int_{\\alpha_1=0}^1{\\pr\\!\\left [ { { b(n-1,\\alpha)=k-1}}\\right ] } ( 1/\\alpha_1 - 1)\\ ; d\\alpha_1\\\\ & = & ( n-1)\\int_{\\alpha_1=0}^1{n-1 \\choose k-1}\\alpha_1^{k-2}(1-\\alpha_1)^{n - k+1}\\ ;   d\\alpha_1\\\\ & = & \\frac{n - k}{k-1}\\end{aligned}\\ ] ]    [ [ discussion-1 ] ] discussion + + + + + + + + + +    for unit weights , uniform sampling without replacement and threshold sampling gets the sample variance on single item weight estimates ; namely @xmath202 .",
    "when @xmath6 is not too small , priority sampling gets nearly the same variance ; namely @xmath203 . weighted sampling with replacement starts doing well , but gets worse and worse as @xmath6 grows . in particular , for any @xmath204 , it has positive variance while all the other schemes have zero variance since they have no replacement .      in this section",
    "we illustrate what happens when different weights are involved .",
    "we consider the case where we have @xmath205 large weights of weight @xmath206 and @xmath207 unit weights .",
    "the large weights are first , that is , @xmath208 while @xmath209 .",
    "we let @xmath210 denote the total weight .",
    "we view @xmath205 , @xmath207 , and @xmath206 as unbounded .",
    "we assume @xmath211 and that @xmath194 .",
    "these assumptions will help simplifying the analysis .",
    "we will use @xmath212 as a representative for the large items and @xmath213 as a representative for the small items .",
    "the results variances from the different sampling schemes will be accumulated in table [ tab : big - small ] .",
    "[ [ uniform - sampling - without - replacement-1 ] ] uniform sampling without replacement + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for uniform sampling without replacement , the large item @xmath76 is picked with probability @xmath214 , hence with @xmath191}=n^2\\frac{1-p_0^{u - r}}{p_0^{u - r}}=n^2\\frac { n+\\ell - k}k \\approx n^2\\frac nk.\\ ] ] for small item @xmath207 , we have the same sampling probability , @xmath215 , so we get @xmath216}=\\frac{1-p_n^{u - r}}{p_n^{u - r}}\\approx \\frac{n}{k}.\\ ] ]    [ [ weighted - sampling - with - replacement ] ] weighted sampling with replacement + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for weighted sampling with replacement , the large item @xmath76 is picked with probability @xmath217 hence with @xmath218}=n^2\\frac{1-p_0^{w+r}}{p_0^{w+r } } \\approx n^2\\frac{e^{-k/\\ell}}{1-e^{-k/\\ell}}=n^2/(e^{k/\\ell}-1).\\ ] ] in particular , this is @xmath219 for @xmath220 .",
    "iff there exist @xmath221 such that @xmath222 for all sufficiently large @xmath207 . ]",
    "yet it saves a factor @xmath207 over uniform sampling with replacement in the case of large weights .    for weighted sampling with replacement ,",
    "the small item @xmath223 is picked with probability @xmath224 , hence with @xmath218}=\\frac{1-p_n^{w+r}}{p_n^{w+r } } \\approx \\ell n / k\\ ] ]    [ [ fixed - threshold-1 ] ] fixed threshold + + + + + + + + + + + + + + +    for the fixed threshold scheme , if @xmath225 , we set @xmath226 .",
    "then for heavy item @xmath76 , @xmath227}=v(w_0,\\tau^{thr})=n(w / k - n)\\approx n^2\\frac{\\ell - k}k\\ ] ] while for a light item @xmath207 , it is @xmath228}=v(w_n , \\tau^{thr})=(w / k-1)\\approx n\\ell / k\\ ] ] on the other hand , for @xmath229 , we pick a threshold below @xmath206 ; namely @xmath230 . then for heavy item @xmath76 , @xmath227}=0\\ ] ] while for a light item @xmath207 , it is @xmath228}=v(w_n,\\tau^{thr})=(n-\\ell)/(k-\\ell)\\approx n/(k-\\ell)\\ ] ]    [ [ priority - sampling-2 ] ] priority sampling + + + + + + + + + + + + + + + + +    first we consider big item @xmath76 . to compute the variance",
    ", we sum over the events @xmath231 that we have @xmath232 small items with priorities bigger than @xmath206 , multiplying the probability of @xmath231 with @xmath233}-w_0 ^ 2={{\\textsf{e}}\\left[{{\\hat w_0 ^ 2|a(m)}}\\right]}-{{\\textsf{e}}\\left[{{\\hat w_0|a(m)}}\\right]}^2= { \\var\\left[{{\\hat w_0|a(m)}}\\right]}.\\ ] ] trivially , @xmath234}={\\pr\\!\\left[{{b(n,1/n)=m}}\\right]}$ ] .",
    "consider a small item @xmath2 .",
    "conditioned on having a big priority @xmath235 , item @xmath2 acts like a heavy item .",
    "conversely , conditioned on having a small priority @xmath236 , item @xmath2 has no impact on the weight estimate of a heavy items .",
    "thus , in the event @xmath231 , the variance of item @xmath76 is as if we had @xmath237 heavy items and no small items .",
    "if @xmath238 , the threshold is at most @xmath206 , and then there is no variance . if @xmath239 , the analysis from the uniform unit case shows that @xmath240}=n^2\\frac{\\ell+m - k}{k-1}\\ ] ] thus @xmath184}=\\sum_{m=0}^n { \\pr\\!\\left[{{a(m)}}\\right]}{\\var\\left[{{\\hat w_0|a(m)}}\\right ] } = \\sum_{m=\\max\\{0,k-\\ell+1\\}}^n{\\pr\\!\\left[{{b(n,1/n)=m}}\\right]}n^2\\frac{\\ell+m - k}{k-1}\\ ] ] since @xmath241 , the first term dominates , so with @xmath242 , we get that @xmath184}\\approx{\\pr\\!\\left[{{b(n,1/n)=m}}\\right]}n^2\\frac{\\ell+m - k}{k-1}\\ ] ] if @xmath243 , we get @xmath244 , and then @xmath184}\\approx { \\pr\\!\\left[{{b(n,1/n)=0}}\\right]}n^2\\frac{\\ell - k}{k-1 } \\approx n^2\\frac{\\ell - k}{k-1}\\ ] ] if @xmath245 , we get @xmath246 , and then @xmath247}&\\approx&{\\pr\\!\\left[{{b(n,1/n)=k-\\ell+1}}\\right]}n^2/k\\\\ & \\leq&(n / n)^{k-\\ell+1}n^2/k\\\\ & = & nn(n / n)^{k-\\ell}/k\\end{aligned}\\ ] ]    we now consider the light item @xmath207 .",
    "we are going to prove that @xmath248}\\approx n\\ell/(k-1)$ ] if @xmath249 , @xmath248}\\approx n\\ln n$ ] if @xmath250 , and @xmath248}\\approx n/(k-\\ell-1)$ ] if @xmath251 .",
    "we consider two different contributions to the variance depending on whether the threshold @xmath7 is greater than @xmath206 .",
    "if @xmath252 , we further distinguish depending on whether @xmath253 .",
    "if @xmath252 and @xmath254 , then @xmath255 so the variance relative to @xmath213 is @xmath256 .",
    "the probability of this event is @xmath257}\\sum_{m=\\max\\{0,k-\\ell+1\\}}^{n-1}{\\pr\\!\\left[{{b(n-1,1/n)=m}}\\right ] } \\approx { \\pr\\!\\left[{{b(n-1,1/n)=\\max\\{0,k-\\ell+1\\}}}\\right]}\\ ] ] if @xmath243 , this is a variance contribution close to @xmath256 , and if @xmath245 , the variance contribution is bounded by @xmath258}<(n / n)^{k-\\ell+1}$ ] . in either case , this contribution to the variance is not significant .    next consider the case that @xmath252 and @xmath253 . the probability that @xmath253 is @xmath259 .",
    "let @xmath260 denote that event that we have @xmath232 small items @xmath261 with @xmath235 . conditioned on @xmath253",
    ", we have @xmath252 if and only if @xmath262 .",
    "in this case , the variance contribution is @xmath263}-1 $ ] .",
    "however , @xmath264 behaves like the weight estimate of heavy item among @xmath265 heavy items , so @xmath266}=n^2\\frac{\\ell+m+1}{k-1}$ ] .",
    "thus we get a variance contribution of @xmath267}\\sum_{m=\\max\\{0,k-\\ell\\}}^{n-1}{\\pr\\!\\left[{{b(n-1,1/n)=m}}\\right ] } ( n^2\\frac{\\ell+i+1}{k-1}-1)\\\\ & \\approx&1/n\\cdot { \\pr\\!\\left[{{b(n-1,1/n)=\\max\\{0,k-\\ell\\}}}\\right ] } n^2\\frac{\\ell+\\max\\{0,k-\\ell\\}}{k-1}\\\\\\end{aligned}\\ ] ] for @xmath249 , this is approximately @xmath268 , which dominates the variance . for @xmath229 ,",
    "this variance contribution is approximately , @xmath269}<(n / n)^{k-\\ell-1}$ ] , which is insignificant .",
    "we now consider the case where @xmath270 .",
    "this requires @xmath229 and is like the unit case , except that we only sample @xmath271 items .",
    "hence we can apply the integral from the unit weight case , but with the restriction that @xmath272 .",
    "we then get a variance contribution of @xmath273 } ( 1/\\alpha-1)\\ ; d\\alpha\\\\   & = & ( n-1)\\int_{\\alpha=1/n}^1{n-1 \\choose k'-1}\\alpha^{k'-2}(1-\\alpha)^{n - k'+1}\\ ; d\\alpha\\\\\\end{aligned}\\ ] ] for @xmath274 , the impact of starting the integral at @xmath259 is insignificant , so we get an variance contribution which is approximately @xmath275 .",
    "for @xmath276 , we get a variance contribution of @xmath277 this completes the analysis of priority sampling for large and small weights .",
    "a comparison of all the sampling schemes is summarized in table [ tab : big - small ] .",
    "@xmath250 & @xmath229 +   + u@xmath154r & + w@xmath155r & + thr & @xmath278 & @xmath279 & + pri & @xmath280 & @xmath279 & +   + u@xmath154r & + w@xmath155r & + thr & & + pri & & @xmath281 & @xmath282 +    .    [ [ discussion-2 ] ] discussion + + + + + + + + + +    with reference to table  [ tab : big - small ] , the problem with uniform sampling is that it does a terrible job on the large weights , performing about @xmath283 times worse than the other schemes . on the other hand ,",
    "it gives the best performance on the small items .",
    "however , the advantage over threshold and priority sampling becomes insignificant when @xmath284 .",
    "this illustrates that if the number of dominant items is small compared with the number of samples , then threshold and priority sampling do very well even on the small items .",
    "the problem in weighted sampling with replacement is that it does poorly compared with threshold and uniform sampling when the number of samples exceed the number of dominant items .",
    "this is both large and small items , illustrating the problem with duplicates .",
    "finally , comparing threshold and priority sampling , we see that priority sampling has positive variance for @xmath229 whereas threshold sampling has no variance . however , this variance of priority sampling is very small compared to a weight of @xmath206 , so it is a case where priority sampling is doing very well anyway .",
    "it is more interesting to see what happens with the small items .",
    "the major differences are in the two boundary cases when @xmath87 and when @xmath250 .",
    "the former case has infinite variance as discussed previously . for @xmath250",
    ", we see that priority sampling does worse by a factor of @xmath285 .",
    "this is only by the logarithm of the ratio of the large weight over the small weight , and it is only for the special boundary case when @xmath250 that we have such a big difference .",
    "it is therefore not surprising that this kind of difference did not show up in any of our experiments .",
    "also , we note that in this special case , weighted sampling with replacement is performing even much worse ; namely be a factor of @xmath286 .",
    "thus , in our analysis , priority sampling performs very well compared with the other schemes for sampling exactly @xmath6 items , and it is only in rather singular cases that it performs a worse than threshold sampling .",
    "[ [ tailoring - a - better - scheme - for - kell1-samples ] ] tailoring a better scheme for @xmath250 samples + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in our large - small weight example , for @xmath6 not too small , priority sampling is only beaten by threshold sampling , which , however , does not sample exactly @xmath6 items . in particular , priority sampling",
    "is outperformed for @xmath250 .",
    "we will now construct a sampling scheme for this particular case which samples exactly @xmath6 items and gets the same performance as threshold sampling for any @xmath229 . like threshold sampling",
    ", the tailored scheme picks all the @xmath6 large items .",
    "moreover , it picks @xmath287 items uniformly without replacement among the small unit items . from our study of the unit case",
    ", we know that uniform sampling gets the same variance as that of priority sampling on the small items .",
    "thus each item gets the same variance with our tailored scheme as threshold sampling , but that our tailored scheme samples exactly @xmath6 items .",
    "recall from  [ sec : threshold ] that threshold sampling minimizes the total variance when we do independent sampling getting an expected number of @xmath6 samples .",
    "we would have liked to provide a somewhat similar result for priority sampling among schemes sampling exactly @xmath6 items , but we know that this is not the case",
    ". for unit items , uniform sampling without replacement got an item variance of @xmath202 while priority sampling got an item variance of @xmath203 .",
    "also , for our large - small item , we found a specialized scheme outperforming priority sampling when @xmath229 .",
    "we formalize our intuition as the conjecture that if priority sampling is allowed just one extra sample , it beats any specialized sampling scheme on any sequence of weights .",
    "more precisely ,    [ conj ] for any weight sequence @xmath15 and positive integer @xmath288 , there is no tailored scheme for picking a sample @xmath289 $ ] of up to @xmath6 items @xmath2 with unbiased weight estimates @xmath12 ( that is , @xmath10 if @xmath23 and @xmath290}=w_i$ ] for all @xmath71 $ ] ) so that the total variance ( @xmath291 } { \\var\\left[{{\\hat w_i}}\\right]}$ ] ) is smaller than with a priority sample of size @xmath14 .",
    "the conjecture also covers tailored schemes where the same item is picked multiple times , or where less than @xmath6 samples may be picked , as in weighted sampling with replacement .",
    "if we have multiple weight estimates for an item @xmath2 , we add them up to a single weight estimate @xmath12 , and if the sample has less than @xmath6 items , we add extra items @xmath36 with @xmath292 .",
    "thus the tailored scheme is transformed into one that always picks exactly @xmath6 distinct items .",
    "in fact , conjecture  [ conj ] is equivalent to the following conjecture relating priority sampling to threshold sampling :    [ conj2 ] for any weight sequence @xmath15 and positive integer @xmath6 , threshold sampling with an expected number of @xmath6 samples gets a total variance which is no smaller than with a priority sample of size @xmath14 .",
    "one consequence of conjecture  [ conj2 ] is that if we only have resources for a certain number @xmath6 of samples , then we are much better off using priority sampling than using threshold sampling for a small enough expected number of samples , e.g. , @xmath293 , that the probability of getting more than @xmath6 samples is small .    to see",
    "that conjectures  [ conj ] and [ conj2 ] are equivalent , we prove    for any weight sequence @xmath15 and positive integer @xmath6 , there is no scheme for picking a sample @xmath289 $ ] of @xmath6 items @xmath2 with unbiased weight estimates @xmath12 so that the total variance is smaller than with threshold sampling with an expected number of @xmath6 samples . in fact , given the weight sequence , we can construct an optimal scheme for picking @xmath6 items getting exactly the same total variance as that of threshold sampling .",
    "let @xmath294 be a scheme for picking a sample @xmath289 $ ] of @xmath6 items @xmath2 with unbiased weight estimates @xmath295 .",
    "we then consider the corresponding scheme @xmath296 for independent sampling .",
    "more precisely , @xmath297 considers each item @xmath2 independently , picking @xmath2 with the same probability @xmath140 as does @xmath294 , and with the same probability distribution on the weight estimate @xmath298 as @xmath294 induces on its weight estimate @xmath295 .",
    "then @xmath299}={{\\textsf{e}}\\left[{{\\hat w_i^{\\psi}}}\\right]}=w_i$ ] and @xmath300}={\\var\\left[{{\\hat w_i^{\\psi}}}\\right]}$ ] .",
    "moreover , by linearity of expectation , the expected number of samples with @xmath296 is @xmath153}{\\pr\\!\\left[{{i\\in s^{\\ci(\\psi)}}}\\right]}= \\sum_{i\\in[n]}{\\pr\\!\\left[{{i\\in s^{\\psi}}}\\right]}=k.\\ ] ] thus the independent sampling scheme @xmath301 has unbiased estimators like @xmath302 , an expected number of @xmath6 samples , and the same item variances as @xmath302 .    now , suppose for some item @xmath2 that @xmath301 has more than one possible non - zero weight estimate @xmath298 .",
    "we then make an improved sampling scheme @xmath303 which picks item @xmath2 with the same probability @xmath140 as @xmath302 and @xmath301 , but which then always uses the same weight estimate @xmath304}$ ] .",
    "then @xmath305}={{\\textsf{e}}\\left[{{\\hat w_i^{\\ci(\\psi)}}}\\right]}=w_i$ ] and @xmath306}\\leq { \\var\\left[{{\\hat w_i^{\\ci(\\psi)}}}\\right ] } = { \\var\\left[{{\\hat w_i^{\\psi}}}\\right]}$ ] with strict inequality if @xmath307}>0 $ ] .",
    "for example , we have strict inequality if @xmath294 is a priority sampling scheme with @xmath18 .",
    "the optimized scheme @xmath303 has the same format as the schemes considered in  [ sec : threshold ] , and we know that threshold sampling minimizes the total variance among these schemes . consequently , with @xmath308 denoting threshold sampling of an expected number of @xmath6 items , it follows that @xmath153 } { \\var\\left[{{\\hat w_i^{thr}}}\\right]}\\leq \\sum_{i\\in[n ] } { \\var\\left[{{\\hat w_i^{\\ci^*(\\psi)}}}\\right ] } \\leq \\sum_{i\\in[n ] } { \\var\\left[{{\\hat w_i^{\\psi}}}\\right]}.\\ ] ]    we will now go the other way .",
    "our staring point is an independent sampling scheme @xmath302 that picks each item @xmath2 independently with probability @xmath140 , and which picks an expected integer number @xmath6 of samples .",
    "if item @xmath2 is picked , it gets weight estimate @xmath309 ; @xmath76 otherwise .",
    "for example , @xmath302 could be our threshold sampling scheme @xmath308 .",
    "we will now define a corresponding sampling scheme @xmath310 picking exactly @xmath6 samples , and getting the same variance for each item .",
    "we are going to describe an iterative procedure defining @xmath310 .",
    "initially , set @xmath311 for all @xmath71 $ ] .",
    "we are going to define different events , and as we do so , reduce @xmath140 and @xmath312 so as to reflect the remaining probability that item @xmath2 is picked or not picked , respectively .",
    "after each iteration , we have a remaining total probability @xmath313 . in each event",
    "we pick exactly @xmath6 items , and since we start with an expected number of @xmath6 items , we will always have an expected number of @xmath6 items in the remainder , that is @xmath314}p_i)/p = k$ ] .",
    "consider an item @xmath2 . if @xmath315 , item @xmath2 is not picked in any remaining event .",
    "conversely , if @xmath316 , item @xmath2 is forced to be picked in all remaining events . if @xmath317 and @xmath318 , item @xmath2 is `` unsettled '' .",
    "if there are no unsettled events , we have a final event , doing what has to be done : since @xmath314}p_i)/p = k$ ] and since each @xmath140 is either @xmath319 or @xmath76 , there are exactly @xmath6 items @xmath2 with @xmath320 , and these are all picked .",
    "assume that we have some unsettled items @xmath2 .",
    "let @xmath321 be the number of unsettled items . also , let @xmath322 be the number of items to be picked among the unsettled items , that is , we subtract the forced items that have to be picked because @xmath316 . in our next event @xmath323 , we want to pick the forced items and @xmath322 items uniformly from the unsettled items",
    ". then item @xmath2 is picked with probability @xmath324 .",
    "hence , if @xmath325 is the probability of the event @xmath323 , then for each unsettled item @xmath2 , we will reduce @xmath140 by @xmath326 , and @xmath312 by @xmath327 .",
    "we choose @xmath325 maximally , subject to the condition that no @xmath140 or @xmath312 may turn negative .",
    "then @xmath328,\\ , p_i>0,\\,r_i>0\\}\\ ] ] with this choice of @xmath325 , the event @xmath323 will settle at least one item , so it will take at most @xmath207 iterations to define the sampling scheme @xmath310 .    by definition",
    ", for each item @xmath2 , we get the same distribution of weight estimates with @xmath310 as with @xmath302 , hence also the same variances .",
    "in particular it follows that @xmath329 has the same total variance as does threshold sampling .",
    "note that when @xmath18 , the total variance of @xmath329 is always smaller than that of priority sampling since @xmath330 } { \\var\\left[{{\\hat w_i^{\\ce(thr)}}}\\right ] } & = & \\sum_{i\\in[n ] } { \\var\\left[{{\\hat w_i^{thr}}}\\right]}\\\\ & \\leq&\\sum_{i\\in[n ] } { \\var\\left[{{\\hat w_i^{\\ci^*(pri)}}}\\right]}\\\\ & < & \\sum_{i\\in[n ] } { \\var\\left[{{\\hat w_i^{pri}}}\\right]}.\\end{aligned}\\ ] ] for example , this was how we improved priority sampling in the special case at the end of the previous section .    as evidence for conjecture  [ conj2 ] , we note from  [ sec : analysis ] that it is true for the unit case .",
    "also , it can be proved to hold for the large - small example using a more refined analysis .",
    "finally , we note that the conjecture conforms nicely with the closeness of priority sampling and threshold sampling in the experiments from  [ sec : experiments ] . also , at appears that we can prove an asymptotic version ; namely that @xmath291 } { \\var\\left[{{\\hat w_i^{pri[k+1]}}}\\right]}\\leq   a\\sum_{i\\in[n ] } { \\var\\left[{{\\hat w_i^{thr[k]}}}\\right]}$ ] where @xmath331 is a large enough constant , pri@xmath332 $ ] is priority sampling of @xmath14 items , and thr@xmath333 $ ] is threshold sampling of an expected number of @xmath6 items .",
    "however , this is complicated , and beyond the scope of the current paper .    very recently , szegedy @xcite has settled conjecture  [ conj2 ] . by the above equivalence",
    ", his proof also implies conjecture  [ conj ] .",
    "thus priority sampling is variance optimal modulo one extra sample .",
    "in this section , we will discuss how we can maintain a sample of size @xmath6 for a stream of items @xmath334 with weights @xmath1 .      in so - called reservoir sampling , at any point in time , we want to have a sample of size @xmath6 from the items seen so far .",
    "thus , if we have seen items @xmath335 , we should have a sample @xmath336 $ ] .",
    "the individual samples are denoted @xmath337, .. ,s[k-1]$ ] .",
    "[ [ uniform - sampling - with - replacement ] ] uniform sampling with replacement + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    this case was studied by vitter @xcite .",
    "let @xmath338 $ ] be the current sample . while @xmath339 , we have @xmath340=i$ ] for @xmath0 . when item @xmath341 arrives , we pick a random number @xmath342 $ ] .",
    "if @xmath343 , we set @xmath344:=n$ ] . finally , we set @xmath345 .",
    "all this takes constant time for each item .",
    "we note that the weight estimates are only maintained implicitly via @xmath207 .",
    "if @xmath346 , then @xmath347 where @xmath207 is the current number of items .    [ [ weighted - sampling - with - replacement-1 ] ] weighted sampling with replacement + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    this case was studied by chaudhuri et al.@xcite . besides maintaining a sample",
    "@xmath348 $ ] , we maintain the total current weight @xmath349 } w_i$ ] .",
    "when item @xmath207 arrives , for @xmath350 , we pick a random number @xmath351 .",
    "if @xmath352 , we set @xmath353:=n$ ] .",
    "when done with all samples , we set @xmath354 . note that if we had @xmath355 , we would expect to change at least half the samples , so for exponentially increasing weight sequences",
    ", we spend @xmath356 time on each item .",
    "however , in @xcite , it is falsely claimed that their algorithm spends constant time on each item .    using the current value of @xmath179",
    ", we can compute the weight estimates of the sampled items as described in  [ sec : experiments ] .",
    "[ [ priority - sampling-3 ] ] priority sampling + + + + + + + + + + + + + + + + +    priority sampling is trivially implemented using a standard priority queue @xcite .",
    "recall that for each item @xmath2 , we generate a random number @xmath17 and a priority @xmath4 .",
    "a priority queue @xmath357 maintains the @xmath14 items of highest priority .",
    "the @xmath6 highest form our sample @xmath5 , and the smallest @xmath27 in @xmath357 is our threshold @xmath7 .",
    "it is convenient to start filling our priority queue @xmath357 with @xmath14 dummy items with weight and priority @xmath76 .",
    "when a new item arrives we simply place it in @xmath357 .",
    "next we remove the item from @xmath357 with smallest priority . with a standard comparison based priority queue",
    ", we spend @xmath358 on each item , but exploiting a floating point representation , we can get down to @xmath359 time for item @xcite ( this counts the number of floating point operations , but is independent of the precision of floating point numbers ) .",
    "this is substantially better than the @xmath356 time we spend on weighted sampling with replacement , but a bit worse than the constant time spent on uniform sampling without replacement .",
    "we shall later show how to get down to constant time if we relax the notion of reservoir sampling a bit .",
    "[ [ reservoir - sampling - for - threshold - sampling ] ] reservoir sampling for threshold sampling + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in @xcite , the threshold @xmath130 was determined before items where considered .",
    "the threshold was adapted to the traffic to get a desired amount of samples , yet bursts in traffic lead to bursts in the sample . here , as a new contribution to threshold sampling , we present a reservoir version of threshold sampling which at any time maintains a sample @xmath134 of expected size @xmath6 .",
    "as items stream by , we generate priorities as in priority sampling . at any point , @xmath207 is the number of items seen so far .",
    "we maintain a threshold @xmath130 that would give an expected number @xmath6 of items , that is , @xmath360}\\min\\{1,w_i/\\tau^{thr}\\}=k\\ ] ] also , we maintain the corresponding threshold sample , that is , @xmath361|q_i>\\tau^{thr}\\}.\\ ] ] the sample @xmath134 is stored in a priority queue .",
    "when a new item @xmath207 arrives it is first added to @xmath134 .",
    "next we have to increase @xmath130 so as to satisfy ( [ eq : thr - k ] ) with @xmath362 .",
    "finally , we remove all the items from @xmath134 with priorities lower than @xmath130 .",
    "thanks to the priority queue , each such item is extracted in @xmath358 time .",
    "we still have to tell how we compute the threshold . together with the sample , we store the set @xmath363 of all items @xmath2 with weight @xmath364 . also , we store the total weight @xmath365 of all smaller items .",
    "we note that the set @xmath363 is contained in @xmath134 .",
    "now , @xmath153}\\min\\{1,w_i/\\tau^{thr}\\}=|l|+u/\\tau^{thr}\\ ] ] the items @xmath2 in @xmath363 are stored in a priority queue ordered not by priority @xmath140 but by weight @xmath1 . when item @xmath207 arrives we do as follows .",
    "if @xmath364 , we add @xmath2 to @xmath363 ; otherwise we add its weight @xmath213 to @xmath365 .",
    "next we increase @xmath130 in an iterative process .",
    "let @xmath366 and let @xmath367 be the smallest weight in @xmath363 . if @xmath363 was empty , @xmath368 . if @xmath369 , we set @xmath370 , and we are done .",
    "otherwise , we set @xmath371 , remove @xmath36 from @xmath363 , add @xmath367 to @xmath365 , and repeat .    in the above process",
    ", each item is inserted and deleted at most once from each priority queue .",
    "also , at any time , the expected size of each priority queue is at most @xmath6 , so the total expected cost per item is @xmath358 . exploiting a floating point representation of priorities ,",
    "this can be reduced to @xmath359 time .",
    "thus we get the same time complexity as for priority sampling , but with a more complicated algorithm .",
    "we will now bring down the time per item to constant for priority sampling . to do this ,",
    "we relax the notion of reservoir sampling , and set aside space for @xmath372 items . instead of using a priority queue",
    ", we use a buffer @xmath373 for up to @xmath372 items .",
    "the buffer is guaranteed to contain the @xmath14 items of highest priority .",
    "when it gets full , a cleanup is performed to reduce the occupancy down to @xmath14 . using a standard selection algorithm @xcite",
    ", we find the @xmath374 highest priority in @xmath373 , and all items of lower priority are deleted , all in time linear in @xmath6 .",
    "the cleaning is executed once for every @xmath14 arrivals , hence at constant cost @xmath375 per item processed .",
    "after cleanup , we resume filling the buffer with fresh arrivals .    a further modification processes every item in constant time without having to wait for the cleanup to execute .",
    "two buffers of capacity @xmath372 are used , one buffer being used for collection while the other is cleaned down to @xmath376 items .",
    "then each item is processed in constant time , plus @xmath377 time at the end of the measurement period in order to find the @xmath14 items of largest threshold from the union of the contents of the two buffers .",
    "thus , provided the between successive arrivals should be bounded below by the @xmath375 processing time per item , the processing associated with each flow can be completed before the next flow arrives .",
    "we note that similar ideas can be used to get constant processing time per item for weighted sampling with replacement and threshold sampling .",
    "we have introduced priority sampling as a simple scheme for weight sensitive without replacement that is very effective for estimating subset sums .",
    "k. park , g. kim , and m. crovella , `` on the relationship between file sizes , transport protocols , and self - similar network traffic '' . in proc .",
    "4th international conference on network protocols ( icnp ) , pp . 171180 , 1996 ) ."
  ],
  "abstract_text": [
    "<S> starting with a set of weighted items , we want to create a generic sample of a certain size that we can later use to estimate the total weight of arbitrary subsets . </S>",
    "<S> applied to internet traffic analysis , the items could be records summarizing the flows of packets streaming by a router , with , say , a hundred records to be sampled each hour . </S>",
    "<S> a subset could be flow records of a worm attack whose signature is only determined after sampling has taken place . </S>",
    "<S> the samples taken in the past allow us to trace the history of the attack even though the worm was unknown at the time of sampling .    </S>",
    "<S> estimation from the samples must be accurate even with heavy - tailed distributions where most of the weight is concentrated on a few heavy items . </S>",
    "<S> we want the sample to be weight sensitive , giving priority to heavy items . at the same time , we want sampling without replacement in order to avoid selecting heavy items multiple times . to fulfill these requirements we introduce priority sampling , which is the first weight sensitive sampling scheme without replacement that is suitable for estimating subset sums . </S>",
    "<S> testing priority sampling on internet traffic analysis , we found it to perform orders of magnitude better than previous schemes .    </S>",
    "<S> priority sampling is simple to define and implement : we consider a steam of items @xmath0 with weights @xmath1 . for each item @xmath2 </S>",
    "<S> , we generate a random number @xmath3 and create a priority @xmath4 . </S>",
    "<S> the sample @xmath5 consists of the @xmath6 highest priority items . </S>",
    "<S> let @xmath7 be the @xmath8 highest priority . </S>",
    "<S> each sampled item @xmath2 in @xmath5 gets a weight estimate @xmath9 , while non - sampled items get weight estimate @xmath10 .    </S>",
    "<S> magically , it turns out that the weight estimates are unbiased , that is , @xmath11=w_i$ ] , and by linearity of expectation , we get unbiased estimators over any subset sum simply by adding the sampled weight estimates from the subset . also , we can estimate the variance of the estimates , and find , surprisingly , that the covariance between estimates @xmath12 and @xmath13 of different weights is zero .    finally , we conjecture an extremely strong near - optimality ; namely that for any weight sequence , there exists no specialized scheme for sampling @xmath6 items with unbiased weight estimators that gets smaller total variance than priority sampling with @xmath14 items . </S>",
    "<S> very recently , szegedy has settled this conjecture .    </S>",
    "<S> [ [ key - words ] ] key words + + + + + + + + +    subset sum estimation , weighted sampling , sampling without replacement . </S>"
  ]
}