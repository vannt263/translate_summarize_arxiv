{
  "article_text": [
    "statistical inference based on the observed likelihood function was initiated by bayes ( @xcite ) .",
    "this was , however , without the naming of the likelihood function or the apparent recognition that likelihood @xmath0 directly records the amount of probability at an observed data point @xmath1 ; such appeared much later ( fisher , @xcite ) .",
    "bayes proposal applies directly to a model with translation invariance that in current notation would be written @xmath2 ; it recommended that a weight function or mathematical prior @xmath3 be applied to the likelihood @xmath4 , and that the product @xmath5 be treated as if it were a joint density for @xmath6 .",
    "then with observed data @xmath1 and the use of the conditional probability lemma , a posterior distribution @xmath7 was obtained ; this was viewed as a description of possible values for @xmath8 in the presence of data @xmath9 . for the location model , as examined by the bayes approach , translation invariance suggests a  constant or flat prior @xmath10 which leads to the posterior distribution @xmath11 and , in the scalar case , gives the posterior survival probability @xmath12 , recording alleged probability to the right of a value @xmath8 .",
    "the probability interpretation that would seemingly attach to this conditional calculation is as follows : if the  @xmath8 values that might have been present in the application can be viewed as coming from the frequency pattern @xmath3 with each @xmath8 value in turn giving rise to a @xmath13 value in accord with the model and if the resulting @xmath13 values that are close to @xmath1 are examined , then the associated @xmath8 values have the pattern @xmath14 .",
    "the complication is that @xmath3 as proposed is a  mathematical construct and , correspondingly , @xmath14 is just a mathematical construct .",
    "the argument using the conditional probability lemma does not produce probabilities from no probabilities : the probability lemma when invoked for an application has two distributions as input and one distribution as output ; and it asserts the descriptive validity of the output on the basis of the descriptive validity of the two inputs ; if one of the inputs is absent and an artifact is substituted , then the lemma says nothing , and produces no probabilities .",
    "of course , other lemmas and other theory may offer something appropriate .",
    "we will see , however , that something different is readily available and indeed available without the special translation invariance .",
    "we will also see that the procedure of augmenting likelihood @xmath15 with a modulating factor that expresses model structure is a powerful first step in exploring information contained in fisher s likelihood function .",
    "an alternative to the bayes proposal was introduced by fisher ( @xcite ) as a confidence distribution .",
    "for the scalar - parameter case we can record the percentage position of the data point @xmath1 in the distribution having parameter value @xmath8 , @xmath16 this records the proportion of the @xmath8 population that is less than the value @xmath1 . for a general data point",
    "@xmath13 we have of course that @xmath17 is uniformly distributed on @xmath18 , and , correspondingly , @xmath19 from the data  @xmath1 gives the upper - tail distribution function or survivor function for confidence , as introduced by fisher(@xcite ) .",
    "a  basic way of presenting confidence is in terms of quantiles .",
    "if we set @xmath20 and solve for  @xmath8 , we obtain @xmath21 which is the value with right tail confidence @xmath22 and left tail confidence @xmath23 ; this would typically be called the @xmath22 lower confidence bound , and @xmath24 would be the corresponding @xmath22 confidence interval .    for two - sided confidence",
    "the situation has some subtleties that are often overlooked . with the large data sets that have come from the colliders of high energy physics",
    ", a poisson count can have a mean at a background count level or at a larger value if some proposed particle is actually present .",
    "a common practice in the high energy physics literature ( mandelkern , @xcite ) has been to form two - sided confidence intervals and to allow the confidence contributions in the two tails to be different , thereby accommodating some optimality criterion ; see also some discussion in section [ sec4 ] . in practice , this meant that the confidence lower bound shied away from the critical parameter lower bound describing just the background radiation .",
    "this mismanaged the detection of a new particle .",
    "accordingly , our view is that two - sided intervals should typically have equal or certainly designated amounts of confidence in the two tails . with this in mind",
    ", we now restrict the discussion to the analysis of the confidence bounds as described in the preceding paragraph and view confidence intervals as being properly built on individual confidence bounds with designated confidence values .    as a simple example consider the @xmath25 , and let @xmath26 and @xmath27 be the standard normal density and distribution functions .",
    "the @xmath28-value from data @xmath1 is @xmath29 which has normal distribution function shape dropping from @xmath30 at @xmath31 to @xmath32 at @xmath33 ; it records the probability position of the data with respect to a possible parameter value @xmath34 ; see figure [ normal](a ) . from the confidence viewpoint",
    ", @xmath35 is recording the right tail confidence distribution function , and the confidence distribution is @xmath36 .    [",
    "cols=\"^ \" , ]     in summary , in a context with a bound on the parameter , the performance error with the bayes calculation can be of asymptotic order @xmath37 .",
    "a bound on a parameter as just discussed is a  rather extreme form of nonlinearity . now consider a  very direct and common form of curvature .",
    "let ( @xmath38 ) be @xmath39 on @xmath40 and consider the quadratic interest parameter @xmath41 , or the equivalent @xmath42 which has the same dimensional units as the @xmath43 ; and let @xmath44 be the observed data . for asymptotic analysis we would view the present variables as being derived from some antecedent sample of size @xmath45 and they would then have the @xmath46 distribution .    from the frequentist view there is an observable variable @xmath47 that in some pure physical sense measures the parameter @xmath48 .",
    "it has a noncentral chi distribution with noncentrality @xmath48 and degrees of freedom 2 .",
    "for convenience we let @xmath49 designate such a variable with distribution function @xmath50 , which is typically available in computer packages ; and its square can be expressed as @xmath51 in terms of standard normal variables and it has the noncentral chi - square distribution with @xmath52 degrees of freedom and noncentrality usually described by @xmath53 .",
    "the distribution of @xmath54 is free of the nuisance parameter which can conveniently be taken as the polar angle @xmath55 .",
    "the resulting @xmath28-value function for @xmath48 is @xmath56 see figure [ circle](a ) , where for illustration we examined the behavior for @xmath57 .        from the frequentist view there is the directly measured @xmath28-value @xmath58 with a @xmath59 distribution , and any @xmath60 level lower confidence quantile is available immediately by solving @xmath61 for @xmath48 in terms of @xmath62 .    from the bayes view there is a uniform",
    "prior as directly indicated by bayes ( @xcite ) for a location model on the plane @xmath40 .",
    "the corresponding posterior distribution for @xmath8 is then @xmath63 on the plane . and the resulting marginal posterior for  @xmath48 is described by the generic variable @xmath64 .",
    "as  @xmath54 is stochastically increasing in @xmath48 , we have that the bayes analog of the @xmath28-value is the posterior survivor value obtained by an upper tail integration @xmath65    the bayes @xmath66 and the frequentist @xmath58 are actually quite different , a direct consequence of the obvious curvature in the parameter @xmath67 .",
    "the presence of the difference is easily assessed visually in figure [ circle ] by noting that in either case there is a rotationally symmetric normal distribution with unit standard deviation which is at the distance @xmath68 from the curved boundary used for the probability calculations , but the curved boundary is cupped away from the @xmath69 distribution in the frequentist case and is cupped toward the @xmath69 distribution in the bayes case ; this difference is the direct source of the bayes error .    from ( [ * ] ) and ( [ * * ] ) we can evaluate the posterior error @xmath70 which is plotted against @xmath48 in figure [ bayes excess ] for @xmath71 .",
    "this bayes error here is always greater than zero .",
    "this happens widely with a parameter that has curvature , with the error in one or other direction depending on the curvature being positive or negative relative to increasing values of the parameter .",
    "some aspects of this discrepancy are discussed in david , stone and zidek ( @xcite ) as a marginalization paradox .",
    "now in more detail for this example , consider the  @xmath60 lower quantile @xmath72 of the bayes posterior distribution for the interest parameter @xmath48 .",
    "this @xmath60 quantile for the parameter @xmath48 is obtained from the @xmath64 posterior distribution for @xmath48 giving @xmath73 where we now use @xmath74 for the @xmath75 quantile of the noncentral chi variable with 2 degrees of freedom and noncentrality @xmath54 , that is , @xmath76 .",
    "we are now in a  position to evaluate the bayes posterior proposal for @xmath48 . for this",
    "let @xmath77 be the proportion of true assertions that @xmath48 is in @xmath78 ; we have @xmath79 where the quantile @xmath80 is seen to be the @xmath81 point of a noncentral chi variable with degrees of freedom @xmath52 and noncentrality @xmath54 , and the noncentrality @xmath54 has a noncentral chi distribution with noncentrality @xmath48 .",
    "the actual proportion under a parameter value @xmath48 can thus be presented as @xmath82 \\\\ & = & \\operatorname{pr } [ 1-\\beta < h_2\\{\\rho ; \\chi_2(\\rho)\\}],\\end{aligned}\\ ] ] which is available by numerical integration on the real line for any chosen @xmath60 value .    .",
    "]     and for claimed @xmath83 : strictly less than the claimed . ]",
    "we plot the actual @xmath84 against @xmath48 in figure [ ex2_1 ] and note that it is always less than the alleged @xmath85 .",
    "we then plot the proportion for @xmath86 and for @xmath87 in figure [ ex2_2 ] against @xmath48 , and note again that the plots are always less than the claimed values @xmath22 and @xmath23 .",
    "this happens generally for all possible quantile levels @xmath60 , that the actual proportion is less than the alleged probability .",
    "it happens for any chosen value for the parameter ; and it happens for any prior average of such @xmath8 values . if by contrast the center of curvature is to the right , then the actual proportion is reversed and is larger than the alleged .    in summary , in the vector parameter context with a curved interest",
    "parameter the performance error with the bayes calculation can be of asymptotic order @xmath88 .     confidence quantile @xmath89 .",
    "the @xmath90 likelihood quantile @xmath91 $ ] is a  vertical rescaling about the origin ; the @xmath90 bayes quantile @xmath92 with prior @xmath93 is a  vertical rescaling plus a lift @xmath94 and a tilt @xmath95 .",
    "can this prior lead to a confidence presentation ?",
    "no , unless the prior depends on the data or on the level @xmath60 .",
    "\\(i ) _ the model and confidence bound_. taylor series expansions provide a powerful means for examining the large sample form of a statistical model ( see , e.g. , abebe et al .",
    ", @xcite ; andrews , fraser and wong , @xcite ; cakmak et al . , @xcite ) . from such expansions we find that an asymptotic model to second order can be expressed as a location model and to third order can be expressed as a location model with an @xmath96 adjustment that describes curvature .",
    "examples arise frequently in the vector parameter context .",
    "but for the scalar parameter context the common familiar models are location or scale models and thus without the curvature of interest here .",
    "a simple example with curvature , however , is  the  gamma distribution model : @xmath97 .    to illustrate the moderate curvature",
    ", we will take a  very simple example where @xmath13 is @xmath98 and @xmath99 depends just weakly on the mean @xmath8 , and then in asymptotic standardized form we would have @xmath100 in moderate deviations . the @xmath60-level quantile for this normal variable @xmath13 is @xmath101    the confidence bound @xmath102 with @xmath60 confidence above can be obtained from the usual fisher inversion of @xmath103 : we obtain @xmath104 thus , the @xmath60 level lower confidence quantile to order @xmath105 is @xmath106 where we add the label @xmath107 for confidence to distinguish it from other bounds soon to be calculated .",
    "see figure  [ f ] .",
    "\\(ii ) _ from confidence to likelihood_. we are interested in examining posterior quantiles for the adjusted normal model and in this section work from the confidence quantile to the likelihood quantile , that is , to the posterior quantile with flat prior@xmath108 ; this route seems computationally easier than directly calculating a likelihood integral .    from section [ sec3 ] and formula ( [ a ] ) above",
    ", we have that the prior @xmath3 that converts a likelihood @xmath109 to confidence @xmath110 is @xmath111 then to convert in the reverse direction , from confidence @xmath112 to likelihood @xmath113 , we need the inverse weight function @xmath114 interestingly , this function is equal to @xmath30 at  @xmath115 and at  @xmath1 , and is less than @xmath30 between these points when @xmath116 .",
    "\\(iii ) _ from confidence quantile to likelihood quantile_. the weight function ( [ c ] ) that converts confidence to likelihood has the form @xmath117 with @xmath118 and @xmath119 .",
    "the effect of such a tilt and bending is recorded in the .",
    "the confidence quantile @xmath120 given at ( [ b ] ) is a @xmath121 quantile of the confidence distribution . then using formula ( [ a.2 ] ) in the",
    ", we obtain the formula for converting confidence quantile to likelihood quantile : @xmath122\\\\ [ -8pt ] & = & \\hat\\theta^c\\biggl(1+\\frac{\\gamma}{2n}\\biggr).\\nonumber\\end{aligned}\\ ] ] thus , the likelihood distribution is obtained from the confidence distribution by a simple scale factor @xmath123 ; this directly records the consequence of the curvature added to the simple normal model by having @xmath99 depend weakly on @xmath8 .",
    "\\(iv ) _ from likelihood quantile to posterior quantile_. now consider a prior applied to the likelihood distribution .",
    "a prior can be expanded in terms of standardized coordinates and takes the form @xmath124 .",
    "the effect on quantiles is available from the and we see that a prior with tilt coefficient @xmath125 would excessively displace the quantile and thus would give posterior quantiles with bad behaving @xmath126 in repetitions ; accordingly , as a possible prior adjustment , we consider a tilt with just a  coefficient @xmath94 .",
    "we then examine the prior @xmath127 .",
    "first , we obtain the bayes quantile in terms of the likelihood quantile as @xmath128 and then substituting for the likelihood quantile in terms of the confidence quantile ( [ d ] ) gives @xmath129 for @xmath92 in ( [ b ] ) to be equal to @xmath130 in ( [ bbb ] ) we would need to have @xmath131 and then @xmath132 . but this would give a data dependent prior . we noted the need for data dependent priors in section [ sec3 ] , but we now have an explicit expression for the effect of priors on quantiles .",
    "now consider the difference in quantiles : @xmath133 where we have replaced @xmath134 by @xmath135 , to order@xmath105 ; figure [ g ] shows this difference as the vertical separation above a data value @xmath13 . from the third expression above we see that in the presence of model curvature @xmath75 the bayesian quantile can achieve the quality of confidence only if the prior is data dependent or dependent on the level @xmath60 .",
    "similarly , we can calculate the horizontal separation corresponding to a @xmath8 value , and obtain @xmath136\\\\ [ -8pt ] \\hspace*{7pt}&= & \\theta\\frac{\\gamma}{2n } + \\frac{a}{n } + \\frac{c}{2n}(2\\theta+ z_\\beta).\\nonumber\\end{aligned}\\ ] ] this gives the quantile difference , the confidence quantile less the bayes quantile , as a function of @xmath8 ; see figure [ g ] , and observe the horizontal separation to the right of a parameter value @xmath8 .    .",
    "]    [ ex3_1 ]    a bayes quantile can not generate true statements concerning a parameter with the reliability of confidence unless the model curvature is zero , that is , unless the model is of the special location form where bayes coincides confidence .",
    "the bayes approach can thus be viewed as having a long history of misdirection .",
    "now let @xmath8 designate the true value of the parameter @xmath8 , and suppose we examine the performance of the bayesian and frequentist posterior quantiles . in repetitions",
    "the actual proportion of instances where @xmath137 is of course @xmath60 .",
    "the actual proportion of cases with @xmath138 is then @xmath139 where for the terms of order @xmath96 it suffices to use the @xmath140 distribution for @xmath13 .",
    "the bayes calculation claims the level @xmath60 .",
    "the choice @xmath141 gives a flat prior in the neighborhood of @xmath115 which is the central point of the model curvature .",
    "with such a choice the actual proportion from the bayes approach is deficient by the amount @xmath142 . for a claimed @xmath143 quantile see figure [ ex3_1 ] for the actual proportion and for a claimed @xmath144 or @xmath83 see figure [ ex3_2 ] .",
    "thus , the @xmath60 quantile by bayes is consistently below the claimed level @xmath60 for positive values of @xmath8 , and consistently above the claimed level for negative values of  @xmath8 .    in summary , even in the scalar parameter context",
    ", an elementary departure from simple linearity can lead to a performance error for the bayes calculation of asymptotic order @xmath96 . and ,",
    "moreover , it is impossible by the bayes method to duplicate the standard confidence bounds : a stunning revelation !     and @xmath83 . ]",
    "the bayes proposal makes critical use of the conditional probability formula @xmath145 . in typical applications",
    "the formula has variables @xmath146and  @xmath147 in a temporal order : the value of the first  @xmath146 is inaccessible and the value of the second @xmath147 is observed with value , say , @xmath148 .",
    "of course , the value of the first  @xmath146 has been realized , say , @xmath149 , but is concealed and is unknown .",
    "indeed , the view has been expressed that the only probabilities possible concerning such an unknown @xmath149 are the values @xmath32 or @xmath30 and we do nt know how they would apply to that @xmath149 .",
    "we thus have the situation where there is an unknown constant  @xmath149 , a constant that arose antecedent in time to the observed value  @xmath148 , and we want to make probability statements concerning that unknown antecedent constant . as part of the temporal order we also have that the joint density became available in the order  @xmath150 for the first variable followed by @xmath151 for the second",
    "; thus , @xmath152 .",
    "the conditional probability formula itself is very much part of the theory and practice of probability and statistics and is not in question .",
    "of course , limit operations are needed when the condition @xmath153 has probability zero leading to a conditional probability expression with a zero in the denominator , but this is largely technical .    a salient concern seemingly centers on how probabilities can reasonably be attached to a constant that is concealed from view ?",
    "the clear answer is  in terms of what _ might _ have occurred given the same observational information : the corresponding picture is of many repetitions from the joint distribution giving pairs @xmath154 ; followed by selection of pairs that have exact or approximate agreement @xmath153 ; and then followed by examining the pattern in the @xmath146 values among the selected pairs .",
    "the pattern records what would have occurred for @xmath146 among cases where @xmath153 ; the probabilities arise both from the density @xmath150 and from the density @xmath151 .",
    "thus , the initial pattern @xmath150 when restricted to instances where @xmath153 becomes modified to the pattern @xmath155 .    bayes ( @xcite ) promoted this conditional probability formula and its interpretation , for statistical contexts that had no preceding distribution for @xmath8 and he did so by introducing the mathematical prior .",
    "he did provide , however , a motivating analogy and the analogy did have something extra , an objective and real distribution for the parameter , one with probabilities that were well defined by translational invariance .",
    "such a use of analogy in science is normally viewed as wrong , but the needs for productive methodology were high at that time .",
    "if @xmath3 is treated as being real and descriptive of how the value of the parameter arose in the application , it would follow that the preceding conditional probability analysis would give the conditional description @xmath156 the interpretation for this would be as follows : in many repetitions from @xmath3 , if each @xmath8 value was followed by a @xmath13 from the model @xmath157 , and if the instances @xmath6 where @xmath13 is close to @xmath1 are selected , then the pattern for the corresponding @xmath8 values would be @xmath158 . in other words , the initial relative frequency @xmath3 for @xmath8 values",
    "is modulated by @xmath15 when we select using @xmath9 ; this gives the modulated frequency pattern @xmath158 .",
    "the conditional probability formula as used in this context is often referred to as the bayes formula or bayes theorem , but as a probability formula it long predates bayes and is generic ; for the present extended usage it is also referred to as the bayes paradigm ( bernardo and smith , @xcite ) .    the bayes example as discussed in sections [ sec2 ] and  [ sec3 ] examined a location model @xmath2 and the only prior that could represent location invariance is the constant or flat prior in the location parameterization , that is , @xmath10 .",
    "this of course does not satisfy the probability axioms , as the total probability would be @xmath159 .",
    "the step , however , from just a  set of  @xmath8 values with related model invariance to a  distribution for  @xmath8 has had the large effect of emphasizing likelihood @xmath15 , as defined by fisher ( @xcite ) .",
    "and it has also had the effect , perhaps unwarranted , of suggesting that the mathematical posterior distribution obtained from the paradigm could be treated as a distribution of real probability . if the parameter to variable relationship is linear , then section [ sec3 ] shows that the calculated values have the confidence ( fisher , @xcite ; neyman , @xcite ) interpretation .",
    "but if the relationship is nonlinear , then the calculated numbers can seriously fail to have that confidence property , as determined in sections [ sec4][sec6 ] ; and indeed fail to have anything with behavior resembling probability .",
    "the mathematical priors , the invariant priors and other generalizations are often referred to in the current bayesian literature as objective priors , a term that is strongly misleading .    in other contexts , however , there may be a real source for the parameter @xmath8 , sources with a known distribution , and thus fully entitled to the term objective prior ; of course , such examples do not need the bayes approach , they are immediately analyzable by probability calculus . and , thus , to use objective to also refer to the mathematical priors",
    "is confusing .    in short , the paradigm does not produce probabilities from no probabilities . and",
    "if the required linearity for confidence is only approximate , then the confidence interpretation can correspondingly be just approximate . and",
    "in other cases even the confidence interpretation can be substantially unavailable .",
    "thus , to claim probability when even confidence is not applicable does seem to be fully contrary to having acceptable meaning in the language of the discipline .",
    "optimality is often cited as support for the bayes approach : if we have a criterion of interest that provides an assessment of a statistical procedure , then optimality under the criterion is available using a  procedure that is optimal under some prior average of the model . in other words , if you want optimality , it suffices to look for a procedure that is optimal for the prior - average version of the model .",
    "thus , restrict one s attention to bayes solutions and just find an appropriate prior to work from .",
    "it sounds persuasive and it is important .    of course , a criterion as mentioned is just a numerical evaluation and optimality under one such criterion may not give optimality under some other criterion ; so the choice of the criterion can be a major concern for the approach .",
    "for example , would we want to use the length of a posterior interval as the criterion or say the squared length of the interval or some other evaluation ; it makes a difference because the optimality has to do with an average of values for the criterion and this can change with change in the criterion .",
    "the optimality approach can lead to interesting results but can also lead to strange trade - offs ; see , for example , cox ( @xcite ) and fraser and mcdunnough ( @xcite ) . for if the model splits with known probabilities into two or several components , then the optimality can create trade - offs between these ; for example , if data sometimes is high precision and sometimes low precision and the probabilities for this are available , then the search for an optimum mean - length confidence interval at some chosen level can give longer intervals in the high precision cases and shorter intervals in the low precision cases as a trade - off toward optimality and toward intervals that are shorter on average .",
    "it does sound strange but the substance of this phenomenon is internal to almost all model - data contexts .",
    "even with a sensible criterion , however , and without the compound modeling and trade - offs just mentioned , there are serious difficulties for the optimality support for the bayes approach .",
    "consider further the example in section [ sec6 ] with a location @xmath69 variable where the variance depends weakly on the mean : @xmath13 is @xmath160 with @xmath161 and where we want a bound @xmath162 for the parameter  @xmath8 with reliability @xmath60 for the assertion that @xmath8 is larger than @xmath162 .    from confidence theory",
    "we have immediately ( [ b ] ) that @xmath163 with @xmath105 accuracy in moderate deviations.what is available from the bayes approach ?",
    "a prior @xmath164 gives the posterior bound @xmath165 the actual proportion for the @xmath60 level confidence bound is exactly @xmath60 .",
    "the actual proportion , however , for the bayes bound as derived ( [ new1 ] ) is @xmath166 and there is no choice for the prior , no choice for @xmath167 and @xmath168 , that will make the actual equal to the nominal unless the model has nonzero curvature @xmath75 .",
    "we thus have that a choice of prior to weight the likelihood function can not produce a @xmath60 level bound . but a @xmath60 level bound is available immediately and routinely from confidence methods , which does use more than just the observed likelihood function .    of course , in the pure location case the bayes approach is linear and gives confidence",
    ". if there is nonlinearity , then the bayes procedure can be seriously inaccurate .",
    "bayes ( @xcite ) introduced the observed likelihood function to general statistical usage .",
    "he also introduced the confidence distribution when the application was to the special case of a location model ; the more general development ( fisher , @xcite ) came much later and the present name confidence was provided by neyman ( @xcite ) .",
    "lindley ( @xcite ) then observed that the bayes derivation and the fisher ( @xcite ) derivation coincided only for location models ; this prompted continuing discord as to the merits and validity of the two procedures in providing a  probability - type assessment of an unknown parameter value .",
    "a distribution for a parameter value immediately makes available a quantile for that parameter , at any percentage level of interest .",
    "this means that the merits of a procedure for evaluating a parameter can be assessed by examining whether the quantile relates to the parameter in anything like the asserted rate or level asserted for that quantile .",
    "the examples in sections [ sec4][sec6 ] demonstrate that departure from linearity in the relation between parameter and variable can seriously affect the ability of likelihood alone to provide reliable quantiles for the parameter of interest .",
    "there is of course the question as to where the prior comes from and what is its validity",
    "? the prior could be just a device as with bayes original proposal , to use the likelihood function directly to provide inference statements concerning the parameter .",
    "this has been our primary focus and such priors can reasonably be called default priors .",
    "and then there is the other extreme where the prior describes the statistical source of the experimental unit or more directly the parameter value being considered .",
    "we have argued that these priors should be called objective and then whether to use them to perform the statistical analysis is a reasonable question .    between these two extremes",
    "are many variations such as subjective priors that describe the personal views of an investigator and elicited priors that represent some blend of the background views of those close to a current investigation .",
    "should such views be kept separate to be examined in parallel with objective views coming directly from the statistical investigation itself or should they be blended into the computational procedure applied to the likelihood function alone ? there would seem to be strong arguments for keeping such information separate from the analysis of the model with data ; any user could then combine the two as deemed appropriate in any subsequent usage of the information .",
    "linearity of parameters and its role in the bayesian frequentist divergence is discussed in fraser , fraser and fraser ( @xcite ) .",
    "higher order likelihood methods for bayesian and frequentist inference were surveyed in bdard , fraser and wong ( @xcite ) , and an original intent there was to include a comparison of the bayesian and frequentist results .",
    "this , however , was not feasible , as the example used there for illustration was of the nice invariant type with the associated theoretical equality of common bayesian and frequentist probabilities ; thus , the anomalies discussed in this paper were not overtly available there .",
    "a probability formula was used by bayes ( @xcite ) to combine a mathematical prior with a model plus data ; it gave just a mathematical posterior , with no consequent objective properties .",
    "an analogy provided by bayes did have a real and descriptive prior , but it was not part of the problem actually being examined .",
    "a familiar bayes example uses a  special model , a location model ; and the resulting intervals have attractive properties , as viewed by many in statistics .",
    "fisher ( @xcite ) and neyman ( @xcite ) defined confidence . and the bayes intervals in the location model case are seen to satisfy the confidence derivation , thus providing an explanation for the attractive properties .",
    "the only source of variation available to support a  bayes posterior probability calculation is that provided by the model , which is what confidence uses .",
    "lindley ( @xcite ) examined the probability formula argument and the confidence argument and found that they generated the same result only in the bayes location model case ; he then judged the confidence argument to be wrong .",
    "if the model , however , is not location and , thus , the variable is not linear with respect to the parameter , then a bayes interval can produce correct answers at a rate quite different from that claimed by the bayes probability calculation ; thus , the bayes posterior may be an unreliable presentation , an unreliable approximation to confidence , and can thus be judged as wrong .    the failure to make true assertions with a promised reliability",
    "can be extreme with the bayes use of mathematical priors ( stainforth et al .",
    ", @xcite ; heinrich , @xcite ) .",
    "the claim of a probability status for a statement that can fail to be approximate confidence is misrepresentation . in other areas of science such false claims",
    "would be treated seriously .    using weighted likelihood ,",
    "however , can be a fruitful way to explore the information available from just a  likelihood function .",
    "but the failure to have even a  confidence interpretation deserves more than just gentle caution .    a personal or a subjective or an elicited prior may record useful background to be recorded in parallel with a confidence assessment .",
    "but to use them to do the analysis and just get approximate or biased confidence seems to overextend the excitement of exploratory procedures .",
    "consider a variable @xmath13 that has a @xmath169 distribution and suppose that its density is subject to an exponential tilt and bending as described by the modulating factor @xmath170 .",
    "it follows easily by completing the square in the exponent that the new variable , say , @xmath171 , is also normal but with mean @xmath172 and variance @xmath173 .",
    "in particular , we can write @xmath174 where @xmath175 is standard normal . and",
    "if we let @xmath176 be the  @xmath60 quantile of the standard normal with @xmath177 , then the  @xmath60 quantile of @xmath178 is @xmath179 thus , with the @xmath180 we have that tilting and bending just produce a location scale adjustment to the initial variable .",
    "now suppose that @xmath181 is @xmath169 to third order , and suppose further that its density receives an exponential tilting and bending described by the factor @xmath182 .",
    "then from the preceding we have that the new variable can be expressed in terms of preceding variables as @xmath183 where succeeding lines use adjustments that are@xmath105 .",
    "the second line on the right gives quantiles in terms of the standard normal and the third line gives quantiles in terms of the initial variable @xmath13 .",
    "one application for this arises with posterior distributions .",
    "suppose that @xmath184 is @xmath185 to third order and that its density receives a tilt and bending described by @xmath186 .",
    "we then have from ( [ a.1 ] ) that the modified variable can be expressed as @xmath187\\\\ [ -8pt ] \\hspace*{35pt } & = & \\theta(1+c/2n)+a / n^{1/2 } + y^0 c/2n,\\nonumber\\end{aligned}\\ ] ] to order @xmath105 .",
    "the author expresses deep appreciation to nancy reid for many helpful discussions from various viewpoints ; and we join to express deep appreciation to pekka sinervo who introduced us to statistical concerns in high energy physics and then took valuable time from decanal duties to clarify contextual issues .",
    "also very special thanks go to ye sun , tara cai and kexin ji for many contributions toward this research , including numerical computation and preparation of figures and work on the manuscript .",
    "the natural sciences and engineering research council of canada has provided financial support .",
    "the author also thanks the reviewers for very helpful comments ."
  ],
  "abstract_text": [
    "<S> bayes [ _ philos . trans . </S>",
    "<S> r. soc . lond . _ </S>",
    "<S> * 53 * ( 1763 ) 370418 ; * 54 * 296325 ] introduced the observed likelihood function to statistical inference and provided a weight function to calibrate the parameter ; he also introduced a  confidence distribution on the parameter space but did not provide present justifications . of course the names likelihood and confidence </S>",
    "<S> did not appear until much later : fisher [ _ philos . </S>",
    "<S> trans . </S>",
    "<S> r. soc . </S>",
    "<S> lond . </S>",
    "<S> ser . a math . </S>",
    "<S> phys . </S>",
    "<S> eng . </S>",
    "<S> sci . _ * 222 * ( 1922 ) 309368 ] for likelihood and neyman [ _ philos . </S>",
    "<S> trans . </S>",
    "<S> r.  soc . </S>",
    "<S> lond . </S>",
    "<S> ser . a math . </S>",
    "<S> phys . </S>",
    "<S> eng . </S>",
    "<S> sci . _ </S>",
    "<S> * 237 * ( 1937 ) 333380 ] for confidence . </S>",
    "<S> lindley [ _ j .  </S>",
    "<S> roy . </S>",
    "<S> statist . </S>",
    "<S> soc . </S>",
    "<S> ser . </S>",
    "<S> b _ * 20 * ( 1958 ) 102107 ] showed that the bayes and the confidence results were different when the model was not location . </S>",
    "<S> this paper examines the occurrence of true statements from the bayes approach and from the confidence approach , and shows that the proportion of true statements in the bayes case depends critically on the presence of linearity in the model ; and with departure from this linearity the bayes approach can be a poor approximation and be seriously misleading . </S>",
    "<S> bayesian integration of weighted likelihood thus provides a first - order linear approximation to confidence , but without linearity can give substantially incorrect results .    . </S>"
  ]
}