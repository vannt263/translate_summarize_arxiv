{
  "article_text": [
    "the mulan experiment was designed to measure the positive muon lifetime to a precision of one part - per - million ( ppm ) , a twenty - fold improvement over earlier experiments . the muon lifetime",
    "@xmath0 determines the fermi constant g@xmath1 , which governs the strength of all weak interaction processes . in combination with other quantities ,",
    "a precision determination of g@xmath1 allows for precision testing of the standard model .",
    "the mulan experiment was located in the @xmath2e3 beamline at paul scherrer institute .",
    "it used a custom - built electrostatic kicker @xcite to produce an intense , low - energy , pulsed muon beam with beam - on periods of typically 5  @xmath3s and beam - off periods of typically 22  @xmath3s . during",
    "the beam - on ( or accumulation ) periods , the incoming muons were accumulated in a stopping target , and during the beam - off ( or measurement ) periods , the outgoing decay positrons were observed in a scintillator array . by recording the times of the outgoing positrons during the measurement period",
    "the @xmath4 decay curve was constructed and the muon lifetime @xmath0 was extracted .    reaching a precision of one part - per - million in @xmath0 requires both enormous statistics ( exceeding 10@xmath5 decays ) and careful control and comprehensive monitoring of systematic effects . to date",
    ", we have published results from our first measurement of the muon lifetime , with a precision of 11  ppm @xcite .",
    "the analysis of data collected in two production runs , each with @xmath6 recorded muon decays , is currently underway .    in this paper",
    "we describe the design , operation and performance of the data acquisition system that was developed for the mulan experiment . in section [ s : setup ] , we briefly describe the experimental design and measuring devices . in section [",
    "s : acquisition ] , we discuss the design and layout of the data acquisition system , give details of our methods for control synchronization and data readout , and discuss the system performance .",
    "finally , we discuss the custom system developed for online control and run monitoring in section [ s : monitoring ] .",
    "a schematic view of the mulan setup is shown in fig .  [",
    "f : schematic ] .",
    "the setup comprised a target assembly for accumulating muons , a high - rate wire chamber for beam monitoring , and a fast - timing , finely - segmented , large - acceptance scintillator array for recording decay positrons .",
    "the stopping targets were mounted inside the beam pipe vacuum and were rotated into the beam for production running and out of the beam for beam studies .",
    "the high - rate wire chamber ( denoted the beam monitor )  with one horizontal and one vertical wire plane  was located downstream of the target assembly and immediately following the beam - pipe exit window .",
    "the scintillator array ( denoted the mulan ball ) consisted of 170 triangular plastic scintillator tile pairs that were arranged in the form of a truncated icosahedron ( _ i.e. _  a soccer ball geometry ) .",
    "the @xmath7@xmath8@xmath9 scintillator tiles were each read out via a light pipe and a photomultiplier tube to a vme - based , 500 mhz , 8  bit waveform digitizer . when an input signal exceeded a programmable threshold the waveform digitizer wrote both a time stamp and a consecutive sequence of flash adc samples to the onboard fifo memory of the corresponding digitizer channel ( fig .",
    "[ f : pulse ] shows a typical digitized pulse ) .",
    "the wire chamber was read out via pre - amplifiers and discriminators to a custom - built fpga that derived the x - y coordinates and hit times of the wire - plane coincidences .",
    "all chamber coincidences during the measurement period , and pre - scaled chamber coincidences during the accumulation period , were written into a vme - based memory unit ( struck sis3600 ) .",
    "additional systems were responsible for the control and the monitoring of the electrostatic kicker , beamline magnets , high - voltage system and other variables relating to experimental conditions .",
    "the data acquisition in mulan had to confront a number of challenges .",
    "first , to accumulate @xmath6 decays implies both very high data rates ( up to 50  mb / sec ) and very large data volumes ( up to 100  tb ) must be handled with small deadtimes , with any necessary deadtimes being scheduled to avoid distorting the positron time spectrum .",
    "second , comprehensive monitoring of experimental variables and straightforward configuration for diagnostic measurements must be accommodated .    in mulan",
    "the acquisition does not read out decay positrons one - by - one . rather , to handle the high data rates and avoid any deadtime - related distortions , we collected data in repeating acquisition cycles of deadtime - free time segments . for details ) . ]",
    "the approach utilized the vme - based memories in the waveform digitizers to temporarily cache the incoming data .",
    "each acquisition cycle thus yielded a complete , distortion - free record of all over - threshold pulse shapes from the scintillator tiles over the entire duration of the measurement period . in normal operation",
    "the duration of one time segment was 5000 fill cycles or approximately 135 milliseconds . between each time segment a short deadtime of roughly 2 - 4  ms",
    "was employed to complete the transactions that manage the acquisition .    to handle high rates and facilitate diagnostic studies ,",
    "the data acquisition was designed with a modular , distributed philosophy and implemented on a parallel , layered processor array .",
    "a frontend layer of seven dual - processors was used for the parallel readout of the waveform digitizers and the beam monitor .",
    "a slow control layer was used for the control and monitoring of other instrumentation such as high - voltage supplies and beamline components .",
    "a backend layer was responsible for the assembly of the event fragments and the storage of the reconstructed events .",
    "lastly , an online analysis layer was responsible for monitoring and diagnostics that ensured the integrity of the recorded data .",
    "the data acquisition system is depicted schematically in fig .",
    "[ f : layout ] .",
    "it shows the frontend layer responsible for waveform digitizer readout and beam monitor readout , the slow control layer responsible for control and monitoring , the backend layer responsible for event assembly and data storage , and the online analysis layer responsible for basic histogramming and integrity checks .",
    "the hardware platform comprised a networked cluster of intel xeon 2.6  ghz processors ( with 1 - 4 gigabytes of memory ) running fedora core 3 linux .",
    "the acquisition software was developed using the midas data acquisition @xcite and root data analysis @xcite frameworks . to eliminate packet collisions that would degrade network traffic flow",
    ", we used two private gigabit ethernet networks .",
    "the first network handled traffic between the frontend layer and the backend layer , while the second handled traffic between the backend layer and the slow control and online analysis layers .",
    "this network segmentation significantly boosted performance and reduced the deadtimes between the frontend and backend layers .",
    "a key component of the acquisition system was the so - called `` magic box '' frontend ( mbfe ) process .",
    "this process was responsible for synchronizing the software defined data acquisition cycles ( _ i.e. _  time segments ) and the hardware beam on / off cycles ( _ i.e. _  fill periods ) . to coordinate the software components ,",
    "the mbfe process used remote procedure calls ( rpcs ) for network communication between data acquisition modules on different processors . to coordinate the hardware components , the mbfe operated an fpga - based programmable pulser that issued command signals to the beam kicker , waveform digitizers and beam monitor . in this manner",
    "all deadtimes were executed between the midas events rather than during the midas events . the mbfe process executed on a dedicated single - processor frontend machine .",
    "the primary source of high - rate data were the 340 channels of waveform digitizers that record pulses from the 340 scintillator tiles of the mulan ball .",
    "the digitizers were distributed over six vme crates and read out by six waveform digitizer frontend ( wfdfe ) processes that executed on six dual - processor frontend machines .",
    "the data were transferred from the fifo memories of the waveform digitizer to the random access memory of the frontend processors via struck sis3100/1100 bridges @xcite .",
    "each bridge consisted of a sis3100 vme card that provided access to each crate s vme bus , a sis1100 pci card that provided access to each processor s pci bus , and a gigabit fiber - optic link connecting the two interface cards .",
    "a further source of high - rate data was the beam monitor wire chamber , which was generally activated for beam studies and de - activated for production running .",
    "the beam monitor data was read out by the beam monitor frontend ( bmfe ) process that executed on an additional dual - processor frontend machine .",
    "the beam monitor data was transferred from a struck sis3600 vme memory unit to the frontend processor random access memory via an additional struck sis3100/1100 bridge .",
    "the read out of beam monitor data was performed continuously during each time segment by an asynchronous readout loop , thus preventing any possibility of memory overflow in the sis3600 memory unit .",
    "the various processes for control and monitoring were executed on two slow control processors and provided the read out of information such as beamline magnet currents , photomultiplier voltages / currents , vme power supply voltages / currents , environmental magnetic fields and environmental temperatures . a beamline frontend ( blfe ) process and",
    "separator frontend ( sepfe ) process were responsible for the control and the read out of the beamline elements .",
    "a high voltage frontend ( hvfe ) process was responsible for the control and the read out of a lecroy 1440 multichannel high voltage system powering the scintillator tiles . additionally , a vme - crate frontend ( psfe ) process enabled monitoring of the vme power supplies and a 1-wire interface frontend ( owfe ) process enabled monitoring of various temperature sensors and magnetic field probes .",
    "lastly , a scaler frontend ( scfe ) process provided the control and read out for the camac scalers that record the primary proton current and beam monitor hits .",
    "note that the slow control processes read out data in so - called `` periodic '' mode , _",
    "i.e. _  at fixed time intervals that were asynchronous with the data acquisition cycle .",
    "every slow control process wrote one copy of its data as a midas databank to be stored in the main data stream and another copy of its data as an ascii data file that was used by the online monitoring system .",
    "note that the ascii text files were written irrespective of whether or not a midas run was currently underway .",
    "the data fragments from the frontend processes were asynchronously transferred to the backend layer across the frontend network .",
    "initially , the data fragments from the six waveform digitizer processes and the beam monitor process were transferred to individual memory segments on the backend machine be01 , _ i.e. _  one memory segment per frontend process .",
    "the midas event builder ( mevb ) process then re - assembled the fragments from the various wfdfe and bmfe processes into complete events .",
    "the reconstructed events were then written by the event builder process to a final memory segment known as the system memory segment .",
    "note that the slow control databanks from the various slow control processes were directly transferred to the system memory segment and thus bypassed event building .",
    "the backend layer used two servers and a three stage pipeline to permanently store two copies of the entire mulan dataset .",
    "first , the data were transferred from the system memory segment on the backend processor be01 to a temporary disk file on a local redundant disk array .",
    "we used an array of ten 250  gb disks controlled by a pci raid controller .",
    "the raid was configured as raid10 ( a nested disk array with striping and mirroring ) in order to provide both fault tolerance and high i / o performance .",
    "next , the data files on be01 were asynchronously migrated from disk to both an lt03 tape robot system , mounted on processor be01 , and a remote redundant disk array , mounted on processor be02 .",
    "finally , the data files on be02 were asynchronously migrated to either a second lt03 tape robot system or the psi central data archive .",
    "this multi - step approach was used to minimize any possible delays in data - taking due to latencies associated with the tape storage or the archive storage .",
    "in addition , the disk files on the be02-redundant disk array were available for any offline analysis work .",
    "the online analysis layer was used for integrity checking and online histogramming .",
    "a dedicated dual - processor machine hosted the online analyzer processes responsible for the various diagnostic and monitoring tasks . on the backend processor be01 .",
    "the online analyzer received events `` as available '' in order to avoid introducing any delays into the read out and the data storage .",
    "this layer is described in detail in sec .",
    "[ s : monitoring ] .",
    "this section gives detailed information on the synchronization of the different components of the data acquisition .",
    "a diagram summarizing the various synchronization signals that were transmitted between the magic box frontend and the waveform digitizer frontends is given in fig .",
    "[ f : operation ] .    to initiate a new time segment",
    "the mbfe process sends a `` start - of - segment '' message to the magic box programmable pulser via a parallel port connection .",
    "this triggers transmission of a pre - programmed number of `` run '' gates to the waveform digitizers , thus causing each digitizer channel to store the pulses that were present during these gates .",
    "each run gate corresponds to one 22  @xmath3s measurement period and was synchronized to the beam on - off transitions which were also controlled by the magic box .    after completing a run gate sequence the magic box pulser set an `` end - of - segment '' bit .",
    "this bit was identified by the mbfe process via a polling routine and then broadcast to the waveform digitizer frontend processes via a remote procedure call .",
    "each rpc generated a program interrupt that causes the digitizer processes to read ( i ) the fill count register for each digitizer module , ( ii ) the data count register for each digitizer channel , and ( iii ) the fifo memories of each digitizer channel .",
    "each digitizer process separately reported the completion of task ( ii ) , denoted `` done - data - count - read '' , and task ( iii ) , denoted `` done - fifo - read '' , to the mbfe process via an rpc .",
    "note that the read out of the digitizer fifo memories could extend into the following time segment , but the requirement that all `` done - data - count - read '' messages were received before the start of that next segment ensured that the cached data in the digitizer fifo memories were correctly assigned to their parent time segment .",
    "after the receipt of all `` done - fifo - read '' rpcs from all digitizer processes , and the identification of the next end - of - segment bit in the magic box pulser , the mbfe initiates a new readout cycle via a new `` end - of - segment '' rpc to the digitizer processes .",
    "the `` done - fifo - read '' requirement ensures the correct sequencing of the vme accesses for the various readout tasks .",
    "the waveform digitizer frontend processes performed several tasks that included : the read out of the digitizer memories , the lossless compression of the digitizer data , and the assembly of the midas databanks .",
    "the wfdfe processes ran on dual - processor frontend machines and used posix multi - threading functions to enable the simultaneous execution of multiple readout threads ( one thread per segment ) .",
    "mutexes were used for the thread - unsafe parts of the frontend code such as reading data from the digitizer memories and transferring data to the backend processor .",
    "the wfdfe threads also performed the lossless compression of digitizer data using the zlib library .",
    "an md5 checksum computation by the acquisition code before data compression and by the analysis code after data decompression was used to ensure the integrity of the compressed databank .",
    "the waveform digitizer data were stored in midas data banks with one midas bank per digitizer channel . within each bank",
    "the data were formatted as digitizer blocks with twenty four consecutive 8-bit adc values , a 32-bit alignment block , a 16-bit fill stamp and a 16-bit time stamp .",
    "a header block stored the data count and fill count that were read from each digitizer  a redundancy enabling additional error checking .      in the mulan experiment ,",
    "the data was stored on disk and tape in `` runs '' , which comprised time - ordered sequences of software acquisition cycles of duration 135  ms , which were further subdivided into hardware - based fill cycles of duration 27  @xmath3s .",
    "a standard run was acquired over roughly 600 seconds , consumed about 10  gb of storage space , and contained @xmath10 acquisition cycles or roughly @xmath11 fill cycles .    under typical running conditions the average rate of recorded tile pulses was about 3  khz per digitizer channel , 200  khz per digitizer frontend , and 1  mhz in total .",
    "we stress these rates were time averages across the fill cycle , the instantaneous rates vary considerably over the fill period .",
    "the above pulse rates were equivalent to approximately 100  kb / s per digitizer channel , approximately 6  mb / s per digitizer frontend , and approximately 36  mb / s in total .",
    "herein we discuss the rate capabilities of the major components of the data acquisition ",
    "_ i.e. _   data read out , data compression , network transfer , event building and event storage  and the resulting performance of the complete system .    between time segments",
    " _ i.e. _  between the magic box process identifying the end of the previous time segment and initiating the start of the following time segment  the waveform digitizer processes must read the data count registers of each digitizer channel and fill count registers of each digitizer module",
    ". the necessary enhanced parallel - port communications between the magic box process and the magic box pulser had typical delays of several microseconds .",
    "the necessary remote procedure calls between the magic box process and the waveform digitizer processes had delays up to 100 microseconds . by comparison",
    "the sequence of about 60 data count reads ( one read per digitizer channel ) and about 15 fill count reads ( one read per digitizer module ) across an entire vme crate took several milliseconds  thereby dominating the intervening deadtime between successive segments .",
    "consequently , the data count and fill count reads imposed a data - rate independent 2 - 4  ms deadtime between successive time segments ( a few percent deadtime for 135  ms time segments ) .",
    "the data volume cached during the time segment determined the time required to read out and compress the digitizer data . below a certain critical data rate",
    "the read out and compression were completed during the acquisition of the next segment , thereby adding no additional deadtime between acquisition cycles . above this critical data rate",
    "the read out and compression were not completed during the acquisition of the next segment , thereby extending the intervening deadtime between acquisition cycles ( for details see secs .  [",
    "s : synchronization ] and [ s : operation ] ) . in practice ,",
    "the critical data rate for minimal deadtime was found to be approximately 60  mb / s of uncompressed data or approximately 40  mb / s of compressed data .    following the read out and compression ,",
    "the potential bottlenecks in data acquisition were the network transfer , event building and data storage .",
    "event building involved copying data between memory segments and event storage involved copying data from memory segments to disk and tape .",
    "these tasks were limited by the rate capabilities of i / o operations , both to and from memory , disk and tape .",
    "if event building was unable to keep pace , it blocked the data transfer from the frontend processes , thus potentially inhibiting the data acquisition .",
    "if data logging was unable to keep pace , it blocked the data transfer from the event builder , again potentially inhibiting the data acquisition . under normal conditions of about 25  mb / s of compressed data",
    "the acquisition performance was not limited by network transfers , event building and data storage rates . however , for rates of 35  mb / sec and higher the system was unable to maintain the storage of two copies of the data - set .",
    "note that data compression in the digitizer frontends  which reduced the data rates through event building and data storage by roughly 30%  was important in circumventing the rate limitations of network transfer , event building and data storage for high rate operation .    in summary ,",
    "our simple model for the rate performance of the data acquisition is ( i ) a fixed deadtime of several percent for raw data rates below 60  mb / s and ( ii ) a linearly increasing deadtime for greater data rates .",
    "the fixed deadtime was dominated by the read out of the data count and fill count registers between the time segments .",
    "the rate - dependent deadtime was dominated by the read out , lossless compression and databank assembly of the digitized pulses on the frontend processors .",
    "this interpretation was supported by results obtained from rate tests of the data acquisition setup shown in figs .",
    "[ f : performance1 ] and [ f : performance2 ] .",
    "[ f : performance1 ] shows a few percent deadtime below tile pulse rates of 2  mhz and a linearly increasing deadtime for tile pulse rates above 2  mhz .",
    "[ f : performance2 ] shows the increasing recorded pulse rate with increasing incident pulse rate for rates below 2  mhz and the saturation of the recorded pulse rate with increasing incident pulse rate for rates above 2  mhz .",
    "note that the limit of 2  mhz ( 60  mb / s ) was the rate capacity of the data read out and the lossless compression in the waveform digitizer frontend processes . to increase the capacity",
    "one could either increase the number of digitizer frontends , increase the speed of the frontend processors , or both .",
    "the online analyzer utilized the midas analyzer package and a modular , multistage approach to the analysis tasks .",
    "specifically , the different analysis tasks were implemented as individual analyzer modules that could be added or removed from the analysis stream as required . each analysis module had access to a global structure that contained both the raw data from the acquisition read out ( _ e.g. _  raw data from the digitizer memories ) and the derived data from the preceding modules ( _ e.g. _  derived data such as fill numbers , time stamps , adc arrays ) .",
    "low - level modules were responsible for decompressing the raw data , decoding the digitizer data , checking the data integrity , and filling derived fields in the global structure .",
    "histogramming modules were responsible for filling the histograms utilized by the low - level data monitoring that ensured the experimental setup was operating correctly .",
    "such histograms included distributions of fill numbers and hit times for the scintillator tile data and x - y profiles and time spectra for the beam monitor data .",
    "high - level modules were responsible for the `` physics '' analysis such as fitting of decay curves by detector position and monitoring of gains and pedestals by scintillator tile .",
    "a detailed record of the run - by - run evolution of all relevant quantities was crucial to ensuring the long term stability of the experimental set - up and maintaining a comprehensive record for systematic studies .",
    "consequently , an important component of the mulan data acquisition was the automated maintenance of an electronic database of the running conditions .",
    "the midas data acquisition package included support for the mysql open source database  the midas logger process being responsible for the transfer of parameters of interest from the midas online database to the mysql database at both the start of a run and the end of a run .",
    "the mysql database contained run - by - run information derived from the midas online database such as run start time , run stop time , the number of events , and hardware settings including the accumulation period and measurement period . the run - by - run comments that were entered by the shift operators  _",
    "e.g. _  target material , magnet orientation  were also copied to the mysql database by the midas logger .",
    "in addition , a system was developed to enable the recording of such physical quantities as the gains , pedestals and fitted lifetimes in the scintillator tiles on a run - by - run basis .",
    "the system was based on a process that read the histogram files that were generated for the individual runs by the online analyzer , evaluated the interesting physical quantities such as gains , pedestals and lifetimes , and wrote the results into the mysql database .",
    "the database provided a convenient approach to sorting data according to running conditions in subsequent analyses .",
    "for example , by using the information recorded in the mysql database we automatically maintained summed histograms for different running configurations such as the target material and the magnetic field orientation .      another important component of the acquisition system was a custom web interface to the online analyzer histograms , slow control data and mysql database .",
    "the interface provided a single , simple gateway to the online analyzer , slow control and database information  irrespective of how the data was derived  that permitted both local control and monitoring and remote control and monitoring of the experiment .",
    "it enabled the plotting of histograms both on a run - by - run basis and by experimental configurations such as the target material and the magnetic field . by using a modular structure",
    "the web interface was straightforward for users to modify or extend .",
    "the interface enabled plotting of online histograms and database information using a dynamic web page that utilized a set of root macros and a php language interface between the root macros and the http server .",
    "the root macros were responsible for generating the histograms and tables and creating the output in graphical and html formats .",
    "the php scripts were responsible for processing the user requests from the web interface , executing the corresponding root macros with appropriate input parameters , and building the html - formatted output web pages .",
    "various templates of root macros were created to assist users in developing new macros that access the information from the mysql database , slow control system and analyzer histograms .",
    "[ f : overview ] shows a sample `` trend '' plot indicating the accumulation of muon decays during the 2007 production run .",
    "[ f : lifetime ] shows a sample run - by - run histogram indicating the time distribution of tile hits during the fill cycle , _ i.e. _  both the accumulation period and the measurement period .",
    "the web interface also incorporated such items as the run plan and shift schedules and checklists .",
    "we have described the data acquisition for the mulan muon lifetime experiment . the acquisition recorded the digitized output from scintillator tiles of outgoing positrons from muon decay .",
    "the acquisition used the onboard vme memories in custom - built waveform digitizers , a repeating cycle of deadtime - free time segments , and a parallel network of dual - processor machines , to achieve our goals for integrity , performance and reliability .",
    "the system also featured a custom web interface for monitoring experimental conditions that enabled access to the online histograms , run database and slow control information .",
    "the system was capable of recording muon decays at rates up to approximately 1  mhz and read out data at rates up to approximately 60  mb / sec with a few percent deadtime .",
    "the data acquisition system was used in mulan production runs in 2006 and 2007 to record a total of @xmath12 positive muon decays .",
    "we would like to thank stefan ritt and pierre amaudruz for many valuable communications concerning the midas data acquisition package .",
    "this work was supported in part by the u.s",
    ". national science foundation ."
  ],
  "abstract_text": [
    "<S> we describe the data acquisition system for the mulan muon lifetime experiment at paul scherrer institute . the system was designed to record muon decays at rates up to 1  mhz and acquire data at rates up to 60  mb / sec . </S>",
    "<S> the system employed a parallel network of dual - processor machines and repeating acquisition cycles of deadtime - free time segments in order to reach the design goals . </S>",
    "<S> the system incorporated a versatile scheme for control and diagnostics and a custom web interface for monitoring experimental conditions .    </S>",
    "<S> @xcite    muon lifetime , high - speed data acquisition , high - speed electronics </S>"
  ]
}