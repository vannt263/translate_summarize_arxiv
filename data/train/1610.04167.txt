{
  "article_text": [
    "generative models have played a crucial part in the early development of the field of machine learning .",
    "however , in recent years they were mostly cast aside in favor of discriminative models , lead by the rise of convnets  @xcite , which were found to perform equally well or better than classical generative counter - parts on almost any task . despite the increased interest in unsupervised learning ,",
    "many of the recent studies on generative models choose to focus solely on the generation capabilities of these models  @xcite .",
    "there is much less emphasis on leveraging generative models to solve actual tasks , e.g. semi - supervised learning  @xcite , image restoration  @xcite or unsupervised feature representation  @xcite .",
    "nevertheless , work on generative models for solving actual problems are yet to show a meaningful advantage over competing discriminative models .    on the most fundamental level ,",
    "the difference between a generative model and a discriminative one is simply the difference between learning @xmath0 and learning @xmath1 , respectively .",
    "while it is always possible to infer @xmath1 given @xmath0 , it might not be immediately apparent why the generative objective is preferred over the discriminative one . in  @xcite",
    ", this question was studied w.r.t .",
    "the sample complexity , proving that under some cases it can be significantly lesser in favor of the generative classifier .",
    "however , their analysis was limited only to specific pairs of discriminative and generative classifiers , and they did not present a general case where the the generative method is undeniably preferred .",
    "we wish to highlight one such case , where learning @xmath0 is provenly better regardless of the models in question , by examining the problem of classification with missing data . despite the artificially well - behave nature of the typical classification benchmarks presented in current publications ,",
    "real - world data is usually riddled with noise and missing values    instead of observing @xmath2 we only have a partial observation @xmath3     a situation that tends to be ignored in modern research .",
    "discriminative models have no natural mechanisms to handle missing data and instead must rely on data imputation , i.e. filling missing data by a preprocessing step prior to prediction .",
    "unlike the discriminative approaches , generative models are naturally fitted to handle missing data by simply marginalizing over the unknown values in @xmath0 , from which we can attain @xmath4 by an application of bayes rule . moreover , under mild assumptions which apply to many real - world settings , this method is proven to be optimal _ regardless of the process by which values become missing _ ( see sec .  [",
    "sec : missing_data ] for a more detailed discussion ) .",
    "while almost all generative models can represent @xmath0 , only few can actually infer its exact value efficiently .",
    "models which possess this property are said to have _",
    "tractable inference_. many studies specifically address the hard problem of learning generative models that do not have this property .",
    "notable amongst those are works based on variational inference  @xcite , which only provide approximated inference , and ones based on generative adversarial networks  @xcite , which completely circumvent the inference problem by restructuring the learning problem as a two - player game of discriminative objectives    both of these approaches are incapable of tractable inference .",
    "there are several advantages to models with tractable inference  ( e.g. they could be simpler to train ) , and as we have shown above , this property is also a requirement for proper handling of missing data in the form of marginalization . in practice , to marginalize over @xmath0 means to perform integration on it , thus , even if it is tractable to compute @xmath0 , it still might not be tractable to compute every possible marginalization . models which are capable of this are said to have _",
    "tractable marginalization_. mixture models  ( e.g. gaussian mixture models ) are the classical example of a generative model with tractable inference , as well as tractable marginalization .",
    "though they are simple to understand , easy to train and even known to be universal    can approximate any distribution given sufficient capacity  ",
    "they do not scale well to high - dimensional data .",
    "the gaussian mixture model is an example of a shallow model    containing just a single latent variable    with limited expressive efficiency . more generally , graphical models are deep and exponentially more expressive , capable of representing intricate relations between many latent variables . while not all kinds of graphical models are tractable , many are , e.g. latent tree models  @xcite and sum - product networks  @xcite .",
    "the main issue with generic graphical models is that by virtue of being too general they lack the inductive bias needed to efficiently model unstructured data , e.g. images or text . despite the success of structure learning algorithms  @xcite on structured datasets , such as discovering a hierarchy among diseases in patients health records ,",
    "there are no similar results on unstructured datasets .",
    "indeed some recent works on the subject have failed to solve even simple handwritten digit classification tasks  @xcite .",
    "thus deploying graphical models on such cases requires experts to manually design the model .",
    "other attempts which harness neural networks blocks  @xcite offer tractable inference , but not tractable marginalization .    to summarize , most generative models do not have tractable inference , and of the few models which do , they all possess one or more of the following shortcomings : ( i )  they do not possess the expressive capacity to model high - dimensional data  ( e.g. images ) , ( ii )  they require explicitly designing all the dependencies of the data , or ( iii )  they do not have tractable marginalization .",
    "we present in this paper a family of generative models we call _ tensorial mixture models _ ( tmms ) , which aim to address the above shortcomings of alternative models . under tmms",
    ", we assume that the data generated by our model is composed of a sequence of local - structures ( e.g. patches in an image ) , where each local - structure is generated from a small set of simple component distributions ( e.g. gaussian ) , and the dependencies between the local - structures are represented by a _",
    "prior tensor _ holding the prior probabilities of assigning a component distribution to each local - structure . in their general form ,",
    "tmms are intractable as the prior tensor is typically of exponential size . however , by _ decomposing the prior tensor _ ,",
    "inference of tmms becomes realizable by _ convolutional arithmetic circuits _",
    "( convacs )    a recently proposed  @xcite convolutional neural network architecture based on two operations , weighted sum and product pooling    which enables both tractable inference as well as tractable marginalization .",
    "while graphical models are typically hard to design as we have previously mentioned , convacs follow the same design conventions of modern convnets , which reduces the task of designing a model to simply choosing the number of channels at each layer , and size of pooling windows .",
    "convacs were also the subject of several theoretical studies on its expressive capacity  @xcite and comparing them to convnets  @xcite , showing they are especially suitable for high - dimensional structured data  ( images , audio , etc . ) with a non - negligible advantage over standard convnets .",
    "sum - product networks are another kind of graphical model realizable by arithmetic circuits , but it does not posses the same theoretical guarantees as our model , nor does it provide a simple method to design efficient and expressive models .",
    "the rest of the article is organized as follows . in sec .",
    "[ sec : pre ] we briefly review mathematical background on tensors required in order to follow our work .",
    "this is followed by sec .",
    "[ sec : model ] which presents our generative model and its theoretical properties . how our model is trained is covered in sec .",
    "[ sec : training ] , and a thorough discussion on the importance of marginalization and its implications on our model is given in sec .",
    "[ sec : missing_data ] .",
    "we conclude the article by presenting our experiments on classification with missing data and a visualization of our model in sec .",
    "[ sec : exp ] , and revisit the main points of the article and future research in sec .  [",
    "sec : summary ] .",
    "we begin by establishing the minimal background in the field of tensor analysis required for following our work .",
    "a tensor is best thought of as a multi - dimensional array @xmath5 , where @xmath6 , d_i \\in [ m_i]$ ] .",
    "the number of indexing entries in the array , which are also called _ modes _ , is referred to as the _ order _ of the tensor .",
    "the number of values an index of a particular mode can take is referred to as the _ dimension _ of the mode .",
    "the tensor @xmath7 mentioned above is thus of order @xmath8 with dimension @xmath9 in its @xmath10-th mode . for our purposes",
    "we typically assume that @xmath11 , and simply denote it as @xmath12 .",
    "the fundamental operator in tensor analysis is the _",
    "tensor product_. the tensor product operator , denoted by @xmath13 , is a generalization of outer product of vectors ( 1-ordered vectors ) to any pair of tensors .",
    "specifically , let @xmath14 and @xmath15 be tensors of order @xmath16 and @xmath17 respectively , then the tensor product @xmath18 results in a tensor of order @xmath19 , defined by : @xmath20 .",
    "the main concept from tensor analysis we use in our work is that of tensor decompositions .",
    "the most straightforward and common tensor decomposition format is the rank-1 decomposition , also known as a candecomp / parafac decomposition , or in short , a _",
    "cp decomposition_. the cp decomposition is a natural extension of low - rank matrix decomposition to general tensors , both built upon the concept of a linear combination of rank-1 elements .",
    "similarly to matrices , tensors of the form @xmath21 , where @xmath22 are non - zero vectors , are regarded as @xmath8-ordered rank-1 tensors , thus the rank-@xmath23 cp decomposition of a tensor @xmath14 is naturally defined by : @xmath24 where @xmath25 and @xmath26 are the parameters of the decomposition . as mentioned above , for @xmath27",
    "it is equivalent to low - order matrix factorization .",
    "it is simple to show that any tensor @xmath14 can be represented by the cp decomposition for some @xmath23 , where the minimal such @xmath23 is known as its _",
    "tensor rank_.        another decomposition we will use in this paper is of a hierarchical nature and known as the hierarchical tucker decomposition @xcite , which we will refer to as _",
    "ht decomposition_. while the cp decomposition combines vectors into higher order tensors in a single step , the ht decomposition does that more gradually , combining vectors into matrices , these matrices into 4th ordered tensors and so on recursively in a hierarchically fashion .",
    "specifically , the following describes the recursive formula of the ht decomposition are diagonal and equal to @xmath28 ( using the notations from eq .",
    "[ eq : ht_decomp ] ) . ] for a tensor @xmath12 where @xmath29 , i.e. @xmath8 is a power of two to be a power of two is solely for simplifying the definition of the ht decomposition . more generally , instead of defining it through a complete binary tree describing the order of operations , the canonical decomposition can use any balanced binary tree .",
    "] : @xmath30 where the parameters of the decomposition are the vectors @xmath31 , \\gamma \\in [ r_l]}$ ] and the top level vector @xmath32 , and the scalars @xmath33 are referred to as the _ ranks of the decomposition_. similar to the cp decomposition , any tensor can be represented by an ht decomposition . moreover , any given cp decomposition can be converted to an ht decomposition by only a polynomial increase in the number of parameters .",
    "the relationship between tensor decomposition and networks arises from the simple observation that through decomposition one can tradeoff storage complexity with computation where the type of computation consists of sums and products .",
    "specifically , tensor decompositions could be seen as a mapping , that takes a tensor of exponential size and converts it into a polynomially sized representation , coupled with a decoding algorithm of polynomial runtime complexity to retrieve the original entries of tensor    essentially trading off space complexity for computational complexity .",
    "examining the decoding algorithms for the cp and ht decompositions , i.e. eq .  [ eq : cp_decomp ] and eq .",
    "[ eq : ht_decomp ] , respectively , reveal a shared framework for representing these algorithms via computation graphs of products and weighted sums , also known as _ arithmetic circuits _  @xcite or sum - product networks  @xcite .",
    "more specifically , these circuits take as input @xmath8 indicator vectors @xmath34 , representing the coordinates @xmath35 , where @xmath36}$ ] , and output the value of @xmath37 . in the case of the cp decomposition",
    ", the matching decoding circuit is defined by eq .",
    "[ eq : cp_decode ] below : @xmath38 the above formula is better represented by the network illustrated in fig .  [ fig : cp_decoder ] , beginning with an input layer of @xmath39 @xmath40-dimensional indicator vectors arranged in a 3d array , followed by a @xmath41  _ conv _ operator , a global product pooling layer , and ends with a dense linear layer outputting @xmath37 .",
    "the _ conv _",
    "operator is not unlike the standard convolutional layer of convnets , with the sole difference being that it may operate without _ coefficient sharing _ , i.e. the filters that generate feature maps by sliding across the previous layer may have different coefficients at different spatial locations .",
    "this is often referred to in the deep learning community as a locally - connected operator  @xcite .",
    "similarly to the cp decomposition , retrieving the entries of a tensor from its ht decomposition can be computed by the circuit represented in fig .",
    "[ fig : ht_decoder ] , where instead of a single pair of conv and pooling layers there are @xmath42 such pairs , with pooling windows of size  2 .",
    "though the canonical ht decomposition dictates size  2 pooling windows , any pooling structure used in practice still results in a valid ht decomposition .",
    "arithmetic circuits constructed from the above conv and product pooling layers are called _ convolutional arithmetic circuits _ , or convacs for short , first suggested by  @xcite as a theoretical framework for studying standard convolutional networks , sharing many of the defining traits of the latter , most noteworthy , the locality , sharing and pooling properties of convnets . unlike general circuits , the structure of the network is determined solely by two parameters , the number of channels of each conv layer and the size of pooling windows , which indirectly controls the depth of the network .",
    "finally , since we are dealing with generative models , the tensors we study are non - negative and sum to one , i.e. the vectorization of @xmath14 ( rearranging its entries to the shape of a vector ) , denoted by @xmath43 , is constrained to lie in the multi - dimensional simplex , denoted by : @xmath44 : x_i \\geq 0\\right\\}\\end{aligned}\\ ] ]",
    "we represent the input signal @xmath2 by a sequence of low - dimensional _ local structures _ @xmath45 this representation is quite natural for many high - dimensional input domains such as images    where the local structures represent patches consisting of @xmath46 pixels    voice through spectrograms , and text through words .",
    "a well - known observation , which has been verified in several empirical studies  ( e.g. by @xcite ) , is that the distributions of local structures typically found in natural data could be sufficiently modeled by a mixture model consisting of only few components ( on the order of 100 ) of simple distributions ( e.g. gaussian ) . assuming the above holds for @xmath47 and let @xmath48 be the mixing components , parameterized by @xmath49 , from which local structures are generated , i.e. for all @xmath50 $ ] there exist @xmath51 $ ] such that @xmath52 , where @xmath53 is a hidden variable specifying the matching component for the @xmath10-th local structure , then the probability density of sampling @xmath2 is fully described by : @xmath54 where @xmath55 represents the prior probability of assigning components @xmath56 to their respective local structures @xmath57 . even though we had to make an assumption on @xmath2 to derive eq .",
    "[ eq : tmm ] , it is important to note that if we allow @xmath40 to become unbounded , then any distribution with support in @xmath58 could be approximated by this equation .",
    "the argument follows from the universality property of the common parametric families of distributions  ( gaussian , laplacian , etc . ) , where any distribution can be approximated given sufficient number of components from these families , and thus the assumption always holds to some degree  ( see app .",
    "[ app : universal ] for the complete proof ) .",
    "the prior probabilities @xmath55 can also be represented by a tensor @xmath37 of order @xmath8 and dimension @xmath40 in each mode , given that the vectorization of  @xmath14 is constrained to the simplex , i.e.  @xmath59  ( see eq .",
    "[ eq : simplex ] ) .",
    "thus , we refer to eq .",
    "[ eq : tmm ] as a _",
    "tensorial mixture model _  ( tmm ) with _ priors tensor _ @xmath14 and _ mixing components _ @xmath60 .",
    "notice that if @xmath61 then we obtain the standard mixture model , whereas for a general @xmath8 it is equivalent to a mixture model with tensorised mixing weights and conditionally independent mixing components .",
    "unlike standard mixture models , we can not perform inference directly from eq .",
    "[ eq : tmm ] , nor can we even store the priors tensor directly given its exponential size of @xmath62 entries .",
    "therefore the tmm as presented by eq .",
    "[ eq : tmm ] is _ not tractable_. the way to make the tmm tractable is to replace the tensor @xmath37 by a tensor decomposition and , as described in the previous section , this gives rise to arithmetic circuits .",
    "but before we present our approach for tractable tmms through tensor decompositions , it is worth examining some of the tmm special cases and how they relate to other known generative models .",
    "we have already shown that tmms can be thought of as a special case of mixture models , but it is important to also note that diagonal gaussian mixture models ( gmms ) , probably the most common type of mixture models , are a strict subset of tmms .",
    "assume @xmath63 , as well as : @xmath64 , \\,\\,\\,d_i { = } n { \\cdot } ( k{-}1 ) { + } i \\\\                0 & \\textrm{otherwise }         \\end{cases } \\\\",
    "p({{\\mathbf x}}|d;\\theta_d ) & = \\mathcal{n}({{\\mathbf x } } ; { { \\boldsymbol{\\mu}}}_{ki } , \\textrm{diag}({{\\boldsymbol{\\sigma}}}^2_{ki } ) ) , \\,\\,\\ , d { = } n { \\cdot } ( k{-}1 ) { + } i\\end{aligned}\\ ] ] then eq .",
    "[ eq : tmm ] reduces to : @xmath65 which is equivalent to a diagonal gmm with mixing weights @xmath66 and gaussian mixture components with means @xmath67 and covariances @xmath68 .",
    "while the previous example highlights another connection between tmms and mixture models , it does not take full advantage of the priors tensor , setting most of its entries to zero .",
    "perhaps the simplest assumption we could make about the priors tensor , without it becoming degenerate , would be to assume that that the hidden variables @xmath56 are statistically independent , i.e.  @xmath69 . then rearranging eq .  [ eq : tmm ] will result in a product of mixture models : @xmath70 if we also assume that the priors are identical in addition to being independent , i.e.  @xmath71 , then this model becomes a bag - of - words model , where the components @xmath48 define a soft dictionary for translating local - structures into `` words '' , as is often done when applying bag - of - words models to images . despite this familiar setting , had we subscribed to only using independent priors , we would lose the universality property of the general tmm model    it would not be capable of modeling dependencies between the local - structures .",
    "we have just seen that tmms could be made tractable through constraints on the priors tensor , but it was at the expense of either not taking advantage of its tensor structure , or losing its universality property .",
    "our approach for tractable tmms is to apply tensor decompositions to the priors tensor , which is the conventional method for tackling the exponential size of high - order tensors .",
    "we have already covered in sec .",
    "[ sec : pre ] two of the main tensor decomposition methods , cp decomposition and ht decomposition , both of which can represent any tensor and thus applying them would not limit the expressivity of our model .",
    "whereas other works have used tensor decompositions as part of the learning algorithms of probabilistic models  @xcite , we employ them strictly for modeling and instead make use of conventional learning algorithms  ( see sec .  [",
    "sec : training ] ) .",
    "next , we will see how these decompositions result in tractable inference mechanisms , and more importantly , that they could be efficiently realized by convacs .",
    "we begin by representing the priors tensor according to the cp  decomposition  eq .",
    "[ eq : cp_decomp ] , which results in the following equation we call _ generative cp - model _ , or gcp - model for short : @xmath72 unlike general tensors , for tmm to represent a valid distribution , the priors tensor is constrained to the simplex and thus not every choice of parameters for the decomposition would result in a tensor holding this constraint . by restricting ourselves to non - negative decomposition parameters , i.e. @xmath73 , it guarantees the resulting tensors would be non - negative as well . additionally , normalizing the non - negative tensor is equivalent to requiring the parameters to be restricted to the simplex , i.e. that for all @xmath74 $ ] and @xmath75 $ ] the vectors @xmath76 and @xmath77 are normalized to sum to one .",
    "notice that restricting ourselves to non - negative decompositions does not limit the expressivity of our model , as we can still represent any non - negative tensor and thus any distribution that the original tmm model could represent .",
    "non - negative matrix and tensor decompositions have a long history together with the development of corresponding generative models , e.g. , plsa  @xcite which uses non - negative matrix decompositions for text analysis , which was later extended for images with the help of `` visual words ''  @xcite .",
    "the non - negative variant of the cp decomposition presented above is related to the more general latent class models  @xcite , which could be seen as a multi - dimensional plsa .",
    "thus the gcp - model we introduced can be represented as a two - levels graphical model  ( see fig .",
    "[ fig : gcp_graphical_model ] ) , where the top level is a latent class model , which represent the priors tensor , and the bottom level represent the local structures which are conditionally sampled from the mixing components of the tmm .",
    "this could also be interpreted as a topic model , where the role of the latent variable @xmath78 serves as the topic @xmath2 belongs to , from which we sample the `` words '' @xmath56 , followed by sampling the local structures according to the soft dictionary defined by the mixing components .        in terms of realizing the gcp - model inference mechanism , eq .",
    "[ eq : gcp_model ] bears a strong resemblance to the formula describing the convac for decoding the cp decomposition  ( see eq .  [ eq : cp_decode ] ) , differing only in the way the input vectors are defined . namely , eq .  [ eq : gcp_model ] is a result of replacing the indicator vectors @xmath79 of eq .",
    "[ eq : cp_decode ] with probabilities , which could be interpreted as the soft version of the indicator vectors .",
    "specifically , the network begins with a _ representation layer _",
    ", mapping the local structures to the likelihood probabilities of belonging to each mixing component , i.e.  @xmath80 . following",
    "the representation layer is the same convac derived from the cp decomposition .",
    "the complete network is illustrated by fig .",
    "[ fig : gcp_model ] .",
    "as was previously discussed , sharing parameters under our model is equivalent to having conditional identical distributions for the latent variables @xmath81 , which would turn the gcp - model into a bag - of - words topic model , invariant to the locations of the local structures .        despite its tractable inference and universality property ,",
    "the gcp - model is only a shallow model , both from a graphical model perspective as well as its realization as a neural network .",
    "it is widely accepted that much of the success of recent years could be attributed to using deeper models than were used in the past .",
    "this leads us to applying the ht  decomposition  ( see eq .",
    "[ eq : ht_decomp ] ) to the priors tensor , which unlike the cp decomposition , decomposes tensors in a hierarchical fashion , which results in a deep model we call the _ generative ht - model _ , or ght - model for short .",
    "as was the case with the gcp - model , the ht decomposition is projected onto the simplex per weight vector @xmath82 .",
    "likewise , the non - negative ht  decomposition is related to the latent tree model  @xcite with the structure of a complete binary tree .",
    "thus , the ght - model can be represented as a deep tree graphical model  ( see fig .  [",
    "fig : ght_graphical_model ] ) , where the inner levels form a latent tree model , representing the priors tensor , and the bottom level represents the local structures .        realizing the inference of the ght - model , by inserting the recursive formula of the ht decomposition to eq .",
    "[ eq : tmm ] , results in the network architecture illustrated in fig .",
    "[ fig : ght_model ] . similarly to the gcp - model , following the representation layer is the respective convac derived from the ht decomposition described in sec .",
    "[ sec : pre ] . while the canonical ht - decomposition dictates fixed size-2 pooling windows , both our framework and our theory support pooling windows of any size . in practice",
    "we have found that @xmath83 windows works best for image data as detailed in our experiments section .",
    "the expressiveness of convacs has been extensively studied , and specifically the non - generative variants of our models , named cp - model and ht - model respectively . in  @xcite it was shown that convacs posses the property known as _ complete depth efficiency_. namely , almost all functions realized by an ht - model of polynomial size , for them to be realized ( or approximated ) by a cp - model , require it to be of exponential size . in other words ,",
    "the expressiveness borne out of depth is exponentially stronger than a shallow network , almost always .",
    "it is worth noting that in the followup paper  @xcite , the authors have shown that the same result does not hold for standard convnets    while there are specific instances where depth efficiency holds , it is not complete , i.e. there is a non - zero probability that a function realized by a polynomially sized deep convnet can also be realized by a polynomially sized shallow convnet . despite the additional simplex constraints put on the parameters",
    ", complete depth efficiency does hold for the generative convacs of our work , proof of which can be found in app .",
    "[ app : depth_efficiency ] , which shows the advantage of the deeper ght - model over the shallow gcp - model .",
    "additionally , this illustrates how the two factors controlling the architecture    number of channels and size of pooling windows    control the expressive capacity of the ght - model .",
    "while the above shows why the deeper ght - model is preferred over the shallow gcp - model , there is still the question of whether a polynomially sized ght - model is sufficient for describing the complexities of natural data .",
    "though a complete and definite answer is unknown as of yet , there are some strong theoretical evidence that it might .",
    "one aspect of being sufficient for modeling natural data is the ability of the model to describe the dependency structures typically found in the data . in  @xcite",
    ", the authors studied the _ separation rank _  ",
    "a measure of correlation , which for a given input partition , measures how far a function is from being separable    and found that a polynomially sized ht - model is capable of exponential separation rank for interleaved partitions , i.e. that it can model high correlations in local areas in the input .",
    "additionally , for non - contiguous partitions , the separation rank can be at most polynomial , i.e. it can only model a limited correlation between far away areas in the input .",
    "these two results combined suggest that the ht - model , and thus also our ght - model , is especially fit for modeling the type of correlations typically found in natural images and audio , even if it is only of polynomial size .",
    "finally , from an empirical perspective , convolutional hierarchical structures have shown great success on multitude of different domains and tasks .",
    "our models leverage these structures , taking them to a probabilistic setting , which leads us to believe that they will be able to effectively model distributions in practice    a belief",
    "we verify by experiments .",
    "we have shown that tmms combined with tensor decompositions result in generative models realized by convacs .",
    "sum - product networks  ( spns ) are a related class of generative models which are also realized by arithmetic circuits , though not strictly convolutional circuits as defined above .",
    "while spns can realize any convac and thus are universal and posses tractable inference , their lack of structure puts them at a disadvantage .    picking the right spn structure from the infinite possible combinations of sum and product nodes",
    "could be perplexing even for experts in the field .",
    "indeed @xcite had to hand - engineer complex structures for each dataset guided by prior knowledge and heuristics , and while their results were impressive for their time , they are poor by current measures .",
    "this lead to many works studying the task of learning the structure directly from the data itself  @xcite , which indeed improved upon manually designed spns on some tasks . nevertheless ,",
    "when compared in absolute terms compared to other models , and not just average log - likelihood , they do not perform well even on simple handwritten digit classification datasets  @xcite .    as opposed to spns",
    ", tmms implemented with convacs have an easily designed architecture with only two set of parameters , size of pooling windows and number of channels , both of which can be directly related to the expressivity of the model as detailed in sec .",
    "[ subsec : expressive ] .",
    "additionally , while spns are typically trained using special em - type algorithms , tmms are trained using the stochastic gradient descent type algorithms as is common in training neural networks ( see sec .",
    "[ sec : training ] for details ) , thereby benefiting from the shared experience of a large and growing community .    to conclude , while spns have tractable inference , they do not have _ tractable architectures _",
    "( hard to design ) , and lack a theoretical understanding of their expressiveness .",
    "tmms can thus be seen as a specialization of spns that reduces complexity at no significant expense , i.e. not sacrificing universality , while gaining tractability of architecture design and training algorithm .",
    "until this point we presented the tmm as a generative model for high - dimensional data , which is universal , and whose structure is tightly coupled to that of convolutional networks .",
    "we have yet to incorporate classification and learning into our framework .",
    "this is the purpose of the current section .",
    "the common way to introduce object classes into a generative framework is to consider a class variable  @xmath84 , and the distributions  @xmath85 of the instance  @xmath2 conditioned on  @xmath84 . if the number of classes ( range of values  @xmath84 can take )",
    "is small , a reasonable approach would be to maintain a separate generative model to capture the distribution of instances under each possible class .",
    "this however becomes expensive as the number of classes grows .",
    "instead , we extend tmms to simultaneously model multiple distributions by harnessing the concept of _ joint tensor decompositions _ , which simply refers to representing multiple tensors through the same decomposition , where the top level weights ( e.g.  @xmath86 in eq .",
    "[ eq : ht_decomp ] ) are duplicated , and differ from tensor to tensor .",
    "the distributions @xmath87 , with  @xmath88 ranging over the possible classes , are all modeled by eq .",
    "[ eq : tmm ] , where the set of mixing components @xmath48 is shared , and the prior tensors @xmath89 are jointly decomposed .",
    "this results in a single convac computing inference , where instead of a single scalar output , multiple outputs are driven by the network    one for each class .",
    "an example for the case of ht decomposition is illustrated in fig .",
    "[ fig : ght_classifier ] .        heading on",
    "to predicting the class of a given instance , we note that in practice , nave implementation of convacs is not numerically stable , the reason being that high degree polynomials ( as computed by such networks ) are easily susceptible to numerical underflow or overflow .",
    "the conventional method for tackling this issue is to perform all computations in log - space , i.e.  instead of computing values directly , their @xmath90 is computed .",
    "this transforms convacs into _ simnets _ , a recently introduced deep learning architecture showing promising results in practice @xcite . for a particular tmm modeling the distribution of instances conditioned on multiple classes , maximum a - posteriori ( _ map _ ) estimation of the class  @xmath84 for a given instance  @xmath2 ,",
    "is computed by the outputs of the corresponding simnet ( convac in log - space ) as follows : @xmath91 in the common setting where class priors are assumed to be equal ( @xmath92 ) , this reduces to simply predicting the class for which the corresponding network output is maximal , in accordance with standard neural network practice .",
    "suppose now that we are given a training set @xmath93 of instances and labels , and would like to accordingly fit the parameters @xmath94 of multi - class tmm . under the assumption that the examples are i.i.d .",
    ", maximum likelihood estimation yields the following objective : @xmath95 denoting by @xmath96:=\\{1,\\ldots , k\\}$ ] the possible classes ( range of values @xmath84 can take ) , this translates to : @xmath97 for @xmath98 $ ] , let @xmath99 stand for the @xmath88th output of the simnet ( convac in log - space ) realizing the tmm with parameters  @xmath94 . in the case of uniform class priors ( @xmath100 )",
    ", the objective may be written as : @xmath101 where the minus signs originate from the fact that the objective is cast with minimization instead of maximization , and the newly introduced multiplicative constant @xmath102 is positive and thus has no effect .",
    "[ eq : objective ] reveals that the maximum likelihood objective for learning a tmm may be viewed as an extension of standard objectives for training neural networks . in particular",
    ", the left term of the objective , which corresponds to maximizing the conditional likelihood @xmath85 , is no other than the standard softmax loss applied to the outputs of the realizing network .",
    "we refer to it as the _ discriminative loss_. the second term corresponds to maximizing the prior likelihood @xmath103 , and has no analogy in standard discriminative neural networks .",
    "it is this term that captures the generative nature of our model , and we accordingly refer to it as the _ generative loss_.    maximum likelihood training of generative models is oftentimes based on dedicated algorithms such as expectation - maximization , which are typically difficult to apply at scale .",
    "we leverage the resemblance between our objective ( eq .  [ eq : objective ] ) and that of standard neural networks , and apply the same optimization procedures used for the latter , which have proven to be extremely effective for training classifiers at scale .",
    "in particular , our implementation of tmms is based on the simnets extension of caffe toolbox @xcite , and uses standard stochastic gradient descent - type methods for optimization ( see sec .",
    "[ sec : exp ] for additional details ) .    to conclude this section ,",
    "we have seen that not only are tmms structurally similar to convolutional networks , but also that their use for classification and the procedures for training them to do so , are almost identical to the respective practices with convnets .",
    "the main difference between the two models is the generative term in the training objective of tmms ( eq .  [ eq : objective ] ) , which has no analogy for standard convnets .",
    "it is this term that facilitates the generative power of tmms , which we will make use of in the next section for classification with missing data .",
    "a major advantage of generative models over discriminative ones lies in the ability to cope with missing data , specifically in the context of classification . by and large , discriminative methods either attempt to complete missing parts of the data before classification , known as _ data imputation _ , or learn directly to classify data with missing values  @xcite .",
    "the first of these approaches relies on the quality of data completion , a much more difficult task than the original one of classification with missing data .",
    "even if it was possible to complete the data optimally , the resulting classification accuracy of the completed data could be half the accuracy of the theoretical optimum ( see app .",
    "[ app : mbayes_proof ] ) .",
    "the second approach does not make this assumption , but nonetheless assumes that the distribution of missing values at train and test times are similar , a condition which often does not hold in practice . indeed , @xcite coined the term `` nightmare at test time '' to refer to the common situation where a classifier must cope with missing data whose distribution is different from that encountered in training .",
    "as opposed to discriminative methods , generative models are endowed with a natural mechanism for classification with missing data .",
    "namely , a generative model can simply marginalize over missing values , effectively classifying under all possible completions , weighing each completion according to its probability . in order for this to be possible ,",
    "the generative model must support tractable inference and marginalization . whereas most of the recent generative methods proposed in the literature do not meet this criteria  ( see sec .",
    "[ sec : intro ] for overview ) , we have shown in sec .  [",
    "sec : model ] that tmms have tractable inference , and moreover , will show in sec .",
    "[ sec : missing_data : margin ] that they bring forth marginalization which is just as efficient .",
    "beforehand , we lay out the formulation of classification with missing data .",
    "let @xmath104 be a random vector in  @xmath105 representing an object , and @xmath106 be a random variable in @xmath96:=\\{1,\\ldots , k\\}$ ] representing its label .",
    "denote by  @xmath107 the joint distribution of  @xmath108 , and by @xmath109)$ ] specific realizations thereof .",
    "assume that after sampling a specific instance @xmath110 , a random binary vector @xmath111 is drawn conditioned on @xmath112 .",
    "more concretely , we sample a binary mask @xmath113 ( realization of  @xmath111 ) according to a distribution @xmath114 .",
    "@xmath115 is considered missing if  @xmath116 is equal to zero , and observed otherwise .",
    "formally , we consider the vector @xmath117 , whose  @xmath10th coordinate is defined to hold  @xmath115 if @xmath118 , and the wildcard  @xmath119 if @xmath120 .",
    "the classification task is then to predict  @xmath88 given access solely to @xmath117 .    following the works of  @xcite , we consider three cases for the missingness distribution @xmath121 : missing completely at random  ( _ mcar _ ) , where  @xmath111 is independent of  @xmath104 , i.e.  @xmath121 is a function of  @xmath122 but not of  @xmath123 ; missing at random  ( _ mar _ ) , where  @xmath111 is independent of the missing values in  @xmath104 , i.e.  @xmath121 is a function of both  @xmath122 and @xmath123 , but is not affected by changes in  @xmath115 if  @xmath120 ; and missing not at random  ( _ mnar _ ) , covering the rest of the distributions for which  @xmath111 depends on missing values in  @xmath104 , i.e.  @xmath121 is a function of both  @xmath122 and @xmath123 , which at least sometimes is sensitive to changes in  @xmath115 when  @xmath120 .",
    "let @xmath124 be the joint distribution of the object  @xmath104 , label  @xmath106 , and missingness mask  @xmath111 : @xmath125{\\text{$=$}}}}{{\\mathbf x}},{{\\mathcal y}}{{\\scalebox{0.8}[1]{\\text{$=$}}}}y,{{\\mathcal m}}{{\\scalebox{0.8}[1]{\\text{$=$}}}}{{\\mathbf m } } ) = { { \\mathcal d}}\\left({{\\mathcal x}}{{\\scalebox{0.8}[1]{\\text{$=$}}}}{{\\mathbf x } } , { { \\mathcal y}}{{\\scalebox{0.8}[1]{\\text{$=$}}}}y\\right ) \\cdot { { \\mathcal q}}({{\\mathcal m}}{{\\scalebox{0.8}[1]{\\text{$=$}}}}{{\\mathbf m}}|{{\\mathcal x}}{{\\scalebox{0.8}[1]{\\text{$=$}}}}{{\\mathbf x}})\\ ] ] for given @xmath126 and @xmath113 , denote by @xmath127 the event where the random vector  @xmath104 coincides with  @xmath123 on the coordinates  @xmath10 for which @xmath118 .",
    "for example , if  @xmath122 is an all - zero vector @xmath127 covers the entire probability space , and if  @xmath122 is an all - one vector @xmath127 corresponds to the event @xmath112 . with these notations in hand , we are now in a position to characterize the optimal predictor in the presence of missing data :    [ claim : optimal_rule ] for any data distribution  @xmath128 and missingness distribution  @xmath129 , the optimal classification rule in terms of 0 - 1 loss is given by : @xmath130{\\text{$=$}}}}y|o({{\\mathbf x}},{{\\mathbf m}})){{\\mathcal p}}({{\\mathcal m}}{{\\scalebox{0.8}[1]{\\text{$=$}}}}{{\\mathbf m}}|o({{\\mathbf x}},{{\\mathbf m}}),{{\\mathcal y}}{{\\scalebox{0.8}[1]{\\text{$=$}}}}y)\\ ] ]    see app .",
    "[ app : mbayes_proof ] .    when the distribution  @xmath129 is mar ( or mcar )",
    ", the classifier admits a simpler form , referred to as the _ marginalized bayes predictor _ :    [ corollary : mar ] under the conditions of claim  [ claim : optimal_rule ] , if the distribution @xmath129 is mar ( or mcar ) , the optimal classification rule may be written as : @xmath131    see app .",
    "[ app : mbayes_proof ] .",
    "corollary  [ corollary : mar ] indicates that in the mar setting , which is frequently encountered in practice , optimal classification does _ not _ require prior knowledge regarding the missingness distribution  @xmath129 . as long as one is able to realize the marginalized bayes predictor ( eq .",
    "[ eq : mbayes ] ) , or equivalently , to compute the likelihoods of observed values conditioned on labels ( @xmath132 ) , classification with missing data is guaranteed to be optimal , regardless of the corruption process taking place .",
    "this is in stark contrast to discriminative methods , which require access to the missingness distribution during training , and thus are not able to cope with unknown conditions at test time .",
    "most of this section dealt with the task of prediction given an input with missing data , where we assumed we had access to a complete and uncorrupted training set , and only faced missingness during prediction .",
    "many times we wish to tackle the reverse problem , where the training set itself is riddled with missing data , and we wish to learn a classifier that is not biased toward the missingness distribution seen during training .",
    "as mentioned in the beginning of this section , when conventional discriminative models are trained on a corrupted training set , they become biased towards that specific corruption process . the most basic method for learning from missing data",
    "is simply to remove corrupted examples from the training set , known as list - wise deletion , however , this means throwing away a large amount of possibly important data , diminishing the generalization of the classifier .",
    "alternatively , data imputation methods could be employed to complete the data before training a standard classifier on it . though simply completing the data does not result in an unbiased classifier , if multiple imputations  @xcite are used    where several possible completions are estimated for each example    then under the mar assumption it results in an unbiased classifier .",
    "though many more methods have been proposed over the years , it is also worth mentioning the recent work of @xcite which does not rely on the mar assumption .",
    "unlike discriminative methods , generative methods can once again leverage their natural ability to handle missing data in the form of marginalization , even during the learning stage . as detailed in sec .",
    "[ sec : training ] , generative models with tractable inference could be learned through the maximum likelihood principle .",
    "when it comes to learning from missing data , the marginalized likelihood objective is used , sometimes known as the _ full information maximum likelihood _ , which could be seen as extending the multiple imputations method to infinity . like multiple imputations , under the mar assumption , this method too results in an unbiased classifier  @xcite , though unlike it , it does not result in the increased computational overhead of completing the data and the artificially enlarged training set .",
    "similarly to the case of prediction , this marginalized maximum likelihood method requires the model to have tractable marginalization .      as discussed above , with generative models optimal classification with missing data ( in the mar setting )",
    "is oblivious to the specific missingness distribution .",
    "however , it requires tractable computation of the likelihood of observed values conditioned on labels , i.e.  tractable marginalization over missing values . the plurality of generative models that have recently gained attention in the deep learning community  @xcite do not meet this requirement , and thus are not suitable for classification with missing data .",
    "tmms on the other hand bring forth extremely efficient marginalization , requiring only a single forward pass through the corresponding network .",
    "details follow .    recall from sec .",
    "[ sec : model ] and  [ sec : training ] that a multi - class tmm realizes the following form for distributions of instances @xmath133 conditioned on labels @xmath134 $ ] : @xmath135{\\text{$=$}}}}y}(d_1,\\ldots , d_n ) \\prod\\nolimits_{i=1}^{n } p({{\\mathbf x}}_i | d_i ; \\theta_{d_i } ) \\label{eq : mc_tmm}\\ ] ] where @xmath136{\\text{$=$}}}}y}(d_1,\\ldots , d_n)\\}_{y\\in[k]}$ ]",
    "are jointly decomposed prior tensors , and @xmath137}$ ] are mixing components shared across all local structures @xmath138 and classes @xmath88 .",
    "computation of these likelihoods ( inference ) is carried out by a convac applied to representation of local structures @xmath138 through marginal likelihoods @xmath139    see illustration in fig .",
    "[ fig : ght_model ] .",
    "suppose now that only the local structures @xmath140 are observed , and we would like to marginalize over the rest .",
    "integrating eq .",
    "[ eq : mc_tmm ] gives : @xmath141{\\text{$=$}}}}y}(d_1,\\ldots , d_n ) \\prod\\nolimits_{v=1}^{v } p({{\\mathbf x}}_{i_v } | d_{i_v } ; \\theta_{d_{i_v}})\\ ] ] from which it is evident that the same convac used to compute @xmath142 by decomposing the prior tensors @xmath143{\\text{$=$}}}}y}(d_1,\\ldots , d_n)$ ] , can be used to compute @xmath144  ",
    "all it requires is a slight adaptation of the representation layer .",
    "namely , the latter would represent observed values through the usual likelihoods , whereas missing ( marginalized ) values would now be represented via constant ones : @xmath145    to conclude , with tmms marginalizing over missing values is just as efficient as plain inference    requires only a single pass through the corresponding convac . accordingly , the marginalized bayes predictor ( eq .  [ eq : mbayes ] ) is realized efficiently , and classification with missing data ( in the mar setting ) is optimal , regardless of the missingness distribution . in other words , if a tmm is trained to accurately model the distribution of instances  @xmath2 conditioned on labels  @xmath84 , it can optimally classify under any distribution of missing values at test time , at a computational price that does not exceed standard classification .",
    "this capability is not provided by discriminative methods , which rely on the distribution of missing values being know at training , and by contemporary generative models , which do not bring forth tractable marginalization . in the next section",
    "we demonstrate this advantage empirically , by comparing tmms to alternative methods in classification of images with missing values .",
    "we demonstrate the properties of our models through both qualitative and quantitative experiments . in sec .",
    "[ subsec : exp : missing ] we present our state - of - the - art results on image classification with missing data , with robustness to various missingness distributions . in sec .",
    "[ subsec : exp : vis ] we show visualizations produced by our models , which gives us insight into its inner workings . our experiments were conducted on the mnist digit classification dataset , consisting of 60000 grayscale images of single digit numbers , as well as the small norb 3d object recognition dataset , consisting of 48600 grayscale stereo images of toys belonging to 5 categories : four - legged animals , human figures , airplanes , trucks , and cars    in all our experiments we use either the gcp or ght model with gaussian mixing components .",
    "the weights of the conv layers are partially shared as described in sec  [ subsec : decomp ] , and are represented in log - space . for the case of the ght model , we use @xmath83 pooling windows for all pooling layers .",
    "we train our model according to the loss described in sec .",
    "[ sec : training ] , using the adam  @xcite variant of sgd and decaying learning rates .",
    "we apply @xmath146-regularization to the weights while taking into account they are stored in log - space .",
    "additionally , we also adapt a probabilistic interpretation of dropout  @xcite by introducing random marginalization layers , that randomly select spatial locations in the input and marginalize over them .",
    "we provide a complete and detailed description of our experiments in app .",
    "[ app : exp_details ] .",
    "following the graphical model perspective of our models allows us to not only generate random instances from the distribution , but to also generate the most likely patches for each neuron in the network , effectively explaining its role in the classification process .",
    "we remind the reader that every neuron in the network corresponds to a possible assignment of a latent variable in the graphical model . by looking for the most likely assignments for each of its child nodes in the graphical tree model",
    ", we can generate a patch that describes that neuron . unlike similar suggested methods to visualize neural networks  @xcite ,",
    "often relying on brute - force search or on solving some optimization problem to find the most likely image , our method emerges naturally from the probabilistic interpretation of our model .    in fig .",
    "[ fig : fulldigits ] , we can see conditional samples generates for each digit , while in fig .",
    "[ fig : visnet ] we can see a visualization of the top - level layers of network , where each small patch matches a different neuron in the network . the common wisdom of how convnets work is by assuming that simple low - level features are composed together to create more and more complex features , where each subsequent layer denotes features of higher abstraction    the visualization of our network clearly demonstate this hypothesis to be true for our case , showing small strokes iteratively being composed into complete digits .",
    "we demonstrate the effectiveness of our method for classification with missing data of unknown missingness distribution  ( see sec .",
    "[ sec : missing_data ] ) , by conducting three kinds of experiments on the mnist dataset , and an additional experiment on the norb dataset .",
    "we begin by following the protocol of @xcite  the binary classification problem of digit pairs with feature deletion noise  where we compare our method to the best known result on that benchmark  @xcite . for our main experiment , we move to the harder multi - class digit classification under two different mar missingness distributions , comparing against other methods which do not assume a specific missingness distribution .",
    "we repeat this experiment on the norb dataset as well .",
    "finally , our last experiment demonstrates the failure of purely discriminative methods to adapt to previously unseen missingness distributions , underlining the importance of the generative approach to missing data . we do wish to emphasize that missing data is not typically found in most image data , nevertheless , experiments on images with missing data are very common , for both classification and inpainting tasks . additionally , there is nothing about our method , nor the methods we compare it against , that is very specific to the image domain , and thus any conclusion drawn should not be limited to the chosen datasets , but be taken in the broader context of the missing data problem .",
    "the problem of learning classifiers which are robust to unforeseen missingness distributions at test time was first proposed by @xcite .",
    "they suggested missing values could be denoted by values which were _ deleted _ , i.e. their values were changed to zero , and a robust classifier would have to assume that any of its zero - value inputs could be the result of such a deletion process , and must be treated as missing .",
    "their solution was to train a linear classifier and formulate the optimization as a quadric program under the constraint that @xmath8 of its features could be deleted . in  @xcite , this solution was improved upon and generalized to other kinds of corruption beyond deletion as well as to an adversarial setting ."
  ],
  "abstract_text": [
    "<S> we introduce a generative model , we call tensorial mixture models ( tmms ) based on mixtures of basic component distributions over local structures ( e.g. patches in an image ) where the dependencies between the local - structures are represented by a `` priors tensor '' holding the prior probabilities of assigning a component distribution to each local - structure </S>",
    "<S> .    in their general form , tmms are intractable as the prior tensor is typically of exponential size . however , when the priors tensor is decomposed it gives rise to an arithmetic circuit which in turn transforms the tmm into a convolutional arithmetic circuit ( convac ) . </S>",
    "<S> a convac corresponds to a shallow ( single hidden layer ) network when the priors tensor is decomposed by a cp ( sum of rank-1 ) approach and corresponds to a deep network when the decomposition follows the hierarchical tucker ( ht ) model .    </S>",
    "<S> the convac representation of a tmm possesses several attractive properties . </S>",
    "<S> first , the inference is tractable and is implemented by a forward pass through a deep network . </S>",
    "<S> second , the architectural design of the model follows the deep networks community design , i.e. , the structure of tmms is determined by just two easily understood factors : size of pooling windows and number of channels . </S>",
    "<S> finally , we demonstrate the effectiveness of our model when tackling the problem of classification with missing data , leveraging tmms unique ability of tractable marginalization which leads to optimal classifiers regardless of the missingness distribution . </S>"
  ]
}