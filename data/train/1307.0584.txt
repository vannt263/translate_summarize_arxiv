{
  "article_text": [
    "we have two principal goals in this work  ( 1 ) introduce an alternative formulation to a maximum likelihood procedure for parameter estimation , which greatly improves estimates when covariance information of model and data is lacking ; ( 2 ) produce estimates of two important parameters critical to making model outcomes of longshore currents compatible with existing data",
    ". we will demonstrate the practicality and usefulness of our procedure by applying it to tune parameters in a model for nearshore dynamics using field data .",
    "we are interested in parameter estimation for geophysical models which involve potentially complex models and a large number of observations .",
    "a bayesian framework is natural for such geophysical problems : it is unrealistic to expect to completely determine ocean states via data alone . commonly used _",
    "data assimilation _",
    "methods for dynamic geophysical models are based upon formulating a variance minimizer of the posterior distribution of model state , given observations ( see @xcite for a review ) .",
    "the parameters are then inferred from the posterior distribution of model states .    for linear / gaussian methods based upon least squares ,",
    "it is prudent that we not declare the parameters as state estimation variables , since the ensuing state estimation problem is typically highly nonlinear and non - gaussian .",
    "there are assimilation methods capable of dealing with nonlinearities , but these tend to be dimensionally - challenged : only capable of handling problems with a small number of effective dynamic state variables ( _ c.f .",
    "_ , @xcite , @xcite , @xcite , to name a few ) . in the data assimilation methods mentioned above the posterior",
    "is written in terms of a likelihood that is informed by observations , and a prior which is instead informed by model outcomes .",
    "fundamentally this presumes that models and data are error - laden and most critically , that these errors are very well known or estimated .",
    "the parameter estimation method we use in this work is the same as the methods discussed above _ if _ we presume that the model is free of any errors .",
    "alternatively , it is estimation based upon maximum likelihood ( see @xcite ) . in its most basic form ,",
    "one writes down a likelihood based upon knowledge of the statistics of the errors between models and data .",
    "one then uses sampling methods to find the most likely parameters . in this work we are only working with 2 parameters",
    "so it is possible to circumvent the use of monte carlo and instead generate a table of the likelihood function ( plots of which will be shown in this paper ) in sample space .",
    "once the parameter space is mapped out , it is then straightforward to pick approximate maximum likelihood parameter combinations that lead to the best compatibility possible between model outcomes and observations . whether one uses monte carlo or",
    "not , the computation of sample space is exceedingly expensive when geophysical models based on partial differential equations and/or many equations are involved . with the aim of improving the efficiency of producing large number of model outcomes",
    "engineering researchers have recently proposed using random - coefficient polynomials expansions of sample model outcomes ( _ cf .",
    "_ , @xcite )",
    ". we will adopt such a strategy here .",
    "the model proxy will be based upon a polynomial chaos expansion of the sample space ( _ cf .",
    "_ , @xcite ) .",
    "demonstrations of the use of this expansion for the purpose of improving the efficiency of a monte carlo maximum likelihood parameter estimation in geophysical models are found in the works of @xcite and @xcite , and references contained therein .",
    "the maximum likelihood method is applied to the estimation of 2 parameters critical to nearshore longshore currents .",
    "we use a _ vortex force formulation _ for the evolution of waves and currents in the nearshore ( see @xcite , and @xcite ) which can capture these currents .",
    "the model was used in @xcite to describe the evolution of rip currents . using",
    "this model @xcite found that longshore current outcomes were most sensitively dependent on the bottom drag force ( and its parametrization ) , and the amplitude of the incoming waves ( which are boundary conditions in the model ) .",
    "since the drag force and the incoming wave forcing are such a critical part of a nearshore calculation using this wave / current model or some other model , it is essential to develop strategies to tune the parameter appropriately , particularly if the model is being used to explore phenomena that are less familiar than the rip currents or longshore currents .",
    "@xcite also found that while different bottom drag parametrizations resulted in different longshore outcomes , it was often the case that one could replicate qualitative and quantitative characteristics of the longshore currents with different models if the coefficients in the parametrization were chosen appropriately .",
    "this suggests , in the setting considered , the type of parameterization is perhaps less important than precise tuning of the parameters for the model to accurately reproduce the physical outcome .",
    "we are thus motivated to test the capabilities of the simplest of drag parametrizations , the linear drag force model , but with an accurate tuning of the parameters through comparison with field data .",
    "the data we use was collected in the field campaigns conducted in duck , north carolina by herbers , elgar , and guza in 1994 ( see @xcite ) .",
    "the data sheets and detailed information is readily available on the web + frf.usace.army.mil/pub/experiments/duck94/spuv .",
    "we use the data collected in the bar region of the sea bed topography , since this is the region where we observe the wave - induced strong longshore current which the vortex force model aims to capture .",
    "the paper is organized as follows .",
    "a summary of the model appears in section [ model ] .",
    "our parameter estimation method using an alternative maximum likelihood formulation is described in section  [ method ] .",
    "we derive a sensitivity analysis estimate that is useful in the interpretation of model and observations .",
    "this analysis is presented in section [ analysis ] .",
    "the outcomes of the test and physical interpretation of the results appear in section [ outcomes ] .",
    "the depth - averaged wave - current interaction model in @xcite is specialized to the nearshore environment .",
    "see figure [ fig : domain ] .",
    "the transverse coordinates of the domain will be denoted by @xmath0 .",
    "the cross - shore coordinate is @xmath1 and increases away from the beach .",
    "time is denoted by @xmath2 .",
    "differential operators depend only on @xmath3 and @xmath4 .",
    "the total water column depth is given by @xmath5 , where @xmath6 is the bottom topography and @xmath7 , is the composite sea elevation ; @xmath8 is the quasi - steady sea elevation adjustment , @xmath9 , where @xmath10 is the wave amplitude and @xmath11 is the magnitude of the peak wavenumber . in the absence of wind forcing and for spatio - temporal scales much larger than those typical of the waves ,",
    "the momentum equation reads @xmath12 where @xmath13 is the depth - averaged transverse velocity , @xmath14 is the gravitational acceleration , @xmath15 is the vortex force due to waves ( see @xcite ) , @xmath16 is the wave breaking acceleration , and @xmath17 is the momentum loss due to the bottom drag .",
    "@xmath18 is the wind stress , which will be taken to be zero in what follows , as is @xmath19 , which represents important dissipative effects from the subscale turbulent flow .",
    "the vortex force term is defined as @xmath20 where @xmath21 is the vorticity .",
    "we use a linear bottom drag formulation : @xmath22 then , the term on the right hand side of ( [ eq : momentum ] ) is @xmath23 where @xmath24 is the fluid density .",
    "the contribution to the flow due to the breaking waves has the form @xmath25 the function @xmath26 is arrived at by hydraulic jump theory . taken from @xcite ) , it reads @xmath27 with @xmath28 , @xmath29 .",
    "the peak wavenumber of the gravity wave field @xmath30 and @xmath31 the wave frequency obey the dispersion relation @xmath32    the evolution of the water column height is given by the continuity equation @xmath33 & = 0,\\label{eq : continuity}\\end{aligned}\\ ] ] where @xmath34 is the stokes drift velocity .",
    "the ray equation for the wave action @xmath35 is given by @xmath36 where @xmath37 is the group velocity , given by the formula @xmath38 the conservation law for the wavenumber reads @xmath39      in the computations to be discussed subsequently we assumed that all of the fields were periodic in @xmath40 .",
    "at the near - shore coordinate @xmath41 , where @xmath42 in the particular configuration used in the numerical simulations , we imposed the condition @xmath43 on the cross - shore component of the current velocity . in numerical computations , @xmath44 was relaxed towards this boundary conditions over a layer .",
    "this layer , hugging the near - shore , @xmath41 side , was selected so as to not affect the statistical comparisons and results in the region of interest .",
    "we imposed the homogeneous neumann boundary condition on @xmath45 at @xmath41 , and we nudged @xmath45 toward zero on the offshore boundary , at @xmath46 , where @xmath47 in the numerical simulations .",
    "both @xmath48 and @xmath49 were prescribed at @xmath46 : @xmath48 is prescribed so as to satisfy the offshore wave amplitude parameter value .",
    "the wave number was chosen to make an angle in the range @xmath50 to @xmath51 degrees , with respect to the shore - normal vector , its magnitude was set once the frequency was set .",
    "near the shore , we imposed perfectly matched layer boundary conditions on @xmath48 , whereas homogeneous neumann boundary condition were imposed on @xmath49 .",
    "we used data from the experiments conducted by herbers , guza , and elgar , conducted in 1994 , in duck , north carolina .",
    "we will refer to the field data as the _ duck data .",
    "_ the duck data also provides information on the mean velocity , pressure , temperature , and depth .",
    "in addition , there is information on the peak frequency , and bottom topography .",
    "we will be making use of sea elevation data as well as depth - averaged velocity data .",
    "the computational bottom topography , shown in figure [ fig : topography ] , was generated by joining their bathymetric information using a cubic spline in the cross - shore direction .",
    "no @xmath40 variation was assumed for the bottom topography .",
    "the model was approximated using second order finite - differences in space , and heun quadrature in time .",
    "the time steps were around @xmath52 s. the computational grid was uniform .",
    "the spatial grid width in @xmath40-direction was about @xmath53 m ( @xmath54 grid points are used ) , and in @xmath1-direction the grid width was about @xmath55 m in length and 257 grid points were used .",
    "the computational domain covered the cross - shore coordinates , @xmath1 , between @xmath56 m and @xmath57 m , and was @xmath58 m wide in the @xmath40-direction .",
    "the effective domain , which was largely free of effects of the relaxation and the matching layer used in the numerical computations , encompassed shore distances from @xmath59 m to @xmath60 m.    among the things we know about the model , as applied to the longshore current problem ( see @xcite ) , is that for high values of the drag parameter , the bottom drag force is effectively in balance with the breaking force , and the inertial effects are largely ignorable .",
    "on the other hand , when the bottom drag parameter is small , the longshore current develops ( non - stationary ) instabilities .",
    "the goal is to find estimates of the offshore wave boundary data @xmath61 , where @xmath62 is the wave amplitude given by the relation @xmath63 ( we will refer to this quantity as the _ wave forcing _ ) , and the bottom _ drag coefficient _ @xmath64 that best agree with the data . in this study , @xmath65 is a boundary condition parameter and is time - independent . specifically , we will create tables of the likelihood estimators for the wave forcing and the bottom drag .",
    "the parameter combinations that lead to the best agreement between model and data are the most likely from the table .",
    "based upon our understanding of the physics of the problem the parameter range for the wave forcing and bottom drag were taken as @xmath66=[0.4,1.2 ] ,   \\quad d \\in [ d_0,d_1]=&[0.002,0.026 ] ,   \\label{eq.ranges } \\end{aligned}\\ ] ] respectively .",
    "a fundamental assumption in the estimate ( but not of the method itself ) , is that at the times @xmath67 , @xmath68 , when data is available , the statistical distribution of @xmath69 is normal . here , @xmath70 are field measurements , @xmath71 is the state vector .",
    "@xmath72 relates the measurements and the state vector . a most common situation in geophysical problems , such as the longshore problem ,",
    "is that @xmath73 , and @xmath74 is small compared with @xmath75 .",
    "an important assumption used here is that correlations in time are considerably shorter than @xmath76 .    in our specific nearshore example , at time @xmath67 , the dimension of @xmath77 is @xmath78 , times the number of space - grid locations ( discounting for periodicity and boundary data ) .",
    "the measurement vector @xmath79 , consists of the time dependent sea elevation and depth - averaged velocities at measurement locations .",
    "@xmath72 is a projection matrix in this problem .",
    "the argument of the likelihood shall measure the absolute weighted distance between data and model outcomes for the sea elevation and velocities at measurement times @xmath67 .",
    "we will denote the model output at time @xmath67 as @xmath80 .",
    "similarly , we will define the data vector as @xmath81 .",
    "( the observation and the model vectors in the argument have the same dimension , thanks to the implied projection matrix ) .",
    "assuming gaussianity in the likelihood , @xmath82^\\top r^{-1}_n [ { \\mathcal m}_{n}-\\mathcal{o}_{n } ] \\right ) { \\mathcal u } ( [ a_{0},a_{1}]){\\mathcal u}([d_{0},d_{1 } ] ) , \\label{eq.pad}\\ ] ] @xmath83 is a covariance matrix , and @xmath84 $ ] denotes the uniform distribution over the range @xmath85 $ ] .",
    "the first term in ( [ eq.pad ] ) is the likelihood @xmath86 .",
    "the likelihood is a product because we are assuming that the distributions of the measurement errors and the model outcomes are independent in time .    instead of inverting the data set to eke out the implicitly - defined model parameters @xmath65 and @xmath64",
    "is circumvented by making the following assumption : the mode of the posterior probability density for @xmath65 and @xmath64 given observations is obtained when the argument of the likelihood distribution is minimized .",
    "compared to a standard proposal for data assimilation , ( [ eq.pad ] ) does not include a prior informed by explicit model error independent of errors due to measurements .",
    "this is not to say that the model is error - free , but rather , that all that can be discerned is discrepancies ( differences ) between model output and measurements . in terms of parameter estimation methodology ,",
    "the maximum likelihood approach described above does not differ in any significant way from the one proposed by @xcite . in the next section",
    "we will argue for an alternative likelihood function , thus distinguishing our work from theirs .",
    "the matrix @xmath87 provides a description of covariances among the different components of the state vector , the degree of confidence in each of these , as well as scale / non - dimensionalization information for each of the state components .",
    "it is essential information that must be known in order to use ( [ eq.pad ] ) for parameter estimation .",
    "if not supplied , one has to turn to whatever model and observations are available in order to estimate the covariance matrix .",
    "in fact , in geoscience applications it is often the case that the covariance is unknown or very poorly constrained .",
    "this is the case in the longshore problem .",
    "let @xmath80 represent the state vector at time @xmath67 , produced by the computer - generated model solution , for a given set of parameter values , @xmath88 .",
    "the state vector will consist of dynamic variables ( _ e.g. _ , the transverse velocity and sea elevation ) at spatial locations with offshore @xmath89 , @xmath90 , ... , @xmath91 .",
    "the individual state vector component , at time @xmath92 and location @xmath93 , @xmath94 , will be denoted by @xmath95 .",
    "we will omit the superscript @xmath96 , if it is clearly implied by the context .",
    "@xmath97 is @xmath40-averaged ( alongshore ) , and time averaged over times @xmath98 and @xmath67 . because of the averaging , we can assume that the model outputs give accurate estimates for the mean longshore velocity for the given parameters at the locations @xmath99 .",
    "the individual component of the observation vector , measured at location @xmath93 , @xmath100 , will be denoted by @xmath101 ( again , we will omit the superscript @xmath96 unless it is not implied by the context ) .",
    "the observations @xmath102 are _ not averaged in @xmath40 or time _ and hence have significant variability .    in order to remove the dependence on the incoming wave angle , we consider the magnitudes of the components @xmath103 instead of the signed quantities @xmath102 , with the reasonable expectation that @xmath103 is comparable to @xmath104 . without any other intrinsic scales in the problem , the likelihood , which is non - dimensional , has to be a function of @xmath105 , with a distribution that is peaked when @xmath106 .",
    "also , the observations are taken at locations that are sufficiently separated and on time scales which are sufficiently large that we can assume that they are uncorrelated .",
    "thus , a natural choice for the likelihood is @xmath107.\\ ] ] where @xmath108 is now a dimensionless measure of the variance of @xmath105 and @xmath109 is a normalization given by @xmath110 @xmath108 is potentially spatially inhomogeneous ( depends on @xmath96 ) and non - stationary ( depends on @xmath92 ) .",
    "absent any prior information on the covariances , it is a reasonable approximation to take @xmath111 , a constant . to make this more precise , we compute the jefferys prior for @xmath112 .",
    "the fisher information is given by @xmath113 ^ 2 l({\\mathcal o}_n|{\\mathcal m}_n , r^j_n ) d { \\mathcal o}_n,\\ ] ] and is independent of @xmath114 .",
    "this is analogous to the fact that the fisher information for the variance of a gaussian variable with a given mean is independent of the value of the mean .",
    "we can compute the jefferys prior @xmath115 , obtaining @xmath116^{1/2}.     \\label{eq.prior}\\ ] ] figure [ fig : priors ] shows a comparison between this prior and the scale invariant prior for the variance of a gaussian variable , @xmath117 .    ) and the scale invatiant prior for the variance of a gaussian variable . ]    as is evident from the figure , to a very good approximation , we have @xmath118 we now have the bayesian statement @xmath119){\\mathcal u}([d_{0},d_{1}]),\\ ] ] where @xmath120 is the projective metric , and @xmath121 is a normalization in with @xmath111 .",
    "this posterior distribution generalizes .",
    "note that @xmath122 , but @xmath123 has theoretically sound applications in a vectorial setup . in principle",
    "we can use this to estimate the various moments of the parameters @xmath65 and @xmath64 _ and also the variance parameter _",
    "@xmath124 from the model output and observations .    for the purposes of this paper , we are only interested in computing the maximal likelihood estimates for @xmath65 and @xmath64 and not any of the higher moments .",
    "this allows for a further approximation which leads to further efficiencies in the computation .",
    "the posterior distribution depends on @xmath124 , through ( 1 ) the prior distribution @xmath125 , ( 2 ) through the normalization factors @xmath126 , and ( 3 ) , through the denominator in the exponent .",
    "the logarithm of the posterior distribution depends only weakly on the first two factors .",
    "( so long as the size of @xmath124 be small , so that @xmath127 be large compared to @xmath128 ) . with this approximation of neglecting the @xmath124 dependence except in the argument of the exponential function ,",
    "the posterior distribution reduces to @xmath129){\\mathcal u}([d_{0},d_{1 } ] ) , \\label{eq.pad2}\\ ] ] where @xmath123 is the projective metric defined above and @xmath130 is a normalization constant . in our computations , the variance parameter @xmath124 in the likelihood function",
    "is set to @xmath78 . as is evident from ,",
    "changing @xmath124 will only change the width of the empirical likelihood , but not the value of maximum likelihood estimates .",
    "figure [ relvsabs ] illustrates the difference between using absolute errors and relative errors .",
    "the striking difference is that there is clearer discernment of likely parameter values ( pure black represents the most likely ) .",
    "the differences portrayed here are very drastic in this case , but this sharpening due to the use of the relative error is a generic outcome of the computations    ( a ) a daily comparison for october 7 - 8 . ( a )",
    "posterior using absolute errors .",
    "see ( [ eq.pad ] ) ; ( b ) likelihood based upon the relative errors .",
    "see ( [ eq.pad2 ] ) .",
    "the latter suggests that a much sharper range of parameters leads to agreement between model and data.,title=\"fig:\",width=230,height=201 ] ( b ) a daily comparison for october 7 - 8 .",
    "( a ) posterior using absolute errors .",
    "see ( [ eq.pad ] ) ; ( b ) likelihood based upon the relative errors . see ( [ eq.pad2 ] ) .",
    "the latter suggests that a much sharper range of parameters leads to agreement between model and data.,title=\"fig:\",width=230,height=201 ]      in a many - parameter estimation problem we would apply a monte carlo procedure to sample parameter space ( see @xcite ) .",
    "full model simulations make this aspect of the methodology computationally demanding .",
    "whether we have to use monte carlo or not the efficiency of this process can be considerably improved by using a parametric approximation of the model outcomes , via polynomial chaos expansions ( see @xcite ) .",
    "the polynomial chaos expansion of a stochastic function @xmath131 of one stochastic parameter @xmath132 is @xmath133 where @xmath134 is a system of orthogonal polynomials of degree @xmath11 , @xmath135 .",
    "orthogonality is with respect to the probability distribution of the parameter @xmath132 .",
    "if the distribution is uniform , for example , the @xmath136 are legendre polynomials . for two parameters ,",
    "the basis consists of polynomials @xmath137 , and the polynomial approximation is @xmath138 exploiting the orthogonality of the polynomials , we can easily find the coefficients @xmath139 : we multiply @xmath140 by @xmath141 and integrate over @xmath132 and @xmath142 with respect to the known joint measure @xmath132 and @xmath142 . in our case , this distribution is just a constant multiple of the lebesgue measure .    for the integration , we chose a simple romberg integration .",
    "however , there are more efficient or accurate quadrature schemes , _",
    "e.g. _ , gaussian quadrature or smolyak tensorisation ( see @xcite and references contained therein ) .",
    "we chose not to resort to monte carlo , since our parameter sample space is two - dimensional .",
    "instead , we opted to discretize sample space in the range given by ( [ eq.ranges ] ) with an equally spaced grid consisting of @xmath143 values .    in producing approximations to oceanic flows with numerical models",
    "there is a computational expense of @xmath144 , where @xmath145 is the state variable dimension , @xmath64 is the number of spatial dimensions ( typically 2 , lately 3 ) , and @xmath146 is the number of time steps in the computation .",
    "@xmath145 is upwards of @xmath147 , typically .",
    "most codes are explicit in time , and thus @xmath148 due to stability constraints , where @xmath149 is a constant .    in the numerical computations that will be described",
    "subsequently we mention that after the initialization , the model was run for @xmath150 model minutes , and the last @xmath151 minutes of the model output was averaged in time and in the @xmath40-coordinate to produce reference model output . on a 2.66 ghz dual - core machine",
    "a single model simulation took approximately 25 wall - clock minutes to complete .",
    "if we did not rely on the legendre polynomial expansion , the cost of the computation would have been @xmath152 . however , with the aid of the legendre polynomial expansion we required 17 runs , to cover the wave forcing @xmath65 parameter range , and 33 runs to cover the drag parameter @xmath64 range .",
    "thus , there were @xmath153 full - scale runs .",
    "once these are performed , generating a @xmath143 grid of parameter - space runs had a trivial computational expense . in terms of wall - clock time",
    "the legendre calculation was about 117 times faster than the direct evaluation of the @xmath143 model runs .",
    "the monochromatic wave description used in the model , allows us to relate the field observable rms wave height , @xmath154 , to the wave action @xmath48 through the wave amplitude @xmath10 ( by taking @xmath155 ) , and @xmath49 the spectrum peak wavenumber .",
    "we use a reynolds decomposition : for @xmath156 , we write @xmath157 where @xmath158 .",
    "the angle brackets denote the averaging in time and in the @xmath40-direction .    assuming steady waves and currents , averaging in the @xmath1-direction yields @xmath159 namely , the anti - stokes condition .",
    "the mean momentum equation in the @xmath40-direction , is then @xmath160 if the flow is steady , as is known to be the case for large bottom drag values , we obtain the balance @xmath161    we recall the definition of @xmath162 , and thus @xmath163 as remarked at the opening of the section , @xmath164 can is related to the wave amplitude in a simple and linear relation .",
    "the wave amplitude is determined mainly by the offshore wave amplitude up until the breaking zone where the relatively strong alongshore current is generated .",
    "we therefore relate @xmath164 to the offshore wave forcing @xmath65 , a boundary - value parameter in our numerical computations , in a linear fashion : _",
    "i.e. _ , we make a first order approximation .",
    "next we assume that the impact of a small change in @xmath64 on @xmath165 and @xmath166 are negligible .",
    "this assumption is supported by the numerics . however , when we change @xmath64 , to satisfy the balance in ( [ eq : longhig2 ] ) and assumptions leading to it , we have to change the other main parameter , the offshore wave forcing , as well .",
    "this will change the mean longshore velocity , as a result .",
    "we thus obtain @xmath167 and @xmath168 subtracting the two relation , for small @xmath169 ( and we assume this results in infitesimal change to @xmath164 to achieve the balance in ( [ eq : longhig2 ] ) ) , gives @xmath170 we further restrict ourselves to the case where @xmath64 is away from zero and relatively large compared to the increment @xmath171 .",
    "thus , by the aforementioned linear approximation of @xmath164 using the boundary wave forcing parameter @xmath65 , we conclude that @xmath172 the parameter estimation results that follow in the next section must reflect this dependency .",
    "there are a variety of drag force parametrizations , but here we want to specifically test the linear drag model . such a model asserts that the drag force is proportional to the local depth - averaged velocity via the constant @xmath64 .",
    "the drag force is thus only time dependent via the velocity itself .",
    "we then expect that the maximum likelihood estimation should be very stable to changes in the drag force parameters .",
    "in contrast , we can expect higher variability in the forcing amplitude . in section [ analysis ]",
    "we derived the structural reasons in the model that lead to this type of sensitivity .    in our study",
    ", we compare the longshore depth - averaged , time - averaged , and spatially @xmath40-averaged velocity component predicted by the model with the mean longshore current reported by herbers , guza , and elgar from experiments conducted in 1994 , in duck , north carolina .",
    "the collected data , is available from www.frf.usace.army.mil/duck94/duck94.stm .",
    "the experimental data we use here were collected by the devices v12 , v13 , v14 , which were located approximately at the offshore coordinates 205 , 220 , and 240 m , respectively .",
    "these devices locations cover the bar region of the domain which has stronger mean longshore currents . a plot showing the device locations as well as a snapshot of the bottom topography appears recreated in figure [ fig : topography ] .",
    "the longshore current data in this experiment was collected at a sampling rate of 2 hz , every 1024 s. there are over 5000 such data sets , spanning the months of september , october , and early november .",
    "a small portion of the data was not used , either because there was a device failure or because the datum was an extreme outlier .",
    "since there were several experimental devices with approximately the same longshore coordinate , model output as well as instrumental data was @xmath40-averaged . as a result",
    ", the model - data discrepancy has no longshore dependence .",
    "the month of september , october , and november observations include 5030 data points for the longshore mean current .",
    "first 2370 data points come from the observations in september 1994 , the next 2450 from october 1994 , the last 210 in november 1994 . in the text , we number these data points consecutively .    the general outcomes can be summarized as follows :    * the proposed likelihood was superior to the more traditional , absolute distance likelihood , given that there was no covariance information available .",
    "in particular , it delivered sharper maximally - likely parameter values for model / data agreement . * with regard to the polynomial chaos expansion , we found that the first few terms in the sequence were of significance in all the cases considered : for the wave forcing amplitude @xmath65 we use the first @xmath173 coefficients , whereas for the drag coefficient @xmath64 we used the first @xmath174 coefficients .",
    "we found that using more coefficients did not significantly improve the results .",
    "* the use of the polynomial chaos expansion improved the efficiency of the parameter estimation by 2 orders of magnitude .",
    "* we found that the most likely wave forcing was in the range , @xmath175 m. the most likely bottom drag coefficient was in the range @xmath176 .",
    "the model is thus most consistent with the data when wave amplitudes are large , but not excessively so , and in the range of dynamics wherein the attractor of solutions reflects a balance of breaking and drag forces and near - steady longshore currents . * with regard to the length of the experiments , we found that a minimum of 3 - 5 hours of data were needed . among the reasons for this",
    "is that below 3 hours the number of time records were too few : less than twelve .",
    "* when several 3 hour data experiments were compared , we found variations in the wave forcing estimate . this is a positive modeling outcome :",
    "variations in the wave forcing are tied to the time scales of wind variation , which is roughly 3 - 6 hours .",
    "figure [ fig : threehour-1 ] is typical of estimates at different times of the day .",
    "the times of these observations are september 25 , 16:51 - 19:51 and september 25 - 26 , 23:59 - 3:16 .",
    "+ ( a ) ( b ) * on the other hand , we found that the drag coefficient was not as sensitive to the length of the experiment : whether using 3 hour or daily data , the results were similar , with relatively high bottom drag coefficients favored ; nevertheless , slightly larger than 0.007 , the marginally stable value used in @xcite .",
    "for the daily runs , partitions of 96 consecutive data points in the observations were used . in figure",
    "[ fig : med - daily ] we show a  daily  case , from september 1 .",
    "+ ( a)b . the estimate in ( [ eq : nusretslaw ] ) qualitatively conforms with the curve.,title=\"fig:\",height=182 ] ( b)b .",
    "the estimate in ( [ eq : nusretslaw ] ) qualitatively conforms with the curve.,title=\"fig:\",width=211,height=177 ] * tracing the maximum likelihood curve in @xmath177 parameter space ( see figure [ fig : med - daily ] ) we obtain a curve , which when combined with ( [ dv ] ) , recovers a semi - empirical law which holds for our case of interest : @xmath178 + in obtaining this relationship we assumed that @xmath179 .",
    "this semi - empirical relationship says that the mean longshore velocity in the balance ( [ eq : longhig2 ] ) depends on the drag parameter in a locally logarithmic manner .",
    "this is consistent with the shapes of the posterior distributions , and the numerical observations in the sample runs : the higher the drag , the lesser the impact of the change in the drag on the velocity field .",
    "this is particularly evident in figure [ fig : med - daily]b , which highlights the maximum likelihood curve in parameter space , corresponding to data points 700 through 795 .",
    "the tuning strategy is not failure - proof , but this is not seen here in a negative light : if the model and the data are irreconcilable it is possible for one to obtain estimates that make little sense .",
    "that we know they do not make sense means that we know something about the physics and limitations of the model for us to make this determination .",
    "we do not have this much knowledge about the outcomes in other complex problems . for models that are exceedingly complicated",
    "this becomes a nontrivial challenge .",
    "nevertheless , priors can be used to narrow the parameter search or for constraining its characteristics : more priors can be embedded into ( [ eq.pad2 ] ) .",
    "the danger , however , is that these priors inform the posterior too strongly ; what is desired is that the priors inform the likelihood .",
    "a non - sensical case is shown in figure [ fig : too - strong - daily ] , using a data set corresponding to measurements 3800 through 3895 .",
    "this is a daily - data experiment suggesting that the likely drag values are exceedingly small and the wave forcing very high .",
    "model outcomes corresponding to this case would correspond to highly unstable modeled longshore currents with variability that is not consistent with measurements .",
    "we proposed an alternative maximum likelihood function for model / data disparity for parameter estimation , suitable in cases when covariance information is unavailable or poorly constrained .",
    "the lack of this information frequently occurs in real - world applications and thus it is argued that this alternative likelihood function will have practical value .",
    "the covariance matrix not only provides weights for model / data disparity .",
    "it is also crucial in nondimensionalizing and setting properly the scales of the state vector .",
    "if the state vector is composed of multi - physics components , the lack of scale information might become an unsurmountable challenge in practical parameter estimation .",
    "one could non - dimensionalize the state vector in the model and the data , but doing so in a large scale model ( or in an existing code that approximates solutions to the model ) can be impractical . moreover ,",
    "even if there is no multi - physics , but rather , a multiplicity of spatio - temporal scales , the challenge will not be easily obviated by a fixed scaling .    with regard to parameter estimation",
    "we showed that using the traditional maximum likelihood variant ( presuming no covariance information is known ) , produces parameter estimates that can be vague , or at worst , uninformative .",
    "the alternative maximum likelihood function replaces the weighted model / data disparity , the absolute error , by a relative error between these . in doing",
    "so we make sure that the likelihood function will automatically adjust in a reasonable way its variance according to the scales of the state vector components ( and of its spatio - temporal characteristics )",
    ". the use of the relative error will make larger demands with regard to the compatibility of the observations and the model based upon the importance , in terms of magnitude , of the model state vector .",
    "this leads to sharper estimates .",
    "this is not generally the case when the absolute error is used : discrepancies between model output and observations are not ordered with regard to their importance , unless judiciously done so in an ad - hoc fashion . a likelihood which is cast in terms of the relative error changes the issue of the relative confidence of the individual elements of the absolute error to one in terms of relative error changes .",
    "that is , one is now required to know the dimensionless measure of the variance of the relative size of measurements to model outcomes . under reasonable assumptions the jeffreys prior can be used to justify the replacement of each dimensionless measure of the variance by a single constant .",
    "large - scale dynamics models make parameter estimation extremely challenging , due to the curse of dimensionality : firstly , because of the combinatoric complexity of choosing parameters within a model , and secondly , because the models tend to have many degrees of freedom . in our case",
    ", we only had 2 parameters and thus we did not have to resort to monte carlo and experimental design . for the latter , the adoption of a legendre polynomial approximation to the model output increased the efficiency of parameter estimation one - hundred - fold .",
    "we also derived an estimate of the parameter dependence , between the drag and the wave forcing ; namely , the rate of variability of the model outcomes to changes in these two parameters ( see ( [ dv ] ) ) .",
    "we used this estimate to verify that the likelihood estimates , derived using our new formulation were qualitatively correct .",
    "the longshore problem has at least two qualitatively different flow regimes .",
    "according to the model , the model tracks the duck data in a flow regime wherein the drag forces and the breaking wave forces are in a near - balance and the longshore current nearly steady ; in that regime we found likely parameter combinations .",
    "the sensitivity of the parameters in the estimation reflected well a theoretical estimate of their relative variability , derived from the model itself .",
    "the linear drag model was validated in this steady - longshore flow regime .",
    "it should be noted that the data did not lend itself to evaluating the drag model in the more time - unstable flow regime .",
    "we received funding from gomri / bp .",
    "jr and sv also received funding from nsf - dms-1109856 .",
    "this work was supported in part by the national science foundation under grant no .",
    "phys-1066293 and the hospitality of the aspen center for physics .",
    "jr wishes to thank andrew stuart , for stimulating discussions , as well as the american institute of mathematics .",
    "alen alexanderian , justin winokur , ihab sraj , ashwanth srinivasan , mohamed iskandarani , william  c. thacker , and omar  m. knio .",
    "global sensitivity analysis in an ocean general circulation model : a sparse spectral projection approach . _ computational geoscience _ , 16:0 757778 , 2012 .",
    "doi : 10.1007/s10596 - 012 - 9286 - 2 .",
    "ihab sraj , justin winokur , alen alexanderian , chia - ying lee , shuyi  s. chen , ashwanth srinivasan , mohamed iskandarani , william  c. thacker , and omar  m. knio .",
    "ayesian inference of wind drag dependence on wind speed 2 using axbt data during typhoon fanapi .",
    "_ ocean modeling _ , 2013 .",
    "y.  uchiyama , j.  c. mcwilliams , and j.  m. restrepo .",
    "wave - current interaction in nearshore shear instability analyzed with a vortex force formalism . _",
    "journal of geophysical research _ , page c06021 , 2009 .",
    "doi : doi:10.1029/2008jc005135 .",
    "b.  weir , y.  uchiyama , e.  lane , j.  m. restrepo , and j.  c. mcwilliams . a vortex force analysis of the interaction of rip currents and surface gravity waves .",
    "_ journal of geophysical research _ , 116:0 c050001 , 2011 ."
  ],
  "abstract_text": [
    "<S> we propose a modification of a maximum likelihood procedure for tuning parameter values in models , based upon the comparison of their output to field data . </S>",
    "<S> our methodology , which uses polynomial approximations of the sample space to increase the computational efficiency , differs from similar bayesian estimation frameworks in the use of an alternative likelihood distribution , is shown to better address problems in which covariance information is lacking , than its more conventional counterpart .    </S>",
    "<S> lack of covariance information is a frequent challenge in large - scale geophysical estimation . </S>",
    "<S> this is the case in the geophysical problem considered here . </S>",
    "<S> we use a nearshore model for long shore currents and observational data of the same to show the contrast between both maximum likelihood methodologies .    beyond a methodological comparison </S>",
    "<S> , this study gives estimates of parameter values for the bottom drag and surface forcing that make the particular model most consistent with data ; furthermore , we also derive sensitivity estimates that provide useful insights regarding the estimation procedure as well as of the model itself .    </S>",
    "<S> * keywords : * polynomial chaos , bayesian data assimilation , maximum likelihood , parameter estimation , longshore currents , bottom drag .    </S>",
    "<S> _ submitted to ocean modeling _ </S>"
  ]
}