{
  "article_text": [
    "object detection is one of the long - standing and important problems in computer vision .",
    "motivated by the recent success of deep learning  @xcite on visual object recognition tasks  @xcite , significant improvements have been made in the object detection problem  @xcite .",
    "most notably , @xcite proposed the `` regions with convolutional neural network '' ( r - cnn ) framework for object detection and demonstrated state - of - the - art performance on standard detection benchmarks ( e.g. , pascal voc  @xcite , ilsvrc  @xcite ) with a large margin over the previous arts , which are mostly based on deformable part model ( dpm )  @xcite .",
    "there are two major keys to the success of the r - cnn .",
    "first , features matter  @xcite . in the r - cnn , the low - level image features ( e.g. , hog  @xcite ) are replaced with the cnn features , which are arguably more discriminative representations .",
    "one drawback of cnn features , however , is that they are expensive to compute .",
    "the r - cnn overcomes this issue by proposing a few hundreds or thousands candidate bounding boxes via the selective search algorithm  @xcite to effectively reduce the computational cost required to evaluate the detection scores at all regions of an image .    despite the success of r - cnn ,",
    "it has been pointed out through an error analysis  @xcite that inaccurate localization causes the most egregious errors in the r - cnn framework  @xcite . for example , if there is no bounding box in the close proximity of ground truth among those proposed by selective search , no matter what we have for the features or classifiers , there is no way to detect the correct bounding box of the object . indeed , there are many applications that require accurate localization of an object bounding box , such as detecting moving objects ( e.g. , car , pedestrian , bicycles ) for autonomous driving  @xcite , detecting objects for robotic grasping or manipulation in robotic surgery or manufacturing  @xcite , and many others .    in this work ,",
    "we address the localization difficulty of the r - cnn detection framework with two ideas .",
    "first , we develop a fine - grained search algorithm to expand an initial set of bounding boxes by proposing new bounding boxes with scores that are likely to be higher than the initial ones . by doing so ,",
    "even if the initial region proposals were poor , the algorithm can find a region that is getting closer to the ground truth after a few iterations .",
    "we build our algorithm in the bayesian optimization framework  @xcite , where evaluation of the complex detection function is replaced with queries from a probabilistic distribution of the function values defined with a computationally efficient surrogate model .",
    "second , we train a cnn classifier with a structured svm objective that aims at classification and localization simultaneously .",
    "we define the structured svm objective function with a hinge loss that balances between classification ( i.e. , determines whether an object exists ) and localization ( i.e. , determines how much it overlaps with the ground truth ) to be used as the last layer of the cnn .    in experiments",
    ", we evaluated our methods on pascal voc 2007 and 2012 detection tasks and compared to other competing methods .",
    "we demonstrated significantly improved performance over the state - of - the - art at different levels of intersection over union ( iou ) criteria .",
    "in particular , our proposed method outperforms the previous arts with a large margin at higher iou criteria ( e.g. , iou = @xmath0 ) , which highlights the good localization ability of our method .",
    "overall , the contributions of this paper are as follows : 1 )  we develop a bayesian optimization framework that can find more accurate object bounding boxes without significantly increasing the number of bounding box proposals , 2 )  we develop a structured svm framework to train a cnn classifier for accurate localization , 3 )  the aforementioned methods are complementary and can be easily adopted to various cnn models , and finally , 4 )  we demonstrate significant improvement in detection performance over the r - cnn on both pascal voc 2007 and 2012 benchmarks .",
    "the dpm  @xcite and its variants  @xcite have been the dominating methods for object detection tasks for years .",
    "these methods use image descriptors such as hog  @xcite , sift  @xcite , and lbp  @xcite as features and densely sweep through the entire image to find a maximum response region . with the notable success of cnn on large scale object recognition  @xcite , several detection methods based on cnns have been proposed  @xcite . following the traditional sliding window method for region proposal",
    ", @xcite proposed to search exhaustively over an entire image using cnns , but made it efficient by conducting a convolution on the entire image at once at multiple scales .",
    "apart from the sliding window method , @xcite used cnns to regress the bounding boxes of objects in the image and used another cnn classifier to verify whether the predicted boxes contain objects . @xcite proposed the r - cnn following the `` recognition using regions '' paradigm  @xcite , which also inspired several previous state - of - the - art methods  @xcite . in this framework ,",
    "a few hundreds or thousands of regions are proposed for an image via the selective search algorithm  @xcite and the cnn is finetuned with these region proposals .",
    "our method is built upon the r - cnn framework using the cnn proposed in  @xcite , but with 1 ) a novel method to propose extra bounding boxes in the case of poor localization , and 2 ) a classifier with improved localization sensitivity .",
    "the structured svm objective function in our work is inspired by @xcite , where they trained a kernelized structured svm on low - level visual features ( i.e. , hog  @xcite ) to predict the object location .",
    "@xcite integrated a structured objective with the deep neural network for object detection , but they adopted the branch - and - bound strategy for training as in @xcite . in our work , we formulate the linear structured objective upon high - level features learned by deep cnn architectures , but our negative mining step is very efficient thanks to the region - based detection framework .",
    "we also present a gradient - based optimization method for training our architecture .",
    "there have been several other related work for accurate object localization .",
    "@xcite incorporated the geometric consistency of bounding boxes with bottom - up segmentation as auxiliary features into the dpm .",
    "@xcite used the structured svm with color and edge features to refine the bounding box coordinates in dpm framework .",
    "@xcite used the height prior of an object . these auxiliary features to aid object localization can be injected into our framework without modifications .",
    "localization refinement can be also taken as a cnn regression problem .",
    "@xcite extracted the middle layer features and linearly regressed the initially proposed regions to better locations . @xcite refined bounding boxes from a grid layout to flexible locations and sizes using the higher layers of the deep cnn architecture . @xcite",
    "jointly conducted classification and regression in a single architecture .",
    "our method is different in that 1 ) it uses the information from multiple existing regions instead of a single bounding box for predicting a new candidate region , and 2 ) it focuses only on maximizing the localization ability of the cnn classifier instead of doing any regression from one bounding box to another .",
    "let @xmath1 denote a detection score of an image @xmath2 at the region with the box coordinates @xmath3 .",
    "the object detection problem deals with finding the local maximum of @xmath1 with respect to @xmath4 of an unseen image @xmath2 .",
    "as it requires an evaluation of the score function at many possible regions , it is crucial to have an efficient algorithm to search for the candidate bounding boxes .",
    "a sliding window method has been used as a dominant search algorithm  @xcite , which exhaustively searches over an entire image with fixed - sized windows at different scales to find a bounding box with a maximum score . however , evaluating the score function at all regions determined by the sliding window approach is prohibitively expensive when the cnn features are used as the image region descriptor .",
    "the problem becomes more severe when flexible aspect ratios are needed for handling object shape variations .",
    "alternatively , the `` recognition using regions ''  @xcite method has been proposed , which requires to evaluate significantly fewer number of regions ( e.g. , few hundreds or thousands ) with different scales and aspect ratios , and it can use the state - of - the - art image features with high computational complexity , such as the cnn features  @xcite .",
    "one potential issue of object detection pipelines based on region proposal is that the correct detection will not happen when there is no region proposed in the proximity of the ground truth bounding box . to resolve this issue",
    ", one can propose more bounding boxes to cover the entire image more densely , but this would significantly increase the computational cost . in this section , we develop a fine - grained search ( fgs ) algorithm based on bayesian optimization that sequentially proposes a new bounding box with a higher _ expected _ detection score than previously proposed bounding boxes without significantly increasing the number of region proposals .",
    "we first present the general bayesian optimization framework ( section  [ sec - general - bayesian - framework ] ) and describe the fgs algorithm using gaussian process as the prior for the score function ( section  [ sec - gp - regression ] ) .",
    "we then present the local fgs algorithm that searches over multiple local regions instead of a single global region ( section  [ sec - l - fgs ] ) , and discuss the hyperparameter learning of our fgs algorithm ( section  [ sec - gp - param ] ) .",
    "let @xmath5 be the set of solutions ( e.g. , bounding boxes ) . in the bayesian optimization framework ,",
    "@xmath6 is assumed to be drawn from a probabilistic model : @xmath7 where @xmath8 @xmath9 @xmath10 and @xmath11 @xmath9 @xmath12 .",
    "here , the goal is to find a new solution @xmath13 that maximizes the chance of improving the detection score @xmath14 , where the chance is often defined as an acquisition function @xmath15 .",
    "then , the algorithm proceeds by recursively sampling a new solution @xmath16 from @xmath17 , and update the set @xmath18 to draw a new sample solution @xmath19 with an updated observation .",
    "bayesian optimization is efficient in terms of the number of function evaluation  @xcite , and is particularly effective when @xmath20 is computationally expensive . when @xmath15 is much less expensive than @xmath20 to evaluate , and the computation for @xmath21 requires only a few function evaluations , we can efficiently find a solution that is getting closer to the ground truth .",
    "a gaussian process ( gp ) defines a prior distribution @xmath22 over the function @xmath23 . due to this property , a distribution over @xmath20 is fully specified by a mean function @xmath24 and a positive definite covariance kernel @xmath25 , i.e. , @xmath20 @xmath26 @xmath27 .",
    "specifically , for a finite set @xmath28 , the random vector @xmath29_{1\\leq j \\leq { n}}$ ] follows a multivariate gaussian distribution @xmath30_{1\\leq j \\leq { n}},[k(y_{i},y_{j})]_{1\\leq i , j\\leq { n}}\\right)$ ] . a random gaussian noise with precision @xmath31 is usually added to each @xmath32 independently in practice .",
    "here , we used the constant mean function @xmath33 @xmath9 @xmath34 and the squared exponential covariance kernel with automatic relevance determination ( seard ) as follows : @xmath35 @xmath36 where @xmath37 is a @xmath38 diagonal matrix whose diagonal entries are @xmath39 . these form a @xmath40-dimensional gp hyperparameter @xmath41 to be learned from the training data .",
    "@xmath42 transforms the bounding box coordinates @xmath4 into a new form : @xmath43,\\ ] ] where @xmath44 and @xmath45 denote the center coordinates , @xmath46 denotes the width , and @xmath47 denotes the height of a bounding box .",
    "we introduce a latent variable @xmath48 to make the covariance kernel scale - invariant .",
    "are scaled down by a certain factor , we can keep @xmath49 invariant by properly setting @xmath48 .",
    "] we determine @xmath48 in a data - driven manner by maximizing the marginal likelihood of @xmath50 , or @xmath51    the gp regression ( gpr ) problem tries to find a new argument @xmath13 given @xmath52 observations @xmath8 that maximizes the value of acquisition function , which , in our case , is defined with the expected improvement ( ei ) as : @xmath53 @xmath54 where @xmath55 .",
    "the posterior of @xmath56 given @xmath57 follows gaussian distribution : @xmath58 @xmath59 with the following mean function and covariance kernels : @xmath60_{1\\leq j\\leq { n}}\\right ) , \\\\",
    "\\sigma^{2}(y_{{n}+1}|\\mathcal{d}_{{n } } ) & = k_{n+1}-\\mathbf{k}_{{n}+1}^{\\top}\\mathbf{k}_{{n}}^{-1}\\mathbf{k}_{{n}+1 } , \\\\",
    "k_{n+1 } & = \\beta^{-1}+ k(y_{{n}+1},y_{{n}+1}),\\\\ \\mathbf{k}_{{n}+1 } & = \\big[k(y_{{n}+1},y_{j})\\big]_{1\\leq j\\leq { n}},\\\\ \\mathbf{k}_{{n } } & = \\big[k(y_{i},y_{j})\\big]_{1\\leq i , j\\leq { n}}+\\beta^{-1}\\mathbf{i}\\;.\\end{aligned}\\ ] ] we refer  @xcite for detailed derivation . by plugging   in  , @xmath61 where @xmath62 .",
    "@xmath63 is the cumulative distribution function of standard normal distribution @xmath64 .      in this section ,",
    "we extend the gpr - based algorithm for global maximum search to local fine - grained search ( fgs ) .",
    "the local fgs steps are described in figure  [ fig : structured - rcnn ] .",
    "we perform the fgs by pruning out easy negatives with low classification scores from the set of regions proposed by the selective search algorithm and sorting out a few bounding boxes with the maximum scores in local regions .",
    "then , for each local optimum @xmath65 ( red boxes in figure  [ fig : structured - rcnn ] ) , we propose a new candidate bounding box ( green boxes in figure  [ fig : structured - rcnn ] ) .",
    "specifically , we initialize a set of local observations @xmath66 for @xmath65 from the set given by the selective search algorithm , whose localness is measured by an iou between @xmath65 and region proposals ( yellow boxes is not a rectangular region around local optimum since we use iou to determine it . ] in figure  [ fig : structured - rcnn ] ) .",
    "@xmath66 is used to fit a gp model , and the procedure is iterated for each local optimum at different levels of iou until there is no more acceptable proposal .",
    "we provide a pseudocode of local fgs in algorithm  [ alg : fgs ] , where the parameters are set as : @xmath67 , @xmath68 .",
    "in addition to the capability of handling multiple objects in a single image , better computational efficiency is another factor making local fgs preferable to global search . as a kernel method , the computational complexity of gpr increases cubically to the number of observations . by restricting the observation set to the nearby region of a local optimum ,",
    "the gp fitting and proposal process can be performed efficiently . in practice",
    ", fgs introduces only @xmath69 computational overhead compared to the original r - cnn .",
    "please see the appendices , which are also available in our technical report  @xcite , for more details on its practical efficiency ( appendix  [ sup : fgs - efficiency ] ) .      as we locally perform the fgs",
    ", the gp hyperparameter @xmath70 also needs to be trained with observations in the vicinity of ground truth objects . to this end , for an annotated object in the training set , we form a set of observations with the structured labels and corresponding classification scores of the bounding boxes that are close to the ground truth bounding box . such an observation set",
    "is composed of the bounding boxes ( given by selective search and random selection ) whose iou with the ground truth exceed a certain threshold .",
    "finally , we fit a gp model by maximizing the joint likelihood of such observations : @xmath71 where @xmath72 is the index set for positive training samples ( i.e. , with ground truth object annotations ) , and @xmath73 is a ground truth annotation of an image @xmath74 . for handling multiple objects in training . ]",
    "we set @xmath75 , where @xmath76 consists of the bounding boxes given by selective search on @xmath74 , @xmath77 is a random subset of @xmath78 , and @xmath79 is the overlap threshold .",
    "the optimal solution @xmath80 can be obtained via l - bfgs .",
    "our implementation relies on the gpml toolbox  @xcite .",
    "this section describes a training algorithm of r - cnn for object detection using structured loss .",
    "we first revisit the object detection framework with structured output regression introduced by  @xcite in section  [ sec - str_detect ] , and extend it to r - cnn pipeline that allows training the network with structured hinge loss in section  [ sec - finetuning ] .",
    "let @xmath81 be the set of training images and @xmath82 be the set of corresponding structured labels .",
    "the structured label @xmath83 is composed of 5 elements @xmath84 ; when @xmath85 , @xmath86 and @xmath87 denote the top - left and bottom - right coordinates of the object , respectively , and when @xmath88 , it implies that there is no object in @xmath74 , and there is no meaning on coordinate elements @xmath89 .",
    "note that the definition of @xmath73 is extended from section  [ sec - gp ] to indicate the presence of an object @xmath90 as well as its location @xmath91 when exists . when there are multiple objects in an image , we crop an image into multiple positive ( @xmath85 ) images , each of which contains a single object , and a negative image ( @xmath88 ) that does nt contain any object .",
    "let @xmath92 represent the feature extracted from an image @xmath2 for a label @xmath4 with @xmath85 . in our case , @xmath92 denotes the top - layer representations of the cnn ( excluding the classification layer ) at location specified by @xmath4 , at location given by @xmath4 to a fixed size ( e.g. , 224@xmath93224 ) to compute the cnn features . ] which are fed into the classification layer .",
    "the detection problem is to find a structured label @xmath83 that has the highest score : @xmath94 where @xmath95 note that includes a trick for setting the detection threshold to @xmath96 .",
    "the model parameter @xmath97 is trained to minimize the structured loss @xmath98 between the predicted label @xmath99 and the ground - truth label @xmath73 : @xmath100 for the detection problem , the structured loss @xmath101 is defined in terms of intersection over union ( iou ) of two bounding boxes defined by @xmath4 and @xmath102 as follows:@xmath103 where @xmath104 . in general , the optimization problem   is difficult to solve due to the complicated form of structured loss . instead , we formulate the surrogate objective in structured svm framework  @xcite as follows : @xmath105 using   and  , the constraint   is written as follows : @xmath106 where @xmath107 , @xmath72 and @xmath108 denote the set of indices for positive and negative training examples , respectively , and @xmath109 .      to learn the r - cnn detector with structured loss , we propose to make several modifications to the original structured svm formulation .",
    "first , we restrict the output space @xmath110 of @xmath111th example to regions proposed via selective search .",
    "this results in a change in notation for every @xmath112 in   and   of @xmath111th example to @xmath113 .",
    "second , the constraints ( [ eq : stsvm_const_reform_pos_pos ] ,  [ eq : stsvm_const_reform_pos_neg ] ,  [ eq : stsvm_const_reform_neg_pos ] ) should be transformed into hinge loss to backpropagate the gradient to lower layers of cnn . specifically , the objective function   is reformulated as follows : @xmath114 where @xmath115 , @xmath116 are given as : @xmath117 @xmath118 note that we use different @xmath119 values for positive and negative examples . in experiments , @xmath120 and @xmath121 .",
    "structured svm objective may cause a slow convergence in parameter estimation since it utilizes at most one instance @xmath4 among a large number of instances in the ( restricted ) output space @xmath76 , whose size varies from few hundreds to thousands . to overcome this issue , we alternately perform a gradient - based parameter estimation and hard negative data mining that effectively adapts the number of training examples to be evaluated for updating the parameters ( appendix  [ sup : hard - mining ] )",
    ".    for model parameter estimation , we use l - bfgs to first learn parameters of the classification layer only .",
    "we found that this already resulted in a good detection performance .",
    "then , we optionally use stochastic gradient descent to finetune the whole cnn classifiers ( appendix  [ sup : finetuning ] ) .",
    "we applied our proposed methods to standard visual object detection tasks on pascal voc 2007  @xcite and 2012  @xcite . in all experiments ,",
    "we consider r - cnns  @xcite as baseline models .",
    "following @xcite , we used the cnn models pretrained on imagenet database  @xcite with @xmath122 object categories  @xcite , and finetuned the whole network using the target database by replacing the existing softmax classification layer to a new one with a different number of classes ( e.g. , @xmath123 classes for voc 2007 and 2012 ) .",
    "we provide the learning details in appendix  [ sup : learning - details ] .",
    "our implementation is based on the caffe toolbox @xcite .    setting the r - cnn as a baseline method",
    ", we compared the detection performance of our proposed methods , such as r - cnn with fgs ( r - cnn + fgs ) , r - cnn trained with structured svm objective ( r - cnn + structobj ) , and their combination ( r - cnn + structobj + fgs ) . since our goal is to localize the bounding boxes more accurately at the object regions , we also consider the iou of @xmath0 for an evaluation criterion , which only counts the detection results as correct when the overlap between the predicted bounding box and the ground truth is greater than @xmath124 .",
    "this is more challenging than common practices ( e.g. , iou @xmath125 ) , but will be a good indicator for a better localization of an object bounding box if successful .",
    "c < > c c < > c c < > c c < > c c < > c & & & & & & & & & + & & & & + & & & & & & & & & + & & & & + & & & & & & & & & + & & & & + & & & & & & & & & + & & & & +      before reporting the performance of the proposed methods in r - cnn framework , we demonstrate the efficacy of fgs algorithm using an oracle detector .",
    "we design a hypothetical oracle detector whose score function is defined as @xmath126 , where @xmath73 is a ground truth annotation for an image @xmath127 .",
    "the score function is ideal in the sense that it outputs high scores for bounding boxes with high overlap with the ground truth and vice versa , overall achieving 100% map .",
    "we summarize the results in figure  [ fig : oracle - gp ] .",
    "we report the performance on the voc 2007 test set at different levels of iou criteria ( @xmath128 ) for the baseline selective search ( ss ; `` fast mode '' in @xcite ) , selective search with objectness  @xcite ( ss + objectness ) , selective search with extended super - pixel similarity measurements ( ss extended )  @xcite , `` quality mode '' of selective search ( ss quality )  @xcite , local random search , and the proposed fgs method with the baseline selective search .    for low values of iou ( @xmath129 ) ,",
    "all methods using the oracle detectors performed almost perfectly due to the ideal score function .",
    "however , we found that the detection performance with different region proposal schemes other than our proposed fgs algorithm start to break down at high values of iou .",
    "for example , the performance of ss , ss + objectness , ss extended , and local random search methods , which used around @xmath130 @xmath26 @xmath131 bounding boxes per image in average , significantly dropped at iou @xmath125 .",
    "ss quality method kept pace with the fgs method until iou of @xmath132 , but again , the performance started to drop at iou @xmath133 .",
    "on the other hand , the performance of fgs dropped @xmath134 in map at iou of @xmath135 by only introducing approximately @xmath136 new bounding boxes per image .",
    "given that ss quality requires @xmath137 region proposals per image , our proposed fgs method is much more computationally efficient ( @xmath138 less bounding boxes ) while localizing the bounding boxes much more accurately .",
    "this provides an insight that , if the detector is accurate , our bayesian optimization framework would limit the number of bounding boxes to a manageable number ( e.g. , few thousands per image on average ) to achieve almost perfect detection results",
    ".    we also report similar experimental analysis for the real detector trained with the proposed structured objective in appendix  [ sup : map - diff - pr ] .      in this section ,",
    "we demonstrate the performance of our proposed methods on pascal voc 2007  @xcite detection task ( comp4 ) , a standard benchmark for object detection problem .",
    "similarly to the training pipeline of r - cnn  @xcite , we finetuned the cnn models ( with softmax classification layer ) pretrained on imagenet database using images from both train and validation sets of voc 2007 and further trained the network with linear svm ( baseline ) or the proposed structured svm objective .",
    "we evaluated on the test set using the proposed fgs algorithm . for post - processing , we performed nms and bounding box regression  @xcite .",
    "figure  [ fig-2007examples ] shows representative examples of successful detection using our method .",
    "for these cases , our method can localize objects accurately even if the initial bounding box proposals do nt have good overlaps with the ground truth .",
    "we show more examples ( including the failure cases ) in appendix  [ sup : example - improvement ] , [ sup : example - fp ] , [ sup : example - rand ] .",
    "the summary results are in table  [ tab - voc2007-iou5 ] with iou criteria of @xmath139 and table  [ tab - voc2007-iou7 ] with @xmath0 .",
    "we report the performance with the alexnet  @xcite and the vggnet ( 16 layers )  @xcite , a deeper cnn model than alexnet that showed a significantly better recognition performance and achieved the best performance on object localization task in ilsvrc 2014 .",
    "first of all , we observed the significant performance improvement by simply having a better cnn model .",
    "building upon the vggnet , the fgs improved the performance by @xmath140 and @xmath141 in map without and with bounding box regression ( table  [ tab - voc2007-iou5 ] ) .",
    "it becomes much more significant when we consider iou criteria of @xmath0 ( table  [ tab - voc2007-iou7 ] ) , improving upon the baseline model by @xmath142 and @xmath143 in map without and with bounding box regression .",
    "the results demonstrate that our fgs algorithm is effective in accurately localizing the bounding box of an object .",
    "further improvement has been made by training a classifier with structured svm objective ; we obtained @xmath144 map in iou criteria of @xmath139 , which , to our knowledge , is higher than the best published results , and @xmath145 map in iou criteria of @xmath0 with fgs and bounding box regression by training the classification layer only (  structobj \" ) . by finetuning the whole cnn classifiers ( `` structobj - ft '' ) , we observed extra improvement for most cases ; for example , we obtained @xmath146 map in iou criteria of @xmath0 , which improves by @xmath147 in map over the method without finetuning .",
    "however , for iou@xmath1480.5 criterion , the overall improvement due to finetuning was relatively small , especially when using bounding box regression . in this case",
    ", considering the high computational cost for finetuning , we found that training only the classification layer is practically a sufficient way to learn a good localization - aware classifier .",
    "we provide in - depth analysis of our proposed methods in the appendices .",
    "specifically , we report the precision - recall curves of different combinations of the proposed methods ( appendix  [ sup : pr - curves ] ) , the performance of fgs with different gp iterations ( appendix  [ sup : per - iter - fgs ] ) , the analysis of localization accuracy ( appendix  [ sup : loc - distr ] ) , and more detection examples .",
    "we also evaluate the performance of the proposed methods on pascal voc 2012  @xcite . as",
    "the data statistics are similar to voc 2007 , we used the same hyperparameters as described in section  [ exp - voc2007 ] for this experiment .",
    "we report the test set map over 20 object categories in table  [ tab - voc2012-iou5 ] .",
    "our proposed method shows improvement by @xmath149 with r - cnn + structobj and @xmath150 with r - cnn + fgs over baseline r - cnn using vggnet .",
    "finally , we obtained @xmath151 map by combining the two methods , which significantly improved upon the baseline r - cnn model and the previously published results on the leaderboard .",
    "in this work , we proposed two complementary methods to improve the performance of object detection in r - cnn framework with 1 ) fine - grained search algorithm in a bayesian optimization framework to refine the region proposals and 2 ) a cnn classifier trained with structured svm objective to improve localization .",
    "we demonstrated the state - of - the - art detection performance on pascal voc 2007 and 2012 benchmarks under standard localization requirements .",
    "our methods showed more significant improvement with higher iou evaluation criteria ( e.g. , iou @xmath152 ) , and hold promise for mission - critical applications that require highly precise localization , such as autonomous driving , robotic surgery and manipulation .",
    "this work was supported by samsung digital media and communication lab , google faculty research award , onr grant n00014 - 13 - 1 - 0762 , china scholarship council , and rackham merit fellowship .",
    "we also acknowledge nvidia for the donation of gpus . finally , we thank scott reed , brian wang , junhyuk oh , xinchen yan , ye liu , wenling shang , and roni mittelman for helpful discussions .",
    "contents of appendices",
    "the model parameters are updated via gradient descent .",
    "the gradient , for example , with respect to the cnn parameters @xmath153 @xmath154 for positive examples is given as follows : @xmath155 where @xmath156 .",
    "similarly , the gradient for negative examples can be computed as follows : @xmath157 where @xmath158 .",
    "the gradient with respect to the parameters of all layers of cnn can be computed efficiently using backpropagation .",
    "when finetuning the entire network , the parameter updated in the hard mining procedure illustrated by algorithm  [ alg : hard - mining ] is done by replacing @xmath159 with the cnn parameters .",
    "the active set consisting of the hard training instances are updated in two steps during the iterative learning process .",
    "first , we include instances @xmath160 to the active set when they are likely to be active , i.e. , affect the gradient : @xmath161 second , once new parameters are estimated , we exclude instances from the current active set when they are likely to be inactive , i.e. , have no effect on the gradient : @xmath162 in our experiments , we used @xmath163 and @xmath164 .",
    "the values of @xmath165 are the same as those for the svm training in r - cnn  @xcite .",
    "we did not observe a noticeable performance fluctuation due to different @xmath165 values .",
    "algorithm  [ alg : hard - mining ] summarizes the hard - mining procedure .",
    "initial parameters @xmath166 , maximum epoch number @xmath167 , training images @xmath168 , positive and negative index @xmath169 final parameters @xmath159 .",
    "the active set @xmath170 @xmath171 , @xmath172 @xmath173 s.t .  , for @xmath174 @xmath175 , @xmath176 update the classifier / network parameters @xmath159 on @xmath177 @xmath178 s.t .  , for @xmath179",
    "for our experiments on pascal voc 2007 and voc 2012 , we first finetune the cnn pretrained on imagenet by stochastic gradient descent with a 21-way softmax classification layer , where 20 ways are for the 20 object categories of interest , and the rest 1 way is for the background . in this step ,",
    "the sgd learning rate starts at 0.0003 , and decreases by 0.3 every 15000 iterations with a mini - batch size of 48 .",
    "we set the momentum to 0.9 , and the weight decay to 0.0005 for all the layers .",
    "after that , we replace the softmax loss layer with a 20-way structured loss layer , where each way is a binary classifier , and the hinge loss for different category are simply summed up .    for classification layer only learning , l - bfgs is adopted , as batch gradient descent for a single layer .",
    "each category has an associated active set for hard negative mining .",
    "the classifier update happens independently for each category when @xmath180 ( @xmath181 in algorithm  [ alg : hard - mining ] ) new hard examples are added to the active set .",
    "it is worth mentioning that , in the beginning of the hard negative mining , significantly more positive images are present than the negative images , resulting in serious unbalance of the active training samples . as a heuristic to avoid this problem , we limit the number of positive image to the number of the active negative images when classifier update happens in the first epoch .",
    "we run the hard negative mining for 2 epochs in total .",
    "the first epoch is for initializing the active set with the above heuristic , and the rest is for learning with the all the training data .",
    "compared to the linear svm training in r - cnn @xcite , our l - bfgs based solution to the structured objective costs @xmath182x longer time .",
    "however , it turns out to be significantly more efficient than svm^struct^  @xcite .    for the entire network finetuning , we initialize the structured loss layer with the weights obtained bythe classification - layer - only learning .",
    "the whole network is then finetuned by backpropagating the gradient from the top layer with a fixed sgd learning rate of @xmath183 . for implementation simplicity , we keep updating the active sets until the end of an epoch , and update the classifiers per epoch ( i.e. , @xmath184 in algorithm  [ alg : hard - mining ] ) . like",
    "before , each category still has one particular active set .",
    "however , the network parameters ( except for the classifier ) are shared across all the category so that the feature extraction time is not scaled up with the number of categories . in practice , we found one epoch was enough for both hard negative mining and sgd in the entire network finetuning case . running more",
    "epochs did not make noticeable improvement on the final detection performance on pascal voc 2007 test set , but cost a significantly larger amount of training time .",
    "in this section , we provide more details on the local fgs presented in algorithm  [ alg : fgs ] of the main text .",
    "* gpr practical efficiency : * for the initial proposals given by selective search , @xmath185 usually turns out to be 20 to 100 , and line  [ alg - ln : latent],[alg - ln : best ] can be efficiently solved in around 9 and 6 l - bfgs  @xcite iterations for structobj , respectively .    * gpu parallelism for cnn : * one image can have multiple search regions ( e.g. , line 6 ) , and 20 object categories together yield more regions .",
    "fgs proposes one extra box per iteration for every search region .",
    "these boxes are fed into the cnn together to utilize gpu parallelism . for vggnet",
    ", we use the batch size of 8 for computing the cnn features within the fgs procedure .    * time overhead : * for pascal voc 2007 , fgs ( @xmath67 ) induced only @xmath186 total overhead compared to initial time cost , which mostly consists of cnn feature extraction from bounding boxes proposed by selective search ( ss ) .",
    "specifically , @xmath187 of the overhead is caused by cnn feature extraction from the newly proposed boxes ( line 11 ) ; the rest is caused by gpr ( line9 , 10 ) , nms ( line 5 ) , and pruning ( line 12 ) .",
    "each gp iteration ( line 2 - 16 ) counts for @xmath188 with respect to the initial time cost , and @xmath189 gp iterations were sufficient for convergence .",
    "figure  [ fig : gp - iter - time ] shows the trends of the accumulated time overhead introduced by fgs per iteration . the time overhead due to fgs may vary with different datasets ( e.g. , voc 2012 ) , but in general",
    ", it is @xmath190 compared to initial time cost .",
    "we evaluated the map at each gp iteration using r - cnn(vgg)+structobj+fgs+bboxreg .",
    "the maps from 0 to 8 gp iterations are reported in table  [ tab : gp - stepwise ] .",
    "map increases rapidly in the first 4 iterations , and becomes stable in the following iterations ."
  ],
  "abstract_text": [
    "<S> object detection systems based on the deep convolutional neural network ( cnn ) have recently made ground - breaking advances on several object detection benchmarks . </S>",
    "<S> while the features learned by these high - capacity neural networks are discriminative for categorization , inaccurate localization is still a major source of error for detection . </S>",
    "<S> building upon high - capacity cnn architectures , we address the localization problem by 1 ) using a search algorithm based on bayesian optimization that sequentially proposes candidate regions for an object bounding box , and 2 ) training the cnn with a structured loss that explicitly penalizes the localization inaccuracy . in experiments , we demonstrate that each of the proposed methods improves the detection performance over the baseline method on pascal voc 2007 and 2012 datasets . </S>",
    "<S> furthermore , two methods are complementary and significantly outperform the previous state - of - the - art when combined . </S>"
  ]
}