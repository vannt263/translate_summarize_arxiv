{
  "article_text": [
    "consider a discrete - time system with infinite horizon : @xmath0 where @xmath1 is the state at time @xmath2  ranging over a finite set @xmath3 , @xmath4 is the action at time @xmath2  to be chosen from a nonempty subset @xmath5 of a given finite set of available actions @xmath6 at time @xmath2 , and @xmath7 is a random disturbance uniformly and independently selected from [ 0,1 ] at time @xmath2 , representing the uncertainty in the system , and @xmath8 is a next - state function such that @xmath9 for @xmath10 , and @xmath11 $ ] .",
    "define a ( stationary non - randomized markovian ) policy @xmath12 with @xmath13 for all @xmath14 and _ value function _ of @xmath15 given by @xmath16 ,   x\\in x ,   \\label{valfun}\\ ] ] with discount factor @xmath17 and one - period reward function @xmath18 such that @xmath19 for @xmath10 , and @xmath11 $ ] and _ constraint value function _ of @xmath15 given by @xmath20 ,   x\\in x , \\label{costvalfun}\\ ] ] with discount factor @xmath21 and one - period cost function @xmath22 such that @xmath23 for @xmath10 , and @xmath11 $ ] .",
    "we let @xmath24 and @xmath25 .",
    "the function @xmath8 , together with @xmath26 , @xmath18 , and @xmath22 comprise a constrained markov decision process ( cmdp )  @xcite . for simplicity ,",
    "we consider one constraint case .",
    "extension to multiple case is straightforward .    for a given @xmath27 , we let @xmath28 and @xmath29 with @xmath30 . we assume throughout that any sample of @xmath31 and @xmath32",
    "is bounded , respectively . without loss of generality ,",
    "we take the bound to be 1 , i.e. , for any @xmath33 , @xmath34 , and @xmath15 , @xmath35 \\mbox { and } j^{\\pi}(x , w ) \\in [ 0,1 ] .",
    "$ ] ( the generalization to an arbitrary bound can be done by appropriate scaling . or by defining a transformation of @xmath18 into @xmath36 such that @xmath37 and @xmath22 to @xmath38 similarly",
    ", we can construct an  equivalent \" cmdp to the given cmdp which satisfies the assumption . )",
    "we also assume that an initial state @xmath39 is _ fixed _ by some @xmath14 and a nonempty finite policy set @xmath40 is given .",
    "a policy @xmath41 is called _ @xmath42-feasible _ if @xmath43 for given real constants @xmath44 and @xmath45 .",
    "we let @xmath42-feasible policy set @xmath46 .",
    "we then say that for @xmath47 , @xmath48 is an _",
    "@xmath42-feasible optimal _ policy if for some nonempty @xmath49 such that @xmath50 , @xmath51 and @xmath52 .",
    "the problem we consider is obtaining a @xmath53-feasible optimal policy ( or estimating it with an @xmath42-feasible optimal policy ) in @xmath40 , if such a policy exists .",
    "the problem of obtaining a @xmath53-feasible optimal policy is known to be np - hard if @xmath40 contains all possible policies ( in which case @xmath54 ) and the problem size is characterized by the maximum of @xmath55 and @xmath56 and the number of constraints  @xcite .",
    "it seems that there exist only two exact iterative algorithms for this problem that exploit structural properties of cmdps .",
    "chen and feinberg  @xcite provided a value - iteration type algorithm based on certain dynamic programming equations and chang  @xcite presented a policy - iteration type algorithm based on a feasible - policy space characterization . unfortunately , both require solving certain finite or infinite horizon mdp problems so that _ computational complexities depend on state and action space sizes . _ note that linear programming used for finding a best _ randomized _ policy can not be applied here due to non - linearity and non - convexity of this problem ( cf .",
    ", p1 in  ( * ? ? ?",
    "* theorem 3.1 ) ) .",
    "even if there exists a body of works on simulation - based algorithms for solving unconstrained mdps in order to break the curse of dimensionality ( see , e.g. ,  @xcite  @xcite and the references therein ) , it seems that there has been no notable approach to cmdps via simulation .",
    "this paper is probably the first step toward developing such algorithms . because the algorithms proposed in this paper work with simulated sample - paths , computational complexities are independent of @xmath55 and @xmath57 as long as @xmath58 is relatively small .",
    "our approach is simple and natural .",
    "we generate a sequence of @xmath59 where @xmath60 is an estimate of @xmath61 , similar to the sample average approximation method  @xcite , by using simulation over a finite horizon @xmath62 . for each @xmath41 , @xmath63 is estimated with a sample mean and if the sample mean is less than or equal to @xmath64 , @xmath15 is included in @xmath60 .",
    "we then generate a sequence of policies @xmath65 from @xmath60 at iteration @xmath66 , where @xmath67 is an estimate of a 0-feasible optimal policy .",
    "the selection of @xmath67 from @xmath60 is based on the two playing strategies , called  follow - the - awake - leader \" ( ftal ) and  awake - upper - estimated - reward \" ( auer ) , for  sleeping experts and bandits \" problems  @xcite .",
    "a major difference between ftal and auer is that for ftal , we simulate each policy in @xmath60 to update the sample mean of each policy but for auer , we simulate only selected policy @xmath67 to update the sample mean of @xmath67 .",
    "we view @xmath60 as the set of currently awaken or non - sleeping experts / bandits in @xmath40 and the sample value of the accumulated reward sum over the horizon @xmath62 as the sample reward of playing the expert / bandit @xmath15 . by proper adaptation of the results of the  expected regret \" defined over the sleeping experts and bandits model",
    "then , we can establish convergence of the expected performance of our approach without the assumption that a 0-feasible optimal policy is unique .",
    "we show that when @xmath68 , the expected performance @xmath69 $ ] approaches the value of a 0-feasible optimal policy @xmath70 as @xmath71 and @xmath72 with a rate of @xmath73 ( for @xmath74 ) in the ftal case and of @xmath75 in the auer case for such @xmath76 . here",
    "@xmath77 $ ] for @xmath78 .",
    "for the ftal case , we further provide almost - sure convergence of @xmath79 to a @xmath53-feasible optimal policy as @xmath76 and @xmath62 go to infinity with an exponential convergence rate at the expense of the assumption that value functions are all different among policies .    the works on the problem of finding the best solution from a finite set of solutions given stochastic objective and constraint functions by simulation are relatively sparse ( see  @xcite and the related references therein ) .",
    "these works study allocating different ( monte - carlo ) simulation budgets to the solutions to ( approximately ) maximize the probability of selecting the best solution from sample - mean estimates but provide explicit forms of such allocation only in an asymptotic limit , i.e. , when the total number of samples approaches infinity .",
    "this is also typically given under the assumption that the best solution is unique and the distribution of samples are normal and in terms of the unknown true means and variances . even if heuristic iterative approximation procedures of such results are given , the convergences of those are not known . in our context ,",
    "the best policy is not necessarily unique and the normality assumption is not necessarily valid . although pasupathy et al .",
    "@xcite consider general distribution case , the optimal allocation is only characterized by an optimization problem so that explicit forms of budget allocation are difficult to obtain even in an asymptotic limit except for some special cases . without the uniqueness and the normality assumptions , li et al .",
    "@xcite consider a sequence of penalty cost functions to combine objective and constraint functions with certain budget allocation strategy among the solutions but obtaining the sequence of the penalty cost functions is not straightforward and their algorithm converges to a locally optimal solution when some restrictive assumptions are satisfied .",
    "our setting also covers that in which explicit forms for @xmath8 , @xmath18 , and @xmath22 are not available , but they can be simulated . in this setting , another approach to consider is to employ a stochastic - approximation based learning - algorithm as for unconstrained mdps ( see , e.g. ,  @xcite  @xcite ) . but this works when @xmath40 is the set of all possible policies and the convergence speed is typically very slow and finite - time behaviours of such methods are not known .",
    "moreover , it s not immediate how to adapt such approach when @xmath40 is a subset of the set of all possible policies .",
    "we first provide the pseudocode of the ftal algorithm below .",
    "it mainly consists of the * feasible - policy set estimation * step and the * feasible optimal policy estimation * step .",
    "the * feasible - policy set estimation * step obtains @xmath80 at iteration @xmath66 . here",
    "@xmath81 is the sample mean obtained by @xmath66 independent samples of @xmath82 for @xmath83 and @xmath78 .",
    "we let @xmath84 $ ] .",
    "the * feasible - policy set estimation * step selects @xmath67 that achieves @xmath85 if @xmath86 and @xmath87 for all @xmath88 .",
    "( that is , we  follow the current best \" among non - sleeping experts . ) similarly , @xmath89 is the sample mean obtained by @xmath66 independent samples of @xmath90 for @xmath83 , and @xmath91 $ ] .",
    "the counter @xmath92 keeps track of the number of times @xmath15 has been simulated to obtain a sample of @xmath93 .",
    "whenever @xmath15 is included in @xmath60 at some @xmath66 , @xmath15 is simulated .",
    "if there exists @xmath15 in @xmath60 such that @xmath94 , @xmath67 is set to be any such @xmath15 . if @xmath95 , @xmath67 is set to be any @xmath96",
    "* follow - the - awake - leader ( ftal ) *    * * initialization : * select @xmath97 and @xmath98 . set @xmath99 and @xmath94 for all @xmath41 and @xmath100 . *",
    "* loop : * while ( @xmath101 ) * * * feasible - policy set estimation : * for each @xmath41 , obtain @xmath102 by generating @xmath83 and set @xmath103 obtain @xmath80 . * * * feasible optimal policy estimation : * + * if * ( @xmath86 ) * then * * * * * if * @xmath104 such that @xmath94 , * then * @xmath105 * * * * else * @xmath106 . * * * for each @xmath88 , obtain @xmath93 by generating @xmath83 and set @xmath107 and @xmath108 . + * elseif * ( @xmath95 ) * then * set @xmath67 to be any policy in @xmath40 . * * @xmath109    as in ftal , the auer algorithm consists of the same two main steps .",
    "the * feasible - policy set estimation * step obtains @xmath60 as in the ftal case . differently from the ftal case , the * feasible - policy set estimation * step selects @xmath67 at iteration @xmath66 which achieves @xmath110 if @xmath86 and @xmath87 for all @xmath88 .",
    "( the term @xmath111 plays the role of estimating  upper confidence bound \" or  upper estimated reward \"  @xcite .",
    "we choose the bandit with the current highest upper estimated reward . ) then , only @xmath67 is simulated and the sample mean of @xmath67 is updated .",
    "the pseudocode of the auer algorithm is given below .",
    "* awake - upper - estimated - reward ( auer ) *    * * initialization : * same as ftal * * loop : * while ( @xmath101 ) * * * feasible - policy set estimation : * same as ftal * * * feasible optimal policy estimation : * + * if * ( @xmath86 ) * then * * * * * if * @xmath104 such that @xmath94 , * then * @xmath105 . * * * * else * @xmath112 . * * * obtain @xmath113 by generating @xmath83 and set @xmath114 * * * @xmath115 + * elseif * ( @xmath95 ) * then * set @xmath67 to be any policy in @xmath40 . * * @xmath109",
    "we start with the convergence result of @xmath116 .",
    "the following theorem establishes that as @xmath71 , @xmath117 approaches @xmath118-feasible policy set with the rate of @xmath119 for some constant @xmath120 .",
    "that is , @xmath117 is arbitrarily close to @xmath121 as @xmath71 . by letting then @xmath72 and @xmath71 , we can see that it goes to the true feasible policy set @xmath61 with an exponential convergence rate @xmath122 .",
    "we use the @xmath123-notation to mean that @xmath124 if there exist real constants @xmath125 and @xmath126 such that @xmath127 for all @xmath128 for @xmath129 and @xmath130 .",
    "[ thm : feaset ] let @xmath131 .",
    "then for any @xmath132 , @xmath133    the following proof is partly based on the proof of proposition 1 in  @xcite .",
    "the complement of the event @xmath134 is @xmath135 .",
    "this event is equal to @xmath136 , which is further equal to @xmath137 .",
    "therefore , @xmath138 where the second step follows from the fact that @xmath139 .    applying hoeffding inequality  @xcite , we finally have that @xmath140    we remark that if @xmath141 for @xmath132 , then @xmath117 goes to the empty set as @xmath71 so that when @xmath142 , @xmath117 goes to the empty set as @xmath71 and @xmath72",
    ". that is , we can ( approximately ) identify the insolvability of the problem by these algorithms . in",
    "what follows , we assume that @xmath143 .",
    "we first establish almost - sure convergence of the ftal algorithm . for this result",
    ", we need an assumption that @xmath144 for all @xmath145 for a technical reason .",
    "[ thm : ftal ] assume that @xmath144 for all @xmath145 .",
    "let @xmath146 and @xmath131 and @xmath147 for nonempty @xmath117 generated by ftal .",
    "then for any @xmath132 , @xmath148 where @xmath149 .    from the assumption , @xmath150 is unique .",
    "then we have that @xmath151 the result follows then from @xmath152    from the above theorem , we can see that @xmath79 generated by the ftal algorithm converges to a 0-feasible optimal policy as @xmath71 and @xmath72 almost surely if it is unique .",
    "the theorem below establishes a finite - time bound on the expected performance of the ftal algorithm without the assumption that the value functions of policies are different .",
    "the convergence of the expected performance of the ftal algorithm follows then from this . because the result is obtained by a direct application of the expected regret bound of the ftal algorithm for sleeping experts  ( * ? ? ?",
    "* theorem 6 ) , a proof is omitted .",
    "we construct a one - to - one mapping @xmath153 such that @xmath154 for all @xmath155 with @xmath156 . for",
    "@xmath157 and @xmath158 , let @xmath159 and @xmath160 , where @xmath161 for @xmath145 .",
    "note that we allow @xmath162 for @xmath145 . in what follows , the expectation is taken over the algorithm s random choices of @xmath163 given a fixed sequence of @xmath116 .",
    "[ thm : exp1 ] for every @xmath164 and @xmath163 generated by ftal , @xmath165}\\\\ & & \\leq 2\\delta +     \\sum_{j = j_0(1)+1}^{|\\pi| } \\frac{o(1)}{n \\max\\{\\delta,\\delta_{i(i_0(j)-1),i(i_0(j))}\\ } } +   \\sum_{i=1}^{j_0(|\\pi|)-1 } \\frac{o(1)}{n \\max\\{\\delta,\\delta_{i(j_0(i)),i(j_0(i)+1)}\\}}\\end{aligned}\\ ] ] for any fixed sequence of @xmath116 generated by ftal .    note that by letting @xmath71 and @xmath72 , @xmath166 approaches arbitrarily close to @xmath167 with an exponential rate @xmath168 by theorem  [ thm : feaset ] .",
    "this implies that @xmath169 $ ] approaches @xmath167 as @xmath71 and @xmath72 with a rate of @xmath73 for @xmath170 by setting @xmath171 . in some sense",
    ", we can view the value of @xmath172 as the level of the difficulty of solving the problem . as it gets closer to zero",
    ", @xmath76 needs to get larger to obtain the rate .      for the auer algorithm , we are not be able to provide almost - sure convergence result as in theorem  [ thm : ftal ] for the ftal algorithm .",
    "this is because it is difficult to establish that an upper bound on the probability of not choosing a 0-feasible optimal policy goes to zero as @xmath71 and @xmath72 due to the term @xmath111 .",
    "however , we can still provide the convergence of the expected performance of the auer algorithm .",
    "the following theorem establishes a finite - time bound on the expected performance of the auer algorithm , again _ without the assumption that the value functions of policies are different_. as before , the result is from a direct application of the expected regret bound of the auer algorithm for sleeping bandits  ( * ? ? ?",
    "* theorem 12 ) .",
    "[ thm : exp2 ] for every @xmath164 and @xmath163 generated by auer , @xmath165}\\\\ & & \\leq 2\\delta +     \\sum_{j = j_0(1)+1}^{|\\pi| } \\frac{o(\\ln n)}{n \\max\\{\\delta,\\delta_{i(i_0(j)-1),i(i_0(j))}\\ } } +   \\sum_{i=1}^{j_0(|\\pi|)-1 }",
    "\\frac{o(\\ln n)}{n \\max\\{\\delta,\\delta_{i(j_0(i)),i(j_0(i)+1)}\\}}.\\end{aligned}\\ ] ] for any fixed sequence of @xmath116 generated by auer .    from the above result",
    ", we see that @xmath169 $ ] approaches @xmath167 as @xmath71 and @xmath72 with a rate of @xmath75 for @xmath170 by setting @xmath171 .",
    "note that the rate of auer is slower than ftal s by a factor of @xmath173 at the expense of simulating only the selected policy at each iteration .",
    "even if the discussions are made under the model of finite cmdps , the proposed algorithms can be applied to cmdps with infinite state and/or infinite action spaces as long as @xmath40 is a finite set and each policy in @xmath40 can be simulated .",
    "all of the results in the paper still hold in this case .",
    "when we estimate feasible policy set in ftal and auer , we need to simulate all policies in @xmath40 .",
    "developing a non - enumerative method for the feasible - policy set generation step is a good future work direction .",
    "s. bhatnagar , n. hemachandra , and v. k. mishra ,  stochastic approximation algorithms for constrained optimization via simulation , \" _ acm trans . on modeling and computer",
    "simulation _ , vol .",
    "3 , article 15 , 2011 .",
    "d. v. djonin and v. krishnamurthy ,  q - learning algorithms for constrained markov decision processes with randomized monotone policies : application to mimo transmission control , \" _ ieee trans . on signal processing _ ,",
    "5 , pp . 21702181 , 2007 .",
    "j. li , a. sava , and x. xie ,  simulation - based discrete optimization of stochastic discrete event systems subject to non closed - form constraints , \" _ ieee trans . on automatic control _ ,",
    "12 , pp . 29002904 , 2009 .",
    "r. pasupathy , s. r. hunter , n. a. pujowidianto , l. h. lee , and c. chen ,  stochastically constrained ranking and selection via score , \" _ acm trans . on modeling and computer simulations _ ,",
    "1 , article 1 , 2014 ."
  ],
  "abstract_text": [
    "<S> this brief paper presents simple simulation - based algorithms for obtaining an approximately optimal policy in a given finite set in large finite constrained markov decision processes . </S>",
    "<S> the algorithms are adapted from playing strategies for  sleeping experts and bandits \" problem and their computational complexities are independent of state and action space sizes if the given policy set is relatively small . </S>",
    "<S> we establish convergence of their expected performances to the value of an optimal policy and convergence rates , and also almost - sure convergence to an optimal policy with an exponential rate for the algorithm adapted within the context of sleeping experts .    constrained markov decision processes , simulation , sleeping expert and bandit , learning algorithm </S>"
  ]
}