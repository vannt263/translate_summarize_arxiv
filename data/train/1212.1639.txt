{
  "article_text": [
    "the state space model ( ssm ) has been one of the indispensable tools for time series analysis and optimal control for decades .",
    "although the archetypal ssm is linear and gaussian , the literature of more general non - linear and non - gaussian ssms have been rapidly growing in the last two decades . for lack of an analytically tractable way to estimate the general ssm ,",
    "numerous approximation methods have been proposed . among them , arguably the most widely applied method is particle filtering ( gordon et al .",
    "( 1993 ) , kitagawa ( 1996 ) ) .",
    "particle filtering is a type of sequential monte carlo method in which the integrals we need to evaluate for filtering are approximated by the monte carlo integration . to improve numerical accuracy and stability of the particle filtering algorithm ,",
    "various extensions such as the auxiliary particle filter ( pitt and shephard ( 1999 ) ) have been proposed , and still actively studied by many researchers . for ssms with unknown parameters , kitagawa ( 1998 ) proposed a self - organizing state space modeling approach in which the unknown parameters are regarded as a subset of the state variables and the joint posterior distribution of the parameters and the state variables is evaluated with a particle filtering algorithm .",
    "other particle filtering methods that can simultaneously estimate parameters have been proposed by liu and west ( 2001 ) , storvik ( 2002 ) , fearnhead ( 2002 ) , polson et al .",
    "( 2008 ) , johannes and polson ( 2008 ) , johannes et al .",
    "( 2008 ) , carvalho et al .",
    "( 2010 ) , just to name a few .",
    "these particle filtering methods that estimate state variables and parameters simultaneously are often called particle learning methods in the literature . although the effectiveness of particle filtering methods have been proven through many different applications ( see zou and chakrabarty ( 2007 ) , mihaylova et at .",
    "( 2007 ) , chai and yang ( 2007 ) , montemerlo et al .",
    "( 2003 ) , dukic et al .",
    "( 2009 ) , and lopes and tsay ( 2011 ) among others ) , it is offset by the fact that it is a time - consuming technique .",
    "some practitioners still shy away from using it in their applications because of this .    this attitude toward particle filtering",
    "would be changed by the latest technology : parallel computing .",
    "as we will discuss in section 2 , some parts of the particle filtering procedure are ready to be executed simultaneously on many processors in a parallel computing environment . in light of inexpensive parallel processing devices such as gpgpus ( general purpose graphics processing units ) available to the general public",
    ", more and more researchers start to jump on the bandwagon of parallel computing .",
    "lee et al .",
    "( 2010 ) reviewed general attempts at parallelization of bayesian estimation methods .",
    "durham and geweke ( 2011 ) developed a sequential monte carlo method designed for gpu computing and applied it to complex nonlinear dynamic models , which are numerically intractable even for the markov chain monte carlo method .",
    "as for parallelization of particle filtering , a few researches ( see montemayor et al .",
    "( 2004 ) , maskell et al .",
    "( 2006 ) , hendeby et al .",
    "( 2007 ) for example ) have been reported , though the field is still in a very early stage . to the best of the authors knowledge , hendeby et al .",
    "( 2010 ) is the only successful implementation of the particle filtering algorithm on the gpu . their implementation , however , depends on device - specific functionalities and its resampling algorithm is not an exact one",
    ".    in developing parallel algorithms designed for the gpu , there are a few bottlenecks one should avoid .",
    "first , processing sequential algorithms on the gpu can be inefficient because of the gpu s device memory architecture .",
    "roughly speaking , a gpu has two types of memory : memory assigned to each core and memory shared by all cores .",
    "access to the core - linked memory is fast while access to the shared memory takes more time .",
    "ideally , one should try as much to keep all calculations on each core without any communications among cores .",
    "the second bottleneck is that it is time - consuming to transfer memory between the host memory , which the cpu uses , and the device memory , which the gpu uses . in other words",
    "the bandwidth between the gpu s device memory and the cpu s host memory is very narrow .",
    "thus an ideal parallel algorithm for the gpu would be to calculate everything within the gpu , preferably within each core ( without inter - core communications ) .    with these bottlenecks in mind , we have developed a new parallel algorithm that computes the full cycle of the particle filtering algorithm in a massively and fully parallel manner , from computing the likelihood for each particle , constructing the cumulative distribution function ( cdf ) for resampling , resampling the particles with the cdf , and propagating new particles for the next cycle . by keeping all of our computations within the gpu and avoiding all memory transfer between the gpu and the cpu during the execution of the particle filtering algorithm . in this way",
    ", we exploit the great benefits of parallel computing on the gpu while avoiding its short comings .",
    "additionally , since our parallel algorithm does not utilize any device - specific functionalities , it can be easily implemented on other parallel computing devices including cloud computing systems .    in order to compare our new parallel algorithm with conventional sequential algorithms",
    ", we conducted a monte carlo experiment in which we applied the competing particle learning algorithms to estimate a simple state space model ( stochastic trend with noise model ) and recorded the execution time of each algorithm .",
    "the results showed that our parallel algorithm on the gpu is far superior to the conventional sequential algorithm on the cpu by around 30x to 200x .",
    "the organization of this paper is as follows . in section 2 ,",
    "we briefly review state space models and particle filtering and learning . in section 3 ,",
    "we describe how to implement a fully parallelized particle filtering algorithm , in particular how to parallelize the cdf construction step and the resampling step . in section 4 , we report the results of our monte carlo experiment and discuss their implications . in section 5 ,",
    "we state our conclusion .",
    "a general form of ssm is given by @xmath0 where @xmath1 stands for the conditional distribution or density of observation @xmath2 given unobservable @xmath3 and @xmath4 stands for the conditional distribution or density of @xmath3 given @xmath5 , which is the previous realization of @xmath3 itself .",
    "in the literature of ssm , unobservable @xmath3 , which dictates the stochastic process of @xmath2 , is called the state variable .",
    "time series data analysis with the ssm is centered on how to dig up hidden structures of the state variable out of the observations @xmath6 .",
    "in particular , the key questions in applications of ssm are ( i ) how to estimate the current unobservable @xmath3 , ( ii ) how to predict the future state variables , and ( iii ) how to infer about the past state variables with the data currently available . these aspects of state space modeling are called _ filtering _ , _ prediction _ , and _ smoothing _ , respectively .",
    "the filtering procedure , which is the main concern in our study , is given by the sequential bayes filter : @xmath7 where @xmath8 @xmath9 and @xmath10 is the conditional density of the state variable @xmath3 given @xmath11 . in essence , equation ( 3 ) is the well - known bayes rule to update the conditional density of @xmath3 while equation ( 2 ) is the one - period - ahead predictive density of @xmath3 given the past observations @xmath12 . by applying ( 2 ) and ( 3 ) repeatedly , one keeps the conditional density @xmath13 updated as a new observation comes in .    in general",
    ", a closed - form of neither ( 2 ) nor ( 3 ) is available , except for the linear gaussian case where we can use the kalman filter ( kalman ( 1960 ) ) .",
    "see west and harrison ( 1997 ) on detailed accounts about the linear gaussian ssm . to deal with this difficulty",
    ", we apply particle filtering , in which we approximate the integrals in ( 2 ) and ( 3 ) with _ particles _ , a random sample of the state variables generated from either the conditional density @xmath10 or the predictive density @xmath14 .",
    "let @xmath15 denote @xmath16 particles generated from @xmath17 .",
    "we can approximate @xmath18 by @xmath19 where @xmath20 is the dirac delta . then the filtering equation ( 3 )",
    "is approximated by @xmath21 ( 5 ) implies that the conditional density @xmath10 is discretized on particles @xmath22 with probabilities @xmath23 .",
    "therefore we can obtain a sample of @xmath3 , @xmath24 , from @xmath10 by drawing each @xmath25 out of @xmath22 with probabilities @xmath23 when the approximation ( 5 ) is sufficiently accurate .",
    "this procedure is called _ resampling_. in reverse , if we have @xmath16 particles @xmath26 generated from @xmath27 , we can approximate @xmath17 by @xmath28 then ( 6 ) implies that we can obtain a sample of @xmath29 , @xmath30 , from @xmath31 by generating each @xmath32 from @xmath33 , which is called _",
    "propagation_. hence we can mimic the sequential bayes filter by repeating the _ propagation _ equation ( 6 ) and the _ resampling _ equation ( 5 ) for @xmath34 this is the basic principle of particle filtering .",
    "the formal representation of the particle filtering algorithm is given as follows .",
    "+   + algorithm : particle filtering    step 0 : : :    set the starting values of @xmath16 particles    @xmath35 .",
    "step 1 : : :    propagate @xmath36 from    @xmath37    @xmath38 .",
    "step 2 : : :    compute weight    @xmath39 .",
    "step 3 : : :    resample @xmath40 from    @xmath15 with weight    @xmath41 .",
    "when a state space model depends on unknown parameters @xmath42 , @xmath43 we need to evaluate the posterior distribution @xmath44 given the observations @xmath11 . in the framework of particle filtering",
    ", @xmath44 is sequentially updated as a new observation arrives , which is called particle learning .",
    "the particle learning algorithm is defined as follows .",
    "let @xmath45 and @xmath46 denote particles jointly generated from @xmath47 and @xmath48 respectively .",
    "then the particle approximation of the bayesian learning process ( kitagawa ( 1998 ) ) is given by @xmath49 this is a rather straightforward generalization of the particle filtering .",
    "+   + algorithm : particle learning    step 0 : : :    set the starting values of @xmath16 particles    @xmath50 .",
    "step 1 : : :    propagate @xmath51 from    @xmath52    @xmath38 .",
    "step 2 : : :    compute weight    @xmath53 .",
    "step 3 : : :    resample @xmath54 from    @xmath55 with weight    @xmath41 .",
    "once we generate @xmath56 by the particle learning , we can treat them as a monte carlo sample of @xmath42 drawn from the posterior density @xmath44 .",
    "thus we calculate the posterior statistics on @xmath42 with @xmath56 in the same manner as the traditional monte carlo method or the state - of - the - art markov chain monte carlo ( mcmc ) method",
    ".    the computational burden of particle filtering will be prohibitively taxing as the number of particles @xmath16 increases . the number of the likelihood @xmath57 to be evaluated , the number of executions for constructing the cdf of particles , and the number of particles to be generated in propagation will increase in @xmath58 .",
    "the number of executions for resampling with the cdf will increase in @xmath59 when we use a naive resampling algorithm , but it can be reduced to @xmath60 with more efficient algorithms , which we will discuss in the next section . thus sequential particle - by - particle execution of each step in the particle filtering ( and learning ) algorithm is inefficient when @xmath16 is large , even though the particle filtering method by construction requires a large number of particles to guarantee precision in the estimation .    to reduce the time for computation",
    ", we propose to parallelize all steps in particle filtering so that we can execute the parallelized particle filtering algorithm completely inside the gpu .",
    "the key to constructing an efficient parallel algorithm is asynchronous out - of - order execution of jobs assigned to each processor .",
    "we need to keep a massive number of processors in the gpu as busy as possible to fully exploit a potential computational power of the gpu .",
    "therefore each processor should waste no milliseconds by waiting for other processors until they complete their jobs . if the order of execution is independent of the end results , asynchronous out - of - order execution",
    "is readily implemented . in this situation ,",
    "parallelization is rather straightforward . in the particle filtering method",
    ", this is the case for computation of the likelihood and the propagation step and these steps can be computed in parallel without any modifications . for constructing the cdf and resampling particles , on the other hand ,",
    "the conventional algorithm does not allow asynchronous out - of - order execution and thus parallelization of these steps can be tricky . in order to devise a fully parallelized particle filtering method",
    ", we need to develop new algorithms for parallelization for these steps . in the next section ,",
    "we describe how to implement the cdf construction and resampling in a parallel computing environment .",
    "the goal of resampling is to generate @xmath16 random integers , which are the indices of particles , from a discrete distribution on @xmath61 with the cumulative distribution function , @xmath62 therefore we need to construct the cdf ( 10 ) before we perform resampling .",
    "hendeby et al .",
    "( 2007 ) developed an algorithm designed for parallel execution of the cdf construction .",
    "the process consists of two procedures : the forward adder and the backward adder .",
    "these are illustrated in table [ table.cdf ] .",
    "suppose that we have four particles and their weights are given by @xmath63 .",
    "what we want to compute is the cumulative sum , @xmath64 .",
    "first we apply the forward adder as described in panel ( a ) of table [ table.cdf ] . in the initial step of the forward adder ,",
    "each weight is assigned to a node ( nodes are corresponding to threads in the gpu ) .",
    "let @xmath65 denote the sum of weights of node @xmath66 in step @xmath67 .",
    "thus the initial states of the nodes are @xmath68 , @xmath69 , @xmath70 , and @xmath71 . then in each step of the forward adder , a neighboring pair of nodes is combined to form a new node which inherits the sum of weights from the two combining nodes as its new weight .",
    "for example , in step 2 of table [ table.cdf](a ) , a pair of nodes , @xmath68 and @xmath69 , is combined into a new node @xmath72 .",
    "we repeat this procedure until all nodes are collapsed into a single node whose weight is the sum of all weights .",
    "then we apply the backward adder as described in panel ( b ) of table [ table.cdf ] .",
    "note that the numbering of steps is reversed in table [ table.cdf ] ( b ) as this is intentional . in the initial step of the backward adder , we start at the last node in the forward adder .",
    "let @xmath73 denote the sum of weights up to particle @xmath66 in step @xmath67 . in panel ( b ) of table [ table.cdf ]",
    ", we already have @xmath74 by the forward adder . as it moves backward , the current node branches into two nodes in each step .",
    "the new right node inherits the same value of the current node , while the partial sum of the new left node equals the value of the current node minus the weight of the right node in the corresponding step of the forward adder .",
    "for example , in step 1 of table [ table.cdf](b ) , the node with @xmath75 branches to @xmath76 and @xmath77 . note that the number of particles should be an integer power of 2 in order to apply this algorithm .",
    "step 1 & & & & + step 2 & & + step 3 & +   + step 3 & + step 2 & & + step 1 & @xmath78 & @xmath79 & @xmath80 & @xmath81 +      before we proceed to describe our parallel resampling algorithm , we briefly review several conventional sequential resampling algorithms . in the most naive manner ,",
    "the resampling algorithm is expressed as    .... for(i in 1:n ) {      u < - runif(1 ) ;      for(j in 1:n ) {          if(u < q[j ] )              break ;      }      sampled[i ] < - j ; } ....    although this is the most basic and exact resampling procedure , it is extremely time - consuming @xmath59 operation , as we need to search out each particle one by one in the whole cdf .",
    "one simple way to improve it is by sorting the uniform variates before the search process starts :    .... u < - sort(runif(n ) ) ; for(i in 1:n ) {      for(j in i : n ) {          if(u[j ] < q[j ] )              break ;      }      sampled[i ] < - j ; } ....    compared to the naive algorithm , this algorithm starts off by sampling all necessary random numbers from the uniform distribution , sorts them in ascending order , and then sequentially resamples particle with them . by sorting the uniform variates , each search for a particle can be started where the last research was left off .",
    "the offset of this algorithm is that , although the resampling procedure is a more efficient @xmath60 operation , sorting uniform variates can be computationally strenuous as the number of particles increase , depending on the sorting algorithm we use .",
    "alternative resampling algorithms have been introduced in order to circumvent the computational strains found in exact resampling algorithms like those above .",
    "the stratified resampling algorithm ,    .... for(i in 1:n ) {      u < - ( i-1+runif(1))/n ; }      for(j in { i}:n ) {          if(u < q[j ] )              break ;      }      sampled[i ] < - j ; } ....    conducts the resampling procedure by generating uniform variates on @xmath16 equally spaced intervals @xmath82 $ ] @xmath83 .",
    "since only one particle will always be picked for each interval , it does not exactly generate random integers from the the cdf @xmath84 .",
    "the systematic resampling ,    ....",
    "u0 < - runif(1 ) ; for(i in 1:n ) {      u < - ( i-1+u0)/n :      for(j in i : n ) {          if(u < q[j ] )              break ;      }      sampled[i ] < - j ; } ....    is similar to the stratified resampling but it always chooses an identical point in @xmath82 $ ] for all @xmath66 .",
    "in essence , the two alternative resampling algorithms are similar to the aforementioned resampling with sorted uniform variates , but without the time - consuming sorting procedure .",
    "however , neither stratified sampling nor systematic sampling is exact",
    ". therefore , they may produce less accurate results compared to exact resampling algorithms .",
    "in particular when estimating unknown parameters in particle learning , they may be more problematic because they can not jointly resample the state variables and the parameters . thus , despite their computational superiority in a sequential framework , we need to be careful of using them in practice . additionally , as these methods benefit only from the sequential framework , it is not obvious how to parallelize them .      to parallelize the resampling procedure while maintaining its exactness , we have developed a parallel resampling algorithm based on the cut - point method by chen and asau ( 1974 ) .",
    "a _ cut - point _",
    ", @xmath85 , for given @xmath86 is the smallest index @xmath66 such that the corresponding probability @xmath87 should be greater than @xmath88 . in other words ,",
    "@xmath89 given cut - points @xmath90 , random integers between 1 and @xmath16 is generated from the cdf @xmath84 by the following procedure : +   + algorithm : cut - point method    step 0 : : :    let @xmath91 step 1 : : :    generate @xmath92 from the uniform distribution on the interval    @xmath93 .",
    "step 2 : : :    let @xmath94 where    @xmath95 stands for the smallest integer greater    than or equal to @xmath96 .",
    "step 3 : : :    if @xmath97 , let @xmath98 and repeat    * step 3 * ; otherwise , go to * step 4*. step 4 : : :    store @xmath99 as the index of the particle .",
    "step 5 : : :    if @xmath100 , let @xmath101 and go back to    * step 1 * ; otherwise , exit the loop .",
    "once all cut - points @xmath90 are given , parallel execution of the cut - point method is straightforward because the execution of * step 1 *  * step 3 * does not depend on the index @xmath67 .",
    "the fully parallel resampling algorithm distributed on @xmath16 threads is given as follows .",
    "+   + algorithm : parallelized cut - point method    step 0 : : :    initiate the @xmath67-th thread .",
    "step 1 : : :    generate @xmath92 from the uniform distribution on the interval    @xmath93 .",
    "step 2 : : :    let @xmath94 where    @xmath95 stands for the smallest integer greater    than or equal to @xmath96 .",
    "step 3 : : :    if @xmath97 , let @xmath98 and repeat    * step 3 * ; otherwise , go to * step 4*. step 4 : : :    store @xmath99 as the index of the particle .",
    "step 5 : : :    wait until all threads complete the job .",
    "otherwise , exit the loop .    however , the conventional algorithm for computation of the cut - points ( see fishman ( 1996 , p.158 ) for example ) is not friendly to parallel execution .",
    "thus , we have developed an efficient algorithm for parallel search of all cut - points .",
    "to devise such a search algorithm , let us introduce @xmath102 and @xmath103 for convention .",
    "due to the monotonicity of the cumulative distribution function , we observe    1 .",
    "2 .   if @xmath105 , a cut - point such that @xmath106 is given as @xmath107 .",
    "if @xmath108 , @xmath67 is not corresponding to any cut - points .",
    "@xmath109 always holds .",
    "the above properties give us a convenient criterion to check whether a particular @xmath110 is a cut - point or not and it leads to the following multi - thread parallel algorithm to find all cut - points .",
    "+   + algorithm : parallelized cut - point search    step 0 : : :    initiate the @xmath67-th thread .",
    "step 1 : : :    compute @xmath111 .",
    "step 2 : : :    let @xmath112 .",
    "step 3 : : :    if @xmath113 , let @xmath107 and end the    thread",
    ". step 4 : : :    let @xmath114 and go to * step 3*.    with a fully parallel resampling algorithm , particle filtering can be executed in a fully parallel manner without any compromise . additionally , as the particle filtering algorithm ( and the particle learning algorithm ) is conducted completely on the gpu and",
    "each particle goes through the algorithm on each designated core without syncing , the advantage of parallel computing is gained to the fullest while its shortcoming is kept to its minimum ( data transfer between the gpu s device memory and the cpu s host memory only occurs at the beginning and the end of the computation ) .",
    "in our experiment , we use a stochastic trend with noise model : @xmath115 as the benchmark model for performance comparison . in",
    ", we set @xmath116 , @xmath117 , @xmath118 , and generate @xmath119 . then we treat @xmath120 and @xmath121 as unknown parameters and apply the particle learning algorithm by carvalho et al .",
    "( 2010 ) to .",
    "the prior distributions are @xmath122    to demonstrate the effectiveness of our new parallel algorithm , we will compare the following types of algorithms :    * sequential algorithm on the cpu + _",
    "cpu(n ) _ : : :    naive resampling with single precision _",
    "cpu(s ) _ : : :    resampling with sorted uniform variates with single precision * parallel algorithm on the gpu + _ gpu(sp ) _ : : :    parallel resampling by the cut - point method with single precision _",
    "gpu(dp ) _ : : :    parallel resampling by the cut - point method with double precision    the first two are conventional sequential algorithms for resampling .",
    "the code for the parallel algorithm is written in cuda while that for the sequential algorithms is in c. both are compiled and executed on the same linux pc with specifications shown in table [ spec ] .",
    "alternative resampling algorithms , such as the stratified and systematic resampling , are not considered here , as they are not exact resampling . however , if we exclude the time consumed by the sorting procedure from the resampling time of _",
    "cpu(s ) _ , we get a very good estimate of how long they might take .",
    ".hardware specifications [ cols=\">,>,>\",options=\"header \" , ]     breaking down the execution time gives us deep insights into how the gpu architecture works and its strong and weak points . examining the results in _",
    "cpu(s ) _ , we first notice that the _ cdf _ step and _ propagation _ step put together occupy the bulk of the total execution time , while the _ resampling _ step only accounts for less than ten percent of the total execution time and much of it coming from the sorting step .",
    "looking closely into the gains by parallelization in each step , the largest comes from the _ cdf _ step with a gain of 248x , followed by the _ propagation _ step with a gain of 45.3x , then followed by the _ resampling _",
    "step with a gain of 11.9x .",
    "although the gain in the _ resampling _ step has less of an impact compared with the overwhelming gain in _ cdf _ and _ propagation _ , it does not overshadow the fact that it gained 2.7x in single precision even if we ignore the time spent in sorting the uniform random variates and focus on the resampling procedure only .",
    "that implies that our parallel resampling on the gpu can beat the stratified resampling on the cpu since the stratified resampling is roughly equivalent to the resampling with sorted uniform variates without sorting in terms of computational complexity . as for the _ other _ step , _",
    "cpu(s ) _ and _ gpu(sp ) _ is identical .",
    "this is because for both algorithms , all executions of this step are conducted only on the cpu .",
    "thus , we observe no difference . finally , we observe a good amount of reduction in initiating the particle learning algorithm by our parallel algorithm ; however , the time spent in initiation is quite trivial , in particular when the number of the sample period @xmath123 is large ( @xmath124 in our experiment ) .",
    "although it is clear that our parallel algorithm is superior to the conventional sequential algorithm through every step , table [ table.breakdown ] indicates that there is one drawback of using the gpu . that is memory transfer .",
    "the _ store _ step measures the time it takes to transfer the generated particles from the gpu s device memory to the cpu s host memory . table [ table.breakdown ] shows that it takes up roughly 15 - 20% of the execution time .",
    "note that , for fairness of the experiment , the gpu returns all of the particles it generated to the cpu s host memory .",
    "if we were to return only the mean , the variance , and other statistics of the state variables and parameters , the time for the _ store _ step can be cut down significantly .",
    "in this study , we have developed a new algorithm to perform particle filtering and learning in a parallel computing environment , in particular on gpgpus .",
    "our new algorithm has several advantages .",
    "first , it enables us to keep all executions of the particle filtering ( and learning ) algorithm within the gpu so that data transfer between the gpu s device memory and the cpu s host memory is minimized .",
    "second , unlike the stratified sampling or the systematic sampling , our parallel sampling algorithm based on the cut - point method can resample particles exactly from their cdf .",
    "lastly , since our algorithm does not utilize any device specific functionalities , it is straightforward to apply our algorithm to a multiple gpu system or a large grid computing system .",
    "then we conducted a monte carlo experiment in order to compare our parallel algorithm with conventional sequential algorithms . in the experiment ,",
    "our algorithm implemented on the gpu yields results far better than the conventional sequential algorithms on the cpu .",
    "although we keep the ssm as simple as possible in the experiment , our parallel algorithm can also be applied to more complex models without any fundamental modifications to the programming code and this little investment will return a significant gain in execution time instantaneously .",
    "our fully parallelized particle filtering algorithm is beneficial for various applications that require estimating powerful but complex models in a shorter span of time ; ranging from motion tracking technology to high - frequency trading .",
    "we even envision that one can perform real - time filtering of the state variables and the unknown parameters in a high - dimensional nonlinear non - gaussian ssm on an affordable parallel computing system in a completely parallel manner .",
    "that would pave the way for a new era of computationally intensive data analysis .",
    "hendeby , g. , hol , j. d. , karlsson , r. , and gustafsson , f. ( 2007 ) , `` a graphics processing unit implementation of the particle filter , '' _ proceedings of the 15th european statistical signal processing conference ( eusipco 07 ) _ , 16391643 , poznan , poland .",
    "lee , a. , yau , c. , giles , m. b. , doucet , a. , and holmes , c. c. ( 2010 ) , `` on the utility of graphics cards to perform massively parallel simulation of advanced monte carlo methods , '' _ journal of computational and graphical statistics _ , 19 , 769789 .",
    "liu , j. and west , m. ( 2001 ) , `` combined parameters and state estimation in simulation - based filtering , '' _ sequential monte carlo methods in practice _",
    "( a. doucet , n. de freitas and n. gordon , eds . ) .",
    "new york : springer - verlag , 197223 .",
    "maskell , s. , alun - jones , b. , and macleod , m. ( 2006 ) , `` a single instruction multiple data particle filter , '' _ proceedings of nonlinear statistical signal processing workshop ( nsspw 06 ) _ , cambridge , uk .",
    "mihaylova , l. , angelova , d. , honary , s. , bull , d. r. , canagarajah .",
    "c. n. , and ristic , b. ( 2007 ) , `` mobility tracking in cellular networks using particle filtering , '' _ ieee transactions on wireless communications _ , 6 , 35893599 .",
    "montemayor , a. s. , pantrigo , j. j. , sanchez , a. , and fernandez , f. ( 2004 ) , `` particle filter on gpus for real time tracking , '' _ proceedings of the international conference on computer graphics and interactive techniques ( siggraph 04 ) _ , 94 , los angeles , ca , usa .",
    "montemerlo , m. , thrun , s. , koller , d. , and wegbreit , b. ( 2003 ) , `` fastslam 2.0 : an improved particle filtering algorithm for simultaneous localization and mapping that provably converges , '' _ proceedings of the 18th international joint conference on artificial intelligence _ , 11511157 ,",
    "acapulco , mexico .",
    "_ computational statistics & data analysis _ , 56 , 36903704"
  ],
  "abstract_text": [
    "<S> we developed a novel parallel algorithm for particle filtering ( and learning ) which is specifically designed for gpus ( graphics processing units ) or similar parallel computing devices . in our new algorithm </S>",
    "<S> , a full cycle of particle filtering ( computing the value of the likelihood for each particle , constructing the cumulative distribution function ( cdf ) for resampling , resampling the particles with the cdf , and propagating new particles for the next cycle ) can be executed in a massively parallel manner . </S>",
    "<S> one of the advantages of our algorithm is that every single numerical computation or memory access related to the particle filtering is executed solely inside the gpu , and no data transfer between the gpu s device memory and the cpu s host memory occurs unless it is under the absolute necessity of moving generated particles into the host memory for further data processing , so that it can circumvent the limited memory bandwidth between the gpu and the cpu . </S>",
    "<S> to demonstrate the advantage of our parallel algorithm , we conducted a monte carlo experiment in which we applied the parallel algorithm as well as conventional sequential algorithms for estimation of a simple state space model via particle learning , and compared them in terms of execution time . </S>",
    "<S> the results showed that the parallel algorithm was far superior to the sequential algorithm . + </S>",
    "<S> * keywords : * bayesian learning , gpgpu , parallel computing , particle filtering , state space model . </S>"
  ]
}