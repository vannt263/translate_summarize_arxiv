{
  "article_text": [
    "extraction of generalized parton distribution ( gpd ) functions @xcite from exclusive scattering data is an important endeavour , related to such practical questions as the partonic decomposition of the nucleon spin @xcite and characterization of multiple - hard reactions in proton - proton collisions at lhc collider @xcite . to reveal the shape of gpds , one employs global or local fits to data @xcite . however , compared to familiar global parton distribution ( pdf ) fits , fitting of gpds is intricate due to their dependence on three kinematical variables ( at fixed input scale @xmath0 ) , and the fact that they can not be fully constrained even by ideal data .",
    "thus , final results can be significantly influenced by the choice of the particular fitting ansatz . to deal with this source of theoretical uncertainties",
    ", we used an alternative approach @xcite , in which _ neural networks _",
    "are used in place of specific models .",
    "this approach has already been successfully applied to extraction of the deeply inelastic scattering ( dis ) structure function @xmath1 and normal pdfs @xcite .",
    "we expect that the power of this approach is even larger in the case of gpds . in the light of the scarce experimental data , in this pilot study we attempted the mathematically simpler extraction of form factor @xmath2 of deeply virtual compton scattering ( dvcs ) .",
    "we used data from the kinematical region where this compton form factor ( cff ) dominates the observables and depends essentially only on two kinematical variables : bjorken s scaling variable @xmath3 and proton momentum transfer squared @xmath4 .",
    "these simplifications make the whole problem more tractable .",
    "neural networks were invented some decades ago in an attempt to create computer algorithms that would be able to classify ( i.e.  recognize ) complex patterns .",
    "the specific neural network type used in this work , known as _ multilayer perceptron _",
    ", is a mathematical structure consisting of a number of interconnected `` neurons '' organized in several layers .",
    "it is schematically shown in fig .",
    "[ fig : perceptron ] , where each blob symbolizes a single neuron .",
    "each neuron has several inputs and one output .",
    "the value at the output is given as a function @xmath5 of a sum of input values @xmath6 , each weighted by a certain number @xmath7 .",
    "the parameters of a neural network ( weights @xmath7 ) are adjusted by a procedure known as `` training '' or `` learning '' .",
    "thereby , the input part of a chosen set of training input - output patterns is presented to the input layer and propagated through the network to the output layer .",
    "the output values are then compared to known values of the output part of training patterns and the calculated differences are used to adjust the network weights .",
    "this procedure is repeated until the network can correctly classify all ( or most of all ) input patterns . if this is done properly ,",
    "the trained neural network is capable of generalization , i.e. , it can successfully classify patterns it has never seen before .",
    "this whole paradigm can be applied also to fitting of functions to data . here ,",
    "measured data are the patterns , the input are the values of the kinematical variables the observable in question depends upon , and the output is the value of this observable , see fig .",
    "[ fig : perceptron ] . in this case",
    ", the generalization property of neural networks represents its ability to provide a reasonable estimate of the actual underlying physical law . for the particular application of neural networks to fits of hadron structure functions we refer the reader to papers of the nnpdf group @xcite .",
    "our approach is similar and is described in detail in @xcite .    to propagate experimental uncertainties into the final result",
    ", we use the `` monte carlo '' method @xcite , where neural networks are not trained on actual data but on a collection of `` replica data sets '' .",
    "these sets are obtained from original data by generating random artificial data points according to gaussian probability distribution with a width defined by the error bar of experimental measurements . taking a large number @xmath8 of such replicas",
    ", the resulting collection of trained neural networks @xmath9 defines a probability distribution @xmath10 $ ] of the represented cff @xmath2 and of any functional @xmath11 $ ] thereof .",
    "thus , the mean value of such a functional and its variance are @xcite @xmath12 \\big\\rangle & =   \\int \\mathcal{d}\\mathcal{h }   \\ : \\mathcal{p}[\\mathcal{h } ] \\ , \\mathcal{f}[\\mathcal{h } ]   \\nonumber \\\\   & =    \\frac{1}{n_{rep}}\\sum_{k=1}^{n_{rep } } \\mathcal{f}[\\mathcal{h}^{(k)}]\\ ; , \\label{eq : funcprob } \\\\",
    "\\big(\\delta \\mathcal{f}[\\mathcal{h}]\\big)^2 & = \\big\\langle \\mathcal{f}[\\mathcal{h}]^2 \\big\\rangle   - \\big\\langle \\mathcal{f}[\\mathcal{h } ] \\big\\rangle^2 \\;. \\label{eq : variance}\\end{aligned}\\ ] ]",
    "to illustrate the neural network fitting method , we shall now present a toy example where we will extract a known function of one variable by fitting to fake data .",
    "first we define some simple target function @xmath13 as a random composition of simple polynomial and logarithm functions constrained by the property @xmath14 this function is plotted in fig .",
    "[ fig : toy ] as a thick dashed line and labeled as `` target '' .",
    "next , @xmath15=10 fake data points @xmath16 are generated equidistantly in @xmath17 .",
    "their mean values @xmath18 are smeared around target values by random gaussian fluctuations with standard deviation @xmath19=0.05 , which is also taken to be the uncertainty of generated points .",
    "these fake data are then used for fits , first using the standard least - squares method with a two - parameter model @xmath20 and , second , utilizing the neural network method . note that the monte carlo method of error propagation , which we use together with neural network fitting , itself requires to generate artificial data sets .",
    "thus , we generated @xmath8=12 replicas from original fake data and used them to train 12 neural networks that represent 12 functions , plotted as thin solid lines on the second panel of fig .",
    "[ fig : toy ] .",
    "these functions define a probability distribution in the space of functions @xmath21 which , according to eqs .",
    "( [ eq : funcprob][eq : variance ] ) , provides an estimate of the sought function @xmath13 , together with its uncertainty .",
    "this estimate is shown on the right panel of fig .",
    "[ fig : toy ] as a ( red ) band with ascending hatches .",
    "the corresponding model fit result , obtained by the standard method of least - squares optimization and error propagation using the hessian matrix , is shown in the left panel of fig .",
    "[ fig : toy ] as a ( green ) band with descending hatches .",
    "we have deliberately chosen the ansatz ( [ eq : ansatz ] ) with two properties , incorporating theoretical biases about endpoints : @xmath22 and @xmath23 .",
    "the first of these actually `` corresponds to the truth '' , i.e. , to eq .",
    "( [ eq : endpoint ] ) , whereas the second one is erroneous . as a result ,",
    "for @xmath24 the model fit is in much better agreement with the target function ( thick dashed line ) than neural networks , which rely only on data and are insensitive to this endpoint behaviour . on the other side , for @xmath25",
    "the model fit is in some small disagreement with the target function , and , what is much worse , it very much underestimates the uncertainty of the fitted function there ( the uncertainty becomes zero at endpoints ! ) , demonstrating the dangers of unwarranted theoretical prejudices",
    ".    we can be more quantitative and say that according to the standard @xmath26 measure , @xmath27 both methods lead to functions that correctly describe data and the degrees of freedom  neural networks have very many free parameters and for them degrees of freedom is not such an important characteristic as in the case of standard model fits . ] : @xmath28 we can now further ask to what extent the two methods extract the underlying target function @xmath13 .",
    "naturally , we can measure this by a kind of @xmath29 criterion @xmath30 where the denominator is now the propagated uncertainty @xmath31 rather than the experimental one @xmath19 . in our toy example",
    "we get @xmath32 showing that the model fit underestimates its uncertainties , while neural networks are much more realistic .",
    "this example shows that the neural network method has a clear advantage if we want bias - free propagation of information from experimental measurements into the cffs .",
    "still , if we want to use some additional input , e.g. , if we rely on the spectral property ( [ eq : endpoint ] ) , we can do so also within the neural network method . for example",
    ", we could take the output of neural networks in this toy example not as an representation of the function @xmath21 itself , but as representing @xmath33 , with some positive power @xmath34 .",
    "then the final neural network predictions for @xmath21 would also be constrained by eq .",
    "( [ eq : endpoint ] ) , without any further loss of generality ( in practice it turns out that the dependence of the results on the choice of power @xmath34 is small ) .",
    "various methods of implementing theoretical constraints in the neural network fitting method are discussed in sect .",
    "5.2.4 of @xcite .",
    "to extract the cff @xmath35 from asymmetries @xcite , measured by the hermes collaboration in photon electroproduction off unpolarized protons , we applied the described neural network fitting method in @xcite .",
    "we used 36 data points : 18 measurements of the first sine harmonic @xmath36 of the beam spin asymmetry , and 18 measurements of the first cosine harmonic @xmath37 of the beam charge asymmetry .",
    "as for the toy model from the previous section , we compare the results with the standard least - squares model fit .",
    "let us first shortly describe this model fit of @xmath35 .",
    "for the partonic decomposition of the imaginary part @xmath38 we used a model , presented in @xcite : @xmath39 .\\ ] ] here , @xmath40 are gpds along the cross - over trajectory @xmath41 , parameterized as : @xmath42 the parameters of @xmath43 were fixed by separate fits @xcite to collider data , and some parameters of @xmath44 were also fixed using information from dis data and regge trajectories @xmath45 .",
    "the real part @xmath46 is expressed in terms of the imaginary one via a dispersion integral @xcite and the subtraction constant @xmath47 , leaving us finally with a model that possesses four parameters : @xmath48 , @xmath49 , @xmath50 and @xmath47 .",
    "this model is fitted to experimental data , resulting in parameter values , which can be found in @xcite , and shapes of @xmath38 and @xmath46 that are plotted on fig .",
    "[ fig : cff ] as ( green ) bands with descending hatches .",
    "the neural network fit was performed by creating 50 neural networks with two neurons in the input layer ( corresponding to kinematical variables @xmath3 and @xmath4 ) , 13 neurons in the hidden middle layer , and two neurons in the output layer ( corresponding to @xmath38 and @xmath46 ) , cf .",
    "[ fig : perceptron ] .",
    "these were trained on @xmath8=50 monte carlo replicas of hermes data .",
    "we checked that the resulting cff @xmath51 does not depend significantly on the precise number of neurons in the hidden layer .",
    "the results are also presented on fig .",
    "[ fig : cff ] , where we show the neural network representation of @xmath38 and @xmath46 as ( red ) bands with ascending hatches .    comparing the two approaches ,",
    "one notices that in the kinematic region of experimental data ( roughly the middle-@xmath3 parts of fig .",
    "[ fig : cff ] panels ) neural network and model fit results coincide , i.e. , error bands are of similar width and they overlap consistently .",
    "however , outside of this data region , we see that the predictions of the two approaches can be different . there",
    "the uncertainty of the model fit is in general smaller , and we observe a strong disagreement in the low @xmath3 region , reflecting the theoretical bias of the chosen model that possesses a @xmath52 regge behavior .",
    "the lesson learned from the toy model example is that , even if we believe in regge behaviour for small @xmath3 , we should still consider the uncertainty from the neural network method as more realistic .",
    "utilizing both a simplified toy example and hermes measurements of photon electroproduction asymmetries , we demonstrated that neural networks and monte carlo error propagation provide a powerful and unbiased tool that extracts information from data .",
    "comparisons with standard least - squares model fits reveal that the uncertainties , obtained from neural network fits , are reliable and realistic .",
    "relying on the hypothesis of @xmath35 dominance , we found the cff @xmath35 from a completely unconstrained neural network fit .",
    "it is expected that the extraction of all four leading twist - two cffs ( @xmath35 , @xmath53 , @xmath54 and @xmath55 , or the corresponding gpds ) from presently or soon - to - be available data will still be an ill - defined optimization problem .",
    "thus , it might be necessary to implement in neural network fits some carefully chosen theoretically robust constraints , such as dispersion relations , sum rules @xcite and lattice input .",
    "this work was supported by the bmbf grant under the contract no .",
    "06ry9191 , by eu fp7 grant hadronphysics2 , by dfg grant , contract no .",
    "436 kro 113/11/0 - 1 and by croatian ministry of science , education and sport , contract no .",
    "119 - 0982930 - 1016 ."
  ],
  "abstract_text": [
    "<S> we describe a method , based on neural networks , of revealing compton form factors in the deeply virtual region . </S>",
    "<S> we compare this approach to standard least - squares model fitting both for a simplified toy case and for hermes data . </S>"
  ]
}