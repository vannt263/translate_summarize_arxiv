{
  "article_text": [
    "in _ reinforcement learning _ ( rl ) @xcite , an agent seeks an optimal control policy for a sequential decision problem in an initially unknown environment .",
    "the environment provides feedback on the agent s behavior in the form of a reward signal .",
    "the agent s goal is to maximize the expected _ return _ , which is the discounted sum of rewards over future timesteps .",
    "an important performance measure in rl is the _ sample efficiency _ , which refers to the number of environment interactions that is required to obtain a good policy .",
    "many solution strategies improve the policy by iteratively improving a _ state - value _ or _ action - value function _ , which provide estimates of the expected return under a given policy for ( environment ) states or state - action pairs , respectively .",
    "different approaches for updating these value functions exist . in terms of sample efficiency ,",
    "one of the most effective approaches is to estimate the environment model using observed samples and to compute , at each time step , the value function that is optimal with respect to the model estimate using planning techniques .",
    "a popular planning technique used for this is _ value iteration _ ( vi )",
    "@xcite , which performs sweeps of backups through the state or state - action space , until the ( action-)value function has converged .",
    "a drawback of using vi is that it is computationally very expensive , making it infeasible for many practical applications .",
    "fortunately , efficient approximations can be obtained by limiting the number of backups that is performed per timestep .",
    "a very effective approximation strategy is _ prioritized sweeping _",
    "@xcite , which prioritizes backups that are expected to cause large value changes .",
    "this paper introduces a new backup that enables a dramatic improvement in the efficiency of prioritized sweeping .    the main idea behind",
    "this new backup is as following .",
    "consider that we are interested in some estimate @xmath0 that is constructed from a sum of other estimates @xmath1 .",
    "the estimate @xmath0 can be computed using a _ full backup _ :",
    "@xmath2 if the estimates @xmath1 are updated , @xmath0 can be recomputed by redoing the above backup . alternatively",
    ", if we know that only @xmath3 received a significant value change , we might want to update @xmath0 for only @xmath3 .",
    "let us indicate the old value of @xmath3 , used to construct the current value of @xmath0 , as @xmath4 .",
    "@xmath0 can then be updated by subtracting this old value and adding the new value : @xmath5    this kind of backup , which we call a _ small backup _",
    ", is computationally cheaper than the full backup .",
    "the trade - off is that , in general , more memory is required for storing the estimates @xmath6 associated with @xmath0 . in planning , where the @xmath7 estimates correspond to state - value estimates and @xmath0 corresponds to a state or state - action estimate , this is not a serious restriction , because a full model is stored already .",
    "the additional memory required has the same order of complexity as the memory required for storage of the model .",
    "the core advantage of small backups over full backups is that they enable finer control over the planning process .",
    "this allows for more effective update strategies , resulting in improved trade - offs between computation time and quality of approximation of the vi solution ( and hence sample efficiency ) .",
    "we demonstrate this empirically by showing that a prioritized sweeping implementation based on small backups yields a substantial performance improvement over the two classical implementations @xcite .",
    "in addition , we demonstrate the relevance of small backups in domains with severe constraints on computation time , by showing that a method that performs one small backup per time step has an equal computation time complexity as td(0 ) , the classical method that performs one _ sample backup _ per timestep .",
    "since sample backups introduce sampling variance , they require a step - size parameter to be tuned for optimal performance .",
    "small backups , on the other hand , do not introduce sampling variance , allowing for a parameter - free implementation .",
    "we empirically demonstrate that the performance of a method that performs one small backup per time step is similar to the optimal performance of td(0 ) , achieved by carefully tuning the step - size parameter .",
    "rl problems are often formalized as _ markov decision processes _ ( mdps ) , which can be described as tuples @xmath8 consisting of @xmath9 , the set of all states ; @xmath10 , the set of all actions ; @xmath11 , the transition probability from state @xmath12 to state @xmath13 when action @xmath14 is taken ; @xmath15 , the reward function giving the expected reward @xmath16 when action @xmath17 is taken in state @xmath18 ; and @xmath19 , the discount factor controlling the weight of future rewards versus that of the immediate reward .",
    "actions are selected at discrete timesteps @xmath20 according to a _",
    "@xmath21 $ ] , which defines for each action the selection probability conditioned on the state .",
    "in general , the goal of rl is to improve the policy in order to increase the _ return _ @xmath22 , which is the discounted cumulative reward @xmath23 where @xmath24 is the reward received after taking action @xmath25 in state @xmath26 at timestep @xmath27 .",
    "the _ prediction task _ consists of determining the _ value function _",
    "@xmath28 , which gives the expected return when policy @xmath29 is followed , starting from state @xmath18 .",
    "@xmath28 can be found by making use of the _ bellman equations _ for state values , which state the following : @xmath30 where @xmath31 and @xmath32 .    _ model - based methods _ use samples to update estimates of the transition probabilities , @xmath33 , and reward function , @xmath34 . with these estimates",
    ", they can iteratively improve an estimate @xmath35 of @xmath36 , by performing _ full backups _",
    ", derived from equation ( [ eq : bellman_state ] ) : @xmath37    in the _ control task _ , methods often aim to find the optimal policy @xmath38 , which maximizes the expected return .",
    "this policy is the greedy policy with respect to the optimal _ action - value function _",
    "@xmath39 , which gives the expected return when taking action @xmath17 in state @xmath18 , and following @xmath38 thereafter .",
    "this function is the solution to the _ bellman optimality equation _ for action - values : @xmath40 the optimal value function is related to the optimal action - value function through : @xmath41 .",
    "model - based methods can iteratively improve estimates @xmath42 of @xmath43 by performing full backups derived from equation ( [ eq : belllman_action ] ) : @xmath44 where @xmath45 and @xmath46 are estimates of @xmath47 and @xmath48 , respectively .",
    "_ model - free methods _ do not maintain an model estimate , but update a value function directly from samples .",
    "a classical example of a _ sample backup _ , based on sample @xmath49 is the td(0 ) backup : @xmath50 where @xmath51 is the step - size parameter .",
    "this section introduces the small backup .",
    "we start with small state - value backups for the prediction task . section [ sec : action - value backups ] discusses small action - value backups for the control task .      in this section",
    ", we introduce a small backup version of the full backup for prediction ( backup [ eq : full value backup ] ) . in the introduction",
    ", we showed that a small backup requires storage of the component values that make up the current value of a variable . in the case of a small value backup ,",
    "the component values correspond to the values of successor states .",
    "we indicate these values by the function @xmath52 .",
    "so , @xmath53 is the value estimate of @xmath54 associated with @xmath18 .",
    "using @xmath55 , @xmath56 can be updated with only the current value of a single successor state , @xmath54 , as demonstrated by the following theorem .",
    "the three backups shown in the theorem form together the small backup .",
    "if the current relation between @xmath56 and @xmath55 is given by @xmath57 then , after performing the following backups : @xmath58 \\quad \\label{eq : small_2}\\\\ u_{s}(s ' ) & \\leftarrow & tmp \\label{eq : small_3}\\thinspace,\\end{aligned}\\ ] ] relation ( [ eq : v_vs_relation ] ) still holds , but @xmath59 is updated to @xmath60.[th : value theorem ]    backup ( [ eq : small_2 ] ) subtract the component in relation ( [ eq : v_vs_relation ] ) corresponding to @xmath54 from @xmath56 and adds a new component based on the current value estimate of @xmath54 : @xmath61 hence , relation ( [ eq : v_vs_relation ] ) is maintained , while @xmath53 is updated .",
    "note that @xmath60 needs to be stored in a temporary variable , since backup ( [ eq : small_2 ] ) can alter the value of @xmath60 if @xmath62 .",
    "< 1.5em - 1.5em plus0em minus0.5em height0.75em width0.5em depth0.25em      theorem [ th : value theorem ] relies on relation ( [ eq : v_vs_relation ] ) to hold .",
    "if the model gets updated , this relation now longer holds . in this section ,",
    "we discuss how to restore relation ( [ eq : v_vs_relation ] ) in a computation - efficient way for the commonly used model estimate : @xmath63 where @xmath64 counts the number of times state @xmath18 is visited , @xmath65 counts the number of times @xmath54 is observed as successor state of @xmath18 , and @xmath66 is the sum of observed rewards for @xmath18 .",
    "if currently , the following relation holds : @xmath67 and a sample @xmath49 is observed , then , after performing the backups : @xmath68/n_{s}\\thinspace .",
    "\\quad\\quad \\label{eq : value correction}\\end{aligned}\\ ] ] the relation still holds , but with updated values for @xmath34 and @xmath69 .",
    "backup ( [ eq : value correction ] ) updates @xmath56 by computing a weighted average of @xmath56 and @xmath70 .",
    "the value change this causes is the same as the value change caused by updating the model and then performing a full backup of @xmath18 based on @xmath55 .",
    "< 1.5em - 1.5em plus0em minus0.5em height0.75em width0.5em depth0.25em    algorithm [ al : small backup evaluation ] shows pseudo - code for a general class of prediction methods based on small backups .",
    "surpisngly , while it is a planning method , @xmath71 is never explicitly computed , saving time and memory .",
    "note that the computation per time step is fully independent of the number of successor states .",
    "members of this class need to specify the number of iterations ( line 8) as well as a strategy for selecting state - successor pairs ( line 9 ) .",
    "initialize @xmath56 arbitrarily for all @xmath18 initialize @xmath72 for all @xmath73 initialize @xmath74 to 0 for all @xmath73 observe transition @xmath49 @xmath75 @xmath76/n_{s}$ ] select a pair @xmath77 with @xmath78 @xmath79 @xmath80 $ ] @xmath81      before we can discuss small action - value backups , we have to discuss a more efficient implementation of the full action - value backup .",
    "backup ( [ eq : full action - value backup ] ) has a computation time complexity of @xmath82 .",
    "a more efficient implementation can be obtained by storing state - values , besides action - values , according to @xmath83 .",
    "backup ( [ eq : full action - value backup ] ) can then be implemented by : @xmath84 the combined computation time of these backups is @xmath85 , a considerable reduction .",
    "backup ( [ eq : qv ] ) is similar in form as the prediction backup .",
    "hence , we can make a small backup version of it similar to the one in the prediction case .",
    "the theorems below are the control versions of the theorems for the prediction case .",
    "they can be proven in a similar way as the prediction theorems .    if the current relation between @xmath86 and @xmath87 is given by @xmath88 then , performing the following backups : @xmath89\\\\ u_{sa}(s ' ) & \\leftarrow & v(s')\\thinspace,\\end{aligned}\\ ] ] maintains this relation while updating @xmath90 to @xmath60 .",
    "if relation ( [ eq : relationqv ] ) holds and a sample @xmath91 is observed , then , after performing backups @xmath92/n_{sa}\\thinspace , \\end{aligned}\\ ] ] relation ( [ eq : relationqv ] ) still holds , but with updated values for @xmath45 and @xmath93 .",
    "a small action - value backup is a finer - grained version of backup ( [ eq : qv ] ) : performing a small backup of @xmath86 for each successor state is equivalent ( in computation time complexity and effect ) as performing backup ( [ eq : qv ] ) once .",
    "while in principle , backup ( [ eq : vq ] ) can be performed after each small backup , it is not very efficient to do so , since small backups make many small changes . more efficient",
    "planning can be obtained when backup ( [ eq : vq ] ) is performed only once in a while .    in section [ sec : ps small backups ] , we discuss an implementation of prioritized sweeping based on small action - value backups .",
    "a small backup has in common with a sample backup that both update a state value based on the current value of only one of the successor states .",
    "in addition , they share the same computation time complexity and their effect is in general smaller than that of a full backup .",
    "a disadvantage of a sample backup , with respect to a small backup , is that it introduces sampling variance , caused by a stochastic environment .",
    "this requires the use of a step - size parameter to enable averaging over successor states ( and rewards ) .",
    "a small backup does not introduce sampling variance , since it is implicitly based on an expectation over successor states .",
    "hence , it does not require tuning of a step - size parameter for optimal performance .",
    "a second disadvantage of a sample backup is that it affects the perceived distribution over action outcomes , which places some restrictions on reusing samples .",
    "for example , a model - free technique like experience replay @xcite , which stores experience samples in order to replay them at a later time , can introduce bias , which reduces performance , if some samples are replayed more often than others . for small backups",
    "this does not hold , since the process of learning the model is independent from the backups based on the model .",
    "this allows small backups to be combined with effective selection strategies like prioritized sweeping .",
    "prioritized sweeping ( ps ) makes the planning step of model - based rl more efficient by using a heuristic ( a ` priority ' ) for selecting backups that favours backups that are expected to cause a large value change .",
    "a priority queue is maintained that determines which values are next in line for receiving backups .",
    "there are two main implementations : one by @xcite and one by @xcite .",
    "all ps methods have in common that they perform backups in what we call _ update cycles_. by adjusting the number of update cycles that is performed per time step , the computation time per time step can be controlled .",
    "below , we discuss in detail what occurs in an update cycle for the two classical ps implementations .      in the moore & atkeson implementation",
    "the elements in the queue are states and the backups are full value backups . in control ,",
    "a full value backup is different from backup ( [ eq : full value backup ] ) .",
    "instead , it is equivalent ( in effect and computation time ) to performing backup ( [ eq : qv ] ) for each action , followed by backup ( [ eq : vq ] ) .",
    "hence , the associated computation time has complexity @xmath94 .",
    "an update cycle consists of the following steps .",
    "first , the top state is removed from the queue , and receives a full value backup .",
    "let @xmath18 bet the top state and @xmath95 the value change caused by the backup .",
    "then , for all predecessor state - action pairs @xmath96 a priority @xmath97 is computed , using : @xmath98 if @xmath99 is not yet on the queue , then it is added with priority @xmath97 .",
    "if @xmath99 is on the queue already , but its current priority is smaller than @xmath97 , then the priority of @xmath99 is upgraded to @xmath97 .",
    "the peng & williams implementation differs from the moore & atkeson implementation in that the backup is not a full value backup .",
    "instead , it is a backup with the same effect as a small action - value backup , but with a computational complexity of @xmath85 .",
    "so , it is a cheaper backup than a full backup , but its value change is ( much ) smaller . the backup requires a state - action - successor triple .",
    "hence , these triples are the elements on the queue .",
    "predecessors are added to the queue with a priorities that estimate the action - value change .",
    "a natural small backup implementation might appear to be an implementation similar to that of peng & williams , but with the main backup implemented more efficiently .",
    "the low computational cost of a small backup , however , allows for a much more powerful implementation .",
    "the pseudo - code of this implementation is shown in algorithm [ al : ps small backups ] .",
    "below , we discuss some key characteristics of the algorithm .",
    "the computation time of a small backup is so low , that it is comparable to the priority computation in the classical ps implementations .",
    "therefore , instead of computing a priority for each predecessor and performing a backup for the element with the highest priority in the next update cycle , we can perform a small backup for all predecessors .",
    "this raises the question of what to put in the priority queue and what type of backup to perform for the top element .",
    "the natural answer is to put states in the priority queue and to perform backup ( [ eq : vq ] ) for the top state .",
    "the priority associated with a state is based on the change in action - value that has occurred due to small backups , since the last value backup .",
    "this priority assures that states with a large discrepancy between the state value and action - values , receive a value backup first .",
    "one surprising aspect of the algorithm is that it does not use the function @xmath87 , which forms an essential part of small action - value backups .",
    "the reason is that due to the specific backup strategy used by the algorithm , @xmath90 is equal to @xmath60 for all state - action pairs @xmath100 and all successor states @xmath54 .",
    "hence , instead of using @xmath87 , @xmath35 can be used , saving memory and simplifying the code .",
    "table [ table : computation time ] shows the computation time complexity of an update cycle for the different ps implementations .",
    "the small backup implementation is the cheapest one among the three .",
    ".computation time associated with one update cycle for the different ps implementations .",
    "@xmath101 indicates the number of predecessors , state - action pairs that transition to the state whose value has just been updated .",
    "[ cols= \" > , > , > \" , ]     initialize @xmath56 arbitrarily for all @xmath18 initialize @xmath102 for all @xmath103 initialize @xmath104 to 0 for all @xmath105 initialize @xmath18 select action @xmath17 , based on @xmath106 take action @xmath17 , observe @xmath16 and @xmath54 @xmath107 @xmath108/n_{sa}$ ] @xmath109 if @xmath18 not on queue or @xmath110 current priority @xmath18 , then promote @xmath99 to @xmath97 remove top state @xmath111 from queue for all @xmath112 : @xmath113 @xmath79 @xmath114 @xmath115 @xmath116 @xmath117 if @xmath99 not on queue or @xmath110 current priority @xmath99 , then promote @xmath99 to @xmath97 @xmath118",
    "in this section , we evaluate the performance of a minimal version of algorithm [ al : small backup evaluation ] , as well as the performance of algorithm [ al : ps small backups ] .",
    "we compare the performance of td(0 ) , which performs one sample backup per time step , with a version of algorithm 1 that performs one small backup per time step .",
    "specifically , its number of iterations ( line 8) is 1 , and the selected state - successor pair ( line 9 ) is the pair corresponding to the most recent transition .",
    "their performance is compared on two evaluation tasks , both consisting of 10 states , laid out in a circle .",
    "state transitions only occur between neighbours .",
    "the transition probabilities for both tasks are generated by a random process .",
    "specifically , the transition probability to a neighbour state is generated by a random number between 0 and 1 and normalized such that the sum of the transition probabilities to the left and right neighbour is 1 .",
    "the reward for counter - clockwise transitions is always + 1 .",
    "the reward for clockwise transitions is different for the two tasks . in the first task , a clockwise transition results in a reward of -1 ; in the second task , it results in a reward of + 1 .",
    "the discount factor @xmath19 is 0.95 and the initial state values are 0 .    for td(0 ) , we performed experiments with a constant step - size for values between 0 and 1 with steps of 0.02 .",
    "in addition , we performed experiments with a decaying , state - dependent step - size , according to @xmath119 where @xmath120 is the number of times state @xmath18 has been visited , and @xmath121 specifies the _ decay rate_. we used values of @xmath121 between 0 and 1 with steps of 0.02 .",
    "note that for @xmath122 , @xmath123 , and for @xmath124 , @xmath125 .",
    "each time a transition is observed and the corresponding backup is performed , the root - mean squared ( rms ) error over all states is determined . the average rms error over the first 10.000 transitions , normalized with the initial error , determines the performance .",
    "figure [ fig : eval_results ] shows this performance , averaged over 100 runs .",
    "the standard error is negligible : the maximum standard error in the first task was 0.0057 ( after normalization ) and in the second task 0.0007 .",
    "note that the performance for @xmath126 is equal to the performance for @xmath127 , as it should , by definition .",
    "the normalized performance for @xmath128 is 1 , since no learning occurs in this case .",
    "these experiments demonstrate three things .",
    "first , the optimal step - size can vary a lot between different tasks .",
    "second , selecting a sub - optimal step - size can cause large performance drops .",
    "third , a small - backup , which is parameter - free , has a performance similar to the performance of td(0 ) with optimized step - size .",
    "since the computational complexity is the same , the small backup is a very interesting alternative to the sample backup in domains with tight constraints on the computation time , where previously only sample backups where viable .",
    "keep in mind that a sample backup does require a model estimate , so if there are also tight constraints on the memory , a sample backup might still be the only option .",
    ", in case of constant step - size , or different values of the decay parameter @xmath121 , in case of decaying step - size .",
    "the top graph corresponds with the first evaluation task ; the bottom graph with the second.,title=\"fig : \" ] , in case of constant step - size , or different values of the decay parameter @xmath121 , in case of decaying step - size .",
    "the top graph corresponds with the first evaluation task ; the bottom graph with the second.,title=\"fig : \" ]      we compare the performance of prioritized sweeping with small backups ( algorithm [ al : ps small backups ] ) with the two classical implementations of moore&atkeson and peng&williams on the maze task depicted in the top of figure [ fig : largemaze2 ] .",
    "the reward received at each time step is -1 and the discount factor is 0.99 . the agent can take four actions , corresponding to the four compass directions , which stochastically move the agent to a different square . the bottom of figure [ fig : largemaze2 ] shows the relative action outcomes of a ` north ' action . in free space",
    ", an action can result in 15 possible successor states , each with equal probability .",
    "when the agent is close to a wall , this number decreases .    to obtain an upper bound on the performance , we also compared against a method that performs value iteration ( until convergence ) at each time step , using the most recent model estimate .",
    "as exploration strategy , the agent select with 5% probability a random action , instead of the greedy one . on top of that",
    ", we use the ` optimism in the face of uncertainty ' principle , as also used by moore & atkeson .",
    "this means that as long as a state - action pair has not been visited for at least m times , it s value is defined as some optimistically defined value ( 0 for our maze task ) , instead of the value based on the model estimate .",
    "we optimized @xmath129 for the value iteration method , resulting in @xmath130 , and used this value for all methods .",
    "we performed experiments for 1 , 3 , 5 and 10 update cycles per time step .",
    "figure [ fig : ps_results ] shows the average return over the first 200 episodes for the different methods .",
    "the results are averaged over 100 runs .",
    "the maximum standard deviation is 0.1 for all methods , except for the method of peng & williams , which had a maximum standard deviation of 1.0 .",
    "the computation time per update cycle was about the same for the three different ps implementations , with a small advantage for the small backup implementation , which shows that the @xmath131 computation ( see table [ table : computation time ] ) is dominant in this task .",
    "the computation time per observation of the value iteration method was more than 400 times as high as a single update cycle .",
    "ps with small backups turns out to be very effective . with only a single update cycle",
    ", the value - iteration result can be closely approximated , in contrast to the two classical implementations .",
    "the results also show that the peng & williams method performs considerably worse than the one of moore & atkeson in the considered domain .",
    "this can be explained by the different backups they perform .",
    "the effect of the backup of peng & williams is proportional to the transition probability , which in most cases is @xmath132 .",
    "in contrast , the moore & atkeson method performs a full backup each update cycle .",
    "while the small backup implementation also uses backups that are proportional to the transition probability , it performs a lot more backups per update cycle . specifically , a number that is proportional to the number of predecessors . in general",
    ", this number will increase when the stochasticity of the domain increases .",
    "tothe @xmath22 .",
    "below , transition probabilities ( @xmath133 ) of a ` north ' action for different positions of the agent ( indicated by the circle ) with respect to the walls ( black squares).,title=\"fig:\",height=5 ]   tothe @xmath22 .",
    "below , transition probabilities ( @xmath133 ) of a ` north ' action for different positions of the agent ( indicated by the circle ) with respect to the walls ( black squares).,title=\"fig:\",width=6 ]",
    "prioritized sweeping can be viewed as a generalization of the idea of replaying of experience in backward order @xcite , which by itself is related to _ eligibility traces _",
    "what all these techniques have in common is that new information ( which can be value changes , but at its core all value changes originate from new data ) is propagated backwards . whereas backward replay and eligibility traces use the recent trajectory for backward propagation of information , prioritized sweeping uses a model estimate for this .",
    "hence , it propagates new information more broadly .",
    "what gives the performance edge to the small backup implementation is that it implements the principle of backward updating in a cleaner and more efficient way .",
    "one update cycle of algorithm [ al : ps small backups ] represents , in a way , the ultimate backwards backup : all predecessors are updated with the current value of a chosen state , which is selected because it recently experienced a large value change .",
    "in contrast , the other ps implementation place the predecessors in a queue and backup only the state with the highest priority in the next update cycle . on top of that , the computation time per update cycle is lower for the small backup implementation ( see table [ table : computation time ] ) .",
    "the new implementation of ps introduced in this paper would be impossible without the new backup .",
    "the small backup allows for very targeted updates that are computationally very cheap .",
    "this enables finer control over how computation time is spend , which is what drives the new ps implementation .",
    "we demonstrated in this paper that the planning step in model - based reinforcement learning method can be done substantially more efficient by making use of small backups .",
    "these backups are finer - grained version of a full backup , which allow for more control over how the available computation time is spend .",
    "this makes new , more efficient , update strategies possible . in addition",
    ", small backups can be useful in domains with very tight time constraints , offering a parameter - free alternative to sample backups , which were up to now often the only feasible option for such domains ."
  ],
  "abstract_text": [
    "<S> efficient planning plays a crucial role in model - based reinforcement learning . traditionally , the main planning operation is a full backup based on the current estimates of the successor states . </S>",
    "<S> consequently , its computation time is proportional to the number of successor states . in this paper </S>",
    "<S> , we introduce a new planning backup that uses only the current value of a single successor state and has a computation time independent of the number of successor states . </S>",
    "<S> this new backup , which we call a _ small backup _ , opens the door to a new class of model - based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods . </S>",
    "<S> we empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations . </S>"
  ]
}