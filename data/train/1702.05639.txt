{
  "article_text": [
    "in the past decade , deep neural networks ( dnns ) have received extensively considerable attention due to its great potential for dealing with computer vision , pattern recognition , natural language processing , etc . @xcite .",
    "the success of deep learning is attributed to its representation capability for visual data @xcite . in view of some empirical evidence",
    ", dnns are becoming increasingly popular because of a hypothesis that a deep learner model can be exponentially more efficient at representing some functions than a shallow one @xcite .",
    "it seems that dnns have a good potential in learning higher - level abstractions ( or multiple levels of features ) , which sounds difficult for models with shallow architecture .",
    "formal analyses of the representation power and learning complexity of dnns can be found in @xcite . in short ,",
    "the community has reached a common sense that dnns are much more expressive than the shallow ones .",
    "although deep learning schemes draw tremendous attention for their overwhelming high performance for some complex data modelling tasks @xcite , two key issues should be concerned seriously in model design : the architecture determination for dnns and the training strategies for deep architectures .",
    "as usually done , the number of layers and of the nodes at each layer of a dnn are set manually , which poses great threats on the effectiveness of the resulting model .",
    "this is because dnns with moderate depth but fewer number of hidden nodes at each layer can not exhibit sufficient learning ability ( i.e. , under fitting ) , while the large sized architecture ( i.e. with quite a few layers and hidden nodes ) may cause over - fitting that degrades the generalization capability .",
    "also , it leads to unnecessary time cost or even unsatisfactory deep models , if end - users determine the architecture by trial and error approaches .",
    "in @xcite , alvarez and salzmann introduced an approach to automatically determine the number of nodes at each layer of the dnn by using group sparsity regularizers .",
    "their method can result in a compact width setting for each layer , but with the prerequisite that the network s depth is determined in advance . except for the architecture setting problem ,",
    "fast training algorithms for dnns are always welcome .",
    "indeed , training standard multilayer perceptrons by the back - propagation algorithm comes up with a complex non - convex optimization problem , and the simultaneous learning of all parameters leads to a high memory and computation cost for very deep models .",
    "empirically , this gradient - based learning method is less effective for representing highly non - linear and highly - varying functions , since the learning curve easily gets stuck in local minima or lost in plateaus .",
    "these obstacles also lie in the process of fine - tuning a pre - trained model obtained by the greedy layer - wise unsupervised learning strategy proposed in @xcite , as the parameters within all layers need to be optimized according to to an objective cost function . to the best of our knowledge on data sciences",
    ", deep learning techniques are bravely and blindly generating dnns with bulk of data , but with less art on architecture design and fast implementation .",
    "motivated by constructive approaches for building shallow neural networks and randomized methods for fast learning , we make efforts to develop randomized learner model with deep architecture .",
    "this paper is built on recent work reported in @xcite where the way used to construct shallow neural networks with randomness ( termed as stochastic configuration networks , scns ) is original , innovative and effective .",
    "extension of scns to deep version is not trivial , because it needs to prove that deepscns share the universal approximation property , and also define some proper and meaningful criteria on node adding , layer growth , and construction termination throughout the learning process .",
    "briefly , a deepscn starts with a small sized network ( e.g , one hidden layer with one hidden node ) , and stochastically configures its nodes for the current layer until the hidden output matrix fails the full - rank condition , then continues to add the next hidden layer by repeating procedures as done in the last layer , keeps proceeding to deeper layers until an acceptable error tolerance is achieved .",
    "specifically , as the constructive process proceeds , the hidden parameters are randomly assigned under a supervisory mechanism , while the read - out weights linking the hidden nodes to the output nodes are calculated by the least squares method .",
    "the success of scns can guarantee the convergence of error sequence approaching to certain accuracy , if a moderate number of hidden nodes are generated @xcite . once starting to add a new hidden layer , the last error function acts as a new target function to be approximated .",
    "an immediate benefit from deepscns lies in the links between all hidden nodes and the output nodes , which allow end - users to manipulate layer - wise approximations , in other words , one can make options to use these nodes freely after off - line building an accurate deepscn model .",
    "our work focuses on framework development for randomized dnns rather than addresses any specific applications , aiming to offer an advanced solution for the aforementioned challenges .",
    "technical contributions are summarized as follows :    * establish the universal approximation theory for deepscns ; * determine the architecture of deepscns automatically without human interventions , and use full - rank based criterion to guide the deployment of deepscns ; * implement fast learning by stochastically assigning the hidden parameters and evaluating the read - out weights by least squares method .",
    "the remainder of this paper is organized as follows : section 2 reviews some related work concerning randomness in dnns and optimization - based learning techniques .",
    "section 3 details deepscns framework with theoretical fundamentals on the universal approximation property , learning representation and algorithmic implementation .",
    "section 4 reports some simulation results with comparisons against scns .",
    "section 5 concludes this paper and provides some further research directions .",
    "some researchers anticipated the usefulness of randomization in the development of neural networks @xcite . in @xcite , it was found that a random filter in convolutional neural networks can perform slightly worse than a well - designed filter , which usually needs pre - training and discriminative fine - tuning .",
    "saxe et al . addressed this issue in @xcite and showed that the results obtained from the randomized learner are comparable to that after regular pre - training and fine - tuning processes .",
    "these experimental results imply that the involvement of randomization in deep architectures has a good potential in enhancing the computational efficiency of the state - of - the - art systems without a significant deterioration in performance .",
    "coates and ng @xcite tried to use random weights in unsupervised learning , and their experimental results suggested that randomization can be helpful to build large sized models very rapidly , with much ease in training and encoding than sparse coding techniques .",
    "randomness was also concerned in the selection of local receptive fields @xcite , and the pooling operations of deep convolutional neural networks @xcite .",
    "arora et al .",
    "investigated the learning of autoencoders with random weights and demonstrated that it is possible to train them in polynomial time under some restrictions on the network depth @xcite . to speed the training process of back - propagation ( bp ) in dnns ,",
    "authors in @xcite proposed a random feedback mechanism by multiplying error signals with random weights , which contributes to fast extracting useful information from signals sent through the connections .",
    "motivated by a series of works reported in @xcite , where they empirically showed some successful learning techniques based on randomization , giryes et al .",
    "@xcite theoretically proved that dnns with random gaussian weights can perform a stable embedding of the original data , permitting a stable recovery of the data from the learning representation . in @xcite",
    ", the authors stated that the capacity of multilayer perceptrons mainly depends on the number of nodes in the last hidden layer and associated output weights .",
    "indeed , their method came up with the same philosophy of random projection for dimensionality reduction @xcite . in @xcite ,",
    "some investigations on the functionality of randomized dnns for deep visualization were empirically conducted .",
    "their findings demonstrate good potential for convolutional neural networks with random weights on visualization .    in @xcite ,",
    "a constructive algorithm was proposed for building cascaded networks , where all hidden nodes are directly linked to the output nodes . at each layer ,",
    "the nodes are successively added by optimizing an objective function ( equivalent to searching for the hidden parameters ) .",
    "this suffers from all troubles occurring in gradient - based methods .",
    "the used criterion on layer growth is heuristic and lacks association with model s generalization . in @xcite , the bp - like algorithm was developed for training this type of cascaded networks with specified architecture . however ,",
    "their approaches are not efficient for building dnns and require more human intervention in terms of learning parameter setting .",
    "this section provides some fundamentals of deepscns , including a theoretical result on the universal approximation property , interpretation on learning representation , algorithm description and some technical remarks .",
    "let @xmath0 denote the space of all lebesgue - measurable vector - valued functions @xmath1:r^{d}\\rightarrow r^{m}$ ] on a compact set @xmath2 , with the @xmath3 norm defined as @xmath4 and inner product defined as @xmath5 where @xmath6:r^{d}\\rightarrow r^{m}$ ]",
    ".    given a target function @xmath7 , suppose a deepscn with @xmath8 hidden layers and each layer has @xmath9 hidden nodes ( @xmath10 ) , has been constructed , that is , @xmath11 where @xmath12 and @xmath13 stand for the hidden parameters within the @xmath14-th hidden layer , @xmath15 is the activation function used in the @xmath14-th hidden layer , @xmath16 and @xmath17 $ ] , with @xmath18 $ ] and @xmath19 $ ] , @xmath10 .",
    "+ then , the residual error function is defined by @xmath20 $ ] .",
    "specifically , @xmath21 and @xmath22 , @xmath23 +   + the universal approximation property of deepscns is stated in the following theorem 1",
    ".    * theorem 1 . *",
    "suppose that span(@xmath24 ) is dense in @xmath25 space and @xmath26 , @xmath27 for some @xmath28 .",
    "given @xmath29 and a nonnegative decreasing sequence @xmath30 with @xmath31 and @xmath32 .",
    "for @xmath33 , and @xmath34 , denoted by @xmath35 stochastically configuring the @xmath36-th hidden node @xmath37 ( within the @xmath8-th hidden layer ) to satisfy the following inequality : @xmath38 if the random basis functions @xmath39 are linearly dependent on a compact set @xmath40 , i.e. , for some real numbers @xmath41 not all zero , the following holds @xmath42 fix @xmath43 and start to add the first hidden node @xmath44 in the @xmath45-th hidden layer according to the following inequality @xmath46 keep adding new hidden nodes within the @xmath45-th hidden layer based on ( [ step2 ] ) , followed by generating the first hidden node in a new hidden layer via ( [ step3 ] ) ( as ( [ linear_ind ] ) holds ) . after adding one hidden node ( either in the present hidden layer or starting a new hidden layer ) , the read - out weights are evaluated by the least squares method , that is , @xmath47 then , we have @xmath48 , where @xmath49 is defined by ( [ random_bases ] ) .    * proof . * based on theorem 7 in @xcite , we can easily obtain that @xmath50 the error after adding one hidden node in the @xmath45-hidden layer can be expressed as @xmath51 where @xmath44 and @xmath52 are obtained by using ( [ step3 ] ) and ( [ step4 ] ) with , respectively . to identify the relationship between @xmath53 and @xmath54 ,",
    "we denote @xmath55^{\\mathrm{t}}$ ] , where @xmath56 replacing @xmath52 by @xmath57 , with @xmath58 evaluated by ( [ step9 ] ) , we have @xmath59 with the help of ( [ step10 ] ) , we can obtain @xmath60 let @xmath61 for both sides of ( [ step20 ] ) , we have @xmath62 . that completes the proof .",
    "the idea of learning internal representation can be traced back to 80 s @xcite . in the past decades",
    ", many exciting researches and progresses have been reported in literature @xcite , where attempts are made to meet the following three properties : fidelity , sparsity and interpretability .",
    "learner models can appear in different forms but essentially they must share the universal approximation property so that the fidelity can be achieved .",
    "theorem 1 above ensures that deepscns fit the essential prerequisite well for learning representation . due to the special architecture of deepscns ,",
    "learning representation becomes quite straightforward and understandable .",
    "the outcomes from the first hidden layer contain two parts : a set of random basis functions with the original inputs , and associated read - out weights between the first hidden layer and the output layer .",
    "apparently , one can define a learning representation from the first hidden layer ( indeed , a scn model ) , which characterizes the main feature of the target function .",
    "next , the outputs from the first hidden layer , as a new set of inputs , can be fed into the next hidden layer to constructively generate a set of random basis functions with the cascaded inputs , and associated read - out weights between the second hidden layer and the output layer .",
    "again , one can obtain a refined learning representation from the second hidden layer .",
    "this procedure keeps going on until some termination criterion is satisfied .",
    "figure 1 shows a typical deepscn with @xmath63 , @xmath64 , and @xmath65 , @xmath66 , @xmath67 .",
    "it should be highlighted that deepscns advocate fully connections between each hidden layer and the output layer , which supports the diversity in learning representation for the target function by using an explicit span of the basis functions stochastically configured ( corresponds to @xmath15 in ( [ random_bases ] ) ) , instead of using only the compositional expression from the last hidden layer to conduct signal representation .",
    "note that the set of random basis functions generated by the proposed deepscns are data dependent and with cascaded inputs . according to ( [ random_bases ] )",
    ", deepscns can be regarded as a producer to output learning representations ( [ rep ] ) .",
    "indeed , such a learning representation offers end - users more flexibility , for instance , some nodes at certain layer can be discarded according to some criteria , leading to a new learning representation with sparsity .",
    "\\begin{array}{ccc}\\hspace{-1mm}\\underbrace{\\rule{20mm}{0mm}}_{\\mbox{hidden layer $ 1$}}&\\hspace{1mm}\\underbrace{\\rule{10mm}{0mm}}_{{\\bullet\\:\\:\\:\\:\\:\\:\\bullet\\:\\:\\:\\:\\:\\:\\bullet}}&\\hspace{0mm}\\underbrace{\\rule{20mm}{0mm}}_{\\mbox{hidden layer $ n$}}\\end{array } & \\end{array}\\ ] ]      given a training dataset with inputs @xmath69 , @xmath70^\\mathrm{t}\\in r^{d}$ ] and outputs @xmath71 , where @xmath72^\\mathrm{t}\\in r^{m}$ ] , @xmath73 .",
    "denote @xmath74^\\mathrm{t}$ ] as the corresponding residual error vector before the @xmath43-th new hidden node of the @xmath8-th hidden layer is added , where @xmath75\\in r^n$ ] , @xmath76 .",
    "+ after adding the @xmath43-th hidden node in the @xmath8-th hidden layer , we get @xmath77^\\mathrm{t},\\ ] ] where @xmath78 is used to simplify @xmath79 , and @xmath80^\\mathrm{t}$ ] , @xmath81 for @xmath82 .",
    "let @xmath83 $ ] represent the hidden layer output matrix and denote a temporary variable @xmath84 to make it convenient in the followed algorithm description .",
    "@xmath85 where @xmath86 denotes the dot product and we omit the argument @xmath87 in @xmath88 and @xmath89 .    [ sc1 ]    [ cols=\"<,<,<\",options=\"header \" , ]      in essence",
    ", the universal approximation property ensures the capacity of deepscns for both data modelling and signal representation .",
    "thus , deepscns can be employed as either predictive models or feature extractors in domain applications . for regression problems ,",
    "one may be more interested in the predictability of a learner model rather than its learning capability .",
    "unfortunately , it is almost impossible to directly establish some certain correlation between these two performances . that is",
    ", a better learning performance does not always imply a sound generalization .",
    "in this regard , consistency concept becomes a meaningful metric to assess the goodness of learning machines . from our hands - on experience ,",
    "deepscns demonstrate very good consistency performance , and it is believed that this merit is associated with both the proposed full - rank terminal criterion and the supervisory mechanism for assigning the random hidden parameters . for classification problems , deepscns can be used directly to put a class label for a testing data by using linear discriminate analysis .",
    "alternatively , we can use deepscns as feature extractors to generate a collection of samples in the deepscn feature space , followed by using variety of feature - classifiers to implement classification . in a nutshell , as results of deep learning for deepscns , a set of data dependent random basis functions and ( [ rep ] ) are generated for signal representation with piece - wise ` resolutions ' .",
    "this section reports some simulation results to illustrate advantages of deepscns compared against scns in terms of the approximation accuracy , quality of learning representation and model s capacity . here",
    ", we also provide a robustness analysis on dscn algorithm with respect to the learning parameter setting .",
    "the following real - valued function is employed in the simulation study : @xmath90.\\end{aligned}\\ ] ]    in this study , we take 1000 points ( randomly generated ) as the training data and another 1000 points ( randomly generated ) as the test data in our experiments . the sigmoidal activation function @xmath91 is used here for all hidden nodes .",
    "figure 2 depicts the training and test performances , respectively , where @xmath92 for scn , @xmath93 and @xmath94 for each layer in deepscn .",
    "it is clear that deepscn approximates the target function within a given tolerance more faster than scn . also , the consistency relationship between learning and generalization can be observed . as a matter of fact , both shallow and deep scns share this nice consistency property .",
    "it should be clearly pointed out that our statement on the consistency comes from a large number of experimental observations , and theoretical justifications are being expected .",
    "figure 2 shows three independent trials , which indicate that deepscn outperforms scn in terms of effectiveness . in this test , we set @xmath95 in step 16 of dscn algorithm . with different settings , the performance on both effectiveness and consistency",
    "could be further verified by the results from figure 5 .      from mathematical perspective",
    ", both scn and deepscn can produce a set of random basis functions .",
    "the difference between them lies in the way how to generate the random basis functions , which relate to the input variables and random parameters of learner models .",
    "it is difficult to quantify the quality of learning representations . in this paper",
    ", we favourably consider a learning representation if the distribution of the read - out weights in ( [ rep ] ) has sound statistical characteristics , for instance , lower expectation and standard deviation . in figure 3 ,",
    "two normalized distributions of the read - out weights ( converted into [ -1,1 ] on the basis of 200 coefficients from both scn and deepscn ) are plotted , and the approximation results for the test dataset are displayed beside the distributions , respectively . as can be seen that deepscn has a set of concentrated read - out weights and performs much better in generalization than scn , which indeed has a quite poor distribution of the read - out weights with a few of peaking values . in this scenario , deepscn has four hidden layers and @xmath96 is applied for each layer ( without use of the full - rank terminal condition ) .      in common sense",
    ", a learner model s capacity can be measured by using a trade - off metric over the learning and generalization performance .",
    "in addition , we may evaluate the capacity of a learner model by looking at the interpretability .",
    "in fact , it is always desirable to have a learner model with balanced performance among learning , generalization and model complexity . for deepscn framework , due to the specific configuration in model building ,",
    "the model capacity is not only associated with the number of nodes for each hidden layer , but also closely related to some algebraic properties of the hidden output matrix at each hidden layer . technically , it is very interesting and useful to look into the relationship between the rank - deficiency property of the joint hidden output matrix and the model s generalization capability . to do so ,",
    "the degree of rank - deficiency is computed by @xmath97 , where @xmath98 represents the hidden output matrix composed of all existing hidden nodes , @xmath99 represents the ( current ) number of hidden nodes after adding @xmath100 hidden layers and @xmath101 hidden nodes @xmath102 .",
    "the values of @xmath103 for both scn and deepscn are plotted in figure 4 ( a ) , in which it can be observed that @xmath98 of deepscn is with higher rank even when the number of hidden nodes is approaching to 100 , however , the rank - deficiency level of scn becomes much more higher as @xmath104 is larger .",
    "note that the results from deepscn reported in figure ( a ) were obtained with a fixed architecture ( i.e. , 4 hidden layers with 25 nodes for each ) and without use of the full - rank terminal criterion .",
    "figure 4 ( b ) depicts six curves of rank - deficiency for scn and deepscn , where @xmath93 and @xmath94 for each hidden layer are used in dscn algorithm . by referring to the previous results in comparison with scn",
    ", we infer that the deepscn model capacity has stronger correlation to the rank of the joint hidden output matrix .",
    "in general , robustness refers to the ability of a system to maintain its performance whilst subjected to noise in external inputs , changes to internal structure and/or shifts in parameter setting .",
    "obviously , the quality or performance of deepscns depends upon the parameter setting . in order to investigate the robustness of dscn algorithm , in this paper we limit our study on the learning parameter @xmath105 with 11 values only",
    "a similar robustness analysis on another set of key learning parameters @xmath106 is not reported here .",
    "figure 5 shows the training and test performances for both scn and deepscn with three different settings of @xmath105 , that is , randomly taking 10 real numbers from the open interval @xmath107 and arranging them in increasing order to form a set @xmath108 , and set @xmath109 . as can be seen that both scn and deepscn perform robustly against this learning parameter setting ( i.e. , not sensitive to the uncertain sequence @xmath105 ) . in this case",
    ", we fixed the architecture of deepscn ( i.e. , with 4 hidden layers and 25 nodes for each ) and kept the full - rank terminal criterion free . a remarkable drop - down",
    "can be observed after completing the construction of the first layer , which clearly exhibits the advantage of deepscn over scn .",
    "although many empirical proofs demonstrate great potential of deep neural networks for learning representation , it is still blind for end - users to deploy the network architecture so that the resulting deep learner model has sufficient capacity to approximately represent signals in some ways .",
    "except for concerns on the architecture , one cares much about fast learning algorithms . in this paper , our proposed deepscns , as a class of randomized deep neural networks , offer a fast and feasible pathway for problem solving . before ending up this paper",
    ", we would like to highlight and summarize again our technical contributions to the working field as follows :    * the supervisory mechanism characterised by the inequalities ( [ step2 ] ) and ( [ step3 ] ) is the key for developing deepscn framework , which was originally proposed in @xcite and could be used as a unique feature to distinguish our scn framework from other randomized learner models . * the scopes of random parameters at each layer of deepscns could be updated adaptively .",
    "indeed , this must be done to make the set of basis functions rich and functional . *",
    "the termination criterion , i.e. , the full - rank condition , for keep adding hidden nodes at the same layer is original and insightful .",
    "it is strongly believed that this criterion is an appropriate option although there is no theoretical justification in place at the moment",
    ".    further researches on this topic move into two main clusters : theoretical aspects , including further explorations on various supervisory mechanisms , scientific justifications on the proposed full - rank criterion , studies on the existence of the global random basis functions ( grbfs ) for a collection of functions , and looking into more complicated supervisory mechanisms to generate grbfs , development of learning representation with sparsity and interpretability ; practical aspects , comparing with other deep neural networks for gaining more understandings on merits and limits of deepscns , and applying deepscns for real world applications .",
    "e. bingham and h. mannila",
    ". random projection in dimensionality reduction : applications to image and text data . in",
    "_ proceedings of the 7th international conference on knowledge discovery and data mining _ , pages 245250 , 2001 .",
    "a. choromanska , m. henaff , m. mathieu , g. arous , and y. lecun .",
    "the loss surfaces of multilayer networks . in _ proceedings of the 18th international conference on artificial intelligence and statistics _ , pages 192204 , 2015 .",
    "a. coates and a. ng .",
    "the importance of encoding versus training with sparse coding and vector quantization . in _ proceedings of the 28th international conference on machine learning _ , pages 921928 , 2011 .",
    "d. cox and n. pinto .",
    "beyond simple features : a large - scale feature search approach to unconstrained face recognition . in _ proceedings of 2011 ieee international conference on automatic face & gesture recognition and workshops _ , pages 815 , 2011 .",
    "y. dauphin , r. pascanu , c. gulcehre , k. cho , s. ganguli , and y. bengio . identifying and attacking the saddle point problem in high - dimensional non - convex optimization . in _ advances in neural information processing systems _",
    ", pages 29332941 , 2014 .",
    "k. jarrett , k. kavukcuoglu , m. ranzato , and y. lecun .",
    "what is the best multi - stage architecture for object recognition ? in _ proceedings of the 12th ieee international conference on computer vision _ , pages 21462153 , 2009 .",
    "a. saxe , p. koh , z. chen , m. bhand , b. suresh , and a. ng . on random weights and unsupervised feature learning . in _ proceedings of the 28th international conference on machine learning _",
    ", pages 10891096 , 2011 .",
    "a. saxe , j. mcclelland , and s. ganguli .",
    "exact solutions to the nonlinear dynamics of learning in deep linear neural networks . in _ proceedings of international conference on learning representations _ , 2014",
    ". s. scardapane and d. wang .",
    "randomness in neural networks : an overview .",
    ", 2017 , 7:e1200 .",
    "doi : 10.1002/widm.1200 .",
    "j. schmidhuber .",
    "deep learning in neural networks : an overview .",
    ", 61:85117 , 2015 ."
  ],
  "abstract_text": [
    "<S> this paper focuses on the development of randomized approaches for building deep neural networks . </S>",
    "<S> a supervisory mechanism is proposed to constrain the random assignment of the hidden parameters ( i.e. , all biases and weights within the hidden layers ) . </S>",
    "<S> full - rank oriented criterion is suggested and utilized as a termination condition to determine the number of nodes for each hidden layer , and a pre - defined error tolerance is used as a global indicator to decide the depth of the learner model . </S>",
    "<S> the read - out weights attached with all direct links from each hidden layer to the output layer are incrementally evaluated by the least squares method . </S>",
    "<S> such a class of randomized leaner models with deep architecture is termed as deep stochastic configuration networks ( deepscns ) , of which the universal approximation property is verified with rigorous proof . </S>",
    "<S> given abundant samples from a continuous distribution , deepscns can speedily produce a learning representation , that is , a collection of random basis functions with the cascaded inputs together with the read - out weights . </S>",
    "<S> simulation results with comparisons on function approximation align with the theoretical findings . </S>"
  ]
}