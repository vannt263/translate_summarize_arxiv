{
  "article_text": [
    "we consider the problem of selecting the `` best '' set of _",
    "exactly @xmath0 columns _ from an @xmath1 matrix @xmath2 .",
    "more precisely , we consider the following column subset selection problem  ( cssp ) :    * ( the cssp ) * given a matrix @xmath16 and a positive integer @xmath0 , pick @xmath0 columns of @xmath2 forming a matrix @xmath17 such that the residual @xmath18 is minimized over all possible @xmath19 choices for the matrix @xmath5 .",
    "here , @xmath20 denotes the projection onto the @xmath0-dimensional space spanned by the columns of @xmath5 and @xmath21 denotes the spectral norm or frobenius norm .",
    "that is , the goal of the cssp is to find a subset of exactly @xmath0 columns of @xmath2 that `` captures '' as much of @xmath2 as possible , with respect to the spectral norm and/or frobenius norm , in a projection sense .",
    "the cssp has been studied extensively in numerical linear algebra , where it has found applications in , e.g. , scientific computing  @xcite .",
    "more recently , a relaxation has been studied in theoretical computer science , where it has been motivated by applications to large scientific and internet data sets  @xcite .",
    "we briefly comment on the complexity of the problem .",
    "clearly , in @xmath22 time we can generate all possible matrices @xmath5 and thus find the optimal solution in @xmath23 time .",
    "however , from a practical perspective , in data analysis applications of the cssp ( see section [ sxn : csspdata ] ) , @xmath24 is often in the order of hundreds or thousands .",
    "thus , in practice , algorithms whose running time depends exponentially on @xmath0 are prohibitively slow even if @xmath0 is , from a theoretical perspective , a constant . finally , the np - hardness of the cssp ( assuming @xmath0 is a function of @xmath24 ) is an open problem",
    ". note , though , that a similar problem , asking for the @xmath0 columns of the @xmath1 matrix @xmath2 that maximize the volume of the parallelepiped spanned by the columns of @xmath5 , is provably np - hard  @xcite .      in data applications , where the input matrix @xmath2 models @xmath25 objects represented with respect to @xmath24 features , the cssp corresponds to unsupervised feature selection .",
    "standard motivations for feature selection include facilitating data visualization , reducing training times , avoiding overfitting , and facilitating data understanding .",
    "consider , in particular , principal components analysis ( pca ) , which is the predominant linear dimensionality reduction technique , and which has been widely applied on datasets in all scientific domains , from the social sciences and economics , to biology and chemistry . in words , pca seeks to map or embed data points from a high dimensional euclidean space to a low dimensional euclidean space while keeping all the relevant linear structure intact .",
    "pca is an unsupervised dimensionality reduction technique , with the sole input parameters being the coordinates of the data points and the number of dimensions that will be retained in the embedding ( say @xmath0 ) , which is typically a constant independent of @xmath25 and @xmath24 ; often it is @xmath26 too .",
    "data analysts often seek a subset of @xmath0 actual features ( that is , @xmath0 actual columns , as opposed to the @xmath0 eigenvectors or eigenfeatures returned by pca ) that can accurately reproduce the structure derived by pca . the cssp is the obvious optimization problem associated with such unsupervised feature selection tasks .    we should note that similar formulations appeared in  @xcite . in addition , applications of such ideas include : ( @xmath27 )  @xcite , where a `` compact cur matrix decomposition '' was applied to static and dynamic data analysis in large sparse graphs ; ( @xmath28 )  @xcite , where these ideas were used for compression and classification of hyperspectral medical data and the reconstruction of missing entries from recommendation systems data in order to make high - quality recommendations ; and ( @xmath29 )  @xcite , where the concept of `` pca - correlated snps '' ( single nucleotide polymorphisms ) was introduced and applied to classify individuals from throughout the world without the need for any prior ancestry information .",
    "see  @xcite for a detailed evaluation of our main algorithm as an unsupervised feature selection strategy in three application domains of modern statistical data analysis ( finance , document - term data , and genetics ) .",
    "we present a novel two - stage algorithm for the cssp .",
    "this algorithm is presented in detail in section  [ sxn : main_alg ] as algorithm  [ alg : main ] . in the first stage of this algorithm ( the _ randomized stage _ ) , we randomly select @xmath6 columns of @xmath30 , i.e. , of the transpose of the @xmath31 matrix consisting of the top @xmath0 right singular vectors of @xmath2 , according to a judiciously - chosen probability distribution that depends on information in the top-@xmath0 right singular subspace of @xmath2 .",
    "then , in the second stage ( the _ deterministic stage _ ) , we apply a deterministic column - selection procedure to select exactly @xmath0 columns from the set of columns of @xmath30 selected by the first stage .",
    "the algorithm then returns the corresponding @xmath0 columns of @xmath2 . in section  [ sxn : main_proof ]",
    "we prove the following theorem .",
    "[ thm : main ] there exists an algorithm ( the two - stage algorithm  [ alg : main ] ) that approximates the solution to the cssp .",
    "this algorithm takes as input an @xmath1 matrix @xmath2 and a positive integer @xmath0 ; it runs in @xmath3 time ; and it returns as output an @xmath4 matrix @xmath5 consisting of exactly @xmath0 columns of @xmath2 such that with probability at least @xmath32 : @xmath33 here , @xmath20 denotes a projection onto the column span of the matrix @xmath5 , and @xmath8 denotes the best rank-@xmath0 approximation to the matrix @xmath2 as computed with the singular value decomposition .    note that we can trivially boost the success probability in the above theorem to @xmath34 by repeating the algorithm @xmath35 times . note also that the running time of our algorithm is linear in the larger of the dimensions @xmath25 and @xmath24 , quadratic in the smaller one , and independent of @xmath0 .",
    "thus , it is practically useful and efficient .",
    "to put our results into perspective , we compare them to the best existing results for the cssp . prior work provided bounds of the form @xmath36 where @xmath37 is a polynomial on @xmath24 and @xmath0 . for @xmath38 ,",
    "i.e. , for the spectral norm , the best previously - known bound for approximating the cssp is @xmath39  @xcite , while for @xmath40 , i.e. , for the frobenius norm , the best bound is @xmath41  @xcite .",
    "both results are algorithmically efficient , running in time polynomial in all three parameters @xmath25 , @xmath24 , and @xmath0 .",
    "the former runs in @xmath42 time and the latter runs in @xmath43 time .",
    "our approach provides an algorithmic bound for the frobenius norm version of the cssp that is roughly @xmath11 better than the best previously - known algorithmic result .",
    "it should be noted that @xcite also proves that by exhaustively testing all @xmath19 possibilities for the matrix @xmath5 , the best one will satisfy eqn .",
    "( [ eqn : priorbound ] ) with @xmath44 .",
    "our algorithmic result is only @xmath45 worse than this existential result .",
    "a similar existential result for the spectral norm version of the cssp is proved in @xcite with @xmath46 .",
    "our spectral norm bound depends on @xmath47 . in a worst case",
    "setting ( e.g. , when all the bottom @xmath48 singular values of @xmath2 are equal ) this quantity is upper bounded by @xmath49 .",
    "this is worse than the best results for the spectral norm version of the cssp by a factor of @xmath50 .",
    "finally , we should emphasize that a novel feature of the algorithm that we present in this paper is that it combines in a nontrivial manner recent algorithmic developments in the theoretical computer science community with more traditional techniques from the numerical linear algebra community in order to obtain improved bounds for the cssp .",
    "first , recall that the @xmath51-notation can be used to denote an asymptotically tight bound : @xmath52 or @xmath53 if there exist positive constants @xmath54 , @xmath55 , and @xmath56 such that @xmath57 for all @xmath58 .",
    "this is similar to the way in which the big-@xmath59-notation can be used to denote an asymptotic upper bound : @xmath60 if there exist positive constants @xmath61 and @xmath56 such that @xmath62 for all @xmath58 .",
    "let @xmath63 $ ] denote the set @xmath64 .",
    "for any matrix @xmath16 , let @xmath65 $ ] denote the @xmath27-th row of @xmath2 as a row vector , and let @xmath66 $ ] denote the @xmath67-th column of @xmath2 as a column vector .",
    "in addition , let @xmath68 denote the square of its frobenius norm , and let @xmath69 denote its spectral norm . if @xmath16 , then the singular value decomposition ( svd ) of @xmath2 can be written as @xmath70 in this expression , @xmath71 denotes the rank of @xmath2 , @xmath72 is an orthonormal matrix , @xmath73 is a @xmath74 diagonal matrix , and @xmath75 is an orthonormal matrix . also , @xmath76 denotes the @xmath77 diagonal matrix containing the top @xmath0 singular values of @xmath2 , @xmath78 denotes the @xmath79 matrix containing the bottom @xmath80 singular values of @xmath2 , @xmath81 denotes the @xmath82 matrix whose columns are the top @xmath0 right singular vectors of @xmath2 , and @xmath83 denotes the @xmath84 matrix whose columns are the bottom @xmath85 right singular vectors of @xmath2 , etc .",
    "the @xmath4 orthogonal matrix @xmath86 consisting of the top @xmath0 left singular vectors of @xmath2 is the `` best '' set of @xmath0 linear combinations of the columns of @xmath2 , in the sense that @xmath87 is the `` best '' rank @xmath0 approximation to @xmath2 .",
    "here , @xmath88 is a projection onto the @xmath0-dimensional space spanned by the columns of @xmath86 . in particular",
    ", @xmath8 minimizes @xmath89 , for both @xmath90 , over all @xmath1 matrices @xmath91 whose rank is at most @xmath0 .",
    "we also denote @xmath92 .",
    "we will use the notation @xmath93 when writing an expression that holds for both the spectral and the frobenius norm . we will subscript the norm by @xmath94 and @xmath95 when writing expressions that hold for one norm or the other . finally ,",
    "the moore - penrose generalized inverse , or pseudoinverse , of @xmath2 , denoted by @xmath96 , may be expressed in terms of the svd as @xmath97 .",
    "finally , we will make frequent use of the following fundamental result from probability theory , known as markov s inequality  @xcite .",
    "let @xmath98 be a random variable assuming non - negative values with expectation @xmath99}$ ] . then , for all @xmath100 , @xmath101}$ ] with probability at least @xmath102 .",
    "we will also need the so - called union bound .",
    "given a set of probabilistic events @xmath103 holding with respective probabilities @xmath104 , the probability that all events hold ( a.k.a .",
    ", the probability of the union of those events ) is upper bounded by @xmath105 .",
    "since solving the cssp exactly is a hard combinatorial optimization problem , research has historically focused on computing approximate solutions to it . since @xmath106 provides an immediate lower bound for @xmath107 , for @xmath108 and for any choice of @xmath5 , a large number of approximation algorithms",
    "have been proposed to select a subset of @xmath0 columns of @xmath2 such that the resulting matrix @xmath5 satisfies @xmath109 for some function @xmath37 . within the numerical linear algebra community ,",
    "most of the work on the cssp has focused on spectral norm bounds and is related to the so - called rank revealing qr ( rrqr ) factorization :    [ def : rrqr ] * ( the rrqr factorization ) * given a matrix @xmath110 ( @xmath111 ) and an integer @xmath0 ( @xmath112 ) , assume partial @xmath113 factorizations of the form : @xmath114 where @xmath115 is an orthonormal matrix , @xmath116 is upper triangular , @xmath117 , @xmath118 , @xmath119 , and @xmath120 is a permutation matrix .",
    "the above factorization is called a rrqr factorization if it satisfies @xmath121 where @xmath122 and @xmath123 are functions bounded by low degree polynomials in @xmath0 and @xmath24 .",
    "the work of golub on pivoted @xmath113 factorizations  @xcite was followed by much research addressing the problem of constructing an efficient rrqr factorization .",
    "most researchers improved rrqr factorizations by focusing on improving the functions @xmath122 and @xmath123 in definition  [ def : rrqr ] .",
    "let @xmath124 denote the first @xmath0 columns of a permutation matrix @xmath125 . then , if @xmath126 is an @xmath127 matrix consisting of @xmath0 columns of @xmath2 , it is straightforward to prove that @xmath128 for both @xmath108 .",
    "thus , in particular , when applied to the spectral norm , it follows that @xmath129 i.e. , any algorithm that constructs an rrqr factorization of the matrix @xmath2 with provable guarantees also provides provable guarantees for the cssp .",
    "see table  [ tb : priorwork ] for a summary of existing results , and see  @xcite for a survey and an empirical evaluation of some of these algorithms .",
    "more recently , @xcite proposed random - projection type algorithms that achieve the same spectral norm bounds as prior work while improving the running time .    [ cols=\"<,<,<,<,<\",options=\"header \" , ]     within the theoretical computer science community",
    ", much work has followed that of frieze , kannan , and vempala  @xcite on selecting a small subset of representative columns of @xmath2 , forming a matrix @xmath5 , such that the projection of @xmath2 on the subspace spanned by the columns of @xmath5 is as close to @xmath2 as possible .",
    "the algorithms from this community are randomized , which means that they come with a failure probability , and focus mainly on the frobenius norm .",
    "it is worth noting that they provide a strong tradeoff between the number of selected columns and the desired approximation accuracy .",
    "a typical scenario for these algorithms is that the desired approximation error ( see @xmath130 below ) is given as input , and then the algorithm selects the minimum number of appropriate columns in order to achieve this error .",
    "one of the most relevant results for this paper is a bound of  @xcite , which states that there exist exactly @xmath0 columns in any @xmath1 matrix @xmath2 such that @xmath131 here , @xmath5 contains exactly @xmath0 columns of @xmath2 .",
    "the only known algorithm to find these @xmath0 columns is to try all @xmath19 choices and keep the best .",
    "this existential result relies on the so - called volume sampling method  @xcite . in  @xcite",
    ", an adaptive sampling method is used to approximate the volume sampling method and leads to an @xmath132 algorithm which finds @xmath0 columns of @xmath2 such that @xmath133 as mentioned above , much work has also considered algorithms choosing slightly more than @xmath0 columns .",
    "this relaxation provides significant flexibility and improved error bounds .",
    "for example , in  @xcite , an adaptive sampling method leads to an @xmath134 algorithm , such that @xmath135 holds with high probability for some matrix @xmath5 consisting of @xmath136 columns of @xmath2 .",
    "similarly , in  @xcite , drineas , mahoney , and muthukrishnan leverage the subspace sampling method to give an @xmath3 algorithm such that @xmath137 holds with high probability if @xmath5 contains @xmath138 columns of @xmath2 .",
    "in this section , we present and describe algorithm  [ alg : main ] , our main algorithm for approximating the solution to the cssp .",
    "this algorithm takes as input an @xmath1 matrix @xmath2 and a rank parameter @xmath0 . after an initial setup ,",
    "the algorithm has two stages : a randomized stage and a deterministic stage . in the _ randomized stage _ , a randomized procedure is run to select @xmath139 columns from the @xmath140 matrix @xmath30 , i.e. , the transpose of the matrix containing the top-@xmath0 right singular vectors of @xmath2 .",
    "the columns are chosen by randomly sampling according to a judiciously - chosen nonuniform probability distribution that depends on information in the top-@xmath0 right singular subspace of @xmath2 .",
    "then , in the _ deterministic stage _ , a deterministic procedure is employed to select exactly @xmath0 columns from the @xmath139 columns chosen in the randomized stage .",
    "the algorithm then outputs exactly @xmath0 columns of @xmath2 that correspond to those columns chosen from @xmath30 .",
    "theorem  [ thm : main ] states that the projection of @xmath2 on the subspace spanned by these @xmath0 columns of @xmath2 is ( up to bounded error ) close to the best rank @xmath0 approximation to  @xmath2 .",
    "in more detail , algorithm  [ alg : main ] first computes a probability distribution @xmath104 over the set @xmath141 , i.e. , over the columns of @xmath30 , or equivalently over the columns of @xmath2 .",
    "the probability distribution depends on information in the top-@xmath0 right singular subspace of @xmath2 .",
    "in particular , for all @xmath142 $ ] , define @xmath143 and note that @xmath144 , for all @xmath145 $ ] , and that @xmath146 .",
    "we will describe the computation of probabilities of this form below .    in the _ randomized stage _",
    ", algorithm  [ alg : main ] employs the following randomized column selection algorithm to choose @xmath6 columns from @xmath30 to pass to the second stage .",
    "let @xmath61 assume the value of eqn .",
    "( [ eqn : valueofc ] ) . in @xmath61 independent",
    "identically distributed ( i.i.d . )",
    "trials , the algorithm chooses a column of @xmath30 where in each trial the @xmath27-th column of @xmath30 is kept with probability @xmath147",
    ". additionally , if the @xmath27-th column is kept , then a scaling factor equal to @xmath148 is kept as well .",
    "thus , at the end of this process , we will be left with @xmath61 columns of @xmath30 and their corresponding scaling factors .",
    "notice that due to random sampling in i.i.d .",
    "trials with replacement we might keep a particular column more than once .    in order to conveniently represent the @xmath61 selected columns and the associated scaling factors , we will use the following sampling matrix formalism .",
    "first , define an @xmath149 sampling matrix @xmath150 as follows : @xmath150 is initially empty ; at each of the @xmath61 i.i.d . trials , if the @xmath27-th column of @xmath30 is selected by the random sampling process , then @xmath151 ( an @xmath24-vector of all - zeros , except for its @xmath27-th entry which is set to one ) is appended to @xmath150 .",
    "next , define the @xmath152 diagonal rescaling matrix @xmath153 as follows : if the @xmath27-th column of @xmath30 is selected , then a diagonal entry of @xmath153 is set to @xmath148 .",
    "thus , we may view the randomized stage as outputting the matrix @xmath154 consisting of a small number of rescaled columns of @xmath30 , or simply as outputting @xmath150 and @xmath153 .    in the _ deterministic stage _",
    ", algorithm  [ alg : main ] applies a deterministic column selection algorithm to the output of the first stage in order to choose _",
    "exactly @xmath0 columns _ from the input matrix @xmath2 . to do so",
    ", we run the algorithm 4 of  @xcite ( with the parameter @xmath155 set to @xmath156 ) on the @xmath157 matrix @xmath158 , i.e. , the column - scaled version of the columns of @xmath30 chosen in the first stage .",
    "thus , a matrix @xmath159 is formed , or equivalently , in the sampling matrix formalism described previously , a new matrix @xmath160 is constructed .",
    "its dimensions are @xmath161 , since it selects exactly @xmath0 columns out of the @xmath61 columns returned after the end of the randomized stage .",
    "the algorithm then returns the corresponding @xmath0 columns of the original matrix @xmath2 , i.e. , after the second stage of the algorithm is complete , the @xmath162 matrix @xmath163 is returned as the final output .",
    "* input : * @xmath1 matrix @xmath2 , integer @xmath0 .    *",
    "output : * @xmath4 matrix @xmath5 with @xmath0 columns of @xmath2",
    ". +    1 .",
    "* initial setup : * * compute the top @xmath0 right singular vectors of @xmath2 , denoted by @xmath164 . *",
    "compute the sampling probabilities @xmath147 , for @xmath142 $ ] , using eqn .",
    "( [ eqn : sampling_probs ] ) or eqn .",
    "( [ eq : probexpression ] ) .",
    "* let @xmath165 ( here @xmath166 is the unspecified constant of theorem  [ thm : theorem7correct ] . ) 2 .",
    "* randomized stage : * * for @xmath167 ( i.i.d .",
    "trials ) select an integer from @xmath168 where the probability of selecting @xmath27 is equal to @xmath147 .",
    "if @xmath27 is selected , keep the scaling factor @xmath148 . *",
    "form the sampling matrix @xmath150 and the rescaling matrix @xmath153 ( see text ) .",
    "* deterministic stage : * * run algorithm 4 , page 853 of  @xcite ( with the parameter @xmath155 set to @xmath156 ) on the matrix @xmath154 in order to select exactly @xmath0 columns of @xmath154 , thereby forming the sampling matrix @xmath160 ( see text ) .",
    "* return the corresponding @xmath0 columns of @xmath2 , i.e. , return @xmath169 .",
    "we now discuss the running time of our algorithm .",
    "note that manipulating the probability distribution of eqn .",
    "( [ eqn : sampling_probs ] ) yields : @xmath170 thus , knowledge of @xmath164 , i.e. , the @xmath31 matrix consisting of the top-@xmath0 right singular vectors of @xmath2 , suffices to compute the @xmath147 s . by eqn .",
    "( [ eq : probexpression ] ) , @xmath3 time suffices for our theoretical analysis . in practice iterative algorithms",
    "could be used to speed up the algorithm .",
    "note also that in order to obtain the frobenius norm bound of theorem  [ thm : main ] , our theoretical analysis holds if the sampling probabilities are of the form : @xmath171 that is , the frobenius norm bound of theorem  [ thm : main ] holds even if the second term in the sampling probabilities of eqns .",
    "( [ eqn : sampling_probs ] ) and ( [ eq : probexpression ] ) is omitted .    finally , we briefly comment on a technical constraint of algorithm 4 of  @xcite .",
    "this algorithm assumes that its input matrix has at least as many rows as columns .",
    "however , in our approach , we will apply it on the @xmath157 matrix @xmath154 , which clearly has fewer rows than columns .",
    "thus , prior to applying the aforementioned algorithm , we first pad @xmath158 with @xmath172 all - zero rows , thus making it a square matrix .",
    "let @xmath173 and let @xmath174 be the @xmath152 matrix after the padding .",
    "( 8) in theorem 3.2 of  @xcite ( with @xmath27 set to @xmath0 and @xmath155 set to @xmath156 ) implies that @xmath175 .",
    "clearly , @xmath176 and @xmath177 .",
    "overall , we get , @xmath178 which is the only guarantee that we need in the deterministic step ( see lemma  [ lemma : sigmabound ] ) .",
    "the running time of the deterministic stage of algorithm  [ alg : main ] is @xmath179 time , since the ( padded ) matrix @xmath154 has @xmath61 columns and rows .",
    "an important open problem would be to identify other suitable importance sampling probability distributions that avoid the computation of a basis for the top-@xmath0 right singular subspace .      intuitively , we achieve improved bounds for the cssp because we apply the deterministic algorithm to a lower dimensional matrix ( the matrix @xmath154 with @xmath139 columns , as opposed to the matrix @xmath2 with @xmath24 columns ) in which the columns are `` spread out '' in a `` nice '' manner . to see this , note that the probability distribution of eqn .",
    "( [ eqn : sampling_probs_firstterm ] ) , and thus one of the two terms in the probability distribution of eqns .  ( [ eqn : sampling_probs ] ) or ( [ eq : probexpression ] ) , equals ( up to scaling ) the diagonal elements of the projection matrix onto the span of the top-@xmath0 right singular subspace . in diagnostic regression analysis",
    ", these quantities have a natural interpretation in terms of _ statistical leverage _ ,",
    "and thus they have been used extensively to identify `` outlying '' data points  @xcite .",
    "thus , the importance sampling probabilities that we employ in the randomized stage of our main algorithm provide a bias toward more `` outlying '' columns , which then provide a `` nice '' starting point for the deterministic stage of our main algorithm .",
    "this also provides intuition as to why using importance sampling probabilities of the form of eqn .",
    "( [ eqn : sampling_probs_firstterm ] ) leads to relative - error low - rank matrix approximations  @xcite .",
    "in this section , we provide a proof of theorem  [ thm : main ] .",
    "we start with an outline of our proof , pointing out conceptual improvements that were necessary in order to obtain improved bounds .",
    "an important condition in the first phase of the algorithm is that when we sample columns from the @xmath140 matrix @xmath30 , we obtain a @xmath157 matrix @xmath154 that does not lose any rank .",
    "to do so , we will apply a result from matrix perturbation theory to prove that if @xmath180 ( see eqn .",
    "( [ eqn : valueofc ] ) ) then @xmath181 .",
    "( see lemma  [ lemma : rank ] below . ) then , under the assumption that @xmath154 has full rank , we will prove that the @xmath4 matrix @xmath5 returned by the algorithm will satisfy : @xmath182 for both @xmath183 .",
    "( see lemma  [ lem : mainlemma ] below . )",
    "next , we will provide a bound on @xmath184 . in order to get a strong accuracy guarantee for the overall algorithm ,",
    "the deterministic column selection algorithm must satisfy @xmath185 where @xmath186 is a polynomial in both @xmath0 and @xmath61 .",
    "thus , for our main theorem , we will employ algorithm 4  @xcite with @xmath187 , which guarantees the above bound with @xmath188 .",
    "( see lemma  [ lemma : sigmabound ] below . ) finally , we will show , using relatively straightforward matrix perturbation techniques , that @xmath189 is not too much more , in a multiplicative sense , than @xmath190 , where we note that the factors differ for @xmath108 .",
    "( see lemmas  [ lemma : twonorm ] and  [ lemma : fnorm ] below . ) by combining these results , the main theorem will follow .",
    "the following lemma provides a bound on the singular values of the matrix @xmath154 computed by the _ randomized phase _ of algorithm  [ alg : main ] , from which it will follow that the matrix @xmath154 is full rank . to prove the lemma , we employ theorem  [ thm : theorem7correct ] of the appendix ( this theorem is a variant of a result of rudelson and vershynin in  @xcite ) . note that probabilities of the form of eqn .",
    "( [ eqn : sampling_probs_firstterm ] ) actually suffice to establish lemma  [ lemma : rank ] .",
    "[ lemma : rank ] let @xmath150 and @xmath153 be constructed using algorithm  [ alg : main ] . then , with probability at least  @xmath192 , @xmath193 in particular",
    ", @xmath191 has full rank .    in order to bound @xmath194 , we will bound @xmath195 . towards that end , we will use theorem [ thm : theorem7correct ] with @xmath196 and @xmath197 , which results in the value for @xmath61 in eqn .",
    "( [ eqn : valueofc ] ) .",
    "note that the sampling probabilities in eqn .",
    "( [ eq : probexpression ] ) satisfy @xmath198 now theorem [ thm : theorem7correct ] and our construction of @xmath150 and @xmath153 guarantee that for @xmath61 as in eqn .",
    "( [ eqn : valueofc ] ) @xmath199}\\leq 1/20.\\ ] ] we note here that the condition @xmath200 in theorem [ thm : theorem7correct ] is trivially satisfied assuming that @xmath166 is at least one ( given our choices for @xmath201 , @xmath130 , and @xmath202 ) . using @xmath203 and markov s inequality",
    "we get that with probability at least  @xmath192 , @xmath204 standard matrix perturbation theory results  @xcite now imply that for all @xmath205 , @xmath206      [ lem : mainlemma ] let @xmath150 , @xmath153 , and @xmath160 be constructed as described in algorithm  [ alg : main ] and recall that @xmath208 .",
    "if @xmath158 has full rank , then for @xmath183 , @xmath209    we seek to bound the spectral and frobenius norms of @xmath210 , where @xmath208 is constructed by algorithm  [ alg : main ] .",
    "to do so , first notice that scaling the columns of a matrix ( equivalently , post - multiplying the matrix by a diagonal matrix ) by any non - zero scale factors does not change the subspace spanned by the columns of the matrix .",
    "thus , @xmath211 where , in the last line , we have introduced the convenient notation @xmath212 that we will use throughout the remainder of this proof . in the sequel",
    "we seek to bound the residual @xmath213 first , note that @xmath214 this implies that in eqn .",
    "( [ eqn : tmp ] ) we can replace @xmath215 with any other @xmath140 matrix and the equality with an inequality .",
    "in particular we replace @xmath215 with @xmath216 , where @xmath8 is the best rank-@xmath0 approximation to @xmath2 : @xmath217 let @xmath218 .",
    "then , @xmath219 and , using the triangle inequality , @xmath220 we now bound @xmath221 , @xmath222 , and @xmath223 .",
    "first , for @xmath221 , note that : @xmath224 in eqn .",
    "( [ eq3 ] ) , we replaced @xmath225 by @xmath226 .",
    "this follows since the statement of our lemma assumes that the matrix @xmath154 has full rank . also",
    ", the construction of @xmath160 guarantees that the columns of @xmath154 that are selected in the second stage of algorithm  [ alg : main ] are linearly independent , and thus the @xmath77 matrix @xmath227 has full rank and is invertible . in eqn .",
    "( [ eq4 ] ) , @xmath86 and @xmath30 can be dropped without increasing a unitarily invariant norm , while eqn .",
    "( [ eq5 ] ) follows since @xmath228 is a full - rank @xmath229 matrix .",
    "next , note that @xmath230 . finally , to conclude the proof , we bound @xmath223 as follows : @xmath231 eqn .",
    "( [ eq10 ] ) follows by the orthogonality of @xmath232 and @xmath164 and the fact that @xmath228 is a @xmath77 invertible matrix ( see above ) .",
    "( [ eq12 ] ) follows from the fact that for any two matrices @xmath98 and @xmath233 and @xmath183 , @xmath234 .",
    "finally , eqn .",
    "( [ eq14 ] ) follows since @xmath235 and @xmath160 is an orthogonal matrix .",
    "[ lemma : sigmabound ] let @xmath150 , @xmath153 , and @xmath160 be constructed using algorithm  [ alg : main ] .",
    "then , with probability at least  @xmath192 , @xmath236    from lemma [ lemma : rank ] we know that @xmath237 holds for all @xmath238 with probability at least  @xmath192 .",
    "the deterministic construction of @xmath160 ( see algorithm 4 of  @xcite with the parameter @xmath155 set to @xmath156 ) guarantees that @xmath239    [ lemma : twonorm]*(@xmath38 ) * if @xmath150 and @xmath153 are constructed as described in  algorithm [ alg : main ] , then , with probability at least  @xmath192 , @xmath240    let @xmath241 . we manipulate @xmath242 as follows :",
    "@xmath243 given our construction of @xmath150 and @xmath153 and applying eqn .",
    "( 9 ) of theorem 1 of @xcite with @xmath196 and @xmath244 , we get that with probability at least  @xmath192 , @xmath245 thus , by combining the above results and using @xmath246 and @xmath247 we get @xmath248 to conclude the proof of the lemma we take the square roots of both sides of the above inequality .",
    "[ lemma : fnorm ] * ( @xmath40 ) * if @xmath150 and @xmath153 are constructed as described in  algorithm [ alg : main ] , then , with probability at least  @xmath192 , @xmath249    it is straightforward to prove that with our construction of @xmath150 and @xmath153 , the expectation of @xmath250 is equal to @xmath251 .",
    "in addition , note that the latter quantity is exactly equal to @xmath252 .",
    "applying markov s inequality , we get that , with probability at least  @xmath192 , @xmath253 taking square roots of both sides of the above inequality concludes the proof of the lemma .      to prove the frobenius norm bound of theorem  [ thm : main ] we combine lemma  [ lem : mainlemma ] ( with @xmath254 ) with lemmas  [ lemma : sigmabound ] and  [ lemma : fnorm ] .",
    "thus , we get @xmath255 using @xmath256 immediately derives the frobenius norm bound of theorem  [ thm : main ] .",
    "notice that lemma  [ lemma : sigmabound ] fails with probability at most @xmath257 and that lemma  [ lemma : fnorm ] fails with probability at most @xmath257 ; thus , applying the standard union bound , it follows that the frobenius norm bound of theorem  [ thm : main ] holds with probability at least @xmath32 . to prove the spectral norm bound of theorem  [ thm : main ] we combine lemma  [ lem : mainlemma ] ( with @xmath258 ) with lemmas  [ lemma : sigmabound ] and  [ lemma : twonorm ] .",
    "thus , we get @xmath259 using @xmath256 immediately derives the spectral norm bound of theorem  [ thm : main ] .",
    "notice that lemma  [ lemma : sigmabound ] fails with probability at most @xmath257 and that lemma  [ lemma : twonorm ] fails with probability at most @xmath257 ; thus , applying the standard union bound , it follows that the spectral norm bound of theorem  [ thm : main ] holds with probability at least @xmath32 .",
    "we are grateful to daniel spielman and ilse ipsen for numerous useful discussions on the results of this paper .",
    "we would also like to thank an anonymous reviewer of an earlier version of this manuscript who provided a counterexample to lemma 4.4 of  @xcite , and thus helped us identify the error in the proof of that lemma .",
    "10    a.  ben - hur and i.  guyon . detecting stable clusters using principal component analysis .",
    ", 224:159182 , 2003 .    c.  h. bischof and g.  quintana - ort . computing rank - revealing qr factorizations of dense matrices .",
    ", 24(2):226253 , 1998 .    c.  boutsidis , m.w .",
    "mahoney , and p.  drineas .",
    "unsupervised feature selection for principal components analysis . in _ proceedings of the 14th annual acm sigkdd conference _ , pages 6169 , 2008 .    c.  boutsidis , m.w .",
    "mahoney , and p.  drineas .",
    "an improved approximation algorithm for the column subset selection problem . in _ proceedings of the 20th annual acm - siam symposium on discrete algorithms _ ,",
    "pages 968977 , 2009 .",
    "rank revealing qr  factorizations .",
    ", 88/89:6782 , 1987 .",
    "chan and p.c .",
    "some applications of the rank revealing qr factorization . , 13:727741 , 1992 .",
    "chan and p.c .",
    "low - rank revealing qr  factorizations . , 1:3344 , 1994 .",
    "s.  chandrasekaran and i.  c.  f. ipsen . on rank - revealing factorizations .",
    ", 15:592622 , 1994 .",
    "s.  chatterjee and a.s .",
    "john wiley & sons , new york , 1988 .",
    "a.  civril and m.  magdon - ismail .",
    "finding maximum volume sub - matrices of a matrix .",
    "technical report 07 - 08 , rennselar polytechnic institute department of computer science , troy , ny , 2007 .",
    "a.  deshpande , l.  rademacher , s.  vempala , and g.  wang .",
    "matrix approximation and projective clustering via volume sampling . in",
    "_ proceedings of the 17th annual acm - siam symposium on discrete algorithms _ , pages 11171126 , 2006 .",
    "a.  deshpande and s.  vempala .",
    "adaptive sampling and fast low - rank matrix approximation . in _ proceedings of the 10th international workshop on randomization and computation _ , pages 292303 , 2006 .",
    "p.  drineas , r.  kannan , and m.w . mahoney .",
    "fast monte carlo algorithms for matrices i : approximating matrix multiplication . , 36:132157 , 2006 .",
    "p.  drineas , i.  kerenidis , and p.  raghavan .",
    "competitive recommendation systems . in _ proceedings of the 34th annual acm symposium on theory of computing _ , pages 8290 , 2002 .",
    "p.  drineas , m.w .",
    "mahoney , and s.  muthukrishnan .",
    "subspace sampling and relative - error matrix approximation : column - based methods . in _ proceedings of the 10th international workshop on randomization and computation _ , pages 316326 , 2006 .",
    "p.  drineas , m.w .",
    "mahoney , and s.  muthukrishnan .",
    "relative - error cur matrix decompositions . , 30:844881 , 2008 .",
    "p.  drineas , m.w .",
    "mahoney , s.  muthukrishnan , and t.  sarls .",
    "faster least squares approximation .",
    "technical report .",
    "preprint : arxiv:0710.1435v3 ( 2007 ) .",
    "l.  v. foster .",
    "rank and null space calculations using matrix decomposition without column interchanges .",
    ", 74:4771 , 1986 .    l.  v. foster and x.  liu .",
    "comparison of rank revealing algorithms applied to matrices with well defined numerical ranks .",
    "a.  frieze , r.  kannan , and s.  vempala .",
    "fast monte - carlo algorithms for finding low - rank approximations . in _ proceedings of the 39th annual ieee symposium on foundations of computer science _ , pages 370378 , 1998 .",
    "g.  golub .",
    "numerical methods for solving linear least squares problems . , 7:206216 , 1965 .",
    "golub and c.f .  van loan . .",
    "johns hopkins university press , baltimore , 1989 .",
    "m.  gu and s.c .",
    "efficient algorithms for computing a strong rank - revealing qr factorization . , 17:848869 , 1996 .",
    "y.  p. hong and c.  t. pan .",
    "rank - revealing qr  factorizations and the singular value decomposition . , 58:213232 , 1992 .",
    "w.  j. krzanowski .",
    "selection of variables to preserve multivariate data structure , using principal components .",
    ", 36(1):2233 , 1987 .",
    "mahoney , m.  maggioni , and p.  drineas .",
    "tensor - cur decompositions for tensor - based data . in _ proceedings of the 12th annual acm sigkdd conference _ ,",
    "pages 327336 , 2006 .",
    "mahoney , m.  maggioni , and p.  drineas .",
    "tensor - cur decompositions for tensor - based data . , 30:957987 , 2008 .",
    "k.  z. mao . identifying critical variables of principal components for unsupervised feature selection .",
    ", 35(2):339344 , 2005 .",
    "martinsson , v.  rokhlin , and m.  tygert . a randomized algorithm for the approximation of matrices .",
    "technical report yaleu / dcs / tr-1361 , yale university department of computer science , new haven , ct , june 2006 .",
    "r.  motwani and p.  raghavan . .",
    "cambridge university press , new york , 1995 .",
    "c .- t . pan . on the existence and computation of rank - revealing lu factorizations .",
    ", 316:199222 , 2000 .    c.  t. pan and p.  t.  p. tang .",
    "bounds on singular values revealed by qr  factorizations .",
    ", 39:740756 , 1999 .",
    "p.  paschou , e.  ziv , e.g. burchard , s.  choudhry , w.  rodriguez - cintron , m.w .",
    "mahoney , and p.  drineas . -correlated snps for structure identification in worldwide human populations .",
    ", 3:16721686 , 2007 .",
    "m.  rudelson and r.  vershynin . sampling from large matrices : an approach through geometric functional analysis .",
    ", 54(4):article 21 , 2007 .",
    "four algorithms for the efficient computation of truncated qr approximations to a sparse matrix .",
    ", 83:313323 , 1999 .",
    "h.  stoppiglia , g.  dreyfus , r.  dubois , and y.  oussar .",
    "ranking a random feature for variable and feature selection .",
    ", 3:13991414 , 2003 .",
    "j.  sun , y.  xie , h.  zhang , and c.  faloutsos .",
    "less is more : compact matrix decomposition for large sparse graphs . in _ proceedings of the 7th siam international conference on data mining _ , 2007 .",
    "l.  wolf and a.  shashua .",
    "feature selection for unsupervised and supervised inference : the emergence of sparsity in a weight - based approach .",
    ", 6:18551887 , 2005 .",
    "f.  woolfe , e.  liberty , v.  rokhlin , and m.  tygert .",
    "a fast randomized algorithm for the approximation of matrices .",
    "technical report yaleu / dcs / tr-1380 , yale university department of computer science , new haven , ct , 2007 .",
    "z.  zhao and h.  liu .",
    "spectral feature selection for supervised and unsupervised learning . in _ proceedings of the 24th international conference on machine learning _ , pages 11511157 , 2007 .",
    "let @xmath16 be any matrix . consider the following algorithm ( which is essentially the algorithm in page 876 of  @xcite ) that constructs a matrix @xmath260 consisting of @xmath61 rescaled columns of @xmath2 .",
    "we state theorem 4 of  @xcite that provides a bound for the approximation error @xmath261 .",
    "[ thm : theorem7correct ] let @xmath16 with @xmath264 .",
    "construct @xmath5 using the exactly(@xmath61 ) algorithm and let the sampling probabilities @xmath147 satisfy @xmath265 for all @xmath142 $ ] for some constant @xmath266 $ ] .",
    "let @xmath267 be an accuracy parameter , assume @xmath268 , and let @xmath269 ( here @xmath166 is the unknown constant of theorem 3.1 , p. 8 of  @xcite . ) then , @xmath270}\\leq \\epsilon.\\ ] ]"
  ],
  "abstract_text": [
    "<S> we consider the problem of selecting the `` best '' subset of _ exactly @xmath0 columns _ from an @xmath1 matrix @xmath2 . </S>",
    "<S> in particular , we present and analyze a novel two - stage algorithm that runs in @xmath3 time and returns as output an @xmath4 matrix @xmath5 consisting of exactly @xmath0 columns of @xmath2 . in the first stage ( the _ randomized _ stage ) , the algorithm randomly selects @xmath6 columns according to a judiciously - chosen probability distribution that depends on information in the top-@xmath0 right singular subspace of @xmath2 . in the second stage ( the _ deterministic _ stage ) </S>",
    "<S> , the algorithm applies a deterministic column - selection procedure to select and return exactly @xmath0 columns from the set of columns selected in the first stage . </S>",
    "<S> let @xmath5 be the @xmath4 matrix containing those @xmath0 columns , let @xmath7 denote the projection matrix onto the span of those columns , and let @xmath8 denote the `` best '' rank-@xmath0 approximation to the matrix @xmath2 as computed with the singular value decomposition . </S>",
    "<S> then , we prove that , with probability at least 0.8 , @xmath9 this frobenius norm bound is only a factor of @xmath10 worse than the best previously existing existential result and is roughly @xmath11 better than the best previous algorithmic result ( both of deshpande et al .  </S>",
    "<S> @xcite ) for the frobenius norm version of this column subset selection problem . </S>",
    "<S> we also prove that , with probability at least 0.8 , @xmath12 this spectral norm bound is not directly comparable to the best previously existing bounds for the spectral norm version of this column subset selection problem ( such as the ones derived by gu and eisenstat in  @xcite ) . more specifically , our bound depends on @xmath13 , whereas previous results depend on @xmath14 ; if these two quantities are comparable , then our bound is asymptotically worse by a @xmath15 factor . </S>"
  ]
}