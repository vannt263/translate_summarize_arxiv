{
  "article_text": [
    "the numerical solution of odes requires the solution of sparse and structured linear systems .",
    "the parallel solution of these problems may be obtained in two ways : for bvps , since the size of the associated linear system is large , we need to develop parallel solvers for the obtained linear systems ; for ivps we need to define appropriate numerical methods that allow to obtain parallelizable linear systems .    in both cases",
    ", the main problem can then be taken back to the solution of special sparse linear systems , whose solution is here approached through the use of _ parallel factorizations _ , originally introduced for deriving efficient parallel tridiagonal solvers @xcite , and subsequently generalized to block tridiagonal , almost block diagonal ( abd ) , and bordered almost block diagonal ( babd ) systems @xcite .    with this premise ,",
    "the structure of the paper is the following : in section  [ pf ] the main facts about _ parallel factorizations _ and their extensions",
    "are briefly recalled ; then , in section  [ par ] their application for solving ode problems is sketched ; finally , in section  [ parareal ] we show that this approach also encompasses the so called `` parareal '' algorithm , recently introduced in @xcite .",
    "in this section we consider several parallel algorithms in the class of partition methods for the solution of linear systems , @xmath0 where @xmath1 is a @xmath2 sparse and structured matrix , and @xmath3 and @xmath4 are vectors of length @xmath5 .",
    "we will investigate the parallel solution of on @xmath6 processors , supposing @xmath7 in order for the number of sequential operations to be much smaller than that of parallel ones .",
    "the coefficient matrices @xmath1 here considered are ( block ) banded , tridiagonal , bidiagonal , or even almost block diagonal ( abd ) .",
    "all these structures may be rearranged in the form @xmath8 where the diagonal blocks are square and the superscript @xmath9 indicates that this block is handled only by processor @xmath10 .",
    "the size of the blocks @xmath11 , @xmath12 , @xmath13 , and @xmath14 is in general independent of both @xmath10 and @xmath15 , and only depends on the sparsity structure of the coefficient matrix @xmath1 .",
    "in particular , the size of the blocks @xmath11 is quite important , since the sequential section of the algorithm is proportional to it .",
    "therefore , the blocks @xmath11 should be as small as possible . as an example , if @xmath1 is ( block ) tridiagonal , @xmath11 reduces to a single ( block ) entry .",
    "vice versa , in case of banded ( block ) matrices , the ( block ) size of @xmath11 equals to @xmath16 , where @xmath17 and @xmath18 denote the number of lower and upper off ( block ) diagonals ( see figure  [ fig1 ] ) , respectively . in case of abd matrices",
    ", @xmath11 is a block of size equal to the number of rows in each block row of the coefficient matrix ( see figure  [ fig2 ] ) . since row and column permutations inside each block do not destroy the sparsity structure of the coefficient matrix , in abd matrices we may permute the elements inside @xmath11 to improve stability properties .",
    "blocks @xmath12 have the same sparsity structure as the original matrix , and are locally handled by using any suitable sequential algorithm .            in order to keep track of any parallel algorithm",
    ", we consider the following factorization @xcite @xmath19 where @xmath20 @xmath21 @xmath22 @xmath23 , @xmath24 and @xmath25 are identity and null matrices of appropriate sizes , and @xmath26 is any suitable factorization of the block @xmath12 .",
    "the remaining entries of @xmath27 , @xmath28 , and @xmath29 can be derived from by direct identification .",
    "factorization may be computed in parallel on the @xmath6 processors . for simplicity",
    ", we analyze the factorization of the sub - matrix identified by the superscript @xmath9 ( with obvious differences for @xmath30 and @xmath31 ) @xmath32 following  we have @xmath33 where @xmath34 and the other entries are the same as defined in .    consequently , by considering any given factorization for @xmath12 , it is possible to derive corresponding parallel extensions of such factorizations , which cover most of the parallel algorithms in the class of partition methods .",
    "in particular , the following ones easily derive for matrices with well conditioned sub - blocks @xmath12 ( this means , for example , that pivoting is unnecessary or does not destroy the sparsity structure ) :    * _ @xmath35 factorization _ , by setting in @xmath36 and @xmath37 , where @xmath38 is the @xmath35 factorization of the matrix @xmath12 . in this case , the ( block ) vectors @xmath39 and @xmath40 maintain the same sparsity structure as that of @xmath41 and @xmath42 , respectively , while the vectors @xmath43 and @xmath44 are non - null fill - in ( block ) vectors , obtained by solving two triangular systems . *",
    "_ @xmath45 factorization _ ( which derives from the gauss - jordan elimination algorithm ) , by setting in @xmath46 , a diagonal matrix , and @xmath47 where @xmath48 and @xmath49 are lower and upper triangular matrices , respectively , with unit diagonal .",
    "therefore , @xmath40 and @xmath44 maintain the same sparsity structure as that of @xmath42 and @xmath50 , respectively , while @xmath43 and @xmath39 are non - null fill - in ( block ) vectors . *",
    "_ cyclic reduction _",
    "algorithm @xcite ( see also @xcite ) , which is one of the best known parallel algorithms but that , in its original form , requires a synchronization at each step of reduction .",
    "in fact , the idea of this algorithm is to perform several reductions that , at each step , halve the size of the system . on the other hand , to obtain a factorization in the form , it is possible to consider cyclic reduction as a sequential algorithm to be applied locally , @xmath51 where @xmath52 are suitable permutation matrices that maintain the first and last row in the reduced matrix .",
    "the computational cost , which is much higher if the algorithm is applied to @xmath1 on a sequential computer , becomes comparable to the previous local factorizations since this algorithm does not compute fill - in block vectors . as a consequence ,",
    "the corresponding parallel factorization algorithm turns out to be one of the most effective . *",
    "_ alternate row and column elimination _",
    "@xcite which is an algorithm suitable for abd matrices .",
    "in fact , for such matrices alternate row and column permutations always guarantee stability without fill - in .",
    "this feature extends to the parallel algorithm , by taking into account that row permutations between the first block row of @xmath12 and the block containing @xmath50 ( see ( [ mi0 ] ) ) , still make the parallel algorithm stable without introducing fill - in .",
    "such parallel factorization is defined by setting @xmath53 and @xmath54 , where @xmath55 and @xmath56 are permutation matrices and @xmath48 and @xmath49 , after a suitable reordering of the rows and of the columns , are @xmath57 block triangular matrices ( see @xcite for full details ) .",
    "finally , the ( block ) vectors @xmath39 and @xmath43 maintain the same sparsity structure as that of @xmath42 and @xmath50 , respectively , whereas @xmath44 and @xmath40 are fill - in ( block ) vectors .    for what concerns the solution of the systems associated to the previous parallel factorizations",
    ", there is much parallelism inside .",
    "the solution of the systems with the matrices @xmath27 and @xmath29 may proceed in parallel on the different processors . conversely , the solution of the system with the matrix @xmath28 requires a sequential part , consisting in the solution of a _ reduced system _ with the ( block ) tridiagonal _ reduced matrix _",
    "@xmath58 we observe that the ( block ) size of @xmath59 only depends on @xmath6 and is independent of @xmath5 .    for a matrix @xmath1 with singular or ill - conditioned sub - blocks @xmath12 , the local factorizations may be unstable or even undefined .",
    "consequently , it is necessary to slightly modify the factorization , in order to obtain stable parallel algorithms .",
    "the basic idea is that factorization may produce more than two entries in the _ reduced system_. in other words , the factorization of @xmath12 is stopped when the considered sub - block is ill - conditioned ( or the local factorization with a singular factor ) . as a consequence , the size of the _ reduced system _ is increased as sketched below .",
    "let then @xmath60 where @xmath61 @xmath62 when the sub - block @xmath63 of @xmath12 , @xmath64 is singular , because the block @xmath65 is singular ( i.e. , @xmath66 , in the scalar case ) .",
    "then , @xmath65 is introduced in the _ reduced system_. by iterating this procedure on @xmath67 , we obtain again the factorization , with the only difference that now the _ reduced matrix _ in @xmath59 may be of ( block ) size larger than @xmath68 ( compare with ( [ redmatr ] ) ) .",
    "however , it may be shown that it still depends only on @xmath6 , whereas it is independent of @xmath5 @xcite . consequently , the scalar section of the whole algorithm is still negligible , when @xmath69 .",
    "the parallel algorithms that fall in this class are @xcite :    * _ @xmath35 factorization with partial pivoting _ , defined by setting @xmath70 and @xmath71 where @xmath72 is a permutation matrix such that @xmath73 is the @xmath35 factorization of @xmath74 .",
    "the remaining ( block ) vectors are defined similarly as in the case of the @xmath35 factorization previously described . * _ @xmath75 factorization _ , defined by setting @xmath76 and @xmath77 . in this case",
    "both @xmath78 and @xmath79 are fill - in ( block ) vectors while @xmath80 and @xmath81 maintain the same sparsity structure as that of the corresponding ( block ) vectors in @xmath82 .",
    "factorization ( [ parfact])([parg ] ) , and the corresponding parallel algorithms mentioned above , are easily generalized to matrices with additional non - null elements in the right - lower and/or left - upper corners .",
    "this is the case , for example , of bordered abd ( babd ) matrices ( see figure [ fig4 ] ) and matrices with a circulant - like structure ( see figure [ fig3b ] ) . supposing the non - null elements are located in the right - upper corner ( this is always possible by means of suitable permutation ) , then the coefficient matrix is partitioned in the form @xmath83 where @xmath84 is the smallest rectangular block containing all the corner elements .",
    "a factorization similar to that in ",
    "( the obvious differences are related to the first and last ( block ) rows ) produces a corresponding _ reduced system _ with the _ reduced matrix _",
    "we observe that , for the very important classes of babd and circulant - like matrices ( the latter , after a suitable row permutation , see figure [ fig3 ] ) , both the matrix and the _ reduced matrix _ have the form of a lower block bidiagonal matrix ( i.e. , @xmath86 and @xmath87 for all @xmath10 and @xmath15 ) with an additional right - upper corner block : @xmath88 and @xmath89     after row permutation .",
    "each point represents a ( block ) entry of the matrix.__,width=340 ]    we have also to note that , for this kind of matrices , the overall computational cost of a parallel factorization algorithm has a very small increase . on a sequential machine , supposing to maintain the same partitioning of the matrix on @xmath90 processors , we have a computational cost which is similar to that of any efficient sequential algorithm ( the corner block @xmath84 implies the construction of a fill - in ( block ) vector ) but with better stability properties ( see @xcite ) . for this reason , a method that is widely and efficiently applied to matrices in the form , also on a sequential computer , is cyclic reduction ( see @xcite where cyclic reduction is applied to babd matrices ) .",
    "the numerical solution of ode - bvps leads to the solution of large and sparse linear systems of equations that represent the most expensive part of a bvp code .",
    "the sparsity structure of the obtained problem depends on the methods implemented . in general ,",
    "one - step methods lead to abd or babd matrices , depending on the boundary conditions ( separated or not , respectively ) , while multistep methods lead to block banded systems ( with additional corner blocks in case of non - separated boundary conditions ) @xcite .",
    "the parallel algorithms previously described perfectly cope with this kind of systems .",
    "for this reason we do not investigate further on ode - bvps .",
    "conversely , we shall now consider the application of _ parallel factorizations _ for deriving parallel algorithms for numerically solving ode - ivps , which we assume , for sake of simplicity , to be linear and in the form    @xmath91 , \\qquad y(t_0)=y_0\\in\\rr^m,\\ ] ]    which is , however , sufficient to grasp the main features of the approach @xcite .",
    "let us consider a suitable _ coarse mesh _ , defined by the following partition of the integration interval in :    @xmath92    suppose , for simplicity , that inside each sub - interval we apply a given method with a constant stepsize    @xmath93    to approximate the problem    @xmath94 , \\qquad y(\\tau_{i-1 } ) = y_{0i } , \\qquad i = 1,\\dots , p.\\mbox{~}\\ ] ]    if @xmath95 denotes the solution of problem , and we denote by    @xmath96    the entries of the discrete approximation , then , in order for the numerical solutions of and to be equivalent , we require that ( see and )    @xmath97    for convention , we also set @xmath98    let now suppose that the numerical approximations to the solutions of are obtained by solving discrete problems in the form    @xmath99    where the matrices @xmath100 and @xmath101 , and the vector @xmath102 , do depend on the chosen method ( see , e.g. , @xcite , for the case of block bvms ) and on the problems . clearly , this is a quite general framework , which encompasses most of the currently available methods for solving ode - ivps . by taking into account all the above facts",
    ", one obtains that the global approximation to the solution of is obtained by solving a discrete problem in the form ( hereafter , @xmath103 will denote the identity matrix of dimension @xmath18 ) :    @xmath104\\in\\rr^{mn\\times mn } , & & \\qquad i=2,\\dots , p .",
    "\\end{aligned}\\ ] ]    obviously , this problem may be solved in a sequential fashion , by means of the iteration ( see - ) :",
    "@xmath105    nevertheless , by following arguments similar to those in the previous section , we consider the factorization :    @xmath106 where ( see )    @xmath107\\in\\rr^{mn\\times mn } , \\qquad { \\mbox{\\boldmath$w$}}_i = m_i^{-1}{\\mbox{\\boldmath$v$}}_i\\in\\rr^{mn\\times m}.\\ ] ]    consequently , at first we solve , in parallel , the systems    @xmath108    and , then , ( see and ) recursively update the local solutions ,    @xmath109    the latter recursion , however , has still much parallelism .",
    "indeed , if we consider the partitionings ( see , , and )    @xmath110    then is equivalent to solve , at first , the _ reduced system _",
    "@xmath111    i.e. , @xmath112 after which performing the @xmath6 parallel updates    @xmath113    we observe that :    * the parallel solution of the @xmath6 systems in is equivalent to compute the approximate solution of the following @xmath6 ode - ivps , @xmath114 , \\quad z(\\tau_{i-1 } ) = 0 , \\quad i = 1,\\dots , p,\\ ] ] in place of the corresponding ones in ; * the solution of the _ reduced system _ - consists in computing the proper initial values @xmath115 for the previous ode - ivps ; * the parallel updates update the approximate solutions of the ode - ivps to those of the corresponding ode - ivps in .    [ uno ] clearly , the solution of the first ( parallel ) system in and the first ( parallel ) update in ( [ w ] ) ( see also ) can be executed together , by solving the linear system ( see ) @xmath116 thus directly providing the final discrete approximation on the first processor ; indeed , this is possible , since the initial condition @xmath117 is given .",
    "we end this section by emphasizing that one obtains an almost perfect parallel speed - up , if @xmath6 processors are used , provided that the cost for the solution of the _ reduced system _ and of the parallel updates is small , with respect to that of ( see @xcite for more details ) .",
    "this is , indeed , the case when the parameter @xmath118 in is large enough and the coarse partition can be supposed to be _ a priori _ given .",
    "we now briefly describe the `` parareal '' algorithm introduced in @xcite , showing the existing connections with the parallel method previously described .",
    "this method , originally defined for solving pde problems , for example linear or quasi - linear parabolic problems , can be directly cast into the ode setting via the semi - discretization of the space variables ; that is , by using the method of lines . in more detail , let us consider the problem                                  @xmath134 are suitably small . in the practice , in case of linear operators , problem becomes , via the method of lines , an ode in the form , with @xmath135 a huge and very sparse matrix .",
    "consequently , problems become in the form .",
    "similarly , the propagator @xmath125 consists in the application of a suitable discrete method for approximating the solution of the corresponding @xmath10th problem in , and the coarse propagator @xmath133 describes the application of a much cheaper method for solving the same problem . as a consequence ,",
    "if the discrete problems corresponding to the propagators @xmath136 are in the form , then the discrete version of the recurrence becomes exactly , as well as the discrete counterpart of the matrix form becomes .",
    "we observe that the previous iterative procedure may be very appropriate , when the matrix @xmath135 is large and sparse since , in this case , the computations of the block vectors @xmath137 in , and then of the matrices @xmath138 ( see ) would be clearly impractical",
    ". moreover , it can be considerably improved by observing that @xmath139 consequently , by considering a suitable approximation to the matrix exponential , the corresponding parallel algorithm turns out to become semi - iterative and potentially very effective , as recently shown in @xcite .",
    "p.amodio , j.r.cash , g.roussos , r.w.wright , g.fairweather , i.gladwell , g.l.kraut and m.paprzycki . almost block diagonal linear systems : sequential and parallel solution techniques , and applications .",
    "linear algebra appl . _ * 7 * , no.5 ( 2000 ) 275317 .",
    "p.amodio and f.mazzia .",
    "parallel iterative solvers for banded linear systems . _ lecture notes in comput .",
    "sci . _ * 1196 * , ( numerical analysis and its applications .",
    "l.vulkov , j.wsniewski , and p.yalamov editors , springer , berlin ) , 1724 , 1997 .",
    "p.amodio and g.romanazzi .",
    "babdcr : a fortran 90 package for the solution of bordered abd systems .",
    "_ acm trans .",
    "math . software _ * 32 * , no.4 ( 2006 ) 597608 .",
    "( available on the url : http://www.netlib.org/toms/859 )"
  ],
  "abstract_text": [
    "<S> in this paper we review the parallel solution of sparse linear systems , usually deriving by the discretization of ode - ivps or ode - bvps . </S>",
    "<S> the approach is based on the concept of _ parallel factorization _ of a ( block ) tridiagonal matrix . </S>",
    "<S> this allows to obtain efficient parallel extensions of many known matrix factorizations , and to derive , as a by - product , a unifying approach to the parallel solution of odes .    </S>",
    "<S> ordinay differential equations ( odes ) , initial value problems ( ivps ) , boundary value problems ( bvps ) , parallel factorizations , linear systems , sparse matrices , parallel solution , `` parareal '' algorithm .    65f05 , 15a09 , 15a23 . </S>"
  ]
}