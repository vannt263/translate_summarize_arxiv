{
  "article_text": [
    "learning is defined as any relatively permanent change in behavior resulting from past experience , and a learning system is characterized by its ability to improve in some sense , its behavior with time , tending towards an ultimate goal . in mathematical psychology , models of learning systems were developed about fifty years ago to explain behavior patterns among living organisms .",
    "these , in turn , were adapted to synthesize engineering systems , which could be considered to show `` learning behavior '' .",
    "in fact , in 1971 , tsypkin [ 1 ] argued that seemingly diverse problems in pattern recognition , filtering , identification , and control could be treated in a unified manner as problems in learning , using probabilistic iterative methods .",
    "reinforcement learning aims to find the optimal decision ( or decision rule ) in uncertain environments , on the basis of qualitative and noisy on - line performance feedback provided by an environment , in the form of a reinforcement signal . during the past four decades , learning theory has grown into a huge field in which a very large number of models have been studied . while the models were initially static , the approaches developed were extended to markov decision processes ( mdp ) with finite states , and later to stochastic nonlinear difference equation models for continuous state cases .",
    "our objective in these reports is to successively consider the principal models suggested in the literature in all the different areas , and investigate how multiple model based methods can be developed to increase the speed and accuracy of learning in all of them .",
    "one of the earliest reinforcement learning models in the literature is the `` learning automaton '' .",
    "an agent can perform one action out of a finite set of r actions at every instant into an environment .",
    "the environment responds to each action with either a `` reward '' or a `` penalty '' .",
    "the probability of reward @xmath0 of each action @xmath1(i=1,2, ... r ) is unknown .",
    "the objective is to device a procedure by which the agent learns the best action ( i.e. @xmath2 corresponding to @xmath3 ) .",
    "a very large number of fixed structure and variable structure schemes have been reported in the literature and these have been treated in great detail in [ 2 ] by narendra and thathachar .",
    "developments in the basic learning automaton models in stationary environments led to non - stationary environments and eventually to markov decision processes ( mdp ) .    in the latter , at every state in a finite markov chain , an agent can use one of a finite number of actions .",
    "a transition matrix defines the probabilities with which the state is transferred to any other state under a specific action , and corresponding to such a transfer , there is a reward attached to it .",
    "both the transition matrix and reward probabilities are unknown .",
    "the objective is to determine the optimal action at every state , which results in the optimization of a performance criterion defined over a finite or infinite horizon .",
    "it is this problem which , in course of time , evolved to the optimal control problem of markov processes.defined by unknown nonlinear difference equations . in such problems ,",
    "identification of the system and optimization are carried out simultaneously at every stage .",
    "the large class of learning schemes which have evolved over the years can be broadly categorized into model - free or mode based classes .",
    "model - free methods are direct methods where the optimal policy is learned without trying to identify the system .",
    "this is in sharp contrast to model based methods , where a model of the process is constantly updated and used to determine the optimal policy .",
    "this is entirely in the spirit of indirect adaptive control",
    ". we shall be interested in both direct and indirect schemes , while evaluating the effect of multiple models . in this report",
    "we shall consider only the simplest direct learning scheme i.e. the learning automaton . in section",
    "ii , a brief description of the scheme is provided and norms of behavior are described . in section iii , many of the important but simple learning schemes are reviewed and their properties are described .",
    "simulation results are provided to indicate the nature of convergence that can be expected in such schemes .",
    "learning based on multiple models , which is the principal subject of this report demonstrates that significantly better responses can be achieved by using them .",
    "as stated in section i , the learning automaton is the simplest scheme considered in detail in this report .",
    "it is a direct method and is concerned with an automaton choosing the best action in a random static environment , based on its responses . in spite of its simplicity ,",
    "it addresses many questions which are of fundamental importance in all the learning schemes treated in the reports follow .",
    "we first consider the learning automaton in its simplest form .",
    "this is shown in figure 1 where a random environment e and an automaton a are connected in a feedback loop .",
    "the random environment is described by a finite input set @xmath4=@xmath5 and an output set @xmath6=@xmath7 . 1",
    "is referred to as a `` reward '' and 0 as a `` penalty '' .",
    "corresponding to every action @xmath1 is a reward probability @xmath0 , where @xmath8,\\;\\;\\;\\;\\;\\alpha_i \\in \\boldsymbol{\\alpha}\\ ] ]    the actions @xmath1 can consequently be ordered using @xmath0 so that @xmath1 is better than @xmath9 if @xmath10 , and the best action @xmath2 is the action that corresponds to @xmath11 , where @xmath12=@xmath11        : is a stochastic decision rule which , at every instant ( n+1 ) selects an action from the set @xmath4 , based on the response of the environment at step n , to an action @xmath13 . in the first part of the twentieth century , deterministic automata were studied extensively in the ( then ) soviet union .",
    "however , our interest is strictly in stochastic automata .",
    "the operation of such an automaton can be described as follows .    at every instant",
    "n the stochastic automaton chooses an action @xmath13 , using a probability distribution on the finite action set .",
    "i.e.@xmath14 where @xmath15 , @xmath16 are the action probabilities used for this choice . based on the response of the enviornment , the probabilities @xmath17 are updated for all i. depending upon the qualitative objectives sought after , the desired asymptotic behavior of @xmath17 ( for all i ) can be specified .",
    "in particular , if the objective is to converge to the `` best '' action , the `` learning algorithm '' has to be such that the probability @xmath18 corresponding to the best action converges to 1 in some stochastic sense , while @xmath19 ( where @xmath20 ) converges to zero .    can be described by a sextuple of the form ( @xmath6 , @xmath21 , @xmath4 , @xmath22 , @xmath23 , @xmath24 ) where @xmath25 is the input set , @xmath26 is the set of internal states @xmath27 is the output or action set , q is the state probability vector @xmath28 corresponding to the s states , @xmath23 is the learning algorithm which generates q(n+1 ) from q(n ) and the response , and @xmath24 : @xmath29 is the output function .",
    "the above representation is merely given for the sake of completeness , but almost all the important results in the literature [ 2 ] , have been derived using directly the action probability vector p(n ) . in this paper , we will consequently confine ourselves to the latter .    in the context of psychology",
    ", a learning automaton may be regarded as a model of the learning behavior of an organism .",
    "in such a case , the environment is controlled by the experimenter . in engineering applications such as pattern recognition , identification , or control ( more generally machine learning )",
    ", the controller corresponds to the automaton , while the rest of the system , with all uncertainties , constitutes the environment .",
    "so far , we have assumed that the output of the environment ( or the input to the automaton ) is a binary set ( 0,1 ) i.e a penalty or a reward .",
    "we shall refer to such a model as a p - model .",
    "if the input is a finite set ( say @xmath30 ) with @xmath31 , we shall refer to it as a q model .",
    "if , however , the input to the automaton ( or the output of the environment ) can be any continuous function , we shall refer to it as an s model .",
    "while it may be desirable to use q models or s models in practical applications , the principal concepts can be explained more easily and succinctly using p - models .",
    "while designing learning schemes , an important question that arises even at the initial stages is whether the updating is done in such a manner as to result in a performance compatible with intuitive notions of learning .",
    "initially , when learning starts , it is natural to assume that all actions are chosen with the same probability i.e ( @xmath32 ) .",
    "this implies that the average reward is @xmath33    hence , if the probabilities @xmath17 are varied on - line , the question arises as to whether m(n ) , the average reward , is increasing .",
    "if @xmath34 , the learning scheme is said to be expedient .",
    "if @xmath35=d_{opt}$ ] , the automaton is said to be optimal . in practice , obtaining strictly optimal schemes is a very difficult undertaking .",
    "if the probabilities of all the actions other than that of the optimal tend to zero , the automaton will never choose them asymptotically .",
    "however , comparison with such responses is needed to assure that the action chosen is indeed optimal .",
    "the above considerations lead to the definition of @xmath36- optimality .",
    "a learning scheme is said to be @xmath36-optimal if @xmath37 = d_{opt}-\\epsilon\\ ] ] can be achieved for any arbitrary @xmath38 , by a suitable choice of the parameters of the reinforcement schemes .",
    "@xmath36-optimality consequently implies that the performance of the learning automaton can be made to be as close to the optimal as desired ( but not zero ) .",
    "finally , if e[m(n ) ] increases monotonically i.e.    @xmath39>m(n)\\ ] ]    for all n and all @xmath0 , the automaton is said to absolutely expedient .",
    "in the previous section we defined a stochastic learning automaton and defined some norms by which the performance of the different learning schemes can be evaluated . the different schemes represent different sequential choices of actions out of an input set , to improve the responses from a random environment . in this section",
    "we present , very briefly , the most significant schemes out of the set of all schemes that have been proposed in the literature for this problem . perhaps more important are the mathematical results related to the convergence of the different schemes , since we will be concerned with similar issues while dealing with the principal topic of this paper i.e. the effect of the use of multiple models on the speed of convergence of learning schemes . for detailed treatment of all aspects of learning automata schemes ,",
    "the reader is referred to the book by narendra and thathachar [ 2 ] .      in general ,",
    "most of the reinforcement schemes that have been proposed in the past can be represented by the difference equation @xmath40 where t is a mapping and @xmath41 and @xmath42 are respectively the input chosen and the response obtained form the environment at time n. t determines how this information is to be used for choosing p(n+1 ) at time ( n+1 ) .",
    "if t is linear , the scheme is referred to as a linear scheme .",
    "similarly , nonlinear and hybrid schemes can also be defined ( in the latter case two or more schemes are combined ) .",
    "learning schemes can also be classified on the basis of their asymptotic behavior as expedient , @xmath36-optimal or optimal .",
    "the theory of markov processes forms the principal vehicle for the study of learning schemes .",
    "when these schemes are used in stationary environments , they result in markov processes that are either ergodic or contain absorbing states .",
    "hence ergodic and non - ergodic schemes are also terms used to describe learning automata schemes .",
    "general reinforcement schemes for updating probabilities can be represented as    @xmath43    @xmath44 $ ] when @xmath45    @xmath46 $ ] when @xmath47    @xmath48    we note that when action @xmath1 is performed at instant n , the output @xmath49 can be either a reward @xmath45 or a penalty @xmath50 .",
    "qualitatively , this would suggest that the probability @xmath17 be increased in the former case and decreased in the latter case .",
    "however , it is seen in equation ( 5 ) that this is mathematically represented by a decrease or increase in the probabilities of the actions not chosen . with a reward ,",
    "the probabilities of ( n-1 ) actions are decreased , and that determines the increase in the probability of the action chosen . to conserve probability measure @xmath51 .",
    "similarly for a penalty @xmath52 .",
    "@xmath53 and @xmath54 are continuous non - negative functions and satisfy the inequalities @xmath55 and @xmath56<1 $ ] .    a very large number of learning schemes have been proposed by different authors .",
    "if @xmath53 and @xmath54 are linear functions , the learning schemes are linear .",
    "if even one of the 2n functions is nonlinear , the scheme is defined as nonlinear . in this brief introduction to learning schemes , we shall consider only linear schemes . which are @xmath36-optimal or ergodic .",
    "if the automaton has two actions , the probabilities @xmath57 and @xmath58 are modified only when there is a reward .",
    "for example if @xmath59 results in a reward , the probabilities are updated as :    @xmath60    @xmath61    for a penalty output , no changes are made in all the probabilities .",
    "( i.e. penalty responses are ignored )    if the learning scheme has r actions and @xmath62 results in a reward @xmath63    @xmath64 and @xmath65    the @xmath66 scheme is an @xmath36-optimal scheme and has absorbing states ( two in a two action scheme and r unit vectors in an r - action scheme ) .    if the action probabilities are increased for a reward and decreased for a penalty , we have ergodic schemes which do not have absorbing states .",
    "@xmath67 schemes were proposed to have many of the advantages of @xmath66 schemes , even while enjoying the property of ergodicity . in this case reward and penalty",
    "are not treated symmetrically , with changes ( increase or decrease in probability ) small for a penalty output , compared to the changes when the output is a reward . as is to be expected , the convergence properties of ergodic schemes are very different from those of the @xmath66 scheme .",
    "since our principal interest in this and the following papers is in the use of multiple models for improving convergence rates of learning algorithms , we now consider simulation results obtained using @xmath66 , @xmath68 , and @xmath69 schemes .",
    "numerous linear and nonlinear learning algorithms have been proposed in the literature by various authors , and extensive simulation studies have been carried out on the computer on all of them .",
    "our objective in this section is not to discuss in detail all the proposed schemes , but merely to provide the reader with an understanding of the factors that govern the speed and accuracy of some of the more commonly used schemes .",
    "the simulations included in this section will serve as benchmark examples to be used for comparison purposes while discussing alternate learning schemes .     with 2 actions for a= 0.015,scaledwidth=50.0%,scaledwidth=25.0% ]     with 10 actions for a= 0.015,scaledwidth=50.0%,scaledwidth=25.0% ]     with 2 actions for a= 0.015 , b=0.005,scaledwidth=50.0%,scaledwidth=25.0% ]    in figure 2 , the simulation study of an @xmath66 algorithm with two actions is shown .",
    "the only parameter that can be adjusted in this case is `` a '' . for the experiment ,",
    "a=0.015 was chosen .",
    "the convergence time for a typical simulation is seen to be approximately 500 steps .",
    "increasing `` a '' improves the rate of convergence , but also increases the probability of convergence to the wrong action .    in figure 3 ,",
    "a ten - action case is considered and the convergence time on a sample path is seen to be 2000 steps . as stated earlier , @xmath68 schemes are ergodic and hence sample paths do no converge to any fixed probability as in the @xmath66 case , but converge in distribution . once again , for a two - action case , the probability of the best action reaches 0.95 in 500 steps .    in the following sections",
    ", we will be interested in the improvement in convergence that can be achieved using modifications in the learning schemes ( i.e 500 steps for 2 actions and 2000 steps for 10 actions ) .",
    "thus far we have discussed ( direct ) learning algorithms in which at the end of every trial the probability vector p(n ) is updated based on the response of the environment .",
    "there are also a number of other updating schemes proposed in the literature for variable structure stochastic automata . in the `` special issue on learning automata of the journal of cybernetics and information science '' edited by the first author [ 3 ] , tyspkin and poznyak ( 1977 ) attempt to unify the various learning algorithms within the general framework of stochastic approximation .",
    "thathachar and sastry ( 1985 ) [ 4 ] incorporate estimates of the reward probabilities in the updating schemes and prove @xmath36-optimality .",
    "such schemes have been called estimator algorithms and have a higher rate of convergence in stationary random environments as compared to @xmath66 and @xmath68 schemes .",
    "one version of the estimator algorithm is called the pursuit algorithm and is described in the following section .      in the @xmath66 or @xmath68 schemes ,",
    "the efficiency of an action was judged on the basis of the output produced by the action at one instant of time . however , our interest is in the action @xmath2 with the largest reward probability @xmath11 . to estimate this ,",
    "the effect of every action over all the past attempts is stored in this procedure . if @xmath70 is the estimate of @xmath0 based on all the past attempts of the @xmath71 action @xmath1 , the probability vector p(n ) is adjusted at every instant in the direction of the current optimal action based on these estimates .",
    "the three steps in the procedure are listed below : + ( 1 ) based on the probability distribution p(n ) at instant n an action @xmath41=@xmath1 is chosen which produces a reward or a penalty .",
    "+ ( 2 ) the estimate @xmath72 of the reward probability is updated on the basis of the above response to @xmath73 .",
    "if the highest value of @xmath73 is @xmath74(j=1,2, ... ,r ) , the action probability vector p(n ) is modified as @xmath75 where @xmath76 is a scalar with @xmath77 , and l is a unit vector with unity as the @xmath71 element and all other elements zero .",
    "this orients the vector p(n ) more towards the optimal action .    in the 1960s , in adaptive control",
    ", it was realized that adaptive parameters should be adjusted on the basis of all the past data , rather than instantaneous values .",
    "the modification described here for learning algorithms was motivated by this .",
    "the pursuit algorithm can also be shown to be @xmath36-optimal , and is significantly faster than the @xmath66 and @xmath68 schemes as shown in the simulations included below for two action and ten action schemes .",
    "figure 5 and 6 indicate typical responses of a two action and a ten action learning automaton using a pursuit algorithm .",
    "it is seen that the typical times for convergence are 300 and 1500 respectively , which are significantly faster than obtained using those simple @xmath66 schemes .",
    "from the previous discussions , a model can be set up to estimate the reward probability of each action . following this ,",
    "a certainty equivalence principle can be used to decide which action appears to be the best , and the action probability vector can be moved by a small step towards the unit vector corresponding to that action .",
    ": let @xmath78 be m fixed probabilities .",
    "let these values represent fixed estimates of the reward @xmath0 corresponding to action @xmath1 . if similar models are also used for all the r actions , there are a total of @xmath79 probabilities that are used to determine the strategy at any instant .",
    "let the output of the @xmath80 action @xmath1 at stage n consist of @xmath81 rewards and @xmath82 penalties .",
    "then corresponding to any probability , ( say @xmath83 ) , the likelihood of the event is @xmath84 .",
    "similarly , @xmath85 are computed and the maximum @xmath86 is determined . at this stage , since the maximum value of @xmath86 for all i actions is known , the probability vector ( as in the pursuit algorithm ) is adjusted incrementally in the direction of the unit vector corresponding to that action .    the relation between the models described above and the pursuit algorithm has been studied extensively , but due to space limitations is not included here . only the simulation for the 10 action case",
    "is shown in figure 7 .",
    "it is seen that there is a substantial improvement both in the speed of convergence as well as the smoothness of the optimal probability .",
    "the convergence time of 300 is seen to be a significant improvement over the 2000 steps needed with the @xmath66 algorithm , and 1500 steps needed with the pursuit algorithm .",
    "the result given in figure 7 are not conclusive but merely indicate that considerably more work needs to be done using multiple models .        :",
    "even though the learning automaton is a direct scheme , the procedure described above was used to estimate which of the fixed probabilities ( models ) was closest to the unknown reward probability corresponding to each action .",
    "a natural extension of the above procedure is to make all the models adaptive ( i.e. adjust the probabilities @xmath87 ) adaptively , on the basis of the responses of the environment to the various actions .",
    "it is immediately evident that all values must converge to the reward @xmath0 for the @xmath80 action , as n tends to infinity .",
    "this fact can be made use of to determine where @xmath11 lies , and the action corresponding to it .",
    "this will be treated in detail in future papers .",
    "in adaptive control , multiple model based approaches have been very effective in indirect control i.e. those situations where identification of the process to be controlled precedes taking a control action .",
    "if a model of the process is known , it can be used to generate a control input which results in the desired response of the model , and consequently ( it is hoped ) of the plant .",
    "if multiple models are used , methods for choosing the `` best '' model or `` a combination of the models '' have been discussed in the adaptive literature .",
    "our objective is to use a similar approach in the future for learning schemes as well .    in the previous section ,",
    "`` multiple models '' were used for a direct learning scheme .",
    "this merely underscores the fact that the term `` model '' can be loosely interpreted as a description of the system which permits a decision to be made concerning its behavior .    in this section ,",
    "we provide a sketch as to how multiple model approaches can be extended to reinforcement learning problems involving dynamic environments .",
    "in such a case , the details of the methods and their theoretical analysis are considerably more complex than those for a static environment involving learning automata , and will be described in detail in future papers .    : a markov decision process ( mdp ) is defined by the following quantities :    * the state space @xmath88 , a finite set , * the action set @xmath89 , a finite set , listing all actions available to the agent in any state * a state transition probability function @xmath90 * an immediate payoff function @xmath91 $ ] where 0 corresponds to no reward and 1 corresponds to reward .",
    "the objective of the agent is to maximize the overall discounted reward , i.e. , the objective is not merely to maximize the reward for the next transition , but to maximize the reward over all future transitions made from an initial state .    : a nonlinear dynamical environment is described by the equation @xmath92\\ ] ] where x(k ) , u(k ) , and n(k ) are the state , agent action , and noise respectively at instant k. the performance index is given by @xmath93\\ ] ] where @xmath94 is a discount factor .",
    "the objective of the agent is to determine a policy @xmath95 $ ] to optimize @xmath96 .    in the model - based approaches to reinforcement learning , an identification model of the environment ( in the form of @xmath97 and @xmath98 matrices for discrete case , and @xmath99 function in the continuous case )",
    "is estimated during on - line learning and this model is used in conjunction with dynamic programming or the hamilton - jacobi - bellman equation to optimize j.    in the discrete - state mdp formulation , since each element of the @xmath100 matrix is a probability , multiple fixed or adaptive models can be set up to estimate them , and the best model can be selected in a manner similar to that used for a learning automaton .",
    "the best selected model(s ) can then be used to carry out the dynamic programming computations .    in the continuous state case , instead of only one such model , a bank of identification models , as described below , is used : @xmath101      i=1 , \\ldots , n\\ ] ] and these models are combined in some manner to compute the predicted state , with the aim of achieving higher accuracy than each of the individual models .",
    "it is worth noting that the structure of the models need not necessarily be the same , but can in principle be heterogeneous .",
    "this approach , as stated earlier , has been a popular one in the adaptive control field , to improve transient performance of adaptive control systems .",
    "the authors believe that such an approach can also be used in reinforcement learning systems as one way to speed up learning .",
    "in this paper , a very simple learning scheme ( i.e. the learning automaton ) was introduced and the properties of different learning algorithms were discussed . it was then shown that approaches , similar to multiple model based approaches in adaptive control , lead to faster and smoother convergence than conventional algorithms .",
    "some comments were made towards the end of the paper as to how the approaches may be extended to significantly more complex learning schemes in dynamic environments .",
    "tsypkin , yakov zalmanovich .",
    "adaptation and learning in automatic systems .",
    "new york : academic press , 1971 .",
    "narendra , kumpati s. , and mandayam al thathachar .",
    "learning automata : an introduction .",
    "courier dover publications , 2012 .",
    "special issue on learning automata of the journal of cybernetics and information science , 1977 l thathachar , m. , and p. shanti sastry .",
    "`` a new approach to the design of reinforcement schemes for learning automata . '' systems , man and cybernetics , ieee transactions on 1 ( 1985 ) : 168 - 175 ."
  ],
  "abstract_text": [
    "<S> this is the first of a series of papers that the authors propose to write on the subject of improving the speed of response of learning systems using multiple models . during the past two decades </S>",
    "<S> , the first author has worked on numerous methods for improving the stability , robustness , and performance of adaptive systems using multiple models and the other authors have collaborated with him on some of them . </S>",
    "<S> independently , they have also worked on several learning methods , and have considerable experience with their advantages and limitations . in particular , they are well aware that it is common knowledge that machine learning is in general very slow . </S>",
    "<S> numerous attempts have been made by researchers to improve the speed of convergence of algorithms in different contexts . in view of the success of multiple model based methods in improving the speed of convergence in adaptive systems , </S>",
    "<S> the authors believe that the same approach will also prove fruitful in the domain of learning . in this paper , a first attempt is made to use multiple models for improving the speed of response of the simplest learning schemes that have been studied . </S>",
    "<S> i.e. learning automata . </S>"
  ]
}