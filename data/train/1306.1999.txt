{
  "article_text": [
    "gaussian process ( gp ) models provide a flexible , probabilistic approach to regression and are widely used .",
    "however , application of gp models to large data sets is challenging as the memory and computational requirements scale as @xmath0 and @xmath1 respectively , where @xmath2 is the number of training data points .",
    "various sparse gp approximations have been proposed to overcome this limitation .",
    "a unifying framework of existing sparse methods is given in @xcite .",
    "we consider the stationary sparse spectrum gp regression model introduced by @xcite , where the spectrum of the covariance function is sparsified instead of the usual spatial domain .",
    "the ssgp algorithm developed by @xcite for fitting this model uses conjugate gradients to optimize the marginal likelihoood with respect to the hyperparameters and spectral points .",
    "comparisons with other state - of - the - art sparse gp approximations such as the fully independent training conditional model ( first introduced as sparse pseudo - input gp in * ? ? ?",
    "* ) and the sparse multiscale gp @xcite , showed that ssgp yielded significant improvements .",
    "however , optimization with respect to spectral frequencies increases the tendency to underestimate predictive uncertainty and poses a risk of overfitting in the ssgp algorithm .    in this paper",
    ", we develop a fast variational approximation scheme for the sparse spectrum gp regression model , which enables uncertainty in covariance function hyperparameters to be treated .",
    "in addition , we propose an adaptive local neighbourhood approach for dealing with nonstationary data .",
    "although accounting for hyperparameter uncertainty may be of little importance when fitting globally to a large data set , local fitting within neighbourhoods results in fitting to small data sets even if the full data set is large , and here it is important to account for hyperparameter uncertainty to avoid overfitting .",
    "our examples show that our methodology is particularly beneficial when combined with the local fitting approach for this reason .",
    "our approach also allows hierarchical models involving covariance function parameters to be constructed .",
    "this idea is implemented in the context of functional longitudinal models by mensah _",
    "( 2014 ) so that smoothness properties of trajectories can be related to individual specific covariates .",
    "gps have diverse applications and various methods have been developed to overcome their computational limitations for handling large data sets .",
    "a good summary of approximations used in modelling large spatial data sets is given in @xcite .",
    "computational costs can also be reduced through local gp regression as a much smaller number of training data is utilized in each partition .",
    "this approach has been considered in machine learning ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) and in spatial statistics ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "@xcite propose fitting gp models in local neighbourhoods which are defined online for each test point .",
    "however , covariance hyperparameters are estimated only for a subset of all possible local neighbourhoods .",
    "different local experts are then combined using a mixture model capable of handling multimodality .",
    "our idea of using adaptive nearest neighbours in gp regression is inspired by techniques in classification designed to mitigate the curse of dimensionality @xcite .",
    "for each test point , we fit two models . in the first instance",
    ", the neighbourhood is determined using the euclidean metric .",
    "lengthscales estimated from the first fitting are then used to redefine the distance measure determining the neighbourhood for fitting the second model .",
    "experiments suggest that this approach improves prediction significantly in data with nonstationarities , as hyperparameters are allowed to vary across neighbourhoods adapted to each query point . weighting dimensions according to lengthscales downweights variables of little relevance and also leads to automatic variable selection .",
    "our approach differs from methods where local neighbourhoods are built sequentially to optimize the choice of the neighbourhood .",
    "examples include @xcite and @xcite , where the gaussian likelihood is approximated by the use of an ordering and conditioning on a subset of past observations . in @xcite , an empirical bayes",
    "mean - square prediction error criterion is optimized .",
    "while greedy searches usually rely on fast updating formulae available only in the gaussian case , our approach works in non - gaussian settings as well . @xcite",
    "suggest making neighbourhoods non - local to improve learning of covariance parameters , but local neighbourhoods may work better when the motivation is to handle nonstationarity .",
    "@xcite make a connection between discrete spatial markov random fields and continuous gaussian random fields with covariance functions in the matrn class .    for fitting the sparse spectrum gp regression model",
    ", we derive a variational bayes ( vb , * ? ? ?",
    "* ) algorithm that uses nonconjugate variational message passing @xcite to derive fast and efficient updates .",
    "vb methods approximate the intractable posterior in bayesian inference by a factorized distribution .",
    "this product density assumption is often unrealistic and can lead to underestimation of posterior variance @xcite .",
    "however , optimization of a factorized variational posterior can be decomposed into local computations that only involve neighbouring nodes in the factor graph and this often gives rise to fast computational algorithms .",
    "vb has also been shown to be able to give reasonably good estimates of the marginal posterior distributions and excellent predictive inferences ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "variational message passing @xcite is a general - purpose algorithm that allows vb to be applied to conjugate - exponential models @xcite .",
    "nonconjugate variational message passing extends variational message passing to nonconjugate models by assuming that the factors in vb are members of the exponential family .",
    "we use nonconjugate variational message passing to derive efficient updates for the variational posteriors of the lengthscales , which are assumed to be gaussian .",
    "@xcite use vb for spatial modelling via gp , where they also treat uncertainty in the covariance function hyperparameters .",
    "however , they propose using importance sampling within each vb iteration to handle the intractable expectations associated with the covariance function hyperparameters .",
    "variational inference has also been considered in machine learning for sparse gps that select the inducing inputs and hyperparameters by maximizing a lower bound to the exact marginal likelihood @xcite , and heteroscedastic gp regression models where the noise is input dependent @xcite .",
    "vb is known to suffer from slow convergence when there is strong dependence between variables in the factors . to speed up convergence ,",
    "@xcite propose parameter expanded vb to reduce coupling in updates , while @xcite considered partially noncentered parametrizations . here",
    ", we introduce an adaptive strategy to accelerate convergence in nonconjugate variational message passing , which is inspired by adaptive overrelaxed bound optimization methods @xcite .",
    "previously , @xcite showed that nonconjugate variational message passing is a natural gradient ascent algorithm with step size one and step sizes smaller than one correspond to damping .",
    "here , we propose using step sizes larger than one which can help to accelerate convergence in fixed point iterations algorithms ( see * ? ? ? * ) . instead of searching for the optimal step size , we use an adaptive strategy which ensures that the lower bound increases after each cycle of updates .",
    "empirical results indicate significant speedups . @xcite",
    "considered combining parameter - wise updates to form a diagonal direction for a line search .",
    "a general iterative algorithm for computing vb estimators ( defined as means of variational posteriors ) has also been proposed by @xcite and its convergence properties investigated for normal mixture models .",
    "section [ ssgpmodel ] describes the sparse spectrum gp regression model and section [ variational inference ] develops the nonconjugate variational message passing algorithm for fitting it .",
    "section [ adaptive strategy ] presents an adaptive strategy for accelerating convergence in nonconjugate variational message passing .",
    "section [ pred distn ] discusses how the predictive distribution can be estimated and the measures used for performance evaluation .",
    "section [ neigh ] describes the adaptive neighbourhood approach for local regression .",
    "section [ eg ] considers examples including real and simulated data and section [ conclusion ] concludes .",
    "given a data set @xmath3 , we assume each output @xmath4 is generated by an unknown latent function @xmath5 evaluated at the input , @xmath6 , and independently corrupted by additive gaussian noise such that @xmath7 a gp prior is assumed over @xmath8 for @xmath9 .",
    "for any set of inputs @xmath10 , @xmath11^t$ ] has a joint gaussian distribution , @xmath12 , where @xmath13 is a covariance matrix .",
    "we assume that the mean of the process is zero .",
    "it is straightforward to allow for a nonzero mean , but a zero mean is sufficient for the examples in this paper .",
    "the entries of @xmath13 are given by @xmath14 , where @xmath15 and @xmath16 is some stationary covariance function .",
    "for example , we consider the stationary squared exponential covariance function , @xmath17 where @xmath18 , @xmath19^t)$ ] and @xmath20 for @xmath21 .",
    "@xcite introduced a novel perspective on gp approximation by sparsifying the spectrum of the covariance function .",
    "they considered the linear regression model , @xmath22 where @xmath23 , @xmath24 are independent and identically distributed as @xmath25 and @xmath26 is a @xmath27-dimensional vector of spectral frequencies .",
    "the power spectral density of a stationary covariance function @xmath16 is @xmath28 and @xmath29 is proportional to a probability density @xmath30 such that @xmath31 .",
    "when @xmath32 are drawn randomly from @xmath30 , @xcite showed that can be viewed as a sparse gp that approximates the full stationary gp by replacing the spectrum with a discrete set of spectral points .    from",
    ", the probability density @xmath30 associated with the squared exponential covariance function in is @xmath33 .",
    "if @xmath32 is generated randomly from @xmath34 , then @xmath35 is a random sample from @xmath30 . from",
    ", a sparse gp approximation to @xmath8 is @xmath36 , \\label{spec2}\\end{aligned}\\ ] ] where @xmath37 , @xmath38^t$ ] is a vector of lengthscales and @xmath39 denotes element by element multiplication of two vectors . within the sparse gp approximation",
    ", we can allow the components of @xmath40 to be negative .",
    "let @xmath41^t$ ] and @xmath42^t$ ] .",
    "note that in ( [ sqexpcov ] ) @xmath43 appears as its square in @xmath44 so that @xmath45 remains positive semidefinite .",
    "ignoring the non - negativity constraint allows us to use a gaussian variational posterior for @xmath40 .",
    "the associated expectations in the variational lower bound can then be derived in closed form ( see section [ variational inference ] ) .",
    "this is a highly novel aspect of our algorithm allowing a fast method that still handles covariance function hyperparameter uncertainty .",
    "this is especially important when fitting locally as described in section 6 where training datasets may be small .",
    "the squared exponential covariance function also implements automatic relevance determination since the magnitude of @xmath43 is a measure of how relevant the @xmath46th variable is .",
    "when @xmath43 goes to zero , the covariance function becomes almost independent of the @xmath46th variable , essentially removing it from inference .",
    "see @xcite for more discussion .    using the stationary sparse gp approximation in ( [ spec2 ] ) , we consider variational inference for @xmath47 \\\\",
    "+ \\epsilon_i , \\;\\;\\text{where}\\;\\ ; \\epsilon_i \\sim n(0,\\gamma^2).\\end{gathered}\\ ] ] let @xmath48^t$ ] , @xmath49^t$ ] , @xmath50^t$ ] and @xmath51^t$ ] , where @xmath52^t.\\end{gathered}\\ ] ] then this model can be written as @xmath53 where @xmath54 . for bayesian inference , we assume the priors : @xmath55 , @xmath56 and @xmath57 , where the hyperparameters @xmath58 , @xmath59 , @xmath60 and @xmath61 are assumed to be known .",
    "the density function of a random variable @xmath62 distributed as @xmath63 is @xmath64 , where @xmath65 and @xmath66 .",
    "while inverse - gamma priors are more commonly used for variance parameters in hierarchical models due to the conditional conjugacy relationship with gaussian families , @xcite recommends use of the half - cauchy family as priors because resulting inferences can be sensitive to inverse - gamma hyperparameters when variance estimates are close to zero .",
    "we made the same observation in our experiments with inverse - gamma priors for @xmath67 and @xmath68 .",
    "in particular , predictive inferences are sensitive to inverse - gamma priors in local regressions ( see section [ neigh ] ) , where only a small neighbourhood is used for fitting at each test point .",
    "we consider variational inference for the sparse spectrum gp regression model in ( [ model2 ] ) .",
    "let @xmath69 be the set of unknown parameters and @xmath70 be the true posterior of @xmath71 . in variational approximation ,",
    "@xmath70 is approximated by a @xmath72 for which inference is more tractable , and the kullback - leibler divergence between @xmath72 and @xmath70 is minimized .",
    "this is equivalent to maximizing a lower bound @xmath73 on the log marginal likelihood @xmath74 , where @xmath75 , @xmath76 and @xmath77 denotes expectation with respect to @xmath72 .",
    "next , we review some important results in vb and nonconjugate variational message passing , which will be used to construct the variational algorithm . in vb , @xmath72 is assumed to factorize into @xmath78 for some partition @xmath79 of @xmath71 .",
    "the optimal densities may be obtained from @xmath80 where @xmath81 denotes expectation with respect to @xmath82 ( see , e.g. * ? ? ?",
    "* ) . for conjugate - exponential models ,",
    "the optimal densities have the same form as the priors and it suffices to update the parameters of @xmath83 , such as in variational message passing @xcite .",
    "however , for nonconjugate models , the optimal densities will not belong to recognizable density families .",
    "apart from the product assumption , nonconjugate variational message passing @xcite further assumes each @xmath84 is a member of some exponential family , that is , @xmath85 where @xmath86 is the vector of natural parameters and @xmath87 are the sufficient statistics .",
    "hence , we only have to find each @xmath86 that maximizes the lower bound @xmath73 .",
    "nonconjugate variational message passing can be interpreted as a fixed point iterations algorithm where updates are obtained from the condition that the gradient or natural gradient ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ) of @xmath73 with respect to each @xmath86 is zero when @xmath73 is maximized .",
    "suppose @xmath88 , @xmath89 and let @xmath90 denote the variance - covariance matrix of @xmath91 .",
    "provided @xmath92 is invertible , @xcite showed that the natural gradient of @xmath73 with respect to @xmath86 is @xmath93 therefore , the update for each @xmath86 is @xmath94 where the summation is over all factors in @xmath95 , the neighbourhood of @xmath96 in the factor graph of @xmath97 .",
    "updates in nonconjugate variational message passing reduce to those in variational message passing when the factors @xmath98 are conjugate ( see * ? ? ? * ; * ? ? ?",
    "however , unlike variational message passing , the lower bound @xmath73 is not guaranteed to increase at each step and convergence problems may be encountered sometimes .",
    "@xcite suggest using damping to fix convergence problems .    when @xmath99 , @xcite showed that the update in can be simplified to @xmath100^{-1 } , \\\\",
    "\\mu_{\\theta_i}^q & \\leftarrow \\mu_{\\theta_i}^q + \\sigma_{\\theta_i}^q\\sum_{a \\in n(\\theta_i)}\\negthickspace \\frac{\\partial s_a}{\\partial   \\mu_{\\theta_i}^q}.   \\end{aligned}\\ ] ] here @xmath101 denotes the vector obtained by stacking the columns of a matrix @xmath102 under each other , from left to right in order .",
    "we consider a variational approximation of the form @xmath103 from , the optimal densities @xmath104 and @xmath105 are @xmath106 and @xmath107 , where @xmath108 and @xmath109 , @xmath110 , @xmath111 . the variational parameter updates of @xmath112 , @xmath113 , @xmath114 and @xmath115 can also be derived from . as @xmath116 can be arbitrarily large or small ,",
    "@xcite suggest evaluating @xmath117 efficiently using quadrature .",
    "a discussion can be found in appendix b of @xcite and we follow their methods . for @xmath118 , @xmath119 is not a conjugate factor and we use nonconjugate variational message passing .",
    "assuming @xmath120 , updates for @xmath121 and @xmath122 can be derived using and matrix differential calculus ( see * ? ? ?",
    "the expectations with respect to @xmath123 in are given in appendices a and b. let @xmath124 denote the set of variational parameters .",
    "an iterative scheme for finding @xmath125 is given in algorithm [ alg1 ] .",
    "a unique aspect of our variational scheme is the way covariance function uncertainty is handled , with the expectations involving @xmath40 in the lower bound computable in closed form . in particular , @xmath126 and @xmath127",
    "can be evaluated in closed form ( see appendix a ) .",
    "let @xmath128 be partitioned as @xmath129 $ ] where @xmath102 , @xmath130 and @xmath131 are all @xmath132 matrices . in algorithm 1 , we define @xmath133 , @xmath134 for @xmath135 , @xmath136 , @xmath137 .    the lower bound @xmath73 defined in is commonly used for monitoring convergence .",
    "it can be evaluated in closed form ( see appendix b ) and is given by @xmath138 the above expression applies only after the updates in steps 5 and 6 of algorithm [ alg1 ] have been made .",
    "in the sparse spectrum gp regression model , @xmath139 and @xmath140 are intimately linked .",
    "each time the lengthscales @xmath141 are changed by a small amount , the amplitudes ( @xmath140 ) will have to respond to this change in order to match the observed @xmath142 . in",
    ", we have assumed that the variational posteriors of @xmath40 and @xmath140 are independent so that expectations with respect to @xmath123 are tractable and closed form updates can be derived for a fast algorithm .",
    "however , strong dependence between @xmath40 and @xmath140 implies that only small steps can be taken in each cycle of updates and a large number of iterations will likely be required for algorithm [ alg1 ] to converge .    to accelerate convergence , we propose modifying the updates in steps 1 and 2 .",
    "let @xmath143 be the natural parameter of @xmath118 and @xmath144 be the update of @xmath143 in nonconjugate variational message passing .",
    "@xcite showed that nonconjugate variational message passing is a natural gradient ascent method with step size one . at iteration @xmath145",
    ", we consider @xmath146 where @xmath147 . when @xmath148 , reduces to the update in nonconjugate variational message passing .",
    "taking @xmath149 may be helpful when updates in nonconjugate variational message passing fail to increase @xmath73 . from our experiments ,",
    "instability in algorithm 1 usually occur within the first few iterations . beyond that , the algorithm is usually quite stable and taking larger steps with @xmath150 can result in significant speed - ups",
    ".     indicates conventional path in fixed point iterations while the dot dash line indicates path to convergence with a step size greater than 1.,scaledwidth=49.0% ]    recall that nonconjugate variational message passing is a fixed point iterations algorithm .",
    "figure [ fixedpointplot ] illustrates in a single variable case ( where we are solving @xmath151 ) how taking steps larger than one can accelerate convergence . instead of taking @xmath152 ,",
    "consider @xmath153 , where @xmath154 and @xmath150 .",
    "the solid line starting from @xmath155 indicates the conventional path in fixed point iterations while the dot dash line indicates the path with a step size greater than 1 .",
    "the dot dash line moves towards the point of convergence faster than the solid line .",
    "however , it may overshoot if @xmath156 is too large . in algorithm [ alg2 ] , we borrow ideas from @xcite to construct an adaptive algorithm where @xmath156 is allowed to increase by a factor @xmath157 after each cycle of updates whilst @xmath73 is on an increasing trend and we revert to @xmath148 when @xmath73 decreases .    the adaptive nonconjugate variational message passing algorithm",
    "is given in algorithm [ alg2 ] .    in appendix c , we show that reduces to the updates : @xmath158^{-1 }   \\\\ \\text{and } \\;\\ ;",
    "\\mu_\\lambda^q \\leftarrow \\mu_\\lambda^q + a_t\\ ,   \\sigma_\\lambda^q \\sum_{a \\in n(\\lambda ) } \\frac{\\partial s_a}{\\partial   \\mu_\\lambda^q}. \\end{gathered}\\ ] ] step 3(b ) has been added as a safeguard as the updated @xmath122 may not be symmetric positive definite due to rounding errors or when @xmath156 is large . in this case , we propose reducing the step size by a factor @xmath159 until all eigenvalues of @xmath122 are positive .",
    "it is useful to insert step 3(b ) in algorithm [ alg1 ] after @xmath122 has been updated as well as it can serve as damping . for both algorithms [ alg1 ] and [ alg2 ] , we initialize @xmath121 as @xmath160^t$ ] ( which is one half of the amplitudes of the inputs after any rescaling ) , @xmath122 as @xmath161^t$ ] , @xmath115 as @xmath162 , @xmath114 as @xmath163 , and @xmath164 and @xmath165 are initialized using the updates in steps 34 of algorithm [ alg1 ] .",
    "we set the maximum number of iterations as 500 and the algorithms are deemed to have converged if the relative increase in @xmath73 is less than @xmath166 .",
    "@xcite recommend taking the factor @xmath159 to be close to but more than 1 .",
    "using this as a guide , we have experimented with @xmath159 taking values 1.1 , 1.5 and 2 . while all these values lead to improvement in efficiency , we find @xmath167 to be more favourable , as the step sizes increase rather slowly when @xmath168 and too fast when @xmath169 , leading to many failed attempts to improve @xmath73 .",
    "while algorithm [ alg2 ] does not necessarily converge to the same local mode as algorithm [ alg1 ] , results from the two algorithms are usually very close .",
    "algorithm [ alg2 ] sometimes demonstrates the ability to avoid local modes with the larger steps that it takes .",
    "we compare and quantify the performance of the two algorithms in section [ pendulum eg ] .",
    "note that in algorithm 2 , each failed attempt to improve @xmath73 is also counted as an additional iteration in step 5(b ) even though step 1 does not have to be reevaluated .",
    "we note that algorithms 1 and 2 are not guaranteed to converge due to the fixed point updates in nonconjugate variational message passing .",
    "however , convergence issues can usually be mitigated by rescaling variables and varying the initialization values .",
    "as the fixed point updates may not result in an increase in @xmath73 , it is possible to compute @xmath73 after performing the updates and reduce @xmath156 if necessary .",
    "however , this requires computing a lower bound of a more complex form than at each iteration .",
    "our experiments indicate that a decline in @xmath73 is often due to @xmath122 not being symmetric positive definite , and hence installing step 3(b ) suffices in most cases .",
    "we also find that checking the simplified form of @xmath73 in at the end of each cycle and simply reverting @xmath156 to 1 if necessary is more economical .",
    "if premature stopping occurs in algorithms 1 or 2 due to a decrease in the lower bound at some iteration , this can be detected by examination of the lower bound values and remedied if needed by damping where values @xmath149 are considered .",
    "let @xmath170 and @xmath171 be the training and testing data sets respectively .",
    "let @xmath172 be the set of spectral frequencies randomly generated from @xmath34 .",
    "bayesian predictive inference is based on the predictive distribution , @xmath173 assuming @xmath174 is conditionally independent of @xmath131 given @xmath140 , @xmath40 and @xmath175 . we replace @xmath176 with our variational approximation @xmath177 so that @xmath178 from ( [ postpred ] ) , the posterior predictive mean of @xmath174 is @xmath179 where @xmath180^t\\end{gathered}\\ ] ] and @xmath181 can be computed using results in appendix a. the posterior predictive variance is @xmath182    in the examples , we follow @xcite and evaluate performance using two quantitative measures : normalized mean square error ( nmse ) and mean negative log probability ( mnlp ) .",
    "these are defined as @xmath183 the mnlp is implicitly based on a normal predictive distribution for @xmath174 with mean @xmath184 and variance @xmath185 , @xmath186 .",
    "we propose a new technique of obtaining predictive inference by fitting models locally using adaptive neighbourhoods .",
    "our proposed approach consists of two stages : for each test point @xmath187 , @xmath186 ,    1 .",
    "we first find the @xmath16 nearest neighbours of @xmath187 in @xmath131 ( that are closest to @xmath187 in terms of euclidean distance ) and denote the index set of these @xmath16 neighbours by @xmath188 .",
    "we use algorithm 2 to fit a sparse spectrum gp regression model , @xmath189 , to @xmath190 .",
    "2 .   next , we use the variational posterior mean of the lengthscales , @xmath121 , from @xmath189 to define a new distance measure : @xmath191 where the dimensions are weighted according to @xmath192 .",
    "this will effectively downweight or remove variables of little or no relevance . using this new distance measure",
    ", we find the @xmath16 nearest neighbours of @xmath187 in @xmath131 and denote the index set of these @xmath16 neighbours by @xmath193 .",
    "we use algorithm 2 to fit a sparse spectrum gp regression model , @xmath194 , to @xmath195 and use the variational posterior from @xmath194 for predictive inference .    in summary ,",
    "the first fitting @xmath196 is used to find out which variables are more relevant in determining the output . from",
    ", a large value of @xmath197 indicates that the covariance drops rapidly along the dimension of @xmath198 and hence the neighbourhood should be shrunk along the @xmath198th dimension .",
    "using @xmath121 from the first fit as an estimate of the lengthscales , the neighbourhood is then adapted before performing a second fitting @xmath199 to improve prediction .",
    "we do not recommend iterating the fitting process further since this may result in cyclical behaviour with the neighbourhood successively expanding and contracting along a certain dimension as the iterations proceed .",
    "in the examples , when the ssgp algorithm is implemented using this adaptive neighbourhood approach , we replace the variational posterior mean value @xmath121 ( which does not exist for the ssgp method since it does not estimate a variational posterior distribution for @xmath40 ) by the point estimates of the lengthscales @xmath200 obtained by the ssgp approach .",
    "the adaptive neighbourhood approach is well - placed to handle data with nonstationarities as stationarity is only assumed locally and local fitting can adapt the noise and the degree of smoothing to the nonstationarities .",
    "adapting the neighbourhood can also be very helpful in improving prediction when there are many irrelevant variables due to automatic relevance determination implemented via the lengthscales .",
    "a major advantage of the variational approach is that it allows uncertainty in the covariance hyperparameters to be modelled within a fast computational scheme .",
    "this is especially important when fitting using local neighbourhoods as plug - in approaches to estimating hyperparameters will tend to underestimate predictive uncertainty when the data set is small .",
    "this approach is advantageous for dealing with large data sets as well . as we only consider fitting models to a small subset @xmath16 of data points at each test point , a smaller number of basis functions @xmath201 might suffice . while the computational requirements grow linearly with the number of prediction locations , this approach is trivially parallelizable to get a linear speed - up with the number of processors",
    "we compare the performance of the variational approach with the ssgp algorithm using three real data sets : the pendulum data set , the rainfall - runoff data set and the auto - mpg data set .",
    "the implementation of ssgp in matlab is obtained from http://www.tsc.uc3m.es/~miguel/simpletutorialssgp.php .",
    "there are two versions of the ssgp algorithm : ssgp ( fixed ) uses fixed spectral points while ssgp ( optimized ) optimizes the marginal likelihood with respect to the spectral points",
    ". we will only consider ssgp ( fixed ) .",
    "we observe some sensitivity in predictive performance to the basis functions and adopt the following strategy for better results : for each implementation of algorithm [ alg1 ] ( or [ alg2 ] ) , we randomly generate ten sets of spectral points from @xmath34 , perform 2 iterations of the algorithm , and select the set with the highest attained lower bound to continue to full convergence .",
    "a similar strategy was used by @xcite to initialize the ssgp algorithm . due to the zero mean assumption , we center all target vectors , @xmath142 by subtracting the mean @xmath202 from @xmath142 . in the examples",
    ", `` va '' refers to the variational approximation approach implemented via algorithm [ alg2 ] , `` global '' refers to using the entire training set for fitting while `` local '' refers to the adaptive neighbourhood approach described in section [ neigh ] .",
    "the pendulum data set ( available at http://www.tsc.uc3m.es/~miguel/simpletutorialssgp.php ) has @xmath203 covariates and contains 315 training points and 315 test points .",
    "the target variable is the change in angular velocity of a simulated mechanical pendulum over 50 ms and the covariates consist of different parameters of the system .",
    "@xcite used this example to show that ssgp ( optimized ) can sometimes fail due to overfitting .",
    "we rescale the input variables in the training set to lie in @xmath204 $ ] and consider the number of basis functions , @xmath205 .",
    "we compare the performance of algorithm [ alg2 ] with ssgp ( fixed ) using nmse and mnlp values averaged over ten repetitions .",
    "we set @xmath167 , @xmath206 for the half - cauchy priors , following @xcite and @xcite and @xmath207 , @xmath208 for the lengthscales in algorithm [ alg2 ] .    for this data",
    "set which is quite small , we note that the adaptive neighbourhood approach did not yield significant improvements as all inputs are relevant and there is no strong nonstationarity .",
    "hence we report only results for global fits , which are shown in figure [ pendulumnmsemnlp ] .",
    "the nmse and mnlp values produced by algorithm [ alg2 ] are comparable with that of ssgp ( fixed ) for small @xmath209 and are better for large @xmath209 . on the whole , algorithm [ alg2 ]",
    "produces reasonably good nmse performance and is less prone to overfitting than the ssgp algorithm .",
    "the ability of the variational approach to treat uncertainty in the covariance function hyperparameters reduces underestimation of predictive uncertainty , resulting in better mnlp performance .",
    "( global va ) and global ssgp ( fixed ) and averaged over ten repetitions plotted against number of basis functions @xmath201.,scaledwidth=49.5% ]     with @xmath167 while empty circles correspond to algorithm [ alg1].,scaledwidth=49.5% ]    next , we compare the performance of algorithm [ alg1 ] with algorithm [ alg2 ] both in terms of efficiency and the lower bound attained at convergence .",
    "we use algorithm [ alg1 ] to re - perform the runs for @xmath210 , using the same sets of spectral points that were used in algorithm 2 .",
    "these runs are indexed from 1 to 40 ( there are ten repetitions for each @xmath209 ) .",
    "figure [ pendulumlbcomparisons ] shows a plot of the lower bound attained at convergence on the left and a plot of the number of iterations required for convergence on the right for each of the 40 runs .",
    "figure [ pendulumlbcomparisons ] indicates that , except for runs 3 and 40 , the lower bound attained by algorithms [ alg1 ] and [ alg2 ] are almost indistinguishable",
    ". however , algorithm [ alg2 ] required a much smaller number of iterations to converge than algorithm [ alg1 ] .",
    "excluding runs 3 and 40 where the lower bound attained by algorithms [ alg1 ] and [ alg2 ] differs significantly , using algorithm [ alg2 ] instead of algorithm [ alg1 ] leads on average to a reduction of 49% in the number of iterations required for convergence .",
    "the highest reduction observed is 84% at run 9 . at run 3 ,",
    "algorithm [ alg2 ] was able to escape a local mode and attained a higher lower bound at convergence .",
    "however , at run 40 , it was caught in a local mode .",
    "we re - perform run 40 using @xmath168 and it turns out that algorithm [ alg2 ] was then able to attain the same lower bound as algorithm [ alg1 ] but in around half the number of iterations .",
    "with @xmath167 while dashed line corresponds to algorithm [ alg1 ] ) .",
    "right : plot of the adaptive step size @xmath211 used in algorithm [ alg2 ] against iteration number @xmath212.,scaledwidth=49.5% ]    the typical behaviour of algorithm [ alg2 ] is illustrated in figure [ pendulumrun33 ] .",
    "on the left is a plot of the lower bound against iteration number and on the right is a plot of the adaptive step size @xmath211 used in algorithm [ alg2 ] against iteration number @xmath212 for run 33 .",
    "the step size typically increases by a factor of 1.5 at each iteration but falls back to 1 when the lower bound fails to increase .",
    "the step size may also be reduced by factors of 1.5 due to the requirement that the covariance matrix be symmetric positive definite in step 2(b ) of algorithm [ alg2 ] .",
    "the reduction in the number of iterations that algorithm [ alg2 ] takes to converge as compared to algorithm [ alg1 ] is 74% for run 33 .      for the next four subsections , our discussion concerns performance of the adaptive neighbourhood approach .",
    "we compare the performance using two real datasets : the rainfall - runoff data set and the auto - mpg data set .",
    "we fit these data globally using ssgp ( fixed ) , va and mcmc , and compare results with the adaptive neighbourhood approach , implemented using both ssgp ( fixed ) and algorithm [ alg2 ] with factor @xmath167 . for the priors , we set @xmath206 , @xmath207 . for @xmath59 , we set @xmath213 for the rainfall - runoff data where a less smooth mean function is expected and @xmath214 for the auto - mpg data . the prior variance for the lengthscales can be chosen empirically by predictive performance on a test set or using prior knowledge .",
    "prior knowledge about the hyperparameters in the covariance function can be elicited by thinking about the prior degree of expected correlation of the mean function for covariates separated by lag one in each dimension when the covariates are standardized . for both the global ssgp ( fixed ) and va approach",
    ", we consider the number of basis functions @xmath215 .",
    "in addition , we generate ten artificial covariates on top of the existing covariates in both the rainfall - runoff and auto - mpg data set to test the capability of algorithm [ alg2 ] in automatic relevance determination .      in this example , we consider data from a deterministic rainfall - runoff model , which is a simplification of the australian water balance model ( awbm , * ? ? ?",
    "the awbm estimates catchment streamflow using time series of rainfall and evapotranspiration data and is widely used in australia for estimating catchment water yield or design flood estimation .",
    "the model has three parameters - the maximum storage capacity @xmath216 , the base flow index _ bfi _ and the baseflow recession factor @xmath13 .",
    "we have model simulations for around eleven years of average monthly potential evapotranspiration and daily rainfall data for the barrington river catchment , located in new south wales , australia .",
    "the model was run for 500 different values of the parameters @xmath217 generated using a maximin latin hypercube design .",
    "this data contains 500 data points for each of 3700 days , with a total of 1.85 million data points . for each day , the total rainfall is also recorded .",
    "a subset of this data has been studied in @xcite .",
    "[ htb ! ]",
    "even though the size of the data is large , the computational demands of the adaptive neighbourhood approach will depend mostly on the number of query points and the neighbourhood size .",
    "this makes our approach highly suitable for this data set .",
    "this is especially true since emulation of the model will be most interesting near values of peak rainfall input and generally for events of hydrological significance , where there might be a flood risk for example .",
    "so the proportion of interesting query points in this example is a small fraction of the total data set size and furthermore we expect the model output to vary rapidly in some parts of the parameter space but very little in other parts so the ability of the local method to smooth adaptively is very attractive for this problem",
    ". we will consider prediction for the two days with the highest rainfall inputs .",
    "we take awbm streamflow response as the target @xmath142 , and @xmath216 and @xmath13 as covariates , omitting _",
    "bfi_. a small amount of independent normal random noise with standard deviation 0.01 was added to @xmath142 to avoid degeneracies in regions of the space where the response tends to be identically zero .",
    "for each day , we randomly selected 100 data points as the test set and use the remaining 400 data points as the training set .",
    "these data are highly nonstationary with large flat regions , a few rapidly varying regions and the noise level changes a lot over the space .",
    "figure [ rainfall2covmnlpnmse ] shows the nmse and mnlp values averaged over ten repetitions for the rainfall - runoff data with peak rainfall . for global ssgp ( fixed )",
    ", we observe a slight improvement in nmse values as @xmath209 increases , while mnlp values remain largely constant at around 3.75 even for large @xmath209 . due to the nonstationary nature of this data",
    ", a global stationary fit does very poorly in mnlp .",
    "for the adaptive neighbourhood approach , we consider neighbourhoods of size @xmath218 , fixing the number of basis functions , @xmath219 . for the local methods ,",
    "the dotted lines correspond to results from the initial fitting where the @xmath16 nearest neighbours are determined based on euclidean distance .",
    "the solid lines correspond to results from the final fit where the @xmath16 nearest neighbours are determined using the new distance measure with dimensions weighted according to the lengthscales .",
    "the improvement brought about by adapting the neighbourhood is more apparent in va than in ssgp ( fixed ) .",
    "! ]     figure [ rainfall2covmnlpnmse_2 ] shows the nmse and mnlp values averaged over ten repetitions for the rainfall - runoff data with the second highest rainfall . in this example",
    "we also observe that a global stationary fit does very poorly in mnlp , again due to the nonstationary nature of the data .",
    "similarly , when adapting the neighbourhood approach , there are greater improvements in va than in ssgp ( fixed ) .",
    "it is clear that the adaptive neighbourhood approach is critical for this data set where the mean function varies rapidly over some parts of the space but very little over other parts .",
    "the variational approach performs very well when using just a small neighbourhood about each test point both in terms of nmse and mnlp .",
    "figure [ s=68 ] illustrates how the neighbourhood of a test point changes from the initial to the final fit for the case @xmath220 , when algorithm [ alg2 ] was being used .",
    "the plot on the left shows the neighbours ( denoted by circles ) of a test point ( denoted by solid circle ) determined using euclidean distance .",
    "the plot on the right shows the neighbours of the same test point determined using the new distance measure . in this case , the component of @xmath121 corresponding to the covariate @xmath216 is much larger than that corresponding to the covariate @xmath13 , resulting in the neighbourhood being shrunk along the @xmath216 axis .",
    "the adapted neighbourhood leads to an improvement in the estimation of the predictive mean and especially the predictive variance of the test point .",
    "+ m & va & ssgp & mcmc ( 2000 iterations ) +   + 20 & 2.626 & 0.177 & + 40 & 8.315 & 0.268 & + 60 & 17.545 & 0.491 & 2068.596 + 80 & 32.920 & 0.638 & + 100 & 55.223 & 0.836 & +   +   + k & va & ssgp & mcmc ( 2000 iterations ) +   + 20 & 257.773 & 16.361 & 653.690 + 40 & 247.744 & 17.964 & 2253.453 + 60 & 265.194 & 22.695 & 5182.298 + 80 & 320.416 & 23.091 & 9229.747 + 100 & 279.356 & 24.774 & 14441.828 +    table [ comtimes ] shows the computation times of the va , ssgp ( fixed ) and mcmc algorithms on the rainfall - runoff data with peak rainfall input .",
    "we ran the mcmc using rstan @xcite on a dual processor windows pc 3.30 ghz workstation and both ssgp ( fixed ) and va in matlab using a 3.2 ghz intel core i5 quad core imac .",
    "for the global approach , computation times for ssgp ( fixed ) and va are averaged over 10 repetitions , while mcmc is based on a single run with 2000 iterations . for the adaptive neighbourhood approach , table [ comtimes ]",
    "shows the total time it takes to run all 100 test points .",
    "note that the timing for the local approaches can be significantly reduced by parallelizing . in terms of computation speed ,",
    "ssgp ( fixed ) is the fastest followed by va .",
    "we observe that mcmc is substantially slower than the other methods and the computation time increases significantly when the size of neighbourhood increases .",
    "we do not observe such significant increase in computation times for va and ssgp ( fixed ) .",
    "we consider rainfall - runoff data on the day with peak rainfall and generate ten additional covariates artificially . as both covariates",
    "@xmath216 and @xmath13 lie in the interval @xmath221 $ ] , we simulate each of the ten additional covariates randomly from the uniform distribution on the interval @xmath221 $ ] .",
    "we compare the performance of ssgp ( fixed ) and algorithm [ alg2 ] using a global fit with the adaptive neighbourhood approach .",
    "we set @xmath167 in algorithm [ alg2 ] and use the same priors as in section [ rainfall ] .",
    "for the global approach , we consider the number of basis functions , @xmath215 while for the adaptive local neighbourhood approach , we consider neighbourhoods of size @xmath218 , fixing the number of basis functions , @xmath219 .",
    "the results are shown in figure [ rainfall12covmnlpnmse ] .",
    "for the global approach , the results of ssgp ( fixed ) are quite similar to those in the 2 covariates case . for the local approach , a small neighbourhood with @xmath222 does not work well for both ssgp ( fixed ) and",
    "va , indicating that a larger neighbourhood is likely required for high dimensional problems .",
    "there is a clear improvement in the mnlp values from adapting the neighbourhood according to the lengthscales and algorithm 2 achieved the lowest mnlp values among the methods that were studied , using a smaller neighbourhood .",
    "the mnlp values achieved by algorithm [ alg2 ] are close to those attained in section [ rainfall ] , indicating that the adaptive neighbourhood approach is effective in eliminating covariates of little relevance .",
    "the variational approach is able to provide significant improvement in this aspect and is much more robust to overfitting for small neighbourhoods .",
    "however , the nmse values obtained in the adaptive neighbourhood approach are higher than those obtained in the global approach .",
    "finally , we note that a good neighbourhood size is dependent on the number of covariance function parameters to be estimated and on the degree of nonstationarity , which is very much problem specific .",
    "some experimentation with different neighbourhood sizes is probably necessary .          in this example",
    ", we consider the automobile city - cycle fuel consumption in miles per gallon ( auto - mpg ) data taken from the cmu statistics library .",
    "this dataset was used in the 1983 american statistical association exposition and is available at http://archive.ics.uci.edu/ml/datasets.html .",
    "the dataset contains 398 instances and nine attributes .",
    "@xcite used this data to predict the attribute  mpg \" , which is the city - cycle fuel consumption in miles per gallon .",
    "the other eight attributes include two multi - valued discrete , four continuous attributes and two categorical variables .",
    "we drop the two categorical variables , car name and origin , and keep the four continuous attributes and two multi - valued discrete variables .",
    "six of the data points are removed as they have missing entries in some of the input variables .",
    "we randomly select 80 data points as the test set , and use the remaining 312 data points as the training set .",
    "figure [ auto ] shows the nmse and mnlp values averaged over ten repetitions . for the global ssgp ( fixed ) and",
    "va methods , we observe slight improvements in both the nmse and mnlp values as @xmath209 increases .",
    "the mnlp and nmse values for mcmc and global va are also better than for global ssgp ( fixed ) . for the adaptive neighbourhood approach ,",
    "we consider neighbourhoods of size @xmath223 , while fixing the number of basis function @xmath219 . for local va ,",
    "the final fit is slightly better than the initial fit .",
    "there is an improvement brought about by adapting the neighbourhood as figure [ auto ] shows that the mnlp values of both the initial and final fits are lower than the mcmc method for neighbourhood size of 40 , 60 , 80 and 100 .    for local ssgp ( fixed )",
    ", we observe that their performance is worse than mcmc .",
    "moreover , it seems that the initial fit is better than the final fit , for neighbourhood size of 80 and 100 .",
    "this may be because the lengthscales are not accurate enough to be used for the final fit .",
    "we also examined the local ssgp approach with larger neighbourhood sizes of 150 , 200 and 250 .",
    "we found that , at a neighbourhood size of 150 , the performance of the final fit of local ssgp ( fixed ) ( mnlp and nmse of 2.38 and 0.129 respectively ) is slightly better than mcmc .",
    "adapting the neighbourhood approach is still more apparent in the variational approach as it is able to achieve mnlp and nmse value of 2.26 and 0.117 respectively at neighbourhood size of 60 .",
    "we now consider the auto - mpg data and look at the influence of irrelevant covariates on the model .",
    "this is again done by generating ten additional covariates artificially and randomly from the uniform distribution on the interval [ 0,1 ] . once again , like the rainfall - runoff data , it seems that a larger neighbourhood is required to attain the best performance for the variational approach when irrelevant covariates are added . in this example , for the variational approach , we found that neighbourhood size of 100 produces the best peformance with mnlp and nmse values of 2.30 and 0.127 respectively .",
    "again , after examining the local ssgp ( fixed ) approach with larger neighbourhood sizes , we found that it attains the best performance ( mnlp and nmse of 2.43 and 0.141 respectively ) at a neighbourhood size of 150 .            in order to explain why there is a difference in the stability of the adaptive neighbourhood approach between va and ssgp ( fixed ) , we examined the estimated predictive mean and variance for one test point from the auto - mpg test set with 10 simulated irrelevant covariates .",
    "we implement the adaptive neighbourhoods approach based on just the initial fitting , which uses the shortest euclidean distance .",
    "figure [ posterior_mean ] shows 100 posterior predictive means and variances from ssgp ( fixed ) and va with the adaptive neighbourhood approach . in the 100 replications , only the spectral points change . since va accounts for hyperparameter uncertainty , it is more robust towards the choice of spectral points .",
    "we observe that the posterior predictive means and variances are concentrated around a smaller range of values even when the size of neighbourhood is small . on the other hand , for local ssgp ( fixed ) , the posterior predictive means vary more for different choices of the spectral points with the values ranging from 0 to 20 and with many of the posterior predictive variances small when the size of the neighbourhood is small .",
    "in this paper , we have presented a nonconjugate variational message passing algorithm for fitting sparse spectrum gp regression models where closed form updates are possible for all variational parameters , except for the evaluation of @xmath116 .",
    "we note that @xmath116 can be evaluated very efficiently using quadrature and there is almost no computational overhead when compared to updates based on conditionally conjugate inverse - gamma priors for the variance parameters .",
    "however , half - cauchy priors lead to much better predictive inference especially in the adaptive neighbourhood approach where the amount of training data is small .",
    "a bayesian approach has been adopted for parameter estimation which allows covariance function hyperparameter uncertainty to be treated and empirical results suggest that this improves prediction ( especially in the mnlp values ) and prevents overfitting .",
    "we also propose a novel adaptive neighbourhood technique for obtaining predictive inference which is adept at handling data with nonstationarities and this approach can be extended to large data sets as well .",
    "the simulated data sets showed that weighting the dimensions according to the lengthscales estimated from an initial fit is very effective at downweighting variables of little relevance , leading to automatic variable selection and improved prediction .",
    "in addition , we introduce a technique for accelerating convergence in nonconjugate variational message passing by taking step sizes larger than one in the direction of the natural gradient of the lower bound .",
    "we do not attempt to search for the optimal step size but adopt an adaptive strategy that can be easily implemented , and empirical results indicate significant speed - ups . algorithm [ alg2 ] is thus an attractive alternative for fitting sparse spectrum gp regression models , which is stable , robust to overfitting for small data sets and capable of dealing with highly nonstationary data as well when used in combination with the adaptive neighbourhood approach .",
    "we thank lucy marshall for supplying the rainfall - runoff data set .",
    "linda tan was partially supported as part of the singapore delft water alliance s tropical reservoir research programme .",
    "david nott , ajay jasra and victor ong s research was supported by a singapore ministry of education academic research fund tier 2 grant ( r-155 - 000 - 143 - 112 ) .",
    "we also thank the referees and associate editor for their comments which have helped improved the manuscript .",
    "references    amari , s. : natural gradient works efficiently in learning .",
    "neural computation 10 , 251276 ( 1998 )    attias , h. : inferring parameters and structure of latent variable models by variational bayes . in laskey ,",
    "k. and prade , h. ( eds . ) proceedings of the 15th conference on uncertainty in artificial intelligence , pp .",
    "morgan kaufmann , san francisco , ca ( 1999 )    attias , h. : a variational bayesian framework for graphical models . in solla , s. a. , leen , t. k. and mller , k .-",
    "advances in neural information processing systems 12 , pp .",
    "mit press , cambridge , ma ( 2000 )    blei , d. m. and jordan , m. i. : variational inference for dirichlet process mixtures .",
    "bayesian analysis 1 , 121144 ( 2006 )    boughton , w. : the australian water balance model .",
    "environmental modelling and software 19 , 943956 ( 2004 )    braun , m. and mcauliffe , j. : variational inference for large - scale models of discrete choice .",
    "journal of the american statistical association 105 , 324335 ( 2010 )    gelman , a. : prior distributions for variance parameters in hierarchical models .",
    "bayesian analysis 1 , 515533 ( 2006 )    gramacy , r. b. and apley , d. w. ( 2014 ) .",
    "local gaussian process approximation for large computer experiments .",
    "journal of computational and graphical statistics . to appear .",
    "haas , t. c. : local prediction of a spatio - temporal process with an application to wet sulfate deposition .",
    "journal of the american statistical association 90 , 11891199 ( 1995 )    hastie , t. and tibshirani , r. : discriminant adaptive nearest neighbor classification .",
    "ieee transactions on pattern analysis and machine intelligence 18 , 607616 ( 1996 )    hoffman , m. d. , blei , d. m. , wang , c. and paisley , j. : stochastic variational inference .",
    "journal of machine learning research 14 , 13031347 ( 2013 )    honkela , a. , valpola , h. and karhunen , j. : accelerating cyclic update algorithms for parameter estimation by pattern searches .",
    "neural processing letters 17 , 191203 ( 2003 )    huang , h. , yang , b. and hsu , c. ( 2005 ) .",
    "triple jump acceleration for the em algorithm . in han , j. , wah , b. w. , raghavan , v. , wu , x. and rastogi , r. ( eds . ) proceedings of the 5th ieee international conference on data mining , pp .",
    "ieee computer society , washington , dc , usa .",
    ", mallicka , b. k. and holmesa , c. c. : analyzing nonstationary spatial data using piecewise gaussian processes . journal of the american statistical association 100 , 653668 ( 2005 )    knowles , d. a. , minka , t. p. : non - conjugate variational message passing for multinomial and binary regression . in shawe - taylor ,",
    "j. , zemel , r. s. , bartlett , p. , pereira , f. and weinberger , k. q. ( eds . ) advances in neural information processing systems 24 , pp .",
    "red hook , ny : curran associates , inc .",
    "( 2011 )    lzaro - gredilla , m. , quionero - candela , j. , rasmussen , c. e. and figueiras - vidal , a. r. : sparse spectrum gaussian process regression .",
    "journal of machine learning research 11 , 18651881 ( 2010 )    lzaro - gredilla , m. and titsias , m. k. : variational heteroscedastic gaussian process regression . in getoor ,",
    "l. and scheffer , t. ( eds . ) proceedings of the 28th international conference on machine learning , pp .",
    "omnipress , madison , mi , usa ( 2011 )    lindgren , f. , rue , h. and lindstrm , j. : an explicit link between gaussian fields and gaussian markov random fields : the stochastic partial differential equation approach .",
    "journal of the royal statistical society : series b 73 , 423498 ( 2011 )    magnus , j. r. and neudecker , h. : matrix differential calculus with applications in statistics and econometrics .",
    "wiley , chichester , uk ( 1988 )    nguyen - tuong , d. , seeger , m. and peters , j. : model learning with local gaussian process regression .",
    "advanced robotics 23 , 20152034 ( 2009 )    nott , d. j. , tan , s. l. , villani , m. and kohn , r. : regression density estimation with variational methods and stochastic approximation .",
    "journal of computational and graphical statistics 21 , 797820 ( 2012 )    ormerod , j. t. and wand , m. p. : explaining variational approximations .",
    "the american statistician 64 , 140153 ( 2010 )    park , s. and choi , s. : hierarchical gaussian process regression . in sugiyama , m. and yang , q. ( eds . ) proceedings of 2nd asian conference on machine learning , pp . 95110 ( 2010 )    qi , y. and jaakkola , t. s. : parameter expanded variational bayesian methods . in schlkopf , b. , platt ,",
    "j. and hofmann , t. ( eds . ) advances in neural information processing systems 19 , pp .",
    "10971104 . mit press , cambridge ( 2006 )    quinlan , r. : combining instance - based and model - based learning . in proceedings on the tenth international conference of machine learning , 236 - 243 .",
    "university of massachusetts , amherst .",
    "morgan kaufmann ( 1993 )    quionero - candela , t. and rasmussen , c. e. : a unifying view of sparse approximate gaussian process regression .",
    "journal of machine learning research 6 , 19391959 ( 2005 )    rasmussen , c. e. and williams , c. k. i. : gaussian processes for machine learning . mit press , cambridge , ma ( 2006 )    ren , q. , banerjee , s. , finley , a. o. and hodges , j. s. : variational bayesian methods for spatial data analysis .",
    "computational statistics and data analysis 55 , 31973217 ( 2011 )    salakhutdinov , r. and roweis , s. : adaptive overrelaxed bound optimization methods . in fawcett ,",
    "t. and mishra , n. ( eds . ) proceedings of the 20th international conference on machine learning , pp",
    ". 664671 .",
    "aaai press , menlo park , california ( 2003 )    snelson , e. and ghahramani , z. : sparse gaussian processes using pseudo - inputs . in weiss ,",
    "y. , schlkopf , b. and platt , j. ( eds . ) advances in neural information processing systems 18 , pp . 12571264 .",
    "mit press , cambridge , ma .",
    "( 2006 )    snelson , e. and ghahramani , z. : local and global sparse gaussian process approximations . in meila , m. and shen , x. ( eds ) jmlr workshop and conference proceedings volume 2 : aistats 2007 , pp . 524531 ( 2007 )",
    "stan development team : rstan : the r interface to stan , version 2.5.0 .",
    "http://mc-stan.org/rstan.html ( 2014 )    stein , m. l. , chi , z. and welty , l. j. : approximating likelihoods for large spatial data sets .",
    "journal of the royal statistical society : series b 66 , 275296 ( 2004 )    tan , l. s. l. and nott , d. j. : variational inference for generalized linear mixed models using partially non - centered parametrizations . statistical science 28 , 168188 ( 2013    tan , l. s. l. and nott , d. j. : a stochastic variational framework for fitting and diagnosing generalized linear mixed models .",
    "bayesian analysis ( 2014 ) .",
    "doi : 10.1214/14-ba885    titsias , m. k. : .",
    "variational learning of inducing variables in sparse gaussian processes . in van dyk , d. and welling , m. ( eds . ) proceedings of the 12th international conference on artificial intelligence and statistics , pp .",
    "567574 ( 2009 )    urtasun , r. and darrell , t. : sparse probabilistic regression for activity - independent human pose inference . in ieee conference on computer vision and pattern recognition 2008 , pp .",
    "18 ( 2008 )    vecchia , a. v. : estimation and model identication for continuous spatial processes .",
    "journal of the royal statistical society : series b 50 , 297312 ( 1988 )    walder , c. , kim , k. i. and schlkopf , b. : sparse multiscale gaussian process regression . in mccallum ,",
    "a. and roweis , s. ( eds . ) proceedings of the 25th international conference on machine learning pp .",
    "acm press , new york ( 2008 )    wand , m. p. , ormerod , j. t. , padoan , s. a. and frhrwirth , r. : mean field variational bayes for elaborate distributions .",
    "bayesian analysis 6 , 847900 ( 2011 )    wand , m. p. : fully simplified multivariate normal updates in non - conjugate variational message passing .",
    "journal of machine learning research 15 , 13511369 ( 2014 )    wang , b. and titterington , d. m. : inadequacy of interval estimates corresponding to variational bayesian approximations . in cowell ,",
    "r. g. and ghahramani , z. ( eds . ) , _ proceedings of the 10th international workshop on artificial intelligence and statistics _ , pp .",
    "373380 . society for artificial intelligence and statistics ( 2005 )    wang , b. and titterington , d. m. : convergence properties of a general algorithm for calculating variational bayesian estimates for a normal mixture model .",
    "bayesian analysis 3 , 625650 ( 2006 )    winn , j. and bishop , c.m . :",
    "variational message passing .",
    "journal of machine learning research 6 , 661694 ( 2005 )",
    "suppose @xmath224 and @xmath225 , @xmath226 are fixed vectors the same length as @xmath40 .",
    "let @xmath227 and @xmath228 , then @xmath229   \\end{gathered}\\ ] ] @xmath230    \\end{gathered}\\ ] ] @xmath231\\end{gathered}\\ ] ] by setting @xmath232 in the first and third expressions , we get @xmath233    @xmath234=\\exp\\{i \\mu^t ( t_1-t_2)- \\tfrac{1}{2}(t_1-t_2)^t \\sigma ( t_1-t_2)\\}$ ] implies @xmath235 & = e\\{\\cos(t_1^t\\lambda)\\cos(t_2^t\\lambda ) \\\\ & \\quad + \\sin(t_1^t\\lambda)\\sin(t_2^t\\lambda)\\ }   \\\\ & = \\exp\\{-\\tfrac{1}{2}(t_1-t_2)^t \\sigma ( t_1-t_2)\\ }   \\\\ & \\quad \\cdot \\cos\\{\\mu^t ( t_1-t_2)\\ } \\end{aligned}\\ ] ] and @xmath236 & = e\\{\\sin(t_1^t\\lambda)\\cos(t_2^t\\lambda ) \\\\ & \\quad -\\cos(t_1^t\\lambda)\\sin(t_2^t\\lambda)\\ }   \\\\ & = \\exp\\{-\\tfrac{1}{2}(t_1-t_2)^t \\sigma ( t_1-t_2)\\ }   \\\\ & \\quad \\cdot \\sin\\{\\mu^t ( t_1-t_2)\\}. \\end{aligned}\\ ] ] replacing @xmath226 by @xmath237 , we get @xmath238 & = e\\{\\cos(t_1^t\\lambda)\\cos(t_2^t\\lambda ) \\\\ & \\quad -\\sin(t_1^t\\lambda)\\sin(t_2^t\\lambda)\\ }   \\\\ & = \\exp\\{-\\tfrac{1}{2}(t_1+t_2)^t \\sigma ( t_1+t_2)\\ } \\\\ & \\quad   \\cdot \\cos\\{\\mu^t ( t_1+t_2)\\ } \\end{aligned}\\ ] ] and @xmath239 & = e\\{\\sin(t_1^t\\lambda)\\cos(t_2^t\\lambda ) \\\\ & \\quad + \\cos(t_1^t\\lambda)\\sin(t_2^t\\lambda)\\ } \\\\ & = \\exp\\{-\\tfrac{1}{2}(t_1+t_2)^t \\sigma ( t_1+t_2)\\ }   \\\\ & \\quad \\cdot   \\sin\\{\\mu^t ( t_1+t_2)\\}. \\end{aligned}\\ ] ] ( [ e1])+([e3 ] ) gives the first equation of the lemma , ( [ e1])-([e3 ] ) gives the second and ( [ e2])+([e4 ] ) gives the third .",
    "@xmath240    using lemma 1 , we have @xmath241^t,\\ ] ] where @xmath242 \\end{aligned}\\ ] ] and @xmath133 for @xmath135 , @xmath136 .",
    "we also have @xmath243 where @xmath244 $ ] , where @xmath245 , @xmath246 , @xmath247 are all @xmath248 matrices and @xmath249 @xmath250 , @xmath251 for @xmath136 , @xmath137 .",
    "from ( [ lb ] ) , the lower bound is given by @xmath252 where @xmath253 the terms in the lower bound can be evaluated as follows : @xmath254 \\\\ \\cdot { \\mathcal{h}(n , c_\\gamma^q , a_\\gamma^2)}/{\\mathcal{h}(n-2,c_\\gamma^q , a_\\gamma^2)}\\end{gathered}\\ ] ] @xmath255 @xmath256 @xmath257 @xmath258 @xmath259 @xmath260 @xmath261 @xmath262 putting these terms together and making use of the updates in steps 5 and 6 of algorithm [ alg1 ] gives the lower bound in .",
    "it can be shown ( see * ? ? ? * ; * ? ? ?",
    "* ) that the natural parameter of @xmath120 is @xmath263 where @xmath264 is a unique @xmath265 matrix that transforms @xmath266 into @xmath101 for any @xmath267 symmetric square matrix @xmath102 , that is , @xmath268 . we use @xmath266 to denote the @xmath269 vector obtained from @xmath101 by eliminating all supradiagonal elements of @xmath102 .",
    "@xcite is a good reference for the matrix differential calculus involved in the derivation below . from and",
    "7 ) , we have @xmath270 where @xmath271 and @xmath272 are evaluated at @xmath273 let @xmath274 the first line of simplifies to @xmath275 the second line of gives @xmath276"
  ],
  "abstract_text": [
    "<S> we develop a fast variational approximation scheme for gaussian process ( gp ) regression , where the spectrum of the covariance function is subjected to a sparse approximation . </S>",
    "<S> our approach enables uncertainty in covariance function hyperparameters to be treated without using monte carlo methods and is robust to overfitting . </S>",
    "<S> our article makes three contributions . </S>",
    "<S> first , we present a variational bayes algorithm for fitting sparse spectrum gp regression models that uses nonconjugate variational message passing to derive fast and efficient updates . </S>",
    "<S> second , we propose a novel adaptive neighbourhood technique for obtaining predictive inference that is effective in dealing with nonstationarity . </S>",
    "<S> regression is performed locally at each point to be predicted and the neighbourhood is determined using a measure defined based on lengthscales estimated from an initial fit . </S>",
    "<S> weighting dimensions according to lengthscales , this downweights variables of little relevance , leading to automatic variable selection and improved prediction . </S>",
    "<S> third , we introduce a technique for accelerating convergence in nonconjugate variational message passing by adapting step sizes in the direction of the natural gradient of the lower bound . </S>",
    "<S> our adaptive strategy can be easily implemented and empirical results indicate significant speedups .    </S>",
    "<S> example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore </S>"
  ]
}