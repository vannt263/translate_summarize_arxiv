{
  "article_text": [
    "the permanent of an @xmath0 matrix @xmath1 $ ] is defined as @xmath2 where @xmath3 denotes the set of all possible permutations of @xmath4 .",
    "the permanent attracts attentions from mathematics , computer science , statistical physics and chemical graph theory @xcite .",
    "however , computing the permanent of a matrix is proved to be a @xmath5-complete problem in counting @xcite , which is no easier than an @xmath6-complete problem in combinatorial optimization . even for 3-regular matrices , which are with 3 nonzero entries in each row and column , evaluating their permanents",
    "is still a @xmath5-complete problem @xcite .",
    "the best - known algorithm for precise evaluation of the permanent of general matrix is due to ryser @xcite , and later improved by nijenhuis and wilf @xcite .",
    "it is @xmath7 in time complexity .",
    "we call the method r - nw algorithm .",
    "the r - nw only works for small matrices .",
    "it is only possible to make the precise calculation faster , if the special structure properties of matrices can be used intensively .",
    "several efficient precise algorithms have been proposed by exploring the structure properties of sparse matrices , such as kallman s method @xcite , hybrid algorithm @xcite . among them , the hybrid algorithm is the best one for very sparse matrix .",
    "the hybrid algorithm is parallel in nature . a parallelized version of the algorithm is developed for the permanent computation problem arising from molecular chemistry @xcite .",
    "the basic idea of the parallelized hybrid algorithm is divide and conquer .",
    "an @xmath8 matrix @xmath9 is divided into a series smaller sub - matrices by using the hybrid algorithm .",
    "when the computational times of the permanents of sub - matrices are known or estimated appropriately , the load balancing strategies for the permanent computation could be further improved with the help of the theory of parallel machine scheduling in combinatorial optimization . in this paper",
    ", we use the statistical methods to explore the factors which are related to the computational time of permanent with the hybrid algorithm .",
    "an efficient estimation for computational time of permanent is obtained .",
    "hence the improved parallel strategy for permanent of sparse graph is proposed .    in the next section , a brief introduction to a hybrid algorithm and its parallelized version for permanent , which are the best methoths for very sparse matrix as far as we know ,",
    "are presented .",
    "the load balancing strategy of parallel algorithm is discussed . in section 3 , the statistical analysis of computational time of permanent is given .",
    "it is shown that the permanent value has strong correlation to its computational time with the hybrid algorithm .",
    "then an improved loading balance strategy based on approximate permanent algorithm is proposed . in section 4",
    ", the numerical results are given .",
    "some discussions are made in section 5 .",
    "taking the advantage of the sparse structure extensively , a hybrid method is proposed @xcite .",
    "consider an expansion @xmath10 where @xmath11 , @xmath12 , @xmath13 and @xmath14 are scalars , @xmath15 is an @xmath16-dimensional row vector , @xmath17 , @xmath18 @xmath19 and @xmath20 are both @xmath21-dimensional column vectors , and @xmath22 is an @xmath23 matrix .",
    "this expansion appears in @xcite , and is used to establish an approximate algorithm for permanent .",
    "@xmath24 \" , where @xmath25 is the minimal number of nonzero entries in one row or column of matrix , one @xmath0 matrix can be divided into no more than two @xmath26 matrices , that is , @xmath27 in the expansion ( [ e2 ] ) . combining the expansion ( [ e2 ] ) with r - nw algorithm",
    ", a hybrid algorithm is constructed@xcite .",
    "* algorithm h@xmath28per(hybrid ) *    input : : :    @xmath9an @xmath8 0 - 1 valued matrix .",
    "output : : :    @xmath29 .",
    "step 1 : : :    find the minimal number of nonzero entries @xmath25 in one row    or column of @xmath9 .",
    "step 2 : : :    if ; ;      @xmath30 and @xmath31 , * then * divide      @xmath9 into @xmath32 as ( [ e2 ] ) , and      @xmath33    else ; ;      return by @xmath34-@xmath35 .",
    "it is an efficient algorithm for very sparse matrix , especially for fullerene - like matrices@xcite .",
    "the parallelization of the algorithm is essential for computing large scale problems .",
    "the hybrid algorithm h_per for permanent is parallel in nature .",
    "first , the @xmath8 matrix @xmath9 is divided into a series of @xmath36 matrices by using the formula ( [ e2 ] ) repeatedly .",
    "then the @xmath36 matrices are computed in parallel .",
    "the @xmath14 is called the depth of pre - expansion .",
    "let @xmath37 denote the @xmath38-th @xmath39 matrix .",
    "based on the algorithm h_per , the following parallel method ph is constructed @xcite .",
    "* algorithm ph ( parallel h_per ) *    step 1 : : :    let @xmath40 be the order of matrix @xmath9 , num be the    number of cpu s used , @xmath41 , @xmath42 ,    set @xmath14 be the depth of pre - expansion .",
    "step 2 : : :    for ; ;      k=1:d      +      : :        t=0 ;      : :        for w=1:s        +        ; ;          divide @xmath43 into @xmath44          and @xmath45 as ( [ e2 ] ) ,          @xmath46 ,          @xmath47 , @xmath48 ;      : :        end      : :        s = t ;    end ; ; step 3 : : :    assign @xmath49 in the natural order to the the    currently least load processor and compute the permanent of    @xmath49 by algorithm h_per , until all    @xmath49 s @xmath50 has been    computed .",
    "step 4 : : :    @xmath51 .",
    "methods for approximating permanents of 0 - 1 matrices attract a great deal of studies in the last decade .",
    "markov chain monte carlo methods @xcite absorb great efforts from computer scientists .",
    "the theoretical analysis for those methods are relatively abundant and a fully - polynomial randomized approximation scheme for the permanent of arbitrary matrix with non - negative entries has been reported @xcite . but the method is unlikely to be practical in computations @xcite .",
    "a kind of practical approximate methods for permanents is monte carlo method , which reduce permanents to determinants by randomizing the elements of matrices @xcite .",
    "the idea is first introduced by godsil and gutman @xcite .",
    "it is improved by karmarkar et .",
    "@xcite , which is one of the most popular practical approximate algorithms for matrix permanent .",
    "assume @xmath52 be the three cube roots of unity .",
    "let @xmath53 be a complex number , and @xmath54 denote the complex conjugate of @xmath53 .",
    "the kklll method is outlined as follows .    * algorithm kklll ( karmarkar / karp / lipton / lovasz / luby ) *    input : : :    @xmath9an @xmath8 0 - 1 valued matrix .",
    "output : : :    @xmath55the estimate for @xmath56 .",
    "step 1 : : :    for all @xmath57 ,    +    if ; ;      @xmath58 then @xmath59 ;    elseif ; ;      @xmath60 then randomly and independently choose      @xmath61 with probability      @xmath62 .",
    "step 2 : : :    @xmath63 .",
    "[ @xcite ] the kklll estimator @xmath55 is unbiased with @xmath64=per(a)$ ] .",
    "an @xmath65-approximation algorithm for @xmath66 is a monte - carlo algorithm that accepts as input @xmath9 and two positive parameters @xmath67 and @xmath68 .",
    "the output of the algorithm is an estimate @xmath69 of @xmath66 , which satisfies @xmath70\\geq 1-\\delta.\\ ] ]    the kklll estimator is unbiased and yields an @xmath65-approximation algorithm for estimating @xmath66 in time @xmath71 @xcite .",
    "however , for the random 0,1-matrix , frieze and jerrum proved the following result .",
    "[ @xcite ] let @xmath72 is any function tending to infinity as @xmath73 . then",
    "only @xmath74 trials using the kklll estimator suffice to obtain a reliable approximation to the permanent of the random 0,1-matrix within a factor @xmath75 of the correct value .",
    "the load balancing strategies for the permanent computation can be further improved with the help of the models of parallel machine scheduling in combinatorial optimization .",
    "consider the following machine scheduling model first .",
    "assume that one has a set of @xmath40 jobs @xmath76 , and @xmath77 identical machines @xmath78 .",
    "each job @xmath79 must be processed without interruption for a time @xmath80 on one of the machines .",
    "each machine can process at most one job at a time .",
    "if all jobs are ready for processing in the very beginning , it is called * offline * machine scheduling ; otherwise if jobs can only be ready for processing one by one , it is called * online*.    an algorithm called ls is designed for the online parallel machine scheduling problems , where jobs are processed in its natural order of coming .",
    "graham @xcite gives the worst - case analysis of the scheduling heuristics and shows that algorithm ls has a worst - case ratio of @xmath81 , where @xmath77 is the number of machines available .",
    "if the jobs are sorted in the non - increasing order of processing times for offline problems , then there is an algorithm known as lpt .",
    "it is proved by graham that algorithm lpt has an improved worst - case ratio of @xmath82 @xcite .",
    "the scheduling problem in algorithm ph is essentially online in which the sub - matrices are sent to the different processors in their natural order of expansion . in this paper",
    ", we will give a approximate order for the computational times of the sub - matrices .",
    "it is observed and checked that there is a strong correlation between the permanent value and its computational time .",
    "hence we use the approximate value of permanent to determine the order of jobs which are sent to processors so as to improve the parallel efficiency than the case with natural order .",
    "we find that the time of computing the permanent of a sparse matrix by hybrid algorithm is strongly correlated to its permanent value .",
    "we also note that the computational time of hybrid algorithm is dependent on the locations of the nonzero elements of matrix .",
    "therefore , for any matrix @xmath1_{n\\times n}$ ] construct a 0 - 1 matrix @xmath83_{n\\times n}$ ] in such a way that @xmath84 if @xmath85 and @xmath86 if @xmath87 for any @xmath88 .",
    "the computational times of @xmath66 and @xmath89 with algorithm h_per are almost equal . for any matrix @xmath9 ,",
    "what we discuss is the relationship between the @xmath89 and the computational time of @xmath66 .",
    "the following two subsections will present the statistical analysis for the correlation . then an improved load balancing strategy is proposed .",
    "the linear regression model is used to investigate which factors are sensitively response to the computational time of matrix permanent with algorithm h_per .",
    "take the computational time @xmath90 with algorithm h_per as the dependent variable .",
    "the following five matrix invariants are considered , which are chosen empirically and may be related to the computational time : the permanent value of the matrix which is denoted as @xmath91 ; the absolute value of the determinant of the matrix which is denoted as @xmath92 ; the number of nonzero elements of the matrix which is denoted as @xmath93 ; the variance of sum of nonzero elements in each row which is denoted as @xmath94 ; the variance of sum of nonzero elements in each column which is denoted as @xmath95 .",
    "we generate 0 - 1 matrices randomly with various size and sparsity .",
    "each group contains 100 matrices . for each 0 - 1 matrix group , consider the multiple regression model as follows .",
    "@xmath96    the coefficient of determination , often referred to @xmath97 , is a frequently used measure of the fit of the regression line .",
    "the definition is , simply , @xmath98 corresponding to linear model @xmath99 , where @xmath100 and @xmath101 , @xmath102 and @xmath103 being the estimates of @xmath104 and @xmath105@xcite .",
    "the coefficient of determination @xmath97 is no less than 0.8 for every data group , which is shown in table  [ t1 ] . in order to find out which factors significantly correlate to the dependent variable we apply stepwise regression method to the linear regression models .",
    "using the stepwise regression , only the permanent value @xmath91 is significant to the computational time @xmath90 .",
    "take the case of n=40 and s=5n as an example .",
    "the regression equation with all five factors is @xmath106 with @xmath107 ;    the result of stepwise regression is @xmath108 with @xmath109 .",
    "@xmath92 , @xmath93 , @xmath94 and @xmath95 are very easily computed .",
    "however , the computational time of the permanent can not be predicted by these factors . only permanent value itself",
    "is strongly correlated to computational time .",
    "the result is reasonable that @xmath66 is just the number of all the nonzeros expansion terms in ( [ e1 ] ) , which determines the complexity of the problem to some extend in nature .",
    "though evaluating @xmath66 is hard , it is fortunate that there are many good practical approximate algorithms developed for permanent .      for improving the load balancing of the algorithm ph , what is essentially needed to know is the non - increasing order of the computational times of all the sub - matrices produced by algorithm ph .",
    "the kendall rank correlation , also referred to kendall @xmath110 coefficient , is a common rank correlation method in the theory of statistical relationship .",
    "this coefficient provides a kind of average measure of the agreement between two measured quantities .",
    "suppose we have a set of @xmath40 objects which are being considered in relation to two properties represented by @xmath111 and @xmath53 .",
    "numbering the objects from @xmath112 to @xmath40 for the purposes of identification in any order we please , we may say that they exhibit values @xmath113 according to @xmath111 and @xmath114 according to @xmath53 . to any pair of individuals ,",
    "say the @xmath115th and the @xmath116th@xmath117 , we will allot an @xmath111-score , denoted by @xmath118 if @xmath119(where @xmath120 is the rank of the @xmath115th member according to the @xmath111-quality ) and @xmath121 if @xmath122 , subject only to the condition that @xmath123 .",
    "similarly we will allot a @xmath53-score , denoted by @xmath124 , where @xmath125 . denoting @xmath93 by summing @xmath126 over all values of @xmath115 and @xmath116 from @xmath112 to @xmath40 ,",
    "the kendall @xmath110 coefficient is defined as@xcite : @xmath127    the denominator is the number of pairs of comparison .",
    "the kendall @xmath110 coefficient have three properties : if the agreement between the rankings is perfect , i.e. every individual has the same rank in both , @xmath110 should be @xmath128 , indicating perfect positive correlation ; if the disagreement is perfect , i.e. one ranking is the inverse of the other , @xmath110 should be @xmath129 , indicating perfect negative correlation ; for other arrangements @xmath110 should lie between these limiting values , and in some acceptable sense increasing values from @xmath129 to @xmath112 should correspond to increasing agreement between the ranks",
    ".    in practical applications of ranking methods there sometimes arise cases in which two or more individuals are so similar that no preference can be expressed between them .",
    "the ranking members are then said to be tied . if there is a tie of @xmath130 consecutive members all the scores arising from any pair chosen from them is zero .",
    "there are @xmath131 such pairs .",
    "if , therefore , we write @xmath132    for ties in one ranking , where @xmath133 stands for the summation over various sets of ties in this ranking , and @xmath134    for ties in the other , where @xmath135 stands for the summation over various sets of ties in this ranking , our alternative form of the coefficient @xmath110 for tied ranks may be written @xmath136    we can use the distribution of the kendall @xmath110 coefficient in testing the significance of @xmath110 under the null hypothesis that the two qualities are independent . in the null hypothesis case",
    "the exact distribution of @xmath110 can be calculated exactly for small samples , and as @xmath40 increases it has been proved that the distribution tends to normality , with @xmath137 and @xmath138@xcite .",
    "we use kendall rank correlation to measure the association between the ranking of the computational times by algorithm h_per and the rankings by the permanents and approximate permanents .    for a set of 0 - 1 matrices @xmath139 , let the @xmath140 denote the computational time of the algorithm h_per , @xmath141 denote the exact value of @xmath142 , @xmath143 denote the approximate value of @xmath142 by kklll algorithm , @xmath144 .",
    "the similarities between rank of @xmath145 and rank of @xmath146 , between rank of @xmath145 and rank of @xmath147 are considered respectively .",
    "* case study 1 : kendall rank correlation analysis for random 0 - 1 matrix : * tested by the kendall rank correlation coefficient , the similarities between the ranking of @xmath145 and that of @xmath146 , between the ranking of @xmath145 and that of @xmath147 are significant for all the matrix groups used in subsection 3.1 . take @xmath148 as an example , the table  [ t2 ] shows the kendall @xmath110 coefficients and @xmath149-values . in the kendall rank corelation analysis ,",
    "the @xmath149-value is used for testing the significance of @xmath110 under the null hypothesis that the two qualities are independent against the alternative that the two qualities are dependent .",
    "if the @xmath149-value is small , say less than @xmath150 , then the two qualities are significantly dependent .",
    "* case stduy 2 : kendall rank correlation analysis for 3-regular matrix : * the second example comes from chemical graph theory .",
    "consider the adjacent matrix of a fullerene with 100 atoms , which is a 3-regular @xmath151 matrix .",
    "it is divided into 159 @xmath152 matrices by using the formula ( [ e2 ] ) repeatedly .",
    "the table  [ t3 ] illustrates the result of kendall @xmath110 test .",
    "* case study 3 : kendall rank correlation analysis for 4-regular matrix : * the third example is computing @xmath153 , where @xmath9 is the adjacent matrix of buckministerfullerene @xmath154 , @xmath155 is identity matrix .",
    "the matrix @xmath156 is 4-regular .",
    "the @xmath157 matrix @xmath156 is divided into 123 @xmath158 matrices by using the formula ( [ e2 ] ) repeatedly .",
    "the table  [ t4 ] illustrates the result of kendall @xmath110 test .",
    "the p - values in the three cases are all extremely small . hence the both rankings of @xmath146 and @xmath147 and that of @xmath145 are dependent significantly .",
    "the results show that the computational time with algorithm h_per has a strong rank correlation with the permanent value .",
    "the parallel algorithm is improved by taking advantage of the ordering of estimated permanents as load balancing strategy .",
    "for the algorithm ph , the step 3 is changed as follows .",
    "step 3 : : :    approximate the permanents of @xmath159    by kklll algorithm , sort the matrices    @xmath159 as the non - increasing order    of the approximate permanents , then assign @xmath49 in    the sorted order to the currently least loaded processor and compute    the permanent of @xmath49 by algorithm h_per , until all    @xmath49 s @xmath50 has been    computed .",
    "we use the approximate permanent to give the approximate order with which the sub - matrices divided by algorithm ph are sent to processors . in this section",
    ", the performance of the improved load balancing strategy is tested by the numerical examples , which are arising from molecular chemistry application and choosing from sparse matrix collection .",
    "all numerical experiments in this paper are carried on a 32-bit intel pentium iii ( 1266 mh ) with 32 processors , and the programming language is fortran 90 .",
    "the example of fullerene @xmath160 used in subsection 3.2 is considered again .",
    "the adjacent matrix @xmath9 of @xmath160 is divided into 159 sub - matrices .",
    "these sub - matrices are sent to the processors with the three order strategies .",
    "one is their natural order of expansion , which is the strategy of ph algorithm .",
    "the second is the non - increasing order of the exact computational times of the sub - matrices , which is the ideal strategy according to parallel machine scheduling .",
    "the third is the non - increasing order of the approximate permanent values of the sub - matrices by kklll algorithm , which is the improved strategy proposed in subsection 3.3 .",
    "approximate permanents play a role of the preconditioning , which consumes only a little time compared with the computational time of permanent .    , title=\"fig:\",width=377,height=302 ] +    the numerical results with the three order strategies are shown in the tables  [ t5]-[t7 ] .",
    "the parallel efficiencies of three order strategies are compared in figure [ f1 ] .",
    "the results of improved strategy in table  [ t7 ] are all better than that of the natural order in table  [ t5 ] .",
    "moreover the parallel efficiency in table  [ t7 ] is almost the same with that in table  [ t6 ] except the case of 32 cpu s .",
    "but the efficiency of 32 cpu s has reached 94.61% , which has been good enough in parallel computation .",
    "the permanental polynomial of a graph @xmath161 is of interest in chemical graph theory @xcite .",
    "it is defined as @xmath162 where @xmath9 is the adjacency matrix of the graph @xmath161 with @xmath40 vertices , and @xmath155 is the identity matrix of order @xmath40 .",
    "the permanental polynomial can be obtained by a series of computations of the permanents formed @xmath163 , where @xmath111 is one of the @xmath164-th roots of unity in complex plane @xcite .",
    "the permanental polynomial of buckminsterfullerene @xmath154 is first computed by parallel algorithm ph @xcite .    , title=\"fig:\",width=377,height=302 ] +    the results of natural order , non - increasing order and estimated non - increasing order are shown in tables  [ t8]-[t10 ] respectively",
    ". the parallel efficiencies of three order strategies are compared in figure [ f2 ] .",
    "the parallel efficiency under exact non - increasing order is 95.66% for 32 cpu s , while that of improved strategy in estimated non - increasing order is 94.95% .",
    "we choose a sparse matrix from the university of florida sparse matrix collection@xcite as our third example , which is a large and actively growing set of sparse matrices that arise in real applications .",
    "this symmetric @xmath165 matrix @xmath166 named @xmath167 has @xmath168 nonzero elements , and comes from symmetric connection table from dtnsrdc(david w. taylor naval ship research and development center ) , washington@xcite .",
    "we divide it into @xmath169 @xmath170 sub - matrices .",
    "these sub - matrices are sent to the processors with the three order strategies : the natural order of expansion , the non - increasing order of the exact computational times of the sub - matrices , the non - increasing order of the approximate permanent values of the sub - matrices by kklll algorithm .",
    ", title=\"fig:\",width=377,height=302 ] +    the numerical results with the three order strategies are shown in the tables  [ t15]-[t17 ] .",
    "the parallel efficiencies of three order strategies are compared in figure [ f3 ] .",
    "the results of improved strategy in table  [ t17 ] are all better than that of the natural order in table  [ t15 ] .",
    "moreover the parallel efficiency in table  [ t17 ] is almost the same with that in table  [ t16 ] except the case of 32 cpu s .",
    "but the efficiency of 32 cpu s has reached 94.79% , which has been good enough in parallel computation comparing with the efficiency of the natural order 80.49% .",
    "we have showed the accelerated ratio and parallel efficiency for the three numerical experiments of parallel algorithm @xmath171 using three different load balancing strategies .",
    "the results roughly increase @xmath172 parallel efficiency using the improved strategy in contrast to the natural order strategy .",
    "also the parallel efficiency using the improved strategy is very close to the one using the increasing order strategy , which is the optimal load balancing strategy for parallel computing .",
    "the permanent values of groups of sub - matrices in our experiments are given in appendices .",
    "the matrix permanent has critical applications in combinatorial counting , statistical physics and molecular chemistry . for large scale matrices , parallel methods are developing quickly in recent years . from the results of parallel machine scheduling in combinatorial optimization",
    ", one knows that the desirable parallel efficiency will be achieved when the jobs are sorted in the non - increasing order of their processing times .",
    "hence it is desired to know the processing times of the jobs for achieving good parallel efficiency . in this paper",
    ", we find that there are strong correlation between the permanent value and its computational time . therefore the approximate algorithms for permanent",
    "are used to estimate the computational times of sub - matrices , which are the jobs in the permanent parallel algorithm .",
    "the numerical experiments on fullerene - type graphs , which are of great interest in fullerene chemistry , show that the parallel efficiency is improved remarkably by our load balancing strategy .",
    "the approximate method for matrix permanent used in the paper can also be regarded as a preconditioner for the parallel hybrid algorithm@xcite .",
    "preconditioning is so successful and valuable in numerical linear algebra .",
    "following the similar idea , it is meaningful to establish the basic concepts and a general framework of the precondition methods for permanent computation , by deeply investigating the mechanism of the existing successful algorithms .",
    "it is our future work to develop the preconditions such that the efficiency of the algorithms for permanents can be highly improved , and the realistic scientific computation problems can be solved .",
    "the adjacent matrix of @xmath160 ( written in matlab ) is given below , and the permanent values of 159 sub - matrices for matrix @xmath9 of @xmath160 is listed in table  [ t12 ] ."
  ],
  "abstract_text": [
    "<S> the research in parallel machine scheduling in combinatorial optimization suggests that the desirable parallel efficiency could be achieved when the jobs are sorted in the non - increasing order of processing times . in this paper </S>",
    "<S> , we find that the time spending for computing the permanent of a sparse matrix by hybrid algorithm is strongly correlated to its permanent value . </S>",
    "<S> a strategy is introduced to improve a parallel algorithm for sparse permanent . </S>",
    "<S> methods for approximating permanents , which have been studied extensively , are used to approximate the permanent values of sub - matrices to decide the processing order of jobs . </S>",
    "<S> this gives an improved load balancing method . </S>",
    "<S> numerical results show that the parallel efficiency is improved remarkably for the permanents of fullerene graphs , which are of great interests in nanoscience . </S>"
  ]
}