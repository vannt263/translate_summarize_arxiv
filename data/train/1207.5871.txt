{
  "article_text": [
    "functions describing natural phenomenon or social activities need to be converted into discrete data that can be handled by modern computers . from this viewpoint , sampling is the foundation for signal processing and communication .",
    "the subject origined from the celebrated shannon sampling theorem @xcite , which gurantees the complete reconstruction of a band - limited function from its values on some equally - spaced points .",
    "the elegant result motivates many follow - up studies , making sampling an important research subject in applied mathematics",
    ". we shall give a brief and partial introduction to the history and progresses .",
    "mathematically , sampling means to evaluate a function . to ensure the stability , it is arguable that sampling should only take place in function spaces where point evaluations are continuous .",
    "such spaces when endowed with an inner product structure arise in many other areas of mathematics .",
    "they are termed as the reproducing kernel hilbert spaces ( rkhs ) , as by the riesz s lemma there exists a function that is able to reproduce the function values through the inner product . in shannon",
    "s theorem , the space of functions that are band - limited to @xmath0 $ ] and are equipped with the inner product of @xmath1 is an rkhs with the sinc function as its reproducing kernel .",
    "this interpretation gives the hope of searching for shannon - type complete reconstruction formula for other rkhs .",
    "it was found in @xcite that as long as one has a frame or a riesz basis formed by the reproducing kernel , then a shannon - type sampling formula is immediately available by the general theory of frames .",
    "they showed that many past sampling formulae can be obtained in this manner .",
    "recently , the approach has been generalized to reproducing kernel banach spaces @xcite by frames for banach spaces via semi - inner - products , @xcite .",
    "shannon type formulae enable us to have lossless representation of a function that is usually defined on an uncountable continuous domain using countable data .",
    "going from uncountable to countable is a remarkable progress .",
    "however , countable is still infinite and computers can not store or handle infinitely many data .",
    "this raises the question of how to reconstruct a function from its finite sample . for the crucial band - limited functions , two modified shannon series",
    "have been proposed in the literature @xcite , where it was shown that over - sampling can lead to exponentially decaying approximation error .",
    "sampling data often comes with some cost . when it is available , one is inclined to use as accurate reconstruction methods as possible .",
    "it has long been known that in the maximum sense , the best way of reconstruction in an rkhs is via the minimal norm interpolation @xcite . the approximation error for over - sampling in the paley - wiener space of band - limited functions is estimated in @xcite .    in this note , we focus on another important question in sampling , which is seldom considered in the literature .",
    "usually the number of sampling points in a practical application is limited . when that number is fixed , we ask what is the best strategy of deploying the sampling points , under the condition that the best reconstruction method is engaged .",
    "the study is also motivated by the recent development in basis pursuit @xcite and compressed sensing @xcite , which seek to extract information from as few samples as possible .",
    "since the number of samples is restricted , we should of course distribute the sampling points wisely .",
    "we shall formulate the question in the next section .",
    "it will become clear that the solution of the problem amounts to approximating the subspace spanned by the first few eigenvectors of a compact operator .",
    "when the operator is of finite rank , the eigenvectors can be obtained by the well - known karhunen - love transform ( also called principal component analysis in engineering ) . to extend the algorithm to operators usually defined by integrals in this application",
    ", we shall establish a general karhunen - love transform in section 3 .",
    "an alternative approach by subspace approximation that can significantly reduce computational cost will be introduced in section 4 .",
    "various examples by numerical experiments will be presented in section 5 .",
    "the study will lead to algorithms for the searching of the optimal distribution of finite sampling points for commonly - used rkhs .",
    "a natural choice of background function spaces for sampling is reproducing kernel hilbert spaces ( rkhs ) .",
    "let @xmath2 be a prescribed metric space where functions of interest are defined .",
    "an rkhs on @xmath2 is a hilbert space @xmath3 of functions on @xmath2 such that for each @xmath4 , the point evaluation functional @xmath5 is continuous .",
    "an rkhs @xmath3 possesses a unique _ reproducing kernel _",
    "@xcite , which is a function on @xmath6 characterized by the properties that for all @xmath7 and @xmath4 , @xmath8 and @xmath9 where @xmath10 denotes the inner product on @xmath3 . on the other hand ,",
    "the reproducing kernel @xmath11 uniquely determines the rkhs @xmath3 .",
    "thus , the rkhs of a reproducing kernel @xmath11 is usually denoted by @xmath12 . for more information on reproducing kernels ,",
    "see @xcite .",
    "we emphasize that an rkhs should first be a hilbert space of functions , which implies that a function in the space has zero norm if and only if it vanishes everywhere .",
    "for instance , the paley - wiener space @xmath13^{d}\\}\\ ] ] is an rkhs . in this paper ,",
    "the fourier transform @xmath14 of @xmath15 is defined by @xmath16 where @xmath17 is the standard inner product on @xmath18 .",
    "the norm on @xmath19 inherits from that in @xmath20 .",
    "the reproducing kernel for the paley - wiener space @xmath19 is the sinc function @xmath21    we consider the deployment of finite sampling points in an rkhs in this paper .",
    "let @xmath12 be an rkhs on a metric space @xmath2 and the number @xmath22 of sampling points be fixed .",
    "the choice of the sampling points depends on the method of reconstruction and the measurement of the approximation error . for most applications ,",
    "one desires to reconstruct values of the function considered on a compact subspace @xmath23 .",
    "the reconstruction error will be measured by the norm in @xmath24 . here",
    "@xmath25 $ ] , @xmath26 is a finite positive borel measure on @xmath27 , and the banach space @xmath24 consists of borel measurable functions @xmath28 on @xmath27 that satisfy @xmath29 and @xmath30    we shall assume throughout this paper that @xmath11 is continuous on @xmath27 .",
    "we observe by the reproducing property ( [ reproducing ] ) for all @xmath31 and @xmath32 that @xmath33 therefore , every function @xmath34 belongs to the space @xmath35 of continuous functions on @xmath27 equipped with the usual maximum norm .",
    "consequently , @xmath36 for all @xmath37 and all finite borel measures @xmath26 on @xmath27 .",
    "let @xmath38 be a choice of @xmath22 sampling points .",
    "the sample data of a function @xmath32 is hence of the form @xmath39 a reconstruction method @xmath40 is then a mapping from @xmath41 to @xmath24 . for a particular @xmath32 ,",
    "the reconstruction error is measured by @xmath42 we then follow the general setting of optimal sampling in @xcite and @xcite , that is , we measure the performance of a reconstruction method @xmath40 by @xmath43 since we are concerned with the optimal choice of sampling points only , we shall try to remove the reconstruction method from the picture . to this end",
    ", we shall use the optimal reconstruction algorithm @xmath44 for each choice of sampling points @xmath45 .",
    "namely , @xmath46 finally , our problem reduces to finding the sampling points @xmath45 that minimizes the function @xmath47    the optimal reconstruction algorithm @xmath40 is known to be the minimal norm interpolation @xcite .",
    "the following lemma also gives the reconstruction error .",
    "[ optimalalgorithm ] for each set of sampling points @xmath48 , the optimal reconstruction method @xmath44 satisfying ( [ cacx ] ) is given by @xmath49 the associated reconstruction error is of the form @xmath50    a reproducing kernel determines everything about the corresponding rkhs .",
    "the following simple observation fulfills this hope .",
    "set @xmath51 we shall impose another assumption through the paper that for every set of pairwise distinct sampling points @xmath45 , the matrix @xmath52:=[k(x_j , x_k):\\ 1\\le j , k\\le n]\\ ] ] is nonsingular .",
    "a reproducing kernel is at the same time a positive - definite function , @xcite .",
    "thus , @xmath53 $ ] is strictly positive - definite . with this assumption , @xmath54 is @xmath22-dimensional with the orthonormal basis @xmath55 where @xmath56=(k[\\cx])^{-1/2}.\\ ] ]    let @xmath57 be defined by @xmath58 then it holds true for each @xmath59 that @xmath60 furthermore , for each @xmath61 @xmath62 and for the special case when @xmath63 , @xmath64    let @xmath28 be an arbitrary function in @xmath12 such that @xmath65 and @xmath66",
    ". then by the reproducing property ( [ erro ] ) , @xmath67 it follows that @xmath28 is orthogonal to every @xmath68 .",
    "we hence see that @xmath69 as the above equation is true for all @xmath70 , we get that @xmath71 , @xmath4 . as a result",
    ", it holds for all @xmath61 that @xmath72    on the other hand , letting @xmath28 be the orthogonal projection of @xmath73 onto @xmath54 and then be normalized to a unit vector yields ( [ reformulation ] ) .",
    "thus , for @xmath63 , @xmath74 which proves ( [ errorinfty ] ) .    by the above corollary",
    ", we shall hence try to minimize the quantity @xmath75 as a way to bound the intrinsic error @xmath76 .",
    "a simple calculation tells that @xmath77 this together with ( [ onb ] ) and ( [ coealpha ] ) gives a function about @xmath45 that needs to be minimized .",
    "the complicated form of the function coped with the nonlinearity of the reproducing kernel makes directly minimizing this function rather difficult . before discussing alternative computational methods , we present two simple examples to demonstrate that the optimal points might not be equally - spaced distributed in the reconstruction domain @xmath27 .    in this trivial example , we let @xmath78 , @xmath27 a compact subset in @xmath18 and @xmath79 .",
    "the reproducing kernel is given by a radial basis function @xmath80 where @xmath81 denotes the standard euclidean norm on @xmath18 .",
    "the function @xmath82 is a univariate function that defines a reproducing kernel in the above manner . by schoenberg",
    "s theorem @xcite , @xmath83 must be a completely monotone function .",
    "in particular , @xmath84 is nonincreasing . for simplicity",
    ", we also assume that @xmath85 .",
    "we shall use the space @xmath35 to measure the reconstruction error .",
    "the optimal sampling point @xmath86 is hence the minimizer of which leads to @xmath87 by the above equation , @xmath86 is the point has a minimal radius @xmath88 for which @xmath89 .",
    "particularly , for @xmath90 , we should choose @xmath86 as the mid - point of the end points of @xmath27 .",
    "unlike the above example , our second example shows that nonlinearity could occur as the number of sampling points exceeds @xmath91 . the analysis of this example of two sampling points is already rather tedious but elementary , and is thus omitted .",
    "[ example2 ] in this example , we let @xmath92 , @xmath93\\subseteq \\br$ ] , @xmath94 and consider the exponential kernel @xmath95 in this case , for @xmath96 , @xmath97 where @xmath98 the optimal sampling points @xmath99 is the minimizer of @xmath100 let @xmath101 . after some",
    "careful but elementary analysis , it can be found that the optimal sampling points are @xmath102 and @xmath103    although measuring the reconstruction error by the maximum norm in @xmath35 seems the most natural and the maximum norm dominates other @xmath104 norms , finding the extrema of a multivariate function is always difficult .",
    "a hilbert space norm can often save computation efforts . from this consideration , we restrict ourself to the choice @xmath105 in the rest of the paper . in the case when @xmath106 with @xmath107 and @xmath108 for @xmath109 , the @xmath22-dimensional subspace @xmath110 that minimizes @xmath111 is given by the karhunen - love transform . more specifically , @xmath110 is spanned by the eigenfunctions corresponding to the largest @xmath22 eigenvalues of the compact positive bounded linear operator @xmath112 on @xmath12 given by @xmath113 the process of computing the eigenfunctions and eigenvalues of this operator is also known as kernel principal component analysis in machine learning @xcite .",
    "of course , the story is not over yet as the space we are looking for should be of the form ( [ kernelspace ] ) .",
    "our idea is to find sampling points @xmath45 for which @xmath54 best approximates the subspace spanned by the first @xmath22 eigenfunctions of @xmath112 .",
    "before we estimate the distance between these two subspaces of @xmath12 , we first show that for general reconstruction error , the minimization problem @xmath114 can still be reduced to computing the first @xmath22 eigenfunctions of a compact positive bounded linear operator on @xmath12 .",
    "we shall prove such a karhunen - love transform exists for general measure @xmath26 .",
    "the purpose of this section is to show that the subspace that minimizes ( [ klproblem ] ) is spanned by the first @xmath22 eigenfunctions of a compact positive bounded linear operator .",
    "we shall prove this result under a very general setting .",
    "let @xmath3 be an infinite - dimensional separable hilbert space , @xmath115 be a measure space , that is , @xmath116 is a @xmath117-algebra consisting of certain subsets of @xmath27 and @xmath26 is a finite positive measure on @xmath116 .",
    "we assume that there is a function @xmath118 such that for each @xmath119 , the function @xmath120 is measurable with respect to @xmath116 and such that @xmath121 . for a fixed @xmath122",
    ", we want to find an @xmath22-dimensional subspace @xmath123 of @xmath3 that approximates @xmath124 well . by measuring the approximation of each candidate subspace @xmath123 as @xmath125 the optimal approximating subspace",
    "@xmath126 is the one that minimizes the above error among all @xmath22-dimensional subspaces of @xmath3 .",
    "a karhunen - love transform for this general question is presented below .",
    "[ kl ] the operator @xmath127 determined by @xmath128 is compact positive bounded linear .",
    "the optimal @xmath22-dimensional subspace @xmath126 that satisfies @xmath129 is given by @xmath130 , where @xmath131 s are the orthonormal eigenfunctions corresponding to the largest @xmath22 eigenvalues of @xmath112 .",
    "let @xmath132 be fixed .",
    "then for each @xmath119 , we observe that @xmath133 where @xmath134 it implies that @xmath135 is a bounded linear functional on @xmath3 . by the riesz representation theorem",
    ", there exists a unique vector @xmath136 associated with @xmath137 such that @xmath138 we denote the mapping sending @xmath137 to @xmath136 by @xmath112 .",
    "it is clear that this operator is linear .",
    "moreover , we have @xmath139 therefore , @xmath140 , implying that @xmath112 is bounded .",
    "we also see that for all @xmath119 @xmath141 thus , @xmath112 is positive .",
    "we next show that @xmath112 is compact . to this end , let @xmath142 be a bounded sequence in @xmath3 .",
    "then @xmath143 is bounded as well . as @xmath3 is reflexive ,",
    "its unit ball is weakly compact .",
    "we may hence assume that @xmath143 converges weakly to some @xmath144 in @xmath3 .",
    "in other words , @xmath145 we shall prove that @xmath143 converges to @xmath144 strongly in @xmath3 .",
    "note that @xmath146 as @xmath147 as @xmath148 , it suffices to show that @xmath149 we observe from the definition of @xmath112 that @xmath150 for each @xmath151 , @xmath152 as @xmath143 converges weakly to @xmath144 . as a result",
    ", there holds @xmath153 furthermore , @xmath154 the above equations together imply by the lebesgue dominated convergence theorem that @xmath155 therefore , @xmath156 as @xmath148 .",
    "we have hence proved that @xmath112 is a positive compact bounded linear operator on @xmath3 .    turning to the last claim of the theorem",
    ", we let @xmath123 be an @xmath22-dimensional subspace of @xmath3 with the orthonormal basis @xmath157 , @xmath158 .",
    "then @xmath159 thus , the question amounts to finding an orthonormal sequence @xmath160 in @xmath3 that maximizes the sum @xmath161 the analysis of this last part is the same as that for the standard karhunen - love transform , that is , the optimal sequence is achieved by the orthonormal eigenfunctions corresponding to the largest @xmath22 eigenvalues of @xmath112 .",
    "returning to the sampling , we specify @xmath27 to be a compact subset of the input space @xmath2 , @xmath26 to be a finite positive borel measure on @xmath2 , @xmath11 to be a continuous kernel on @xmath2 , and @xmath162 by theorem [ kl ] , the bounded linear operator @xmath112 from @xmath12 to @xmath12 determined by @xmath163 is positive and compact .",
    "it is of the explicit form @xmath164 for each @xmath22-dimensional subspace @xmath123 of @xmath12 with the orthonormal basis @xmath165 , @xmath166 an orthonormal basis for @xmath167 is given by ( [ onb ] ) .",
    "thus , @xmath168 setting @xmath169 we conclude that the optimal sampling set @xmath45 is the solution of @xmath170)^{-1/2}\\bk ( k[\\cx])^{-1/2}\\right)=\\max_{\\cx\\in x^n}\\mbox{tr}\\left(\\bk^{1/2 } ( k[\\cx])^{-1}\\bk^{1/2}\\right),\\ ] ] where @xmath171 stands for the trace of a square matrix @xmath172 .",
    "when the eigenfunctions and eigenvalues of the operator @xmath112 is known , one has a different formulation of the above optimization problem .",
    "let @xmath173 , @xmath174 be all the orthonormal eigenfunctions of @xmath112 with a positive eigenvalue @xmath175 .",
    "we see for all @xmath176 that @xmath177    practically , we are most concerned with the case when @xmath27 has finite cardinality that is considerably larger than @xmath22 . in this situation , @xmath112 has finite rank .",
    "assume that @xmath178 and set @xmath179 with these notations , @xmath180 .",
    "when @xmath112 is of finite rank , this together with the fact that for a square matrix @xmath181 , @xmath182 yields an equivalent formulation of ( [ algorithm1 ] ) @xmath183)^{-1}d\\lambda\\right).\\ ] ] computing all the eigenfunctions and eigenvalues of @xmath112 can be costly when @xmath184 is large",
    ". instead of attacking ( [ algorithm1 ] ) or ( [ algorithm2 ] ) directly , we shall relax ( [ algorithm2 ] ) to use only the @xmath22 eigenfunctions of @xmath112 corresponding to the first @xmath22 largest eigenvalues of @xmath112 , which can often be obtained efficiently by the standard karhunen - love algorithm .",
    "following the idea described at the end of section 2 , we shall achieve this by estimating the distance between @xmath54 and the one spanned by the first @xmath22 eigenfunctions of @xmath112 .",
    "we now let @xmath131 , @xmath158 be the orthonormal eigenfunctions of @xmath112 , defined as in ( [ operatortk1 ] ) , corresponding to the largest @xmath22 eigenvalues of @xmath112 .",
    "we assume that these eigenvalues are positive . by theorem [ kl ] ,",
    "the subspace @xmath185 is a minimizer of optimization problem ( [ klproblem ] ) .",
    "we wish to find sampling points @xmath45 such that @xmath186 is small , where for a closed subspace @xmath123 of @xmath12 , @xmath187 to this end , we first observe that for any closed subspaces @xmath188 and @xmath123 of @xmath12 , @xmath189 can be bounded by the subspace distance between @xmath188 and @xmath123 .",
    "denote by @xmath190 the orthogonal projection operator from @xmath12 onto @xmath123 .",
    "the distance between two closed subspaces @xmath188 and @xmath123 of @xmath12 is defined by @xmath191 where @xmath192 is the operator norm of @xmath193 , that is , @xmath194 apparently , the above supremum can be restricted to the closed subspace spanned by the union of @xmath188 and @xmath123 .    [ boundbydist ]",
    "it holds for any two closed subspaces @xmath188 and @xmath123 of @xmath12 that @xmath195    denote by @xmath196 the identity operator .",
    "we estimate that @xmath197 from which ( [ boundbydisteq ] ) follows .",
    "according to the above lemma , we face to figure out the distance between subspaces @xmath54 and @xmath198 . to this end",
    ", we introduce some notations . set @xmath199 in other words",
    ", @xmath157 is the orthogonal projection of @xmath200 onto @xmath198 .",
    "also , set @xmath201 accordingly , we define two positive definite matrices by letting @xmath202\\ \\mbox{and}\\ \\mathbf{b}:=[(h_k , h_j)_{\\ch_k}:1\\le j , k\\le n]\\ ] ] we shall assume that @xmath203 and @xmath204 are both nonsingular . it will be shown in the proof below that @xmath205^t$ ] .",
    "we assume in this section that @xmath206 $ ] is nonsingular as well .",
    "[ subspacedistlemma ] if the matrix @xmath207 $ ] is nonsingular then @xmath208^t(\\mathbf{e}\\mathbf{e}^{*})^{-1})}}.\\ ] ] where @xmath209 denotes the largest eigenvalue of a square matrix @xmath172 .",
    "if @xmath210 is singular then @xmath211 .",
    "if @xmath210 is singular then there exists a nonzero function in @xmath54 that is orthogonal to @xmath198 .",
    "it follows immediately that @xmath211 .",
    "suppose that @xmath210 is nonsingular . by the nonsingularity of @xmath210 , @xmath198 is identical with the following subspace of @xmath12 : @xmath212 the space @xmath54 coincides with @xmath213 . for later use",
    ", we also introduce another two subspaces of @xmath12 : @xmath214 we first observe that @xmath215 any @xmath216 can be represented as @xmath217 , where @xmath218 and @xmath219 . by definition",
    ", we have @xmath220 for all @xmath221 , which yields that @xmath188 is orthogonal to @xmath123 .",
    "we get that @xmath222 to estimate ( [ dist ] ) , we first give @xmath223 explicitly . to this end , we assume that @xmath224 for some @xmath225 . by the characterization of orthogonal projections , we get the equations @xmath226 which leads to @xmath227 set @xmath228^t$ ] . for each",
    "@xmath229 we set @xmath230^t$ ] and @xmath231^t$ ]",
    ". then equation ( [ cequation ] ) can be rewritten in a matrix form @xmath232 thus we obtain @xmath233 by ( [ projection ] ) , we have @xmath234 substituting ( [ c ] ) into the above equation , we get that @xmath235 together with the fact that @xmath236 the above equation leads to @xmath237 let @xmath238 and @xmath239 . by introducing a matrix @xmath240",
    "we get that @xmath241^*\\mathbf{m}\\left[\\begin{array}{cc } \\mathbf{a}\\\\ \\mathbf{b } \\end{array } \\right]}{\\left\\|\\left[\\begin{array}{cc } \\mathbf{a}\\\\ \\mathbf{b } \\end{array } \\right]\\right\\|_2 ^ 2}=\\|\\mathbf{m}\\|_2,\\ ] ] where @xmath242 denotes the standard euclidean norm of a vector or the spectral norm of a square matrix . on the one hand , we have @xmath243 since the matrix @xmath203 is nonsingular",
    ", the matrix @xmath244 has the same eigenvalues with the matrix @xmath245 .",
    "hence , we have @xmath246 on the other hand , by the nonsingularity of the matrix @xmath204 , we also have @xmath247 combining ( [ estimate1 ] ) with ( [ estimate2 ] ) , we get that @xmath248 for each @xmath221 , there holds @xmath249 which leads to @xmath250^t-\\mathbf{a}$ ] .",
    "hence , we obtain @xmath251^t-\\mathbf{a})(k[\\mathcal{x}]^t)^{-1})=1- \\rho_{\\mbox{min}}(\\mathbf{a}(k[\\mathcal{x}]^t)^{-1})=1-\\frac{1}{\\lambda_{\\mbox{max}}(k[\\mathcal{x}]^t\\mathbf{a}^{-1})}.\\ ] ] it follows from @xmath252 that there holds ( [ subspacedisteq ] ) .",
    "combining lemmas [ boundbydist ] and [ subspacedistlemma ] , we obtain a bound for the distance between @xmath54 and the optimal subspace @xmath198 and give the last optimization problem for the searching of optimal sampling points .",
    "[ subspacedist ] if @xmath210 is nonsingular then @xmath253^t(\\mathbf{e}\\mathbf{e}^{*})^{-1})}}.\\ ] ]    we conclude that the subspace approximation approach leads to the following problem @xmath254^t(\\mathbf{e}\\mathbf{e}^{*})^{-1})\\ ] ] to be solved for the searching of optimal sampling points .",
    "we remark that when the measure @xmath26 is discrete as in most practical applications , ( [ algorithm3 ] ) is computationally favorable over ( [ algorithm1 ] ) .",
    "the reason is that in this case , an orthonormal basis for the optimal subspace @xmath198 can be easily computed by the karhunen - love transform . at each stage of searching for the candidate sampling points @xmath45",
    ", the matrix @xmath255 can be obtained efficiently and the major computation occurs with taking the inverse of a matrix . as comparison ,",
    "algorithm ( [ algorithm1 ] ) additional requires the computation of the matrix @xmath256 and its square root .",
    "in this section , we give some numerical experiments to illustrate the performance of algorithms ( [ algorithm1 ] ) and ( [ algorithm3 ] ) for the searching of optimal sampling points . to this end , we first recall by lemma [ optimalalgorithm ] that for an obtained @xmath22 sampling points @xmath257 , the optimal method of reconstructing @xmath258 of a given function @xmath32 from the sampled data @xmath259 is given by @xmath260 where the coefficients @xmath261 are the unique solution of the linear system @xmath262 here we assume throughout the section that the kernel matrix @xmath206 $ ] is nonsingular .    therefore , our procedure of experiments is as follows .",
    "we shall consider the gaussian kernel @xmath263 and the sinc kernel @xmath264 let @xmath11 be one of these two kernels , @xmath265 be compact , and @xmath26 be a selected borel measure on @xmath27 .",
    "we then solve the optimization problem ( [ algorithm1 ] ) or ( [ algorithm3 ] ) to obtain @xmath22 sampling points @xmath266 , which are to be compared with the commonly used equally - spaced sampling points @xmath267 .",
    "for this purpose , we randomly generate 100 finite linear combinations @xmath28 of the kernel @xmath268 as the target functions to be sampled , where both the coefficients @xmath269 s and the locations @xmath270 s will be randomly generated by the uniform distribution .",
    "for each of those target functions @xmath28 , we then compute by ( [ reconstruction ] ) and ( [ coefficients ] ) the reconstructed functions @xmath271 and @xmath272 from the sampled values of @xmath28 on @xmath266 and @xmath267 , respectively .",
    "finally , the relative approximation errors @xmath273 are calculated .    to present the results , we shall first plot @xmath266 against @xmath267 .",
    "the mean and standard deviation of the difference @xmath274 for the 100 pairs of relative errors will then be tabulated . finally , we plot the 100 pairs of relative errors for a visual comparison , followed by discussion .",
    "* experiment 1 : algorithm ( [ algorithm1 ] ) , @xmath275 the one - dimensional gaussian kernel , @xmath276 , @xmath277 $ ] , @xmath278 the lebesgue measure on @xmath27 .",
    "*    * figure 5.1 * distribution of the obtained 12 optimal sampling points ( marked with a star ) and the equally - spaced points ( marked with a circle ) on @xmath277 $ ] .    [ 0.6 ]    * table 5.1 * the mean and standard deviation of the improvement @xmath274 .",
    "@xmath279    * figure 5.2 * relative approximation errors @xmath280 ( marked with a circle ) and @xmath281 ( marked with a star ) .    [ 0.6 ]    we observe that for the 100 pairs of relative approximation errors , there are only 20 pairs for which @xmath280 is larger than @xmath281 . recall that the optimal sampling points are designed to ensure that it is best in average for all the functions in the rkhs @xmath12 .",
    "therefore , situations where the optimal sampling points perform worse than the equally - spaced sampling points could indeed occur .",
    "for this experiment , one sees that in those 20 instances , the relative errors @xmath280 and @xmath281 are comparable . more importantly , for all the instances where the relative error corresponding to the equally - spaced sampling points exceeds @xmath282 , the usage of the optimal sampling points can always bring down the relative error to below @xmath282 . we conclude that for this example the obtained optimal sampling points are superior to the equally - spaced points .",
    "* experiment 2 : algorithm ( [ algorithm1 ] ) , @xmath275 the one - dimensional sinc kernel , @xmath283 , @xmath277 $ ] , @xmath278 the lebesgue measure on @xmath27 .",
    "*    * figure 5.3 * distribution of the obtained 8 optimal sampling points ( marked with a star ) and the equally - spaced points ( marked with a circle ) on @xmath277 $ ] .",
    "[ 0.5 ]    * table 5.2 * the mean and standard deviation of the improvement @xmath274 .",
    "@xmath284    * figure 5.4 * relative approximation errors @xmath280 ( marked with a circle ) and @xmath281 ( marked with a star ) .    [ 0.6 ]    for the 100 pairs of relative approximation errors , there are 23 pairs for which @xmath280 is larger than @xmath281 .",
    "there are 34 @xmath281 ( compared to 10 @xmath280 ) that are larger than @xmath285 .",
    "and in 26 instances among those 34 , replacing the equally - spaced points with the optimal sampling points reduces the relative approximation error to below @xmath285 .",
    "we also conclude that for this example the obtained optimal sampling points perform better than the equally - spaced points , although the improvement is not as drastic as experiment 1 .",
    "* experiment 3 : algorithm ( [ algorithm1 ] ) , @xmath275 the two - dimensional gaussian kernel , @xmath286 , @xmath287\\times[-2,2]$ ] , @xmath278 the lebesgue measure on @xmath27 .",
    "*    * figure 5.5 * distribution of the obtained 36 optimal sampling points ( marked with a star ) and the equally - spaced points ( marked with a circle ) on @xmath288\\times[-2,2]$ ] .    [ 0.5 ]    * table 5.3 * the mean and standard deviation of the improvement @xmath274 .",
    "@xmath289    * figure 5.6 * relative approximation errors @xmath280 ( marked with a circle ) and @xmath281 ( marked with a star ) .",
    "[ 0.45 ]    for the 100 pairs of relative approximation errors , there are 23 pairs for which @xmath280 is larger than @xmath281 . in these pairs ,",
    "@xmath281 and @xmath280 are rather close .",
    "we see that the value of the optimal sampling points lies in that they could dramatically reduce the relative error when the equally - spaced points perform badly .",
    "there are 10 such examples in figure 5.6 .",
    "* experiment 4 : algorithm ( [ algorithm1 ] ) , @xmath275 the two - dimensional sinc kernel , @xmath290 , @xmath287\\times[-2,2]$ ] , @xmath278 the lebesgue measure on @xmath27 .",
    "*    * figure 5.7 * distribution of the obtained 25 optimal sampling points ( marked with a star ) and the equally - spaced points ( marked with a circle ) on @xmath287\\times[-2,2]$ ] .    [ 0.5 ]    * table 5.4 * the mean and standard deviation of the improvement @xmath274 .",
    "@xmath291    * figure 5.8 * relative approximation errors @xmath280 ( marked with a circle ) and @xmath281 ( marked with a star ) .    [ 0.5 ]    in the 100 pairs of relative approximation errors , there are 31 pairs for which @xmath280 is larger than @xmath281 .",
    "we see from figure 5.7 that for this example , the obtained optimal sampling points are rather close to the equally - spaced points . as a consequence ,",
    "the relative approximation errors shown in figure 5.8 are comparable .    in the following , we present two experiments about algorithm ( [ algorithm3 ] ) .",
    "* experiment 5 : algorithm ( [ algorithm3 ] ) , @xmath275 the one - dimensional gaussian kernel , @xmath276 , @xmath277 $ ] , @xmath26 is the uniform discrete measure supported at the 30 equally - spaced points in @xmath27 . *",
    "* figure 5.9 * distribution of the obtained 12 optimal sampling points ( marked with a star ) and the equally - spaced points ( marked with a circle ) on @xmath277 $ ] .",
    "[ 0.5 ]    * table 5.5 * the mean and standard deviation of the improvement @xmath274 .",
    "@xmath292 * figure 5.10 * relative approximation errors @xmath280 ( marked with a circle ) and @xmath281 ( marked with a star ) .    [ 0.5 ]    in the 100 pairs of relative approximation errors , there are only 16 pairs for which @xmath280 is larger than @xmath281 .",
    "one sees that in those 16 instances , the relative errors @xmath280 and @xmath281 are comparable . for the remaining 84 instances ,",
    "the improvement brought by the optimal sampling points resulting from algorithm ( [ algorithm3 ] ) is drastic . in particular , there are 39 instances where @xmath281 exceeds @xmath293 while only three @xmath280 do so . comparing results here with those in experiment 1 , one sees that algorithm ( [ algorithm3 ] ) is superior to ( [ algorithm1 ] ) for this problem .",
    "* experiment 6 : algorithm ( [ algorithm3 ] ) , @xmath275 the one - dimensional sinc kernel , @xmath283 , @xmath277 $ ] , @xmath26 is the uniform discrete measure supported at the 20 equally - spaced points in @xmath27 . *",
    "* figure 5.11 * distribution of the obtained 8 optimal sampling points ( marked with a star ) and the equally - spaced points ( marked with a circle ) on @xmath277 $ ] .",
    "[ 0.5 ]    * table 5.6 * the mean and standard deviation of the improvement @xmath274 .",
    "@xmath294 * figure 5.12 * relative approximation errors @xmath280 ( marked with a circle ) and @xmath281 ( marked with a star ) .    [ 0.5 ]    in the 100 pairs of relative approximation errors , there are only 28 pairs for which @xmath280 is larger than @xmath281 . except for 5 outliers , @xmath295 for those instances . for the remaining 72 improved instances ,",
    "there are 21 for which @xmath296 and 8 for which @xmath297 .",
    "we conclude that the optimal sampling points yielding from algorithm ( [ algorithm3 ] ) are significantly better than the equally - spaced points .",
    "the results here outperform those in experiment 2 .",
    "10 n. aronszajn , theory of reproducing kernels , _ trans .",
    "* 68 * ( 1950 ) , 337404 .",
    "a. berlinet and c. thomas - agnan , _ reproducing kernel hilbert spaces in probability and statistics _ , kluwer , dordrecht , 2004 .",
    "e. j. candes , j. romberg and t. tao , robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , _ ieee trans .",
    "inform . theory _",
    "* 52 * ( 2006 ) , 489509 .",
    "s. s. chen , d. l. donoho and m. a. saunders , atomic decomposition by basis pursuit , _",
    "siam j. sci .",
    "comput . _ * 20 * ( 1998 ) , 3361 .",
    "t. evgeniou , m. pontil , and t. poggio , regularization networks and support vector machines , _ adv .",
    "_ * 13 * ( 2000 ) , 150 .",
    "d. jagerman , bounds for truncation error of the sampling expansion , _ siam j. appl .",
    "_ * 14 * ( 1966 ) , 714723 .",
    "p. e. t. jorgensen and m .- s .",
    "song , entropy encoding , hilbert space , and karhunen - love transforms , _",
    "* 48 * ( 2007 ) , 103503 .",
    "g. kimeldorf and g. wahba , some results on tchebycheffian spline functions , _ j. math .",
    "appl . _ * 33 * ( 1971 ) , 8295 .",
    "j. liu , c. a. micchelli , r. wang and y. xu , finite rank kernels for multi - task learning , _ adv . comput .",
    "_ , in press .    c. a. micchelli , y. xu , and h. zhang , universal kernels , _ j. mach",
    "_ * 7 * ( 2006 ) , 26512667 .    c. a. micchelli , y. xu and h. zhang , optimal learning of bandlimited functions from localized sampling ,",
    "_ j. complexity _ * 25 * ( 2009 ) , 85114 .",
    "m. z. nashed and g. g. walter , general sampling theorems for functions in reproducing kernel hilbert spaces , _ math .",
    "control signals systems _ * 4 * ( 1991 ) , 363390 .",
    "l. qian , on the regularized whittaker - kotelnikov - shannon sampling formula , _ proc .",
    "* 131 * ( 2003 ) , 11691176 .    c. a. micchelli and t. j. rivlin , _ lectures on optimal recovery _ , lecture notes in mathematics * 1129 * , springer - verlag , berlin , 1985 .",
    "i. j. schoenberg , metric spaces and completely monotone functions , _ ann . of math.(2 ) _ * 39 * , ( 1938 ) , 811841 .",
    "b. schlkopf and a. j. smola , _ learning with kernels : support vector machines , regularization , optimization , and beyond _ , mit press , cambridge , mass , 2001 .    c. e. shannon , communication in the presence of noise , _ proc",
    "* 37 * ( 1949 ) , 1021 .    v. n.",
    "vapnik , _ statistical learning theory _ , wiley , new york , 1998 .",
    "h. zhang , _ sampling with reproducing kernels _ , ph.d .",
    "thesis , syracuse university , 2010 .",
    "h. zhang , y. xu , and j. zhang , reproducing kernel banach spaces for machine learning , _ j. mach .",
    "res . _ * 10 * ( 2009 ) , 27412775 .",
    "h. zhang and j. zhang , frames , riesz bases , and sampling expansions in banach spaces via semi - inner products , _ appl .",
    "_ * 31 * ( 2011 ) , 125 .",
    "we shall prove that the optimal sampling points for example [ example2 ] are given by ( [ optpoint1 ] ) and ( [ optpoint2 ] ) . the proof is done by considering each case of the relative location of the two sampling point with respect to the reconstruction domain @xmath93 $ ] .",
    "case 2 : @xmath99 lie on the left hand and the right hand of @xmath27 , respectively .",
    "we set @xmath303 and @xmath304 . for each @xmath59",
    ", we also let @xmath305 . by these notations",
    ", we get that @xmath306}\\frac{e^{-2(u+t)}+e^{-2(l - u+s)}-2e^{-2(l+s+t)}}{1-e^{-2(l+s+t)}}.\\ ] ] if @xmath307 , we obtain that the minimum achieves at @xmath302 and @xmath308 which is decreasing with respect to @xmath309 and @xmath310 .",
    "hence , we get the conclusion that @xmath311 where the supremum achieves at @xmath312 and @xmath313 .",
    "similarly , for the case when @xmath314 , we also get that @xmath315 for the case when @xmath316 , the minimum achieves at @xmath317 and there holds @xmath318 by taking the supremum of the above equation , we have @xmath319 it follows from the inequality @xmath320 that in case ( 2 ) , there holds @xmath321    case 3 : @xmath322 .",
    "we set @xmath323 and @xmath324 .",
    "if @xmath325 , we have that @xmath326 similarly , we also get for @xmath327 that @xmath328 if @xmath329 $ ] , there holds @xmath330 thus the minimum of @xmath123 achieves at @xmath331 and there holds @xmath332}v(x , x_1,x_2)=\\frac{2e^{-r}}{1+e^{-r}}.\\ ] ] according to the above discussion , we need to consider @xmath333 it is not difficult to see that @xmath334 where there holds @xmath335 by solving equation ( [ equationu ] ) , we obtain @xmath336 and @xmath337    it remains to compare ( [ max1 ] ) , ( [ max2 ] ) and ( [ max3 ] ) . by calculation , we have @xmath338 which implies the optimal two sampling points should be placed inside @xmath27 by ( [ optpoint1 ] ) and ( [ optpoint2 ] ) ."
  ],
  "abstract_text": [
    "<S> the recent developments of basis pursuit and compressed sensing seek to extract information from as few samples as possible . in such applications , since the number of samples is restricted , one should deploy the sampling points wisely . </S>",
    "<S> we are motivated to study the optimal distribution of finite sampling points . </S>",
    "<S> formulation under the framework of optimal reconstruction yields a minimization problem . in the discrete case </S>",
    "<S> , we estimate the distance between the optimal subspace resulting from a general karhunen - love transform and the kernel space to obtain another algorithm that is computationally favorable . </S>",
    "<S> numerical experiments are then presented to illustrate the performance of the algorithms for the searching of optimal sampling points .    </S>",
    "<S> * keywords : * sampling points , optimal distribution , reproducing kernels , the karhunen - love transform </S>"
  ]
}