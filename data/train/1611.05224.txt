{
  "article_text": [
    "tests of goodness - of - fit constitute an important component of statistical methodology .",
    "they assess the agreement between the observed data and the model family through appropriate test statistics indicating the extent of possible error in our model based inference .",
    "the standard approach of assessing the goodness - of - fit is generally based on suitable chi - square type divergences , such as the pearson s chi - square ( pcs ) and the likelihood divergence ( ld ) .",
    "the latter divergence generates the likelihood ratio chi - square .",
    "many other chi - square type divergences are available in the literature .",
    "for example the power divergence ( pd ) family of @xcite provides several other choices beyond these two .",
    "standard approaches to goodness - of - fit testing are also described in many other sources such as @xcite , @xcite , @xcite etc . however , as noted in @xcite , all these tests generally fail to pick out any specific model when the sample size is sufficiently large as the specified model would be eventually edged out by a more complicated model and hence these tests fail to assess if the specified model is actually a good approximation or not .",
    "it may be that the specified model ( simple and easily workable ) is already very close to the true data generating distribution ( i.e , the discrepancy between them is very small , within practically acceptable range ) , yet these chi - square type goodness - of - fit tests might get rejected for large sample sizes in favor of more complicated models which have marginally lower discrepancy but are otherwise hard to work with . in such situations",
    "any practitioner will prefer to accept the simpler model as the error is already in acceptable range , but these chi - square type tests will not help them in this context .    to get around this problem , @xcite developed a measure of  model adequacy \" based on a mixture method for contingency tables .",
    "let @xmath1 denote the true data generating probabilities for the cells in the contingency table and consider a parametric model family of cell probabilities denoted by @xmath2 .",
    "then , one can express @xmath1 as a two - point mixture of a model element @xmath3 and an unspecified modeling error probability @xmath4 ; this mixture is given by @xmath5 , where @xmath6 is the weight of error in modeling through @xmath7 .",
    "then , the model adequacy index or the mixture index of fit @xmath8 can be defined as the smallest possible value of this @xmath6 indicating the minimum error proportion that can not be explained through the model family @xmath7 .",
    "so , whenever @xmath8 is large the assumed model is far from the true distributions .",
    "on the other hand , if @xmath8 is small and less than a pre - specified error limit ( say , @xmath9 ) one can deduce any inference based on the assumed model within the allowed error margin .",
    "thus , the quantity @xmath10 indeed measures the  distance \" of the assumed model from the true distribution allowing the evaluation of the model downplaying the role of the sample size .",
    "note that this @xmath8 can be re - written as @xcite @xmath11,\\ ] ] where @xmath12 is the cell index and the quantity within the third bracket is in fact a statistical distance ( or , divergence ) between the model element @xmath13 and the true probability @xmath1 .",
    "then the model is said to be  adequate at level @xmath14 \" if @xmath15 and the test for this hypothesis is known as the  model adequacy test \" .",
    "more generally , let us use the term  density \" for both the discrete or continuous probabilities and consider the class @xmath16 of all density functions which are absolute continuous with respect to some common dominating measure @xmath17 .",
    "in case of continuous variables this dominating measure is the lebeague measure while in case of discrete variables it is a suitable counting measure .",
    "define a general statistical divergence @xmath18 between two density functions @xmath1 and @xmath13 as a non - negative function from @xmath19 to @xmath20 which equals zero if and only if @xmath21 identically .",
    "for such a general statistical divergence , we can say that the assumed model @xmath22 is adequate at level @xmath14 with respect to the true probability @xmath1 if the distance of @xmath1 from the model family @xmath7 defined as @xmath23 is less than or equal to @xmath14 .",
    "so our general model adequacy test considers the null hypothesis @xmath24 note that , this null hypothesis indeed says that the true probability @xmath1 is within a tolerance region @xmath25 about @xmath7 ( also called the  model tube \" ) defined as @xmath26 further the model tube @xmath25 is an union of open balls of radius @xmath14 , the pre - specified tolerance limit , around each model elements and hence the null hypothesis in ( [ eq : mat ] ) , alternatively expressed as @xmath27 , indicates that the true probability @xmath1 belongs to the radius-@xmath14 open ball around at least one model element .",
    "note that , in the cases where the null hypothesis is true , then @xmath1 may belong to the radius-@xmath14 open ball around more than one model element all of which can be adopted given the tolerance limit @xmath14 .    given @xmath14",
    ", one can construct suitable test statistics for solving the problem of model adequacy .",
    "@xcite , with their particular choice of @xmath28 , considered the likelihood ratio test for the corresponding model adequacy hypothesis .",
    "however , the distance function involved in @xmath8 is not everywhere differentiable which leads to non - standard asymptotics @xcite as well as severe computational difficulties @xcite . other works in the related context of tolerance region based goodness - of - fit were done by @xcite , @xcite and @xcite ; these works were mostly based on euclidean distances and not quite general for all parametric models .",
    "further , a challenging task in model adequacy test is the selection of the tolerance limit @xmath14 , which was also not quite easy for these approaches . to resolve these issues",
    ", @xcite proposed to construct the tolerance region in the null hypothesis ( [ eq : mat ] ) by the simpler , easily interpretable and everywhere differentiable kullback - leibler divergence defined as @xmath29 note that the kld is adjoint to the popular likelihood divergence ( ld ) given by @xmath30 @xcite went on to construct the likelihood ratio test statistic ( based on the ld ) for the model adequacy test with @xmath31kld and illustrated several nice properties of this approach .",
    "specifically , the particular geometric structure of the kld in connection with the likelihood ratio yields a nice algorithm to choose the tolerance limit @xmath14 with interesting applications .",
    "the above discussion invokes a natural question : can we obtain some larger superfamily of statistical divergences , that could be further helpful in robust statistical inference like the pd or the gkl families , through more general model adequacy tests ? in this paper , we try to answer this interesting question by exploring some model adequacy tests based on more general divergences ( rather than kld and ld only ) .",
    "we first consider a recent generalization of the pd family , namely the two - parameter @xmath0-divergence family of @xcite , and illustrate that it can also be obtained from a suitable model adequacy test based on appropriate generalizations of the ld and kld in section [ sec : inerpret_sd ] .",
    "these generalizations of the ld and the kld are indeed seen to be the special cases of the @xmath0-divergence family .",
    "so , we further generalize our model adequacy test by replacing kld and ld by appropriate members of the general @xmath0-divergence family and explore its properties in section [ sec : mat_sd ] .",
    "these require some non - trivial generalization of the work of @xcite in order to provide a general class of divergence based model adequacy tests which increases the novelty of this work .",
    "further , as expected , this general @xmath0-divergence based model adequacy tests again generate another _ new _ and larger super - family of divergences which , interestingly , contains both the @xmath0-divergence family and the gkl family as its subclasses .",
    "this is another novel discovery within the literature of the density based divergences and the related robust statistical inference .",
    "we refer to this larger three parameter superfamily of divergences as the generalized @xmath0-divergence ( gsd ) family and explore some of its basic properties in section [ sec : gsd ] .",
    "the potential applications of this gsd superfamily in robust statistical inference have been investigated in section [ sec : gsd_inf ] along with the asymptotic and robustness properties of the resulting estimators .",
    "section [ sec : illustratios ] presents some numerical illustrations of the performance of gsd based inference and the paper ends with some concluding remarks in section [ sec : conclusion ] .    before concluding this section , we summarize , in an itemized manner , the major contributions of this paper .",
    "* we explore the process of generating new divergence families from suitable model adequacy tests .",
    "this technique is not entirely unknown in the literature ; however it certainly has not received adequate attention and its application has been limited to the one parameter gkl and pd families .",
    "* we demonstrate that the two parameter family of @xmath0-divergences , which has been recently proposed and studied in the literature , can be generated through appropriate model adequacy tests in the same spirit . *",
    "we demonstrate that a further application of this technique leads to the generation of an entirely new family of divergences which is a super - class containing the @xmath0-divergence family as a special case . *",
    "an exploration of the properties of the divergences within this super - class demonstrates that the divergences within this super - class which appear to provide the best combinations of robustness and efficiency do not belong to any of the previously studied subclasses of this super - class .",
    "this indicates that studying the properties of the individual subclasses may not be sufficient to identify the ",
    "best \" divergence in respect of robust parametric estimation , so that this super - class does make some positive value addition to the literature . * a study of the minimum divergence estimators generated by the above superclass reveals the limitation of the first order influence function analysis in assessing their robustness .",
    "the @xmath0-divergence family of density - based divergences , recently developed by @xcite , is a two - parameter @xmath47 generalization of the pd family that connects each member of the pd family ( having parameter @xmath48 ) at @xmath49 to the @xmath50 divergence at @xmath51 .",
    "this family is defined as @xmath52d\\mu ,   \\label{eq : s_div_gen}\\ ] ] where @xmath53 and @xmath54 . clearly , @xmath55 .",
    "also the above form is defined only when @xmath56 and @xmath57 ; for @xmath58 or @xmath59 the corresponding @xmath0-divergence measure is defined by the continuous limit of ( [ eq : s_div_gen ] ) as @xmath60 or @xmath61 respectively .",
    "further , at the choice @xmath62 , this @xmath0-divergence family contains another popular divergence family , namely the density power divergence ( dpd ) family of @xcite having parameter @xmath63 , defined as @xmath64    note that the @xmath0-divergences measures are not symmetric in general .",
    "but it becomes symmetric if and only if either @xmath65 ( which generates the @xmath50 divergence ) , or @xmath66 . the latter case represents an interesting subclass referred to as the @xmath0-hellinger distances ( shd ) in @xcite and is defined by @xmath67 it connects the hellinger distance at @xmath68 to @xmath50-distance at @xmath51 .",
    "note that just as the hellinger distance represents the self adjoint member of the pd family , any other cross section of the class of @xmath0-divergences for a fixed value @xmath69 has a self adjoint member in @xmath70 .",
    "the applications of @xmath0-divergences in robust parametric inferences have been described in @xcite , @xcite and @xcite .",
    "it has been illustrated that some members of the @xmath0-divergence family generate more robust inference compared to its existing members within the pd and dpd subfamilies . in this paper , we demonstrate that one can obtain the @xmath0-divergence family from the suitable model adequacy test following the arguments of the previous section .",
    "since the @xmath0-divergence family is an extension of the pd family , we should be able to obtain it as a solution of suitable generalization of the testing problem ( [ eq : mat_pd ] ) .",
    "so , it is intuitive to construct the required model adequacy test based on similar extensions of the kld and ld divergences through the parameter @xmath69 .",
    "we consider the particular subfamily within the @xmath0-divergence family having this property ( corresponding to @xmath58 and @xmath59 respectively ) which are given by @xmath71 and @xmath72 for all @xmath73 .",
    "in particular , at the choice @xmath68 , the above families simplify , respectively , to the kld and the ld measures .",
    "further , @xcite have noted a special geometric structure between the kld and the ld which cancels the curvatures of each other to generate the new family of divergence measures .",
    "it is the property that any one of these two divergences can be obtained from the other just by interchanging @xmath1 and @xmath13 , i.e. , in the language of @xcite , the ld and kld are adjoint to each other ; they are also symmetrically opposite to each other within the pd family , with the hellinger distance ( hd ) measure being the point of symmetry .",
    "this special geometry also holds for our sld and skl families at any given @xmath69 , i.e. , @xmath74 further , they are also symmetrically opposite to the only self - adjoint subfamily , namely the shd family , within the @xmath0-divergence family of divergences ; this fact can be easily verified from the plots of the lines @xmath58 and @xmath59 in the @xmath75 plane , as shown in figure [ fig : ab=0 ] , which are clearly symmetric around the line @xmath76 for each @xmath77 .",
    "the line @xmath51 is the asymptote of both these lines as @xmath78 @xmath79 respectively which justifies the fact that @xmath0-divergences with @xmath51 are independent of the choice of @xmath48 and are always self - adjoint too .",
    "these geometrical properties of the skl and sld families intuitively indicate that the use of the members of the skl and sld families at any particular @xmath77 in constructing the model adequacy test would possibly generate the corresponding cross - section of the @xmath0-divergence family with the same @xmath69 .",
    "we prove this intuition rigorously in the rest of this section .    ) and the sld family ( @xmath59 ) on the @xmath75-plane of the @xmath0-divergence family , relative to the self - adjoint members ( dashed line , at @xmath80-@xmath81),scaledwidth=50.0% ]    take any fixed @xmath77 and a tolerance limit @xmath14 .",
    "we want to test whether our model is adequate in terms of the @xmath82 divergence at level @xmath14 .",
    "this is equivalent to testing for the null hypothesis @xmath83 let us construct a divergence based test ( rather than the lrt ) for this null hypothesis using the @xmath84 with the same tuning parameter @xmath69 .",
    "these divergence based tests are quite robust compared to the lrt as explored in @xcite and @xcite .",
    "then , based on a sample of size @xmath85 , the resulting test statistic would be @xmath86 , where @xmath87 is the estimated density under the null hypothesis obtained by minimizing the skl divergence between the true density @xmath1 and the model @xmath13 over all possible model densities satisfying the null hypothesis .",
    "thus , @xmath87 is nothing but the solution of the constrained optimization problem @xmath88 as in @xcite , we transfer this constrained optimization problem to an equivalent simpler unconstrained optimization problem .",
    "for simplicity , let us first assume that the model family contains only one element , i.e. , @xmath89 .",
    "then , the constrained problem ( [ eq : constrained1_sd ] ) simplifies to @xmath90    the constrained optimization problem ( [ eq : constrained2_sd ] ) has a unique solution on the boundary of @xmath25 having the form @xmath91 for some @xmath42 $ ] , a function of given @xmath14 satisfying @xmath92 .",
    "[ thm : solution_sd_mat ]    * proof : * the proof follows easily by applying the lagrange multiplier method .",
    "@xmath93 .",
    "further , starting from a fixed @xmath42 $ ] we can see that the much simpler unconstrained problem @xmath94 , ~~\\bar{\\tau } = 1- \\tau , \\label{eq : unconstrained_sd}\\ ] ] has the solution @xmath95 of the same form as ( [ eq : sol_form_sd ] ) . as argued in @xcite , there is a direct one to one correspondence between the parameters @xmath96 and @xmath14 in the two optimization problems in ( [ eq : unconstrained_sd ] ) and ( [ eq : constrained2_sd ] ) respectively through the relation @xmath97 .",
    "thus our targeted quantity @xmath87 in our model adequacy test statistic @xmath86 can be easily obtained as the solution to the simpler unconstrained optimization problem in ( [ eq : unconstrained_sd ] ) with a suitable @xmath96-value satisfying @xmath97 .    now",
    ", suppose the model family @xmath7 contains more than one element indexed by the parameter @xmath32 as @xmath98 .",
    "then , we should extend the unconstrained problem in ( [ eq : unconstrained_sd ] ) to the optimization problem @xmath99 \\equiv \\min_\\theta\\min_{p } \\left[\\tau { \\rm skl}_\\alpha(g , p ) + \\bar{\\tau } { \\rm sld}_\\alpha(p , f_\\theta)\\right ] .",
    "\\label{eq : unconstrained2_sd}\\ ] ] again we can show that the required solution to the constrained optimization problem in ( [ eq : constrained1_sd ] ) is nothing but the solution to this unconstrained problem in ( [ eq : unconstrained2_sd ] ) with some appropriate value of @xmath96 satisfying @xmath100 .",
    "further , the inner minimization in ( [ eq : unconstrained2_sd ] ) is of the form ( [ eq : unconstrained_sd ] ) and has the explicit solution @xmath101 .",
    "substituting this solution , the final optimization problem in ( [ eq : unconstrained2_sd ] ) becomes @xmath102 .",
    "\\label{eq : unconstrained3_sd}\\ ] ] however , a routine algebra shows that @xmath103\\nonumber\\\\ & = & \\left[\\tau { \\rm skl}_\\alpha(g , g^\\tau f_\\theta^{\\bar{\\tau } } ) + \\bar{\\tau } { \\rm sld}_\\alpha(g^\\tau f_\\theta^{\\bar{\\tau } } , f_\\theta)\\right]\\nonumber\\\\ & = & \\tau \\bar{\\tau}s_{(\\alpha , \\lambda_\\tau)}(g , f_\\theta),\\end{aligned}\\ ] ] where @xmath104 and @xmath105 is a function of @xmath96 .",
    "therefore , our required solution @xmath95 is nothing but the model element @xmath106 with @xmath107 being the minimum @xmath0-divergence estimator defined as @xmath108 thus , our model adequacy test based on skl@xmath109 independently yields the form of the @xmath0-divergence measure with appropriate parameter and also directly depends on the corresponding minimum @xmath0-divergence estimator which has nice robustness properties with respect to the outliers in the sample data @xcite .",
    "further , noting that , given @xmath110 , @xmath111 is a one - to - one function of @xmath96 , and any @xmath0-divergence measure with parameter @xmath112 and @xmath113 $ ] can be obtained from the model adequacy based on the skl with the same @xmath69 and the appropriate value of @xmath14 obtained from @xmath114 .",
    "therefore the @xmath0-divergence family has a direct one to one correspondence with the model adequacy test based on skl and sld .",
    "the nice and interesting interpretation of the @xmath0-divergence measures as discussed in the previous section invokes a natural question .",
    "what if we start with a more general divergence family instead of the sld and skl ?",
    "will we get an even larger superfamily of divergences in that case ? in order to answer these questions , we now develop a general model adequacy test based on suitable members of the @xmath0-divergence family .",
    "noting the special geometric curvatures of the skl and sld families and the fact that the whole @xmath0-divergence family is symmetric about @xmath115 , we may try to replace the skl and sld by suitable members of the general @xmath0-divergence family .",
    "more precisely , for any @xmath112 , we want to consider two @xmath0-divergence measures with parameters @xmath116 and @xmath117 in place of sld@xmath109 and skl@xmath109 in such a way that their geometric curvature cancel each other just like skl and sld . that is , these two members should be symmetrically opposite with respect to the line @xmath76 and satisfy @xmath118 from the derivations of the previous section , we may take @xmath119 for some @xmath120 and then the above requirements give us the choice @xmath121 .",
    "so , we consider the null hypothesis @xmath122 and construct the model adequacy test based on @xmath123 .",
    "note that the choice @xmath124 yields the model adequacy test of the previous section based on skl and sld .",
    "based on a sample of size @xmath85 , we take an estimate @xmath1 of the true density and define the test statistics for the model adequacy test of the hypothesis ( [ eq : mat_gen ] ) as given by @xmath125 where @xmath87 is the best model element ( with respect to the @xmath126 measure ) in the model tube @xmath25 and is defined as @xmath127 thus , the @xmath87 can be obtained as the solution of the constrained optimization problem @xmath128    proceeding as in the previous section , we can again show that the unique solution to the above constrained optimization problem with general model family @xmath129 is nothing but the unique solution @xmath95 of the unconstrained optimization problem @xmath130 \\equiv \\min_\\theta\\min_{p}\\left[\\tau s_{(\\alpha , \\frac{\\gamma}{1-\\alpha})}(g , p ) + \\bar{\\tau}s_{(\\alpha , \\frac{\\alpha-1-\\gamma}{1-\\alpha})}(p , f_\\theta)\\right ] , \\label{eq : unconstrained_gen}\\ ] ] for some @xmath131 $ ] that depends on the tolerance limit @xmath14 through the relation @xmath132 to see this , we again consider the one element model family as @xmath89 .",
    "then , the following theorem presents the solution of the simplified version of the constrained problem ( [ eq : constrained1_gen ] ) given by @xmath133 the proof of the theorem again follows from the method of lagrange multiplier and is hence omitted .",
    "however , we can easily check that the solution @xmath95 as given in the above theorem is also the unique solution to the much simpler unconstrained optimization problem @xmath136 , \\label{eq : unconstrained1_gen}\\ ] ] for the @xmath96 value satisfying @xmath135 .",
    "thus , our general model adequacy test based on the @xmath0-divergences yields a divergence function through the quantity @xmath140 .",
    "this function @xmath140 is can be easily extended to the cases @xmath144 and @xmath145 through continuous limits of the above form and define a three - parameter family of proper statistical divergence measures .",
    "further the choice @xmath124 reduces the general divergence measure @xmath140 to the suitable @xmath0-divergence measure with parameters @xmath69 and @xmath111 ( defined in ( [ eq : lambda_tau ] ) ) .",
    "so , we refer to this general three parameter family as the  generalized super - divergence ( gsd ) family \" and discuss its properties in detail in the next section",
    ".    note that given the generated sample data , we can easily compute the model adequacy test statistics by using the minimum gsd estimators ( mgsdes ) of the model parameter @xmath32 . and the performance and robustness of the proposed model adequacy test directly depend on that of the minimum gsd estimators . in the rest of this paper",
    ", we primarily illustrate the properties of the gsd and the corresponding estimators in detail and leave the detailed analysis of this general model adequacy test resulting from the gsd and the mgsde for our future research work .",
    "the generalized @xmath0-divergence ( gsd ) family , as derived in the previous section , is a three parameter family of statistical divergences @xmath140 defined in equation ( [ eq : def0_gsd ] ) for @xmath112 , @xmath146 and @xmath147 with @xmath148 .",
    "we can easily extend this family to be defined over the parameter values @xmath149 $ ] , @xmath42 $ ] and @xmath150 through their continuous limits in the form given by equation ( [ eq : def0_gsd ] ) . in table",
    "[ tab : spcl ] , we have listed several divergences or families of divergences which belong to the gsd family .",
    "many ( although not all ) of the divergences ( or , classes of divergences ) listed in table [ tab : spcl ] are obtained as limiting members of the gsd family .",
    ".existing divergences as special cases of the gsd family [ cols= \" < , < \" , ]     [ tab : spcl ]    thus , the generalized @xmath0-divergence family contains the existing divergence families like the @xmath0-divergence family ( and hence the pd and dpd families ) and the gkl family of @xcite as its limiting special cases and yields many more interesting new divergence measures .",
    "however , one interesting limiting subfamily of this gsd family arises at the choice @xmath145 which gives @xmath151 this family is an interesting generalization of the ordinary gkl family over the parameter @xmath149 $ ] defined in equation ( [ eq : gkl ] ) and simplifies to the gkl family at @xmath68 .",
    "first , we show that all the members of the gsd family represent proper statistical divergences . to see this",
    ", we can rewrite the gsd family ( [ eq : def0_gsd ] ) in terms of the pearson residual @xmath152 as @xmath153 where @xmath154 , ~~\\delta\\geq -1.\\ ] ] this function @xmath155 is strictly convex in on @xmath156 and satisfies the relations @xmath157 here @xmath158 and @xmath159 denote the first and second order derivatives respectively with respect to the argument . using these properties , we get the following theorem .    all the functions @xmath140 in the generalized @xmath0-divergence family for @xmath160 $ ] and @xmath150 define proper statistical divergence in the sense that , for any two densities @xmath161 , @xmath162 [ thm : gsd_pro1 ]    examining the forms of the generalized @xmath0-divergence measures , we can obtain several interesting properties of this superfamily , which are listed in the following theorem .",
    "for any two densities @xmath163 , the generalized @xmath0-divergence measure @xmath140 satisfies the following properties :    1 .   for any given @xmath69 and @xmath164 , the gsd is adjoint with respect to @xmath165 , i.e. , @xmath166 2 .   at @xmath165 ,",
    "the gsd measure is self - adjoint and symmetric in its arguments yielding an interesting subfamily of gsd given by @xmath167.\\ ] ] 3 .",
    "the gsd measure also become symmetric and self - adjoint at @xmath168 , where it becomes independent of the parameter @xmath96 and coincides with the shd family .",
    "[ thm : gsd_pro2 ]    from the above properties and the limiting forms of the gsd measures , we can clearly observe that all the members of the gsd family are not distinct ; they become identically equal at two or more choice of the parameter combination @xmath169 . for example , @xmath170 therefore , it would be an interesting future challenge for a mathematician to find the underlying geometry and functional topological properties of the gsd family and characterize all the distinct members within this large superfamily .",
    "for the present paper , we try to illustrate this fact through a suitable numerical example in section [ sec : illustratios ] .",
    "let us consider the parametric model family of densities @xmath171 .",
    "let @xmath172 denote the distribution function corresponding to @xmath35 .",
    "we are interested in the estimation of the parameter @xmath32 .",
    "let @xmath173 denote the distribution function corresponding to the true density @xmath1 .",
    "the minimum gsd functional @xmath174 at @xmath173 is then defined by the relation @xmath175 provided the minimum exists . from its definition ,",
    "the gsd functional @xmath174 is fisher consistent under the assumption that the model is identifiable .",
    "when @xmath173 is outside the model , @xmath176 represents the best fitting parameter , and @xmath177 is the model element closest to @xmath1 in the gsd sense . for simplicity , we suppress the subscript @xmath178 for @xmath179 .    given the observed sample data , we estimate the parameter @xmath32 by minimizing the divergence @xmath180 over @xmath181 , where @xmath34 is some non - parametric estimate of the true density @xmath1 based on the observed sample . when the model is discrete , a simple choice for @xmath34 is given by the relative frequencies ; for continuous models there is no such simple choice and we need some nonparametric density estimator .",
    "thus , the estimating equation for the minimum gsd estimator ( mgsde ) is given by @xmath182 where @xmath183 and @xmath184 .",
    "\\label{eq : k_delta}\\ ] ] note that , the function @xmath185 satisfies the relations @xmath186 and @xmath187 .",
    "therefore , this estimating equation of the mgsde is an _ unbiased estimating equation _ at the model @xmath188 .",
    "further , at any fixed @xmath69 , the estimating equation of the mgsdes differ only in terms of the function @xmath189 , just like the case of minimum @xmath0-divergence estimators ( msdes ) .",
    "hence , the robustness properties of the resulting estimators are expected to depend at least partially on the form of this @xmath189 function . however , all the divergences within the gsd family produce affine invariant estimators as described in the following proposition .",
    "consider the affine transformation @xmath190 for a non - singular matrix @xmath191 and vector @xmath192 of the dimension as of @xmath193 .",
    "then we have @xmath194 where @xmath195 .",
    "therefore the divergence measure @xmath140 is not itself affine invariant , but the corresponding minimum divergence estimator is affine equivariant .    *",
    "proof : * note that , under the above transformation , the jacobian formula gives us the relation between the densities of @xmath193 and @xmath196 as @xmath197 then the proposition follows from this relation and the form of the @xmath0-divergence .",
    "@xmath198      the influence function is a popular and useful indicator of the first - order robustness and efficiency of any estimator .",
    "influence function in fact indicates the asymptotic effect of the infinitesimal contamination on the properties of the estimator in the neighborhood of the true distribution @xcite .",
    "more precisely , the influence function of any statistical functional @xmath199 at the true distribution @xmath173 is defined as @xmath200 where @xmath201 is the contaminated distribution @xmath202 , @xmath203 is the contamination proportion and @xmath204 denote the degenerate distribution at the contamination point @xmath205 .",
    "the influence function of a robust estimator should be bounded ; its non - boundedness indicates that the first order asymptotic bias of the estimator may diverge to infinity under contamination .",
    "we will now examine the first order robustness of the proposed minimum gsd estimator by deriving its influence function .",
    "let us consider the minimum gsd functional @xmath206 as defined in ( [ eq : msde_functional ] ) , which satisfies the estimating equation ( [ eq : s - divergence_est_equation ] ) with @xmath34 replaced by @xmath1 .",
    "then , a straightforward differentiation of the estimating equation yields the required influence function as presented in the following theorem .",
    "[ thm : msde_if ] consider the general set - up of the minimum gsd estimator as mentioned in the previous subsection . then",
    ", the influence function of the minimum gsd functional @xmath207 is given by @xmath208\\ ] ] where @xmath209 , @xmath210 and @xmath211 with @xmath212 and @xmath213 denoting the gradient with respect to @xmath32 .",
    "[ cor : msde_if0 ] under the set - up of theorem [ thm : msde_if ] , suppose the true density @xmath1 belongs to the model family @xmath7 with @xmath214",
    ". then the influence function of the minimum gsd functional has the simpler form @xmath215 .",
    "\\label{eq : s_div_if_model}\\end{aligned}\\ ] ]    the most interesting and remarkable observation here is that the influence function of the mgsde at the model is independent of the parameters @xmath164 and @xmath96 and depends only on the parameter @xmath69 .",
    "hence the influence function analysis predicts similar behavior ( in terms of first order robustness and efficiency ) for all minimum gsd estimators with the same value of the parameter @xmath69 irrespective of the other two parameters @xmath164 and @xmath96 . also note that",
    ", this influence function is the same as that of the minimum dpd estimators and also the minimum @xmath0-divergence estimators with the same values of @xmath69 .",
    "thus the influence functions of the mgsdes have bounded re - descending natures except in the case @xmath49 where it is unbounded .",
    "figure [ fig : if_s_div ] shows the nature of the influence functions for the poisson - mean ( discrete case ) and normal - mean ( continuous case ) .",
    "therefore , as per the first order influence function analysis , all the mgsdes are robust for all @xmath216 and non - robust at @xmath68 .     under the @xmath217 model at the @xmath218 ( first panel ) and the normal @xmath219 model at the @xmath220 ( second panel),title=\"fig:\",scaledwidth=45.0%,scaledwidth=35.0% ]   under the @xmath217 model at the @xmath218 ( first panel ) and the normal @xmath219 model at the @xmath220 ( second panel),title=\"fig:\",scaledwidth=45.0%,scaledwidth=35.0% ]    in actual practice , the picture given by the first order influence function analysis often leads to an inaccurate prediction of the actual performance of the mgsde .",
    "our simulation studies in section [ sec : illustratios ] will demonstrate that some members of the gsd family having bounded influence functions generate highly non - robust estimators ; these choices include small positive @xmath69 , @xmath221 and @xmath222 .",
    "on the other hand some members of the gsd family with @xmath68 , @xmath223 and @xmath224 ( large ) , in spite of having unbounded influence functions , generate highly robust estimators .",
    "hence , the classical first order influence function analysis can not portray the true robustness picture for the minimum gsd estimators .",
    "as explained above , it may fail in both counts ; it may label strongly robust estimators as unstable , and may declare highly unstable estimators as being strongly robust .",
    "an appropriate second order influence function analysis may provide a more accurate description of the distortion of the estimators due to contamination . for brevity we do not present the second order analysis in this paper ; however this analysis reaffirms and strengthens all the illustrations and conclusions of @xcite .",
    "let us now describe the consistency and asymptotic distribution of the proposed minimum gsd estimators . for simplicity , we will consider only the case of discrete models in this paper .",
    "suppose @xmath225 are @xmath85 independent and identically distributed observations from a discrete probability mass function ( pmf ) @xmath1 modeled by the parametric family @xmath226 and let the distributions be supported , without loss of generality , on @xmath227 .",
    "we assume that both @xmath1 and @xmath7 belong to the class @xmath16 , where the dominating measure @xmath17 is now the counting measure over the support @xmath228 .    under this set - up",
    ", we can easily get an estimate @xmath34 of the true pmf @xmath1 through the relative frequencies defined as @xmath229 , where @xmath230 denote the indicator function of the event @xmath231 .",
    "so , we can get the mgsde of @xmath32 by minimizing the gsd measure between two probability vectors @xmath232 and @xmath233 and hence the corresponding estimating equation is given by ( [ eq : s - divergence_est_equation ] ) with @xmath234 replaced by @xmath235 and the integral replaced by summation over @xmath228 : @xmath236 where now we have @xmath237 .",
    "now , in order to prove the asymptotic properties of the mgsde , we consider the matrix @xmath238 as defined in ( [ eq : j_g ] ) and define @xmath239 ,   \\label{eq : v_g}\\ ] ] where @xmath240 represents the variance under the true density @xmath1 .",
    "further , we also make the following assumptions :    1 .",
    "the model family @xmath241 is identifiable .",
    "2 .   the probability mass functions @xmath35 of the model have common support so that the set @xmath242 is independent of @xmath32 .",
    "the true pmf @xmath1 also supported on @xmath228 .",
    "3 .   there exists open subset @xmath243 for which the best fitting parameter @xmath244 is an interior point and for almost all @xmath245 , the pmf @xmath246 admits all third derivatives of the type @xmath247 for all @xmath248 .",
    "the matrix @xmath238 , defined in equation ( [ eq : j_g ] ) , is positive definite .",
    "the quantities @xmath249 and + @xmath250 are bounded for all @xmath251 and for all @xmath248 .",
    "6 .   for almost all @xmath245",
    ", there exists functions @xmath252 , @xmath253 , @xmath254 that dominate , in absolute value , @xmath255 for all @xmath256 and which are uniformly bounded in expectation with respect to @xmath1 and @xmath257 for all @xmath248 .",
    "the functions @xmath258 and @xmath259 with @xmath260 are uniformly bounded for all @xmath248 .",
    "then we have the following theorem stating the consistency and asymptotic normality of the mgsde under the discrete models . for brevity in presentation",
    ", its proof has been moved to the appendix .",
    "[ thm : gs - divergence_asymptotic ] under the set - up of discrete models as mentioned above and assumptions ( a1)(a7 ) , the following results hold :    1 .",
    "there exists a consistent sequence @xmath261 of roots to the minimum gsd estimating equation ( [ eq : discrete_est_equation ] ) .",
    "the asymptotic distribution of @xmath262 is @xmath263-dimensional normal with vector mean @xmath264 and covariance matrix @xmath265 .    under the assumption of theorem [ thm : gs - divergence_asymptotic ] ,",
    "let us further assume that the true pmf @xmath1 belongs to the model family @xmath7 with @xmath188 .",
    "then , @xmath266 has a simpler asymptotic distribution given by @xmath267 , where @xmath268 , @xmath269 and @xmath270 .",
    "interestingly , the asymptotic distribution of the mgsde at the model is independent of the parameters @xmath164 and @xmath96 .",
    "this is also expected from the fact that first order influence functions of these estimators do not depend on @xmath164 and @xmath96 .",
    "hence , all the mgsde with the same @xmath69 ( but with different @xmath164 and @xmath96 ) have the same asymptotic efficiency at the assumed model and this efficiency is also quite high for all most common parametric models for small positive values of @xmath69 .",
    "this can be seen by noting the fact that the asymptotic variance and hence the efficiency at the model is exactly same as that of the msde or the minimum dpd estimators with the same value of @xmath69 and their high efficiencies have already been illustrated by ( * ? ? ?",
    "* table 1 ) and ( * ? ? ?",
    "* table 9.1 ) respectively .",
    "in this section , we will present a simulation study to describe the finite sample performances of the proposed mgsdes for different choice of the parameters @xmath69 , @xmath164 and @xmath96 .",
    "since the minimum gsd estimation under continuous model requires complicated kernel smoothing techniques , in this paper we will consider the discrete model only so that the minimization of the gsd measure can be done simply by using the relative frequencies .",
    "let us consider the poisson distribution with the true parameter value @xmath271 .",
    "we generate samples of size @xmath272 from the @xmath218 distribution and compute the minimum gsd estimators of @xmath32 for different values of the tuning parameters @xmath69 , @xmath164 and @xmath96 . repeating the process 1000 times , we compute the empirical bias and mse of the mgsdes of @xmath32 as given by @xmath273 where @xmath274 denotes the mgsde obtained at the @xmath275-th replication . these empirical biases and the mses are reported in table [ tab : sim0 ] for some particular values of tuning parameters .",
    "these show the performance of the proposed mgsdes under pure data where there are no outlier issues .    with any sample of finite size",
    ", the poisson model inevitably leads to zero frequencies for infinitely many cells .",
    "some of the divergences within our gsd family have positive powers of the density @xmath1 in the denominator and consequently are not defined when there are one or more empty cells .",
    "this includes , for example , all cressie - read divergences with @xmath276 .",
    "the empty cell problem is a specific and extreme case of the general  inlier problem \" , where the observed frequencies are substantially lower than the expected frequencies .",
    "the gsd with @xmath68 , @xmath124 and @xmath222 is a case in question , where the divergence is not defined for empty cells . in order to keep a clear focus in the present context",
    ", we defer the consideration of the inlier issue in relation to the gsd to be taken up on a future occasion .",
    "next we examine the robustness of our proposal with respect to contamination/ outliers in the observed data .",
    "so , we repeat the above simulation study but after randomly contaminating 10% of each sample by observations from a @xmath277 distribution ; the latter distribution is well separated from the true data generating distribution , which is poisson(5 ) .",
    "the corresponding empirical bias and mse values obtained are reported in table [ tab : sim10 ] . if these bias and mse under the contaminated scenario turn out to be similar to that obtained in case of pure data that will indicate the stability / robustness of the corresponding estimators against the insertion of outlying observations .",
    "the major findings from these simulation results given in tables [ tab : sim0][tab : sim10 ] about the robustness of our proposed mgsdes along with their pure data performances have been listed below .    1 .   under pure data ,",
    "the absolute bias is minimum at the maximum likelihoods estimator ( mle , corresponding to @xmath278 ) as expected .",
    "however most of the other members of the gsd family also generate competitive values of the empirical biases",
    "the mse under pure data is also minimum at mle and it increases as @xmath69 increases for any fixed @xmath48 and @xmath96 .",
    "this fact is also expected from the ( theoretical ) asymptotic variance of the mgsdes which increases with @xmath69 at the model .",
    "however , the mses of most of the other mgsdes are also quite competitive and hence the loss in efficiency under pure data is not a very serious concern while using those mgsdes .",
    "3 .   under the contaminated scenario ,",
    "the bias and mse become quite high for the mle with respect to the pure data case . but several other members of the gsd family generate stable estimators that do not depart significantly away from their values in pure data .",
    "the stable mgsdes generally correspond to the larger choice of @xmath69 and @xmath96 .",
    "in particular , both the bias and mse generally show a decreasing pattern as we increase the value of the tuning parameters @xmath69 or @xmath96 . thus , in terms of stability our proposed estimators appear to do better as @xmath69 ( or @xmath96 ) increases when the other two parameters are held constant .",
    "the effect of the third tuning parameter @xmath164 on the robustness of the mgsdes is also quite interesting . for larger values of @xmath96 and @xmath69",
    ", there does not have any significant effect of @xmath164 on the robustness of the estimator .",
    "but for small values of @xmath69 or @xmath96 , @xmath164 affects the robustness of the corresponding mgsdes significantly  the mgsdes with negative values of @xmath164 still generate stable bias and mses under contamination indicating a strong degree of robustness , but those corresponding to positive @xmath164 become even more unstable than the mle .",
    "[ tab : sim0 ]    therefore , combining the above findings , we can see that the proposed minimum gsd estimators are highly robust under data contaminations and yield only a small loss in efficiency under pure data .",
    "this makes our proposal very useful in real practice with contaminated data , generating highly robust inference .",
    "further note that , under the contamination scenario considered here , the best estimators within the mgsde family in terms of both bias and mse correspond to the generalized @xmath0-divergence measure with @xmath279 , @xmath280 and @xmath281 .",
    "roughly , estimators within these choices of tuning parameters appear to provide the best compromise between efficiency at the model and robustness under data contamination .",
    "interestingly , these gsd measures do not appear to belong to any of the existing divergence measures like pd , gkl and @xmath0-divergences ; in fact they are far separated from the existing ones .",
    "hence the development of this larger gsd family does not limited only to a theoretical generalization in an academic interest .",
    "rather , they produce new minimum gsd estimators which generates more robust inference compared to the other existing minimum divergence estimators",
    ".    another interesting findings of this simulation exercise is that the robustness of the proposed mgsdes depends directly on all the three tuning parameters @xmath69 , @xmath164 and @xmath96 .",
    "however , the classical ( first order ) influence functions of the mgsdes predict robustness behavior to be independent of the tuning parameters @xmath96 and @xmath164 . this clearly indicates the limitation of the ( first order ) influence function analysis in assessing the robustness of the minimum gsd estimators .",
    "[ tab : sim10 ]",
    "in this paper , we have discussed the divergence based model adequacy tests and its link with some new divergence families . we have shown that the recent general family of the @xmath0-divergence can also be obtained from suitable model adequacy tests .",
    "we have also considered a more general model adequacy test based on the @xmath0-divergence measures which further generates a much larger superfamily of divergences that contains the @xmath0-divergence family and also the well - know generalized kullback - leibler divergence family as its special case , among others .",
    "we have also discussed several interesting properties of this new divergence family that we call the generalized @xmath0-divergence family ( gsd ) .",
    "we have indicated also the potential application of the gsd family in robust parametric inference ; derived the asymptotic properties of the minimum gsd estimators and illustrated their robustness through influence function analysis and suitable numerical illustrations . in this pursuit , we have observed and demonstrated the limitations of the first order influence function in assessing the robustness of these minimum divergence estimators .",
    "the research of this paper in fact opens up a lots of interesting questions for future researchers , both mathematicians and statisticians .",
    "the new generalized @xmath0-divergence family is seen to have several interesting properties with some identical members within this family .",
    "so , it would be an interesting future problem to investigate the topological properties of the gsd measures to characterize the distinct members of this divergence family .",
    "further , the application of the minimum gsd estimators in several statistical inference problems other than parametric estimation will also provide a great value addition to the literature of the robust inference .",
    "however , the most interesting future work in this case would be to generalize the connection between the model adequacy test and resulting divergence family .",
    "we have seen that starting from a model adequacy test based on one particular divergence one gets a one - parameter family of divergences ( pd family or the gkl family ) ; the model adequacy test based on a one parameter divergence family ( skl or sld ) generates a two - parameter divergence family ( @xmath0-divergence family ) and staring from the model adequacy test based on the two parameter @xmath0-divergence family we obtained a three - parameter divergence family . up to this point",
    "it seems meaningful since each time we generate a divergence which is better than the existing ones in terms of combining good efficiency and robustness properties .",
    "so , a natural question in this regard is how long we can extend this fact to generate even larger and larger superfamily of divergences .",
    "noting that all the members of the three parameter gsd family are not distinct , this process may eventually end after finite steps where we will get the largest such divergence family including all possible distinct divergences ; however this needs many more research to arrive at any such conclusions .",
    "we hope to pursue some of these extensions in our future endeavors .",
    "we will assume that the assumptions ( a1)(a7 ) hold and consider some useful lemmas first from @xcite .",
    "1 .   @xmath285 \\le n^{\\frac{k}{2}}e_g[|\\delta_n(x ) - \\delta_g(x)|]^k \\le               \\left [ \\frac{g(x)(1-g(x))}{f_{\\theta}^2(x)}\\right]^{\\frac{k}{2}}$ ] .",
    "@xmath286 \\le \\frac{2g(x)(1-g(x))}{f_{\\theta}(x)}$ ] .",
    "now , let us define the quantities @xmath289 and + @xmath290 , where @xmath291 and @xmath292 note that these two quantities @xmath293 and @xmath294 are different from those considered in @xcite in the context of msdes since the function @xmath189 involved here is completely different from that in case of @xmath0-divergences .",
    "so , the next two lemmas , which yield the asymptotic distributions of these quantities in analogue to lemma 3 and lemma 4 of @xcite , needs suitably adjusted proof as described below .      * proof :* by lemma 2.15 of @xcite ( or , * ? ? ? * lemma 25 ) , there exists some positive constant @xmath299 such that @xmath300 also , by lemma [ lem : lemma_3.1 ] , @xmath301 \\le \\beta \\frac{g^{1/2}(x)}{f_{\\theta}(x)}$ ] .",
    "+ and by lemma [ lem : lemma_3.2 ] , @xmath302 = \\beta e_g[\\eta_n(x ) ] \\rightarrow 0 $ ] as @xmath287 .",
    "thus we get , @xmath303 f_{\\theta}^{1+\\alpha}(x)|u_{\\theta}(x)| \\nonumber \\\\      & \\le &   \\beta \\sum_x g^{1/2}(x ) f_{\\theta}^{\\alpha}(x)|u_{\\theta}(x)|   %    \\nonumber \\\\ & < &     < \\infty ~~\\mbox { ( by assumption ( a5))}. \\nonumber      \\end{aligned}\\ ] ] so , by dominated convergence theorem ( dct ) , @xmath304 as @xmath287 .",
    "+ hence , by markov inequality , @xmath305 as @xmath287",
    ".      * proof :* note that , by the previous lemma [ lem : lemma_3.3 ] , the asymptotic distribution of @xmath293 and @xmath294 are the same .",
    "now , we have @xmath308\\right )       \\nonumber \\\\      & ~\\displaystyle\\mathop{\\rightarrow}^\\mathcal{d } & z \\sim n ( 0 , v_g ) ~~~ \\mbox{[by central limit theorem ( clt)]}. \\nonumber      ~~~~~~~~~~~~~~~~{\\square }       \\end{aligned}\\ ] ]    now , we proof our theorem [ thm : gs - divergence_asymptotic ] using these lemmas .",
    "+ * proof of consistency ( part(a ) ) : * consider the behavior of @xmath309 on a sphere @xmath310 which has radius @xmath311 and center at @xmath244 .",
    "we will show , for sufficiently small @xmath311 , the probability tends to one that @xmath312 so that the gsd measure has a local minimum with respect to @xmath32 in the interior of @xmath310 . at this local minimum",
    ", the estimating equation must be satisfied .",
    "hence , for any @xmath313 sufficiently small , the minimum gsd estimating equation has a solution @xmath261 within @xmath310 with probability tending to one as @xmath287 .    now taking taylor series expansion of @xmath309 about @xmath314 , we get @xmath315 where @xmath316 lies between @xmath244 and @xmath32 .",
    "now , let us consider each terms one - by - one .",
    "for the linear term @xmath317 , we consider @xmath318 where @xmath319 is @xmath320 evaluated at @xmath314 and @xmath189 is defined in ( [ eq : k_delta ] ) . here , we will prove that @xmath321 as @xmath287 and note that the right hand side of above is zero by definition of the minimum gsd estimator . clearly by assumption ( a7 ) and",
    "the fact that @xmath322 almost surely ( a.s . ) by strong law of large number ( slln ) , it follows that @xmath323 for any @xmath324 in between @xmath319 and @xmath325 , uniformly in @xmath245 .",
    "so , by using the one - term taylor series expansion , @xmath326 however , by lemma [ lem : lemma_3.1](1 ) , we have @xmath327 \\le \\frac{\\left[g(x)(1-g(x))\\right]^{1/2}}{f_{\\theta^g}(x ) \\sqrt n } \\rightarrow 0 ,      ~~~ ~~~ as ~ ~ n \\rightarrow \\infty.\\nonumber      \\end{aligned}\\ ] ] and , by lemma [ lem : lemma_3.1](2 ) along with assumption ( a5 ) , we get @xmath328   %    \\nonumber \\\\ & & ~~~~~",
    "\\leq 2c_1 \\sum_x g^{1/2}(x ) f_{\\theta^g}^{\\alpha}(x ) |u_{j\\theta^g}(x ) |   ~ < ~ \\infty .",
    "\\nonumber % \\\\ & & ~~~~~~~~~~~~~~~~~~~~~~\\mbox{[by assumption ( a5 ) ] } \\nonumber      \\end{aligned}\\ ] ] hence , by dominated convergence theorem ( dct ) , we get , @xmath329 \\rightarrow 0 , \\nonumber      \\end{aligned}\\ ] ] as @xmath287",
    ". then the markov inequality yields the desired result .",
    "thus , we get @xmath330 therefore , with probability tending to one , @xmath331 , where @xmath311 is the radius of @xmath310 and @xmath263 is the dimension of @xmath32 .",
    "next let us consider the quadratic term @xmath332 .",
    "clearly @xmath333 . \\nonumber        \\end{aligned}\\ ] ] first we will prove that @xmath334 to see this we get , from assumption ( a7 ) as in ( [ eq:39 ] ) , that @xmath335 for every @xmath324 lying in between @xmath319 and @xmath325 , uniformly in @xmath245 .",
    "so , by using the one - term taylor series expansion , @xmath336 thus , we get @xmath337 since by assumption ( a5 ) , @xmath338 , the desired claim in ( [ eq:45 ] ) follows by the similar argument that were used to prove the claim in ( [ eq:38 ] ) . in a similar fashion",
    ", we also get @xmath339 so , combining ( [ eq:45 ] ) , ( [ eq:47 ] ) and ( [ eq:48 ] ) , we have @xmath340 but we know that @xmath341 here the absolute value of the first term in the above expression ( [ eq:50 ] ) is @xmath342 with probability tending to one . and , the second term in ( [ eq:50 ] ) is a negative definite quadratic form in the variables @xmath343 . letting @xmath344",
    "be the largest eigenvalue of @xmath345 , the quadratic form is @xmath346 . combining them ,",
    "we get appropriate constants @xmath347 and @xmath348 such that for @xmath349 , we have @xmath350 with probability tending to one .",
    "finally , considering the cubic term @xmath351 , we note that @xmath352 or , @xmath353 where @xmath354 . here",
    "we will prove that all the terms in the rhs of the above expression are bounded .",
    "for the first term ( [ i ] ) , we use ( [ eq:46 ] ) to get @xmath355   < \\infty .   ~~~~",
    "\\mbox{[by clt and assumption ( a6)]}\\nonumber      \\end{aligned}\\ ] ] thus term ( [ i ] ) is bounded .",
    "now for the second term ( [ ii ] ) , we again use ( [ eq:39 ] ) to get @xmath356   < \\infty ,   ~~~~   \\mbox{[by clt and assumption ( a6)]}\\nonumber      \\end{aligned}\\ ] ] so that term ( [ ii ] ) is also bounded . similarly the terms ( [ iii ] ) , ( [ iv ] ) , ( [ v ] ) and ( [ viii ] ) are bounded as in case of term ( [ ii ] ) and using ( [ eq:39 ] ) and assumption ( a6 ) .",
    "next for the term ( [ vi ] ) , we will consider the following : @xmath357 so that @xmath358 also , @xmath359 so , @xmath360   < \\infty ,   ~   \\mbox{[by assumption ( a6)]}\\nonumber   %    \\\\ & & ~~~~~~~ \\mbox{[by assumption ( a6 ) ] } ,       \\end{aligned}\\ ] ] where @xmath361",
    ". thus the term ( [ vi ] ) is bounded and also similarly the rest of the terms ( [ vii ] ) , ( [ ix ] ) , ( [ x ] ) and ( [ xi ] ) are bounded .",
    "hence , we get @xmath362 on the sphere @xmath310 with probability tending to one . combining",
    "these three inequalities we have @xmath363 which is strictly negative for @xmath364 .",
    "thus , for any sufficiently small @xmath311 , there exists a sequence of roots @xmath365 to the minimum gsd estimating equation such that @xmath366 converges to one , where @xmath367 denotes the @xmath50-norm .",
    "it remains to show that we can determine such a sequence independent of @xmath311 . to see this ,",
    "we let @xmath368 to be the root closest to @xmath244 .",
    "this exists since the limit of a sequence of roots is again a root by the continuity of the gsd measure .",
    "this completes our proof of the consistency part . + * proof of the asymptotic normality ( part ( b ) ) : * to prove the asymptotic normality , we have to expand @xmath369 in taylor series about @xmath314 to get @xmath370 where @xmath371 lies in between @xmath32 and @xmath244 .",
    "suppose @xmath261 is the solution of the minimum gsd estimating equation that is consistent by part ( a ) of the theorem .",
    "replace @xmath32 by @xmath261 in the above equation ( [ eq:59 ] ) so that the lhs of the equation becomes zero and we get that      here , the first term within the bracketed quantity in the rhs of ( [ eq:60 ] ) converges to @xmath238 with probability tending to one and the second bracketed term is an @xmath373 term ( as shown in the proof of the consistency part ) .",
    "further lemma [ lem : lemma_3.4 ] gives us that @xmath374        f_{\\theta^g}^{1+\\alpha}(x)u_{\\theta^g}(x ) \\nonumber \\\\      & = &   s_{1n}|_{\\theta=\\theta^g } \\mathop{\\rightarrow}^\\mathcal{d } n_p(0 , v_g ) .",
    "\\nonumber      \\end{aligned}\\ ] ]                bregman , l.  m. ( 1967 ) .",
    "the relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming .",
    "_ 7 _ , 200217 .",
    "original article is in _ zh .",
    "vychisl .  mat .  mat .",
    "_ , * 7 * , pp .",
    "620631 , 1997 .",
    "ghosh , a. , harris , i. r. , maji , a. , basu , a. and pardo , l. ( 2016 ) . a generalized divergence for statistical inference .",
    "_ bernoulli _ , to appear .",
    "pre - print technical report ( 2013 ) at _ biru/2013/3 _ , bayesian and interdisciplinary research unit , indian statistical institute , kolkata , india ."
  ],
  "abstract_text": [
    "<S> minimum divergence methods are popular tools in a variety of statistical applications . </S>",
    "<S> we consider tubular model adequacy tests , and demonstrate that the new divergences that are generated in the process are very useful in robust statistical inference . in particular we show that family of @xmath0-divergences can be alternatively developed using the tubular model adequacy tests ; a further application of the paradigm generates a larger superfamily of divergences . </S>",
    "<S> we describe the properties of this larger class and its potential applications in robust inference . along the way </S>",
    "<S> , the failure of the first order influence function analysis in capturing the robustness of these procedures is also established . </S>"
  ]
}