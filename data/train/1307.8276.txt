{
  "article_text": [
    "thanks to their high computational power and high memory bandwidth , as well as to the availability of a complete programming environment , graphics processing units ( gpus ) have quickly risen in popularity and acceptance in the hpc world .",
    "the challenge is now demonstrating that they are suitable for capability computing , _",
    "i.e._they are still effective when scaling on systems .",
    "is needed to decrease ( strong scaling ) , to match the requirements of modern demanding applications , _ e.g._by overcoming the gpu memory size limitations ( bigger simulation volumes , finer grained meshes , _ etc . _ ) or even to enable new types of computation which otherwise would not be possible .    in many cases ,",
    "applications show poor scaling properties because , on increasing the number of computing elements , the computation locally performed by processing nodes shrinks faster than the amount of communication . to fix it ,",
    "a well - established optimization technique is _ overlapping _ computation and communication . additionally , due to _ staging _ of gpu data in host buffers prior and/or after the communication phase",
    ", network exchange of large buffers typically needs a proper coding ( pipelining ) to obtain good bandwidth .",
    "these two techniques are inter - related : _ staging _ , when not properly implemented , can hurt _ overlapping _ due to unexpected synchronizations of gpu kernels ; on the other hand , _ staging _ data , while computing is underway , is essential to obtain the _",
    "overlap_.    the use of the _ peer - to - peer_exchange of data among multiple gpus in a single box , instead of using a simple intra - node message - passing approach , is reported to provide a 50% performance gain on capability problems , as for example recently discussed in the literature  @xcite . in principle , the same _ peer - to - peer_technology , applied to an interconnection network , can also be employed to enable remote transfers of gpu buffers without staging to host memory .",
    "anyway , a few capabilities are required to do so efficiently , at least on nvidia fermi - class gpus , which are beyond those available in current commercial cluster interconnects ( infiniband , 10 g ) . those capabilities ,",
    "if cost effective , will probably appear in next generation silicon devices with the typical delay due to vlsi design and production cycles ( 18 - 24 months ) .    in the meantime , it is entirely possible to experiment with gpu _ peer - to - peer_networking by using reconfigurable components , like the altera statix iv in the apenet+card  @xcite , which offers both specialized transmission logic blocks and on - chip memory banks .    in this paper",
    "we report on our experiences in adding gpu _ peer - to - peer_capabilities to the apenet+network interconnection , presenting some early performance results .",
    "the paper is organized as follows : in section  [ sec : rel ] we cite some related works ; section  [ sec : bgnd ] briefly introduces the gpu _ peer - to - peer_technology and the apenet+network adapter , while section  [ sec : p2ponnet ] describes the implementation issues related to how the technology has been introduced in apenet+ ; section  [ sec : bmarks ] contains the preliminary results obtained on apenet+ , in particular on synthetic benchmarks and on two applications ; the last section contains conclusions and some final remarks .",
    "previous attempts at integrating gpus with network interconnect middlewares , although showing interesting results , all fall in the category .",
    "while early attempts at integrating with mpi date back to 2009  @xcite , two of the most widely used mpi implementations , openmpi  @xcite and mvapich2  @xcite , have recently started to offer the possibility of specifying gpu memory pointers in mpi functions , suppressing the chore of explicitly coding the data transfers between gpu and cpu memories .",
    "this feature represents an essential part of the research efforts aimed towards the definition of a general mechanism for direct communication among gpus . despite that , both openmpi and mvapich2 rely on a software approach which eases programming and that can increase communication performance for messages , thanks to pipelining implemented at the mpi library level . on the other hand , this approach can even hurt performance  @xcite for messages , due to them not using independent cuda streams , thereby introducing an implicit synchronization that ruins the overlap in applications .    on a related note ,",
    "mpi - acc @xcite experimented with alternative approaches at integrating gpus with mpi , even discovering unexpected and bugs in previous cuda releases .",
    "fermi is the first nvidia gpu architecture which externally exposes a proprietary protocol to exchange data among gpus directly across the pci express bus ( pcie ) , a technique which is generically referred to as _ peer - to - peer _ , and publicly advertised under the gpudirect peer - to - peer moniker .",
    "the nvidia _ peer - to - peer_protocol comprises a number of hardware resources ( registers , mailboxes ) implemented on the gpu and set of rules to use them .",
    "this protocol basically allows one gpu to read and write the memory of another gpu , provided that they are on a compliant platform ( suitable pciebus topology , bug - free chipsets ) . at the api level , it is used by the ` cudamemcpypeer ( ) ` and ` cudamemcpypeerasync ( ) ` apis , or simply by ` cudamemcpy ( ) ` on selected platforms ( uva mode ) , when memory pointers refers to device memory residing on two different gpus .",
    "this protocol can be used by a third - party device to gain direct access to the gpu memory , provided that it is able to consistently mimic the correct hardware behaviour and that the right bindings to the nvidia kernel driver are established at the software level . since cuda 4.1",
    ", the sw support to gpu _ peer - to - peer_has been shipped as an internal preview technology , subject to nda .    beyond the _ peer - to - peer_protocol , there is an additional access method for third - party devices , the so called bar1 .",
    "this methods is alternative to the _ peer - to - peer _ , and cuda 5.0 has officially introduced a public api to support it on kepler - based tesla and quadro gpus .",
    "with bar1 , it is possible to expose , or _ map _ , _",
    "i.e._to make it available , a region of device memory on the second pciememory - mapped address space of the gpu , from which it can be read or written with standard pciememory operations .",
    "due to platform constraints ( 32-bits bios ) , this address space is limited to a few hundreds of megabytes , so it is a scarce resource .",
    "additionally , mapping a gpu memory buffer is an expensive operation , which require a full reconfiguration of the gpu .",
    "as it is shown below , the bar1 reading bandwidth on fermi is quite limited , suggesting that the fermi architecture was not optimized for this access method .",
    "that is why we have never used the bar1 method on fermi .",
    "apenet+has been supporting _ peer - to - peer_on fermi since the end of 2011 , and since recently also bar1 and _ peer - to - peer_on kepler .      in cuda , the _ peer - to - peer_/bar1 support for third - party device",
    "is split into a user- and a part .",
    "the function ` cupointergetattribute ( ) ` with the ` cu_pointer_attribute_p2p_tokens ` parameter is used to retrieve special handles from pointers to gpu memory buffers .",
    "those handles are then used in to properly map the gpu buffers , _",
    "i.e._one page descriptor for each 64  kb page , comprising the physical page address plus additional low - level protocol tokens which are used to physically read and write gpu memory .    technically , _",
    "peer - to - peer_writing of gpu memory is only slightly more difficult than host memory writing , the only difference being the managing of a sliding window to access different pages .",
    "gpu memory reading is instead more complex because it is designed around a protocol between the initiator and the target devices .",
    "this design is justified by the need to work around bugs in pciechip - sets , related to a traffic pattern among devices which is still quite uncommon at least on the x86 platform . the ability to use the _ peer - to - peer_protocol among gpus , and its performance ,",
    "is constrained by the pcietopology ; performance is excellent when two gpus share the same pcie , _",
    "e.g._they are directly connected to a pcieswitch or to the same hub .",
    "otherwise , when gpus are linked to different bus branches , performance may suffers or malfunctionings can arise .",
    "this can be an issue on sandy bridge xeon platforms , where pcieslots might be connected to different processors , therefore requiring gpu _ peer - to - peer_traffic to cross the qpi channel(s ) .",
    "apenetis a 3d torus interconnection technology originally proposed , in its first version , back in 2004  @xcite and which is now being developed in its second generation version , called apenet+  @xcite .",
    "it has a direct network design which combines the two traditional components : the network interface ( ni ) and the router ( rtr ) . the router implements a static routing algorithm and directly controls an switch , with 6 ports connecting the external torus link blocks ( @xmath0 , @xmath1 , @xmath2 , @xmath3 , @xmath4 , @xmath5 ) and 2 local packet injection / extraction ports .",
    "the apenet+network interface comprises the pcie x8 gen2 link to the host system , for a maximum data transfer rate of 4 + 4  gb / s , the packet injection logic ( tx ) with a 32  kb transmission buffer , and the rx rdma logic which converts the virtual memory address of the destination buffer into a scatter list of physical memory addresses .",
    "the core architecture is depicted on fig  [ fig : internals ] .",
    "the receiving ( rx ) data path manages buffer validation ( the ` buf_list ` ) and address translation ( the ` host_v2p`map ) ; these tasks are currently partly implemented in software running on a ( ` nios  ii ` ) , which is synthesized onto the stratix iv fpga . on the transmit ( tx ) data path ,",
    "equivalent tasks are carried on by the kernel device driver , which implements the message fragmentation and pushes transaction descriptors with validated and translated physical memory addresses .",
    "the apenet+architecture is designed around a simple remote direct memory access ( rdma ) programming model .",
    "the model has been extended with the ability to read and write the gpu private memory  global device memory in cuda wording  directly over the pcie bus , by exploiting the nvidia gpudirect _ peer - to - peer_(p2p ) hw protocol .",
    "apenet+is relatively easy to extend thanks to the presence of the reconfigurable hardware component ( an altera fpga ) which , among other resources like transceiver blocks , memory banks , _ etc .",
    "_ , provides a 32  bit ( ` nios  ii ` ) that can run up to 200  mhz and is easily programmable in c.    introducing gpu _ peer - to - peer_in apenet+has been relatively easy for the receive data path , in that the 64  kb page gpu windowing access has been implemented as a variation of the 4  kb page host memory writing flow .",
    "either the relevant data structures have been extended ( the ` buf_list ` ) to accept both host and gpu buffers , or new ones have been added ( the new ` gpu_v2p`map , one per gpu ) .    for both read and write ,",
    "physical gpu memory addresses are needed to generate the transactions on the pcielink , so a proper gpu address translation module , ` gpu_v2p ` , has been implemented on apenet+ , very similar to but not exactly the same as the host one . for each gpu card on the bus , a gpu_v2p page table",
    "is maintained , which resolves virtual addresses to gpu page descriptors .    currently , the processing time of an incoming gpu data packet is of the order of 3  @xmath6s(1.2  gb / s for 4  kb packets ) and it is equally dominated by the two main tasks running on the ` nios  ii ` : the ` buf_list`traversal ( which linearly scales with the number of registered buffers ) and the address translation ( which has constant traversal time thanks to the page table ) .",
    "good performance in gpu memory reading , which is used during transmission of gpu buffers , has instead been by far the most difficult task to achieve , requiring two major redesigns of the related sw / hw blocks . unlike host buffer transmission , which is completely handled by the kernel driver",
    ", gpu data transmission is delegated to the apenet+ .",
    "this is so not only to the host , which would be a minor requirement , but mainly for the architectural need of maintaining the correct data flow among the different actors involved : the read request queue of the gpu ( arrow 1 in fig .",
    "[ fig : gpu_p2p ] ) , the data flow ( arrow 2 ) insisting on the apenet+data transmission buffers and the outgoing channel buffers .",
    "this flow needs proper management to avoid buffer overflowing but at the same time it has to be carefully tuned to obtain enough performances . with the first generation gpu memory reading and control flow logic ( ` gpu_p2p_tx ` ) ( v1 ) , which was able to process a single packet request of up to 4 kb , the peak gpu reading bandwidth  @xcite was throttled to 600  mb / s .",
    "the reasons for the poor performances were : the slow rate of read requests emitted by the ` gpu_p2p_tx`block towards the gpu and the long latency of the gpu in responding with data , which is quite understandable as the gpu memory subsystem is optimized for throughput rather than for latency . besides , the ` gpu_p2p_tx`was impacting the rx processing path due to the high computation load on the ` nios  ii ` .    the second generation ` gpu_p2p_tx`implements two key improvements : an hardware acceleration block which generates the read requests towards the gpu with a steady rate of one every 80  ns ; a pre - fetch logic which attempts to hide the response latency of the gpu .",
    "additionally , thanks to the acceleration blocks , the ` nios  ii ` can allot a larger to the receive data path ( rx processing ) . by using a 32  kb prefetch window , which is related to the size of the transmission buffers ( tx fifo ) , the ` gpu_p2p_tx`was able to reach the current peak of 1.5  gb / s for the gpu reading bandwidth .    in the last generation ` gpu_p2p_tx`(v3 ) , the _ new flow - control _ block is able to an unlimited amount of data so as to keep the gpu read request queue full , while at the same time to conditions ( arrow 3 in fig .",
    "[ fig : gpu_p2p ] ) of the different temporary buffers ( tx data fifo , tx header fifo , _ peer - to - peer_request fifo , _ etc . _ ) .    fig .",
    "[ fig : gpu_tx_prefetch_bw ] and [ fig : gpu_prefetch_bw ] show the effect of the different ` gpu_p2p_tx`implementations onto the gpu reading bandwidth and on the whole loop - back bandwidth .",
    "the apenet+apis have been extended to handle transmission and reception of gpu buffers , in a way that makes extensive use of the uniform virtual address ( uva ) capability of cuda . with uva",
    " available on most platforms and os s ,  gpu buffers are assigned unique addresses , and they can be distinguished from plain host memory pointers by using the ` cupointergetattribute ( ) ` call , which also returns other important buffer properties like the gpu index and the cuda context .",
    "the apenet+buffer pinning and registration api now accepts gpu buffers , which are mapped on - the - fly if not already present in an internal cache .",
    "buffer mapping consists in retrieving the _ peer - to - peer_informations , then passing them down to the kernel driver and from there to the ` nios  ii ` , in the ` buf_list`and ` gpu_v2p`data structures . after registration , a buffer",
    " either a host or gpu , uniquely identified by its ( uva ) virtual address and process i d  can be the target of a put operation coming from another node .",
    "network packets carry the destination virtual memory address in the header , so when they land onto the destination card , the ` buf_list`is used to distinguish gpu from host buffers .    on the transmitting node , the source memory buffer type is chosen at compilation time by passing a flag to the put api .",
    "this is useful to avoid a call to ` cupointergetattribute ( ) ` , which is possibly expensive  @xcite , at least on early cuda 4 releases .",
    "when a gpu buffer is transmitted , the buffer mapping is automatically done , if necessary , and a simplified descriptor list containing only gpu virtual addresses is generated by the kernel driver and passed to the ` nios  ii ` . as in the rx processing phase , the ` nios  ii`is once again in charge of the translation of source memory page addresses , in addition to driving the gpu _ peer - to - peer_protocol together with the hw acceleration blocks .",
    "in this section we report the results of gpu peer - to - peer enabled benchmarks and applications on apenet+ .",
    "the apenet+test platform ( cluster  iin the following ) is made of eight xeon westmere nodes , arranged in a @xmath7 torus topology , each one equipped with a single gpu ( all fermi 2050 but one 2070 ) and a mellanox connectx-2 board , plugged in a pciex4 slot ( due to motherboard constraints ) and connected to a mellanox mts3600 switch .",
    "infiniband results were collected on a second 12-nodes xeon westmere cluster ( cluster  ii ) , each node equipped with two fermi 2075 gpus ( tesla s2075 ) and a mellanox connectx-2 board , plugged in a pciex8 slot and connected to a mellanox is5030 switch .",
    "ecc is off on both clusters .",
    "mvapich2 1.9a2 and osu micro benchmarks v3.6 were used for all mpi ib tests .",
    "the successive revisions of the apenet+_peer - to - peer_support has been guided by a low - level analysis of the performance on the pciebus , through the use of a bus analyzer .    in fig .",
    "[ fig : pcie_timings ] , we report the timings of the most important pciebus transactions , as seen by an active interposer sitting between the apenet+card and the motherboard slot .",
    "the most interesting informations are : the apenet+`gpu_p2p_tx`(v2 , in this case ) implementation has an overhead which is a substantial part of those 3  @xmath6sin the initial delay ( transaction 1 to 2 ) .",
    "that overhead partially overlaps with previous transmissions , so it is paid in full either at the beginning of a long communication phase ( with minor effects ) or on short - message round - trip tests ( with visible effects on the network latency ) .",
    "the head reading latency of gpu is 1.8  @xmath6s(transaction 2 to 3 ) , then it sustains a 1536  mb / s data throughput towards apenet+transmission buffers ( transaction 3 to 4 is 663  @xmath6sfor a single 1  mb message , 53% link utilization ) .",
    "the read requests generated towards the gpu by the ` gpu_p2p_tx`hardware accelerator are regularly emitted once every 76  @xmath6s , _",
    "i.e._96  mb / s of protocol traffic and 13% link utilization .    as of the _ peer - to - peer_write bandwidth , judging from pciebus traces , the gpu has no problem sustaining the pcie x8 gen2 traffic , even though apenet+is currently not able to use all the available bandwidth due to limitations of its rx packet processing ( more on this below ) .",
    "to give an idea of the performance and limitations of the current implementation , in table  [ tab : lowlevel ] we collected the memory read performance , as measured by the apenet+device , for buffers located on either host or gpu memory .",
    "tasks + host mem read & 2.4  gb / s & & none + gpu mem read & 1.5  gb / s & fermi / p2p & ` gpu_p2p_tx ` + gpu mem read & 150  mb / s & fermi / bar1 & ` gpu_p2p_tx ` + gpu mem read & 1.6  gb / s & kepler / p2p & ` gpu_p2p_tx ` + gpu mem read & 1.6  gb / s & kepler / bar1 & ` gpu_p2p_tx ` + gpu - to - gpu loop - back & 1.1  gb / s & fermi / p2p & ` gpu_p2p_tx`+ rx + host - to - host loop - back & 1.2  gb / s & & rx +    as discussed in the previous section , the complexity of the gpu _ peer - to - peer_read protocol and the limitations of our implementation set a limit of 1.5  gb / s to the fermi gpu memory read bandwidth , which is roughly half that obtained for host memory read ( 2.4  gb / s ) . for reference , the gpu - to - host reading bandwidth , as obtained by ` cudamemcpy ` , which uses the gpu dma engines , peaks at about 5.5  gb / s on the same platform .",
    "we also report very early results on kepler gpus , for both k10 and k20 , which show a 10% increase in the available _ peer - to - peer_reading bandwidth with respect to fermi in p2p mode , and a more impressive factor 10 using the bar1 approach ( 150  mb / s on fermi vs. 1.6  gb / s on k20 ) .",
    "tests on kepler gpus have been run on pre - release cards , so the reported performance is subject to change .",
    "we underline that this is the reading bandwidth as measured from apenet+through the gpu _ peer - to - peer_protocol , neither the internal device bandwidth , which is instead available to kernels running on the gpu , nor the gpu dma engine bandwidth , _ e.g._`cudamemcpy ( ) ` .",
    "the last two lines of table  [ tab : lowlevel ] show that , when the packet rx processing is taken into account by doing a loop - back test , the peak bandwidth decreases from 2.4  gb / s to 1.2  gb / s in the host - to - host case , and from 1.5  gb / s to 1.1  gb / s in the gpu - to - gpu case , _ i.e._an additional 10% price to pay in the latter case .",
    "the last column in the table shows that the ` nios  ii ` is the main performance bottleneck .",
    "we are currently working on adding more hardware blocks to accelerate the rx task .",
    "the values reported in table  [ tab : lowlevel ] are obtained as the peak values in a loop - back performance test , coded against the apenet+rdma api .",
    "the test allocates a singe receive buffer ( host or gpu ) , then it enters a tight loop , enqueuing as many rdma put as possible as to keep the transmission queue constantly full .",
    "[ fig : gpu_tx_prefetch_bw ] is a plot of gpu reading bandwidth at varying message sizes , estimated by using the test above and by flushing tx injection fifos , effectively simulating a zero - latency infinitely fast switch .",
    "the original ` gpu_p2p_tx`v1 implementation ( no pre - fetching and software - only implementation on ` nios  ii ` ) shows its limits . `",
    "gpu_p2p_tx`v2 ( hw acceleration of read requests and limited pre - fetching ) shows a 20% improvement while increasing the pre - fetch window size from 4 kb to 8 kb .",
    "unlimited pre - fetching and more sophisticated flow - control in ` gpu_p2p_tx`v3 partially shows its potential only in the full loop - back plot of fig .",
    "[ fig : gpu_prefetch_bw ] here the ` nios  ii`handles both the ` gpu_p2p_tx`and the rx tasks , so therefore any processing time spared thanks to a more sophisticated gpu tx flow - control logic reflects to an higher bandwidth .",
    "this also suggests that the apenet+bi - directional bandwidth , which is not reported here , will reflect a similar behaviour .",
    "as shown above , reading bandwidth from gpu memory and rx processing are the two key limiting factors of the current apenet+implementation .",
    "therefore , it can be expected that they influence the communication bandwidth between two nodes in different ways , depending of the type of the buffers used . to measure the effect of those factors independently ,",
    "we run a two node bandwidth test on apenet+ , in principle similar to the mpi osu  @xcite uni - directional bandwidth test , although this one is coded in terms of the apenetrdma apis .    the plot in fig .",
    "[ fig : apenet_bw ] shows the bandwidth of apenet+for the four different possible combinations of source and destination buffer types : for source buffers located in host memory , the best performance of 1.2  gb / s is reached , with a 10% penalty paid when receive buffers are on the gpu , probably related to the additional actions involved , _ i.e._switching gpu _ peer - to - peer_window before writing to it . for gpu source buffers ,",
    "the gpu _ peer - to - peer_reading bandwidth is the limiting factor , so the curves are less steep and only for larger buffer sizes , _ i.e._beyond 32  kb , the plateau is reached .",
    "clearly , the asymptotic bandwidth is limited by the rx processing , but the overall performance is affected by the transmission of gpu buffers .",
    "interestingly , the host - to - gpu performance seems to be a very good compromise bandwidth - wise , _",
    "e.g._for 8  kb message size the bandwidth is twice that of the gpu - to - gpu case .",
    "of course this plot is good for analyzing the quality of the apenet+implementation , but it says nothing about which method is the best for exchanging data between gpu buffers , _",
    "i.e._in which ranges gpu _ peer - to - peer_is better than staging on host memory . to this end ,",
    "[ fig : apenet_vs_ib_bw ] is a plot of the gpu - to - gpu communication bandwidth , with three different methods : apenet+using gpu _ peer - to - peer _ ; apenet+with staging of gpu data to host memory ; osu bandwidth test , using mvapich2 over infiniband , which uses a pipelining protocol above a certain threshold , used for reference .",
    "the gpu _ peer - to - peer_technique is definitively effective for small buffer sizes , _",
    "i.e._up to 32  kb ; after that limit , staging seems a better approach .",
    "[ fig : apenet_vs_ib_lat ] is more useful to explore the behaviour of gpu _ peer - to - peer_on small buffer size . here",
    "the latency , estimated as half the round - trip time in a ping - pong test , shows a clear advantage of the _ peer - to - peer_implementation with respect to staging ( p2p = off in the figure ) , even on a very low - latency network as infiniband .",
    "indeed , the apenet+_peer - to - peer_latency is 8.2  @xmath6s , while for apenet+with staging and mvapich2/ib it is respectively 16.8  @xmath6sand 17.4  @xmath6s . in the latter case ,",
    "most of the additional latency comes from the overhead of the two cuda memory copy ( ` cudamemcpy ` ) calls necessary to move gpu data between temporary transmission buffers . by subtracting the apenet+h - h latency ( 6.3  @xmath6sin fig .",
    "[ fig : apenet_lat ] ) from the apenet+latency with staging ( 16.8  @xmath6s ) , the single ` cudamemcpy ` overhead can be estimated around 10  @xmath6s , which was confirmed by doing simple cuda tests on the same hosts .",
    "the run times of the bandwidth test , for short message size , are plot in fig .",
    "[ fig : host_overhead ] . in the logp model  @xcite , this is the host overhead , _",
    "i.e._the fraction of the whole message send - to - receive time which does not overlap with subsequent transmissions .",
    "of those 5  @xmath6sin the host - to - host case , at least a fraction can be accounted to the rx processing time ( 3  @xmath6sestimated by cycle counters on the ` nios  ii`firmware ) . the additional 3  @xmath6sin the gpu - to - gpu ( p2p = on ) case should be quite related to _ peer - to - peer_protocol as implemented by apenet+ , _ e.g._the 3 + 1.8  @xmath6s`gpu_p2p_tx`overhead in fig .  [",
    "fig : pcie_timings ] .",
    "when staging is used instead ( p2p = off ) , out of the additional 12  @xmath6s(17 - 5  @xmath6sof the host - to - host case ) , at least 10  @xmath6sare due to the ` cudamemcpy ` device - to - host , which is fully synchronous with respect to the host , therefore it does not overlap .    in conclusion ,",
    "the gpu _ peer - to - peer _ , as implemented in apenet+ , shows a bandwidth advantage for message sizes up to 32  kb . beyond that threshold , at least on apenet+it",
    "is convenient to give up on _ peer - to - peer_by switching to the staging approach .",
    "eventually that could have been expected , as architecturally gpu _ peer - to - peer_cannot provide any additional bandwidth , which is really constrained by the underling pci - express link widths ( x8 gen2 for both apenet+and infiniband ) and bus topology .      .",
    "for each @xmath8 we show three variants , relative to the use of p2p ( off , rx only , rx and tx ) . at @xmath9 a super linear speedup is observed . ]    in this section we show an early evaluation of gpu _ peer - to - peer_networking on a simulation code for the heisenberg spin glass model  @xcite .",
    "thanks to its regular communication pattern , we consider it a good model application for typical simulations .",
    "the gpu part of the code is highly optimized ; it uses parallel update trick ; the 3d domain is decomposed among the computing nodes along a single dimension , and the overlap method is used : first compute the local lattice boundary , then exchange it with the remote nodes , while computing the bulk .",
    "the computation consists of multiple steps applied to the whole spin lattice of size @xmath10 :    .hsg : on cluster  i , single - spin update time in picoseconds , strong scaling on apenet+ , @xmath11 , gpu _ peer - to - peer _ networking enabled for both rx and tx . [ cols=\"<,^,^,^\",options=\"header \" , ]     according to the specs of the ` graph500 ` benchmark @xcite , we use , as a performance metrics , the number of traversed edges per second ( teps ) , so that higher numbers correspond to better performances .",
    "our preliminary results , for p2p = on case only , are summarized in table [ tab : ss ] .",
    "table [ tab : ss ] shows the strong scaling ( the size of the graph is fixed ) obtained for a graph having @xmath12 vertices and compares the results on the two clusters .",
    "apenet+performs better than infiniband up to four nodes / gpus ; after that point we speculate that the current implementation of the apenet+3d torus network suffers on this kind of all - to - all traffic .",
    "this is as of now the topic of further investigations .",
    "although all cuda kernels and the rest of the code are identical in the and the apenet+version of the code , we checked that the difference in performances is actually due to the communication part . in the four - nodes case , for this particular graph traversal , the communication time is 50% lower in the apenet+case ( figure  [ fig : breakdown ] ) .",
    "as it often happens , it is not easy to draw definitive conclusions on the effectiveness of gpu _ peer - to - peer _ , as it is strongly influenced by the maturity and efficiency of the particular apenet+implementation .",
    "anyway , we can state that the gpu _ peer - to - peer_write protocol is quite effective ; it has a small overhead and need minor modifications with respect to writing host memory .    on the other end ,",
    "the _ peer - to - peer_reading protocol is complicated for third - party devices , though minor technical modifications could improve it a lot ; in some sense , it seems too close to the internal fine - grained memory architecture of the gpu .",
    "moreover , the reading bandwidth limit around 1.5  gb / s ( on fermi ) seems architectural , verified both at the pcietransaction level and by the scaling with the pre - fetch window . on the other hand , the gpu _ peer - to - peer_is by design more resilient to host platform idiosyncrasies , like pciebus topology and chip - sets bugs .",
    "on kepler , the bar1 technique seems more promising . in many ways",
    "it supports the normal pcieprotocol for memory - mapped address spaces , both for reading and writing , so it requires minimal changes at the hardware level .",
    "the drawback is in platform support , as the pciesplit - transaction protocol among devices is known to be deadlock - prone , or at least sub - performing , on some pciearchitectures .",
    "judging from our early experience , the bar1 reading bandwidth could be positively affected by the proximity of the gpu and the third - party device , _",
    "e.g._both being linked to a plx pcieswitch .    as of the gpu _ peer - to - peer_implementation on apenet+ , it seems to be effective especially in latency - sensitive situations . as synthetic benchmarks",
    "have shown , apenet+is able to outperform ib for message sizes when using gpu _ peer - to - peer_. the advantage provided to the applications by this technique depends on several factors related to their communication pattern , i.e. message sizes , destination nodes , source and receive buffer types ( host or gpu ) etc . , which in turn depend on simulation parameters like volume size , number of gpus per node and number of cluster nodes .",
    "it depends also on the possibility of overlapping computation and communication .",
    "anyway _ peer - to - peer_on apenet+should provide a boost in strong scaling situations , where the communication pattern is usually dominated by small - size messages .",
    "unfortunately , we are currently limited to an 8-nodes test environment ; this is going to change in the next few months , when we will be able to scale up to 16/24 nodes .",
    "the apenet+development is partially supported by the eu framework programme 7 project euretile under grant number 247846 .",
    "d.  rossetti , r.  ammendola , p.  vicini , a.  biagioni , o.  frezza , f.  lo  cicero , a.  lonardo , p.s .",
    "paolucci , f.  simula , l.  tosoratto `` apenet+ project status '' proceedings of the xxix international symposium on lattice field theory ( lattice 2011 ) .",
    "ashwin aji , james dinan , darius buntinas , pavan balaji , wu - chun feng , keith bisset , and rajeev thakur .",
    "`` mpi - acc : an integrated and extensible approach to data movement in accelerator - based systems '' proc .",
    "14th ieee intl .",
    "conf . on high performance computing and communications .",
    "liverpool , uk .",
    "june 2012 .",
    "r.  ammendola , m.  guagnelli , g.  mazza , f.  palombi , r.  petronzio , d.  rossetti , a.  salamon , and p.  vicini .",
    "`` apenet : lqcd clusters _  la _ ape '' , 140(0):826  828 , 2005 .",
    "proceedings of the xxii international symposium on lattice field theory ( lattice 2004 ) .",
    "r.  ammendola , a.  biagioni , o.  frezza , f.  lo  cicero , a.  lonardo , p.  paolucci , r.  petronzio , d.  rossetti , a.  salamon , g.  salina , f.  simula , n.  tantalo , l.  tosoratto , and p.  vicini .",
    "`` apenet+ : a 3d toroidal network enabling petaflops scale lattice qcd simulations on commodity clusters '' proceedings of the xxviii international symposium on lattice field theory pos(lattice 2010)022 and arxiv:1012.0253v1 .",
    "d.  bureddy , h.  wang , a.  venkatesh , s.  potluri , d.k .",
    "`` omb - gpu : a micro - benchmark suite for evaluating mpi libraries on gpu clusters '' recent advances in the message passing interface , lecture notes in computer science , 7490:110 - 120 , 2012 .",
    "s.  hong , t.  oguntebi , and k.  olukotun .",
    "`` efficient parallel graph exploration on multi - core cpu and gpu '' in _ international conference on parallel architectures and compilation techniques ( pact ) , 2011 _ , pages 78 88 , oct .",
    "2011 .",
    "d.  culler et al .",
    "`` logp : towards a realistic model of parallel computation . ''",
    "proceedings of the fourth acm sigplan symposium on principles and practice of parallel programming , volume 28 , issue 7 , p. 1 - 12 ,",
    "m bernaschi , m bisson , e mastrostefano , d rossetti .",
    "`` breadth first search on apenet+ . ''",
    "@xmath13 workshop on irregular applications : architectures & algorithms , nov .",
    "2012 . to appear on sc 2012 conference proceedings"
  ],
  "abstract_text": [
    "<S> modern gpus support special protocols to exchange data directly across the pci express bus . while these protocols could be used to reduce gpu data transmission times , basically by avoiding staging to host memory , they require specific hardware features which are not available on current generation network adapters . in this paper </S>",
    "<S> we describe the architectural modifications required to implement _ peer - to - peer_access to nvidia fermi- and gpus on an cluster interconnect .    besides </S>",
    "<S> , the current software implementation , which integrates this feature by minimally extending the rdma programming model , is discussed , as well as some issues raised while employing it in a higher level api like mpi .    finally , the current limits of the technique are studied by analyzing the performance improvements on low - level benchmarks and on two gpu - accelerated applications , showing when and how they seem to benefit from the gpu _ peer - to - peer_method . </S>"
  ]
}