{
  "article_text": [
    "we present a status update of apenet+ , which is the high performance , low latency custom interconnect system developed at infn targeting hybrid hpc platforms .",
    "the apenet+hardware , a pciex8 gen2 card described in  @xcite , allows building a 3d toroidal mesh topology of computing nodes .    moreover , we implemented nvidia gpudirect v1.0 and v2.0  @xcite to directly access data on fermi and kepler gpus . in this way ,",
    "real , and transactions can be issued using a remote dma programming paradigm .    at the moment , apenet+is able to outperform commercial solutions ( like infiniband  @xcite ) for message size when using gpu . for large message sizes",
    ", host memory staging techniques are still winning , also due to highest bandwidth of latest commercial cards , which are already and guarantee 56  gbps on the link .",
    "of course , our focus is in upgrading the apenet+hardware in order to keep pace with the advances of technology standards .",
    "in the period , we focused on the improvement of our internal architecture .",
    "three major reworkings have been undertaken regarding the pcieinterface , the memory management and the interface .",
    "on the pcieside , we noticed that effective bandwidth on data transactions was quite low compared to the theoretical one ( @xmath0 ) .",
    "this is due to the time elapsed between issuing a request on the pciebus and its completion ; this time is system dependent and can be very large . in order to optimize the performances ( in addition to parameters tuning like maximum payload size ) , the card must be able to manage more than one outstanding request on the pciebus . in this way , multiple transactions can overlap and total transaction time is shorter .    at hardware level we needed to implement two concurrent dma engines fed by a prefetchable command queue . the difference between a single dma and a double dma implementation",
    "can be seen in fig .",
    "[ fig:2dma ] .",
    "we estimated an efficiency gain up of to 40% in time  @xcite .",
    "we also focused on removing a known bottleneck on the receiving path , when virtual to physical address translation is necessary in order to dispatch data payloads in the correct physical memory areas ( whether they be on host memory or gpu memory ) .",
    "on apenet+this task was initially executed by the nios ii embedded processor but the impact on the resulting execution time was higher than expected .    thus , a novel implementation of a translation buffer ( tlb ) has been developed on the fpga , to accelerate address translation at hardware level  @xcite .",
    "as shown in fig .",
    "[ fig : tlb ] , the tlb block can store a limited amount of page entries and , in case of page hit , the nios ii processor is completely bypassed .",
    "a speedup of up to 60% in bandwidth on synthetic benchmarks has been measured with this enhancement .",
    "signal integrity of the transmission system was analyzed in order to push the embedded transceiver operating frequency to its limits .",
    "currently , for reliable operations and upper level firmware and software validation , the altera transceiver are set at 7.0  gbps , yielding a raw aggregated bandwidth of 28  gbps per apelink  @xcite channel . to estimate the efficiency of the apelinktransmission control logic operation  managing the data flow by encapsulating packets into a light , , protocol",
    " we devised a mathematical model ; current implementation yields a total efficiency of @xmath1 over a channel able to sustain @xmath2  2.6  gb / s bandwidth with a memory footprint limited to @xmath2  40  kb per channel .",
    "the described architectural improvements yielded significant performance gains with respect to our previously published results on tests of latency and bandwidth .",
    ".32     .32        .32     on fig .",
    "[ fig : rtlat ] we show the round trip latency .",
    "all and combinations are presented ; the plots clearly show that involvement of the gpu in the transaction either as sender or receiver causes roughly a 30% latency increase for small message sizes . on fig .",
    "[ fig : lat ] we show the advantage of apenet+p2p technique over infiniband , used with mvapich sw stack for message size up to 128  kb ; we measure @xmath3 in latency when using p2p ; @xmath4 is measured when p2p is not used ; @xmath5 is measured with infiniband , on the same platform .    on fig .",
    "[ fig : band ] we show the results of several bandwidth tests ; apart from the cases  where show that gpu memory read transactions incur into a bottleneck within the gpu itself  in all other transactions ( cpu memory read , gpu and cpu memory write ) we can reach the apenet+link limit , which is @xmath6 on current hardware .",
    "fault awareness is the first step when applying fault tolerance techniques in hpc ( _ e.g._task migration , checkpoint / restart ,  ) . on the quongplatform , thanks to some apenet+hardware features , each node is able to be aware of faults and critical events occurring to its components and to components of its neighbouring nodes .    even in case of multiple faults",
    "no area of the mesh can be isolated and no fault can remain undetected at global level . at",
    "the core of this approach , named lofamo(local fault monitor ) , there is a lightweight mutual watchdog protocol between the host node and apenet+and the 3d network topology  @xcite .",
    "the apenet+core contains a lofamohardware component and a set of lofamowatchdog registers , containing information about the host status , the apenet+status and the status of first neighbouring hosts . on each host in the platform",
    "a dedicated lofamosoftware component is able to periodically update the host watchdog register and read the apenetwatchdog register .    in fig .",
    "[ fig : lofamo ] it is depicted how a global fault awareness is obtained , for example , in case a host node stops working ; as the faulty host misses to update its watchdog register , the apenet+lofamohardware on the same node becomes aware of the fault and sends diagnostic messages via the 3d network towards its own neighbours .",
    "these hosts can retrieve data about faults occurring on the neighbour nodes from the watchdog registers and can inform about them ( _ e.g._via a service network ) a master node . in this way , the master node has the global picture of the platform health status and can take decisions about proper countermeasures .    note that time elapsed since fault occurrence to global fault awareness is dominated by the watchdog period : for a @xmath7 , @xmath8 . in the time range of interest for hpc ( watchdog period @xmath9 ) , the addition of lofamofeatures has no impact on apenet+data transfer latency , as the diagnostic messages are hidden in the communication protocol .",
    "the largest and most significant deployment of apenet+cards is within the quongcluster which is our hybrid , x86_64 dual gpu cluster with a @xmath10 apenet+ network topology ; it is used for testing , development and production run of scientific codes .",
    "several projects are developing applications that can fully exploit our peculiar interconnect solution with promising results . among these",
    ", we mention :    * the simulation of polychronous spiking neural networks  @xcite ; * a algorithms implementation for graph traversal  @xcite ; * a benchmark based on 3d heisenberg spin glass model by using the algorithm  @xcite ;    furthermore , in the context of hep experiments we are testing designs derived from the apenet+for gpu stream processing  @xcite and online track reconstruction with gpus  @xcite .",
    "newer fpga families  _ e.g._altera stratix v  are now available on the market , driving redesign of apenet+in two major hardware logic areas : gen3 migration for pcieinterface and new transceivers for increased link speed .",
    "pciegen3 migration allows an increase in bandwidth for the host interface .",
    "it is based on @xmath11 lanes using a 128/130 bit encoding ( thus the protocol overhead is reduced to less than 1% from 20% for previous generations ) .",
    "the total raw bandwidth that can be obtained with a @xmath12 interface is @xmath13 . to support this data rate , on the back - end",
    "the data - path must be wide , with a clock reference of 250 mhz .",
    "the standard used is axi4  @xcite , which needed a redesign of apenet+internal pcieinterface , as depicted in fig .",
    "[ fig : pcie ] .",
    "axi4 migration is also preparatory for future use of embedded arm hard ip processors , foreseen on class fpgas only like the future stratix 10 devices .",
    "new altera devices are capable of 14.1  gb / s transceivers , which can be bonded in 4 lanes to build up a 56  gb / s link . as a physical medium we can rely on qsfp+ standard that has been upgraded to work at these data rates ( the same as infiniband fdr ) .    in order to develop pciegen3 migration and 56  gb / s class links , we used an altera development board with a stratix v gx fpga @xcite .",
    "we implemented the link using the single 40  gb / s qsfp+ onboard connector and performed data transfer tests between 2 such boards .",
    "as a preliminary result we achieved a link speed of 11.3  gbps / lane ( 45.2  gbps / channel ) , still using cables .",
    "we presented a status update of the development of our custom interconnect system apenet+ .",
    "several architectural improvements have been discussed , that brought to substantial performance enhancements .",
    "we also introduced the fault - aware capability now embedded in the apenet+communication protocol , which can be used in advanced high - level fault tolerance techniques .",
    "we finally reported on latest developments on 28 nm fpga devices , that allow us to upgrade the pcieinterface to gen3 , and the off - board link to 56 gb / s data rate .",
    "this work was partially supported by the eu framework programme 7 project euretile under grant number 247846 ; roberto ammendola was supported by miur ( italy ) through infn suma project .",
    "10 url # 1#1urlprefix[2][]#2 ammendola r _",
    "_ 2012 _ journal of physics : conference series _ * 396 * 042059 http://stacks.iop.org/1742-6596/396/i=4/a=042059      bureddy d , wang h , venkatesh a , potluri s and panda d 2012 _ recent advances in the message passing interface _ ( _ lecture notes in computer science _ vol 7490 ) ed trff j  l , benkner s and dongarra j  j ( springer berlin heidelberg ) pp 110120 isbn 978 - 3 - 642 - 33517 - 4 http://dx.doi.org/10.1007/978-3-642-33518-1_16",
    "ammendola r _",
    "_ 2013 _ track on interconnect architectures for reconfigurable computing systems , held at reconfigurable computing and fpgas ( reconfig ) , 2013 international conference on _ to be published            bisson m , bernaschi m , mastrostefano e and rossetti d breadth first search on apenet+http://cass - mt.pnnl.gov / docs / session 2 - 1.pdf[http://cass-mt.pnnl.gov/docs/session 2 - 1.pdf ] ia3 workshop on irregular applications : architectures and algorithms , in conjunction with super computing 2012"
  ],
  "abstract_text": [
    "<S> modern graphics processing units ( gpus ) are now considered accelerators for general purpose computation . a tight interaction between the gpu and the interconnection network is the strategy to express the full potential on capability computing of a system on large hpc clusters ; that is the reason why an efficient and scalable interconnect is a key technology to finally deliver gpus for scientific hpc . in this paper </S>",
    "<S> we show the latest architectural and performance improvement of the apenet+network fabric , a pcieboard with 6 fully bidirectional links with 34  gbps of raw bandwidth per direction , and x8 gen2 bandwidth towards the host pc . </S>",
    "<S> the board implements a remote direct memory access ( rdma ) protocol that leverages upon ( p2p ) capabilities of fermi- and nvidia gpus to obtain real , transfers . </S>",
    "<S> finally , we report on the development activities for 2013 focusing on the adoption of the latest generation 28  nm fpgas and the preliminary tests performed on this new platform . </S>"
  ]
}