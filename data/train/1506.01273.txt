{
  "article_text": [
    "input media for automatic summarization has varied from text  @xcite to speech  @xcite and video  @xcite , but the application domain has been , in general , restricted to informative sources : news  @xcite , meetings  @xcite , or lectures  @xcite .",
    "nevertheless , application areas within the entertainment industry are gaining attention : e.g. summarization of literary short stories  @xcite , music summarization  @xcite , summarization of books  @xcite , or inclusion of character analyses in movie summaries  @xcite .",
    "we follow this direction , creating extractive , text - driven video summaries for films and documentaries .",
    "documentaries started as cinematic portrayals of reality  @xcite .",
    "today , they continue to portray historical events , argumentation , and research . they are commonly understood as capturing reality and therefore , seen as inherently non - fictional .",
    "films , in contrast , are usually associated with fiction .",
    "however , films and documentaries do not fundamentally differ : many of the strategies and narrative structures employed in films are also used in documentaries  @xcite .    in the context of our work , films ( fictional ) tell stories based on fictive events , whereas documentaries ( non - fictional ) address , mostly , scientific subjects .",
    "we study the parallelism between the information carried in subtitles and scripts of both films and documentaries .",
    "extractive summarization methods have been extensively explored for news documents  @xcite .",
    "our main goal is to understand the quality of automatic summaries , produced for films and documentaries , using the well - known behavior of news articles as reference .",
    "generated summaries are evaluated against manual abstracts using rouge metrics , which correlate with human judgements  @xcite .",
    "this article is organized as follows : section  [ sec : generic - summarization ] presents the summarization algorithms ; section  [ sec : datasets ] presents the collected datasets ; section  [ sec : setup ] presents the evaluation setup ; section  [ sec : results ] discusses our results ; section  [ sec : conclusions ] presents conclusions and directions for future work .",
    "six text - based summarization approaches were used to summarize newspaper articles , subtitles , and scripts . they are described in the following sections .    [",
    "[ sub : mmr ] ]    is a query - based summarization method  @xcite .",
    "it iteratively selects sentences via equation  [ eq : e1 ] ( @xmath0 is a query ; @xmath1 and @xmath2 are similarity metrics ; @xmath3 and @xmath4 are non - selected and previously selected sentences , respectively ) .",
    "@xmath5 balances relevance and novelty . can generate generic summaries by considering the input sentences centroid as a query  @xcite .",
    "@xmath6\\ ] ]      lexrank  @xcite is a centrality - based method based on google s pagerank  @xcite .",
    "a graph is built using sentences , represented by tf - idf vectors , as vertexes .",
    "edges are created when the cosine similarity exceeds a threshold .",
    "equation  [ eq : lexrank ] is computed at each vertex until the error rate between two successive iterations is lower than a certain value . in this equation",
    ", @xmath7 is a damping factor to ensure the method s convergence , @xmath8 is the number of vertexes , and @xmath9 is the score of the @xmath10th vertex .",
    "@xmath11}\\frac{\\operatorname{sim}\\left(v_{i},v_{j}\\right)}{\\sum_{v_{k}\\in \\operatorname{adj}\\left[v_{j}\\right]}\\operatorname{sim}\\left(v_{j},v_{k}\\right)}s\\left(v_{j}\\right)\\ ] ]    [ [ sub : lsa ] ]    infers contextual usage of text based on word co - occurrence  @xcite .",
    "important topics are determined without the need for external lexical resources  @xcite : each word s occurrence context provides information concerning its meaning , producing relations between words and sentences that correlate with the way humans make associations .",
    "is applied to each document , represented by a @xmath12 term - by - sentences matrix @xmath13 , resulting in its decomposition @xmath14 .",
    "summarization consists of choosing the @xmath15 highest singular values from @xmath16 , giving @xmath17 . @xmath18 and @xmath19 are reduced to @xmath20 and @xmath21 , respectively , approximating @xmath13 by @xmath22 .",
    "the most important sentences are selected from @xmath21 .",
    "documents are typically composed by a mixture of subjects , involving a main and various minor themes .",
    "support sets are defined based on this observation  @xcite .",
    "important content is determined by creating a support set for each passage , by comparing it with all others .",
    "the most semantically - related passages , determined via geometric proximity , are included in the support set .",
    "summaries are composed by selecting the most relevant passages , i.e. , the ones present in the largest number of support sets . for a segmented information source @xmath23 , support sets @xmath24 for each passage @xmath25",
    "are defined by equation  [ eq : e4 ] , where @xmath26 is a similarity function , and @xmath27 is a threshold .",
    "the most important passages are selected by equation  [ eq : e5 ] .",
    "@xmath28 @xmath29    [ [ sub : support - sets_luis ] ]    @xcite proposed an extension of the centrality algorithm described in section  [ sub : support - sets ] , which uses a two - stage important passage retrieval method .",
    "the first stage consists of a feature - rich supervised key phrase extraction step , using the maui toolkit with additional semantic features : the detection of rhetorical signals , the number of named entities , tags , and 4 n - gram domain model probabilities @xcite .",
    "the second stage consists of the extraction of the most important passages , where key phrases are considered regular passages .",
    "[ [ sub : grasshopper ] ]    @xcite is a re - ranking algorithm that maximizes diversity and minimizes redundancy .",
    "it takes a weighted graph @xmath30 ( @xmath31 : @xmath32 vertexes representing sentences ; weights are defined by a similarity measure ) , a probability distribution @xmath33 ( representing a prior ranking ) , and @xmath34 $ ] , that balances the relative importance of @xmath30 and @xmath33 .",
    "if there is no prior ranking , a uniform distribution can be used .",
    "sentences are ranked by applying the teleporting random walks method in an absorbing markov chain , based on the @xmath31 transition matrix @xmath35 ( calculated by normalizing the rows of @xmath30 ) , i.e. , @xmath36 .",
    "the first sentence to be scored is the one with the highest stationary probability @xmath37 according to the stationary distribution of @xmath38 : @xmath39 .",
    "already selected sentences may never be visited again , by defining @xmath40 and @xmath41 .",
    "the expected number of visits is given by matrix @xmath42 ( where @xmath43 is the expected number of visits to the sentence @xmath44 , if the random walker began at sentence @xmath10 ) .",
    "we obtain the average of all possible starting sentences to get the expected number of visits to the @xmath44th sentence , @xmath45 .",
    "the sentence to be selected is the one that satisfies @xmath46 .",
    "we use three datasets : newspaper articles ( baseline data ) , films , and documentaries .",
    "film data consists of subtitles and scripts , containing scene descriptions and dialog .",
    "documentary data consists of subtitles containing mostly monologue .",
    "reference data consists of manual abstracts ( for newspaper articles ) , plot summaries ( for films and documentaries ) , and synopses ( for films ) .",
    "plot summaries are concise descriptions , sufficient for the reader to get a sense of what happens in the film or documentary .",
    "synopses are much longer and may contain important details concerning the turn of events in the story . all datasets were normalized by removing punctuation inside sentences and timestamps from subtitles .",
    "temrio  @xcite is composed by 100 newspaper articles in brazilian portuguese ( table  [ tab : news_corpus ] ) , covering domains such as  world \" ,  politics \" , and  foreign affairs \" .",
    "each article has a human - made reference summary ( abstract ) .",
    ".temrio corpus properties . [ cols=\"<,<,^,^,^\",options=\"header \" , ]     ; support sets used cosine distance and threshold=@xmath47 ; kp - centrality used 50 key phrases . ]      news articles intend to answer basic questions about a particular event : who , what , when , where , why , and often , how . their structure is sometimes referred to as  inverted pyramid \" , where the most essential information comes first . typically , the first sentences provide a good overview of the entire article and are more likely to be chosen when composing the final summary . although documentaries follow a narrative structure similar to films , they can be seen as more closely related to news than films , especially regarding their intrinsic informative nature . in spite of their different natures , however ,",
    "summaries created by humans produce similar scores for all of them .",
    "it is possible to observe this behavior in figure  [ fig : originais ] .",
    "note that documentaries achieve higher scores than news articles or films , when using the original subtitles documents against the corresponding manual plot summaries .",
    "figure  [ fig : graphconcl1 ] presents an overview of the performance of each summarization algorithm across all domains .",
    "the results concerning news articles were the best out of all three datasets for all experiments .",
    "however , summaries for this dataset preserve , approximately , 31% of the original articles , in terms of sentences , which is significantly higher than for films and documentaries ( which preserve less than @xmath48 ) , necessarily leading to higher scores .",
    "nonetheless , we can observe the differences in behavior between these domains .",
    "notably , documentaries achieve the best results for plot summaries , in comparison with films , using scripts , subtitles , or the combination of both .",
    "the relative scores on the films dataset are influenced by two major aspects : the short sentences found in the films dialogs ; and , since the generated summaries are extracts from subtitles and scripts , they are not able to represent the film as a whole , in contrast with what happens with plot summaries or synopses . additionally , the experiments conducted for script+subtitles for films , in general , do not improve scores above those of scripts alone , except for support sets for r-1 .",
    "overall , lsa performed consistently better for news articles and documentaries .",
    "similar relatively good behavior had already been observed for meeting recordings , where the best summarizer was also lsa  @xcite .",
    "one possible reason for these results is that lsa tries to capture the relation between words in sentences . by inferring contextual usage of text based on these relations ,",
    "high scores , apart from r-1 , are produced for r-2 and r - su4 . for films ,",
    "lexrank was the best performing algorithm for subtitles , scripts and the combination of both , using plot synopses , followed by lsa and support sets for plot summaries .",
    "mmr has the lowest scores for all metrics and all datasets .",
    "we observed that sentences closer to the centroid typically contain very few words , thus leading to shorter summaries and the corresponding low scores .",
    "interestingly , by observing the average of r-1 , r-2 , and r - su4 , it is possible to notice that it follows very closely the values of r - su4 .",
    "these results suggest that r - su4 adequately reflects the scores of both r-1 and r-2 , capturing the concepts derived from both unigrams and bigrams .",
    "overall , considering plot summaries , documentaries achieved higher results in comparison with films .",
    "however , in general , the highest score for these two domains is achieved using films scripts against plot synopses .",
    "note that synopses have a significant difference in terms of sentences in comparison with plot summaries .",
    "the average synopsis has 120 sentences , while plot summaries have , on average , 5 sentences for films , and 4 for documentaries .",
    "this gives synopses a clear advantage in terms of rouge ( recall - based ) scores , due to the high count of words .",
    "we analyzed the impact of the six summarization algorithms on three datasets .",
    "the newspaper articles dataset was used as a reference .",
    "the other two datasets , consisting of films and documentaries , were evaluated against plot summaries , for films and documentaries , and synopses , for films . despite the different nature of these domains , the abstractive summaries created by humans , used for evaluation , share similar scores across metrics .",
    "the best performing algorithms are lsa , for news and documentaries , and lexrank for films .",
    "moreover , we conducted experiments combining scripts and subtitles for films , in order to assess the performance of generic algorithms by inclusion of redundant content .",
    "our results suggest that this combination is unfavorable . additionally , it is possible to observe that all algorithms behave similarly for both subtitles and scripts .",
    "as previously mentioned , the average of the scores follows closely the values of r - su4 , suggesting that r - su4 is able to capture concepts derived from both unigrams and bigrams .",
    "we plan to use subtitles as a starting point to perform video summaries of films and documentaries . for films ,",
    "the results from our experiments using plot summaries show that the summarization of scripts only marginally improved performance , in comparison with subtitles .",
    "this suggests that subtitles are a viable approach for text - driven film and documentary summarization .",
    "this positive aspect is compounded by their being broadly available , as opposed to scripts .",
    "this work was supported by national funds through fundao para a cincia e a tecnologia ( fct ) with reference uid / cec/50021/2013 .",
    "40 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , , , , , . , in : . , pp . .",
    ", , , . . ,",
    ", , . , in : , pp . .",
    ", , . , in : , pp . . , .",
    ", , , . , in : , pp . .",
    ", , , , . , in : , pp . . , , . , in : , pp .",
    ", , , , , , . , in : , pp . .",
    ", , , . . ,",
    ", in : , pp . . , , . , in : , pp . .",
    ", , . , . ,",
    ". . , . , , , , , . , in : , , , , , , , , ( eds . ) , , . , , , . , in : , .",
    ", in : , pp . . , , , , . , in : , pp . .",
    ", , , , , , , , , , . , in : , pp . .",
    ", , . , in : , pp . . , , , a. , in : , pp . .",
    ", , , b. , in : , pp . .",
    "ncleo interinstitucional de lingustica computacional ( nilc ) .",
    ", in : , pp . . , , , , . . , . , , ,",
    ". . , . , , , , , ,",
    ", , . , in : , pp . .",
    ", , . , in : , pp . .",
    ", , . , in : , pp .",
    ". , . . , . , , . , in : , pp . . , , ,",
    ", , , , . , in : , pp . ."
  ],
  "abstract_text": [
    "<S> we assess the performance of generic text summarization algorithms applied to films and documentaries , using extracts from news articles produced by reference models of extractive summarization . </S>",
    "<S> we use three datasets : ( i ) news articles , ( ii ) film scripts and subtitles , and ( iii ) documentary subtitles . </S>",
    "<S> standard rouge metrics are used for comparing generated summaries against news abstracts , plot summaries , and synopses . </S>",
    "<S> we show that the best performing algorithms are lsa , for news articles and documentaries , and lexrank and support sets , for films . despite the different nature of films and documentaries , </S>",
    "<S> their relative behavior is in accordance with that obtained for news articles .    </S>",
    "<S> automatic text summarization , generic summarization , summarization of films , summarization of documentaries </S>"
  ]
}