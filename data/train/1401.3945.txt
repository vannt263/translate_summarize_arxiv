{
  "article_text": [
    "a wireless acoustic sensor network ( wasn ) is a set of wireless microphones equipped with some communication , signal processing , and possibly memory units , which are randomly scattered in the environment .",
    "the communication unit allows a sensor node to communicate with a base station and also with other nodes , and the signal processing unit enables a node to perform local processing .",
    "the random structure of the network removes the array - size limitations imposed by classical microphone arrays , thereby providing better performance in sense of high - snr spatio - temporal sampling of the sound field and spatial diversity .",
    "the interested reader is referred to @xcite for a review of wasns .",
    "due to strict power and bandwidth constraints on wireless microphones , it might not be possible for nodes which are far from the base station to directly deliver their messages . a multi - hop scenario where the message goes along adjacent nodes until reaching",
    "the base - station is a commonplace alternative .",
    "we assume that a node is supposed to receive messages from multiple neighboring nodes and then combine them with its own measurement of the sound field into a new message to be forwarded through the network .",
    "this is illustrated in fig.[net ] .    in @xcite , this problem was solved for scalar gaussian sources in a non - distributed setting i.e. , without making use of the availability of the nodes measurements as side information .",
    "it was shown that coding at intermediate nodes can result in significant gains in terms of sum - rate or distortion . in this paper",
    ", we consider vector gaussian sources and also take into account the fact that messages received by a node are correlated with the node s measurement of the sound field .",
    "thus , we consider a distributed scenario , and make use of the destination node s measurement as side information to decrease the required rate for transmission to each node @xcite , @xcite,@xcite .",
    "we derive the rate - distortion ( rd ) function for an arbitrary node in the network with a distortion constraint defined in form of a covariance matrix , cf .",
    "@xcite , @xcite . in section",
    "ii , we introduce the notation and formulate the problem , which turns out to involve joint coding of multiple sources . in section iii ,",
    "we derive a conditional sufficient statistic for multiple gaussian sources and show that for the above - mentioned distributed source coding ( dsc ) problem , one can encode a conditional sufficient statistic instead of joint encoding of multiple sources .",
    "the rd function for the resulting problem will be derived in section iv . the paper is concluded in section v.",
    "we denote by @xmath0 and @xmath1 $ ] the information theoretic operations of mutual information , differential entropy , and expectation , respectively .",
    "probability density functions are denoted by @xmath2 and covariance and cross - covariance matrices are denoted by @xmath3 .",
    "we assume that all covariance matrices are of full rank .",
    "we denote markov chains by two - headed arrows ; e.g. @xmath4 .",
    "we assume that the source generates independent gaussian vectors of length @xmath5 denoted by @xmath6 . while the vectors are independent , the components of each vector are correlated .",
    "node @xmath7 makes a noisy measurement @xmath8 of the source given by :    @xmath9    where @xmath10 is the additive gaussian noise at node @xmath7 and the matrix @xmath11 models the mixing effect of the acoustic channel .",
    "the noise is assumed to be independent of @xmath12 .",
    "node @xmath7 also receives messages from nodes @xmath13 denoted by @xmath14 , from which it can make estimations @xmath15 of the source by decoding the messages with @xmath8 as the decoder side information .",
    "the problem is to find the minimum rate @xmath16 to jointly encode @xmath17 into a message @xmath18 to be sent to node @xmath19 , while satisfying the given distortion constraint @xmath20 and considering @xmath21 as side information .",
    "this is illustrated in figs.[net ] and [ 1node ] .",
    "note that one could further decrease the network sum - rate by taking into account the correlation between the messages which are sent to a common node .",
    "however , we leave out this possibility in this paper .    throughout this work , we assume that the acoustic mixing matrices are fixed and known .",
    "we also assume that joint statistics of the source and the noise variances are available",
    ". finally , although the model in ( [ measure ] ) is appropriate for acoustic networks , we do not consider real acoustic signals in this work .",
    "instead , we consider gaussian sources for simplicity of mathematical analysis .",
    "the case of real audio measurements is the focus of our future work .",
    "assume that @xmath22 is a random vector or a collection of random vectors with probability density function @xmath23 .",
    "[ define ] a sufficient statistic for the estimation of @xmath12 from @xmath22 is a function @xmath24 of @xmath22 for which @xmath25 is not a function of @xmath12 . in other words , @xmath26 .",
    "[ suff ] @xmath24 is a sufficient statistic of @xmath22 for estimating @xmath12 if and only if @xmath27 can be factorized as :    @xmath28    where @xmath29 ( depending on @xmath22 only ) and @xmath30 ( depending on @xmath12 and @xmath24 but not on @xmath22 directly ) are nonnegative functions .     of the network ]    the factorization theorem enables us to find a sufficient statistic for e.g. random vectors in gaussian noise as shown in the following lemma .",
    "[ gauss ] if @xmath31 are measurements of a random vector @xmath12 in mutually independent gaussian noises as in ( [ measure ] ) , then @xmath32 is a sufficient statistic of @xmath8 for estimating @xmath12 , where    @xmath33    and @xmath34 is the covariance matrix of the noise @xmath10 .",
    "we prove the lemma for @xmath35 .",
    "the proof for the general case is similar .",
    "since @xmath36 and @xmath37 are independent , the joint conditional density function of @xmath38 and @xmath39 can be written as :    @xmath40    where @xmath41 is independent of @xmath12 , @xmath38 and @xmath39 , and @xmath42 is defined as :    @xmath43    expanding @xmath42 , rearranging the terms , and substituting in ( [ joint ] ) , leads to    @xmath44    which shows that @xmath45 is a sufficient statistic according to the factorization theorem .    to make the notion of sufficient statistics applicable to our dsc problem",
    ", we need to introduce a conditional version :    [ define1 ] a conditional sufficient statistic for the estimation of @xmath12 from @xmath22 given @xmath46 is a function @xmath24 of @xmath22 for which @xmath47 is not a function of @xmath12 .",
    "[ gauss1 ] if @xmath48 , then any sufficient statistic @xmath24 of @xmath22 for estimating @xmath12 is also a conditional sufficient statistic of @xmath22 for estimating @xmath12 given @xmath46 .",
    "@xmath49    which is independent of @xmath12 .",
    "[ lem3 ] given side information @xmath46 at the decoder , there is no loss in terms of rate and distortion by encoding a conditional sufficient statistic of @xmath50 given @xmath46 instead of joint encoding of these sources .",
    "the proof follows along the lines of the proof for the unconditional case presented in @xcite , and is therefore omitted .    combining the results of this section",
    ", we have the following theorem for the problem formulated in section ii :    [ th1 ] given side information @xmath21 at the decoder , the rd function for the problem of joint encoding of multiple sources @xmath17 coincides with the rd function for the dsc problem of encoding the single source :    @xmath51    where @xmath52 is the distortion matrix for node @xmath53 .    using the backward channel model we have @xmath54 where the covariance matrix of @xmath55 is @xmath52 .",
    "this means that @xmath56 can be written as :    @xmath57    where    @xmath58    substituting ( [ xhat1 ] ) and ( [ xhat2 ] ) in ( [ xhat ] ) and using ( [ gauss_suff ] ) yields ( [ t ] ) .",
    "since @xmath59 is a sufficient statistic of @xmath60 and @xmath61 , it follows from lemma [ gauss1 ] that it is also a conditional sufficient statistic given @xmath62 . from lemma [ lem3 ] ,",
    "one can then replace @xmath60 by @xmath59 in ( [ t ] ) and get the same rd function .    at this point",
    ", we have shown that the above problem with multiple sources can be converted into a single source dsc problem for gaussian sources with covariance distortion constraint .",
    "this problem is illustrated in fig.[prob ] .",
    "for the case of mean - squared error distortion constraint , the rd function was found in @xcite , while the case of covariance distortion was not treated in that work . in the next section ,",
    "we derive the rd function for the covariance distortion constraint under some mild technical assumptions .",
    "for the sake of simplicity of derivations , we write @xmath12 and @xmath59 in terms of their linear estimations based on the known gaussian vectors in fig.[prob ] . in particular , we have that    @xmath63    where @xmath64 are estimation errors with covariance matrices @xmath65 , respectively , and @xmath66 and @xmath67 depend only on the covariance and cross - covariance matrices of @xmath68 and @xmath69 .",
    "( see appendix for the mathematical statement for @xmath70 as an example . )",
    "we will show that if the mixing matrices @xmath11 in ( [ measure ] ) are invertible , then for @xmath71 the rd function for node @xmath7 is given by :    @xmath72          first we need the following lemma :    [ inv ] if the mixing matrices @xmath73 , are invertible , the matrix @xmath74 in ( [ xty ] ) is also invertible .",
    "see the appendix .    from the first equality in ( [ xty ] ) we have :    @xmath75    from the second equaltiy in ( [ xty ] ) we can write :    @xmath76    assume now that the sequence of independent vectors @xmath77 generated by the source is encoded in block vectors @xmath78 each containing @xmath79 vectors . from the distortion constraint and ( [ new ] ) we can write :    @xmath80 }   = { \\bf{c}}{{\\bf{\\sigma } } _ { { { \\bf{\\upsilon } } _ 1}}}{{\\bf{c}}^t } + { { \\bf{\\sigma } } _ { { { \\bf{\\upsilon } } _ 2}}}\\ ] ]    or equivalently :    @xmath81    denoting the blocks of @xmath79 vectors @xmath82 by @xmath83 , respectively , we have :    @xmath84    where the block vector @xmath85 contains @xmath86 vectors @xmath87 , ( [ lb5 ] ) is because conditioning reduces the entropy , ( [ lb3 ] ) is because @xmath74 is invertible , and ( [ lb4 ] ) is the result of applying ( [ lb1 ] ) and ( [ lb2 ] ) to ( [ lb3 ] ) .",
    "the lower bound derived in the previous part can be used as a guideline for the best possible performance for any coding scheme .",
    "the question is then , given the source @xmath59 , how to encode it into a discrete source @xmath69 , so that for the given distortion constraint , the required rate for transmission of @xmath69 achieves the lower bound ?",
    "let us assume that @xmath59 is quantized to @xmath69 , in a way that satisfies the distortion constraint . from the results of dsc , due to the availability of the side information",
    "@xmath88 at the decoder , it is possible to noiselessly encode @xmath69 in blocks of sufficiently large length @xmath79 with a rate arbitrarily close to @xmath89 @xcite , @xcite , @xcite .",
    "the remaining task is to design a scheme for which @xmath90 achieves ( [ rd ] ) .",
    "this is possible by using the following scheme :    @xmath91    where we have denoted the eigenvalue decomposition of @xmath92 by @xmath93 , and the covariance @xmath94 of the coding noise @xmath95 is defined as :    @xmath96    to verify this , one can write @xmath90 as @xmath97 , substitute ( [ scheme])([scheme1 ] ) , and utilize ( [ ty ] ) and ( [ lb1 ] ) .",
    "it is worth noting that the rd function ( [ rd ] ) generalizes the rd function of @xcite , which treated the scalar case .",
    "we showed that the rate - distortion function for a distributed source coding problem with multiple sources at the encoder is identical to the rate - distortion function for the distributed encoding of a so - called conditional sufficient statistic of the sources .",
    "we derived a conditional sufficient statistic for the case that additive noises are gaussian and mutually independent , and calculated the rate - distortion function in case that the sources are vector gaussian and the distortion constraint is defined as a covariance matrix . since",
    "vector sources were considered in order to take the memory into account , and the covariance constraint on the distortion is a more flexible fidelity criterion compared to mean - squared error , these results can be applied to the problem of source coding for audio signals in presence of reverberation , which will be the scope of our future work .",
    "where the covariance matrix of the estimation error @xmath99 is @xmath100 , and @xmath22 and @xmath46 are related to @xmath12 via @xmath101 and @xmath102 , and @xmath103 and @xmath104 are invertible .",
    "therefore we have :            @xmath108    { { \\bf{\\sigma } } _ { yz}}{{\\bf{\\delta } } ^ { - 1}}{\\bf{\\sigma } } _ { yz}^t \\nonumber \\\\ \\label{ap3 }   & & = { { \\bf{\\sigma } } _ x}{{\\bf{a}}^t}\\left ( { { \\bf{i } } + { { \\bf{h}}^ { - 1}}{{\\bf{\\sigma } } _ { yz}}{{\\bf{\\delta } } ^ { - 1}}{\\bf{\\sigma } } _ { yz}^t } \\right),\\end{aligned}\\ ] ]          @xmath111{{\\bf{b}}^t } + { { \\bf{\\sigma } } _ { { n_2 } } } \\nonumber \\\\   & & = { \\bf{b}}{{\\bf{\\sigma } } _ x}{{\\bf{a}}^t}\\left [ { { { \\bf{a}}^ { - t}}{\\bf{\\sigma } } _ x^ { - 1}{{\\bf{a}}^ { - 1 } } - { { \\left ( { { \\bf{a}}{{\\bf{\\sigma } } _ x}{{\\bf{a}}^t } + { { \\bf{\\sigma } } _ { { n_1 } } } } \\right)}^ { - 1 } } } \\right ]   { \\bf{a}}{{\\bf{\\sigma } } _ x}{{\\bf{b}}^t } + { { \\bf{\\sigma } } _ { { n_2 } } } \\nonumber \\\\ \\label{ap4 }   & & = { \\bf{\\sigma } } _ { yz}^t\\left ( { - { { \\bf{h}}^ { - 1 } } } \\right){{\\bf{\\sigma } } _ { yz } } + { { \\bf{\\sigma } } _ { { n_2}}}.\\end{aligned}\\ ] ]                                  c. tian and j. chen , _ remote vector gaussian source coding with decoder side information under mutual information and distortion constraints _ , ieee transactions on information theory , vol .",
    "10 , pp.4676 - 4680 , oct . 2009 .",
    "s. c. draper and g. w. wornell , _ side information aware coding strategies for estimation under communication constraints _ , ieee journal on selected areas in communications , vol .",
    "22 , no . 6 , pp . 1 - 11 , aug ."
  ],
  "abstract_text": [
    "<S> in this paper , we consider the problem of remote vector gaussian source coding for a wireless acoustic sensor network . </S>",
    "<S> each node receives messages from multiple nodes in the network and decodes these messages using its own measurement of the sound field as side information . the node s measurement and the estimates of the source resulting from decoding the received messages </S>",
    "<S> are then jointly encoded and transmitted to a neighboring node in the network . </S>",
    "<S> we show that for this distributed source coding scenario , one can encode a so - called conditional sufficient statistic of the sources instead of jointly encoding multiple sources . </S>",
    "<S> we focus on the case where node measurements are in form of noisy linearly mixed combinations of the sources and the acoustic channel mixing matrices are invertible . for this problem </S>",
    "<S> , we derive the rate - distortion function for vector gaussian sources and under covariance distortion constraints . </S>"
  ]
}