{
  "article_text": [
    "in this section we explain in detail proposed hashing mechanism for initial dimensionality reduction that is used to preprocess data before it is given as an input to the autoencoder . as mentioned earlier , the mechanism is of its own interest .",
    "we introduce first the aforementioned family of @xmath0-regular matrices @xmath1 that is a key ingredient of the method .",
    "assume that @xmath2 is the size of the hash and @xmath3 is the dimensionality of the data .",
    "let @xmath4 be the size of the pool of independent random gaussian variables @xmath5 , where each @xmath6 .",
    "assume that @xmath7 .",
    "we say that a random matrix @xmath1 is @xmath0-regular if @xmath1 is of the form :    @xmath8    where @xmath9 for @xmath10 , @xmath11 , @xmath12 for @xmath13 , @xmath14 for @xmath10 , @xmath15 , @xmath16 and furthermore the following holds :    * for every column of @xmath1 every @xmath17 appears in at most @xmath18 entries from that column .",
    "notice that all structured matrices that we mentioned in the abstract are special cases of the @xmath19-regular matrix .",
    "indeed , each toeplitz matrix is clearly @xmath19-regular , where subsets @xmath20 are singletons .",
    "let @xmath21 be a function satisfying @xmath22 and @xmath23 .",
    "we will consider two hashing methods .",
    "the first one , called by us _ extended @xmath0-regular hashing _ , applies first random diagonal matrix @xmath24 to the datapoint @xmath25 , then the @xmath26-normalized hadamard matrix @xmath27 , next another random diagonal matrix @xmath28 , then the @xmath0-regular projection matrix @xmath29 and finally function @xmath21 ( the latter one applied pointwise ) .",
    "the overal scheme is presented below : @xmath30 the diagonal entries of matrices @xmath24 and @xmath28 are chosen independently from the binary set @xmath31 , each value being chosen with probability @xmath32 .",
    "we also propose a shorter pipeline , called by us _",
    "short @xmath0-regular hashing _ , where we avoid applying first random matrix and hadamard matrix @xmath24 and the hadamard matrix , i.e. the overall pipeline is of the form : @xmath33 the goal is to compute good approximation of the angular distance between given @xmath26-normalized vectors @xmath34 , given their compact hashed versions : @xmath35 . to achieve this goal we consider the @xmath36-distance in the @xmath2-dimensional space of hashes .",
    "let @xmath37 denote the angle between vectors @xmath38 and @xmath39 .",
    "we define the _ normalized approximate angle between @xmath38 and @xmath39 _ as : @xmath40 in the next section we will show that the normalized approximate angle between vectors @xmath38 and @xmath39 is a very precise estimation of the actual angle if the chosen parameter @xmath0 is not large enough . furthermore , we show an intriguing connection between theoretical guarantess regarding the quality of the produced hash and the chromatic number of some specific undirected graph encoding the structure of @xmath1 . for many of the structured matrices under consideration",
    "this graph is induced by an algebraic group operation defining the structure of @xmath1 ( for istance , for the circular matrix the group is a single shift and the underlying graph is a collection of pairwise disjoint cycles and trees thus its chromatic number is at most @xmath41 ) .",
    "we are ready to provide theoretical guarantees regarding the quality of the produced hash .",
    "our guarantees will be given for a _ sign _ function , i.e for @xmath21 defined as : @xmath42 for @xmath43 , @xmath44 for @xmath45 .",
    "however we should emphasize that empirical results showed that other functions ( that are often used as nonlinear maps in deep neural networks ) such as sigmoid function , also work well .",
    "it is not hard to show that @xmath46 is an unbiased estimator of @xmath47 , i.e. @xmath48 .",
    "what we will focus on is the concentration of the random variable @xmath46 around its mean @xmath47",
    ". we will prove strong exponential concentration results regarding the extended @xmath0-regular hashing method .",
    "interestingly , the application of the hadamard mechanism is not necessary and it is possible to get concentration results , yet weaker than in the former case , also for short @xmath0-regular hashing . as a warm up , let us prove the following .",
    "[ mean_lemma ] let @xmath49 be a @xmath0-regular hashing model ( either extended or short ) .",
    "then @xmath46 is an unbiased estimator of @xmath37 , i.e. @xmath50    notice first that the @xmath51th row , call it @xmath52 , of the matrix @xmath1 is a @xmath3-dimensional gaussian vector with mean @xmath19 and where each element has standard deviation @xmath53 for @xmath54 ( @xmath13 ) .",
    "thus , after applying matrix @xmath28 the new vector @xmath55 is still gaussian and of the same distribution .",
    "let us consider first the short @xmath0-regular hashing model .",
    "fix some @xmath26-normalized vectors @xmath34 ( without loss of generality we may assume that they are not collinear ) and denote by @xmath56 the @xmath57-dimensional hyperplane spanned by @xmath58 .",
    "denote by @xmath59 the projection of @xmath55 into @xmath60 and by @xmath61 the line in @xmath60 perpendicular to @xmath59 .",
    "let @xmath21 be a _ sign _ function .",
    "notice that the contribution to the @xmath36-sum @xmath62 comes from those @xmath52 for which @xmath61 divides an angel between @xmath38 and @xmath39 , i.e. from those @xmath52 for which @xmath59 is inside the union @xmath63 of two @xmath57-dimensional cones bounded by two lines in @xmath60 perpendicular to @xmath38 and @xmath39 respectively .",
    "observe that , from what we have just said , we can conclude that @xmath64 , where : @xmath65    now it suffices to notice that vector @xmath59 is a gaussian random variable and thus its direction is uniformly distributed over all directions .",
    "thus each @xmath66 is nonzero with probability exactly @xmath67 and the theorem follows .",
    "for the extended @xmath0-regular hashing model the analysis is very similar .",
    "the only difference is that data is preprocessed by applying @xmath68 linear mapping first .",
    "both @xmath27 and @xmath24 are matrices of rotations though , thus their product is also a rotation matrix . since rotations do not change angular distance , the former analysis can be applied again and yields the proof .      as we have already mentioned , the highly well organized structure of the projection matrix @xmath1 gives rise to the underlying undirected graph that encodes dependencies between different entries of @xmath1 .",
    "more formally , let us fix two rows of @xmath1 of indices @xmath69 .",
    "we define a graph @xmath70 as follows :    * @xmath71 , * there exists an edge between vertices @xmath72 and @xmath73 iff @xmath74 .",
    "the chromatic number @xmath75 of the graph @xmath76 is the minimal number of colors that can be used to color the vertices of the graph in such a way that no two adjacent vertices have the same color .",
    "let @xmath1 be a @xmath0-regular matrix .",
    "we define the @xmath1-chromatic number @xmath77 as : @xmath78      we present now our main theoretical results .",
    "let us consider first the extended @xmath0-regular hashing model .",
    "the following is true .",
    "[ ext_technical_theorem ] take the extended @xmath0-regular hashing model @xmath49 with @xmath4 independent gaussian random variables : @xmath79 , each of distribution @xmath80 .",
    "let @xmath81 be the size of the dataset .",
    "denote by @xmath2 the size of the hash and by @xmath3 the dimensionality of the data .",
    "let @xmath82 be arbitrary positive function .",
    "let @xmath83 be two fixed vectors @xmath84 with angular distance @xmath37 between them .",
    "then for every @xmath85 the following is true : @xmath86 where @xmath87 and @xmath88 .",
    "notice how the upper bound on the probability of failure @xmath89 depends on the @xmath1-chromatic number .",
    "the theorem above guarantees strong concentration of @xmath90 around its mean and therefore justifies theoretically the effectiveness of the structured hashing method .",
    "it becomes more clearly below .    as a corollary",
    ", we obtain the following result :    [ ext_theorem ] take the extended @xmath0-regular hashing model @xmath49 with .",
    "assume that the projection matrix @xmath1 is toeplitz .",
    "let @xmath81 be the size of the dataset .",
    "denote by @xmath2 the size of the hash and by @xmath3 the dimensionality of the data .",
    "let @xmath82 be an arbitrary positive function .",
    "let @xmath83 be two vectors @xmath84 with angular distance @xmath37 between them .",
    "then for every @xmath91 the following is true : @xmath92    theorem [ ext_theorem ] follows from theorem [ ext_technical_theorem ] by taking : @xmath93 , @xmath94 , @xmath95 and noticing that every toeplitz matrix is @xmath19-regular and the corresponding @xmath1-chromatic number @xmath77 is at most @xmath41 .",
    "let us switch now to the short @xmath0-regular hashing model .",
    "the theorem presented below is the application of the chebyshev s inequality preceded by the careful analysis of the variance @xmath96 .",
    "[ short_theorem ] take the short @xmath0-regular hashing model @xmath49 , where @xmath1 is a toeplitz matrix .",
    "let @xmath81 be the size of the dataset .",
    "denote by @xmath2 the size of the hash and by @xmath3 the dimensionality of the data .",
    "let @xmath83 be two vectors @xmath84 with angular distance @xmath37 between them .",
    "then the following is true for any @xmath97 : @xmath98    the proofs of theorem [ ext_technical_theorem ] and theorem [ short_theorem ] will be given in the appendix .",
    "in this section we prove theorem [ ext_technical_theorem ] and theorem [ short_theorem ] .",
    "we will use notation from lemma [ mean_lemma ] .",
    "[ first_lemma ] let @xmath99 be the set of @xmath2 independent random variables defined on @xmath100 such that each @xmath101 has the same distribution and @xmath102 .",
    "let @xmath103 be the set of events , where each @xmath104 is in the @xmath105-field defined by @xmath101 ( in particular @xmath104 does not depend on the @xmath106 @xmath107 ) .",
    "assume that there exists @xmath108 such that : @xmath109 for @xmath13 .",
    "let @xmath110 be the set of @xmath2 random variables such that @xmath111 and @xmath112 for @xmath13 , where @xmath113 stands for the random variable @xmath114 truncated to the event @xmath115 .",
    "assume furthermore that @xmath116 for @xmath13 .",
    "denote @xmath117 .",
    "then the following is true .",
    "@xmath118                                [ hadamard_lemma ]",
    "let @xmath3 denote data dimensionality and let @xmath82 be an arbitrary positive function .",
    "let @xmath145 be the set of all @xmath26-normalized datapoints , where no two datapoints are identical .",
    "assume that @xmath146 .",
    "consider the @xmath147 hyperplanes @xmath56 spanned by pairs of different vectors @xmath58 from @xmath145 .",
    "then after applying linear transformation @xmath68 each hyperplane @xmath56 is transformed into another hyperplane @xmath148 .",
    "furthermore , the probability @xmath149that for every @xmath148 there exist two orthonormal vectors @xmath150 in @xmath148 such that : @xmath151 satisfies : @xmath152    we have already noticed in the proof of lemma [ mean_lemma ] that @xmath68 is a matrix of the rotation transformation . thus , as an isometry , it clearly transforms each @xmath57-dimensional hyperplane into another @xmath57-dimensional hyperplane . for every pair @xmath58",
    "let us consider an arbitrary fixed orthonormal pair @xmath153 spanning @xmath56 .",
    "denote @xmath154 .",
    "let us denote by @xmath155 vector obtained from @xmath156 after applying transformation @xmath68 .",
    "notice that the @xmath157 coordinate of @xmath155 is of the form : @xmath158 where @xmath159 are independent random variables satisfying :            similar analysis is correct for @xmath164 .",
    "notice that @xmath164 is orthogonal to @xmath155 since @xmath165 and @xmath156 are orthogonal .",
    "furthermore , both @xmath164 and @xmath155 are @xmath26-normalized .",
    "thus @xmath166 is an orthonormal pair .      from the lemma",
    "above we see that applying hadamard matrix enables us to assume with high probability that for every hyperplane @xmath56 there exists an orthonormal basis consisting of vectors with elements of absolute values at most @xmath168 .",
    "we call this event @xmath169 . notice that whether @xmath169 holds or not is determined only by @xmath27 , @xmath24 and the initial dataset @xmath145 .",
    "let us proceed with the proof of theorem [ ext_technical_theorem ] .",
    "let us assume that event @xmath169 holds . without loss of generality we may assume that we have the short @xmath0-regular hashing mechanism with an extra property that every @xmath56 has an orthonormal basis consisting of vectors with elements of absolute value at most @xmath168 .",
    "fix two vectors @xmath34 from the dataset @xmath145 .",
    "denote by @xmath170 the orthonormal basis of @xmath56 with the above property .",
    "let us fix the @xmath51th row of @xmath1 and denote it as @xmath171 .",
    "after being multiplied by the diagonal matrix @xmath28 we obtain another vector : @xmath172        we have already noticed that in the proof of lemma [ mean_lemma ] that it is the projection of @xmath174 into @xmath56 that determines whether the value of the associated random variable @xmath66 is @xmath19 or @xmath175 . to be more specific , we showed that @xmath176 iff the projection is in the region @xmath63 .",
    "let us write down the coordinates of the projection of @xmath174 into @xmath56 in the @xmath170-coordinate system .",
    "the coordinates are the dot - products of @xmath174 with @xmath25 and @xmath177 respectively thus in the @xmath170-coordinate system we can write @xmath174 as : @xmath178    notice that both coordinates are gaussian random variables and they are independent since they were constructed by projecting a gaussian vector into two orthogonal vectors . now notice that from our assumption about the structure of @xmath1 we can conclude that both coordinates may be represented as sums of weighted gaussian random variables @xmath179 for @xmath180 , i.e. : @xmath181    where each @xmath182 is of the form @xmath183 or @xmath184 for some @xmath185 that depends only on @xmath186 .",
    "notice also that @xmath187 the latter inequality comes from the fact that , by [ coord_eq ] , both coordinates of @xmath188 have the same distribution .      [ small_dot_product_lemma ]",
    "let us assume that @xmath169 holds .",
    "let @xmath82 be an arbitrary positive function .",
    "then for every @xmath162 with probability at least @xmath192 , taken under coin tosses used to construct @xmath28 , the following is true for every @xmath193 : [ pseudo_ortho_lemma ] @xmath194 @xmath195",
    "@xmath196 @xmath197    notice that the we get the first inequality for free from the fact that @xmath25 is orthogonal to @xmath177 ( in other words , @xmath198 can be represented as @xmath199 and the latter expression is clearly @xmath19 ) .",
    "let us consider now one of the three remaining expressions .",
    "notice that they can be rewritten as : @xmath200 or @xmath201 or @xmath202 for some @xmath203 .",
    "notice also that from the @xmath0-regularity condition we immediately obtain that @xmath204 for at most @xmath0 elements of each sum .",
    "get rid of these elements from each sum and consider the remaining ones . from the definition of the @xmath1-chromatic number , those remaining ones can be partitioned into at most @xmath77 parts , each consisting of elements that are independent random variables ( since in the corresponding graph there are no edges between them ) .",
    "thus , for the sum corresponding to each part one can apply lemma [ azuma_general ] .",
    "thus one can conclude that the sum differs from its expectation ( which clearly is zero since @xmath205 for @xmath206 ) by a with probability at most @xmath207 or @xmath208 or @xmath209    now it is time to use the fact that event @xmath169 holds .",
    "then we know that : @xmath151 for @xmath210 . substituting this upper bound for @xmath211 in the derived expressions on the probabilities coming from lemma [ azuma_general ] , and then taking the union bound , we complete the proof .",
    "we can finish the proof of theorem [ ext_technical_theorem ] .",
    "from lemma [ small_dot_product_lemma ] we see that + @xmath191 are close to pairwise orthogonal with high probability .",
    "let us fix some positive function @xmath212 and some @xmath162 .",
    "denote        let us consider again @xmath217 .",
    "replacing @xmath218 by @xmath219 and @xmath220 by @xmath221 in the formula on @xmath217 , we obtain another gaussian vector : @xmath222 for each row @xmath51 of the matrix @xmath1 .",
    "notice however that vectors @xmath222 have one crucial advantage over vectors @xmath217 , namely they are independent .",
    "that comes from the fact that @xmath223,@xmath224 are pairwise orthogonal .",
    "notice also that from [ ineq1 ] and [ ineq2 ] we obtain that the angular distance between @xmath217 and @xmath222 is at most @xmath225 .",
    "let @xmath101 for @xmath226 be an indicator random variable that is zero if @xmath222 is inside the region @xmath63 and zero otherwise .",
    "let @xmath227 for @xmath226 be an indicator random variable that is zero if @xmath217 is inside the region @xmath63 and zero otherwise .",
    "notice that @xmath228 .",
    "furthermore , random variables @xmath229 satisfy the assumptions of lemma [ first_lemma ] with @xmath230 , where @xmath231 .",
    "indeed , random variables @xmath101 are independent since vectors @xmath222 are independent . from what we have said so far we know that each of them takes value one with probability exactly @xmath67 .",
    "furthermore @xmath232 only if @xmath217 is inside @xmath63 and @xmath222 is outside @xmath63 or vice versa .",
    "the latter event implies ( thus it is included in the event ) that @xmath217 is near the border of the region @xmath63 , namely within an angular distance @xmath233 from one of the four semilines defining @xmath63 .",
    "thus in particular an event @xmath232 is contained in the event of probability at most @xmath234 that depends only on one @xmath217 .",
    "but then we can apply lemma [ first_lemma ] .",
    "all we need is to assume that the premises of lemma [ small_dot_product_lemma ] are satisfied .",
    "but this is the case with probability specified in lemma [ hadamard_lemma ] and this probability is taken under random coin tosses used to product @xmath27 and @xmath24 , thus independently from the random coin tosses used to produce @xmath28 . putting it all together we obtain the statement of theorem [ ext_technical_theorem ] .",
    "[ variance_lemma ] define @xmath235 as in the proof of theorem [ ext_technical_theorem ] .",
    "assume that the following is true : @xmath236 @xmath237 @xmath238 @xmath239 for some @xmath240 .",
    "the the following is true for every fixed @xmath241 : @xmath242                      it suffices to estimate parameter @xmath249 .",
    "we proceed as in the previous proof .",
    "we only need to be a little bit more cautious since the condition : @xmath151 can not be assumed right now .",
    "we select two rows : @xmath250 of @xmath1 .",
    "notice that , again we see that applying gram - schmidt process we can obtain a system of pairwise orthogonal vectors @xmath251 such that @xmath252 and @xmath253    the fact that right now the above upper bounda are not multiplied by @xmath2 , as it was the case in the previous proof , plays key role in obtaining nontrivial concentration results even when no hadamard mechanism is applied .",
    "we consider the related sums : + @xmath254 as before .",
    "we can again partition each sum into at most @xmath77 subchunks , where this time @xmath255 ( since @xmath1 is toeplitz ) .",
    "the problem is that applying lemma [ azuma_general ] , we get bounds that depend on the expressions of the form @xmath256 and @xmath257 where indices are added modulo @xmath3 and this time we can not assume that all @xmath211 are small . fortunately we have : @xmath258 and @xmath259    let us fix some positive function @xmath260 .",
    "we can conclude that the number of variables @xmath261 such that @xmath262 is at most @xmath263 .",
    "notice that each such @xmath261 and each such @xmath264 corresponds to a pair @xmath265 of rows of the matrix @xmath1 and consequently to the unique element @xmath266 of the entire covariance sum ( scaled by @xmath267 ) .",
    "since trivially we have @xmath268 , we conclude that the contribution of these elements to the entire covariance sum is of order @xmath269 .",
    "let us now consider these @xmath261 and @xmath264 that are at most @xmath270 .",
    "these sums are small ( if we take @xmath271 ) and thus it makes sense to apply lemma [ azuma_general ] to them . that gives us upper bound @xmath272 with probability : @xmath273 taking @xmath274 and @xmath275 , we conclude that : @xmath276 thus , from the chebyshev s inequality , we get the following for every @xmath97 and fixed points @xmath34 : @xmath98 that completes the proof ."
  ],
  "abstract_text": [
    "<S> we present here new mechanisms for hashing data via binary embeddings . contrary to most of the techniques presented before , the embedding matrix of our mechanism is highly structured . </S>",
    "<S> that enables us to perform hashing more efficiently and use less memory . </S>",
    "<S> what is crucial and nonintuitive is the fact that imposing structured mechanism does not affect the quality of the produced hash . to the best of our knowledge , we are the first to give strong theoretical guarantees of the proposed binary hashing method by proving the efficiency of the mechanism for several classes of structured projection matrices . as a corollary </S>",
    "<S> , we obtain binary hashing mechanisms with strong concentration results for circulant and topelitz matrices . </S>",
    "<S> our approach is however much more general . </S>"
  ]
}