{
  "article_text": [
    "we investigate the shrinkage properties of the partial least squares ( pls ) regression estimator .",
    "it is known ( e.g. @xcite ) that we can express the pls estimator obtained after @xmath2 steps in the following way : @xmath3 where @xmath4 is the component of the ordinary least squares ( ols ) estimator along the @xmath5th principal component of the covariance matrix @xmath6 and @xmath7 is the corresponding eigenvalue .",
    "the quantities @xmath8 are called shrinkage factors . we show that these factors are determined by a tridiagonal matrix ( which depends on the input  output matrix @xmath9 ) and can be calculated in a recursive way .",
    "combining the results of @xcite and @xcite , we give a simpler and clearer proof of the shape of the shrinkage factors of pls and derive some of their properties .",
    "in particular , we show that some of the values @xmath8 are greater than @xmath1 ( this was first proved in @xcite ) .",
    "+ we argue that these `` peculiar shrinkage properties '' @xcite do not necessarily imply that the mean squared error ( mse ) of the pls estimator is worse compared to the mse of the ols estimator : in the case of deterministic shrinkage factors , i.e. factors that do not depend on the output @xmath10 , any value @xmath11 is of course undesirable .",
    "but in the case of pls , the shrinkage factors are stochastic  they also depend on @xmath12 . even if latexmath:[$p\\left ( \\left    that the mse is worse than the mse of the ols estimator . in particular , bounding the absolute value of the shrinkage factor by @xmath1 does not automatically yield a lower mse , in disagreement to what was conjectured in e.g. @xcite .",
    "+ having issued this warning , we explore whether bounding the shrinkage factors leads to a lower mse or not .",
    "it is very difficult to derive theoretical results , as the quantities of interest - @xmath14 and @xmath8 respectively - depend on @xmath10 in a complicated , nonlinear way . as a substitute , we study the problem on several artificial data sets and one real world example .",
    "it turns out that in most cases the mse of the bounded version of pls is indeed smaller than the one of pls , although the improvement is tiny .",
    "+ the paper is organized as follows : in section [ pre ] we introduce the notation and in section [ kspaces ] we recall some propertie of krylov spaces . in section [ pls ]",
    "we define the pls estimator and in section [ sectiontri ] we provide mathematical results that are needed in the rest of the paper . after explaining the notion of shrinkage in section [ shrinkage ] we derive the formulas for the pls shrinkage factors in section [ shrinkpls ] and derive some of their properties . in section [ simulation ]",
    ", we report on the results of the experiments .",
    "the paper ends with a conclusion .",
    "we consider the multivariate linear regression model @xmath15 with @xmath16 the numbers of variables is @xmath17 , the number of examples is @xmath18 . for simplicity , we assume that @xmath19 and @xmath10 are scaled to have zero mean , so we do not have to worry about intercepts .",
    "we have @xmath20 we set @xmath21 .",
    "the singular value decomposition of @xmath19 is of the form @xmath22 with @xmath23 we have @xmath24 and @xmath25 . + set @xmath26 .",
    "the eigendecomposition of @xmath27 is @xmath28    the eigenvalues @xmath7 of @xmath27 ( and any other matrix ) are ordered in the following way : @xmath29 the moore - penrose inverse of a matrix @xmath30 is denoted by @xmath31 .",
    "+ the ordinary least squares ( ols ) estimator @xmath32 is the solution of the optimization problem @xmath33 set @xmath34 the ols estimator is given by the formula @xmath35 set @xmath36    finally , we need a result on the shape of the moore - penrose inverse of a symmetric matrix .",
    "[ penrose ] let @xmath37 be a symmetric matrix with eigendecomposition @xmath38 with eigenvalues @xmath7 . set @xmath39 as @xmath40 we can write @xmath41 then @xmath42    the four properties that we have to check are    1 .",
    "@xmath43 , 2 .",
    "@xmath44 , 3 .",
    "@xmath45 , 4 .",
    "@xmath46 .",
    "as @xmath47 is symmetric , the polynomial @xmath48 is symmetric as well , which proves the first two conditions .",
    "next note that it suffices to prove the and properties @xmath49 and @xmath50 for the diagonal matrix @xmath51 with @xmath52 .",
    "this is true as @xmath53 we have @xmath54 the third property of the moore - penrose inverse is @xmath55 which is equivalent to @xmath56 which is obviously true .",
    "the fourth property follows as easily .",
    "the degree of the polynomial @xmath57 is @xmath58 .",
    "the proposition is valid no matter if we count the non - zero eigenvalues with or without multiplicities .",
    "we count the eigenvalues with multiplicities in order to connect the polynomial to the characteristical polynomial in the regular case : if @xmath47 is a regular matrix , @xmath57 is linked to the characterictical polynomial @xmath59 in the following way : @xmath60",
    "set @xmath61 the columns of @xmath62 are called the krylov sequence of @xmath27 and @xmath63 .",
    "+ the space spanned by the columns of @xmath62 is called the krylov space of @xmath27 and @xmath63 and denoted by @xmath64 .",
    "we recall some basic facts on the dimension of the krylov space that are needed in the rest of the paper .",
    "set @xmath65 ( the vector @xmath66 is defined in ( [ te ] ) ) and @xmath67    we have @xmath68    suppose that @xmath69 for some @xmath70 . using the eigendecompostion of @xmath27 this equation is equivalent to @xmath71 as @xmath72 is an invertible matrix , this is equivalent to @xmath73 for @xmath74 .",
    "hence , each element @xmath75 is a zero of the polynomial @xmath76 this is a polynomial of degree @xmath77 .",
    "as it has @xmath78 different zeroes , it must be trivial , i.e. @xmath79 .",
    "if @xmath80 we have @xmath81 .",
    "it is clear that @xmath82 as @xmath83 .",
    "assume that there is a set @xmath84 of @xmath85 linear independent vectors in the krylov sequence @xmath62 .",
    "set @xmath86 hence @xmath87 . the condition that @xmath84 is linear independent is equivalent to the following : there is no nontrivial polynomial @xmath88 such that @xmath89 for @xmath75 . as the polynomial @xmath90 is of degree @xmath91 and @xmath92 ,",
    "there is always a nontrivial solution of equation ( [ dep ] ) .",
    "we sum up the two results :    [ dimkm ] we have @xmath93 in particular @xmath94",
    "it is not our aim to give an introduction to the partial least squares ( pls ) method and refer to @xcite .",
    "we take a purely algebraic point of view as in @xcite .",
    "the @xmath95 estimator @xmath96 is the solution of the constrained minimization problem @xmath97 we call @xmath2 the number of steps of pls .",
    "it follows that any solution of this problem is of the form @xmath98 where @xmath99 is the solution of the unconstrained problem @xmath100    plugging this into the formula for the ols estimator ( cf .",
    "section [ pre ] ) we get    the pls estimator obtained after @xmath2 steps can be expressed in the following way : @xmath101^{- }   \\left(k^{(m)}\\right)^t b\\,.\\end{aligned}\\ ] ]    it should be clear that we can replace the matrix @xmath62 in equation ( [ bhat ] ) by any matrix @xmath102 , as long as its columns span the space @xmath64 .",
    "in fact , in the nipals algorithm ( see @xcite ) , an orthogonal basis of @xmath64 is calculated with the help of the gram - schmidt procedure .",
    "denote by @xmath103 this orthogonal basis of @xmath64 .",
    "of course , this basis only exists if @xmath104 , which might not be true for all @xmath105 . the maximal number for which this holds is @xmath106 ( see proposition [ dimkm ] ) .",
    "note however that @xmath107 ( see ( [ chainkm ] ) ) and the solution of the optimization problem does not change anymore .",
    "hence for the rest of the paper , we make the assumption that @xmath108    we have @xmath109    we show that @xmath110 . by definition @xmath111 with @xmath112 ( recall that @xmath113 is the rank of @xmath27 ) . on the other hand ,",
    "any vector @xmath114 lies in @xmath115 if and only if there is a polynomial @xmath90 of degree @xmath116 such that @xmath117 it follows that @xmath118 . as @xmath119",
    "we have @xmath120 .",
    "set @xmath121 where @xmath102 is as defined in equation ( [ gram ] ) .",
    "[ tri ] the matrix @xmath122 is symmetric and positive semidefinite .",
    "furthermore @xmath122 is tridiagonal , i.e @xmath123 for @xmath124 .",
    "the first two statements are obvious .",
    "let @xmath125 .",
    "as @xmath126 , the vector @xmath127 lies in the subspace @xmath128 . as @xmath129 , the vector @xmath130 is orthogonal on @xmath128 , in other words @xmath131 as @xmath122 is symmetric , we also have @xmath123 which proves the assertion .",
    "we will see in section [ shrinkpls ] that the matrices @xmath122 and their eigenvalues determine the shrinkage factors of the pls estimator . to prove this",
    ", we list some properties of @xmath122 in teh following sections .",
    "a symmetric tridiagonal matrix @xmath132 is called unreduced if all subdiagonal entries are non - zero , i.e @xmath133 for all @xmath5 .",
    "all eigenvalues of an unreduced matrix are distinct",
    ".    set @xmath134    [ unreduced ] if @xmath135 , the matrix @xmath122 is unreduced .",
    "more precisely @xmath136 for all @xmath137 .",
    "set @xmath138 and denote by @xmath139 the basis obtained by gram - schmidt .",
    "its existence is guaranteed as we assume that @xmath135 . for simplicity of notation",
    ", we assume that the vectors @xmath140 are not normalized to have length 1 . by definition @xmath141 as the vectors @xmath140 are pairwisse orthogonal , it follows that @xmath142 we conclude that @xmath143    note that the matrix @xmath144 is obtained from @xmath122 by deleting the last column and row of @xmath122 .",
    "it follows that we can give a recursive formula for the characteristical polynomials @xmath145 of @xmath122 .",
    "we have @xmath146 and @xmath147 .",
    "+ we want to deduce properties of the eigenvalues of @xmath122 and @xmath27 and explore their relationship .",
    "denote the eigenvalues of @xmath122 by @xmath148    all eigenvalues of @xmath149 are eigenvalues of @xmath27 .",
    "first note that @xmath150 as the columns of the matrix @xmath151 form an orthonormal basis of @xmath152 , @xmath153 is the matrix that represents @xmath154 with repect to this basis .",
    "as any eigenvalue of @xmath155 is obviously an eigenvalue of @xmath27 , the proof is complete    the following theorem is a special form of the cauchy interlace theorem . in this version ,",
    "we use a general result from @xcite and exploit the tridiagonal structure of @xmath122 .",
    "[ ci2 ] each interval @xmath156\\end{aligned}\\ ] ] @xmath157 contains a different eigenvalue of @xmath158 ( @xmath159 ) .",
    "in addition , there is a different eigenvalue of @xmath160 outside the open interval @xmath161 .",
    "this theorems ensures in particular that there is a different eigenvalue of @xmath27 in the interval @xmath162 $ ] .",
    "theorem [ ci2 ] holds independently of assumption ( [ ass ] ) .    by definition , for @xmath163 @xmath164 here @xmath165",
    ", so @xmath166 an application of theorem 10.4.1 in @xcite gives the desired result .",
    "[ distinct ] if @xmath122 is unreduced , the eigenvalues of @xmath122 and the eigenvalues of @xmath144 are distinct .",
    "suppose the two matrices have a common eigenvalue @xmath167 .",
    "it follows from ( [ charpol ] ) and the fact that @xmath122 is unreduced that @xmath167 is an eigenvalue of @xmath168 . repeating this",
    ", we deduce that @xmath169 is an eigenvalue of @xmath170 , a contradiction , as @xmath171    [ not1 ] in general it is not true that @xmath122 and a submatrix @xmath172 have distinct eigenvalues .",
    "consider the case where @xmath173 for all @xmath5 .",
    "using equation ( [ charpol ] ) we conclude that @xmath174 is an eigenvalue for all submatrices with @xmath2 odd .",
    "if @xmath135 , we have @xmath175 .",
    "@xmath122 is positive semidefinite , hence all eigenvalues of @xmath122 are @xmath176 . in other words , @xmath177 if and only if its smallest eigenvalue @xmath178 is @xmath179 . using theorem [ ci2 ] we have @xmath180 as @xmath135 , the matrix @xmath122 is unreduced , which implies that @xmath122 and @xmath144 have no common eigenvalues ( see [ distinct ] ) .",
    "we can therefore replace the first @xmath181 by @xmath182 , i.e. the smallest eigenvalue of @xmath144 is @xmath179 .    in general",
    ", it is not true that @xmath183 .",
    "an easy example is @xmath184 we have @xmath185 i.e. @xmath186 .",
    "on the other hand @xmath187    it is well known that the matrices @xmath122 are closely related to the so - called rayleigh - ritz procedure , a method that is used to approximate eigenvalues .",
    "for details consult e.g. @xcite .",
    "we have presented two estimators for the regression parameter @xmath188  ols and pls  which also define estimators for @xmath189 via @xmath190 one possibility to evaluate the quality of an estimator is to determine its mean squared error ( mse ) . in general , the mse of an estimator @xmath191 for a vector - valued parameter @xmath192 is defined as @xmath193 \\\\ & = & e\\left [ \\left(\\hat\\theta -\\theta\\right)^t\\left(\\hat \\theta -\\theta\\right ) \\right]\\\\ & = & \\left(e\\left [ \\hat \\theta \\right ] -\\theta \\right)^t\\left(e\\left [ \\hat \\theta \\right ] -\\theta \\right)+ e\\left [ \\left(\\hat \\theta^t - e\\left[\\hat\\theta\\right]\\right)^t \\left(\\hat \\theta^t - e\\left[\\hat\\theta\\right]\\right ) \\right]\\,.\\end{aligned}\\ ] ] this is the well - known bias - variance decomposition of the mse . the first part is the squared bias and the second part is the variance term .",
    "we start by investigating the class of linear estimators , i.e. estimators that are of the form @xmath194 for some matrix @xmath84 that does not depend on @xmath10 .",
    "the ols estimators are linear : @xmath195 @xmath196 is the projection @xmath197 onto the space that is spanned by the columns of @xmath19 .",
    "+ recall the regression model ( [ linreg ] ) .",
    "let @xmath198 be a linear estimator .",
    "we have @xmath199&= & sx\\beta \\\\ \\text{var}\\left[\\hat \\theta \\right]&=&\\sigma^2 \\text{tr}\\left(ss^t\\right)\\,.\\end{aligned}\\ ] ]    the estimator @xmath200 is unbiased as @xmath201 & = & s_2 x\\beta \\\\ & = & p_{l(x ) } x\\beta\\\\ & = & x\\beta\\,.\\end{aligned}\\ ] ] the estimator @xmath32 is only unbiased if @xmath202 : @xmath203&=&e\\left[\\left(x^tx\\right)^- x^t y\\right]\\\\ & = & \\left(x^tx\\right)^- x^t e\\left[y\\right]\\\\ & = &   \\left(x^tx\\right)^- x^t x\\beta \\\\ & = & \\beta\\,.\\end{aligned}\\ ] ]    let us now have a closer look at the variance term .",
    "for @xmath32 we have @xmath204 hence @xmath205 next note that @xmath196 is the operator that projects on the space spanned by the columns of @xmath19 .",
    "it follows that @xmath206 and that @xmath207 we conclude that the mse of the estimator @xmath32 depends on the eigenvalues @xmath208 of @xmath209 .",
    "small eigenvalues of @xmath27 correspond to directions in @xmath19 that have very low variance .",
    "equation ( [ varbeta ] ) shows that if some eigenvalues are small , the variance of @xmath32 is very high , which leads to a high mse .",
    "+ one possibility to ( hopefully ) decrease the mse is to modify the ols estimator by shrinking the directions of the ols estimator that are responsible for a high variance .",
    "this of course introduces bias .",
    "we shrink the ols estimator in the hope that the increase in bias is small compared to the decrease in variance .",
    "+ in general , a shrinkage estimator for @xmath188 is of the form @xmath210 where @xmath211 is some real - valued function .",
    "the values @xmath212 are called shrinkage factors",
    ".    examples are    * principal component regression @xmath213 and * ridge regression @xmath214 where @xmath215 is the ridge parameter .",
    "we will see in section [ shrinkpls ] that pls is a shrinkage estimator as well .",
    "it will turn out that the shrinkage behavior of pls regression is rather complicated .",
    "+ let us investigate in which way the mse of the estimator is influenced by the shrinkage factors .",
    "if the shrinkage estimators are linear , i.e. the shrinkage factors do not depend on @xmath10 , this is an easy task .",
    "let us first write the shrinkage estimator in matrix notation .",
    "we have @xmath216 the diagonal matrix @xmath217 has entries @xmath212 .",
    "the shrinkage estimator for @xmath10 is @xmath218 we calculate the variance of these estimators . @xmath219 and @xmath220 next , we calculate the bias of the two shrinkage estimators . we have @xmath221&= & s_{shr,1 } x\\beta\\\\ & = & u\\sigma d_{shr }   \\sigma^-u^t \\beta \\,.\\end{aligned}\\ ] ]",
    "it follows that @xmath222 -\\beta \\right)^t\\left(e\\left[s_{shr,1 } y\\right ] -\\beta    \\right)\\\\ & = & \\left(u^t \\beta\\right)^t \\left(\\sigma d_f   \\sigma^- -\\text{id}\\right)^t   \\left(\\sigma d_f   \\sigma^-",
    "-\\text{id}\\right)\\left(u^t \\beta\\right)\\\\ & = & \\sum_{i=1 } ^{p^ * } \\left(f(\\lambda_i ) -1\\right)^2 \\left(u_i ^t \\beta \\right)^2\\,.\\end{aligned}\\ ] ] replacing @xmath223 by @xmath224 it is as easy to show that @xmath225    for the shrinkge estimator @xmath226 and @xmath227 defined above we have @xmath228    if the shrinkage factors are deterministic , i.e. they do not depend on @xmath10 , any value @xmath229 increases the bias . values @xmath230 decrease the variance , whereas values @xmath231 increase the variance .",
    "hence an absolute value @xmath0 is always undesirable .",
    "the situation is completely different for stochastic shrinkage factors .",
    "we will discuss this in the following section .",
    "+ note that there is a different notion of shrinkage , namely that the @xmath232- norm of an estimator is smaller than the @xmath232-norm of the ols estimator . why is this a desirable property ?",
    "let us again consider the case of linear estimators .",
    "set @xmath233 for @xmath234 .",
    "we have @xmath235 the property that for all @xmath236 @xmath237 is equivalent to the condition that @xmath238 is negative semidefinite .",
    "the trace of negative semidefinite matrices is @xmath239 .",
    "furthermore @xmath240 , so we conclude that @xmath241 it is known ( see @xcite ) that @xmath242",
    "in this section , we give a simpler and clearer proof of the shape of the shrinkage factors of pls . basically , we combine the results of @xcite and @xcite .",
    "it turns out that some of the factors @xmath8 are greater than 1 .",
    "we try to explain why these `` peculiar shrinkage properties '' do not necessarily imply that the mse of the pls estimator is increased .",
    "+ denote by @xmath243 the polynomial associated to @xmath122 that was defined in proposition [ penrose ] , i.e. @xmath244 recall that the eigenvalues of @xmath122 are denoted by @xmath245 .",
    "it follows that @xmath246 by definition of pls , @xmath247 hence there is a polynomial @xmath248 of degree @xmath249 with @xmath250 .",
    "suppose that @xmath135 .",
    "we have @xmath251    by proposition [ penrose ] , @xmath252 we plug this into equation ( [ bhat ] ) and obtain @xmath253 recall that the columns of @xmath102 form an orthonormal basis of @xmath254 .",
    "it follows that @xmath255 is the operator that projects on the space @xmath254 .",
    "in particular @xmath256 for @xmath257 .",
    "this implies that @xmath258    suppose that @xmath135 .",
    "if we denote by @xmath4 the component of @xmath259 along the @xmath5th eigenvector of @xmath27 then @xmath260 where @xmath261 is the polynomial defined in ( [ tm ] ) .",
    "( @xcite ) this follows immediately from the proposition above .",
    "we have @xmath262    we now show that some of the shrinkage factors of pls are @xmath263 .    for each @xmath264 , we can decompose the interval @xmath265 $ ] into @xmath266 disjoint intervals if @xmath267 . ]",
    "@xmath268 such that @xmath269    set @xmath270 .",
    "it follows from equation ( [ tm ] ) that the zero s of @xmath271 are @xmath272 .",
    "as @xmath122 is unreduced , all eigenvalues are distinct . set @xmath273 and @xmath274 .",
    "define @xmath275\\mu_{i } ^{(m)},\\mu_{i+1 } ^{(m)}[$ ] for @xmath276 . by definition ,",
    "hence @xmath271 is non - negative on the intervals @xmath278 if @xmath279 is odd and @xmath271 is non - positive on the intervals @xmath278 if @xmath279 is even .",
    "it follows from theorem [ ci2 ] that all interval @xmath278 contain at least one eigenvalue @xmath7 of @xmath280 .",
    "in general it is not true that @xmath281 for all @xmath7 and @xmath282 . using the example in remark [ not1 ] and the fact that @xmath283 is equivalent to the condition that @xmath7 is an eigenvalue of @xmath122",
    ", it is easy to construct a counterexample . using some of the results of section [ sectiontri ]",
    ", we can however deduce that some factors are indeed @xmath284 .",
    "as all eigenvalues of @xmath285 and @xmath286 are distinct ( c.f . proposition [ distinct ] ) , we see that @xmath287 for all @xmath5 . in particular @xmath288 more generally , using proposition [ distinct ] , we conclude that @xmath289 and @xmath290 is not possible . in practice ",
    "i.e. calculated on a data set ",
    "the factors seem to be @xmath284 all of the time .",
    "+ furthermore @xmath291 to proove this , we set @xmath270 .",
    "we have by definition @xmath277 .",
    "furthermore , the smallest positive zero of @xmath271 is @xmath245 and it follows from theorem [ ci2 ] and proposition [ distinct ] that @xmath292 .",
    "hence @xmath2930,1]$ ] .",
    "+ using theorem [ ci2 ] , more precisely @xmath294 it is possible to bound the terms @xmath295 from this we can derive bounds on the shrinkage factors . we will not pursue this further , readers who are interested in the bounds should consult @xcite .",
    "instead , we have a closer look at the mse of the pls estimator .",
    "+ in section [ shrinkage ] we showed that a value @xmath296 is not desirable , as the variance of the estimator increases .",
    "note however , that in the case of pls , the factors @xmath8 are stochastic ; they depend on @xmath10 - in a nonlinear way .",
    "for @xmath297 we have the following situation : if we set @xmath298 and @xmath299 , we have to compare @xmath300 note that the rhs is not necessarily smaller than the lhs , even if @xmath301 . an easy counterexample is @xmath302  the lhs is @xmath303 .",
    "+ among others , @xcite proposed to bound the shrinkage factors of the pls estimator in the following way . set @xmath304 and define a new estimator : @xmath305    if the shrinkage factors are numbers , this will improve the mse ( cf . section [ shrinkage ] ) .",
    "but in the case of stochastic shrinkage factors , the situation is completely unclear .",
    "consider again the example @xmath306 .",
    "set @xmath307 in this case @xmath308    so it is not clear whether the modified estimator bound leads to a lower mse , which was conjectured in e.g. @xcite .",
    "+ the above example ( involving @xmath309 and @xmath310 ) is of course purely artificial .",
    "it is not clear whether the shrinkage factors behave this way .",
    "it is hard if not infeasable to derive statistical properties of the pls estimator or its shrinkage factors , as they depend on @xmath10 in a complicated , nonlinear way . as an alternative",
    ", we compare the two different estimators on different data .",
    "in this section , we explore the difference between the methods pls and bound .",
    "we investigate three artificial datasets and one real world example . in all examples ,",
    "we rescale @xmath19 and @xmath10 to have zero mean and unit variance .",
    "+ let us start with the artificial datasets .",
    "of course , artificial datasets do not reflect many real world situations , but we have the advantage that we know the true regression coefficient @xmath188 and that we have an unlimited amount of examples at hand .",
    "we can estimate the mse of any of the four estimators : for @xmath311 we generate a sample @xmath10 and calculate the estimator @xmath312 .",
    "we define @xmath313 for all examples , we choose @xmath314 .      in our first example",
    "we generate @xmath315 examples in the following way : the input data is the realistion of a @xmath316 dimensional normally distributed variable with expectation @xmath317 and covariance matrix @xmath318 defined as @xmath319 the regression coefficient @xmath188 is the random permutation of @xmath320 with @xmath321 . + next",
    "we determine the variance of the error term .",
    "we do this by considering several signal - to - noise - ratios ( stnr ) . this quantity is defined as @xmath322 we set @xmath323 and determine the corresponding value of @xmath324 .",
    "we generate @xmath325 samples @xmath10 and calculate the four estimators .",
    "+ the following figures show the estimated mse for @xmath188 and @xmath189 respectively .",
    "the solid lines with the @xmath326 s correspond to pls .",
    "the lines with the @xmath327 s correspond to bound .",
    "+    we see that bound is better in all cases , although the improvement is not dramatic .",
    "we should remark that both method pick the same ( optimal ) number of steps most of the times .",
    "the difference between the two methods is especially tiny ( but non - zero ) in the first step .",
    "we do not have an explanation for this phenomenon .",
    "the mse is the same for the last step @xmath328 as in this case @xmath329      in this example , we generate @xmath330 examples .",
    "the input data is the realisation of a @xmath331 dimensional random variable with distribution @xmath332 .",
    "the covariance matrix is defined as in the first example ( with @xmath316 replaced by @xmath331 ) .",
    "again , the coefficients of @xmath188 are a random permutation @xmath333 with @xmath334 .",
    "we consider the signal - to - noise - ratios @xmath335 .",
    "+    the results are qualitatively the same as those from the first example .",
    "bound is better all of the times , the optimal number of steps are the same for both methods .",
    "the input data is generated as in the second example , in particular , we have @xmath336 .",
    "this time , we only generate @xmath337 examples .",
    "the coefficients of the regression vector @xmath188 are realizations of a @xmath338 distibuted random variable .",
    "we investigate the signal - to - noise - ratios @xmath335 .",
    "as we have more variables than examples , we do not investigate estimators for @xmath339 : different vectors @xmath340 can lead to @xmath341 , so it does not make sense to determine the bias of an estimator for @xmath339 .",
    "instead , we only show the figures for @xmath342 and @xmath343 .",
    "again , the estimated mse of bound is lower than the estimated mse of pls .",
    "this example is taken from @xcite .",
    "a survey investigated the degree of job satisfaction of the employees of a company .",
    "the employees filled in a questionnaire that consisted of @xmath344 questions regarding their work environment and one question ( the response variable ) regarding the degree to which they are satisfied with their job .",
    "the answers of the employees were summerized for each of the @xmath345 departments of the company .",
    "+ we compare the two methods pls and bound on this data set . for each @xmath346",
    "we determine the 10fold crossvalidation error .",
    "[ cv ]    the method bound is slightly better than pls on this data set : the cv error for the optimal number of components ( which is @xmath347 ) is 0.2698 for bound and 0.2747 for pls .",
    "it is remarkable that in this example the cv error of bound exceeds the cv error of pls in some cases .",
    "it is not clear if this is due to the small number of examples ( which makes the estimation unprecise ) or if this can also happen `` in theory '' .",
    "this paper consists of two parts . in the first part , we gave alternative and hopefully clearer proofs of the shrinkage factors of pls .",
    "in particular , we derived the fact that some of the shrinakge factors are @xmath0 .",
    "we explained in detail that this would lead to an unnecessarily high mse if pls was a linear estimator .",
    "this is however not the case and we emphasized that bounding the absolute value of the shrinkage factors by @xmath1 does not automatically lead to a lower mse .",
    "+ in the second part , we investigated the problem numerically .",
    "experiments on simulated and real world data showed that it might be better to adjust the shrinkage factors so that their absolute value is @xmath348 - a method that we called bound .",
    "the difference between bound and pls was not dramatic however . besides , the scale of the experiments was of course way too small",
    ", so it would be light - headed if we concluded that we should always use bound instead of pls .",
    "+ nevertheless , the experiments show that it is worth exploring the method bound in more detail .",
    "one drawback of this method is that we have to adjust the shrinkage factors `` by hand '' .",
    "if bounding the shrinkage factors tends to lead to better results , we might modify the original optimization problem of pls such that the shrinkage factors of the solution are bounded .",
    "we might modfify @xmath27 and @xmath63 to obtain a different krylov space or replace @xmath64 by a different set of feasible solutions .",
    "i would like to thank ulrich kockelkorn who eliminated innumerable errors from earlier versions of this paper and who gave a lot of helpful remarks .",
    "i would also like to thank jrg betzin for our extensive discussions on pls ."
  ],
  "abstract_text": [
    "<S> we present a formula for the shrinkage factors of the partial least squares regression estimator and deduce some of their properties , in particular the known fact that some of the factors are @xmath0 . </S>",
    "<S> we investigate the effect of shrinkage factors for the mean squared error of linear estimators and illustrate that we can not extend the results to nonlinear estimators . </S>",
    "<S> in particular , shrinkage factors @xmath0 do not automatically lead to a poorer mean squared error . </S>",
    "<S> we investigate empirically the effect of bounding the the absolute value of the partial least squares shrinkage factors by @xmath1 .     </S>"
  ]
}