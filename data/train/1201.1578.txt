{
  "article_text": [
    "let @xmath0 be independent and identically distributed ( i.i.d . )",
    "non - negative random variables ( r.v.s ) with mean @xmath1 variance @xmath2 and cumulative distribution function ( cdf ) @xmath3suppose that the tail of @xmath4 is regularly varying at infinity with tail index @xmath5 that is@xmath6 ( see , e.g. , @xcite ) .",
    "such cdf s constitute a major subclass of the family of heavy - tailed distributions .",
    "it includes distributions such as pareto , burr , student , @xmath7stable @xmath8 and log  gamma , which are known to be appropriate models for fitting large insurance claims , large fluctuations of prices , log  returns , etc .",
    "( see , e.g. @xcite ; @xcite ; @xcite ) . in this paper",
    ", we are concerned with the construction of a bias - reduced asymptotically normal estimator for the mean@xmath9 which could be rewritten , in terms of the quantile function ( corresponding to the cdf @xmath10@xmath11 as@xmath12 for a given sample @xmath13 let@xmath14 denote the sample quantile function ( classical non - parametric estimator of @xmath15associated to the empirical cdf defined on the real line by @xmath16with  @xmath17  being  the  indicator  function .",
    "the natural ( unbiased ) estimator of @xmath18 is the sample mean@xmath19 from the central limit theorem ( clt ) , the sequence of r.v.s @xmath20 converges in distribution to the standard gaussian r.v .",
    ", provided that the second - order moment @xmath21   $ ] is finite .",
    "this is a very restrictive condition in the context of heavy - tailed distributions as the following considerations show .",
    "assume that the r.v .",
    "@xmath22 follows the pareto law with index @xmath23 that is , @xmath24 for @xmath25 . when @xmath26 the mean @xmath18 exists , but @xmath21   $ ] is only finite for @xmath27 hence , the range @xmath28 is not covered by the clt and thus we need to seek another approach to handle this situation .",
    "making use of weissman s estimator of high quantiles @xcite , @xcite proposed an alternative estimator for @xmath18 and established its asymptotic normality for any @xmath29 let us define the following estimator for @xmath30@xmath31{lcc}q_{n}^{w}(1-s ) & \\text{for } & 0<s < k / n\\medskip\\\\ q_{n}(1-s ) & \\text{for } & k / n\\leq s<1 , \\end{array } \\right.\\ ] ] where@xmath32 is weissman s estimator of high quantiles , with@xmath33 being the well - known hill estimator @xcite of the tail index @xmath34 and @xmath35  denoting the order statistics pertaining to the sample @xmath36 the number @xmath37 represents the number of upper order statistics used in the computation of @xmath38 it is an integer sequence @xmath39 satisfying@xmath40 by replacing @xmath41 by @xmath42 in formula ( [ mu ] ) , @xcite proposed an alternative estimator for @xmath18 as follows:@xmath43 which , by a straightforward calculation , is equal to@xmath44 provided that @xmath45 moreover , the same author showed that , under suitable regularity assumptions , for any @xmath46@xmath47 where@xmath48 throughout this paper , the standard notations @xmath49 @xmath50 and @xmath51 respectively stand for convergence in probability , convergence in distribution and equality in distribution , while @xmath52  denotes the normal distribution with mean @xmath53 and variance @xmath54    actually , @xcite defined his estimator in the more general situation where the r.v",
    ". @xmath55 is real ( not necessarily non - negative ) with lower and upper heavy tails .",
    "he simultaneously took into account the regular variations of both tails of @xmath56 and the balance condition@xmath57   .\\ ] ] in this paper , we only consider non - negative r.v.s .",
    "our motivation comes from the actuarial risk theory where insurance losses are represented by such r.v.s . in this case , @xmath58 may be interpreted as an estimator of a risk measure called the net premium , see for instance @xcite and @xcite .",
    "note that in our case , since r.v .",
    "@xmath55 is non - negative , we have @xmath59 for @xmath60 which yields @xmath61 in the above balance condition .",
    "hill s estimator @xmath62 plays a pivotal role in statistical inference on distribution tails .",
    "this estimator has been thoroughly studied , improved and even generalized to any real parameter @xmath63 weak consistency of  @xmath62 was established by @xcite assuming only that the underlying cdf @xmath4 satisfies condition ( [ f ] ) .",
    "the asymptotic normality of @xmath62 has been established ( see @xcite ) under the following stricter condition that characterizes hall s model ( see @xcite and @xcite).@xmath64 for some @xmath65 @xmath66 and @xmath67 note that ( [ a2 ] ) , which is a special case of a more general second - order regular variation condition ( see @xcite ) , is equivalent to@xmath68 the constants @xmath69 and @xmath70 are called , respectively , first - order ( tail index , shape parameter ) and second - order parameters of cdf @xmath71    extreme value based estimators essentially rely on the number @xmath37 of upper order statistics involved in estimate computation .",
    "hill s estimator has , in general , a substantial variance for small values of @xmath37 and a considerable bias for large values of @xmath72 hence ,  one has to look for a @xmath37 value , denoted by @xmath73 that balances between these two vices .",
    "the choice of this optimal value @xmath74 represents a thorny issue in the process of estimating the tail index and related quantities . to solve this problem , several adaptive procedures are available , see , e.g. , @xcite , @xcite , @xcite , @xcite , @xcite , and the references therein . a theoretical optimal choice of @xmath37 is obtained by minimizing the asymptotic mean squared error ( rmse ) of @xmath75 indeed , under condition ( [ a2 ] ) , we have ( see @xcite)@xmath76 though peng",
    "s estimator @xmath58 enjoys the asymptotic normality property , it still has a problem due to the fact that , it is based on weissman s estimator @xmath77 known to be largely biased .",
    "fortunately , many estimators with reduced biases are proposed in the literature as an alternative to @xmath78 see , for instance , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and @xcite .    in this paper",
    ", we use the bias - reduced estimator of the high quantile @xmath79 recently proposed by @xcite who exploited the censored maximum likelihood ( cml ) based estimators @xmath80 of the couple of regular variation parameters @xmath81 introduced by @xcite .",
    "the cml estimators @xmath82 are defined as the solution of the two equations ( under the constraint @xmath83@xmath84",
    "where@xmath85 with@xmath86 @xcite obtained their bias - reduced estimators @xmath87 of the high quantiles",
    "@xmath79 by substituting @xmath82 to @xmath88 in ( [ a3 ] ) . that is@xmath89 where@xmath90{l}\\widehat{c}=\\dfrac{\\widehat{\\alpha}\\widehat{\\beta}}{\\widehat{\\alpha}-\\widehat{\\beta}}\\dfrac{k}{n}x_{n - k , n}^{\\widehat{\\alpha}}\\left (   \\dfrac { 1}{\\widehat{\\beta}}-\\dfrac{1}{k}{\\displaystyle\\sum\\limits_{i=1}^{k } } \\log\\dfrac{x_{n - i+1,n}}{x_{n - k , n}}\\right )   , \\bigskip\\\\ \\widehat{d}=\\dfrac{\\widehat{\\alpha}\\widehat{\\beta}}{\\widehat{\\beta}-\\widehat{\\alpha}}\\dfrac{k}{n}x_{n - k , n}^{\\widehat{\\beta}}\\left (   \\dfrac { 1}{\\widehat{\\alpha}}-\\dfrac{1}{k}{\\displaystyle\\sum\\limits_{i=1}^{k } } \\log\\dfrac{x_{n - i+1,n}}{x_{n - k , n}}\\right )   . \\end{array } \\right .",
    "\\label{c - d}\\ ] ] the consistency and asymptotic normality of @xmath91 are established by the same authors .",
    "now we can define another estimator for the quantile function @xmath92 as follows:@xmath93{lcc}q_{n}^{lpy}\\left (   1-s\\right )   &",
    "\\text{for } & 0<s < k / n\\medskip\\\\ q_{n}(1-s ) & \\text{for } & k / n\\leq s<1 .",
    "\\end{array } \\right.\\ ] ] by replacing @xmath92 by @xmath94 in formula ( [ mu ] ) , we get@xmath95 an elementary integral calculation leads to a new bias - reduced estimator for @xmath18 defined by the following formula:@xmath96 provided that @xmath97 so that @xmath98 be finite .",
    "the rest of the paper is organized as follows . in section [ sec2 ] , we briefly discuss the third order - condition of regular variation before establishing the asymptotic normality of @xmath99  in section [ simulation ] , we carry out a simulation study to illustrate the performance of our new estimator @xmath98 and compare it with peng s one .",
    "proofs are relegated to section [ secproof ] .",
    "some concluding remarks notes made in section [ concluding notess ] . finally , some of the main results used in section [ secproof ] are gathered in the appendix , as well as a very brief description of the algorithm of reiss and thomas applied , in section [ simulation ] , to select the optimal sample fraction @xmath72",
    "in the theory of extremes , a function , denoted by @xmath100 and ( sometimes ) called tail quantile function ,  is used quite often .",
    "it is defined by@xmath101 in terms of this function , hall s conditions ( [ a2 ] ) and ( [ a3 ] ) are equivalent to@xmath102 this implies that@xmath103   -\\alpha^{-1}\\log x}{a_{1}\\left (   t\\right ) } = \\dfrac{x^{1-\\beta/\\alpha}-1}{1-\\beta/\\alpha},\\text { for any } x>0 , \\label{second - order}\\ ] ] where@xmath104 the function @xmath105 which tends to zero as @xmath106 ( because @xmath107 determines the rate of convergence of @xmath108   $ ] to its limit @xmath109 relation ( [ second - order ] ) is known as the second - order condition of regular variation ( see , e.g. , @xcite ) .    unfortunately , the second - order regular variation  is not sufficient to find asymptotic distributions for the estimators defined by the systems ( [ alpha - beta ] ) and ( [ c - d ] ) .",
    "we strengthen it into a condition , called third - order condition of regular variation and given by ( [ third - order ] ) , that specifies the rate of ( [ second - order ] ) ( see , e.g. , @xcite or @xcite).@xmath110   -\\alpha^{-1}\\log x}{a_{1}\\left ( t\\right )   } -\\dfrac{x^{1-\\beta/\\alpha}-1}{1-\\beta/\\alpha}}{a_{2}\\left ( t\\right )   } = d\\left (   \\alpha,\\beta,\\rho\\right )   , \\label{third - order}\\ ] ] where @xmath111 as @xmath112 with constant sign near infinity and@xmath113 with @xmath114 being a positive constant called third - order parameter .",
    "@xcite established the asymptotic normality of @xmath115 @xmath116 and @xmath117 under the following extract conditions on the sample fraction @xmath118 as @xmath119@xmath120 as for @xmath121 it is asymptotically normal under the assumption @xmath122 added to @xmath123 and @xmath124    [ example]consider the frchet cdf with shape parameter @xmath125@xmath126 the corresponding tail quantile function is defined by @xmath127 for @xmath128applying taylor s expansion ( to the third order ) to @xmath100 and identifying with ( [ u ] ) , yield @xmath129 @xmath130 and @xmath131the condition ( [ third - order ] ) holds for @xmath132 @xmath133 and @xmath134 other examples may be found in the recent paper @xcite .",
    "the frchet cdf will be employed , in section [ simulation ] , as a model in our simulation study .",
    "note that , from a theoretical point of view , assumptions ( [ k ] ) and ( [ a1a2 ] ) are realistic , as the following example shows .",
    "indeed , let us choose @xmath135   , $ ] @xmath136 then it easy to verify that these assumptions hold for any @xmath137 the notation @xmath138   $ ] stands for the integer part of real numbers .",
    "our main result , namely the asymptotic normality of the bias - reduced estimator @xmath139 is formulated in the last of the following four theorems . in theorem",
    "[ theorem1 ] , we give an approximation of @xmath140 in terms of brownian bridges , which leads to its asymptotic normality stated in theorem [ theorem2 ] .",
    "we do the same thing to @xmath98 in theorem [ theorem3 ] .",
    "it is worth mentioning that the asymptotic normality of @xmath140 was first established by @xcite .",
    "but , this does not meet our needs to achieve the major object of this paper . then",
    ", we need to approximate both @xmath140 and @xmath98 by linear functional of the same sequence of standard brownian bridges @xmath141    [ theorem1]assume that the third order condition ( [ third - order ] ) holds with @xmath142 and let @xmath39 be an integer sequence satisfying ( [ k ] ) and ( [ a1a2 ] ) .",
    "then  there exists a sequence of brownian bridges @xmath143  such that@xmath144 where @xmath145 @xmath146 and @xmath147 are sequences of centered gaussian r.v.s defined by@xmath148{c}:= \\end{array } \\sqrt{n / k}b_{n}\\left (   1-k / n\\right )   -\\sqrt{n / k}{\\displaystyle\\int_{0}^{1 } } s^{-1}b_{n}\\left (   1-ks / n\\right )   ds,\\\\ &   w_{2n}\\begin{array } [ c]{c}:= \\end{array } \\left (   \\lambda^{-1}-1\\right )   \\sqrt{n / k}b_{n}\\left (   1-k / n\\right )   + \\left ( \\lambda-1\\right )   \\sqrt{n / k}{\\displaystyle\\int_{0}^{1 } } s^{\\lambda-2}b_{n}\\left (   1-ks / n\\right )   ds,\\\\ &   w_{3n}\\begin{array } [ c]{c}:= \\end{array } \\left (   1-\\lambda\\right )   \\sqrt{n / k}{\\displaystyle\\int_{0}^{1 } } s^{\\lambda-2}\\left (   \\log s\\right )   b_{n}\\left (   1-ks / n\\right )   ds\\\\ &   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\lambda^{-2}\\sqrt{n / k}b_{n}\\left ( 1-k / n\\right )   -\\sqrt{n / k}{\\displaystyle\\int_{0}^{1 } } s^{\\lambda-2}b_{n}\\left (   1-ks / n\\right )   ds,\\end{aligned}\\ ] ] and@xmath149    [ theorem2]under the assumptions of theorem [ theorem1 ] , we have@xmath150    [ theorem3]under the assumptions of theorem [ theorem1 ] , we have , as @xmath151@xmath152 where @xmath145 @xmath146 and @xmath147 are those of theorem [ theorem1 ] and@xmath153    [ theorem4]under the assumptions of theorem [ theorem1 ] , we have@xmath154 where@xmath155    the following corollary to theorem [ theorem4 ] provides a straightforward practical way to build confidence intervals for @xmath156    [ corollary]under the assumptions of theorem [ theorem1 ] , we have @xmath157 where @xmath115 @xmath116 and @xmath117 are the estimates of @xmath158 and @xmath159 given in ( [ alpha - beta ] ) and ( [ c - d ] ) respectively .",
    "let @xmath160 denote @xmath161-quantile of the standard normal r.v . given a realization @xmath162 of @xmath163 from a population@xmath55 satisfying the required assumptions , we construct a @xmath164 confidence interval for @xmath18 via the following four steps :    * step 1 : * applying reiss and thomas algorithm ( see subsection [ rt ] of the appendix ) , we select the optimal sample fraction",
    "@xmath165    * step 2 : * resolving the system ( [ alpha - beta ] ) with @xmath166 we obtain estimate values for @xmath69 and @xmath70 that we respectively denote by @xmath167 and @xmath168 then , we use the first equation of ( [ c - d ] ) to get the corresponding estimate @xmath169 of @xmath170    * step 3 : * using formulas ( [ newestim ] ) and ( [ var ] ) , we compute @xmath171 and @xmath172 respectively .",
    "* step 4 : *  finally , corollary [ corollary ] yields the @xmath164 confidence interval for @xmath173 @xmath174 our simulation study , which is based on @xmath175 samples of various sizes from the frchet distribution ( [ frechet ] ) with two distinct tail index values @xmath176 and @xmath177 consists of three parts .",
    "first , we compare , in terms of bias and root of the mean squared error ( rmse ) , the performances of the new estimator @xmath98 and peng s estimator @xmath178 the results of this part are summarized in tables [ tab1a ] and [ tab1b ] .",
    "second , we check the asymptotic normality of both estimators via four of the most popular goodness - of - fit tests at the @xmath179 significance level : cramr - von mises ( cvm ) , kolmogrov - smirnov ( ks ) , shapiro - wilk ( sw ) and pearson ( p )",
    ". the results of this part are summarized in tables [ tab3 ] and [ tab4 ] and illustrated by figures [ fig15 ] and [ fig17 ] .",
    "finally , we investigate the accuracy of the confidence intervals , built from the new estimator @xmath139 by computing their lengths and coverage probabilities ( denoted by ` covpr ' ) .",
    "the results of this part are summarized in table [ tab2 ] ( where ` lcb ' and ` ucb ' respectively stand for the lower and upper confidence bounds ) and illustrated by figure [ fig - cb ] .",
    "[ c]lrrrrrrrr & & + sample size & @xmath180 & @xmath181 & @xmath182 & @xmath183 & @xmath180 & @xmath181 & @xmath182 & @xmath183 + estimated value & @xmath184 & @xmath185 & @xmath186 & @xmath187 & @xmath188 & @xmath189 & @xmath189 & @xmath190 + bias & @xmath191 & @xmath192 & @xmath193 & @xmath194 & @xmath195 & @xmath196 & @xmath196 & @xmath197 + rmse & @xmath198 & @xmath199 & @xmath200 & @xmath201 & @xmath202 & @xmath203 & @xmath203 & @xmath204 + & & & & & & & &    [ c]lrrrrrrrr & & + sample size & @xmath180 & @xmath181 & @xmath182 & @xmath183 & @xmath180 & @xmath181 & @xmath182 & @xmath183 + estimated value & @xmath205 & @xmath206 & @xmath207 & @xmath208 & @xmath209 & @xmath210 & @xmath211 & @xmath212 + bias & @xmath213 & @xmath214 & @xmath215 & @xmath216 & @xmath217 & @xmath218 & @xmath219 & @xmath220 + rmse & @xmath221 & @xmath222 & @xmath223 & @xmath224 & @xmath225 & @xmath226 & @xmath227 & @xmath228 + & & & & & & & &    [ c]ccccc & & & & + & cvm & ks & sw & + @xmath229 & @xmath230 & @xmath231 & @xmath204 & + @xmath232 & @xmath233 & @xmath234 & @xmath235 & + @xmath236 & @xmath237 & @xmath238 & @xmath239 & + @xmath180 & @xmath240 & @xmath241 & @xmath242 & + @xmath243 & @xmath244 & @xmath245 & @xmath246 & + @xmath181 & @xmath247 & @xmath248 & @xmath249 & + & & & &    [ c]ccccc & & & & + & cvm & ks & sw & p + & @xmath250 & @xmath251 & @xmath251 & @xmath252 + & @xmath253 & @xmath254 & @xmath255 & @xmath256 + & @xmath257 & @xmath258 & @xmath259 & @xmath260 + & @xmath261 & @xmath262 & @xmath263 & @xmath264 + & @xmath265 & @xmath192 & @xmath266 & @xmath267 + & @xmath268 & @xmath269 & @xmath270 & @xmath271 + & & & &    [ c]ccccc & & & & + & cvm & ks & sw & + @xmath229 & @xmath272 & @xmath273 & @xmath274 & + @xmath232 & @xmath275 & @xmath276 & @xmath277 & + @xmath236 & @xmath278 & @xmath279 & @xmath280 & + @xmath180 & @xmath281 & @xmath282 & @xmath283 & @xmath284 + @xmath243 & @xmath285 & @xmath286 & @xmath287 & + @xmath181 & @xmath288 & @xmath289 & @xmath290 & + & & & &    [ c]ccccc & & & & + & cvm & ks & sw & p + & @xmath291 & @xmath292 & @xmath293 & @xmath294 + & @xmath295 & @xmath296 & @xmath297 & @xmath298 + & @xmath299 & @xmath300 & @xmath301 & @xmath302 + & @xmath303 & @xmath304 & @xmath305 & @xmath306 + & @xmath307 & @xmath308 & @xmath309 & @xmath310 + & @xmath311 & @xmath312 & @xmath313 & @xmath314 + & & & &    @xmath315    [ c]ccccccclllll@xmath316 & lcb & @xmath317 & ucb & covpr & length & & lcb & @xmath317 & ucb & covpr & length + @xmath229 & & & & & & & & & & & + @xmath232 & & & & & & & & & & & + @xmath236 & & & & & & & & & & & + @xmath180 & & & & & & & & & & & + @xmath243 & & & & & & & & & & & + @xmath181 & & & & & & & & & & & + & & & & & & & & & & &    .[tab2 ]    [ h ]    15.eps    [ ptb ]    17.eps    [ ptb ]    estimateur.eps    tables [ tab1a ] and [ tab1b ] show that , regardless of the sample size , our new estimator performs better than peng s one as far as the bias and rmse are concerned .",
    "moreover , from the left panels of figure [ fig17 ] and table [ tab4 ] ( corresponding to the case of the lighter tail @xmath318 we see that the normality of @xmath98 can not be rejected by any of the tests when the sample size exceeds @xmath319 while the right panels of figure [ fig17 ] and table [ tab4 ] show that the normality of @xmath58 is rejected for sample sizes ranging between @xmath320 and @xmath321 in the case of the heavier tail @xmath322 the right panels of figure [ fig15 ] and table [ tab3 ] show that the sample size needs to be larger then @xmath323 for the estimator @xmath58 to pass the normality tests , while the left panels of figure [ fig15 ] and table [ tab3 ] indicate that the normality of @xmath98 is accepted even for sample sizes smaller than @xmath321",
    "first recall that @xmath324then , from expansion @xmath325 in @xcite , we have , as @xmath151@xmath326 where@xmath327 and@xmath328 with @xmath329 being the order statistics pertaining to a sample @xmath330 of i.i.d .",
    "r.v.s , defined on the same probability space as the @xmath331 with cdf @xmath332 @xcite have constructed a probability space @xmath333 carrying an infinite sequence @xmath334 of independent @xmath335uniform r.v.s and a sequence of brownian bridges @xmath336 @xmath337 having , amongst others , the property stated in lemma [ lemma1 ] .",
    "let @xmath338 denote the order statistics pertaining to @xmath339 and define the empirical quantile function @xmath340 as@xmath341    [ lemma1]on the probability space of @xcite , for every @xmath342 we have , as @xmath151@xmath343    * proof .",
    "* see the proof of theorem 2.1 in @xcite.@xmath344    without loss of generality , we assume that@xmath345 and@xmath346 where @xmath347 denotes the quantile function pertaining to cdf @xmath56 given by formula ( [ g]).@xmath348then , this allows us to write@xmath349 making use of the previous representation of the order statistics @xmath350 , we may rewrite the three statistics in ( [ p0 ] ) and ( [ p ] ) into@xmath351 and@xmath352 next , we show that , as @xmath151@xmath353 and@xmath354 where @xmath355 and @xmath147 are the gaussian r.v.s defined in theorem [ theorem1].@xmath348we will only consider the asymptotic distribution of @xmath356 the proofs for @xmath357 and @xmath358 use similar arguments . by letting @xmath359",
    "the statistic @xmath360 becomes@xmath361 an application of standard calculus gives @xmath362 therefore@xmath363   ds.\\ ] ] let us follow similar techniques as those used in the proof of lemma 9 in @xcite .",
    "we divide the integral above in two parts , then we study the asymptotic behavior of each integral .",
    "observe that@xmath364   ds\\\\ &   \\ \\ \\ \\ \\ \\ \\ -\\left (   n / k\\right ) { \\displaystyle\\int_{1/n}^{k / n } } \\left [   f\\left (   \\dfrac{1-v_{n}\\left (   1-s\\right )   } { 1-u_{n - k , n}}\\right ) -f\\left (   \\frac{s}{k / n}\\right )   \\right ]   ds\\\\ & \\begin{array } [ c]{c}= : \\end{array } -\\delta_{n}-\\omega_{n}.\\end{aligned}\\ ] ] next , we show that @xmath365 converges to @xmath366 in probability . indeed , we have @xmath367 for @xmath368 it follows that@xmath369   ds\\\\ &   = k^{-1}f\\left (   \\dfrac{1-u_{n , n}}{1-u_{n - k , n}}\\right )   -{\\displaystyle\\int_{0}^{1/k } } f\\left (   s\\right )   ds.\\end{aligned}\\ ] ] an elementary calculation gives @xmath370 and from lemma 2.2.3 of page 41 in @xcite , we have @xmath371 as @xmath372 therefore@xmath373 since @xmath374 and @xmath375 then @xmath376 and @xmath377 it follows that @xmath378 as @xmath379 consider now the second term @xmath380 which may be rewritten into@xmath381",
    "ds\\\\ &   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\left (   n / k\\right ) { \\displaystyle\\int_{1/n}^{k / n } } \\left [   f\\left (   \\dfrac{s}{1-u_{n - k , n}}\\right )   -f\\left (   \\dfrac{s}{k / n}\\right )   \\right ]   ds\\\\ & \\begin{array } [ c]{c}= : \\end{array } \\omega_{n1}+\\omega_{n2}.\\end{aligned}\\ ] ] making use of taylor s expansion of @xmath382 we get@xmath383 and@xmath384 where@xmath385 and@xmath386 observe now that @xmath387 and @xmath388 may be rewritten into @xmath389   ds+\\omega_{n1}^{\\ast},\\ ] ] and@xmath390   ds+\\omega_{n2}^{\\ast},\\ ] ] where @xmath391{c}:= \\end{array } \\left (   n / k\\right ) { \\displaystyle\\int_{1/n}^{k / n } } \\left [   f^{\\prime}\\left (   \\dfrac{\\varphi_{n}\\left (   s\\right )   } { 1-u_{n - k , n}}\\right )   -f^{\\prime}\\left (   \\dfrac{s}{1-u_{n - k , n}}\\right )   \\right ] \\\\ & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\times \\left [   \\dfrac{1-v_{n}\\left (   1-s\\right )   } { 1-u_{n - k , n}}-\\dfrac{s}{1-u_{n - k , n}}\\right ]   ds,\\end{aligned}\\ ] ] and@xmath392{c}:= \\end{array } \\left\\ {   1+o_{p}\\left (   1\\right )   \\right\\ }   \\left (   n / k\\right ) { \\displaystyle\\int_{1/n}^{k / n } } \\left [   f^{\\prime}\\left (   \\dfrac{s\\psi_{n}}{k / n}\\right )   -f^{\\prime}\\left ( \\dfrac{s}{k / n}\\right )   \\right ] \\\\ & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\",
    "\\times \\left [   \\dfrac{s}{1-u_{n - k , n}}-\\dfrac{s}{k / n}\\right ]   ds.\\end{aligned}\\ ] ] from lemma [ lemma3 ] ( see the appendix ) , both @xmath393 and @xmath394 converge to @xmath366 in probability . since @xmath395 then@xmath396   ds+o_{p}\\left (   k^{-1/2}\\right )   ,",
    "\\ ] ] and@xmath397 the derivative of function @xmath398 equals @xmath399 then @xmath400   dt\\\\ &   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\left (   n / k\\right ) { \\displaystyle\\int_{1/k}^{1 } } t^{\\lambda-2}\\left [   1-v_{n}\\left (   1-kt / n\\right )   -kt / n\\right ] dt+o_{p}\\left (   k^{-1/2}\\right )   , \\end{aligned}\\ ] ] and@xmath401 { \\displaystyle\\int_{1/k}^{1 } } t^{\\lambda-1}\\log tdt\\\\ &   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\left (   n / k\\right )   \\left [ k / n-\\left (   1-u_{n - k , n}\\right )   \\right ] { \\displaystyle\\int_{1/k}^{1 } } t^{\\lambda-1}dt+o_{p}\\left (   k^{-1/2}\\right ) \\\\ &   = \\lambda^{-2}\\left (   n / k\\right )   \\left [   k / n-\\left (   1-u_{n - k , n}\\right ) \\right ]   + o_{p}\\left (   k^{-1/2}\\right )   .\\end{aligned}\\ ] ] fix @xmath402 then using approximation ( [ approxi ] ) , in lemma [ lemma1 ] , yields@xmath403 and@xmath404 where@xmath405 and@xmath406 for @xmath407 @xmath408 and @xmath409 are finite integrals .",
    "then both quantities @xmath410 and @xmath411 are equal to @xmath412 for all large @xmath413 which tends in probability to @xmath366 as @xmath379 recall that up to now we have showed that@xmath414 and @xmath415 it remains to prove that@xmath416{c}:= \\end{array } \\left (   \\lambda-1\\right )   \\sqrt{n / k}{\\displaystyle\\int_{0}^{1/k } } t^{\\lambda-2}\\left (   \\log t\\right )   b_{n}\\left (   1-kt / n\\right )   dt\\\\ & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\sqrt { n / k}{\\displaystyle\\int_{0}^{1/k } } t^{\\lambda-2}b_{n}\\left (   1-kt / n\\right )   dt,\\end{aligned}\\ ] ] converges , in probability , to @xmath417 indeed , since @xmath418 then@xmath419 since@xmath420 which tends to @xmath366 as @xmath119 then @xmath421 converges to @xmath366 in probability .",
    "this completes the proof of theorem [ theorem1].@xmath344      to establish the asymptotic normality of @xmath140 , given in @xmath422 we proceed by similar arguments as for @xmath98 in the proof of theorem [ theorem4 ] .",
    "@xmath344      let us divide the integral ( [ mu ] ) , in two parts , as follows:@xmath423 where@xmath424 recall that , in section 1 formula ( [ sum ] ) , we have defined estimator @xmath98 of @xmath18 by @xmath425 where@xmath426 and @xmath427 to simplify notations , let us set@xmath428 first , we consider @xmath429 it is easy to verify that , as @xmath430@xmath431 and , under the condition ( [ k ] ) , we have@xmath432 it follows that @xmath433 let us write @xmath434 where@xmath435 and@xmath436 we begin by showing that @xmath437 as @xmath438 first observe that @xmath439 may be rewritten into @xmath440 assumptions @xmath441 and @xmath123 of theorem [ theorem1 ] imply that @xmath442also , from theorem 1 of @xcite , the asymptotic normality of @xmath140 gives @xmath443 therefore @xmath444 this implies that @xmath445 as @xmath379 on the other hand , from equation @xmath446 in @xcite , we have @xmath447 since @xmath117 is a consistent estimator of @xmath448 then taylor s expansion gives @xmath449 it suffices now to show that @xmath450 converges to @xmath366 in probability.@xmath348indeed , again by using the fact that @xmath451 yields@xmath452 which tends in probability to @xmath453 because we already have @xmath454 now , we consider the term @xmath455 since @xmath140 is a consistent estimator of @xmath34 then it is easy to show that@xmath456 from [ theorem1 ] , we infer that @xmath457 it follows that @xmath458 let us now consider the asymptotic distribution of @xmath459 in ( [ twostat ] ) .",
    "it is shown in @xcite or more recently in @xcite that@xmath460 on the other hand , from ( [ a3 ] ) , we have @xmath461 as @xmath119 it follows that@xmath462 combining ( [ zn1 ] ) and ( [ zn2 ] ) achieves the proof of theorem [ theorem3].@xmath344      now , we investigate the asymptotic normality of @xmath98 given in@xmath463 .  since @xmath464 @xmath465 are sequences of centred gaussian r.v.s , then@xmath466 where @xmath467 @xmath468 is the transpose of @xmath469 and @xmath470 is the variance - covariance matrix of the vector @xmath471 defined by@xmath472{cccc}1 & \\frac{1}{\\lambda^{2}}-\\frac{1}{\\lambda } & \\frac{2}{\\lambda^{3}}-\\frac { 1}{\\lambda^{2 } } & -1\\\\ \\frac{1}{\\lambda^{2}}-\\frac{1}{\\lambda } & \\frac{1}{2\\lambda-1}-\\frac { 1}{\\lambda^{2 } } & \\frac{1}{\\left (   2\\lambda-1\\right )   ^{2}}-\\frac{1}{\\lambda^{3 } } & \\frac{\\lambda-1}{\\lambda}\\\\ \\frac{2}{\\lambda^{3}}-\\frac{1}{\\lambda^{2 } } & \\frac{1}{\\left (   2\\lambda -1\\right )   ^{2}}-\\frac{1}{\\lambda^{3 } } & \\frac{2}{\\left (   2\\lambda-1\\right ) ^{3}}-\\frac{1}{\\lambda^{4 } } & -\\frac{1}{\\lambda^{2}}\\\\ -1 & \\frac{\\lambda-1}{\\lambda } & -\\frac{1}{\\lambda^{2 } } & \\frac{2}{2-\\alpha}\\end{array } \\right ]   .\\ ] ] note that the elements of @xmath473 were obtained after tedious computations of the limits of the expectations @xmath474   $ ] for @xmath475 @xmath476 .",
    "analogue calculus of these quantities may be found in @xcite , @xcite and @xcite . finally , a standard calculation of the product @xmath477 yields@xmath478 which is denoted by @xmath479 this completes the proof of theorem [ theorem4]**.**@xmath344",
    "the main objective of this paper was to propose a bias - reduced estimator for the mean of a heavy - tailed distribution .",
    "this was achieved on the basis of the bias - reduction of the first and second order parameter estimators of regularly varying distributions developed by @xcite and the corresponding high quantiles estimators introduced by @xcite .",
    "in addition , the newly introduced estimator is asymptotically normal , making confidence intervals easily constructible .",
    "we conclude by simulation that , compared to that of peng , our new estimator has smaller bias and rmse and consequently it performs better .",
    "9999999999999999999999999999999999999    beirlant , j. , matthys , g. , diercks , g. , 2001 .",
    "heavy - tailed distributions and rating .",
    "astin bull .",
    "31 , no . 1 , 37 - 58 .    beirlant , j. , diercks , g. , guillou , a. , stric , c. , 2002 . on exponential representations of log - spacings of extreme order statistics .",
    "extremes 5 , no . 2 , 157 - 180 .",
    "beirlant , j. , figueiredo , f. , gomes , m.i . , vandewalle , b. , 2008 .",
    "improved reduced - bias tail index and quantile estimators .",
    "j. statist .",
    "inference 138 , no . 6 , 1851 - 1870 .",
    "brahimi , b. , meraghni , d. , necir , a. , zitikis , r. , 2011 .",
    "estimating the distortion parameter of the proportional hazard premium for heavy - tailed losses .",
    "insurance : mathematics and economics 49 , 325 - 334 .",
    "caeiro , f. , figueiredo , f. , gomes , m. i. , 2004 . bias reduction of a tail index estimator through an external estimation of the second order parameter . statistics .",
    "38 ( 6 ) , 497510 .",
    "caeiro , f. , gomes , m.i . ,",
    "rodrigues , l.h . , 2009 .",
    "reduced - bias tail index estimators under a third - order framework .",
    "theory methods 38 , no .",
    "6 - 7 , 1019 - 1040 .",
    "cheng , s. , peng , l. , 2001 .",
    "confidence intervals for the tail index .",
    "bernoulli 7 , no . 5 , 751 - 760 .",
    "csrg , s. , mason , d.m . , 1985 .",
    "central limit theorems for sums of extreme values .",
    "cambridge philos .",
    "_ 98 , no . 3 , 547 - 558 .",
    "csrg , s. , deheuvels , p , mason , d.m . , 1985 .",
    "kernel estimates of the tail index of a distribution .",
    "3 , 10501077 .",
    "csrg , m. , csrg , s. , horvth , l. , mason , d.m . , 1986 .",
    "weighted empirical and quantile processes .",
    "14 , no . 1 , 31 - 85 .",
    "danielsson , j. , de haan , l. , peng , l. , de vries , c.g . ,",
    "2001 . using a bootstrap method to choose the sample fraction in tail index estimation .",
    "j. multivariate anal .",
    "76 , no . 2 , 226 - 248 .",
    "dekkers , a.l.m . , de haan , l. , 1993 .",
    "optimal choice of sample fraction in extreme - value estimation .",
    "j. multivariate anal .",
    "47 , no . 2 , 173 - 195 .",
    "drees , h. , kaufmann , e. , 1998 . selecting the optimal sample fraction in univariate extreme value estimation .",
    "stochastic process .",
    "75 , no . 2 , 149 - 172 .",
    "feureverger , a. , hall , p. , 1999 .",
    "estimating a tail exponent by modelling departure from a pareto distribution .",
    "27 , no . 2 , 760 - 781 .",
    "fraga alves , m.i . , gomes , m.i .",
    ", de haan , l. , neves , c. , 2007 . a note on second order conditions in extreme value theory : linking general and heavy tail conditions .",
    "revstat . 5 , no . 3 , 285 - 304 .",
    "goegebeur , y. , de wet , t. , 2011 .",
    "estimation of the third - order parameter in extreme value statistics .",
    "test doi : 10.1007/s11749 - 011 - 0246 - 2 .",
    "gomes , m.i . ,",
    "martins , m.j . , 2002 .",
    "`` asymptotically unbiased '' estimators of the tail index based on external estimation of the second order parameter . extremes . 5 , no . 1 , 5 - 31 .    gomes , m.i . , martins , m.j .",
    ", 2004 . bias reduction and explicit semi - parametric estimation of the tail index .",
    "j. statist .",
    "plann . inference .",
    "124 , no . 2 , 361 - 378 .",
    "gomes , m.i .",
    ", figueiredo , f. , 2006 . bias reduction in risk modelling : semi - parametric quantile estimation .",
    "test 15 , no . 2 , 375 - 396 .",
    "gomes , m.i .",
    ", pestana , d. , 2007 .",
    "a sturdy reduced - bias extreme quantile ( var ) estimator .",
    "477 , 280 - 292 .",
    "de haan , l. , stadtmller , u. , 1996 .",
    "generalized regular variation of second order .",
    "j. austral .",
    "a 61 , no . 3 , 381 - 395 .",
    "de haan , l. , peng , l. , 1998 .",
    "comparison of tail index estimators .",
    "52 , no . 1 , 60 - 70 .",
    "de haan , l. , ferreria , a. , 2006 .",
    "extreme values theory : an introduction .",
    "springer .",
    "hall , p. 1982 . on some simple estimators of an exponent of regular variation .",
    "b. 44 , 37 - 42 .",
    "hall , p. , welsh , a. h. , 1985 .",
    "adaptive estimates of parameters of regular variation .",
    "13 , 331 - 341 .",
    "hill , b.m . , 1975 .",
    "a simple general approach to inference about the tail of a distribution .",
    "3 , no . 5 , 1163 - 1174 .",
    "li , d. , peng , l. , yang , j. , 2010 .",
    "bias reduction for high quantiles .",
    "j. statist .",
    "inference 140 , no .",
    "9 , 2433 - 2441 .",
    "mason , d. , 1982 .",
    "laws of large numbers for sums of extreme values .",
    "10 , no . 3 , 754 - 764 .",
    "matthys , g. , delafosse , e. , guillou , a. , beirlant , j. , 2004 .",
    "estimating catastrophic quantile levels for heavy - tailed distributions .",
    "insurance math .",
    "34 , no . 3 , 517 - 537 .",
    "necir , a. , meraghni , d. , 2009 . empirical estimation of the proportional hazard premium for heavy - tailed claim amounts . insurance math .",
    "45 , no . 1 , 49 - 58 .    necir , a. , meraghni , d. , 2010 .",
    "estimating l - functionals for heavy - tailed distributions and applications .",
    "journal of probability and statistics 2010 , i d 707146 .",
    "neves , c. , fraga alves , m.i .",
    "reiss and thomas automatic selection of the number of extremes .",
    "statist . data anal .",
    "47 , no . 4 , 689 - 704 .",
    "peng , l. , 2001 . estimating the mean of a heavy tailed distribution .",
    "3 , 255264 .    peng , l. , qi , y. , 2004 . estimating the first- and second - order parameters of a heavy - tailed distribution .",
    "n. z. j. stat .",
    "2 , 305312 .    reiss , r .- d . ,",
    "thomas , m. , 2007 . statistical analysis of extreme values with applications to insurance , finance , hydrology and other fields , 3rd ed .",
    "birkhuser verlag , basel , boston , berlin .",
    "rolski , t. , schimidli , h. , schimd , v. , teugels , j.l . , 1999 .",
    "stochastic processes for insurance and finance .",
    "john wiley and sons , new york .",
    "weissman , i. , 1978 .",
    "estimation of parameters and large quantiles based on the @xmath37 largest observations .",
    "statist . assoc .",
    "364 , 812 - 815 .",
    "wellner , j.a . ,",
    "limit theorems for the ratio of the empirical distribution function to the true distribution function .",
    "z. wahrsch .",
    "gebiete 45 , no . 1 , 73 - 88 .",
    "* proof . *",
    "we have @xmath483 as @xmath119 then @xmath484   .\\ ] ] a straightforward calculation of the derivative of @xmath398 yields@xmath485   .",
    "\\label{eql}\\ ] ] observe now , that inequalities ( [ phi ] ) imply@xmath486 from @xcite , we have@xmath487 it follows that@xmath488 on the other hand , in view lemma 3 in @xcite , we infer that@xmath489 by applying the mean value theorem to the functions @xmath490 and @xmath491 respectively , then by using ( [ sup2 ] ) and ( [ sup1 ] ) , we show readily that , as @xmath151@xmath492 note that the first result of ( [ t1 ] ) implies that @xmath493 by using equations ( [ t1 ] ) and ( [ t2 ] ) together , we show that , uniformly in @xmath481   , $ ] the right - hand side of equation @xmath494 is equal to @xmath495@xmath344      proof .",
    "we only show the first result . the second one is obtained by similar arguments .",
    "recall that@xmath498 \\\\ &   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\times\\left [ \\dfrac{1-v_{n}\\left (   1-s\\right )   } { 1-u_{n - k , n}}-\\dfrac{s}{1-u_{n - k , n}}\\right ]   ds.\\end{aligned}\\ ] ] using lemma [ lemma2 ] , we get , as @xmath151@xmath499 by a change of variables , we get@xmath500 making use of approximation@xmath501 yields@xmath502 in other words@xmath503 the expectation of the first term of right - hand side of the previous equation is less than or equal to@xmath504   ds.\\ ] ] using the fact that @xmath505 we show that the previous quantity is less than or equal to @xmath506 since both integrals @xmath507 and @xmath507 are finite , then @xmath508 as @xmath509      reiss and thomas in @xcite , proposed a heuristic method for choosing the optimal number of upper extremes used in the computation of the tail index estimate . in this paper , we adopt this algorithm by making use of peng and qi estimator @xmath510 which is defined by the system of two equations ( [ alpha - beta ] ) . by this methodology",
    ", one defines the optimal sample fraction of upper order statistics @xmath74 by@xmath511 with suitable constant @xmath512 the quantity @xmath513 corresponds to peng and qi estimator of the shape parameter @xmath34 based on the @xmath514 upper order statistics.@xmath348on the light of our simulation study , * *  * * we obtained reasonable results by choosing @xmath515 the same value for @xmath516 has also been observed by @xcite when employing hill s estimator .",
    "the software programs of this methodology are incorporated in the `` xtremes '' package accompanying the book of @xcite ."
  ],
  "abstract_text": [
    "<S> we use bias - reduced estimators of high quantiles , of heavy - tailed distributions , to introduce a new estimator of the mean in the case of infinite second moment . </S>",
    "<S> the asymptotic normality of the proposed estimator is established and checked , in a simulation study , by four of the most popular goodness - of - fit tests for different sample sizes . moreover , we compare , in terms of bias and mean squared error , our estimator with peng s estimator ( peng , 2001 ) and we evaluate the accuracy of some resulting confidence intervals .    </S>",
    "<S> * keywords : * bias reduction ; extreme values ; heavy - tailed distributions ; hill estimator ; regular variation ; tail index . </S>"
  ]
}