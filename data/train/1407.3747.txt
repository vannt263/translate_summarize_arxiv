{
  "article_text": [
    "switching autoregressive processes with markov regime can be looked at as a combination of hidden markov models ( hmm ) and threshold regression models .",
    "the switching autoregressive process have been introduced in an econometric context by goldfeld and quandt ( 1973 ) @xcite and they have become quite popular in the literature ever since hamilton ( 1989 ) @xcite employed them in the analysis of the gross internal product of the usa for two regimes : one of contraction and another of expansion .",
    "this family of models , combines different autoregressive models to describe the evolution of the process at different periods of time , the transition between these different autoregressive models being controlled by a hmm .    switching linear autoregressive processes with markov regime",
    "have been extensively studied and several applications in economics and finance can be found in , for instance , krolzig ( 1997 ) @xcite , kim and nelson ( 1999 ) @xcite , hamilton and raj ( 2003 ) @xcite .",
    "this models are also widely used in several electrical engineering areas including tracking of maneuvering targets , failure detection , wind power production and stochastic adaptive control ; see for instance , tugnait ( 1982 ) , doucet _ et al . _",
    "( 2000 ) , douc _ et al . _",
    "( 2005 ) @xcite , and ailliot and monbet ( 2012 ) @xcite .    switching non linear autoregressive models with markov regime have considerable interest in the statistical community , especially for econometric series modelling .",
    "such models were considered in particular by francq and roussignol ( 1997 ) , @xcite .",
    "they consider models that admit an additive decomposition , with particular interest in the switching arch models , franq _ et al . _",
    "( 2001 ) @xcite .",
    "krishnarmurthy and rydn ( 1998 ) @xcite , douc _ et al . _",
    "( 2004 ) @xcite studied an even more general class of switching non linear autoregressive processes that do not necessarily admit an additive decomposition .",
    "we consider a particular type of switching non linear autoregressive models with markov regime , called markov switching nonlinear autorregresive process ( ms - nar ) and defined by : @xmath1 where @xmath2 are i.i.d .",
    "random variables , the sequence @xmath3 is an homogeneous markov chain with state space @xmath4 , and @xmath5 are the regression functions , assumed to be unknown .",
    "we denote by @xmath6 the probability transition matrix of the markov chain @xmath7 , i.e. @xmath8 $ ] , with @xmath9 .",
    "we assume that the variable @xmath10 , the markov chain @xmath3 and the sequence @xmath2 are mutually independents .",
    "this model is a generalization of switching linear autoregressive models with markov regime , also known as ms - ar model .",
    "when the regression functions @xmath11 are linear the ms - nar process is simply a ms - ar model .    in the parametric case , i.e. when the regression functions depend on an unknown parameter , the maximum likelihood estimation method is commonly used .",
    "the consistency of the maximum likelihood estimator for the ms - nar model is given in krishnarmurthy and rydn ( 1998 ) @xcite , while the consistency and asymptotic normality are proved in a more general context in douc _",
    "( 2004 ) @xcite .",
    "several versions of the em algorithm and its variants , for instance sem , mcem , saem are implemented for the computations of the maximum likelihood estimator , we refer to @xcite .",
    "a semiparametric estimation for the ms - nar model was studied in ros and rodrguez ( 2008 ) @xcite , where the authors consider a conditional least square approach for the parameter estimation and a kernel density estimator for the estimation of the innovation density probability .",
    "nevertheless , the first question that must be addressed before studying  consistency \" of any estimation procedure for this models is whether the autoregresion functions are identified . in the context of the nonparametric estimation of ms - nar model",
    ", this problem has not yet been discussed in the literature .",
    "a treatment of this problem can be approached following the recent results given by de castro _ et al_. ( 2015 ) @xcite for nonparametric estimation of hmm models .",
    "the general idea is to identify the markov regime , and then to ensure that the estimation method provides a unique estimate .",
    "we considere nonparametric estimators obtained through the minimization of a quadratic contrast , which has a unique minimum given by the nadaraya - watson estimator when the markov chain is observed , ensuring in this case that the regression functions are identified non - parametrically .",
    "however , in the case of partially observed data we can prove the identifiability up to label swapping of the hidden states .",
    "we do not address in this paper a rigorous proof of this statement , because its content is enough material for other work , moving away from the objectives and techniques outlined in this article .",
    "in this work we consider a nonparametric regression model .",
    "that is , for @xmath12 , we define a nadaraya - watson type kernels estimator , given by @xmath13 this nadaraya - watson type estimator was introduced for hmm models in harel and puri @xcite .    in the first part",
    ", we establish the uniform consistency , assuming that a realization of the complete data @xmath14 is known ; i.e. the convergence over compact subsets @xmath15 , @xmath16    this is an interesting asymptotic result , but the key feature of ms - nar models is that the state sequence @xmath3 is generally not observable , so that statistical inference has to be carried out by means of the observations @xmath17 only .    in the nonparametric context , the estimators of regression functions @xmath18 , for each @xmath19 and @xmath20 , can be interpreted as solutions @xmath21 of the local weighted least - squares problem @xmath22 where the weights are specified by the kernel @xmath23 , so that the observations @xmath24 near to @xmath19 has the largest influence on the estimate of the regression function at @xmath19 .",
    "this is , @xmath25    when a realization of the state sequence @xmath3 is observed the solutions of this problem is the nadaraya - watson kernel estimators @xmath26 defined in .",
    "nevertheless , when @xmath3 is a hidden markov chain , the solution do not have closed - form , and we need to approximate it .    in the second part , we propose a recursive algorithm for the estimation of the regression functions @xmath27 with a monte - carlo step which restores the missing data @xmath3 by @xmath28 , and a robbins - monro procedure in order to estimate the unknown value of @xmath29 .",
    "this approximation minimizes the potential @xmath30 by the gradient algorithm @xmath31 with @xmath32 any sequence of real positive numbers decreasing to @xmath33 , and @xmath34 the gradient of @xmath35 with respect to the vector @xmath36 .    in a general context ,",
    "the robbins - monro approach is study in duflo @xcite .",
    "whereas em - type algorithms with kernel estimation are used in benaglia _ et .",
    "@xcite for finite mixtures of nonparametric multivariate densities , and for finite mixture of nonparametric autoregression with independents regime in franke _ et .",
    "we establish the consistency of the estimator obtained by our robbins - monro algorithm .",
    "this asymptotic property is obtained for each fixed point @xmath19 .",
    "the paper is organized as follows . in section 2 ,",
    "we present general conditions on the model that ensure the existence of a probability density distribution , the stability of the model , and we prove that it satisfies the strong mixing dependence condition .",
    "futhermore , we prove the uniform consistency of the naradaya - watson kernels estimator in the case of complete data .",
    "section 3 , we prove the main result , the consistency of estimator related to our robbins - monro algorithm .",
    "section 4 contains some numerical experiments on simulated data illustrating the performances of our nonparametric estimation procedure .",
    "the some proofs are deferred to the appendix [ appa ] .",
    "in this section we shall review the key properties of ms - nar model , that we shall need for proving results .",
    "then , we prove the uniform consistency of the nadaraya - watson kernel estimator when is assumed that a realization of complete data is available .",
    "the study of the stability of the model is relatively complex for the ms - nar model . in this section",
    "we recall known results over the stability of this model given by yao and attali @xcite .",
    "our aim is to resume the sufficient conditions which ensure the existence and the uniqueness of a stationary ergodic solution for the model , as well as the existence of moment of order @xmath37 of the respective stationary distribution .    1 .",
    "the markov chain @xmath38 is positive recurrent .",
    "hence , it has an invariant distribution that we denote by @xmath39 .",
    "2 .   the functions @xmath11 , for @xmath40 , are continuous .",
    "3 .   there are exists positive constants @xmath41 , @xmath40 , such that for @xmath42 , the following holds @xmath43 4 .",
    "@xmath44 , where @xmath7 is random variable with values in @xmath4 and distribution @xmath45 .",
    "@xmath46 , for some @xmath37 . 6 .",
    "the sequence @xmath2 of random variables has common density probability function @xmath47 with respect to the lebesgue measure .",
    ".   there exists @xmath48 such that @xmath49 , where @xmath50 is a compact set of @xmath51 .",
    "condition e1 implies that @xmath52 with space states @xmath53 is a markov process . under condition e2 this is a feller chain and it is a strong feller chain if in addition the condition e6 holds .",
    "the model is called sublinear if conditions e2 and e3 hold . for ms - nar sublinear the following result is given .",
    "consider a sublinear ms - nar .",
    "assuming e1-e7 , we have    * there exists a uniqueness stationary geometric ergodic solution . *",
    "if the spectral radius of the matrix @xmath54 is strictly less than 1 , with @xmath55 the same that in condition e4 , then @xmath56 .    for the markov chain s stability",
    "the moment condition @xmath37 is enough , but for the asymptotic properties of kernel estimator will be necessary @xmath57 .",
    "we present a technical lemma where one of the results states the existence of conditional densities of model , and in addition we give a factorization of this density probability .",
    "this factorization will be very useful in the next sections .",
    "let us first introduce some notations :    * @xmath58 stands for the random vector @xmath59 , and @xmath60 we mean a realization of the respective random vector . *",
    "the symbol @xmath61 denotes the indicator function of set @xmath62 , which assigns the value @xmath63 if @xmath64 and @xmath33 otherwise . *",
    "@xmath65 denotes the density distribution of random vector @xmath58 evaluated at @xmath66 .",
    "we consider the following assumption :    1 .",
    "the random variable @xmath10 has a density function @xmath67 with respect to lebesgue measure .",
    "the following lemma is relevant in the frame of kernel estimation .",
    "[ exisdensidad ] under conditions d1 and e6 ,    * the random vector @xmath68 admits the probability density function @xmath69 , equal to @xmath70 with respect to the product measure @xmath71 , where @xmath72 and @xmath73 denote lebesgue and counting measures , respectively . *",
    "if @xmath74 is a bounded density , then the joint density of @xmath75 satisfies @xmath76    for the proof of this lemma we refered to the reader to appendix [ appa ] .      a strictly stationary stochastic process @xmath77 is called strongly mixing , if @xmath78 as @xmath79 , where @xmath80 , with @xmath81 , is the @xmath82-algebra generated by @xmath83 , and is absolutely regular mixing , if @xmath84 as @xmath79 .",
    "the values @xmath85 are called strong mixing coefficients , and the values @xmath86 are the regular mixing coefficients . for properties and examples under mixing assumptions see doukhan @xcite . in general , we have @xmath87 .    note that the @xmath0-mixing coefficients can be rewritten as : @xmath88 in the case of a strictly stationary markov process @xmath7 , with space state @xmath89 , kernel probability transition @xmath6 and invariant probability measure @xmath45 , the @xmath90-mixing coefficients take the following form ( see doukhan @xcite , section 2.4 ) : @xmath91    [ modeloesmixing ] the ms - nar model under condition e1 is strictly stationary , is @xmath0-mixing and their coefficients @xmath92 decrease geometrically .",
    "the proof is postponed to appendix [ appa ] .      in this section",
    "we assume that a realization of the complete data @xmath93 is available .",
    "we focus in the uniform convergence over compact sets of the nadaraya - watson kernel estimator defined in .    for a stationary ms - nar model ,",
    "the quantity of interest in the autoregression function estimation is @xmath94 which can be rewritten as @xmath95 hence , it is sufficient to estimate each autoregression function @xmath96 for @xmath12 and @xmath42 .",
    "let us denote @xmath97 the nadaraya - watson kernel estimator of @xmath11 is @xmath98 with @xmath99 and @xmath100 .    in order to obtain the convergence of the ratio estimator @xmath101",
    "we apply the method used by g. collomb , see ferraty _ et .",
    "@xcite , which studies simultaneously the convergence of @xmath102 and @xmath103 , when @xmath104 tend to @xmath105 . but before this , we relate the conditions that will allow us to obtain the asymptotic results .",
    "let us take a kernel @xmath106 , positive , symmetric , with compact support such that @xmath107 .",
    "we assume that the kernel @xmath23 as well as the density @xmath74 are bounded , i.e    1 .",
    "2 .   @xmath109 .    under condition b1 ,",
    "the kernel @xmath23 is of order 2 , i.e. @xmath110 and @xmath111 .",
    "let @xmath50 be a compact subset of @xmath51 , we assume the following regularity conditions :    1 .",
    "there exist finite constants @xmath112 , such that @xmath113 2 .",
    "the density @xmath114 of @xmath10 , @xmath74 , and @xmath11 has continuous second derivatives on the interior of @xmath50 .",
    "3 .   for all @xmath115 , the functions @xmath116 are continuous .",
    "we define @xmath117 , which is continous from condition r3 .",
    "let @xmath118 be a sequence of real number satisfying the following condition    1 .   for all @xmath119 , @xmath120 , @xmath121 and @xmath122 .",
    "finally , we impose one of the two following moment conditions :    1 .",
    "@xmath123 and @xmath124 .",
    "2 .   @xmath125 and @xmath46 , for some @xmath57 .",
    "note that m1 implies m2 , and m2 implies e5 , which is a sufficient condition for the stability of ms - nar model .    from independence between @xmath10 and @xmath126",
    ", condition m1 implies @xmath127 moreover , e3 and m2 imply @xmath128 , this condition is also implied by m1 .",
    "condition m1 is assumed in order to obtain the a.s .",
    "uniform convergence over compact sets and m2 for the a.s pointwise convergence .",
    "now , we establish the uniform convergence over compact sets of the nadaraya - watson kernel estimator @xmath26 defined in . for this , we need the following three technical lemmas .",
    "their respectives proofs are reported to appendix [ appa ] .",
    "the first lemma allows to treat in a unified way the asymptotic behavior of the variances and covariances of @xmath129 and a truncated version of @xmath130 .",
    "the others two lemmas given the asymptotic bound for the bias and variance term in the estimation of the regression functions @xmath11 s .",
    "[ asympt_cov ] assume the model ms - nar satisfying conditions e1-e2 , e5-e6 , d1 , b1-b2 , r3 and s1 .",
    "let @xmath131 then , the following statements hold :    * @xmath132 .",
    "* @xmath133 .    for all @xmath134 ,",
    "* @xmath135 .",
    "[ cvarianza ] assume that the model ms - nar satisfies conditions e1-e2 , e5-e6 , d1 , b1-b2 , r2-r3 , s1 and m2 .",
    "let @xmath136 be a positive sequence and @xmath137 , then the following asymptotic inequalities hold true .",
    "* @xmath138 , * @xmath139 .    [ lsesgo ]",
    "assume that the model ms - nar satisfies conditions e1 , e6 , d1 , b1 and r2 .",
    "then the following statements hold true",
    ".    * @xmath140 . *",
    "@xmath141    lemma [ asympt_cov ] is a preliminary result necessary in order to prove the lemma [ cvarianza ] .",
    "[ conuniforme ] assume that the model ms - nar satisfies conditions e1-e4 , e6-e7 , d1 , b1-b2 , r1-r3 and s1 .",
    "then ,    1 .   if @xmath142 and condition m2 holds , @xmath143 2 .",
    "if @xmath142 and condition m1 holds , @xmath144    the proof of theorem is reported to appendix [ appa ] .",
    "in this section we present our robbins - monro type algorithm for the nonparametric estimation of ms - nar model in the partial observed data case , and we prove the consistency of the estimator .    the nadaraya - watson estimator @xmath145 , for each @xmath19 , can be interpreted as the solution of a local weighted least - squares problem , in our case this consists to find the minimum of the potential @xmath35 defined by @xmath146 with respect to @xmath21 in a convex open set @xmath147 of @xmath148 .",
    "thus , the regression estimator @xmath149 is given by @xmath150    in the partial observed data case , this is when we not observe @xmath38 , we can not obtain an explicit expresion for the solution @xmath151 .",
    "then , we must consider a recursive algorithm for the aproximation of this solution .",
    "our approach approximates the estimator @xmath151 by a stochastic recursive algorithm similar to that of robbins  monro , @xcite .",
    "this involves two steps : first a monte - carlo step which restores the missing data @xmath152 , and a second step where we consider a robbins - monro s approximation in order to minimize the potential @xmath35 .    here are some further notations that we will take into account .",
    "* for each @xmath153 , @xmath154 is the number of visits of the markov chain @xmath155 to state @xmath156 in the first @xmath104 steps , and @xmath157 is the number of transitions from @xmath156 to @xmath158 in the first @xmath104 steps .",
    "* @xmath159 is a vector containing the estimated functions @xmath160 and the estimated probability transition matrix @xmath161 , in the t - th iteration of the robbins - monro algorithm .",
    ": :    pick an arbitrary initial realization    @xmath162 .",
    "compute the    estimated regresion functions    @xmath163    from equation in term of the observed data @xmath164 and    the initial realization @xmath165 , and compute the    estimated transition matrix @xmath166 , by    @xmath167 for    @xmath168 .",
    "define    @xmath169 .",
    "for @xmath170 ,    step r. : :    restore the corresponding unobserved data by drawing an sample    @xmath28 from the conditional distribution    @xmath171 .",
    "step e. : :    update the estimation @xmath159 by    @xmath172    where    @xmath173 , and    @xmath174 .",
    "step a. : :    reduce the asymptotic variance of the algorithm by using the averages    @xmath175 instead of    @xmath176 , which can be recursively computed by    @xmath177 , and    @xmath178    the following result enables us to write the algorithm as a stochastic gradient algorithm .",
    "let , @xmath179 with @xmath180 and @xmath181 the @xmath82-algebra generated by @xmath182 . the proof is given in appendix [ appa ] .",
    "[ lema_gradest ] for each @xmath183 we have , @xmath184 and @xmath185    therefore , the restauration - estimation algorithm is a stochastic gradient algorithm that minimizes @xmath186 and it can be written as @xmath187 where @xmath188    so , the stochastic gradient algorithm is obtained by perturbation of the following gradient system @xmath189    in the following we describe in detail each step of the algorithm .",
    "we use a stochastic approximation version of em algorithm , proposed by delyon et al .",
    "@xcite , in order to maximize the likelihood of the data .",
    "assume that the regression functions are linear and the noise is gaussian .",
    "this algorithm proved to be more computationally efficient than a classical monte carlo em algorithm due to the recycling of simulations from one iteration to the next in the smoothing phase of the algorithm .",
    "the used saem algorithm is detailed in section 11.1.6 of @xcite .",
    "the r step of the algorithm corresponds to conditional simulation given @xmath190 .",
    "we describe the sampling method for the conditional distribution @xmath191 for all @xmath192 .",
    "carter and kohn @xcite obtained samples @xmath193 following a stochastic version of the hidden markov model forward - backward algorithm first proposed by baum _",
    "et al_. @xcite .",
    "this follows by noting that @xmath171 can be decomposed as @xmath194 provided that @xmath195 is known , @xmath196 is a discrete distribution , suggesting the following sampling strategy : for @xmath197 and @xmath12 , compute recursively the optimal filter as @xmath198    then , sample @xmath199 from @xmath200 and for @xmath201 , @xmath202 is sampled from @xmath203    following the proof in rosales @xcite , we will show that the sequence @xmath204 is an ergodic markov chain with invariant distribution @xmath205 .",
    "it is sufficient to note that the sequence @xmath206 is an irreducible and aperiodic markov chain on a finite state space , @xmath207 .",
    "irreducibility and aperiodicity follow directly from the positivity of the kernel , @xmath208    in this case the standard ergodic result for finite markov chains applies , ( kemeny y snell @xcite ) @xmath209    moreover , is satisfied with @xmath210 , @xmath211 and @xmath212 for @xmath213 .      in each iteration of this algorithm , we evaluate @xmath214 the gradient of the potential . for each @xmath153 , we compute the components @xmath215 in each iteration this quantity is updated .",
    "it has the advantage of that the ratio @xmath26 is not computed directly , avoiding the zeros of the function @xmath129 .      to reduce the asymptotic variance of the estimated parameters @xmath216",
    ", we adopt the averaging technique introduced by polyak and juditsky ( see @xcite ) .",
    "the idea is to use the averages @xmath217 defined by @xmath218 instead of @xmath219 , which can be recursively computed by means of the equation .",
    "the convergence analysis of robbins - monro approximations are well studied in duflo @xcite in the general case . in this paper",
    "we use a similar framework as in cappe _ et .",
    "( @xcite , pg .",
    "431 ) , for the convergence of the stochastic gradient algorithm for the likelihood function in hidden markov models , considering that in our particular case @xmath220 is a continuously differentiable function of @xmath29 .",
    "the following convergence result is given for each @xmath19 fixed .",
    "[ conv_rm ] assume that @xmath32 is a positive sequence such that @xmath221 and that the closure of the set @xmath222 is a compact subset of @xmath147 .",
    "then , almost surely , the sequence @xmath222 satisfies @xmath223 . furthermore , @xmath224 and @xmath225 , a.s .",
    "let @xmath226 .",
    "the sequence @xmath227 is an @xmath228 martingale , in fact @xmath229 moreover , it satisfies @xmath230 .",
    "indeed , @xmath231 and @xmath232 where @xmath233 are bernoulli centered random variables . then , @xmath234 is @xmath235 with @xmath236 .",
    "thus , by cauchy - schwarz s inequality we have @xmath237 and @xmath238 where @xmath239 and @xmath240 by compactness @xmath241 is finite , therefore @xmath242 thus , by applying conditional borel - cantelli lemma in cappe _ et .",
    "al _ ( @xcite , lemma 11.2.9 ) the sequence @xmath227 has a finite limit a.s . and according to theorem 11.3.2 in @xcite the sequence @xmath216 satisfies @xmath243 by continuity of the function @xmath244 we proved that @xmath245 satisfies @xmath225 , and by cesro theorem , @xmath246 .",
    "the critical point @xmath247 of the gradient of @xmath248 , has @xmath156-th component @xmath249 given by @xmath250 we can prove , following the theorem [ conuniforme ] , that @xmath251 , _ a.s .",
    "_ , whenever @xmath79 . as a consequence",
    "we obtain @xmath252",
    "in this section we illustrate the performances of the algorithms developed in the previous section by applying them to simulated data .",
    "we work a ms - nar with @xmath253 states and autoregressive functions @xmath254 where @xmath255 is a bump function and @xmath256 is a decreasing logistic function .",
    "these functions was considered by franke _ et .",
    "let @xmath74 be a gaussian white noise with variance @xmath257 .",
    "the transition probability matrix is given by @xmath258    we used a straightforward implementation of the algorithms described .",
    "we generate a sample of length @xmath259 . for each @xmath260",
    ", we simulate @xmath202 and then use it to simulate @xmath24 .",
    "the simulated data is plotted in figure [ fig1 ] ( left ) .    for the estimation of the regression function @xmath11 , we use the standard gaussian density as the kernel function @xmath23 , in spite of the fact that it is not compactly supported . as bandwidth parameter",
    "we take @xmath261 .    assuming that the complete data @xmath262 is available , we show in figure [ fig1 ] ( right ) the performance of @xmath255 and @xmath256 ( solid curved line ) and their respective kernel estimates ( dotted curved line ) .",
    "we implemented the restauration - estimation algorithm for the data described above .",
    "the initial estimates for the markov chain @xmath263 in the step 0 of our algorithm was obtain by using a saem algorithm for the ms - ar model , @xmath264 the parameters estimates obtained using this implementation are , @xmath265 and the linear functions estimates are of the form @xmath266 and @xmath267 .",
    "figure [ fig2 ] ( left ) shows the scatter plot of @xmath24 against @xmath268 and the linear adjustment .",
    "we implement our robbins - monro procedure with @xmath269 iterations and the smoothing step was defined as @xmath270 in figure [ fig2 ] ( right ) we show the scatter plot of @xmath24 against @xmath268 , @xmath255 and @xmath256 ( solid curved lines ) and the respective robbins - monro estimates ( dotted curved lines ) for the ultimate iteration .",
    "take @xmath271 , with @xmath272 , for @xmath273 . then",
    ", the jacobian matrix of transformation @xmath274 is triangular and the absolute value of the jacobian is equal to @xmath63 , hence in virtue of the change of variables theorem @xmath275 from independence of @xmath10 , @xmath3 and @xmath2 , we have the following factorization @xmath276 and by conditions d1 and e6 we obtain @xmath277 thus , the first result follows .",
    "let @xmath282 .",
    "since we assume that @xmath10 , the markov chain @xmath3 and the sequence @xmath2 are mutually independents , we have by the properties of the conditional expectation @xmath283 since the sequence @xmath3 is strictly stationary under condition e1 , then @xmath284 thus , the sequence @xmath285 is strictly stationary .",
    "now , we only need to prove that @xmath290 decrease geometrically to 0 , when @xmath104 goes to @xmath105 . for this",
    ", we use that if the initial distribution of the strictly stationary markov chain @xmath7 is the invariant measure @xmath45 then @xmath291 and @xmath292 is geometric ergodic , i.e. there exists @xmath293 such that @xmath294 , obtaining that @xmath295 thus , the result follows .                  * for @xmath313",
    "we consider only the case @xmath314 , where it holds , @xmath315 so that , by continuity of the function @xmath316 and the moment condition of @xmath126 , @xmath317 * for @xmath318 , assume @xmath319 , @xmath320 then by continuity of the function @xmath321 , then @xmath322 and for @xmath323 the result is followed in the same way exchanging @xmath324 by @xmath325 .",
    "secondly , we use tran s device to split the covariance of @xmath340 into terms : @xmath341 in a similar way that in the first bound , applying part ii ) of lemma [ asympt_cov ] with @xmath337 , @xmath338 , we obtain for @xmath342 @xmath343 for @xmath344 we apply part iii ) of lemma [ asympt_cov ] obtaining @xmath345      therefore , @xmath348 .",
    "+ the fuk - nagaev s inequality ( see rio @xcite theorem 6.2 ) , applied to random variables @xmath349 , allows as to obtain @xmath350 in order to the rate of convergence of the precendent term , we take @xmath351 obtaining the asymptotic inequality @xmath352        since @xmath359 , then the equation ( [ sesgo ] ) implies that @xmath360 by second order taylor s expansion of @xmath329 at @xmath19 we obtain , @xmath361 with @xmath362 for some @xmath363 $ ] . + as the kernel @xmath23 is assumed to be of order 2 , substituting the taylor s approximation into gives @xmath364 from condition r2 , we have that @xmath365 is continuous",
    ". then @xmath366 converge uniformly to @xmath367 over the compact set @xmath50 .",
    "hence @xmath368 thus , + @xmath369 the same proof works for the bias of @xmath370 , starting from @xmath371    we start with the following triangle inequality on the positivity set of @xmath372 , @xmath373 according to the bias - variance decomposition , the proof of the theorem is achieved through the lemmas [ cvarianza ] and [ lsesgo ] , warranting the existence of strictly positivity of @xmath374 .",
    "thus , applying lemma [ cvarianza ] with @xmath375 , @xmath376 , @xmath377 , and @xmath378 large enough so that @xmath379 , we have @xmath380 for @xmath381 and @xmath382 then , @xmath383 applying the borel - cantelli lemma , the almost surely pointwise convergence of @xmath384 to @xmath33 is proved .",
    "we proceed analogously in order to obtain the almost surely pointwise convergence of @xmath385 .        in order to obtain the uniform convergence on a compact set @xmath50",
    ", we only need to prove an asymptotic inequality of type for the term @xmath388 , and analogously for @xmath389 , in inequality . for this",
    ", we proceed by using truncation device as in ango nze _ et .",
    "@xcite , assuming the moment condition m1 .",
    "let us set @xmath390 and the truncated variable @xmath326 .",
    "then , we define the truncated kernel estimator of @xmath329 by @xmath330 since @xmath108 , taking @xmath391 , then clearly we obtain @xmath392 and , by the cauchy - schwarz inequality and condition r3 , @xmath393 now , we reduce computations to a chaining argument , ( see @xcite , pgs . 32 and 78 ) for the case of a kernel estimator with bounded variables .",
    "let @xmath50 be covered by a finite number @xmath394 of intervals @xmath395 with diameter @xmath396 and center at @xmath397 .",
    "then , @xmath398    now , let us examine each term in the right - hand side above .",
    "first , we have from lemma [ cvarianza ] @xmath399 for the second and third terms , we use the following inequality obtained from condition r1 , @xmath400 for some constants @xmath401 . therefore , @xmath402 let us set @xmath403 and @xmath404 we obtain @xmath405            taking expectation in ( [ potencialu ] ) , it follows that ( [ constrastelimite ] ) is true .",
    "for the second part , we use simply the fact that the potential @xmath35 is absolutly integrable with respect to the measure @xmath410 with @xmath411 the counting measure on @xmath412 .",
    "so , by the dominated convergence theorem we have"
  ],
  "abstract_text": [
    "<S> we consider nonparametric estimation for autoregressive processes with markov switching . in this context , the nadaraya - watson type estimator of regression funtions is interpreted as solution of a local weighted least - square problem , which does not closed - form solution in the case of hidden markov switching . </S>",
    "<S> we introduce a nonparametric recursive algorithm to approximate the estimator . </S>",
    "<S> our algorithm restores the missing data by means of a monte - carlo step and estimate the regression function via a robbins - monro step . </S>",
    "<S> consistency of the estimator is proved using the strong @xmath0-mixing property of the model . </S>",
    "<S> finally , we present some simulations illustrating the performances of our nonparametric estimation procedure .    ,     + </S>"
  ]
}