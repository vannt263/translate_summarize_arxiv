{
  "article_text": [
    "constraints or prior information derived from sparsity is widely used for regularization in signal processing . depending on the application domain ,",
    "the signal of interest may exhibit additional features than mere sparsity . in this paper , we consider signals whose coefficients can be clustered in a few groups where each group itself has few active members .",
    "sparse signals with isolated non - zeros may be considered to fall in that category .",
    "we propose a prior function that promotes such signals and demonstrate how to use the function in basic inverse problems of potential interest .",
    "many natural phenomena can be associated with a sparse underlying process with isolated non - zero components .",
    "for instance , the dft coefficients of a periodic signal are equidistant with respect to the frequency variable .",
    "consequently , quasi - periodic audio signals like speech , music can be represented in the time - frequency domain ( via linear transforms @xcite ) using components that appear isolated along the frequency axis ( i.e. , harmonics ) .",
    "another example is related to reflection seismology , where one aims to discover the subsoil layers by sending seismic waves and processing the returning seismic trace @xcite",
    ". the seismic trace can be modelled as the convolution of the input seismic wave and the reflection function .",
    "the reflection function is a sparse signal with non - zeros occuring due to difference in acoustic impedance between the boundaries of different layers .",
    "since the layers are expected to have some non - zero thickness , the non - zeros , which occur at the boundaries , are isolated .",
    "other than these natural signals , isolated sparsity is also relevant for designed systems .",
    "for instance , in frequency hopping systems , the parameters of the signal components are constant in between the hopping instances , during which transmission occurs @xcite .",
    "since transmission has to last for some finite amount of time , the hopping instances may be regarded as isolated non - zeros of a sparse signal .    in order to isolate the non - zeros",
    ", we work with non - overlapping groups of variables and process the groups independently . we propose a penalty whose threshold function ( to be specified below ) has the following properties :    * if the magnitudes of all variables in a group fall below a threshold , the whole group is set to zero .",
    "* otherwise , a group - dependent threshold is applied so as to eliminate the relatively insignificant coefficients in the group .",
    "the group - dependent threshold serves to separate the large magnitude coefficients from the rest .",
    "specifically , if there are @xmath0 large - magnitude coefficients in the group , they are kept with little modification , while the rest are set to zero . for @xmath1 ,",
    "this isolates the non - zeros within the group .",
    "we remark that this behavior is achieved with a non - adaptive penalty function and without reweighting .    to be more precise , assuming that the size of the groups is @xmath2 , and that @xmath3 denotes the coefficients belonging to the @xmath4 group of @xmath5 , we define a penalty function for @xmath6 as , @xmath7 where for @xmath8 , @xmath9 is defined as , @xmath10 the term enclosed in parentheses in grows rapidly as the number of large magnitude coefficients in the group increases .",
    "therefore @xmath9 strongly penalizes groups containing many large coefficients .",
    "given this penalty , we describe how to realize the associated threshold function ( or the proximity operator @xcite ) defined for @xmath11 as @xmath12 we show that @xmath13 is well - defined when @xmath14 and study its behavior .",
    "we also show that , as @xmath15 , the threshold function suppresses all but the largest coefficient in each group , provided the magnitude of the largest coefficient exceeds the threshold @xmath16 .",
    "we demonstrate the use of the proposed penalty and the threshold function in a convex formulation for audio denoising and a non - convex formulation for non - blind deconvolution .",
    "we provide convergent algorithms for both formulations and demonstrate that the reconstructions perform favorably compared to those obtained using other penalties / threshold functions .",
    "the proposed penalty function may be regarded as a member of the family of group - based penalty functions ( see e.g. @xcite for a sample of the literature ) .",
    "in contrast to our interest , many of these works seek to set whole groups of coefficients to zero , thus achieving sparsity across groups , and do not enforce sparsity within groups .",
    "for instance , the @xmath17 norm @xcite is obtained by replacing @xmath18 with @xmath19 in .",
    "the proximity operator associated with the @xmath17 norm sets a whole group to zero if the energy of the group is below a threshold but keeps the group with little modification otherwise .",
    "on the other hand , in the elitist - lasso ( e - lasso ) formulation @xcite ( see also @xcite where the method is referred to as exclusive - lasso ) , the target signal contains few non - zeros within each group and sparsity is not enforced across groups .",
    "sparsity within groups is also addressed by the sparse - group lasso ( sgl ) proposed in @xcite .",
    "sgl uses a convex combination of an @xmath20 norm and an @xmath17 norm as the penalty  it may also be interpreted as a sum of elastic - net - like penalties @xcite applied to each group . therefore sgl uses a convex penalty function .",
    "sgl was extended to non - overlapping groups and its performance is thoroughly analyzed in @xcite .",
    "another class of related penalties are based on correlations extracted from the observation matrix @xcite .",
    "given an observation model of the form @xmath21 , the idea is to derive a positive semi - definite weighting matrix @xmath22 from the correlations between the columns of @xmath23  and use it to define a penalty of the form @xmath24 .",
    "since @xmath22  does not depend on @xmath5 , the penalty in @xcite is convex .",
    "the targeted effect is uniform treatment of the components of @xmath5  that produce similar responses .",
    "this contrasts with the proposed penalty because if two components of @xmath5  have similar responses , the proposed penalty @xmath9 would prefer to single out one of the components and suppress the other .",
    "another recent paper that takes into account correlations between the columns of @xmath23  is @xcite .",
    "a bivariate non - convex penalty is proposed so as to enforce sparsity stronger than alternative convex penalties , while maintaining the convexity of the overall problem .",
    "sparsity within groups is not specifically sought in @xcite .",
    "the penalty proposed in this paper , @xmath9 is non - convex .",
    "however , its degree of non - convexity is controlled by the parameter @xmath25 and this in turn allows to formulate convex problems . as will be clarified in the sequel ( see the proof of prop .",
    "[ prop : weaklyconvex ] ) , @xmath9 can be related to the e - lasso penalty .",
    "however , the e - lasso penalty is convex and can be shown to contain an additive energy term , which in turn penalizes higher coefficients more strongly .",
    "further , the e - lasso threshold never sets the whole group to zero , unless the group is zero to start with ( see @xcite , remark-6 ) .",
    "thus if a group consists entirely of noise , it will not be totally eliminated , even if it has components with small magnitudes .",
    "the threshold function associated with the proposed penalty function contains a deadzone such that if the coefficients in the group fall in the deadzone , the whole group is eliminated .",
    "therefore the proposed penalty / threshold functions aim to achieve sparsity within and across groups .      throughout the paper ,",
    "vectors are denoted using small case letters , as in @xmath5 .",
    "the @xmath4 component of @xmath5  is denoted as @xmath26 .",
    "we are interested in partitions of @xmath5 into groups in this paper .",
    "we already used @xmath27  to denote the @xmath4  subgroup of @xmath5 .",
    "that is , for a length-4 vector @xmath28 , if we form two groups of size two , by collecting together consecutive components , we have @xmath29  and @xmath30 .",
    "however , with the exception of sec .",
    "[ sec : hybrid ] , the functions under study are separable with respect to groups .",
    "therefore , whenever separability applies , we suppress the group superscript in @xmath27 and use @xmath5 to simplify notation , with the understanding that the same discussion applies to all of the groups .    for a scalar @xmath31 ,",
    "the soft threshold function with threshold @xmath32 is defined as , @xmath33 if @xmath5  is a vector , the soft thresholding operator applies to each component of @xmath5  separately .    the proximity operator of a convex , lower semi - continuous function @xmath34  is defined as @xcite @xmath35 we also refer to @xmath36 as the threshold function , if @xmath34  under dicsussion is a penalty function .    throughout the paper , for a given length-@xmath2 vector @xmath37 ( complex or real valued ) , we define the cost function @xmath38 as @xmath39 where @xmath9  is given in . the threshold function @xmath40 is defined , in line with , as , @xmath41 we remark that the penalty function used in this paper is not convex but weakly convex . in order for the threshold function to be well - defined , the minimizer of @xmath42 ( i.e. , the point that minimizes @xmath42 ) must be unique .",
    "to ensure uniqueness , we will check that @xmath42 is strictly convex .",
    "we motivate the proposed penalty function and derive the associated threshold function in section  [ sec : penalty ] .",
    "we discuss how the non - convex penalty function may be employed to formulate a convex denoising problem with a sparsifying frame and present a minimization algorithm in section  [ sec : denoise ] . in section  [ sec :",
    "deconvolution ] , we present a non - convex deconvolution formulation , study the convergence of an iterative thresholding algorithm for the presented formulation and demonstrate its performance .",
    "section  [ sec : conc ] is the conclusion .",
    "the penalty function @xmath43 introduced in is separable with respect to the groups and the groups are non - overlapping .",
    "thanks to these properties , it suffices to study the component function @xmath44 and the associated threshold function @xmath45 , with domain @xmath46 or @xmath47 .",
    "once @xmath45 ,  is specified , @xmath13  can be realized by applying @xmath45 to each group separately .",
    "we start our discussion in section  [ sec : r2 ] with penalty / threshold functions defined on @xmath48 , since this case is easier to visualize and interpret .",
    "after that , we generalize the discussion to @xmath46 in section  [ sec : rn ] .",
    "a discussion of how to tune the parameters and a numerical demonstration of the discussions is provided in sec .",
    "[ sec : parameters ] .",
    "extension of the study to @xmath47 is done in sec .",
    "[ sec : complex ] . finally , in sec .",
    "[ sec : hybrid ] , we briefly consider how the proposed penalty can be combined with @xmath17  norms to achieve a modified effect .      .",
    "notice that the function is not convex .",
    "[ fig : contour ] ]      for a fixed energy vector @xmath50 , we seek a penalty function @xmath51 such that ,    * if @xmath52 or @xmath53 , @xmath54 assumes a low value , * if @xmath55 , @xmath54 assumes a high value .",
    "observe that @xmath56 satisfies these requirements .",
    "however , @xmath57 is exactly zero when one of the components is zero . in order to penalize small non - zero components ,",
    "we add an @xmath20 term and propose the penalty @xmath58 where @xmath6 is a tuning parameter .",
    "notice that this is the restriction of the function in to @xmath48 .",
    "mesh and contour plots of this function are shown in fig .",
    "[ fig : contour ] .",
    "@xmath9 is not convex but it becomes convex when we add a quadratic .",
    "such functions are called weakly convex @xcite . for @xmath59",
    ", it can be shown that the function ` @xmath60 ' is convex ( see sec .",
    "[ sec : rn ] ) .",
    "therefore , @xmath25 may be regarded as a parameter that controls how much @xmath9 deviates from being convex . as a consequence of the weak - convexity of @xmath9 , we find that if @xmath61 , for a given @xmath62 , the cost function @xmath38 is strictly convex with respect to @xmath5 .",
    "thus , @xmath40 is well - defined when @xmath63 .    before we further discuss the threshold function",
    ", we would like to compare @xmath9 to the elitist - lasso ( e - lasso ) penalty function , which is known to favor large components in a group @xcite . for groups of size two ,",
    "the e - lasso penalty uses @xmath64 . expanding this , we can write , @xmath65 both @xmath66 and @xmath9 employ the term @xmath67 and this term is responsible for the ` elitist ' character of the penalties , by which we mean that the penalty assumes lower values for vectors with unequal components .",
    "the difference between the two penalties lies in the remaining components .",
    "@xmath9 contains an additive @xmath20 term which helps enforce sparsity if the components have small magnitudes .",
    "in contrast , @xmath66 contains an additive energy term , which renders the overall penalty convex at the expense of penalizing high magnitude coefficients more strongly .",
    "the threshold function @xmath45 can be derived via the optimality conditions for the minimization problem .",
    "let us assume that @xmath68 , i.e. , @xmath37 lies in the first quadrant ( extension to the other quadrants can be achieved by symmetry ) .",
    "let @xmath69 .",
    "for the regions @xmath70 , @xmath71 , @xmath72 , @xmath73 , defined in fig .",
    "[ fig : regions]a , @xmath74 is given as follows .",
    "@xmath75    ]    note that the regions are determined by the two parameters @xmath16 and @xmath25 ( see @xmath76 and @xmath77 in fig .  [",
    "fig : regions]a ) .",
    "@xmath73 is the deadzone ( i.e. , the collection of input vectors which are mapped to zero by the threshold function ) for the bivariate threshold function and is determined by the weight @xmath16 .",
    "the first component of the threshold function ( i.e. , the mapping that takes @xmath78 to @xmath79 ) is shown in fig .  [",
    "fig : t1]a .",
    "note that for this function , @xmath71 ( on which @xmath80 ) is also a deadzone .    ]",
    "the proposed threshold function behaves quite differently than a threshold function derived from a separable penalty of the form ` @xmath81 '",
    ". if the penalty is separable , even if @xmath82 is non - convex , the deadzone of the threshold function is rectangular ( and hence @xmath79 does not depend on @xmath83 ) .    the relevant regions and the first component of the e - lasso threshold are shown in fig .",
    "[ fig : regions]b and fig .",
    "[ fig : t1]b respectively .",
    "note that , unlike the proposed threshold ( which contains four different regions with constant gradient ) , the e - lasso threshold has three regions where its gradient is constant .",
    "also , unlike the proposed threshold , the e - lasso threshold does not contain a deadzone that eliminates both components .",
    "we now consider the function @xmath85 defined in and the associated threshold function . unlike @xmath48 , it is not easy to express the threshold function in closed form in @xmath46 . instead , we will derive a procedure to realize the threshold function . in order to justify the procedure",
    ", we will need to first study the properties of the penalty and the threshold functions .",
    "we start with the penalty @xmath9 .",
    "this function is not convex but it becomes convex if we add a quadratic .",
    "such functions are called weakly - convex @xcite .    for @xmath6",
    ", a function @xmath34 is said to be @xmath25-weakly convex if @xmath86 is convex .",
    "[ prop : weaklyconvex ] the function @xmath9 in is @xmath25-weakly convex .",
    "consequently , the cost function @xmath38 in is strictly convex with respect to @xmath5 if @xmath87 .    to see the first claim ,",
    "observe that , @xmath88 since @xmath89 is convex , this observation implies that the term in the parentheses in is @xmath90-weakly convex .",
    "since @xmath91 is convex , it follows that @xmath9 is @xmath25-weakly convex .    to see the strict convexity of @xmath92 with respect to @xmath5 , note that it can be written as the sum of a convex function and @xmath93 but",
    "the function in is strictly convex if @xmath87 and thus follows the claim .",
    "it also follows from prop .",
    "[ prop : weaklyconvex ] that , when @xmath87 , @xmath45 is well - defined thanks to the strict convexity of @xmath92 .    in the following ,",
    "we will derive two finite - terminating algorithms that realize @xmath45 .",
    "for that , we discuss the properties of @xmath45 .",
    "we start by showing that @xmath40 shrinks @xmath37 towards zero and it is monotone in the sense that it preserves the ordering of @xmath94 with respect to @xmath95 .",
    "more precisely , we have the following result .    [",
    "prop : monotone ] let @xmath69 , for @xmath87 .",
    "a.   if @xmath68 , then @xmath96 . if @xmath97 , then @xmath98 . b.   if @xmath99 , then @xmath100 . c.   if @xmath101 , then @xmath102 .",
    "see appendix  [ sec : proofmonotone ] .",
    "we now derive an expression for @xmath45 using the optimality conditions .",
    "notice that @xmath92 can be written as , @xmath103 using this expression , the optimality conditions are found as , @xmath104 where @xmath105 is a set valued separable function of the vector @xmath106 , whose @xmath107 component is given as , @xmath108 , & \\text{if } { \\ensuremath{\\hat{x}}}_k = 0,\\\\ \\{-1\\ } , & \\text{if } { \\ensuremath{\\hat{x}}}_k < 0 . \\end{cases}\\ ] ]    in the following , we assume for simplicity that @xmath109 s are non - negative and ordered , i.e. , @xmath110 .",
    "the general case can be recovered by a permutation of the vector components and changing signs , thanks to prop .",
    "[ prop : monotone ] .",
    "[ prop : monotone ] implies that there is an index @xmath111 , such that @xmath112 if @xmath113 and @xmath114 if @xmath115 . that is , @xmath0 denotes the number of non - zeros in @xmath106 .",
    "for this special integer @xmath0 , the optimality conditions can be written as ,    [ eqn : opt ] @xmath116    in order to find an expression for @xmath117 , let us define @xmath118 we can now express as @xmath119 multiplying both sides by @xmath120 ( noting @xmath121 ) and rearranging , we have @xmath122 therefore , @xmath123 the rhs of will be of interest in the following .",
    "let us therefore define , for each @xmath95 , @xmath124 plugging the expression in back into , we find the equivalent conditions    [ eqn : opt2 ] @xmath125    notice that the requirement @xmath112 for @xmath113 implies that @xmath126 for @xmath113 .",
    "the foregoing discussion is summarized in the following proposition .",
    "[ prop : thold ] let @xmath69 , for @xmath87 .",
    "also , let @xmath0 denote the number of non - zeros of @xmath127 .",
    "then , @xmath128 where @xmath129 ( with the convention @xmath130 ) .",
    "we remark that the description of the threshold function in prop .",
    "[ prop : thold ] is implicit because the integer @xmath0 in , namely the number of non - zeros in @xmath127 , depends on @xmath127 .",
    "we next discuss how to determine the integer @xmath0 .",
    "we will present two different search schemes for finding the correct value of @xmath0 .",
    "the following lemma will be useful for that end .",
    "[ lem : selectk ] suppose @xmath131 and @xmath87 .",
    "let @xmath132 be defined as in . then ,    a.   if @xmath133 , then @xmath134 , b.   if @xmath135 , then @xmath136",
    ".    see appendix  [ sec : proofselectk ] .",
    "we can use this lemma to develop a procedure for determining @xmath0 .",
    "it follows from the lemma that we can start from @xmath137 and keep increasing @xmath0 until @xmath138 .",
    "this procedure is summarized in algorithm  [ algo : klinear ] .",
    "@xmath139 @xmath140 @xmath141 @xmath142 @xmath143 .",
    "if @xmath0 is suspected to be small , then this algorithm terminates quickly . in the worst case",
    ", the algorithm will execute the ` while ' loop @xmath2 times .",
    "if , however , @xmath2 is large and @xmath0 is not expected to be small , then a binary search for @xmath0 might be computationally more suitable .",
    "the following discussion , that relies on lemma  [ lem : selectk ] implies that such a binary search terminates .",
    "suppose we pick an arbitrary @xmath95 and check the following conditions .",
    "@xmath144    notice that since @xmath145 , the conditions can not be violated simultaneously .",
    "now observe that    * if both conditions hold , the current guess of @xmath95 is equal to the sought @xmath0 . *",
    "if holds and is violated , then by lemma  [ lem : selectk ] , @xmath0 must be greater than @xmath95 . *",
    "if holds and is violated , then again by lemma  [ lem : selectk ] , @xmath0 must be less than @xmath95 .",
    "these observations lead to an implementation of @xmath45 as in algorithm  [ algo : nthold ] .",
    "in contrast to the @xmath146 complexity of algorithm  [ algo : klinear ] , this algorithm has @xmath147 complexity .",
    "@xmath139 @xmath140 @xmath148 @xmath141 @xmath149 @xmath150 @xmath149 @xmath151 @xmath152 @xmath153 @xmath149 @xmath154 @xmath155 @xmath156 .",
    "let us now discuss some special cases to better understand the role of the parameters @xmath16 and @xmath25 .",
    "as in the previous subsection , we will assume that @xmath157 and @xmath69 .",
    "observe first that @xmath158 . if @xmath159 for all @xmath95 , then @xmath160 .",
    "thus the deadzone of the threshold function is a cube of width @xmath16 in @xmath46 .",
    "suppose now that @xmath161 . in that case , we will definitely have @xmath162 .",
    "we find that @xmath163 notice that in order for @xmath164 to be non - zero , the threshold that @xmath83 needs to exceed has increased from @xmath16 by an amount proportional to @xmath165 .",
    "the higher @xmath166 is , the higher will be the new threshold .",
    "in fact , observe that as @xmath167 , the threshold converges to @xmath166 .",
    "since @xmath168 , we can therefore force only a single component to survive by choosing @xmath25 close to @xmath169 .",
    "when @xmath170 , we find that , @xmath171 thus the single surviving component is obtained by soft thresholding the largest component with @xmath16 .",
    "the following proposition provides further information on how the potential thresholds @xmath132 behave for arbitrary @xmath95 .",
    "[ prop : hi ] suppose @xmath131 and @xmath87 .",
    "let @xmath132 be defined as in .",
    "if @xmath133 , then @xmath172 .",
    "see appendix  [ sec : proofhi ] .    we know",
    "that if @xmath133 , then @xmath132 is not the actual threshold and @xmath173 .",
    "[ prop : hi ] implies that @xmath174 is actually greater than @xmath132 , but it is bounded above by @xmath175 .",
    "in fact , we can deduce from prop .",
    "[ prop : hi ] that @xmath176 in the case where the observations are purely noise , we would like to set @xmath177 for all @xmath95 .",
    "this motivates choosing @xmath178 , where @xmath179 denotes the noise standard deviation and @xmath180 is a constant around unity .",
    "once we fix the value of @xmath16 , the number of non - zero components , @xmath0 , and the threshold @xmath174 will depend on @xmath25 ( and @xmath37 ) .",
    "the following proposition provides precise bounds on @xmath25 .",
    "[ prop : gamma ] let @xmath69 where @xmath87 and @xmath68 for all @xmath95 .",
    "@xmath181 and @xmath182 if and only if    [ eqn : gamma ] @xmath183    see appendix  [ sec : proofgamma ] .    , where @xmath16 is fixed and @xmath25 is varied .",
    "@xmath184 denotes the number of non - zero components present in the clean signal.[fig : gamma ] ]    prop .",
    "[ prop : gamma ] suggests that , if we would like to retain more components in the estimate @xmath106 , then we need to choose a small @xmath25 so that @xmath185 is small . also , if the signal is scaled by multiplying with a factor @xmath186 , then the numbers on the rhs in stay approximately the same . therefore , the constant @xmath187 controls the number of non - zeros , almost independently of the scale . to summarize ,",
    "the following may serve as a guide for the selection of the parameters @xmath16 and @xmath25 .",
    "* @xmath16 can be chosen proportional to the noise standard deviation .",
    "* once @xmath16 is chosen , @xmath25 can be selected independently of the scale of the signal but should satisfy @xmath87 . the product @xmath187 determines the number of non - zeros in the estimate .",
    "higher it is , lower the number of non - zeros will be .    in order to demonstrate the foregoing discussion , we now consider two simple experiments on synthetic signals .    in both experiments ,",
    "the desired signal is of length 10 and it has @xmath184 non - zero components , where the non - zero values are obtained by sampling from a gaussian distribution .",
    "we add gaussian noise to this signal so that the observation snr is 5  db .",
    "for the first experiment , we apply @xmath45 to the observation for a fixed @xmath188 and varying @xmath25 .",
    "we repeat the experiment for 10k trials to obtain an average figure . the average gain in snr ( db ) with respect to @xmath187 is shown in fig .  [ fig : gamma ] .",
    "we see from the figure that the best @xmath187 value decreases with increasing @xmath184 .",
    "this is in line with prop .",
    "[ prop : gamma ] , which suggests that in order for the reconstruction to have more non - zeros , the product @xmath187 must be smaller .    , where @xmath16 is fixed and @xmath25 is varied .",
    "@xmath184 denotes the number of non - zero components present in the clean signal .",
    "as @xmath179  varies , the value of @xmath185  that maximizes the snr stays roughly constant.[fig : constgam ] ]    in a second experiment , we demonstrate that the best choice of the product @xmath185   is not affected much by the scale of the observations . for this , we vary the standard deviation of the noise , @xmath179 ,  as well as the signal energy so that the snr stays constant at 5  db .",
    "we set @xmath188 as in the previous experiment .",
    "for fixed @xmath184  value and varying @xmath179 , we apply the threshold using a range of @xmath25  values satisfying @xmath189 and compute the gain in snr . for each value of @xmath179  and @xmath25 ,",
    "an average snr gain value is obtained by repeating this for 10k trials .",
    "the resulting snr gain images are shown in fig .",
    "[ fig : constgam ] , for @xmath190  and @xmath191 .",
    "observe that for both @xmath190  and @xmath191 , the best choice of @xmath187  stays approximately constant as @xmath179  varies .      for @xmath192 , we extend @xmath9 straightforwardly as , @xmath193 the threshold function is similarly defined as",
    ", @xmath194 fortunately , the threshold function derived for @xmath46 applies for @xmath47 with a little modification .",
    "the following observation is useful for showing that .",
    "[ lem : arg ] suppose @xmath195 and @xmath196 . if @xmath197 , then @xmath198 .",
    "suppose @xmath199 .",
    "define @xmath200 such that @xmath201 for all @xmath95 and for @xmath202 , set @xmath203 .",
    "then , @xmath204 but @xmath205 , contradicting the fact that @xmath106 minimizes the cost in .    with the help of this lemma",
    ", we obtain an expression for @xmath206 in terms of @xmath45 .",
    "suppose @xmath195 .",
    "let @xmath207 denote the vector containing the magnitudes of the components of @xmath37 .",
    "let @xmath208 and @xmath209 .",
    "then , @xmath210 .",
    "notice that @xmath211 . using this observation ,",
    "it can be shown by a change of variables that if @xmath212 , then @xmath213 .",
    "now since @xmath214 , for all @xmath0 , it follows by lemma  [ lem : arg ] that @xmath215 are real and non - negative .",
    "thus , for the input @xmath207 , we can restrict the minimization in to @xmath46 .",
    "thus @xmath216 and the claim follows .",
    "it follows from this proposition that the threshold function on @xmath47 can be realized by applying @xmath45 to the magnitudes of the input , followed by a correction of the argument of the complex number .",
    "for this reason , in the following , we will not differentiate between @xmath45 and @xmath206 .",
    "we have so far considered a vector @xmath217 to comprise a group belonging to a larger signal .",
    "we now add an additional layer and consider subgroups of @xmath5  to define a hybrid penalty , that can be used to complement @xmath17  norms . in this setting",
    ", we refer to @xmath5  as a ` super - group ' .",
    "specifically , suppose @xmath5  is partitioned into @xmath218  non - overlapping  sub - groups , i.e. @xmath219 .",
    "also , let @xmath220  denote the length-@xmath218  vector whose @xmath4  component is @xmath221 .",
    "we define a hybrid penalty @xmath222 as , @xmath223 observe that , @xmath224 where @xmath225 .",
    "therefore , @xmath226  is @xmath25-weakly convex .",
    "the threshold function of @xmath226  is similarly defined as @xmath227 thanks to the weak - convexity of @xmath228 , @xmath229 is well - defined for @xmath63 .",
    "@xmath229 can be easily described using @xmath45 , as follows ( see also @xcite for a discussion of convex multi - layer penalties ) .",
    "[ prop : hybrid ] suppose a vector @xmath37 partitioned into @xmath218  groups where the @xmath4  group is denoted as @xmath230 . also , let @xmath220  denote the length-@xmath218  vector whose @xmath4  component is @xmath231 .",
    "let @xmath232 , @xmath233 .",
    "if we partition @xmath127 into @xmath218 groups similarly as @xmath37 ( with @xmath234  denoting the @xmath4  group ) , we have , @xmath235    see appendix  [ sec : proofhybrid ] .    in words",
    ", this proposition implies that the orientation of @xmath27  is the same as that of @xmath230 . to find the length of @xmath236 , i.e. , @xmath237",
    ", we apply the thresholding operator @xmath45  to @xmath220 .",
    "we will consider an application of this penalty and threshold function in sec .",
    "[ sec : denoise ] for a convex denoising formulation .",
    "we now consider the application of the proposed penalty in a denoising problem , when a sparsifying frame is given .",
    "that is , we assume that we have available a linear transform with a stable inverse ( see @xcite for a detailed discussion ) which allows to represent the signal with high fidelity using a small number of transform domain coefficients .",
    "we will specifically seek a convex formulation for this problem .",
    "let @xmath238 be a noisy observation of a clean signal @xmath239 for which a sparsifying frame is given .",
    "let @xmath240 and @xmath241 denote the analysis and synthesis operators for the frame @xcite .",
    "we assume that @xmath242 , i.e. , the frame is parseval @xcite .",
    "we have two choices for formulating the denoising problem , namely synthesis and analysis prior formulations @xcite .",
    "the two behave quite differently under a non - convex penalty such as the one considered in this paper .    in the setting described above",
    ", the synthesis prior denoising formulation is , @xmath243 if we denote the minimizer as @xmath244 , the denoised estimate is given as @xmath245 . in order to investigate convexity , let us rewrite the cost function in as @xmath246 + \\left [ \\frac{\\alpha}{2}\\|t\\|_2 ^ 2 + \\lambda\\,\\mathbf{p}_{\\gamma}(t ) \\right].\\ ] ] notice that the second term in square brackets in is convex if @xmath247 .",
    "the hessian of the first term in square brackets is @xmath248 .",
    "thus , the first term is also convex if @xmath249 . in that case",
    ", the problem in will be convex . however , if the frame is overcomplete , the condition @xmath249 is not satisfied for @xmath250 .",
    "therefore , we can guarantee the convexity of the synthesis prior problem only for @xmath251 , for which @xmath43 is equivalent to an @xmath20 norm .",
    "this leads us to consider the analysis prior formulation given as , @xmath252    suppose @xmath240 is the analysis operator of a parseval frame . the problem in is convex if @xmath253 .",
    "since the frame is tight , we have @xmath254 .",
    "therefore the cost function in can be written as , @xmath255 in , @xmath256 is an affine function and is therefore convex .",
    "the function @xmath257 in is convex when @xmath253 , by prop .",
    "[ prop : weaklyconvex ] . since pre - composition with",
    "a linear operator preserves convexity @xcite , @xmath258 is also convex .",
    "thus the cost function in can be expressed as the sum of two convex functions and is therefore convex .      in order to obtain a minimizer of",
    ", we adapt the douglas - rachford algorithm @xcite .",
    "the douglas - rachford algorithm is suitable for minimization problems of the form @xmath259 where both @xmath34 and @xmath260 are convex .",
    "the douglas - rachford iterations for such a problem are , @xmath261 where @xmath250 is a parameter and @xmath262 , @xmath263 are proximity operators associated with @xmath34 and @xmath260 ( as defined in ) . the sequence @xmath264 constructed in converges to some @xmath265 such that @xmath266 minimizes .",
    "the problem in is not readily suitable for the application of the douglas - rachford algorithm .",
    "we now transform the problem to write it in a suitable form .",
    "since @xmath242 , we have @xmath267 now if @xmath268 denotes the range of @xmath240 , we can change variables and obtain a problem equivalent to as , @xmath269 where @xmath270 is the indicator function of @xmath268 @xcite . if @xmath271 denotes a minimizer of , then @xmath272 minimizes . in this formulation ,",
    "both @xmath34 and @xmath260 are convex , provided that @xmath253 .",
    "thus the douglas - rachford algorithm is applicable for this splitting .",
    "we remark that in this setting , the proximity operator for @xmath273 is simply a projection onto @xmath268 ( see e.g. @xcite ) , which can be achieved by applying @xmath274 , thanks to the parseval property of the frame .",
    "the proximity operator for @xmath34 can be expressed in terms of the threshold function as follows .",
    "@xmath275    where @xmath276 .",
    "we remark that in passing from to , we discarded an additive term and removed a positive factor from the cost function ( equalities remain valid because we are seeking the minimizer ) .",
    "resulting pseudocode for the douglas - rachford iterations for this problem is given in algorithm  [ algo : denoise ] .",
    "initialize @xmath250 , @xmath277 .",
    "set @xmath278 @xmath279",
    "@xmath280@xmath281 @xmath282      we now demonstrate how the denoising formulation / algorithm performs on an audio signal and compare it against formulations that employ different regularizers .",
    "the clean signal is a speech signal , sampled at 16  khz , whose spectrogram is shown in fig .",
    "[ fig : exp1in]a .",
    "we use the short - time fourier transform ( stft ) as the tight frame in this experiment .",
    "the window size is 60  ms ( 960 samples ) and the hop - size is 15  ms ( 240 samples ) .    for regularization",
    ", we compare the @xmath20  norm , e - lasso , the @xmath17  norm and two different versions of the proposed penalty .",
    "our aim in this experiment is two - fold .",
    "first , we demonstrate the difference between the proposed penalty , e - lasso and the @xmath20  norm .",
    "second , we show that the proposed penalty can be used to complement the @xmath17 norm to obtain enhanced reconstructions .    in order to describe the group penalties ,",
    "consider fig .",
    "[ fig : tfgroups ] , which introduces notation for @xmath66 ( e - lasso penalty ) , @xmath283 ( proposed ) and @xmath17 norm . for @xmath66 and @xmath9 , we select the length along the time axis as @xmath284 and the width along the frequency axis as @xmath285 . this covers a frequency bandwidth of 320  hz . our aim is to exploit the isolated appearance of the harmonics viewed along the frequency axis . in contrast , for the @xmath17 norm , we take @xmath286 , @xmath287 . with this choice , we aim to collect the coefficients belonging to a harmonic into a single group .",
    "however , unlike @xmath66  and @xmath9 , @xmath17",
    "norm regularization does not specifically seek to isolate the harmonics like @xmath288 and @xmath9 . in order to obtain such an effect , we add an additional layer of grouping as depicted in fig .",
    "[ fig : tfgroups ] and use the penalty @xmath226  introduced in sec.[sec : hybrid ] .",
    "we stack 16 neighboring groups along the frequency axis , used in the @xmath17  norm to define non - overlapping super - groups for @xmath226 .    .",
    "the parameters @xmath289 and @xmath220 denote the length along the time axis and the width along the frequency axis respectively . ]",
    "we produce a noisy observation by adding noise ( see fig .",
    "[ fig : exp1in]b ) consisting of ambient sounds recorded in a casino ( machine sounds and crowd noise ) .",
    "notice that energy of the ambient noise is not uniform over the frequencies and decreases with increasing frequency .",
    "therefore , this noise can be considered pink .",
    "the input snr is 5  db .",
    "the spectrogram of the noisy signal is shown in fig .",
    "[ fig : exp1out]a . for this observation",
    ", we perform denoising using the different regularizers in the analysis prior formulation , namely .",
    "the denoising algorithms for the different regularizers can be obtained by replacing @xmath13 with the corresponding threshold functions in algorithm  [ algo : denoise ] . the value of the regularizer weight @xmath16 is selected by a sweep search for the @xmath20 norm , e - lasso and the @xmath17 norm ( output snr maximizing value is chosen ) .",
    "for @xmath290 , we set @xmath16 to be half the value of @xmath16 used for the @xmath20 norm and perform a sweep search for @xmath25 subject to @xmath291 , to maximize the output snr . for @xmath226",
    ", we similarly set @xmath16  equal to half the value used for the @xmath17  norm and search for the best @xmath25 .",
    "the optimal choices of @xmath25  were found as 2.68 and 7.85 for @xmath9 and @xmath226 , respectively .",
    "the resulting reconstructions are shown in fig .",
    "[ fig : exp1out ] .",
    "the output snrs are 6.42  db , 6.54  db , 6.67  db for @xmath20 regularization , @xmath288  and @xmath9  respectively .",
    "although the snrs are close to each other , the reconstructions show different behaviors .",
    "both @xmath20 regularization and the proposed regularization have been successful in removing noise in the time - frequency regions with no activity .",
    "however since e - lasso always keeps a component within a group , it has been less successful in suppressing noise in silent regions .",
    "we see that especially for higher frequencies , @xmath20 regularization suppresses the harmonics of the speech signal .",
    "in contrast , the proposed penalty admits a smaller weight @xmath16 and is able to retain high frequency harmonics , while achieving a similar suppression of noise as @xmath20 regularization .    for @xmath17",
    "regularization and @xmath226  regularization , the output snrs are 6.09 and 7.55 db , respectively .",
    "we observe that despite its lower snr , @xmath17  preserves the harmonics better than the @xmath20  norm .",
    "however , especially in the low frequency region , noise suppression is modest .",
    "while this can be overcome with a higher threshold @xmath16 for lower frequencies , such an approach requires choosing @xmath16  in a frequency dependent manner , which complicates the application .",
    "we observe however that @xmath226  achieves suppression of noise in a wide frequency range , without having to tune parameters for each frequency separately . specifically , noise within harmonics",
    "is eliminated similarly as in the reconstructions obtained by @xmath288 and @xmath9 .",
    "we also remark that specialized single - channel enhancement methods or more sophisticated penalties ( such as weighted group penalties , like those in @xcite ) can be used to achieve a superior performance than that of the proposed denoising method .",
    "nevertheless , we think that the proposed penalty function can be used to complement or modify such alternatives as demonstrated here .    ]",
    "norm , ( c )  the e - lasso penalty , ( d )  the proposed penalty , ( e ) the @xmath17  norm , ( f ) the hybrid penalty in sec .",
    "[ sec : hybrid ] obtained by combining the @xmath17  norm with the proposed penalty .",
    "[ fig : exp1out ] ]",
    "in a second application , we consider a sparse deconvolution problem . in order to be able to handle an arbitrary convolution operator , we forgo convexity and consider a non - convex formulation .",
    "we provide an algorithm for the provided formulation and discuss its convergence .",
    "we also compare the performance of the penalty / threshold function with a state - of - the - art iterative thresholding method .",
    "consider a minimization formulation as @xmath292 where @xmath23 denotes a convolution operator and @xmath43 is the proposed penalty .",
    "we remark that if @xmath23 is not invertible , then @xmath293 may not be convex unless @xmath251 ( see the discussion in the begining of sec .  [ sec : analysissynthesis ] ) .",
    "but if @xmath251 , @xmath43 is the @xmath20 norm .",
    "for this reason , unlike section  [ sec : denoise ] , we will not restrict the cost function to be convex in this section .",
    "we employ the forward - backward splitting algorithm ( fbs ) @xcite for obtaining a local minimizer of . in the current context",
    ", fbs constructs a sequence defined as , @xmath294 we remark that @xmath295 is well - defined when @xmath296 .",
    "this sets an upper bound on @xmath297 .",
    "if , in addition , @xmath298 , where @xmath299 denotes the spectral norm of @xmath23 , it can also be shown using majorization - minimization techniques @xcite that the sequence in monotonically decreases the cost , i.e. , @xmath300 .",
    "attouch et al .",
    "show in @xcite that the algorithm converges under the following additional conditions ,    a.   @xmath51 is a kurdyka - lojasiewicz function ( @xcite , defn .",
    "2.4 ) , b.   @xmath301 s form a bounded sequence .",
    "both of these conditions are satisfied for our setup .",
    "we first remark that the proposed penalty function @xmath9 is continuous and for each orthant in @xmath46 , it can be expressed as a polynomial function .",
    "therefore @xmath9 ( therefore @xmath43 ) is semi - algebraic ( see defn . 2.1 in @xcite ) and hence is a kurdyka - lojasiewicz function ( see the discussion at the end of sec .  2.2 in @xcite ) . also , the proposed penalty is coercive , i.e. , @xmath302 increases without bound as @xmath303 increases . therefore @xmath293 is also coercive and any sequence that monotonically decreases @xmath293 lies in a bounded set .",
    "to summarize , the following proposition is a corollary of thm .",
    "5.1 of @xcite .",
    "[ prop : kl ] if @xmath304 , then @xmath301 s in decrease the cost @xmath293 monotonically , and converge to a local minimizer .      in exploration seismology",
    "@xcite , the goal is to estimate an unknown reflectivity signal @xmath5 from the observed seismic trace @xmath238 , which is related to @xmath5 as , @xmath305 where @xmath306 represents the seismic wavelet and @xmath220 denotes white noise . denoting the convolution operator with @xmath306 as @xmath23 , we can use the formulation in to estimate @xmath5 from @xmath238 .",
    "we will assume that the seismic wavelet , @xmath306 , is known .",
    "specifically , we experiment with the band - pass ricker wavelet ( dominant frequencies in the range @xmath307 hz ) , sampled at @xmath308  hz , which is shown in fig .",
    "[ fig : seismicobservation]a .",
    "we remark that the operator @xmath23 used in the experiments is severely ill conditioned .",
    "we use a synthetic sparse reflectivity signal for this experiment .",
    "the signal is selected by sampling a stochastic process where the probability of observing a non - zero at a specific sample is 0.1 , provided that a non - zero has not occured in the last 10 samples .",
    "the value of the non - zero sample is obtained by sampling a normal distribution . notice that , this process is not a sparse bernoulli process but a markov process due to the dependence on the past .",
    "the resulting synthetic @xmath5 , of length @xmath309 is shown in fig .",
    "[ fig : seismicobservation]b .",
    "the observed seismic trace @xmath238 , generated according to is shown in fig .",
    "[ fig : seismicobservation]c .",
    "we used zero - mean white gaussian noise to produce @xmath238 .",
    "the input snr for this observation is 5  db .",
    "we compared the performance of the proposed algorithm with the sparse - group lasso formulation ( sgl ) @xcite and the iterated @xmath82-shrinkage ( ips ) algorithm @xcite , which was observed to perform very well for sparse deconvolution ( see e.g. the comparisons in @xcite ) .",
    "sgl aims to achieve sparsity within groups and uses as few groups as possible for reconstruction .",
    "the sgl penalty is given as , @xmath310 where @xmath311 and @xmath19 denotes the @xmath312 norm of the @xmath4 group .",
    "replacing @xmath43 with @xmath313 in , we obtain the sgl formulation .",
    "we set @xmath314 as in @xcite and make a sweep search for selecting @xmath16 .",
    "the groups consist of neighboring intervals of length @xmath315 .",
    "ips employs a threshold function depending on two parameters , namely @xmath16 and @xmath82 .",
    "the parameter @xmath82 determines the shape of the threshold function and defines a family of functions that lie between soft ( @xmath316 ) and hard threshold ( @xmath317 ) .",
    "we selected @xmath318 , which gave fairly good results .",
    "the parameter @xmath16 is the threshold value and is selected with a sweep search .    finally ,",
    "for @xmath43 , we use the same groups as sgl .",
    "we set @xmath319 and select @xmath16 with a sweep search .",
    "we remark that for the current setup , since the distance between two non - zeros of @xmath5  is at least 10 , each group of size 8 contains at most a single non - zero .",
    "this is the reason for choosing @xmath320 close to unity .",
    "if multiple zeros were expected within a group , a lower value of @xmath25  would be more feasible .",
    ".srer performance comparison for deconvolution [ table : srer ] [ cols=\"^,^,^,^\",options=\"header \" , ]     we considered four different input snrs ( 5 , 10 , 15 , 20  db ) and evaluated the deconvolution performance using signal to reconstruction error ratio ( srer ) , @xmath321 , where @xmath106  denotes the estimate . for each input snr value , we repeat the experiment for 500 different noise realizations to obtain average and standard deviation statistics of the performance .",
    "we set @xmath297 to be near the upper bound allowed in prop .",
    "[ prop : kl ] .",
    "we remark that the proposed formulation and ips are essentially non - convex formulations but we have seen that both algorithms converge in our experiments ( as claimed by prop .",
    "[ prop : kl ] and in @xcite ) .",
    "we also experimented with debiasing but for the snr range considered in these experiments , we found that debiasing actually results in lower srer for all of the methods .",
    "-shrinkage ( ips ) @xcite .",
    "each figure shows the signal to reconstruction error with respect to iterations .",
    "solid and dashed lines belong to the proposed algorithm and ips respectively .",
    "the means are marked with white circles .",
    "the other lines indicate three times the standard deviation from the means for each method . ]",
    "we observed an interesting trend with respect to different input snrs . for low input snrs , the proposed formulation performs better than the other two methods , in terms of average srer .",
    "as the input snr increases , the best srer achieved with ips sometimes surpasses those of the proposed formulation and sgl .",
    "however , the performance of ips varies more with respect to different trials . on average , the proposed formulation performs better than both methods .",
    "also , the proposed threshold function requires fewer iterations to fairly converge , although the iterations are computationally more costly . in order to visualize these",
    ", we show in fig .",
    "[ fig : reliabilityplot ] the srer performance with respect to iterations for input snrs 5 and 15  db . here , in addition to average srer , three times the standard deviation of the srer with respect to iterations is also shown .",
    "for the limits , the average srer and the standard deviation of the srer for the different algorithms are tabulated in table  [ table : srer ] . in conclusion ,",
    "the proposed penalty / formulation yields a better average srer over sgl , which targets a similar property . however , its performance is less consistent with respect to sgl as seen by a comparison of @xmath322 . on the other hand ,",
    "the proposed method is favorable compared to ips . especially for high input snrs",
    ", the average srer returned by ips can be higher but stability is poorer .",
    "we think this is partly due to the clean sparse reflectivity signal which enforces a certain distance between the zeros .",
    "this property is taken into account by the proposed formulation but the ips , which performs very well for arbitrary sparse signals , does not make use of this information .",
    "we proposed a group separable penalty function suitable for signals showing a sparse behavior both across and within groups of coefficients .",
    "we derived an associated threshold function @xmath13 and studied how it behaves as its parameters vary .",
    "we argued that a good strategy is to choose @xmath16 proportional to the noise standard deviation and @xmath25 according to how many non - zero coefficients are expected in each group .",
    "specifically , as @xmath15 , we showed that in each group , there remains at most one non - zero coefficient ( when the largest coefficient exceeds the threshold @xmath16 ) .",
    "we think that the proposed penalty / threshold would be of interest in several areas , such as eeg source localization @xcite , seismic deconvolution @xcite , audio processing , specifically decomposing audio signals into transient and tonal components @xcite , low - rank matrix recovery @xcite with a bound on the rank .",
    "we hope to consider such applications in future work .    * acknowledgement *    we thank pavel rajmic , brno university of technology , czech republic , for discussions and comments .",
    "recall that @xmath323 is the minimizer of @xmath38 in with respect to @xmath5 .",
    "a.   let @xmath68 .",
    "+ assume @xmath324 .",
    "define a new vector @xmath325 as @xmath326 then , @xmath327 and @xmath328 .",
    "therefore , @xmath329 . in words , @xmath325 achieves a strictly lower cost than @xmath106 , which is a contradiction .",
    "thus we must have , @xmath330 .",
    "+ assume @xmath331 .",
    "define a new vector @xmath325 as @xmath332 then , @xmath333 and @xmath328 .",
    "therefore , @xmath325 achieves a strictly lower cost than @xmath106 , which is a contradiction .",
    "thus we must have , @xmath334 .",
    "+ the second part of the claim follows similarly .",
    "b.   by part ( a ) , we can assume without loss of generality that @xmath37 has non - negative components .",
    "assume @xmath335 .",
    "note that by part ( a ) , @xmath334 , @xmath336 .",
    "suppose now that @xmath337 .",
    "define a new vector @xmath325 as @xmath338 then , @xmath339 . we obtain after some algebraic manipulations that @xmath340 it thus follows @xmath341 , which is a contradiction . thus @xmath342 .",
    "c.   without loss of generality , assume @xmath37 has non - negative components .",
    "assume also that @xmath343 . by part ( a )",
    ", we will have @xmath334 , @xmath336 .",
    "suppose now that @xmath344 .",
    "set @xmath345 and define a new vector @xmath325 as @xmath346 observe that @xmath347 and @xmath348 . using @xmath349 , we first note , @xmath350 also ,",
    "since @xmath351 , we find @xmath352 using these , we obtain , @xmath353 thus @xmath325 achieves a lower cost than @xmath106 , which is a contradiction",
    ". therefore @xmath354 . changing the roles of the indices @xmath218 and @xmath95",
    ", we must also have @xmath355 .",
    "therefore , @xmath356 .",
    "a.   since @xmath109 s are ordered , the assumption @xmath357 implies @xmath358 this in turn implies @xmath359 subtracting @xmath360 from both sides and rearranging , we obtain @xmath361 b.   the proof of this part is similar . since @xmath362 , the assumption @xmath135 implies @xmath363 adding @xmath364 to both sides and rearranging , we obtain @xmath365",
    "assume @xmath133 . this inequality implies , by the definition of @xmath132 in that @xmath366    we first show that @xmath367 . using in @xmath368 , we find @xmath369    let us now show that @xmath370 . notice that for positive @xmath371 , @xmath372 , @xmath180 , @xmath373 , @xmath374 if and only if @xmath375 .",
    "now if we set @xmath376 then @xmath377 and @xmath378 .",
    "but we have , by @xmath379 thus @xmath370 .",
    "we have that @xmath181 and @xmath182 if and only if @xmath380 . plugging in the definition of @xmath174 in , into the inequality @xmath381 , we obtain @xmath382 redistributing @xmath383 s we can write , @xmath384 this is equivalent to @xmath385 dividing both sides by the positive @xmath386 , we find , @xmath387 rearranging this equation , we obtain . can be shown similarly .",
    "in addition to the notation introduced in the proposition statement , let us also define @xmath388  to be length-@xmath218  vector such that @xmath389 . we first observe that if @xmath390 , then @xmath391 , for otherwise we could reduce the cost by setting @xmath391 .",
    "let us denote @xmath392 where @xmath393  is the set valued mapping defined in .",
    "then , the optimality conditions for imply that @xmath394 where @xmath395 is a unit norm vector such that @xmath396 .",
    "that is , if @xmath397 , then @xmath398 .",
    "this in turn implies that if @xmath399 , then since @xmath400 ( by the observation noted above ) , we must also have @xmath401 , or @xmath402 . taking inner products with @xmath395 in , we thus find , @xmath403 but these are the optimality conditions for the problem of minimizing @xmath404 .",
    "therefore , @xmath405 and the claim follows .",
    "h.  attouch , j.  bolte , and b.  f. svaiter .",
    "convergence of descent methods for semi - algebraic and tame problems : proximal algorithms , forward ",
    "backward splitting , and regularized gauss  seidel methods .",
    ", 137(1):91129 , february 2013 .",
    "p.  l. combettes and j .- c .",
    "proximal splitting methods in signal processing . in h.",
    "h. bauschke , r.  s. burachik , p.  l. combettes , v.  elser , d.  r. luke , and h.  wolkowicz , editors , _ fixed - point algorithms for inverse problems in science and engineering_. springer , new york , 2011 .",
    "a.  k. takahata , e.  z. nadalin , r.  ferrari , l.  t. duarte , r.  suyama , r.  r. lopes , j.  m.  t. romano , and m.  tygel .",
    "unsupervised processing of geophysical signals : a review of some key aspects of blind deconvolution and blind source separation .",
    ", 29(4):2735 , july 2012 ."
  ],
  "abstract_text": [
    "<S> we introduce a new weakly - convex penalty function for signals with a group behavior . </S>",
    "<S> the penalty promotes signals with a few number of active groups , where within each group , only a few high magnitude coefficients are active . </S>",
    "<S> we derive the threshold function associated with the proposed penalty and study its properties . </S>",
    "<S> we discuss how the proposed penalty / threshold function can be useful for signals with isolated non - zeros , such as audio with isolated harmonics along the frequency axis , or reflection functions in exploration seismology where the non - zeros occur on the boundaries of subsoil layers . </S>",
    "<S> we demonstrate the use of the proposed penalty / threshold functions in a convex denoising and a non - convex deconvolution formulation . </S>",
    "<S> we provide convergent algorithms for both formulations and compare the performance with state - of - the - art methods . </S>"
  ]
}