{
  "article_text": [
    "the integration of gpus in trigger and data acquisition systems is currently being investigated in several hep experiments . at higher trigger levels , when the efficient parallelization of event reconstruction algorithms is possible , the benefit of significantly reducing the number of the farm computing nodes is evident  @xcite . at lower levels , where tipically severe real - timeconstraints are present and custom hardware",
    "is used , the advantages of gpus adoption are less straightforward . a pilot project within the cern na62 experiment@xcite is investigating the usage of gpus in the central low level trigger processor , exploiting their computing power to implement efficient , high throughput event selection algorithms while retaining the real - timerequisites of the system .",
    "one of the project preliminary results was that employing commodity nics and standard software stack caused data transfer over gbelinks from readout boards to gpu memories to consume the largest part of the time budget and was the main source of fluctuations in the overall system response time . in order to reduce data transfer latency and its fluctuations",
    ", we envisioned the usage of the gpudirect rdma technology , injecting readout data directly from the nic into the gpu memories without any intermediate buffering and the offloading of the network stack protocol management from the cpu , avoiding os jitter effects .",
    "we implemented these two features in the nanet : the first was inherited from the apenet+3d nic development  @xcite while the second was realized integrating an open ip provided by the fpga vendor .",
    "nanetnic currently supports three apelink34  gbps channels  @xcite and one gbeone ; a 10gbeversion of the design is under development .    after introducing the na62 trigger system and motivating the usage of gpus in its trigger processor",
    ", we will provide a description of the nanetarchitecture , implementation and performances focusing on its usage as 1  gbenic in the case study of the l0 trigger processor for the rich detector .",
    "the na62 experiment at cern  @xcite has the goal of measuring the branching ratio of the decay of the charged kaon into a pion and a pair . due to the very high precision of theoretical prediction on this branching ratio",
    ", a precise measurement at the level of 100 events would be a stringent test of the standard model , also being this branching ratio highly sensitive to any new physics particle .",
    "compared to first observations of this decay  @xcite , the na62 experiment aims at collecting more events ( @xmath0 ) with a signal to background ratio 10:1 , using a novel technique with a ( 75  gev ) unseparated hadron beam decaying in flight .",
    "the experiment is currently in the final preparation stage , with the first period foreseen for fall 2014 .",
    "the expected standard model branching ratio is @xmath1 , requiring a very intense beam ( main kaon branching ratios is @xmath2 ) and efficient background rejection .",
    "the @xmath3 rate of particle decays reaching the detectors must be reduced by a set of trigger levels down to a @xmath4 rate .",
    "the entire trigger chain works on the main digitized data stream  @xcite . the first level ( l0 )",
    "is implemented in hardware ( fpgas ) on the readout boards and performs rather crude and simple cuts on the fastest detectors , reducing the data stream by a factor 10 to cope with the maximum design rate for event readout of 1  mhz .",
    "events passing l0 are transferred to the upper trigger levels ( l1 and l2 ) which are on a commodity pc farm . in the standard implementation ,",
    "the readout boards fpgas compute simple trigger primitives on the fly , then and send them to a central processor for matching and trigger decision .",
    "thus , the maximum latency allowed for the synchronous l0 trigger is related to the maximum data storage time available on the daq boards . for na62 this value is up to 1  ms , in principle allowing use of more compute demanding implementations at this level , _",
    "i.e._the gpus .",
    "the rich identifies pions and muons in the momentum range 15  @xmath5 to 35  @xmath5 with a @xmath6 suppression factor better than @xmath7 with good time resolution .",
    "erenkov light is produced in a 18  m long , 3.7  m wide tube filled with neon at atmospheric pressure . the light is reflected by a composite mirror of 17  m focal length , focused on two separated spots",
    "the two spots are equipped with @xmath8  pms of 1.8  cm in diameter each .",
    "after amplification and discrimination , the pm signal time is digitized by high resolution tdcs . a typical pion ring , for averaged accepted momentum , is identified with @xmath9 firing pms , as predicted by monte carlo and confirmed with a prototype  @xcite .",
    "time resolution was measured to be better than 100  ps for all momenta in the considered range .",
    "good time resolution and particle identification capability make this detector ideal for use in the trigger system to build stringent conditions .",
    "as a first example of gpu application in the na62 trigger system we studied the possibility to reconstruct rings in the rich .",
    "the center and the radius of the erenkov rings in the detector are related to particle angle / velocity .",
    "this information can be employed at trigger level to increase the purity and the rejection power for many triggers of interest .",
    "the ring reconstruction could be useful both at l0 and l1 . in both cases , because of the high rate of 10 and 1  mhz respectively , the computing power required is significant .",
    "the gpus can offers a simple solution of the problem .",
    "the use of video cards in the l1 is straightforward : the gpu can act as `` coprocessor '' to speed up the processing . on the other hand ,",
    "the l0 is a low latency synchronous level and feasibility of gpu usage must be verified . to test feasibility and performances , as a starting point",
    "we have implemented five algorithms for single ring finding in a sparse matrix of 1000 points ( centered on the pms in the rich spot ) with 20 firing pms ( `` hits '' ) on average .",
    "we tested these algorithms on tesla c1060 , c2050 , m2070 and k20  @xcite . in the following we focus on the fastest algorithm",
    " math  where the method is applied in a coordinate system in which the problem can be analytically solved  @xcite with a linear inversion .",
    "processing times with input data and results in gpu memory , for the math algorithm measured both on tesla m2070 and k20xm are plotted in fig .",
    "[ fig : lat_calc_math_m2070_k20x ] .",
    "the contribution of processing to the overall system latency can be kept under control due to the very small fluctuations in gpu kernel execution times .",
    "data communication between the tel62 readout boards and the l0 trigger processor ( l0tp ) happens over multiple gbelinks using udp streams .",
    "the main requisite for the communication system comes from the request for @xmath101  ms and deterministic response latency of the l0tp : communication latency and its fluctuations are to be kept under control .",
    "the requisite on bandwidth is 400@xmath11700  mb / s , depending on the final choice of the primitives data protocol which in turn depends on the amount of preprocessing actually be implemented in the tel62 fpga .",
    "so in the final system , 4@xmath116 gbelinks will be used to extract primitives data from the readout board towards the l0tp .",
    "we studied several options for the implementation of this multiple data communication system , benchmarking any of them for a single gbechannel in order to collect indications for the design of the system .",
    "a first result was that any solution matched the bandwidth specification for a gbelink at significant buffer sizes , so we concentrated on measuring communication latency and , most important in the context of the design of a real - timecommunication system , latency fluctuations . to perform benchmarks we used two different hardware platforms :    a supermicro superserver 6016gt - tf with x8dtg - df motherboard ( intel 5520-tylersburg chipset ) , dual intel xeon x5570 @2.93  ghz cpu , intel 82576 gbeand nvidiafermi m2070 gpu ( from here on m2070 system )    a supermicro superserver 7047gr - tprf with x9drg - qf motherboard ( intel c602-patsburg chipset ) , dual intel xeon e5 - 2609 @2,40  ghz cpu , intel i350 gbeand nvidiafermi k20xm gpu ( from here on k20xm system ) .",
    "first option considered was a standard linux installation ( centos 6.3 , kernel 2.6.33 ) with integrated gbeinterface in the m2070 system ; to measure latencies we used the network benchmarking utility _ _ sockperf__@xcite .",
    "results are shown in fig .",
    "[ fig : lat_comm_vanilla_rt_nanet_m2070_k20x ] ; at lower buffer sizes latencies are higher than desirable but main drawback of this setup is the great latency variability .",
    "next option in the attempt of reducing latency fluctuations was trying a real - timekernel on the m2070 system .",
    "a great effort has been recently done by os developers in improving rt features in kernels : predictability in response times , reduced jitters , @xmath6s accuracy and improved time granularity . in fig .",
    "[ fig : lat_comm_vanilla_rt_nanet_m2070_k20x ] results obtained with a ` 2.6.33.9-rt31-el6rt ` kernel are plotted ; cpuspeed and irqbalance daemons were stopped and interrupt moderation was disabled to avoid other possible sources of latency fluctuations .",
    "this approach was successful in minimizing fluctuations on latency but increased the latency values up to an incompatible level with the l0tp 1  ms time budget .",
    "another considered option was usage of ` pf ring `  @xcite , which is a framework for accelerating packet capture implementing a buffer allocated at socket creation , _",
    "i.e._where incoming packets are copied . `",
    "pf ring ` can use either standard drivers or drivers and works with gbenics .",
    "promising results obtained using this approach are reported and discussed in  @xcite .    finally , to tackle the real - timerequirement of the l0tp , we considered reusing the gpudirect rdma technology that we already implemented in the apenet+project for network card .",
    "this led to the design and implementation of the nanet nic featuring , besides gpudirect rdma capability , a udp offloading engine .",
    "latency benchmarks obtained using nanetboth in the m2070 and the k20xm system are shown in  [ fig : lat_comm_vanilla_rt_nanet_m2070_k20x ] .",
    "latency and its variability are significantly reduced when compared to other benchmarked solutions . in the following sections we describe the internal architecture of nanetand report a performance analysis for it and the rich l0tp using nanetas a communication channel from the readout boards .",
    "nanetis an apenet+rehaul for data acquisition able to inject directly data from the nic into the cpu / gpu memory with no intermediate buffering , reusing the apenet+gpudirect rdma implementation .",
    "moreover , it adds a network stack protocol management offloading engine to the logic to avoid os jitter effects .",
    "nanetdesign supports a configurable number and kind of i / o channels ( see figure  [ fig : nanet ] ) ; incoming data streams are processed by a physical link coding block feeding the data protocol manager that in turns extracts the payload data .",
    "these payload data are encapsulated in the apenet+data packet protocol by the nanetcontroller and sent to the apenet+network interface , taking care of their delivery to the destination memory .",
    "the nanet-1is a pciegen2 x8 nic featuring a standard gbeinterface able to directly inject an udp data stream into the memory of a fermi- or nvidiagpu leveraging on gpudirect rdma capabilities , implemented on a stratix iv gx fpga dev kit .",
    "moreover , it provides 3 apelinkchannels , with the addition of a custom mezzanine equipped with 3 qsfp+ connectors .",
    "the gbetrasmission is designed following the general i / o interface architecture pointed out in figure  [ fig : nanet ] .",
    "physical link coding is altera triple speed ethernet megacore ( tse mac ) , providing 10/100/1000  mbps ethernet ip modules .",
    "the _ udp offloader _ collects data coming from the tse mac , extracting udp packets payload and providing a wide channel achieving 6.4  gbps , discharging the ` nios  ii`from the data protocol management . finally , the _ nanet ctrl _ is the hardware module in charge of encapsulating the udp data in the proprietary apenet+protocol , parallelizing incoming data words into apenet+ones .",
    "the _ network interface _ , the packet injection / processing logic providing hardware support for remote direct memory access ( rdma ) protocol for cpu and gpu and the _ router _ with i / o channels multiplexing tasks are inherited from apenet+ .      software components for nanet-1operation are needed both on the x86 host and on the ` nios  ii ` @xmath6controller . on the x86 host , a gnu / linux kernel driver and an application library are present .",
    "the application library provides an api mainly for ` open / close ` device operations , registration ( _ i.e._allocation , pinning and returning of virtual addresses of buffers to the application ) and deregistration of circular lists of persistent receiving buffers ( clops ) in gpu and/or host memory and signalling of receive events on these registered buffers to the application ( _ e.g._to invoke a gpu kernel to process data just received in gpu memory ) . on the @xmath6controller ,",
    "a single process application is in charge of device configuration , generation of the destination virtual address inside the clop for incoming packets payload and virtual to physical memory address translation performed before the pciedma transaction to the destination buffer takes place .",
    "we measured nanet-1latency and bandwidth using different methods , then we tested it integrated in a simulated rich l0 trigger processor , measuring performances ( latency and throughput ) of the overall system .",
    "latency of nanet-1nic was benchmarked using several methods .",
    "firstly , we instrumented the fpga logic with a dedicated hardware path traversal latency measurement system able to add a `` profiling '' footer to the packet payload , storing up to 4 cycle counters values recorded at different packet processing stages .",
    "we were thus able to characterize the latency associated to processing in relevant nanet-1subsystems , namely the udp offloader , the ` nios  ii`@xmath6controller and the tx block in the network interface . in fig .",
    "[ fig : lat_hw_path ] a histogram is plotted with hardware processing path traversal latency inside nanet-1 : values show an appreciable variability , due to the ` nios  ii`@xmath6controller performing address generation and virtual to physical translation tasks .",
    "this clearly indicates the need for a redesign , implementing dedicated fpga logic blocks performing these two tasks .",
    "a second method was using one of the host gbeports to send udp packets according to the na62 rich readout data protocol to the nanet-1gbeinterface : using the x86 tsc register as a common reference time , it was possible in a single process test application to measure latency as time difference between when a received buffer is signalled to the application and the moment before the first udp packet of a bunch ( needed to fill the receive buffer ) is sent through the host gbeport . within this measurement setup",
    "( `` system loopback '' ) , the latency of the send process is also taken into account .",
    "measurements in fig .",
    "[ fig : lat_comm_vanilla_rt_nanet_m2070_k20x ] were taken using this method ; udp packets with a payload size of 1168  b ( 16 events ) were sent to a gpu memory receiving buffer of size variable between 1 and 64 udp packet payload sizes .",
    "connecting a tel62 readout board sending events stored onto the fpga through one of its gbeports to a nanet-1board , we were able , besides testing the integration of our nic in the working environment , to perform oscilloscope latency measurements as depicted in fig .",
    "[ fig : lat_tel62_32pkt_clop8 ] : a bunch of 32 udp packets is sent from the tel62 readout board ( red signal ) and 4 pciecompletion ( yellow signal ) show the end of the pciedma write transaction towards the gpu memory buffers , each sized 8 times the udp packet payload size .",
    "as anticipated , bandwidth measurement was also performed , both for the m2070 and the k20xm system : results are in fig .",
    "[ fig : throughput ] .",
    "a l0tp setup scaled down in bandwidth was reproduced by using a system loopback configuration , with the host system simulating the tel62 udp traffic through one of its gbeports towards a nanet-1nic redirecting incoming data stream towards a gpu memory circular list of receive buffers ; once received , such buffers are consumed by a cuda kernel implementing the math algorithm .",
    "communication and kernel processing tasks were serialized in order to perform the measure ; these are the results for the k20xm system in fig .  [",
    "fig : ap3iron2 ] , representing a situation . during normal operation , this serialization constraint can be relaxed , and kernel processing task overlaps with data communication . actually this is what has been done to measure system throughput , results are shown in fig .",
    "[ fig : throughput ] . combining the two results",
    ", we see that using gpu receive buffer sizes ranging from 128 to 1024 events allow the system to remain within the 1  ms time budget while keeping a @xmath12  mevents / s throughput .",
    "our nanetdesign proved to be efficient in performing real - timedata communication between the na62 rich readout system and the gpu - based l0 trigger processor over a single gbe link .",
    "these encouraging results are corroborated by benchmarks carried on using one apelink34  gbps channel supported by nanet-1  @xcite . to cope with the full system bandwidth requirement we started developing a nanetdesign supporting dual 10gbeon sfp+ ports .",
    "this work was partially supported by the eu framework programme 7 project euretile under grant number 247846 .",
    "g. lamanna , f. pantaleo , r. piandani and m. sozzi thank the gap project , partially supported by miur under grant rbfr12jf2z `` futuro in ricerca 2012 '' ; r. ammendola was supported by miur through infn suma project .",
    "10 url # 1#1urlprefix[2][]#2 clark p  j , jones c , emeilyanov d , rovatsou m , washbrook a and the atlas  collaboration 2011 _ journal of physics : conference series _ * 331 * 022031 http://stacks.iop.org/1742-6596/331/i=2/a=022031                    crawford j 1983 _ nuclear instruments and methods in physics research _ * 211 * 223  225 issn 0167 - 5087 http://www.sciencedirect.com / science / article / pii/016750878390% 5756[http://www.sciencedirect.com / science / article / pii/016750878390% 5756 ]"
  ],
  "abstract_text": [
    "<S> we implemented the nanetfpga - based pciegen2 gbe / apelinknic , featuring gpudirect rdma capabilities and udp protocol management offloading . </S>",
    "<S> nanetis able to receive a udp input data stream from its gbeinterface and redirect it , without any intermediate buffering or cpu intervention , to the memory of a fermi / kepler gpu hosted on the same pciebus , provided that the two devices share the same upstream root complex . </S>",
    "<S> synthetic benchmarks for latency and bandwidth are presented . </S>",
    "<S> we describe how nanetcan be employed in the prototype of the rich trigger processor of the na62 cern experiment , to implement the data link between the tel62 readout boards and the low level trigger processor . </S>",
    "<S> results for the throughput and latency of the integrated system are presented and discussed . </S>"
  ]
}