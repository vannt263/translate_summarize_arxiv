{
  "article_text": [
    "minimax risk is one of the most widely used benchmarks for optimality , and substantial efforts have been made on developing minimax theories in the statistics literature .",
    "a key step in establishing a minimax theory is the derivation of minimax lower bounds and several effective lower bound arguments based on hypothesis testing have been introduced in the literature .",
    "well - known techniques include le cam s method , assouad s lemma and fano s lemma .",
    "see @xcite and @xcite for more detailed discussions on minimax lower bound arguments",
    ".    driven by a wide range of applications in high dimensional data analysis , estimation of large covariance matrices has drawn considerable recent attention .",
    "see , for example , @xcite ( @xcite ) , @xcite , @xcite , @xcite , @xcite and @xcite .",
    "many theoretical results , including consistency and rates of convergence , have been obtained .",
    "however , the optimality question remains mostly open in the context of covariance matrix estimation under the spectral norm , mainly due to the technical difficulty in obtaining good minimax lower bounds .",
    "in this paper we consider optimal estimation of sparse covariance matrices and establish the minimax rate of convergence under a range of matrix operator norm and bregman divergence losses .",
    "a major focus is on the derivation of a rate sharp lower bound under the spectral norm loss .",
    "conventional lower bound techniques such as the ones mentioned earlier are designed and well suited for problems with parameters that are scalar or vector - valued .",
    "they have achieved great successes in solving many nonparametric function estimation problems which can be treated exactly or approximately as estimation of a finite or infinite dimensional vector and can thus be viewed as `` one - directional '' in terms of the lower bound arguments .",
    "in contrast , the problem of estimating a sparse covariance matrix under the spectral norm can be regarded as a truly `` two - directional '' problem where one direction is along the rows and another along the columns .",
    "it can not be essentially reduced to a problem of estimating a single or multiple vectors . as a consequence ,",
    "standard lower bound techniques fail to yield good results for this matrix estimation problem .",
    "new and more general technical tools are thus needed .    in the present paper",
    "we first develop a minimax lower bound technique that is particularly well suited for treating `` two - directional '' problems such as estimating sparse covariance matrices .",
    "the result can be viewed as a simultaneous generalization of le cam s method in one direction and assouad s lemma in another .",
    "this general technical tool is of independent interest and is useful for solving other matrix estimation problems such as optimal estimation of sparse precision matrices .",
    "we then consider specifically the problem of optimal estimation of sparse covariance matrices under the spectral norm .",
    "let @xmath2 be a random sample from a @xmath3-variate distribution with covariance matrix @xmath4 .",
    "we wish to estimate the unknown matrix @xmath5 based on the sample @xmath6 . in this paper",
    "we shall use the weak @xmath7 ball with @xmath8 to model the sparsity of the covariance matrix @xmath5 .",
    "the weak @xmath7 ball was originally used in @xcite for a sparse normal means problem .",
    "a weak @xmath7 ball of radius @xmath9 in @xmath10 contains elements with fast decaying ordered magnitudes of components , @xmath11 where @xmath12 denotes the @xmath13th largest element in magnitude of the vector @xmath14 . for a covariance matrix @xmath15 ,",
    "denote by @xmath16 the @xmath17th column of @xmath5 with @xmath18 removed .",
    "we shall assume that @xmath16 is in a weak @xmath7 ball for all @xmath19 .",
    "more specifically , for @xmath8 , we define the parameter space @xmath20 of covariance matrices by @xmath21 in the special case of @xmath22 , a matrix in @xmath23 has at most @xmath24 nonzero off - diagonal elements on each column .",
    "the problem of estimating sparse covariance matrices under the spectral norm has been considered , for example , in @xcite , @xcite , @xcite and @xcite .",
    "thresholding methods were introduced , and rates of convergence in probability were obtained for the thresholding estimators .",
    "the parameter space @xmath20 given in ( [ sparseparaspace ] ) also contains the uniformity class of covariance matrices considered in @xcite as a special case .",
    "we assume that the distribution of the @xmath25 s is subgaussian in the sense that there is @xmath26 such that @xmath27 let @xmath28 denote the set of distributions of @xmath29 satisfying ( [ subgau ] ) and with covariance matrix @xmath30 .    our technical analysis used in establishing a rate - sharp minimax lower bound has three major steps .",
    "the first step is to reduce the original problem to a simpler estimation problem over a carefully chosen subset of the parameter space without essentially decreasing the level of difficulty .",
    "the second is to apply the general minimax lower bound technique to this simplified problem , and the final key step is to bound the total variation affinities between pairs of mixture distributions with specially designed sparse covariance matrices .",
    "the technical analysis requires ideas that are quite different from those used in the typical function / sequence estimation problems .",
    "the minimax upper bound is obtained by studying the risk properties of thresholding estimators .",
    "it will be shown that the optimal rate of convergence under mean squared spectral norm error is achieved by a thresholding estimator introduced in @xcite . we write @xmath31",
    "if there are positive constants @xmath9 and  @xmath32 independent of @xmath33 such that @xmath34 . for @xmath35 ,",
    "the matrix @xmath36 operator norm of a matrix @xmath37 is defined by @xmath38 . the commonly used spectral norm @xmath39 coincides with the matrix @xmath40 operator norm @xmath41 .",
    "( throughout the paper , we shall write @xmath39 without a subscript for the matrix spectral norm . ) for a symmetric matrix @xmath37 , it is known that the spectral norm @xmath42 is equal to the largest magnitude of the eigenvalues of @xmath37 . throughout the paper",
    "we shall assume that @xmath43 for some constants @xmath44 .",
    "combining the results given in sections  [ lowbdsec ] and  [ upperbdsec ] , we have the following optimal rate of convergence for estimating sparse covariance matrices under the spectral norm .",
    "[ minimaxope ] assume that @xmath45 for @xmath46 .",
    "the minimax risk of estimating the covariance matrix @xmath5 under the spectral norm over the class @xmath28 satisfies @xmath47 where @xmath48 denotes a distribution in @xmath49 with the covariance matrix @xmath5 .",
    "furthermore , ( [ rateoper ] ) holds under the squared @xmath36 operator norm loss for all @xmath35 .",
    "we shall focus the discussions on the spectral norm loss .",
    "the extension to the general matrix @xmath36 operator norm is given in section [ discussionssec ] .",
    "in addition , we also consider optimal estimation under a class of bregman matrix divergences which include stein s loss , squared frobenius norm and von neumann entropy as special cases .",
    "bregman matrix divergences provide a flexible class of dissimilarity measures between symmetric matrices and have been used for covariance and precision matrix estimation as well as matrix approximation problems .",
    "see , for example , @xcite , ravikumar et al .",
    "( @xcite ) and @xcite .",
    "we give a unified result on the minimax rate of convergence in section  [ bdsec ] .",
    "besides the sparsity assumption considered in this paper , another commonly used structural assumption in the literature is that the covariance matrix is `` bandable '' where the entries decay as they move away from the diagonal .",
    "this is particularly suitable in the setting where the variables exhibit a certain ordering structure , which is often the case for time series data .",
    "various regularization methods have been proposed and studied under this assumption .",
    "@xcite proposed a banding estimator and obtained rate of convergence for the estimator .",
    "@xcite established the minimax rates of convergence and introduced a rate - optimal tapering estimator .",
    "in particular , @xcite derived rate sharp minimax lower bounds for estimating bandable matrices .",
    "it should be noted that the lower bound techniques used there do not lead to a good result for estimating sparse covariance matrices under the spectral norm .    the rest of the paper is organized as follows .",
    "section  [ glowbdsec ] introduces a general technical tool for deriving minimax lower bounds on the minimax risk .",
    "section  [ lowbdsec ] establishes the minimax lower bound for estimating sparse covariance matrices under the spectral norm .",
    "the upper bound is obtained in section  [ upperbdsec ] by studying the risk properties of thresholding estimators .",
    "section  [ bdsec ] considers optimal estimation under the bregman divergences .",
    "a uniform optimal rate of convergence is given for a class of bregman divergence losses .",
    "section  [ discussionssec ] discusses extensions to estimation under the general @xmath0 norm for @xmath50 and connections to other related problems including optimal estimation of sparse precision matrices .",
    "the proofs are given in section  [ proofssec ] .",
    "in this section we develop a new general minimax lower bound technique that is particularly well suited for treating `` two - directional '' problems such as estimating sparse covariance matrices .",
    "the new method can be viewed as a generalization of both le cam s method and assouad s lemma . to help motivate and understand the new lower bound argument , it is useful to briefly review le cam s method and assouad s lemma .",
    "le cam s method is based on a two - point testing argument and is particularly well used in estimating linear functionals .",
    "see @xcite and @xcite .",
    "let @xmath51 be an observation from a distribution @xmath52 where @xmath48 belongs to a parameter set @xmath53 . for two distributions @xmath54 and @xmath55 with densities @xmath3 and @xmath56 with respect to any common dominating measure @xmath57 ,",
    "the total variation affinity is given by @xmath58 .",
    "le cam s method works with a finite parameter set @xmath59 .",
    "let @xmath60 be a loss function .",
    "define @xmath61 $ ] and denote @xmath62 .",
    "le cam s method gives a lower bound for the maximum estimation risk over the parameter set @xmath53 .",
    "[ lecam ] let @xmath63 be any estimator of @xmath48 based on an observation @xmath51 from a distribution @xmath52 with @xmath64 , then @xmath65    write @xmath66",
    ". one can view the lower bound in ( [ lecamlbd ] ) as obtained from testing the simple hypothesis @xmath67 against the composite alternative @xmath68 .",
    "assouad s lemma works with a hypercube @xmath69 .",
    "it is based on testing a number of pairs of simple hypotheses and is connected to multiple comparisons . for a parameter @xmath70 where @xmath71 , one tests whether @xmath72 or @xmath73 for each @xmath74 based on the observation @xmath51 . for each pair of simple hypotheses , there is a certain loss for making an error in the comparison .",
    "the lower bound given by assouad s lemma is a combination of losses from testing all pairs of simple hypotheses .",
    "let @xmath75 be the hamming distance on @xmath53 .",
    "assouad s lemma gives a lower bound for the maximum risk over the hypercube @xmath53 of estimating an arbitrary quantity @xmath76 belonging to a metric space with metric @xmath77 .",
    "[ assouad ] let @xmath78 with @xmath79 , and let @xmath80 be an estimator of @xmath81 based on @xmath51 . then for all @xmath82 , @xmath83 \\\\[-8pt ] \\nonumber & & \\qquad\\geq\\min_{h ( \\theta , \\theta ^{\\prime } ) \\geq1}\\frac{d^{s } ( \\psi ( \\theta ) , \\psi ( \\theta^{\\prime } ) ) } { h ( \\theta,\\theta^{\\prime } ) } \\cdot \\frac{r}{2}\\cdot\\min_{h ( \\theta,\\theta ^{\\prime } ) = 1}{\\vert}\\mathbb{p}_{\\theta}\\wedge \\mathbb{p}_{\\theta ^{\\prime}}{\\vert}.\\end{aligned}\\ ] ]    we now introduce our new lower bound technique .",
    "again , let @xmath84 where @xmath85 .",
    "the parameter space @xmath53 of interest has a special structure which can be viewed as the cartesian product of two components @xmath86 and @xmath87 . for a given positive integer @xmath88 and a finite set @xmath89 , let @xmath90 and @xmath91 .",
    "define @xmath92 in comparison , the standard lower bound arguments work with either @xmath86 or @xmath87 alone .",
    "for example , assouad s lemma considers only the parameter set @xmath86 and the le cam s method typically applies to a parameter set like @xmath87 with @xmath93 . for @xmath94 ,",
    "denote the projection of @xmath48 to @xmath86 by @xmath95 and to @xmath87 by @xmath96 .",
    "it is important to understand the structure of the parameter space @xmath53 .",
    "one can view an element @xmath97 as an @xmath98 matrix with each row coming from the set @xmath99 and view @xmath86 as a set of parameters along the rows indicating whether a given row of @xmath100 is present or not .",
    "let @xmath101 .",
    "for a given @xmath102 and @xmath74 , denote @xmath103 where @xmath104 and @xmath105 is the @xmath106th coordinate of of the first component of @xmath48 .",
    "it is easy to see that @xmath107 .",
    "define the mixture distribution @xmath108 by @xmath109 so @xmath108 is the mixture distribution over all @xmath52 with @xmath105 fixed to be @xmath110 while all other components of @xmath48 vary over all possible values in @xmath53 .",
    "the following lemma gives a lower bound for the maximum risk over the parameter set @xmath53 of estimating a functional @xmath111 belonging to a metric space with metric @xmath77 .",
    "[ al ] for any @xmath82 and any estimator @xmath63 of @xmath111 based on an observation from the experiment @xmath112 where @xmath53 is given in ( [ theta ] ) , @xmath113 where @xmath108 is defined in equation ( [ avepi ] ) and @xmath114 is given by @xmath115    the idea behind this new lower bound argument is similar to the one for assouad s lemma , but exists in a more complicated setting . based on an observation @xmath78 where @xmath116 , we wish to test whether @xmath117 or @xmath73 for each @xmath74 .",
    "the first factor @xmath114 in the lower bound ( rhslemma1 ) is the minimum cost of making an error per comparison .",
    "the second factor @xmath118 is the expected number of errors one makes to estimate @xmath119 when @xmath52 and @xmath120 are indistinguishable from each other in the case @xmath121 , and the last factor is the lower bound for the total probability of making type i and type ii errors for each comparison .",
    "a major difference is that in this third factor the distributions @xmath122 and @xmath123 are both complicated mixture distributions instead of the typically simple ones in assouad s lemma .",
    "this makes the lower bound argument more generally applicable , while the calculation of the affinity becomes much more difficult .    in applications of lemma  [ al ] , for a @xmath124 where @xmath125 takes value @xmath126 or @xmath73 , and a @xmath127 where each @xmath128 is a @xmath3-dimensional nonzero row vector ,",
    "the element @xmath129 can be equivalently viewed as an @xmath98 matrix @xmath130 where the product @xmath131 is taken elementwise : @xmath132 if @xmath133 and the @xmath106th row of @xmath48 is the zero vector if @xmath134 .",
    "the term @xmath135 of equation ( [ rhslemma1 ] ) is then the lower bound for the total probability of making type i and type ii errors for testing whether or not the @xmath106th row of @xmath48 is zero .",
    "note that the lower bound ( [ rhslemma1 ] ) reduces to the classical assouad lemma when @xmath87 contains only one matrix for which every row is nonzero , and becomes a two - point argument of le cam with one point against a mixture when @xmath93 .",
    "the proof of this lemma is given in section [ proofssec ] .",
    "the technical argument is an extension of that of assouad s lemma .",
    "see @xcite , @xcite and @xcite .",
    "the advantage of this method is the ability to break down the lower bound calculations for the whole matrix estimation problem into calculations for individual rows so that the overall analysis is simplified and more tractable .",
    "although the tool is introduced here for the purpose of estimating a sparse covariance matrix , it is of independent interest and is expected to be useful for solving other matrix estimation problems as well .",
    "bounding the total variation affinity between two mixture distributions in ( [ rhslemma1 ] ) is quite challenging in general .",
    "the following well - known result on the affinity is helpful in some applications .",
    "it provides lower bounds for the affinity between two mixture distributions in terms of the affinities between simpler distributions in the mixtures .",
    "[ avedis ] let @xmath136 and @xmath137 where @xmath138 and @xmath139 .",
    "then @xmath140    more specifically , in our construction of the parameter set for establishing the minimax lower bound , @xmath88 is the number of possibly nonzero rows in the upper triangle of the covariance matrix , and @xmath87 is the set of matrices with @xmath88 rows to determine the upper triangle matrix .",
    "recall that the projection of @xmath85 to @xmath86 is @xmath141 and the projection of @xmath48 to @xmath87 is @xmath142 .",
    "more generally , for a subset @xmath143 , we define a projection of @xmath48 to a subset of @xmath86 by @xmath144 .",
    "a particularly useful example of set @xmath37 is @xmath145 for which @xmath146 and in this case for convenience we set @xmath147 .",
    "@xmath148 and @xmath149 are defined similarly .",
    "we also define the set @xmath150 .",
    "a special case is @xmath151 .",
    "now we define a subset of @xmath53 to reduce the problem of estimating @xmath53 to a problem of estimating @xmath152 . for @xmath153 , @xmath154 and @xmath155 ,",
    "let @xmath156 and @xmath157 .",
    "note that the cardinality of @xmath158 on the right - hand side does not depend on the value of @xmath110 due to the cartesian product structure of @xmath159 .",
    "define the mixture distribution @xmath160 in other words , @xmath161 is the mixture distribution over all @xmath52 with @xmath162 varying over all possible values while all other components of @xmath48 remain fixed .",
    "it is helpful to observe that when @xmath163 , we have @xmath164 for which @xmath165 is degenerate in the sense that it is an average of identical distributions .",
    "lemmas  [ al ] and  [ avedis ] together immediately imply the following result which is based on the total variation affinities between slightly less complicated mixture distributions .",
    "we need to introduce a new notation @xmath166 to denote the average of a function  @xmath167 over @xmath53 , that is,@xmath168 the parameter @xmath48 is seen uniformly distributed over @xmath53 .",
    "let @xmath169 and an average of @xmath170 over the set @xmath171 is defined as follows:@xmath172 where the distribution of @xmath173 is induced by the uniform distribution over @xmath53 .",
    "[ lowbdlemma1 ] for any @xmath82 and any estimator @xmath63 of @xmath174 based on an observation from the experiment @xmath175 where the parameter space @xmath53 is given in ( [ theta ] ) , @xmath176 where @xmath114 and @xmath177 are defined in equations ( [ alpha ] ) and ( [ avepibd ] ) , respectively .",
    "a key technical step in applying lemma  [ al ] in a typical application is to show that the affinity @xmath178 is uniformly bounded away from @xmath126 by a constant for all @xmath106 .",
    "then the term @xmath179 on the right - hand side of equation ( [ rhslemma1 ] ) in lemma  [ al ] gives the lower bound for the minimax rate of convergence .",
    "as mentioned earlier , the affinity calculations for two mixture distributions can be very much involved .",
    "corollary  [ lowbdlemma1 ] gives two lower bounds in terms of the affinities . as noted earlier , @xmath180 in the affinity in equations ( [ avelwbd ] ) and ( [ minlwbd ] ) is in fact a single normal distribution , not a mixture .",
    "thus the lower bounds given in equations  ( [ avelwbd ] ) and  ( [ minlwbd ] ) require simpler , although still involved , calculations . in this paper",
    "we will apply equation ( [ avelwbd ] ) , which has an average of affinities on the right - hand side .",
    "we now turn to the minimax lower bound for estimating sparse covariance matrices under the spectral norm .",
    "we shall apply the lower bound technique developed in the previous section to establish rate sharp results .",
    "the same lower bound also holds under the general @xmath0 norm for @xmath181 .",
    "upper bounds are discussed in section  [ upperbdsec ] and optimal estimation under bregman divergence losses is considered in section [ bdsec ] .    in this section",
    "we shall focus on the gaussian case and wish to estimate the covariance matrix @xmath182 under the spectral norm based on the sample @xmath183 .",
    "the parameter space @xmath20 for sparse covariance matrices is defined as in ( [ sparseparaspace ] ) . in the special case of @xmath22",
    ", @xmath23 contains matrices with at most @xmath184 nonzero elements on each row / column .",
    "the parameter space @xmath20 also contains the uniformity class @xmath185 considered in @xcite as a special case , where @xmath186 is defined as , for @xmath8 , @xmath187 the columns of @xmath188 are assumed to belong to a strong @xmath7 ball .",
    "we now state and prove the minimax lower bound for estimating a sparse covariance matrix over the parameter space @xmath20 under the spectral norm .",
    "the derivation of the lower bounds relies heavily on the general lower bound technique developed in the previous section .",
    "it also requires a careful construction of a finite subset of the parameter space and detailed calculations of an effective lower bound for the total variation affinities between mixtures of multivariate gaussian distributions .",
    "[ operlowerbdthm ] let @xmath189 .",
    "the minimax risk for estimating the covariance matrix @xmath5 over the parameter space @xmath20 with @xmath190 satisfies @xmath191 for some constant @xmath192 , where @xmath39 denotes the matrix spectral norm@xmath193    theorem  [ operlowerbdthm ] yields immediately a minimax lower bound for the more general subgaussian case under assumption ( [ subgau ] ) , @xmath194    it has been shown in @xcite that@xmath195 by constructing a parameter space with only diagonal matrices .",
    "it then suffices to show that@xmath196 to establish theorem  [ operlowerbdthm ] .",
    "the proof of theorem  [ operlowerbdthm ] contains three major steps . in the first step",
    "we construct in detail a finite subset @xmath197 of the parameter space @xmath198 such that the difficulty of estimation over @xmath197 is essentially the same as that of estimation over @xmath198 .",
    "the second step is the application of lemma  [ al ] to the carefully constructed parameter set @xmath199 .",
    "finally in the third step we calculate the factor @xmath114 defined in ( alpha ) and the total variation affinity between two multivariate normal mixtures . bounding the affinity",
    "is technically involved .",
    "the main ideas of the proof are outlined here , and detailed proofs of some technical lemmas used here are deferred to section  [ proofssec ] .",
    "proof of theorem  [ operlowerbdthm ] the proof is divided into three main steps .",
    "_ step _ 1 : _ constructing the parameter set_. let @xmath200 , where @xmath201 denotes the largest integer less than or equal to @xmath202 , and let @xmath99 be the collection of all row vectors @xmath203 such that @xmath204 for @xmath205 and @xmath204 or @xmath73 for @xmath206 under the constraint the total number of 1s is @xmath207 , where the value of @xmath13 will be specified later .",
    "we shall treat each @xmath208 as an @xmath98 matrix with the @xmath106th row equal to @xmath209 .",
    "set @xmath90 .",
    "define @xmath210 to be the set of all elements in @xmath211 such that each column sum is less than or equal to @xmath212 . for each component @xmath213 , @xmath214 , of @xmath215 ,",
    "define a @xmath216 symmetric matrix @xmath217 by making the @xmath218th row of @xmath217 equal to @xmath213 , the @xmath218th column equal to @xmath219 and the rest of the entries @xmath126 .",
    "note that for each @xmath215 , each column / row sum of the  matrix @xmath220 is less than or equal to @xmath212 .",
    "define @xmath221 and let @xmath222 be fixed .",
    "( the exact value of @xmath223 will be chosen later . ) for each @xmath224 with @xmath225 and @xmath215 , we associate @xmath48 with a covariance matrix @xmath226 by @xmath227 it is easy to see that in the gaussian case @xmath228 is a sufficient condition for ( [ subgau ] ) . without loss of generality",
    "we assume that @xmath229 in the subgaussianity assumption ( [ subgau ] ) ; otherwise we replace @xmath230 in ( [ lowbdspace ] ) by @xmath231 with a small constant @xmath192 .",
    "finally we define a collection @xmath232 of covariance matrices as @xmath233 note that each @xmath234 has value @xmath73 along the main diagonal , and contains an @xmath235 submatrix , say , @xmath37 , at the upper right corner , @xmath236 at the lower left corner and @xmath126 elsewhere .",
    "each row of @xmath37 is either identically @xmath126 ( if the corresponding @xmath119 value is @xmath126 ) or has exactly @xmath13 nonzero elements with value @xmath237 .",
    "we now specify the values of @xmath237 and @xmath13 to ensure @xmath238 .",
    "set @xmath239 for a fixed small constant @xmath240 , and let @xmath241 which implies @xmath242 we require @xmath243^{{1}/{(1-q ) } } \\quad\\mbox{and}\\quad \\upsilon^{2}<\\frac{\\beta-1}{54\\beta}. \\label{v}\\ ] ] note that @xmath237 and @xmath13 satisfy @xmath244 and consequently every @xmath226 is diagonally dominant and positive definite , and @xmath245 .",
    "thus we have @xmath238 , and the subgaussianity assumption ( [ subgau ] ) is satisfied",
    ".    _ step _ 2 : _ applying the general lower bound argument_. let @xmath246 with @xmath85 and denote the joint distribution by @xmath52 .",
    "applying lemma  [ al ] to the parameter space @xmath53 with @xmath247 , we have @xmath248 where @xmath249 and @xmath122 and @xmath250 are defined as in ( [ avepi ] ) .",
    "_ step _ 3 : _ bounding the affinity and per comparison loss_. we shall now bound the two factors @xmath114 and @xmath251 in ( [ lowerbound * ] ) .",
    "this is done separately in the next two lemmas which are proved in detail in section  [ proofssec ] .",
    "lemma  [ dffbd ] gives a lower bound to the per comparison loss , and it is easy to prove .",
    "[ dffbd ] for @xmath114 defined in equation ( [ alpha1 ] ) we have @xmath252    the key technical difficulty is in bounding the affinity between the gaussian mixtures .",
    "the proof is quite involved .",
    "[ affbd ] let @xmath253 with @xmath254 defined in equation  ( [ thetaall ] ) , and denote the joint distribution by @xmath52 .",
    "for @xmath102 and @xmath74 , define @xmath255 as in ( [ avepi ] ) .",
    "then there exists a constant @xmath256 such that @xmath257    finally , the minimax lower bound for estimation over @xmath20 is obtained by putting together the bounds given in lemmas [ dffbd ] and  [ affbd ] , @xmath258 for some constant @xmath259 .",
    "it is easy to check that the proof of theorem [ operlowerbdthm ] also yields a lower bound for estimation under the general matrix @xmath36 operator norm for any @xmath260 @xmath261 by applying lemma",
    "[ al ] with @xmath262 .",
    "section  [ lowbdsec ] developed a minimax lower bound for estimating a sparse covariance matrix under the spectral norm over @xmath20 . in this section we shall show that the lower bound is rate - sharp and therefore establish the optimal rate of convergence . to derive a minimax upper bound",
    ", we shall consider the properties of a thresholding estimator introduced in @xcite . given a random sample @xmath263 of @xmath3-variate observations drawn from a distribution in @xmath28 , the sample covariance matrix is @xmath264 which is an unbiased estimate of @xmath5 , and the maximum likelihood estimator of @xmath5 is @xmath265 when",
    "@xmath266 s are normally distributed .",
    "these two estimators are close to each other for large @xmath33",
    ". we shall construct estimators of the covariance matrix @xmath5 by thresholding the maximum likelihood estimator @xmath267 .",
    "note that the subgaussianity condition ( [ subgau ] ) implies@xmath268 \\leq\\int_{0}^{\\infty } e^{-x/ ( 2\\tau ) } \\,dx=2 \\tau.\\ ] ] then the empirical covariance @xmath269 satisfies the following large deviation result that there exist constants @xmath270 and @xmath271 such that @xmath272 for @xmath273 , where @xmath274 @xmath119 and @xmath275 are constants and depend only on @xmath276 .",
    "see saulis and statuleviius ( @xcite ) and @xcite .",
    "inequality ( emsigmatail ) implies @xmath277 behaves like a subgaussian random variable .",
    "in particular for @xmath278 we have @xmath279 define the thresholding estimator @xmath280 by @xmath281 this thresholding estimator was first proposed in @xcite in which a rate of convergence of the loss function in probability was given over the uniformity class @xmath186 . here",
    "we provide an upper bound for mean squared spectral norm error over the parameter space @xmath20 .    throughout the rest of the paper",
    "we denote by @xmath32 a generic positive constant which may vary from place to place .",
    "the following theorem shows that the thresholding estimator defined in ( [ sigmahatij ] ) is rate optimal over the parameter space @xmath20 .",
    "[ minimaxope1 ] the thresholding estimator @xmath282 given in ( sigmahatij ) satisfies , for some constant @xmath283 , @xmath284 .",
    "\\label{rateoper2}\\ ] ] consequently , the minimax risk of estimating the sparse covariance matrix @xmath5 over @xmath20 satisfies @xmath285    a similar argument to the proof of equation ( [ rateoper2 ] ) in section  [ uppersec ] yields the following upper bound for estimation under the matrix @xmath286 norm : @xmath287 .\\ ] ]    theorem  [ minimaxope1 ] shows that the optimal rate of convergence for estimating a sparse covariance matrix over @xmath20 under the squared spectral norm is @xmath288 . in @xcite the uniformity class @xmath289 defined in ( [ uniformityclass ] )",
    "was considered .",
    "we shall now show that the same minimax rate of convergence holds for estimation over @xmath186 .",
    "it is easy to check in the proof of the lower bound that for every @xmath290 defined in ( [ f * ] ) , we have @xmath291 and consequently @xmath292 .",
    "thus the lower bound established for @xmath232 automatically yields a lower bound for @xmath186 . on the other hand , since a strong @xmath7 ball is always contained in a weak @xmath7 ball by the markov inequality , the upper bound in equation ( [ rateoper2 ] ) for the parameter space @xmath293 also holds for @xmath186 .",
    "let @xmath294 denote the set of distributions of @xmath29 satisfying ( [ subgau ] ) and with covariance matrix @xmath295 .",
    "then we have the following result .    [ blprop ] the minimax risk for estimating the covariance matrix under the spectral norm over the uniformity class @xmath296 satisfies @xmath297",
    "the thresholding estimator @xmath298 defined by ( [ sigmahatij ] ) is positive definite with high probability , but it is not guaranteed to be positive definite .",
    "a simple additional step can make the final estimator positive semi - definite and achieve the optimal rate of convergence . write the eigen - decomposition of @xmath282 as @xmath299 where @xmath300 s and @xmath301 s are the eigenvalues and eigenvectors of @xmath282 , respectively .",
    "let @xmath302 be the positive part of @xmath300 and define @xmath303 then@xmath304 the resulting estimator @xmath305 is positive semi - definite and attains the same rate as the original thresholding estimator @xmath306 .",
    "this method can be applied to the tapering estimator in @xcite as well to make the estimator positive semi - definite , while still achieving the optimal rate .",
    "we have so far focused on the optimal rate of convergence under the spectral norm . in this section",
    "we turn to minimax estimation of sparse covariance matrices under a class of bregman divergence losses which include stein s loss , frobenius norm and von neumann s entropy as special cases .",
    "bregman matrix divergences have been used for matrix estimation and matrix approximation problems ; see , for example , @xcite , ravikumar et al .",
    "( @xcite ) and @xcite . in this section",
    "we establish the optimal rate of convergence uniformly for a class of bregman divergence losses .",
    "bregman ( @xcite ) introduced the bregman divergence as a dissimilarity measure between vectors , @xmath307 where @xmath308 is a differentiable , real - valued , and strictly convex function defined over a convex set in a euclidean space @xmath10 , and @xmath309 is the gradient of @xmath308 .",
    "the well - known mahalanobis distance is a bregman divergence .",
    "this concept can be naturally extended to the space of real and symmetric matrices as @xmath310,\\ ] ] where @xmath51 and @xmath311 are real symmetric matrices , and @xmath308 is a differentiable strictly convex function over the space .",
    "see @xcite and @xcite .",
    "a particularly interesting class of @xmath308 is @xmath312 where @xmath313 s are the eigenvalues of @xmath51 , and @xmath314 is a differentiable , real - valued , and strictly convex function over a convex set in @xmath315 .",
    "see @xcite and @xcite .",
    "examples of this class of bregman divergences include :    * @xmath316 @xmath317 or equivalently @xmath318",
    ". the corresponding bregman divergence can be written as@xmath319 which is often called stein s loss in the statistical literature .",
    "* @xmath316 @xmath320 or equivalently @xmath321 , where @xmath51 is positive definite such that @xmath322 is well defined .",
    "the corresponding bregman divergence is the von neumann divergence@xmath323 * @xmath316 @xmath324 or equivalently @xmath325 .",
    "the resulting bregman divergence is the squared frobenius norm@xmath326 = |\\ !",
    "x_{ij}-y_{ij } ) ^{2}\\ ] ] for @xmath327 and @xmath328 .",
    "define a class @xmath329 of functions @xmath314 satisfying the following conditions :    @xmath314 is twice differentiable , real - valued and strictly convex over @xmath330 ;    @xmath331 for some @xmath283 and some real number @xmath88 uniformly over @xmath332 ;    for every positive constants @xmath333 and @xmath334 there are some positive constants @xmath335 and @xmath336 depending on @xmath337 and @xmath334 such that @xmath338 for all @xmath339 $ ] .    in this paper , we shall consider the following class of bregman divergences : @xmath340 it is easy to see that stein s loss , von neumann s divergence and the squared frobenius norm are in this class .",
    "let @xmath341 be a positive constant .",
    "let @xmath342 denote the set of distributions of @xmath29 satisfying ( [ subgau ] ) and with covariance matrix @xmath343 here @xmath344 denotes the minimum eigenvalue of @xmath5 .",
    "the assumption that all eigenvalues are bounded away from @xmath126 is necessary when @xmath345 is not well defined at  @xmath126 .",
    "an example is the stein loss where @xmath346 . under this assumption",
    "all losses @xmath347 are equivalent to the squared frobenious norm .",
    "the following theorem gives a unified result on the minimax rate of convergence for estimating the covariance matrix over the parameter space @xmath348 for all bregman divergences @xmath349 defined in ( [ assumpphi ] ) .",
    "[ minimaxbd ] assume that @xmath350 for some @xmath351 and @xmath46 .",
    "the minimax risk over @xmath348 under the loss function @xmath352 for all bregman divergences @xmath353 defined in ( [ assumpphi ] ) satisfies @xmath354    note that theorem  [ minimaxbd ] gives the minimax rate of convergence uniformly under all bregman divergences defined in ( [ assumpphi ] ) .",
    "for an individual bregman divergence loss , the condition that all eigenvalues are bounded away from @xmath126 is not needed if the function @xmath314 is well behaved at @xmath126 .",
    "for example , such is the case for the frobenius norm .",
    "the optimal rate of convergence is attained by a modified thresholding estimator .",
    "let @xmath355 be the thresholding estimator given in ( [ sigmahatij ] ) .",
    "define the final estimator of @xmath5 by @xmath356 it will be proved in section  [ bregmanproofsec ] that the estimator @xmath357 given in ( [ sigmahatstein ] ) is rate optimal uniformly under all bregman divergences satisfying ( [ assumpphi ] ) . note that the modification of @xmath282 given in ( [ sigmahatstein ] ) is needed . without it ,",
    "the loss @xmath358 may not be well behaved under some bregman divergences such as stein s loss and von neumann s divergence .",
    "let @xmath359 denote the set of distributions of @xmath29 satisfying  ( [ subgau ] ) and with covariance matrix @xmath360 .",
    "then under the same conditions as in theorem  [ minimaxbd ] , @xmath361",
    "the focus of this paper is mainly on the optimal estimation under the spectral norm .",
    "however , both the lower and upper bounds can be easily extended to the general matrix @xmath0 norm for @xmath362 by using similar arguments given in sections  [ lowbdsec ] and  [ upperbdsec ] .",
    "[ minimaxoper ] under the assumptions in theorem [ minimaxope ] , the minimax risk of estimating the covariance matrix @xmath5 under the matrix @xmath36-norm for @xmath35 over the class @xmath28 satisfies @xmath363 moreover , the thresholding estimator @xmath282 defined in ( sigmahatij ) is rate - optimal .    as noted in section  [ lowbdsec ] , a rate - sharp lower bound for the minimax risk under the @xmath36 norm can be obtained by using essentially the same argument with the same parameter space @xmath232 and a slightly modified version of lemma  [ dffbd ] .",
    "the upper bound can be proved by applying the riesz ",
    "thorin interpolation theorem , which yields @xmath364 for all @xmath365 , and by using the facts @xmath366 , when @xmath37 is symmetric . in section [ upperbdsec ]",
    "we have in fact established the same risk bound for both the spectral norm and matrix @xmath367-norm .",
    "the spectral norm of a matrix depends on the entries in a subtle way and the `` interactions '' among different rows / columns must be taken into account . the lower bound argument developed in this paper",
    "is aimed at treating `` two - directional '' problems by mixing over both rows and columns .",
    "it can be viewed as a simultaneous application of le cam s method in one direction and assouad s lemma in another .",
    "in contrast , for sequence estimation problems , we typically need one or the other , but not both at the same time . the lower bound techniques developed in this paper can be used to solve other matrix estimation problems . for example , @xcite applied the general lower bound argument to the problem of estimating sparse precision matrices under the spectral norm and established the optimal rate of convergence .",
    "this problem is closely connected to graphical model selection .",
    "the derivations of both the lower and upper bounds are involved . for reasons of space",
    ", we shall report the results elsewhere .",
    "in this paper we also developed a unified result on the minimax rate of convergence for estimating sparse covariance matrices under a class of bregman divergence losses which include the commonly used frobenius norm as a special case .",
    "the optimal rate of convergence given in theorem minimaxbd is identical to the minimax rate for estimating a row / column as a vector with the weak @xmath7 ball constraint under the squared error loss .",
    "our result shows that this class of bregman divergence losses are essentially the same and thus can be studied simultaneously in terms of the minimax rate of convergence .    estimating a sparse covariance matrix is intrinsically a heteroscedastic problem in the sense that the variances of the entries of the sample covariance matrix are not equal and can vary over a wide range .",
    "a natural approach is to adaptively threshold the entries according to their individual variabilities .",
    "@xcite considered such an adaptive approach for estimation over the weighted @xmath368 balls which contains the strong @xmath368 balls as subsets .",
    "the lower bound given in proposition [ blprop ] in the present paper immediately yields a lower bound for estimation over the weighted @xmath368 balls .",
    "a data - driven thresholding procedure was introduced and shown to adaptively achieve the optimal rate of convergence over a large collection of the weighted @xmath368 balls under the spectral norm .",
    "in contrast , universal thresholding estimators are sub - optimal over the same parameter spaces .",
    "in addition to the hard thresholding estimator used in @xcite , @xcite considered a class of thresholding rules with more general thresholding functions , including soft thresholding and adaptive lasso .",
    "it is straightforward to show that these thresholding estimators with the same choice of threshold level used in ( [ sigmahatij ] ) also attains the optimal rate of convergence over the parameter space @xmath20 under mean squared spectral norm error as well as under the class of bregman divergence losses considered in section  [ bdsec ] with the same modification as in ( [ sigmahatstein ] ) .",
    "therefore , the choice of the thresholding function is not important as far as the rate optimality is concerned .",
    "in this section we prove the general lower bound result given in lemma al , theorems  [ minimaxope1 ] and  [ minimaxbd ] as well as some of the important technical lemmas used in the proof of theorem  [ operlowerbdthm ] given in section  [ lowbdsec ] .",
    "the proofs of a few technical results used in this section are deferred to the supplementary material [ @xcite ] . throughout this section ,",
    "we denote by @xmath32 a generic constant that may vary from place to place .",
    "we first bound the maximum risk by the average over the whole parameter set , @xmath369 \\\\[-9pt ] \\nonumber & = & \\frac{1}{2^{r}d_{\\lambda}}\\sum _",
    "{ \\theta}\\mathbb{e}_{\\mathbf{x}|\\theta } \\bigl [ 2d \\bigl ( t,\\psi ( \\theta )",
    "\\bigr ) \\bigr]^{s}.\\end{aligned}\\ ] ] set @xmath370 .",
    "note that the minimum is not necessarily unique . when it is not unique , pick @xmath371 to be any point in the minimum set . then the triangle inequality for the metric @xmath77 gives@xmath372^{s } \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & \\leq&\\mathbb{e}_{\\mathbf { x}|\\theta } \\bigl [ 2d \\bigl ( t,\\psi ( \\theta )",
    "\\bigr ) \\bigr]^{s},\\end{aligned}\\ ] ] where the last inequality is due to the fact @xmath373 from the definition of @xmath371 . equations ( [ maxave ] ) and ( [ tri ] ) together yield@xmath374 where the last step follows from the definition of @xmath114 in equation ( [ alpha ] ) .",
    "we now show@xmath375 which immediately implies @xmath376 and lemma  [ al ] follows . from the definition of @xmath377 in equation ( [ h ] ) we write",
    "@xmath378 the right - hand side can be further written as @xmath379 \\\\ & & \\qquad=\\frac{1}{2}\\sum_{i=1}^{r } \\biggl [ \\frac{1}{2^{r-1}d_{\\lambda}}\\sum _ { \\ { \\rho:\\rho_{i}=0 \\ } } \\sum _ { \\ { \\theta : \\gamma ( \\theta)=\\rho\\ } } \\int\\gamma_{i}(\\hat{\\theta})\\,d\\mathbb { p}_{\\theta}\\\\ & & \\hspace*{61pt}{}+\\frac{1}{2^{r-1}d_{\\lambda}}\\sum _ { \\ { \\rho:\\rho_{i}=1 \\ } } \\sum _ { \\ { \\theta:\\gamma(\\theta)=\\rho\\ } } \\int\\bigl(1- \\gamma_{i}(\\hat{\\theta})\\bigr)\\,d\\mathbb{p}_{\\theta^{\\prime } } \\biggr ] \\\\ & & \\qquad=\\frac{1}{2}\\sum_{i=1}^{r } \\biggl [ \\int\\gamma_{i}(\\hat{\\theta } ) \\biggl(\\frac{1}{2^{r-1}d_{\\lambda}}\\sum _ { \\ { \\rho:\\rho_{i}=0 \\ } } \\sum _ { \\ { \\theta:\\gamma(\\theta)=\\rho\\ } } \\,d \\mathbb{p}_{\\theta}\\biggr)\\\\ & & \\hspace*{61pt}{}+\\int\\bigl(1-\\gamma_{i}(\\hat{\\theta } ) \\bigr ) \\biggl(\\frac{1}{2^{r-1}d_{\\lambda}}\\sum _ { \\ { \\rho:\\rho_{i}=1 \\ } } \\sum _ { \\ { \\theta:\\gamma(\\theta ) = \\rho \\ } } \\,d\\mathbb{p}_{\\theta}\\biggr ) \\biggr ] \\\\ & & \\qquad=\\frac{1}{2}\\sum_{i=1}^{r } \\biggl [ \\int\\gamma_{i}(\\hat{\\theta})\\,d\\bar\\mathbb{p}_{i,0}+ \\int\\bigl(1-\\gamma_{i}(\\hat{\\theta})\\bigr)\\,d \\bar\\mathbb{p}_{i,1}\\biggr ] .\\end{aligned}\\ ] ]        it follows immediately from lemma  [ aff ] that @xmath381 & \\geq&\\frac{1}{2}\\sum _ { i=1}^{r}{\\vert}\\bar\\mathbb{p}_{i,0 } \\wedge\\bar\\mathbb{p}_{i,1}{\\vert}\\\\ & \\geq&\\frac{r}{2 } \\min_{i}{\\vert}\\bar\\mathbb{p}_{i,0}\\wedge \\bar\\mathbb{p}_{i,1}{\\vert},\\end{aligned}\\ ] ] and so equation ( [ part2 ] ) is established .",
    "let @xmath382 be a column @xmath3-vector with @xmath383 for @xmath384 and @xmath385 for @xmath386 , that is , @xmath387 . set @xmath388 v$ ] .",
    "note that for each @xmath106 , if @xmath389 , we have @xmath390",
    ". then there are at least @xmath391 number of elements @xmath392 with @xmath393 , which implies@xmath394 v\\bigr{\\vert}_{2}^{2}\\geq h\\bigl(\\gamma(\\theta ) , \\gamma\\bigl(\\theta^{\\prime } \\bigr)\\bigr)\\cdot ( k\\epsilon_{n , p } ) ^{2}.\\ ] ] since @xmath395 , the equation above yields @xmath396 v{\\vert}_{2}^{2}}{{\\vert}v{\\vert}^{2}}\\geq\\frac { h(\\gamma ( \\theta),\\gamma(\\theta^{\\prime}))\\cdot(k\\epsilon_{n , p})^{2}}{p},\\ ] ] that is,@xmath397 when @xmath398 .",
    "the proof of the bound for the affinity given in lemma  [ affbd ] is involved .",
    "we break the proof into a few major technical lemmas which are proved in section  [ chisquaresec ] and the supplementary material . without loss of generality",
    "we consider only the case @xmath399 and prove that there exists a constant @xmath256 such that @xmath400 .",
    "the following lemma is the key step which turns the problem of bounding the total variation affinity into a chi - squared distance calculation on gaussian mixtures .",
    "the proof of lemma  [ chisquareslem](ii ) is relatively easy and is given in the supplementary material .",
    "our goal in the remainder of this proof is to establish ( [ chi - squarebd ] ) , which requires detailed understanding of @xmath404 and the mixture distribution @xmath405 as well as a careful analysis of the cross - product terms in the chi - squared distances on the left - hand side of ( [ chi - squarebd ] ) .    from the definition of @xmath48 in equation ( [ theta ] ) and @xmath404 in equation ( avepibd )",
    ", @xmath406 implies @xmath407 is a single multivariate normal distribution with a covariance matrix , @xmath408 here @xmath409 is a symmetric matrix uniquely determined by @xmath410 where for @xmath411 , @xmath412 let @xmath413 which gives the set of all possible values of the first row with the rest of the rows fixed , that is , @xmath414 .",
    "let @xmath415 be the number of columns of @xmath416 with the column sum equal to @xmath212 for which the first row has no choice but to take value @xmath126 in this column .",
    "set @xmath417 .",
    "it is helpful to observe that @xmath418 . since @xmath419 , the total number of @xmath73s in the upper triangular matrix by the construction of the parameter set",
    ", we thus have @xmath420 , which immediately implies @xmath421 .",
    "it follows @xmath422 .",
    "then , from the definitions in equations ( [ theta ] ) and ( [ avepibd ] ) , @xmath423 is an average of @xmath424 multivariate normal distributions with covariance matrices of the following form:@xmath425 where @xmath426 with nonzero elements of @xmath88 equal @xmath237 and the submatrix @xmath427 is the same as the one for @xmath428 given in ( [ sigma0 ] ) .    recall that for each @xmath85 , @xmath429 is the joint distribution of the @xmath33 i.i.d .",
    "multivariate normal variables @xmath430 .",
    "so each term in the chi - squared distance on the left - hand side of ( [ chi - squarebd ] ) is of the form @xmath431 where @xmath432 are the density function of @xmath433 for @xmath434 and @xmath435 , with @xmath436 defined in ( [ sigma0 ] ) and @xmath437 and @xmath438 of the form  ( [ matrixform ] ) .",
    "let @xmath428 be defined in ( [ sigma0 ] ) and determined by @xmath441 .",
    "let @xmath437 and @xmath438 be of the form ( [ matrixform ] ) with the first row @xmath442 and @xmath443 , respectively .",
    "set @xmath444 we sometimes drop the indices @xmath445 , @xmath446 and @xmath447 from @xmath448 to simplify the notation whenever there is no ambiguity .",
    "then each term in the chi - squared distance on the left - hand side of ( [ chi - squarebd ] ) can be expressed as in the form of @xmath449    define @xmath450 it is a subset of @xmath451 in which the element can pick both @xmath452 and @xmath453 as the first row to form parameters in @xmath53 . from lemma crossproduct",
    "the average of the chi - squared distance on the left - hand side of equation ( [ chi - squarebd ] ) can now be written as @xmath454 \\biggr\\ } \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\qquad=\\tilde\\mathbb{e } _ { ( \\lambda_{1},\\lambda_{1}^{\\prime } ) } \\biggl\\ { \\tilde\\mathbb{e } _ { ( \\gamma_{-1},\\lambda _ { -1 } )    \\biggl [ \\exp\\biggl(\\frac{n}{2}\\cdot r_{\\lambda_{1},\\lambda_{1}^{\\prime}}^{\\gamma_{-1},\\lambda _ { -1 } } \\biggr)-1\\biggr ] \\biggr\\},\\end{aligned}\\ ] ] where @xmath442 and @xmath455 are independent and uniformly distributed over @xmath456 ( not over @xmath99 ) for given @xmath416 , and the distribution of @xmath441 given @xmath457 is uniform over @xmath451 @xmath458 , but the marginal distribution of @xmath442 and @xmath455 are not independent and uniformly distributed over @xmath99 .",
    "let @xmath437 and @xmath438 be two covariance matrices of the form ( [ matrixform ] ) .",
    "note that @xmath437 and @xmath438 differ from each other only in the first row / column .",
    "then @xmath459 , @xmath460 or @xmath435 , has a very simple structure .",
    "the nonzero elements only appear in the first row / column , and in total there are at most @xmath212 nonzero elements .",
    "this property immediately implies the following lemma which makes the problem of studying the determinant in lemma  [ crossproduct ] relatively easy .",
    "the proof of lemma  [ sigmai ] below is given in the supplementary material .",
    "[ sigmai ] let @xmath428 be defined in ( [ sigma0 ] ) and let @xmath437 and @xmath438 be two covariance matrices of the form ( [ matrixform ] ) .",
    "define @xmath461 to be the number of overlapping @xmath462 s between @xmath437 and @xmath438 on the first row , and @xmath463 there are index subsets @xmath464 and @xmath465 in @xmath466 with @xmath467 and @xmath468 such that @xmath469 and the matrix @xmath470 has rank @xmath435 with two identical nonzero eigenvalues @xmath471 when @xmath472 .",
    "the matrix @xmath473 is determined by two interesting parts , the first element @xmath474 and a very special @xmath475 square matrix @xmath476 with all elements equal to @xmath477 .",
    "the following result , which is proved in the supplementary material , shows that @xmath478 is approximately equal to @xmath479 where @xmath461 is defined in lemma  [ sigmai ] .",
    "define@xmath480          equation ( [ rdecomp ] ) in lemma  [ rlem ] yields that @xmath485 \\biggr\\ } \\\\ & & \\qquad=\\tilde\\mathbb{e}_{j } \\biggl\\ { \\exp\\bigl [ -n\\log\\bigl ( 1-j \\epsilon_{n , p}^{2 } \\bigr ) \\bigr ] \\\\ & & \\hspace*{50pt}{}\\times\\tilde\\mathbb{e } _ { ( \\lambda _ { 1},\\lambda_{1}^{\\prime } ) |j } \\biggl [ \\tilde\\mathbb{e } _ { ( \\gamma_{-1},\\lambda_{-1 } ) | ( \\lambda_{1},\\lambda _ { 1}^{\\prime } ) } \\exp\\biggl ( \\frac{n}{2}r_{1,\\lambda_{1},\\lambda _ { 1}^{\\prime } } ^{\\gamma_{-1},\\lambda_{-1 } } \\biggr ) \\biggr ] -1 \\biggr\\ } .\\end{aligned}\\ ] ]    recall that @xmath461 is the number of overlapping @xmath237 s between @xmath437 and @xmath438 on the first row .",
    "it is easy to see that @xmath461 has the hypergeometric distribution as @xmath442 and @xmath486 vary in @xmath99 for each given @xmath416 .",
    "for @xmath487 , @xmath488^{2}}}\\cdot\\frac{1}{j!}\\\\ & \\leq&\\biggl ( \\frac { k^{2}}{p_{\\lambda_{-1}}-k } \\biggr)^{j } , \\nonumber\\end{aligned}\\ ] ] where @xmath489 is a product of @xmath17 term with each term @xmath490 and for @xmath491^{2 } } $ ] it is bounded below by a product of @xmath17 term with each term @xmath492 . since @xmath418 for all @xmath416 , we have @xmath493 \\leq\\biggl ( \\frac { k^{2}}{p/4 - 1-k}\\biggr)^{j}.\\ ] ] thus@xmath494 \\cdot{\\frac{3}{2}}-1 \\biggr\\ } \\\\ & & \\qquad={\\frac{3}{2}}\\sum_{j\\geq1 } \\biggl ( \\frac{k^{2}}{p/4 - 1-k } \\biggr)^{j}\\exp\\bigl [ 2j \\bigl ( \\upsilon^{2}\\log p \\bigr ) \\bigr ]",
    "\\nonumber\\\\ & & \\qquad\\quad{}+ \\biggl ( \\frac{k^{2}}{p/4 - 1-k } \\biggr)^{0 } \\biggl \\ { \\exp\\bigl [ -n \\log\\bigl ( 1 - 0\\cdot\\epsilon_{n , p}^{2 } \\bigr ) \\bigr ] \\cdot { \\frac { 3}{2}}-1 \\biggr\\ } \\nonumber \\\\ & & \\qquad\\leq c\\sum_{j\\geq1 } \\bigl ( p^{{(\\beta-1)}/{\\beta } } \\cdot p^{-2\\upsilon ^{2 } } \\bigr)^{-j}+{\\frac{1}{2}}<c\\sum _ { j\\geq1 } \\bigl ( p^{{(\\beta-1)}/{(2\\beta ) } } \\bigr)^{-j}+ \\frac{1}{2}<c_{2}^{2 } \\nonumber\\end{aligned}\\ ] ] by setting @xmath495 , where the last step follows from @xmath496 and @xmath497 as defined in section [ lowbdsec ] .            let @xmath505 with @xmath506 . then @xmath507^{2}+2 \\mathbb{e}_{\\mathbf{x}|\\theta } |\\!|\\!|d |\\!|\\!|_{1}^{2}+c \\frac{\\log p}{n } \\\\ & & \\qquad\\leq 32 \\biggl [ \\sup_{j}\\sum_{i\\neq j}\\min \\biggl\\ { \\vert\\sigma_{ij}\\vert,\\gamma\\sqrt{\\frac{\\log p}{n } } \\biggr\\ } \\biggr]^{2}+2\\mathbb{e}_{\\mathbf{x}|\\theta }",
    "|\\!|\\!|d     we will see that the first term in equation ( [ thm31bnd ] ) is dominating and is bounded by @xmath508 , while the second term @xmath509 .",
    "then we have @xmath510 j } \\vert,\\sqrt{\\frac{\\log p}{n } } \\biggr\\ } \\nonumber \\\\ & \\leq&c_{5}k^{\\ast}\\sqrt{\\frac{\\log p}{n}}+c_{5 } \\sum_{i > k^{\\ast } } \\biggl ( \\frac{c_{n , p}}{i } \\biggr)^{{1}/{q } } \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & \\leq&c_{6 } \\biggl [ k^{\\ast}\\sqrt{\\frac{\\log p}{n}}+c_{n , p}^{{1}/{q } } \\cdot\\bigl ( k^{\\ast } \\bigr)^{1-{{1}/{q } } } \\biggr]\\\\ & \\leq & c_{7}c_{n , p } \\biggl ( \\frac{\\log p}{n } \\biggr)^{{(1-q)}/{2 } } , \\nonumber\\end{aligned}\\ ] ] which immediately implies equation ( [ rateoper2 ] ) if @xmath511 .",
    "we shall now show that @xmath512 .",
    "note that @xmath513 \\bigr\\ } \\\\ & = & p\\sum_{ij}\\mathbb{e}_{\\mathbf{x}|\\theta } \\bigl\\ { \\bigl ( \\sigma_{ij}^{\\ast}-\\sigma_{ij } \\bigr)^{2}i\\bigl(a_{ij}^{c}\\bigr ) \\bigr\\ } + p\\sum _ { ij}\\mathbb{e}_{\\mathbf{x}|\\theta } \\sigma_{ij}^{2}i\\bigl(a_{ij}^{c}\\cap\\ { \\hat{\\sigma}_{ij}=0 \\}\\bigr ) \\\\ & \\equiv & r_{1}+r_{2}.\\end{aligned}\\ ] ]    lemma  [ a0bound ] yields that @xmath514 , and the whittle inequality implies @xmath515 has all finite moments [ cf . @xcite ] under the subgaussianity condition ( [ subgau ] ) . hence @xmath516^{1/3}\\mathbb{p}^{2/3 }",
    "\\bigl ( a_{ij}^{c } \\bigr ) \\\\ & \\leq&c_{8}p\\cdot p^{2}\\cdot\\frac{1}{n}\\cdot p^{-3}=c_{8}/n.\\end{aligned}\\ ] ] on the other hand , @xmath517 \\cdot\\exp\\biggl ( -{\\frac{4}{\\gamma^{2}}}n\\sigma_{ij}^{2 } \\biggr ) i\\biggl(\\vert\\sigma_{ij}\\vert\\geq4\\gamma\\sqrt { \\frac{\\log p}{n}}\\biggr ) \\\\ & \\leq&c_{9}\\frac{p}{n}\\cdot p^{2}\\cdot p^{-16}\\leq c_{9}/n.\\end{aligned}\\ ] ] putting @xmath518 and @xmath519 together yields that for some constant @xmath283 , @xmath520 theorem  [ minimaxope1 ] is proved by combining equations ( [ thm31bnd ] ) , ( [ bounddominate ] ) and ( [ d1 ] ) .        [ frobd]assume that all eigenvalues of two symmetric matrices @xmath51 and @xmath521 belong to @xmath522 $ ] . then there exist constants @xmath523 depending only on @xmath524 and @xmath525 such that for all @xmath526 defined in ( [ assumpphi ] ) , @xmath527    let the eigen decompositions of @xmath51 and @xmath311 be@xmath528 for every @xmath529 it is easy to see that @xmath530 .\\label{taylorbd}\\ ] ] see @xcite , lemma 1 . the taylor expansion gives@xmath531 where @xmath532 is in between @xmath313 and @xmath533 and then contained in @xmath522 $ ] . from the assumption in ( [ assumpphi ] ) , there are constants @xmath335 and @xmath336 such that @xmath534 for all @xmath100 in @xmath522 $ ] , which immediately implies @xmath535 it is trivial to see that@xmath536 by constructing a parameter space with only diagonal matrices .",
    "it is then enough to show that there exists some constant @xmath192 such that @xmath537 for all @xmath353 defined in ( [ assumpphi ] ) .",
    "equation ( taylorbd ) implies @xmath538 convexity of @xmath314 implies @xmath539 is nonnegative and increasing when @xmath313 moves away from the range @xmath540 $ ] of those eigenvalues @xmath533",
    "s of @xmath541 . from lemma  [ frobd ]",
    "there is a universal constant @xmath335 such that@xmath542 where the last equality is from the same argument for equation ( restrictedminimax ) .",
    "it then suffices to study the lower bound under the frobenius norm .",
    "similar to the lower bound under the spectral norm one has @xmath543 it is easy to see@xmath544 and it follows from lemma  [ affbd ] that there is a constant @xmath192 such that @xmath545    _ upper bound under bregman matrix divergences_. we now show that there exists an estimator @xmath282 such that@xmath546 \\label{rateoper3}\\ ] ] some constant @xmath192 , uniformly over all @xmath353 and @xmath547 .",
    "let @xmath548 , where @xmath502 is defined in ( [ aij ] ) .",
    "lemma  [ a0bound ] yields that @xmath549        lemma  [ frobd ] implies@xmath556 \\\\[-8pt ] \\nonumber & & \\qquad\\leq c\\mathbb{e}_{\\mathbf{x}|\\theta } \\biggl\\ { \\frac{1}{p } |\\!|\\!| \\hat{\\sigma}-\\sigma|\\!|\\!|_{f}^{2}i(a_{0 } ) \\biggr\\ } + \\mathbb{e}_{\\mathbf{x}|\\theta } \\bigl\\ { \\mathrm{l}_{\\phi } ( \\hat { \\sigma}_{b},\\sigma ) i\\bigl(a_{0}^{c}\\bigr ) \\bigr \\ } \\\\ & & \\qquad\\leq 16c\\sup_{j}\\sum_{i\\neq j}\\min\\biggl\\ { \\vert\\sigma_{ij}\\vert^{2},\\gamma\\frac{\\log p}{n } \\biggr\\ } + \\mathbb{e}_{\\mathbf{x}|\\theta } \\bigl\\ { \\mathrm{l}_{\\phi } ( \\hat { \\sigma}_{b},\\sigma ) i\\bigl(a_{0}^{c}\\bigr ) \\bigr \\ } + c\\frac{1}{n}.\\nonumber\\end{aligned}\\ ] ] the second term in ( [ sparsel1bd ] ) is negligible since@xmath557^{\\vert r\\vert}\\cdot \\mathbb{p } \\bigl ( a_{0}^{c }",
    "\\bigr ) \\\\ & \\leq&c\\cdot\\bigl [ \\max\\ { \\log n,\\log p \\ } \\bigr]^{\\vert r\\vert } c_{1}p^{-5/4}\\\\ & = & o \\biggl ( c_{n , p } \\biggl ( \\frac{\\log p}{n } \\biggr)^{1-q/2 } \\biggr)\\end{aligned}\\ ] ] by applying the cauchy ",
    "schwarz inequality twice .",
    "we now consider the first term in equation ( [ sparsel1bd ] ) .",
    "set @xmath558 .",
    "then we have@xmath559 j}\\vert^{2},\\frac{\\log p}{n } \\biggr\\ } \\\\ & \\leq&c_{3}k^{\\ast}\\frac{\\log p}{n}+c_{3}\\sum _ { i > k^{\\ast } } \\biggl ( \\frac{c_{n , p}}{i } \\biggr)^{2/q } \\\\ & \\leq&c_{4 } \\biggl [ k^{\\ast}\\frac{\\log p}{n}+c_{n , p}^{2/q}k^{\\ast } \\cdot\\bigl ( k^{\\ast } \\bigr)^{-2/q } \\biggr ] \\\\ & \\leq & c_{5}c_{n , p } \\biggl ( \\frac{\\log p}{n } \\biggr)^{1-q/2},\\end{aligned}\\ ] ] which immediately yields equation ( [ rateoper3 ] ) ."
  ],
  "abstract_text": [
    "<S> this paper considers estimation of sparse covariance matrices and establishes the optimal rate of convergence under a range of matrix operator norm and bregman divergence losses . </S>",
    "<S> a major focus is on the derivation of a rate sharp minimax lower bound . </S>",
    "<S> the problem exhibits new features that are significantly different from those that occur in the conventional nonparametric function estimation problems . </S>",
    "<S> standard techniques fail to yield good results , and new tools are thus needed .    </S>",
    "<S> we first develop a lower bound technique that is particularly well suited for treating `` two - directional '' problems such as estimating sparse covariance matrices . </S>",
    "<S> the result can be viewed as a generalization of le cam s method in one direction and assouad s lemma in another . </S>",
    "<S> this lower bound technique is of independent interest and can be used for other matrix estimation problems .    </S>",
    "<S> we then establish a rate sharp minimax lower bound for estimating sparse covariance matrices under the spectral norm by applying the general lower bound technique . </S>",
    "<S> a thresholding estimator is shown to attain the optimal rate of convergence under the spectral norm . </S>",
    "<S> the results are then extended to the general matrix @xmath0 operator norms for @xmath1 . </S>",
    "<S> in addition , we give a unified result on the minimax rate of convergence for sparse covariance matrix estimation under a class of bregman divergence losses . </S>"
  ]
}