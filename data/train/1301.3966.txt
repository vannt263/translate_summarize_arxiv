{
  "article_text": [
    "the objective of _ reinforcement learning _ ( rl ) is to let an agent optimize its decision - making policy through interaction with an unknown environment @xcite . among possible approaches , _ policy search _",
    "has become a popular method because of its direct nature for policy learning @xcite .",
    "particularly , in high - dimensional problems with continuous states and actions , policy search has been shown to be highly useful in practice @xcite .    among policy search methods",
    "@xcite , gradient - based methods are popular in physical control tasks because policies are changed gradually @xcite and thus steady performance improvement is ensured until a local optimal policy has been obtained .",
    "however , since the gradients estimated with these methods tend to have large variance and thus they may suffer from slow convergence .",
    "recently , a novel approach to using policy gradients called _ policy gradients with parameter based exploration _ ( pgpe ) was proposed @xcite .",
    "pgpe tends to produce gradient estimates with low variance by removing unnecessary randomness from policies and introducing useful stochasticity by considering a prior distribution for policy parameters .",
    "pgpe was shown to be more promising than alternative approaches experimentally @xcite .",
    "however , pgpe still requires a relatively large number of samples to obtain accurate gradient estimates , which can be a critical bottleneck in real - world applications that require large costs and time in data collection .    to overcome this weakness , an _ importance sampling _",
    "technique @xcite is useful under the _ off - policy _ rl scenario , where a data - collecting policy and the current target policy are different in general @xcite .",
    "an importance sampling technique allows us to reuse previously collected data , which are collected following policies different from the current one in a consistent manner @xcite .",
    "however , naively using an importance sampling technique significantly increases the variance of gradient estimates , which can cause sudden changes in policy updates @xcite . to mitigate this problem ,",
    "variance reduction techniques such as decomposition @xcite , truncation @xcite , normalization @xcite , and flattening @xcite of importance weights are often used .",
    "however , these methods commonly suffer from the bias - variance trade - off , meaning that the variance is reduced at the expense of increasing the bias .",
    "the purpose of this paper is to propose a new approach to systematically addressing the large variance problem in policy search . basically , this work is an extension of our previous research @xcite to an _ off - policy _ scenario using an importance weighting technique .",
    "more specifically , we first give an off - policy implementation of pgpe called the _ importance - weighted pgpe _",
    "( iw - pgpe ) method for consistent sample reuse .",
    "we then derive the optimal baseline for iw - pgpe to minimize the variance of importance - weighted gradient estimates , following @xcite .",
    "we show that the proposed method can achieve significant performance improvement over alternative approaches in experiments with an artificial domain .",
    "we also investigate that combining the proposed method with the truncation technique can further improve the performance in high - dimensional problems .",
    "in this paper , we consider the standard framework of episodic reinforcement learning ( rl ) in which an agent interacts with an environment modeled as a _ markov decision process _",
    "( mdp ) @xcite . in this section ,",
    "we first review a standard formulation of policy gradient methods @xcite .",
    "then we show an alternative formulation adopted in the pgpe ( policy gradients with parameter based exploration ) method @xcite .",
    "we assume that the underlying control problem is a discrete - time mdp . at each discrete time step @xmath0 , the agent observes a state @xmath1 , selects an action @xmath2 , and then receives an immediate reward @xmath3 resulting from a state transition in the environment .",
    "the state @xmath4 and action @xmath5 are both defined as continuous spaces in this paper .",
    "the dynamics of the environment are characterized by @xmath6 , which represents the transition probability density from the current state @xmath7 to the next state @xmath8 when action @xmath9 is taken , and @xmath10 is the probability density of initial states .",
    "the immediate reward @xmath3 is given according to the reward function @xmath11 .",
    "the agent s decision making procedure at each time step @xmath0 is characterized by a parameterized policy @xmath12 with parameter @xmath13 , which represents the conditional probability density of taking action @xmath9 in state @xmath7 .",
    "we assume that the policy is continuously differentiable with respect to its parameter @xmath13 .",
    "a sequence of states and actions forms a _ trajectory _ denoted by @xmath14,\\ ] ] where @xmath15 denotes the number of steps called horizon length . in this paper",
    ", we assume that @xmath15 is a fixed deterministic number .",
    "note that the action @xmath9 is chosen independently of the trajectory given @xmath7 and @xmath13 .",
    "then the discounted cumulative reward along @xmath16 , called the _ return _ , is given by @xmath17 where @xmath18 is the discount factor for future rewards .",
    "the goal is to optimize the policy parameter @xmath13 so that the _ expected return _ is maximized .",
    "the expected return for policy parameter @xmath13 is defined by @xmath19 where @xmath20 the most straightforward way to update the policy parameter is to follow the gradient in policy parameter space using gradient ascent : @xmath21 where @xmath22 is a small positive constant , called the learning rate .",
    "this is a standard formulation of policy gradient methods @xcite .",
    "the central problem is to estimate the policy gradient @xmath23 accurately from trajectory samples .",
    "however , standard policy gradient methods were shown to suffer from high variance in the gradient estimation due to randomness introduced by the stochastic policy model @xmath24 @xcite . to cope with this problem , an alternative method called _ policy gradients with parameter based exploration _",
    "( pgpe ) was proposed recently @xcite .",
    "the basic idea of pgpe is to use a deterministic policy and introduce stochasticity by drawing parameters from a prior distribution .",
    "more specifically , parameters are sampled from the prior distribution at the start of each trajectory , and thereafter the controller is deterministic . thanks to this per - trajectory formulation , the variance of gradient estimates in pgpe does not increase with respect to trajectory length @xmath15 .",
    "below , we review pgpe .",
    "pgpe uses a deterministic policy with typically a linear architecture : @xmath25 where @xmath26 is the _ dirac delta function _",
    ", @xmath27 is an @xmath28-dimensional basis function vector , and @xmath29 denotes the transpose .",
    "the policy parameter @xmath13 is drawn from a prior distribution @xmath30 with hyper - parameter @xmath31 .",
    "the expected return in the pgpe formulation is defined in terms of expectations over both @xmath16 and @xmath13 as a function of hyper - parameter @xmath31 : @xmath32 in pgpe , the hyper - parameter @xmath31 is optimized so as to maximize @xmath33 , i.e. , the optimal hyper - parameter @xmath34 is given by @xmath35    in practice , a gradient method is used to find @xmath34 : @xmath36 where @xmath37 is the derivative of @xmath38 with respect to @xmath31 : @xmath39 note that , in the derivation of the gradient , the logarithmic derivative , @xmath40 was used .",
    "the expectations over @xmath16 and @xmath13 are approximated by the empirical averages : @xmath41 where each trajectory sample @xmath42 is drawn independently from @xmath43 and parameter @xmath44 is drawn from @xmath45 .",
    "we denote samples collected at the current iteration as @xmath46    following @xcite , in this paper we employ a gaussian distribution as the distribution of the policy parameter @xmath13 with the hyper - parameter @xmath31 .",
    "however , other distributions can also be allowed . when assuming a gaussian distribution , the hyper - parameter @xmath31 consists of a set of means @xmath47 and standard deviations @xmath48 , which determine the prior distribution for each element @xmath49 in @xmath13 of the form @xmath50 where @xmath51 denotes the normal distribution with mean @xmath52 and variance @xmath53 .",
    "then the derivative of @xmath54 with respect to @xmath52 and @xmath55 are given as @xmath56 which can be substituted into eq.([emp_gra ] ) to approximate the gradients with respect to @xmath57 and @xmath58 .",
    "these gradients give the pgpe update rules .",
    "an advantage of pgpe is its low variance of gradient estimates : compared with a standard policy gradient method reinforce @xcite , pgpe was empirically demonstrated to be better in some settings @xcite .",
    "the variance of gradient estimates in pgpe can be further reduced by subtracting an optimal baseline ( theorem 4 of @xcite ) .",
    "another advantage of pgpe is its high flexibility : in standard policy gradient methods , the parameter @xmath13 is used to determine a stochastic policy model @xmath24 , and policy gradients are calculated by differentiating the policy with respect to the parameter . however , because pgpe needs not calculate the derivative of the policy , a non - differentiable controller is also allowed .",
    "in real - world applications such as robot control , gathering roll - out data is often costly .",
    "thus , we want to keep the number of samples as small as possible . however , when the number of samples is small , policy gradients estimated by the original pgpe are not reliable enough .",
    "the original pgpe is categorized as an _ on - policy _ algorithm @xcite , where data drawn from the current target policy is used to estimate policy gradients . on the other hand , _ off - policy _",
    "algorithms are more flexible in the sense that a data - collecting policy and the current target policy can be different . in this section ,",
    "we extend pgpe to an _ off - policy _ scenario using importance - weighting , which allows us to reuse previously collected data in a consistent manner .",
    "we also theoretically analyze properties of the extended method .",
    "let us consider an off - policy scenario where a data - collecting policy and the current target policy are different in general . in the context of pgpe , we consider two hyper - parameters , @xmath31 for the target policy to learn and @xmath59 for data collection .",
    "let us denote data samples collected with hyper - parameter @xmath59 by @xmath60 : @xmath61 if we naively use data @xmath60 to estimate policy gradients by eq .",
    ", we have an inconsistency problem : @xmath62 which we refer to as `` _ _ non - importance - weighted pgpe _ _ '' ( niw - pgpe ) .    _ importance sampling _",
    "@xcite is a technique to systematically resolve this distribution mismatch problem .",
    "the basic idea of importance sampling is to weight samples drawn from a sampling distribution to match the target distribution , which gives a consistent gradient estimator : @xmath63 where @xmath64 is called the _ importance weight_.    an intuition behind importance sampling is that if we know how `` important '' a sample drawn from the sampling distribution is in the target distribution , we can make adjustment by importance weighting .",
    "we call this extended method _ importance - weighted pgpe _",
    "( iw - pgpe ) .",
    "now we analyze the variance of gradient estimates in iw - pgpe . for a multi - dimensional space",
    ", we consider the _ trace _ of the covariance matrix of gradient vectors .",
    "that is , for a random vector @xmath65 , we define @xmath66)(\\bm{a}-{\\mathbb{e}}[\\bm{a}])^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\big]\\big)\\nonumber,\\\\ = & \\sum_{m=1}^\\ell { \\mathbb{e}}\\big[(a_m-{\\mathbb{e}}[a_m])^2\\big],\\end{aligned}\\ ] ] where @xmath67 denotes the expectation .",
    "let @xmath68 where @xmath28 is the dimensionality of the basis function vector @xmath69 .",
    "for a @xmath70 , we have the following theorem :    [ theorem : variance - bound - pgpe ] assume that for all @xmath71 , @xmath72 , and @xmath73 , there exists @xmath74 such that @xmath75 $ ] , and , for all @xmath13 , there exists @xmath76 such that @xmath77",
    ". then we have the following upper bounds : @xmath78&\\le \\frac{\\beta^2(1-\\gamma^t)^2 b}{n'(1-\\gamma)^2}w_{\\max},\\\\ \\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\tau } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}(\\bm{\\rho})\\right]&\\le \\frac{2\\beta^2(1-\\gamma^t)^2 b}{n'(1-\\gamma)^2}w_{\\max}.\\end{aligned}\\ ] ]    theorem [ theorem : variance - bound - pgpe ] shows that the upper bound of the variance of @xmath79 is proportional to @xmath80 ( the upper bound of squared rewards ) , @xmath81 ( the upper bound of the importance weight @xmath82 ) , @xmath83 ( the trace of the inverse gaussian covariance ) , and @xmath84 , and is inverse - proportional to sample size @xmath85 .",
    "it is interesting to see that the upper bound of the variance of @xmath86 is twice larger than that of @xmath79 .",
    "it is also interesting to see that the upper bounds are the same as the upper bounds for the plain pgpe ( theorem 1 of @xcite ) except for the factor @xmath87 ; when @xmath88 , the bounds are reduced to those of the plain pgpe method .",
    "however , if the sampling distribution is significantly different from the target distribution , @xmath81 can take a large value and thus iw - pgpe tends to produce a gradient estimator with large variance ( at least in terms of its upper bound ) . therefore , iw - pgpe may not be a reliable approach as it is .",
    "below , we give a variance reduction technique for iw - pgpe , which leads to a highly effective policy gradient algorithm .",
    "to cope with the large variance of gradient estimates in iw - pgpe , several techniques have been developed in the context of sample reuse , for example , by flattening @xcite , truncating @xcite , and normalizing @xcite the importance weight .",
    "indeed , from theorem  [ theorem : variance - bound - pgpe ] , we can see that decreasing @xmath81 by flattening or truncating the importance weight reduces the upper bounds of the variance of gradient estimates .",
    "however , all of those techniques are based on the bias - variance trade - off , and thus they lead to biased estimators .",
    "another , and possibly more promising variance reduction technique is subtraction of a constant _ baseline _ @xcite , which reduces the variance _ without _ increasing the bias . here",
    ", we derive an optimal baseline for iw - pgpe to minimize the variance , and analyze its theoretical properties .",
    "a policy gradient estimator with a baseline @xmath89 is defined as @xmath90 it is well known that @xmath91 is still a consistent estimator of the true gradient for any constant @xmath92 @xcite . here , we determine the constant baseline @xmath92 so that the variance is minimized , following the line of @xcite .",
    "let @xmath93 be the optimal constant baseline for iw - pgpe that minimizes the variance : @xmath94.\\ ] ] then the following theorem gives the optimal constant baseline for iw - pgpe :    [ theorem : optimal - baseline ] the optimal constant baseline for iw - pgpe is given by @xmath95 }        { { \\mathbb{e}}_{p(h,\\bm{\\theta}|\\bm{\\rho'})}[w^2(\\bm{\\theta})\\|\\nabla_{\\bm{\\rho } } \\log p(\\bm{\\theta}|\\bm{\\rho})\\|^2]},\\ ] ] and the excess variance for a constant baseline @xmath92 is given by @xmath96-\\operatorname{\\mathbf{var}}[\\nabla_{\\bm{\\rho } } \\widehat{{\\mathcal{j}}}^{b^*}_{\\mathrm{iw}}(\\bm{\\rho } ) ] = \\frac{(b - b^*)^2}{n ' } { \\mathbb{e}}_{p(h,\\bm{\\theta}|\\bm{\\rho'})}[w^2(\\bm{\\theta})\\| \\nabla_{\\bm{\\rho } } \\log p(\\bm{\\theta}|\\bm{\\rho})\\|^2],\\ ] ] where @xmath97 $ ] denotes the expectation of the function of random variables @xmath16 and @xmath13 with respect to @xmath98 .",
    "the above theorem gives an analytic expression of the optimal constant baseline for iw - pgpe .",
    "it also shows that the excess variance is proportional to the squared difference of baselines @xmath99 and the expectation of the product of squared importance weight @xmath82 and the squared norm of characteristic eligibility @xmath100 , and is inverse - proportional to sample size @xmath85 .",
    "next , we analyze contributions of the optimal baseline to variance reduction in iw - pgpe :    [ theorem : variance - bound - gap ] assume that for all @xmath71 , @xmath72 , and @xmath73 , there exists @xmath101 such that @xmath102 , and , for all @xmath13 , there exists @xmath103 such that @xmath104 .",
    "then we have the following lower bounds : @xmath105-\\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\eta } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}^{b^*}(\\bm{\\rho})\\right ] & \\geq   \\frac{\\alpha^2(1-\\gamma^t)^2 b}{n'(1-\\gamma)^2}w_{\\min},\\\\ \\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\tau } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}(\\bm{\\rho})\\right]-\\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\tau } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}^{b^*}(\\bm{\\rho})\\right ] & \\geq \\frac{2\\alpha^2(1-\\gamma^t)^2 b}{n'(1-\\gamma)^2}w_{\\min}.\\end{aligned}\\ ] ] assume that for all @xmath71 , @xmath72 , and @xmath73 , there exists @xmath74 such that @xmath75 $ ] , and , for all @xmath13 , there exists @xmath76 such that @xmath77 .",
    "then we have the following upper bounds : @xmath78-\\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\eta } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}^{b^*}(\\bm{\\rho})\\right]&\\le \\frac{\\beta^2(1-\\gamma^t)^2 b}{n'(1-\\gamma)^2}w_{\\max},\\\\ \\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\tau } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}(\\bm{\\rho})\\right]-\\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\tau } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}^{b^*}(\\bm{\\rho})\\right]&\\le \\frac{2\\beta^2(1-\\gamma^t)^2 b}{n'(1-\\gamma)^2}w_{\\max}.\\end{aligned}\\ ] ]    this theorem shows that the bounds of the variance reduction in iw - pgpe brought by the optimal constant baseline depend on the bounds of the importance weight . if importance weights are larger , using the optimal baseline can reduce the variance more .    based on theorems [ theorem :",
    "variance - bound - pgpe ] and [ theorem : variance - bound - gap ] , we get the following corollary :    [ corollary : variance - ob ] assume that for all @xmath71 , @xmath72 , and @xmath73 , there exists @xmath106 such that @xmath107 $ ] , and , for all @xmath13 , there exists @xmath108 such that @xmath109 .",
    "then we have the following upper bounds : @xmath110 & \\le \\frac{(1-\\gamma^t)^2 b}{n'(1-\\gamma)^2}(\\beta^2 w_{\\max}-\\alpha^2 w_{\\min}),\\\\ \\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\tau } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}^{b^*}(\\bm{\\rho})\\right ] & \\le \\frac{2(1-\\gamma^t)^2 b}{n'(1-\\gamma)^2}(\\beta^2 w_{\\max}-\\alpha^2 w_{\\min}).\\end{aligned}\\ ] ]    comparing theorem [ theorem : variance - bound - pgpe ] and this corollary , we can see that the upper bounds for iw - pgpe with the optimal constant baseline are smaller than those for iw - pgpe with no baseline because @xmath111 .",
    "although they are just upper bounds , they can still intuitively show that subtraction of the optimal constant baseline contributes to mitigating the large variance caused by importance weighting .",
    "if @xmath112 is larger , then the upper bounds for iw - pgpe with the optimal constant baseline can be much smaller than those for iw - pgpe with no baseline .",
    "in this section , we experimentally investigate the usefulness of the proposed method , importance - weighted pgpe with the optimal constant baseline ( which we denote by iw - pgpe~ob~ hereafter ) . in the experiments",
    ", we estimate the optimal constant baseline using all collected data , as suggested in @xcite .",
    "this approach introduces bias into the method because the same sample - set is used both for estimating the gradient and the baseline .",
    "another possibility is to split the data into two parts : one is used for estimating the optimal constant baseline and the other is used for estimating the gradient .",
    "however , we found that this splitting approach does not work well in our preliminary experiments .",
    "the matlab implementation of iw - pgpe~ob~ is available from : http://sugiyama-www.cs.titech.ac.jp/~tingting/software.html .",
    "first , we illustrate the behavior of pgpe methods using a toy dataset .",
    "the dynamics of the environment is defined as @xmath113 where @xmath114 , @xmath115 , and @xmath116 is stochastic noise .",
    "the initial state @xmath117 is randomly chosen from the standard normal distribution .",
    "the linear deterministic controller is represented by @xmath118 for @xmath119 .",
    "the immediate reward function is given by @xmath120 which is bounded in @xmath121 $ ] .",
    "in the toy dataset experiments , we always set the discount factor at @xmath122 , and we always use the adaptive learning rate @xmath123 @xcite .    here , we compare the following pgpe methods :    * * pgpe : * plain pgpe without data reuse @xcite . *",
    "* pgpe~ob~ : * plain pgpe with the optimal constant baseline without data reuse @xcite . *",
    "* niw - pgpe : * data - reuse pgpe without importance weights . * * niw - pgpe~ob~ : * data - reuse pgpe~ob~ without importance weights . * * iw - pgpe : * importance - weighted pgpe . * * iw - pgpe~ob~ : * importance - weighted pgpe with the optimal baseline .",
    "suppose that a small amount of samples consisting of @xmath124 trajectories with length @xmath15 is available at each iteration .",
    "more specifically , given the hyper - parameter @xmath125 at the @xmath126th iteration , we first choose the policy parameter @xmath127 from @xmath128 , and then run the agent to generate trajectory @xmath129 according to @xmath130 .",
    "initially , the agent starts from a randomly selected state @xmath117 following the initial state probability density @xmath131 and chooses an action based on the policy @xmath132 .",
    "then the agent makes a transition following the dynamics of the environment @xmath133 and receives a reward @xmath134",
    ". the transition is repeated @xmath15 times to get a trajectory , which is denoted as @xmath135 .",
    "we repeat the procedure @xmath124 times , and , the samples gathered at the @xmath126th iteration is obtained , which is expressed as @xmath136 .    in the data - reuse methods",
    ", we estimate gradients at each iteration based on the current data and all previously collected data @xmath137 , by the estimated gradients to update the policy hyper - parameters ( i.e. , mean @xmath138 and standard deviation @xmath139 ) . in the plain pgpe method and the plain pgpe~ob~ method , we only use the on - policy data @xmath140 to estimate the gradients at each iteration , by the estimated gradients to update the policy hyper - parameters . if the deviation parameter @xmath139 takes a value smaller than @xmath141 during the parameter - update process , we set it at @xmath141 .",
    "below , we experimentally evaluate the variance , bias , and mean squared error of the estimated gradients , trajectories of learned hyper - parameters , and obtained returns .",
    "we investigate how data reuse influences estimated gradients over iterations .",
    "below , we focus on gradients with respect to the mean parameter @xmath138 .",
    "we randomly choose initial mean parameter @xmath138 from the standard normal distribution , and fix the initial deviation parameter at @xmath142 .",
    "we collect @xmath143 trajectories with the trajectory length @xmath144 at each iteration , and update hyper - parameters over @xmath145 iterations . here , the variance and squared bias of estimated gradients at each iteration ( e.g. , at the @xmath126th iteration , @xmath146 ) are investigated for @xmath147 trials : @xmath148 where @xmath149 is an estimated gradient in the @xmath150-th trial .",
    "more specifically , we estimate the gradients @xmath151 times with different random seeds at the @xmath126th iteration as follows : we generate samples @xmath152 following the corresponding distributions @xmath153 in each trial ( @xmath154 ) , and we estimate the gradient @xmath155 with the generated samples @xmath156 .",
    "the variance and squared bias at the @xmath126th iteration are calculated based on the estimated gradients from @xmath151 trials . in this experiment ,",
    "the true gradient @xmath157 at the @xmath126th iteration is approximated by the plain pgpe method using eq.([emp_gra ] ) with @xmath158 on - policy samples .",
    "note that the sum of the variance and squared bias agrees with the mean squared error : @xmath159 we update the hyper - parameters @xmath160 based on the estimated true gradient @xmath157 , and obtain @xmath161 .",
    "then , we investigate the variance and bias at the next iteration , i.e. , the @xmath162th iteration , following the above procedures .",
    "figure  [ fig : var - bias - mse ] shows the variance and squared bias over @xmath145 iterations .    from figure",
    "[ fig : var ] , we can see that iw - pgpe~ob~ provides gradient estimates with the lowest variance among the compared methods .",
    "iw - pgpe has a larger variance than niw - pgpe , which well agrees with our theoretical analysis : according to theorem  [ theorem : variance - bound - pgpe ] , upper bounds of the variance are proportional to the importance weight , which is always @xmath163 in niw - pgpe , but is very large in iw - pgpe if the target distribution is significantly different from the sampling distribution . in order to see whether the upper bound of importance weights is really large , we measure the maximum value of importance weights over iterations , which is shown in figure  [ fig : maxvalue ] .",
    "figure  [ fig : maxiw ] shows that the maximum value of importance weights tends to be larger over iterations , which further illustrates how importance weights influence the variance of gradient estimates in iw - pgpe .",
    "we can also see that the gap in the variance between iw - pgpe and iw - pgpe~ob~ tends to be larger over iterations , which is also consistent with our theoretical analysis : according to theorem  [ theorem : variance - bound - gap ] , the larger the importance weight is , the more the optimal constant baseline contributes to reducing the variance .",
    "the importance weight may get larger at later iterations , because distributions in the first and the last iterations may be significantly different ( figure  [ fig : maxvalue ] exactly illustrates this phenomenon ) . thus , variance reduction from iw - pgpe to iw - pgpe~ob~ by the optimal constant baseline tends to be more significant in later iterations .",
    "gradient estimates in both niw - pgpe~ob~ and iw - pgpe~ob~ are with smaller variance than the plain pgpe~ob~ method , because the more data we use , the smaller variance of gradient estimates we can obtain as expected from the theory . iw - pgpe~ob~ provides smaller variance than niw - pgpe~ob~ , which is our expected result : according to theorem [ theorem : variance - bound - gap ] , if the importance weights are larger , using the optimal constant baseline can reduce variance more , while the importance weights are always @xmath163 in niw - pgpe~ob~ ( see figure  [ fig : maxobiw ] ) .",
    "the plain pgpe~ob~ has smaller variance than the plain pgpe , which well agrees with the results reported in @xcite .",
    "figure  [ fig : bias ] shows that introduction of the optimal baseline does not increase the bias .",
    "niw - pgpe and niw - pgpe~ob~ have very large bias , because naively reusing previous data leads to an inconsistent and biased gradient estimator .",
    "the bias of gradient estimates in iw - pgpe is fairly small , because iw - pgpe is not only consistent , but also unbiased .",
    "the plain pgpe and plain pgpe~ob~ are also with small bias , as expected .",
    "because our proposed iw - pgpe~ob~ has small bias and the smallest variance among the compared methods , it also gives the smallest mean squared error ( see eq . ) .",
    "next , we illustrate how learned hyper - parameters change over iterations .",
    "here we compare the behavior of the following three methods : niw - pgpe , iw - pgpe and our proposed method iw - pgpe~ob~. we fix the initial deviation parameter at @xmath142 , and test the three different initial mean parameters : @xmath164 , @xmath165 , and @xmath166 .",
    "figure  [ fig : parchange - stable ] depicts the contour of the expected return , where the maximum of the return surface is located at the middle bottom .",
    "first , let us investigate how the hyper - parameters change over 20 iterations in a large - sample case with @xmath143 . from figure",
    "[ fig : parniw - n10 ] , we can see that niw - pgpe can not properly update the solutions , which means that the inconsistency can not be overcome by increasing the number of samples . on the other hand ,",
    "figure  [ fig : pariw - n10 ] shows that iw - pgpe can lead the solutions to an area with large returns sometimes , but can not always reach an area with large returns after 20 iterations .",
    "this indicates that the consistency of importance weighting tends to be helpful when the number of samples is large , but it can not converge rapidly because of the large variance .",
    "figure  [ fig : parobiw - n10 ] shows that iw - pgpe~ob~ gives the reliable update directions and the three paths converge rapidly to the vicinity of the maximum point without detours .",
    "this shows that the optimal constant baseline highly contributes to improving the convergence property of iw - pgpe .",
    "next , we investigate the performance over @xmath167 iterations with only @xmath168 . figure  [ fig : parniw - n1 ] shows that niw - pgpe can not properly update the solutions to the maximum point because of the inconsistency , and figure  [ fig : pariw - n1 ] shows that the iw - pgpe solutions can not always reach an area with large returns ( middle bottom ) after 200 iterations , which is because the variance in iw - pgpe is crucial in this extreme scenario .",
    "however , figure  [ fig : parobiw - n10 ] shows that the proposed iw - pgpe~ob~ can still find fairly reliable update directions with only @xmath168 .",
    "next , we investigate the directions of estimated gradients more systematically .",
    "we fix the starting point at @xmath169 and @xmath170 .",
    "the true gradient direction is calculated by the plain pgpe method with 10000 on - policy samples . in this experiment",
    ", we first collect @xmath171 off - policy samples , which are drawn from @xmath172 .",
    "we then reuse these off - policy samples to estimate the gradients in the data - reuse methods .",
    "we calculate the gradients 20 times with different random seeds , and investigate the angle between the true gradient and the estimated gradients .",
    "the results are summarized in figure  [ fig : one - change ] . in figure",
    "[ fig : one - niw ] , the red line denotes the true gradient and blue lines are the estimated gradients by the niw - pgpe method .",
    "the histograms of angles between the true gradient and the estimated gradients are plotted in figure  [ fig : hist - niw ] .",
    "the graph shows that the angles are concentrated in @xmath173 $ ] , which further explains the inconsistent property of the niw - pgpe method . observing the angle distribution for iw - pgpe in figure  [ fig : hist - iw ]",
    ", we can see that the angles are widely distributed in @xmath174 $ ] , which clearly illustrates the large variance problem of iw - pgpe .",
    "on the other hand , the angles for the iw - pgpe~ob~ method are concentrated in @xmath175 $ ] , which highlights the small variance and consistent properties of iw - pgpe~ob~.      finally , we evaluate average expected returns obtained by each method over @xmath145 runs .",
    "the expected return at each trial is approximated using @xmath176 newly - drawn test episodic data ( which are not used for policy learning ) .",
    "the initial mean parameter @xmath138 is chosen randomly from the standard normal distribution , and the deviation parameter is fixed at @xmath142 .",
    "figure  [ fig : performance - n10 ] shows that iw - pgpe~ob~ improves the performance over iterations and converges very fast .",
    "the performance of niw - pgpe is not largely improved over iterations , which is caused by biased gradient estimates ( see figure  [ fig : parniw - n10 ] again ) .",
    "iw - pgpe works better than niw - pgpe , but the performance is saturated after @xmath177 iterations .",
    "iw - pgpe~ob~ does not outperform niw - pgpe~ob~ that much at the first several iterations , because the difference between the target distribution and a sampling distribution is not that large at the beginning .",
    "however , the upper bound of importance weights tends to become larger over iterations ( see figure  [ fig : maxobiw ] again ) , which makes iw - pgpe~ob~ more reliable than niw - pgpe~ob~ in the latter iterations .",
    "the plain pgpe~ob~ method works fairly well with @xmath143 on - policy samples , but it is still not as good as iw - pgpe~ob~.     runs for toy data .",
    "error bars denote standard errors . ]      next , we evaluate our proposed method in the _ mountain car _ task , which is illustrated in figure  [ fig : car ] .",
    "the task consists of a car and two hills whose landscape is described as @xmath178 .",
    "the top of the right hill is the goal to which we want to guide the car .",
    "we compare the following @xmath179 methods :    * * tiw - enac : * truncated importance - weight episodic natural actor - critic , which is an episodic version of the sample - reuse nac method @xcite .",
    "following the same line as @xcite , we truncate the importance weight as @xmath180 . * * iw - reinforce~ob~ : * importance - weighted reinforce with the optimal baseline , which is basically a combination of the off - policy implementation of the episodic reinforce method @xcite and the optimal baseline @xcite , although we could not exactly find this method in literature .",
    "* * r^3^ : * reward - weighted regression with sample reuse @xcite . *",
    "* pgpe~ob~ : * plain pgpe~ob~ without data reuse .",
    "* * niw - pgpe~ob~ : * data - reuse pgpe~ob~ without importance weighting . * * iw - pgpe : * importance - weighted pgpe . * * iw - pgpe~ob~ : * importance - weighted pgpe with the optimal baseline .",
    "the state space @xmath4 is two - dimensional and continuous , which consists of the horizontal position @xmath181\\in[-1.2 , 0.5]$ ] and the velocity @xmath182\\in[-1.5 , 1.5]$ ] , i.e. , @xmath183 .",
    "this is non - linearly transformed to a feature space via a basis function vector @xmath69 .",
    "we use @xmath184 gaussian kernels with mean @xmath185 and standard deviation @xmath186 as the basis functions , @xmath187 where the kernel centers @xmath185 are distributed over the following grid points : @xmath188    the action space @xmath5 is one - dimensional and continuous , which corresponds to the force applied to the car ( note that the force of the car is not strong enough to climb up the slope to directly reach the goal ) .",
    "we use the gaussian policy model for iw - reinforce~ob~ , tiw - enac , and r^3^ : @xmath189 where @xmath190 is the mean policy parameter and @xmath191 is the deviation policy parameter . we employ a linear deterministic policy model for the pgpe methods , which corresponds to eq . with @xmath192 .",
    "the dynamics of the car ( i.e. , the update rules of the position and the velocity ) are given by @xmath193 where @xmath9 is the action taken at time @xmath0 .",
    "we set the problem parameters as follows : the mass of the car @xmath194[kg ] , the friction coefficient @xmath195 , and the simulation time step @xmath196[s ] .",
    "the reward function is defined as @xmath197    the initial mean parameter @xmath57 is chosen randomly from the standard normal distribution , and the initial deviation parameter is set at @xmath142 .",
    "the initial state of the car is set at the bottom of the mountain with the velocity @xmath198 .",
    "the agent collects @xmath143 episodic samples with trajectory length @xmath199 at each iteration . in the data reuse methods",
    ", we reuses all previous data at later iterations . in the plain pgpe~ob~ method",
    ", we just use @xmath143 on - policy samples at each iteration to estimate policy gradients .",
    "the discount factor is set at @xmath200 .",
    "the learning rate is @xmath201",
    ".     runs as functions of the number of iterations for the mountain - car task .",
    "error bars are standard errors . ]",
    "runs as functions of the number of iterations for the mountain - car task .",
    "error bars are standard errors . ]",
    "we investigate average expected returns over @xmath202 trials as functions of policy - update iterations . the expected return at each trial is computed over @xmath176 newly - drawn test episodic samples ( which are not used for policy learning ) .",
    "the experimental results are plotted in figure  [ fig : mountain ] .",
    "this shows that iw - pgpe~ob~ improves the performance very fast over policy - update iterations , and it achieves superior performance improvement than all other methods .",
    "iw - pgpe can also improve the performance over iterations well , implying that the consistency of the iw estimator is useful in this task .",
    "however , it is outperformed by the proposed iw - pgpe~ob~ , perhaps because the estimation variance in iw - pgpe is large .",
    "niw - pgpe~ob~ performs fairly well , which maybe because the bias of policy gradient estimators is not that crucial in this experiment .",
    "the plain pgpe~ob~ can improve the performance throughout the iterations , which indicates that @xmath143 on - policy samples is enough for this mountain - car task .",
    "other data - reuse methods can improve the performance over iterations , but slowly , and they are outperformed by the compared pgpe methods .",
    "iw - reinforce~ob~ outperforms tiw - enac , which maybe because the optimal constant baseline contributes significantly in iw - reinforce~ob~ and truncating the importance weights can lead to a larger bias over iterations in tiw - enac .",
    "r^3^ can not improve the performance over iterations .",
    "overall , thanks to the low variance , iw - pgpe~ob~ achieves smooth and fast policy improvement throughout iterations , and its performance is the best among the compared methods .      finally , we evaluate the performance of our proposed method on a highly nonlinear dynamic control problem of the simulated upper - body model of the humanoid robot _ cb - i _ @xcite ( see figure [ fig : cbi ] ) .",
    "we use its simulator in our experiments ( see figure [ fig : simulator ] ) .",
    "the goal is to lead the end - effector of the right arm ( right hand ) to a target object .",
    "we compare the performance of the following @xmath203 methods :    * * iw - reinforce~ob~ : * importance - weighted reinforce with the optimal baseline . * * niw - pgpe~ob~ : * data - reuse pgpe~ob~ without importance weighting . *",
    "* pgpe~ob~ : * plain pgpe~ob~ without data reuse . * * iw - pgpe~ob~ : * importance - weighted pgpe with the optimal baseline .    the simulation is based on the upper body of the cb - i humanoid robot illustrated in figure [ fig : simulator ] , which has 9 degrees of freedom corresponding to main joints of the upper body : the shoulder pitch , shoulder roll , elbow pitch of the right arm , shoulder pitch , shoulder roll , elbow pitch of the left arm , waist yaw , torso roll , and torso pitch .    at each time step , the controller receives states from the system and sends out actions .",
    "the state space is 18-dimensional , which corresponds to the current angle and the current angular velocity of each joint .",
    "the action space is 9-dimensional , which corresponds to the target angle of each joint .",
    "both states and actions are continuous .",
    "the initial positions of the robot and an object are fixed , where the initial position of the robot is set at the state of standing up straight with the arms down , and the position of the target object depends on the task .",
    "note that the position of the target object is only used in the designing of the reward function .",
    "the reward function is given by @xmath204 where @xmath205 , @xmath206 , @xmath207 is the distance between the robot s right hand and the target object at the time step @xmath0 , and @xmath208 is the sum of control costs for each joint .",
    "note that the results may change with different @xmath209 and @xmath210 for the reward function . in order to keep the value of @xmath211 and @xmath208 in the reward function to the same order of magnitude , we need to choose @xmath209 and @xmath210 reasonably .",
    "we use the same policy model as the mountain car experiment , i.e. , the linear deterministic policy for pgpe and the gaussian policy for iw - reinforce~ob~ with the basis function @xmath212 .    the initial mean parameter @xmath138",
    "is randomly chosen from the standard normal distribution , and the initial standard deviation parameter @xmath139 is set to @xmath163 . to evaluate the usefulness of the data reuse methods with a small number of samples",
    ", the agent collects only @xmath213 on - policy samples with trajectory length @xmath214 at each iteration . in the data reuse methods",
    ", we reuse all previous data at later iterations . in the plain pgpe~ob~ ,",
    "we just use the on - policy samples to estimate the gradients .",
    "the discount factor is set at @xmath122 , and the learning rate is set at @xmath215      first , we investigate the performance on the reaching task with only 2 degrees of freedom .",
    "we fix the body of the robot and use only the right shoulder pitch and right elbow pitch .",
    "figure [ fig : j2-return ] depicts the averaged expected return over 10 trials as a function of the number of iterations .",
    "the expected return at each trial is computed from 50 newly - drawn test episodic data ( which are not used for policy learning ) .",
    "the graph shows that iw - pgpe~ob~ nicely improves the performance over iterations only with a small number of on - policy samples .",
    "the plain pgpe~ob~ can also improve the performance over iterations , but slowly .",
    "niw - pgpe~ob~ is not as good as iw - pgpe~ob~ especially at the later iterations , which is because of the inconsistent property of the niw estimator .",
    "the initial mean parameter is randomly chosen in this experiment , which makes iw - reinforce~ob~ not able to improve the performance significantly over iterations .",
    "this result is consistent with the observation that the reinforce method is sensitive to the initial parameter values @xcite .",
    "the distance from the right hand to the object and the control costs along the trajectory are also investigated .",
    "we test the initial policy , the policy obtained at the @xmath145th iteration by iw - pgpe~ob~ , and the policy obtained at the 50th iteration by iw - pgpe~ob~. the results are shown in figure [ fig : j2_dtct ] . from figure",
    "[ fig : distance ] , it is clear to see that the policy obtained at the 50th iteration decreases the distance fastest compared with the initial policy and the policy obtained at the @xmath145th iteration .",
    "this means the robot can reach the object fast by using the learned policy . on the other hand ,",
    "figure [ fig : cost ] shows that the control cost required for executing the policy obtained at the 50th iteration decreases steadily until the reaching task is completed .",
    "this is because the robot mainly adjusts the shoulder pitch in the beginning , which consumes a larger amount of energy than the energy required for controlling the elbow pitch .",
    "then , once the right hand gets closer to the target object , the robot starts to adjust the elbow pitch reach the target object .",
    "the policy obtained at the 20th iteration actually consumes less control costs , but it can not move the arm to the target object .",
    "figure [ fig : j2_trajectory ] shows a typical solution of the reaching task with 2 degrees of freedom by iw - pgpe~ob~ ( with the policy obtained at the 50th iteration ) .",
    "the images show that the policy learned by our proposed method successfully leads the right hand to the target object within only 10 time steps .",
    "next , we evaluate the performance on the reaching task with 4 degrees of freedom .",
    "we use the right shoulder pitch , right elbow pitch , right shoulder roll , and torso yaw joint . by using the torso yaw joint ,",
    "the robot can reach a distant object which can not be achieved by only using the right arm .",
    "the results are shown in figure [ fig : j4-return ] .",
    "the graph shows that iw - pgpe~ob~ achieves fast policy improvement throughout iterations , and the performance is the best among the compared methods .",
    "figure [ fig : j4_trajectory ] depicts a representative example of object reaching with @xmath203 degrees of freedom by iw - pgpe~ob~. note that the object is distant from the robot and it can not be reached by only using the right arm . the robot first adjusts the torso yaw joint , and then uses the right arm to reach the object .",
    "the images show that the policy learned by our proposed method successfully leads the right hand to the distant object",
    ".         runs as functions of the number of iterations for the reaching task with all degrees of freedom . ]      at last , we evaluate the performance on the reaching task with all degrees of freedom .",
    "the position of the target object is the same as the task in the 4-degrees - of - freedom setting .    in this experiment",
    ", we use all degrees of freedom to reach the object .",
    "this increases the dimensionality of the state space , which actually may grow the values of importance weights exponentially @xcite . in order to mitigate the large values of importance weights , we decided not to reuse all previously collected samples , but only samples collected in the last 5 iterations",
    "this allows us to keep the difference between the sampling distribution and the target distribution reasonably small , and thus the values of importance weights can be suppressed to some extent .",
    "furthermore , following @xcite , we truncate the importance weights as @xmath180 .",
    "this version of iw - pgpe~ob~ is denoted as truncated iw - pgpe~ob~ below .",
    "the results are shown in figure [ fig : j9-return ] .",
    "the graph shows that the performance of truncated iw - pgpe~ob~ is the best , which implies that the truncation of importance weights is helpful when applying our proposed method to high - dimensional problems .    through all the arm - reaching experiments",
    ", we can see that the returns tend to be lower as the dimension is increased , even though we run the higher - dimensional experiment for a larger number of iterations . in the task with all degrees of freedom ( figure [ fig : j9-return ] ) , the largest number of iteration is 400 . if we continue the experiment for more iterations , the returns may sligtly increase , but are still less than the returns in the low - dimensional experiments .",
    "this is because the more joints the robot uses , the larger energy will be consumed , and thus the returns tend to be lower in high - dimensional cases .",
    "overall , the proposed iw - pgpe~ob~ is shown to be a promising method , although in the last experiment it is obvious that just like other importance weight - based methods , the performance degrades in high - dimensional problems without the use of additional correction techniques such as weight truncation .",
    "in many real - world reinforcement learning problems , reducing the number of training samples is desirable because the sampling cost is often much higher than the computational cost . in this paper",
    ", we proposed a new policy gradient method equipped with efficient sample reuse , which systematically combines a reliable policy gradient method , pgpe , with importance sampling and the optimal constant baseline .",
    "we showed that the introduction of the optimal constant baseline can mitigate the large - variance problem of importance weighting under some conditions . through experiments with an artificial domain",
    ", the usefulness of the proposed method was demonstrated .",
    "more over , through robotic experiments , we found that the truncation technique was helpful when applying the proposed method to high - dimensional problems .",
    "the low variance of pgpe was brought by considering a deterministic policy and introducing the stochasticity by drawing a policy parameter from a prior distribution .",
    "this per - trajectory formulation was indeed shown to be useful in reducing the variance of policy gradient estimates . however , pgpe has limitations , too .",
    "for example , the use of a finite horizon is essential in pgpe , because the gradient estimates need full trajectories . in particular , it is not straightforward to handle the infinite - horizon case .",
    "another issue is an extension to a partially - observable case .",
    "it is known that for every finite markov decision problem ( mdp ) there exists a deterministic policy that is optimal @xcite . however , in a partially - observable mdp ( pomdp ) , the best stationary stochastic policy can be arbitrarily better than the best stationary deterministic policy @xcite .",
    "thus , the deterministic policy in pgpe can be a limitation when extending it to the pomdp framework .",
    "it is trivial to extend the current formulation to consider stochastic policies .",
    "however , this may lead to an increase of variance and thus slow down convergence .",
    "these issues need to be further investigated in the future work .",
    "the baseline and importance weighting techniques are two independent techniques .",
    "more specifically , importance weighting is used in the off - policy scenario to efficiently reuse previously collected samples , by using importance weighting the consistency between the data sampling distribution and the target distribution is kept . on the other hand ,",
    "the optimal constant baseline is used to reduce the variance of gradient estimates .",
    "the use of a baseline technique has been first proposed in terms of reinforcement comparison in @xcite , which intuitively means the comparison between the expected return @xmath216 and the baseline @xmath92 : if @xmath217 we adjust learned parameters @xmath218 so as to increase the probability of @xmath219 , and , if @xmath220 , we do the opposite . based on this idea , williams @xcite demonstrated that a baseline technique did not introduce bias , which is because the expectation of the coefficient of @xmath92 is zero , i.e. , @xmath221=0 $ ] .",
    "the effect of the baseline on variance is considered in @xcite .",
    "the intuition behind the baseline is that subtracting a baseline from the return reduces the magnitude , and thus reduces the variance .",
    "technically , subtracting a baseline can be viewed as a _",
    "control variate technique _",
    "@xcite , which is an effective approach to reducing variance of monte carlo estimates of integrals .",
    "the experimental results in the paper suggest that the removal of the baseline is possibly the primary factor in improving performance compared with the importance weighting techniques .    in episodic policy gradient methods ,",
    "the optimal baseline which does not bias policy gradient estimates is given by a single scalar for all trajectories @xcite .",
    "however , in the non - episodic policy gradient methods , the optimal baseline can depend on the current state @xcite . thus ,",
    "if a good parameterization for the baseline is known , e.g. , in a generalized linear form @xmath222 , this can significantly improve the gradient estimation process .",
    "however , the selection of the basis function can be difficult and often impractical in robotics @xcite .",
    "on the other hand , it is interesting to see that if the value function is used as the baseline function in non - episodic policy gradient methods , such as in @xcite , the term @xmath223 will lead to the _ advantage function _",
    "@xcite , where @xmath224 is action value function and @xmath225 is the value function .",
    "the authors would like to thank anonymous reviewers for their feedback on our earlier manuscript , which highly contributed to improving the readability of this paper .",
    "tz , vt , jm , and ms were supported by mext kakenhi 23120004 . hh was supported by the first program .",
    "tz was also supported by the mext scholarship , vt was also supported by the jasso scholarship , and jm was also supported by the srbps and mext .",
    "in the appendix , we give proofs of the theorems .",
    "due to the fact that the sampled data @xmath226 are independent and identically distributed , we have    @xmath227 = \\frac { 1 } { n ' } \\operatorname{\\mathbf{var}}\\left[w(\\bm{\\theta } ) \\nabla_{\\bm{\\eta } } \\log p(\\bm{\\theta}|\\bm{\\rho})r(h ) \\right ] , \\ ] ]    where @xmath16 and @xmath13 are random variables and follow the distributions @xmath228 .",
    "note that we consider the trace of the covariance matrix of gradient vectors , that is , the sum of the variance of the components of the vector . then by upper - bounding the variance with the second moment , we have the following upper bound :",
    "@xmath229\\\\ & \\le\\sum_{i=1}^\\ell { \\mathbb{e}}_{p(h,\\bm{\\theta}|\\bm{\\rho'})}\\left[(w(\\bm{\\theta } ) r(h ) \\nabla_{{\\eta}_i } \\log p(\\bm{\\theta}|\\bm{\\rho}))^2\\right]\\\\ & = \\sum_{i=1}^\\ell \\iint p(h|\\bm{\\theta})p(\\bm{\\theta}|\\bm{\\rho ' } ) \\left(\\frac{p(\\bm{\\theta}|\\bm{\\rho})}{p(\\bm{\\theta}|\\bm{\\rho}')}\\right)^2 ( r(h))^2 ( \\nabla_{{\\eta}_i } \\log p(\\bm{\\theta}|\\bm{\\rho } ) ) ^2 \\mathrm{d}h \\mathrm{d}\\bm{\\theta}\\\\ & = \\sum_{i=1}^\\ell \\iint p(h|\\bm{\\theta})p(\\bm{\\theta}|\\bm{\\rho } ) w(\\bm{\\theta})(r(h))^2 ( \\nabla_{{\\eta}_i } \\log p(\\bm{\\theta}|\\bm{\\rho } ) ) ^2 \\mathrm{d}h \\mathrm{d}\\bm{\\theta}\\\\ & \\le \\sum_{i=1}^\\ell\\left(\\frac{\\beta(1-\\gamma^t)}{1-\\gamma}\\right)^2   w_{\\max }   \\iint p(h|\\bm{\\theta})p(\\bm{\\theta}|\\bm{\\rho})(\\nabla_{{\\eta}_i } \\log p(\\bm{\\theta}|\\bm{\\rho } ) ) ^2 \\mathrm{d}h \\mathrm{d}\\bm{\\theta}\\\\ & = \\sum_{i=1}^\\ell\\left(\\frac{\\beta(1-\\gamma^t)}{1-\\gamma}\\right)^2   w_{\\max } { \\mathbb{e}}_{p(\\bm{\\theta}|\\bm{\\rho})}\\left[(\\nabla_{{\\eta}_i } \\log p(\\bm{\\theta}|\\bm{\\rho } ) ) ^2\\right],\\end{aligned}\\ ] ]    where @xmath230 $ ] denotes the expectation of the function of random variable @xmath13 with respect to @xmath231 .",
    "subsequently , given the proof of the first part of theorem 1 in @xcite , we get the upper bound of @xmath232 $ ] .",
    "similarly , given the same technique and the proof of the later part of theorem 1 in @xcite , we could get the conclusion of the upper bound of @xmath233 $ ] .",
    "first , let us derive some elementary expressions . let @xmath234 , @xmath235 be random variables taking values in the @xmath28-dimensional space and let @xmath92 be a scalar . then",
    ", @xmath236=\\operatorname{\\mathbf{var}}[\\bm{a}]+b^2\\operatorname{\\mathbf{var}}[\\bm{c}]-b\\operatorname{\\mathbf{cov}}[\\bm{a},\\bm{c}]-b\\operatorname{\\mathbf{cov}}[\\bm{c},\\bm{a}].\\ ] ] we still consider the trace of the covariance matrix of gradient vectors for multi - dimensional space .",
    "assume that @xmath237=\\bm{0}$ ] .",
    "then , we could have @xmath238=&\\operatorname{\\mathbf{var}}[\\bm{a}]+b^2\\operatorname{\\mathbf{var}}[\\bm{c}]-2b\\operatorname{\\mathbf{cov}}[\\bm{a},\\bm{c } ] \\nonumber\\\\ = & \\operatorname{\\mathbf{var}}[\\bm{a}]+ { \\mathbb{e}}[\\bm{c}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c } ] \\left\\{b^2 - 2b\\frac{{\\mathbb{e}}[\\bm{a}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}]}{{\\mathbb{e}}[\\bm{c}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}]}\\right\\ } \\label{varb}\\\\ = & \\operatorname{\\mathbf{var}}[\\bm{a}]+{\\mathbb{e}}[\\bm{c}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}]\\left\\ { \\left(b-\\frac{{\\mathbb{e}}[\\bm{a}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}]}{{\\mathbb{e}}[\\bm{c}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}]}\\right)^2-\\left(\\frac{{\\mathbb{e}}[\\bm{a}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}]}{{\\mathbb{e}}[\\bm{c}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}]}\\right)^2\\right\\ } \\nonumber.\\end{aligned}\\ ] ] simple calculus shows that the foregoing is minimized when @xmath239}{{\\mathbb{e}}[\\bm{c}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}]}.\\ ] ] the optimal baseline for iw - pgpe follows immediately by plugging in @xmath240 and @xmath241 for @xmath234 and @xmath235 .",
    "note that eq.([varb ] ) uses the conclusion of @xmath242=\\bm{0}$ ] , which can be found in the proof of theorem 4 in @xcite .",
    "as the sampled data are independent and identically distributed , we have @xmath243=\\frac{1}{n'}\\operatorname{\\mathbf{var}}[\\bm{a}-b\\bm{c}].\\ ] ] then , according to eq.([varb ] ) and the definition of @xmath93 , we could have @xmath244-\\operatorname{\\mathbf{var}}[\\nabla_{\\bm{\\rho } } \\widehat{{\\mathcal{j}}}^{b^*}_{\\mathrm{iw}}(\\bm{\\rho})]\\\\ & = \\frac{1}{n ' } \\left ( b^2{\\mathbb{e}}[\\bm{c}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}]-2b{\\mathbb{e}}[\\bm{a}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}]+\\frac{({\\mathbb{e}}[\\bm{a}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}])^2}{{\\mathbb{e}}[\\bm{c}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}]}\\right)\\\\ & = \\frac{1}{n ' } \\left(b - b^*\\right)^2 { \\mathbb{e}}[\\bm{c}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\bm{c}],\\end{aligned}\\ ] ] where the expectation is over random variables @xmath16 and @xmath13 such that @xmath245 .",
    "this completes the proof of theorem [ theorem : optimal - baseline ] .",
    "we define @xmath246 and @xmath247 as @xmath248 we still denote the subscripts @xmath59 as @xmath249 . according to theorem  [ theorem : optimal - baseline ] , by setting @xmath250 ,",
    "it is easy to know that @xmath251-\\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\eta } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}^{b^*}(\\bm{\\rho})\\right ] = \\frac{\\left({\\mathbb{e}}_{\\bm{\\rho}'}[r(h)w^2(\\bm{\\theta } ) \\nabla_{\\bm{\\eta}}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\nabla_{\\bm{\\eta}}]\\right)^2}{n'{\\mathbb{e}}_{\\bm{\\rho}'}[w^2(\\bm{\\theta})\\nabla_{\\bm{\\eta } } ^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\nabla_{\\bm{\\eta}}]}.\\ ] ] we already know that @xmath252 \\le \\frac{\\beta(1-\\gamma^t)}{(1-\\gamma)}{\\mathbb{e}}_{\\bm{\\rho}'}\\left[w^2(\\bm{\\theta } ) \\nabla_{\\bm{\\eta}}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\nabla_{\\bm{\\eta}}\\right].\\ ] ] hence , @xmath253-\\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\eta } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}^{b^*}(\\bm{\\rho})\\right ] \\nonumber\\\\ & \\le\\frac{\\beta^2(1-\\gamma^t)^2}{n'(1-\\gamma)^2}{\\mathbb{e}}_{\\bm{\\rho}'}\\left[w^2(\\bm{\\theta } ) \\nabla_{\\bm{\\eta}}^{{\\hspace{-0.25ex}\\top\\hspace{-0.25ex}}}\\nabla_{\\bm{\\eta}}\\right]\\nonumber\\\\ & \\le \\frac{\\beta^2(1-\\gamma^t)^2}{n'(1-\\gamma)^2}w_{\\max } \\sum_{i=1}^{\\ell } { \\mathbb{e}}_{p(\\bm{\\theta}|\\bm{\\rho})}\\left[(\\nabla_{{\\eta}_i})^2\\right ] \\label{uppervar1 } \\\\ & = \\frac{\\beta^2(1-\\gamma^t)^2b}{n'(1-\\gamma)^2}w_{\\max } \\label{uppervar2 } , \\end{aligned}\\ ] ] where eq.([uppervar1 ] ) is based on the same technique used in section [ proof : variance - bound - pgpe ] , and eq.([uppervar2 ] ) is given by results of the proof of theorem 1 in @xcite .",
    "similarly , we can have the lower bound as @xmath254-\\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\eta } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}^{b^*}(\\bm{\\rho})\\right ] \\geq \\frac{\\alpha^2(1-\\gamma^t)^2b}{n'(1-\\gamma)^2}w_{\\min}.\\ ] ]    by using the same techniques , we get the bounds of the variance reduction of gradient estimation with respect to the deviation parameter @xmath58 , @xmath255-\\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\tau } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}^{b^*}(\\bm{\\rho})\\right ] & \\le \\frac{2\\beta^2(1-\\gamma^t)^2b}{n'(1-\\gamma)^2}w_{\\max},\\\\ \\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\tau}}\\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}(\\bm{\\rho})\\right]-\\operatorname{\\mathbf{var}}\\left[\\nabla_{\\bm{\\tau } } \\widehat{{\\mathcal{j}}}_{\\mathrm{iw}}^{b^*}(\\bm{\\rho})\\right ] & \\ge \\frac{2\\alpha^2(1-\\gamma^t)^2b}{n'(1-\\gamma)^2}w_{\\min},\\end{aligned}\\ ] ] which completes the proof .                                              c.  r. shelton .",
    "policy improvement for pomdps using normalized importance sampling . in _ proceedings of the seventeenth international conference on uncertainty in artificial intelligence _ , pages 496503 , 2001 .",
    "s.  p. singh , t.  jaakkola , and m.  i. jordan .",
    "learning without state - estimation in partially observable markovian decision processes . in _ proceedings of the eleventh international conference on machine learning _ , pages 284292 .",
    "morgan kaufmann , 1994 .",
    "r.  s. sutton , d.  mcallester , s.  singh , and y.  mansour .",
    "policy gradient methods for reinforcement learning with function approximation . in _ advances in neural information processing systems 12",
    "_ , pages 10571063 . mit press , 1999",
    ".    e.  uchibe and k.  doya .",
    "competitive - cooperative - concurrent reinforcement learning with importance sampling . in _ proceedings of international conference on simulation of adaptive behavior : from animals and animats _ , pages 287296 .",
    "mit press , 2004 .",
    "l.  weaver and n.  tao . the optimal reward baseline for gradient - based reinforcement learning . in _ processings of the seventeeth conference on uncertainty in artificial intelligence",
    ", pages 538545 , 2001 ."
  ],
  "abstract_text": [
    "<S> the policy gradient approach is a flexible and powerful reinforcement learning method particularly for problems with continuous actions such as robot control . </S>",
    "<S> a common challenge in this scenario is how to reduce the variance of policy gradient estimates for reliable policy updates . in this paper , we combine the following three ideas and give a highly effective policy gradient method : ( a ) the _ policy gradients with parameter based exploration _ , which is a recently proposed policy search method with low variance of gradient estimates , ( b ) an _ importance sampling technique _ , which allows us to reuse previously gathered data in a consistent way , and ( c ) an _ optimal baseline _ , which minimizes the variance of gradient estimates with their unbiasedness being maintained . </S>",
    "<S> for the proposed method , we give theoretical analysis of the variance of gradient estimates and show its usefulness through extensive experiments . </S>"
  ]
}