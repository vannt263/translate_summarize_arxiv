{
  "article_text": [
    "neural networks have become ubiquitous in applications ranging from computer vision @xcite to speech recognition @xcite and natural language processing @xcite .",
    "we consider convolutional neural networks used for computer vision tasks which have grown over time . in 1998",
    "_ designed a cnn model lenet-5 with less than 1 m parameters to classify handwritten digits @xcite , while in 2012 , krizhevsky _ et al . _",
    "@xcite won the imagenet competition with 60 m parameters .",
    "deepface classified human faces with 120 m parameters @xcite , and coates _ et al . _",
    "@xcite scaled up a network to 10b parameters .    while these large neural networks are very powerful , their size consumes considerable storage , memory bandwidth , and computational resources . for embedded mobile applications ,",
    "these resource demands become prohibitive .",
    "figure [ energy ] shows the energy cost of basic arithmetic and memory operations in a 45 nm cmos process . from this data",
    "we see the energy per connection is dominated by memory access and ranges from 5pj for 32 bit coefficients in on - chip sram to 640pj for 32bit coefficients in off - chip dram @xcite .",
    "large networks do not fit in on - chip storage and hence require the more costly dram accesses . running",
    "a 1 billion connection neural network , for example , at 20hz would require @xmath2 just for dram access - well beyond the power envelope of a typical mobile device .",
    "our goal in pruning networks is to reduce the energy required to run such large networks so they can run in real time on mobile devices .",
    "the model size reduction from pruning also facilitates storage and transmission of mobile applications incorporating dnns .    [ cols=\"<,<,<\",options=\"header \" , ]      + & 42.78% & & & @xmath3 + & 44.40% & & & @xmath4 + & 41.93% & & & @xmath5 + & 42.90% & & & @xmath6 + & 44.40% & & & @xmath7 + & 47.18% & & & @xmath8 + & 44.02% & & & @xmath9 + & * 42.77% & & & @xmath0 + *    [ table : compare ]    [ 0.6 ] smaller scale.,title=\"fig : \" ]    [ 0.6 ] smaller scale.,title=\"fig : \" ]    [ fig : w1 ]    figure [ fig : w1 ] shows histograms of weight distribution before ( left ) and after ( right ) pruning .",
    "the weight is from the first fully connected layer of alexnet .",
    "the two panels have different y - axis scales .",
    "the original distribution of weights is centered on zero with tails dropping off quickly .",
    "almost all parameters are between @xmath10 $ ] . after pruning the large center region",
    "is removed .",
    "the network parameters adjust themselves during the retraining phase .",
    "the result is that the parameters form a bimodal distribution and become more spread across the x - axis , between @xmath11 $ ] .",
    "we have presented a method to improve the energy efficiency and storage of neural networks without affecting accuracy by finding the right connections .",
    "our method , motivated in part by how learning works in the mammalian brain , operates by learning which connections are important , pruning the unimportant connections , and then retraining the remaining sparse network .",
    "we highlight our experiments on alexnet and vggnet on imagenet , showing that both fully connected layer and convolutional layer can be pruned , reducing the number of connections by @xmath12 to @xmath13 without loss of accuracy .",
    "this leads to smaller memory capacity and bandwidth requirements for real - time image processing , making it easier to be deployed on mobile systems ."
  ],
  "abstract_text": [
    "<S> neural networks are both computationally intensive and memory intensive , making them difficult to deploy on embedded systems . also , conventional networks fix the architecture before training starts ; as a result , training can not improve the architecture . to address these limitations , </S>",
    "<S> we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections . </S>",
    "<S> our method prunes redundant connections using a three - step method . </S>",
    "<S> first , we train the network to learn which connections are important . </S>",
    "<S> next , we prune the unimportant connections . finally , we retrain the network to fine tune the weights of the remaining connections . on the imagenet dataset , </S>",
    "<S> our method reduced the number of parameters of alexnet by a factor of @xmath0 , from 61 million to 6.7 million , without incurring accuracy loss . </S>",
    "<S> similar experiments with vgg-16 found that the total number of parameters can be reduced by @xmath1 , from 138 million to 10.3 million , again with no loss of accuracy . </S>"
  ]
}