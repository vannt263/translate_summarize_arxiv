{
  "article_text": [
    "deep neural networks have recently proven successful at various tasks , such as computer vision @xcite , speech recognition @xcite , and in other domains . recurrent neural networks based on long short - term memory ( lstm ) cells",
    "@xcite have been successfully applied to a number of natural language processing tasks .",
    "sequence - to - sequence recurrent neural networks with such cells can learn very complex tasks in an end - to - end manner , such as translation @xcite , parsing @xcite , speech recognition @xcite or image caption generation @xcite . since so many tasks can be solved with essentially one model , a natural question arises : is this model the best we can hope for in supervised learning ?",
    "despite its recent success , the sequence - to - sequence model has limitations . in its basic form ,",
    "the entire input is encoded into a single fixed - size vector , so the model can not generalize to inputs much longer than this fixed capacity .",
    "one way to resolve this problem is by using an attention mechanism @xcite .",
    "this allows the network to inspect arbitrary parts of the input in every decoding step , so the basic limitation is removed .",
    "but other problems remain , and @xcite show a number of basic algorithmic tasks on which sequence - to - sequence lstm networks fail to generalize .",
    "they propose a stack - augmented recurrent network , and it works on some problems , but is limited in other ways .    in the best case one would desire a neural network model able to learn arbitrarily complex algorithms given enough resources .",
    "neural turing machines @xcite have this theoretical property . however , they are not computationally efficient because they use soft attention and because they tend to be of considerable depth .",
    "their depth makes the training objective difficult to optimize and impossible to parallelize because they are learning a sequential program .",
    "their use of soft attention requires accessing the entire memory in order to simulate @xmath1 step of computation , which introduces substantial overhead .",
    "these two factors make learning complex algorithms using neural turing machines difficult . these issues are not limited to neural turing machines , they apply to other architectures too , such as stack - rnns @xcite or ( de)queue - rnns @xcite .",
    "one can try to alleviate these problems using hard attention and reinforcement learning , but such non - differentiable models do not learn well at present @xcite .    in this work we present a neural network model , the _ neural gpu _",
    ", that addresses the above issues .",
    "it is a turing - complete model capable of learning arbitrary algorithms in principle , like a neural turing machine .",
    "but , in contrast to neural turing machines , it is designed to be as parallel and as shallow as possible",
    ". it is more similar to a gpu than to a turing machine since it uses a smaller  number of parallel computational steps .",
    "we show that the neural gpu works in multiple experiments :    * a neural gpu can learn _ long binary multiplication _ from examples .",
    "it is the first neural network able to learn an algorithm whose run - time is superlinear in the size of its input . trained on up - to @xmath0-bit numbers , we see _ no single error _ on any inputs we tested , and we tested on numbers up - to @xmath2 bits long .",
    "* the same architecture can also learn _ long binary addition _ and a number of other algorithmic tasks , such as counting , copying sequences , reversing them , or duplicating them .",
    "the learning of algorithms with neural networks has seen a lot of interest after the success of sequence - to - sequence neural networks on language processing tasks @xcite .",
    "an attempt has even been made to learn to evaluate simple python programs with a pure sequence - to - sequence model @xcite , but more success was seen with more complex models .",
    "neural turing machines @xcite were shown to learn a number of basic sequence transformations and memory access patterns , and their reinforcement learning variant @xcite has reasonable performance on a number of tasks as well .",
    "stack , queue and dequeue networks @xcite were also shown to learn basic sequence transformations such as bigram flipping or sequence reversal .",
    "the grid lstm @xcite is another powerful architecture that can learn to multiply @xmath3-digit decimal numbers .",
    "as we will see in the next section , the grid - lstm is quite similar to the neural gpu ",
    "the main difference is that the neural gpu is less recurrent and is explicitly constructed from the highly parallel convolution operator .    in image processing",
    ", _ convolutional lstms _ , an architecture similar to the neural gpu ,",
    "have recently been used for weather prediction @xcite and image compression @xcite .",
    "we find it encouraging as it hints that the neural gpu might perform well in other contexts .",
    "most comparable to this work are the prior experiments with the stack - augmented rnns @xcite .",
    "these networks manage to learn and generalize to unseen lengths on a number of algorithmic tasks .",
    "but , as we show in section  [ sec : res ] , stack - augmented rnns trained to add numbers up - to @xmath0-bit long generalize only to @xmath4-bit numbers , never to @xmath5-bit ones , and never without error .",
    "still , their generalization is the best we were able to obtain without using the neural gpu and far surpasses a baseline lstm sequence - to - sequence model with attention .    the quest for learning",
    "algorithms has been pursued much more widely with tools other than neural networks .",
    "it is known under names such as program synthesis , program induction , automatic programming , or inductive synthesis , and has a long history with many works that we do not cover here ; see , e.g. ,  @xcite and @xcite for a more general perspective .    since one of our results is the synthesis of an algorithm for long binary addition ,",
    "let us recall how this problem has been addressed without neural networks .",
    "importantly , there are two cases of this problem with different complexity .",
    "the easier case is when the two numbers that are to be added are aligned at input , i.e. , if the first ( lower - endian ) bit of the first number is presented at the same time as the first bit of the second number , then come the second bits , and so on , as depicted below for @xmath6 and @xmath7 written in binary with least - significant bit left .    [ cols=\"^,^,^,^,^,^,^,^,^,^ \" , ]      here we describe the training methods that we used to improve our results .",
    "note that we applied these methods to the lstm+a baseline as well , to keep the above comparison fair .",
    "we focus on the most important elements of our training regime , all less relevant details can be found in the code which is released as open - source .    [ [ grid - search . ] ] grid search .",
    "+ + + + + + + + + + + +    each result we report is obtained by running a grid search over @xmath8 instances .",
    "we consider @xmath9 settings of the learning rate , initial parameters scale , and @xmath10 other hyperparameters discussed below : the relaxation pull factor , curriculum progress threshold , gradient noise scale , and dropout .",
    "an important effect of running this grid search is also that we train @xmath11 models with different random seeds every time .",
    "usually only a few of these models generalize to @xmath2-bit numbers , but a significant fraction works well on @xmath5-bit numbers , as discussed below .    [ [ curriculum - learning . ] ] curriculum learning .",
    "+ + + + + + + + + + + + + + + + + + + +    we use a curriculum learning approach inspired by @xcite .",
    "this means that we train , e.g. , on @xmath12-digit numbers only after crossing a curriculum progress threshold ( e.g. , over @xmath13 fully correct outputs ) on @xmath14-digit numbers .",
    "however , with @xmath15 probability we pick a minibatch of @xmath16-digit numbers with @xmath16 chosen uniformly at random between @xmath1 and @xmath0 .    [ [ gradients - noise . ] ] gradients noise .",
    "+ + + + + + + + + + + + + + + +    to improve training speed and stability we add noise to gradients in each training step .",
    "inspired by the schedule from @xcite , we add to gradients a noise drawn from the normal distribution with mean @xmath17 and variance inversely proportional to the square root of step - number ( i.e. , with standard deviation proportional to the @xmath10-th root of step - number ) .",
    "we multiply this noise by the gradient noise scale and , to avoid noise in converged models , we also multiply it by the fraction of non - fully - correct outputs ( which is @xmath17 for a perfect model ) .    [ [ gate - cutoff . ] ] gate cutoff .",
    "+ + + + + + + + + + + +    in section  [ sec : cgrn ] we defined the gates in a cgru using the sigmoid function , e.g. , we wrote @xmath18 .",
    "usually the standard sigmoid function is used , @xmath19 .",
    "we found that adding a hard threshold on the top and bottom helps slightly in our setting , so we use @xmath20 cut to the interval @xmath21 $ ] , i.e. , @xmath22 .",
    "dropout is a widely applied technique for regularizing neural networks .",
    "but when applying it to recurrent networks , it has been counter - productive to apply it on recurrent connections ",
    "it only worked when applied to the non - recurrent ones , as reported by @xcite .",
    "since a neural gpu does not have non - recurrent connections it might seem that dropout will not be useful for this architecture .",
    "surprisingly , we found the contrary  it is useful and improves generalization .",
    "the key to using dropout effectively in this setting is to set a small dropout rate .",
    "when we run a grid search for dropout rates we vary them between @xmath23 and @xmath24 , meaning that over @xmath25 of the values are always preserved .",
    "it turns out that even this small dropout has large effect since we apply it to the whole mental image @xmath26 in each step @xmath27 .",
    "presumably the network now learns to include some redundancy in its internal representation and generalization benefits from it .    without dropout",
    "we usually see only a few models from a @xmath11 grid search generalize reasonably , while with dropout it is a much larger fraction and they generalize to higher lengths . in particular , dropout was necessary to train models for multiplication that generalize to @xmath2 bits .",
    "to improve optimization of our deep network we use a _ relaxation _",
    "technique for shared parameters which works as follows . instead of training with parameters shared across time - steps",
    "we use @xmath28 identical sets of non - shared parameters ( we often use @xmath29 , larger numbers work better but use more memory ) . at time - step @xmath30 of the neural gpu we use the @xmath27-th set if @xmath31 .",
    "the procedure described above relaxes the network , as it can now perform different operations in different time - steps .",
    "training becomes easier , but we now have @xmath28 parameters instead of the single shared set we want . to unify them",
    "we add a term to the cost function representing the distance of each parameter from the average of this parameter in all the @xmath28 sets .",
    "this term in the final cost function is multiplied by a scalar which we call the _ relaxation pull_. if the relaxation pull is @xmath17 , the network behaves as if the @xmath28 parameter sets were separate , but when it is large , the cost forces the network to unify the parameters across different set .    during training",
    ", we gradually increase the relaxation pull .",
    "we start with a small value and every time the curriculum makes progress , e.g. , when the model performs well on @xmath14-digit numbers , we multiply the relaxation pull by a relaxation pull factor . when the curriculum reaches the maximal length we average the parameters from all sets and continue to train with a single shared parameter  set .",
    "this method is crucial for learning multiplication . without it , a neural gpu with @xmath32 has trouble to even fit the training set , and the few models that manage to do it do not generalize . with relaxation almost all models in our",
    "@xmath11 runs manage to fit the training data .",
    "we prepared a video of the neural gpu trained to solve the tasks mentioned above .. it shows the state in each step with values of @xmath33 drawn in white , @xmath1 in black , and other in gray .",
    "this gives an intuition how the neural gpu solves the discussed problems , e.g. , it is quite clear that for the duplication task the neural gpu learned to move a part of the embedding downwards in each step .",
    "* what did not work well ?",
    "* for one , using decimal inputs degrades performance .",
    "all tasks above can easily be formulated with decimal inputs instead of binary ones .",
    "one could hope that a neural gpu will work well in this case too , maybe with a larger @xmath34 .",
    "we experimented with this formulation and our results were worse than when the representation was binary : we did not manage to learn long decimal multiplication .",
    "increasing @xmath34 to @xmath35 allows to learn all other tasks in the decimal setting .",
    "another problem is that often only a few models in a @xmath11 grid search generalize to very long unseen instances . among those @xmath11 models ,",
    "there usually are many models that generalize to @xmath36 or even @xmath5 bits , but only a few working without error for @xmath2-bit numbers .",
    "using dropout and gradient noise improves the reliability of training and generalization , but maybe another technique could help even more .",
    "how could we make more models achieve good generalization ?",
    "one idea that looks natural is to try to reduce the number of parameters by decreasing @xmath34 .",
    "surprisingly , this does not seem to have any influence .",
    "in addition to the @xmath32 results presented above we ran experiments with @xmath37 and the results were similar .",
    "in fact using @xmath38 we got the most models to generalize . additionally , we observed that ensembling a few models , just by averaging their outputs , helps to generalize : ensembles of @xmath39 models almost always generalize perfectly on binary tasks .    *",
    "why use width ? *",
    "the neural gpu is defined using two - dimensional convolutions and in our experiments one of the dimensions is always set to @xmath10 .",
    "doing so is not necessary since a one - dimensional neural gpu that uses four times larger @xmath34 can represent every function representable by the original one .",
    "in fact we trained a model for long binary multiplication that generalized to @xmath2-bit numbers using a neural gpu with width @xmath1 and @xmath40 .",
    "however , the width of the neural gpu increases the amount of information carried in its hidden state without increasing the number of its parameters . thus it can be thought of as a factorization and might be useful for other tasks .",
    "* speed and data efficiency . *",
    "neural gpus use the standard , heavily optimized convolution operation and are fast .",
    "we experimented with a @xmath41-layer neural gpu for @xmath42 and @xmath40 .",
    "after unfolding in time it has @xmath35 layers of cgrus , each operating on @xmath43 mental images , each @xmath44 .",
    "the joint forward - backward step time for this network was about @xmath45s on an nvidia gtx 970 gpu .",
    "we were also surprised by how data - efficient a neural gpu can be .",
    "the experiments presented above were all performed using @xmath46k random training data examples for each training length . since we train on up - to @xmath0-bit numbers this adds to about @xmath5k training examples .",
    "we tried to train using only @xmath47 examples per length , so about @xmath2 total training instances .",
    "we were surprised to see that it actually worked well for binary addition : there were models that generalized well to @xmath5-bit numbers and to all lengths below despite such small training set .",
    "but we never managed to train a good model for binary multiplication with that little training data .",
    "the results presented in table  [ tab : res ] show clearly that there is a qualitative difference between what can be achieved with a neural gpu and what was possible with previous architectures . in particular , for the first time",
    ", we show a neural network that learns a non - trivial superlinear - time algorithm in a way that generalized to much higher lengths without errors .    this opens the way to use neural networks in domains that were previously only addressed by discrete methods , such as program synthesis . with the surprising data efficiency of neural gpus it could even be possible to replicate previous program synthesis results , e.g. , @xcite , but in a more scalable way .",
    "it is also interesting that a neural gpu can learn symbolic algorithms without using any discrete state at all , and adding dropout and noise only improves its performance .",
    "another promising future work is to apply neural gpus to language processing tasks .",
    "good results have already been obtained on translation with a convolutional architecture over words @xcite and adding gating and recursion , like in a neural gpu , should allow to train much deeper models without overfitting . finally , the parameter sharing relaxation technique can be applied to any deep recurrent network and has the potential to improve rnn training in general .",
    "cho , kyunghyun , van merrienboer , bart , gulcehre , caglar , bougares , fethi , schwenk , holger , and bengio , yoshua .",
    "learning phrase representations using rnn encoder - decoder for statistical machine translation . _",
    "corr _ , abs/1406.1078 , 2014 .",
    "url http://arxiv.org/abs/1406.1078 .",
    "chung , junyoung , glehre , aglar , cho , kyunghyun , and bengio , yoshua .",
    "empirical evaluation of gated recurrent neural networks on sequence modeling . _ corr _ , abs/1412.3555 , 2014 .",
    "url http://arxiv.org/abs/1412.3555 .",
    "dahl , george  e. , yu , dong , deng , li , and acero , alex .",
    "context - dependent pre - trained deep neural networks for large - vocabulary speech recognition . _ ieee transactions on audio , speech & language processing _ , 200 ( 1):0 3042 , 2012 .",
    "pham , vu , bluche , thodore , kermorvant , christopher , and louradour , jrme .",
    "dropout improves recurrent neural networks for handwriting recognition . in _ international conference on frontiers in handwriting recognition ( icfhr ) _ , pp .",
    "ieee , 2014 .",
    "url http://arxiv.org/pdf/1312.4569.pdf .",
    "shi , xingjian , chen , zhourong , wang , hao , yeung , dit - yan , kin wong , wai , and chun woo , wang . convolutional lstm network : a machine learning approach for precipitation nowcasting . in",
    "_ advances in neural information processing systems _ , 2015 .",
    "url http://arxiv.org/abs/1506.04214 .",
    "sutskever , ilya , vinyals , oriol , and le , quoc  vv .",
    "sequence to sequence learning with neural networks . in _ advances in neural information processing systems",
    "_ , pp . 31043112 , 2014 .",
    "url http://arxiv.org/abs/1409.3215 .",
    "toderici , george , omalley , sean  m. , hwang , sung  jin , vincent , damien , minnen , david , baluja , shumeet , covell , michele , and sukthankar , rahul .",
    "variable rate image compression with recurrent neural networks . in _ international conference on learning representations _ , 2016 .",
    "url http://arxiv.org/abs/1511.06085 ."
  ],
  "abstract_text": [
    "<S> learning an algorithm from examples is a fundamental problem that has been widely studied . </S>",
    "<S> it has been addressed using neural networks too , in particular by neural turing machines ( ntms ) . </S>",
    "<S> these are fully differentiable computers that use backpropagation to learn their own programming . </S>",
    "<S> despite their appeal ntms have a weakness that is caused by their sequential nature : they are not parallel and are are hard to train due to their large depth when unfolded .    </S>",
    "<S> we present a neural network architecture to address this problem : the _ neural gpu_. it is based on a type of convolutional gated recurrent unit and , like the ntm , is computationally universal . unlike the ntm , </S>",
    "<S> the neural gpu is highly parallel which makes it easier to train and efficient to run .    </S>",
    "<S> an essential property of algorithms is their ability to handle inputs of arbitrary size . </S>",
    "<S> we show that the neural gpu can be trained on short instances of an algorithmic task and successfully generalize to long instances . </S>",
    "<S> we verified it on a number of tasks including long addition and long multiplication of numbers represented in binary . </S>",
    "<S> we train the neural gpu on numbers with up - to @xmath0 bits and observe no errors whatsoever while testing it , even on much longer numbers .    to achieve these results we introduce a technique for training deep recurrent networks : parameter sharing relaxation . </S>",
    "<S> we also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization . </S>"
  ]
}