{
  "article_text": [
    "_ low - rank approximation of a matrix _ has a variety of applications to the most fundamental matrix computations @xcite and numerous problems of data mining and analysis ,  ranging from term document data to dna snp data \" @xcite .",
    "classical solution algorithms use svd or rank - revealing factorizations , but the alternative solution by means of random sampling is numerically reliable , robust , and computationally and conceptually simple and has become highly and increasingly popular in the last decade ( see @xcite , @xcite , and ( * ? ? ?",
    "* section 10.4.5 ) for surveys and ample bibliography ) .",
    "in particular the paper @xcite proves that random sampling algorithms applied with gaussian multipliers produce low - rank approximation with a probability close to 1 , but empirically the algorithms work as efficiently with various random structured multipliers .",
    "adequate formal support for this empirical evidence has been elusive so far , but based on our new insight we obtain such a support and furthermore derandomize these algorithms and simplify them by applying them with some sparse and structured multipliers .",
    "the known links enable immediate extensions of our results to various important computations in numerical linear algebra and data mining and analysis , but we also extend them to other fundamental computational problems solved by using random multipliers .",
    "we outline our results in this section .",
    "they are in good accordance with our numerical tests of section [ ststs ] .",
    "* typically we use the concepts  large \" ,  small \" ,  near \" ,  close \" ,  approximate \" ,  ill - conditioned \" and  well - conditioned \" quantified in the context , but we specify them quantitatively as needed . * hereafter  @xmath0 \" means  much less than \" ; _  flop \" _ stands for  floating point arithmetic operation \" .",
    "* @xmath1 is the @xmath2 identity matrix .",
    "@xmath3 is a @xmath4 matrix filled with zeros .",
    "@xmath5 is a vector filled with zeros .",
    "* @xmath6 denotes a @xmath7 block matrix with the blocks @xmath8 .",
    "* @xmath9 denotes a @xmath10 block diagonal matrix with diagonal blocks @xmath8 . * @xmath11 , @xmath12 , and @xmath13 denote the _ rank _ , _ numerical rank _ , and the _ spectral norm _ of a matrix @xmath14 , respectively . *",
    "@xmath15 and @xmath16 , and denote its transpose and hermitian transpose , respectively . * an @xmath17 matrix @xmath14 is called _ unitary _ if @xmath18 or if @xmath19 .",
    "if this matrix is known to be real , then it is also and preferably called _ orthogonal_. + ( @xmath20 and @xmath21 if the matrix @xmath22 is unitary . ) * @xmath23 is _ compact svd _ of a matrix @xmath14 of rank @xmath24 with @xmath25 and @xmath26 denoting the unitary matrices of its singular vectors and @xmath27 the diagonal matrix of its singular values in non - increasing order , @xmath28 .",
    "( @xmath29 ) * @xmath30 denotes the _ condition number _ of a matrix @xmath14 .",
    "a matrix is called _ ill - conditioned _ if its condition number is large in context and is called _ well - conditioned _ if this number @xmath31 is reasonably bounded .",
    "+ ( an @xmath17 matrix is ill - conditioned if and only if it has a matrix of a smaller rank nearby or equivalently if and only if its rank exceeds its numerical rank ; an @xmath17 matrix is well - conditioned if and only if it has full numerical rank @xmath32 .",
    "a matrix @xmath14 is unitary if and only if @xmath33 . )",
    "* _  likely \" _ means  with a probability close to 1 \" , the acronym",
    " _ i.i.d .",
    "_ \" stands for  independent identically distributed \" , and we refer to  standard gaussian random \" variables just as  _ gaussian _ \" . *",
    "we call an @xmath17 matrix _ gaussian _ and denote it @xmath34 if all its entries are i.i.d .",
    "gaussian variables .",
    "* @xmath35 , @xmath36 , and @xmath37 denote the classes of @xmath17 gaussian , real , or complex matrices , respectively .",
    "* @xmath38 , @xmath39 , and @xmath40 , for @xmath41 , denote the classes of @xmath17 matrices @xmath42 ( of rank at most @xmath43 ) where both @xmath44 matrix @xmath22 and @xmath45 matrix @xmath46 are gaussian , real , and complex , respectively . * if @xmath47 and @xmath48 , then we call @xmath42 an @xmath17 _ factor - gaussian matrix of expected rank _ @xmath43 .",
    "( in this case the matrices @xmath22 , @xmath46 and @xmath49 have rank @xmath43 with probability 1 by virtue of theorem [ thrnd ] . )      a matrix @xmath49 can be represented ( respectively , approximated ) by a product @xmath50 of two matrices @xmath51 and @xmath52 if and only if @xmath53 ( respectively , @xmath54 ) , and our main goal is the computation of such a representation or approximation .",
    "we begin with the following basic algorithm for the _ fixed rank problem _",
    ", where the integer @xmath55 or @xmath56 is known . otherwise we can compute it by means of binary search based on recursive application of the algorithm or proceed , e.g. , as in our algorithm [ alg11 ] of section [ strrs ] .",
    "[ alg1 ] range finder ( see figure 1 and compare ( * ? ? ?",
    "* algorithms 4.1 and 4.2 ) ) .",
    "input : : :    an @xmath17 matrix @xmath49 , a nonnegative    tolerance @xmath57 , and an integer @xmath43 such that    @xmath58 .",
    "initialization : : :    fix an integer @xmath59 such that    @xmath60 . generate an    @xmath61 matrix @xmath62 .",
    "computations : : :    1 .",
    "compute the @xmath63 matrix @xmath64 .",
    "remove its columns that have small norms .",
    "2 .   orthogonalize its remaining columns ( cf .",
    "* theorem 5.2.3 ) ) ,    compute and output the resulting @xmath65 matrix    @xmath66 where @xmath67 .",
    "3 .   estimate the error norm @xmath68 for    @xmath69 . if @xmath70 , output    success ; otherwise failure .",
    ", title=\"fig : \" ] [ fig01 ]    at stage 3 probabilistic estimate for the norm @xmath71 can be given by the norm @xmath72 for a random @xmath73 matrix @xmath74 and a reasonably small positive integer @xmath75 ( this would extend the frievalds probabilistic test of @xcite ( cf .",
    "@xcite , @xcite ) ) .",
    "* algorithm 4.2 ) chooses random matrix @xmath62 at stage 1 and then proceeds with @xmath76 and @xmath77 .",
    "if stage 3 outputs success , then a rank-@xmath78 approximation to the matrix @xmath49 is given by the matrix @xmath69 , approximation of the matrix @xmath49 .",
    "] but the papers ( * ? ? ?",
    "* section 5 ) and @xcite avoid costly multiplication of @xmath79 by @xmath49 .",
    "their alternative solution relies on the extension of stage 1 to randomized approximation of the leading part of the compact svd of the matrix @xmath49 , associated with its @xmath43 largest singular values .",
    "the complexity of these algorithms is dominated at stage 1 , which uses @xmath80 flops in the case of generic matrices @xmath49 and @xmath62 . with intricate application of",
    "_ sparse embedding multipliers _",
    "@xmath62 , the paper @xcite computes a low - rank approximation ( with failure probability at most 1/5 ) by using about @xmath81 flops for generic input @xmath49 , but alternative computations using @xmath82 flops can be still of interest ( see section [ sextprg ] ) .",
    "we readily verify the following theorem ( see section [ sprth1 ] ) :    [ thall ] given an @xmath17 matrix @xmath49 with @xmath83 and a reasonably small positive tolerance @xmath57 , algorithm [ alg1 ] outputs success if and only if @xmath84 .    [ defbd ] for two integers @xmath59 and @xmath85 , @xmath86 , and any fixed @xmath61 multiplier @xmath62 , partition the set of @xmath17 matrices @xmath49 with @xmath83 into the set @xmath87 of  @xmath62-good \" matrices such that @xmath88 and the set @xmath89 of  @xmath62-bad \" matrices such that @xmath90 .",
    "the following simple observations should be instructive .",
    "[ thbd ] ( cf . remark [ rewcd ] . ) consider a vector @xmath91 of dimension @xmath85 , an @xmath92 unitary matrix @xmath22 , and an @xmath61 unitary matrix @xmath62 , so that @xmath61 matrix @xmath93 is unitary",
    ". then    \\(i ) @xmath94 , that is , the map @xmath95 multiplies the class @xmath96 of @xmath62-good @xmath17 matrices by the unitary matrix @xmath22 ,    \\(ii ) @xmath97 , that is , appending a column to a multiplier @xmath62 can only expand the class @xmath98 , and    \\(iii ) this class fills the whole space @xmath40 or @xmath39 if @xmath99 .",
    "part ( i ) follows because @xmath100 .",
    "part ( ii ) follows because @xmath101 .",
    "part ( iii ) follows because @xmath102 if @xmath62 is an @xmath92 unitary matrix .",
    "based on theorem [ thbd ] we devise the following algorithm where @xmath103 is not known .    [ alg11 ] recursive low - rank representation / approximation of a matrix .",
    "see figure 2 and cf .",
    "* algorithm 4.2 ) .",
    "input : : :    an @xmath17 matrix @xmath49 and a nonnegative    tolerance @xmath57 .",
    "computations : : :    1 .",
    "generate an @xmath92 unitary matrix    @xmath104 .    2 .",
    "fix positive integers @xmath105 such that    @xmath106 ( in particular @xmath107 for    all @xmath108 if @xmath109 ) and represent the matrix    @xmath104 as a block vector    @xmath6 where the block    @xmath110 has size @xmath111 for    @xmath112 .",
    "recursively , for @xmath113 , apply algorithm [ alg1 ]    to the matrix @xmath49 by substituting    @xmath114 for @xmath59 and    @xmath115 for @xmath62 .",
    "stop when the algorithm outputs success .",
    ", title=\"fig : \" ] [ fig05 ]    by virtue of part ( iii ) of theorem [ thbd ] the algorithm stops and outputs success either at the @xmath116th stage ( when @xmath117 ) or earlier , and we are surely interested in yielding success already for @xmath118 and in saving flops for matrix multiplications at stage 3 .    [ resprs01 ] we are likely to save some flops if we compute approximate matrix products by using leverage scores @xcite ( a.k.a . sampling probabilities ( * ? ? ?",
    "* sections 3 and 5 ) ) .",
    "[ rewcd ] clearly , the blocks @xmath119 and @xmath120 of the unitary matrix @xmath104 are unitary as well , but we can readily extend both theorem [ thbd ] and algorithm [ alg11 ] to the case where we apply them to a nonsingular and well - conditioned ( rather than unitary ) @xmath92 matrix @xmath104 . in that case all multipliers @xmath120 and all their blocks @xmath119 are also well - conditioned matrices of full rank , and moreover @xmath121 and @xmath122 for all @xmath123 and @xmath108 ( cf .",
    "* corollary 8.6.3 ) ) .",
    "[ th0 ] let algorithm [ alg1 ] be applied with a gaussian multiplier @xmath124 .",
    "then    \\(i ) @xmath125 with probability 1 if @xmath126 ( cf .",
    "theorem [ thlrnk ] ) and    \\(ii ) it is likely that @xmath127 if @xmath128 , and the probability that @xmath127 approaches 1 fast as @xmath59 increases from @xmath129 ( cf .",
    "theorem [ th1 ] ) .",
    "the theorem implies that algorithm [ alg11 ] is likely to output success at stage @xmath116 for the smallest @xmath116 such that @xmath130 in the case where @xmath62 denotes a gaussian ( rather than unitary ) matrix .",
    "an @xmath61 matrix of _ subsample random fourier or hadamard transform _ is defined by @xmath131 random variables ( see remark [ resrft ] ) , and we can pre - multiply it by a vector by using @xmath132 flops , for @xmath59 of order @xmath133 , ( see ( * ? ? ?",
    "* sections 4.6 and 11 ) , ( * ? ? ?",
    "* section 3.1 ) , and @xcite ) . for comparison ,",
    "an @xmath61 gaussian matrix is defined by its @xmath134 random entries , and we need @xmath135 flops in order to pre - multiply it by a vector .",
    "srft and srht multipliers @xmath62 are _ universal _ , like gaussian ones : algorithm [ alg1 ] applied with such a multiplier is likely to approximate closely a matrix @xmath49 having numerical rank at most @xmath43 , although the estimated failure probability @xmath136 , for @xmath137 with gaussian multipliers increases to order of @xmath138 in the case of srft and srht multipliers ( cf .",
    "* theorems 10.9 and 11.1 ) , ( * ? ? ?",
    "* section 5.3.2 ) , and @xcite ) .",
    "empirically algorithm [ alg1 ] with srft multipliers fails very rarely even for @xmath139 , although for some special input matrices @xmath49 it is likely to fail if @xmath140 ( cf . or ) .",
    "researchers have consistently observed similar empirical behavior of the algorithm applied with srht and various other multipliers ( see @xcite , @xcite , @xcite , @xcite , and the references therein ) , , the results cited for the classes of srft and srht matrices also hold for the products of these classes with any unitary matrix , in particular for the class of @xmath61 submatrices of an @xmath92 circulant matrix , each made up of @xmath59 randomly chosen columns ( see remark [ resrft ] ) . ] but so far no adequate formal support for that empirical observation has appeared in the huge bibliography on this highly popular subject .      in this paper",
    "we are going to    \\(i ) fill the void in the bibliography by supplying a missing _ formal support _ for the cited observation , with far reaching implications ( see parts ( ii ) and ( iv ) below ) ,    \\(ii ) define _ new more efficient policies of generation and application of multipliers _ for low - rank approximation , ( iii ) _ test our policies numerically _ , and    \\(iv ) _ extend our progress _ to other important areas of matrix computations .",
    "our _ dual _ theorem [ th0d ] below reverses the assumptions of our _ primal _ theorem [ th0 ] that a multiplier @xmath62 is gaussian , while a matrix @xmath49 is fixed .",
    "[ th0d ] let @xmath141 and @xmath142 ( in which case @xmath143 and , with a probability close to 1 , @xmath83 ) .",
    "furthermore let @xmath144 and @xmath145",
    ". then + ( i ) algorithm [ alg1 ] outputs a rank-@xmath43 representation of a matrix @xmath49 with probability 1 if @xmath146 and if @xmath147 , and + ( ii ) it outputs a rank-@xmath59 approximation of that matrix with a probability close to 1 if @xmath148 and approaching 1 fast as the integer @xmath59 increases from @xmath129 .",
    "see theorem [ th1d ] .",
    "[ co0d ] under the assumptions of theorem [ th0d ] , algorithm [ alg11 ] is likely to produce a rank-@xmath59 approximation to the matrix @xmath49 at its first stage @xmath123 at which @xmath149 , and the probability that this occurs approaches 1 fast as @xmath150 increases from @xmath129 .",
    "part ( ii ) of theorem [ th0d ] implies that _ algorithm [ alg1 ] succeeds for the average input matrix @xmath49 that has a small numerical rank _",
    "@xmath151 ( and thus in a sense to most of such matrices ) if the multiplier @xmath62 is any unitary matrix ( or even any well - conditioned matrix of full rank ) and if the average matrix is defined under the gaussian probability distribution .",
    "the former provision , that @xmath145 , is natural for otherwise we could have replaced the multiplier @xmath62 by an @xmath152 matrix for some integer @xmath153 .",
    "the latter customary provision is natural in view of the central limit theorem .    for an immediate implication of theorem [ th0d ] , on the average input",
    "@xmath49 having numerical rank at most @xmath43 , algorithm [ alg11 ] applied with any unitary or even any nonsingular and well - conditioned @xmath92 multiplier @xmath62 outputs success at its earliest recursive stage @xmath123 at which the dimension @xmath154 exceeds @xmath155 .",
    "this can be viewed as _",
    "derandomization _ of algorithms [ alg1 ] and [ alg11 ] versus their application with gaussian sampling .",
    "part ( ii ) of our theorem [ th0 ] is implied by ( * ? ? ?",
    "* theorem 10.8 ) , but our specific supporting estimates are more compact , cover the case of any @xmath148 ( whereas @xcite assumes that @xmath156 ) , and we deduce them by using a shorter proof ( see remark [ repfr ] ) . our approach and our results of section [ sglsdl ] are new , and so are our families of sparse and structured multipliers and the policies of their generation , combination , and application in sections [ smngflr ] and [ ssprsml ] as well .    by applying the well - known links , we can extend our results for low - rank approximation to various fundamental problems of matrix computations and data mining and analysis , but our duality techniques can be extended to other important computational problems as well . in section [ sext ] ( conclusions )",
    "we show such an extension to the least squares regression .",
    "another extension in @xcite supports numerically safe performance of gaussian elimination with no pivoting and block gaussian elimination .",
    "the extensions provide new insights and new opportunities and should motivate further effort and further progress .    extensive decade - long work of a number of authors on an alternative approach to low - rank approximation and least squares regression has culminated in the paper @xcite .",
    "its algorithms succeed for these problems with a probability at least 4/5 , whereas we only reach solution for the average input .",
    "our study , however , leads to some benefits , which should compensate for this deficiency .",
    "we show the power of a very large class of multipliers , including various sparse and structured ones .",
    "this can be interesting , e.g. , for some special structured inputs ( see remark [ restr ] ) , but not only for them .",
    "indeed , see our remark [ resprs0 ] and compare the following excerpt from @xcite :  the traditional metric for the efficiency of a numerical algorithm has been the number of arithmetic operations it performs .",
    "technological trends have long been reducing the time to perform an arithmetic operation , so it is no longer the bottleneck in many algorithms ; rather , communication , or moving data , is the bottleneck \" .    \\2 . in order to make the probability of failure less than @xmath157 ,",
    "the complexity bound of @xcite involve overhead of order @xmath158 , which greatly exceeds the overhead in the case of our average case estimates .",
    "@xcite studies the fixed rank problem ; in the case where the input numerical rank is not known , our algorithm [ alg11 ] substantially decreases the computational overhead versus binary search .    \\4 .",
    "unlike @xcite we cover the case where the ratio @xmath159 is not very large , which can still be interesting in some applications .",
    "our analysis is quite simple and conceptually distinct and should be of independent interest because it provides elusive explanation of a well - known empirical phenomenon ( cf .",
    "section [ sprbpr ] ) .",
    "we organize our presentation as follows :    * in section [ smngflr ] we describe our policies for management of the rare failures of algorithm [ alg1 ] and amend algorithm [ alg11 ] . * in section [ ssprsml ]",
    "we present some efficient sparse and structured multipliers for low - rank approximation . * in section [ sprf ]",
    "we prove theorems [ thall ] , [ th0 ] , and [ th0d ] , extending their claims with more detailed estimates . *",
    "section [ ststs ] ( the contribution of the second and the third authors ) covers our numerical tests . * in section [ sext ] we extend our approach to the lsr computations . * the appendix covers some auxiliary results for computations with random matrices .",
    "* our conflicting goals and simple recipes . *    we try to decrease :    \\(i ) the cost of the generation of a multiplier @xmath62 and of the computation of the matrix @xmath64 ,    \\(ii ) the chances for the failure of algorithm [ alg1 ] , and    \\(iii ) the rank of the computed approximation of a matrix @xmath49 .    towards goal ( i )",
    "we propose using sparse and structured @xmath61 multipliers in the next section .",
    "they are pre - multiplied by a vector and by a matrix @xmath49 at a low cost even for @xmath99 .    towards goal ( ii ) we can expect to succeed whenever integer parameter @xmath59 exceeds @xmath129 , but our chances for success grow fast as @xmath59 increases .",
    "such an increase is in conflict with our goal ( iii ) , but we can alleviate the problem by using the following simple technique .    *",
    "randomized compression algorithm * ( see figure 3 ) .    1 .",
    "fix a sufficiently large dimension @xmath59 , which is still much smaller than @xmath32 , generate a sparse and structured @xmath61 multiplier @xmath62 , and compute the @xmath63 product @xmath64 ( by using @xmath80 flops ) .",
    "2 .   fix a smaller integer @xmath160 such that @xmath161 , generate a gaussian @xmath162 multiplier @xmath163 , and compute and output the @xmath164 matrix @xmath165 ( by using @xmath166 flops , dominated at stage 1 if @xmath167 ) .",
    ", title=\"fig : \" ] [ fig04 ]    by virtue of theorem [ th0 ] this algorithm is likely to succeed , but in our extensive tests in section [ ststs ] even the following simple heuristic recipe has always worked .",
    "* heuristic compression algorithm * ( linear combination of failed multipliers ) : if the first @xmath116 recursive steps of algorithm [ alg11 ] have failed for @xmath168 , then apply algorithm [ alg1 ] with a multiplier @xmath169 where @xmath170 for all @xmath108 and for a fixed or random choice of the signs @xmath171 .",
    "( more generally , one can choose complex values @xmath172 on the unit circle , letting @xmath173 for all @xmath108 . )",
    "[ respectsts ] in our study above we rely on the results of theorem [ th0d ] , which cover the average input matrices .",
    "real computations can deal with ",
    "rare special \" input matrices @xmath49 , not covered by theorem [ th0d ] , but in our tests in section [ ststs ] with a variety of inputs , our small collection of sparse and structured multipliers of the next section turned out to be powerful enough for handling various important classes of special matrices as well .",
    "[ rereus ] _ reusing multipliers .",
    "_ recall from ( * ? ? ?",
    "* sections 8 and 9 ) that for all @xmath108 the matrices @xmath174 and @xmath175 , for @xmath176 , @xmath177 , and @xmath178 , are the orthogonal projections of the matrices @xmath179 and @xmath180 , respectively , onto the range of the matrix @xmath49 .",
    "hence @xmath181 , and so at the @xmath116-th stage of algorithm [ alg11 ] , for @xmath168 , we can reuse such projections computed at its stage @xmath182 rather than recompute them .",
    "in our tests we have consistently succeeded by using multipliers from a _ limited family _ of very sparse and highly structured orthogonal matrices of classes 1317 of section [ s17 m ] , but in this section we also cover a greater variety of other sparse and structured matrices , which form an _ extended family _ of multipliers .",
    "we proceed in the following order . given two integers @xmath59 and @xmath85 , @xmath183 ,",
    "we first generate four classes of very sparse primitive @xmath92 unitary matrices , then combine them into some basic families of @xmath92 matrices ( we denote them @xmath104 in this section ) , and finally define multipliers @xmath62 as @xmath61 submatrices made up of @xmath59 columns , which can be fixed ( e.g. , leftmost ) or chosen at random .",
    "the matrix @xmath62 is unitary if so is the matrix @xmath104 , and more generally @xmath184 ( cf .",
    "* theorem 8.6.3 ) ) .      1 .   a fixed or random _ permutation matrix _ @xmath185 .",
    "their block submatrices form the important class of countsketch matrices from the data stream literature ( cf .",
    "* section 2.1 ) , @xcite , @xcite ) .",
    "2 .   a _ diagonal matrix _ @xmath186 , with fixed or random diagonal entries @xmath187 such that @xmath188 for all @xmath123 ( and so all @xmath85 entries @xmath187 lie on the unit circle @xmath189 , being either nonreal or @xmath190 ) .",
    "3 .   an @xmath191-_circular shift matrix _ + @xmath192 and its transpose @xmath193 for a scalar @xmath191 such that either @xmath194 or @xmath195 .",
    "we write @xmath196 , call @xmath197 _ unit down - shift matrix _ , and call the special permutation matrix @xmath198 the _ unit circulant matrix_. 4 .   a @xmath199 _ hadamard primitive matrix _",
    "@xmath200 for a positive integer @xmath201 ( cf .",
    "@xcite , @xcite ) .",
    "the latter primitive @xmath92 matrices are very sparse , have nonzero entries evenly distributed throughout , and can be pre - multiplied by a vector by using from 0 to @xmath202 flops .",
    "all our primitive matrices , except for the matrix @xmath197 , are unitary or real orthogonal .",
    "hence , for the average input matrix @xmath49 , algorithm [ alg1 ] succeeds with any of their @xmath61 submatrix @xmath62 by virtue of theorem [ th0d ] , and similarly with any @xmath61 submatrix of the matrix @xmath197 of full rank @xmath59 .    for specific input matrices the algorithm can fail with some of our @xmath61 primitive multipliers @xmath62 ( e.g. , this is frequently the case where both input matrix @xmath49 and multiplier @xmath62 are sparse ) , but in the next subsections we readily combine primitives 14 into families of @xmath92 sparse and/or structured matrices , and in section [ ststs ] we consistently and successfully test their @xmath61 submatrices @xmath62 as multipliers .      at",
    "first we recall the following recursive definition of dense and orthogonal ( up to scaling by constants ) @xmath92 matrices @xmath203 of _ walsh - hadamard transform _ for @xmath204 ( cf .",
    "* section 3.1 ) and our remark [ recmb ] ) : @xmath205 for @xmath206 , @xmath207 , and the hadamard primitive matrix @xmath208 of type 4 for @xmath209 .    for demonstration",
    ", here are the matrices @xmath210 and @xmath211 shown with their entries , @xmath212 but for larger dimensions @xmath85 , recursive representation ( [ eqrfd ] ) enables much faster pre - multiplication of a matrix @xmath213 by a vector , namely it is sufficient to use @xmath214 additions and subtractions for @xmath204 .",
    "next we sparsify this matrix by defining it by a shorter recursive process , that is , by fixing a _ recursion depth _",
    "@xmath215 , @xmath216 , and applying equation ( [ eqrfd ] ) where @xmath217 , @xmath218 , and @xmath219 for @xmath220 . for two positive integers @xmath215 and @xmath201 ,",
    "we denote the resulting @xmath92 matrix @xmath221 and for @xmath222 call it @xmath215_abridged hadamard ( ah ) matrix_. in particular , @xmath223 @xmath224 for a fixed @xmath215 , the matrix @xmath221 is still orthogonal up to scaling , has @xmath225 nonzero entries in every row and column , and hence is sparse unless @xmath226 is a small integer .    then again , for larger dimensions",
    "@xmath85 , we can pre - multiply such a matrix by a vector much faster if , instead of the representation by its entries , we apply recursive process ( [ eqrfd ] ) , which involves just @xmath227 additions and subtractions and allows highly efficient parallel implementation ( cf . remark [ resprs0 ] ) .",
    "we similarly obtain sparse matrices by shortening a recursive process of the generation of the @xmath92 matrix @xmath228 of _ discrete fourier transform ( dft ) _ at @xmath85 points , for @xmath204 : @xmath229 in particular @xmath230 , @xmath231    the matrix @xmath228 is unitary up to scaling by @xmath232",
    ". we can pre - multiply it by a vector by using @xmath233 flops , and we can efficiently parallelize this computation if , instead of representation by entries , we apply following recursive representation ( cf .",
    "* section 2.3 ) and our remark [ recmb ] ) : @xmath234 here @xmath235 is the matrix of odd / even permutations such that @xmath236 , @xmath237 , @xmath238 , @xmath239 , @xmath240 , @xmath241 ; @xmath206 , @xmath242 , and @xmath243 is the scalar 1 .",
    "we sparsify this matrix by defining it by a shorter recursive process , that is , by fixing a recursion depth @xmath215 , @xmath216 , replacing @xmath244 for @xmath245 by the identity matrix @xmath246 , and then applying equation ( [ eqfd ] ) for @xmath206 , @xmath218 .    for @xmath216 and @xmath220 , we denote the resulting @xmath92 matrix @xmath247 and call it @xmath215-_abridged fourier ( af ) matrix_. it is also unitary ( up to scaling ) , has @xmath225 nonzero entries in every row and column , and thus is sparse unless @xmath226 is a small integer .",
    "we can represent such a matrix by its entries , but then again its pre - multiplication by a vector involves just @xmath248 flops and allows highly efficient parallel implementation if we rely on recursive representation ( [ eqfd ] ) .    by applying fixed or random permutation and scaling to ah matrices @xmath221 and af matrices @xmath247 , we obtain the families of @xmath215_abridged scaled and permuted hadamard ( asph ) _ matrices , @xmath249 , and @xmath215_abridged scaled and permuted fourier ( aspf ) _",
    "@xmath92 matrices , @xmath250 where @xmath185 and @xmath251 are two matrices of permutation and diagonal scaling of primitive classes 1 and 2 , respectively .",
    "likewise we define the families of ash , asf , aph , and apf matrices , @xmath252 , @xmath253 , @xmath254 , and @xmath255 , respectively .",
    "each random permutation or scaling contributes up to @xmath85 random parameters .",
    "[ recmb ] the following equations are equivalent to ( [ eqrfd ] ) and ( [ eqfd ] ) : @xmath256 where @xmath257 denotes a @xmath258 hadamard s primitive matrix of type 4 . by extending the latter recursive representation we can define matrices that involve more random parameters .",
    "namely we can recursively incorporate random permutations and diagonal scaling as follows : @xmath259 here @xmath260 are @xmath258 random permutation matrices of primitive class 1 and @xmath261 are @xmath258 random matrices of diagonal scaling of primitive class 2 , for all @xmath262 .",
    "then again we define @xmath215abridged matrices @xmath263 and @xmath264 by applying only @xmath215 recursive steps ( [ eqhfspd ] ) initiated at the primitive matrix @xmath246 , for @xmath245 .    with these recursive steps we can pre - multiply matrices @xmath263 and @xmath264 by a vector by using at most @xmath265 additions and subtractions and at most @xmath266 flops , respectively , provided that @xmath267 divides @xmath85 .",
    "@xmath191-circulant matrix _",
    "@xmath268 for the matrix @xmath269 of @xmath191-circular shift , is defined by a scalar @xmath270 and by the first column @xmath271 and is called _ circulant _ if @xmath272 and _ skew - circulant _ if @xmath273 .",
    "such a matrix is nonsingular with probability 1 ( see theorem [ thrnd ] ) and is likely to be well - conditioned @xcite if @xmath195 and if the vector @xmath91 is gaussian .    [ restr ] one can compute the product of an @xmath92 circulant matrix with an @xmath92 toeplitz or toeplitz - like matrix by using @xmath274 flops ( see ( * ? ? ?",
    "* theorem 2.6.4 and example 4.4.1 ) ) .",
    "* family ( ii ) * of _ sparse _ @xmath191-_circulant matrices _",
    "@xmath275 is defined by a fixed or random scalar @xmath191 , @xmath195 , and by the first column having exactly @xmath262 nonzero entries , for @xmath276 .",
    "the positions and the values of nonzeros can be randomized ( and then the matrix would depend on up to @xmath277 random values ) .    such a matrix can be pre - multiplied by a vector by using at most @xmath278 flops or , in the real case where @xmath279 and @xmath280 for all @xmath123 , by using at most @xmath281 additions and subtractions .    the same cost estimates apply in the case of the generalization of @xmath282 to a _ uniformly sparse matrix _ with exactly @xmath262 nonzeros entries , @xmath190 , in every row and in every column for @xmath283 .",
    "such a matrix is the sum @xmath284 for fixed or random matrices @xmath285 and @xmath286 of primitive types 1 and 2 , respectively .",
    "first recall the following well - known expression for a @xmath287-circulant matrix : @xmath288 where @xmath289 , @xmath290 , @xmath291 , @xmath292 , and @xmath293 ( cf .",
    "* theorem 2.6.4 ) ) . for @xmath272 ,",
    "the expression is simplified : @xmath294 , @xmath295 , and @xmath296 is a circulant matrix : @xmath297 pre - multiplication of an @xmath191-circulant matrix by a vector is reduced to pre - multiplication of each of the matrices @xmath298 and @xmath299 by a vector and in addition to performing @xmath300 flops ( or @xmath202 flops in case of a circulant matrix ) .",
    "this involves @xmath274 flops overall and then again allows highly efficient parallel implementation .    for a fixed scalar @xmath191 and @xmath289",
    ", we can define the matrix @xmath301 by any of the two vectors @xmath302 or @xmath91 .",
    "the matrix is unitary ( up to scaling ) if @xmath195 and if @xmath303 for all @xmath123 and is defined by @xmath304 real parameters ( or by @xmath85 such parameters for a fixed @xmath191 ) , which we can fix or choose at random .",
    "now suppose that @xmath220 , @xmath216 , @xmath215 and @xmath75 are integers , and substitute a pair of af matrices of recursion length @xmath215 for two factors @xmath228 in the above expressions . then the resulting _ abridged @xmath191-circulant matrix _",
    "@xmath305 _ of recursion depth _",
    "@xmath215 is still unitary ( up to scaling ) , defined by @xmath304 or @xmath85 parameters @xmath306 and @xmath191 , is sparse unless the positive integer @xmath226 is small , and can be pre - multiplied by a vector by using @xmath307 flops . instead of af matrices",
    ", we can substitute a pair of aspf , apf , asf , ah , asph , aph , or asf matrices for the factors @xmath228 .",
    "all such matrices form * family ( iii ) * of @xmath215_abridged @xmath191-circulant matrices_.    [ resrft ] recall that an @xmath61 srft and srht matrices are the products @xmath308 and @xmath309 , respectively , where @xmath203 and @xmath228 are the matrices of ( [ eqrfd ] ) and ( [ eqdft ] ) , @xmath293 , @xmath306 are i.i.d .",
    "variables uniformly distributed on the circle @xmath310 , and @xmath311 is the @xmath61 submatrix formed by @xmath59 columns of the identity matrix @xmath312 chosen uniformly at random . equation ( [ eqcrcl ] ) shows that we can obtain a srft matrix by pre - multiplying a circulant matrix by the matrix @xmath228 and post - multiplying it by the above matrix @xmath311 .",
    "* family ( iv ) * is formed by the _",
    "inverses of @xmath92 bidiagonal matrices _",
    "@xmath313 for a matrix @xmath251 of primitive type 2 and the down - shift matrix @xmath197 .",
    "in particular ,    @xmath314 if @xmath315 in order to pre - multiply a matrix @xmath316 by a vector @xmath91 , however , we do not compute its entries , but solve the linear system of equations @xmath317 by using @xmath318 flops or , in the real case , just @xmath319 additions and subtractions .",
    "we can randomize the matrix @xmath104 by choosing up to @xmath319 random diagonal entries of the matrix @xmath251 ( whose leading entry makes no impact on @xmath104 ) .",
    "finally , @xmath320 because nonzero entries of the lower triangular matrix @xmath316 have absolute values 1 , and clearly @xmath321 .",
    "hence @xmath322 ( the spectral condition number of @xmath104 ) can not exceed @xmath323 for @xmath316 , and the same bound holds for @xmath324 .",
    "table [ tabmlt ] shows upper bounds on    \\(a ) the numbers of random variables involved into the @xmath92 matrices @xmath104 of the four families ( i)(iv ) and    \\(b ) the numbers of flops for pre - multiplication of such a matrix by a vector . + for comparison , using a gaussian @xmath92 multiplier involves @xmath325 random variables and @xmath326 flops .",
    "one can readily extend the estimates to @xmath61 submatrices @xmath62 of the matrices @xmath104 .",
    "( iv ) + random variables & 0 & @xmath202 & 0 & @xmath202 & @xmath327 & @xmath85 & @xmath319 + flops complex & @xmath227 & @xmath328 & @xmath248 & @xmath329 & @xmath278 & @xmath330 & @xmath318     + flops in real case & @xmath227 & @xmath328 & * & * & @xmath281 & * & @xmath319 +    [ resprs0 ] other observations besides flop estimates can be decisive .",
    "e.g. , a special recursive structure of an arsph matrix @xmath331 and an arspf matrix @xmath332 allows highly efficient parallel implementation of their pre - multiplication by a vector based on application specific integrated circuits ( asics ) and field - programmable gate arrays ( fpgas ) , incorporating butterfly circuits @xcite .",
    "there is a number of other interesting basic matrix families . according to (",
    "* remark 4.6 ) ,  among the structured random matrices .... one of the strongest candidates involves sequences of random givens rotations \" .",
    "they are dense unitary matrices @xmath333 for the dft matrix @xmath228 , three random diagonal matrices @xmath334 , @xmath335 and @xmath336 of primitive type 2 , and two chains of givens rotations @xmath337 and @xmath338 , each of the form @xmath339 for a random permutation matrix @xmath185 , @xmath340 here @xmath341 denote @xmath319 random angles of rotation uniformly distributed in the range @xmath342 .    the dft factor @xmath228 makes the resulting matrices dense , but we can sparsify them by replacing that factor by an af , asf , apf , or aspf matrix having recursion depth @xmath343 . this would also decrease the number of flops involved in pre - multiplication of such a multiplier by a vector from order @xmath344 to @xmath345 .",
    "we can turn givens sequences into distinct candidate families of efficient multipliers by replacing either or both of the givens products with sparse matrices of householder reflections matrices of the form @xmath346 for fixed or random sparse vectors @xmath347 ( cf .",
    "* section 5.1 ) ) .    we can obtain a variety of efficient multiplier families by properly combining the matrices of basic families ( i)(iv ) and the above matrices .",
    "we can use just linear combinations , but can also apply block representation as in the following real @xmath348 block matrix @xmath349 for two vectors @xmath302 and @xmath91 and a matrix @xmath251 of primitive class 2 .",
    "we can define new matrix families by intertwining the hadamard and fourier recursive steps .",
    "the reader can find other useful families of multipliers in our section [ ststs ] .",
    "e.g. , according to our tests in section [ ststs ] , it turned out to be efficient to use nonsingular well - conditioned ( rather than unitary ) diagonal factors in the definition of some of our basic matrix families .",
    "hereafter @xmath350 denotes the range ( column span ) of a matrix @xmath14 .",
    "[ thlrnk ] ( i ) for an @xmath17 input matrix @xmath49 of rank @xmath351 , its rank-@xmath43 representation is given by the products @xmath352 provided that @xmath311 is an @xmath353 matrix such that @xmath354 and that @xmath355 is a matrix obtained by means of column orthogonalization of @xmath311 .",
    "\\(ii ) @xmath354 , for @xmath356 and an @xmath353 matrix @xmath62 , with probability @xmath357 if @xmath62 is gaussian , and    \\(iii ) with a probability at least @xmath358 if an @xmath353 matrix @xmath62 has i.i.d .",
    "random entries sampled uniformly from a finite set @xmath359 of cardinality @xmath360 .    readily verify part ( i ) ( cf .",
    "@xcite ) . then note that @xmath361 , for an @xmath353 multiplier @xmath62 .",
    "hence @xmath362 if and only if @xmath363 , and therefore if and only if a multiplier @xmath62 has full rank @xmath43 .    now parts ( ii ) and ( iii )",
    "follow from theorem [ thrnd ] .    parts ( i ) and ( ii ) of theorem [ thlrnk ] imply parts ( i ) of theorems [ th0 ] and [ th0d ] .",
    "hereafter @xmath364(w^hw))^1/2@xmath365 denotes the _ frobenius norm _ of a matrix @xmath14 and @xmath366 denotes the _ moore  penrose pseudo inverse _ of a matrix @xmath14 of rank @xmath24 having compact svd @xmath367 .",
    "( note that @xmath368 . )    in our proofs of theorems [ thall ] , [ th0 ] , and [ th0d ] we rely on the following lemma and theorem .",
    "[ letrnc ] ( cf .",
    "* theorem 2.4.8 ) . ) for an integer @xmath43 and an @xmath17 matrix @xmath49 where @xmath369 , set to 0 the singular values @xmath370 , for @xmath371 , let @xmath372 denote the resulting matrix , which is a closest rank-@xmath43 approximation of @xmath49 , and write @xmath373 then @xmath374    [ thrrap ] the error norm in terms of @xmath375 . assume dealing with the matrices @xmath49 and @xmath376 of algorithm [ alg1 ] , @xmath372 and @xmath377 of lemma [ letrnc ] , and @xmath378 of rank @xmath59 . let @xmath379 and write @xmath380 and @xmath68 . then @xmath381    lemma [ letrnc ] implies bound ( [ eqe ] ) .",
    "next apply part ( i ) of theorem [ thlrnk ] for matrix @xmath372 replacing @xmath49 , recall that @xmath382 , and obtain @xmath383 furthermore @xmath384",
    ". therefore @xmath385    consequently , @xmath386 , and so ( cf . lemma [ letrnc ] ) @xmath387 and @xmath377 replaced by @xmath388 , and obtain @xmath389 combine this bound with ( [ eqrrnm0 ] ) and obtain ( [ eqdltsgm ] ) .    by combining bounds ( [ eqe ] ) and ( [ eqdltsgm ] ) obtain    @xmath390    in our applications the value @xmath391 is small , and so the value @xmath392 is small unless the norm @xmath375 is large .      if @xmath393 , then @xmath394 , @xmath395 . in this case",
    "@xmath71 is not small because @xmath396 , and so algorithm [ alg1 ] applied to @xmath49 with the multiplier @xmath62 outputs failure .",
    "if @xmath397 , then @xmath398 for a small - norm perturbation matrix @xmath377 .",
    "hence @xmath399 , and then again algorithm [ alg1 ] applied to @xmath49 with the multiplier @xmath62 outputs failure .",
    "this proves the  only if \" part of the claim of theorem [ thall ] .",
    "now let @xmath88 and assume that we scale the matrix @xmath62 so that @xmath400",
    ". then @xmath363 ( and so we can apply bound ( [ eqdlt ] ) ) , and furthermore @xmath401 .",
    "equation ( [ eqdlt ] ) implies that @xmath402 .",
    "therefore @xmath71 is a small positive value because @xmath83 .",
    "thus the value @xmath403 is small , and part  if \" of theorem [ thall ] follows .      the following theorem , proven in the next subsection , bounds the approximation errors and the probability of success of algorithm [ alg1 ] for @xmath124 .",
    "together these bounds imply part ( ii ) of theorem [ th0 ] .",
    "[ th1 ] suppose that algorithm [ alg1 ] has been applied to an @xmath17 matrix @xmath49 having numerical rank @xmath43 and that the multiplier @xmath62 is an @xmath61 gaussian matrix .",
    "\\(i ) then the algorithm outputs an approximation @xmath376 of a matrix @xmath49 by a rank-@xmath59 matrix within the error norm bound @xmath71 such that @xmath404 where @xmath405 and @xmath406 and @xmath407 are random variables of definition [ defnrm ] .",
    "\\(ii ) @xmath408 , for @xmath409 and @xmath410 .",
    "[ reopt ] @xmath411 is the optimal upper bound on the norm @xmath71 , and the expected value @xmath412 is reasonably small even for @xmath413 . if @xmath414 , then @xmath412 is not defined , but the random variable @xmath71 estimated in theorem [ th1 ] is still likely to be reasonably close to @xmath411 ( cf . part ( ii ) of theorem [ thsiguna ] ) .    in section [ sdlrnd ]",
    "we prove the following elaboration upon dual theorem [ th0d ] .",
    "[ th1d ] suppose that algorithm [ alg1 ] , applied to a small - norm perturbation of an @xmath17 factor - gaussian matrix with expected rank @xmath415 , uses an @xmath61 multiplier @xmath62 such that @xmath145 and @xmath148 .",
    "\\(i ) then the algorithm outputs a rank-@xmath59 matrix @xmath376 that approximates the matrix @xmath49 within the error norm bound @xmath71 such that @xmath416 , where @xmath417 , @xmath418 , and @xmath419 and @xmath407 are random variables of definition [ defnrm ] .",
    "\\(ii ) @xmath420 , for @xmath409 and @xmath410 .",
    "[ redl ] the expected value @xmath421 converges to 0 as @xmath422 provided that @xmath423 .",
    "consequently the expected value @xmath424 converges to the optimal value @xmath411 as @xmath425 provided that @xmath62 is a well - conditioned matrix of full rank and that @xmath426 .",
    "[ repfr ] ( * ? ? ?",
    "* theorem 10.8 ) also estimates the norm @xmath71 , but our estimate in theorem [ th1 ] , in terms of random variables @xmath406 and @xmath407 , is more compact , and our proof is distinct and shorter than one in @xcite , which involves the proofs of ( * ? ? ?",
    "* theorems 9.1 , 10.4 and 10.6 ) .",
    "[ reprob ] by virtue of theorems [ thrnd ] , @xmath379 with probability 1 if the matrix @xmath62 or @xmath49 is gaussian , which is the case of theorems [ th1 ] and [ th1d ] , and under the equation @xmath379 we have proven bound ( [ eqdlt ] ) .    [ rernlrpr ] _ the power scheme of increasing the output accuracy of algorithm [ alg1 ] .",
    "_ see @xcite , @xcite .",
    "define the power iterations @xmath427 , @xmath113 .",
    "then @xmath428 for all @xmath123 and @xmath108 ( * ? ? ?",
    "* equation ( 4.5 ) ) .",
    "therefore , at a reasonable computational cost , one can dramatically decrease the ratio @xmath429 and thus decrease the bounds of theorems [ th1 ] and [ th1d ] accordingly .    in the next two subsections",
    "we deduce reasonable bounds on the norm @xmath375 in both cases where @xmath49 is a fixed matrix and @xmath62 is a gaussian matrix and where @xmath62 is fixed matrix and @xmath49 is a factor gaussian matrix ( cf .",
    "theorems [ thprm ] and [ thdual ] ) .",
    "the bounds imply theorems [ th1 ] and [ th1d ] .",
    "[ thprm ] for @xmath430 , @xmath431 , and @xmath432 of definition [ defnrm ] , it holds that @xmath433 be compact svd .    by applying lemma [ lepr3 ] ,",
    "deduce that @xmath434 is a @xmath435 gaussian matrix",
    ".    denote it @xmath436 and obtain @xmath437 .",
    "write @xmath438 and let @xmath439 be compact svd where @xmath440 is a @xmath441 unitary matrix .",
    "it follows that @xmath442 is an @xmath44 unitary matrix .",
    "hence @xmath443 and @xmath444 are compact svds of the matrices @xmath445 and @xmath446 , respectively .    therefore @xmath447 and @xmath448 and obtain the theorem .",
    "combine bounds ( [ eqdlt ] ) , ( [ eqn+ ] ) , and equation @xmath449 and obtain part ( i ) of theorem [ th1 ] . combine that part with parts ( ii ) of theorem [ thsignorm ] and ( iii ) of theorem [ thsiguna ] and obtain part ( ii ) of theorem [ th1 ] .",
    "[ thdual ] suppose that @xmath450 , @xmath48 , @xmath451 , @xmath42 , and @xmath62 is a well - conditioned @xmath61 matrix of full rank @xmath59 such that @xmath452 and @xmath400 .",
    "then @xmath453 if in addition @xmath47 , that is , if @xmath49 is an @xmath17 factor - gaussian matrix with expected rank @xmath43 , then @xmath454 and @xmath455 and obtain @xmath456 . here",
    "@xmath22 , @xmath46 , @xmath62 , @xmath457 , @xmath458 , @xmath459 , @xmath460 , @xmath461 , and @xmath462 are matrices of the sizes @xmath44 , @xmath45 , @xmath61 , @xmath44 , @xmath441 , @xmath441 , @xmath61 , @xmath463 , and @xmath463 , respectively .",
    "now observe that @xmath464 is a @xmath435 gaussian matrix , by virtue of lemma [ lepr3 ] ( since @xmath46 is a gaussian matrix ) .",
    "therefore @xmath465 , for @xmath466 .",
    "let @xmath467 denote compact svd where @xmath468 and @xmath469 and @xmath470 are unitary matrices of sizes @xmath441 and @xmath435 , respectively .",
    "both products @xmath471 and @xmath472 are unitary matrices , and we obtain compact svd @xmath473 where @xmath474 , @xmath475 , and @xmath476 . therefore @xmath477 note that @xmath478 because @xmath461 and @xmath479 are square nonsingular diagonal matrices .",
    "consequently @xmath480 and ( [ eqdlt1 ] ) follows .",
    "we also need the following result implied by ( * ? ? ?",
    "* corollary 1.4.19 ) for @xmath481 :    [ thpert ] suppose @xmath482 and @xmath483 are two nonsingular matrices of the same size and @xmath484 then @xmath485 e.g. , @xmath486 if @xmath487",
    ".    combine ( [ eqdlt ] ) , ( [ eqdlt1 ] ) and @xmath488 and obtain theorem [ th1d ] provided that @xmath49 is a factor - gaussian matrix @xmath50 with expected rank @xmath43 .",
    "apply theorem [ thpert ] to extend the results to the case where @xmath489 and the norm @xmath490 is small , completing the proof of theorem [ th1d ] .",
    "[ regnrl ] if @xmath47 , for @xmath491 , then it is likely that @xmath492 by virtue of theorem [ thsiguna ] , and our proof of bound ( [ eqdlt1 ] ) applies even if we assume that @xmath492 rather than @xmath47 .",
    "numerical experiments have been performed by xiaodong yan for tables [ tab67][tab614 ] and by john svadlenka and liang zhao for the other tables .",
    "the tests have been run by using matlab in the graduate center of the city university of new york on a dell computer with the intel core 2 2.50 ghz processor and 4 g memory running windows 7 ; in particular the standard normal distribution function randn of matlab has been applied in order to generate gaussian matrices .",
    "we calculated the @xmath493-rank , i.e. , the number of singular values exceeding @xmath493 , by applying the matlab function `` svd ( ) '' .",
    "we have set @xmath494 in sections [ ststssvd ] and [ ststslo ] and @xmath495 in section [ s17 m ] .      in the tests of this subsection we generated @xmath92 input matrices @xmath49 by extending the customary recipes of [ h02 , section 28.3 ] .",
    "namely , we first generated matrices @xmath496 and @xmath497 by means of the orthogonalization of @xmath92 gaussian matrices .",
    "then we defined @xmath92 matrices @xmath49 by their compact svds , @xmath498 , for @xmath499 ; @xmath500 , @xmath501 , and @xmath502 .",
    "( hence @xmath503 and @xmath504 . )",
    "table [ tab1 ] shows the average output error norms @xmath71 over 1000 tests of algorithm [ alg1 ] applied to these matrices @xmath49 for each pair of @xmath85 and @xmath43 , @xmath502 , @xmath505 , and each of the following three groups of multipliers : 3-ah multipliers , 3-asph multipliers , both defined by hadamard recursion ( [ eqfd ] ) , for @xmath506 , and dense multipliers @xmath507 having i.i.d .",
    "entries @xmath190 and 0 , each value chosen with probability 1/3 .     @xmath508 + 256 & 8 & 2.25e-08 & 2.70e-08 & 2.52e-08 + 256 & 32 & 5.95e-08 & 1.47e-07 & 3.19e-08 + 512 & 8 & 4.80e-08 & 2.22e-07 & 4.76e-08 + 512 & 32 & 6.22e-08 & 8.91e-08 & 6.39e-08 + 1024 & 8 & 5.65e-08 & 2.86e-08 & 1.25e-08 + 1024 & 32 & 1.94e-07 & 5.33e-08 & 4.72e-08 +    tables [ tab67][tab614 ] show the mean and maximal values of such an error norm in the case of ( a ) real gaussian multipliers @xmath62 and dense real gaussian subcirculant multipliers @xmath62 , for @xmath509 , each defined by its first column filled with either ( b ) i.i.d .",
    "gaussian variables or ( c ) random variables @xmath190 . here and hereafter in this section",
    "we assigned each random signs @xmath510 or @xmath511 with probability 0.5 .",
    ".error norms for svd - generated inputs and gaussian multipliers [ cols=\"^,^,^,^,^ \" , ]     the tests show quite accurate outputs even where we applied algorithm [ alg1 ] with very sparse multipliers of classes 1317 .",
    "our duality techniques for the average inputs can be extended to the acceleration of various matrix computations involving random multipliers . in this concluding section",
    "we describe such a sample extension to the following fundamental problem of matrix computations ( cf .",
    "@xcite ) .    [ pr1 ] _ least squares solution of an overdetermined linear system of equations .",
    "_ given two integers @xmath512 and @xmath215 such that @xmath513 , a matrix @xmath514 , and a vector @xmath515 , compute a vector @xmath516 that minimizes the norm @xmath517",
    ".    if a matrix @xmath518 has full rank @xmath85 , then unique solution is given by the vector @xmath519 , satisfying the linear system of normal equations @xmath520 .",
    "otherwise solution is not unique , and a solution @xmath516 having the minimum norm is given by the vector @xmath521 . in the important case where @xmath522 and an approximate solution is acceptable , sarls in @xcite proposed to simplify the computations as follows :    [ algapprls ] least squares regression ( lsr ) .",
    "initialization : : :    fix an integer @xmath75 such that @xmath523 .",
    "computations : : :    1 .",
    "generate a scaled @xmath524 gaussian matrix    @xmath525 .    2 .",
    "compute the matrix @xmath526 and the vector    @xmath527 .",
    "output a solution @xmath528 to the compressed    problem [ pr1 ] where the matrix @xmath518 and the vector    @xmath529 are replaced by the matrix @xmath526 and    the vector @xmath527 , respectively .",
    "now write @xmath530 and @xmath531 and compare the error norms @xmath532 ( of the output @xmath533 of the latter algorithm ) and @xmath534 ( of the solution @xmath516 of the original problem [ pr1 ] ) .",
    "[ thlsrd ] _",
    "* theorem 2.3 ) .",
    "_ suppose that we are given two tolerance values @xmath157 and @xmath493 , @xmath535 and @xmath536 , three integers @xmath75 , @xmath512 and @xmath215 such that @xmath513 and @xmath537 for a certain constant @xmath538 , and a matrix @xmath539 .",
    "then , with a probability at least @xmath540 , it holds that @xmath541 for all matrices @xmath542 and all vectors @xmath543 normalized so that @xmath544 .",
    "the theorem implies that with a probability at least @xmath540 , algorithm [ algapprls ] outputs an approximate solution to problem [ pr1 ] within the error norm bound @xmath493 provided that @xmath545 and @xmath546 .",
    "of least squares computation ( * ? ? ?",
    "* section 4.5 ) , @xcite , @xcite . ]    for @xmath547 , the computational cost of performing the algorithm for approximate solution dramatically decreases versus the cost of computing exact solution , but can still be prohibitively high at the stage of computing the matrix product @xmath548 for @xmath549 . in a number of papers",
    "the former cost estimate has been dramatically decreased by means of replacing a multiplier @xmath546 with various random sparse and structured matrices ( see ( * ? ? ?",
    "* section 2.1 ) ) , for which the bound of theorem [ thlsrd ] still holds for all matrices @xmath542 , although at the expense of increasing significantly the dimension @xmath75 .",
    "can we achieve similar progress without such an increase ?",
    "@xcite provides positive probabilistic answer based on the johnson ",
    "lindenstrauss theorem , while the following theorem does this by using our duality approach in the case where @xmath49 is the average matrix in @xmath550 under the gaussian probability distribution :    [ thlsrdd ] _ dual lsr . _",
    "the bound of theorem [ thlsrd ] holds with a probability at least @xmath540 where @xmath551 and @xmath552 is an orthogonal matrix .    theorem [ thlsrd ] has been proven in ( * ? ? ?",
    "* section 2 ) in the case where @xmath553 .",
    "this is the case where @xmath554 and @xmath49 is an orthogonal @xmath555 matrix , but is also the case under the assumptions of theorem [ thlsrdd ] , by virtue of lemma [ lepr3 ] .",
    "theorem [ thlsrdd ] supports the computation of an approximate randomized solution of lsr problem [ pr1 ] for any orthogonal multiplier @xmath525 ( e.g. , an abridged scaled hadamard s multiplier or a count sketch multiplier ) and for an input matrix @xmath542 average under the gaussian probability distribution .",
    "it follows that in this case algorithm [ algapprls ] can fail only for a narrow class of pairs @xmath525 and @xmath49 where @xmath525 denotes orthogonal matrices in @xmath556 and @xmath49 denotes matrices in @xmath550 , and even in the unlikely case of failure we can still have good chances for success if we apply heuristic recipes of our section [ smngflr ] .    * *",
    "[ thrnd ] suppose that @xmath518 is an @xmath17 matrix of full rank @xmath557 , @xmath525 and @xmath74 are @xmath558 and @xmath353 matrices , respectively , for @xmath559 , and the entries of these two matrices are nonconstant linear combinations of finitely many i.i.d",
    ". random variables @xmath560 .",
    "the determinant , @xmath564 , of any @xmath441 block @xmath62 of a matrix @xmath525 , @xmath526 , @xmath74 , or @xmath561 is a polynomial of degree @xmath43 in the variables @xmath560 , and so the equation @xmath565 defines an algebraic variety of a lower dimension in the linear space of these variables ( cf .",
    "* proposition 1 ) ) . clearly , such a variety has lebesgue and gaussian measures 0 , both being absolutely continuous with respect to one another .",
    "this implies part ( i ) of the theorem .",
    "derivation of part ( ii ) from a celebrated lemma of @xcite , also known from @xcite and @xcite , is a well - known pattern , specified in some detail in @xcite .",
    "[ lepr3 ] ( rotational invariance of a gaussian matrix . )",
    "suppose that @xmath75 , @xmath512 , and @xmath85 are three positive integers , @xmath163 is an @xmath17 gaussian matrix , and @xmath566 and @xmath567 are @xmath524 and @xmath73 orthogonal matrices , respectively .",
    "[ defnrm ] norms of random matrices and expected value of a random variable .",
    "write @xmath570 , @xmath571 , and @xmath572 , for a gaussian @xmath17 matrix @xmath163 , and write @xmath573 for the expected value of a random variable @xmath574 .",
    "( @xmath575 , @xmath576 , and @xmath577 , for all pairs of @xmath512 and @xmath85 . )                    the probabilistic upper bounds of theorem [ thsiguna ] on @xmath590 are reasonable already in the case of square matrices , that is , where @xmath591 , but are strengthened very fast as the difference @xmath592 grows from 1 .",
    "theorems [ thsignorm ] and [ thsiguna ] combined imply that an @xmath17 gaussian matrix is well - conditioned unless the integer @xmath593 is large or the integer @xmath592 is close to 0 . with some grain of salt",
    "we can still consider such a matrix well - conditioned even where the integer @xmath592 is small or vanishes provided that the integer @xmath512 is not large .",
    "clearly , these properties can be extended immediately to all submatrices .",
    "m. baboulin , d. becker , g. bosilca , a. danalis , j. dongarra , an efficient distributed randomized algorithm for solving large dense symmetric indefinite linear systems , _ parallel computing ( 7th workshop on parallel matrix algorithms and applications ) , _ * 40 ,  7 * , 213223 , 2014 .",
    "b. a. cipra , the best of the 20th century : editors name top 10 algorithms , _ siam news ( society for industrial and applied mathematics ) _ , * 33 , 4 * , 2 , may 16 , 2000",
    ". m. charikar , k. chen , m. farach - colton , finding frequent items in data streams , _ theoretical computer science _ , * 312 ,  1 * , 315 , 2004 .",
    "k. l. clarkson , d. p. woodruff , low rank approximation and regression in input sparsity time , _ proceedings of the forty - fifth annual acm symposium on theory of computing ( stoc 13 ) _ , 8190 , acm new york , ny , usa , 2013 .",
    "full version in arxiv:1207.6365 .",
    "k. r. davidson , s. j. szarek , local operator theory , random matrices , and banach spaces , in _",
    "handbook on the geometry of banach spaces _",
    "( w. b. johnson and j. lindenstrauss editors ) , pages 317368 , north holland , amsterdam , 2001 .",
    "n. j. higham , _ accuracy and stability in numerical analysis _ , siam , philadelphia , 2002 .",
    "n. halko , p. g. martinsson , y. shkolnisky , m. tygert , an algorithm for the principal component analysis of large data sets , _",
    "siam j. scientific computation _ , * 33 ,  5 * , 25802594 , 2011 .      m. w. mahoney , randomized algorithms for matrices and data , _ foundations and trends in machine learning _ , now publishers , * 3 ,  2 * , 2011 .",
    "preprint : arxiv:1104.5557 ( 2011 ) ( abridged version in : _ advances in machine learning and data mining for astronomy _ , edited by m. j. way et al . , pp . 647672 , 2012 .",
    ")            v. y. pan , g. qian , x. yan , random multipliers numerically stabilize gaussian and block gaussian elimination : proofs and an extension to low - rank approximation , _ linear algebra and its applications _ , * 481 * , 202234 , 2015 .        v. y. pan , l. zhao , low - rank approximation of a matrix : novel insights , new progress , and extensions , proc . of the _ eleventh international computer science symposium in russia ( csr2016 )",
    "_ , ( alexander kulikov and gerhard woeginger , editors ) , st .",
    "petersburg , russia , june 2016 , _ lecture notes in computer science ( lncs ) _ , * 9691 * , 352366 , springer international publishing , switzerland ( 2016 ) .",
    "j. a. tropp , improved analysis of the subsampled randomized hadamard transform , _ adv . adapt .",
    "data anal .",
    "_ , * 3 ,  12 * ( special issue `` sparse representation of data and images '' ) , 115126 , 2011 . also arxiv math.na 1011.1595 ."
  ],
  "abstract_text": [
    "<S> * low - rank approximation of a matrix by means of structured random sampling has been consistently efficient in its extensive empirical studies around the globe , but adequate formal support for this empirical phenomenon has been missing so far . * based on our novel insight into the subject , we provide such an elusive formal support and derandomize and simplify the known numerical algorithms for low - rank approximation and related computations . * </S>",
    "<S> our techniques can be applied to some other areas of fundamental matrix computations , in particular to the least squares regression , gaussian elimination with no pivoting and block gaussian elimination . </S>",
    "<S> * our formal results and our numerical tests are in good accordance with each other .    </S>",
    "<S> [ [ key - words ] ] * key words : * + + + + + + + + + + + +    low - rank approximation , random sampling , derandomization    [ [ math .- subject - classification ] ] * 2000 math . </S>",
    "<S> subject classification : * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    15a52 , 68w20 , 65f30 , 65f20 </S>"
  ]
}