{
  "article_text": [
    "the collection and analysis of data is widespread nowadays across many industries .",
    "as the size of modern data sets exceeds the disk and memory capacities of a single computer , it is imperative to store them and analyze them distributively .",
    "designing efficient and scalable distributed optimization algorithms is a challenging , yet increasingly important task .",
    "there exists a large body of literature studying algorithms where either the features or the observations associated with a machine learning task are stored in distributed fashion .",
    "nevertheless , little attention has been given to settings where the data is doubly distributed , i.e. , when both features and observations are distributed across the nodes of a computer cluster .",
    "this scenario arises in practice as a result of a data collection process . in this work , we propose two algorithms that are amenable to the doubly distributed setting , namely d3ca ( doubly distributed dual coordinate ascent ) and radisa ( random distributed stochastic algorithm ) .",
    "these methods can solve a broad class of problems that can be posed as minimization of the sum of convex functions plus a convex regularization term ( e.g. least squares , logistic regression , support vector machines ) .",
    "d3ca builds on previous distributed dual coordinate ascent methods @xcite , allowing features to be distributed in addition to observations .",
    "the main idea behind distributed dual methods is to approximately solve many smaller sub - problems ( also referred to herein as partitions ) instead of solving a large one . upon the completion of the local optimization procedure",
    ", the primal and dual variables are aggregated , and the process is repeated until convergence . since each sub - problem contains only a subset of the original features , the same dual variables are present in multiple partitions of the data .",
    "this creates the need to aggregate the dual variables corresponding to the same observations . to ensure dual feasibility , we average them and retrieve the primal variables by leveraging the primal - dual relationship , which we discuss in section  [ algorithms ] .",
    "in contrast with d3ca , radisa is a primal method and is related to a recent line of work @xcite on combining coordinate descent ( cd ) methods with stochastic gradient descent ( sgd ) .",
    "its name has the following interpretation : the randomness is due to the fact that at every iteration , each sub - problem is assigned a random sub - block of local features ; the stochastic component owes its name to the parameter update scheme , which follows closely that of the sgd algorithm .",
    "the work most pertinent to radisa is rapsa @xcite .",
    "the main distinction between the two methods is that rapsa follows a distributed gradient ( mini - batch sgd ) framework , in that in each global iteration there is a single ( full or partial ) parameter update .",
    "such methods suffer from high communication cost in distributed environments .",
    "radisa , which follows a local update scheme similar to d3ca , is a communication - efficient generalization of rapsa , coupled with the stochastic variance reduction gradient ( svrg ) technique @xcite .",
    "the contributions of our work are summarized as follows :    * we address the problem of training a model when the data is distributed across observations and features .",
    "we propose two doubly distributed optimization methods .",
    "* we perform a computational study to empirically evaluate the two methods .",
    "both methods outperform on all instances the block splitting variant of admm @xcite , which , to the best of our knowledge , is the only other existing doubly distributed optimization algorithm .",
    "the remainder of the paper is organized as follows : section  [ related_work ] discusses related works in distributed optimization ; section  [ algorithms ] provides an overview of the problem under consideration , and presents the proposed algorithms ; in section  [ numexperiments ] we present the results for our numerical experiments , where we compare d3ca and two versions of radisa against admm .",
    "[ [ stochastic - gradient - descent - methods ] ] stochastic gradient descent methods + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    sgd is one of the most widely - used optimization methods in machine learning .",
    "its low per - iteration cost and small memory footprint make it a natural candidate for training models with a large number of observations . due to its popularity , it has been extensively studied in parallel and distributed settings .",
    "one standard approach to parallelizing it is the so - called mini - batch sgd framework , where worker nodes compute stochastic gradients on local examples in parallel , and a master node performs the parameter updates .",
    "different variants of this approach have been proposed , both in the synchronous setting @xcite , and the asynchronous setting with delayed updates @xcite .",
    "another notable work on asynchronous sgd is hogwild !",
    "@xcite , where multiple processors carry out sgd independently and one can overwrite the progress of the other .",
    "a caveat of hogwild !",
    "is that it places strong sparsity assumptions on the data .",
    "an alternative strategy that is more communication efficient compared to the mini - batch framework is the parallelized sgd ( p - sgd ) method @xcite , which follows the research direction set by @xcite .",
    "the main idea is to allow each processor to independently perform sgd on the subset of the data that corresponds to it , and then to average all solutions to obtain the final result .",
    "note that in all aforementioned methods , the observations are stored distributively , but not the features .",
    "[ [ coordinate - descent - methods ] ] coordinate descent methods + + + + + + + + + + + + + + + + + + + + + + + + + +    coordinate descent methods have proven very useful in various machine learning tasks . in its simplest",
    "form , cd selects a single coordinate of the variable vector , and minimizes along that direction while keeping the remaining coordinates fixed @xcite .",
    "more recent cd versions operate on randomly selected blocks , and update multiple coordinates at the same time @xcite .",
    "primal cd methods have been studied in the parallel @xcite and distributed settings @xcite . distributed cd as it appears in @xcite can be conducted with the coordinates ( features ) being partitioned , but requires access to all observations .",
    "recently , dual coordinate ascent methods have received ample attention from the research community , as they have been shown to outperform sgd in a number of settings @xcite . in the dual problem , each dual variable is associated with an observation , so in the distributed setting one would partition the data across observations .",
    "examples of such algorithms include @xcite .",
    "cocoa @xcite , which serves as the starting point for d3ca , follows the observation partitioning scheme and treats each block of data as an independent sub - problem .",
    "due to the separability of the problem over the dual variables , the local objectives that are maximized are identical to the global one .",
    "each sub - problem is approximately solved using a dual optimization method ; the stochastic dual coordinate ascent ( sdca ) method @xcite is a popular algorithm for this task . following the optimization step ,",
    "the locally updated primal and dual variables are averaged , and the process is repeated until convergence . similar to sgd - based algorithms , dual methods have not yet been explored when the feature space is distributed .    [",
    "[ sgd - cd - hybrid - methods ] ] sgd - cd hybrid methods + + + + + + + + + + + + + + + + + + + + +    there has recently been a surge of methods combining sgd and cd @xcite .",
    "these methods conduct parameter updates based on stochastic partial gradients , which are computed by randomly sampling observations and blocks of variables .",
    "with the exception of rapsa @xcite , which is a parallel algorithm , all other methods are serial , and typically assume that the sampling process has access to all observations and features .",
    "although this is a valid assumption in a parallel ( shared - memory ) setting , it does not hold in distributed environments .",
    "rapsa employs an update scheme similar to that of mini - batch sgd , but does not require all variables to be updated at the same time .",
    "more specifically , in every iteration each processor randomly picks a subset of observations and a block of variables , and computes a partial stochastic gradient based on them .",
    "subsequently , it performs a single stochastic gradient update on the selected variables , and then re - samples feature blocks and observations .",
    "despite the fact that rapsa is not a doubly distributed optimization method , its parameter update is quite different from that of radisa . on one hand",
    ", rapsa allows only one parameter update per iteration , whereas radisa permits multiple updates per iteration , thus leading to a great reduction in communication .",
    "finally , radisa utilizes the svrg technique , which is known to accelerate the rate of convergence of an algorithm .",
    "[ [ admm - based - methods ] ] admm - based methods + + + + + + + + + + + + + + + + + +    a popular alternative for distributed optimization is the alternating direction method of multipliers ( admm ) @xcite .",
    "the original admm algorithm is very flexible in that it can be used to solve a wide variety of problems , and is easily parallelizable .",
    "a block splitting variant of admm was recently proposed that allows both features and observations to be stored in distributed fashion @xcite .",
    "one caveat of admm - based methods is their slow convergence rate . in our numerical experiments",
    "we show empirically the benefits of using radisa or d3ca over block splitting admm .",
    "in this section we present the d3ca and radisa algorithms .",
    "we first briefly discuss the problem of interest , and then introduce the notation used in the remainder of the paper .          in a typical supervised learning task ,",
    "there is a collection of input - output pairs @xmath0 , where each @xmath1 represents an observation consisting of @xmath2 features , and is associated with a corresponding label @xmath3 .",
    "this collection is usually referred to as the training set .",
    "the general objective under consideration can be expressed as a minimization problem of a finite sum of convex functions , plus a smooth , convex regularization term ( where @xmath4 is the regularization parameter , and @xmath5 is parametrized by @xmath3 ) : @xmath6    an alternative approach for finding a solution to is to solve its corresponding dual problem .",
    "the dual problem of has the following form :    @xmath7    where @xmath8 is the convex conjugate of @xmath5 .",
    "note that for certain non - smooth primal objectives used in models such as support vector machines and least absolute deviation , the convex conjugate imposes lower and upper bound constraints on the dual variables .",
    "one interesting aspect of the dual objective is that there is one dual variable associated with each observation in the training set . given a dual solution @xmath9 , it is possible to retrieve the corresponding primal vector by using @xmath10    for any primal - dual pair of solutions @xmath11 and @xmath12 , the duality gap is defined as @xmath13 , and it is known that @xmath14 .",
    "duality theory guarantees that at an optimal solution @xmath15 of , and @xmath16 of , @xmath17 .",
    "+ _ notation : _ we assume that the data @xmath0 is distributed across observations and features over @xmath18 computing nodes of a cluster .",
    "more specifically , we split the features into @xmath19 partitions , and the observations into @xmath20 partitions ( for simplicity we assume that @xmath21 ) .",
    "we denote the labels of a partition by @xmath22}$ ] , and the observations of the training set for its subset of features by @xmath23}$ ] .",
    "for instance , if we let @xmath24 and @xmath25 , the resulting partitions are @xmath26},y_{[1]})$ ] , @xmath27},y_{[1]})$ ] , @xmath28},y_{[2]})$ ] and @xmath29},y_{[2]})$ ] .",
    "furthermore , @xmath30}$ ] represents all observations and features ( across all @xmath31 ) associated with partition @xmath32 ( @xmath33}$ ] is defined similarly ) ",
    "figure  [ fig : setupnotation ] illustrates this partitioning scheme .",
    "we let @xmath34 denote the number of observations in each partition , such that @xmath35 , and we let @xmath36 correspond to the number of features in a partition , such that @xmath37 . note that partitions corresponding to the same observations all share the common dual variable @xmath38}$ ] . in a similar manner ,",
    "partitions containing the same features share the common primal variable @xmath39}$ ] .",
    "in other words , for some pre - specified values @xmath40 and @xmath41 , the partial solutions @xmath42}$ ] and @xmath43}$ ] represent aggregations of the local solutions @xmath44}$ ] for @xmath45 and @xmath46}$ ] for @xmath47 . at any iteration of d3ca",
    ", the global dual variable vector can be written as @xmath48},\\alpha_{[2,.]}, ... ,\\alpha_{[p,.]}]$ ] , whereas for radisa the global primal vector has the form @xmath49},w_{[.,2]}, ... ,w_{[.,q]}]$ ] , i.e. the global solutions are formed by concatenating the partial solutions .",
    "the d3ca framework presented in algorithm  [ alg : d3ca ] hinges on cocoa @xcite , but it extends it to cater for the features being distributed as well .",
    "the main idea behind d3ca is to approximately solve the local sub - problems using a dual optimization method , and then aggregate the dual variables via averaging .",
    "the choice of averaging is reasonable from a dual feasibility standpoint when dealing with non - smooth primal losses  the localdualmethod guarantees that the dual variables are within the lower and upper bounds imposed by the convex conjugate , so their average will also be feasible .",
    "although in cocoa it is possible to recover the primal variables directly from the local solver , in d3ca , due to the averaging of the dual variables , we need to use the primal - dual relationship to obtain them . note that in the case where @xmath50 , d3ca reduces to cocoa .",
    "* data : * @xmath51},y_{[p]})$ ] for @xmath47 and @xmath45    * initialize : @xmath52 * , * @xmath53 *    * in parallel * @xmath54}^{(t)}=$]localdualmethod@xmath55},w^{(t-1)}_{[.,q]})$][lst : line : localdualstep ] * in parallel * @xmath38}^{(t)}=\\alpha_{[p,.]}^{(t-1)}+{\\frac{1}{p\\cdot q } } { \\sum_{q=1}^{q}}\\delta\\alpha_{[p , q]}^{(t)}$ ] [ lst : line : dualave ] * in parallel * @xmath39}^{(t)}=\\frac{1}{\\lambda n}{\\sum_{p=1}^{p}}((\\alpha_{[p , q]}^{(t)})^{t}x_{[p , q]})$][lst : line : getprimal ]    d3ca requires the input data to be doubly partitioned across @xmath18 nodes of a cluster . in step  [ lst : line : localdualstep ] , the algorithm calls the local dual solver , which is shown in algorithm  [ alg : sdca ] .",
    "the localdualmethod of choice is sdca @xcite , with the only difference that the objective that is maximized in step  [ lst : line : maxstep ] is divided by @xmath19 .",
    "the reason for this is that each partition now contains @xmath56 variables , so the factor @xmath57 ensures that the sum of the local objectives adds up to .",
    "step  [ lst : line : dualave ] of algorithm  [ alg : d3ca ] shows the dual variable update , which is equivalent to averaging the dual iterates coming from sdca .",
    "finally , step  [ lst : line : getprimal ] retrieves the primal variables in parallel using the primal - dual relationship .",
    "the new primal and dual solutions are used to warm - start the next iteration .",
    "the performance of the algorithm turns out to be very sensitive to the regularization parameter @xmath58 . for small values of @xmath58 relative to the problem size",
    ", d3ca is not always able to reach the optimal solution .",
    "one modification we made to alleviate this issue was to add a step - size parameter when calculating the @xmath59 s in the local dual method ( algorithm  [ alg : sdca ] , step  [ lst : line : maxstep ] ) . in the case of linear support vector machines ( svm ) where the closed form solution for step  [ lst : line : maxstep ] is given by @xmath60 ,",
    "we replace @xmath61 with a step - size parameter @xmath62 @xcite . in our experiments",
    "we use @xmath63 , where @xmath64 is the global iteration counter .",
    "although , a step - size of this form does not resolve the problem entirely , the performance of the method does improve .",
    "* input * : @xmath65}\\in\\mathbb{r}^{n_{p}}$ ] , @xmath66}\\in\\mathbb{r}^{m_{q}}$ ]    * data : * local @xmath51},y_{[p]})$ ]    * initialize : @xmath67}$]*,*@xmath68}$ ] * , @xmath54}\\leftarrow 0 $ ]    choose @xmath69 at random    find @xmath59 maximizing @xmath70 [ lst : line : maxstep ]    @xmath71})_{i}||^{2})$ ]    @xmath72    @xmath73})_{i}=(\\delta\\alpha_{[p , q]})_{i}+\\delta\\alpha$ ]    @xmath74@xmath51})_{i}$ ]    * output : @xmath54}$ ] *    in terms of parallelism , the @xmath75 sub - problems can be solved independently .",
    "these independent processes can either be carried out on separate computing nodes , or in distinct cores in the case of multi - core computing nodes .",
    "the only steps that require communication are step  [ lst : line : dualave ] and step  [ lst : line : getprimal ] .",
    "the communication steps can be implemented via _",
    "reduce _ operations  in spark we use _ treeaggregate _ , which is superior to the standard _",
    "reduce _ operation .",
    "similar to d3ca , radisa , outlined in algorithm  [ alg : radisa ] , assumes that the data is doubly distributed across @xmath18 partitions . before reaching step  [ lst : line : outter_for_loop ] of the algorithm , all partitions associated with the same block of variables ( i.e. @xmath76 $ ] for @xmath45 ) are further divided into @xmath20 non - overlapping sub - blocks .",
    "the reason for doing this is to ensure that at no time more than one processor is updating the same variables .",
    "although the blocks remain fixed throughout the runtime of the algorithm , the random exchange of sub - blocks between iterations is allowed ( step  [ lst : line : sub - block_assignment ] ) .",
    "the process of randomly exchanging sub - blocks can be seen graphically in figure  [ fig : radisa ] .",
    "for example , the two left - most partitions that have been assigned the coordinate block @xmath77}$ ] , exchange sub - blocks @xmath78}$ ] and @xmath79}$ ] from one iteration to the next . the notation @xmath80 in step  [ lst :",
    "line : sub - block_assignment ] of the algorithm essentially implies that sub - blocks are partition - specific , and , therefore , depend on @xmath20 and @xmath19 .",
    "a possible variation of algorithm  [ alg : radisa ] is one that allows for complete overlap between the sub - blocks of variables . in this setting , however , concatenating all local variables into a single global solution ( step  [ lst : line : sol_concat ] ) is no longer an option .",
    "other techniques , such as parameter averaging , need to be employed in order to aggregate the local solutions . in our numerical experiments , we explore a parameter averaging version of radisa ( radisa - avg ) .",
    "the optimization procedure of radisa makes use of the stochastic variance reduce gradient ( svrg ) method @xcite , which helps accelerate the convergence of the algorithm .",
    "svrg requires a full - gradient computation ( step  [ lst : line : full_gradient ] ) , typically after a full pass over the data .",
    "note that for models that can be expressed as the sum functions , like in , it is possible to compute the gradient when the data is doubly distributed .",
    "although radisa by default computes a full - gradient for each global iteration , delaying the gradient updates can be a viable alternative .",
    "step  [ lst : line : svrg_update ] shows the standard svrg step , which is applied to the sub - block of coordinates assigned to that partition .",
    "the total number of inner iterations is determined by the batch size @xmath81 , which is a hyper - parameter .",
    "as is always the case with variants of the sgd algorithm , the learning rate @xmath82 ( also known as step - size ) typically requires some tuning from the user in order to achieve the best possible results . in section  [ numexperiments ]",
    "we discuss our choice of step - size .",
    "the final stage of the algorithm simply concatenates all the local solutions to obtain the next global iterate .",
    "the new global iterate is used to warm - start the subsequent iteration .",
    "similar to d3ca , the @xmath75 sub - problems can be solved independently . as far as communication is concerned , only the gradient computation ( step  [ lst : line : full_gradient ] ) and parameter update ( step  [ lst : line : svrg_update ] ) stages require coordination among the different processes . in spark",
    ", the communication operations are implemented via @xmath83 .",
    "* input : * batch size @xmath81 , learning rate @xmath82    * data : * @xmath51},y_{[p]})$ ] for @xmath47 and @xmath45    * initialize : @xmath84 *    partition each @xmath76 $ ] into @xmath20 blocks , such that @xmath39}=[w_{[.,q_{1}]},w_{[.,q_{2}]}, ... ,w_{[.,q_{p}]}]$ ]    [ lst : line : outter_for_loop ] @xmath85 @xmath86 [ lst : line : full_gradient ] * in parallel * [ lst : line : opt_for ] randomly pick sub - block @xmath87 in non-[lst : line : sub - block_assignment ]    overlapping manner @xmath88}$ ] randomly pick @xmath89 @xmath90})$][lst : line : svrg_update ]    @xmath91})$ ] [ lst : line : end_opt_for ] @xmath92},w_{[.,2]}, ... ,w_{[.,q]}]$ ] [ lst : line : sol_concat ] , where @xmath93}=[{w}^{(l)}_{[.,\\bar{q}^{q}_{1}]}, ... ,{w}^{(l)}_{[.,\\bar{q}^{q}_{p}]}]$ ]",
    "in this section we present two sets of experiments .",
    "the first set is adopted from @xcite , and we compare the block distributed version of admm with radisa and d3ca . in the second set of experiments we explore the scalability properties of the proposed methods .",
    "we implemented all algorithms in spark and conducted the experiments in a hadoop cluster with 4 nodes , each containing 8 intel xeon e5 - 2407 2.2ghz cores .",
    "for the admm method , we follow the approach outlined in @xcite , whereby the cholesky factorization of the data matrix is computed once , and is cached for re - use in subsequent iterations .",
    "since the computational time of the cholesky decomposition depends substantially on the underlying blas library , in all subsequent figures reporting the execution time of admm , we have excluded the factorization time .",
    "this makes the reported times for admm lower than in reality .",
    "the problem solved in @xcite was lasso regression , which is not a model of the form .",
    "instead , we trained one of the most popular classification models : binary classification hinge loss support vector machines ( svm ) .",
    "the data for the first set of experiments was generated according to a standard procedure outlined in @xcite : the @xmath94 s and @xmath11 were sampled from the @xmath95 $ ] uniform distribution ; @xmath96 , and the sign of each @xmath3 was randomly flipped with probability 0.1 .",
    "the features were standardized to have unit variance .",
    "we take the size of each partition to be dense @xmath97 , , but due to the blas issue mentioned earlier , we resorted to smaller problems to obtain comparable run - times across all methods . ] and set @xmath20 and @xmath19 accordingly to produce problems at different scales .",
    "for example , for @xmath98 and @xmath24 , the size of the entire instance is @xmath99 .",
    "the information about the three data sets is summarized in table  [ tab : data1 ] .",
    "as far as hyper - parameter tuning is concerned , for admm we set @xmath100 .",
    "for radisa we set the step - size to have the form @xmath101 , and select the constant @xmath102 that gives the best performance .",
    "to measure the training performance of the methods under consideration , we use the relative optimality difference metric , defined as @xmath103 where @xmath104 is the primal objective function value at iteration @xmath64 , and @xmath105 corresponds to the optimal objective function value obtained by running an algorithm for a very long time .    .datasets for numerical experiments ( part 1 ) [ cols=\"^,>,>,>\",options=\"header \" , ]     as we can see in figure  [ fig : experiments2 ] , radisa exhibits strong scaling properties in a consistent manner . in both data",
    "sets the run - time decreases significantly when introducing additional computing resources .",
    "it is interesting that early configurations with @xmath106 perform significantly worse compared to the alternate configurations where @xmath107 .",
    "let us consider the configurations ( 4,1 ) and ( 1,4 ) .",
    "in each case , the number of variable sub - blocks is equal to @xmath108 .",
    "this implies that the dimensionality of the sub - problems is identical for both partition arrangements .",
    "however , the second partition configuration has to process four times more observations compared to the first one , resulting in an increased run - time .",
    "it is noteworthy that the difference in performance tails away as the number of partitions becomes large enough .",
    "overall , to achieve consistently good results , it is preferable that @xmath107 .",
    "the strong scaling performance of d3ca is mixed . for the smaller data set ( realsim ) , introducing additional computing resources deteriorates the run - time performance . on the larger data set ( news20 ) , increasing the number of partitions @xmath18 pays dividends when @xmath107 . on the other hand , when @xmath109 , providing additional resources has little to no effect .",
    "the pattern observed in figure  [ fig : experiments2 ] is representative of the behavior of d3ca on small versus large data sets ( we conducted additional experiments to further attest this ) .",
    "it is safe to conclude that when using d3ca , it is desirable that @xmath109 .        in the weak scaling experiments the workload assigned to each processor stays constant as additional resources are used to solve a larger problem . given that problems can increase either in terms of observations or features , we set up our experiments as follows .",
    "we generate artificial data sets in the same manner as outlined earlier , but we take the size of each partition to be @xmath110 .",
    "we vary the number of observation partitions @xmath20 from @xmath111 to @xmath112 , and study the performance of our algorithms for @xmath113 and @xmath114 .",
    "we also consider two distinct sparsity levels : @xmath115 : @xmath116 and @xmath117 . in terms of measuring performance",
    ", we consider the weak scaling efficiency metric as follows .",
    "let @xmath118 denote the time to complete a run when @xmath119 for fixed @xmath19 and @xmath115 , and let @xmath120 represent the time to solve a problem with given @xmath20 ( for the same values of @xmath19 and @xmath115 ) .",
    "the weak scaling efficiency is given as : @xmath121 note that the termination criterion for a run is reaching a @xmath122 relative optimality difference .",
    "furthermore , we use regularization values of @xmath123 and @xmath124 for radisa and d3ca , respectively . in figure",
    "[ fig : experiments3 ] , we can see that neither of the two methods is able to achieve a linear decrease in scaling efficiency as @xmath20 becomes larger . for @xmath24 , radisa manages to scale well at first , but when @xmath125 , its performance deteriorates .",
    "we should note that the scaling efficiency seems to flatten out for large values of @xmath19 and @xmath20 , which is a positive characteristic . as far as d3ca is concerned , it is interesting that the scaling efficiency is very close for different values of @xmath19 . finally , sparsity has a negative impact on the scaling efficiency of both methods .",
    "in this work we presented two doubly distributed algorithms for large - scale machine learning .",
    "such methods can be particularly flexible , as they do not require each node of a cluster to have access to neither all features nor all observations of the training set .",
    "it is noteworthy that when massive datasets are already stored in a doubly distributed manner , our algorithms are the only option for the model training procedure .",
    "our numerical experiments show that both methods outperform the block distributed version of admm .",
    "there is , nevertheless , room to improve both methods .",
    "the most important task would be to derive a step - size parameter for d3ca that will guarantee the convergence of the algorithm for all regularization parameters .",
    "furthermore , removing the bottleneck of the primal vector computation would result into a significant speedup .",
    "as far as radisa is concerned , one potential extension would be to incorporate a streaming version of svrg @xcite , or a variant that does not require computation of the full gradient at early stages @xcite .",
    "finally , studying the theoretical properties of both methods is certainly a topic of interest for future research ."
  ],
  "abstract_text": [
    "<S> as the size of modern data sets exceeds the disk and memory capacities of a single computer , machine learning practitioners have resorted to parallel and distributed computing . given that optimization is one of the pillars of machine learning and predictive modeling , distributed optimization methods have recently garnered ample attention in the literature . </S>",
    "<S> although previous research has mostly focused on settings where either the observations , or features of the problem at hand are stored in distributed fashion , the situation where both are partitioned across the nodes of a computer cluster ( doubly distributed ) has barely been studied . in this work </S>",
    "<S> we propose two doubly distributed optimization algorithms . </S>",
    "<S> the first one falls under the umbrella of distributed dual coordinate ascent methods , while the second one belongs to the class of stochastic gradient / coordinate descent hybrid methods . </S>",
    "<S> we conduct numerical experiments in spark using real - world and simulated data sets and study the scaling properties of our methods . </S>",
    "<S> our empirical evaluation of the proposed algorithms demonstrates the out - performance of a block distributed admm method , which , to the best of our knowledge is the only other existing doubly distributed optimization algorithm .    </S>",
    "<S> machine learning , distributed optimization , big data , spark . </S>"
  ]
}