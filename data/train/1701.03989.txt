{
  "article_text": [
    "in this paper , we consider the use of krylov subspace methods for solving large , sparse linear systems @xmath5 , where @xmath6 .",
    "we will focus on the conjugate gradient ( cg ) method  @xcite , which is used when @xmath7 is symmetric positive definite .",
    "given an initial approximate solution @xmath8 and corresponding residual @xmath9 , the cg method iteratively updates the approximate solution using a sequence of nested krylov subspaces @xmath10 , where @xmath11 denotes the @xmath12-dimension krylov subspace with matrix @xmath7 and starting vector @xmath13 . in iteration @xmath12 , the updated approximate solution @xmath14 is chosen by imposing the galerkin condition @xmath15 .",
    "thus each iteration of cg requires a matrix - vector product with @xmath7 in order to expand the dimension of the krylov subspace and a number of inner products to perform the orthogonalization . on modern computer architectures ,",
    "the speed with which the matrix - vector products and inner products can be computed is limited by communication ( i.e. , the movement of data ) .",
    "this limits the potential speed of individual iterations attainable by an implementation of cg . to perform a sparse matrix - vector product in parallel ,",
    "each processor must communicate entries of the source vector and/or the destination vector that it owns to neighboring processors .",
    "inner products require a global synchronization , i.e. , the computation can not proceed until all processors have finished their local computation and communicated the result to other processors . for large - scale sparse problems on large - scale machines ,",
    "the cost of synchronization between parallel processors can dominate the run - time ( see , e.g. , the exascale computing report  @xcite ) .",
    "research efforts toward removing the performance bottlenecks caused by communication in cg and other krylov subspace methods have produced various approaches .",
    "one such approach are the @xmath0-step krylov subspace methods ( also called `` communication - avoiding '' krylov subspace methods ) ; for a thorough treatment of background , related work , and performance experiments , see , e.g. , the theses  @xcite . in @xmath0-step krylov",
    "subspace methods , instead of performing one iteration at a time , the iterations are performed in blocks of @xmath0 ; i.e. , in each iteration , the krylov subspace is expanded by @xmath1 dimensions by computing @xmath1 new basis vectors and then all inner products between the new basis vectors needed for the next @xmath0 iterations are computed in one block operation . in this way , computing the inner products for @xmath0 iterations only requires a single global synchronization , decreasing the synchronization cost per iteration by a factor of @xmath1 .",
    "this approach has been shown to lead to significant speedups for a number of problems and real - world applications ( see , e.g. ,  @xcite ) . in the remainder of the paper",
    ", we will refer to the matrices whose columns consist of the @xmath1 basis vectors computed in each block as _",
    "@xmath0-step basis matrices_. further details of the @xmath0-step cg method are discussed in section  [ sec : sksms ] .",
    "we emphasize that our use of the overloaded term `` @xmath0-step methods '' here differs from other works , e.g. ,  @xcite and  @xcite , in which ` @xmath0-step method ' refers to a type of restarted lanczos procedure . in exact arithmetic",
    "the @xmath0-step cg method produces the exact same iterates as the classical cg method , but their behavior can differ significantly in finite precision . in both @xmath0-step and classical krylov subspace methods ,",
    "rounding errors due to finite precision arithmetic have two basic effects : a decrease in attainable accuracy and a delay of convergence .",
    "it has long been known that for @xmath0-step krylov subspace methods , as @xmath0 is increased ( and so the condition numbers of the @xmath0-step basis matrices increase ) , the attainable accuracy decreases and the convergence delay increases relative to the classical cg method ( see , e.g. ,  @xcite ) . at the extreme ,",
    "if the parameter @xmath0 is chosen to be too large , the @xmath1-dimensional bases computed for each block can be numerically rank deficient and the @xmath0-step method can fail to converge .",
    "this sensitive numerical behavior poses a practical obstacle to optimizing the performance of @xmath0-step methods , and diminishes their usability and reliability . in a setting where the performance of cg is communication - bound",
    ", we expect that up to some point , increasing @xmath0 will decrease the time per iteration .",
    "if we pick @xmath0 only based on minimizing the time per iteration , however , we can run into problems .",
    "first , the finite precision error may cause a large convergence delay , negating any potential performance gain with respect to the overall runtime . since the number of iterations required for convergence for a given @xmath0 value is not known a priori",
    ", choosing the @xmath0 value that results in the fastest time - to - solution is a difficult problem .",
    "second , the chosen @xmath0 parameter may cause @xmath0-step cg to fail to converge to the user - specified accuracy . in this case , the particular problem is _ unsolvable _ by the @xmath0-step cg method .    requiring the user to choose the parameter @xmath0 thus diminishes the practical usability of @xmath0-step krylov subspace methods",
    "it is therefore imperative that we develop a better understanding of the convergence rate and accuracy in finite precision @xmath0-step cg and other @xmath0-step krylov subspace methods .",
    "our hope is that by studying the theoretical properties of methods designed for large - scale computations in finite precision , we can develop methods and techniques that are efficient , capable of meeting application - dependent accuracy constraints , and which do not require that the user have extensive knowledge of numerical linear algebra .    toward this goal ,",
    "in this paper we develop a variable @xmath0-step cg method in which the @xmath0 values can vary between blocks and are chosen automatically by the algorithm such that a user - specified accuracy is attained .",
    "we call this the _ adaptive @xmath0-step cg method_. the condition for determining @xmath0 in each block is derived using a bound on the size of the gap between the true residuals @xmath16 and the updated residuals @xmath17 computed in finite precision .",
    "we show that in order to prevent a loss of attainable accuracy , in a given block of iterations , the condition number of the @xmath0-step basis matrix should be no larger than a quantity related to the inverse of the 2-norms of the residuals computed within that block .",
    "in other words , as the method converges , more ill - conditioned bases ( i.e. , larger @xmath0 values ) can be used without a loss of attainable accuracy .",
    "furthermore , we show that the adaptive @xmath0-step cg method can be implemented in a way that does not increase the synchronization cost per block versus ( fixed ) @xmath0-step cg .    whereas the fixed @xmath0-step cg method may not attain the same accuracy as classical cg ,",
    "our numerical experiments on a set of example matrices demonstrate that our adaptive @xmath0-step cg method can achieve the same accuracy as classical cg , while still reducing the number of synchronizations required by up to over a factor of @xmath18 . in section  [ sec : sksms ] we give a brief background on @xmath0-step krylov subspace methods and detail the @xmath0-step cg method , and in section  [ sec : relatedwork ] we discuss related work .",
    "section  [ sec : varscg ] presents our main contribution ; in section  [ sec : theory ] , we derive a theoretical bound on how large the @xmath0-step basis matrix condition number can be in terms of the norms of the updated residuals without affecting the attainable accuracy , and in section  [ sec : algorithm ] , we detail the adaptive @xmath0-step cg algorithm that makes use of this bound in determining the number of iterations to perform in each block .",
    "numerical experiments for a small set of test matrices are presented in section  [ sec : experiments ] .",
    "we discuss possible extensions and future work and briefly conclude in section  [ sec : conclusion ] .",
    "the basic idea behind @xmath0-step krylov subspace methods is to grow the underlying krylov subspace @xmath1 dimensions at a time and perform the necessary orthogonalization with a single global synchronization . within a block of @xmath0 iterations ,",
    "the vector updates are performed implicitly by updating the coordinates of the iteration vectors in the @xmath1-dimensional basis spanned by the columns of the @xmath0-step basis matrix .",
    "the algorithmic details vary depending on the particular krylov subspace method considered , but the general approach is the same .",
    "there is a wealth of literature related to @xmath0-step krylov subspace methods .",
    "we give a brief summary of early related work below ; a thorough bibliography can be found in  ( * ? ? ?",
    "* table 1.1 ) .",
    "the @xmath0-step approach was first used in the context of krylov subspace methods by van rosendale  @xcite , who developed a variant of the parallel conjugate gradient method which minimizes inner product data dependencies with the goal of exposing more parallelism .",
    "the term `` @xmath0-step krylov subspace methods '' was first used by chronopoulos and gear , who developed an @xmath0-step cg method  @xcite .",
    "this work was followed in subsequent years by the development of other @xmath0-step variants of orthomin and gmres  @xcite , arnoldi and symmetric lanczos  @xcite , minres , gcr , and orthomin  @xcite , nonsymmetric lanczos  @xcite , and orthodir  @xcite .",
    "walker similarly used @xmath0-step bases , but with the goal of improving stability in gmres by replacing the modified gram - schmidt orthogonalization process with householder qr @xcite .",
    "many of the earliest @xmath0-step krylov subspace methods used monomial bases ( i.e. , @xmath19 $ ] ) for the computed @xmath0-step basis matrices , but it was found that convergence often could not be guaranteed for @xmath20 ( see , e.g. ,  @xcite ) .",
    "this motivated research into the use of other more well - conditioned bases for the krylov subspaces , including scaled monomial bases  @xcite , chebyshev bases  @xcite , and newton bases  @xcite .",
    "the growing cost of communication in large - scale sparse problems has created a recent resurgence of interest in the implementation , optimization , and development of @xmath0-step krylov subspace methods ; see , e.g. , the recent works  @xcite .",
    "in this work , we focus on the @xmath0-step conjugate gradient method ( algorithm  [ alg : scg ] ) , which is equivalent to the @xmath0-step cg method ( algorithm  [ alg : scg ] ) in exact arithmetic . in the remainder of this section ,",
    "we give a brief explanation of the @xmath0-step cg method ; further details on the derivation , implementation , and performance of the @xmath0-step cg method can be found in , e.g. ,  @xcite .",
    "@xmath21 @xmath22    @xmath23 @xmath24    @xmath25    @xmath26    the @xmath0-step cg method consists of an outer loop , indexed by @xmath2 , which iterates over the blocks of iterations , and an inner loop , which iterates over iterations @xmath27 within a block . for clarity ,",
    "we globally index iterations by @xmath28 .",
    "it follows from the properties of cg that for @xmath29 , @xmath30 then the cg vectors for the next @xmath0 iterations lie in the sum of the column spaces of the matrices @xmath31 , \\text { with }    \\text{span}(\\mathcal{p}_{k , s})={\\mathcal{k}}_{s+1}(a , p_{sk+1 } ) \\quad\\text{and } \\nonumber \\\\ \\mathcal{r}_{k , s } & = [ \\rho_0(a)r_{sk+1 } , \\dots , \\rho_{s-1}(a)r_{sk+1 } ] , \\text { with }    \\text{span}(\\mathcal{r}_{k , s})={\\mathcal{k}}_{s}(a , r_{sk+1 } ) , \\label{eq : cg - krylovbasis}\\end{aligned}\\ ] ] where @xmath32 is a polynomial of degree @xmath33",
    "satisfying the three - term recurrence @xmath34    we define the @xmath0-step basis matrix @xmath35 $ ] and define @xmath36 to be the same as @xmath37 except with all zeros in columns @xmath38 and @xmath39 .",
    "note that the @xmath0 s in the subscripts are unnecessary here , but will be useful in describing the variable @xmath0-step cg method later on . under certain assumptions on the nonzero structure of @xmath7 ,",
    "@xmath37 can be computed in parallel in each outer loop for the same asymptotic latency cost as a single matrix - vector product using the so - called `` matrix powers kernel '' ( see  @xcite for details ) . since the columns of @xmath37 satisfy  , we can write @xmath40 where @xmath41 is a @xmath42 block tridiagonal matrix defined by the 3-term recurrence coefficients in  .    by  , there exist vectors @xmath43 , @xmath44 , and @xmath45 that represent the coordinates of the cg iterates @xmath46 , @xmath47 , and @xmath48 , respectively , in @xmath37 for @xmath29 .",
    "that is , @xmath49 therefore , in the inner loop of @xmath0-step cg , rather than update the cg vectors explicitly , we instead update their coordinates in @xmath37 , i.e. , for @xmath27 , @xmath50 thus the matrix - vector product with @xmath7 in each iteration becomes a matrix - vector product with the much smaller matrix @xmath51 .",
    "this along with the length-@xmath52 vector updates can be performed locally by each processor in each inner loop iteration .",
    "we can also reduce communication in computing the inner products .",
    "letting @xmath53 , and using   and  , @xmath54 and @xmath55 can be computed by @xmath56 the matrix @xmath57 can be computed with one global synchronization per outer loop iteration , which , in terms of asymptotic parallel latency , costs the same as a single inner product .",
    "as @xmath41 and @xmath57 are dimension @xmath42 , @xmath54 and @xmath55 can be computed locally by each processor within the inner loop .",
    "[ alg : cacg : akx ]    [ alg : cacg : g ]    assemble @xmath41 such that   holds    @xmath58^{t}$ ] , @xmath59^{t}$ ] , @xmath60^t$ ]    @xmath61 [ alg:@xmath0-step cg - method : alpha ] @xmath62    @xmath63 [ alg:@xmath0-step cg - method : x ] @xmath64 [ alg:@xmath0-step cg - method : r ]    @xmath65 [ alg:@xmath0-step cg - method : beta ]    @xmath66 [ alg:@xmath0-step cg - method : p ]    [ alg : cacg : recover ]",
    "in this section , we discussed related work in the areas of variable @xmath0-step krylov subspace methods , the analysis of maximum attainable accuracy in finite precision cg , and inexact krylov subspace methods .",
    "williams et al .",
    "@xcite use a variable @xmath0-step bicgstab as the coarse grid solver in a multigrid method .",
    "the technique used here , termed `` telescoping '' , is motivated by the observation than some coarse grid solves converge after only a few iterations , whereas other solves take significantly longer . by starting with a small @xmath0 value and gradually increasing it ,",
    "they ensure that @xmath0-step bicgstab with larger @xmath0 is only used when the solve takes enough iterations to amortize the additional costs associated with @xmath67-step cg .",
    "imberti and erhel  @xcite have recently developed an @xmath0-step gmres method that allows variable @xmath0 sizes .",
    "they recommend choosing @xmath3 values according to the fibonacci sequence up to some maximum value @xmath68 , i.e. , starting with @xmath69 and for @xmath70 , @xmath71 . in this way ,",
    "the sequence @xmath72 used by imberti and erhel is predetermined .",
    "in contrast , in our approach , the sequence @xmath72 is dynamically chosen based on the 2-norm of the updated residual ( which is not necessarily monotonically decreasing in cg ) .",
    "in addition , our method is designed such that a user - specified accuracy @xmath73 can be attained when @xmath74 , where @xmath75 is the accuracy attained by classical cg for the same problem .      in both @xmath0-step and classical variants of krylov subspace methods , finite precision roundoff error in updates to the approximate solution @xmath76 and the residual @xmath17 in each iteration can cause the updated residual @xmath17 and the true residual @xmath16 to grow further and further apart as the iterations proceed .",
    "if this deviation grows large , it can limit the _ maximum attainable accuracy _ , i.e. , the accuracy with which we can solve @xmath5 on a computer with unit round - off @xmath77 . analyses of maximum attainable accuracy in cg and other classical ksms are given by greenbaum  @xcite , van der vorst and ye  @xcite , sleijpen , van der vorst , and fokkema  @xcite , sleijpen , van der vorst , and modersitzki  @xcite , bjrck , elfving , and strako  @xcite , and gutknecht and strako  @xcite .",
    "one important result of these analyses is the insight that loss of accuracy can be caused at a very early stage of the computation , which can not be corrected in later iterations .",
    "analyses of the maximum attainable accuracy in @xmath0-step cg and the @xmath0-step biconjugate gradient method ( bicg ) can be found in  @xcite .",
    "the term `` inexact krylov subspace methods '' refers to krylov subspace methods in which the matrix - vector products @xmath78 are computed inexactly as @xmath79 where @xmath80 represents some error matrix ( due to either finite precision computation or intentional approximation ) .",
    "it is assumed that all other computations ( orthogonalization and vector updates ) are performed exactly . using an analysis of the deviation of the true and updated residuals",
    ", it is shown that under these assumptions the size of the perturbation term @xmath80 can be allowed to grow inversely proportional to the norm of the updated residual without adversely affecting the attainable accuracy .",
    "this theory has potential application in mixed - precision krylov subspace methods , as well as in situations where the operator @xmath7 is expensive to apply and can be approximated in order to gain performance .",
    "much work has been dedicated to inexact krylov subspace methods . in  @xcite , simoncini and szyld",
    "provide a general theory for inexact variants applicable to both symmetric and nonsymmetric linear systems and eigenvalue problems and use this to derive criteria for bounding the size of @xmath80 in such a way that does not diminish the attainable accuracy of the method .",
    "a similar analysis was given by van de eschof and sleijpen   @xcite .",
    "these analyses confirm and improve upon the earlier empirical results for the inexact gmres method of bouras and fraysse  @xcite and the inexact conjugate gradient method of bouras , frayss , and giraud  @xcite .",
    "our work is very closely related to the theory of inexact krylov subspace methods in that our analysis results in a somewhat analogous `` relaxation strategy '' , where instead of relaxing the accuracy of the matrix - vector products we are instead relaxing a constraint on the condition numbers of the computed @xmath0-step basis matrices .",
    "in addition , our analysis assumes that all parts of the computation are performed in a fixed precision @xmath77 .",
    "assemble @xmath81 such that   holds    @xmath82^{t}$ ] , @xmath83^{t}$ ] , @xmath84^t$ ]    @xmath85 @xmath86    @xmath63 @xmath87    @xmath88    @xmath89    as discussed , the conditioning of the @xmath0-step basis matrices in @xmath0-step krylov subspace methods affects the rate of convergence and attainable accuracy . in the @xmath0-step cg method ( algorithm  [ alg : scg ] ) , the krylov subspace basis is computed in blocks of size @xmath0 in each outer loop .",
    "this method can be generalized to allow the blocks to be of varying size ; in other words , a different @xmath0 value can be used in each outer loop iteration .",
    "we denote the block size in outer iteration @xmath2 by @xmath3 , where @xmath90 for some maximum @xmath0 value @xmath91 .",
    "we call this a _ variable @xmath0-step krylov subspace method_. a general variable @xmath0-step cg method is shown in algorithm  [ alg : vscg ] .    in this section ,",
    "we derive a variable @xmath0-step cg method which automatically determines @xmath3 in each outer loop such that a user - specified accuracy can be attained , which we call the adaptive @xmath0-step cg method .",
    "our analysis is based on the maximum attainable accuracy analysis for @xmath0-step cg in  @xcite , which shows that the attainable accuracy of @xmath0-step cg depends on the condition numbers of the @xmath0-step basis matrices @xmath92 computed in each outer loop . in summary , we show that if in outer loop @xmath2 , @xmath3 is selected such that the condition number of the basis matrix @xmath93 is inversely proportional to the maximum 2-norm of the residual vectors computed within outer loop @xmath2 , then the approximate solution produced by the variable @xmath0-step cg method can be as accurate as the approximate solution produced by the classical cg method .",
    "further , our method allows for the user to specify the desired accuracy , so in the case that a less accurate solution is required , our method automatically adjusts to allow for higher @xmath3 values .",
    "this effectively exploits the tradeoff between accuracy and performance in @xmath0-step krylov subspace methods .    in this section",
    "we derive our adaptive @xmath0-step approach , which stems from a bound on the growth in outer loop @xmath2 of the gap between the true and updated residuals in finite precision .      in the remainder of the paper ,",
    "hats denote quantities computed in finite precision , @xmath77 denotes the unit round - off , @xmath94 denotes the 2-norm , and @xmath95 denotes the 2-norm condition number @xmath96 . to simplify the analysis",
    ", we drop all @xmath97 terms ( and higher order terms in @xmath77 ) .",
    "writing the true residual @xmath98 , we can write the bound @xmath99 then as @xmath100 ( i.e. , as the method converges ) , we have @xmath101 .",
    "the size of the _ residual gap _ , i.e. , @xmath102 , therefore determines the attainable accuracy of the method ( see the related references in section  [ sec : maa ] ) .",
    "we begin by considering the rounding errors made in the variable @xmath0-step cg method ( algorithm  [ alg : vscg ] ) . in outer loop @xmath2 and",
    "inner loops @xmath103 of finite precision variable @xmath0-step cg , using standard models of floating point error ( e.g. ,  @xcite ) we have @xmath104 where @xmath105 denotes the maximum number of nonzeros per row in @xmath7 and @xmath106 .",
    "let @xmath107 denote the residual gap in iteration @xmath12 .",
    "similar to the analysis in  @xcite , for variable @xmath0-step cg we can write the growth of the residual gap in iteration @xmath108 in outer loop @xmath2 as @xmath109 our goal is now to manipulate a bound on this quantity in order to derive a method for determining the largest @xmath3 value we can use in outer loop @xmath2 such that the desired accuracy is attained . using standard techniques , we have the norm - wise bound @xmath110 where we have used @xmath111 . to simplify the notation , we let @xmath112 and @xmath113 .",
    "we note that @xmath114 depends on the polynomial basis used and is not expected to be too large ; e.g. , for the monomial basis , @xmath115 .",
    "the above bound can then be written @xmath116    note that assuming @xmath117 is full rank , we have @xmath118 to bound @xmath119 in terms of the size of the residuals , notice that by  , @xmath120 so @xmath121 thus together with   we can write @xmath122    this allows us to write the bound   in the form @xmath123 using @xmath124 , we have @xmath125 and writing @xmath126 , we have @xmath127 then @xmath128 and letting @xmath129 , this bound can be written @xmath130 this gives a norm - wise bound on the growth of the residual gap due to finite precision errors made in outer iteration @xmath2 .    letting @xmath131 , notice that we have @xmath132",
    "suppose that at the end of the iterations we want the size of the residual gap to be on the order @xmath73 ( where @xmath133 , as this is the best we could expect ) . then assuming the number of outer iterations @xmath2 is not too high , this can be accomplished by requiring that in each outer loop , for @xmath134 , @xmath135 from  , this means that we must have , for all inner iterations @xmath136 , @xmath137 the bound   tells us that as the 2-norm of the residual decreases , we can tolerate a more ill - conditioned basis matrix @xmath138 without detriment to the attainable accuracy .",
    "since @xmath139 grows with increasing @xmath3 , this suggests that @xmath3 can be allowed to increase as the method converges at a rate proportional to the inverse of the residual 2-norm .",
    "this naturally gives a relaxation strategy that is somewhat analogous to those derived for inexact krylov subspace methods ( see section  [ sec : inexact ] ) .",
    "we discuss the implementation of this strategy in the following subsection .      in this section , we propose a variable @xmath0-step cg method that makes use of the constraint   in determining @xmath3 in each outer loop @xmath2 , which we call the _ adaptive _ @xmath0-step cg method .",
    "our approach uses the largest @xmath3 value possible in each outer iteration @xmath2 such that   is satisfied up to some @xmath91 set by the user ( e.g. , @xmath91 could be selected by auto - tuning to find the @xmath0 value that minimizes the time per iteration in @xmath0-step cg ) . since",
    "a variable @xmath0-step cg method produces the same iterates as classical cg when @xmath140 for all outer loops @xmath2 , we expect that our adaptive @xmath0-step cg method can attain accuracy @xmath73 when @xmath74 , where @xmath75 is the attainable accuracy of classical cg .",
    "when the right - hand - side of   is less than one ( due to , e.g. , @xmath73 being set too small ) , the inequality   can not be satisfied . in this case , we default to using @xmath140 ( i.e. , we perform an iteration of classical cg ) .    in cg ,",
    "it is the @xmath7-norm of the error rather than the @xmath141-norm of the residual that is minimized , which means that in some inner loop iteration @xmath33 , we can have @xmath142 . since @xmath143 for @xmath144 are not known at the start of the outer loop iteration , we must determine @xmath3 online , i.e. , during the iterations .",
    "we therefore first construct a basis matrix that satisfies   under the assumption that @xmath145 . within each inner loop",
    ", we can perform an inexpensive check to ensure that   is still satisfied after computing the next residual ; if not , we break from the inner loop , discard the current basis matrix , and begin a new outer loop iteration .",
    "we elaborate on this approach in the remainder of this section .",
    "an optimized high - performance implementation of our variable @xmath0-step approach is outside the scope of the present work , although we will briefly discuss how one might implement this strategy efficiently such that each outer loop still requires only a single global synchronization and the number of wasted matrix - vector products is kept to a minimum .",
    "the bound   indicates that when @xmath117 is constructed at the beginning of outer loop @xmath2 , we should enforce @xmath146 therefore we first construct an @xmath147-step basis matrix @xmath148 using some @xmath149 , compute the gram matrix @xmath150 , and then use the condition numbers of the leading principle submatrices of the blocks of @xmath151 to compute the condition numbers of the leading columns of the blocks of @xmath148 . to be concrete , for @xmath152 , we have ( using matlab notation ) @xmath153\\ ] ] and @xmath154 where @xmath155 .",
    "then since @xmath156 , we can easily compute the condition number of the @xmath12-step basis matrix @xmath157 , i.e. , the basis matrix needed to perform @xmath12 inner loop iterations , for @xmath158 .",
    "we therefore let @xmath159 be the largest value possible such that @xmath160 holds .    to handle the case where @xmath161 for some inner iteration @xmath33",
    ", we can add a check within the inner loop iterations : after computing @xmath162 , we check if @xmath163 is still satisfied .",
    "note that since @xmath164 , performing this check does not require any additional communication .",
    "if   is satisfied for all @xmath165 , we will perform all @xmath166 inner loop iterations .",
    "if this is not satisfied for some @xmath33 , we break from the current inner loop iteration and begin the next outer loop ; i.e. , we only perform @xmath167 inner loop iterations in outer loop @xmath2 .",
    "although performing the checks for determining when to break from the current outer loop do not incur any additional communication cost in terms of the number of messages sent between processors , there is , however , potential wasted effort in terms of computation and the number of words moved when @xmath168 and/or @xmath169 , since in this case we have computed more basis vectors than we need to perform @xmath3 inner loop updates .",
    "one way to mitigate this is to only allow @xmath147 to grow by at most @xmath170 vectors in each outer loop , i.e. , @xmath171 , where @xmath170 is some small positive integer ( e.g. , @xmath172 or @xmath173 ) .",
    "the approach outlined above is summarized in algorithm  [ alg : vscg2 ] .",
    "note that in algorithm  [ alg : vscg2 ] , we have specified that the function @xmath174 can be input by the user .",
    "because our bounds are not tight , in many cases using @xmath175 ( from the bound  ) overestimates the error , which results in the use of smaller @xmath3 values than necessary .",
    "note that although there is no expectation that @xmath176 decreases between iterations in cg , as long as the method is converging , we still expect the residual norm to trend downward",
    ". therefore one could make the simplifying assumption that @xmath177 for @xmath178 . in this case , the value @xmath166 can be determined at the beginning of outer loop @xmath2 before performing any inner iterations , and so the checks   are unnecessary .",
    "this simplification comes at the potential cost of reduced reliability of the adaptive @xmath0-step method , although in our experimenting we have observed that taking @xmath166 usually works well even when the residual norm is nonmonotonic .",
    "because our strategy is based on a loose upper bound on the deviation of residuals , we do not require particularly accurate estimates of @xmath179 .",
    "therefore alternative methods for inexpensively approximating this quantity could also be used in practice .",
    "assemble @xmath180 such that   holds    @xmath181^{t}$ ] , @xmath182^{t}$ ] , @xmath183^t$ ] @xmath167 @xmath184 @xmath86    @xmath63 @xmath185    @xmath186    @xmath89",
    "in this section , we test the adaptive @xmath0-step cg method ( algorithm  [ alg : vscg2 ] ) on a number of small examples from the university of florida sparse matrix collection  @xcite using matlab .",
    "the test examples were chosen to exemplify various convergence trajectories of cg , which allows us to demonstrate the validity of our relaxation condition in which @xmath3 depends on the rate of convergence .",
    "the test problems used here are small , so we would not use ( fixed or variable ) @xmath0-step methods here in practice ; nevertheless , they still serve to exhibit the numerical behavior of the methods .",
    "all tests were run in double precision , i.e. , @xmath187 , and use a right - hand side @xmath188 with entries @xmath189 where @xmath190 is the dimension of @xmath7 .",
    "we use matrix equilibration , i.e. , @xmath191 where @xmath192 is a diagonal matrix of the largest entries in each row of @xmath7 .",
    "properties of the test matrices ( post - equilibration ) are shown in table  [ mats ] .",
    "our results are plotted in figures  [ fig : gr3030]-[fig : ex5 ] . for each test matrix , we ran classical cg ( algorithm  [ alg : cg ] ) , as well as both ( fixed ) @xmath0-step cg ( algorithm  [ alg : scg ] ) and adaptive @xmath0-step cg ( algorithm  [ alg : vscg2 ] ) for @xmath91 values @xmath18 , @xmath193 , and @xmath194 ( for fixed @xmath0-step cg , @xmath195 ) . for each matrix and @xmath91 value , we tested two different @xmath73 values : @xmath196 , the maximum attainable accuracy of classical cg , and @xmath197 .",
    "the monomial basis was used for both variable and fixed @xmath0-step methods in all tests ( note that there is no technical restriction to the monomial basis ; the analysis used in deriving the adaptive @xmath0-step cg method still applies regardless of the polynomials used in constructing @xmath148 ) . in all tests for adaptive @xmath0-step cg , we set @xmath198 ( see algorithm  [ alg : vscg2 ] ) , as this gives the optimal choice of @xmath3 in each iteration .",
    "( we also tried using smaller values of @xmath170 , e.g. , @xmath172 and @xmath173 ; this did not significantly change the total number of outer loop iterations performed ) .",
    "we used @xmath199 in our condition for determining the number of inner loop iterations to perform in all experiments except for tests with matrices bcsstk09 ( figure  [ fig : bcsstk09 ] ) and ex5 ( figure  [ fig : ex5 ] ) ; for these problems , convergence is more irregular , and thus the bound   is tighter .",
    "each figure has a corresponding table ( tables  [ tab : gr3030]-[tab : ex5 ] ) which lists the number of outer loop iterations ( a proxy for the latency cost or number of global synchronizations ) required for convergence of the true residual @xmath141-norm to level @xmath73 .",
    "for classical cg , the number reported gives the total number of iterations .",
    "dashes in the tables indicate that the true residual diverged or stagnated before reaching level @xmath73 .    .test matrix properties . [ cols=\"<,<,<,<,<\",options=\"header \" , ]     [ tab : ex5 ]      in all experiments , the adaptive @xmath0-step cg method was able to converge to the desired tolerance @xmath73 , whereas for @xmath196 and higher @xmath0 values ( @xmath200 and/or @xmath201 ) , fixed @xmath0-step cg may not achieve convergence to this level .",
    "we point out that even when the matrix is very well - conditioned , choosing @xmath0 too large can very negatively affect the convergence rate in fixed @xmath0-step cg ; see , e.g. , the case with @xmath201 and @xmath197 in figure  [ fig : mesh3e1 ] , where @xmath0-step cg requires more than 8 times the number of synchronizations than classical cg . in tests where both fixed @xmath0-step cg and adaptive @xmath0-step cg converge , adaptive @xmath0-step cg takes at most @xmath141 additional outer iterations versus fixed @xmath0-step cg ( although in most cases it takes as many or fewer outer iterations ) .",
    "we note that in all cases , variable @xmath0-step cg uses fewer outer loop iterations ( synchronization points ) than classical cg , so we still expect a performance advantage on latency - bound problems .",
    "the experiments using @xmath202 demonstrate the ability of the adaptive @xmath0-step cg method to automatically adjust to use the largest @xmath3 values possible ( according to our bound ) that will eventually attain the desired accuracy .",
    "for example , in figure  [ fig : gr3030 ] for gr_30_30 , for all tests using @xmath202 , adaptive @xmath0-step cg uses @xmath203 in every outer iteration , defaulting to the fixed @xmath0-step method .",
    "in contrast , in plots on the left using @xmath196 , adaptive @xmath0-step cg uses smaller @xmath3 values at the beginning and then increases up to @xmath203 and is able to attain accuracy @xmath75 whereas fixed @xmath0-step cg fails when for @xmath200 and @xmath201 .",
    "below we list the sequence of @xmath3 values chosen in adaptive @xmath0-step cg for tests with @xmath196 and @xmath204 ( plots in the lower left in each figure ) :    * gr_30_30 : 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 , 4 , 5 , 6 , 8 , 10 , 10 * mesh3e1 : 1 , 1 , 2 , 4 , 6 , 9 , 10 * nos6 : 6 , 1 , 2 , 3 , 4 , 5 , 4 , 4 , 1 , 4 , 4 , 5 , 6 , 7 , 8 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 * bcsstk09 : 2 , 2 , 3 , 3 , 3 , 3 , 3 , 1 , 2 , 3 , 3 , 3 , 2 , 2 , 2 , 2 , 3 , 3 , 3 , 3 , 4 , 4 , 4 , 4 , 5 , 5 , 6 , 7 , 1 , 6 , 6 , 7 , 7 , 7 , 7 , 8 , 8 , 8 , 9 , 9 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 , 10 * ex5 : 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 3 , 1 , 1 , 2 , 1 , 1 , 1 , 3 , 3 , 2 , 1 , 1 , 1 , 2 , 1 , 1 , 2 , 4 , 6 , 2 , 4 , 5 , 7 , 2 , 6 , 1 , 7 , 6    there are a few things to notice . first ,",
    "as expected , the sequence of @xmath3 values is monotonically increasing when the residual 2-norm decreases monotonically or close to it ( matrices gr_30_30 and mesh3e1 ) , but when the residual 2-norm oscillates , we no longer have monotonicity of the @xmath3 values ( matrices nos6 , bcsstk09 , and ex5 ) .",
    "second , it may be that @xmath91 is never reached , as is the case for matrix ex5 .",
    "it is also worth pointing out that the sequence of @xmath3 values need not start with @xmath205 ; for nos6 , @xmath206 and for bcsstk09 , @xmath207 .",
    "it may be that on a particular machine for a particular matrix structure , it is not beneficial in terms of minimizing time per iteration to go above a small value of @xmath91 , say @xmath208 . in this case ,",
    "as long as the matrix is reasonably well - conditioned and the accuracy required is not too high , the fixed @xmath0-step cg method can probably be used without too much trouble .",
    "however , even in this case the adaptive @xmath0-step cg method can provide reliability while still maintaining the reduction in synchronizations of the fixed @xmath0-step method , at the cost of potentially wasting a few matrix - vector multiplications in each outer iteration .",
    "looking at the number of outer loop iterations in tables  [ tab : gr3030]-[tab : ex5 ] , we see that in our tests for adaptive @xmath0-step cg it is often the case that increasing the @xmath91 value past a certain point gives diminishing returns in terms of reducing synchronization .",
    "this may represent a fundamental limit of the possible performance using the monomial basis for these problems . in these cases",
    "it may be that better performance ( higher @xmath3 values ) can be achieved only by using a better - conditioned polynomial basis for constructing @xmath148 ( e.g. , newton or chebyshev bases ) .",
    "in this work , we developed the adaptive @xmath0-step cg method , which is a variable @xmath0-step cg method where the parameter @xmath3 is determined automatically in each outer loop such that a user - specified accuracy is attainable .",
    "the method for determining @xmath3 is based on a bound on the residual gap within outer loop @xmath2 , from which we derive a constraint on the condition number of the @xmath0-step basis matrix generated in outer loop @xmath2 .",
    "the computations required for determining @xmath3 can be performed without introducing any additional synchronizations in each outer loop .",
    "our numerical experiments demonstrate that the adaptive @xmath0-step cg method is able to attain up to the same accuracy as classical cg while still significantly reducing the number of outer loop iterations ( i.e. , synchronizations ) performed .",
    "in contrast , the fixed @xmath0-step cg method can fail to converge to this accuracy when @xmath209 .",
    "additionally , the adaptive @xmath0-step cg method adjusts to use larger @xmath3 when an application requires a less accurate solution , automatically optimizing the tradeoff between speed and accuracy . in this way",
    ", a user can use the matrix structure and machine parameters to estimate the value @xmath91 that will minimize the time per iteration ( e.g. , by guessing , or offline auto - tuning ) , input the accuracy they require , and the adaptive @xmath0-step cg method will choose @xmath3 as large as it can ( i.e. , reducing as much synchronization as it can ) while still converging to the desired level .",
    "this takes the burden of parameter selection off the user .",
    "our adaptive @xmath0-step cg method thus makes significantly advances in improving reliability and usability in @xmath0-step krylov subspace methods .",
    "our future work includes a high - performance implementation and performance experiments in order to compare classical cg , fixed @xmath0-step cg , and adaptive @xmath0-step cg on large - scale problems .",
    "another potential improvement is to obtain a tighter bound to replace  , which could avoid the requirement of adjusting the function @xmath174 ( which represents a multiplicative factor in the bounds   and  ) in algorithm  [ alg : vscg2 ] .",
    "the adaptive @xmath0-step cg method is designed to maximize @xmath3 subject to a constraint on accuracy , but obviously it would also be useful to optimize @xmath3 to achieve the _ overall _ lowest latency / synchronization cost .",
    "this would require a better understanding of the finite precision convergence behavior of both @xmath0-step and classical krylov subspace methods , which is an active area of research .",
    "we believe that this adaptive approach based on a bound on the residual gap can be extended to other @xmath0-step krylov subspace methods as well .",
    "the same approach outlined here can be used in @xmath0-step bicg ( see , e.g. ,  @xcite ) , which uses the same recurrences for the solution and updated residual .",
    "we conjecture that a similar approach applies to @xmath0-step variants of other recursively computed residual methods ( see  @xcite ) , like @xmath0-step bicgstab  @xcite , as well as methods like @xmath0-step gmres . in the case of @xmath0-step gmres",
    ", it may be possible to simplify the adaptive algorithm : since the residual norm monotonically decreases , the sequence of @xmath3 values should monotonically increase .",
    "such a sequence of @xmath3 values was used in the variable @xmath0-step gmres method of imberti and erhel  @xcite .    as a final comment",
    ", we note that in our analysis in section  [ sec : varscg ] , we used equation   to determine a bound on @xmath210 that guarantees accuracy on the level @xmath73 .",
    "we could have just as easily used equation   to develop a mixed precision @xmath0-step cg method where @xmath0 is fixed , but the precision used in computing in outer loop @xmath2 varies . in other words , rearranging the inequality   we see that , given that we ve computed an @xmath0-step basis with condition number @xmath139 , if we want to achieve accuracy @xmath73 , we must have that @xmath211 investigating the potential for mixed precision @xmath0-step krylov subspace methods based on this analysis remains future work .",
    ", _ a relaxation strategy for inner - outer linear solvers in domain decomposition methods _ , tech .",
    "report cerfacs tr / pa/00/17 , european centre for research and advanced training in scientific computation , 2000 .                      , _ a block lanczos algorithm for computing the q algebraically largest eigenvalues and a corresponding eigenspace of large , sparse , real symmetric matrices _ , in proc . of the 1974 ieee conf . on decision and control , ieee , 1974 , pp ."
  ],
  "abstract_text": [
    "<S> on modern large - scale parallel computers , the performance of krylov subspace iterative methods is limited by global synchronization . </S>",
    "<S> this has inspired the development of @xmath0-step ( or communication - avoiding ) krylov subspace method variants , in which iterations are computed in blocks of @xmath0 . </S>",
    "<S> this reformulation can reduce the number of global synchronizations per iteration by a factor of @xmath1 , and has been shown to produce speedups in practical settings .    </S>",
    "<S> although the @xmath0-step variants are mathematically equivalent to their classical counterparts , they can behave quite differently in finite precision depending on the parameter @xmath0 . </S>",
    "<S> if @xmath0 is chosen too large , the @xmath0-step method can suffer a convergence delay and a decrease in attainable accuracy relative to the classical method . </S>",
    "<S> this makes it difficult for a potential user of such methods - the @xmath0 value that minimizes the time per iteration may not be the best @xmath0 for minimizing the overall time - to - solution , and further may cause an unacceptable decrease in accuracy . towards improving the reliability and usability of @xmath0-step krylov subspace methods , in this work </S>",
    "<S> we derive the _ adaptive @xmath0-step cg method _ </S>",
    "<S> , a variable @xmath0-step cg method where in block @xmath2 , the parameter @xmath3 is determined automatically such that a user - specified accuracy is attainable . the method for determining @xmath3 </S>",
    "<S> is based on a bound on growth of the residual gap within block @xmath2 , from which we derive a constraint on the condition numbers of the computed @xmath4-dimensional krylov subspace bases . </S>",
    "<S> the computations required for determining the block size @xmath3 can be performed without increasing the number of global synchronizations per block . </S>",
    "<S> our numerical experiments demonstrate that the adaptive @xmath0-step cg method is able to attain up to the same accuracy as classical cg while still significantly reducing the total number of global synchronizations . </S>"
  ]
}