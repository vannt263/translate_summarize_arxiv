{
  "article_text": [
    "nonlinear time series analysis generally assumes stationarity ( see * ? ? ?",
    "* ; * ? ? ?",
    "* for an overview ) .",
    "however , many time series are actually nonstationary for various reasons , such as temperature drift in the experimental setup , decreasing reservoirs in ( bio)chemical reactors or ecological systems , global warming in climate data , varying heart rate in cardiology , or vibrato / tremor in vocal fold vibrations .",
    "such nonstationarities can be modeled by underlying parameters , referred to as driving forces , that change the dynamics of the system smoothly on a slow time scale or abruptly but rarely , e.g.  if the dynamics switches between different discrete states .",
    "if a test reveals that a time series is nonstationary  @xcite , one can still apply methods of stationary time series analysis if one determines sections where the driving force has similar values and analyzes these sections as one stationary time series .",
    "this can be done by first slicing the time series into windows of equal size and then grouping the windows based on similarity measures of the dynamics  @xcite .",
    "this division of the time series can be avoided by the technique of overembedding .",
    "if the embedding dimension is sufficiently high , then similar embedding vectors automatically belong to similar values of the driving forces and all available data can be used for analysis , such as forecasting or nonlinear noise reduction  @xcite",
    ".    however , in some cases , e.g.  in the analysis of eeg data , one is particularly interested in revealing the driving forces themselves , which is in principle only possible up to an invertible transformation .",
    "one standard method for visualizing driving forces is the recurrence plot  @xcite , but it is often difficult to interpret .",
    "methods have been developed to estimate driving forces based on the finding that the recurrence plot of a time series is similar to the recurrence plot of its underlying driving forces  @xcite .",
    "another technique for the reconstruction of driving forces has been presented in  @xcite . here",
    "i present an alternative approach based on slow feature analysis , a new technique developed in the field of theoretical neurobiology .",
    "slow feature analysis ( sfa ) has been originally developed in context of an abstract model of unsupervised learning of invariances in the visual system of vertebrates  @xcite and is described in detail in  @xcite .",
    "the general objective of sfa is to extract slowly varying features from a quickly varying signal . for a scalar output signal",
    "it can be formalized as follows .",
    "let @xmath0 be an @xmath1-dimensional input signal where @xmath2 indicates time and @xmath3^t$ ] is a vector .",
    "find the input - output function @xmath4 that generates a scalar output signal @xmath5 with most slowly temporal variation possible as measured by the variance of the time derivative : @xmath6 with @xmath7 indicating the temporal mean .",
    "for convenience and to avoid the trivial constant solution the output signal has to meet the following constraints : @xmath8 this is an optimization problem of variational calculus and as such difficult to solve .",
    "however , if we constrain the input - output function to be a linear combination of some fixed and possibly nonlinear basis functions , the problem becomes tractable and can be solved in the following way .",
    "let @xmath9 be a vector of some fixed basis functions . to be concrete assume @xmath10 contains all monomials of degree  1 and  2 .",
    "applying @xmath10 to the input signal @xmath11 yields the nonlinearly expanded signal @xmath12 : @xmath13^t , \\\\   { \\bm{{z}}}'(t ) & : = & { \\bm{{h}}}'({\\bm{{x}}}(t ) ) \\,.\\end{aligned}\\ ] ] assume @xmath4 is a linear combination of the basis functions plus a constant @xmath14 , generating the output signal @xmath15 : @xmath16 since @xmath10 contains all monomials of degree  1 and  2 , @xmath4 can be any polynomial of degree  2 , if the constant @xmath14 and the coefficient vector @xmath17 are chosen correspondingly .    for reasons that become clear below , it is convenient to sphere ( or whiten ) the expanded signal and transform the basis functions accordingly : @xmath18 with the sphering matrix @xmath19 chosen such that the signal components have a unit covariance matrix , i.e.  @xmath20 , which can be easily done with the help of principal component analysis ( pca ) .",
    "the signal components have also zero mean , @xmath21 , since the mean values have been subtracted .    the input - output function and the output signal",
    "can now be written in these transformed basis functions and sphered expanded signal , respectively : @xmath22 this is a form in which the constraints ( [ eq : constr0 ] , [ eq : constr1 ] ) are particularly easy to meet , since we find for any coefficient vector @xmath23 with norm 1 , @xmath24 which also motivates why constant @xmath14 has been dropped in ( [ eq : g2 ] , [ eq : y2 ] ) .",
    "thus the optimization problem reduces to finding the normalized coefficient vector @xmath23 that minimizes @xmath25 which is obviously the normalized eigenvector of the time - derivative covariance matrix @xmath26 with the smallest eigenvalue , which again can be easily found by pca .",
    "the output signal @xmath27 is then given by ( [ eq : y2 ] ) . notice that @xmath27 is usually uniquely determined up to the sign .    in summary sfa",
    "consists of basically four steps : ( i )  expand the input signal with some set of fixed possibly nonlinear functions ; ( ii )  sphere the expanded signal to obtain components with zero mean and unit covariance matrix ; ( iii )  compute the time derivative of the sphered expanded signal and determine the normalized eigenvector of its covariance matrix with the smallest eigenvalue ; ( iv )  project the sphered expanded signal onto this eigenvector to obtain the output signal .",
    "the input - output function is given by ( [ eq : g2 ] ) but is not needed here , since we are only interested in the extracted output signal .    in practice one has to work with time series @xmath28 instead of continuous signals @xmath11 , but the transfer of the algorithm described above to time series is straight forward .",
    "the time derivative is simply computed as the difference between successive data points without wrap - around assuming a constant sampling spacing @xmath29 .",
    "the description of sfa given above assumes that all monomials of degree  1 and  2 are used for signal expansion .",
    "however , any other set of basis functions could be used as well , e.g.  monomials of higher degree or radial basis functions .",
    "it is also straight forward to extend sfa to more than one output signal component , in which case additional components should be uncorrelated to earlier ones . under this constraint additional components can be computed by using the normalized eigenvectors of ( [ eq : ydot22 ] ) with the next larger eigenvalues  @xcite .",
    "sfa requires that all components vary to some extent .",
    "thus if there are constant components in the input signal or the expanded signal , these are simply discarded .",
    "in the following i will present two examples with time series @xmath30 derived from a tent map and a logistic map to illustrate the properties of sfa .",
    "i have also done simulations with the lorenz system , but results were very unreliable and sensitive to parameter variations and noise .",
    "the underlying driving force will always be denoted by @xmath31 and may vary between @xmath32 and @xmath33 either smoothly or rarely , but with a comparable time scale of variation ( as defined by the variance of its time derivative ( [ eq : slowness ] ) ) .",
    "taking the time series @xmath30 directly as an input signal would not give sfa enough information to estimate the driving force , because sfa considers only data ( and its derivative ) from one time point at a time .",
    "thus it is necessary to generate an embedding - vector time series as an input . here",
    "embedding vectors at time point @xmath34 with delay @xmath35 and dimension @xmath36 are defined by @xmath37^t \\,,\\end{aligned}\\ ] ] for scalar @xmath30 and odd @xmath36 .",
    "the definition can be easily extended to even @xmath36 , which requires an extra shift of the indices by @xmath38 or its next lower integer to center the used data points at @xmath34 . centering the embedding vectors results in an optimal temporal alignment between estimated and true driving force .",
    "the following simulations are based on 6000 data points each and were done with matlab ( release 13 ) .      as a first example consider a time series generated by an iterated tent map  @xcite @xmath39 which maps the interval @xmath40 $ ] onto itself with a functional form `` @xmath41 '' for @xmath42 ( or @xmath43 ) .",
    "the parameter @xmath31 shifts this wedge cyclically to the left until it becomes a `` @xmath44 '' for @xmath45 .",
    "figure  [ fig : smoothtent ] shows the true driving force , the time series , and the estimated driving force with @xmath46 , @xmath47 , and third order polynomials for sfa .",
    "the correlation between true and estimated driving force is @xmath48 .",
    "note that the scale and offset of the estimated driving force are arbitrarily fixed by the constraints and that the sign is random .",
    "the axes were therefore chosen such that the curves in the bottom graphs of this and the following figures are optimally aligned with each other .              as a second example consider a time series derived from a logistic map @xmath49 which maps the interval @xmath40 $ ] onto the interval @xmath50 $ ] and has the shape of an upside - down parabola crossing the abscissa at 0 and 1 .",
    "parameter @xmath31 governs the height of the parabola .",
    "figure  [ fig : smoothlogistic ] shows the true driving force , the time series , and the estimated driving force with @xmath51 , @xmath47 , and second order polynomials for sfa .",
    "the correlation between true and estimated driving force is @xmath52 .",
    "[ [ choice - of - parameters ] ] choice of parameters + + + + + + + + + + + + + + + + + + + +    sfa is basically parameter - free except for the general choice of the class of nonlinear basis functions .",
    "the only other parameters to choose in this method are the dimension @xmath36 and the time delay @xmath35 of the embedding vectors .",
    "the value of @xmath53 is a fairly reliable indicator for a good choice of values for @xmath36 and @xmath35 .",
    "the smaller @xmath53 the better , since there is no trivial way of achieving small @xmath53-values ( except if the number of basis functions comes close to the number of data points ) .",
    "i typically test a certain range of @xmath35-values and successively increase the @xmath36-value until performance ( measured in terms of @xmath53 or @xmath54 ) is satisfactory .",
    "[ [ rarely - varying - driving - forces ] ] rarely varying driving forces + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    if sfa is designed to extract slowly varying features from a signal , how about driving forces that apparently violate this assumption heavily , e.g.  if they vary abruptly and jump between different discrete values ?",
    "the definition of slowness given in ( [ eq : slowness ] ) does actually not depend on the smoothness of the output signal , it may equally well be a signal that switches abruptly between different discrete values as long as the jumps occur rarely enough to lead to the same variance of the time derivative .",
    "whether the order of the values can be recovered depends on the dynamics of the driving force .",
    "sfa will tend to order the values such that jumps occur between nearby values of the estimated driving force .",
    "thus , if the original values are -0.5 , 0 , and + 0.5 and there are many direct jumps between -0.5 and + 0.5 , sfa might map the three values onto -1 , + 1 , and 0 , respectively , thereby changing the order of the second and third value .",
    "figures  [ fig : stepstent ] and  [ fig : stepslogistic ] show results for the two systems if a rarely instead of a slowly and smoothly varying driving force is used .",
    "[ [ high - dimensional - input - data ] ] high - dimensional input data + + + + + + + + + + + + + + + + + + + + + + + + + + +    probably the most severe limitation of sfa is the fact that the number of monomials ( or any other basis functions ) grows quickly with the dimensionality of the embedding vectors ( curse of dimensionality ) .",
    "one therefore might run out of computer memory before a high enough @xmath36-value is reached .",
    "however , higher - dimensional problems could be dealt with in a hierarchical fashion by breaking the embedding vectors into smaller parts which are first analyzed separately and the results of which are then combined for a final analysis . applying this hierarchical scheme to 65-dimensional input vectors has been demonstrated in  @xcite .    [",
    "[ accuracy - of - the - estimated - driving - force ] ] accuracy of the estimated driving force + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    it might be surprising that in the examples above sfa is able to estimate the driving forces with such an accuracy up to a factor and a constant offset , even though the estimation is undetermined up to any invertible transformation , not only scaling and shift .",
    "one reason for that is that the driving forces used are relatively slow already and could probably not be improved much by an invertible nonlinear transformation .",
    "however , even if that were not the case one might hope that in practice for the lower - dimensional embedding vectors ( @xmath46 instead of , e.g. , @xmath55 ) sfa has to find a relatively simple input - output function @xmath4 , which is more likely to preserve the exact shape of the driving - force curve than a more complicated one .",
    "further experiments are needed to verify this .",
    "[ [ noise - sensitivity ] ] noise sensitivity + + + + + + + + + + + + + + + + +    how sensitive is the method to noise ?",
    "one might suspect that sfa is quite sensitive to noise , since eigenvectors of smallest eigenvalues are used .",
    "however , focusing on small eigenvalues is only done after sphering and taking the time derivative , so that small but quickly varying noise components typically have large eigenvalues and are discarded by sfa . graceful degradation with noise",
    "was also found in  @xcite .",
    "thus , using the eigenvectors with smallest eigenvalues in itself does not induce noise sensitivity .",
    "i have done simulations with the examples presented here by adding gaussian white noise to the signals before embedding .",
    "adding 10% , 20% , and 50% noise to the tent map time series reduced the correlation between true and estimated driving force from @xmath48 to about 0.94 , 0.90 , and 0.71 , respectively ; adding 2% , 5% , and 10% noise to the logistic map time series reduced the correlation from @xmath52 to about 0.97 , 0.87 , and 0.70 , respectively .",
    "thus we see , that in these examples the method is in fact fairly robust with respect to noise .",
    "[ [ multiple - driving - forces ] ] multiple driving forces + + + + + + + + + + + + + + + + + + + + + + +    sfa can be easily extended to the extraction of multidimensional output signals  @xcite , which could be used to estimate multiple driving forces .",
    "however , this might require longer time series and higher - dimensional embedding vectors .",
    "if the different driving forces are not clearly separated by different time scales , sfa might only be able to estimate them up to a linear mixing transformation .",
    "in that case independent component analysis ( ica ) might be able to separate them , if they are statistically independent and not more than one is gaussian  ( see * ? ? ?",
    "* for an overview of ica methods ) .",
    "in this paper i have demonstrated that slow feature analysis ( sfa ) can be applied to the problem of estimating a driving force of a nonstationary time series .",
    "the estimates are fairly accurate except for a scaling factor and a constant offset , which can not be extracted in general .",
    "the method works for slowly as well as rarely varying driving forces , although in the latter case the order of the different discrete values might not be recoverable .",
    "the slowness principle also provides robustness to noise , which is typically quickly varying and therefore suppressed by sfa as far as possible .",
    "there are still a number of open questions .",
    "the next steps of investigation will have to include a comparison with standard methods , such as the technique of recurrence plots , and an exploration of the conditions under which sfa can be applied with similar success as demonstrated here .",
    "it is also necessary to apply sfa to real world data .",
    "this has been successfully done in the context of learning receptive field properties of the visual cortex based on natural image sequences @xcite , but that was not for extracting driving forces . in any case , since sfa works very differently from other techniques that have been applied to the estimation of driving forces , one can hope that it at least complements these other techniques in some cases .",
    "i am grateful to hanspeter herzel and isao tokuda for useful hints and hanspeter herzel also for critically reading the manuscript .",
    "this work has been supported by the volkswagenstiftung .",
    "( 2003 ) . http://itb.biologie.hu-berlin.de/~wiskott/abstracts/berkwisk2003a.html[slow feature analysis yields a rich repertoire of complex - cell properties ] .",
    "cognitive sciences eprint archive ( cogprints ) 2804 , http://cogprints.ecs.soton.ac.uk / archive/00002804/.                  ( 1998 ) .",
    "http://itb.biologie.hu-berlin.de/~wiskott/abstracts/wis98a.html[learning invariance manifolds ] . in _ proc .",
    "5th joint symp .  on neural computation , san diego _ , pages 196203 .",
    "university of california , san diego .",
    "http://itb.biologie.hu-berlin.de/~wiskott/abstracts/wissej2002.html[slow feature analysis : unsupervised learning of invariances ] .",
    "http://neco.mitpress.org/cgi/content/abstract/14/4/715[_neural computation _ , 14(4):715770 ] ."
  ],
  "abstract_text": [
    "<S> slow feature analysis ( sfa ) is a new technique for extracting slowly varying features from a quickly varying signal . </S>",
    "<S> it is shown here that sfa can be applied to nonstationary time series to estimate a single underlying driving force with high accuracy up to a constant offset and a factor . </S>",
    "<S> examples with a tent map and a logistic map illustrate the performance . </S>"
  ]
}