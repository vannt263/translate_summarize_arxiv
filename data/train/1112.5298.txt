{
  "article_text": [
    "loopy belief propagation @xcite is a well - known algorithm to approximate marginals of the gibbs distribution defined by an undirected graphical model . for acyclic graphs",
    ", bp always converges and yields the exact marginals . for graphs with cycles ,",
    "it is not guaranteed to converge but when it does , it often yields surprisingly good approximations of the true marginals .",
    "one informal argument for this is that at a bp fixed point , marginals are exact in every sub - tree of the factor graph @xcite .",
    "attempts to understand loopy bp has generated a large body of literature , see e.g.  the survey @xcite .",
    "bp has a modification , known as the max - product bp , where summations are replaced with maximizations . in statistical mechanics terminology , this can be understood as the zero - temperature limit of the ordinary bp .",
    "max - product bp computes ( or approximates ) max - marginals rather than ordinary marginals .    after the discovery @xcite that bp fixed points coincide with stationary points of the bethe free energy , several researchers proposed provably convergent algorithms to find a local minimum of the bethe free energy @xcite .",
    "these algorithms have been proposed only for the sum - product and their possible extension to the max - product is not obvious .",
    "we reformulate the double - loop algorithm @xcite by heskes such that taking its zero - temperature limit becomes straightforward , which results in an algorithm that always converges to a max - product bp fixed point .",
    "the inner loop of the algorithm is max - sum diffusion @xcite .",
    "we empirically observed that with a uniform initialization , the algorithm always yielded the same approximation of ground states that would be obtained by max - sum diffusion ( or other algorithms for map inference based on lp relaxation , such as trw - s @xcite ) .",
    "thus , it combines the complementary advantages of max - sum belief propagation and lp relaxation : unlike the former , it yields good approximation of ground states and , unlike the latter , it yields a good approximation of max - marginals .",
    "the text is organized as follows .",
    "we first (  [ sec : gibbs ] ) review the basics of inference in graphical models .",
    "we thoroughly discuss the zero - temperature limit of the gibbs distribution and related quantities and how to obtain their approximation by variational inference .",
    "then we review two basic cases of variational inference , with a convex free energy (  [ sec : diffusion ] ) and with the bethe free energy (  [ sec : bp ] ) .",
    "then (  [ sec : diffusion : inf ] ,  [ sec : bp : inf ] ) we discuss their zero - temperature limits in detail .",
    "finally (  [ sec : double - loop ] ) we reformulate the double - loop algorithm @xcite and modify it for the zero temperature .",
    "let @xmath0 be a set of variables , each variable @xmath1 taking states @xmath2 from a finite domain @xmath3 .",
    "an assignment to a variable subset @xmath4 is @xmath5 , where @xmath6 is the cartesian product of domains @xmath3 for @xmath7 . in particular",
    ", @xmath8 is an assignment to all the variables .",
    "let @xmath9 , thus @xmath10 is a hypergraph .",
    "each variable @xmath1 and hyperedge @xmath11 is assigned a potential function @xmath12 and @xmath13 , respectively , where @xmath14 .",
    "all numbers @xmath15 and @xmath16 are understood as a single vector @xmath17 ( or mapping @xmath18 ) with @xmath19 the gibbs probability distribution over the hypergraph @xmath10 is given by @xmath20\\ ] ] where the mapping @xmath21 is such that @xmath22 for infinite weights , we set @xmath23 in the scalar product @xmath24 .",
    "since unary terms are included in   explicitly , we assume that @xmath25 contains no singletons .",
    "the distribution is normalized by the _ log - partition function _",
    "@xmath26    in  , we used @xmath27 to denote the _ log - sum - exp operation_. it will be useful to keep in mind algebraic properties of this operation .",
    "it is associative and commutative , and addition distributes over it .",
    "thus , @xmath28 is a commutative semiring .",
    "this semiring is , via the logarithm map , isomorphic to the ` sum - product ' semiring @xmath29 .",
    "[ [ marginals . ] ] marginals .",
    "+ + + + + + + + + +    the marginals of the distribution are @xmath30 where we abuse notation by writing @xmath31 instead of @xmath32 .",
    "the numbers   are understood as a vector @xmath33^i$ ] .",
    "all realizable marginal vectors @xmath34 form the _ marginal polytope _",
    "@xmath35 , where @xmath36 .",
    "besides ( an exponential number of ) other constraints , @xmath34  satisfies normalization and marginalization constraints @xmath37 all vectors @xmath38 satisfying   form the _ local marginal polytope _ @xmath39 .",
    "we have @xmath40 , with equality if and only if hypergraph @xmath10 is acyclic ( i.e. , its factor graph is a tree ) .",
    "we also introduce a symbol for log - marginals , @xmath41 ( and similarly for @xmath42 ) . for log - marginals , constraints",
    "read @xmath43    [ [ reparameterizations . ] ] reparameterizations .",
    "+ + + + + + + + + + + + + + + + + + + +    a _ reparameterization _ is an affine transformation of vector @xmath44 that preserves   for all assignments @xmath45 .",
    "we first define the _ local reparameterization _ on a pair @xmath46 as follows : subtract an arbitrary unary function @xmath47 from @xmath48 and add the same function to @xmath49 , @xmath50 this preserves   because @xmath51 cancels out .",
    "we understand   as ` passing a message ' @xmath51 . applying local reparameterization   to all pairs @xmath46 with @xmath52 yields the general reparameterization @xmath53 where @xmath54 is the vector of all messages and the transformed vector @xmath44 is denoted @xmath55 . thus @xmath56 .",
    "in fact , we have more generally @xmath57 for all @xmath34 satisfying   and all @xmath58 .",
    "reparameterizations can be done either by directly modifying the vector @xmath44 or by keeping @xmath44 unchanged and storing the messages @xmath58 . while the former may be better for theoretical analysis ,",
    "the latter is preferable in algorithms . in the sequel we freely switch between these two views .      in this section",
    ", we will use @xmath59 and @xmath60 , @xmath61 to explicitly denote the dependence of distribution   and its marginals on  @xmath44 .    in statistical physics , the gibbs distribution is usually considered in a more general form as @xmath62 , where @xmath63 is the inverse temperature @xcite .",
    "the limit @xmath64 is then known as the _ zero - temperature limit_.    it is elementary to show that the distribution @xmath65 is zero everywhere except at _ ground states _ , which are the maximizers of @xmath59 or , equivalently , @xmath24 .",
    "if there are multiple ground states then the mass is distributed evenly among them .",
    "the zero - temperature limit of the log - partition function is @xmath66 which follows from the limit @xmath67 the zero - temperature limit of log - marginals   yields _ max - marginals _",
    "@xmath68 ( similarly for @xmath69 ) .",
    "observe that   and   differs from   and   only by replacing the log - sum - exp operation ` @xmath70 ' with ` @xmath71 ' .",
    "this corresponds , by the limit  , to transition from the semiring @xmath28 to the max - sum semiring @xmath72 .",
    "similarly , max - marginals satisfy normalization and marginalization conditions in which ` @xmath70 ' has been replaced with ` @xmath71 ' .",
    "max - marginals should not be confused with the marginals of @xmath73 .",
    "these are different quantities and one can not be computed from the other .    [",
    "[ recovering - ground - states - from - max - marginals . ] ] recovering ground states from max - marginals .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    ground states can be recovered from max - marginals . to show that",
    ", we first recall what is the _ constraint satisfaction problem _ ( csp ) @xcite . the csp instance is defined by a vector @xmath74 , where functions @xmath75 and @xmath76 are understood as relations .",
    "a solution of the csp is an assignment @xmath77 satisfying all the relations , i.e. , @xmath78 for all @xmath1 and @xmath79 for all @xmath11 .    for a vector @xmath17",
    "we define vector @xmath80 by @xmath81 i.e. , a component of @xmath82 equals @xmath83 iff the corresponding component of @xmath44 is maximal in its potential function .",
    "we say that such a components of @xmath44 is _",
    "active_. now the set @xmath84 of ground states is the solution set of the csp defined by vector @xmath85 of active max - marginals .",
    "let @xmath86 denote the entropy of the distribution as a function of its marginals .",
    "the functions @xmath87 and @xmath88 are convex and they are related by convex conjugacy , @xmath89 , \\label{eq : conjugacy}\\ ] ] where the optimum is attained for @xmath34 equal to the marginals . in statistical physics ,",
    "the quantity @xmath90 is known as the _ gibbs free energy _ of the system . by taking the limit @xmath64 of the expression @xmath91 \\label{eq : conjugacy : beta}\\ ] ] we similarly obtain @xmath92 and max - marginals @xmath93 .",
    "the idea behind _ variational inference _",
    "@xcite is to replace the marginal polytope @xmath35 and the entropy @xmath86 in   with their tractable approximations .",
    "then the optimal value and the optimal argument of   is an approximation of the log - partition function and marginals , respectively . for @xmath64 ,    * the optimal value of   is an approximation of @xmath92 , * the logarithm of the optimal argument of   is an approximation of max - marginals @xmath93 , * the solution set of the csp defined by active approximate max - marginals is an approximation of the set @xmath84 of ground states .    as the entropy term in   approaches @xmath94 for @xmath64 , one may think that it could be simply omitted .",
    "however , as pointed out in  @xcite , if the approximate entropy is non - convex ( such as the bethe entropy ) , the problem   can have multiple local minima for arbitrarily large @xmath95 .",
    "thus , if our algorithm finds only a local minimum of  , the entropy term is crucial .",
    "let the marginal polytope in   be approximated by the local marginal polytope @xmath39 and the true entropy by @xmath96 . this entropy approximation is concave , thus we obtained a simple ( arguably , the simplest possible ) variational inference method with a convex free energy @xcite .",
    "the approximation of   now reads @xmath97    the problem   can be solved as described e.g.  in @xcite .",
    "its dual reads as follows : find a reparameterization of the original vector @xmath44 that minimizes the function @xmath98 this is a majorant of the log - partition function , @xmath99 for every @xmath44 .",
    "a  sufficient condition for dual optimality is that @xmath100 for all @xmath101 and @xmath102 .",
    "the primal and dual optimum are related by @xmath103 since function is convex and differentiable , its global minimum over reparameterizations of @xmath44 can be found by coordinate descent .",
    "this leads to a simple message passing algorithm .",
    "the iteration of this algorithm enforces equality for a single pair @xmath46 by local reparameterization  , which determines  @xmath104 in   uniquely .",
    "the iteration decreases @xmath105 , and this decrease is strict unless @xmath105 is already minimal . on convergence , holds globally .",
    "if reparameterizations are represented by messages rather than by directly modifying @xmath44 , the dual of reads @xmath106 and the coordinate descent becomes algorithm  [ alg : diffusion ] . to correctly handle infinite weights , the algorithm expects that @xmath107\\leftrightarrow[\\max_{x_{a\\setminus v } } \\theta_a(x_a)>-\\infty]$ ] for all @xmath101 and @xmath102 .",
    "@xmath108 $ ]      the zero - temperature limit of the optimization problem above is obtained by replacing @xmath44 with @xmath109 and taking the limit @xmath64 .",
    "this results in replacing ` @xmath70 ' with ` @xmath71 ' in   and algorithm  [ alg : diffusion ] .",
    "we assume that this has been done .",
    "this yields the lp relaxation approach to maximizing the gibbs distribution first proposed by schlesinger et al.@xcite , see also @xcite . in these works ,",
    "the zero - temperature limit of algorithm  [ alg : diffusion ] is called _ max - sum diffusion_.    let function   after replacing ` @xmath70 ' with ` @xmath71 ' be denoted by @xmath110 we have @xmath111 for every @xmath44 . algorithm  [ alg : diffusion ] tries to minimize @xmath112 by reparameterizing @xmath44 .",
    "however , the function @xmath113 is non - differentiable now  therefore algorithm  [ alg : diffusion ] may converge only to a local ( with respect to coordinate moves ) minimum of @xmath112 . while it is easy to prove convergence of the algorithm in value , convergence in argument is only a conjecture to date @xcite and",
    "only a weaker property has been proved recently @xcite .    according to   [ sec : var - inference ] ,",
    "when @xmath44 is optimal then @xmath112 is an approximation of @xmath92 and   is an approximation of the max - marginals  @xmath93 .",
    "note that the approximate max - marginals   are , up to normalization , directly equal to  @xmath44 . since vector @xmath82 is not affected by normalization of @xmath44 , the solution set of the csp  @xmath82 is an approximation of the ground states .    but this is in agreement with @xcite , where it is shown that the inequality @xmath114 ( and hence the lp relaxation ) is tight if and only if the csp defined by @xmath82 has a solution",
    ". then , @xmath115 for every solution @xmath77 of csp @xmath82 .",
    "there are two important problem subclasses for which the lp relaxation is tight : if hypergraph @xmath10 is acyclic or if the functions @xmath49 are ( permuted ) supermodular @xcite .",
    "besides , it is tight for many other instances met in applications .",
    "this makes this method very suitable for approximating ground states , which has been also observed empirically @xcite .    however , even when the lp relaxation is tight , are a very poor approximation of max - marginals .",
    "they are inexact even for acyclic hypergraphs .",
    "let the true entropy in   be approximated by the _ bethe entropy _",
    "@xmath116 where @xmath117 is the number of hyperedges containing variable @xmath118 . for acyclic",
    "hypergraphs the bethe entropy is equal to @xmath86 , otherwise it can be non - concave and even negative on  @xmath39 . then reads @xmath119 .",
    "\\label{eq : primal - bethe}\\ ] ] the negative objective of   is the _ bethe free energy_.    next we formulate loopy belief propagation . unlike in the ` traditional ' formulation @xcite , we identify messages with reparameterizations , which agrees with ( * ? ?",
    "* eq.(2 ) ) and @xcite .",
    "let the marginals   be approximated as @xmath120 where @xmath121 note that @xmath122 and @xmath123 is the gibbs distribution for the simple graphical model with hypergraph @xmath124 and @xmath125 , respectively .",
    "this corresponds to decomposing @xmath10 into small sub - hypergraphs . in general",
    ", @xmath34 fails to satisfy the local marginalization conditions of  . plugging   into these conditions yields",
    "@xmath126 = \\theta_v(x_v ) + { \\rm const}_{av } , \\label{eq : fixed : bp : nocancel}\\ ] ] which by cancelling @xmath15 simplifies to @xmath127 = { \\rm const}_{av } .",
    "\\label{eq : fixed : bp}\\ ] ] here , @xmath128 is a constant independent on @xmath2 .",
    "we define a _ belief propagation fixed point _ to be a vector  @xmath44 satisfying   for all @xmath101 and @xmath102 .",
    "the bp algorithm then tries to reparameterize @xmath44 to make it satisfy  .    as discovered by yedidia et al .",
    "@xcite , bp fixed points   correspond to stationary points of problem   via the map  .",
    "heskes @xcite showed that every _",
    "stable _ bp fixed point is a local maximum ( rather than minimum or saddle point ) of  , but not necessarily _ vice versa_.      in the zero - temperature limit , ` @xmath70 ' in   is replaced with ` @xmath71 ' .",
    "we assume in   [ sec : bp : inf ] that this has been done .",
    "then , defines a fixed point of _ max - sum belief propagation _ rather than ( as is usual ) in the ( isomorphic ) sum - product semiring @xmath29 . for zero temperature ,",
    "we are then in the max - sum semiring @xmath129 rather than in the max - product semiring @xmath130 . ] .    according to   [ sec : var - inference ] ,",
    "numbers   are approximations of max - marginals @xmath93 and the solution set of the csp defined by active approximate max - marginals is an approximation of the set @xmath84 of ground states . since approximate max - marginals",
    "are , up to normalization , equal to numbers  , this csp is defined by @xmath131 .",
    "this formulation is consistent because ( as is easy to verify ) the value @xmath24 is the same for all solutions @xmath77 of the csp @xmath131 .",
    "it is well - known that the approximation of ground states obtained by max - sum belief propagation is often poor ( letting alone that the algorithm may not converge ) . in our formalism , the value @xmath24 for the solutions @xmath77 of csp @xmath131 are often far are inevitably ground states . ] from @xmath92 .",
    "it may of course also happen that the csp @xmath131 has no solution .",
    "the situation is especially intriguing if the functions @xmath49 are supermodular , csp @xmath131 always has a solution .",
    "this is easy to prove : since function @xmath49 are supermodular , functions @xmath132 are supermodular as well , and then the proof proceeds like the proof @xcite that max - sum diffusion exactly solves ( permuted ) supermodular problems . ] .",
    "then maximizing @xmath24 is tractable but the approximation obtained from max - sum bp can be inexact @xcite .    on the other hand ,",
    "if the approximation of ground states from max - sum bp is good , then usually also the approximation   of max - marginals is good .",
    "this is intuitively justified by the fact that at a bp fixed point , the ( max-)marginals are exact in every subtree of the factor graph @xcite .",
    "heskes @xcite proposed a class of convergent algorithms to find a local minimum of bethe and kikuchi free energies , based on the _ minorize - maximize approach _ @xcite . we now describe a simple representant of this class , which finds a local maximum of the non - concave maximization problem  .",
    "let @xmath133 denote the objective of .",
    "a family of minorants of @xmath134 is constructed as @xmath135 where @xmath136 is a collections of variable distributions @xmath137 , non - negative and normalized . for any @xmath34 and @xmath136 we have @xmath138 , with equality if and only if @xmath139 for all @xmath1 .",
    "this follows from the well - known fact that any non - negative and normalized vectors @xmath122 and @xmath137 satisfy @xmath140 , which holds with equality only if @xmath139 .    the problem   is now split into two nested problems @xmath141 the inner problem is a concave maximization , which can be solved optimally  in fact , it has the form  . the objective @xmath142 of the outer problem is a non - concave function of @xmath136 and thus we can only hope to find its local maximum .",
    "the algorithm has two nested loops , corresponding to the inner and outer problem .",
    "the outer iteration has two steps :    1 .",
    "keeping @xmath136 fixed , find @xmath143 that maximizes @xmath144 .",
    "2 .   for all",
    "@xmath1 , set @xmath145 .",
    "each of these two steps increases @xmath144 . for step 1 ,",
    "this is true by definition . for step 2 ,",
    "it follows from the minorization property of @xmath146 .",
    "the algorithm converges to a state when @xmath34 is the maximum of @xmath144 and @xmath139 , therefore @xmath34 is a local maximum of  .    in step 1",
    ", @xmath144 needs to be maximized over @xmath143 .",
    "this can be cast in the form  .",
    "first we substitute @xmath147 .",
    "note that after this substitution , the normalization condition @xmath148 reads @xmath149 .",
    "then @xmath150 where , using that @xmath151 , the vector @xmath152 is given by as @xmath153 , @xmath154 .",
    "but since   directly compares to  , the choice   more clearly shows the connection with bp fixed points . ]",
    "@xmath155    the inner problem is dualized , which changes   to a saddle - point problem . as described in   [ sec : diffusion ] , the dual is solved by reparameterizing @xmath152 such that @xmath156 satisfies   ( which minimizes @xmath157 ) and then computing @xmath34 from @xmath152 using  . since @xmath158 , we can reparameterize  @xmath44 instead of  @xmath156 .",
    "the outer iteration now reads as follows :",
    "reparameterize @xmath44 such that @xmath159 = \\theta_v(x_v ) .",
    "\\label{eq : step1-reparam}\\ ] ] 2 .   for all @xmath1 , set @xmath160 .",
    "the number @xmath161\\ ] ] is decreased by step 1 and it is increased by steps 1 + 2 combined .",
    "the algorithm converges to a state when @xmath162 .",
    "then , @xmath44 is a bp fixed point .",
    "this is indeed very obvious : since @xmath163 and @xmath48 are equal up to an additive constant , becomes the same as  , therefore   holds . if reparameterizations are represented by messages , we obtain algorithm  [ alg : double - loop ] .",
    "let us remark that the normalization in step  2 is not necessary , we could just set @xmath164 .",
    "this would not affect convergence to a bp fixed point but @xmath165 would lose its meaning and @xmath166 might grow unbounded .",
    "choose any @xmath167 with @xmath168 .",
    "choose any @xmath58 .",
    "@xmath169 \\big]$ ] for all @xmath1 , set @xmath170 . [",
    "line : outer ]    the outer loop is guaranteed to converge only if the inner loop reaches full convergence .",
    "there is no theoretical guarantee ensuring convergence with a finite number of inner iterations  this unpleasant feature is common to double - loop algorithms applied to saddle - point problems .",
    "however , this does not seem to be an issue in practice .      replacing @xmath44 with @xmath171 in all the formulas and taking the limit @xmath64 again results in replacing ` @xmath70 ' with ` @xmath71 ' .",
    "then , algorithm  [ alg : double - loop ] converges to a max - sum belief propagation fixed point .",
    "though we never observed the algorithm fail to converge , its convergence ( with the inner loop run to full convergence ) is only a conjecture .",
    "the argument is that if it converges for any @xmath172 then it is reasonable to assume that it will converge also in the limit .",
    "but we suspect that finding a formal proof for @xmath64 may be difficult , especially when convergence of the inner loop ( max - sum diffusion ) itself is a conjecture to date .",
    "note that , unlike for @xmath172 , the proof can not be based on the fact that the value of @xmath173 monotonically decreases because it often remains constant after the first several outer iterations .    [ [ uniform - initialization . ] ] uniform initialization .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    depending on the initial @xmath166 , the algorithm can converge to different fixed points ( as we indeed observed ) .",
    "particularly interesting is the case when the initial @xmath166 are all uniform  due to the normalization condition @xmath174 , this means @xmath175 .",
    "next we focus only on this case .",
    "figure  [ fig : resid ] shows how the algorithm converged for different types of pairwise interactions and different types of graph .",
    "occasionally ( e.g. , for repulsive interactions and two labels ) the residuals approached zero non - monotonically .",
    "the inner iteration was run to almost full convergence , however the results were not qualitatively affected by this .    [ cols=\"^,^,^ \" , ]     we made the following key observation :    this observation is only empirical , currently we have neither a formal proof nor a counterexample .",
    "it has an important consequence .",
    "if initially @xmath175 , then the first outer iteration is just algorithm  [ alg : diffusion ] applied to @xmath176 . if all subsequent outer iterations do not change  @xmath131 , then csp  @xmath131 after convergence of algorithm  [ alg : double - loop ] is the same as csp  @xmath82 that would be obtained by running algorithm  [ alg : diffusion ] on @xmath44 .",
    "thus , the approximate ground states obtained by algorithm  [ alg : double - loop ] are the same as those obtained by algorithm  [ alg : diffusion ] . however , since algorithm  [ alg : double - loop ] converges to a max - sum bp fixed point , approximate max - marginals obtained by algorithm  [ alg : double - loop ] are expected to be much more accurate than those obtained by algorithm  [ alg : diffusion ] .",
    "we showed in   [ sec : diffusion : inf ] and   [ sec : bp : inf ] that the properties of max - sum diffusion ( and all map inference algorithms based on lp relaxation ) and max - sum belief propagation are complementary : the former yields good approximation of ground states but poor approximation of max - marginals , the latter _",
    "vice versa_. the double - loop algorithm initialized with @xmath175 combines advantages of both : it yields approximate ground states that are exact for supermodular problems and approximate max - marginals that are exact in every sub - tree of the factor graph .",
    "we have not pursued another potentially interesting application of the double - loop algorithm with non - uniform initialization @xmath177 .",
    "it is known that max - sum bp occasionally yields better approximate ground states than lp relaxation .",
    "this has been observed e.g.  for some problems on highly connected graphs @xcite .",
    "however , the max - sum bp algorithm does not always converge , thus the convergent double loop algorithm might be useful here ."
  ],
  "abstract_text": [
    "<S> after the discovery that fixed points of loopy belief propagation coincide with stationary points of the bethe free energy , several researchers proposed provably convergent algorithms to directly minimize the bethe free energy . </S>",
    "<S> these algorithms were formulated only for non - zero temperature ( thus finding fixed points of the sum - product algorithm ) and their possible extension to zero temperature is not obvious . </S>",
    "<S> we present the zero - temperature limit of the double - loop algorithm by heskes , which converges a max - product fixed point . the inner loop of this algorithm is max - sum diffusion . under certain conditions , </S>",
    "<S> the algorithm combines the complementary advantages of the max - product belief propagation and max - sum diffusion ( lp relaxation ) : it yields good approximation of both ground states and max - marginals . </S>"
  ]
}