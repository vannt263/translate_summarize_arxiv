{
  "article_text": [
    "principal components analysis ( pca ) , factor analysis ( fa ) and independent components analysis ( ica ) are models which explain observed data , @xmath3 , in terms of a linear superposition of independent hidden factors , @xmath4 , so @xmath5 where @xmath6 is the factor loading matrix and @xmath7 is a noise vector , usually assumed to be gaussian . these algorithms can be expressed in terms of performing inference in appropriate probabilistic models .",
    "the latent factors are usually considered as random variables , and the mixing matrix as a parameter to estimate . in both pca and fa the latent factors are given a standard ( zero mean , unit variance ) normal prior .",
    "in pca the noise is assumed to be isotropic , whereas in fa the noise covariance is only constrained to be diagonal .",
    "a standard approach in these models is to integrate out the latent factors and find the maximum likelihood estimate of the mixing matrix . in ica",
    "the latent factors are assumed to be heavy - tailed , so it is not usually possible to integrate them out . in this paper",
    "we take a fully bayesian approach , viewing not only the hidden factors but also the mixing coefficients as random variables whose posterior distribution given data we aim to infer .",
    "sparsity plays an important role in latent feature models , and is desirable for several reasons .",
    "it gives improved predictive performance , because factors irrelevant to a particular dimension are not included .",
    "sparse models are more readily interpretable since a smaller number of factors are associated with observed dimensions . in many real world situations",
    "there is an intuitive reason why we expect sparsity : for example , in gene regulatory networks a transcription factor will only regulate genes with specific motifs . in our previous work [ @xcite ] we investigated the use of sparsity on the latent factors @xmath8 , but this formulation is not appropriate in the case of modeling gene expression , where , as described above , a transcription factor will regulate only a small set of genes , corresponding to sparsity in the factor loadings , @xmath6 . here",
    "we propose a novel approach to sparse latent factor modeling where we place sparse priors on the factor loading matrix , @xmath6 .",
    "the bayesian factor regression model of @xcite is closely related to our work in this way , although the hierarchical sparsity prior they use is somewhat different .",
    "an alternative `` soft '' approach to incorporating sparsity is to put a @xmath9 ( usually exponential , i.e. , @xmath10 ) prior on the precision of each element of @xmath6 independently , resulting in the elements of @xmath6 being marginally student-@xmath11 distributed a priori ; see @xcite , @xcite , and @xcite . a lasso - based approach to generating a sparse factor loading has also been developed [ @xcite ; @xcite ] .",
    "we compare these sparsity schemes empirically in the context of gene expression modeling .",
    "a problematic issue with this type of model is how to choose the latent dimensionality of the factor space , @xmath12 .",
    "model selection can be used to choose between different values of @xmath12 , but generally requires significant manual input and still requires the range of @xmath12 over which to search to be specified .",
    "@xcite applied reversible jump mcmc to pca , which has many of the advantages of our approach : a posterior distribution over the number of latent dimensions can be approximated , and the number of latent dimensions could potentially be unbounded .",
    "however , rj mcmc is considerably more complex to implement for sparse factor analysis than our proposed framework .",
    "we use the indian buffet process [ @xcite ] , which defines a distribution over infinite binary matrices , to provide sparsity and a framework for inferring the appropriate latent dimension of the data set using a straightforward gibbs sampling algorithm . the indian buffet process ( ibp ) allows a unbounded number of latent factors , so we do not have to specify a maximum number of latent dimensions a priori .",
    "we denote our model `` nsfa '' for `` nonparametric sparse factor analysis . ''",
    "our model is closely related to that of @xcite , and is a  simultaneous development .",
    "we will define our model in terms of equation ( [ eq : fa ] ) .",
    "let  @xmath13 be a binary matrix whose @xmath14th element represents whether observed dimension @xmath15 includes any contribution from factor @xmath16 . we then model the mixing matrix by @xmath17 where @xmath18 is the inverse variance ( precision ) of the @xmath16th factor and @xmath19 is a delta function ( point - mass ) at 0",
    ". distributions of this type are sometimes known as `` spike and slab '' distributions .",
    "we allow a potentially infinite number of hidden sources , so that @xmath13 has infinitely many columns , although only a  finite number will have nonzero entries .",
    "this construction allows us to use the ibp to provide sparsity and define a generative process for the number of latent factors .",
    "we assume independent gaussian noise , @xmath7 , with diagonal covariance matrix @xmath20 .",
    "we find that for many applications assuming isotropic noise is too restrictive , but this option is available for situations where there is strong prior belief that all observed dimensions should have the same noise variance .",
    "the latent factors , @xmath8 , are given gaussian priors .",
    "figure [ fig : iica ] shows the complete graphical model .",
    "we now define our infinite model by taking the limit of a series of finite models .",
    "we derive the distribution on @xmath13 by defining a finite @xmath12 model and taking the limit as @xmath21 .",
    "we then show how the infinite case corresponds to a simple stochastic process .",
    "we have @xmath22 dimensions and @xmath12 hidden sources .",
    "recall that @xmath23 of matrix  @xmath13 tells us whether hidden source @xmath16 contributes to dimension @xmath15 .",
    "we assume that the probability of a source @xmath16 contributing to any dimension is @xmath24 , and that the rows are generated independently .",
    "we find @xmath25 where @xmath26 is the number of dimensions to which source @xmath16 contributes .",
    "the inner term of the product is a binomial distribution , so we choose the conjugate beta@xmath27 distribution for @xmath24 .",
    "for now we take @xmath28 and @xmath29 , where @xmath30 is the strength parameter of the ibp .",
    "the model is defined by @xmath31 due to the conjugacy between the binomial and beta distributions , we are able to integrate out @xmath32 to find @xmath33 where @xmath34 is the gamma function .      griffiths and ghahramani ( @xcite )",
    "define a scheme to order the nonzero rows of @xmath13 which allows us to take the limit @xmath21 and find @xmath35 where @xmath36 is the number of active features ( i.e. , nonzero columns of @xmath13 ) , @xmath37 is the @xmath22th harmonic number , and @xmath38 is the number of rows whose entries correspond to the binary number @xmath39 .",
    "this distribution corresponds to a simple stochastic process , the indian buffet process .",
    "consider a buffet with a seemingly infinite number of dishes ( hidden sources ) arranged in a line .",
    "the first customer ( observed dimension ) starts at the left and samples @xmath40 dishes .",
    "the @xmath41th customer moves from left to right sampling dishes with probability @xmath42 where @xmath43 is the number of customers to have previously sampled dish @xmath16 .",
    "having reached the end of the previously sampled dishes , he tries @xmath44 new dishes .",
    "figure [ fig : ibp1 ] shows two draws from the ibp for two different values of @xmath30 .    [ cols=\"^,^ \" , ]      we assess these algorithms in terms of _ predictive performance _ on the breast cancer data set of @xcite , including 226 genes across 251 individuals .",
    "we find that all the finite models are sensitive to the choice of the number of factors , @xmath12 .",
    "the samplers were found to have converged after around 1000 samples according to standard multiple chain convergence measures , so 3000 mcmc iterations were used for all models .",
    "the predictive log likelihood was calculated using the final 100 mcmc samples .",
    "figure [ fig8](a ) shows test set log likelihoods for 10 random divisions of the data into training and test sets .",
    "factor analysis ( fa ) shows significant overfitting as the number of latent features is increased from 20 to 40 .",
    "using the ard prior prevents this overfitting ( afa ) , giving improved performance when using 20 features and only slightly reduced performance when 40 features are used .",
    "the sparse finite model ( sfa ) shows an advantage over afa in terms of predictive performance as long as underfitting does not occur : performance is comparable when using only 10 features .",
    "however , the performance of sfa is sensitive to the choice of the number of factors , @xmath12 .",
    "the performance of the sparse nonparametric model ( nsfa ) is comparable to the sparse finite model when an appropriate number of features is chosen , but avoids the time consuming model selection process .",
    "fokoue s method ( fok ) was run with @xmath45 and various settings of the hyperparameter @xmath15 which controls the overall sparsity of the solution .",
    "the model s predictive performance depends strongly on the setting of this parameter , with results approaching the performance of the sparse models ( sfa and nsfa ) for @xmath46 .",
    "the performance of bfrm on this data set is noticeably worse than the other sparse models .",
    "we now consider the _ computation cost _ of the algorithms . as described in section [ sec : inference ] , sampling @xmath13 and @xmath6 takes order @xmath47 operations per iteration , and sampling @xmath48 takes @xmath49 .",
    "however , for the moderate values encountered for data sets 1 and 2 , the main computational cost is sampling the nonzero elements of @xmath6 , which takes @xmath50 where @xmath51 is the sparsity of the model .",
    "figure [ fig8](c ) shows the mean cpu time per iteration divided by the number of features at that iteration . naturally , straight fa is the fastest , taking only around @xmath52 per iteration per feature .",
    "the value increases slightly with increasing @xmath12 , suggesting that here the @xmath53 calculation and inversion of @xmath54 , the precision of the conditional on @xmath48 , must be contributing .",
    "the computational cost of adding the ard prior is negligible ( afa ) .",
    "the cpu time per iteration is just over double for the sparse finite model ( sfa ) , but the cost actually decreases with increasing @xmath12 , because the sparsity of the solution increases to avoid overfitting .",
    "there are fewer nonzero elements of @xmath6 to sample per feature , so the cpu time",
    "_ per feature _ decreases .",
    "the cpu time per iteration per feature for the nonparametric sparse model ( nsfa ) is somewhat higher than for the finite model because of the cost of the feature birth and death process .",
    "however , figure [ fig8](b ) shows the absolute cpu time per iteration , where we see that the nonparametric model is only marginally more expensive than the finite model of appropriate size @xmath55 and cheaper than choosing an unnecessarily large finite model ( sfa with @xmath45 , 40 ) .",
    "fokoue s method ( fok ) has comparable computational performance to the sparse finite model , but interestingly has increased cost for the optimal setting of @xmath46 .",
    "the parameter space for fok is continuous , making search easier but requiring a normal random variable for every element of @xmath6 .",
    "bfrm pays a considerable computational cost for both the hierarchical sparsity prior and the dp prior on @xmath48 .",
    "spca was not run on this data set , but results on the synthetic data in section [ sec : syndata ] suggest it is somewhat faster than the sampling methods , but not hugely so .",
    "the computational cost of spca is @xmath56 in the @xmath57 case ( where @xmath58 is the number of iterations to convergence ) and @xmath59 in the @xmath60 case , taking the limit @xmath61 . in either case",
    "an individual iteration of spca is more expensive than one sampling iteration of nsfa ( since @xmath62 ) , but fewer iterations will generally be required to reach convergence of spca than are required to ensure mixing of nsfa .",
    "figure [ fig : big_ll ] shows the predictive performance of afa , fok and nsfa on the prostate cancer data set of @xcite , for ten random splits into training and test data .",
    "the boxplots show variation from ten random splits into training and test data .",
    "the large number of genes ( @xmath63 across @xmath64 individuals ) in this data set makes inference slower , but the problem is manageable since the computational complexity is linear in the number of genes . despite the large number of genes , the appropriate number of latent factors , in terms of maximizing predictive performance , is still small , around 10 ( nsfa infers a median of 12 factors ) .",
    "this may seem small relative to the number of genes , but it should be noted that the genes included in the breast cancer and _ e. coli _ data sets are those capturing the most variability .",
    "surprisingly , sfa actually performed slightly worse on this data set than afa .",
    "both are highly sensitive to the number of latent factors chosen .",
    "nsfa , however , gives better predictive log likelihoods than either finite model for any fixed number of latent factors @xmath12 . running 1000 iterations of nsfa on this data set takes under 8 hours .",
    "bfrm and fok were impractically slow to run on this data set .",
    "we have seen that in both the _ e. coli _ and breast cancer data sets that sparsity can improve predictive performance , as well as providing a more easily interpretable solution .",
    "using the ibp to provide sparsity is straightforward , and allows the number of latent factors to be inferred within a well - defined theoretical framework .",
    "this has several advantages over manually choosing the number of latent factors .",
    "choosing too few latent factors damages predictive performance , as seen for the breast cancer data set .",
    "although choosing too many latent factors can be compensated for by using appropriate ard - like priors , we find this is typically more computationally expensive than the birth and death process of the ibp .",
    "manual model selection is an alternative but is time consuming .",
    "finally , we show that running nsfa on full gene expression data sets with 10000@xmath65 genes is feasible so long as the number of latent factors remains relatively small . an interesting direction for this research",
    "is how to incorporate prior knowledge , for example , if certain transcription factors are known to regulate specific genes .",
    "incorporating this knowledge could both improve the performance of the model and improve interpretability by associating latent variables with specific transcription factors .",
    "another possibility is incorporating correlations in the indian buffet process , which has been proposed for simpler models [ @xcite ; @xcite ] .",
    "this would be appropriate in a gene expression setting where multiple transcription factors might be expected to share sets of regulated genes due to common motifs .",
    "unfortunately , performing mcmc in all but the simplest of these models suffers from slow mixing .",
    "we would like to thank the anonymous reviewers for helpful comments ."
  ],
  "abstract_text": [
    "<S> a nonparametric bayesian extension of factor analysis ( fa ) is proposed where observed data @xmath0 is modeled as a linear superposition , @xmath1 , of a potentially infinite number of hidden factors , @xmath2 . </S>",
    "<S> the indian buffet process ( ibp ) is used as a prior on @xmath1 to incorporate sparsity and to allow the number of latent features to be inferred . </S>",
    "<S> the model s utility for modeling gene expression data is investigated using randomly generated data sets based on a known sparse connectivity matrix for _ e. coli _ , and on three biological data sets of increasing complexity .    and    . </S>"
  ]
}