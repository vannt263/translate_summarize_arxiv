{
  "article_text": [
    "learning linear separators is one of the central challenges in machine learning .",
    "they are widely used and have been long studied both in the statistical and computational learning theory .",
    "a seminal result of @xcite , using tools due to @xcite , showed that @xmath0-dimensional linear separators can be learned to accuracy @xmath1 with probability @xmath2 in the classic pac model in polynomial time with @xmath3 examples .",
    "the best known lower bound for linear separators is @xmath4 , and this holds even in the case in which the distribution is uniform  @xcite . whether the upper bound can be improved to match the lower bound via a polynomial - time algorithm is been long - standing open question , both for general distributions  @xcite and for the case of the uniform distribution in the unit ball  @xcite . in this work",
    "we resolve this question in the case where the underlying distribution belongs to the class of log - concave and nearly log - concave distributions , a wide class of distributions that includes the gaussian distribution and uniform distribution over any convex set , and which has played an important role in several areas including sampling , optimization , integration , and learning  @xcite .",
    "we also consider active learning , a major area of research of modern machine learning , where the algorithm only receives the classifications of examples when it requests them  @xcite .",
    "our main result here is a polynomial - time active learning algorithm with label complexity that is exponentially better than the label complexity of any passive learning algorithm in these settings .",
    "this answers an open question in  @xcite and it also significantly expands the set of cases for which we can show that active learning provides a clear exponential improvement in @xmath5 ( without increasing the dependence on @xmath0 ) over passive learning .",
    "remarkably , our analysis for passive learning is done via a connection to our analysis for active learning  to our knowledge , this is the first paper using this technique .",
    "we also study active and passive learning in the case that the data might not be linearly separable .",
    "we specifically provide new improved bounds for the widely studied tsybakov low - noise condition  @xcite , as well as new bounds on the disagreement coefficient , with implications for the agnostic case ( i.e. , arbitrary forms of noise ) .",
    "* passive learning *   in the classic passive supervised machine learning setting , the learning algorithm is given a set of labeled examples drawn i.i.d .  from some fixed but unknown distribution over the instance space and labeled according to some fixed but unknown target function , and the goal is to output a classifier that does well on new examples coming from the same distribution .",
    "this setting has been long studied in both computational learning theory ( within the pac model  @xcite ) and statistical learning theory  @xcite , and has played a crucial role in the developments and successes of machine learning .",
    "however , despite remarkable progress , the basic question of providing polynomial - time algorithms with _ tight _",
    "bounds on the sample complexity has remained open .",
    "several milestone results along these lines that are especially related to our work include the following .",
    "the analysis of @xcite , proved using tools from  @xcite , implies that linear separators can be learned in polynomial time with @xmath6 labeled examples .",
    "@xcite proved a bound that implies an @xmath4 lower bound for linear separators and explicitly posed the question of providing tight bounds for this class .",
    "@xcite established an upper bound of @xmath7 , which can be achieved in polynomial - time for linear separators .",
    "@xcite achieved polynomial - time learning by finding a consistent hypothesis ( i.e. , a hypothesis which correctly classifies all training examples ) ; this is a special case of erm @xcite .",
    "an intensive line of research in the empirical process and statistical learning theory literature has taken account of `` local complexity '' to prove stronger bounds for erm   @xcite . in the context of learning , local complexity takes account of the fact that really bad classifiers can be easily discarded , and the set of `` local '' classifiers that are harder to disqualify is sometimes not as rich . a recent landmark result of @xcite",
    "( see also  @xcite ) is the bound for consistent algorithms of @xmath8 where @xmath9 is the alexander capacity , which depends on the distribution  @xcite ( see section  [ se : dis ] and appendix  [ rel - appendix ] for further discussion ) .",
    "however , this bound can be suboptimal for linear separators .    in particular , for linear separators in the case",
    "in which the underlying distribution is uniform in the unit ball , the sample complexity is known @xcite to be @xmath10 , when computational considerations are ignored .",
    "@xcite , using the doubling dimension @xcite , another measure of local complexity , proved a bound of @xmath11 for a polynomial - time algorithm . as a lower bound of @xmath12 on @xmath9 for @xmath13 for the case of linear separators and",
    "the uniform distribution is implicit in @xcite , the bound of @xcite given by ( [ e : cap ] ) can not yield a bound better than @xmath14 in this case .    in this paper",
    "we provide a _ tight _ bound ( up to constant factors ) on the sample complexity of polynomial - time learning of linear separators with respect to log - concave distributions . specifically , we prove an upper bound of @xmath15 using a polynomial - time algorithm that holds for any zero - mean log - concave distribution .",
    "we also prove an information theoretic lower bound that matches our ( computationally efficient ) upper bound for _ each _ log - concave distribution .",
    "this provides the first bound for a polynomial - time algorithm that is tight for an interesting non - finite class of hypothesis functions under a general class of data - distributions , and also characterizes ( up to a constant factor ) the distribution - specific sample complexity for each distribution in the class . in the special case of the uniform distribution ,",
    "our upper bound closes the existing @xmath16 gap between the upper bounds ( [ e : doubling ] ) and ( [ e : cap.con ] ) and the lower bound of @xcite .    *",
    "active learning *   we also study learning of linear separators in the active learning model ; here the learning algorithm can access unlabeled ( i.e. , unclassified ) examples and ask for labels of unlabeled examples of its own choice , and the hope is that a good classifier can be learned with significantly fewer labels by actively directing the queries to informative examples .",
    "this has been a major area of machine learning research in the past fifteen years mainly due the availability of large amounts of unannotated or raw data in many modern applications  @xcite , with many exciting developments on understanding its underlying principles as well  @xcite .",
    "however , with a few exceptions  @xcite , most of the theoretical developments have focused on the so called disagreement - based active learning paradigm  @xcite ; methods and analyses developed in this context are often suboptimal , as they take a conservative approach and consider strategies that query even points on which there is a small amount of uncertainty ( or disagreement ) among the classifiers still under consideration given the labels queried so far .",
    "the results derived in this manner often show an improvement in the @xmath5 factor in the label complexity of active versus passive learning ; however , unfortunately , the dependence on the @xmath0 term typically gets worse .    by analyzing a more aggressive , margin - based active learning algorithm",
    ", we prove that we can efficiently ( in polynomial time ) learn homogeneous linear separators when the underlying distribution is log - concave by using only @xmath17 label requests , answering an open question in  @xcite .",
    "this represents an exponential improvement of active learning over passive learning and it significantly broadens the cases for which we can show that the dependence on @xmath5 in passive learning can be improved to only @xmath18 in active learning , but without increasing the dependence on the dimension @xmath0 .",
    "we note that an improvement of this type was known to be possible only for the case when the underlying distributions is ( nearly ) uniform in the unit ball  @xcite ; even for this special case , our analysis improves by a multiplicative @xmath19 factor the results of  @xcite ; it also provides better dependence on @xmath0 than any other previous analyses implementable in a computationally efficient manner ( both disagreement - based  @xcite and more aggressive ones  @xcite ) , and over the inefficient splitting index analysis of  @xcite .    * techniques *   at the core of our results is a novel characterization of the region of disagreement of two linear separators under a log - concave measure .",
    "we show that for any two linear separators specified by normal vectors @xmath20 and @xmath21 , for any constant @xmath22 we can pick a margin as small as @xmath23 , where @xmath24 is the angle between @xmath20 and @xmath21 , and still ensure that the probability mass of the region of disagreement outside of band of margin @xmath25 of one of them is @xmath26 ( theorem  [ lemma : vectors - sophist ] ) . using this fact ,",
    "we then show how we can use a margin - based active learning technique , where in each round we only query points near the hypothesized decision boundary , to get an exponential improvement over passive learning .",
    "we then show that any passive learning algorithm that outputs a hypothesis consistent with @xmath27 random examples will , with probability at least @xmath28 , output a hypothesis of error at most @xmath29 ( theorem  [ t : passive ] ) .",
    "interestingly , our analysis is quite dissimilar to the classic analyses of erm .",
    "it proceeds by conceptually running the algorithm online on progressively larger chunks of examples , and using the intermediate hypotheses to track the progress of the algorithm .",
    "we show , using the same tools as in the active learning analysis , that it is always likely that the algorithm will receive informative examples .",
    "our analysis shows that the algorithm would also achieve @xmath30 accuracy with high probability even if it periodically built preliminary hypotheses using some of the examples , and then only used borderline cases for those preliminary classifiers for further training . to achieve the optimal sample complexity",
    ", we have to carefully distribute the confidence parameter , by allowing higher probability of failure in the later stages , to compensate for the fact that , once the hypothesis is already pretty good , it takes longer to get examples that help to further improve it .",
    "* non - separable case *   we also study label - efficient learning in the presence of noise .",
    "we show how our results for the realizable case can be extended to handle ( a variant of ) the tsybakov noise , which has received substantial attention in statistical learning theory , both for passive and active learning  @xcite ; this includes the random classification noise commonly studied in computational learning theory  @xcite , and the more general bounded ( or massart ) noise  @xcite .",
    "our analysis for massart noise leads to optimal bounds ( up to constant factors ) for active and passive learning of linear separators when the marginal distribution on the feature vectors is log - concave , improving the dependence on @xmath0 over previous best known results .",
    "our analysis for tsybakov noise leads to bounds on active learning with improved dependence on @xmath0 over previous known results in this case as well .",
    "we also provide a bound on the alexander s capacity  @xcite and the closely related disagreement coefficient notion  @xcite , which have been widely used to characterize the sample complexity of various ( active and passive ) algorithms  @xcite .",
    "this immediately implies concrete bounds on the labeled data complexity of several algorithms in the literature , including active learning algorithms designed for the purely agnostic case ( i.e. , arbitrary forms of noise ) , e.g. , the @xmath31 algorithm  @xcite and the dhm algorithm  @xcite .",
    "* nearly log - concave distributions *   we also extend our results both for passive and active learning to deal with nearly log - concave distributions ; this is a broader class of distributions introduced by  @xcite , which contains mixtures of ( not too separated ) log - concave distributions . in deriving our results ,",
    "we provide new tail bounds and structural results for these distributions , which might be of independent interest and utility , both in learning theory and in other areas including sampling and optimization .",
    "we note that our bounds on the disagreement coefficient improve by a factor of @xmath32 over the bounds of @xcite ( matching what was known for the much less general case of nearly uniform distribution over the unit sphere ) ; furthermore , they apply to the nearly log - concave case where we allow an arbitrary number of discontinuities , a case not captured by the @xcite conditions at all .",
    "we discuss other related papers in appendix  [ rel - appendix ] .",
    "we focus on binary classification problems ; that is , we consider the problem of predicting a binary label @xmath33 based on its corresponding input vector @xmath34 . as in the standard machine learning formulation ,",
    "we assume that the data points @xmath35 are drawn from an unknown underlying distribution @xmath36 over @xmath37 ; @xmath38 is called the _ instance space _ and @xmath39 is the _ label space_. in this paper we assume that @xmath40 and @xmath41 ; we also denote the marginal distribution over @xmath38 by @xmath42 .",
    "let @xmath43 be the class of linear separators through the origin , that is @xmath44 . to keep the notation simple",
    ", we sometimes refer to a weight vector and the linear classifier with that weight vector interchangeably .",
    "our goal is to output a hypothesis function @xmath45 of small error , where @xmath46.$ ] we consider two learning protocols : passive learning and active learning . in the passive learning setting",
    ", the learning algorithm is given a set of labeled examples @xmath47 drawn i.i.d . from @xmath36 and",
    "the goal is output a hypothesis of small error by using only a polynomial number of labeled examples . in the ( pool - based )",
    "active learning setting , a set of labeled examples @xmath48 is also drawn i.i.d .  from @xmath36 ; the learning algorithm is permitted direct access to the sequence of @xmath49 values ( unlabeled data points ) , but has to make a label request to obtain the label @xmath50 of example @xmath49 .",
    "the hope is that in the active learning setting we can output a classifier of small error by using many fewer label requests than in the passive learning setting by actively directing the queries to informative examples ( while keeping the number of unlabeled examples polynomial ) . for added generality",
    ", we also consider the selective sampling active learning model , where the algorithm visits the unlabeled data points @xmath49 in sequence , and , for each @xmath51 , makes a decision on whether or not to request the label @xmath50 based only on the previously - observed @xmath52 values ( @xmath53 ) and corresponding requested labels , and never changes this decision once made .",
    "both our upper and lower bounds will apply to both selective sampling and pool - based active learning .    in the `` realizable case ''",
    ", we assume that the labels are deterministic and generated by a target function that belongs to @xmath43 . in the non - realizable case ( studied in sections  [ se : dis ] and  [ se : tsy ] ) we do not make this assumption and instead aim to compete with the best function in @xmath43",
    ".    given two vectors @xmath20 and @xmath21 and any distribution @xmath54 we denote by @xmath55 ; we also denote by @xmath56 the angle between the vectors @xmath20 and @xmath21 .",
    "throughout this paper we focus on the case where the underlying distribution @xmath42 is log - concave or nearly log - concave .",
    "such distributions have played a key role in the past two decades in several areas including sampling , optimization , and integration algorithms  @xcite , and more recently for learning theory as well  @xcite . in this section",
    "we first summarize known results about such distributions that are useful for our analysis and then prove a novel structural statement that will be key to our analysis ( theorem  [ lemma : vectors - sophist ] ) . in section  [ sec : more_distr ] we describe extensions to nearly log - concave distributions as well .",
    "a distribution over @xmath57 is log - concave if @xmath58 is concave , where @xmath59 is its associated density function .",
    "it is isotropic if its mean is the origin and its covariance matrix is the identity .",
    "log - concave distributions form a broad class of distributions : for example , the gaussian , logistic , and uniform distribution over any convex set are log - concave distributions .",
    "the following lemma summarizes known useful facts about isotropic log - concave distributions ( most are from  @xcite ; the upper bound on the density is from @xcite ) .",
    "[ isotropic - basic ] assume that @xmath42 is log - concave in @xmath57 and let @xmath59 be its density function .    1 .",
    "if @xmath42 is isotropic then @xmath60 } \\leq e^{-\\alpha + 1}.$ ] if @xmath61 then : @xmath62 } \\leq |b - a|.$ ] 2 .",
    "if @xmath42 is isotropic , then @xmath63 whenever @xmath64 . furthermore , @xmath65 and @xmath66 where @xmath67 is @xmath68 and @xmath69 is @xmath70 , for all @xmath34 of any norm .",
    "3 .   all marginals of @xmath42 are log - concave . if @xmath42 is isotropic , its marginals are isotropic as well .",
    "4 .   if @xmath71= c^2 $ ] , then @xmath72 } \\leq e^{-r + 1}.$ ] 5 .   if @xmath42 is isotropic and @xmath61 we have @xmath73 and @xmath74 for all @xmath34 .    throughout our paper",
    "we will use the fact that there exists a universal constant @xmath75 such that the probability of disagreement of any two homogeneous linear separators is lower bounded by the c times the angle between their normal vectors .",
    "this follows by projecting the region of disagreement in the space given by the two normal vectors , and then using properties of log - concave distributions in 2-dimensions .",
    "the proof is implicit in earlier works ( e.g. , @xcite ) ; for completeness , we include a proof in appendix  [ a : angle ] .",
    "l : angle ] [ l : angle ] assume @xmath42 is an isotropic log - concave in @xmath57 .",
    "then there exists @xmath75 such that for any two unit vectors @xmath20 and @xmath21 in @xmath76 we have @xmath77    to analyze our active and passive learning algorithms we provide a novel characterization of the region of disagreement of two linear separators under a log - concave measure :    [ lemma : vectors - sophist ] for any @xmath78 , there is a @xmath79 such that the following holds .",
    "let @xmath20 and @xmath21 be two unit vectors in @xmath57 , and assume that @xmath80 . if @xmath42 is isotropic log - concave in @xmath57 , then : @xmath81    choose @xmath82 .",
    "we will show that , if @xmath83 is large enough relative to @xmath84 , then ( [ e : largemargin ] ) holds .",
    "let @xmath85 .",
    "let @xmath86 be the set whose probability we want to bound .",
    "since the event under consideration only concerns the projection of @xmath34 onto the span of @xmath20 and @xmath21 , lemma  [ isotropic - basic](c ) implies we can assume without loss of generality that @xmath87 .",
    "next , we claim that each member @xmath34 of @xmath86 has @xmath88 .",
    "assume without loss of generality that @xmath89 is positive .",
    "( the other case is symmetric . )",
    "then @xmath90 , so the angle of @xmath34 with @xmath20 is obtuse , i.e. @xmath91 . since @xmath92 , this implies that @xmath93 .",
    "but @xmath94 , and @xmath21 is unit length , so @xmath95 , which , since @xmath93 , implies @xmath96 this , since @xmath97 for all @xmath98 $ ] , in turn implies @xmath96 this implies that , if @xmath99 is a ball of radius @xmath100 in @xmath101 , that @xmath102 = \\sum_{i=1}^{\\infty }                  { \\mathbb p } [ e \\cap ( b((i+1 ) c_2 ) - b(i c_2 ) ) ] .\\ ] ] to obtain the desired bound , we carefully bound each term in the rhs . choose @xmath103 .",
    "let @xmath104 be the density of @xmath42 .",
    "we have @xmath105 =    \\int_{(x_1,x_2 ) \\in b((i+1 ) c_2 ) - b(i c_2 ) } 1_e(x_1,x_2 ) f(x_1,x_2 ) \\ ;",
    "dx_1 dx_2.\\ ] ] applying the density upper bound from lemma  [ isotropic - basic ] with @xmath87 , there are constants @xmath106 and @xmath107 such that @xmath108 & \\leq    \\int_{(x_1,x_2 ) \\in b((i+1 ) c_2 ) - b(i c_2 ) }       1_e(x_1,x_2 ) c_1 \\exp(- c_2 c_2 i ) \\ ; dx_1 dx_2 \\\\ & =   c_1 \\exp(- c_2 c_2 i )       \\int_{(x_1,x_2 ) \\in b((i+1 ) c_2 ) - b(i c_2 ) } 1_e(x_1,x_2 ) \\ ; dx_1 dx_2.\\end{aligned}\\ ] ] if we include @xmath109 in the integral again , we get @xmath105   \\leq   c_1 \\exp(- c_2 c_2 i )   \\int_{(x_1,x_2 ) \\in b((i+1 ) c_2 ) } 1_e(x_1,x_2 ) \\ ; dx_1 dx_2.\\ ] ] now , we exploit the fact that the integral above is a rescaling of a probability with respect to the uniform distribution .",
    "let @xmath110 be the volume of the unit ball in @xmath101 .",
    "then , we have @xmath105    \\leq   c_1 \\exp(- c_2 c_2",
    "i )   c_3 ( i+1)^2 c_2 ^ 2 \\alpha/\\pi \\\\   = c_4 c_2 ^ 2 \\alpha ( i+1)^2 \\exp(-c_2 c_2 i ) , \\ ] ] for @xmath111 . returning to ( [ e : shells ] ) , we get @xmath112 = \\sum_{i=1}^{\\infty }   c_4 c_2 ^ 2 \\alpha ( i+1)^2 \\exp(-c_2 c_2 i )   =   c_4   c_2 ^ 2 \\times \\frac{4 e^{2 c_2 c_2 } - 3 e^{c_2 c_2 } + 1}{\\left(e^{c_2 c_2 } - 1 \\right)^3 } \\times \\alpha.\\ ] ] since @xmath113 this completes the proof .",
    "we note that a weaker result of this type was proven ( via different techniques ) for the uniform distribution in the unit ball in  @xcite .",
    "in addition to being more general , theorem  [ lemma : vectors - sophist ] is tighter and more refined even for this specific case  this improvement is essential for obtaining tight bounds for polynomial time algorithms for passive learning ( section  [ passive ] ) and better bounds for active learning as well .",
    "in this section we analyze a margin - based algorithm for actively learning linear separators under log - concave distributions  @xcite ( algorithm  [ fig : active - uniform - simple - offline ] ) .",
    "lower bounds proved in section  [ s : lower ] show that this algorithm needs exponentially fewer labeled examples than any passive learning algorithm .",
    "this algorithm has been previously proposed and analyzed in  @xcite for the special case of the uniform distribution in the unit ball . in this paper",
    "we analyze it for the much more general class of log - concave distributions .",
    "* input * : a sampling oracle for @xmath114 , a labeling oracle , sequences @xmath115 , @xmath116 ( sample sizes ) and @xmath117 , @xmath116 ( cut - off values ) .",
    "* output * : weight vector @xmath118 .",
    "* draw @xmath119 examples from @xmath114 , label them and put them in @xmath120 . * * iterate * @xmath121 * * find a hypothesis @xmath122 with @xmath123 consistent with all labeled examples in @xmath124 .",
    "* * let @xmath125 * * until @xmath126 additional data points are labeled , draw sample @xmath34 from @xmath114 * * * if @xmath127 , then reject @xmath34 , * * * else , ask for label of @xmath34 , and put into @xmath128 .",
    "[ th : agg.margin ] assume @xmath42 is isotropic log - concave in @xmath57 .",
    "there exist constants @xmath129 s.t .",
    "for @xmath130 , and for any @xmath131 , @xmath132 , using algorithm  [ fig : active - uniform - simple - offline ] with @xmath133 and @xmath134 , after @xmath135 iterations , we find a separator of error at most @xmath29 with probability @xmath28 .",
    "the total number of labeled examples needed is @xmath17 .",
    "let @xmath75 be the constant from lemma  [ l : angle ] .",
    "we will show , using induction , that , for all @xmath136 , with probability at least @xmath137 any @xmath138 consistent with the data in the working set @xmath124 has @xmath139 , so that , in particular , @xmath140 .    the case where @xmath141 follows from the standard vc bounds ( see e.g.,@xcite ) .",
    "assume now the claim is true for @xmath142 ( @xmath143 ) , and consider the @xmath144th iteration .",
    "let @xmath145 , and @xmath146 by the induction hypothesis , we know that , with probability at least @xmath147 all @xmath138 consistent with @xmath148 , including @xmath149 , have errors at most @xmath150 .",
    "consider an arbitrary such @xmath138 . by lemma  [ l : angle ] we have @xmath151 and @xmath152 , so @xmath153 . applying theorem  [ lemma : vectors - sophist ] ,",
    "there is a choice of @xmath106 ( the constant such that @xmath154 ) that satisfies @xmath155 and @xmath156 so @xmath157    now let us treat the case that @xmath158 .",
    "since we are labeling @xmath159 data points in @xmath160 at iteration @xmath142 , classic vapnik - chervonenkis bounds  @xcite imply that , if @xmath107 is a large enough absolute constant , then with probability @xmath161 , for all @xmath138 consistent with the data in @xmath124 , @xmath162    finally , since @xmath160 consists of those points that , after projecting onto the direction @xmath149 , fall into an interval of length @xmath163 , lemma  [ isotropic - basic ] implies that @xmath164 putting this together with ( [ e : s2.ok ] ) and ( [ e : er.given.s1 ] ) , with probability @xmath165 , we have @xmath166 , completing the proof .",
    "[ passive ] in this section we show how an analysis that was inspired by active learning leads to optimal ( up to constant factors ) bounds for polynomial - time algorithms for passive learning .",
    "[ t : passive ] assume that @xmath42 is zero mean and log - concave in @xmath57 .",
    "there exists an absolute constant @xmath110 s.t .  for @xmath130 , and for any @xmath131 , @xmath132 , any algorithm that outputs a hypothesis that correctly classifies @xmath167 examples finds a separator of error at most @xmath29 with probability @xmath168 .",
    "we focus here on the case that @xmath42 is isotropic .",
    "we can treat the non - isotropic case by observing that the two cases are equivalent ; one may pass between them by applying the whitening transform .",
    "( see appendix  [ appendix : mainpassiverealizable ] for details . )    while our analysis will ultimately provide a guarantee for any learning algorithm that always outputs a consistent hypothesis , we will use intermediate hypothesis of algorithm  [ fig : active - uniform - simple - offline ] in the analysis .",
    "let @xmath75 be the constant from lemma  [ l : angle ] . while proving theorem  [ th : agg.margin ] , we proved that , if algorithm  [ fig : active - uniform - simple - offline ] is run with @xmath133 and @xmath169 , that for all @xmath136 , with probability @xmath170 any @xmath138 consistent with the data in @xmath124 has @xmath171 .",
    "thus , after @xmath172 iterations , with probability at least @xmath173 , any linear classifier consistent with _ all _ the training data has error @xmath174 , since any such classifier is consistent with the examples in @xmath175 .",
    "now , let us analyze the number of examples used , including those examples whose labels were not requested by algorithm  [ fig : active - uniform - simple - offline ] .",
    "lemma  [ isotropic - basic ] implies that there is a positive constant @xmath176 such that @xmath177 : again , @xmath160 consists of those points that fall into an interval of length @xmath163 after projecting onto @xmath149 .",
    "the density is lower bounded by a constant when @xmath178 , and we can use the bound for @xmath179 when @xmath180 .",
    "the expected number of examples that we need before we find @xmath181 elements of @xmath160 is therefore at most @xmath182 . using a chernoff bound ,",
    "if we draw @xmath183 examples , the probability that we fail to get @xmath181 members of @xmath160 is at most @xmath184 , which is at most @xmath185 if @xmath107 is large enough .",
    "so , the total number of examples needed , @xmath186 , is at most a constant factor more than @xmath187 we can show @xmath188 , completing the proof .",
    "we conclude this section by pointing out several important facts and implications of theorem  [ t : passive ] and its proof .    1 .   the separator in theorem  [ t : passive ] ( and the one in theorem  [ th : agg.margin ] ) can be found in _",
    "polynomial time _ , for example by using linear programming .",
    "the analysis of theorem  [ t : passive ] also bounds the number of unlabeled examples needed by the active learning algorithm of theorem  [ th : agg.margin ] .",
    "this shows that an algorithm can request a nearly optimally small number of labels without increasing the total number of examples required by more than a constant factor .",
    "specifically , in round @xmath144 , we only need @xmath189)$ ] unlabeled examples ( whp ) , where @xmath190 , so the total number of unlabeled examples needed over all rounds is @xmath191",
    ".      * theorem  [ t : passive ] . * _ assume that @xmath42 is zero mean and log - concave in @xmath57 .",
    "there exists an absolute constant @xmath110 s.t .  for @xmath130 , and for any @xmath131 , @xmath132 , any algorithm that outputs a hypothesis that correctly classifies @xmath167 examples finds a separator of error at most @xmath29 with probability @xmath168 .",
    "_    first , let us prove the theorem in the case that @xmath42 is isotropic .",
    "we will then treat the general case at the end of the proof .",
    "while our analysis will ultimately provide a guarantee for any learning algorithm that always outputs a consistent hypothesis , we will use intermediate hypothesis of algorithm  [ fig : active - uniform - simple - offline ] in the analysis .",
    "let @xmath75 be the constant from lemma  [ l : angle ] . while proving theorem  [ th : agg.margin ] , we proved that , if algorithm  [ fig : active - uniform - simple - offline ] is run with @xmath133 and @xmath169 , that for all @xmath136 , with probability @xmath170 any @xmath138 consistent with the data in @xmath124 has @xmath171 .",
    "thus , after @xmath172 iterations , with probability at least @xmath173 , any linear classifier consistent with _ all _ the training data has error @xmath174 , since any such classifier is consistent with the examples in @xmath175 .",
    "now , let us analyze the number of examples used , including those examples whose labels were not requested by algorithm  [ fig : active - uniform - simple - offline ] .",
    "lemma  [ isotropic - basic ] implies that there is a positive constant @xmath176 such that @xmath177 : again , @xmath160 consists of those points that fall into an interval of length @xmath163 after projecting onto @xmath149 .",
    "the density is lower bounded by a constant when @xmath178 , and we can use the bound for @xmath179 when @xmath180 .",
    "the expected number of examples that we need before we find @xmath181 elements of @xmath160 is therefore at most @xmath182 . using a chernoff bound ,",
    "if we draw @xmath183 examples , the probability that we fail to get @xmath181 members of @xmath160 is at most @xmath184 , which is at most @xmath185 if @xmath107 is large enough .",
    "so , the total number of examples needed , @xmath186 , is at most a constant factor more than @xmath320 we claim that @xmath321 .",
    "we have @xmath322 completing the proof in the case that @xmath42 is isotropic .",
    "now let us treat the case in which @xmath42 is not isotropic .",
    "suppose that @xmath323 is the covariance matrix of @xmath42 , so that @xmath324 is the `` whitening transform '' .",
    "suppose , for @xmath325 , an algorithm is given a sample @xmath326 of examples @xmath327 for @xmath328 drawn according to @xmath42 , and @xmath329 labeled by a target hypothesis with weight vector @xmath21 .",
    "note that @xmath282 is consistent with @xmath326 if and only if @xmath330 is consistent with @xmath331 ( so those examples are consistent with @xmath332 ) .",
    "so our analysis of the isotropic case implies that , with probability @xmath2 , for any @xmath282 consistent with @xmath327 , we have @xmath333 which of course means that @xmath334",
    "in this section we consider learning with respect to a more general class of distributions .",
    "we start by providing a general set of conditions on a set @xmath192 of distributions that is sufficient for efficient passive and active learning w.r.t .",
    "distributions in @xmath192 .",
    "we now consider nearly log - concave distributions , an interesting , more general class containing log - concave distributions , considered previously in  @xcite and  @xcite .",
    "we then prove that isotropic nearly log - concave distributions satisfy our sufficient conditions ; in appendix  [ app : more_distr ] , we also show how to remove the assumption that the distribution is isotropic .",
    "a set @xmath193 of distributions is _ admissible _ if it satisfies the following :    * there exists @xmath75 such that for any @xmath194 and any two unit vectors @xmath20 and @xmath21 in @xmath76 we have @xmath77 * for any @xmath78 , there is a @xmath79 such that the following holds for all @xmath194 . let @xmath20 and @xmath21 be two unit vectors in @xmath57 s.t . @xmath80 .",
    "then @xmath195 * there are positive constants @xmath196 such that , for any @xmath197 , for any projection @xmath42 of @xmath198 onto a one - dimensional subspace , the density @xmath59 of @xmath42 satisfies @xmath199 for all @xmath34 and @xmath200 for all @xmath34 with @xmath201 .",
    "the proofs of theorem  [ th : agg.margin ] and theorem  [ t : passive ] can be used without modification to show :    [ t : admissible ] if @xmath192 is admissible , then arbitrary @xmath202 can be learned with respect to arbitrary distributions in @xmath192 in polynomial time in the active learning model from @xmath17 labeled examples , and in the passive learning model from @xmath15 examples .",
    "a density function @xmath203 is @xmath204 log - concave if for any @xmath205 $ ] , @xmath206 , @xmath207 , we have @xmath208 .    clearly , a density function @xmath59 is log - concave if it is @xmath209-log - concave .",
    "an example of a @xmath210-log - concave distribution is a mixture of two log - concave distributions whose covariance matrices are @xmath211 , and whose means @xmath212 and @xmath213 have @xmath214 .    in this section we prove that for any sufficiently small constant @xmath215 , the class of isotropic @xmath204 log - concave distribution in @xmath57 is admissible and has light tails ( this second fact is useful for analyzing the disagreement coefficient in sections  [ se : dis ] ) . in doing so we provide several new properties for such distributions , which could be of independent interest .",
    "detailed proofs of our claims appear in appendix  [ app : more_distr ] .",
    "we start by showing that for any isotropic @xmath204 log - concave density @xmath59 there exists a log - concave density @xmath216 whose center is within @xmath217 of @xmath59 s center and that satisfies @xmath218 , for @xmath219 as small as @xmath220 .",
    "the fact @xmath219 depends only exponentially in @xmath221 ( as opposed to exponentially in @xmath0 ) is key for being able to argue that such distributions have light tails .",
    "[ basic - betalc ] for any isotropic @xmath204 log - concave density function @xmath59 there exists a log - concave density function @xmath216 that satisfies @xmath218 and @xmath222 , for @xmath223 .",
    "moreover , we have @xmath224 for every unit vector @xmath20 .",
    "note that if the density function @xmath59 is @xmath204 log - concave we have that @xmath225 satisfies that for any @xmath205 $ ] , @xmath226 , @xmath207 , we have @xmath227 .",
    "let @xmath228 be the function whose subgraph is the convex hull of the subgraph of @xmath229 . by using caratheodory s theorem of @xmath57",
    "lies in the convex hull of a set @xmath230 , then there is a subset @xmath231 of @xmath230 consisting of @xmath232 or fewer points such that @xmath34 lies in the convex hull of @xmath231 . ]",
    "we can show that @xmath233 this implies @xmath234 and we can prove by induction on @xmath235 that @xmath236 if we further normalize @xmath237 to make it a density function , we obtain @xmath216 that is log - concave and satisfies @xmath238 where @xmath239 this implies that for any @xmath34 we have @xmath240 .    using this fact and concentration properties of @xmath216 ( in particular lemma  [ isotropic - basic ] )",
    ", we can show that the center of @xmath216 is close to the center of @xmath59 , as desired .",
    "[ l : angle.beta ] assume @xmath204 is a sufficiently small non - negative constant and let @xmath193 be the set of all isotropic @xmath204 log - concave distributions .",
    "( a ) @xmath193 is admissible .",
    "( b ) any @xmath194 has light tails .",
    "that is : @xmath241 , for @xmath223 .",
    "\\(a ) choose @xmath194 .",
    "as in lemma  [ l : angle ] , consider the plane determined by @xmath20 and @xmath21 and let @xmath242 denote the projection operator that given @xmath243 , orthogonally projects @xmath34 onto this plane . if @xmath244 then @xmath245 by using the prekopa - leindler inequality  @xcite one can show that @xmath246 is @xmath204 log - concave ( see e.g. ,  @xcite ) . moreover , if @xmath42 is isotropic , than @xmath246 is isotropic as well . by lemma  [ basic - betalc ] we know that there exists a @xmath219-isotropic log - concave distribution @xmath247 centered at @xmath248 , @xmath249 , satisfying @xmath218 and @xmath250 for every unit vector @xmath20 , for constants @xmath251 and @xmath252 . for @xmath204 sufficiently small we have @xmath253 .",
    "using this , by applying the whitening transform ( see theorem  [ almost isotropic ] in appendix  [ app : more_distr ] ) , we can show @xmath254 , for @xmath255 , which implies @xmath256 , for @xmath255 .",
    "using a reasoning as in lemma  [ l : angle ] we get @xmath77 the generalization of theorem  [ lemma : vectors - sophist ] follows from a similar proof , except using theorem  [ almost isotropic ] .",
    "the density bounds in the @xmath257 case also follow from theorem  [ almost isotropic ] as well .",
    "\\(b ) since @xmath38 is isotropic , we have @xmath258 = d$ ] ( where @xmath59 is its associated density ) . by lemma  [ basic - betalc ] , there exists a log - concave density @xmath216 such that @xmath218 , for @xmath223 .",
    "this implies @xmath259 \\leq cd$ ] . by lemma  [ isotropic - basic ]",
    "we get that that under @xmath216 , @xmath260 , so under @xmath59 we have @xmath241 .",
    "using theorem  [ t : admissible ] and theorem  [ l : angle.beta](a ) we obtain :    [ th : agg.margin : beta ] let @xmath215 be a sufficiently small constant .",
    "assume that @xmath42 is an isotropic @xmath204 log - concave distribution in @xmath57 .",
    "then arbitrary @xmath202 can be learned with respect to @xmath42 in polynomial time in the active learning model from @xmath17 labeled examples , and in the passive learning model from @xmath15 examples .",
    "* lemma  [ basic - betalc ] . * _ for any isotropic @xmath204 log - concave density function @xmath59 there exists a log - concave density function @xmath216 that satisfies @xmath218 and @xmath222 , for @xmath223 .",
    "moreover , we have @xmath224 for every unit vector @xmath20 .",
    "_    note that if the density function @xmath59 is @xmath204 log - concave we have that @xmath225 satisfies that for any @xmath205 $ ] , @xmath226 , @xmath207 , we have @xmath227 .",
    "let @xmath228 be the function whose subgraph is the convex hull of the subgraph of @xmath229 .",
    "that is , @xmath335 is the maximum of all values of @xmath336 for any @xmath337 and @xmath338 $ ] such that @xmath339 and @xmath340 .",
    "note that , if the components of @xmath341 are @xmath342 , we can get @xmath335 by starting with @xmath343 taking the convex combination of the members of @xmath344 with mixing coefficients @xmath345 , and then reading off the last component .",
    "caratheodory s theorem of @xmath57 lies in the convex hull of a set @xmath230 , then there is a subset @xmath231 of @xmath230 consisting of @xmath232 or fewer points such that @xmath34 lies in the convex hull of @xmath231 . ]",
    "implies that we can get the same result using a mixture of at most @xmath232 members of @xmath344 . in other words , we can assume without loss of generality that @xmath346 , so that @xmath347    because of the case where @xmath348 concentrates all its weight on one component , we have @xmath234",
    ".    we also claim that @xmath349 we will prove this by induction on @xmath235 , treating the case in which @xmath232 is a power of @xmath350 .",
    "( by padding with zeroes if necessary , we may assume without loss of generality that @xmath232 is a power of @xmath350 . )",
    "the base case , in which @xmath351 , follows immediately from the definitions .",
    "let @xmath346 .",
    "assume that @xmath352 , @xmath353 , @xmath354 .",
    "we can write this as : @xmath355 where @xmath356 , for all @xmath51 .",
    "now , by induction we have : @xmath357 the last inequality follows from the fact that @xmath358 .",
    "so , we have proved ( [ e : logbound ] ) .",
    "if we further normalize @xmath237 to make it a density function , we obtain @xmath216 that is log - concave and satisfies @xmath238 where @xmath359 this implies that for any @xmath34 we have @xmath240 .",
    "we now show that the center of @xmath216 is close to the center of @xmath59 .",
    "we have : @xmath360 dr}.\\end{aligned}\\ ] ] using concentration properties of @xmath216 ( in particular lemma  [ isotropic - basic ] ) we get @xmath361 as desired .",
    "[ almost isotropic ] ( i ) let @xmath362 be the density function of a log - concave distribution centered at @xmath248 and with covariance matrix @xmath363 $ ] .",
    "assume @xmath59 satisfies @xmath364 and @xmath250 for every unit vector @xmath20 , for @xmath365 constant close to @xmath366 .",
    "we have : ( a ) assume @xmath367 .",
    "then there exist an universal constant @xmath75 s.t .",
    "we have @xmath368 , for all @xmath34 with @xmath369 . ( b )",
    "assume @xmath370 . there exist universal constants @xmath176 and @xmath83 such that @xmath371 for all @xmath34 .",
    "\\(ii ) let @xmath372 be the density function of a log - concave distribution centered at @xmath373 with standard deviation @xmath374 .",
    "then @xmath375 for all @xmath34 .",
    "if furthermore @xmath59 satisfies @xmath376 \\leq c$ ] for @xmath365 and @xmath377 , then we have @xmath378 for some universal constant @xmath75 .",
    "\\(i ) let @xmath379 .",
    "then @xmath380 is a log - concave distribution in the isotropic position .",
    "moreover , the density function of @xmath381 is given by @xmath382 let @xmath383 $ ] .",
    "we have @xmath384={\\mathbb e}[x x^t]- z z^t= m- z z^t.\\ ] ] also , the fact @xmath250 for every unit vector @xmath20 is equivalent to @xmath385 u \\leq c\\ ] ] for every unit vector @xmath20 .",
    "using @xmath386 , @xmath387 , and @xmath388 we get that @xmath389 $ ] , @xmath390 $ ] , and @xmath391 $ ] .",
    "we also have @xmath364 and @xmath392 .",
    "all these imply that @xmath393    \\(a ) for @xmath394 we have @xmath395 , where @xmath396 is a unit vector , so @xmath397 . if @xmath255 we have @xmath398 , so by lemma  [ isotropic - basic ] we have @xmath399 , so @xmath400 , for some universal constants @xmath401 , as desired .",
    "\\(b ) we have @xmath402 . by lemma",
    "[ isotropic - basic ] ( b ) we have @xmath403}.\\ ] ] by the triangle inequality we further obtain : @xmath404 } \\exp{\\left[-c \\norm{a^{-1/2}x}\\right]}.\\ ] ]    for @xmath370 , we can show that @xmath405 .",
    "it is enough to show @xmath406 , or that @xmath407 , where @xmath408 ( so @xmath409 ) .",
    "this is equivalent to @xmath410 , which is true since the matrix @xmath411 is positive semi - definite .",
    "\\(ii ) define @xmath412 .",
    "we have @xmath413=0 $ ] and @xmath414 = 1 $ ] .",
    "the density @xmath381 of @xmath380 is given by @xmath415 .",
    "now , since @xmath381 is isotropic and log - concave , we can apply lemma  [ isotropic - basic](e ) to @xmath381 .",
    "so @xmath416 for all @xmath33 .",
    "so , @xmath417 for all @xmath33 , which implies @xmath375 for all @xmath34 .",
    "the second part follows as in theorem  [ almost isotropic ] .      in this section , we extend theorem  [ th : agg.margin ] to the case of arbitrary covariance matrices .",
    "[ t : any.cov.active ] [ t : any.cov.active ] if all distributions in @xmath192 are zero - mean and log - concave in @xmath57 , then arbitrary @xmath202 be learned in polynomial time from arbitrary distributions in @xmath193 in the active learning model from @xmath17 labeled examples , and in the passive learning model from @xmath15 examples .",
    "our proof is through a series of lemma .",
    "first , @xcite have shown how to reduce to the nearly isotropic case .",
    "[ l : lv.reduce ] for any constant @xmath418 , there is a polynomial time algorithm that , given polynomially many samples from a log - concave distribution @xmath42 , outputs an estimate @xmath323 of the covariance matrix of @xmath42 such that , with probability @xmath2 the distribution @xmath198 obtained by sampling @xmath34 from @xmath42 and producing @xmath419 has @xmath420 for all unit vectors @xmath20 .    as a result of lemma  [ l : lv.reduce ] , we can assume without loss of generality that the distribution @xmath42 satisfies @xmath421 for an arbitrarily small constant @xmath422 . by theorem  [ almost isotropic ] , this implies that , without loss of generality , there are constants @xmath423 such that , for the density @xmath59 of any one or two - dimensional marginal @xmath198 of @xmath42 , we have @xmath424 and for all @xmath34 , @xmath425    we will show that these imply that @xmath193 is admissible .",
    "[ l : angle.anycov ] ( a ) there exists @xmath75 such that for any two unit vectors @xmath20 and @xmath21 in @xmath76 we have @xmath426    \\(b ) for any @xmath427 , there is a @xmath428 such that the following holds .",
    "let @xmath20 and @xmath21 be two unit vectors in @xmath57 , and assume that @xmath80 .",
    "then @xmath429    \\(a ) projecting @xmath42 onto a subspace can only reduce the norm of its mean , and its variance in any direction .",
    "therefore , as in the proof of lemma  [ l : angle ] , we may assume without loss of generality that @xmath430 . here",
    ", let us define @xmath314 to be the region of disagreement between @xmath315 and @xmath316 intersected with the ball @xmath431 of radius @xmath83 in @xmath317",
    ". then we have @xmath432 ( b ) this proof basically amounts to observing that everything that was needed for the proof of theorem  [ lemma : vectors - sophist ] is true for @xmath42 , because of ( [ e : dens.low ] ) and ( [ e : dens.up ] ) .",
    "armed with lemma  [ l : angle.anycov ] , to prove theorem  [ t : any.cov.active ] , we can just apply theorem  [ t : admissible ] .",
    "in this section we give lower bounds on the label complexity of passive and active learning of homogeneous linear separators when the underlying distribution is @xmath204 log - concave , for a sufficiently small constant @xmath204 .",
    "these lower bounds are information theoretic , applying to any procedure , that might not be necessarily computationally efficient .",
    "the proof is in appendix  [ a : lower ] .",
    "[ t : lower ]    for a small enough constant @xmath204 we have : ( 1 ) for any @xmath204 log - concave distribution @xmath42 whose covariance matrix has full rank , the sample complexity of learning origin - centered linear separators under @xmath42 in the passive learning model is @xmath261 ( 2 ) the sample complexity of active learning of linear separators under @xmath204 log - concave distributions is @xmath262    note that , if the covariance matrix of @xmath42 does not have full rank , the number of dimensions is effectively less than @xmath0 , so our lower bound essentially applies for all log - concave distributions .",
    "the proof of our lower bounds ( theorem  [ t : lower ] ) relies on a lower bound on the packing numbers @xmath433 .",
    "recall that the @xmath29-packing number , @xmath433 , is the maximal cardinality of an @xmath29-separated set with classifiers from @xmath43 , where we say that @xmath434 are @xmath29-separated w.r.t @xmath192 if @xmath435 for any @xmath436 .",
    "[ packing - logconcave ] there is a positive constant @xmath75 such that , for all @xmath437 , the following holds .",
    "assume that @xmath42 is @xmath204 log - concave in @xmath57 , and that its covariance matrix has full rank .",
    "for all sufficiently small @xmath29 , @xmath438 , we have @xmath439    we first prove the lemma in the case that @xmath42 is isotropic .",
    "the proof in this case follows the outline of a proof for the special case of the uniform distribution in @xcite .",
    "let @xmath440 be the uniform distribution on the surface of the unit ball in @xmath76 .",
    "by theorem  [ l : angle.beta ] , there exists @xmath75 such that for any two unit vectors @xmath20 and @xmath21 in @xmath76 we have @xmath441 this implies that for a fixed @xmath20 the probability that a randomly chosen @xmath21 has @xmath442 is upper bounded by the volume of those vectors in the interior of the unit ball whose angle is at most @xmath443 divided by the volume of the unit ball . using known bounds on this ratio ( see @xcite ) we have @xmath444 \\leq \\frac{1}{\\sqrt{d } } \\left(\\frac{2    \\epsilon}{c}\\right)^{d-1}$ ] , so @xmath445 \\leq \\frac{1}{\\sqrt{d } } \\left(\\frac{2    \\epsilon}{c}\\right)^{d-1}$ ] .",
    "that means that for a fixed @xmath446 if we pick @xmath446 normal vectors at random from the unit ball , then the expected number of pairs of half - spaces that are @xmath29-close according to @xmath42 is at most @xmath447 . removing one element of each pair from @xmath326 yields a set of @xmath448 halfspaces that are @xmath29-separated . setting @xmath449 , leads the desired result .    to handle the non - isotropic case ,",
    "suppose that @xmath323 is the covariance matrix of @xmath42 , so that @xmath324 is the whitening transform .",
    "let @xmath198 be the whitened version of @xmath42 , i.e.  the distribution obtained by first choosing @xmath34 from @xmath42 , and then producing @xmath419 .",
    "we have @xmath450 ( because @xmath451 iff @xmath452 ) .",
    "so we can use an @xmath29-packing w.r.t .",
    "@xmath198 to construct an @xmath29-packing of the same size w.r.t .",
    "@xmath42 .",
    "now we are ready to prove theorem  [ t : lower ]",
    ".    * theorem  [ t : lower ] . *    _ _    for a small enough constant @xmath204",
    "we have : ( 1 ) for any @xmath204 log - concave distribution @xmath42 whose covariance matrix has full rank , the sample complexity of learning origin - centered linear separators under @xmath42 in the passive learning model is @xmath261 ( 2 ) the sample complexity of active learning of linear separators under @xmath204 log - concave distributions is @xmath262    first , let us consider passive pac learning .",
    "it is known @xcite that , for any distribution @xmath42 , the sample complexity of passive pac learning origin - centered linear separators w.r.t .",
    "@xmath42 is at least @xmath453 applying lemma  [ packing - logconcave ] gives an @xmath454 lower bound .",
    "it is known @xcite that , if for each @xmath29 , there is a pair of classifier @xmath455 such that @xmath456 , then the sample complexity of pac learning is @xmath457 ; this requirement is satisfied by @xmath42 .",
    "now let us consider the sample complexity of active learning .",
    "as shown in  @xcite , in order to output a hypothesis of error at most @xmath29 with probabality at least @xmath28 , where @xmath458 and active learning algorithm that is allowed to make arbitrary yes - no queries must make @xmath459 queries . using this together with lemma  [ packing - logconcave ] we get the desired result .",
    "[ se : dis ] we consider two closely related distribution dependent capacity notions : the alexander capacity and the disagreement coefficient ; they have been widely used for analyzing the label complexity of non - aggressive active learning algorithms  @xcite .",
    "we begin with the definitions . for @xmath263 , define @xmath264 . for any @xmath265 ,",
    "define the region of disagreement as @xmath266 define the alexander capacity function @xmath267 for @xmath268 w.r.t .",
    "@xmath42 as : @xmath269 define the disagreement coefficients for @xmath268 w.r.t .",
    "@xmath42 as : @xmath270.$ ]    the following is our bound in the disagreement coefficient .",
    "its proof is in appendix  [ a : dis - coeff : beta ] .",
    "[ th : dis - coeff : beta ] let @xmath215 be a sufficiently small constant .",
    "assume that @xmath42 is an isotropic @xmath204 log - concave distribution in @xmath57 .",
    "for any @xmath271 , for any @xmath29 , @xmath272 is @xmath273 .",
    "thus @xmath274 .",
    "theorem  [ th : dis - coeff : beta ] immediately leads to concrete bounds on the label complexity of several algorithms in the literature  @xcite .",
    "for example , by composing it with a result of  @xcite , we obtain a bound of @xmath275 for agnostic active learning when @xmath42 is isotropic log - concave in @xmath57 ; that is we only need @xmath276 label requests to output a classifier of error at most @xmath277 , where @xmath278",
    ".      * theorem  [ th : dis - coeff : beta ] . * _ let @xmath215 be a sufficiently small constant . assume that @xmath42 is an isotropic @xmath204 log - concave distribution in @xmath57 . for any @xmath271 , for any @xmath29 , @xmath272 is @xmath273 .",
    "thus @xmath274 . _",
    "roughly , we will show that almost all @xmath34 classified by a large enough margin by @xmath280 are not in @xmath460 , because all hypotheses agree with @xmath280 about how to classify such @xmath34 , and therefore all pairs of hypotheses agree with each other .",
    "consider @xmath282 such that @xmath461 ; by theorem  [ l : angle.beta ] we have @xmath462 .",
    "define @xmath463 as in the proof of theorem  [ l : angle.beta ] . for any @xmath34",
    "such that @xmath464 we have @xmath465 thus , if @xmath34 also satisfies @xmath466 we have @xmath467 . since this is true for all @xmath282 , any such @xmath34 is not in @xmath468 . by theorem  [ l : angle.beta ]",
    "we have , for a constant @xmath83 , that @xmath469 moreover , by theorem  [ l : angle.beta ] we also have @xmath470 } \\leq r.\\ ] ] these both imply @xmath471 .",
    "in this section we consider a variant of the tsybakov noise condition @xcite .",
    "we assume that the classifier @xmath229 that minimizes @xmath279 is a linear classifier , and that , for the weight vector @xmath280 of the optimal classifier , there exist known parameters @xmath281 such that , for all @xmath282 , we have @xmath283 by generalizing theorem  [ lemma : vectors - sophist ] so that it provides a stronger bound for larger margins , and combining the result with the other lemmas of this paper and techniques from @xcite , we get the following .    [ th : agg.margin : tsybakov ] let @xmath284 .",
    "assume that the distribution @xmath36 satisfies the tsybakov noise condition for constants @xmath285 and @xmath286 , and that the marginal @xmath42 on @xmath287 is isotropic log - concave .",
    "( 1 ) if @xmath288 , we can find a separator with excess error @xmath174 with probability @xmath28 using @xmath289 labeled examples in the active learning model , and @xmath15 labeled examples in the passive learning model .",
    "( 2 ) if @xmath290 , we can find a separator with excess error @xmath174 with probability @xmath28 using @xmath291 labeled examples in the active learning model .",
    "in the case @xmath288 ( that is more general than the massart noise condition ) our analysis leads to optimal bounds for active and passive learning of linear separators under log - concave distributions , improving the dependence on @xmath0 over previous best known results  @xcite .",
    "our analysis for tsybakov noise ( @xmath292 ) leads to bounds on active learning with improved dependence on @xmath0 over previous known results  @xcite in this case as well .",
    "proofs and further details appear in appendix  [ a : massart ] .",
    "the label sample complexity of our active learning algorithm for learning homogeneous linear separators under isotropic logconcave distributions is @xmath17 , while our lower bound for this setting is @xmath293 our upper bound is achieved by an algorithm that uses a polynomial number of unlabeled training examples , and polynomial time . if an unbounded amount of computation time and an unbounded number of unlabeled examples are available , it seems to be easy to learn to accuracy @xmath29 using @xmath294 label requests , no matter what the value of @xmath295 .",
    "( roughly , the algorithm can construct an @xmath29-cover to initialize a set of candidate hypotheses , then repeatedly wait for an unlabeled example that evenly splits the current list of candidates , and ask its label , eliminated roughly half of the candidates . ) it would be interesting to know what is the best label complexity for a polynomial - time algorithm , or even an algorithm that is constrained to use a polynomial number of unlabeled examples .",
    "conceptually , our analysis of erm for passive learning under ( nearly ) log - concave distributions is based on a more aggressive localization than those considered previously in the literature .",
    "it would be very interesting to extend this analysis as well as our analysis for active learning to arbitrary distributions and more general concept spaces .",
    "[ [ acknowledgements ] ] acknowledgements + + + + + + + + + + + + + + + +    we thank steve hanneke for a number of useful discussions .",
    "this work was supported in part by nsf grant ccf-0953192 , afosr grant fa9550 - 09 - 1 - 0538 , and a microsoft research faculty fellowship .",
    "56 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    k.s .",
    "rates of growth and sample moduli for weighted empirical processes indexed by sets . _ probability theory and related fields _ , 1987 .",
    "n.  alon . a non - linear lower bound for planar epsilon - nets .",
    "_ focs _ , pages 341346 , 2010 .",
    "d.  applegate and r.  kannan .",
    "sampling and integration of near log - concave functions . in _",
    "stoc _ , 1991 .",
    "p.  assouad .",
    "plongements lipschitziens dans .",
    "france _ , 1110 ( 4):0 429448 , 1983 .",
    "m.  f. balcan , a.  beygelzimer , and j.  langford .",
    "agnostic active learning . in _",
    "icml _ , 2006 .",
    "balcan , a.  broder , and t.  zhang .",
    "margin based active learning . in _ colt _ , 2007 .",
    "balcan , s.  hanneke , and j.  wortman . the true sample complexity of active learning . in _",
    "colt _ , 2008 .",
    "p.  l. bartlett , o.  bousquet , and s.  mendelson .",
    "local rademacher complexities . _",
    "annals of statistics _ , 2005 .",
    "a.  beygelzimer , d.  hsu , j.  langford , and t.  zhang .",
    "agnostic active learning without constraints . in _ nips _ , 2010 .",
    "a.  blumer , a.  ehrenfeucht , d.  haussler , and m.  k. warmuth .",
    "learnability and the vapnik - chervonenkis dimension .",
    "_ jacm _ , 360 ( 4):0 929965 , 1989 .",
    "s.  boucheron , o.  bousquet , and g.  lugosi .",
    "theory of classification : a survey of recent advances .",
    "_ esaim : probability and statistics _ , 2005 .",
    "n.  h. bshouty , y.  li , and p.  m. long . .",
    "_ jcss _ , 2009 .    c.  caramanis and s.  mannor .",
    "an inequality for nearly log - concave distributions with applications to learning .",
    "_ ieee transactions on information theory _ , 2007 .    r.  castro and r.  nowak .",
    "minimax bounds for active learning . in _",
    "colt _ , 2007 .",
    "n.  cesa - bianchi , c.  gentile , and l.  zaniboni .",
    "learning noisy linear classifiers via adaptive and selective sampling .",
    "_ machine learning _ , 2010 .",
    "k.  l. clarkson and k.  varadarajan .",
    "improved approximation algorithms for geometric set cover .",
    "_ discrete comput .",
    "_ , 370 ( 1):0 4358 , 2007 .",
    "d.  cohn , l.  atlas , and r.  ladner . .",
    "in _ icml _ , 1994 .",
    "s.  dasgupta .",
    "coarse sample complexity bounds for active learning . in _ nips _ , volume  18 , 2005 .",
    "s.  dasgupta",
    ". active learning . _ encyclopedia of machine learning _",
    ", 2011 .",
    "s.  dasgupta , a.  kalai , and c.  monteleoni .",
    "analysis of perceptron - based active learning . in _ colt _ , 2005 .",
    "s.  dasgupta , d.j .",
    "hsu , and c.  monteleoni .",
    "a general agnostic active learning algorithm .",
    "_ advances in neural information processing systems _ , 20 , 2007 .",
    "o.  dekel , c.  gentile , and k.  sridharan .",
    "selective sampling and active learning from single and multiple teachers .",
    "_ journal of machine learning research _",
    ", 2012 .",
    "a.  ehrenfeucht , d.  haussler , m.  kearns , and l.  g. valiant .",
    "a general lower bound on the number of examples needed for learning . _ information and computation _ , 1989 .",
    "y.  freund , h.s .",
    "seung , e.  shamir , and n.  tishby .",
    "selective sampling using the query by committee algorithm .",
    "_ machine learning _",
    ", 280 ( 2 - 3):0 133168 , 1997 .    e.  j. friedman .",
    "active learning for smooth problems . in _ colt _ , 2009 .",
    "r.  j. gardner . .",
    "soc . _ , 2002 .",
    "e.  gin and v.  koltchinskii .",
    "concentration inequalities and asymptotic results for ratio type empirical processes . _",
    "the annals of probability _ , 340 ( 3):0 11431216 , 2006 .",
    "a.  gonen , s.  sabato , and s.  shalev - shwartz .",
    "efficient pool - based active learning of halfspaces . in _",
    "icml _ , 2013 .",
    "s.  hanneke .",
    "a bound on the label complexity of agnostic active learning . in _ icml _ , 2007 .",
    "s.  hanneke .",
    "rates of convergence in active learning .",
    "_ the annals of statistics _ , 390 ( 1):0 333361 , 2011 .",
    "s.  hanneke and l.  yang .",
    "surrogate losses in passive and active learning , 2012 . http://arxiv.org/abs/1207.3772 .",
    "d.  haussler , n.  littlestone , and m.  k. warmuth .",
    "predicting @xmath296-functions on randomly drawn points . _ information and computation _",
    ", 1150 ( 2):0 129161 , 1994 .",
    "david haussler and emo welzl .",
    "epsilon nets and simplex range queries . _ disc .",
    "comp . geometry _ , 2:0 127151 , 1987 .",
    "a.  kalai , a.  klivans , y.  mansour , and r.  servedio .",
    "agnostically learning halfspaces . in _ proceedings of the 46th annual symposium on the foundations of computer science ( focs ) _ , 2005 .",
    "m.  kearns and u.  vazirani .",
    "_ an introduction to computational learning theory_. mit press , 1994 .",
    "a.  r. klivans , p.  m. long , and r.  a. servedio . learning halfspaces with malicious noise .",
    "_ jmlr _ , 2009 .",
    "a.  r. klivans , p.  m. long , and a.  tang .",
    "s algorithm learns intersections of halfspaces with respect to log - concave distributions . in _",
    "random _ , 2009 .",
    "v.  koltchinskii .",
    "rademacher complexities and bounding the excess risk in active learning . _ journal of machine learning research _ , 11:0 24572485 , 2010 .",
    "j.  komls , j.  pach , and g.  woeginger .",
    "almost tight bounds on epsilon - nets . _ discrete and computational geometry _ , 7:0 163173 , 1992 .",
    "s.  r. kulkarni , s.  k. mitter , and j.  n. tsitsiklis . .",
    "_ machine learning _ , 1993 .",
    "p.  m. long . on the sample complexity of pac learning halfspaces against the uniform distribution .",
    "_ ieee transactions on neural networks _ , 60 ( 6):0 15561559 , 1995 .",
    "p.  m. long .",
    "an upper bound on the sample complexity of pac learning halfspaces with respect to the uniform distribution . _ information processing letters _ , 2003 .",
    "l.  lovasz and s.  vempala . .",
    "_ random structures and algorithms _ , 2007 .",
    "e.  mammen and a.b . tsybakov .",
    "smooth discrimination analysis . _ the annals of statistics _ , 27:0 18081829 , 1999 .",
    "p.  massart and e.  nedelec .",
    "risk bounds for statistical learning . _",
    "the annals of statistics _ , 2006 .",
    "s.  mendelson . estimating the performance of kernel classes .",
    "_ journal of machine learning research _ , 4:0 759771 , 2003 .",
    "r.  nowak . .",
    "_ ieee transactions on information theory _ , 2011 .",
    "j.  pach and p.k .",
    "agarwal . _ combinatorial geometry_. john wiley and sons , 1995 .",
    "m.  raginsky and a.  rakhlin .",
    "lower bounds for passive and active learning . in _ nips _ , 2011 .",
    "l.g . valiant .",
    "a theory of the learnable . _ communications of the acm _ , 270 ( 11):0 11341142 , 1984 .",
    "s.  van de geer .",
    "_ empirical processes in m - estimation_. cambridge series in statistical and probabilistic methods , 2000 .",
    "a.  van der vaart and j.  a. wellner .",
    "_ weak convergence and empirical processes with applications to statistics_. springer , 1996 .    v.  vapnik and a.  chervonenkis . on the uniform convergence of relative frequencies of events to their probabilities",
    "_ theory of probability and its applications _ , 160 ( 2):0 264280 , 1971 .    v.  n. vapnik . _ estimation of dependencies based on empirical data_. springer verlag , 1982 .    v.  n. vapnik .",
    "_ statistical learning theory_. john wiley and sons , 1998 .",
    "s.  vempala .",
    "a random - sampling - based algorithm for learning intersections of halfspaces .",
    "_ jacm _ , 570 ( 6 ) , 2010 .",
    "* learning with noise . alexander capacity and the disagreement coefficient *   roughly speaking the alexander capacity  @xcite quantifies how fast the region of disagreement of the set of classifiers at distance @xmath100 of the optimal classifier collapses as a function @xmath100 ; of a set of classifiers @xmath43 is the of set of instances @xmath34 s.t . for each @xmath297 there",
    "exist two classifiers @xmath298 that disagree about the label of @xmath34 . ]",
    "the disagreement coefficient  @xcite additionally involves the supremum of @xmath100 over a range of values .",
    "@xcite provides guarantees on these quantities ( for sufficiently small @xmath100 ) for general classes of functions in @xmath76 if the underlying data distribution is sufficiently smooth .",
    "our analysis implies much tighter bounds for linear separators under log - concave distributions ( matching what was known for the much less general case of nearly uniform distribution over the unit sphere ) ; furthermore , we also analyze the nearly log - concave case where we allow an arbitrary number of discontinuities , a case not captured by the @xcite conditions at all .",
    "this immediately implies concrete bounds on the labeled data complexity of several algorithms in the literature including the @xmath31 algorithm  @xcite and the dhm algorithm  @xcite , with implications for the purely agnostic case ( i.e. , arbitrary forms of noise ) , as well as the koltchinskii s algorithm  @xcite and the cal algorithm  @xcite .",
    "furthermore , in the realizable case and under tsybakov noise , we show even better bounds , by considering aggressive active learning algorithms .",
    "note that as opposed to the realizable case , all existing active learning algorithms analyzed under massart and tsybakov noise conditions using the learning model analyzed in this paper ( including our algorithms in theorem  [ th : agg.margin : tsybakov ] ) , as well as those for the agnostic setting , are not known to run in time @xmath299 .",
    "in fact , even ignoring the optimality of sample complexity , there are no known algorithms for passive learning that run in time @xmath299 for general values of @xmath29 , even for the massart noise condition and under log - concave distributions .",
    "existing works on agnostic passive learning under log - concave distributions either provide running times @xmath300 ( e.g. , the work of  @xcite ) or can only achieve values of @xmath29 that are significantly larger than the noise rate  @xcite .    *",
    "other work on active learning *   several papers  @xcite present efficient online learning algorithms in the selective sampling framework , where labels must be actively queried before they are revealed . under the assumption that the label conditional distribution is linear function determined by a fixed target vector , they provide bounds on the regret of the algorithm and on the number of labels it queries when faced with an adaptive adversarial strategy of generating the instances .",
    "as pointed by  @xcite , these results can be converted to a statistical setting when the instances @xmath301 are drawn i.i.d and they further assume a margin condition .",
    "in this setting they obtain exponential improvement in label complexity over passive learning .",
    "while very interesting , these results are incomparable to ours ; their techniques significantly exploit the linear noise condition to get these improvements  note that such an improvement would not be possible in the realizable case ( as pointed for example in  @xcite ) .",
    "@xcite considers an interesting abstract `` generalized binary search '' problem with applications to active learning ; while these results apply for more general concept spaces , it is not clear how to implement the resulting procedures in polynomial time and by using access to only a polynomial number of unlabeled samples from the underlying distribution ( as required by the active learning model ) .",
    "another interesting recent work is that of  @xcite , which study active learning of linear separators via an aggressive algorithm using a margin condition , using a general approximation guarantee on the number of labels requested ; note that while these results work for potentially more general distributions , as opposed to ours , they do not come with explicit ( tight ) bounds on the label complexity .",
    "* @xmath29-nets , learning , and geometry *   small @xmath29-nets are useful for many applications , especially in computational geometry ( see @xcite ) .",
    "the same fundamental techniques of @xcite have been applied to establish the existence of small @xmath29-nets @xcite and to bound the sample complexity of learning  @xcite , and a number of interesting upper and lower bounds on the smallest possible size of @xmath29-nets have been obtained @xcite .",
    "our analysis implies a @xmath302 upper bound on the size of an @xmath29-net for a set of regions of disagreement between all possible linear classifiers and the target , when the distribution is zero - mean and log - concave . in particular",
    ", since in theorem  [ t : passive ] we prove that any hypothesis consistent with the training data has error rate @xmath174 with probability @xmath2 , setting @xmath295 to a constant gives a proof of a @xmath302 bound on the size of an @xmath29-net for the following set : @xmath303",
    "* lemma  [ l : angle ] . * _ assume @xmath42 is an isotropic log - concave in @xmath57",
    ". then there exists @xmath75 such that for any two unit vectors @xmath20 and @xmath21 in @xmath76 we have @xmath77 _    consider two unit vectors @xmath20 and @xmath21 .",
    "let @xmath242 denote the projection operator that , given @xmath243 , orthogonally projects @xmath34 onto the plane determined by @xmath20 and @xmath21 .",
    "that is , if we define an orthogonal coordinate system in which coordinates @xmath304 lie in this plane and coordinates @xmath305 are orthogonal to this plane , then @xmath306 . also , given distribution @xmath42 over @xmath57 , define @xmath307 to be the distribution given by first picking @xmath308 and then outputting @xmath309 .",
    "that is , @xmath307 is just the marginal distribution over coordinates @xmath304 in the above coordinate system .",
    "notice that if @xmath309 then @xmath310 where @xmath311 and @xmath312 .",
    "so , if @xmath244 then @xmath313    by lemma  [ isotropic - basic](c ) , we have that if @xmath42 is isotropic and log - concave , then @xmath246 is as well . let @xmath314 to be the region of disagreement between @xmath315 and @xmath316 intersected with the ball of radius @xmath179 in @xmath317 .",
    "the probability mass of @xmath314 under @xmath246 is at least the volume of @xmath314 times @xmath318 .",
    "so , using lemma  [ isotropic - basic](b ) @xmath319 as desired .",
    "in this section we analyze label complexity for active learning under the popular massart and tsybakov noise conditions , proving theorem  [ th : agg.margin : tsybakov ] .",
    "we consider a variant of the tsybakov noise condition @xcite .",
    "we assume that the classifier @xmath229 that minimizes @xmath279 is a linear classifier , and that , for the weight vector @xmath280 of that optimal classifier , there exist known parameters @xmath281 such that , for all @xmath282 , we have @xmath472    by generalizing theorem  [ lemma : vectors - sophist ] so that it provides a stronger bound for larger margins , and combining the result with the other lemmas of this paper and techniques from @xcite , we get the following .    * theorem  [ th : agg.margin : tsybakov ] . * _ let @xmath284 .",
    "assume that the distribution @xmath36 satisfies the tsybakov noise condition for constants @xmath285 and @xmath286 , and that the marginal @xmath42 on @xmath287 is isotropic log - concave .",
    "( 1 ) if @xmath288 , we can find a separator with excess error @xmath174 with probability @xmath28 using @xmath289 labeled examples in the active learning model , and @xmath15 labeled examples in the passive learning model .",
    "( 2 ) if @xmath290 , we can find a separator with excess error @xmath174 with probability @xmath28 using @xmath291 labeled examples in the active learning model .",
    "_    note that the case where @xmath288 is more general than the well - known massart noise condition @xcite . in this case , for active learning , theorem  [ th : agg.margin : tsybakov ] improves over the previously best known results  @xcite by a ( disagreement coefficient ) @xmath473 factor . for passive learning ,",
    "the bound on the total number of examples needed improves by @xmath474 factor the previously known best bound of  @xcite .",
    "it is consistent with recent lower bounds of  @xcite that include @xmath474 because those bounds are for a worst - case domain distribution , subject to a constraint on @xmath475 .",
    "when @xmath290 , the previously best result for active learning  @xcite is @xmath476 combining this with our new bound on @xmath473 ( theorem  [ th : dis - coeff : beta ] ) we get a bound of @xmath477 for log - concave distributions .",
    "so our theorem  [ th : agg.margin : tsybakov ] saves roughly a factor of @xmath478 , at the expense of an extra @xmath479 factor .          * input * : a sampling oracle for @xmath114 , and a labeling oracle a sequence of sample sizes @xmath115 , @xmath116 ; a sequence of cut - off values @xmath117 , @xmath116 a sequence of hypothesis space radii @xmath480 , @xmath116 ; a sequence of precision values @xmath481 , @xmath116      * pick random @xmath482 : @xmath483 . *",
    "draw @xmath119 examples from @xmath484 , label them and put into @xmath485 . *",
    "* * iterate * @xmath121 * * * find @xmath486 ( @xmath123 ) to approximately minimize training error : @xmath487 .",
    "* * * clear the working set @xmath485 * * * until @xmath126 additional data points are labeled , draw sample @xmath34 from @xmath484 * * * * if @xmath127 , reject @xmath34 * * * * otherwise , ask for label of @xmath34 , and put into @xmath485 + * end iterate *              assume now the claim is true for @xmath142 ( @xmath491 ) .",
    "then at the @xmath144-th iteration , we can let @xmath492 @xmath149 has excess errors at most @xmath493 , implying , using ( [ e : angle.up.massart ] ) , that @xmath494 . by assumption , @xmath495 .",
    "taking the sum , we obtain : @xmath498 therefore : @xmath499 by standard vapnik - chervonenkis bounds , we can choose @xmath219 s.t .  with @xmath159 samples ,",
    "we obtain @xmath500 with probability @xmath501 .",
    "therefore @xmath502 with probability @xmath490 , as desired .            [ lemma : vectors - sophist.bbig ] there is a positive constant @xmath75 such that the following holds .",
    "let @xmath20 and @xmath21 be two unit vectors in @xmath57 , and assume that @xmath503 .",
    "assume that @xmath42 is isotropic log - concave in @xmath57 .",
    "then , for any @xmath504 , we have @xmath505 for absolute constants @xmath506 and @xmath507 .",
    "next , we claim that each member @xmath34 of @xmath86 has @xmath508 .",
    "assume without loss of generality that @xmath89 is positive .",
    "( the other case is symmetric . )",
    "then @xmath90 , so the angle of @xmath34 with @xmath20 is obtuse , i.e. @xmath91 . since @xmath509 , this implies that @xmath510 but @xmath94 , and @xmath21 is unit length , so @xmath95 , which , using ( [ e : theta.big.bbig ] ) , implies @xmath96 which , since @xmath511 for all @xmath512 $ ] , in turn implies @xmath96 this implies that , if @xmath99 is a ball of radius @xmath100 in @xmath101 , that @xmath513 = \\sum_{i=1}^{\\infty }                  { \\mathbb p } [ e \\cap ( b((i+1 ) ( b/\\eta ) ) - b(i ( b/\\eta ) ) ) ] .\\ ] ] let us bound one of the terms in rhs .",
    "choose @xmath103 .",
    "let @xmath515 . applying the density upper bound from lemma  [ isotropic - basic ] with @xmath87",
    ", there are constants @xmath106 and @xmath107 such that @xmath514 \\\\ & & \\leq    \\int_{(x_1,x_2 ) \\in r_i }       1_e(x_1,x_2 ) c_1 \\exp(- ( b/\\eta ) c_2 i ) dx_1 dx_2 \\\\ & & =   c_1 \\exp(- ( b/\\eta ) c_2 i ) \\cdot \\\\ & &      \\int_{(x_1,x_2 ) \\in r_i } 1_e(x_1,x_2 ) \\ ; dx_1 dx_2.\\end{aligned}\\ ] ] if we include @xmath516 in the integral again , we get @xmath514 \\\\ & & \\leq   c_1 \\exp(- ( b/\\eta ) c_2 i )   \\int_{(x_1,x_2 ) \\in b((i+1 ) ( b/\\eta ) ) } 1_e(x_1,x_2 ) \\ ; dx_1 dx_2.\\end{aligned}\\ ] ] now , we exploit the fact that the integral above is a rescaling of a probability with respect to the uniform distribution .",
    "let @xmath110 be the volume of the unit ball in @xmath101 .",
    "then , we have @xmath514 \\\\ & & \\leq   c_1 \\exp(- ( b/\\eta ) c_2 i )   c_3 ( i+1)^2 ( b/\\eta)^2 \\eta/\\pi \\\\ & & = c_4 ( b/\\eta)^2 \\eta ( i+1)^2 \\exp(-(b/\\eta ) c_2 i ) , \\end{aligned}\\ ] ] for @xmath111 .",
    "returning to ( [ e : shells.bbig ] ) , we get @xmath517 & = & \\sum_{i=1}^{\\infty }   c_4 ( b/\\eta)^2 \\eta ( i+1)^2 \\exp(-(b/\\eta ) c_2 i )   \\\\ & = & c_4   ( b/\\eta)^2 \\eta \\sum_{i=1}^{\\infty } ( i+1)^2 \\exp(-(b/\\eta ) c_2 i ) \\\\ & = & c_4   ( b/\\eta)^2 \\times \\frac{4 e^{2 ( b/\\eta ) c_2 } - 3 e^{(b/\\eta ) c_2 } + 1}{\\left(e^{(b/\\eta ) c_2 } - 1 \\right)^3 } \\times \\eta.\\end{aligned}\\ ] ] now , if @xmath518 , we have @xmath517 & \\leq &     c_4   ( b/\\eta)^2 \\times \\frac{5 e^{2 ( b/\\eta ) c_2}}{\\left(e^{(b/\\eta ) c_2}/2\\right)^3 } \\times \\eta \\\\     & \\leq &     c_5   \\eta \\times ( b/\\eta)^2 \\exp(-(b/\\eta ) c_2 ) \\mbox{(where                                    $ c_5 = 40 c_4 $ ) } \\\\     & = &     c_5   \\eta \\times \\exp(-(b/\\eta ) c_2 + 2 \\ln ( b/\\eta ) ) \\\\     & \\leq &     c_5   \\eta \\times \\exp(-(b/\\eta ) c_2/2),\\end{aligned}\\ ] ] completing the proof .      under the noise condition  [ eq : nonsep - tsybakov ] and from the log - concavity assumption , we obtain that there exists @xmath75 such that for all @xmath282 we have : @xmath519 let us denote by @xmath520 . for all @xmath282",
    ", we have : @xmath521      assume now the claim is true for @xmath142 ( @xmath491 ) . then at the @xmath144-th iteration , we can let @xmath524",
    ", @xmath149 has excess errors at most @xmath525 , implying @xmath526 by assumption , @xmath527 .      by standard bounds",
    ", we can choose @xmath106 , @xmath107 and @xmath110 s.t .  with @xmath159 samples",
    ", we obtain @xmath531 with probability @xmath532 .",
    "therefore @xmath533 with probability @xmath165 , as desired , completing the proof of theorem  [ th : agg.margin : tsybakov ] ."
  ],
  "abstract_text": [
    "<S> we provide new results concerning label efficient , polynomial time , passive and active learning of linear separators . we prove that active learning provides an exponential improvement over pac ( passive ) learning of homogeneous linear separators under nearly log - concave distributions . </S>",
    "<S> building on this , we provide a computationally efficient pac algorithm with optimal ( up to a constant factor ) sample complexity for such problems . </S>",
    "<S> this resolves an open question of  @xcite concerning the sample complexity of efficient pac algorithms under the uniform distribution in the unit ball . </S>",
    "<S> moreover , it provides the first bound for a polynomial - time pac algorithm that is tight for an interesting infinite class of hypothesis functions under a general and natural class of data - distributions , providing significant progress towards a longstanding open question of  @xcite .    </S>",
    "<S> we also provide new bounds for active and passive learning in the case that the data might not be linearly separable , both in the agnostic case and and under the tsybakov low - noise condition . to derive our results , we provide new structural results for ( nearly ) log - concave distributions , which might be of independent interest as well .    active learning , pac learning , erm , nearly log - concave distributions , tsybakov low - noise condition , agnostic learning . </S>"
  ]
}