{
  "article_text": [
    "many methods have been developed so far for solving differential equations .",
    "some of them produce a solution in the form of an array that contains the value of the solution at a selected group of points .",
    "others use basis - functions to represent the solution in analytic form and transform the original problem usually in a system of linear equations .",
    "most of the previous work in solving differential equations using neural networks is restricted to the case of solving the linear systems of algebraic equations which result from the discretization of the domain .",
    "the solution of a linear system of equations is mapped onto the architecture of a hopfield neural network .",
    "the minimization of the network s energy function provides the solution to the system of equations @xcite .",
    "another approach to the solution of ordinary differential equations is based on the fact that certain types of splines , for instance linear b - splines , can be derived by the superposition of piecewise linear activation functions @xcite .",
    "the solution of a differential equation using linear b - splines as basis functions , can be obtained by solving a system of linear or non - linear equations in order to determine the parameters of the splines .",
    "such a solution form is mappped directly on the architecture of a feedforward neural network by replacing each spline with the sum of piecewise linear activation functions that correspond to the hidden units .",
    "this method considers local basis - functions and in general requires many splines ( and consequently network parameters ) in order to yield accurate solutions .",
    "furthermore it is not easy to extend these techniques to multidimensional domains .",
    "in this article we view the problem from a different angle .",
    "we present a general method for solving both ordinary differential equations ( odes ) and partial differential equations ( pdes ) , that relies on the function approximation capabilities of feedforward neural networks and results in the construction of a solution written in a diferentiable , closed analytic form .",
    "this form employs a feedforward neural network as the basic approximation element , whose parameters ( weights and biases ) are adjusted to minimize an appropriate error function . to train the network",
    "we employ optimization techniques , which in turn require the computation of the gradient of the error with respect to the network parameters . in the proposed approach",
    "the model function is expressed as the sum of two terms : the first term satisfies the initial / boundary conditions and contains no adjustable parameters .",
    "the second term involves a feedforward neural network to be trained so as to satisfy the differential equation . since it is known that a multilayer perceptron with one hidden layer can approximate any function to arbitrary accuracy",
    ", it is reasonable to consider this type of network architecture as a candidate model for treating differential equations .",
    "the employement of a neural architecture adds to the method many attractive features :    * the solution via ann s is a _ differentiable , closed analytic form _",
    "easily used in any subsequent calculation .",
    "most other techniques offer a discrete solution ( for example predictor - corrector , or runge - kutta methods ) or a solution of limited differentiability ( for example finite elements ) .",
    "* such a solution is characterized by the generalization properties of neural networks , which are known to be superior .",
    "( comparative results presented in this work illustrate this point clearly . ) * the required number of model parameters is far less than any other solution technique and therefore , compact solution models are obtained , with very low demand on memory space . *",
    "the method is general and can be applied to odes , systems of odes and to pdes as well . *",
    "the method can be realized in hardware , using neuroprocessors , and hence offer the opportunity to tackle in real time difficult differential equation problems arising in many engineering applications . *",
    "the method can also be efficiently implemented on parallel architectures .    in the next section",
    "we describe the general formulation of the proposed approach and derive formulas for computing the gradient of the error function .",
    "section 3 illustrates some classes of problems where the proposed method can be applied and describes the appropriate form of the trial solution .",
    "section 4 presents numerical examples from the application of the technique to several test problems and provides details concerning the implementation of the method and the accuracy of the obtained solution .",
    "we also make a comparison of our results with those obtained by the finite element method for the examined pde problems .",
    "finally , section 6 contains conclusions and directions for future research .",
    "the proposed approach will be illustrated in terms of the following general differential equation definition : @xmath0 subject to certain boundary conditions ( b.cs ) ( for instance dirichlet and/or neumann ) , where @xmath1 , @xmath2 denotes the definition domain and @xmath3 is the solution to be computed .",
    "the proposed approach can be also applied to differential equations of higher order , but we have not considered any problems of this kind in the present work .    to obtain a solution to the above differential equation",
    "the collocation method is adopted @xcite which assumes a discretization of the domain @xmath4 and its boundary @xmath5 into a set points @xmath6 and @xmath7 respectively .",
    "the problem is then transformed into the following system of equations : @xmath8 subject to the constraints imposed by the b.cs .    if @xmath9 denotes a trial solution with adjustable parameters @xmath10 , the problem is transformed to :    @xmath11    subject to the constraints imposed by the b.cs .    in the proposed approach the trial solution @xmath12 employs a feedforward neural network and the parameters",
    "@xmath10 correspond to the weights and biases of the neural architecture .",
    "we choose a form for the trial function @xmath13 such that by construction satisfies the bcs .",
    "this is achieved by writing it as a sum of two terms : @xmath14 where @xmath15 is a single - output feedforward neural network with parameters @xmath10 and @xmath16 input units fed with the input vector @xmath17 .",
    "the term @xmath18 contains no adjustable parameters and satisfies the boundary conditions .",
    "the second term @xmath19 is constructed so as not to contribute to the bcs , since @xmath13 must also satisfy them .",
    "this term employs a neural network whose weights and biases are to be adjusted in order to deal with the minimization problem .",
    "note at this point that the problem has been reduced from the original constrained optimization problem to an unconstrained one ( which is much easier to handle ) due to the choice of the form of the trial solution that satisfies by construction the b.cs .    in the next section we present a systematic way to construct the trial solution , i.e. the functional forms of both @xmath20 and @xmath19 .",
    "we treat several common cases that one frequently encounters in various scientific fields .",
    "as indicated by our experiments , the approach based on the above formulation is very effective and provides in reasonable computing time accurate solutions with impressive generalization ( interpolation ) properties .",
    "the efficient minimization of equation ( 3 ) can be considered as a procedure of training the neural network where the error corresponding to each input vector @xmath21 is the value @xmath22 which has to become zero .",
    "computation of this error value involves not only the network output ( as is the case in conventional training ) but also the derivatives of the output with respect to any of its inputs .",
    "therefore , in computing the gradient of the error with respect to the network weights , we need to compute not only the gradient of the network but also the gradient of the network derivatives with respect to its inputs .    consider a multilayer perceptron with @xmath16 input units , one hidden layer with @xmath23 sigmoid units and a linear output unit .",
    "the extension to the case of more than one hidden layers can be obtained accordingly . for a given input vector",
    "@xmath24 the output of the network is @xmath25 where @xmath26 , @xmath27 denotes the weight from the input unit @xmath28 to the hidden unit @xmath29 , @xmath30 denotes the weight from the hidden unit @xmath29 to the output , @xmath31 denotes the bias of hidden unit @xmath29 and @xmath32 is the sigmoid transfer function .",
    "it is straightforward to show that : @xmath33 where @xmath34 and @xmath35 denotes the @xmath36 order derivative of the sigmoid .",
    "moreover it is readily verifiable that : @xmath37 where @xmath38 and @xmath39",
    ".    equation ( 6 ) indicates that the derivative of the network with respect to any of its inputs is equivalent to a feedforward neural network @xmath40 with one hidden layer , having the same values for the weights @xmath27 and thresholds @xmath31 and with each weight @xmath30 being replaced with @xmath41 .",
    "moreover the transfer function of each hidden unit is replaced with the @xmath42 order derivative of the sigmoid .",
    "therefore the gradient of @xmath43 with respect to the parameters of the original network can be easily obtained as : @xmath44 @xmath45 @xmath46    once the derivative of the error with respect to the network parameters has been defined it is then straightforward to employ almost any minimization technique . for example",
    "it is possible to use either the steepest descent ( i.e. the backpropagation algorithm or any of its variants ) , or the conjugate gradient method or other techniques proposed in the literature . in our experiments we have employed the bfgs method @xcite that is quadraticly convergent and has demonstrated excellent performance .",
    "it must also be noted that for a given grid point the derivatives of each network ( or gradient network ) with respect to the parameters may be obtained simultaneously in the case where parallel harware is available .",
    "moreover , in the case of backpropagation , the on - line or batch mode of weight updates may be employed .",
    "to illustrate the method , we consider the _ first order ode _ : @xmath47 with @xmath48 $ ] and with the ic @xmath49 .",
    "a trial solution is written as : @xmath50 where @xmath51 is the output of a feedforward neural network with one input unit for @xmath52 and weights @xmath53 .",
    "note that @xmath54 satisfies the ic by construction .",
    "the error quantity to be minimized is given by : @xmath55=\\sum_i\\ { \\frac{d\\psi_t(x_i)}{dx}-f(x_i,\\psi_t(x_i))\\}^2\\ ] ] where the @xmath56 s are points in @xmath57 $ ] . since @xmath58 , it is straightforward to compute the gradient of the error with respect to the parameters @xmath10 using equations ( 5)-(10 ) .",
    "the same holds for all subsequent model problems .",
    "the same procedure can be applied to the _ second order ode _ : @xmath59 for the _ initial value _ problem : @xmath49 and @xmath60 , the trial solution can be cast as : @xmath61 for the _ two point dirichlet _ bc : @xmath49 and @xmath62 , the trial solution is written as : @xmath63 in the above two cases of second order odes the error function to be minimized is given by equation ( 12 ) .    for _ systems of @xmath64",
    "first order odes _",
    "@xmath65 with @xmath66 , @xmath67 we consider one neural network for each trial solution @xmath68 @xmath67 which is written as : @xmath69 and we minimize the following error quantity : @xmath55=\\sum_{k=1}^k\\sum_i\\ { \\frac{d\\psi_{t_k}(x_i)}{dx }     -f_k(x_i,\\psi_{t_1},\\psi_{t_2 } , \\ldots , \\psi_{t_k})\\}^2\\ ] ]      we treat here two  dimensional problems only .",
    "however it is straightforward to extend the method to more dimensions .",
    "for example consider the _ poisson equation _ : @xmath70 @xmath48 , y \\in [ 0,1]$ ] with _ dirichlet _ bc : @xmath71 , @xmath72 and @xmath73 , @xmath74 .",
    "the trial solution is written as : @xmath75 where @xmath76 is chosen so as to satisfy the bc , namely : @xmath77\\ } + y\\{g_1(x)-[(1-x)g_1(0)+xg_1(1)]\\}\\ ] ]    for _ mixed boundary conditions _ of the form : @xmath71 , @xmath72 , @xmath73 and @xmath78 ( i.e. _ dirichlet _ on part of the boundary and _ neumann _ elsewhere ) , the trial solution is written as : @xmath79\\ ] ] and @xmath80 is again chosen so as to satisfy the bcs : @xmath81 +   y\\{g_1(x)-[(1-x)g_1(0)+xg_1(1)]\\}\\ ] ]    note that the second term of the trial solution does not affect the boundary conditions since it vanishes at the part of the boundary where dirichlet bcs are imposed and its gradient component normal to the boundary vanishes at the part of the boundary where neumann bcs are imposed .",
    "in all the above pde problems the error to be minimized is given by : @xmath82=\\sum_i   \\ { \\frac{\\partial^2}{\\partial x^2}\\psi(x_i , y_i ) + \\frac{\\partial^2}{\\partial y^2}\\psi(x_i , y_i ) - f(x_i , y_i)\\}^2\\ ] ] where @xmath83 are points in @xmath57\\times [ 0,1]$ ] .",
    "in this section we report on the solution of a number of model problems . in all cases",
    "we used a multilayer perceptron having one hidden layer with 10 hidden units and one linear output unit .",
    "the sigmoid activation of each hidden unit is @xmath84 . for each test problem",
    "the exact analytic solution @xmath85 was known in advance .",
    "therefore we test the accuracy of the obtained solutions by computing the deviation @xmath86 . to perform the error minimization we employed the _ merlin _",
    "@xcite optimization package .",
    "_ merlin _ provides an environment for multidimensional continuous function optimization . from the several algorithms that are implemented therein , the quasi  newton _ bfgs _",
    "@xcite method seemed to perform better in these kind of problems and hence we used it in all of our experiments .",
    "a simple criterion for the gradient norm was used for termination .    in order to illustrate the characteristics of the solutions provided by the neural method",
    ", we provide figures displaying the corresponding deviation @xmath87 both at the few points ( training points ) that were used for training and at many other points ( test points ) of the domain of each equation .",
    "the second kind of figures are of major importance since they show the interpolation capabilities of the neural solutions which seem to be superior compared to other solutions . moreover , in the case of odes we also consider points outside the training interval in order to obtain an estimate of the extrapolation performance of the obtained solution .",
    "@xmath88 with @xmath89 and @xmath48 $ ] .",
    "the analytic solution is @xmath90 and is displayed in figure 1a . according to equation ( 11 ) the trial neural form of the solution is taken to be : @xmath91 .",
    "the network was trained using a grid of 10 equidistant points in [ 0,1 ] .",
    "figure 2 displays the deviation @xmath92 from the exact solution corresponding at the grid points ( small circles ) and the deviation at many other points in @xmath57 $ ] as well as outside that interval ( dashed line ) .",
    "it is clear that the solution is of high accuracy , although training was performed using a small number of points .",
    "moreover , the extrapolation error remains low for points near the equation domain .",
    "@xmath93 with @xmath94 and @xmath95 $ ] .",
    "the analytic solution is @xmath96 and is presented in figure 1b .",
    "the trial neural form is : @xmath97 according to equation ( 11 ) . as before we used a grid of 10 equidistant points in [ 0,2 ] to perform the training . in analogy with the previous case ,",
    "figure 3 display the deviation @xmath92 at the grid points ( small circles ) and at many other points inside and outside the training interval ( dashed line ) .",
    "@xmath98 consider the _",
    "initial value _ problem : @xmath94 and @xmath99 with @xmath95 $ ] .",
    "the exact solution is : @xmath100 and the trial neural form is : @xmath101 ( from equation ( 13 ) ) .",
    "+ consider also the _ boundary value _ problem : @xmath94 and @xmath102 , @xmath48 $ ] .",
    "the exact solution is the same as above , but the appropriate trial neural form is : @xmath103 ( from equation ( 14 ) ) .",
    "again as before we used a grid of 10 equidistant points and the plots of the deviation from the exact solution are displayed at figures 4 and 5 for the initial value and boundary value problem respectively .",
    "the interpretation of the figures is the same as in the previous cases .    from all the above cases it",
    "is clear that method can handle effectively all kinds of odes and provides analytic solutions that remain to be of the same accuracy at points other from the training ones .",
    "consider the system of two coupled first order odes : @xmath104 @xmath105 with @xmath106 $ ] and @xmath107 and @xmath108 .",
    "the analytic solutions are @xmath109 and @xmath110 and are displayed at figure 6a and 6b , respectively . following equation ( 15 )",
    "the trial neural solutions are : @xmath111 and @xmath112 where the networks @xmath113 and @xmath114 have the same architecture as in the previous cases .",
    "results concerning the accuracy of the obtained solutions at the grid points ( small circles ) and at many other points ( dashed line ) are presented in figure 7 .",
    "we consider boundary value problems with dirichlet and neumann bcs .",
    "all subsequent problems were defined on the domain @xmath57\\times [ 0,1]$ ] and in order to perform training we consider a mesh of 100 points obtained by considering 10 equidistant points of the domain @xmath57 $ ] of each variable . in analogy with the previous cases the neural architecture was considered to be a mlp with two inputs ( accepting the coordinates @xmath52 and @xmath115 of each point ) , 10 sigmoid hidden units and one linear output unit .",
    "@xmath116 with @xmath117 $ ] and the dirichlet bcs : @xmath118 , @xmath119 and @xmath120 , @xmath121 .",
    "the analytic solution is @xmath122 and is displayed in figure 8 . using equation ( 17 ) the trial neural form",
    "must be written : @xmath123 and @xmath76 is obtained by direct substitution in the general form given by equation ( 18 ) : @xmath124\\ ] ] figure 9 presents the deviation @xmath125 of the obtained solution at the 100 grid points that were selected for training while figure 10 displays the deviation at 900 other points of the equation domain .",
    "it clear that the solution is very accurate and the accuracy remains high at all points of the domain .",
    "@xmath126 cos(a^2x^2+y ) + [ \\frac{1}{25}-1 -4a^4x^2+\\frac{a^2}{25 } ] sin(a^2x^2+y)\\}\\ ] ] with @xmath127 , @xmath117 $ ] and the dirichlet bcs as defined by the exact solution @xmath128 ( presented in figure 11 ) .",
    "again the trial neural form is : @xmath123 and @xmath76 is obtained similarly by direct substitution in equation ( 18 ) .",
    "accuracy results are presented in figure 12 for the training points and in figure 13 for test points .",
    "it can be shown that the accuracy is not the same as in the previous example , but it can be improved further by considering a neural network with more than 10 hidden units . from the figures it is also clear that the test error lies in the same range as the training error .",
    "@xmath129 with @xmath117 $ ] and with mixed bcs : @xmath130 , @xmath131 and @xmath132 , @xmath133 .",
    "the analytic solution is @xmath134 and is presented in figure 14 .",
    "the trial neural form is specified according to equation ( 19 ) @xmath135\\ ] ] where @xmath80 is obtained by direct substitution in equation ( 20 ) .",
    "the accuracy of the neural solution is depicted in figures 15 and 16 for training and test points respectively .",
    "this is an example of a _ non - linear _ pde .",
    "@xmath136 with the same mixed bcs as in the previous problem .",
    "the exact solution is again @xmath134 and the parametrization of the trial neural form is the same as in problem 7 .",
    "no plots of the accuracy are presented since they are almost the same with those of problem 7 .",
    "the above pde problems were also solved with the finite element method which has been widely acknowledged as one of the most effective approaches to the solution of differential equations .",
    "the characteristics of the finite element method employed in this work are briefly summarized below . in the finite element approach",
    "the unknowns are expanded in piecewise continuous biquadratic elements @xcite : @xmath137 where @xmath138 is the biquadratic basis function and @xmath139 is the unknown at the @xmath140 node of the element .",
    "the physical domain @xmath141 is mapped on the computational domain @xmath142 through the isoparametric mapping : @xmath143 @xmath144 where @xmath145 and @xmath16 are the local coordinates in the computational domain @xmath146 and @xmath56 , @xmath147 the @xmath140 node coordinates in the physical domain for the mapped element .    the used galerkin finite element method ( gfem )",
    "calls for the weighted residuals @xmath148 to vanish at each nodal position @xmath29 : @xmath149 where @xmath150 is given by equation ( 1 ) and @xmath151 is the jacobian of the isoparametric mapping : @xmath152 this requirement along with the imposed boundary conditions constitute a set of nonlinear algebraic equations ( @xmath153 ) .",
    "the inner products involved in the finite element formulation are computed using the nine - node gaussian quadrature .",
    "the system of equations is solved for the nodal coefficients of the basis function expansion using the newton s method forming the jacobian of the system explicitly ( for both linear and nonlinear differential operators ) : @xmath154 @xmath155 where the superscript @xmath16 denotes the iteration number and @xmath156 is the global jacobian of the system of equations @xmath157 : @xmath158 the initial guess @xmath159 is chosen at random . for linear problems convergence",
    "is achieved in one iteration and for non - linear problems in 1 - 5 iterations .",
    "all pde problems 5 - 8 are solved on a rectangular domain of 18@xmath160 18 elements resulting in a linear system with 1369 unknowns .",
    "this is in contrast with the neural approach which assumes a small number of parameters ( 30 for odes and 40 for pdes ) , but requires more sophisticated minimization algorithms .",
    "as the number of employed elements increases the finite element approach requires an excessive number of parameters .",
    "this fact may lead to memory requirements that exceed the available memory resources .",
    ".maximum deviation from the exact solution for the neural and the finite element methods .",
    "[ cols=\"^,^,^,^,^ \" , ]     in the finite element case , interpolation is performed using a rectangular grid of 23 @xmath160 23 equidistant points ( test points ) . for each pair of nodal",
    "coordinates @xmath161 of this grid , we correspond a pair of local coordinates @xmath142 of a certain element of the original grid where we have performed the computations . the interpolated values are computed as : @xmath162 for the element that corresponds to the global coordinates @xmath161 .",
    "it is clear that the solution is not expressed in closed analyical form as in the neural case , but additional computations are required in order to find the value of the solution at an arbitrary point in the domain .",
    "figures 17 - 22 display the deviation @xmath163 for pde problems 5 - 7 ( figures concerning problem 8 are similar to those of problem 7 ) . for each problem two figures are presented displaying the deviation at the training set and at the interpolation set of points respectively .",
    "table 1 reports the maximum deviation corresponding to the neural and to the finite element method at the training and at the interpolation set of points .",
    "it is obvious that at the training points the solution of the finite element method is very satisfactory and in some cases it is better than that obtained using the neural method .",
    "it is also clear that the accuracy at the interpolation points is orders of magnitude lower as compared to that at the training points . on the contrary",
    ", the neural method provides solutions of excellent interpolation accuracy , since , as table 1 indicates , the deviations at the training and at the interpolation points are comparable .",
    "it must also be stressed that the accuracy of the finite element method decreases as the size of the grid becomes smaller , and that the neural approach considers a mesh of 10@xmath16010 points while the in the finite element case a 18@xmath16018 mesh was employed .",
    "a method has been presented for solving differential equations that relies upon the function approximation capabilities of the feedforward neural networks and provides accurate and differentiable solutions in a closed analytic form .",
    "the success of the method can be attributed to two factors .",
    "the first one is the employment of neural networks that are excellent function approximators and the second is the form of the trial solution that satisfies by construction the bcs and therefore the constrained optimization problem becomes a substantially simpler unconstrained one .    unlike most previous approaches ,",
    "the method is general and can be applied to both odes and pdes by constructing the appropriate form of the trial solution . as indicated by our experiments",
    "the method exhibits excellent generalization performance since the deviation at the test points was in no case greater than the maximum deviation at the training points .",
    "this is in contrast with the finite element method where the deviation at the testing points was significantly greater than the deviation at the training points .",
    "we note that the neural architecture employed was fixed in all the experiments and we did not attempt to find optimal configurations or to study the effect of the number of hidden units on the performance of the method .",
    "moreover , we did not consider architectures containing more than one hidden layers .",
    "a study of the effect of the neural architecture on the quality of the solution constitutes one of our research objectives .",
    "another issue that needs to be examined is related with the sampling of the grid points that are used for training . in the above experiments",
    "the grid was constructed in a simple way by considering equidistant points .",
    "it is expected that better results will be obtained in the case where the grid density will vary during training according to the corresponding error values .",
    "this means that it is possible to consider more training points at regions where the error values are higher .",
    "it must also be stressed that the proposed method can easily be used for dealing with domains of higher dimensions ( three or more ) .",
    "as the dimensionality increases , the number of training points becomes large .",
    "this fact becomes a serious problem for methods that consider local functions around each grid point since the required number of parameters becomes excessively large and , therefore , both memory and computation time requirements become intractable . in the case of the neural method",
    "the number of training parameters remains almost fixed as the problem dimensionality increases .",
    "the only effect on the computation time stems from the fact that each training pass requires the presentation of more points , i.e. the training set becomes larger .",
    "this problem can be tackled by considering either parallel implementations , or implementations on a neuroprocessor that can be embedded in a conventional machine and provide considerably better execution times .",
    "such an implementation on neural hardware is one of our near future objectives , since it will permit the treatment of many difficult real world problems .",
    "finally we aim at extending the approach to treat other problems of similar nature , as for example eigenvalue problems for differential operators .",
    "yentis , r. and zaghoul , m.e . , _ vlsi implementation of locally connected neural network for solving partial differential equations _ , ieee trans on circuits and systems - i , vol .",
    "43 , no . 8 , pp .",
    "687 - 690 , 1996 ."
  ],
  "abstract_text": [
    "<S> we present a method to solve initial and boundary value problems using artificial neural networks . a trial solution of the differential equation is written as a sum of two parts . </S>",
    "<S> the first part satisfies the initial / boundary conditions and contains no adjustable parameters . </S>",
    "<S> the second part is constructed so as not to affect the initial / boundary conditions . </S>",
    "<S> this part involves a feedforward neural network , containing adjustable parameters ( the weights ) . hence by construction the initial / boundary conditions are satisfied and the network is trained to satisfy the differential equation . </S>",
    "<S> the applicability of this approach ranges from single ode s , to systems of coupled ode s and also to pde s . in this article </S>",
    "<S> we illustrate the method by solving a variety model problems and present comparisons with finite elements for several cases of partial differential equations .    </S>",
    "<S> = -1.0 cm 18.0 cm    = cmr12 = cmr11 = cmr16 = cmr24 </S>"
  ]
}