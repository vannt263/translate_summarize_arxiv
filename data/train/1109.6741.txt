{
  "article_text": [
    "the dark energy survey data management ( desdm ) is the part of the dark energy survey project   @xcite that will transfer , process and distribute the data generated by survey s camera decam  @xcite .",
    "desdm is a large , scalable system led by the national center for supercomputing applications at the university of illinois at urbana - champaign ( ncsa / uiuc ) consisting of :    1 .   an archive system for different levels of data .",
    "2 .   scientific codes to process raw data .",
    "3 .   database to support calibration , provenance and data analyses .",
    "4 .   web portals providing process control and easy access to images and catalogs . 5 .   hardware platforms for execution and storage .    as of august 2011 ,",
    "the desdm team consists of around 20 computing professionals and physicists from several institutions around the world , each providing their own expertise to its development . in this contribution",
    "we describe the basic design of the desdm and make particular emphasis on how the raw data in the form of ccd images is reduced to its final form as science - ready catalogs .",
    "we also present the testing campaigns that the system is undergoing and the outlook in the short term , in the context of the des project . for more information on other aspects of desdm , see also  @xcite  @xcite .",
    "the main functions of the system are schematically summarized in fig .",
    "[ fig : desdm_diagram ] .",
    "the design is driven by the science requirements document of the des project , flowed down to the technical level .",
    "additional requirements include the timely and reliable processing and the archiving of the data .",
    "the functions of the desdm are the following :    1 .",
    "* transfer .",
    "* raw images must be transferred from the telescope site at the rate of approximately 100 mbps .",
    "this rate takes into account that 360 science and associated calibration exposures are produced each night , totalling @xmath0 gb of data that have to be transported in less than 18 hours , to allow for nightly processing and feedback . some overhead is included to consider possible network outages .",
    "recent studies indicate normal data delivery will occur in near real time .",
    "this step is taken care of by the national optical astronomy observatory data transport system through a microwave downlink off the mountain and then by network to ncsa .",
    "2 .   * processing . *",
    "as the data arrive at ncsa , it is ingested into the system , and sent to high performance computing ( hpc ) resources on xsede @xcite and possibly the open science grid @xcite .",
    "data are staged using using gridftp and globusonline @xcite .",
    "the pipeline containing the parallelization , scientific codes ( see section [ sec : algorithms ] ) and quality assurance is executed for the night s images .",
    "archive and distribution .",
    "* results are then returned to the ncsa primary archive , and releases built .",
    "data files in releases are replicated to the secondary archive at fermilab and tertiary archives at collaborator s sites .",
    "data are also released generally to the collaboration .",
    "catalogs of objects from coadded images and single epoch images , as well as other meta - data are served to the collaboration using an ncsa - provided oracle rac system . after a proprietary period",
    ", data will be released to the community using the same methods .",
    "we expect the system will evolve to use vao protocols and tools in time for the public release .    ]",
    "in this section we describe what algorithms are involved in the reduction of the raw data once the process that sends it to the hpc platform has been started .",
    "the basic event in an astronomical observation is the _",
    "exposure_. the decam camera is exposed to the night sky for @xmath1 seconds , generating a file of slightly less than 1 gb in size , which is written in fits format . an exposure ( see fig.[fig : sample_image ] for an example ) consists of 62 ccd images of a part of the sky , covering a total solid angle of 3 square degrees , showing multiple sources and instrumental effects . around 300 scientific images are generated per night , for different pointings , together with about 60 calibration images .",
    "the former have to be corrected for instrumental effects ( detrending ) as well as calibrated for the absolute position ( astrometry ) and absolute flux ( photometry ) .    ]",
    "this pipeline contains several astronomy modules that together remove the decam instrumental signatures .",
    "it includes the following steps :    * exposure segmentation and crosstalk correction .",
    "the exposure is divided into 62 ccd images and crosstalk between them is accounted for , using the crosstalk coefficients measured from the calibration dataset @xmath2 where @xmath3 stands for the image of a particular ccd and @xmath4 is the crosstalk coefficient describing the fraction of image @xmath5 appearing in image @xmath6 .",
    "* image correction .",
    "this involves correcting for the pedestal or bias level of the ccds , eliminating non - imaging sections of the exposure , eliminating ghost images from multiple scattered light in the optics , removing illumination and fringing effects and correcting for pixel - to - pixel sensitivity variations ( flat - fielding ) .",
    "thermal noise from electrons is also treated but is usually negligible at the camera s working temperature . in this step additional calibration images ( taken during the same night , every season or during commissioning )",
    "are required and must be part of the job submitted to the hpc resource .",
    "after this process , raw data has been turned into reduced images with associated maps containing information on bad pixels and the weights of the image at each particular pixel ( inverse variance ) .",
    "this step requires the identification of positional standards in the exposure , i.e. , stars with very well - known positions in celestial coordinates .",
    "the coordinates of the brightest sources in the exposure are extracted as well ( using the sextractor  @xcite package ) in the exposures s own reference system in _",
    "x_-_y _ coordinates . knowing an approximate initial solution ( provided by the telescope s control system ) a match of both catalogs can be performed , as well as the fitting of the best transformation parameters from the image _",
    "x_-_y _ system to the celestial reference system ( absolute system in spherical coordinates ) .",
    "this part is performed by the scamp  @xcite software .",
    "additionally , the effect of the distortion caused by the optics towards the edges of the field , has to be included when calculating the astrometric solution .",
    "the actual digital counts observed in each pixel of the detrended image , have to be translated into physical flux units . in order to make this conversion , on every night in which conditions are sufficiently good ( moonless , stable skies ) specific calibration star fields will be imaged .",
    "for these stars , the flux is known or can be obtained ( @xmath7 ) and this can be compared to the total light measured in the image ( @xmath8 ) .",
    "this relationship is called the photometric equation and contains several unknowns which are ccd and filter dependent : @xmath9 where @xmath10 is the photometric zeropoint , representing the normalization value of the image ; @xmath11 is an instrumental coefficient known as _ color term _ which takes into account the shape of the response of the ccd to light in different filters ( @xmath12 ) ; and @xmath13 is the atmospheric extinction coefficient which considers the increased absorption by the atmosphere at the airmass denoted by @xmath14 ( dependent on sky angle ) .",
    "measuring multiple reference stars in the ccds for different filters and angles in the sky provides us with enough equations to solve the system , thus allowing us to find the photometric solution ( values of these constants and their errors ) .      in order to reach the scientific requirements of des in terms of depth ,",
    "it is necessary to perform a process called image coaddition .",
    "as the name implies , this is the combination of several overlapping single - epoch images in a given filter to improve the signal to noise of real sources in the image .",
    "moreover , if the different images thus co - added are slightly offset from one another , this procedure has the added bonus of improving the photometric calibration with a more robust determination of the solution , as the photometric solution will incorporate information from different parts of the sky simultaneously .",
    "another advantage is the possibility of eliminating transient effects from the final coadded image such as satellite trails or cosmic rays , as they will show up in only one of the images being added and are easily identifiable .",
    "prior to the coaddition itself , it is necessary to transform the flux values in the individual overlapping ccd pixels into a uniform pixel grid in which the coaddition can be performed ( _ remapping _ ) .",
    "to do this , artificial _ tiles _ in the sky are created , one degree on a side , and one coadded image per band is produced for every tile . for each single - epoch image",
    ", it is determined to which tiles it contributes to ( using swarp  @xcite ) .",
    "the photometric solution ( in particular , the zeropoints ) is re - evaluated for these new images .",
    "there are two main caveats to be pointed out :    * the color terms of the ccds , which are simple to take into account in single - epoch images , are not easily combined if they vary across the field .",
    "a solution is currently under implementation ; * the point spread function ( psf ) changes within an image and from one image to another , due to varying conditions of the exposure and quality of the sky .",
    "therefore the psf of the coadded image is subject to discontinuous jumps .    to deal with the second point above",
    ", a psf homogenization procedure has been developed .",
    "the model psf and its variation has been computed for each image during nightly processing using the psfex  @xcite package .",
    "we define the target psf to be used as a circular moffat @xcite function with a full width at half - maximum which is the median of the seeing distribution one in the whole set of images contributing to the coadd .",
    "going back to the individual component images with psf @xmath15 , we find the kernel @xmath16 which minimizes the difference with respect to this median target psf @xmath17 .",
    "@xmath18 where @xmath19 are the elements of a polynomial basis in @xmath20 .",
    "the kernel elements are stored and during the homogenization process , they are recovered to convolve with the image .",
    "once every image has been treated this way , the coaddition will not introduce any discontinuities from the psf variations .",
    "image coaddition takes place off - season due to its increased cpu requirements with respect to the nightly processing .",
    "coadd construction is to be carried out multiple times depending on the specific needs of each science working group , which have different requirements in the balance between depth and source morphology .",
    "for instance , the weak lensing group would be interested in no coaddition at all given their stringent requirements on the shape measurement ( though they will benefit from multiple imaging of the same objects , improving the solution of the extracted shear , see section  [ sec : wl ] ) .",
    "the final step in the processing consists in transforming the coadded images in different filters into single object entries in a catalog .",
    "desdm uses the sextractor software in this step too , running over a master image which uses coadd images in all filters to detect where the sources are , and then extracting the relevant information from single - filter coadded images .",
    "an object is identified as such when the convolution of the psf with the image is above a certain local background estimation ( see @xcite for details ) . in this step , it is important to approach the deblending of sources .",
    "this is currently being done by producing several isophotal layers for each object and at each layer where two light-islands join making the decision on whether to merge them into the same object using as a criterion the relative integrated intensity between the branch and full object .",
    "the type of information extracted in this step is positional , photometric and morphological , besides identification and other bookkeeping variables .    concerning the astrometry ,",
    "the barycenter for each object is derived using several estimators .",
    "the most reliable one makes use of an iterative calculation through a gaussian window .",
    "this is transformed to celestial coordinates using the astrometric solution found during nightly processing .",
    "photometry is measured using several methods :    * simple flux counting inside a fixed circular aperture for several apertures ( in the arcseconds range ) ; * using an elliptical aperture adjusted to the morphological properties of the object .",
    "from the second order moments of the object , we would find the elongation and orientation of the ellipse representing it .",
    "the ellipse scaling factor is derived from the first order moment of the radial distribution @xcite ; * using a fit to the measured psf shape ( suitable for stars ) ; * using a one- or two- component model convolved with a local model of the psf ( exponential and/or spheroidal , suitable for galaxies ) .    among the morphological measurements , currently there are two variables appraising the deviation of the object from a point - like shape , therefore providing a star - galaxy separation handle .",
    "one of them relies on a previously trained neural network and is the well - known stellarity parameter class_star from sextractor .",
    "the other arises from the calculation of a discriminant function measuring the deviation of the image from the psf shape .",
    "most of the variables in the catalogs have their corresponding errors and quality flags to guide the selection of sources for analysis .",
    "additional columns are included in the final catalog , and these are produced in pipelines running over images and catalogs , to produce the final coadded catalogs .",
    "two of these are briefly described in the following sections .",
    "an additional catalog is built to identify transient objects , as described in section  [ sec : diff_imaging ] .",
    "redshifts in the des will be estimated from the photometric information of the objects in different filters .",
    "traditionally , the approach to this has been to either find the best fit of the spectral energy distribution to a collection of templates for different types of galaxies at different redshifts , or use a neural network trained with spectroscopic information ( see  @xcite and  @xcite for examples of both ) .",
    "currently the default estimation in desdm uses the latter method , using ten input magnitudes ( fixed circular and automatic elliptical apertures , described above , for five filters each ) . in fig .",
    "[ fig : photoz ] a comparison of photometric redshifts versus spectroscopic ( true ) ones is shown , as obtained in one of the latest data challenges ( section [ sec : dcs ] ) .",
    "there is a large code comparison project within a specific photo - z science working group .    ]",
    "the weak lensing probe will require measuring the distortion in the shapes of galaxies to extract the shear produced by gravity from the intervening matter between the source and observer .",
    "it will require a very precise measurement of the local psf shape independently of the standard determination used in the pipeline .",
    "this is done using bright isolated stars and additional instrument data .",
    "the psf is interpolated with a polynomial throughout the image , and it is deconvolved from each galaxy .",
    "the shear will be extracted using all the images where the object shows up ( in multiple filters ) to provide a more robust determination .",
    "the des will contain two modes of operation : the survey mode in which a wide area of the sky will be scanned several times for each filter over the course of the five - year survey ; and a time - domain survey where particular regions will be observed repeatedly on short time - scales ( @xmath21 weekly ) in search of transient phenomena ( from which supernovae of type ia have to be identified ) .    from every reduced image coming from the nightly processing of these regions ,",
    "a template is extracted from the coadd corresponding to that position .",
    "this template is subtracted from the reduced image , and all objects remaining above a certain threshold are cataloged .",
    "the desdm also plans to automate the generation of a polygon - based survey mask which would encode information on which single epoch images contributed to the coadded image at a given point and track the coadded magnitude depth , coadded color terms , regions blocked by saturated stars and any other information which is relevant to be included as a survey map rather than in a source - by - source basis .",
    "the mask is generated offline using the mangle  @xcite software , which is currently being incorporated to the pipeline .",
    "during the execution of the pipelines , in - built quality assurance modules make sure that the images and catalogs are good enough for scientific analysis and diagnose possible problems .",
    "some sample results are shown in fig.[fig : photometry ] and fig.[fig : astrometry ] .",
    "a broader approach to the testing and validation of the desdm is the data challenge ( dc ) process , in which a small sample of the survey is simulated in detail and fed to the pipelines to generate realistic images and catalogs .",
    "the scale for the sample ranges from a single 0.6 square degree tile to 200 square degrees corresponding to about 10 nights of observations .",
    "it starts with the creation of galaxy catalogs stemming from an n - body simulation@xcite and detailed models of the milky way galaxy for the star component @xcite .",
    "these are merged and fed to an image simulator which includes atmospheric and instrumental effects .",
    "the resulting images serve as inputs for the desdm , as if they had been really observed at the telescope .",
    "this whole process is a joint effort of the stanford , brazil and barcelona teams , for the catalog side , and fermilab , for the image simulation aspect .",
    "the resulting catalogs are then examined by members of the desdm and science working groups .",
    "several of these dcs have taken place during the project s lifetime .",
    "currently the testing approach consists on large data challenges every 6 months to verify the compliance with science requirements , while maintaining a 2-week cycle with the generation of small samples for quick feedback to the developers .",
    "in addition , the system is being used on real images of the blanco cosmology survey  @xcite which are proving to be very valuable for the development of the algorithms .",
    "the desdm system has been fully developed and all elements are in place to process the large dataset that will be generated by the des camera .",
    "current testing results point to requirements being met in terms of astrometry , photometry and depth .",
    "others such as completeness and star - galaxy separation are compliant up to shallower magnitudes though improvements are in the works .    in parallel , a community pipeline is being developed using essentially the same structure and algorithms but with the goal of addressing non - des user needs , given that two - thirds of the year the instrument will be available to the general astronomical community with access to noao facilities .",
    "funding for the des projects has been provided by the u.s .",
    "department of energy , the u.s",
    ". national science foundation , the ministerio of ciencia y educacin of spain , the science and technology facilities council of the united kingdom , the higher education funding council for england , the national center for supercomputing applications at the university of illinois at urbana - champaign , the kavli institute for cosmological physics at the university of chicago , financiadora de estudos e projetos , fundao carlos chagas filho de amparo  pesquisa do estado do rio de janeiro , conselho nacional de desenvolvimento cientfico e tecnolgico and the ministrio da cincia e tecnologia , the deutsche forschungsgemeinschaft and the collaborating institutions in the dark energy survey .",
    "the collaborating institutions are argonne national laboratories , the university of california at santa cruz , the university of cambridge , centro de investigaciones energticas , medioambientales y tecnolgicas - madrid , the university of chicago , university college london , des - brazil , fermilab , the university of edinburgh , the university of illinois at urbana - champaign , the institut de cincies de lespai ( ieec / csic ) , the institut de fsica daltes energies , the lawrence berkeley national laboratory , ludwig - maximilians universitt and the associated excellence cluster universe , the university of michigan , the national optical astronomy observatory , the university of nottingham , the ohio state university , the university of pennsylvania , the university of portsmouth , universidade federal do rio grande do sul , slac , stanford university , the university of sussex and texas a&m university .    99 f.abdalla these proceedings .",
    "j.hao these proceedings .",
    "j.mohr et al .",
    "observatory operations : strategies , proceses , and systems ii .",
    "proceedings of the spie 7016(2008)70160l k.kotwani et al .",
    "8th international workshop on middleware for grids , clouds and e - science mgc ( 2010 ) http://mgc2010.lncc.br/ n.wilkins-diehr et al .",
    "41(11)(2008)32 r.pordes et al . journal of physics : conference series 78(2007)012057 i.foster ieee internet computing may - june(2011)70 s.arnouts & e.bertin a&as 117(1996)393 e.bertin astronomical data analysis software and systems xv .",
    "astronomical society of the pacific conference series 351(2006)112 e.bertin et al .",
    "astronomical data analysis software and systems xi .",
    "astronomical society of the pacific conference series 281(2002)228 e.bertin astronomical data analysis software and systems xx .",
    "astronomical society of the pacific conference series 442(2011)435 a.f.j.moffat a&a 3(1969)455 r.g.kron apjs 43(1980)305 n.bentez apj 536(2000)571 h.oyaizu et al .",
    "apj 674(2008)768 m.swanson et al .",
    "mnras 387(2008)1391 http://space.mit.edu/@xmath21molly/mangle/ m.busha et al .",
    "( 2011 ) in preparation b.rosetto et al .",
    "aj 141(2011)185 j.mohr http://www.usm.uni-muenchen.de/@xmath21jmohr/bcs"
  ],
  "abstract_text": [
    "<S> the dark energy survey ( des ) is a project with the goal of building , installing and exploiting a new 74 ccd - camera at the blanco telescope , in order to study the nature of cosmic acceleration . </S>",
    "<S> it will cover 5000 square degrees of the southern hemisphere sky and will record the positions and shapes of 300 million galaxies up to redshift 1.4 . </S>",
    "<S> the survey will be completed using 525 nights during a 5-year period starting in 2012 . </S>",
    "<S> about o(1 tb ) of raw data will be produced every night , including science and calibration images . </S>",
    "<S> the des data management system has been designed for the processing , calibration and archiving of these data . </S>",
    "<S> it is being developed by collaborating des institutions , led by ncsa . in this contribution </S>",
    "<S> , we describe the basic functions of the system , what kind of scientific codes are involved and how the data challenge process works , to improve simultaneously the data management system algorithms and the science working group analysis codes . </S>"
  ]
}