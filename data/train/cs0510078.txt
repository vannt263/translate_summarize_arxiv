{
  "article_text": [
    "in the multiple description problem , an information source is encoded into @xmath0 packets and these packets are sent through parallel communication channels .",
    "there are several receivers , each of which can receive a subset of the packets and needs to reconstruct the information source based on the received packets . in the most general case ,",
    "there are @xmath1 receivers and the packets received in each receiver correspond to one of @xmath1 subsets of @xmath2 .",
    "a long standing open problem in the literature @xcite is to characterize the information - theoretic rate region subject to the specified distortion constraints .",
    "practical multiple description codes have been discussed in @xcite and recent work @xcite has considered the multiple description problem in the context of the distributed source coding scenario .",
    "optimal descriptions of even the gaussian source with quadratic distortion measures have not been fully characterized . in the special case of two descriptions of a scalar gaussian source with quadratic distortion measures ,",
    "however , the entire rate region has been characterized in @xcite .",
    "our focus is on @xmath0 descriptions of a memoryless _ vector _ gaussian source forwhere @xmath0 individual and a single common receiver ( cf .",
    "figure  [ fig : md ] ) .",
    "each receiver needs to reconstruct the original source such that the empirical covariance matrix of the difference is less than , in the sense of a positive semidefinite ordering , a `` distortion '' matrix . in this setting",
    ", the symmetric rate multiple description problem of a scalar gaussian source with symmetric distortion constraints has been characterized in @xcite , but a complete understanding of all other rate - distortion settings is open .",
    "our main result is an _",
    "exact _ characterization of the sum rate for any specified @xmath3 distortion matrix constraints . with @xmath4 , we characterize the entire rate region . our contribution is two fold :    * first , we derive a novel information - theoretic inequality that provides a lower bound to the sum of the description rates .",
    "the key step is to avoid using the entropy power inequality , which was a central part of the proof of two descriptions of the scalar gaussian source in @xcite : the vector entropy power inequality is tight only with a certain covariance alignment condition , which arbitrary distortion matrix requirements do not necessarily allow . *",
    "second , we show that jointly gaussian descriptions actually achieve the lower bound not by resorting to a direct calculation and comparison , which appears to be difficult for @xmath5 , but instead by arguing the equivalence of certain optimization problems .",
    "consider another two description problem of a pair of jointly gaussian memoryless sources as depicted in figure  [ fig : md_special ] .",
    "there are two encoders that describe this source to three receivers : receiver @xmath6 gets the description of encoder @xmath6 , with @xmath7 and the third receiver receives both the descriptions .",
    "suppose receiver @xmath6 is interested in reconstructing the @xmath6th marginal of the jointly gaussian source , with @xmath7 .",
    "the third receiver is interested in reconstructing the entire vector source .",
    "this description problem is closely related to the vector gaussian description problem that is the main focus of this paper .",
    "we exploit this connection and characterize the rate region where the reconstructions have a constraint on the covariance of error at each of the receivers ( in the sense of a positive semidefinite order ) .",
    "we have organized the results in this paper as follows . in section  [ sec2 ]",
    "we give a formal description of the problem and summarize our main result .",
    "the derivation of a lower bound is in section  [ sec3 ] . in section  [ sec4 ]",
    "we provide an upper bound and provide conditions for the achievable sum rate to meet the lower bound .",
    "we see in section  [ sec5 ] that the conditions are indeed satisfied in the special case of a scalar gaussian source .",
    "the solution in the case of the more complicated vector gaussian source is in section  [ sec6 ] .",
    "the solution to the multiple description problem depicted in figure  [ fig : md_special ] is the topic of section  [ sec71 ] . finally ,",
    "while the characterization of the rate region of general multiple descriptions of the gaussian source ( with each receiver having access to some subset of the descriptions ) is still open , we can use the insights derived via our sum rate characterization to solve this problem for a nontrivial set of covariance distortion constraints ; this is done in section  [ sec72 ] .    a note about the notation in this paper : we use lower case letters for scalars , lower case and bold face for vectors , upper case and bold face for matrices .",
    "the superscript @xmath8 denotes matrix transpose .",
    "we use @xmath9 and @xmath10 to denote the identity matrix and the all zero matrix respectively , and @xmath11 to denote a diagonal matrix with the diagonal entries equal to @xmath12 .",
    "the partial order @xmath13 ( @xmath14 ) denotes positive definite ( semidefinite ) ordering : @xmath15 ( @xmath16 ) means that @xmath17 is a positive definite ( semidefinite ) matrix .",
    "we write @xmath18 to denote a gaussian random vector with mean @xmath19 and covariance @xmath20 .",
    "all logarithms in this paper are to the natural base .",
    "the information source @xmath21\\}$ ] is an i.i.d .  random process with the marginal distribution @xmath22 , i.e. , a collection of i.i.d .",
    "gaussian random vectors . denoting the dimension of @xmath21\\}$ ] by @xmath23",
    ", we suppose that @xmath24 is an @xmath25 positive definite matrix .",
    "there are @xmath0 encoding functions at the source , encoder @xmath26 encodes a source sequence , of length @xmath27 , @xmath28,\\;\\dots , \\ ; { { { \\mathbf{x}}}}[n])^t$ ] to a source code @xmath29 , for @xmath30 .",
    "this code @xmath31 is sent through @xmath26th communication channel at the rate @xmath32 .",
    "there are @xmath0 individual receivers and one central receiver .    for @xmath33",
    ", the @xmath26th individual receiver uses its information ( the output of the @xmath26th channel ) to generate an estimate @xmath34 @xmath35 of the source sequence @xmath36 .",
    "the central receiver uses the output of all the @xmath0 channels to generate an estimate @xmath37 of the source sequence @xmath36 .",
    "since we are interested in covariance constraints , the decoder maps can be restricted to be the minimal mean square error ( mmse ) estimate of the source sequence based on the received codewords .",
    "so , @xmath38 , \\quad l=1,\\;\\dots,\\;l \\\\ \\hat{{{{\\mathbf{x}}}}}_0^{n } & = { { \\mathbb{e}}}\\left[{{{\\mathbf{x}}}}^n|f_1^{(n)}({{{\\mathbf{x}}}}^n ) , \\ ; \\dots , \\ ; f_l^{(n)}({{{\\mathbf{x}}}}^n)\\right ] .",
    "\\end{split}\\ ] ] suppose the reconstructed sequences satisfy the covariance constraints @xmath39-\\hat{{{{\\mathbf{x}}}}}_l[m])^t({{{\\mathbf{x}}}}[m]-\\hat{{{{\\mathbf{x}}}}}_l[m])\\big ] & \\preccurlyeq \\mathbf{d}_l , \\quad l=1,\\;\\dots,\\;l , \\\\",
    "\\frac{1}{n}\\sum\\limits_{m=1}^n { { \\mathbb{e}}}\\big[({{{\\mathbf{x}}}}[m]-\\hat{{{{\\mathbf{x}}}}}_0[m])^t({{{\\mathbf{x}}}}[m]-\\hat{{{{\\mathbf{x}}}}}_0[m])\\big ] & \\preccurlyeq \\mathbf{d}_0 , \\end{split}\\ ] ] then we say that multiple descriptions with distortion constraints @xmath40 are achievable at the rate tuple @xmath41 .",
    "the closure of the set of all achievable rate tuples is called the rate region and is denoted by @xmath42 . throughout this paper , we suppose that @xmath43 . , is without loss of generality is seen by applying the data processing inequality for mmse estimation errors ; having more access to information can only reduce the covariance of the error in a positive semidefinite sense .",
    "similarly , @xmath44 is also not interesting ; here we simplify this condition and take @xmath45 . ]",
    "our main result is the precise characterization of the sum rate of multiple descriptions for individual and central receivers .    for distortion constraints @xmath40 ,",
    "the sum rate is @xmath46    this sum rate is achieved by a _ jointly gaussian random multiple description scheme _ :",
    "let @xmath47 be zero mean jointly gaussian random vectors independent of @xmath48 , with the positive definite covariance matricex @xmath49 denoted by @xmath50 .",
    "defining @xmath51 we consider @xmath52 such that @xmath53 \\overset{\\text{def}}{= } & { { \\mathbb{e}}}\\big[({{{\\mathbf{x}}}}-{{\\mathbb{e}}}[{{{\\mathbf{x}}}}|{{{\\mathbf{u}}}}_l])^t({{{\\mathbf{x}}}}-{{\\mathbb{e}}}[{{{\\mathbf{x}}}}|{{{\\mathbf{u}}}}_l])\\big ]   \\preccurlyeq \\mathbf{d}_l , \\quad l=1,\\;\\dots,\\;l , \\\\ { { \\text{cov}}}[{{{\\mathbf{x}}}}|{{{\\mathbf{u}}}}_1,\\;\\dots,\\;{{{\\mathbf{u}}}}_l ] \\overset{\\text{def}}{= } & { { \\mathbb{e}}}\\big[({{{\\mathbf{x}}}}-{{\\mathbb{e}}}[{{{\\mathbf{x}}}}|{{{\\mathbf{u}}}}_1,\\;\\dots,\\;{{{\\mathbf{u}}}}_l])^t({{{\\mathbf{x}}}}-{{\\mathbb{e}}}[{{{\\mathbf{x}}}}|{{{\\mathbf{u}}}}_1,\\;\\dots,\\;{{{\\mathbf{u}}}}_l])\\big ]    \\preccurlyeq \\mathbf{d}_0 . \\end{split}\\ ] ] to construct the code book for the @xmath26th description , draw @xmath54 @xmath55 vectors randomly according to the marginal of @xmath56 .",
    "the encoders observe the source sequence @xmath36 , look for codewords @xmath57 that are jointly typical with @xmath36 and send the index of the resulting @xmath55 through the @xmath26th channel , respectively .",
    "the @xmath26th individual receiver uses this index and generates a reproduction sequence @xmath58 $ ] for @xmath30 , the central receiver uses all the @xmath0 indices to generate a reproduction sequence @xmath59 $ ] . for every @xmath50 satisfying ,",
    "the rate tuple @xmath60 satisfying @xmath61 is achievable by using this coding scheme , where @xmath62 is the covariance matrix for all @xmath63 , and @xmath64 $ ] . in particular ,",
    "the achievable sum rate is @xmath65    we denote this ensemble of descriptions , throughout this paper , as the jointly gaussian description scheme and the time sharing between them as the jointly gaussian description strategy .",
    "we show that jointly gaussian description schemes are optimal in achieving the sum rate .      for two descriptions",
    ", we can characterize the entire rate region .",
    "given distortion constraints @xmath66 , the rate region for the two description problem for an i.i.d.@xmath67 vector gaussian source is @xmath68    we show that if the distortion constraints @xmath66 satisfy @xmath69 and @xmath70 , we can get the optimizing @xmath71 by solving a matrix riccati equation .",
    "an illustration of the rate region is shown in figure  [ fig : region ] . in this case , if we let @xmath72^{-1}$ ] for @xmath73 , then the optimizing @xmath71 is @xmath74 where @xmath75^{\\frac{1}{2}}({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{\\frac{1}{2 } } -{{\\mathbf{k}}}_{w_0}.\\ ] ] letting @xmath76 denote the optimal sum rate , the two corner points in figure  [ fig : region ] are @xmath77 @xmath78",
    "[ sec3 ] by fairly procedural steps , we have the following lower bound to the sum rate of the multiple descriptions : @xmath79 where we have defined @xmath80 and called it the symmetric mutual information between @xmath81 .",
    "note that @xmath82 and is also well defined even when @xmath81 are continuous random variables .",
    "our main result is the following information theoretic inequality which gives a lower bound to the sum of symmetric mutual information between @xmath83 and mutual information between @xmath84 and @xmath36 for given covariance constraints .",
    "[ lemma : inform ] let @xmath28,\\;\\dots,\\;{{{\\mathbf{x}}}}[n])$ ] , where @xmath85 $ ] s are i.i.d .",
    "@xmath86 gaussian random vectors for @xmath87 .",
    "let @xmath81 be random variables jointly distributed with @xmath36 .",
    "let @xmath88 $ ] and @xmath89 $ ] for @xmath90 .",
    "given positive definite matrices @xmath91 , if @xmath92 - \\hat{{{{\\mathbf{x}}}}}_l[m])^t({{{\\mathbf{x}}}}[m ] - \\hat{{{{\\mathbf{x}}}}}_l[m ] ) ] & \\preccurlyeq \\mathbf{d}_l , \\quad   l=1,\\;\\dots,\\;l ,",
    "\\\\ \\frac{1}{n}\\sum\\limits_{m=1}^n { { \\mathbb{e}}}[({{{\\mathbf{x}}}}[m ] - \\hat{{{{\\mathbf{x}}}}}_0[m])^t({{{\\mathbf{x}}}}[m ] - \\hat{{{{\\mathbf{x}}}}}_0[m ] ) ] & \\preccurlyeq \\mathbf{d}_0 , \\end{split}\\ ] ] then @xmath93 furthermore , there exists a jointly gaussian distribution of @xmath94 such that the inequality in is tight .",
    "this is a fundamental information - theoretic inequality which involves only the joint distribution are not simply functions of @xmath36 and can also be continuous random variables . ] between @xmath84 and @xmath36 and bounds on mean square error estimation of @xmath36 from @xmath84 ; we delegate the proof of this result to appendix  [ app : inform ] .",
    "we can now use lemma [ lemma : inform ] to derive a lower bound to the sum rate @xmath95    by letting @xmath96 in the lemma above , we can derive a simple lower bound to the rate of the individual descriptions as well : @xmath97 this bound is actually the point - to - point rate - distortion function for individual receivers , since each individual receiver only faces a point - to - point compression problem .",
    "note that for any positive definite @xmath71 , @xmath98 is a lower bound to the sum rate of the multiple descriptions .",
    "two special choices of @xmath71 are of particular interest :    * letting @xmath99 and 0 @xmath100 , we have the following lower bound : @xmath101 this bound is actually the summation of the bounds on the individual rates . *",
    "letting some eigenvalues of @xmath102 goes to infinity , we have the following lower bound : @xmath103 this bound is the point - to - point rate - distortion function when we only have the central distortion constraint .",
    "we will see later that for some distortion constraints @xmath40 , and can be tight .",
    "[ sec4 ]    in the previous section we gave a lower bound to the sum rate .",
    "now we give a upper bound to the sum rate by using the jointly gaussian description scheme described in section [ sec : sumrate ] .",
    "first we give a sketch of the achievable rate region by using jointly gaussian description scheme . given the source sequence @xmath36 , as long as we can find a combination of codewords @xmath57 that are jointly typical with @xmath36 , all the receivers can generate reproduction sequences that satisfy their given distortion constraints .",
    "an intuitive way to understand is the following : since @xmath57 are jointly typical with @xmath36 , then for any @xmath104 , we have that @xmath105 are jointly typical with @xmath36 . now the probability that a randomly generated combination of codewords @xmath105 are jointly typical with @xmath36 is roughly @xmath106 and the number of possible combination of codewords @xmath107 are @xmath108 .",
    "thus , as long as @xmath109 we can find a combination of codewords @xmath105 that are jointly typical with @xmath36 . rigorously speaking , we need to show that as long as is satisfied , then for any given source sequence @xmath36 we can find a combination of codewords @xmath57 such that @xmath105 are jointly typical with @xmath36 for all @xmath110 .",
    "second moment method_@xcite is commonly used to address this aspect , and a proof can be found in @xcite .",
    "evaluating based on the jointly gaussian distribution of @xmath48 and @xmath111 , we get that all the rate tuples @xmath60 satisfying @xmath112 are achievable by the jointly gaussian description scheme . in particular , we have that the achievable sum rate is @xmath113 the resulting distortions @xmath114 by using jointly gaussian description scheme can be calculated as @xmath115 = [ { { \\mathbf{k}}}_x^{-1 } + \\mathbf{k}_{w_l}^{-1}]^{-1 } , \\quad l=1,\\;\\dots,\\ ; l ,   \\\\ { { \\mathbf{d}}}^*_0 = & { { \\text{cov}}}[{{{\\mathbf{x}}}}|{{{\\mathbf{u}}}}_1,\\;\\dots,\\;{{{\\mathbf{u}}}}_l ] = [ { { \\mathbf{k}}}_x^{-1 } + ( { { \\mathbf{i}}},\\ ; \\dots , \\ ; { { \\mathbf{i}}})\\mathbf{k}_w^{-1}({{\\mathbf{i}}},\\ ; \\dots , \\ ; { { \\mathbf{i}}})^t]^{-1}. \\end{split}\\ ] ]      the achievable region given in has useful combinatorial properties ; in particular it belongs to the class of _ contra - polymatroids_@xcite . certain rate regions of the multiple access channel @xcite and distributed source coding problems @xcite are also known to have this specific combinatorial property . to see this , let @xmath116 we can readily verify that @xmath117 by definition",
    ", we conclude that the achievable rate region of a jointly gaussian multiple description scheme is a contra - polymatroid .",
    "the key advantage of this combinatorial propety is that we can exactly characterize the vertices of the achievable rate region .",
    "letting @xmath118 to be a permutation on @xmath2 , define @xmath119 and @xmath120 .",
    "then the @xmath121 points @xmath122 are the vertices of the contra - polymatroid .",
    "our goal is to show that the jointly gaussian description scheme achieves the lower bound to the sum rate . in general",
    "it does not seem facile to do a direct calculation and comparison .",
    "we forgo this strategy and , instead , provide an alternative characterization of the achievable sum rate which is much easier to compare with the lower bound .",
    "similar to the derivation of the lower bound ( in appendix  [ app : inform ] ) , we consider an @xmath123 gaussian random vector @xmath124 , independent of @xmath48 and all @xmath125 s .",
    "defining @xmath126 , we have the following achievable sum rate : @xmath127+\\mathbf{k}_z\\big| } { \\big|{{\\text{cov}}}[{{{\\mathbf{x}}}}|{{{\\mathbf{u}}}}_1,\\;\\dots,\\;{{{\\mathbf{u}}}}_l]\\big|\\prod\\limits_{l=1}^l\\big|{{\\text{cov}}}[{{{\\mathbf{x}}}}|{{{\\mathbf{u}}}}_l]+\\mathbf{k}_z\\big|},\\end{aligned}\\ ] ] where the last step is from a procedural gaussian mmse calculation .",
    "note that if we have @xmath128 then ( a ) in is actually an equality .",
    "thus , if our choice of @xmath52 and @xmath102 satisfy the following two conditions :    * is true .",
    "* distortion constraints are met with equality , i.e. , @xmath129   = \\mathbf{d}_l , \\quad l=1,\\;\\dots,\\ ; l , \\\\ & { { \\text{cov}}}[{{{\\mathbf{x}}}}|{{{\\mathbf{u}}}}_1,\\;\\dots,\\;{{{\\mathbf{u}}}}_l ]   = \\mathbf{d}_0 , \\end{split}\\ ] ]    then the upper bound matches the lower bound and we have characterized the sum rate . in the following we examine under",
    "what circumstances the above two conditions are true .",
    "first , we give a necessary and sufficient condition for to be true , delegating the proof to appendix  [ app : nec_suff_condition ] .    [",
    "prop : nec_suff_condition ] there exists some choice of positive definite @xmath71 such that is true if and only if @xmath50 , the covariance matrix of @xmath130 , takes the following form @xmath131 where @xmath132 .",
    "next , we look at the conditions for to be true . from , we have @xmath133^{-1 }   = { { \\mathbf{k}}}_x^{-1 } + \\mathbf{k}_{w_l}^{-1 } , \\quad l=1,\\;\\dots,\\ ; l \\\\",
    "\\mathbf{d}_0^{-1 } & = { { \\text{cov}}}[{{{\\mathbf{x}}}}|{{{\\mathbf{u}}}}_1,\\;\\dots,\\;{{{\\mathbf{u}}}}_l]^{-1 }   = { { \\mathbf{k}}}_x^{-1 } + ( { { \\mathbf{i}}},\\ ; \\dots , \\ ; { { \\mathbf{i}}})\\mathbf{k}_w^{-1}({{\\mathbf{i}}},\\ ; \\dots , \\ ; { { \\mathbf{i}}})^t . \\end{split}\\ ] ] @xmath134 , is calculated in the following lemma ; the proof is available in appendix  [ app : inverse ] .",
    "[ lemma : inverse ] let @xmath135 if @xmath136 and @xmath137 , then @xmath138^{-1}.\\ ] ]    using this lemma , from we arrive at @xmath139^{-1 } = \\sum\\limits_{l=1}^l\\left[({{\\mathbf{d}}}_l^{-1}-{{\\mathbf{k}}}_x^{-1})^{-1}+{{\\mathbf{a}}}\\right]^{-1}.\\ ] ] defining @xmath140 is equivalent to @xmath141^{-1 }   = \\sum\\limits_{l=1}^l\\left[{{\\mathbf{k}}}_{w_l}+{{\\mathbf{a}}}\\right]^{-1}.\\ ] ] thus , if there exists a positive definite solution @xmath142 to , and the corresponding @xmath50 is positive definite , then the distortion constraints are met with equality , i.e. , holds .",
    "it turns out that as long as @xmath142 is a solution to , the resulting @xmath50 is always positive definite ; we state this formally below , delegating the proof to appendix [ app : positivedefinite ] .",
    "[ lemma : positivedefinite ] if for some @xmath143 and @xmath144 is true , then the covariance matrix @xmath50 defined in is positive definite .",
    "we summarize the state of affairs in the following theorem .",
    "[ th : main ] given distortion constraints @xmath145 , let @xmath146 if there exists an solution @xmath147 to and @xmath148 , then the jointly gaussian description scheme with @xmath50 defined in with @xmath149 achieves the optimal sum rate , and the optimal @xmath71 for lower bound is @xmath150 .    thus we show that if the given distortion constraints @xmath151 satisfy the condition for theorem [ th : main ] , then the jointly gaussian description scheme achieves the optimal sum rate and we can calculate the optimal @xmath50 by solving a matrix equation .",
    "however , for arbitrarily given distortion constraints , may not have a solution @xmath147 such that @xmath148 . in this case",
    ", we can show that there exists a jointly gaussian description scheme that achieves the sum rate lower bound , and resulting in distortions @xmath152 such that @xmath153 for @xmath154 . in the following we first study the relatively simpler case of scalar gaussian source , and",
    "then move to discuss the vector gaussian source .",
    "[ sec5 ] here we suppose that the information source is an i.i.d .  sequence of @xmath155 scalar gaussian random variables .",
    "let individual distortion constraints be @xmath156 and the central distortion constraints be @xmath157 , where @xmath158 for @xmath159 .",
    "we consider the jointly gaussian description scheme with the following covariance matrix for @xmath160 .",
    "@xmath161    consider the condition for theorem [ th : main ] to hold : to meet the individual distortion constraint with equality , we need @xmath162 let @xmath163 we need @xmath164^{-1 }   = \\sum\\limits_{l=1}^l\\left[\\sigma_l^2+a\\right]^{-1}\\ ] ] to have a solution @xmath165 , to meet the central distortion constraint with equality . towards this , define @xmath166 and we have @xmath167 using induction , we can show that @xmath168 thus we have @xmath169 then given distortions @xmath170",
    ", @xmath171 and @xmath172 falls into the following three cases .",
    "* case 1 : * @xmath173 and @xmath174 .    in this case , since @xmath175 is a continuous function , there exists an @xmath176 such that @xmath177 . in this case",
    "the condition for theorem [ th : main ] holds and from theorem [ th : main ] we know that jointly gaussian description scheme with covariance matrix for @xmath160 being with @xmath178 achieves the optimal sum rate .    * case 2 : * @xmath179 .",
    "alternatively , @xmath180 .    in this case",
    ", the condition for theorem [ th : main ] does not hold .",
    "but the jointly gaussian description scheme can still achieve the sum rate . to see this , choosing @xmath181 in @xmath52 we can meet individual distortions with equality and get a central distortion @xmath182 .",
    "from we have @xmath183 hence we have achieved distortion @xmath184 where @xmath185 , and from the achievable sum rate is @xmath186 which equals the sum of our bounds on individual rates .",
    "* case 3 : * @xmath187 , alternatively , @xmath188 .    in this case",
    ", the conditions for theorem [ th : main ] do not hold as well .",
    "but the jointly gaussian description strategy still achieves the sum rate . to see this , note that we can find a @xmath189 such that @xmath190 and @xmath191 and we choose @xmath192 , @xmath193 for @xmath194 , and @xmath195 in @xmath196 . defining @xmath197 , is equivalent to the following equation : @xmath198^{-1 }   = \\sum\\limits_{l=1}^l\\left[\\sigma_l^2+\\sigma_x^2\\right]^{-1}.\\ ] ] from lemma [ lemma : positivedefinite ] , our choice of @xmath196 is positive definite .",
    "thus the resulting distortions are @xmath199 , where @xmath190 .    using the determinant equation @xmath200 and",
    ", we have an achievable sum rate @xmath201 we conclude that in this case the point - to - point rate - distortion bound for the central receiver is achievable .    in summary , we have shown that the jointly gaussian description scheme achieves the lower bound on the sum rate .",
    "further , the sum rate can be calculated either trivially ( by choosing @xmath202 in case ii or @xmath203 in case iii ) or by solving a polynomial equation in a single variable ( case i ) .",
    "the essence of our proof of the optimality of jointly gaussian description scheme for scalar gaussian sources is the use of the _ intermediate value theorem _ for scalar continuous functions .",
    "however , there is no natural extension of this theorem for vector valued functions . to avoid this problem",
    ", we first explicitly solve the two description problem and characterize the optimality of jointly gaussian description scheme .",
    "next , we show that the jointly gaussian description scheme is optimal for @xmath204 by showing an equivalence of certain optimization problems . in the last part of this section ,",
    "we show that the jointly gaussian description strategy can achieve the optimal rate region for the two description problem .      with only two descriptions , we can explicitly solve , thus generalizing the corresponding solution for the scalar gaussian source , derived in @xcite .",
    "suppose the distortion constraints are denoted by @xmath205 and let @xmath206 we now solve , which is equivalent to , for @xmath207 , @xmath208 and @xmath147 .",
    "from we get @xmath209 and @xmath210 expanding out @xmath211 using lemma [ lemma : blockinverse ] in appendix [ app : matrix ] , we get @xmath212 taking inverse on both sides , we have @xmath213 defining @xmath214 as @xmath215^{-1 } , \\ ] ] is equivalent to @xmath216 defining @xmath217 is equivalent to @xmath218 which is further equivalent to @xmath219 this is a version of the so - called _ algebraic riccati equation _ ; the corresponding hamiltonian is readily seen to be positive semidefinite and we can even write down the following explicit solution : @xmath220^{\\frac{1}{2}}({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{\\frac{1}{2}}. \\end{split}\\ ] ] thus @xmath75^{\\frac{1}{2}}({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{\\frac{1}{2 } } -{{\\mathbf{k}}}_{w_0}.\\ ] ] now , if @xmath148 then we can appeal to theorem  [ th : main ] and arrive at the explicit jointly gaussian description scheme parameterized by @xmath50 that achieves the sum rate .",
    "analogous to the scalar case ( cf .",
    "@xcite ) , we have the following sufficient condition for when this is true ; the proof is available in appendix  [ app : twod ] .",
    "[ prop : twod ] if the distortion constraints @xmath66 satisfy @xmath221 then @xmath148 .",
    "we now complete the proof by considering the cases that are not covered by the conditions in proposition  [ prop : twod ] .",
    "* when @xmath222 we can choose @xmath223 to achieve the sum of point - to - point individual rate - distortion functions .",
    "thus in this case , the sum rate is equal to this natural lower bound .",
    "* when @xmath224 we can choose @xmath225 to achieve the point - to - point rate distortion - function for central receiver , also a natural lower bound .",
    "* when neither @xmath226 nor @xmath227 is positive or negative semidefinite ( this case can not happen in the scalar case ) , we can not use theorem  [ th : main ] , and the trivial choice of @xmath228 or @xmath225 does not meet the lower bound . in the next subsection",
    "we will address this case and prove that the jointly gaussian description scheme indeed achieves the lower bound on the sum rate for @xmath229 .",
    "if we let the source to be scalar gaussian , our result reduces to ozarow s solution of the two description problem for a scalar gaussian source@xcite : this is because the last case described above does not happen in the scalar case .      while we exactly characterized the optimal jointly gaussian description scheme and used this characterization in arguing that it achieves the fundamental lower bound to the sum rate",
    ", such exact calculations do not appear to be as immediate when @xmath230 .",
    "so , we eschew this somewhat brute - force approach and resort to a more subtle proof that involves exploring the structure of the solution to an optimization problem .",
    "first , note that by a linear transformation at the encoders and the decoders , we have the following result on rate region for multiple description with individual and central receivers .",
    "@xmath231    thus , throughout this subsection we will suppose , for notation simplicity , that @xmath232 .",
    "given distortion constraints @xmath233 , let @xmath234 and define @xmath235^{-1 } - \\sum\\limits_{l=1}^l\\left[{{\\mathbf{k}}}_{w_l}+{{\\mathbf{a}}}\\right]^{-1 } , \\\\",
    "f({{\\mathbf{a } } } ) \\overset{\\text{def}}{= } & \\log |{{\\mathbf{k}}}_{w_0}+{{\\mathbf{a}}}| - \\sum\\limits_{l=1}^l \\log |{{\\mathbf{k}}}_{w_l}+{{\\mathbf{a}}}|.\\end{aligned}\\ ] ] note that @xmath236 consider the following optimization problem : @xmath237 now , since @xmath238 is a continuous map and @xmath239 is a compact set , there exists an optimal solution @xmath147 to where @xmath147 satisfies the karush - kuhn - tucker ( kkt ) conditions : there exist @xmath240 and @xmath241 such that @xmath242 now @xmath147 falls into the following four cases .",
    "* case 1 : * @xmath243 .",
    "alternatively , 0 and 1 are not eigenvalues of @xmath147 . in this case ,",
    "@xmath244 and @xmath245 ; thus the kkt conditions in reduce to @xmath246 equivalently , @xmath247^{-1 } = \\sum\\limits_{l=1}^l\\left[{{\\mathbf{k}}}_{w_l}+{{\\mathbf{a}}}^*\\right]^{-1}.\\ ] ] from theorem  [ th : main ] , the jointly gaussian description scheme with covariance matrix for @xmath248 being @xmath249 achieves the lower bound to the sum rate .",
    "thus in this case , we have characterized the optimality of the jointly gaussian description scheme parameterized by in terms of achieving the sum rate .",
    "* case 2 : * @xmath250 .",
    "alternatively , some eigenvalues of @xmath147 are 0 , but no eigenvalues of @xmath147 are 1 . thus @xmath251 and the kkt conditions in reduce to @xmath252 for some @xmath253 satisfying @xmath254 .",
    "the _ key idea _ now is to see that the distortion constraint on the central receiver is too loose and we can in fact achieve a _ lesser _ distortion ( in the sense of positive semidefinite ordering ) for the same sum rate .",
    "we first identify this lower distortion : defining @xmath255 consider the smaller distortion matrix on the central receiver @xmath256 this new distortion matrix on the central receiver satisfies two key properties , that we state as a lemma ( whose proof is available in appendix [ app : enhanced0 ] ) .",
    "[ lemma : enhanced0 ] @xmath257    comparing with , we have @xmath258^{-1 } = \\sum\\limits_{l=1}^{l}\\left[{{\\mathbf{k}}}_{w_l}+{{\\mathbf{a}}}^*\\right]^{-1}.\\ ] ] now , the corresponding @xmath259 is singular .",
    "if it had nt been , then by theorem  [ th : main ] we could have concluded that jointly gaussian description scheme achieves the lower bound to the sum rate .",
    "we now address this technical difficulty .",
    "our first observation is that there exists @xmath260 such that for all @xmath261 we have @xmath262 , and @xmath263 , @xmath264 , and we can rewrite as @xmath265^{-1 } = \\sum\\limits_{l=1}^l\\big[({{\\mathbf{k}}}_{w_l}-\\epsilon { { \\mathbf{i}}})+({{\\mathbf{a}}}^*+\\epsilon { { \\mathbf{i}}})\\big]^{-1}.\\ ] ] thus if the distortion constraints were @xmath266 with @xmath267^{-1 } , \\quad { { l=1,\\;\\dots,\\;l } } , \\\\ { { \\mathbf{d}}}_0(\\epsilon ) &   = \\left[({{\\mathbf{k}}}_{w_0}^*-\\epsilon { { \\mathbf{i}}})^{-1}+{{\\mathbf{i}}}\\right]^{-1 } , \\end{split}\\ ] ] then @xmath268 is a solution to .",
    "this situation corresponds to that discussed in case i ; we can conclude that sum rate for this modified distortion multiple description problem is @xmath269 where @xmath270^{-1}-{{\\mathbf{i}}}$ ] .",
    "we would like to let @xmath271 approach zero and consider the limiting multiple description problem .",
    "in particular , we show that @xmath272 as @xmath273 in appendix  [ app : depsilon ] .",
    "further , we show that @xmath274 as @xmath273 in appendix  [ app : kzepsilon ] .",
    "thus we can conclude that the sum rate approaches , using , @xmath275 as @xmath276 ; here @xmath259 .",
    "we observe that this sum rate is achievable using the jointly gaussian multiple scheme .",
    "further , this sum rate is identical to the lower bound to sum rate for the original distortions @xmath277 .",
    "thus we conclude the optimality of the jointly gaussian description scheme in this case as well .",
    "* case 3 : * @xmath278 .",
    "alternatively , some eigenvalues of @xmath147 are 1 , but no eigenvalues of @xmath147 are 0 . in this case , the @xmath244 and the kkt conditions in reduce to @xmath279 for some @xmath280 satisfying @xmath281 . defining @xmath282^{-1}-{{\\mathbf{i}}},\\ ] ] we have , as in , that @xmath283 the observation @xmath284^{-1}+{{\\mathbf{\\lambda}}}_2,\\ ] ] combined with the proof of suffices to justify .",
    "now , from , @xmath285 as in the previous case , the key step is to identify smaller distortion matrices at each of the individual receivers ( ordered in the positive semidefinite sense ) that is achievable at the same sum rate : @xmath286^{-1 } , \\quad l = 1 , \\ ; \\ldots , \\ ;   l.\\ ] ] to see that this is indeed a smaller distortion matrix , observe that since @xmath50 is positive definite , it follows that @xmath287 and @xmath288^{-1 } \\\\ & = \\left[\\left(\\left ( ( { { \\mathbf{k}}}_{w_l}+{{\\mathbf{i}}})^{-1}+{{\\mathbf{\\lambda}}}_2\\right)^{-1}-{{\\mathbf{i}}}\\right)^{-1}+{{\\mathbf{i}}}\\right]^{-1 } \\\\ & = \\left [ { { \\mathbf{i}}}- ( { { \\mathbf{k}}}_{w_l}+{{\\mathbf{i}}})^{-1}-{{\\mathbf{\\lambda}}}_2\\right ] \\\\ & = \\left [ { { \\mathbf{i}}}+ { { \\mathbf{k}}}_{w_l } \\right]^{-1 } -{{\\mathbf{\\lambda}}}_2 \\\\ & = { { \\mathbf{d}}}_l - { { \\mathbf{\\lambda}}}_2 , \\quad l = 1 , \\ ; \\ldots , \\ ;   l. \\end{split}\\ ] ] since @xmath289 , it follows that @xmath290 .",
    "define @xmath291^{-1 } , \\quad l=0,\\;1,\\;\\dots,\\;l-1 , \\\\",
    "{ { \\mathbf{d}}}_l(\\epsilon ) & = \\left[({{\\mathbf{k}}}_{w_l}^*+\\epsilon { { \\mathbf{i}}})^{-1}+{{\\mathbf{i}}}\\right]^{-1 } , \\end{split}\\ ] ] then there exists @xmath260 such that for all @xmath261 we have @xmath292 , and @xmath293 .",
    "we can rewrite as @xmath294^{-1 } = \\sum\\limits_{l=1}^{l-1}\\left[({{\\mathbf{k}}}_{w_l}+\\epsilon { { \\mathbf{i}}})+({{\\mathbf{a}}}^*-\\epsilon { { \\mathbf{i}}})\\right]^{-1}+\\left[({{\\mathbf{k}}}_{w_l}^*+\\epsilon { { \\mathbf{i}}})+({{\\mathbf{a}}}^*-\\epsilon { { \\mathbf{i}}})\\right]^{-1}.\\ ] ] thus if the distortion constraints were @xmath295 , then @xmath296 is a solution to .",
    "this situation corresponds to that discussed in case i ; we conclude that the sum rate for this modified distortion multiple description problem is @xmath269 where @xmath297^{-1}-{{\\mathbf{i}}}$ ] .",
    "we would like to let @xmath271 approach zero and consider the limiting multiple description problem .",
    "similar to equations   and  , we have @xmath298 further , we show that @xmath299 .",
    "we can now conclude that the sum rate approaches @xmath300 as @xmath271 approaches 0 .",
    "in other words , the point - to - point rate - distortion function for central receiver with distortion @xmath301 can be achieved by using the jointly gaussian description scheme , and the resulting distortion is @xmath302 where @xmath303 . in conclusion , the jointly gaussian description scheme is also optimal in this case .",
    "* case 4 : * @xmath304 .",
    "i.e. , both 0 and 1 are eigenvalues of @xmath147 . in this case , the kkt conditions are : there exist @xmath305 and @xmath306 such that equations , and hold .",
    "we can combine equations and to get @xmath307 where @xmath308^{-1}-{{\\mathbf{i}}}. \\end{split}\\ ] ]    as in cases 2 and 3 , we want to show the optimality of the jointly gaussian multiple description scheme through a limiting procedure .",
    "we do this by first perturbing @xmath147 so that it has no eigenvalue equal to 0 or 1 as follows .    without loss of generality ,",
    "suppose that @xmath147 has @xmath309 eigenvalues equal to 0 and @xmath310 eigenvalues equal 1 , where @xmath311 and @xmath312 , and there exists @xmath25 orthogonal matrix @xmath20 such that @xmath313 with @xmath314 .",
    "we need to perturb the eigenvalues of @xmath147 away from both 0 and 1 . towards this , we define two @xmath25 diagonal matrices : @xmath315 also define @xmath316 further , defining @xmath317 there exists @xmath260 such that for all @xmath318 and @xmath319 we have @xmath320 , and @xmath321 .",
    "now , we can rewrite as @xmath322^{-1 } = \\sum\\limits_{l=1}^{l}\\big[{{\\mathbf{k}}}_{w_l}(\\epsilon_1,\\epsilon_2)+{{\\mathbf{a}}}^*(\\epsilon_1,\\epsilon_2)\\big]^{-1}.\\ ] ] thus if the distortion constraints were @xmath323 , then @xmath324 is a solution to .",
    "this situation corresponds to that discussed in case i ; we conclude that the sum rate for this modified distortion multiple description problem is @xmath325^{-1}-{{\\mathbf{i}}}$ ] .",
    "we would like to let @xmath326 and @xmath327 approach zero and consider the limiting multiple description problem .",
    "similar to equations   and  , when @xmath326 and @xmath327 approach 0 , we get @xmath328 where @xmath329 as in case 3 and @xmath330^{-1}$ ] as in case 2 .",
    "further , we show that @xmath331 in appendix [ app : case4e1e2 ] .",
    "we conclude that the sum rate approaches @xmath300 as @xmath326 and @xmath327 approach 0 . thus the point - to - point rate - distortion function for central receiver with distortion @xmath301 can be achieved by using the jointly gaussian description scheme , and",
    "the resulting distortions are @xmath332 where @xmath333 and @xmath334 .",
    "in other words , the jointly gaussian multiple description scheme is also optimal in this case .    to summarize",
    ", we see that the jointly gaussian description scheme achieves the limiting sum rate .",
    "the limiting sum rate is the solution to an optimization problem .",
    "for some specific distortion constraints , the sum rate can be characterized as the solution to a matrix polynomial equation ( case i ) .      applying the result in section [ sec : vecl ] to the case of @xmath4 , i.e. , the two description problem",
    ", we can see that jointly gaussian description scheme achieves the optimal sum rate .",
    "this resolves the case left out in section [ sec : vector2 ] .",
    "it also turns out that in the two description problem , we can show that jointly gaussian description strategy achieves the entire rate region .",
    "this is the main result of this subsection .    from section [ sec : outerbound ]",
    "we have a outer bound to the rate region for the two description problem @xmath335    following the discussion in section [ sec : vecl ] , we show in the following that the jointly gaussian description strategy ( jointly gaussian multiple description schemes and the time sharing between them ) achieves the outer bound to the rate region .",
    "let @xmath336 and @xmath337 now consider the optimization problem : @xmath338 as in section [ sec : vecl ] , the optimal solution @xmath147 falls into four cases .",
    "* case 1 : * @xmath339 . in this case , we know from section [ sec : innerbound ] that the rate pair @xmath340 satisfying @xmath341 is achievable using the jointly gaussian multiple description scheme with the covariance matrix of @xmath342 being @xmath206 denoting the resulting distortions as @xmath343 , we readily calculate @xmath344 for @xmath345 . from the discussion in section [ sec : vecl ] , we know that the lower bound to sum rate is achieved using this jointly gaussian description scheme .",
    "thus , in this case , the jointly gaussian description scheme achieves the rate region . as an aside , we note in this case that , @xmath147 satisfies @xmath346^{-1 } = \\left[{{\\mathbf{k}}}_{w_1}+{{\\mathbf{a}}}^*\\right]^{-1}+\\left[{{\\mathbf{k}}}_{w_2}+{{\\mathbf{a}}}^*\\right],\\ ] ] and , from the discussion in section [ sec : vector2 ] , that a sufficient condition for this case to happen is .",
    "* case 2 : * @xmath347 .",
    "this case is similar to case 1 : the jointly gaussian description scheme with covariance matrix for @xmath342 being @xmath206 achieves the lower bound on the rate region .",
    "we note that in this case the resulting distortions are @xmath348 , with @xmath349 .",
    "further , we know from the discussion in [ sec : vector2 ] , that a sufficient condition for this case to happen is @xmath350    * case 3 : * @xmath351 . in this case , we know from the discussion in section [ sec : vecl ] that for another two description problem with distortions @xmath352 such that @xmath353 , the jointly gaussian description scheme with covariance matrix for @xmath342 being @xmath354 achieves the lower bound to sum rate @xmath355 to the original distortions @xmath66 .",
    "we can see , from the contra - polymatroid structure of the achievable region of jointly gaussian description scheme , that the corner point @xmath356 in figure [ fig : region ] is achievable by this jointly gaussian description scheme",
    ".    now observe that the discussion in case 3 of section [ sec : vecl ] is symmetric with respect to the individual receivers .",
    "thus , by exchanging the role of receiver 1 and receiver 2 , we can achieve the other corner point @xmath357 in figure [ fig : region ] by another appropriate jointly gaussian description scheme .",
    "finally , time sharing between these two jointly gaussian multiple description schemes allows us to achieve the lower bound on the rate region . as an aside",
    ", we note , as a consequence of the discussion in section [ sec : vector2 ] , that a sufficient condition for this case to happen is @xmath358    * case 4 : * @xmath359 . in this case , we know , from the discussion in section [ sec : vecl ] , that for another two description problem with distortions @xmath360 such that @xmath361 and @xmath349 , the jointly gaussian description scheme with covariance matrix for @xmath362 being @xmath354 achieves the lower bound to sum rate @xmath355 to the original distortions @xmath66 . using an argument entirely analogous to that applied that the jointly gaussian description strategy achieves the rate region .",
    "to summarize : the jointly gaussian description strategy achieves the rate region for the two description problem . for a class of distortion constraints ,",
    "the corner points of the rate region can be characterized by solving a matrix polynomial equation , as already seen in section [ sec : vector2 ] .",
    "although multiple description for individual and central receivers is a special case of the most general multiple description problem , the solution to this problem sheds substantial insight to the issue - at - large . in this section , we discuss two instances of other multiple description problems that can be resolved using the insights developed so far .",
    "in particular , we discuss the problem of two descriptions with separate distortion constraints and the general multiple description problem for some special sets of distortion constraints .",
    "the problem of two descriptions with separate distortion constraints is ilustrated in figure [ fig : md_special ] .",
    "suppose the vector gaussian source @xmath85=({{{\\mathbf{x}}}}_1[m],{{{\\mathbf{x}}}}_2[m])$ ] , the dimension of @xmath363 $ ] is @xmath364 and the dimension of @xmath365 $ ] is @xmath366 .",
    "this implies that the dimension of @xmath85 $ ] is @xmath367 .",
    "let @xmath368^t{{{\\mathbf{x}}}}[m]]$ ] , @xmath369^t{{{\\mathbf{x}}}}_1[m]]$ ] , and @xmath370^t{{{\\mathbf{x}}}}_2[m]]$ ] .",
    "there are two encoders at the source providing two descriptions of @xmath85 $ ] .",
    "there are three receivers : the individual receivers 1 and 2 are only interested in generating reproduction of @xmath363 $ ] with mean square distortion constraint @xmath371 ( an @xmath372 positive definite matrix ) from description 1 and @xmath365 $ ] with mean square distortion constraint @xmath373 ( an @xmath374 positive definite matrix ) from description 2 , respectively .",
    "the central receiver uses both descriptions to generate a reproduction of @xmath85 $ ] with the error covariance meeting a distortion constraint @xmath301 ( an @xmath25 positive definite matrix ) from both descriptions .",
    "this situation is closely related to the two description problem and we can harness our results thus far to completely characterize the rate region of the problem at hand .",
    "the rate region of two description with separate distortion constraints is @xmath375 where @xmath376 is defined as @xmath377    it is clear that any rate pair @xmath378 for some @xmath379 is in the rate region for the two description with separate distortion constraints , and so @xmath380 on the other hand , although receiver 1 ( 2 ) is only interested in reconstructing @xmath381 ( @xmath382 ) , they can actually reconstruct the entire source @xmath48 based on their received descriptions .",
    "hence , any coding scheme for the two description with separate distortion constraints will result in some achievable distortions @xmath383 with @xmath384 and @xmath385 . thus any rate pair @xmath386 achieved by this coding scheme is in the rate region @xmath387 for the two description problem .",
    "thus @xmath388 from equivalence of the two regions in , the proof is now complete .",
    "consider the general gaussian multiple description problem with source covariance @xmath24 and @xmath389 distortion constraints @xmath390 for each @xmath104 .",
    "following arguments similar to that used in arriving at the lower bound for sum rate , we have an outer bound on the rate region : @xmath391 following arguments similar to those used in arriving at the upper bound for the sum rate , we can use a jointly gaussian description scheme with covariance matrix of @xmath125 s ( @xmath50 ) taking the form , any tuple @xmath60 satisfying @xmath392+\\mathbf{k}_z\\big| } { \\big|{{\\text{cov}}}[{{{\\mathbf{x}}}}|{{{\\mathbf{u}}}}_l,\\;l \\in s]\\big|\\prod\\limits_{l \\in s}\\big|{{\\text{cov}}}[{{{\\mathbf{x}}}}|{{{\\mathbf{u}}}}_l]+\\mathbf{k}_z\\big|},\\quad \\forall s \\subseteq \\{1 , \\ ; \\dots , \\ ; l\\ } \\end{array } \\right\\}\\ ] ] is achievable .",
    "thus if we can find a @xmath50 of the form in such that all of the @xmath389 distortion constraints are met with equality , i.e. , @xmath393 = [ { { \\mathbf{k}}}_x^{-1 } + ( { { \\mathbf{i}}},\\ ; \\dots , \\ ; { { \\mathbf{i}}})\\mathbf{k}_{w_s}^{-1}({{\\mathbf{i}}},\\ ; \\dots , \\ ; { { \\mathbf{i}}})^t]^{-1 } , \\quad \\forall s \\subseteq \\{1 , \\ ; \\dots , \\ ; l\\},\\ ] ] where @xmath62 is the covariance matrix for all @xmath394 , then the achievable region matches the outer bound and we would have characterized the rate region of the multiple description problem .    from the above discussion , we see that for some choice of distortion constraints of the multiple description problem , we can indeed do this : first choose @xmath3 distortions @xmath395 such that they satisfy the condition for theorem [ th : main ] for the multiple description problem with individual and central receivers .",
    "next we can solve for the @xmath50 which is the covariance matrix of @xmath396 for the sum - rate - achieving jointly gaussian description scheme .",
    "for any other @xmath397 , this scheme results in distortion @xmath398^{-1}$ ] .",
    "finally we choose these @xmath399 s as the other distortion constraints .",
    "now we have a general multiple description problem with @xmath389 distortion constraints @xmath390 for each @xmath104 , and hence we can find a @xmath50 of form such that all of the @xmath389 distortion constraints are met with equality . thus is actually the rate region and it can be achieved by a jointly gaussian description scheme .",
    "in this appendix we provide some useful results in matrix analysis that are extensively used in this paper .",
    "* theorem 2.5 ) let @xmath142 be an @xmath400 nonsingular matrix and @xmath401 be an @xmath402 nonsingular matrix and let @xmath403 and @xmath404 be @xmath405 and @xmath406 matrices , respectively . if the matrix @xmath407 is nonsingular , then @xmath408    [ lemma : blockinverse ] ( * ? ? ?",
    "* theorem 2.3 ) suppose that the partitioned matrix @xmath409 is invertible and that the inverse is conformally partitioned as @xmath410 if @xmath142 is a nonsingular principal sub - matrix of @xmath411 , then @xmath412    ( * ? ? ?",
    "* theorem 6.13 ) let @xmath413 be a positive definite matrix and let @xmath414 be an @xmath406 matrix .",
    "then for any @xmath415 positive definite matrix @xmath416 , @xmath417    ( * ? ? ?",
    "* theorem 6.8 and 6.9 ) let @xmath142 and @xmath401 be positive definite matrices such that @xmath418 .",
    "then , @xmath419",
    "define an i.i.d .  random process @xmath420\\}$ ] , @xmath87 of @xmath123 gaussian random vectors , where @xmath421 $ ] , @xmath87 are independent of @xmath36 and @xmath422 , @xmath423 .",
    "form a random process @xmath424,\\;\\dots,\\;{{{\\mathbf{y}}}}[n])^t$ ] by @xmath425 = { { { \\mathbf{x}}}}[m]+{{{\\mathbf{z}}}}[m ] , \\quad m=1,\\ ; \\dots,\\ ; n.\\ ] ] it follows that @xmath426\\}$ ] is an i.i.d .  random process of @xmath427 gaussian random vectors , where @xmath428 .",
    "then @xmath429 since @xmath36 and @xmath430 are gaussian vectors , for the first two terms in , we have @xmath431 we also have the following bound on @xmath432 for @xmath90 : @xmath433|c_l ) \\\\ & \\le \\sum\\limits_{m=1}^n \\frac{1}{2}\\log(2\\pi e)^n\\big|{{\\text{cov}}}[{{{\\mathbf{y}}}}[m]|c_l]\\big| \\\\ & \\le \\frac{1}{2}\\log(2\\pi e)^{nn}+\\frac{n}{2}\\log\\left|\\frac{1}{n}\\sum\\limits_{m=1}^n { { \\text{cov}}}[{{{\\mathbf{y}}}}[m]|c_l]\\right| \\\\ & = \\frac{1}{2}\\log(2\\pi e)^{nn}+\\frac{n}{2}\\log\\left|\\frac{1}{n}\\sum\\limits_{m=1}^n { { \\text{cov}}}[({{{\\mathbf{x}}}}[m]+{{{\\mathbf{z}}}}[m])|c_l]\\right| \\\\ & = \\frac{1}{2}\\log(2\\pi e)^{nn}+\\frac{n}{2}\\log\\left|\\frac{1}{n}\\sum\\limits_{m=1}^n { { \\text{cov}}}[{{{\\mathbf{x}}}}[m]|c_l]+{{\\mathbf{k}}}_z\\right| \\\\ & \\le \\frac{1}{2}\\log(2\\pi e)^{nn}+\\frac{n}{2}\\log\\left|{{\\mathbf{d}}}_l+{{\\mathbf{k}}}_z\\right| \\\\ & = \\frac{1}{2}\\log(2\\pi e)^{nn}\\left|{{\\mathbf{d}}}_l+{{\\mathbf{k}}}_z\\right|^n . \\end{split}\\ ] ] next we bound the last two terms of as follows .",
    "@xmath434 letting @xmath435 \\stackrel{\\rm def}{= } { { \\text{cov}}}[{{{\\mathbf{x}}}}[m]-\\hat{{{{\\mathbf{x}}}}}_0[m]],\\label{eq : covdefn}\\ ] ] we have @xmath436)-h({{{\\mathbf{z}}}}[m]|{{{\\mathbf{z}}}}[1],\\;\\dots,\\;{{{\\mathbf{z}}}}[m-1],{{{\\mathbf{y}}}}^n-\\hat{{{{\\mathbf{x}}}}}_0^n)\\big ) \\\\ & \\ge \\sum\\limits_{m=1}^n \\big(h({{{\\mathbf{z}}}}[m])-h({{{\\mathbf{z}}}}[m]|{{{\\mathbf{y}}}}[m]-\\hat{{{{\\mathbf{x}}}}}_0[m])\\big ) \\\\ & = \\sum\\limits_{m=1}^ni({{{\\mathbf{z}}}}[m];{{{\\mathbf{x}}}}[m]-\\hat{{{{\\mathbf{x}}}}}_0[m]+{{{\\mathbf{z}}}}[m ] ) \\\\ & \\overset{(a)}{\\ge } \\sum\\limits_{m=1}^n\\frac{1}{2}\\log\\frac{|{{\\mathbf{k}}}_c[m]+{{\\mathbf{k}}}_z[m]|}{|{{\\mathbf{k}}}_c[m]| } \\\\ & \\overset{(b)}{\\ge}\\frac{n}{2}\\log\\frac{|{{\\mathbf{d}}}_0+{{\\mathbf{k}}}_z|}{|{{\\mathbf{d}}}_0| } , \\end{split}\\ ] ] where ( a ) is from and ( * ? ? ?",
    "* lemma ii.2 ) .",
    "the justfication for ( b ) is from the convexity of @xmath437 in @xmath438 and . from",
    "and we have @xmath439 combining , and , we have @xmath440 taking the supremum over all positive definite @xmath102 , we can sharpen the lower bound in : @xmath441      first we assume @xmath456 , and hence @xmath457^{-1 } \\\\",
    "= & { { \\mathbf{a}}}- { { \\mathbf{a}}}\\left({{\\mathbf{i}}}\\ ; { { \\mathbf{i}}}\\ ; \\dots { { \\mathbf{i}}}\\right)\\left[{{\\mathbf{k}}}_w+\\left({{\\mathbf{i}}}\\ ; { { \\mathbf{i}}}\\ ; \\dots \\ ; { { \\mathbf{i}}}\\right)^t { { \\mathbf{a}}}\\left({{\\mathbf{i}}}\\ ; { { \\mathbf{i}}}\\ ; \\dots \\ ; { { \\mathbf{i}}}\\right)\\right]^{-1}\\left({{\\mathbf{i}}}\\ ; { { \\mathbf{i}}}\\ ; \\dots \\ ; { { \\mathbf{i}}}\\right)^t { { \\mathbf{a}}}\\\\   = &   { { \\mathbf{a}}}- { { \\mathbf{a}}}\\left({{\\mathbf{i}}}\\ ; { { \\mathbf{i}}}\\ ; \\dots { { \\mathbf{i}}}\\right ) \\big[{{\\text{diag}}}\\{{{\\mathbf{k}}}_{w_1}+{{\\mathbf{a}}},\\ ; { { \\mathbf{k}}}_{w_2}+{{\\mathbf{a } } } , \\ ; \\dots \\ ; { { \\mathbf{k}}}_{w_l}+{{\\mathbf{a}}}\\ } \\big]^{-1}\\left({{\\mathbf{i}}}\\ ; { { \\mathbf{i}}}\\ ; \\dots { { \\mathbf{i}}}\\right)^t { { \\mathbf{a}}}\\\\ = & { { \\mathbf{a}}}- { { \\mathbf{a}}}\\sum\\limits_{l=1}^l [ { { \\mathbf{k}}}_{w_l}+{{\\mathbf{a}}}]^{-1 } { { \\mathbf{a}}}. \\\\ \\end{split}\\ ] ] thus , @xmath458^{-1}-{{\\mathbf{a}}}^{-1 }",
    "\\\\ = & { { \\mathbf{a}}}^{-1 } -   { { \\mathbf{a}}}^{-1 } { { \\mathbf{a}}}\\left [ -\\left(\\sum\\limits_{l=1}^l ( { { \\mathbf{k}}}_{w_l}+{{\\mathbf{a}}})^{-1}\\right)^{-1 } + { { \\mathbf{a}}}{{\\mathbf{a}}}^{-1}{{\\mathbf{a}}}\\right]^{-1}{{\\mathbf{a}}}{{\\mathbf{a}}}^{-1 } - { { \\mathbf{a}}}^{-1 } \\\\ = & \\left[\\left(\\sum\\limits_{l=1}^l ( { { \\mathbf{k}}}_{w_l}+{{\\mathbf{a}}})^{-1}\\right)^{-1}-{{\\mathbf{a}}}\\right]^{-1}. \\end{split}\\ ] ] when @xmath142 is singular , we can choose @xmath260 such that @xmath459 for @xmath460 , and thus we can apply the previous argument and let @xmath100 in the end .",
    "we use induction .",
    "first consider the matrix @xmath461 we have @xmath462 where ( a ) is from .",
    "next we define @xmath463 and suppose @xmath464 for @xmath465 . then @xmath466^{-1}{{\\mathbf{a}}}\\\\ \\ ;   \\longleftrightarrow   \\ ; & { { \\mathbf{k}}}_{w_l}+{{\\mathbf{a}}}\\succ { { \\mathbf{a}}}\\left[\\left(\\sum\\limits_{k=1}^{l-1}({{\\mathbf{k}}}_{w_k}+{{\\mathbf{a}}})^{-1}\\right)^{-1}-{{\\mathbf{a}}}\\right]^{-1}{{\\mathbf{a}}}+{{\\mathbf{a}}}\\\\ \\ ;   \\longleftrightarrow   \\ ; & ( { { \\mathbf{k}}}_{w_l}+{{\\mathbf{a}}})^{-1 } \\prec \\left[{{\\mathbf{a}}}\\left[\\left(\\sum\\limits_{k=1}^{l-1}({{\\mathbf{k}}}_{w_k}+{{\\mathbf{a}}})^{-1}\\right)^{-1}-{{\\mathbf{a}}}\\right]^{-1}{{\\mathbf{a}}}+{{\\mathbf{a}}}\\right]^{-1 } \\\\ \\ ;   \\longleftrightarrow   \\ ; & ( { { \\mathbf{k}}}_{w_l}+{{\\mathbf{a}}})^{-1 } \\prec { { \\mathbf{a}}}^{-1 } - \\left[\\left(\\sum\\limits_{k=1}^{l-1}({{\\mathbf{k}}}_{w_k}+{{\\mathbf{a}}})^{-1}\\right)^{-1}-{{\\mathbf{a}}}+{{\\mathbf{a}}}\\right]^{-1 } \\\\ \\ ;   \\longleftrightarrow   \\ ; & ( { { \\mathbf{k}}}_{w_l}+{{\\mathbf{a}}})^{-1 } \\prec { { \\mathbf{a}}}^{-1 } - \\sum\\limits_{k=1}^{l-1}({{\\mathbf{k}}}_{w_k}+{{\\mathbf{a}}})^{-1 } \\\\ \\ ; { \\longleftarrow } \\ ; & \\sum\\limits_{k=1}^{l}({{\\mathbf{k}}}_{w_k}+{{\\mathbf{a}}})^{-1 } \\prec { { \\mathbf{a}}}^{-1 } \\\\ \\ ;   \\overset{(b)}{\\longleftrightarrow } \\ ; & ( { { \\mathbf{k}}}_{w_0}+{{\\mathbf{a}}})^{-1 } \\prec { { \\mathbf{a}}}^{-1 } \\\\ \\ ;   \\longleftrightarrow   \\ ; & { { \\mathbf{k}}}_{w_0}+{{\\mathbf{a}}}\\succ { { \\mathbf{a}}}\\\\ \\ ;   \\longleftrightarrow   \\ ; & { { \\mathbf{k}}}_{w_0 } \\succ 0 , \\end{split}\\ ] ] where ( b ) is from .",
    "@xmath470^{-1 } & = \\left[({{\\mathbf{k}}}_{w_0}+{{\\mathbf{a}}}^*)^{-1}({{\\mathbf{i}}}+ ( { { \\mathbf{k}}}_{w_0}+{{\\mathbf{a}}}^*){{\\mathbf{\\lambda}}}_1)\\right]^{-1 } \\\\ & \\overset{(a)}{= } ( { { \\mathbf{i}}}+{{\\mathbf{k}}}_{w_0}{{\\mathbf{\\lambda}}}_1)^{-1}({{\\mathbf{k}}}_{w_0}+{{\\mathbf{a}}}^ * ) \\\\ & = ( { { \\mathbf{i}}}+{{\\mathbf{k}}}_{w_0}{{\\mathbf{\\lambda}}}_1)^{-1}({{\\mathbf{k}}}_{w_0}+{{\\mathbf{a}}}^ * - ( { { \\mathbf{i}}}+{{\\mathbf{k}}}_{w_0}{{\\mathbf{\\lambda}}}_1){{\\mathbf{a}}}^*)+{{\\mathbf{a}}}^ * \\\\ & \\overset{(b)}{= } ( { { \\mathbf{i}}}+{{\\mathbf{k}}}_{w_0}{{\\mathbf{\\lambda}}}_1)^{-1}{{\\mathbf{k}}}_{w_0}+{{\\mathbf{a}}}^ * \\\\ & = \\left({{\\mathbf{k}}}_{w_0}^{-1}({{\\mathbf{i}}}+{{\\mathbf{k}}}_{w_0}{{\\mathbf{\\lambda}}}_1)\\right)^{-1 } + { { \\mathbf{a}}}^ * \\\\ & = \\left({{\\mathbf{k}}}_{w_0}^{-1}+{{\\mathbf{\\lambda}}}_1\\right)^{-1 } + { { \\mathbf{a}}}^ * , \\end{split}\\ ] ]    where ( a ) and ( b ) are from @xmath471 .",
    "@xmath472 where ( c ) is from @xmath471 .",
    "conditioned on @xmath442 , the collection of random variables @xmath443 are jointly gaussian and thus we have latexmath:[\\[\\sum\\limits_{l=1}^lh({{{\\mathbf{u}}}}_l|{{{\\mathbf{y}}}})-h({{{\\mathbf{u}}}}_1,\\;\\dots,\\;{{{\\mathbf{u}}}}_l|{{{\\mathbf{y } } } } ) = \\frac{1}{2}\\log\\frac{\\prod\\limits_{l=1}^l    from mmse of @xmath56 from @xmath442 we have @xmath445   = { { \\mathbf{k}}}_x+\\mathbf{k}_{w_l}-{{\\mathbf{k}}}_x({{\\mathbf{k}}}_x+\\mathbf{k}_z)^{-1}{{\\mathbf{k}}}_x , \\quad l=1,\\;\\dots,\\ ; l\\ ] ] and @xmath446 where @xmath447 is an @xmath448 matrix of all ones and @xmath449 is the kronecker product .    by fischer inequality (",
    "the block matrix version of hadamard inequality , see ( * ? ? ? * theorem 6.10 ) ) we know that @xmath450| } = { \\big|{{\\text{cov}}}[{{{\\mathbf{u}}}}_1,\\;\\dots,\\;{{{\\mathbf{u}}}}_l|{{{\\mathbf{y}}}}]\\big|}$ ] if and only if the off - diagonal block matrices of @xmath451 $ ] are all zero matrices .",
    "thus we have @xmath452 if and only if @xmath453 or equivalently , if and only if @xmath454 to get a valid @xmath455 , we need the additional condition @xmath132 .",
    "first we prove that @xmath467    we have @xmath75^{\\frac{1}{2}}({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{\\frac{1}{2 } } -{{\\mathbf{k}}}_{w_0}.\\ ] ] thus @xmath468^{\\frac{1}{2}}({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{\\frac{1}{2 } } \\succ { { \\mathbf{k}}}_{w_0 } \\\\ \\ ;   \\longleftrightarrow   \\ ;   & \\left[({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{-\\frac{1}{2 } } ( { { \\mathbf{k}}}_{w_2}-{{\\mathbf{k}}}_{w_0})({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{-\\frac{1}{2}}\\right]^{\\frac{1}{2 } } \\succ ( { { \\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{-\\frac{1}{2}}{{\\mathbf{k}}}_{w_0}({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{-\\frac{1}{2 } } \\\\ \\ ; { \\longleftarrow } \\ ; & ( { { \\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{-\\frac{1}{2 } } ( { { \\mathbf{k}}}_{w_2}-{{\\mathbf{k}}}_{w_0})({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{-\\frac{1}{2 } }   \\\\ & \\quad \\succ ( { { \\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{-\\frac{1}{2}}{{\\mathbf{k}}}_{w_0}({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{-1}{{\\mathbf{k}}}_{w_0}({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{-\\frac{1}{2 } } \\\\ \\ ;   \\longleftrightarrow   \\ ; & { { \\mathbf{k}}}_{w_2}-{{\\mathbf{k}}}_{w_0 } \\succ { { \\mathbf{k}}}_{w_0}\\left({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0}\\right)^{-1}{{\\mathbf{k}}}_{w_0 } \\\\ \\ ;   \\longleftrightarrow   \\ ; & { { \\mathbf{k}}}_{w_2}-{{\\mathbf{k}}}_{w_0 } \\succ { { \\mathbf{k}}}_{w_0}\\left(-{{\\mathbf{i}}}+ ( { { \\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{-1}\\right){{\\mathbf{k}}}_{w_1 } \\\\ \\ ;   \\longleftrightarrow   \\ ; & { { \\mathbf{k}}}_{w_2}-{{\\mathbf{k}}}_{w_0 } \\succ -{{\\mathbf{k}}}_{w_0 } + { { \\mathbf{k}}}_{w_0}({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{-1}{{\\mathbf{k}}}_{w_1 } \\\\ \\ ;   \\longleftrightarrow   \\ ; & { { \\mathbf{k}}}_{w_2 } \\succ { { \\mathbf{k}}}_{w_0}({{\\mathbf{k}}}_{w_1}-{{\\mathbf{k}}}_{w_0})^{-1}{{\\mathbf{k}}}_{w_1 } \\\\ \\ ;   \\longleftrightarrow   \\ ; & { { \\mathbf{k}}}_{w_2 } \\succ { { \\mathbf{k}}}_{w_0}{{\\mathbf{k}}}_{w_0}^{-1}({{\\mathbf{k}}}_{w_0}^{-1}-{{\\mathbf{k}}}_{w_1}^{-1})^{-1}{{\\mathbf{k}}}_{w_1}^{-1}{{\\mathbf{k}}}_{w_1 } \\\\ \\ ;   \\longleftrightarrow   \\ ; & { { \\mathbf{k}}}_{w_2 } \\succ ( { { \\mathbf{k}}}_{w_0}^{-1}-{{\\mathbf{k}}}_{w_1}^{-1})^{-1 } \\\\ \\ ;   \\longleftrightarrow   \\ ; & { { \\mathbf{k}}}_{w_1}^{-1 } + { { \\mathbf{k}}}_{w_2}^{-1 } \\prec { { \\mathbf{k}}}_{w_0}^{-1 } \\\\ \\ ;   \\longleftrightarrow   \\ ; & { { \\mathbf{d}}}_0^{-1 } + { { \\mathbf{k}}}_x^{-1 } - { { \\mathbf{d}}}_1^{-1}-{{\\mathbf{d}}}_2^{-1 } \\succ \\mathbf{0}. \\end{split}\\ ] ]    the proof of @xmath469 is similar and hence is omitted .",
    "we first prove the following lemma .",
    "[ lemma : depsilon ] let @xmath473 be an @xmath25 matrix such that @xmath474 .",
    "let @xmath475 .",
    "choose @xmath476 such that @xmath477 .",
    "define @xmath478^{-1}.\\ ] ] then , there exist constants @xmath479 , such that @xmath480    there exists an @xmath25 orthogonal matrix @xmath20 such that @xmath481 where @xmath482 are eigenvalues of @xmath483 .",
    "we have @xmath484 and @xmath485^{-1 } { { \\mathbf{q}}}^t \\\\ & = \\big[({{\\text{diag}}}\\{k_1,\\;\\dots,\\;k_n\\}-\\epsilon { { \\mathbf{i}}})^{-1}+{{\\mathbf{i}}}\\big]^{-1 } \\\\ &",
    "= { { \\text{diag}}}\\left\\{\\frac{k_1-\\epsilon}{1+k_1-\\epsilon},\\;\\dots,\\;\\frac{k_n-\\epsilon}{1+k_n-\\epsilon}\\right\\}\\\\ & = { { \\text{diag}}}\\left\\{\\frac{k_1}{1+k_1}-\\frac{\\epsilon}{(1+k_1)^2}+o(\\epsilon ) , \\;\\dots,\\;\\frac{k_n}{1+k_n}-\\frac{\\epsilon}{(1+k_n)^2}+o(\\epsilon ) \\right\\}. \\end{split}\\ ] ] we now have @xmath486 where @xmath487 are some constants . hence @xmath488    equations and are a direct consequence of this lemma .",
    "we first prove the following lemma .",
    "[ lemma : kzepsilon ] let @xmath142 be an @xmath25 matrix such that @xmath489 .",
    "let @xmath490 .",
    "choose @xmath476 such that @xmath491 .",
    "define @xmath492^{-1}-{{\\mathbf{i}}}.\\ ] ] then , there exist constants @xmath493 such that @xmath494    there exists an @xmath25 orthogonal matrix @xmath20 such that @xmath495 where @xmath496 are the eigenvalues of @xmath142 .",
    "we have @xmath497 and @xmath498 we now have @xmath499 where @xmath500 are some constants .",
    "hence @xmath501    equation is a direct result of this lemma .",
    "we first prove the following lemma .",
    "[ lemma : kzinf ] let @xmath142 be an @xmath25 matrix such that @xmath502 . choose @xmath476 such that @xmath503 . define @xmath504^{-1}-{{\\mathbf{i}}}.\\",
    "] ] then , for any @xmath505 and @xmath414 such that @xmath506 and @xmath507 , we have @xmath508                  using this lemma , we can show a property similar to as @xmath326 approaches zero .",
    "first note that similar to case 2 , we have @xmath519 where @xmath520 and @xmath521 are constants .",
    "hence we have @xmath522 similarly , we have @xmath523 thus @xmath524 where the last step is similar to .",
    "r.  puri , s.  s. pradhan , and k.  ramchandran , `` n - channel symmetric multiple descroption - part ii : an achievable rate - distortion region , '' _ ieee trans .",
    "inform . theory _",
    "51 , no .  4 , pp .",
    "13771392 , apr .",
    "2005 .",
    "v.  a. vaishampayan , n.  sloane , and s.  servetto , `` multiple description vector quantizers with lattice codebooks : design and analysis , '' _ ieee trans .",
    "inform . theory _",
    "47 , no .  4 , pp .",
    "17181734 , july 2001 .",
    "d.  tse and s.  hanly , `` multi - access fading channels : part i : polymatroid structure , optimal resource allocation and throughput capacities , '' _ ieee trans .",
    "inform . theory _",
    "44 , no .  7 , pp . 27962815 , nov .",
    "p.  viswanath , `` sum rate of a class of gaussian multiterminal source coding problems , '' in _ advances in network information theory _ , p. gupta , g. kramer and a. wijngaarden editors , rutgers , nj , 2004 , pp . 4360 ."
  ],
  "abstract_text": [
    "<S> @xmath0 multiple descriptions of a vector gaussian source for individual and central receivers are investigated . </S>",
    "<S> the sum rate of the descriptions with covariance distortion measure constraints , in a positive semidefinite ordering , is exactly characterized . for two descriptions , </S>",
    "<S> the entire rate region is characterized . </S>",
    "<S> jointly gaussian descriptions are optimal in achieving the limiting rates . </S>",
    "<S> the key component of the solution is a novel information - theoretic inequality that is used to lower bound the achievable multiple description rates .    _ </S>",
    "<S> submitted to the ieee transactions on information theory , oct . 2005 _    </S>",
    "<S> 1.3ex * vector gaussian multiple description with individual and central receivers * + hua wang and pramod viswanath </S>"
  ]
}