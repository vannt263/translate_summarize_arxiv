{
  "article_text": [
    "a web crawler traditionally fulfills two purposes : discovering new pages and refreshing already discovered pages .",
    "both of these problems have been extensively investigated over the past decade ( see the survey paper by olston and najork @xcite ) .",
    "however , recently , the role of the web as a media source became increasingly important as more and more people start to use it as their primary source of up - to - date information .",
    "this evolution forces crawlers of web search engines to continuously collect newly created pages as fast as possible , especially high - quality ones .    surprisingly , user traffic to many of these newly created pages grows really quickly right after they appear , but lasts only for a few days .",
    "for example , it was discussed in several papers that the popularity of news decreases exponentially with time @xcite .",
    "this observation naturally leads to distinguishing two types of new pages appearing on the web : _ ephemeral _ and _ non - ephemeral _ pages . note that here we do not consider the ephemeral content , which might be removed before it hits the index as in @xcite ( e.g. advertisements or the `` quote of the day '' ) , but we consider persistent content that is ephemeral in terms of user interest ( e.g. news , blog and forum posts ) .",
    "we clustered user interest patterns of some new pages discovered in one week ( see section [ contentsources ] for details ) , and figure  [ fig : yabar ] shows the centroids of the obtained clusters . in section [ contentsources ] , we show that a significant fraction of new pages appearing on the web every day are ephemeral pages .",
    "the cost of the time delay between the appearance of such ephemeral new pages and their crawl is thus very high in terms of search engine user satisfaction . moreover , if a crawler fails to find such a page during its period of peak interest , then there might be no need to crawl it at all .",
    "it was reported in @xcite , that 1 - 2% of user queries are extremely recency sensitive , while even more are also recency sensitive to some extent .",
    "the problem of timely finding and crawling ephemeral new pages is thus important , but , to the best of our knowledge , is not studied in the literature . indeed ,",
    "different metrics were suggested to measure the coverage and freshness of the crawled corpus @xcite , but they do not take into account the degradation of the profit to a search engine contributed by these pages . crawling policies based on such metrics may then crawl such new pages not quickly enough , and even crawl already obsolete content .",
    "thus , we need a new quality metric , well thought out for this task , and a crawling algorithm optimized to maximize this metric over time , that takes into account this degradation of pages utility .",
    "our daily experience of using the web also suggests that such ephemeral new pages can be found from a relatively small set of `` hubs '' or _ content sources_. we investigate this intuition and show that it is possible and practical to find such sources at scale .",
    "examples of content sources are main pages of blogs , news sites , category pages of such news sites ( e.g.  politics , economy ) , rss feeds , sitemaps @xcite , etc . , and one needs to periodically recrawl such sources in order to find and crawl ephemeral new pages way before their peak of user interest . however",
    ", frequent recrawling of all these sources and all new pages found on them requires a huge amount of resources and is quite inefficient . in order to solve this problem efficiently ,",
    "we analyze the problem of dividing limited resources between different tasks ( coined as _ holistic crawl ordering _ by oslton and najork in @xcite ) , i.e. , here between the task of crawling ephemeral new pages and the task of recrawling content sources in order to discover those new pages . a possible solution for this problem",
    "is to give a fixed quota to each policy ( see , e.g. , @xcite ) , but we will show that such solutions based on fixed quotas are far from being optimal . in this paper , we propose a new algorithm that dynamically estimates , for each content source , the rate of new links appearance in order to find and crawl newly created pages as they appear . as a matter of fact , it is next to impossible to crawl all these new pages immediately due to resource constraints , therefore , a reasonable crawling policy has to crawl the highest quality pages in priority .",
    "the quality of a page can be measured in different ways , and it can , for example , be based on the link structure of the web graph ( e.g. , in - degree @xcite or pagerank @xcite ) , or on some external signals ( e.g. , query log @xcite or the number of times a page was shown in the results of a search engine @xcite ) . in this paper , we propose to use the number of clicks in order to estimate the quality of pages , and predict the quality of newly created pages by using the quality of pages previously linked from each content source . by the number of clicks , we mean the number of times a user clicked on a link to this page on a search engine results page ( serp ) , which most reliably indicates a certain level of user interest in the page s content . in this way , we are able , in fact , to incorporate user feedback into the process of crawling for our algorithm to find and crawl the best new pages .    to sum up , this paper makes the following contributions :    * we formalize the problem of timely crawling of high - quality ephemeral new web content by suggesting to optimize a new quality metric , which measures the ability of a crawing algorithm to solve this specific problem ( section [ formalization ] ) .",
    "* we show that most of such ephemeral new content can be found at a small set of content sources , and we propose a method to find such sources ( section [ contentsources ] ) . *",
    "we propose a practical algorithm , which periodically recrawls content sources and crawls newly created pages linked from them , as a solution of this problem .",
    "this algorithm uses user feedback to estimate the quality of content sources ( section [ algo ] ) .",
    "* we validate our algorithm by comparing it to other crawling strategies on real - world data ( section  [ exp ] ) .    besides , in section [ relatedwork ] , we review related work , while in section [ conclusion ] , we conclude the paper and discuss possible directions for future research .",
    "in this section , we formalize the problem under consideration by introducing an appropriate quality metric , which measures the ability of a crawling algorithm to solve this problem . as we discussed in the introduction",
    ", we deal with pages for which user interest grows within hours after they appear , but lasts only for several days .",
    "the profit of crawling such ephemeral new pages thus decreases dramatically with time .",
    "assume that for each page @xmath0 , we know a decreasing function @xmath1 , which is the profit of crawling this page with delay @xmath2 seconds after its creation time @xmath3 ( by profit , one can mean the expected number of clicks or shows on serp ) .",
    "if , finally , each page @xmath0 was crawled with a delay @xmath4 , we can define the _ dynamic quality _ of a crawler as : @xmath5 }   p_i ( \\delta t_i).\\ ] ] in other words , the dynamic quality is the average profit gained by a crawler per second in a time window of size @xmath6 . the dynamic quality defined above",
    "can be useful to understand the influence of daily and weekly trends on the performance of a crawler .",
    "let us now define the _ overall quality _ of a crawler , which allows to easily compare different algorithms over larger time windows .",
    "it is natural to expect that if @xmath6 is large enough then the influence of season and weekly trends of user interest will be reduced . in other words",
    ", the function @xmath7 tends to a constant while @xmath6 increases .",
    "thus , we can consider the _ overall quality _ : @xmath8 }   p_i ( \\vartriangle t_i),\\ ] ] which does not depend on @xmath9 and @xmath6 .    in this paper , by @xmath1 profit of crawling a page @xmath0 at time @xmath10 , we mean the total number of clicks this page will get on a serp after this time ( ignoring any indexing delay ) . in this way",
    ", we can approximate the relevance of a page to current interests of users . from a crawler s perspective",
    ", it is thus an appropriate measure of the contribution of new pages to a search engine performance ( given a specific ranking method ) .    alternatively , instead of the number of clicks , we could use the number of shows , i.e. , the number of times a page was shown in the top @xmath11 results of a search engine .",
    "this value also reflects a crawler s performance in the sense that we only want to crawl ephemeral new pages , which are going to be shown to users .",
    "but , as we discuss further in this section , the number of clicks and the number shows behave similarly and we thus use clicks since they reflect the actual preference of users .    at this point",
    ", we have defined a new metric to measure the quality of crawling ephemeral new pages .",
    "we are going to use this metric to validate our algorithm in section  [ exp ] .",
    "note , that this metric can only be evaluated with some delay because we need to wait when crawled pages are not shown anymore to take their profit , i.e. , their number of clicks , into account .",
    "0.5     0.5     however , for our crawling algorithm , we do not want to wait for such a long period of time to be able to use the profit of freshly crawled pages in order to quickly adapt to changes of content sources properties .",
    "of course , we do not know the function @xmath1 for pages that just appeared , but we can try to predict it .",
    "it is natural to expect that pages of similar nature exhibit similar distributions of user interest . in order to demonstrate this , on figure  [ fig : avg_shows ] and figure  [ fig : avg_clicks ] we plot respectively the average number of cumulative shows and clicks depending on the page s age for all pages published on a news site and a blog ( both being randomly chosen ) over @xmath12 week .",
    "we can see that almost all clicks and shows appear in the first week of a page s life , and that the dependency of the cumulative number of clicks ( shows ) gathered by a page on the page s age is pretty well described by the function : @xmath13 , where @xmath14 is the total number of clicks ( shows ) a page gathers during its life .",
    "we thus propose the following approximation of the profit @xmath1 ( i.e. , the number of future clicks ) : @xmath15 where the _ rate of decay _ @xmath16 and the _ profit _",
    "@xmath17 are content - source - specific and should be estimated using historical data ( see section  [ p&mu ] for details ) .",
    "we use this approximation in section [ algo ] in order to analyze the problem under consideration theoretically .",
    "[ howtofind ]    in this section , we show that most ephemeral new content can indeed be found at a small set of content sources and then describe a simple procedure for finding such a set , that fits our use case .",
    "our hypothesis is that one can find the most of ephemeral new pages appearing on the web at a small set of content sources , but links from these sources to new pages are short living so a crawler needs to frequently recrawl these sources to avoid missing links to new pages , especially to high - quality pages .    in order to validate this hypothesis about content sources ,",
    "we need to follow the evolution over time of the link structure of the web , to understand which content sources refer which new pages as they appear . our web crawler logs could be used for this , but there are two main issues with this approach : 1 ) keeping the full history of new pages linked from each content source , even for some small time period , is impractical due to resource constraints ; and 2 ) existing crawlers do not revisit each content source often enough to provide more than a really partial view of the evolution of the link structure of the web .",
    "so , instead , we used the toolbar logs continuously collected at yandex ( russia s most popular search engine ) to monitor user visits to web pages . in this way",
    ", we can easily track the appearance of new pages that are of interest to at least one user , know which content sources referred them , and also follow the evolution of user interest over time for each of these new pages .",
    "this data is a representative sample of the web as this toolbar is used by millions of people across different countries .",
    "but we can not use this data in the algorithm itself since it is not available in all countries , and thus we use it only in order to validate our hypothesis about the existence of relatively small set of content sources . using this toolbar data , we randomly sampled 50k pages from the set of new pages that appeared over a period of one week and were visited by at least one user .",
    "these pages were distributed over @xmath18k different hosts . for each page",
    ", we computed its number of daily visits for a period of two weeks after it appeared .",
    "then , using this 14-dimensional ( one per day ) feature vector ( scaled to have its maximum value equal to 1 ) , we clustered these pages into 6 clusters by applying the k - means method .",
    "let us note that when we tried less clusters , non - ephemeral pages were not assigned to one cluster .",
    "finally , we obtained only @xmath19% non - ephemeral pages .",
    "the percentage of new pages that are ephemeral ( and were visited at least once ) for this week is thus 96% , which is really significant .",
    "centroids of these clusters are plotted on figure  [ fig : centroids-6 ] ( in section  [ intro ] we showed only two of them ) .    our toolbar logs also contain , in most cases , the referrer page for each recorded visit to a page , i.e. , the source page from which the user came to visit this target page .",
    "we extracted all these _ links _ ( users transitions between pages ) found in the logs pointing to one of the ephemeral new pages in our sample over the same period of one week plus several days ( for the most recent pages to become obsolete ) , and obtained @xmath20k links . using these links , we studied the percentage of ephemeral new pages reached depending on the number of content sources .",
    "we want to find the smallest set of content sources that allows to reach the most of new pages and thus proceed as follows ( in a greedy manner ) .",
    "we first take the source page , which allows to reach the most of pages , then remove it and all covered pages , then select the second one in the same way , and so on .",
    "we see on figure  [ fig : hubs ] , that only 3k content sources are required to cover 80% of new content , which validates our hypothesis about content sources .",
    "interestingly , 42% of these 3k content sources are main pages of web sites , while 44% are category pages , which can be accessed from the main page .",
    "so , overall , 86% of them are at most 1 hop away from the main page .",
    "now , we need to understand how to effectively find these content sources at scale without relying on toolbar logs , which , as said , are not available world - wide .",
    "our crawling algorithm ( described in the next section ) focuses on high - quality ephemeral new content .",
    "therefore , even if content sources that produce low quality content or almost no new content at all are given to this algorithm , they will almost never be crawled or crawled just in case much later when some spare resources are available ( see section [ algo ] ) .",
    "we are thus , to some extent , only interested in recall when finding such sources here , i.e. , to get most of them , which makes this task much easier to solve in practice .",
    "analyzing the set of content sources discovered using toolbar data , we noticed that 86% of content sources that we found are actually at most 1 hop away from the main page of their host , as said .",
    "the following procedure will thus yield a relatively small set of content sources that generate most of the ephemeral new content on a given set of hosts .    1 .",
    "crawl the main page of each host ( once ) and keep pages linked from it ( all pages 1-hop away from the main page ) ; 2 .   select the main page , and all found pages older than few days , by using historical data , as content sources ( as new pages are almost never content sources ) .",
    "let us note that this procedure is easy to run periodically to refresh the set of content sources , and to find new content sources .",
    "it is possible to use url patterns as described in @xcite in order to get a better precision , but this optimization is not required here , because our crawling algorithm optimizes precision itself by avoiding to crawl low - quality content sources ( see section [ algo ] ) .    the input list of hosts for this procedure can be obtained during a standard web crawling routine by ranking found hosts by their tendency to generate new content .",
    "this simple method also fits our usage scenario considering that , as said , only recall is important for us when finding content sources as input for our algorithm .",
    "in this section , we assume that we are given a relatively small set of content sources , which regularly generate new content ( see section [ howtofind ] for the procedure to select such a set ) .",
    "our current aims are to ( 1 ) find an optimal schedule to recrawl content sources in order to quickly discover high - quality ephemeral new pages , and ( 2 ) understand how to spread resources between crawling new pages and recrawling content sources .",
    "first , we analyze this problem theoretically and find an optimal solution .",
    "then , we describe an algorithm , which is based on this solution .",
    "assume that we are given a set of content sources @xmath21 .",
    "note that the rate of new content appearance may differ from source to source .",
    "for example , usually there are much more news about politics than about art , and therefore , different categories of a news site generate new content with different rates .",
    "let @xmath22 be _ the rate of new links appearance _ on the source @xmath23 , i.e. , the average number of links to new pages , which appear in one second .",
    "let us consider an algorithm , which recrawls each source @xmath23 every @xmath24 seconds , discovers links to new pages , and also crawls all new pages found .",
    "we want to find a schedule for recrawling content sources , which maximizes the overall quality @xmath25 ( see equation ( [ q ] ) ) , i.e. , our aim is to find optimal values of @xmath24 .",
    "suppose that our infrastructure allows us to crawl @xmath26 pages per second ( @xmath26 can be non - integer ) .",
    "due to these resource constraints , we have the following restriction : @xmath27 on average , the number of new links linked from a source @xmath23 is equal to @xmath28 , therefore every @xmath24 seconds we have to crawl @xmath29 pages ( the source itself and all new pages found ) .",
    "obviously , the optimal solution requires to spend all the resources : @xmath30 and we want to maximize the overall quality ( see equation  ( [ q ] ) ) , i.e. , @xmath31 } p_j(\\vartriangle t_j ) \\rightarrow \\max.\\ ] ] note that this expression is exactly the average profit per second .",
    "content sources may not be equal in quality , i.e. , some content sources may provide users with better content than others .",
    "we now assume that , on average , the pages from one content source exhibit the same behavior of the profit decay function and hence substitute @xmath32 by the approximation @xmath33 discussed in section  [ formalization ] .",
    "we treat the profit @xmath17 and the rate of profit decay @xmath16 as the parameters of each content source @xmath23 .",
    "thus , we obtain : @xmath34 @xmath35 here @xmath36 and @xmath37 . without loss of generality",
    ", we can assume that @xmath38 .",
    "we now want to maximize @xmath39 subject to ( [ restriction ] ) .",
    "we use the method of lagrange multipliers : @xmath40 where @xmath41 is a lagrange multiplier .",
    "note that @xmath42 , so we get : @xmath43 the function @xmath44 increases monotonically for @xmath45 with @xmath46 and @xmath47 .",
    "if we are given @xmath48 , then we can find the unique value @xmath49 as shown in figure  [ fig : omega ] .",
    "one can easily compute @xmath50 using a binary search algorithm .",
    "note that bigger values of @xmath51 lead to bigger values of @xmath52 .",
    "that is why @xmath53 is a monotonic function of @xmath51 and we can , here also , apply a binary search algorithm ( see algorithm [ algo_disjdecomp ] ) to achieve the condition @xmath54 .",
    "@xmath55    let @xmath56 and @xmath57 be , respectively , the lower and upper bounds for @xmath51 .",
    "at the first step , we can put @xmath58 and @xmath59 .",
    "indeed , @xmath60 is the obvious upper bound for @xmath51 , since in this case we do not crawl any content source . at each step of the algorithm",
    ", we consider @xmath61 .",
    "for this value of @xmath51 , we recompute intervals @xmath62 .",
    "note that if we get @xmath63 for some @xmath64 , then @xmath65 and we never recrawl this content source .",
    "after that , if @xmath66 , then we can put @xmath67 , since it is an upper bound .",
    "if not , we put @xmath68 .",
    "we proceed in this way until we reach the required precision @xmath69 .",
    "the value of @xmath51 may be interpreted as the threshold we apply to content sources utility .",
    "actually , we can find the minimal crawl rate required for the optimal crawling policy not to completely refuse to crawl content sources with the least utility .",
    "we completely solved the optimization problem for the metric suggested in section [ formalization ] , the solution of ( [ solution2 ] ) is theoretically optimal ( we use the name _ echo - based crawler _ for the obtained algorithm , where echo is an abbreviation for ephemeral content holistic ordering ) . however , some further efforts are required in order to make this algorithm practically useful .",
    "there are parameters , which we need to estimate for each source : the profit @xmath17 , the rate of profit decay  @xmath16 , and the rate of new links appearance  @xmath22 . in the following section",
    ", we describe a practical crawling algorithm , which is based on our theoretical results .",
    "let us describe a concrete algorithm based on the results of section [ theory ] .",
    "first , we use the results from section [ howtofind ] to obtain an input set of content sources .",
    "then , in order to apply algorithm [ algo_disjdecomp ] for finding an optimal recrawl schedule for these content sources , we need to know for each source its profit @xmath17 , the rate of profit decay  @xmath16 , and the rate of new links appearance  @xmath22 .",
    "we propose to estimate all these values dynamically using the crawling history and search engine logs .",
    "since these parameters are constantly changing , we need to periodically re - estimate time intervals @xmath24 ( see algorithm [ algo_disjdecomp ] ) , i.e. , to update the crawling schedule . obviously , the more often we re - estimate @xmath24 , the better results we will obtain , and the choice of this period depends on the computational resources available .    thus , we first discuss how to estimate these sources characteristics ( sections [ p&mu ] and [ lambda ] ) and then how to deal with deviations of content sources behavior from our idealistic assumptions to make a practical scheduling algorithm ( section [ scheduling ] ) .",
    "for this part , we need search engine logs to analyze the history of clicks on new pages . we want to approximate the average cumulative number of clicks depending on the page s age by an exponential function .",
    "this approximation for two chosen content sources is shown on figure  [ fig : avg_clicks ] .",
    "let us consider a cumulative histogram of all clicks for all new pages linked from a content source , with the histogram bin size equals to @xmath70 minutes .",
    "let @xmath71 be the number of times all @xmath26 new pages linked from this content source were clicked during the first @xmath72 minutes after they appeared .",
    "so , @xmath73 is the average number of times a new page was clicked during the first @xmath72 minutes .",
    "we can now use the least squares method , i.e. , we need to find : @xmath74    in other words , we want to find the values of @xmath75 and @xmath14 , that minimize the sum of the squares of the differences between the average cumulative number of clicks and its approximation @xmath76 .",
    "it is hard to find an analytical solution of ( [ leastsquares ] ) , but we can use the gradient descent method to solve it :    [ gradientdescent ] @xmath77 ;    from the production point of view , it is very important to decide , how often to push data from search engine logs to re - estimate the values of @xmath16 and @xmath17 as it is quite an expensive operation .",
    "we denote this _ logs push period _ by @xmath78 . in section",
    "[ exp ] , we analyze how the choice of @xmath78 affects the performance of the algorithm .",
    "the rate of new links appearance @xmath79 may change during the day or during the week .",
    "we thus dynamically estimate this rate for each content source . in order to do this ,",
    "we use historical data : we consider the number of new links found at each content source during the last @xmath6 crawls .",
    "we analyze how different values for @xmath6 affect the performance of the algorithm in section  [ exp ] .",
    "finally , in order to apply our algorithm , we should solve the following problem : in reality the number of new links that appear on a content source during a fixed time period is random and we can not guarantee that we find exactly @xmath28 new links after each crawl .",
    "we can find more links than expected after some recrawl and if we crawl all of them , then we will deviate from the schedule .",
    "therefore , we can not both stick to the schedule for the content sources and crawl all new pages .",
    "so we propose the two following variants to deal with these new pages , that we can not crawl without deviating from the schedule .",
    "* echo - newpages .",
    "* in order to avoid missing clicks , we always crawl newly discovered pages right after finding them .",
    "if there are no any new pages in the crawl frontier , we try to come back to the schedule .",
    "we crawl the content source , which is most behind the schedule , i.e. , with the highest value of @xmath80 , where @xmath81 is time passed after the last crawl of the @xmath0-th content source .    *",
    "* echo - schedule.**we always crawl content sources with intervals @xmath24 and when we have some resources to crawl new pages , we crawl them ( most recently discovered first ) .",
    "we compare these two variants experimentally in the next section .",
    "we finish this section by presenting a possible production architecture for our algorithm ( see figure  [ fig : architecture ] ) to emphasize that it is highly practical to implement it in a production system .",
    "initially , a set of content sources with good recall of new pages linked from these sources is created using the procedure described in section  [ contentsources ] .",
    "this procedure must be run periodically to refresh this set and include new content sources .",
    "then , _ scheduler _ finds the optimal crawling schedule for content sources , while _ fetcher _ crawls these sources according to this schedule .",
    "_ scheduler _ dynamically estimates the rate of new links appearance for content sources and also estimates the profit decay function using the number of clicks from the search engine logs .",
    "given a fixed ranking method , the number of clicks measures the direct effect of the crawling algorithm on the search engine s performance .",
    "_ scheduler _ dynamically uses this feedback in order to improve the crawling policy .",
    "in this section , we compare our algorithm with some other crawling algorithms on real - world data .      since it is impractical and unnecessary to conduct research experiments at a production scale , we selected some sites that provide a representative sample of the web , on which we performed our experiments .",
    "we selected the top 100 most visited russian news sites and the top 50 most visited russian blogs using publicly available data from trusted sources .",
    "we consider these web - sites to be a representative sample of the web for our task as they produce 5 - 6% out of the @xmath82k new pages ( visited by at least one user ) that appear in this country daily ( we estimated this second value using toolbar logs ) . for each such site",
    ", we applied the procedure described in section [ contentsources ] and obtained about 3k content sources .",
    "then , we crawled each of these content sources every 10 minutes for a period of 3 weeks ( which is frequent enough to be able to collect all new content appearing on them before it disappears ) .",
    "the discovery time of new pages we observed is thus at most delayed by these 10 minutes .",
    "we considered all pages found at the first crawl of each source ( each content source was crawled @xmath83 times ) to be old and discovered @xmath84k new pages during these @xmath85 weeks . keeping track of",
    "when links to new pages were added and deleted from the content sources , we created a dynamic graph that we use in the following experiments .",
    "this graph contains @xmath86 m unique links .    additionally , we used search engine logs of a major search engine to collect user clicks for each of the newly discovered pages in our dataset for the same period of @xmath85 weeks plus @xmath12 week for the most recent pages to become obsolete .",
    "we observed that @xmath87% of the pages were clicked at least once during this 4 weeks period .",
    "we compare the algorithm suggested in section [ algo ] with several other algorithms .",
    "there are no state - of - the - art algorithms for the specific task we discuss in this paper , but one can think of several natural ones :    * * breadth - first search ( bfs ) * we crawl content sources sequentially in some fixed random order . after crawling each source ,",
    "we crawl all new pages linked from this source , which have not been crawled yet .",
    "we also compare our algorithm with the following simplifications to understand the importance of 1 ) the holistic crawl ordering and 2 ) the usage of clicks from search engine logs .    *",
    "* fixed - quota * this algorithm is similar to _",
    "echo- + schedule _ , but we use a fixed quota of @xmath88 for recrawling content sources and for crawling new pages that have not been crawled before . * * frequency * this algorithm is also similar to _ echo - schedule _ , but we do not use clicks from search engine logs , i.e. , all content sources have the same quality and content sources are ordered only by their frequency of new pages appearance .",
    "we also propose a simplification of our algorithm , based on section [ algo ] , which could be much easier to implement in a production system .    * * echo - greedy * we crawl the content source with the highest expected profit , i.e. , with the highest value of @xmath89 , where @xmath90 is the time passed since the last crawl of the content source , @xmath22 is its rate of new links appearance , and @xmath17 is the average profit of new pages linked from the content source . then , we crawl all new pages linked from this source , which have not been crawled yet , and repeat this process .        in this section ,",
    "we experimentally investigate the influence of parameters on our algorithm s performance and compare the algorithm with the approaches from section [ benchmarks ] on real - world data .",
    "we simulated , for each algorithm , the crawl of the dynamic graph described in section [ data ] , using the content sources as seed pages .",
    "each algorithm can thus , at each step , decide to either crawl a newly discovered page or to recrawl a content source in order to find new pages .    in the following experiments analyzing parameters influence , we used the crawl rate per second @xmath91 .",
    "this crawl rate is enough to crawl a significant fraction of the new pages as shown on figure  [ fig : lambda ] , but is not too high to let bfs algorithm crawl all new pages ( which is highly unrealistic in a production context ) .",
    "we then also use two other crawl rates @xmath92 and @xmath93 per second to investigate the influence of this value .",
    "we apply algorithm  [ algo_disjdecomp ] to re - estimate @xmath24 values every 30 minutes , which is frequent enough so that smaller intervals have almost no influence on its performance , and which is also realistic in a production context .",
    "we also set the bin size @xmath70 used in algorithm 2 to 20 minutes , which is good enough to have robust estimations of @xmath17 and @xmath16 , as , typically , the profit decay function @xmath94 does not change significantly in such a small time period .",
    "we do not study in details , here , the influence of these two parameters due to space constraints as , according to the experiments we performed , it is negligible , for values below these realistic choices , in comparison with other parameters .    besides that",
    ", we need default values for profits @xmath17 as we start crawling without knowing anything about the quality of each content source .",
    "please , note that we need to use _",
    "pessimistic _ default values because we want to avoid crawling low quality sources too frequently , while we do not have enough feedback to have precise estimations .",
    "we can not use @xmath95 as according to algorithm  [ algo_disjdecomp ] , we do not crawl content sources with zero profit , so , we used some small non - zero value @xmath96 .",
    "we compared the two variants of echo - based crawler from section  [ scheduling ] with different values for : 1 ) the crawl history size used to estimate the rate of new links appearance @xmath79 discussed in section [ lambda ] ( from 3 to 10 crawls ) , and 2 ) the logs push period @xmath78 , which was described in section [ p&mu ] ( we considered 1h , 12h , 24h , and 1 week ) . interestingly , for both variants we noticed no difference , and we therefore conclude that these parameters do not affect the final quality of the algorithm in our setup . on the other hand ,",
    "the logs push period has a really big influence during the warm - up period and the smaller the logs push period , the better the results ( see figure  [ fig : profit_7 ] ) .",
    "there is nothing interesting to observe for the crawl history size .",
    "let us also note that the optimal schedule of echo - based algorithms almost does not recrawl 70% of content sources , which means that it does not spend much resources on low quality content sources .",
    ".average dynamic profit for a 1-week window . [ cols=\"<,^,^,^,^\",options=\"header \" , ]     then , we took the crawl history of size 7 and the logs push period of 1 hour ( randomly , following the discussion in section [ influence ] ) , and compared echo - based crawlers with other algorithms on three different crawl rates . in order to compare our algorithms during the last week of our observations ( after the warm - up period ) we measured the dynamic profit every two minutes using a time window of one week ( enough to compensate daily trends ) .",
    "table  [ table1 ] shows average values and their standard deviations .",
    "note that we also include the upper bound of algorithms performance that we computed using bfs algorithm with an unbounded amount of resources , which allows to crawl all new pages right after they appear .",
    "this upper bound therefore does not depend on the crawl rate and equals @xmath97 of profit per second .",
    "echo - newpages shows the best results , which are really close to the upper bound , although the crawl rate used is much smaller than the rate of new links appearance .",
    "this means that our algorithm effectively spends its resources and crawls highest quality pages first .",
    "note that the smallest crawl rate that allows bfs to reach 99% of the upper bound is 1 per second ( this value is measured , but not present in the table ) , as bfs wastes lots of resources recrawling content sources to find new pages , while echo - newpage and echo - schedule reach this bound with crawl rate 0.2 per second ( see the last column of the table ) .    note that the profit of echo - greedy is also high .",
    "this fact can be a good motivation for using it in a production system , where ease of implementation is a strong requirement ( as it is much easier to implement ) .",
    "first , it only requires a priority queue of content sources rather than a recrawl schedule updated using the binary search method from algorithm  [ algo_disjdecomp ] .",
    "second , it does not use @xmath16 , so @xmath17 is thus simply the average number of clicks on pages linked from the @xmath0-th content source , and can therefore be computed easier than by using the gradient descent method from algorithm  2 .",
    "let us show a representative example ( at @xmath91 ) demonstrating the advantage of echo - based algorithms over the baselines ( see figure  [ fig : dynamic ] ) .",
    "one can observe that echo - based algorithms perform the best most of the time .",
    "it is interesting to note though that during the night bfs shows better results .",
    "it happens as bfs is `` catching up '' by crawling pages , which were crawled by other algorithms earlier .",
    "this follows how the dynamic profit is defined : we take into account the profit of the pages , which were crawled during the last 5 hours .",
    "we also see that the algorithm with fixed quota for crawl and recrawl perform well during the weekend because less new pages appear during this period and the crawl rate we use is thus enough to crawl practically all good content without additional optimizations .",
    "most papers on crawling are devoted to discovering new pages or refreshing already discovered pages . both of these directions are , to some extent , related to the problem we deal with , though can not serve as solutions to it .",
    "[ [ refresh - policies ] ] refresh policies + + + + + + + + + + + + + + + +    the purpose of refresh policies is to recrawl known pages that have changed in order to keep a search engine s index fresh . usually , such policies are based on some model , which predicts changes on web pages . in pioneering works",
    "@xcite , the analysis of pages changes was made in the assumption of a time - homogeneous poisson process , i.e. , it was assumed that the pages change rate does not depend on time .",
    "however , in @xcite , it was noted that there are daily and weekly trends in the pages change rate .",
    "then , a history - based estimator , which takes such trends into account , was proposed in @xcite . a more sophisticated approach based on machine learning",
    "is used in @xcite , where the page s content , the degree of observed changes and other features are taken into account .    for our specific task ,",
    "refresh policies can be used to find links to ephemeral new pages , that appeared on already known pages ( content sources ) .",
    "so , pages changes are relevant for us only if new links to such new pages can be found .",
    "interestingly , this simplifies the estimation of pages change rate as one can easily understand , given two successive snapshots of a page , that two new links appeared , while it is much harder to know if the page s text changed once or twice .",
    "this fact allows us to use a simple estimator for the rate of new links appearance , which reflects timely trends .",
    "of course , more sophisticated methods ( e.g. , using machine learning @xcite ) , can be applied here , but it was out of focus of the current paper .",
    "moreover , our method actually monitors content sources updates to avoid missing new links . in this way , our work is more related to the problem of creating an efficient rss feeds reader , which needs to monitor rss feeds updates to avoid missing new postings @xcite .",
    "the rss reader described in these papers learns the general posting pattern of each rss feed to create an efficient scheduling algorithm that optimizes the retrieval of rss feeds in order to provide timely content to users .",
    "this rss reader uses a short description from the rss feed , when presenting an article to users , and there is thus no need for it to crawl these articles .",
    "it is not exactly our case as we need to crawl and index newly discovered pages to allow users to access them via the search engine .",
    "thus , although rss monitoring policies are somehow similar to the problem of finding ephemeral new pages on content sources , one can not use such policies out of box for our task .",
    "the main reason is that we need to spend significant amount of resources for crawling new pages and , if we want to do this in an efficient way , then we need to change the recrawl schedule to take this fact into account",
    ". however , rss feeds themselves can be used in our approach as content sources .",
    "[ [ discovery - policies ] ] discovery policies + + + + + + + + + + + + + + + + + +    the main idea behind discovery policies is to focus breadth - first search on high quality content , i.e. , to prioritize discovered , but not yet crawled pages ( the crawl frontier ) according to some quality measure .",
    "some approaches are based on the link structure of the web graph , e.g. , in @xcite pages with the largest number of incoming links are crawled first , while pages with largest pagerank are prioritized in @xcite . in @xcite ,",
    "such approaches were compared in their impact on web search effectiveness . however , as pandey and olston discussed in @xcite , the correlation between link - based importance measures and user interest is weak , and hence they proposed to use search engine logs of user queries to drive the crawler towards pages with higher potential to be interesting for users . in turn , we follow recent trends and do not rely on the link structure of the web , but use clicks from search engine logs .",
    "our crawler discovers and crawls new pages , but there is a principal difference between our approach and previous ones .",
    "previous approaches are based on the assumption that the web is relatively deep and therefore , starting from some seed pages , a crawler needs to go deeper , in direction of high - quality pages if possible , to find new pages .",
    "we instead argue that one can find most ephemeral new pages that are appearing on the web at a relatively small set of content sources , but that a crawler needs to frequently recrawl these sources to avoid missing short living links to new pages .",
    "this observation , on one hand , simplifies the problem but , one the other hand , introduces new challenges to find the right balance between crawling new pages and recrawling content sources .",
    "[ [ holistic - crawl - ordering ] ] holistic crawl ordering + + + + + + + + + + + + + + + + + + + + + + +    usually , papers about crawling focus either on discovery of new pages or on refreshing already known pages , but the important question of how to divide limited resources between refreshing and discovery policies is usually underestimated .",
    "some authors proposed to give a fixed quota to each policy @xcite .",
    "however , as it follows , e.g. , from our analysis ( see section [ theory ] ) , such fixed quotas can be far from optimal .",
    "in contrast , our optimization framework simultaneously deals with refreshing and discovery and can thus find an optimal way to share resources .",
    "moreover , the problem of making a holistic crawl ordering , i.e. , to unify different policies into a unified strategy , was proposed by oslton and najork as a future direction in their extensive survey on web crawling @xcite and we tried to make a step forward in this direction .",
    "to the best of our knowledge , the problem of timely and holistic crawling of ephemeral new content is novel . in this paper , we introduce the notion of ephemeral new pages , i.e. , pages that exhibit the user interest pattern shown on figure  [ fig : yabar ] , and emphasize the importance of this problem by showing that a significant fraction of the new pages that are appearing on the web are ephemeral .",
    "we formalized this problem by proposing to optimize a new quality metric , which measures the ability of an algorithm to solve this specific problem .",
    "we showed that most of the ephemeral new content can be found at a relatively small set of content sources and suggested an algorithm for finding such a set .",
    "then , we proposed a practical algorithm , which periodically recrawls content sources and crawls newly created pages linked from them as a solution of this problem .",
    "this algorithm estimates the quality of content sources using user feedback .    finally , we compared this algorithm with other crawling strategies on real - world data and demonstrated that the suggested algorithm shows the best results according to our metric .",
    "our theoretical and experimental analysis aims at giving a better insight into the current challenges in crawling the web .    in this paper",
    ", we predict the expected profit of a new page using two features : the time when this page was discovered by a crawler , and the content source where a link to this page was found .",
    "the natural next step , which we leave for future work , is to predict this profit using more features , e.g. , give a higher priority to pages having an anchor text related to the current trends in user queries like in @xcite , or to the pages with more incoming links .",
    "also , url tokens and its hyperlink context ( anchor text , surrounding text , etc . ) may be useful for such prediction .",
    "this will help to prioritize new pages with seemingly higher quality found on the same content source at the same time .",
    "brewington , b.e . ,",
    "cybenko , g. : how dynamic is the web ?",
    "computer networks , vol .",
    "33(16 ) , 257276 , 2000 .",
    "brewington , b.e . ,",
    "cybenko , g. : keeping up with the changing web .",
    "computer , vol .",
    "33(5 ) , 5258 , 2000 . cho , j. , garcia - molina , h. : effective page refresh policies for web crawlers .",
    "acm transactions on database systems , vol .",
    "28(4 ) , 2003 .",
    "kumar , r. , lang , k. , marlow , c. , tomkins , a. : efficient discovery of authoritative resources . in data engineering , 2008 .",
    "liu , m. , cai , r. , zhang , m. , zhang , l. : user browsing behavior - driven web crawling . in proc .",
    "cikm conference , 2011 ."
  ],
  "abstract_text": [
    "<S> nowadays , more and more people use the web as their primary source of up - to - date information . in this context , fast crawling and indexing of newly created web pages has become crucial for search engines , especially because user traffic to a significant fraction of these new pages ( like news , blog and forum posts ) grows really quickly right after they appear , but lasts only for several days .    in this paper , we study the problem of timely finding and crawling of such _ ephemeral _ new pages ( in terms of user interest ) . </S>",
    "<S> traditional crawling policies do not give any particular priority to such pages and may thus crawl them not quickly enough , and even crawl already obsolete content . </S>",
    "<S> we thus propose a new metric , well thought out for this task , which takes into account the decrease of user interest for ephemeral pages over time .    </S>",
    "<S> we show that most ephemeral new pages can be found at a relatively small set of content sources and present a procedure for finding such a set . </S>",
    "<S> our idea is to periodically recrawl content sources and crawl newly created pages linked from them , focusing on high - quality ( in terms of user interest ) content . </S>",
    "<S> one of the main difficulties here is to divide resources between these two activities in an efficient way . </S>",
    "<S> we find the adaptive balance between crawls and recrawls by maximizing the proposed metric . </S>",
    "<S> further , we incorporate search engine click logs to give our crawler an insight about the current user demands . efficiency of our approach is finally demonstrated experimentally on real - world data . </S>"
  ]
}