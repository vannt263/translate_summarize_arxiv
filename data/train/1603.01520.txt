{
  "article_text": [
    "code optimization is the process that tries to improve the code by making it consume less resources such us cpu cycles , memory , or communication in distributed systems .",
    "the word optimization comes from the root `` optimal '' ( which comes from the latin word _ optimus _ ) , meaning that it can not be better .",
    "but it is very rare that this process can produce really optimal code .",
    "the optimized code can in most cases be optimal for a given use in a determined hardware system .",
    "one can often reduce the execution time by making it consume more memory , but in systems where the memory space is scare it might be beneficial to code a slower algorithm which reduces memory usage . in summary , in most cases",
    "there is `` no size that fits all '' code that executes optimal in all cases .",
    "software developers that write code for generic systems must adjust their code to perform reasonably well in most common situations .",
    "the code optimization process can also take big amount of time , that can be thought as a cost ; it is possible that beyond a certain level of optimization it is not cost effective to invest more time in improving the execution performance .",
    "software developer time is not the only price to pay for code optimization as often the process will lead to source code that is obfuscated , more difficult to maintain or modify and reduces the portability across different hardware systems .",
    "another drawback is that it will also reduce the opportunities to reuse certain parts of the code .    to deal with",
    "all this factors the ideal situation would be when the source code of the application resembles as close as possible the mathematical formulation of the algorithm , leaving the optimization of the code to the compiler .",
    "unfortunately current compilers are not able to make the same level of aggressive transformations on a near to mathematics code because it is not aware of the intention of the computation ( i.e. what is the final result that we want to achieved ) .",
    "for this reason the optimizations are limited to a small set of optimizations that guarantee that the computation is correct .",
    "introducing mathematical information to the compiler can allow it to perform optimizations at the algorithmic level , similarly to what a software developer might do , to better exploit the characteristics of the underlying hardware .",
    "after this step the usual set of current optimizations can be applied .    in this paper",
    "we will show how the c resembling the formulation of equivalent polynomial functions affects the execution time and we will discuss how the mathematical information can be introduced through semantic annotations into programming models so that it can be exploited by compilers , allowing them to perform this set of algorithmic optimizations .",
    "in mathematics , a polynomial is an expression consisting of variables and coefficients that involves only the operations of addition , subtraction , multiplication and non - negative integer exponents .",
    "polynomials appear in a wide range of problem complexity of different areas . in mathematics",
    "they are used in calculus and numerical analysis to approximate to other functions , or in advanced mathematics they are used to construct polynomial rings and algebraic varieties , central concepts in algebra .",
    "they are also often used in physics and chemistry to , for example , describe the trajectory of projectiles , express equations such as the ideal gas law or , polynomial integrals ( the sums of many polynomials ) , can be used to express energy , inertia and voltage difference , to name a few applications .",
    "other natural sciences also use polynomial , like the construction of astronomical and meteorological models . as polynomials are very useful to express curves , thus engineers use them to design roads , bridges , railways lines , and roller coasters",
    ". combinations of polynomial functions can be used in more sophisticated analysis to retrieve more data , for this , they are applied in the field of economics to do cost analysis .",
    "mathematical equations can often be expressed under different formulations that are functionally equivalent , but that if taken directly into code will be executed in a different way , resulting in a different resource usage of the available hardware . to illustrate this we will present a polynomial equations and transform its mathematical representation in different steps .",
    "the resulting different , but equivalent , functions will be _ directly _ coded in c language and then evaluated against each other .",
    "let the quartic polynomial function ( degree four ) be given by : + @xmath0 + in figure  [ pic : poly1 ] the execution structure , _",
    "i.e. _ data / work flow , of this polynomial function is graphically represented : @xmath1 is represented by a sequence of multiplications with @xmath2 , starting from @xmath3 , and the result is multiplied with @xmath4 . then , the results for every @xmath5 are added .    ]    listing  [ lst : poly1 ] shows the source code implementation in c that matches the execution model of the equation @xmath6 .    .... # define a0 ... ...    int polycalc(int x ) {    int res ;      res = a4*x*x*x*x + a3*x*x*x + a2*x*x + a1*x + a0 ;      return res ; } ....    however , this execution model is rather inefficient , as it will be shown in the benchmark section , as the outcomes of all terms @xmath1 are calculated separately .",
    "instead , we may calculate the corresponding values @xmath7 incrementally : + @xmath8 + and then define an improved version of the polynomial function : + @xmath9 + figure  [ pic : poly2 ] shows this second execution model where the @xmath1 terms are being reused .    ]    and listing  [ lst : poly2 ] shows the source code that represents this execution model directly , with the extra variable ` _ x ` that stores the incrementally exponentiation .",
    ".... # define a0 ... ...    int polycalc(int x ) {    int res , _ x ;      res = a0 ;    res + = a1*x ;    _ x = x*x ;    res + = a2*_x ;    _ x * = x ;    res + = a3*_x ;    _ x * = x ;    res + = a4*_x ;      return res ; } ....    but it can still done better .",
    "the function can also be rewritten to further reduce the number of multiplications .",
    "this is the function @xmath10 that also specifies an execution model , given in figure  [ pic : poly3 ] .",
    "this model is apparently more efficient than the previous ones , in the sense that fewer operations are required , although later we will test this empirically .",
    "+ @xmath11 + and its execution model is expressed graphically in figure  [ pic : poly3 ] .    ]",
    "the direct implementation of @xmath10 in c code is shown in listing  [ lst : poly3 ] .",
    ".... # define a0 ... ...    int polycalc(int x ) {    int res ;      res = ( ( ( a4*x + a3)*x + a2)*x + a1)*x + a0 ;      return res ; } ....    as the equivalence of @xmath6 , @xmath12 , and @xmath10 can be proven and the correspondence between the function definitions , the execution model and the derived c code is direct , this also means that the equivalence of the corresponding execution model and c code is guaranteed ( as long as we do not care for the numeric differences that can appear due to the lost precision and rounding of floating point variables or cater for possible overflows in integer operations when executing on cpus ) .",
    "the three mathematical - equivalent formulations have a different number of operations to be performed when directly implemented on c code .",
    "the number of operations has a direct impact on the performance on configurable hardware , such us fpgas  @xcite but the situation might be different on cpus due to their complex behavior and diversity of architectures .",
    "the impact of a particular code is very difficult to predict in the case of superscalar architectures with out - of - order execution . in this chapter",
    "we will analyze the theoretical number of computations to be performed for each version and in next chapters we will measure the performance empirically .",
    "the number of operations can be directly counted from the mathematical representations or the c code previously shown for each version .",
    "the summary of theoretical operations is summarized in table  [ tbl : compev-05 ] . as it can be observed",
    "the number of additions is constant but the number of multiplications has been significantly reduced with each new representation of the polynomial function .",
    ".computational analysis [ cols=\"<,^,^\",options=\"header \" , ]     in this case the @xmath13 version of the code is the fastest for both compilers , being llvm almost a  3% faster than gcc .",
    "the binary generated by gcc was faster the further the number of arithmetic operations were reduced in the code .",
    "this is not the same case for llvm as @xmath14 performs better than @xmath15 .",
    "even if the c implementation of @xmath14 has more operations than @xmath15 , llvm is able to optimize better @xmath14 producing a binary with slightly less arithmetic instrucions , thus performing better . as with the degree 4 polynomial function , for llvm , @xmath13 performs better than the optimized @xmath14 . in haswell",
    "based cpu , the multiple - independent shorter branched @xmath14 binary does not perform as good as the single fully - dependnent branch code of @xmath13 .",
    "the code optimizations needed to move from @xmath6 to @xmath12 and finally to @xmath10 are not possible in current compilers because current programming models do not convey enough information to enable such transformations without the risk of violating correctness of behaviour .",
    "we can say that compilers are missing information about the intention ( in terms of `` semantics '' ) of the programmer and the structure and principle behaviour of the program .    for these reasons , projects such as",
    "polca are currently researching in providing programming models that convey sufficient information to meet the following goals  @xcite :    * provide structural ( dependencies and operational behaviour ) and mathematical information , * enable transformation of the source code , * allow the toolchain to assess the `` appropriateness '' of the algorithm and transformations for specific hardware platforms * and maintain programmability and correctness .    to enable transformations exposed in this work we need to provide enough information to the compiler so that it `` understand '' the mathematical properties of the code running , by using semantic annotations , and strongly binding them to the c code so that the later can be correctly manipulated , reformulating the original implemented algorithm and code for an equivalent but with better performance .",
    "a proposal of the annotations applied to the code of listing  [ lst : poly1 ] is shown in listing  [ lst : acode ] .",
    "first the ` ring_prop ` ( ring properties ) pragma annotation declares that , for the function ` polycalc ` that we assume a number of algebraic laws to hold : associativity of @xmath16 and @xmath17 , that @xmath18 and @xmath19 are neutral elements for @xmath16 and @xmath17 , that @xmath17 distributes over @xmath16 for ` int ` data types .",
    "a mathematical ring is one of the fundamental algebraic structures used in abstract algebra .",
    "it consists of a set equipped with two binary operations that generalize the arithmetic operations of addition and multiplication . through this generalization ,",
    "theorems from arithmetic are extended to non - numerical objects such as polynomials , series , matrices and functions .",
    "a ring is an abelian group with a second binary operation that is associative , is distributive over the abelian group operation , and has an identity element . by extension from the integers ,",
    "the abelian group operation is called addition and the second binary operation is called multiplication  @xcite@xcite .    in summary , a _ ring _ is a set @xmath20 equipped with binary operations @xmath16 and @xmath21 satisfying the following three sets of axioms , called the ring axioms  @xcite  @xcite  @xcite :    1 .",
    "@xmath20 is an abelian group under addition , meaning that : * @xmath22 for all @xmath23 in @xmath20 ( @xmath16 is associative ) . * @xmath24 for all @xmath25 in @xmath20 ( @xmath16 is commutative ) .",
    "* there is an element @xmath18 in @xmath20 such that @xmath26 for all @xmath27 in @xmath20 ( @xmath18 is the additive identity ) .",
    "* for each a in @xmath20 there exists @xmath28 in @xmath20 such that @xmath29 ( @xmath28 is the additive inverse of @xmath27 ) .",
    "2 .   @xmath20 is a monoid under multiplication , meaning that : * @xmath30 for all @xmath23 in @xmath20 ( @xmath21 is associative ) .",
    "* there is an element @xmath19 in @xmath20 such that @xmath31 for all @xmath27 in @xmath20 ( @xmath19 is the multiplicative identity ) .",
    "multiplication is distributive with respect to addition : * @xmath32 for all @xmath23 in @xmath20 ( left distributivity ) . * @xmath33 for all @xmath23 in @xmath20 ( right distributivity ) .    by stating that these mathematical properties apply we enable the compiler to be able to manipulate the code instructions accordingly without taking into consideration possible side effects that could make the code not valid under certain conditions ( _ e.g. _ integer overflow ) .",
    "then the second pragma annotation , ` math_exp ` ( mathematical expression ) , states the mathematical computation that is performed in the ` polycalc ` function to which it applies . in it the terms that appear share the name with the constants are variables used in the code , being the evaluation of the expression the value of the function itself , _",
    "i.e. _ its return value .    .... # define a0 ... ...    # pragma ring_prop ( + , 0 , - , * , 1 ) int # pragma math_exp ( a0 + a1*x + a2*x^2 + a3*x^3 + a4*x^4 ) int polycalc(int x ) {    int res ;      res = a4*x*x*x*x + a3*x*x*x + a2*x*x + a1*x + a0 ;      return res ; } ....    by capturing this information , the mathematical expression and the operators properties , and making the compiler aware of it , the compiler may decide to transform the body of the function , modifying the original code resembling the expression @xmath6 into code that resembles @xmath12 or @xmath10 .",
    "in this paper we have shown , by using polynomial equations , that mathematical expressions may have different but equivalent formulations and that we can write code that resembles each of these formulations .",
    "the different versions of the code will perform different depending on the compiler used to generate the binary code and the hardware that will execute it .",
    "we have also discussed that it is possible to manually search for a good combination of specific and optimized code , for an specific algorithm , and the hardware where it is going to run .",
    "but if later the hardware is changed the performance of the code might be worse that the original not - optimize code due to specific hardware optimizations for the old platform that now ruin performance .",
    "for example it has been shown that the @xmath15 code looks more optimal ( and complex ) from a theoretical point of view than the original @xmath14 , and it behaves better when compiled with gcc , but the performance decreases if compiled with llvm .    to address these issues",
    "we have proposed the usage of semantic annotations that introduce mathematical information with the aim to allow the compiler to produce optimize code beyond current capabilities from a generic clean code .",
    "this will allow to write programs with less bugs , easier to maintain and easier to extend , while at the same time increasing portability .",
    "these annotations have to be further tested and extended to be able to address a wider set of mathematical expressions and enable their usability in future compilers .    regarding the polynomial functions , we will extend the analysis of the performance effects , in terms of time to completion and energy used , of using floating point data types and not only of integers .",
    "these effects should also be measured in other architecture families , such as arm .",
    "it would also be interesting to determine the effects of even larger polynomial degrees and introduce further manually optimize code with independent executable branches trying to exploit superscalar architectures ."
  ],
  "abstract_text": [
    "<S> in this paper we discuss how semantic annotations can be used to introduce mathematical algorithmic information of the underlying imperative code to enable compilers to produce code transformations that will enable better performance . by using this </S>",
    "<S> approaches not only good performance is achieved , but also better programmability , maintainability and portability across different hardware architectures . to exemplify this </S>",
    "<S> we will use polynomial equations of different degrees .    </S>",
    "<S> programming , polynomial , optimization , performance    programming models , polynomial functions , code optimization </S>"
  ]
}