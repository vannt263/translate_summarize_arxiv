{
  "article_text": [
    "techniques for inferencing statistical values on markov random fields ( mrfs ) are fundamental techniques used in various fields involving pattern recognition , machine learning , and so on .",
    "the computation of statistical values on mrfs is generally intractable because of exponentially increasing computational costs . _ mean - field methods",
    "_ , which originated in the field of statistical physics , are the major techniques for computing statistical values approximately .",
    "loopy belief propagation methods are the most widely - used techniques in the mean - field methods  , and various studies on them have been conducted .",
    "many mean - field methods on mrfs with discrete random variables have been developed thus far .",
    "however , as compared with the discrete cases , mean - field methods on mrfs with continuous random variables have not been developed well .",
    "one reason for this is thought to be that the application of loopy belief propagation methods to continuous mrfs is not straightforward and has not met with no success except in gaussian graphical models  .",
    "although it is possible to approximately construct a loopy belief propagation method on a continuous mrf by approximating continuous functions using histograms divided by bins , this is not practical .",
    "_ nonnegative boltzmann machines _ ( nnbms ) are recurrent probabilistic neural network models that can describe multi - modal nonnegative data  . for continuous nonnegative data ,",
    "an nnbm is expressed in terms of a maximum entropy distribution that matches the first and second order statistics of the data . because random variables in nnbms are bounded , nnbms are not standard multivariate gaussian distributions , but",
    "_ rectified gaussian distributions _ that appear in biological neural network models of the visual cortex , motor cortex , and head direction system  , positive matrix factorization  , nonnegative matrix factorization  , nonnegative independent component analysis  , and so on . a learning algorithm for nnbms",
    "was formulated by means of a variational bayes method  .",
    "because nnbms are generally intractable , we require an effective approximate approach in order to provide probabilistic inference and statistical learning algorithms for the nnbm .",
    "a mean - field method for nnbms was first proposed in reference .",
    "this method corresponds to a naive mean - field approximation , which is the most basic mean - field technique .",
    "a higher - order approximation than the naive mean - field approximation for nnbms was proposed by downs  @xcite and recently , his method was improved by the author  .",
    "these methods correspond to _ the thouless  anderson  palmer _",
    "( tap ) equation in statistical physics , and they are constructed using a perturbative approximative method referred to in statistical physics as the plefka expansion  @xcite .",
    "recently , a mean - field method referred to as _ the diagonal consistency method _ was proposed  @xcite .",
    "the diagonal consistency method is a powerful method that can increase the performance of mean - field methods by using a simple extension for them  @xcite .    in this paper",
    ", we propose an effective mean - field inference method for nnbms in which the tap equation , proposed in reference , is combined with the diagonal consistency method .",
    "the remainder of this paper is organized as follows . in section [ sec : nnbm ] the nnbm is introduced , and subsequently , the tap equation for the nnbm is formulated in section [ sec : gibbs&tap ] in accordance with reference .",
    "the proposed methods are described in section [ sec : propose ] . the main proposed method combined with the diagonal consistency method",
    "is formulated in section [ subsec : i - susp ] . in section [ sec",
    ": numerical ] , we present some numerical results of statistical machine learning on nnbms and verify the performance of the proposed method numerically .",
    "finally , in section [ sec : conclusion ] we present our conclusions .",
    "in probabilistic information processing , various applications involve continuous nonnegative data , e.g. , digital image filtering , automatic human face recognition , etc . in general , standard gaussian distributions , which are unimodal distributions , are not good approximations for such data .",
    "nonnegative boltzmann machines are probabilistic machine learning models that can describe multi - modal nonnegative data , and were developed for the specific purpose of modeling continuous nonnegative data  .",
    "let us consider an undirected graph @xmath0 , where @xmath1 is the set of vertices in the graph and @xmath2 is the set of undirected links .",
    "on the undirected graph , an nnbm is defined by the gibbs - boltzmann distribution , @xmath3 where @xmath4 is the energy function .",
    "the second summation in the energy function is taken over all the links in the graph .",
    "the expression @xmath5 is the normalized constant called the partition function .",
    "the notations @xmath6 denote the bias parameters and the notations @xmath7 denote the coupling parameters of the nnbm . all variables",
    "@xmath8 take nonnegative real values , and @xmath9 and @xmath10 are assumed .",
    "the distribution in equation ( [ eq : nnbm ] ) is called the rectified gaussian distribution , and it can represent a multi - modal distribution , unlike standard gaussian distributions . in the rectified gaussian distribution ,",
    "the symmetric matrix @xmath7 is _ a co - positive matrix_. the set of co - positive matrices is larger than the set of positive definite matrices that can be used in a standard gaussian distribution , and is employed in co - positive programming .",
    "to lead to the tap equation for the nnbm , which is an alternative form of the tap equation proposed in reference , and a subsequent proposed approximate method for the nnbm in this paper , let us introduce a variational free energy , which is called the gibbs free energy in statistical physics , for the nnbm .",
    "the variational free energy of the nnbm in equation ( [ eq : nnbm ] ) is expressed by @xmath11 where @xmath12 a brief derivation of this variational free energy is given in appendix [ app : derivation - vfe ] . the notations @xmath13 and @xmath14 are variables that originated in the lagrange multiplier .",
    "it should be noted that the independent variables @xmath15 and @xmath16 represent the expectation value and the variance of @xmath17 , respectively .",
    "in fact , the values of @xmath18 and @xmath19 , which minimize the variational free energy , coincide with expectations , @xmath20 , and variances , @xmath21 , on the nnbm , respectively , where @xmath22 .",
    "the minimum of the variational free energy is equivalent to @xmath23 .",
    "however , the minimization of the variational free energy is intractable , because it is not easy to evaluate the partition function @xmath24 .",
    "therefore , an approximative instead of the exact minimization . by using the plefka expansion  @xcite ,",
    "the variational free energy can be expanded as @xmath25 where @xmath26 where the function @xmath27 is the scaled complementary error function defined by @xmath28 equation ( [ eq : plefkaexpansion ] ) is the perturbative expansion of the variational free energy with respect to @xmath29 .",
    "the notations @xmath30 and @xmath31 are defined by @xmath32 let us neglect the higher - order terms and approximate the true variational free energy , @xmath33 , by the variational free energy in equation ( [ eq : gibbs_nnbm_2nd ] ) .",
    "the minimization of the approximate variational free energy in equation ( [ eq : gibbs_nnbm_2nd ] ) is straightforward .",
    "it can be done by computationally solving the nonlinear simultaneous equations , @xmath34 by using an iteration method , such as a successive iteration method .",
    "the notation @xmath35 is the set of nearest - neighbor vertices of vertex @xmath36 .",
    "equations ( [ eq : l_nnbm_2nd ] ) and ( [ eq : r_nnbm_2nd ] ) come from the minimum condition of equation ( [ eq : gibbs_nnbm_2nd ] ) with respect to @xmath37 and @xmath38 , respectively , and equations ( [ eq : mi_nnbm_2nd ] ) and ( [ eq : vi_nnbm_2nd ] ) come from the maximum conditions in equation ( [ eq : def - l&r_2nd ] )",
    ".    equations ( [ eq : mi_nnbm_2nd])([eq : r_nnbm_2nd ] ) are referred to as the tap equation for the nnbm .",
    "although the expression of this tap equation is different from the expression proposed in reference , they are essentially almost the same . since we neglect higher - order effects in the true variational free energy , the expectation values and the variances obtained by the tap equation are approximations , except in special cases .",
    "it is known that , on nnbms which are defined on complete graphs whose sizes are infinitely large and whose coupling parameters @xmath7 are distributed in accordance with a gaussian distribution whose variance is scaled by @xmath39 , the tap equation gives true expectations  .",
    "in this section , we propose an effective mean - field inference method for the nnbm in which the tap equation in equations ( [ eq : mi_nnbm_2nd])([eq : r_nnbm_2nd ] ) is combined with the the diagonal consistency method proposed in reference @xcite .",
    "the diagonal consistency method is a powerful method that can increase the performance of mean - field methods by using a simple extension for them .      before addressing the proposed method ,",
    "let us formulate _ the linear response relation _ for the nnbm .",
    "the linear response relation enables us to evaluate higher - order statistical values , such as covariances . here",
    ", we define the matrix @xmath40 as @xmath41 where @xmath42 is the values of @xmath37 that minimize the ( approximate ) variational free energy .",
    "since relations @xmath43 exactly hold when @xmath44 are the true expectation values , we can interpret the matrix @xmath45 as the covariant matrix on the nnbm .",
    "the quantity @xmath46 is referred to as _ the susceptibility _ in physics , and is interpreted as a response on vertex @xmath36 for a quite small change of the bias on vertex @xmath47 . in practice , we use @xmath44 obtained by a mean - field method instead of the intractable true expectations and approximately evaluate @xmath45 by means of the mean - field method .    let us use @xmath44 obtained by the minimization of the approximate variational free energy in equation ( [ eq : gibbs_nnbm_2nd ] ) , that is the solutions to the tap equation in equations ( [ eq : mi_nnbm_2nd])([eq : r_nnbm_2nd ] ) , in equation ( [ eq : lrr ] ) in order to approximately compute the susceptibilities . by using equations ( [ eq : mi_nnbm_2nd])([eq : lrr ] )",
    ", the approximate susceptibilities are obtained by using the solutions to the nonlinear simultaneous equations , which are @xmath48 where @xmath49 , @xmath50 , and @xmath51 .",
    "the expression @xmath52 is the kronecker delta .",
    "the variables @xmath37 , @xmath38 , @xmath53 , and @xmath54 in equations ( [ eq : susp - chi ] ) and ( [ eq : susp - v ] ) are the solutions to the tap equation in equations ( [ eq : mi_nnbm_2nd])([eq : r_nnbm_2nd ] ) . equations ( [ eq : susp - chi])([eq : susp - r ] ) are obtained by differentiating equations ( [ eq : mi_nnbm_2nd])([eq : r_nnbm_2nd ] ) with respect to @xmath55 .",
    "note that , the expression in equations ( [ eq : susp - chi ] ) and ( [ eq : susp - v ] ) is simplified by using equations ( [ eq : mi_nnbm_2nd ] ) and ( [ eq : vi_nnbm_2nd ] ) .    after obtaining solutions to the tap equation in equations ( [ eq : mi_nnbm_2nd])([eq : r_nnbm_2nd ] ) , by computationally solving equations ( [ eq : susp - chi])([eq : susp - r ] ) by using an iterative method , such as the successive iteration method , we can obtain approximate susceptibilities @xmath45 in terms of the tap equation .",
    "this scheme for computing susceptibilities has a decade - long history and is referred to as _ the susceptibility propagation method _ or to as _ the variational liner response method _  . by using this method , we can obtain not only the expectation values of one variable but also the covariances on nnbms . in the next section ,",
    "we propose a version of this susceptibility propagation method that is improved by combining it with the diagonal consistency method .      in order to apply the diagonal consistency method to the present framework ,",
    "i modify the approximate variational free energy in equation ( [ eq : gibbs_nnbm_2nd ] ) as @xmath56 where the variables @xmath57 are auxiliary independent parameters that play an important role in the diagonal consistency method  @xcite .",
    "we formulate the tap equation and the susceptibility propagation for the modified variational free energy , @xmath58 , in the same manner as that described in the previous sections . by this modification , equations ( [ eq : l_nnbm_2nd ] ) , ( [ eq : r_nnbm_2nd ] ) , and ( [ eq : susp - l ] ) are changed to @xmath59 and @xmath60 respectively .",
    "equations ( [ eq : mi_nnbm_2nd])([eq : l_nnbm_2nd ] ) and ( [ eq : r_nnbm_2nd - new ] ) are the tap equation , and equations ( [ eq : susp - chi ] ) , ( [ eq : susp - v ] ) , ( [ eq : susp - r ] ) , and ( [ eq : susp - l - new ] ) are the susceptibility propagation for the modified variational free energy .",
    "the solutions to these equations obviously depend on the values of @xmath61 .",
    "since the values of @xmath61 can not be specified by using only the tap equation and the susceptibility propagation , we need an additive constraint in order to specify them .    as mentioned in sections [ sec : gibbs&tap ] and [ subsec : lrr&susp ] , relations @xmath62 and @xmath63 hold in the scheme with no approximation .",
    "hence , the relations @xmath64 should always hold in the exact scheme , and therefore , these relations can be regarded as important information that the present system has .",
    "the diagonal consistency method is a technique for inserting these diagonal consistency relations into the present approximation scheme . in the diagonal consistency method , the values of the auxiliary parameters",
    "@xmath61 are determined so that the solutions to the modified tap equation in equations ( [ eq : mi_nnbm_2nd])([eq : l_nnbm_2nd ] ) and ( [ eq : r_nnbm_2nd - new ] ) and the modified susceptibility propagation in equations ( [ eq : susp - chi ] ) , ( [ eq : susp - v ] ) , ( [ eq : susp - r ] ) , and ( [ eq : susp - l - new ] ) satisfy the diagonal consistency relations in equation ( [ eq : diagonal - consistency ] ) . from equations ( [ eq : susp - chi ] ) , ( [ eq : susp - l - new ] ) , and ( [ eq : diagonal - consistency ] ) , we obtain @xmath65 where @xmath66 equation ( [ eq : diagmatch ] ) is referred to as _ the diagonal matching equation _ , and we determine the values of @xmath61 by using this equation in the framework of the diagonal consistency method  @xcite . by",
    "computationally solving the modified tap equation , the modified susceptibility propagation , and the diagonal matching equation , the expectations @xmath37 and susceptibilities ( covariances ) @xmath45 that we can obtain are improved by applying the diagonal consistency method .",
    "it is noteworthy that the order of the computational cost of the proposed method is the same as that of the normal susceptibility propagation method , which is @xmath67 .    in the normal scheme presented in section",
    "[ subsec : lrr&susp ] , the results obtained from the susceptibility propagation method are not fed back to the tap equation .",
    "in contrast , in the improved scheme proposed in this section , they are fed back to the tap equation through parameters @xmath61 , because the tap equation and the susceptibility propagation method share the parameters @xmath61 in this scheme ( see figure [ fig : scheme ] ) .     in the improved susceptibility propagation.,height=151 ]    instinctually , the role of the parameters @xmath61 can be interpreted as follows . as mentioned in section [ subsec : lrr&susp ] ,",
    "the susceptibilities @xmath45 are interpreted as responses to the small changes in the biases on the vertices .",
    "self - response @xmath68 , which is a response on vertex @xmath36 to a small change in the bias on vertex @xmath36 , is interpreted as a response coming back through the whole system .",
    "thus , it can be expected that the self - responses have some information of the whole system . in the proposed scheme ,",
    "the self - responses , which differ from the true response by the approximation , are corrected by the diagonal consistency condition in equation ( [ eq : diagonal - consistency ] ) , and subsequently the information of the whole system , which the self - responses have , are embedded into parameters @xmath61 and are transmitted to the tap equation through parameters @xmath61 .",
    "to verify performance of the proposed method , let us consider statistical machine learning on nnbms . for a given complete data set @xmath69 generated from a generative nnbm ,",
    "the log - likelihood function of the learning nnbm is defined as @xmath70 this log - likelihood is rewritten as @xmath71 where notation @xmath72 denotes the sample average with respect to the given data set .",
    "the learning is achieved by maximizing the log - likelihood function with respect to @xmath6 and @xmath7 .",
    "the gradients of the log - likelihood function are expressed as follows : @xmath73 we use @xmath37 , @xmath38 , and @xmath45 obtained by the approximative methods presented in the previous sections in these gradients , and approximately maximize the log - likelihood function .",
    "more accurate estimates can be expected to give better learning solutions .",
    "we measured the quality of the learning by the mean absolute errors ( maes ) between the true parameters and learning solutions defined by @xmath74 where @xmath75 and @xmath76 are the true parameters on the generative nnbm . in the following sections , the results of the learning on two kinds of generative nnbms are shown . in both cases , we set to @xmath77 and @xmath78 and generated the data sets by using the markov chain monte carlo method on the generative nnbms .",
    "let us consider the generative nnbm on a @xmath79 square grid graph whose parameters are @xmath80 where @xmath81 is the unique distribution from @xmath82 to @xmath83 .",
    "data set @xmath84 was generated from this generative nnbm , and used to train the learning nnbm .",
    "table [ tab : mae-1 ] shows the maes obtained using the approximative methods .",
    ".maes between true parameters and learning parameters in equation ( [ eq : maes ] ) .",
    "the complete data set @xmath84 are generated from the square grid nnbm in equation ( [ eq : sqnnbm ] ) .",
    "each result is the average over 50 trials . [",
    "cols=\"^,^,^,^\",options=\"header \" , ]     we can see that the proposed improved method ( i - susp ) again outperformed the other methods .",
    "in this paper , an effective inference method for nnbms was proposed .",
    "this inference method was constructed by using the tap equation and the diagonal consistency method , which was recently proposed , and performed better than the normal susceptibility propagation in the numerical experiments of statistical machine learning on nnbms described in section [ sec : numerical ] . moreover",
    ", the order of the computational cost of the proposed inference method is the same as that of the normal susceptibility propagation , which is @xmath67 .",
    "the proposed method was developed based on the second - order perturbative approximation of the free energy in equation ( [ eq : gibbs_nnbm_2nd ] ) . since the plefka expansion allows us to derive higher - order approximations of the free energy , we should be able to formulate improved susceptibility propagations base on the higher - order approximations in a similar way to the presented method .",
    "it should be addressed in future studies . in reference",
    "@xcite , a tap equation for more general continuous mrfs , where random variables are bounded within finite values , was proposed .",
    "we can formulate an effective inference method for continuous mrfs by combining the tap equation with the diagonal consistency method in a way similar to that described in this paper .",
    "this topic should be also considered in future studies .",
    "to derive the variational free energy in equation ( [ eq : gibbsfreeenergy_nnbm ] ) , let us consider the following functional with a trial probability density function @xmath85 : @xmath86:=\\int_{0}^{\\infty}e(\\bm{x};\\bm{b},\\bm{w})q(\\bm{x})\\diff\\bm{x}+\\int_{0}^{\\infty}q(\\bm{x})\\ln q(\\bm{x})\\diff\\bm{x } , \\end{aligned}\\ ] ] and consider the minimization of this functional under constraints : @xmath87 by using the lagrange multipliers , this minimization yields equation ( [ eq : gibbsfreeenergy_nnbm ] ) , i.e. , @xmath88 \\mid \\mrm{constraints}\\big\\}.\\end{aligned}\\ ] ] this variational free energy is referred to as the gibbs free energy in statistical physics , and the minimum of the variational free energy coincides with @xmath23  .",
    "this work was partly supported by grants - in - aid ( no . 24700220 ) for scientific research from the ministry of education , culture , sports , science and technology , japan .",
    "j. pearl : probabilistic reasoning in intelligent systems : networks of plausible inference ( 2nd ed . ) , san francisco , ca : morgan kaufmann , 1988 . j. s. yedidia and w. t. freeman : constructing free - energy approximations and generalized belief propagation algorithms , ieee trans . on information theory , vol .",
    "51 , pp . 22822312 , 2005 . j. s. yedidia and w. t. freeman : correctness of belief propagation in gaussian graphical models of arbitrary topology , neural computation , vol . 13 , pp . 2173 - 2200 , 2001",
    ". o. b. downs , d. j. c. mackay and d. d. lee : the nonnegative boltzmann machine , advances in neural information processing systems , vol .",
    "428434 , 2000 .",
    "o. b. downs : high - temperature expansions for learning models of nonnegative data , advances in neural information processing systems , vol .",
    "13 , pp . 465471 , 2001 .",
    "n. d. socci , d. d. lee and h. s. seung : the rectified gaussian distribution , advances in neural information processing systems , vol .",
    "10 , pp . 350356 , 1998 .",
    "p. paatero , u. tapper : positive matrix factorization : a nonnegative factor model with optimal utilization of error estimates of data values , environmetrics , vol . 5 , pp . 111126 , 1994 .",
    "d. d. lee , h. s. seung : learning the parts of objects by nonnegative matrix factorization , nature , vol .",
    "401 , pp . 788791 , 1999 . m. plumbley , e. oja : a `` nonnegative pca '' algorithm for independent component analysis , ieee trans .",
    "neural networks , vol .",
    "15 , pp . 6676 , 2004 . m. harva and a. kabn : variational learning for rectified factor analysis , signal processing , vol.87 , pp.509527 , 2007 .",
    "m. yasuda and k. tanaka : tap equation for nonnegative boltzmann machine , philosophical magazine , vol .",
    "192209 , 2012 .",
    "t. plefka : convergence condition of the tap equation for the infinite - range ising spin glass model , j. phys . a : math .",
    "15 , pp . 19711978 , 1982 . m. yasuda and k. tanaka : susceptibility propagation by using diagonal consistency , physical review e , vol .",
    "87 , pp . 012134 , 2013 .",
    "j. raymond and f. ricci - tersenghi : correcting beliefs in the mean - field and bethe approximations using linear response , proc .",
    "ieee icc13 workshop on networking across disciplines : communication networks , complex systems and statistical physics ( netstat ) , 2013 .",
    "m. yasuda : a generalization of improved susceptibility propagation , journal of physics : conference series , vol .",
    "473 , pp . 012006 , 2013 .",
    "k. tanaka : probabilistic inference by mean of cluster variation method and linear response theory , ieice trans . on information and systems , vol .",
    "e86-d , pp . 12281242 , 2003 . m. welling and y. w. teh : approximate inference in boltzmann machines , artificial intelligence , vol.143 , pp.1950 , 2003",
    ". m. welling and y. w. teh : linear response algorithms for approximate inference in graphical models , neural computation , vol.16 , 197221 , 2004 . m. mzard and t. mora : constraint satisfaction problems and neural networks : a statistical physics perspective , journal of physiology - paris , vol .",
    "103 , pp . 107113 , 2009 .",
    "r. ben - yishai , r. lev bar - or and h. sompolinsky : theory of orientation tuning in visual cortex , proc .",
    "usa , vol .",
    "92 , pp . 38443848 , 1995 . m. yasuda and k. tanaka : boltzmann machines with bounded continuous random variables , interdisciplinary information sciences , vol .",
    "2531 , 2007 ."
  ],
  "abstract_text": [
    "<S> nonnegative boltzmann machines ( nnbms ) are recurrent probabilistic neural network models that can describe multi - modal nonnegative data . </S>",
    "<S> nnbms form rectified gaussian distributions that appear in biological neural network models , positive matrix factorization , nonnegative matrix factorization , and so on . in this paper , an effective inference method for nnbms </S>",
    "<S> is proposed that uses the mean - field method , referred to as the thouless  anderson  </S>",
    "<S> palmer equation , and the diagonal consistency method , which was recently proposed . </S>"
  ]
}