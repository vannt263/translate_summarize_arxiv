{
  "article_text": [
    "a wide variety of real - life problems involve the simultaneous optimization of several objectives or criteria .",
    "objectives that contradict each are common , thus resulting in situations where one objective can not be improved without worsening at least one other objective .",
    "for example , in cancer radiotherapy treatment , one goal is to maximize the amount of cancerous tissue exposed to radiation treatment while another ( contradicting ) goal is to minimize the radiation exposure of healthy organs . here",
    ", there is typically no trivial compromise .",
    "problems of this kind are called _ multi - criteria optimization _ ( mco ) problems .",
    "the number of competing goals in the treatment planning for _ intensity modulated radiation therapy _ ( imrt ) is quite large ( about 10 - 25 ) . exploring the solutions to the mco problem",
    "involves a high - dimensional pareto boundary @xcite . approximating",
    "this set is computationally expensive , and experience shows that there are many tradeoffs that are either clinically unacceptable or irrelevant .",
    "thus a significant part of the computational effort is wasted on identifying useless solutions .",
    "the complexity of the approximated high - dimensional pareto boundary ( see e.g. @xcite ) makes it difficult for the decision maker to navigate .",
    "an alternative approach would be to first identify the most important aspects of the imrt plan and make sure that these are met by the solution ; compromises , if necessary , would only be acceptable for subsidiary goals .    toward this end , we want to incorporate a priori knowledge about priorities into the treatment plan optimization process .",
    "this information translates naturally into _ lexicographic optimization _ ( lo ) problems , which constitute a special class of mco problems .",
    "classical lo methods minimize the objective functions sequentially , starting with the most important one and proceeding according to the lexicographic order of the objectives . here , the optimal value found for each objective is added as a constraint for subsequent optimizations",
    ".    these constraints can be quite rigorous , however , depending on the correlation of the corresponding functions and the decision maker might not always want to sacrifice so much optimizing freedom for the sake of a single goal .",
    "we therefore propose the more intuitive approach of gathering goals in groups of descending importance , as in @xcite , and formulating objective functions as weighted sums of the functions contained in these groups .",
    "this reduces the number of objective functions to be minimized , which leads to fewer optimization problems to be solved and thus lowers the computational effort .",
    "typically , the subsequent objectives are in conflict with each other , which implies that the minimum of one objective function is a relatively bad starting point for subsequent optimization levels .    to address this issue",
    ", we propose to combine the superiorization methodology with an mco algorithm .",
    "superiorization can speed up convergence by steering the mco algorithm towards solutions that are better suited to minimizing subsequent objective functions . in the best case",
    ", one could even find the minimizer of all subsequent objective functions before reaching the last optimization level .",
    "here , subsequent minimization becomes unnecessary , and one need only verify that the solution is indeed the minimizer of all subsequent objective functions under the given constraints .",
    "the superiorization methodology @xcite was recently developed as a framework for algorithms that lie conceptually between feasibility - seeking and optimization algorithms .",
    "it is a heuristic tool that does not guarantee to find the optimum value of a given functional , rather it obtains a solution that is superior ( with respect to a given objective function ) to the solution achieved by a classical feasibility seeking algorithmic operator .",
    "the main advantage of an algorithm that uses superiorization as the driving tool ( as opposed to optimization ) is that it requires less computational resources - given a good choice of parameters - while providing comparable solutions , from the point of view of real - world applications , to those that one would get with algorithms that use optimization .    within this framework",
    ", we wish to include the class of projection methods , which are applied successfully in many real - world feasibility problems , see e.g. , @xcite .",
    "this class has witnessed great progress in recent years and its member algorithms have been applied with success to problems in sensor networks , image reconstruction , image processing , imrt planning , resolution enhancement and in many others ; see @xcite . apart from theoretical interest , the main advantage of projection methods is their computational efficiency .",
    "they commonly have the ability to handle huge - size problems of dimensions in which other , more sophisticated methods cease to be efficient .    in this paper",
    "we propose to apply the superiorization methodology with projection methods to improve the speed of convergence of lexicographic optimization methods .",
    "we demonstrate the plausibility of using superiorization with a simple example with two optimization variables .",
    "we also present results from calculations on four real head neck cases in imrt for two different choices of superiorization parameter sets suited to yield fast convergence for each case individually or robust behavior for all four cases .",
    "the paper is organized as follows . in section [ sec : methods ] we present some preliminaries , definitions and algorithms that will be needed in the sequel . later , in section [ sec : numerical illustration ] our general scheme is illustrated on a 2d example and we present the results of our calculations on the aforementioned imrt head neck cases .",
    "finally we discuss our findings and conclusions in sections [ sec : discussion ] and [ sec : conclusion ] .",
    "let us start by giving a definition of what is a multicriteria optimization problem",
    ".    given a mapping @xmath2 with @xmath3 for all @xmath4 and let @xmath5 for @xmath6 .",
    "the ` multicriteria optimization ` ( mco ) problem is the following .",
    "@xmath7    where for all @xmath8 , @xmath9 .      in this work we focus on a particular mco method which is called _ lexicographic optimization _",
    "( lo ) . for lo",
    "we define an order for the objectives according to their importance and minimize them in that order under the given constraints . after having minimized one objective ,",
    "we impose an additional constraint which ensures that the objective function value of subsequent solutions do not deviate from the previously found minimum by more than a user - defined value .",
    "in the context of imrt we consider a more general setting for the formulation of the optimization problems . here , we gather the objective functions @xmath10 in @xmath11 priority groups which are ranked from most to least important ( compare e.g. , @xcite ) . denote by @xmath12 an ordered subset of objective function indices such that @xmath13 is a _ partition _ of @xmath14 , meaning that    @xmath15    it is clear that if for all @xmath16 the set @xmath17 is a singleton then this reformulation reduces to the classical lexicographic optimization problem .",
    "now we define@xmath18 where for each @xmath19 , @xmath20 is the weight of the @xmath21-th objective function @xmath22 .",
    "so @xmath23 gathers the objective functions whose indices are elements of @xmath17 . at optimization level @xmath24 @xmath25",
    "we thus solve the following problem .",
    "@xmath26    where @xmath27 are the minimum values found for each @xmath28 during the previous optimization levels , @xmath29 are small user - chosen constants for @xmath30 and @xmath31 are as in ( [ problem : mco ] ) .",
    "let us illustrate the above for a two - stage lo also known as _ bi - level optimization_. assume that @xmath32 @xmath33 , and @xmath34 ; thus @xmath35 and @xmath36 .",
    "at the first optimization level we solve the following single - objective optimization problem.@xmath37after we have solved ( [ eq : p1 ] ) and obtained @xmath38 ( @xmath39 ) , we continue with the second optimization level , where our problem formulates as @xmath40 where @xmath41 for some small user - chosen constant @xmath42 .    in the above , the choice of @xmath43 and in general @xmath44 for @xmath45 ,",
    "defines in a fixed way how far from previously found optima of objective functions of higher priority the algorithm is allowed to deviate in the current optimization level .",
    "a different approach has been taken by @xcite which allows the decision maker to choose the tradeoff between two subsequent ( in terms of their lexicographic order ) objective functions in an interactive way .",
    "observe that if the constraint set of ( [ eq : p1 ] )    @xmath46    ( or of ( [ eq : p2 ] ) with @xmath47 ) contains only one element , then this is the optimal solution of the bi - level optimization problem and it is found already at the first optimization level . otherwise , in case of multiple feasible points , a feasibility seeking algorithm for finding an element of ( [ eq : p1 ] ) might end up with a point which is relatively far from the optimal solution of the second optimization level , this is due to the fact that the second objective function has not been taken into account yet .",
    "this effect is likely to occur when the objective functions of the subsequent optimization levels represent conflicting goals as commonly happens in cancer therapy .    motivated by these difficulties we propose to steer the algorithm towards solutions that provide good starting points for subsequent optimization levels by applying the superiorization methodology together with projection methods .",
    "before we can do so we need to transform the optimization problem such that we can use projection methods to solve it .",
    "for the aforementioned purpose we use the _ level set scheme _",
    "@xcite which is a new projection - based scheme for solving convex optimization problems .",
    "it transforms a convex optimization problem into a sequence of auxiliary feasibility problems by iteratively constraining the objective function from above until the feasibility problem is inconsistent .",
    "to illustrate the idea let @xmath48 be convex and continuously differentiable function and @xmath49 nonempty , closed and convex set .",
    "following the level set scheme we can reformulate the problem @xmath50 in the following form    @xmath51    now let @xmath52 be some user chosen positive sequence , for example @xmath53or @xmath54 .    in the initial step of the level set scheme",
    "we choose any projection method to find a feasible point @xmath55 , set @xmath56 and @xmath57 .",
    "now at the iterative step @xmath58 , when we are given the current point @xmath59 , we try to solve the following _ convex feasibility problem _ ( cfp )    @xmath60    if there exists a feasible solution , set @xmath61 and continue , else there exists no feasible solution , and @xmath62 is an @xmath63-optimal solution .      in this work ,",
    "we use _ projection methods _ to solve the auxiliary convex feasibility problems generated by the level set scheme .",
    "projection methods are iterative algorithms that use projections onto sets .",
    "they rely on the general principle that when a family of ( usually closed and convex ) sets is present , projections onto the given individual sets are easier to perform than projections onto other sets ( intersections , image sets under some transformation , etc . ) that are derived from the given individual sets .",
    "projection methods come in various algorithmic structures , some of which are particularly suitable for parallel computing , and they demonstrate nice convergence properties , for example bounded perturbations resilience .",
    "this fact allows us to use them to solve the cfps resulting from the level set scheme and to incorporate superiorization , which is presented in the following paragraph , while retaining convergence to the optimal point .",
    "consider the cfp : @xmath64 with @xmath31 defined as in .",
    "the particular method we use in this paper is the _ simultaneous subgradient projections method _ which is defined as follows .",
    "let @xmath65 be an arbitrary starting point .",
    "given the current iterate @xmath62 , calculate the next iterate @xmath66 via    @xmath67    where @xmath68 ( subgradient of @xmath69 at @xmath62 ) , @xmath70 are _ weights _ and @xmath71 $ ] ( relaxation parameters ) for arbitrary @xmath72 .",
    "simultaneous projection methods are also referred to as parallel  methods . in this case , in order to evaluate the next iterate , all ( or a block of more than one ) constraints are taken into account . in case of a system of linear equalities , this method is known as cimmino s method @xcite .",
    "see @xcite for more details",
    ".    given a problem @xmath73 , an algorithmic operator @xmath74 is said to be ` bounded perturbations resilient ` if the following is true .",
    "if the sequence @xmath75 generated by @xmath76 , for all @xmath77 , converges to a solution of @xmath73 , then any sequence @xmath78 of points in @xmath79 generated by @xmath80 for all @xmath77 , also converges to a solution of @xmath73 provided that , for all @xmath77 , @xmath81 are bounded perturbations , meaning that @xmath82 for all @xmath77 such that @xmath83 and the sequence @xmath84 is bounded .",
    "the _ superiorization methodology _ of @xcite was recently developed as a framework for algorithms that lie conceptually between feasibility - seeking and optimization algorithms .",
    "it is designed to find a solution to a cfp which is superior with respect to a given objective function @xmath85 , meaning with a value of @xmath85 at least as low , but possibly lower , compared to the solution obtained by a classical feasibility seeking algorithmic operator .",
    "the state of current research on superiorization can best be appreciated from the superiorization and perturbation resilience of algorithms : a bibliography compiled and continuously updated by yair censor  which is at : http://math.haifa.ac.il/yair/bib-superiorization-censor.html",
    ". in particular , @xcite and @xcite are recent reviews of interest .    given a function @xmath86 and a point @xmath87",
    ", we say that a vector @xmath88 is ` nonascending ` for @xmath85 at @xmath89 if and only if @xmath90 and there is a @xmath91 such that for all @xmath92 $ ] we have @xmath93    consider again the convex feasibility problem :    @xmath94    where @xmath95 is a closed and convex set and let @xmath96 be any feasibility - seeking algorithmic operator , that defines the basic algorithm @xmath76 to solve the convex feasibility problem .",
    "let @xmath86 be a given convex and continuously differentiable function .",
    "the _ superiorized _",
    "( with respect to @xmath85 ) version of @xmath96 is    @xmath97    where @xmath98 is nonascending for @xmath85 at @xmath99 .",
    "one option for @xmath98 , as it is used in @xcite , is @xmath100    if the solution set of the feasibility problem contains several elements , the superiorized version of @xmath96 obtains a solution that is superior with respect to @xmath85 to the solution achieved by the basic algorithmic operator @xmath96 .",
    "we now combine all of the mentioned concepts to derive our approach .",
    "we consider the multicriteria optimization problem ( [ problem : mco ] ) and define the priority groups @xmath17 and weights @xmath101 , @xmath102 .",
    "we then transform it into a lo problem like ( [ eq : mclo ] ) .",
    "now we apply the level set scheme and consequently solve the following optimization problem at every optimization level @xmath103 :    @xmath104    where    @xmath105    to do that , we transform ( [ cfplexopt ] ) into a sequence of auxiliary cfps of the form :    @xmath106    for decreasing sequences of @xmath107 .",
    "we solve the auxiliary cfps by using the simultaneous subgradient projections method as the basic algorithm @xmath96 .",
    "let @xmath108 be a subset of indices contained in the subsequent priority groups @xmath109 and let @xmath110 be weights to the corresponding objective functions .    then",
    ", after having successfully solved @xmath111 auxiliary cfps we include superiorization with respect to @xmath112 by defining @xmath98 as in ( [ eq : nabla ] ) with @xmath113 and setting    @xmath114    where @xmath115 for all @xmath116 such that @xmath117 and @xmath118 , meaning that @xmath98 is a direction of nonascend for @xmath85 at @xmath62 .    in the following we refer to this method as _ superiorized lexicographic",
    "( slo ) .",
    "the imrt planning problem is to find a treatment plan , i.e.  energy fluence intensities , which results in a dose distribution in the patient s body that irradiates the tumor as homogeneously as possible while sparing critical healthy organs .",
    "the dose ( measured in gy ) delivered to the patient s body is evaluated according to clinical goals which are translated into the functions given below .",
    "the clinical goals are ordered according to their importance and can also be grouped together which reduces the number of optimization levels and therefore accelerates the optimization process . for our calculations on four head neck cases we chose one group of goals as constraints and three subsequent priority groups .",
    "the volume of the patient s body is discretized into a three - dimensional grid of voxels .",
    "the voxels are assigned to different planning structures , e.g.  organs or tumor tissue .",
    "let @xmath119 denote the so - called dose matrix that maps a vector @xmath120 of fluence intensities to the dose @xmath121 received by the individual voxels .",
    "thus a planning structure @xmath122 is essentially a subset of indices of the vector @xmath123 .    to evaluate the dose received by different planning structures we used lower and upper tail penalty functions as well as mean upper tail penalty functions .",
    "the lower tail penalty function of the dose @xmath123 for a planning structure @xmath122 penalizes dose values below a given threshold @xmath124 .",
    "it is given by    @xmath125    the upper tail penalty function is    @xmath126    and is used to penalize dose values exceeding a given threshold @xmath127 .",
    "the mean upper tail function is defined as    @xmath128    this function is used to keep the mean dose values below a given threshold @xmath11 , which is a less strict way of avoiding overdosage than using the upper tail penalty function .",
    "table [ tab : problemformulation ] shows our problem formulation for the first head neck cases according to @xcite .",
    "the other cases were treated similarly , depending on the individual geometry of the tumor tissue .",
    "to gain an impression of the geometry of some of the planning structures involved see figure [ fig : imrtrsloct ] .",
    "note that the function values of the evaluation functions listed as constraints were required to be 0 which translates to the dose prescription of the corresponding planning structure being met by the dose distribution .",
    "the weights for the evaluation functions were chosen empirically for each case individually such that the optimization would yield a clinically acceptable treatment plan .",
    "non - tumor tissue 1 denotes the three - dimensional margin of 1 cm around the lymphatic drainage pathways ( which provide the largest of the target volumes and contain both ptv 60 and ptv 70 ) .",
    "non - tumor tissue 2 is the tissue that consists of everything but the lymphatic drainage pathways and non - tumor tissue 1 .",
    "these structures are introduced to reduce the radiation to non - tumor tissue , especially to areas which are not covered by any of the organs at risk .",
    "additionally , it offers an incentive to the optimization algorithm to reduce the dose values of the organs at risk that lie outside of the lymphatic drainage pathways even below the prescribed maximum or maximum mean dose values .",
    "in optimization level @xmath103 we minimize the objective function @xmath129 which is a weighted sum of the functions belonging to priority group @xmath17 .",
    ".evaluation functions of the structures under consideration gathered in different priority groups . [ cols=\"<,^,<,^\",options=\"header \" , ]      in this section we present pseudo codes of the algorithms we implemented to solve the imrt optimization problems .",
    "algorithm [ alg:1 ] implements the lexicographic optimization problem turned into a sequence of convex feasibility problems and algorithm [ alg:2 ] implements the superioriation methodology .    in the following paragraph",
    "we give some details about the submethods mentioned in the codes .    `",
    "findfeasiblesolution(t^{(\\mu ) } ) ` tries to solve the cfp with @xmath130 within @xmath131 simultaneous projections .",
    "if this is successful , it returns [ @xmath120 , true ] with @xmath132 and @xmath120 feasible .",
    "if this is not successful , the method returns [ @xmath120 , false ] with @xmath133 , where @xmath134 and @xmath120 feasible .",
    "@xmath135 is the smallest upper bound for @xmath129 that the algorithm was able to find a feasible solution for .    `",
    "reduce(\\phi_{\\mu}(\\mathbf{p}x ) ) ` returns a new upper bound @xmath136 for @xmath129 that is smaller than @xmath137 .",
    "` addconstraint(\\phi_{\\mu}^{\\ast } ) ` adds @xmath138 to the set of constraints .    `",
    "supdirection(x ) ` returns the direction of superiorization at @xmath120 . in our calculations we used superiorization with respect to the objective function @xmath139 of the following optimization level .",
    "@xmath24 = 1 ; @xmath140 @xmath141 reduce(@xmath142 )    levelissolved = false counter = 0    = findfeasiblesolution(@xmath136 ) counter++ levelissolved = true @xmath143 addconstraint(@xmath144 ) @xmath24++ x = superiorize ( ) @xmath136 = reduce(@xmath142 ) levelissolved = true @xmath143 addconstraint(@xmath144 ) @xmath24++    coefficientbigenough = true base @xmath145 @xmath146 = 0 direction = supdirection(@xmath120 ) loop = true exponent = 1 coefficient = base@xmath147 @xmath148 = @xmath120 + coefficient @xmath149 direction @xmath150 loop = false @xmath146++ exponent++ coefficientbigenough = false",
    "in the first part of this section we demonstrate the plausibility of using the superiorized lexicographic optimization methodology on a simple example with two optimization variables which is taken from stanimirovi stanimirovic12 . in the second part we present the performance of our method on four real imrt head neck cases .      to demonstrate the mechanism of superiorization and how it can be useful for lexicographic optimization we consider the following example .",
    "solve the following multicriteria optimization problem    @xmath151    where the functions @xmath152 @xmath153 and @xmath154 are given in lexicographic order .",
    "the optimal solution @xmath155 is @xmath156 with @xmath157 .",
    "we stopped the optimization process at the iterate @xmath62 if @xmath158 . in the following ,",
    "points are given with precision @xmath159 .",
    "in this example calculating the gradients and projections is computationally very cheap , but for imrt cases gradient- and dose - evaluations consume most of the computation time , so we compare the number of projections and gradient evaluations .",
    "figure [ fig : toyextrajectoryboth ] shows the feasible region and the trajectories of the iterates of the classical and the superiorized lo for the starting point @xmath160 .",
    "both methods first seek feasibility and find the point @xmath161 . in optimization level 1",
    ", the classical level set scheme minimizes with respect to @xmath162 only and ends up at the solution @xmath163 of the first optimization level with @xmath164 using 327 projections and 327 gradient calculations .",
    "additionally , we need another 260 projections and 518 gradient evaluations to find out that we can not improve @xmath162 any further while staying feasible . in optimization level 2",
    ", we want to preserve @xmath165 for all subsequent iterates @xmath62 and minimize @xmath166 under this additional condition .",
    "we arrive at the solution @xmath167 of optimization level 2 after a total of 4743 projections and 9107 gradient calculations .",
    "we can not improve any further with respect to @xmath168 and have found the optimal solution .",
    "the superiorized level set scheme minimizes with respect to @xmath162 in optimization level 1 but uses superiorization with respect to @xmath166 .",
    "it arrives at the solution @xmath169 of optimization level 1 after 108 projections and 144 gradient calculations , 34 of which were needed for the superiorization . using the superiorized lo we get already sufficiently close to the optimum in optimization level 1 .      from the formulae  of the utilized functions it is clear that the minimal value we can hope to achieve for any of the @xmath170 is 0 .",
    "thus we considered an optimization level @xmath103 to be solved , if @xmath171 given that @xmath120 is feasible .",
    "what takes most of the computational time with imrt cases are multiplications of the dose matrix @xmath119 with the fluence vector @xmath120 which occur when we evaluate the dose or calculate the objective function gradient for a given @xmath120 .",
    "the number of these multiplications is what we will use as units to measure progress , i.e. , reduction of objective function values , of the classical and the superiorized lo .    in algorithm",
    "[ alg:1 ] and [ alg:2 ] in subsection [ subsec : implementation ] we find parameters @xmath111 and @xmath172 which determine the behavior of the method .",
    "@xmath111 is the number of cfps that have to be successfully solved before superiorization is applied and @xmath172 is the maximum number of superiorization steps that are taken .",
    "if the superiorization step size is smaller than a predefined threshold or the maximum number of steps is reached , the superiorization stops and we solve the next cfp .    in tables [ tab",
    ": imrtfastresults1 ] and [ tab : imrtfastresults2 ] we first present results we achieved by trying different parameter sets @xmath173 and then picking for each case individually the set that resulted in the fastest convergence of the algorithm measured in the number of multiplications .",
    "we refer to the sets of parameters picked according to this strategy as @xmath174 .",
    "all values presented in the tables in this section are rounded to @xmath175 .",
    "the results presented in table [ tab : imrtfastresults1 ] show that fast slo results in equal or lower objective values than unsuperiorized lo within less multiplications . in table",
    "[ tab : imrtfastresults2 ] we see that fast slo produces solutions of the single optimization levels with potentially lower objective values of the subsequent objective functions than unsuperiorized lo .",
    "fast slo thus offers a better starting point to the single optimization levels than unsuperiorized lo which reduces the overall number of multiplications .",
    "@xmath176 denotes the value of @xmath170 at the solution @xmath177 of optimization level @xmath103 obtained using the classical lo , @xmath178 denotes the value of @xmath170 at the solution @xmath179 of optimization level @xmath103 obtained using the superiorized lo with @xmath174 .",
    "@xmath180 is the number of multiplications needed by the classical lo to solve optimization level @xmath181 to level @xmath182 .",
    "@xmath183 is the total number of multiplications .",
    "analogously , @xmath184 denotes the number of multiplications needed by the superiorized lo with @xmath174 to solve optimization level @xmath181 to level @xmath182 and @xmath185 .    for our calculations we chose @xmath186 for the classical lo and @xmath187 for the superiorized lo .",
    "lcccc    ' '' ''    & @xmath188 & @xmath189 & @xmath190 & @xmath191/@xmath192 + case 1 & @xmath193 & 0.9598 & 0.9923 & 0.8998 + case 2 & @xmath193 & 0.9743 & 0.9960 & 0.6612 + case 3 & @xmath193 & 0.9996 & 1.0000 & 0.9210 + case 4 & @xmath193 & 0.9708 & 0.9941 & 0.8959 +    lccccc    ' '' ''    & @xmath194 & @xmath195 & @xmath196 & @xmath197 & @xmath198 + case 1 & 1.6521 & 0.5669 & 0.9895 & 0.3741 & 0.9856 + case 2 & 1.4066 & 0.3588 & 1.0065 & 0.3682 & 1.0008 + case 3 & 1.0403 & 0.8594 & 1.0000 & 0.5625 & 1.0075 + case 4 & 0.3818 & 1.0536 & 0.8591 & 0.8196 & 0.9939 +    it is not clear a priori how to choose values for @xmath111 and @xmath172 for a given problem .",
    "therefore we aimed to find a robust set of parameters that yields good results for all imrt cases and could possibly be applied for other imrt head neck cases as well .",
    "we refer to this set of parameters as @xmath199 .",
    "the results for the robust parameter set are presented in table [ tab : imrtrobustresults1 ] and tab : imrtrobustresults2 .",
    "the results in table [ tab : imrtrobustresults1 ] show that robust slo achieves comparable objective values to unsuperiorized lo within fewer multiplications . in table",
    "[ tab : imrtrobustresults2 ] we see that robust slo , too , produces solutions of the single optimization levels with potentially lower objective values of the subsequent objective functions than unsuperiorized lo . like fast slo",
    ", robust slo also offers a better starting point to the single optimization levels than unsuperiorized lo which again reduces the overall number of multiplications .",
    "following the previous notation @xmath200 is the value of @xmath170 at the solution @xmath201 of optimization level @xmath103 obtained using the superiorized lo with @xmath199 .",
    "@xmath202 denotes the number of multiplications needed by the superiorized lo with @xmath199 to solve optimization level @xmath181 to level @xmath182 and @xmath203 .",
    "lcccc    ' '' ''    & @xmath204 & @xmath205 & @xmath206 & @xmath207/@xmath208 + case 1 & @xmath193 & 1.0046 & 1.0002 & 0.9177 + case 2 & @xmath193 & 1.0020 & 1.0043 & 0.8877 + case 3 & @xmath193 & 0.9829 & 0.9969 & 0.9541 + case 4 & @xmath193 & 1.0245 & 1.0071 & 0.8859 +    lccccc    ' '' ''    & @xmath209 & @xmath210 & @xmath211 & @xmath212 & @xmath213 + case 1 & 1.1825 & 0.6972 & 0.9849 & 0.3916 & 1.0008 + case 2 & 0.7447 & 0.8409 & 0.9999 & 0.5194 & 1.0058 + case 3 & 1.1320 & 0.9029 & 0.9999 & 0.5530 & 1.0007 + case 4 & 1.2453 & 0.8575 & 0.8664 & 0.4959 & 1.0103 +    in figure [ fig : imrtrsloct ] we present one slice of the ct of case 1 with the dose distribution resulting from the solution of superiorized lo with the robust parameter set @xmath214 .",
    "the corresponding dvh is shown in figure [ fig : imrtrslodvh ] .",
    "both the images and the dose matrix were created using cerr @xcite .",
    "note that the dose calculation via the dose matrix , which is used in the optimization algorithm , provides only an approximation to the dose values calculated by cerr which are depicted in figure [ fig : imrtrsloct ] and [ fig : imrtrslodvh ] .",
    "thus the dvh curves show e.g. that dose values below 66.5 gy occur in the ptv70 even though in the optimization the mathematical constraint ensuring that this does not happen is not violated .",
    "+   +   +   +    in figure [ fig : imrtobjfunctions ] we present the objective values of @xmath215 and @xmath154 plotted against the number of dosematrix - vector - multiplications for case 1 . to show the fundamental behavior of the superiorization method we intentionally did not give any actual numbers in the plots in figure [ fig : imrtobjfunctions ] .    in figure",
    "[ fig : imrtobjfunctionsa ] we can see how @xmath153 increases while we minimize @xmath216 in level 1 using classical lo .",
    "we see how the superiorization with respect to @xmath153 disturbs the minimization of @xmath217 and leads to @xmath218 being smaller than @xmath219 .",
    "figure [ fig : imrtobjfunctionsb ] shows that @xmath220 is a better starting point for the minimization of @xmath153 in level 2 than @xmath221 because its objective value of @xmath222 is significantly lower .",
    "this results in @xmath223 being notably smaller than @xmath224 .    due to the correlation of the clinical goals there are relatively few auxiliary cfps to be solved in optimization level 2 for all cases . when superiorization is applied ( both with optimal and robust @xmath173 ) we even get so close to the minimum of @xmath166 at the end of level 1 that for these parameter pairs the cfp in level 2 turns inconsistent within less than @xmath225 or @xmath226 reductions of @xmath227 .",
    "therefore no superiorization is applied in level 2 and the objective value of @xmath228 is about the same for fast slo , robust slo and unsuperiorized lo .    as we can see from figure",
    "[ fig : imrtobjfunctionsc ] superiorization with respect to @xmath153 during level 1 can in some cases  depending on the correlation of the objective functions  reduce the value of @xmath168 and @xmath229 .",
    "similar behavior can be observed for the superiorized lo that uses the robust parameter set @xmath199 .",
    "we used lo as an intuitive approach to lower the complexity of decision making in imrt and to include a priori knowledge about the priorities of the decision maker .",
    "we combined this mco technique with the superiorization methodology to speed up the optimization in the next optimization level .",
    "this is not the only application for superiorization .",
    "one could for example choose as superiorization function @xmath230 a weighted sum of different evaluation functions than we did , or one could choose @xmath230 as a function that is not included in the lexicographic optimization model at all.in this work we identified the superiorization parameters manually .",
    "this process takes of course too much time to be of any practical use . to be able to find robust parameters that work for many optimization problems in an automated way",
    ", one should make sure to identify problems with similar tradeoffs among the objective functions and the superiorization function(s ) .    an issue with using projection methods to solve cfps is that in general we do not have a criterion that tells us whether a cfp is inconsistent .",
    "therefore we defined a maximum number @xmath131 of attempts , i.e.  simultaneous projections , to solve it . if the cfp can not be solved within @xmath131 simultaneous projections , we assume it to be inconsistent .",
    "this means that finding out that a cfp actually is inconsistent takes @xmath131 projections , which drastically increases the total number of projections .    on the other hand , low values of @xmath131 might not suffice to reach the actual minimum of an objective function under the given constraints , but only an approximation thereof .",
    "we decided to stop solving an optimization level @xmath24 after encountering the first inconsistent auxilary cfp and continue with level @xmath231 , accepting that we might miss the minimal value of @xmath129 accessible with the chosen value of @xmath131 by a certain gap ( which is bounded by the reduction strategy of @xmath136 , in our case by 10 @xmath232 ) .    with this strategy , the computationally expensive process of finding out that a cfp is inconsistent occurs ( at most ) once per optimization level .",
    "for this reason it is a big advantage to know the objective values of the optimal solution a priori as in example [ toyex ] .",
    "in general we do not have this information but as in our calculations for the imrt cases , sometimes it is possible to exploit a priori information about the objective functions to define a stopping criterion that is computationally cheap to evaluate .",
    "a question arising in the context of optimization problems is how algorithm 1 performs compared to other solvers . in the following , we will refer to algorithm 1 as ( superiorized ) projection solver .",
    "we implemented the projection solver in matlab . to keep computation time for algebraic operations comparable we chose to compare our solutions to the solutions of matlab s fmincon function using the interior point solver .",
    "we did our calculations in matlab 2015b on an intel core i7 - 5600u processor with 2.6 ghz and 8 gb ram .",
    "matlab s interior point solver took 16904 to 30690 seconds to solve the lexicographic optimization problem for our four cases . within a maximum of 1000 iterations per optimization level it yielded solutions which were slightly infeasible ( maximal constraint violation of @xmath233 - @xmath234 ) . due to the nature of the evaluation functions this infeasibility had no major effect on the dose values .",
    "these solutions were still acceptable from a medicinal point of view .    for a detailed comparison of computation time see table [ tab : matlabsolvercomparisoncomptime ] .",
    "lcccc    ' '' ''    & ip & proj & robustsproj & fastsproj + case 1 & 30689.8 & 55.3 & 50.8 & 49.8 + case 2 & 17649.8 & 14.4 & 12.8 & 9.5 + case 3 & 26596.2 & 13.9 & 13.6 & 12.9 + case 4 & 16903.9 & 14.8 & 13.0 & 13.5 +    all iterates produced by the projection solver are feasible by construction of the algorithm .",
    "this enables us to decide how much computational effort we want to put into the calculations - of course at the expense of mathematical solution quality .    for our calcuations",
    "we chose the maximum number @xmath235 of simultaneous projections , to solve a given cfp for th projection solver .",
    "when comparing the resulting solutions to the ones produced by matlab s solver ( see tables [ tab : matlabsolvercomparisonobjvalues ] and [ tab : projsolvercomparisonobjvalues ] ) it becomes apparent that they are suboptimal .",
    "solutions with equal or smaller objective function values can be found by the projection solver , too , if @xmath131 is raised .",
    "lcccc    ' '' ''    & @xmath236 & @xmath237 & @xmath238 & maximal constraint + & & & & violation +    ' '' ''    case 1 & 9.1529@xmath239 & 425 & 145 & 9.1419@xmath240 + case 2 & 3.6419@xmath241 & 363 & 795 & 7.2779@xmath240 + case 3 & 0.3834 & 172 & 371 & 9.8338@xmath241 + case 4 & 0.4778 & 66 & 382 & 9.6511@xmath242 +    lcccc    ' '' ''    & @xmath236 & @xmath237 & @xmath238 & maximal constraint + & & & & violation +    ' '' ''    case 1 & @xmath243 & 720 & 97 & @xmath244 + case 2 & @xmath243 & 502 & 1725 & @xmath244 + case 3 & @xmath243 & 1477 & 1918 & @xmath244 + case 4 & @xmath243 & 1318 & 851 & @xmath244 +    the dvh curves of case 1 in figure [ fig : imrtsolvercomparison ] show that the additional 30639 seconds of computation time the matlab solver takes compared to robustsproj result in an improvement of 3 gy in the average dose value of the ldp ( whose dose evaluation function is contained in @xmath245 ) at the cost of 2 gy increase in the average dose value of non - tumor tissue 1 ( whose dose evaluation function is contained in @xmath246 and thus considered less important ) .    for case 1",
    ", the matlab solver yields a solution with has a lower evaluation function value corresponding to the ldp than the projection solver .",
    "because in case 1 the ldp has a relatively big weight @xmath247 in , this difference is amplified .",
    "we observed similar behavior with the other imrt cases .    concerning the question of performance compared to other solvers",
    ", we observe that the projection solver with @xmath235 offers a fast way to reach a solution that is in many aspects already as good as the solution produced by matlab s solver .",
    "if one is willing to invest additional hours of computation time , however , matlab s solver offers solutions with lower objective function values .",
    "we demonstrated the concept of superiorization in combination with an mco method on a simple example as well as on four imrt head neck cases .",
    "we showed that for lo we can speed up the optimization process by providing a better starting point for the next level of optimization .",
    "we found a common set of superiorization parameters which yielded robust results for all four cases .",
    "we thank the department of radiation oncology ( head : prof . dr . c. belka ) of the ludwig - maximilians university ( lmu ) , munich , germany , for providing the clinical data for our calculations and visualizations .",
    "d. butnariu , r. davidi , g. t. herman and i. g. kazantsev , stable convergence behavior under summable perturbations of a class of projection methods for convex feasibility and optimization problems , _ ieee journal of selected topics in signal processing _ * 1 * ( 2007 ) , 540547 .      y. censor , w. chen , p. l. combettes , r. davidi and g. t. herman , on the effectiveness of projection methods for convex feasibility problems with linear inequality constraints , _ computational optimization and applications _ * 51 * ( 2012 ) , 10651088 .      y. censor , r. davidi , g. t. herman , r. w. schulte and l. tetruashvili , projected subgradient minimization versus superiorization , _ journal of optimization theory and applications _ * 160 * ( 2014 ) , 730747 .",
    "r. davidi , y. censor , r. w. schulte , s. geneser , and l. xing , feasibility - seeking and superiorization algorithms applied to inverse treatment planning in radiation therapy , _ contemporary mathematics _ * 636 * ( 2015 ) , 8392 .                  w. jin , y. censor and m. jiang , a heuristic superiorization - like approach to bioluminescence , _ international federation for medical and biological engineering ( ifmbe ) proceedings _ * 39 * ( 2013 ) , 10261029 .        k. izui , t. yamada and s. nishiwaki , a gradient - based multiobjective optimization technique using an adaptive weighting method , _ 10th world congress on structural and multidisciplinary optimization _",
    "may 1924 , 2013 , orlando , florida , usa .        k .- h .",
    "kfer , m. monz , a. scherrer , p. sss , f. alonso , a. azizi sultan , t. bortfeld and c. thieke , multicriteria optimization in intensity modulated radiotherapy planning , _ springer optimization and its applications , ( handbook of optimization in medicine ) _ * 30 * ( 2009 ) , 123168 .",
    "t. long , m. matuszak , m. feng , b. a. fraass , r. k. ten haken and h. e. romeijn , sensitivity analysis for lexicographic ordering in radiation therapy treatment planning , _ medical physics _",
    "* 39 * ( 2012 ) , 34453455 ."
  ],
  "abstract_text": [
    "<S> multicriteria optimization problems occur in many real life applications , for example in cancer radiotherapy treatment and in particular in intensity modulated radiation therapy ( imrt ) . in this work we focus on optimization problems with multiple objectives that are ranked according to their importance . </S>",
    "<S> we solve these problems numerically by combining lexicographic optimization with our recently proposed level set scheme , which yields a sequence of auxiliary convex feasibility problems ; solved here via projection methods . </S>",
    "<S> the projection enables us to combine the newly introduced superiorization methodology with multicriteria optimization methods to speed up computation while guaranteeing convergence of the optimization . </S>",
    "<S> we demonstrate our scheme with a simple 2d academic example ( used in the literature ) and also present results from calculations on four real head neck cases in imrt ( radiation oncology of the ludwig - maximilians university , munich , germany ) for two different choices of superiorization parameter sets suited to yield fast convergence for each case individually or robust behavior for all four cases .    </S>",
    "<S> @xmath0 optimization department , fraunhofer - itwm , kaiserslautern 67663 , germany + @xmath1 mathematics department , ort braude college , karmiel 2161002 , israel    e - mail : esther.bonacker@itwm.fraunhofer.de and avivg@braude.ac.il    _ keywords _ : superiorization , projection methods , feasibility problems , multicriteria optimization , iterative methods + _ msc _ : 65k10 , 65k15 , 90c25 , 90c90 + </S>"
  ]
}