{
  "article_text": [
    "with the advent of the internet , many machine learning applications are faced with very large and inherently high - dimensional datasets , resulting in challenges in scaling up training algorithms and storing the data . especially in the context of search and machine translation , corpus sizes used in industrial practice",
    "have long exceeded the main memory capacity of single machine .",
    "for example , @xcite experimented with a dataset with potentially 16 trillion ( @xmath1 ) unique features .",
    "@xcite discusses training sets with ( on average ) @xmath2 items and @xmath3 distinct features , requiring novel algorithmic approaches and architectures . as a consequence ,",
    "there has been a renewed emphasis on scaling up machine learning techniques by using massively parallel architectures ; however , methods relying solely on parallelism can be expensive ( both with regards to hardware requirements and energy costs ) and often induce significant additional communication and data distribution overhead .    this work approaches the challenges posed by large datasets by leveraging techniques from the area of _ similarity search _",
    "@xcite , where similar increase in dataset sizes has made the storage and computational requirements for computing exact distances prohibitive , thus making data representations that allow compact storage and efficient approximate distance computation necessary .",
    "the method of @xmath0-bit minwise hashing  @xcite is a very recent progress for efficiently ( in both time and space ) computing _ resemblances _ among extremely high - dimensional ( e.g. , @xmath4 ) binary vectors . in this paper , we show that @xmath0-bit minwise hashing can be seamlessly integrated with linear support vector machine ( svm )  @xcite and logistic regression solvers . in  @xcite , the authors addressed a critically important problem of training linear svm when the data can not fit in memory . in this paper",
    ", our work also addresses the same problem using a very different approach .      in the context of search , a standard procedure to represent documents ( e.g. , web pages ) is to use @xmath5-shingles ( i.e. , @xmath5 contiguous words ) , where @xmath5 can be as large as 5 ( or 7 ) in several studies  @xcite .",
    "this procedure can generate datasets of extremely high dimensions .",
    "for example , suppose we only consider @xmath6 common english words .",
    "using @xmath7 may require the size of dictionary @xmath8 to be @xmath9 .",
    "in practice , @xmath10 often suffices , as the number of available documents may not be large enough to exhaust the dictionary . for @xmath5-shingle data , normally only abscence / presence ( 0/1 ) information is used , as it is known that word frequency distributions within documents approximately follow a power - law  @xcite , meaning that most single terms occur rarely , thereby making a @xmath5-shingle unlikely to occur more than once in a document .",
    "interestingly , even when the data are not too high - dimensional , empirical studies  @xcite achieved good performance with binary - quantized data .",
    "when the data can fit in memory , linear svm is often extremely efficient after the data are loaded into the memory .",
    "it is however often the case that , for very large datasets , the data loading time dominates the computing time for training the svm  @xcite .",
    "a much more severe problem arises when the data can not fit in memory .",
    "this situation can be very common in practice .",
    "the publicly available _ webspam _ dataset needs about 24 gb disk space ( in libsvm input data format ) , which exceeds the memory capacity of many desktop pcs .",
    "note that _ webspam _ , which contains only 350,000 documents represented by 3-shingles , is still a small dataset compared to the industry applications  @xcite .",
    "we propose a solution which leverages _ b - bit minwise hashing_. our approach assume the data vectors are binary , very high - dimensional , and relatively sparse , which is generally true of text documents represented via shingles .",
    "we apply @xmath0-bit minwise hashing to obtain a compact representation of the original data . in order to use the technique for efficient learning",
    ", we have to address several issues :    * we need to prove that the matrices generated by @xmath0-bit minwise hashing are indeed positive definite , which will provide the solid foundation for our proposed solution . *",
    "if we use @xmath0-bit minwise hashing to estimate the resemblance , which is nonlinear , how can we effectively convert this nonlinear problem into a linear problem ? * compared to other hashing techniques such as random projections , count - min ( cm ) sketch  @xcite , or vowpal wabbit ( vw )  @xcite , does our approach exhibits advantages ?",
    "it turns out that our proof in the next section that @xmath0-bit hashing matrices are positive definite naturally provides the construction for converting the otherwise nonlinear svm problem into linear svm .",
    "+ @xcite proposed solving the memory bottleneck by partitioning the data into blocks , which are repeatedly loaded into memory as their approach updates the model coefficients .",
    "however , the computational bottleneck is still at the memory because loading the data blocks for many iterations consumes a large number of disk i / os .",
    "clearly , one should note that our method is not really a competitor of the approach in  @xcite .",
    "in fact , both approaches may work together to solve extremely large problems .",
    "_ minwise hashing _  @xcite has been successfully applied to a very wide range of real - world problems especially in the context of search  @xcite , for efficiently computing set similarities .",
    "minwise hashing mainly works well with binary data , which can be viewed either as 0/1 vectors or as sets .",
    "given two sets , @xmath11 , a widely used ( normalized ) measure of similarity is the _ resemblance _ @xmath12 : @xmath13 in this method , one applies a random permutation @xmath14 on @xmath15 and @xmath16 .",
    "the collision probability is simply @xmath17 one can repeat the permutation @xmath18 times : @xmath19 , @xmath20 , ... ,",
    "@xmath21 to estimate @xmath12 without bias , as @xmath22    the common practice of minwise hashing is to store each hashed value , e.g. , @xmath23 and @xmath24 , using 64 bits  @xcite .",
    "the storage ( and computational ) cost will be prohibitive in truly large - scale ( industry ) applications  @xcite .",
    "b - bit minwise hashing _",
    "@xcite provides a strikingly simple solution to this ( storage and computational ) problem by storing only the lowest @xmath0 bits ( instead of 64 bits ) of each hashed value . for convenience ,",
    "we define the minimum values under @xmath25 : @xmath26 and @xmath27 , and define @xmath28th lowest bit of @xmath29 , and @xmath30th lowest bit of @xmath31 .",
    "@xcite[the_basic ] assume @xmath32 is large .",
    "@xmath33^{2^b-1}}{1-\\left[1-r_1\\right]^{2^b}},\\hspace{1 in } a_{2,b } = \\frac{r_2\\left[1-r_2\\right]^{2^b-1}}{1-\\left[1-r_2\\right]^{2^b}}.\\box\\end{aligned}\\ ] ]    this ( approximate ) formula ( [ eqn_basic ] ) is remarkably accurate , even for very small @xmath32 .",
    "some numerical comparisons with the exact probabilities are provided in appendix  [ app_basic_error ] .",
    "we can then estimate @xmath34 ( and @xmath12 ) from @xmath18 independent permutations : @xmath19 , @xmath20 , ... ,",
    "@xmath21 , @xmath35@xmath36 ^ 2 } = \\frac{1}{k}\\frac{\\left[c_{1,b}+(1-c_{2,b})r\\right]\\left[1-c_{1,b}-(1-c_{2,b})r\\right]}{\\left[1-c_{2,b}\\right]^2}\\end{aligned}\\ ] ] we will show that we can apply @xmath0-bit hashing for learning without explicitly estimating @xmath12 from ( [ eqn_basic ] ) .",
    "this section proves some theoretical properties of matrices generated by resemblance , minwise hashing , or @xmath0-bit minwise hashing , which are all positive definite matrices .",
    "our proof not only provides a solid theoretical foundation for using @xmath0-bit hashing in learning , but also illustrates our idea behind the construction for integrating @xmath0-bit hashing with linear learning algorithms .",
    "+ * definition * : a symmetric @xmath37 matrix @xmath38 satisfying @xmath39 , for all real vectors @xmath40 is called _ positive definite ( pd)_. note that here we do not differentiate pd from _ nonnegative definite_.    [ thm_pd ] consider @xmath41 sets @xmath15 , @xmath16 , ... , @xmath42 .",
    "apply one permutation @xmath25 to each set and define @xmath43 .",
    "the following three matrices are all pd .    1 .   the _ resemblance matrix _ @xmath44 , whose @xmath45-th entry is the resemblance between set @xmath46 and set @xmath47 : @xmath48 2 .",
    "the _ minwise hashing matrix _",
    "@xmath49 : @xmath50 3 .   the _ b - bit minwise hashing matrix _",
    "@xmath51 : @xmath52 , where @xmath53 is the @xmath54-th lowest bit of @xmath55 .",
    "consequently , consider @xmath18 independent permutations and denote @xmath56 the b - bit minwise hashing matrix generated by the @xmath57-th permutation .",
    "then the summation @xmath58 is also pd . + * proof : *   a matrix @xmath59 is pd if it can be written as an inner product @xmath60 . because @xmath61 @xmath62 is the inner product of two d - dim vectors .",
    "thus , @xmath63 is pd .",
    "similarly , the b - bit minwise hashing matrix @xmath64 is pd because @xmath65    the resemblance matrix @xmath66 is pd because @xmath67 and @xmath62 is the @xmath45-th element of the pd matrix @xmath63 .",
    "note that the expectation is a linear operation .",
    "@xmath68    our proof that the @xmath0-bit minwise hashing matrix @xmath64 is pd provides us with a simple strategy to expand a nonlinear ( resemblance ) kernel into a linear ( inner product ) kernel .",
    "after concatenating the @xmath18 vectors resulting from ( [ eqn_m_b ] ) , the new ( binary ) data vector after the expansion will be of dimension @xmath69 with exactly @xmath18 ones .",
    "linear algorithms such as linear svm and logistic regression have become very powerful and extremely popular .",
    "representative software packages include svm@xmath70  @xcite , pegasos  @xcite , bottou s sgd svm  @xcite , and liblinear  @xcite .",
    "given a dataset @xmath71 , @xmath72 , @xmath73 , the @xmath74-regularized linear svm solves the following optimization problem : @xmath75 and the @xmath74-regularized logistic regression solves a similar problem : @xmath76 here @xmath77 is an important penalty parameter .",
    "since our purpose is to demonstrate the effectiveness of our proposed scheme using @xmath0-bit hashing , we simply provide results for a wide range of @xmath78 values and assume that the best performance is achievable if we conduct cross - validations .    in our approach , we apply @xmath18 independent random permutations on each feature vector @xmath79 and store the lowest @xmath0 bits of each hashed value .",
    "this way , we obtain a new dataset which can be stored using merely @xmath80 bits . at run - time , we expand each new data point into a @xmath69-length vector .",
    "+ for example , suppose @xmath81 and the hashed values are originally @xmath82 , whose binary digits are @xmath83 .",
    "consider @xmath84 .",
    "then the binary digits are stored as @xmath85 ( which corresponds to @xmath86 in decimals ) . at run - time , we need to expand them into a vector of length @xmath87 , to be @xmath88 , which will be the new feature vector fed to a solver : @xmath89    clearly , this expansion is directly inspired by the proof that the @xmath0-bit minwise hashing matrix is pd in theorem  [ thm_pd ] .",
    "note that the total storage cost is still just @xmath80 bits and each new data vector ( of length @xmath69 ) has exactly @xmath18 1 s .",
    "also , note that in this procedure we actually do not explicitly estimate the resemblance @xmath12 using ( [ eqn_basic ] ) .",
    "our experimental settings follow the work in  @xcite very closely . the authors of",
    "@xcite conducted experiments on three datasets , of which the _ webspam _ dataset is public and reasonably high - dimensional ( @xmath90 , @xmath91 ) .",
    "therefore , our experiments focus on _ webspam_. following  @xcite , we randomly selected @xmath92 of samples for testing and used the remaining @xmath93 samples for training .",
    "we chose liblinear as the tool to demonstrate the effectiveness of our algorithm .",
    "all experiments were conducted on workstations with xeon(r ) cpu ( w5590@3.33ghz ) and 48 gb ram , under windows 7 system .",
    "thus , in our case , the original data ( about 24 gb in libsvm format ) fit in memory . in applications for which the data do not fit in memory",
    ", we expect that @xmath0-bit hashing will be even more substantially advantageous , because the hashed data are relatively very small .",
    "in fact , our experimental results will show that for this dataset , using @xmath94 and @xmath95 can achieve the same testing accuracy as using the original data .",
    "the effective storage for the reduced dataset ( with 350k examples , using @xmath94 and @xmath95 ) would be merely about 70 mb .",
    "we implemented a new resemblance kernel function and tried to use libsvm to train the _",
    "webspam _ dataset .",
    "we waited for * * over one week * * but libsvm still had not output any results .",
    "fortunately , using @xmath0-bit minswise hashing to estimate the resemblance kernels , we were able to obtain some results .",
    "for example , with @xmath96 and @xmath95 , the training time of libsvm ranged from 1938 seconds ( @xmath97 ) to 13253 seconds ( @xmath98 ) .",
    "in particular , when @xmath99 , the test accuracies essentially matched the best test results given by liblinear on the original _ webspam _ data .    therefore , there is a significant benefit of _ data reduction _ provided by @xmath0-bit minwise hashing , for training nonlinear svm .",
    "this experiment also demonstrates that it is very important ( and fortunate ) that we are able to transform this nonlinear problem into a linear problem .",
    "since there is an important tuning parameter @xmath78 in linear svm and logistic regression , we conducted our extensive experiments for a wide range of @xmath78 values ( from @xmath100 to @xmath101 ) with fine spacings in @xmath102 $ ] .",
    "we mainly experimented with @xmath97 to @xmath98 , and @xmath103 , 2 , 4 , 8 , and 16 .",
    "figures  [ fig_acc ] ( average ) and  [ fig_acc_std ] ( std , standard deviation ) provide the test accuracies .",
    "figure  [ fig_acc ] demonstrates that using @xmath104 and @xmath105 achieves about the same test accuracies as using the original data . since our method is randomized , we repeated every experiment 50 times .",
    "we report both the mean and std values .",
    "figure  [ fig_acc_std ] illustrates that the stds are very small , especially with @xmath106 . in other words ,",
    "our algorithm produces stable predictions . for this dataset ,",
    "the best performances were usually achieved when @xmath107 .    compared with the original training time ( about 100 seconds )",
    ", we can see from figure  [ fig_training ] that our method only need about @xmath108 seconds near @xmath96 ( about 3 seconds for @xmath95 ) .",
    "note that here the training time did not include the data loading time .",
    "loading the original data took about 12 minutes while loading the hashed data took only about 10 seconds .",
    "of course , there is a cost for processing ( hashing ) the data , which we find is efficient , confirming prior studies  @xcite .",
    "in fact , data processing can be conducted during data collection , as is the standard practice in search . in other words , prior to conducting the learning procedure , the data may be already processed and stored by ( @xmath0-bit ) minwise hashing , which can be used for multiple tasks including learning , clustering , duplicate detection , near - neighbor search , etc . +    compared with the original testing time ( about @xmath109 seconds ) , we can see from figure  [ fig_testing ] that the testing time of our method is merely about 1 or 2 seconds .",
    "note that the testing time includes both the data loading time and computing time , as designed by liblinear .",
    "the efficiency of testing may be very important in practice , for example , when the classifier is deployed in an user - facing application ( such as search ) , while the cost of training or pre - processing ( such as hashing ) may be less critical and can often be conducted off - line .",
    "figure  [ fig_acc_logit ] presents the test accuracies and figure  [ fig_training_logit ] presents the training time using logistic regression . again , with @xmath105 ( or even @xmath110 and @xmath104 , @xmath0-bit minwise hashing can achieve the same test accuracies as using the original data .",
    "figure  [ fig_acc_std_logit ] presents the standard deviations , which again verify that our algorithm produces stable predictions for logistic regression .    from figure  [ fig_training_logit ]",
    ", we can see that the training time is substantially reduced , from about 1000 seconds to about @xmath111 seconds only ( unless @xmath112 and @xmath18 is large ) .",
    "+ in summary , it appears @xmath0-bit hashing is highly effective in reducing the data size and speeding up the training ( and testing ) , for both ( nonlinear and linear ) svm and logistic regression .",
    "we notice that when using @xmath112 , the training time can be much larger than using @xmath113 .",
    "interestingly , we find that @xmath0-bit hashing can be combined with _",
    "vowpal wabbit ( vw )",
    "_  @xcite to further reduce the training time , especially when @xmath0 is large .",
    "the two methods , random projections  @xcite and vowpal wabbit ( vw )  @xcite are not limited to binary data ( although for ultra high - dimensional used in the context of search , the data are often binary ) .",
    "the vw algorithm is also related to the count - min sketch  @xcite . in this paper , we use  vw  particularly for the algorithm in  @xcite .    for convenience ,",
    "we denote two @xmath32-dim data vectors by @xmath114 .",
    "again , the task is to estimate the inner product @xmath115 .      the general idea is to multiply the data vectors , e.g. , @xmath116 and @xmath117 , by a random matrix @xmath118 , where @xmath119 is sampled i.i.d . from the following generic distribution with  @xcite @xmath120 we must have @xmath121 because @xmath122 .",
    "this generates two @xmath18-dim vectors , @xmath123 and @xmath124 : @xmath125    the general distributions which satisfy ( [ eqn_r_ij ] ) includes the standard normal distribution ( in this case , @xmath126 ) and the `` sparse projection '' distribution specified as @xmath127    @xcite provided the following unbiased estimator @xmath128 of @xmath129 and the general variance formula : @xmath130\\end{aligned}\\ ] ] which means @xmath131 achieves the smallest variance .",
    "the only elementary distribution we know that satisfies ( [ eqn_r_ij ] ) with @xmath131 is the two point distribution in @xmath132 with equal probabilities , i.e. , ( [ eqn_sparse_r ] ) with @xmath131 .",
    "again , in this paper , `` vw '' always refers to the particular algorithm in  @xcite .",
    "vw may be viewed as a `` bias - corrected '' version of the count - min ( cm ) sketch algorithm  @xcite . in the original cm algorithm , the key step is to independently and uniformly hash elements of the data vectors to buckets @xmath133 and the hashed value is the sum of the elements in the bucket .",
    "that is @xmath134 with probability @xmath135 , where @xmath136 . for convenience ,",
    "we introduce an indicator function : @xmath137 which allow us to write the hashed data as @xmath138    the estimate @xmath139 is ( severely ) biased for the task of estimating the inner products .",
    "the original paper  @xcite suggested a `` count - min '' step for positive data , by generating multiple independent estimates @xmath140 and taking the minimum as the final estimate . that step can not remove the bias and makes the analysis ( such as variance ) very difficult .",
    "here we should mention that the bias of cm may not be a major issue in other tasks such as sparse recovery ( or `` heavy - hitter '' , or `` elephant detection '' , by various communities ) .",
    "+ @xcite proposed a creative method for bias - correction , which consists of pre - multiplying ( element - wise ) the original data vectors with a random vector whose entries are sampled i.i.d . from the two - point distribution in @xmath141 with equal probabilities , which corresponds to @xmath142 in ( [ eqn_sparse_r ] ) .    here",
    ", we consider a more general situation by considering any @xmath121 . after",
    "applying multiplication and hashing on @xmath116 and @xmath117 as in @xcite , the resultant vectors @xmath143 and @xmath144 are @xmath145 where @xmath146 is defined as in ( [ eqn_r_ij ] ) , i.e. , @xmath147 .",
    "we have the following lemma .",
    "[ lem_vw ] @xmath148\\end{aligned}\\ ] ] * proof : *  see appendix  [ proof_lem_vw].@xmath68    interestingly , the variance ( [ eqn_var_vw ] ) says we do need @xmath131 , otherwise the additional term @xmath149 will not vanish even as the sample size @xmath150 .",
    "in other words , the choice of random distribution in vw is essentially the only option if we want to remove the bias by pre - multiplying the data vectors ( element - wise ) with a vector of random variables .",
    "of course , once we let @xmath131 , the variance ( [ eqn_var_vw ] ) becomes identical to the variance of random projections ( [ eqn_var_rp ] ) .",
    "we implemented vw ( which , in this paper , always refers to the algorithm developed in  @xcite ) and tested it on the same webspam dataset .",
    "figure  [ fig_vw ] shows that @xmath0-bit minwise hashing is substantially more accurate ( at the same sample size @xmath18 ) and requires significantly less training time ( to achieve the same accuracy ) .",
    "for example , @xmath151-bit minwise hashing with @xmath94 achieves about the same test accuracy as vw with @xmath152 .",
    "note that we only stored the non - zeros of the hashed data generated by vw .",
    "this empirical finding is not surprising , because the variance of @xmath0-bit hashing is usually substantially smaller than the variance of vw ( and random projections ) . in appendix",
    "[ app_compare_vw ] , we show that , at the same storage cost , @xmath0-bit hashing usually improves vw by 10- to 100-fold , by assuming each sample of vw requires 32 bits storage .",
    "of course , even if vw only stores each sample using 16 bits , an improvement of 5- to 50-fold would still be very substantial .",
    "note that this comparison makes sense for the purpose of _ data reduction _",
    ", i.e. , the sample size @xmath18 is substantially smaller than the number of non - zeros in the original ( massive ) data .",
    "+ there is one interesting issue here . unlike random projections ( and minwise hashing )",
    ", vw is a _ sparsity - preserving _ algorithm , meaning that in the resultant sample vector of length @xmath18 , the number of non - zeros will not exceed the number of non - zeros in the original vector .",
    "in fact , it is easy to see that the fraction of zeros in the resultant vector would be ( at least ) @xmath153 , where @xmath40 is the number of non - zeros in the original data vector . when @xmath154 , then @xmath155 . in other words , if our goal is _ data reduction _ ( i.e. , @xmath156 ) , then the hashed data by vw are dense .",
    "+ in this paper , we mainly focus on _ data reduction_. as discussed in the introduction , for many industry applications , the relatively sparse datasets are often massive in the absolute scale and we assume we can not store all the non - zeros .",
    "in fact , this is the also one of the basic motivations for developing minwise hashing .",
    "however , the case of @xmath157 can also be interesting and useful in our work .",
    "this is because vw is an excellent tool for achieving _ compact indexing _ due to the _ sparsity - preserving _ property .",
    "basically , we can let @xmath18 be very large ( like @xmath158 in  @xcite ) .",
    "as the original dictionary size @xmath32 is extremely large ( e.g. , @xmath4 ) , even @xmath159 will be a meaningful reduction of the indexing . of course , using a very large @xmath18 will not be useful for the purpose of _ data reduction_. +",
    "in our algorithm , we reduce the original massive data to @xmath160 bits only , where @xmath41 is the number of data points . with ( e.g. , ) @xmath94 and @xmath95",
    ", our technique achieves a huge _ data reduction_. in the run - time , we need to expand each data point into a binary vector of length @xmath161 with exactly @xmath18 1 s",
    ". if @xmath0 is large like @xmath162 , the new binary vectors will be highly sparse .",
    "in fact , in figure  [ fig_training ] and figure  [ fig_training_logit ] , we can see that when using @xmath112 , the training time becomes substantially larger than using @xmath113 ( especially when @xmath18 is large ) .    on the other hand ,",
    "once we have expanded the vectors , the task is merely computing inner products , for which we can actually use vw .",
    "therefore , in the run - time , after we have generated the sparse binary vectors of length @xmath163 , we hash them using vw with sample size @xmath164 ( to differentiate from @xmath18 ) .",
    "how large should @xmath164 be ? lemma  [ lem_b - bit+vw ] may provide some insights",
    ". + recall section  [ sec_minwise ] provides the estimator , denoted by @xmath165 , of the resemblance @xmath12 , using @xmath0-bit minwise hashing . now , suppose we first apply vw hashing with size @xmath164 on the binary vector of length @xmath163 before estimating @xmath12 , which will introduce some additional randomness ( on top of @xmath0-bit hashing ) .",
    "we denote the new estimator by @xmath166 .",
    "lemma  [ lem_b - bit+vw ] provides its theoretical variance .",
    "[ lem_b - bit+vw ] @xmath167",
    "^ 2}\\left(1+p_b^2-\\frac{p_b(1+p_b)}{k}\\right),\\\\\\notag & \\hspace{0.73 in } = \\frac{1}{k}\\frac{p_b(1-p_b)}{\\left[1-c_{2,b}\\right]^2 } + \\frac{1}{m}\\frac{1+p_b^2}{\\left[1-c_{2,b}\\right]^2 } - \\frac{1}{mk}\\frac{p_b(1+p_b)}{\\left[1-c_{2,b}\\right]^2}\\end{aligned}\\ ] ] where @xmath168 ^ 2}$ ] is given by ( [ eqn_var_b ] ) and @xmath169 is the constant defined in theorem  [ the_basic ] . + * proof * :  the proof is quite straightforward , by following the conditional expectation formula : @xmath170 , and the conditional variance formula @xmath171 .",
    "recall , originally we estimate the resemblance by @xmath172}$ ] , where @xmath173 and @xmath174 is the number of matches in the two hashed data vectors of length @xmath18 generated by @xmath0-bit hashing . @xmath175 and @xmath176 .",
    "now , we apply vw ( of size @xmath164 and @xmath131 ) on the hashed data vectors to estimate @xmath174 ( instead of counting it exactly ) . we denote this estimates by @xmath177 and @xmath178 .",
    "because we know the vw estimate is unbiased , we have @xmath179 using the conditional variance formula and the variance of vw ( [ eqn_var_vw ] ) ( with @xmath131 ) , we obtain @xmath180\\right ) + var\\left(t\\right)\\right]\\\\\\notag = & \\frac{1}{k^2}\\left[\\frac{1}{m}\\left(k^2+kp_b(1-p_b)+k^2p_b^2 - 2kp_b\\right ) + kp_b(1-p_b)\\right]\\\\\\notag = & \\frac{p_b(1-p_b)}{k } + \\frac{1}{m}\\left(1+p_b^2-\\frac{p_b(1+p_b)}{k}\\right)\\end{aligned}\\ ] ] this completes the proof .",
    "@xmath68 +    compared to the original variance @xmath181 ^ 2}$ ] , the additional term @xmath182 ^ 2}$ ] in ( [ eqn_var_b - bit+vw ] ) can be relatively large if @xmath164 is not large enough . therefore , we should choose @xmath183 ( to reduce the additional variance ) and @xmath184 ( otherwise there is no need to apply this vw step ) .",
    "if @xmath112 , then @xmath185 may be a good trade - off , because @xmath186 .",
    "+ figure  [ fig_16-bit+vw ] provides an empirical study to verify this intuition .",
    "basically , as @xmath187 , using vw on top of 16-bit hashing achieves the same accuracies at using 16-bit hashing directly and reduces the training time quite noticeably .",
    "[ fig_16-bit+vw ]    we also experimented with combining 8-bit hashing with vw .",
    "we found that we need @xmath185 to achieve similar accuracies , i.e. , the additional vw step did not bring more improvement ( without hurting accuracies ) in terms of training speed when @xmath95 .",
    "this is understandable from the analysis of the variance in lemma  [ lem_b - bit+vw ] .",
    "minwise hashing has been widely used in ( search ) industry and @xmath0-bit minwise hashing requires only very minimal ( if any ) modifications ( by doing less work ) .",
    "thus , we expect @xmath0-bit minwise hashing will be adopted in practice .",
    "it is also well - understood in practice that we can use ( good ) hashing functions to very efficiently simulate permutations .    in many real - world scenarios ,",
    "the preprocessing step is not critical because it requires only one scan of the data , which can be conducted off - line ( or on the data - collection stage , or at the same time as n - grams are generated ) , and it is trivially parallelizable .",
    "in fact , because @xmath0-bit minwise hashing can substantially reduce the memory consumption , it may be now affordable to store considerably more examples in the memory ( after @xmath0-bit hashing ) than before , to avoid ( or minimize ) disk ios . once the hashed data have been generated , they can be used and re - used for many tasks such as supervised learning , clustering , duplicate detections , near - neighbor search , etc .",
    "for example , a learning task may need to re - use the same ( hashed ) dataset to perform many cross - validations and parameter tuning ( e.g. , for experimenting with many @xmath78 values in svm ) .",
    "+ nevertheless , there might be situations in which the preprocessing time can be an issue .",
    "for example , when a new unprocessed document ( i.e. n - grams are not yet available ) arrives and a particular application requires an immediate response from the learning algorithm , then the preprocessing cost might ( or might not ) be an issue .",
    "firstly , generating n - grams will take some time . secondly ,",
    "if during the session a disk io occurs , then the io cost will typically mask the cost of preprocessing for @xmath0-bit minwise hashing .",
    "note that the preprocessing cost for the vw algorithm can be substantially lower .",
    "thus , if the time for pre - processing is indeed a concern ( while the storage cost or test accuracies are not as much ) , one may want to consider using vw ( or _ very sparse random projections _",
    "@xcite ) for those applications .",
    "as data sizes continue to grow faster than the memory and computational power , machine - learning tasks in industrial practice are increasingly faced with training datasets that exceed the resources on a single server . a number of approaches have been proposed that address this by either scaling out the training process or partitioning the data , but both solutions can be expensive .    in this paper , we propose a compact representation of sparse , binary datasets based on @xmath0-bit minwise hashing .",
    "we show that the @xmath0-bit minwise hashing estimators are positive definite kernels and can be naturally integrated with learning algorithms such as svm and logistic regression , leading to dramatic improvements in training time and/or resource requirements .",
    "we also compare @xmath0-bit minwise hashing with the vowpal wabbit ( vw ) algorithm , which has the same variances as random projections .",
    "our theoretical and empirical comparisons illustrate that usually @xmath0-bit minwise hashing is significantly more accurate ( at the same storage ) than vw for binary data .",
    "interestingly , @xmath0-bit minwise hashing can be combined with vw to achieve further improvements in terms of training speed when @xmath0 is large ( e.g. , @xmath188 ) .",
    "10    dimitris achlioptas .",
    "database - friendly random projections : with binary coins . , 66(4):671687 , 2003 .",
    "alexandr andoni and piotr indyk .",
    "near - optimal hashing algorithms for approximate nearest neighbor in high dimensions . in _ commun .",
    "volume  51 , pages 117122 , 2008 .",
    "harald baayen . , volume  18 of _ text , speech and language technology_. kulver academic publishers , 2001 .",
    "michael bendersky and w.  bruce croft .",
    "finding text reuse on the web . in _ wsdm _ , pages 262271 , barcelona , spain , 2009 .",
    "leon bottou .",
    "available at http://leon.bottou.org/projects/sgd .",
    "andrei  z. broder . on the resemblance and containment of documents . in _ the compression and complexity of sequences _ , pages 2129 , positano , italy , 1997 .",
    "andrei  z. broder , steven  c. glassman , mark  s. manasse , and geoffrey zweig .",
    "syntactic clustering of the web . in _",
    "www _ , pages 1157  1166 , santa clara , ca , 1997 .",
    "gregory buehrer and kumar chellapilla .",
    "a scalable pattern mining approach to web graph compression with communities . in _",
    "wsdm _ , pages 95106 , stanford , ca , 2008 .",
    "olivier chapelle , patrick haffner , and vladimir  n. vapnik .",
    "support vector machines for histogram - based image classification .",
    ", 10(5):10551064 , 1999 .",
    "ludmila cherkasova , kave eshghi , charles b.  morrey iii , joseph tucek , and alistair  c. veitch . applying syntactic similarity algorithms for enterprise information management . in _",
    "kdd _ , pages 10871096 , paris , france , 2009 .",
    "flavio chierichetti , ravi kumar , silvio lattanzi , michael mitzenmacher , alessandro panconesi , and prabhakar raghavan . on compressing social networks .",
    "in _ kdd _ , pages 219228 , paris , france , 2009 .",
    "graham cormode and s.  muthukrishnan .",
    "an improved data stream summary : the count - min sketch and its applications .",
    ", 55(1):5875 , 2005 .",
    "yon dourisboure , filippo geraci , and marco pellegrini .",
    "extraction and classification of dense implicit communities in the web graph .",
    ", 3(2):136 , 2009 .",
    "rong - en fan , kai - wei chang , cho - jui hsieh , xiang - rui wang , and chih - jen lin .",
    "liblinear : a library for large linear classification .",
    ", 9:18711874 , 2008 .",
    "dennis fetterly , mark manasse , marc najork , and janet  l. wiener .",
    "a large - scale study of the evolution of web pages . in _ www _ , pages 669678 , budapest , hungary , 2003 .",
    "george forman , kave eshghi , and jaap suermondt .",
    "efficient detection of large - scale redundancy in enterprise file systems .",
    ", 43(1):8491 , 2009 .    sreenivas gollapudi and aneesh sharma . an axiomatic approach for result diversification . in _ www _ , pages 381390 , madrid , spain , 2009 .",
    "matthias hein and olivier bousquet .",
    "hilbertian metrics and positive definite kernels on probability measures . in _ aistats _ , pages 136143 , barbados , 2005 .",
    "cho - jui hsieh , kai - wei chang , chih - jen lin , s.  sathiya keerthi , and s.  sundararajan . a dual coordinate descent method for large - scale linear svm . in _ proceedings of the 25th international conference on machine learning _ ,",
    "icml , pages 408415 , 2008 .",
    "yugang jiang , chongwah ngo , and jun yang .",
    "towards optimal bag - of - features for object categorization and semantic video retrieval . in _ civr _ , pages 494501 , amsterdam , netherlands , 2007 .",
    "nitin jindal and bing liu .",
    "opinion spam and analysis . in _",
    "pages 219230 , palo alto , california , usa , 2008 .",
    "thorsten joachims . training linear svms in linear time . in _ kdd _ , pages 217226 , pittsburgh , pa , 2006 .",
    "konstantinos kalpakis and shilang tang .",
    "collaborative data gathering in wireless sensor networks using measurement co - occurrence .",
    ", 31(10):19791992 , 2008 .",
    "ping li , trevor  j. hastie , and kenneth  w. church . very sparse random projections . in _",
    "kdd _ , pages 287296 , philadelphia , pa , 2006 .",
    "ping li and arnd  christian knig . theory and applications b - bit minwise hashing . in _ commun .",
    "acm _ , to appear .",
    "ping li and arnd  christian .",
    "b - bit minwise hashing . in _ www _ , pages 671680 , raleigh , nc , 2010 .",
    "ping li , arnd  christian , and wenhao gui .",
    "b - bit minwise hashing for estimating three - way similarities . in _ nips _ , vancouver , bc , 2010 .",
    "gurmeet  singh manku , arvind jain , and anish  das sarma . etecting near - duplicates for web - crawling . in _ www _ , banff , alberta , canada , 2007 .",
    "marc najork , sreenivas gollapudi , and rina panigrahy .",
    "less is more : sampling the neighborhood graph makes salsa better and faster . in _ wsdm _ , pages 242251 ,",
    "barcelona , spain , 2009 .",
    "shai shalev - shwartz , yoram singer , and nathan srebro .",
    "pegasos : primal estimated sub - gradient solver for svm . in _ icml _ , pages 807814 , corvalis , oregon , 2007 .",
    "qinfeng shi , james petterson , gideon dror , john langford , alex smola , and s.v.n . vishwanathan .",
    "hash kernels for structured data .",
    ", 10:26152637 , 2009 .",
    "simon tong .",
    "lessons learned developing a practical large scale machine learning system .",
    "available at http://googleresearch.blogspot.com/2010/04/lessons-learned-developing-practical.html , 2008 .",
    "tanguy urvoy , emmanuel chauveau , pascal filoche , and thomas lavergne .",
    "tracking web spam with html style similarities .",
    ", 2(1):128 , 2008 .",
    "kilian weinberger , anirban dasgupta , john langford , alex smola , and josh attenberg .",
    "feature hashing for large scale multitask learning . in _ icml _ , pages 11131120 , 2009 .",
    "hsiang - fu yu , cho - jui hsieh , kai - wei chang , and chih - jen lin .",
    "large linear classification when data can not fit in memory . in _ kdd _ , pages 833842 , 2010 .",
    "note that the only assumption needed in the proof of theorem  [ the_basic ] is that @xmath32 is large , which is virtually always satisfied in practice .",
    "interestingly , ( [ eqn_basic ] ) is remarkably accurate even for very small @xmath32 .",
    "figure  [ fig_approximateerror ] shows that when @xmath189 , the absolute error caused by using ( [ eqn_basic ] ) is @xmath190 .",
    "the exact probability , which has no closed - form , can be computed by exhaustive enumerations for small @xmath32 .",
    "the vw algorithm  @xcite provides a bias - corrected version of the count - min ( cm ) sketch algorithm  @xcite .",
    "the key step in cm is to independently and uniformly hash elements of the data vectors to @xmath191 .",
    "that is @xmath192 with equal probabilities , where @xmath193 . for convenience ,",
    "we introduce the following indicator function : @xmath194    thus , we can denote the cm `` samples '' after the hashing step by @xmath195 and estimate the inner product by @xmath196 , whose expectation and variance can be shown to be @xmath197\\end{aligned}\\ ] ]    from the definition of @xmath198 , we can easily infer its moments , for example , @xmath199    the proof of the mean ( [ eqn_mean_cm ] ) is simple : @xmath200= \\sum_{i=1}^d u_{1,i}u_{2,i } + \\frac{1}{k}\\sum_{i\\neq j } u_{1,i}u_{2,j}.\\end{aligned}\\ ] ]    the variance ( [ eqn_var_cm ] is more complicated : @xmath201 the following expansions are helpful : @xmath202 ^ 2 = & \\sum_{i=1}^d a_i^2b_i^2 + \\sum_{i\\neq j } a_i^2b_j^2 + 2a_i^2b_ib_j+2b_i^2a_ia_j+2a_ib_ia_jb_j\\\\\\notag + & \\sum_{i\\neq j\\neq c } a_i^2b_jb_c+b_i^2a_ja_c + 4a_ib_ia_jb_c+\\sum_{i\\neq j\\neq c\\neq t}a_ib_ja_cb_t\\\\\\notag \\left[\\sum_{i\\neq j } a_i b_j\\right]^2 = &   \\sum_{i\\neq j } a_i^2b_j^2+a_ib_ia_jb_j + \\sum_{i\\neq j\\neq c } a_i^2b_jb_c+b_i^2a_ja_c + 2a_ib_ia_jb_c+\\sum_{i\\neq j\\neq c\\neq t}a_ib_ja_cb_t\\\\\\notag \\sum_{i=1}^da_ib_i\\sum_{i\\neq j } a_i b_j = & \\sum_{i\\neq j } a_i^2b_ib_j+b_i^2a_ia_j+\\sum_{i\\neq j\\neq c } a_i^2b_jb_c\\end{aligned}\\ ] ]    which , combined with the moments of @xmath198 , yield @xmath203 @xmath204\\\\\\notag = & ( k-1)\\left[\\frac{1}{k}\\sum_{i\\neq j } u_{1,i}u_{2,i}u_{1,j}u_{2,j}+\\frac{2}{k^2}\\sum_{i\\neq j \\neq c } u_{1,i}u_{2,i}u_{1,j}u_{2,c}+\\frac{1}{k^3}\\sum_{i\\neq j\\neq c\\neq t } u_{1,i}u_{2,j}u_{1,c}u_{2,t}\\right]\\end{aligned}\\ ] ]    @xmath205    therefore ,    @xmath206\\end{aligned}\\ ] ]      the nice approach proposed in the vw paper  @xcite is to pre - element - wise - multiply the data vectors with a random vector @xmath207 before taking the hashing operation .",
    "we denote the two resultant vectors ( samples ) by @xmath143 and @xmath144 respectively : @xmath208 where @xmath209 with equal probabilities . here",
    ", we provide a more general scheme by sampling @xmath146 from a sub - gaussian distribution with parameter @xmath57 and @xmath210 which include normal ( i.e. , @xmath126 ) and the distribution on @xmath211 with equal probabilities ( i.e. , @xmath131 ) as special cases .",
    "let @xmath212 .",
    "the goal is to show @xmath213\\end{aligned}\\ ] ]    we can use the previous results and the conditional expectation and variance formulas : @xmath214    @xmath215 , @xmath216 , @xmath217 , @xmath218 .",
    "@xmath219    as @xmath220 , we need to compute @xmath221 thus , @xmath222      by examining the expectation ( [ eqn_mean_cm ] ) , the bias of cm can be easily removed , because @xmath223\\end{aligned}\\ ] ] is unbiased with variance @xmath224,\\end{aligned}\\ ] ] which is essentially the same as the variance of vw .",
    "we compare vw ( and random projections ) with @xmath0-bit minwise hashing for the task of estimating inner products on binary data . with binary data ,",
    "i.e. , @xmath225 , we have @xmath226 , @xmath227 , @xmath228 .",
    "the variance ( [ eqn_var_vw ] ) ( by using @xmath131 ) becomes @xmath229    we can compare this variance with the variance of @xmath0-bit minwise hashing . because the variance ( [ eqn_var_b ] ) is for estimating the resemblance , we need to convert it into the variance for estimating the inner product @xmath129 using the relation : @xmath230      for @xmath0-bit minwise hashing , each sample is stored using only @xmath0 bits . for vw ( and random projections ) , we assume each sample is stored using 32 bits ( instead of 64 bits ) for two reasons : ( i ) for binary data , it would be very unlikely for the hashed value to be close to @xmath4 , even when @xmath10 ; ( ii ) unlike @xmath0-bit minwise hashing , which requires exact bit - matching in the estimation stage , random projections only need to compute the inner products for which it would suffice to store hashed values as ( double precision ) real numbers .      if @xmath233 , then @xmath0-bit minwise hashing is more accurate than binary random projections .",
    "equivalently , when @xmath233 , in order to achieve the same level of accuracy ( variance ) , @xmath0-bit minwise hashing needs smaller storage space than random projections .",
    "there are two issues we need to elaborate on :    1 .   here",
    ", we assume the purpose of using vw is for _ data reduction_. that is , @xmath18 is small compared to the number of non - zeros ( i.e. , @xmath234 , @xmath235 ) .",
    "we do not consider the case when @xmath18 is taken to be extremely large for the benefits of _ compact indexing _ without achieving _",
    "data reduction_. 2 .",
    "because we assume @xmath18 is small , we need to represent the sample with enough precision .",
    "that is why we assume each sample of vw is stored using 32 bits .",
    "in fact , since the ratio @xmath236 is usually very large ( e.g. , @xmath237 ) by using 32 bits for each vw sample , it will remain to be very large ( e.g. , @xmath238 ) even if we only need to store each vw sample using 16 bits .    without loss of generality",
    ", we can assume @xmath239 ( hence @xmath240 ) .",
    "figures  [ fig_gb8 ] to  [ fig_gb1 ] display the ratios ( [ eqn_g_vw ] ) for @xmath241 , respectively . in order to achieve high learning accuracies , @xmath0-bit minwise hashing requires @xmath242 ( or even 8) . in each figure , we plot @xmath243 for @xmath244 and full ranges of @xmath235 and @xmath129 .",
    "we can see that @xmath236 is much larger than one ( usually 10 to 100 ) , indicating the very substantial advantage of @xmath0-bit minwise hashing over random projections .",
    "note that the comparisons are essentially independent of @xmath32 .",
    "this is because in the variance of binary random projection ( [ eqn_g_vw ] ) the @xmath245 term is negligible compared to @xmath246 in binary data as @xmath32 is very large . to generate the plots",
    ", we used @xmath247 ( although practically @xmath32 should be much larger ) .",
    "+    * conclusion * :  our theoretical analysis has illustrated the substantial improvements of @xmath0-bit minwise hashing over the vw algorithm and random projections in binary data , often by 10- to @xmath248-fold .",
    "we feel such a large performance difference should be noted by researchers and practitioners in large - scale machine learning ."
  ],
  "abstract_text": [
    "<S> in this paper , we first demonstrate that @xmath0-bit minwise hashing , whose estimators are positive definite kernels , can be naturally integrated with learning algorithms such as svm and logistic regression . we adopt a simple scheme to transform the nonlinear ( resemblance ) kernel into linear ( inner product ) kernel ; and </S>",
    "<S> hence large - scale problems can be solved extremely efficiently . </S>",
    "<S> our method provides a simple effective solution to large - scale learning in massive and extremely high - dimensional datasets , especially when data do not fit in memory . </S>",
    "<S> + we then compare @xmath0-bit minwise hashing with the vowpal wabbit ( vw ) algorithm ( which is related the count - min ( cm ) sketch ) . </S>",
    "<S> interestingly , vw has the same variances as random projections . </S>",
    "<S> our theoretical and empirical comparisons illustrate that usually @xmath0-bit minwise hashing is significantly more accurate ( at the same storage ) than vw ( and random projections ) in binary data . </S>",
    "<S> furthermore , @xmath0-bit minwise hashing can be combined with vw to achieve further improvements in terms of training speed , especially when @xmath0 is large . </S>"
  ]
}