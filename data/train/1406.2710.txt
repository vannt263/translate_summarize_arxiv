{
  "article_text": [
    "distributed word representations have enjoyed success in several nlp tasks @xcite .",
    "more recently , the use of distributed representations have been extended to model concepts beyond the word level , such as sentences , phrases and paragraphs @xcite , entities and relationships @xcite and embeddings of semantic categories @xcite .    in this paper",
    "we propose a general framework for learning distributed representations of attributes : characteristics of text whose representations can be jointly learned with word embeddings .",
    "the use of the word attribute in this context is general .",
    "table @xmath0 illustrates several of the experiments we perform along with the corresponding notion of attribute .",
    "for example , an attribute can represent an indicator of the current sentence or language being processed .",
    "this allows us to learn sentence and language vectors , similar to the proposed model of @xcite .",
    "attributes can also correspond to side information , or metadata associated with text .",
    "for instance , a collection of blogs may come with information about the age , gender or industry of the author .",
    "this allows us to learn vectors that can capture similarities across metadata based on the associated body of text .",
    "the goal of this work is to show our notion of attribute vectors can achieve strong performance on a wide variety of nlp related tasks .    to capture these kinds of interactions between attributes and text",
    ", we propose the use of a third - order model where attribute vectors act as gating units to a word embedding tensor .",
    "that is , words are represented as a tensor consisting of several prototype vectors .",
    "given an attribute vector , a word embedding matrix can be computed as a linear combination of word prototypes weighted by the attribute representation . during training ,",
    "attribute vectors reside in a separate lookup table which can be jointly learned along with word features and the model parameters .",
    "this type of three - way interaction can be embedded into a neural language model , where the three - way interaction consists of the previous context , the attribute and the score ( or distribution ) of the next word after the context .    using a word embedding tensor gives rise to the notion of conditional word similarity .",
    "more specifically , the neighbours of word embeddings can change depending on which attribute is being conditioned on .",
    "for example , the word ` joy ' when conditioned on an author with the industry attribute ` religion ' appears near ` rapture ' and ` god ' but near ` delight ' and ` comfort ' when conditioned on an author with the industry attribute ` science ' .",
    "another way of thinking of our model would be the language analogue of @xcite .",
    "they used a factored conditional restricted boltzmann machine for modelling motion style defined by real or continuous valued style variables .",
    "when our factorization is embedded into a neural language model , it allows us to generate text conditioned on different attributes in the same manner as @xcite could generate motions from different styles . as we show in our experiments ,",
    "if attributes are represented by different books , samples generated from the model learn to capture associated writing styles from the author .",
    "multiplicative interactions have also been previously incorporated into neural language models .",
    "@xcite introduced a multiplicative model where images are used for gating word representations .",
    "our framework can be seen as a generalization of @xcite and in the context of their work an attribute would correspond to a fixed representation of an image . @xcite",
    "introduced a multiplicative recurrent neural network for generating text at the character level . in their model ,",
    "the character at the current timestep is used to gate the network s recurrent matrix .",
    "this led to a substantial improvement in the ability to generate text at the character level as opposed to a non - multiplicative recurrent network ."
  ],
  "abstract_text": [
    "<S> in this paper we propose a general framework for learning distributed representations of attributes : characteristics of text whose representations can be jointly learned with word embeddings . </S>",
    "<S> attributes can correspond to document indicators ( to learn sentence vectors ) , language indicators ( to learn distributed language representations ) , meta - data and side information ( such as the age , gender and industry of a blogger ) or representations of authors . </S>",
    "<S> we describe a third - order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence . </S>",
    "<S> this leads to the notion of conditional word similarity : how meanings of words change when conditioned on different attributes . </S>",
    "<S> we perform several experimental tasks including sentiment classification , cross - lingual document classification , and blog authorship attribution . </S>",
    "<S> we also qualitatively evaluate conditional word neighbours and attribute - conditioned text generation . </S>"
  ]
}