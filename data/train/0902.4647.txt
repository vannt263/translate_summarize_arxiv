{
  "article_text": [
    "the time - varying nature of the underlying channel is one of the most significant design challenges in wireless communication systems . in particular",
    ", real - time media traffic typically has a stringent delay constraint , so the exploitation of long blocklength frames is infeasible and the entire frame may fall into deep fading channel states . furthermore , the receiver may have limited resources to feed the estimated channel state information back to the transmitter , which precludes adaptive transmission and forces the transmitter to use a stationary coding strategy .",
    "the above described situation is modeled as a slowly fading channel with receiver side information only , which is an example of a non - ergodic _",
    "composite channel_. a composite channel is a collection of component channels @xmath0 parameterized by @xmath1 , where the random variable @xmath1 is chosen according to some distribution @xmath2 at the beginning of transmission and then held fixed .",
    "we assume the channel realization is revealed to the receiver but not the transmitter .",
    "this class of channel is also referred to as the _ mixed channel _ @xcite or the _ averaged channel _",
    "@xcite in literature .",
    "the shannon capacity of a composite channel is given by the verd - han generalized capacity formula @xcite @xmath3 where @xmath4 is the liminf in probability of the normalized information density .",
    "this formula highlights the pessimistic nature of the shannon capacity definition , which is dominated by the performance of the `` worst '' channel , no matter how small its probability . to provide more flexibility in capacity definitions for composite channels , in @xcite",
    "we relax the constraint that all transmitted information has to be correctly decoded and derive alternate definitions including the _ capacity versus outage _ and the _ expected capacity_. the capacity versus outage approach allows certain data loss in some channel states in exchange for higher rates in other states .",
    "it was previously examined in @xcite for single - antenna cellular systems , and later became a common criterion for multiple - antenna wireless fading channels @xcite .",
    "see ( * ? ? ?",
    "* ch .  4 ) and references therein for more details .",
    "the expected capacity approach also requires the transmitter to use a single encoder but allows the receiver to choose from a collection of decoders based on channel states .",
    "it was derived for a gaussian slow - fading channel in  @xcite , and for a composite binary symmetric channel ( bsc ) in @xcite .",
    "channel capacity theorems deal with data transmission in a communication system . when extending the system to include the source of the data , we also need to consider the data compression problem which deals with source representation and reconstruction .",
    "for the overall system , the end - to - end distortion is a well - accepted performance metric . when both the source and channel are stationary and ergodic , codes are usually designed to achieve the same end - to - end distortion level for any source sequence and channel realization .",
    "nevertheless , practical systems do not always impose this constraint .",
    "if the channel model is generalized to such scenarios as the composite channel above , it is natural to relax the constraint that a single distortion level has to be maintained for all channel states . in parallel with the development of alternative capacity definitions ,",
    "we introduce generalized end - to - end distortion metrics including the _ distortion versus outage _ and the _ expected distortion_. the distortion versus outage is characterized by a pair @xmath5 , where the distortion level @xmath6 is guaranteed in receiver - recognized non - outage states of probability no less than @xmath7 .",
    "this definition requires csir based on which the outage can be declared .",
    "the expected distortion is defined as @xmath8 , i.e. the achievable distortion @xmath9 in channel state @xmath1 averaged over the underlying distribution @xmath2 .",
    "these alternative distortion metrics are also considered in prior works . in @xcite",
    "the average distortion @xmath10 , obtained by averaging over outage and non - outage states , was adopted as a fidelity criterion to analyze a two - hop fading channel . here",
    "@xmath11 is the variance of the source symbols .",
    "the expected distortion was analyzed for the mimo block fading channel in the high snr regime @xcite and in the finite snr regime @xcite .",
    "various coding schemes for expected distortion were also studied in a slightly different but closely related broadcast scenario @xcite .",
    "data compression ( source coding ) and data transmission ( channel coding ) are two fundamental topics in shannon theory . for transmission of a discrete memoryless source ( dms ) over a discrete memoryless channel ( dmc ) , the renowned source - channel separation theorem ( * ? ? ?",
    "* theorem 2.4 ) asserts that a target distortion level @xmath12 is achievable if and only if the channel capacity @xmath13 exceeds the source rate distortion function @xmath14 , and a two - stage separate source - channel code suffices to meet the requirement .",
    "this theorem enables separate designs of source and channel codes with guaranteed optimal performance .",
    "it also extends to stationary and ergodic source and channel models @xcite @xcite .",
    "separate source - channel coding schemes provide flexibility through modularized design . from the source s point of view",
    ", the source can be transmitted over any channel with capacity greater than @xmath14 and be recovered at the receiver subject to a certain fidelity criterion ( the distortion @xmath12 ) .",
    "the source is indifferent to the statistics of each individual channel and consequently focuses on source code design independent of channel statistics . despite their flexibility and optimality for certain systems ,",
    "separation schemes also have their disadvantages .",
    "first of all , the source encoder needs to observe a long - blocklength source sequence in order to determine the output , which causes infinite delay .",
    "second , separation schemes may increase complexity in encoders and decoders because the two processes of source and channel coding are acting in opposition to some extent .",
    "source coding is essentially a data compression process , which aims at removing redundancy from source sequences to achieve the most concise representation . on the other hand , channel coding deals with data transmission , which tries to add some redundancy to the transmitted sequence for robustness against the channel noise .",
    "if the source redundancy can be exploited by the channel code , then a joint source - channel coding scheme may avoid this overhead .",
    "in particular , transmission of a gaussian source over a gaussian channel , and a binary symmetric source over a bsc , are both examples where optimal performance can be achieved without any coding @xcite .",
    "this is because the source and channel are  matched \" to each other in the sense that the transition probabilities of the channel solve the variational problem defining the source rate - distortion function @xmath14 and the letter probabilities of the source drive the channel at capacity @xcite .",
    "a careful inspection of the shannon separation theorem reveals some important underlying assumptions : a single - user channel , a stationary and ergodic source and channel , and a single distortion level maintained for all transmissions .",
    "violation of any of these assumptions will likely prompt reexamination of the separation theorem .",
    "for example , cover et .",
    "al . showed that for a multiple access channel with correlated sources , the separation theorem fails @xcite . in @xcite vembu et al .",
    "gave an example of a non - stationary system where the source is transmissible through the channel with zero error , yet its minimum achievable source coding rate is twice the channel capacity .",
    "in this work , we illustrate that different end - to - end distortion metrics lead to different conclusions about separability even for the same source and channel model .",
    "in fact , source - channel separation holds under the distortion versus outage metric but fails under the expected distortion metric . in @xcite",
    "we proved the direct part of source - channel separation under the distortion versus outage metric and established the converse for a system of gaussian source and slow - fading gaussian channels . here",
    "we extend the converse to more general systems of stationary sources and composite channels .",
    "source - channel separation implies that the operation of source and channel coding does not depend on the statistics of the counterpart .",
    "however , the source and channel do need to communicate with each other through a _",
    "negotiation interface _ even before the actual transmission starts . in the classical view of shannon separation for stationary ergodic sources and channels",
    ", the source requires a rate @xmath14 based on the target distortion @xmath12 and the channel decides if it can support the rate based on its capacity @xmath13 . for generalized source / channel models and distortion metrics ,",
    "the interface is not necessarily a single rate and may allow multiple parameters to be agreed upon between the source and channel .",
    "after communication through the appropriate negotiation interface , the source and channel codes may be designed separately and still achieve the optimal performance .",
    "vembu et al .",
    "studied the transmission of non - stationary sources over non - stationary channels and observed that the notion of ( strict ) domination ( * ? ? ?",
    "* theorem 7 ) dictates whether a source is transmissible over a channel , instead of the simple comparison between the minimum source coding rate and the channel capacity . the notion of ( strict )",
    "domination requires the source to provide the distribution of the _ entropy density _ and the channel to provide the distribution of the _ information density _ as the appropriate interface .",
    "the source - channel interface concept also applies after the actual transmission starts . at the transmitter end , we see examples where the source sequence is directly supplied to the channel , such as the uncoded transmission of a gaussian source over a gaussian channel . but",
    "more generally there is certain processing on the source side , and the processed output , instead of the original source sequence , is supplied to the channel .",
    "the _ transmitter interface _ contains what the source actually delivers to the channel .",
    "for example , in separation schemes the interface is the source encoder output ; in hybrid digital - analog schemes @xcite the interface is a combination of vector quantizer output and quantization residue .",
    "similarly we can introduce the concept of a _ receiver interface_. instead of directly delivering the channel output sequence to the destination , the receiver may implement certain decoding and choose the channel decoder output as the interface .",
    "the interfaces at the transmitter and the receiver are the same in classical shannon separation schemes , since the channel code requires all transmitted information to be correctly decoded with vanishing error , but in general the two interfaces can be different .",
    "for example , the receiver interface may include an outage indicator or partial decoding when considering generalized capacity definitions .",
    "different transmission schemes can be compared by their end - to - end performance .",
    "nevertheless , the concept of source - channel interface opens a new dimension for comparison .",
    "ideally the interface complexity should be measured by some quantified metrics .",
    "transmission schemes with low interface complexity are also appealing in view of simplified system design .",
    "we expect a performance enhancement when the source and channel exchange more information through a more sophisticated interface , and illustrate the tradeoff between interface complexity and end - to - end performance through some examples in this work .",
    "the rest of the paper is organized as follows .",
    "we review alternative channel capacity definitions and define corresponding end - to - end distortion metrics in section [ sec : performance ] . in section [ sec : source - channel ]",
    "we provide a new perspective of source - channel separation generalized from shannon s classical view and also introduce the concept of source - channel interface . in section [ sec : outagedistortion ]",
    "we establish the separation optimality for transmission of stationary ergodic sources over composite channels under the distortion versus outage metric . in section [ sec : interfacebsc ] we consider various schemes to transmit a binary symmetric source ( bss ) over a composite bsc and show the tradeoff between achievable expected distortion and interface complexity .",
    "conclusions are given in section [ sec : con ] .",
    "we first review alternate channel capacity definitions derived in @xcite to provide some background information .",
    "we then define alternate end - to - end performance metrics for the entire communication system , including the source and the destination .      the channel @xmath15 is statistically modeled as a sequence of @xmath16-dimensional conditional distributions @xmath17 . for any integer @xmath16 , @xmath18 is the conditional distribution from the input space @xmath19 to the output space @xmath20 .",
    "let @xmath21 and @xmath22 denote the input and output processes , respectively .",
    "each process is specified by a sequence of finite - dimensional distributions , e.g. @xmath23 .    in a composite channel ,",
    "when the channel side information is available at the receiver , we represent it as an additional channel output .",
    "specifically , we let @xmath24 , where @xmath1 is the channel side information and @xmath25 is the output of the channel described by parameter @xmath1 . throughout , we assume the random variable @xmath1 is independent of @xmath21 and unknown to the encoder . thus for each @xmath16 @xmath26 the information density is defined similarly as in @xcite @xmath27      consider a sequence of @xmath28 codes .",
    "let @xmath29 be the probability that the receiver declares an outage , and @xmath30 be the decoding error probability given that no outage is declared .",
    "we say that a rate @xmath31 is outage-@xmath32 achievable if there exists a sequence of @xmath28 channel codes such that @xmath33 and @xmath34 .",
    "the _ capacity versus outage _",
    "@xmath35 is defined to be the supremum over all outage-@xmath32 achievable rates , and is shown to be @xcite @xmath36 \\leq q \\right\\}. \\label{eqn : outage}\\ ] ]    the operational implication of this definition is that the encoder uses a single codebook and sends information at a fixed rate @xmath35 . assuming repeated channel use and independent channel state at each use , the receiver can correctly decode the information a proportion @xmath7 of the time and turn itself off a proportion @xmath32 of the time .",
    "we further define the _ outage capacity _ @xmath37 as the long - term average rate , which is a meaningful metric if we are only interested in the fraction of correctly received packets and approximate the unreliable packets by surrounding samples , or if there is some repetition mechanism where the receiver requests retransmission of lost information from the sender .",
    "the value @xmath32 can be chosen to maximize the long - term average throughput @xmath38 .",
    "this notion provides another strategy for increasing reliably - received rate .",
    "although the transmitter is forced to use a single encoder at a rate @xmath39 without channel state information , the receiver can choose from a collection of decoders , each parameterized by @xmath40 and decoding at a rate @xmath41 , based on csir .",
    "denote by @xmath42 the error probability associated with channel state @xmath40 .",
    "the expected capacity @xmath43 is the supremum of all achievable rates @xmath44 of any code sequence that has @xmath45 approaching zero .    in a composite channel",
    ", different channel states can be viewed as virtual receivers , and therefore the expected capacity is closely related to the capacity region of a broadcast channel ( bc ) . in the broadcast system the channel from the input to the output of receiver @xmath40",
    "is @xmath46 under certain conditions , it is shown that the expected capacity of a composite channel equals to the maximum weighted sum - rate over the capacity region of the corresponding broadcast channel , where the weight coefficient is the state probability @xmath47 ( * ? ? ?",
    "* theorem  1 ) . using broadcast channel codes ,",
    "the expected capacity is derived in @xcite for a gaussian slow - fading channel and in @xcite for a composite bsc .",
    "the expected capacity is a meaningful metric if _ partial _ received information is useful .",
    "for example , consider sending an image using a multi - resolution ( mr ) source code over a composite channel .",
    "decoding all transmitted information leads to reconstructions with the highest fidelity .",
    "however , in the case of inferior channel quality , it still helps to decode partial information and get a coarse reconstruction .",
    "next we introduce alternative end - to - end distortion metrics as performance measures for transmission of a stationary ergodic source over a composite channel .",
    "we denote by @xmath48 the source alphabet and the source symbols @xmath49 are generated according to a sequence of finite - dimensional distributions @xmath50 , and then transmitted over a composite channel @xmath51 with conditional output distribution @xmath52 it is possible that the source generates symbols at a rate different from the rate at which the channel transmits symbols , i.e. a length-@xmath16 source sequence may be transmitted in @xmath53 channel uses with @xmath54 .",
    "the channel _ bandwidth expansion ratio _ is defined to be @xmath55 . for simplicity",
    "we assume @xmath56 in this and the next two sections , but the discussions can be easily extended to general cases with @xmath57 .",
    "the numerical examples in section [ sec : interfacebsc ] will explicitly address this issue .",
    "here we design an encoder @xmath58 that maps the source sequence to the channel input .",
    "note that the source and channel encoders , whether joint or separate , do not have access to channel state information @xmath1 .",
    "however , the receiver can declare an outage with probability @xmath59 based on csir .",
    "in non - outage states , we design a decoder @xmath60 that maps the channel output to a source reconstruction .",
    "we say a distortion level @xmath12 is outage-@xmath32 achievable if @xmath61 and @xmath62 where @xmath63 is the distortion measure between the source sequence @xmath64 and its reconstruction @xmath65 . the _ distortion versus outage",
    "@xmath6 is the infimum over all outage-@xmath32 achievable distortions . in order to evaluate",
    "we need the conditional distribution @xmath66 .",
    "assuming the encoder @xmath67 and the decoder @xmath68 are deterministic , this distribution is given by @xmath69 here @xmath70 is the indicator function .",
    "note that the channel statistics @xmath18 and the source statistics @xmath50 are fixed , so the code design is essentially the appropriate choice of the outage states and the encoder - decoder pair @xmath71 .",
    "we denote by @xmath9 the achievable average distortion when the channel is in state @xmath1 , and it is given by @xmath72 where the summation is over all @xmath73 such that @xmath74 and @xmath75 .",
    "notice that the transmitter can not access channel state information so the encoder @xmath76 is independent of @xmath1 ; nevertheless the receiver can choose different decoders @xmath77 based on csir .",
    "in a composite channel , each channel state is assumed to be stationary and ergodic , so for a fixed channel state @xmath1 we can design source - channel codes such that @xmath78 approaches a constant limit @xmath9 for large @xmath16 ; however , it is possible that @xmath78 approaches different limits for different channel states .",
    "the expected distortion metric captures the distortion averaged over various channel states .",
    "using the conditional distribution @xmath66 in and the definition of @xmath9 in , the average distortion can be written as @xmath79 the expected distortion @xmath80 is the infimum of all achievable average distortions @xmath81 .",
    "for transmission of a source over a channel , the system consists of three concatenated blocks : the encoder @xmath67 that maps the source sequence @xmath64 to the channel input @xmath82 ; the channel @xmath18 that maps the channel input @xmath82 to channel output @xmath83 , and the decoder @xmath68 that maps the channel output @xmath83 to a reconstruction of the source sequence @xmath65 .",
    "in contrast , a separate source - channel coding scheme consists of five blocks .",
    "the encoder @xmath67 is separated into a source encoder @xmath84 and a channel encoder @xmath85 where the index set @xmath86 of size @xmath87 serves as both the source encoder output and the channel encoder input .",
    "equivalently , each index in @xmath86 can be viewed as a block of @xmath88 bits ( * ? ? ?",
    "* defn . 5 ) .",
    "the decoder @xmath68 is also separated into a channel decoder @xmath89 and a source decoder @xmath90 .",
    "the difference between a general system and a separate source - channel coding system is summarized in fig . [",
    "fig : system3block ] .",
    "separation does not imply isolation - the source and channel encoders and decoders still need to agree on certain aspects of their respective designs .",
    "there are three interfaces through which they exchange information , the negotiation interface , the transmitter interface and the receiver interface . for classical shannon separation schemes with an end - to - end distortion target @xmath12",
    ", these interfaces are summarized in table [ table : shannoninterface ] .",
    "the negotiation interface is a single rate comparison between @xmath14 and @xmath13 .",
    "since the shannon capacity definition requires that all transmitted information be correctly decoded , the transmission rate @xmath39 is the same as the receiving rate @xmath91 . assuming stationary and ergodic systems",
    ", these rates do not depend on the blocklength @xmath16 .",
    "however , these constraints can be relaxed to include more source - channel transmission strategies as separation schemes .",
    ".interface for shannon separation schemes [ cols=\"<,<\",options=\"header \" , ]     [ table : interfaceresiduesplitting ]      we provide some numerical examples to compare different schemes in this section .",
    "we assume the two states of the composite bsc have crossover probabilities @xmath92 and @xmath93 , and the bandwidth expansion ratio @xmath94 .     for various schemes.,width=288 ]    in fig .",
    "[ fig : d1d2 ] we plot the achievable distortion pair @xmath95 for each scheme . for the broadcast coding scheme , by varying the auxiliary variable @xmath96 from @xmath97 and @xmath98",
    ", we change the rate allocation between the base layer @xmath99 and the refinement layer @xmath100 .",
    "the separation schemes using the shannon capacity code and the capacity versus outage code are the special cases of @xmath101 and @xmath98 , respectively .",
    "they are marked by the two end - points of the broadcast distortion region boundary .",
    "for the quantization residue splitting scheme , we calculate the distortion pairs @xmath95 for different parameters @xmath102 and @xmath103 .",
    "the plotted curve is the convex hull of all achievable distortion pairs .",
    "note that the broadcast scheme is a special case of the residue splitting scheme with @xmath104 , so the broadcast distortion region lies strictly within the residue splitting distortion region .",
    "there are two systematic codes , one targeting at each channel state .",
    "they are represented by two points , both out of the residue splitting distortion region .        in fig .",
    "[ fig : edcompare ] we plot the expected distortion of various schemes for different channel state distributions .",
    "each systematic code achieves a single distortion pair , so the expected distortion is simply the weighted average and increases linearly with the bad channel state probability @xmath105 . for broadcast and residue splitting schemes ,",
    "we need to choose the optimal point on the distortion region boundary at each channel state probability . since the broadcast scheme is a special case of the residue splitting scheme , its expected distortion is no less , and sometimes strictly larger , than that of the residue splitting scheme . for different ranges of @xmath105 , the scheme that achieves the lowest expected distortion is also different . for @xmath106 or",
    "@xmath107 it is the residue splitting scheme , for @xmath108 it is the systematic code for the good channel state , and for @xmath109 it is the systematic code for the bad channel state .            expected distortion alone does not provide the complete picture for comparison of the schemes . in fig .",
    "[ fig : interfaceenc ] and [ fig : interfacedec ] we assume the channel state probability @xmath110 and illustrate the tradeoff between the expected distortion and the transmitter / receiver interface complexity for different schemes , where the complexity is measured by bits per source symbol delivered through the interface .",
    "for the broadcast scheme , we can reduce the expected distortion by increasing @xmath96 , which reduces the base layer rate but increases the refinement layer rate and the total rate , hence a higher interface complexity . however , the distortion - complexity curve is not strictly decreasing . after we reach the minimum expected distortion",
    ", it does not provide any more benefit to further increase the interface complexity .",
    "the same trend is also observed in the residue splitting scheme . at channel state probability @xmath111 , the systematic code targeting the good state has the lowest expected distortion",
    ", nevertheless it also has the highest interface complexity .",
    "the choice about the appropriate scheme and operating points ( parameters ) depends on the system designer s view about this distortion - complexity tradeoff .",
    "we consider transmission of a stationary ergodic source over non - ergodic composite channels with channel state information at the receiver ( csir ) . to study the source - channel coding problem for the entire system",
    ", we include a broader class of transmission schemes as separation schemes by relaxing the constraint of shannon separation , i.e. a single - number comparison between source coding rate and channel capacity , and introducing the concept of a source - channel interface which allows the source and channel to agree on multiple parameters .",
    "we show that different end - to - end distortion metrics lead to different conclusions about separation optimality , even for the same source and channel models .",
    "specifically , one such generalized scheme guarantees the separation optimality under the distortion versus outage metric .",
    "separation schemes are in general suboptimal under the expected distortion metric .",
    "we study the performance enhancement when the source and channel coders exchange more information through a more sophisticated interface , and illustrate the tradeoff between interface complexity and end - to - end performance through the example of transmission of a binary symmetric source over a composite binary symmetric channel .",
    "in fig . [ fig : expected_enc ] , the multi - resolution source code can be constructed as follows .",
    "consider three independent auxiliary random variables @xmath112@xmath113bernoulli@xmath114 , @xmath115@xmath113bernoulli@xmath116 , and @xmath117@xmath113bernoulli@xmath118 , where @xmath119 and @xmath120 , @xmath121 are given by .",
    "also define @xmath122 which has a bernoulli distribution with parameter @xmath123 .",
    "these variables are related to the source symbol through the relationship @xmath124    _ random codebook generation _ : generate @xmath125 sequences @xmath126 , @xmath127 , by uniform and independent sampling over the strong typical set @xmath128 .",
    "similarly , generate @xmath129 sequences @xmath130 , @xmath131 , drawn uniformly and independently over @xmath132 .",
    "_ decoding _ : if only the index @xmath139 is received , the decoder declares the estimate of the source sequence as @xmath140 .",
    "if both indices are received , the source is reconstructed as @xmath141 . following the procedures in @xcite and (",
    "* theorem 1 ) we can easily verify the following distortion targets are achievable : @xmath142 , @xmath143 . in practice",
    "the mr source code can be implemented as a multi - stage vector quantization , which has an _ additive _ successive refinement structure @xcite . as shown in fig .",
    "[ fig : expected_enc ] , in channel state 2 only the base layer description is received and source dec 2 determines the base reconstruction @xmath144 .",
    "when both layers are received , source dec 1 determines a refinement sequence @xmath145 based on the refinement layer encoding index only , and add it to the base reconstruction @xmath144 to obtain the overall reconstruction @xmath65 . on the contrary , for general",
    "mr source codes the overall reconstruction may require a joint decoding of indices from both layers .",
    "the additive refinement structure reduces coding complexity , provides scalability , and does not incur any performance loss under certain conditions ( * ? ? ?",
    "* theorem 3 ) , which are all satisfied in this example .",
    "the broadcast channel code design , for a chosen @xmath146 , is summarized as follows .    _ random codebook generation _ : generate @xmath147 independent codewords @xmath148 , @xmath149 , by i.i.d .",
    "sampling of a bernoulli@xmath116 distribution .",
    "generate @xmath150 independent codewords @xmath151 , @xmath152 , by i.i.d .",
    "sampling of a bernoulli@xmath153 distribution .",
    "_ decoding _ : given channel output @xmath155 , in state 2 we determine the unique @xmath156 such that @xmath157 in state 1 we look for the unique indices @xmath158 such that @xmath159 following the analysis of ( * ? ? ?",
    "* theorem 14.6.2 ) , we can show that the channel decoding error probability approaches zero as long as the encoding rates satisfy .    roughly speaking , in channel state 2 , we observe @xmath160 where the channel noise @xmath161 is a bernoulli@xmath162 sequence .",
    "we want to decode the @xmath163 sequence subject to the overall interference - plus - noise @xmath164 , which is a bernoulli sequence with parameter @xmath165 , hence the achievable rate @xmath166 . in channel state 1 , we observe @xmath167 since @xmath168 , the sequence @xmath163 can be decoded and then subtracted off .",
    "we then decode @xmath169 subject to the noise @xmath170 , and the rate @xmath171 is achievable .",
    "m.  effros and a.  goldsmith .",
    "capacity definitions and coding strategies for general channels with receiver side information . in _ proc .",
    "inform . theory ( isit ) _",
    ", page  39 , cambridge ma , august 1998 .",
    "m.  effros , a.  goldsmith , and y.  liang .",
    "capacity definitions of general channels with receiver side information . in _ proc .",
    "inform . theory ( isit ) _ , pages 921925 , nice , france , june 2007 .        c.  t.k .",
    "ng , d.  gndz , a.  goldsmith , and e.  erkip .",
    "recursive power allocation in gaussian layered broadcast coding with successive refinement . in _",
    "communications _ , glasgow , scotland , june 2007 . to appear .    c.  t.k .",
    "ng , d.  gndz , a.  goldsmith , and e.  erkip .",
    "minimum expected distortion in gaussian layered broadcast coding with successive refinement . in _ proc .",
    "inform . theory ( isit ) _ , pages 22262230 , nice , france , june 2007 .",
    "g.  d. hu . on shannon theorem and its converse for sequence of communication schemes in the case of abstract random variables . in _ proc .",
    "3rd prague conf . on inform .",
    "theory , stat .",
    "decision functions , random processes _ , pages 285333 , czechoslovak academy of sciences , prague , 1964 .                      c.  tian , a.  steiner , s.  shamai(shitz ) , and s.  diggavi . expected distortion for gaussian source with a broadcast transmission strategy over a fading channel . in _ proc .",
    "ieee inform .",
    "theory workshop on wireless networks _ ,",
    "pages 15 , bergen norway , july 2007 ."
  ],
  "abstract_text": [
    "<S> we consider transmission of stationary and ergodic sources over non - ergodic composite channels with channel state information at the receiver ( csir ) . </S>",
    "<S> previously we introduced alternate capacity definitions to shannon capacity , including the capacity versus outage and the expected capacity . </S>",
    "<S> these generalized definitions relax the constraint of shannon capacity that all transmitted information must be decoded at the receiver . in this work alternate end - to - end distortion metrics such as the distortion versus outage and the expected distortion </S>",
    "<S> are introduced to relax the constraint that a single distortion level has to be maintained for all channel states . for transmission of stationary and ergodic sources over stationary and ergodic channels , </S>",
    "<S> the classical shannon separation theorem enables separate design of source and channel codes and guarantees optimal performance . for generalized communication systems , </S>",
    "<S> we show that different end - to - end distortion metrics lead to different conclusions about separation optimality even for the same source and channel models .    </S>",
    "<S> separation does not imply isolation - the source and channel still need to communicate with each other through some interfaces . for shannon separation schemes , </S>",
    "<S> the interface is a single - number comparison between the source coding rate and the channel capacity . </S>",
    "<S> here we include a broader class of transmission schemes as separation schemes by relaxing the constraint of a single - number interface . </S>",
    "<S> we show that one such generalized scheme guarantees the separation optimality under the distortion versus outage metric . under the expected distortion metric , </S>",
    "<S> separation schemes are no longer optimal . </S>",
    "<S> we expect a performance enhancement when the source and channel coders exchange more information through more sophisticated interfaces , and illustrate the tradeoff between interface complexity and end - to - end performance through the example of transmitting a binary symmetric source over a composite binary symmetric channel . </S>"
  ]
}