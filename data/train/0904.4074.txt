{
  "article_text": [
    "modelling dependence between different risk cells and factors is an important challenge in operational  risk ( oprisk ) management .",
    "the difficulties of correlation modelling are well known and , hence , regulators typically take a conservative approach when considering correlation in risk models . for example , the basel ii oprisk regulatory requirements for the advanced measurement approach , bis ( 2006 ) p.152 , states _",
    "risk measures for different operational risk estimates must be added for purposes of calculating the regulatory minimum capital requirement .",
    "however , the bank may be permitted to use internally determined correlations in operational risk losses across individual operational risk estimates , provided it can demonstrate to the satisfaction of the national supervisor that its systems for determining correlations are sound , implemented with integrity , and take into account the uncertainty surrounding any such correlation estimates ( particularly in periods of stress ) .",
    "the bank must validate its correlation assumptions using appropriate quantitative and qualitative techniques . _",
    "the current risk measure specified by regulatory authorities is value - at - risk ( var ) at the 0.999 level for a one year holding period . in this case",
    "simple summation over vars corresponds to an assumption of perfect dependence between risks .",
    "this can be very conservative as it ignores any diversification effects .",
    "if the latter are allowed in the model , capital reduction can be significant providing a strong incentive to model dependence in the banking industry . at the same time",
    ", limited data does not allow for reliable estimates of correlations and there are attempts to estimate these using expert opinions .",
    "in such a setting a transparent dependence model is very important from the perspective of model interpretation , understanding of model sensitivity and with the aim of minimizing possible model risk .",
    "however , we would also like to mention that var is not a coherent risk measure , see artzner , delbaen , eber and heath ( 1999 ) .",
    "this means that in principal dependence modelling could also increase var , see embrechts , nelehov and wthrich ( 2009 ) and embrechts , lambrigger and wthrich ( 2009 ) .    under basel ii requirements , the financial institution intending to use the advanced measurement approach ( ama ) for quantification of oprisk should demonstrate accuracy of the internal model within 56 risk cells ( eight business lines times seven event types ) .",
    "to meet regulatory requirements , the model should make use of internal data , relevant external data , scenario analysis and factors reflecting the business environment and internal control systems .",
    "the definition of oprisk , basel ii requirements and the possible loss distribution approach for ama were discussed widely in the literature , see e.g. cruz ( 2004 ) , chavez - demoulin , embrechts and nelehov ( 2006 ) , frachot , moudoulaud and roncalli ( 2004 ) , shevchenko ( 2009 ) .",
    "it is more or less widely accepted that under the loss distribution approach of ama basel ii requirements , the banks should quantify distributions for frequency and severity of oprisk for each business line and event type over a one year time horizon .",
    "these are combined into an annual loss distribution for the bank top level ( as well as business lines and event types if required ) and the bank capital ( unexpected loss ) is estimated using the 0.999 quantile of the annual loss distribution . if the severity and frequency distribution parameters are known , then the capital estimation can be accomplished using different techniques . in the case of single risks",
    "there are : hybrid monte carlo approaches , see peters , johansen and doucet ( 2007 ) ; panjer recursions , see panjer ( 1981 ) ; integration of the characteristic functions , see luo and shevchenko ( 2009 ) ; fast fourier transform techniques , see e.g. embrechts and frei ( 2009 ) , temnov and warnung ( 2008 ) . to account for parameter uncertainty ,",
    "see shevchenko ( 2008 ) , and in multivariate settings monte carlo methods are typically used .    the commonly used model for an annual loss in a risk cell ( business line / event type ) is a compound random variable,@xmath0here @xmath1 in our framework is discrete time ( in annual units ) with @xmath2 corresponding to the next year .",
    "the upper script @xmath3 is used to identify the risk cell .",
    "the annual number of events @xmath4 is a random variable distributed according to a frequency counting distribution @xmath5 , typically poisson , which also depends on time dependent parameter(s ) @xmath6 .",
    "the severities in year @xmath7 are represented by random variables @xmath8 , @xmath9 , distributed according to a severity distribution @xmath10 , typically lognormal , weibull or generalized pareto distributions with parameter(s ) @xmath11 .",
    "note , the index @xmath3 on the distributions @xmath12 and @xmath13 reflects that distribution type can be different for different risks , for simplicity of notation we shall omit this @xmath3 , using @xmath14 and @xmath15 , hereafter",
    ". the variables @xmath6 and @xmath16 generically represent distribution ( model ) parameters of the @xmath17 risk that we refer hereafter to as the risk profiles .",
    "typically , it is assumed that given @xmath18 and @xmath11 , the frequency and severities of the @xmath19 risk are independent and the severities within the @xmath17 risk are independent too .",
    "the total bank s loss in year @xmath7 is calculated as @xmath20where formally for oprisk under the basel ii requirements @xmath21 ( seven event types times eight business lines ) .",
    "however , this may differ depending on the financial institution and type of problem .    conceptually under model ( [ model1 ] ) , the dependence between the annual losses @xmath22 and @xmath23 can be introduced in several ways .",
    "for example via :    * modelling dependence between frequencies @xmath24 and @xmath25 directly through e.g. copula methods , see e.g. frachot , roncalli and salomon ( 2004 ) , bee ( 2005 ) and aue and klakbrener ( 2006 ) or common shocks , see e.g. lindskog and mcneil ( 2003 ) , powojowski , reynolds and tuenter ( 2002 ) .",
    "we note that the use of copula methods , in the case of discrete random variables , needs to be done with care .",
    "the approach of common shocks is proposed as a method to model events affecting many cells at the same time .",
    "formally , this leads to dependence between frequencies of the risks if superimposed with cell internal events .",
    "one can introduce the dependence between event times of different risks , e.g. the @xmath26 event time of the @xmath17 risk correlated to the @xmath26 event time of the @xmath27 risk , etc . , but it can be problematic to interpret such a model . *",
    "considering dependence between severities ( e.g. the first loss amount of the @xmath17 risk is correlated to the first loss of the @xmath27 risk , second loss in the @xmath17 risk is correlated to second loss in the @xmath27 risk , etc ) , see e.g. chavez - demoulin , embrechts and nelehov ( 2006 )",
    ". this can be difficult to interpret especially when one considers high frequency versus low frequency risks .",
    "* modelling dependence between annual losses directly via copula methods , see giacometti , rachev , chernobai and bertocchi ( 2008 ) , bcker and klppelberg ( 2008 ) and embrechts and puccetti ( 2008 ) .",
    "however , this may create irreconcilable problems with modelling insurance for oprisk that directly involves event times .",
    "additionally , it will be problematic to quantify these correlations using historical data , and the lda model ( [ model1 ] ) will loose its structure . though one can consider dependence between losses aggregated over shorter periods .    in this paper , we assume that all risk profiles are stochastically evolving in time .",
    "that is we model risk profiles @xmath28 and @xmath29 by random variables @xmath30 and @xmath31 respectively .",
    "we introduce dependence between risks by allowing dependence between their risk profiles @xmath32 and @xmath33 .",
    "note that , independence between frequencies and severities in ( [ model1 ] ) is conditional on risk profiles @xmath34 only . additionally we assume , for the sake of simplicity , that all risks are independent conditional on risk profiles .",
    "stochastic modelling of risk profiles may appeal to intuition .",
    "for example consider the annual number of events for the @xmath19 risk modelled as random variables from poisson distribution @xmath35 .",
    "conditional on @xmath36 , the expected number of events per year is @xmath37 .",
    "the latter is not only different for different banks and different risks but also changes from year to year for a risk in the same bank .",
    "in general , the evolution of @xmath38 , can be modelled as having deterministic ( trend , seasonality ) and stochastic components . in actuarial mathematics this",
    "is called a mixed poisson model . for simplicity , in this paper , we assume that @xmath36 is purely stochastic and distributed according to a gamma distribution .    now consider a sequence @xmath39 .",
    "it is naive to assume that risk profiles of all risks are independent .",
    "intuitively these are dependent , for example , due to changes in politics , regulations , law , economy , technology ( sometimes called drivers or external risk factors ) that jointly impact on many risk cells at each time instant . in this paper",
    "we focus on dependence between risk profiles .",
    "we begin by presenting the general model and then we perform analysis of relevant properties of this model in a bivariate risk setting .",
    "next , we demonstrate how to perform inference under our model by adopting a bayesian approach that allows one to combine internal data with expert opinions and external data .",
    "we consider both the single risk and multiple risk settings for the example of modelling claims frequencies .",
    "then we present an advanced simulation procedure utilizing a class of markov chain monte carlo ( mcmc ) algorithms which allow us to sample from the posterior distributions developed .",
    "finally , we demonstrate the performance of both the model and the simulation procedure in several examples , before finishing with a discussion and conclusions .",
    "the main objective of the paper is to present the framework we develop for the multivariate problem and to demonstrate estimation in this setting .",
    "application of real data is the subject of further research . to clarify notation",
    ", we shall use upper case symbols to represent random variables , lower case symbols for their realizations and bold for vectors .",
    "consider @xmath40 risks each with a general model ( [ model1 ] ) for the annual loss in year @xmath7 , @xmath22 , and each modelled by severity @xmath41 and frequency @xmath24 .",
    "the frequency and severity risk profiles are modelled by random vectors @xmath42 and @xmath43 respectively and parameterized by risk characteristics @xmath44 @xmath45 ) and @xmath46 correspondingly .",
    "additionally , the dependence between risk profiles is parameterized by @xmath47 .",
    "assume that , given @xmath48 :    1 .",
    "the random vectors , @xmath49are independent .",
    "that is , given @xmath50 , between different years , the risk profiles for frequencies and severities as well as the number of losses and actual losses are independent .",
    "the vectors @xmath51 are _ i.i.d .",
    "_ from a joint distribution with marginal distributions @xmath52 , @xmath53 and @xmath54-dimensional copula @xmath55 .",
    "3 .   given @xmath56 and @xmath57 : the compound random variables @xmath58 are independent with @xmath24 and @xmath59 independent ; frequencies @xmath60 ; and severities @xmath61    [ generalmodelassumptions ]    calibration of the above model",
    "requires estimation of @xmath50 .",
    "a thorough discussion about the interpretation and role of @xmath50 is provided in section 4 , where it will be treated within a bayesian framework as a random variable @xmath62 to incorporate expert opinions and external data into the estimation procedure .",
    "also note that for simplicity of notation , we assumed one severity risk profile @xmath63 and one frequency risk profile @xmath64 per risk - extension is trivial if more risk profiles are required to model risk",
    ".       * copula models . * to define the above model , a copula function @xmath65 should be specified to model dependence between the risk profiles . for a description of copulas in the context of financial risk modelling see mcneil , frey and embrechts ( 2005 ) . in general ,",
    "a copula is a @xmath66-dimensional multivariate distribution on @xmath67^{d}$ ] with uniform marginal distributions .",
    "given a copula function @xmath68 , the joint distribution of rvs @xmath69 with marginal distributions @xmath70 can be constructed as @xmath71a well known theorem due to sklar , published in 1959 , says that one can always find a unique copula @xmath65 for a joint distribution with given continuous marginals .",
    "note that in the case of discrete distributions this copula may not be unique .",
    "given ( [ copuladefinition ] ) , the joint density can be written as @xmath72    where @xmath73 is a copula density and @xmath74 are marginal densities . in this paper , for illustration purposes we consider the gaussian , clayton and gumbel copulas ( clayton and gumbel copulas belong to the so - called family of the archimedean copulas ) :    * * gaussian copula : * @xmath75 where @xmath76 and @xmath77 are the standard normal distribution and its density respectively and @xmath78 is a multivariate normal density with zero means , unit variances and correlation matrix @xmath79 . * * clayton copula * : @xmath80 where @xmath81 is a dependence parameter . * * gumbel copula * : @xmath82 where @xmath83 is a dependence parameter .",
    "+ in the bivariate case the explicit expression for gumbel copula is given by @xmath84 ^{2\\left ( \\frac{1}{\\rho } -1\\right ) } \\left [ \\log \\left ( u_{1}\\right ) \\log \\left ( u_{2}\\right ) \\right ] ^{\\rho -1 } \\\\ & & \\times \\left [ 1+\\left ( \\rho -1\\right ) \\left [ \\sum\\nolimits_{i=1}^{2}\\left ( -\\log \\left ( u_{i}\\right ) \\right ) ^{\\rho } \\right ] ^{-\\frac{1}{\\rho } } \\right ] .\\end{aligned}\\ ] ]    an important difference between these three copulas is that they each display different tail dependence properties .",
    "the gaussian copula has no upper or lower tail dependence , the clayton copula produces lower tail dependence , whereas the gumbel copula produces upper tail dependence , see mcneil , frey and embrechts ( 2005 ) .       * common factor models . *",
    "the use of common ( systematic ) factors is useful to identify dependent risks and to reduce the number of required correlation coefficients that must be estimated .",
    "for example , assuming a gaussian copula between risk profiles , consider one common factor @xmath85 affecting all risk profiles as follows @xmath86where @xmath87 and @xmath85 are iid from the standard normal distribution and all rvs are independent between different time steps @xmath7 .",
    "given @xmath85 , all risk profiles are independent but unconditionally the risk profiles are dependent if the corresponding @xmath88 are nonzero . in this example , one should identify @xmath54 correlation parameters @xmath89 only instead of @xmath90 parameters of the full correlation matrix . often",
    ", common factors are unobservable and practitioners use generic intuitive definitions such as : changes in political , legal and regulatory environments , economy , technology , system security , system automation , etc .",
    "several external and internal factors are typically considered .",
    "the factors may affect the frequency risk profiles ( e.g. system automation ) , the severity risk profiles ( e.g. changes in legal environment ) or both the frequency and severity risk profiles ( e.g. system security ) . for more details on the use and identification of the factor models , see section 3.4 in mcneil , frey and embrechts ( 2005 ) ; also , see sections 5.3 and 7.4 in marshall ( 2001 ) for the use in the operational risk context .",
    "in general , a copula can be introduced between all risk profiles .",
    "though , for simplicity , in the simulation examples below , presented for two risks , we consider dependence between severities and frequencies separately . also , in this paper , the estimation procedure is presented for frequencies only .",
    "the actual procedure can be extended in the same manner as presented to severities but it is the subject of further work .",
    "we start with a bivariate model , where we study the strength of dependence at the annual loss level obtained through dependence in risk profiles , as discussed above .",
    "we consider two scenarios .",
    "the first involves independent severity risk profiles and dependent frequency risk profiles .",
    "the second involves dependence between the severity risk profiles and independence between the frequency profiles . in both scenarios ,",
    "we consider three bivariate copulas ( gaussian , clayton and gumbel copulas ( [ gaussiancopula])-([gambelcopula ] ) ) denoted as @xmath91 and parameterized by one parameter @xmath92 which controls the degree of dependence . in the case of gaussian copula",
    ", @xmath92 is a non - diagonal element of correlation matrix @xmath93 in ( [ gaussiancopula ] ) .       * bivariate model for risk profiles .",
    "* we assume that model assumptions 2.1 are fullfilled for the aggregated losses @xmath94as marginals , for @xmath95 we choose :    * @xmath96 and @xmath97 , @xmath9 .",
    "* @xmath98 , @xmath99 .    here",
    ", @xmath100 is a gamma distribution with mean @xmath101 and variance @xmath102 , @xmath103 is a gaussian distribution with mean @xmath104 and standard deviation @xmath105 , and @xmath106 is a lognormal distribution .    in analyzing the induced dependence between annual losses ,",
    "we consider two scenarios :    * scenario 1 : @xmath107 and @xmath108 are dependent via copula @xmath109 while @xmath110 and @xmath111 are independent .",
    "* scenario 2 : @xmath110 and @xmath111 are dependent via copula @xmath109 while @xmath112 and @xmath113 are independent .    here",
    ", parameter @xmath92 corresponds to @xmath114 in model assumptions 2.1 .",
    "the simulation of the annual losses when risk profiles are dependent via a copula can be accomplished as shown in appendix a. utilizing this procedure , we examine the strength of dependence between the annual losses if there is a dependence between the risk profiles . in the next sections we will demonstrate the bayesian inference model and associated methodology to perform estimation of the model parameters . here , we assume the parameters are known _ a priori _ with the following values used in our specific example :    * @xmath115    these parameters correspond to @xmath116 and @xmath117 in model assumptions 2.1 . in figure [ fig1 ] , we present three cases where @xmath118 is a gaussian , clayton or gumbel copula under both scenario 1 and scenario 2 . in each of these examples",
    "we vary the parameter of the copula model @xmath92 from weak to strong dependence .",
    "the annual losses are not gaussian distributed and to measure the dependence between the annual losses we use a non - linear rank correlation measure , spearman s rank correlation , denoted by @xmath119 .",
    "the spearman s rank correlation between the annual losses was estimated using @xmath120 simulated years for each value of @xmath92 . in these and other numerical experiments we conducted , the range of possible dependence between the annual losses of different risks induced by the dependence between risk profiles is very wide and should be flexible enough to model dependence in practice .",
    "note , the degree of induced correlation can be further extended by working with more flexible copula models at the expense of estimation of a larger number of model parameters .",
    "in this section we estimate the model introduced in section 2 using a bayesian inference method . to achieve this",
    "we must consider that the requirements of basel ii ama ( see bis , p.152 ) clearly state that : _ `` any operational risk measurement system must have certain key features to meet the supervisory soundness standard set out in this section .",
    "these elements must include the use of internal data , relevant external data , scenario analysis and factors reflecting the business environment and internal control systems '' .",
    "_ hence , basel ii requires that oprisk models include use of several different sources of information .",
    "we will demonstrate that to satisfy such requirements it is important that methodology such as the one we develop in this paper be considered in practice to ensure one can soundly combine these different data sources .",
    "it is widely recognized that estimation of oprisk frequency and severity distributions can not be done solely using historical data .",
    "the reason is the limited ability to predict future losses in a banking environment which is constantly changing .",
    "assume that a new policy was introduced in a financial institution with the intention of reducing an oprisk loss .",
    "this can not be captured in a model based solely on historical loss data .    for the above reasons ,",
    "it is very important to include scenario analysis ( sa ) in oprisk modelling .",
    "sa is a process undertaken by banks to identify risks ; analyze past events experienced internally and jointly with other financial institutions including near miss losses ; consider current and planned controls in the banks , etc .",
    "usually , it involves surveying of experts through workshops .",
    "a template questionnaire is developed to identify weaknesses , strengths and other factors . as a result",
    "an imprecise , value driven quantitative assessment of risk frequency and severity distributions is obtained . on its own",
    ", sa is very subjective and we argue it should be combined ( supported ) by actual loss data analysis .",
    "it is not unusual that correlations between risks are attempted to be specified by experts in the financial institution , typically via sa .",
    "external loss data is also an important source of information which should be incorporated into modelling .",
    "there are several sources available to obtain external loss data , for a discussion on some of the data related issues associated with external data see peters and teruads ( 2007 ) .",
    "additionally , the combination of expert opinions with internal and external data is a difficult problem and complicated ad - hoc procedures are used in practice .",
    "some prominent risk professionals in industry have argued that statistically consistent combining of these different data sources is one of the most pertinent and challenging aspects of oprisk modelling .",
    "it was quoted in davis ( 2006 ) _",
    "`` another big challenge for us is how to mix the internal data with external data ; this is something that is still a big problem because i do nt think anybody has a solution for that at the moment '' _ and _ `` what can we do when we do nt have enough data [ @xmath121 how do i use a small amount of data when i can have external data with scenario generation ? [ @xmath121",
    "i think it is one of the big challenges for operational risk managers at the moment.''_. using the methodology that we develop in this paper , one may combine these data sources in a statistically sound approach , addressing these important practical questions that practitioners are facing under basel ii ama .",
    "bayesian inference methodology is well suited to combine different data sources in oprisk , for example see shevchenko and wthrich ( 2006 ) .",
    "a closely related credibility theory toy example was considered in bhlmann , shevchenko and wthrich ( 2007 ) .",
    "we also note that in general questions of bayesian model choice must be addressed , adding to this there is the additional complexity that estimation of the required posterior distributions will typically require mcmc , see peters and sisson ( 2006 ) .",
    "a bayesian model to combine three data sources ( internal data , external data and expert opinion ) for the case of a single risk cell was presented in lambrigger , shevchenko and wthrich ( 2007 ) . in this paper",
    "we extend this approach to the case of many risk cells with the dependence between risks introduced as in section 2 . hereafter , for illustrative purposes we restrict to modelling frequencies only .",
    "hence , our objective will be to utilise bayesian inference to estimate the parameters of the model through the combination of expert opinions and observed loss data ( internal and external ) .",
    "we note that as part of this bayesian model formualtion an information flow can be incorporated into the model .",
    "this could be introduced in many forms .",
    "the most obvious example involves incorporation of new data from actual observed losses .",
    "however , we stress that more general ideas are possible . for example , if new information becomes available ( new policy introduced , etc ) then experts can update their prior distributions to incorporate this information into the model .",
    "additionally , under a bayesian model we note that sa could naturally form part of a subjective bayesian prior elicitation procedure , see ohagan ( 2006 ) .      here",
    "we follow the lambrigger , shevchenko and wthrich ( 2007 ) approach to combine different data sources for one risk cell in the case of the model assumptions [ generalmodelassumptions ] .",
    "define a model in which every risk cell of a financial company @xmath122 is characterized by a risk characteristic @xmath123 that describes the frequency risk profile @xmath64 in risk cell @xmath3 .",
    "this @xmath123 represents a vector of unknown distribution parameters of risk profile @xmath124 .",
    "the true value of @xmath123 is not known and modelled as a random variable . _ a priori _ , before having any company specific information , the prior distribution of @xmath125 is based on external data only .",
    "our aim then is to specify the distribution of @xmath123 when we have company specific information about risk cell @xmath3 such as observed losses and expert opinions .",
    "this is achieved by developing a bayesian model and numerical estimation of relevant quantities is performed via mcmc methods . for simplicity , in this section , we drop the risk cell specific superscript @xmath3 since we concentrate on modelling frequencies for single risk cell @xmath3 , where @xmath123 is a scalar @xmath126 and all other parameters are assumed known .",
    "@xmath3    1 .",
    "@xmath3 2 .    3 .    4",
    "given @xmath127 , @xmath128 and @xmath129 are independent for all @xmath130 and @xmath7       * in items 1 ) and 2 ) we choose a gamma distribution for the underlying parameters . often , the available data is not sufficient to support such a choice .",
    "in such cases , in actuarial practice , one often chooses a gamma distribution .",
    "a gamma distribution is neither conservative nor aggressive and it has the advantage that it allows for transparent model interpretations . if other distributions are more appropriate then , of course , one should replace the gamma assumption .",
    "this can easily be done in our simulation methodology . *",
    "given that @xmath131 , @xmath132 = a / b$ ] and @xmath133 .",
    "these are the prior two moments of the underlying risk characteristics the prior can be determined by external data ( or regulator ) .",
    "in general , parameters @xmath134 and @xmath135 can be estimated by the maximum likelihood method using the data from all banks . *",
    "note that we have for the first moments @xmath136 & = & \\theta_{\\lambda } , \\text { } e\\left [ \\lambda _ { t}\\right ] = \\frac{a}{b},\\text { } e\\left [ \\left .",
    "n_{t}\\right\\vert \\theta_{\\lambda } , \\lambda _ { t}\\right ] = v~\\lambda _ { t } ,   \\notag \\\\ e\\left [ \\left . n_{t}\\right\\vert \\theta_{\\lambda } \\right ] & = & v~\\theta_{\\lambda } , \\text { } e\\left [ n_{t}\\right ] = v~\\frac{a}{b}.   \\notag\\end{aligned}\\]]the second moments are given by @xmath137for model interpretation purposes , consider the results for the coefficient of variation ( cv ) , a convenient dimensionless measure of uncertainty commonly used in the insurance industry : @xmath138 } = \\alpha ^{-1}>0 ,   \\label{div1}\\]]and @xmath139 } = \\alpha ^{-1}+\\left ( \\alpha ^{-1}+1\\right ) ~a^{-1}>0 .",
    "\\label{div2}\\]]that is , our model makes perfect sense from a practical perspective .",
    "namely , as volume increases , @xmath140 , there always remains a non - diversifiable element , see and .",
    "this is exactly what has been observed in practice and what regulators require from internal models .",
    "note , if we model @xmath141 as constant and known then @xmath142 .",
    "* contrary to the developments in lambrigger , shevchenko and wthrich ( 2007 ) , where the intensity @xmath141 was constant overtime , now @xmath141 is a stochastic process . from a practical point of view , it is not plausible that the intensity of the annual counts is constant over time . in such a setting",
    "parameter risks completely vanish if we have infinitely many observed years or infinitely many expert opinions , respectively ( see theorem 3.6 ( a ) and ( c ) in lambrigger , shevchenko and wthrich ( 2007 ) ) .",
    "this is because @xmath143 can then be perfectly forecasted . in the present model",
    ", parameter risks will also decrease with increasing information .",
    "as we gain information the posterior standard deviation of @xmath144 will converge to @xmath145 .",
    "however , since @xmath146 viewed from time @xmath147 is always random , the posterior standard deviation for @xmath148 will be finite . * note that , conditionally given @xmath127 , @xmath149 has a negative binomial distribution with probability weights for @xmath150 , @xmath151 = { \\binom{{\\alpha + n-1}}{{n}}}\\left ( \\frac{\\alpha } { \\alpha + \\theta_{\\lambda } v}\\right ) ^{\\alpha } \\left ( \\frac{\\theta_{\\lambda } v}{\\alpha + \\theta_{\\lambda } v}\\right ) ^{n}.\\]]that is , at this stage we could directly work with a negative binomial distribution . as we will see below , only in the marginal case",
    "can we work with ( 4.4 ) . in the multidimensional model we require @xmath152@xmath153 * @xmath128 denotes the expert opinion of expert @xmath130 which predicts the true risk characteristics @xmath126 of his company .",
    "we have @xmath154 & = & e\\left . \\left",
    "[ \\lambda _ { j}\\right\\vert \\theta_{\\lambda } \\right ] = e\\left .",
    "\\left [ n_{j}/v\\right\\vert \\theta_{\\lambda } \\right ] = \\theta_{\\lambda } ,   \\notag \\\\ var\\left .",
    "\\left ( \\delta _ { k}\\right\\vert \\theta_{\\lambda } \\right ) & = & \\theta_{\\lambda } ^{2}/\\xi , ~~\\text { } cv\\left .",
    "\\left ( \\delta _ { k}\\right\\vert \\theta_{\\lambda } \\right ) = \\xi ^{-1/2}.\\end{aligned}\\]]that is , the relative uncertainty cv in the expert opinion does not depend on the value of @xmath126 .",
    "that means that @xmath155 can be given externally , e.g. by the regulator , who is able to give a lower bound to the uncertainty .",
    "moreover , we see that the expert predicts the average frequency for his company . alternatively , @xmath156 can be estimated using method of moments as presented in lambrigger , shevchenko and wthrich ( 2007 ) .",
    "denote @xmath157 , @xmath158 and @xmath159 .",
    "then the joint posterior density of the random vector @xmath160 given observations @xmath161",
    ", @xmath162 is by bayes theorem@xmath163here , the likelihood terms and the prior are made explicit , @xmath164note that the intensities @xmath165 are non - observable . therefore we take the integral over their densities to obtain the posterior distribution of the random variable @xmath126 given @xmath166    @xmath167    given @xmath126 , the distribution of the number of losses @xmath168 is negative binomial .",
    "hence , one could start with a negative binomial model for @xmath168 .",
    "the reason for the introduction of the random intensities @xmath152 is that we will utilize them to model dependence between different risk cells , by introducing dependence between @xmath169 .",
    "typically , a closed form expression for the marginal posterior function of @xmath126 , given @xmath170 can not be obtained , except in this single risk cell setting . in general",
    ", we will integrate out the latent variables @xmath171 numerically through a mcmc approach to obtain an empirical distribution for the posterior of @xmath172 .",
    "this empirical posterior distribution then allows for the simulation of @xmath146 and @xmath173 , respectively , conditional on the observations @xmath174 .      as in the previous section",
    "we will illustrate our methodology by presenting the frequency model construction . in this section",
    "we will extend the single risk cell frequency model to the general multiple risk cell setting .",
    "this will involve formulation of the multivariate posterior distribution .",
    "@xmath3    1 .",
    "the copula parameters @xmath47 are modelled by a random vector @xmath176 with the prior density @xmath177 ; @xmath178 and @xmath176 are independent .",
    "2 .   @xmath179 and @xmath180 : @xmath181 @xmath32 @xmath182 @xmath40 with marginal distributions @xmath183 and the copula @xmath184 .",
    "thus the joint density of @xmath185 is given by @xmath186where @xmath187 denotes the marginal density .",
    "3 .    4 .",
    "@xmath7    [ multivariatecasemodelassumptions ]    for convenience of notation , define :    * @xmath188 $ ] - frequency intensities for all risk profiles and years ; * @xmath189 $ ] - annual number of losses for all risk profiles and years ; * @xmath190 $ ] - expert opinions on mean frequency intensities for all experts and risk profiles .    *",
    "prior structure * @xmath191 and @xmath192 in the following examples , _ a priori , _ the risk characteristics @xmath193 are independent gamma distributed : @xmath194 with hyper - parameters @xmath195 and @xmath196 .",
    "this means that _ a priori _ the risk characteristics for the different risk classes are independent .",
    "that is , if the company has a bad risk profile in risk class @xmath3 then the risk profile in risk class @xmath197 need not necessarily also be bad .",
    "dependence is then modelled through the dependence between the intensities @xmath198 .",
    "if this is not appropriate then , of course , this can easily be changed by assuming dependence within @xmath199    in the simulation experiments below we consider cases when the copula is parameterized by a scalar @xmath114 .",
    "additionally , we are interested in obtaining inferences on @xmath114 implied by the data only so we use an uninformative constant prior on the ranges [ -1,1 ] , ( 0,30 ] and [ 1,30 ] in the case of gaussian , clayton and gumbel copulas respectively .    * posterior density . * the marginal posterior density of random vector @xmath200 given data of counts @xmath201 and expert opinions @xmath202 is    @xmath203",
    "posterior ( [ posteriormultivariate ] ) involves integration and sampling from this distribution is difficult . here",
    "we present a specialized mcmc simulation methodology known as a slice sampler to sample from the desired target posterior distribution @xmath204 , @xmath205 .",
    "marginally taken samples of @xmath178 and @xmath206 are samples from @xmath207 which can be used to make inference for required quantities",
    ".    it will be convenient to define the exclusion operators , @xmath208 , @xmath209 and @xmath210 .",
    "for example :    * @xmath211 $ ] - frequency intensities for all risk profiles and years , excluding risk profile 1 from year 2 ; * @xmath212 $ ] - frequency intensities for all risk profiles and years , excluding all profiles for year 2 ; * @xmath213.$ ]    sampling from @xmath214 or @xmath207 via closed form inversion sampling or via rejection sampling is not typically an option .",
    "there are many reasons for this .",
    "firstly , only for specific copula models will closed form tractable expression for the marginal @xmath215 be attainable , certainly not for the models we consider in this paper . secondly , even for the expression of the joint posterior",
    "@xmath216 it is typically only possible to sample from the conditional distributions sequentially via numerical inversion sampling techniques which is highly computational and inefficient in high dimensions . additionally , we would like a technique which is independent of the potentially arbitrary choice in specifying a copula function for the dependence between @xmath169 .",
    "hence , we utilize an mcmc framework which we make general enough to work for any choice of copula model , developed next .",
    "we separate the analysis into two parts .",
    "firstly , we condition on knowledge of the copula parameters @xmath217 , where @xmath92 denotes the true copula parameters used to generate the data .",
    "this allows us to demonstrate that if the copula parameters @xmath114 are known , we can perform estimation of other parameters accurately under joint inference .",
    "the second part involves joint estimation of @xmath114 and @xmath116 to demonstrate the accuracy of the joint inference procedure developed .",
    "note that , the model for this second part has not been formally introduced but is a simple extension of model assumptions multivariatecasemodelassumptions .",
    "here we assume the copula parameter has been estimated _ a priori _ and so estimation only involves model parameters .",
    "such a setting may arise for example if the copula parameter is already estimated via a ml estimator .",
    "the proposed sampling procedure we develop is a particular class of algorithms in the toolbox of mcmc methods .",
    "it is an alternative to a gibbs sampler known as a univariate slice sampler .",
    "we note that to implement a gibbs sampler or a univariate slice sampler one needs to know the form of the full conditional distributions .",
    "however , unlike the basic gibbs sampler the slice sampler does not require sampling from these full conditional distributions .",
    "derivations of the posterior full conditionals,@xmath218are presented in appendix b.      to include the estimation of the copula parameter @xmath114 jointly with the parameters @xmath178 and latent intensities @xmath219 in our bayesian framework , we assume that it is constant in time and model it by a random variable @xmath206 with some prior density @xmath220 .",
    "the full conditional posterior of the copula parameter , denoted @xmath221 is given by@xmath222 for the full derivation of the scalar case see , appendix b.    in the following section we provide intuition for our choice of univariate slice sampler as compared to alternative markov chain algorithms .",
    "in particular we describe the advantages that the slice sampler has compared to more standard markov chain samplers , though we also point out the additional complexity involved .",
    "we verify the validity of the slice sampling algorithm for those not familiar with this specialized algorithm and we then describe some intricacies associated with implementation of the algorithm .",
    "this is followed by a discussion of some extensions we developed when analyzing the oprisk model .",
    "the technical details of the actual algorithm are provided in appendix c.      the full conditionals given in equations ( [ postfullcondtheta ] ) , ( postfullcondlambda ) , ( [ postfullcondrho ] ) do not take standard explicit closed forms and typically the normalizing constants are not known in closed form .",
    "therefore this will exclude straightforward inversion or basic rejection sampling being used to sample from these distributions .",
    "therefore one may adopt a metropolis hastings ( mh ) within gibbs sampler to obtain samples , see for example gilks , richardson and spiegelhalter ( 1996 ) and robert and casella ( 2004 ) for detailed expositions of such approaches . to utilize such algorithms",
    "it is important to select a suitable proposal distribution .",
    "quite often in high dimensional problems such as ours , this requires tuning of the proposal for a given target distribution .",
    "hence , one incurs a significant additional computational expense in tuning the proposal distribution parameters off - line so that mixing of the resulting markov chain is sufficient .",
    "an alternative not discussed here would include an adaptive metropolis hastings within gibbs sampling algorithm , see atachade and rosenthal ( 2005 ) and rosenthal ( 2009 ) .",
    "here we take a different approach which utilizes the full conditional distributions , known as a univariate slice sampler , see neal ( 2003 ) .",
    "we demonstrate how effective a univariate slice sampler is for our model .",
    "slice sampling was developed with the intention of providing a `` black box '' approach for sampling from a target distribution which may not have a simple form .",
    "the slice sampling methodology we develop will be automatically tailored to the desired target posterior . as such it does not require pretuning and in many cases",
    "will be more efficient than a mh within gibbs sampler .",
    "the reason for this , pointed out by neal ( 2003 ) , is that a mh within gibbs has two potential problems .",
    "the first arises when a mh approach attempts moves which are not well adapted to local properties of the density , resulting in slow mixing of the markov chain .",
    "secondly , the small moves arising from the slow mixing typically lead to traversal of a region of posterior support in the form of a random walk .",
    "therefore , @xmath223 steps are required to traverse a distance that could be traversed in only @xmath224 steps if moving consistently in the same direction .",
    "a univariate slice sampler can adaptively change the scale of the moves proposed avoiding problems that can arise with the mh sampler when the appropriate scale of proposed moves varies over the support of the distribution .",
    "a single iteration of the slice sampling distribution for a toy example is presented in figure [ slicesamplerfigure ] .",
    "the intuition behind slice sampling arises from the fact that sampling from a univariate distribution @xmath225 can always be achieved by sampling uniformly from the region under the distribution @xmath226 obtaining a slice sample follows two steps : sample a value @xmath227 $ ] and then sample a value uniformly from @xmath228 @xmath229 .$ ] this procedure is repeated and by discarding the auxiliary variable sample @xmath230 one obtains correlated samples @xmath231 from @xmath232 .",
    "neal ( 2003 ) , demonstrates that a markov chain @xmath233 constructed in this way will have stationary distribution defined by a uniform distribution under @xmath225 and the marginal of @xmath234 has desired stationary distribution @xmath226 additionally , mira and tierney ( 2002 ) proved that the slice sampler algorithm , assuming a bounded target distribution @xmath225 with bounded support , is uniformly ergodic .    similar to a deterministic scan gibbs sampler , the simplest way to extend the slice sampler to a multivariate distribution is by considering each full conditional distribution in turn .",
    "note , discussion relating to the benefits provided by random walk behaviour suppression , as achieved by the slice sampler , are presented in the context of non - reversible markov chains , see diaconis , holmes and neal ( 2000 ) .",
    "additionally , we only need to know the target full conditional posterior up to normalization , see neal ( 2003 ) p. 710 .",
    "this is important in this example since solving the normalizing constant in this model is not possible analytically . to make more precise",
    "the intuitive description of the slice sampler presented above , we briefly detail the argument made by neal on this point .",
    "suppose we wish to sample from a distribution for a random vector @xmath235 whose density @xmath236 is proportional to some function @xmath237 .",
    "this can be achieved by sampling uniformly from the @xmath238-dimensional region that lies under the plot of @xmath239 .",
    "this is formalised by introducing the auxiliary random variable @xmath240 and defining a joint distribution over @xmath62 and @xmath240 which is uniform over the region @xmath241 below the surface defined by @xmath242 , given by @xmath243where @xmath244 .",
    "then the target marginal density for @xmath245 is given by@xmath246as required .",
    "there are many possible procedures to obtain samples of @xmath247 .",
    "the details of the implemented algorithm undertaken in this paper are provided in appendix c.    * * extensions**we note that in the bayesian model we develop , in some cases strong correlation between the parameters of the model will be present in the posterior , see figure [ fig2 ] . in more extreme cases ,",
    "this can cause slow rates of convergence of a univariate sampler to reach the ergodic regime , translating into longer markov chain simulations . in such a situation several approaches",
    "can be tried to overcome this problem.the first involves the use of a mixture transition kernel combining local and global moves .",
    "for example , we suggest local moves via a univariate slice sampler and global moves via an independent metropolis hastings ( imh ) sampler with adaptive learning of its covariance structure , such an approach is known as a hybrid sampler , see comparisons in brewer , aitken and talbot ( 1996 ) .",
    "alternatively , for the global move if determination of level sets in multiple dimensions is not problematic , for the model under consideration , then some of the multivariate slice sampler approaches designed to account for correlation between parameters can be incorporated , see neal ( 2003 ) for details .",
    "this is beyond the scope of this paper .",
    "another approach to break correlation between parameters in the posterior is via transformation of the parameter space .",
    "if the transformation is effective this will reduce correlation between parameters of the transformed target posterior .",
    "sampling can then proceed in the transformed space , and then samples can be transformed back to the original space .",
    "it is not always straightforward to find such transformations .",
    "a third alternative is based on simulated tempering , introduced by marinari and parisi ( 1992 ) and discussed extensively in geyer and thompson ( 1995 ) . in particular a special version of simulated tempering , first introduced by neal ( 1996 )",
    "can be utilised in which on considers a sequence of target distributions @xmath248 constructed such that they correspond to the objective posterior in the following way , @xmath249 ^{\\gamma _ { l}}\\]]with sequence @xmath250 then one uses the slice sampling algorithm presented and replaces @xmath251 with @xmath252 .    running a markov chain such that at each iteration @xmath253 we target posterior @xmath254 and then only keeping samples from the markov chain corresponding to situations in which @xmath255 can result in a significant improvement in exploration around the posterior support .",
    "this can overcome slow mixing arising from a univariate sampling regime .",
    "the intuition for this is that for values of @xmath256 the target posterior is almost uniform over the space , resulting in large moves being possible around the support of the posterior , then as @xmath257 returns to a value of @xmath258 , several iterations later , it will be in potentially new unexplored regions of posterior support .    as an extension we developed a simulated tempering slice sampler to obtain samples from the posterior @xmath259 . in our development",
    "we utilize a sine function , @xmath260 , for @xmath257 which has its amplitude truncated to ensure it ranges between @xmath261 $ ] . that is the function is truncated at @xmath255 for extended iteration periods for our simulation index @xmath253 to ensure the sampler spends significant time sampling from the actual posterior distribution .",
    "we note that application of the tempering proved useful and improved mixing of the markov chain .",
    "however , for simulation examples presented in the remainder of this paper it was sufficient to use the basic univariate slice sampler presented previously , which is more computationally efficient than the tempered version .",
    "note , in the application of tempering one must discard many simulated states of the markov chain , whenever @xmath262 .",
    "there is however a computational way to avoid discarding these samples , see gramacy , samworth and king ( 2007 ) .",
    "finally , we note that there are several alternatives to a mh within gibbs sampler such as a basic gibbs sampler combined with adaptive rejection sampling ( ars ) , gilks and wild _ _",
    "_ _ ( 1992 ) .",
    "note ars requires distributions to be log concave .",
    "alternatively an adaptive version of this known as the adaptive metropolis rejection sampler could be used , see gilks , best and tan ( 1995 ) .",
    "in this section we demonstrate and compare the performance of our sampling methodology on several different copula models . we intend to demonstrate the appropriate behaviour of our bayesian models as a function of the number of annual years , in the presence of highly biased expert opinions .",
    "this will be achieved through simulation studies using the sampling techniques detailed above to perform inference on model parameters .",
    "the intention will be to demonstrate the appropriate convergence and accuracy as a function of data sample size .",
    "hereafter , we study the case of dependence between intensities of two risks and set risk cell volumes @xmath263      here , we study the estimation of model parameters in two cases .",
    "the first case involves two low frequency risks . in the second case",
    ", one risk has low frequency while another risk has high frequency . in these two cases we present results for the univariate slice sampler under scenarios involving :",
    "data generated independently for each risk profile and data generated using a gaussian , clayton and gumbel copulas .",
    "only one expert opinion is assumed for each risk .",
    "we present the parameter estimates as a function of data size for each of the specified correlation levels .",
    "that is , we study the accuracy of the parameter estimates as the number of observations increases .",
    "simulation results are obtained by creating independently 20 data sets each of length 20 years , then for each data set simulations are performed for subsets of the data going for 1 , 2 , 5 , 10 , 15 and 20 years .",
    "we then average the performance of posterior estimates over these independent simulations .",
    "the markov chains are run for 50,000 iterations with 10,000 iterations discarded as burnin .",
    "the simulation time depends on the number of risk profiles , the number of observations and expert opinions and the length of the markov chain . in performing the analysis we studied three cases and in each case we performed the following steps ,    1 .   simulate a data set of appropriate number of years according to the procedure specified in appendix a. 2 .",
    "obtain correlated mcmc samples from the target posterior distribution after discarding burnin samples , @xmath264 .",
    "3 .   estimate desired posterior quantities such as posterior mean of parameters of interest and posterior standard deviations .",
    "repeat stages 1 - 3 for 20 independent data realisations and then average the results .    * * joint : * the results are obtained by mcmc samples taken from @xmath265 with the correct copula model and copula parameter used in the sampler .",
    "this is the procedure that should be performed in a real application . * * marginal : * results are obtained by mcmc samples taken from @xmath266which is the posterior in the case of independence .",
    "this is equivalent to marginal estimation where single risk cell data is analyzed separately , see section 4.1 .",
    "* * benchmark : * to verify the results we also consider the case where we assume perfect knowledge of the realized random process for random vector @xmath267 .",
    "we then perform inference on @xmath178 without the additional uncertainty arising from estimating @xmath268 in this regard this represents a benchmark for which we may compare the performance of our simulations .",
    "in particular , it is obtained by samples taken from @xmath269 conditional on the true simulated realizations of random variables @xmath267 .",
    "* example 1 : low frequency risk profiles . *",
    "we set the true parameter values of @xmath270 and @xmath271 to be @xmath272 and @xmath273 respectively .",
    "also we choose the expert s opinion on the true parameters to be an underestimate in risk profile 1 with @xmath274 and an overestimate for risk profile 2 with @xmath275 the model parameters were set to @xmath276 , @xmath277 , and prior distribution parameters @xmath278 , @xmath279 .",
    "the results for this simulation study , presented in tables [ table1 ] and [ table2 ] , show the appropriate convergence of the estimates of parameters @xmath270 and @xmath271 as a function of the data size , demonstrating how well this simulation procedure works under these models .",
    "in addition we note that as expected from credibility theory we observe that joint estimation is better than the marginal , i.e. the posterior standard deviations for @xmath270 and @xmath271 are less when joint estimation is used .",
    "in addition the rate of convergence of the posterior mean for @xmath144 to the true value is faster under the joint estimation .",
    "note , the standard errors in the posterior mean and standard deviation were calculated and found to be strictly in the range of 1 - 5% for the simulations presented .    in figure [ fig2 ] , corresponding to gaussian ,",
    "clayton and gumbel copula models respectively , we demonstrate the estimated density @xmath280 if we had perfect knowledge of the latent process parameters @xmath267 . in this way we compare the exact posterior with perfect knowledge of the correlation structure as captured by the copula model which here we assume is known .",
    "obtaining these plots involved a particular realized data set of length 20 years .",
    "for all copulas two values of @xmath114 were considered : @xmath281 and @xmath282 for gaussian copula ; @xmath283 and @xmath284 for clayton copula ; and @xmath285 and @xmath286 for the gumbel copula . these plots of the joint marginal posterior distribution of @xmath178",
    "demonstrate clearly that the standard practice in the industry of performing marginal estimation of risk profiles will lead to incorrect results when estimating quantities based on the distribution of @xmath287 .",
    "* example 2 : one low frequency and one high frequency risk profile . *",
    "we set the true values of @xmath270 and @xmath271 to be @xmath288 and @xmath289 respectively .",
    "also we choose the expert s opinion on the true parameters to be an under estimate in risk profile 1 with @xmath290 and an over estimate for risk profile 2 with @xmath291 the model parameters were set to @xmath292 , @xmath293 , @xmath278 , @xmath294 .",
    "the simulation results and comparisons are developed in the same approach as example 1 and again the standard errors in the estimates were in the range 1 - 5% .",
    "the results can be found in tables [ table3 ] and [ table4 ] .      here",
    "we estimate @xmath295 and @xmath206 jointly .",
    "for this example , the model settings from example 1 were used and one data set of length 20 years randomly generated was utilized .",
    "the simulation was performed by taking 150,000 iterations of the sampler and discarding the first 20,000 as burnin .",
    "results for these simulations are contained in table [ table5 ] .",
    "these results demonstrate that our model and estimation methodology is successfully able to estimate jointly the risk profiles and the correlation parameter .",
    "this is seen to be the case for all the models we consider in this paper .",
    "it is also clear that with few observations , e.g. @xmath296 , and a vague prior for the copula parameter , it will be difficult to accurately estimate the copula parameter .",
    "this is largely due to the fact that the posterior distribution in this case is diffuse .",
    "additionally , with a small amount of data it appears that accurately estimating the copula parameter is most difficult in the gumbel model .",
    "however , as the number of observations increases the accuracy of the estimate improves in all models and the estimates are reasonable in the case of @xmath297 or @xmath298 years of data .",
    "additionally , we could further improve the accuracy of this prediction if we incorporated expert opinions into the prior specification of the copula parameter , instead of using a vague prior .",
    "overall , we have demonstrated that combination of all the relevant sources of data can be achieved under our model .",
    "then with this study we show that our sampling methodology has the ability to estimate jointly all the model parameters including the copula parameter .",
    "this is a key step forward in model development and estimation for oprisk models .",
    "we further envisage that one can extend this methodology to more sophisticated and flexible copula based models with more than one parameter .",
    "this should be relatively trivial since the methodology we developed applies directly .",
    "however , the challenge in the case of a more sophisticated copula model relates to finding a relevant choice of prior distribution on the correlation structure .",
    "* full predictive distribution .",
    "* as a final comment in this section we point out an important additional outcome of obtaining samples from the joint posterior distribution of the model parameters and the correlation .",
    "this relates to construction of the full predictive annual loss distribution , accounting for parameter uncertainty .",
    "typically practitioners will take point estimates of all parameters and then condition on these point estimates to empirically construct the predictive distribution and then calculate risk measures to be reported such as var . here",
    "we comment that a more robust approach to prediction can now be performed . using our methodology",
    ", we can construct the full predictive distribution after removing the parameter uncertainty from the model considered , including the uncertainty arising from the correlation parameter . to achieve this",
    "we would consider the full predictive distribution : latexmath:[\\[\\pi\\left(z_{t+1}|n_{1:t},\\delta_{1:t}\\right ) = \\int \\pi\\left(z_{t+1}|\\bm{\\theta}_{\\lambda},\\theta_{\\rho}\\right)\\pi\\left(\\bm{\\theta}_{\\lambda},\\theta_{\\rho }    here , we used the model assumptions that given @xmath178 and @xmath206 we have that @xmath300 is independent from the observations @xmath301 . in practice to obtain samples from this full predictive distribution involves taking the steps demonstrated in appendix a with a minor modification .",
    "if one wanted to simulate @xmath224 annual losses from the full predictive distribution , this would involve first running the slice sampler for @xmath224 iterations after burnin .",
    "then for each iteration @xmath253 one would use the state of the markov chain @xmath302 in the simulation procedure detailed in appendix a. we also note that it is trivial under our methodology to extend this full predictive distribution sampling to the case of frequency and severities .",
    "this paper introduced a dynamic oprisk model which allows for significant flexibility in correlation structures introduced between risk profiles . next a bayesian framework was established to allow inference and estimation under this model to be performed , whilst at the same time allowing incorporation of alternative data sources into the inferential procedure .",
    "then a novel simulation procedure was developed for the bayesian model presented , in the case of dependence between frequency risk profiles .",
    "simulations were performed to demonstrate the accuracy of this procedure in multiple bivariate examples .",
    "comparisons were made between marginal estimation and a benchmark estimation procedure . in all simulations ,",
    "the estimation of the model parameters was accurate and behaviour of the estimates of posterior mean and standard deviation presented , smoothed over multiple data realizations , was as expected .",
    "initially the influence of the biased expert opinion observation influenced the results and as the size of the data set for actual annual loss counts grew , the estimations improved in accuracy .",
    "clearly , the joint estimation will outperform marginal estimation when forming predictions of future counts and rates in year @xmath2 , given estimates based on data up to year @xmath147 .",
    "additionally , we demonstrated highly accurate estimation of the copula parameter , jointly with the model parameters .",
    "additionally , simulations were performed in the models @xmath303 and @xmath304 for the clayton copula model in which the copula parameter is also unknown . though the simulation time was increased as a factor of the number of risk cells , the results and performance were as presented for the bivariate models , making this approach suitable for practical purposes .    finally , the main objective of the paper is to preset the framework for the multivariate problem and to demonstrate estimation in this setting .",
    "application of the framework to real data is the subject of further research . in this paper ,",
    "the estimation procedure is presented for frequencies only but it can be extended in the same manner as presented to severities .",
    "in general , given marginal and copula parameters @xmath305 , the simulation of the annual losses for year @xmath306 , when risk profiles are dependent , can be done as described below .    1 .",
    "simulate @xmath54-variate @xmath307 from a @xmath54 dimensional copula @xmath308 2 .   calculate @xmath309 and @xmath310 3 .",
    "sample @xmath311 from @xmath312 .",
    "4 .   sample iid @xmath313 from @xmath314 .",
    "calculate annual losses @xmath315 .",
    "repeat steps 1 - 5 , @xmath316 times to get @xmath316 random samples of the annual losses @xmath317 .",
    "note , to simulate from the full predictive distribution of annual losses , add simulation of @xmath305 from the posterior distribution ( e.g. using slice sampler methodology ) as an extra step before step 1 .",
    "simulation of the random variates from a copula in step 1 in the case of gaussian , clayton and gumbel copulas can be done as described below .    * gaussian copula : *    1 .",
    "simulate @xmath66-variate @xmath318 from @xmath319 , where @xmath320 is a normal distribution with zero means , unit variances and correlation matrix @xmath93 .",
    "2 .   calculate @xmath321 .",
    "obtained @xmath322 is a @xmath66-variate from a gaussian copula .",
    "* archimedean copulas : * the clayton and gumbel copulas are members of the archimedean family of copulas . the @xmath66-dimensional archimedean copulas can be written as @xmath323 with @xmath324 a decreasing function known as the generator for the given copula , see frees and valdez ( 1998 ) .",
    "the generator and inverse generator for the clayton ( @xmath325 ) and gumbel ( @xmath326 ) copulas are given by @xmath327 where @xmath328 is a copula parameter .",
    "simulation from such a copula can be achieved following the algorithm provided in melchiori ( 2006 ) :    1 .   sample @xmath66 independent random variates @xmath329 from a uniform distribution @xmath330 $ ] .",
    "2 .   simulate @xmath331 from @xmath332 such that laplace transform of @xmath333 satisfies @xmath334 and @xmath335 3 .",
    "find @xmath336 for @xmath337 4 .",
    "calculate @xmath338 for @xmath339    the obtained @xmath340 is a @xmath66-variate from @xmath66-dimensional archimedean copula .",
    "what remains is to define the relevant distribution @xmath332 for the clayton and gumbel copulas . for the clayton copula",
    ", @xmath332 is a gamma distribution with shape parameter given by @xmath341 and unit scale .",
    "for the gumbel copula , @xmath332 is from the @xmath342stable family @xmath343 with the following parameters shape @xmath344 , skewness @xmath345 , scale @xmath346 and location @xmath347 in the gumbel case , the density for @xmath333 has no analytic form and the simulation from this distribution can be achieved using the algorithm from nolan ( 2007 ) to efficiently generate the required samples from the univariate stable distribution .",
    "note , in part 1 and part 2 we are conditioning on the copula parameter @xmath114 , this notation is dropped for simplicity .",
    "it is only explicitly introduced in part 3 .    *",
    "part 1 : * using bayes theorem @xmath348 using the model structure to exploit conditional independence properties we note that @xmath349 which specifies the full conditional distributions for the @xmath17 component @xmath350 as @xmath351    * part 2 : * the next full conditional distribution we must specify is given by @xmath352we then use conditional independence properties of the model to get@xmath353 giving the full conditional we are interested in , up to proportionality , @xmath354 we now demonstrate that this expression simplifies significantly",
    ". we can show that the terms @xmath355 simplify as follows : @xmath356 finally , we are left with the full conditional distribution @xmath357    * part 3 : * the full conditional distribution for the copula parameter is given by @xmath358",
    "here , we provide the explicit details involved into implementation of a slice sampler algorithm within a gibbs sampler framework discussed in section 5 . the iterations of the slice sampler are denoted by simulation index @xmath359 .      1",
    ".   initialize @xmath360 the parameter vector @xmath361 $ ] randomly or deterministically .",
    "2 .   repeat while @xmath362 1 .",
    "set @xmath363 = \\left [ \\bm{\\theta } _ { \\lambda , l-1},\\bm{\\lambda } _ { 1:t , l-1},\\theta_{\\rho , l-1}\\right ] $ ] 2 .",
    "sample @xmath3 uniformly from set @xmath364 + sample new parameter value @xmath365 from the full conditional posterior distribution @xmath366 .",
    "+ set @xmath367 .",
    "sample @xmath3 uniformly from set @xmath368 and @xmath7 uniformly from set @xmath369 + sample new parameter value @xmath370 from the full conditional posterior distribution @xmath371 .",
    "+ set @xmath372 .",
    "4 .   sample new parameter value @xmath373 from the full conditional posterior distribution + @xmath374 .",
    "+ set @xmath375 .",
    "3 .   @xmath376 and return to 2 .",
    "the sampling from the full conditional posteriors in stage 2 uses a univariate slice sampler , see figure [ slicesamplerfigure ] .",
    "we present the case where we wish to sample the next iteration of the markov chain from @xmath377          there are many approaches that could be used in determination of the level sets @xmath382 of our density @xmath383see neal ( 2003 ) [ p.712 , section 4 ] . for simplicity in our proceeding examples",
    "we assume that we can restrict our parameter space to the finite ranges and we argue that this is reasonable since we can consider the finite bounds for example set according to machine precision for the smallest and largest number we can represent on our computing platform .",
    "this is not strictly required , but simplifies the coding of the algorithm .",
    "we then perform what neal ( 2003 ) terms a stepping out and a shrinkage procedure , the details of which are contained in neal ( 2003 ) [ p.713 , figure 1 ] .",
    "the basic idea is that given a sampled vertical level @xmath378 then the level sets @xmath382 can be found by positioning an interval of width @xmath384 randomly around @xmath385 .",
    "this interval is expanded in step sizes of width @xmath384 until both ends are outside the slice .",
    "then a new state is obtained by sampling uniformly from the interval until a point in the slice @xmath382 is obtained .",
    "points that fail can be used to shrink the interval .            + marginal & 3.72 ( 2.04 ) & 4.10 ( 1.98 ) & 4.08 ( 1.62 ) & 4.64 ( 1.42 ) & 5.13 ( 1.31 ) & 5.24 ( 1.27 ) +   + benchmark & 4.32 ( 1.88 ) & 4.50 ( 1.67 ) & 4.84 ( 1.46 ) & 5.17 ( 1.26 ) & 5.19 ( 1.12 ) & 5.21 ( 1.02 ) + joint & 3.91 ( 2.01 ) & 4.41 ( 1.72 ) & 4.37 ( 1.56 ) & 4.76 ( 1.33 ) & 5.10 ( 1.21 ) & 4.95 ( 1.05 ) + marginal & 3.72 ( 2.05 ) & 4.09 ( 1.97 ) & 4.06 ( 1.61 ) & 4.48 ( 1.37 ) & 5.07 ( 1.29 ) & 5.04 ( 1.13 ) +   + benchmark & 4.81 ( 1.82 ) & 5.17 ( 1.72 ) & 5.13 ( 1.42 ) & 4.96 ( 1.13 ) & 5.10 ( 0.98 ) & 5.00 ( 0.84 ) + joint & 4.19 ( 2.03 ) & 4.92 ( 1.87 ) & 5.05 ( 1.56 ) & 4.87 ( 1.26 ) & 4.96 ( 1.08 ) & 4.90 ( 0.93 ) + marginal & 3.91 ( 2.12 ) & 4.43 ( 2.10 ) & 4.54 ( 1.74 ) & 4.47 ( 1.36 ) & 4.75 ( 1.22 ) & 4.72 ( 1.08 ) +   + benchmark & 4.32 ( 1.98 ) & 4.46 ( 1.70 ) & 4.86 ( 1.41 ) & 5.08 ( 1.16 ) & 5.16 ( 1.01 ) & 5.11 ( 0.88 ) + joint & 4.33 ( 2.06 ) & 4.21 ( 1.80 ) & 4.54 ( 1.56 ) & 4.96 ( 1.23 ) & 5.01 ( 1.05 ) & 4.98 ( 0.93 ) + marginal & 3.84 ( 2.08 ) & 3.76 ( 1.87 ) & 4.17 ( 1.62 ) & 4.63 ( 1.41 ) & 4.74 ( 1.22 ) & 4.72 ( 1.07 ) +      + marginal & 6.74 ( 2.74 ) & 6.84 ( 2.59 ) & 6.46 ( 2.16 ) & 5.91 ( 1.67 ) & 5.74 ( 1.40 ) & 5.47 ( 1.31 ) +   + benchmark & 5.98 ( 2.29 ) & 5.84 ( 2.04 ) & 5.46 ( 1.60 ) & 5.47 ( 1.31 ) & 5.43 ( 1.14 ) & 5.41 ( 1.04 ) + joint & 6.37 ( 2.55 ) & 6.01 ( 2.23 ) & 5.63 ( 1.80 ) & 5.40 ( 1.43 ) & 5.43 ( 1.25 ) & 5.36 ( 1.12 ) + marginal & 6.59 ( 2.72 ) & 6.49 ( 2.54 ) & 6.01 ( 2.07 ) & 5.75 ( 1.64 ) & 5.62 ( 1.43 ) & 5.57 ( 1.26 ) +   + benchmark & 5.57 ( 1.91 ) & 5.41 ( 1.69 ) & 5.20 ( 1.40 ) & 4.90 ( 1.10 ) & 5.09 ( 0.96 ) & 5.07 ( 0.85 ) + joint & 6.39 ( 2.48 ) & 5.92 ( 1.92 ) & 5.36 ( 1.64 ) & 5.06 ( 1.22 ) & 5.13 ( 1.17 ) & 5.00 ( 1.02 ) + marginal & 6.69 ( 2.74 ) & 6.56 ( 2.55 ) & 5.92 ( 2.04 ) & 5.40 ( 1.56 ) & 5.37 ( 1.36 ) & 5.24 ( 1.17 ) +   + benchmark & 5.83 ( 2.35 ) & 5.51 ( 2.02 ) & 5.38 ( 1.57 ) & 5.15 ( 1.18 ) & 5.20 ( 1.02 ) & 5.12 ( 0.89 ) + joint & 6.05 ( 2.47 ) & 5.96 ( 2.17 ) & 5.47 ( 1.76 ) & 5.21 ( 1.27 ) & 5.12 ( 1.07 ) & 5.12 ( 0.94 ) + marginal & 6.42 ( 2.67 ) & 6.26 ( 2.50 ) & 5.92 ( 2.04 ) & 5.67 ( 1.62 ) & 5.52 ( 1.37 ) & 5.36 ( 1.18 ) +      + marginal & 3.72 ( 2.04 ) & 4.07 ( 1.97 ) & 4.05 ( 1.61 ) & 4.48 ( 1.37 ) & 4.94 ( 1.26 ) & 5.13 ( 1.13 ) +   + benchmark & 4.07 ( 1.76 ) & 4.22 ( 1.53 ) & 4.61 ( 1.36 ) & 5.10 ( 1.20 ) & 5.10 ( 1.08 ) & 5.20 ( 0.99 ) + joint & 3.86 ( 1.96 ) & 4.39 ( 1.86 ) & 4.46 ( 1.51 ) & 4.84 ( 1.33 ) & 5.10 ( 1.22 ) & 5.24 ( 1.10 ) + marginal & 3.72 ( 2.04 ) & 4.08 ( 1.97 ) & 4.05 ( 1.61 ) & 4.48 ( 1.37 ) & 4.94 ( 1.26 ) & 5.13 ( 1.13 ) +   + benchmark & 4.45 ( 1.65 ) & 4.86 ( 1.55 ) & 4.89 ( 1.32 ) & 4.82 ( 1.07 ) & 5.00 ( 0.95 ) & 4.92 ( 0.83 ) + joint & 4.15 ( 2.01 ) & 4.84 ( 1.95 ) & 4.92 ( 1.59 ) & 4.69 ( 1.33 ) & 4.97 ( 1.20 ) & 4.96 ( 1.04 ) + marginal & 3.98 ( 2.10 ) & 4.53 ( 2.09 ) & 4.54 ( 1.74 ) & 4.47 ( 1.36 ) & 4.74 ( 1.21 ) & 4.72 ( 1.08 ) +   + benchmark & 4.14 ( 1.90 ) & 4.20 ( 1.58 ) & 4.65 ( 1.32 ) & 4.95 ( 1.11 ) & 5.06 ( 0.97 ) & 5.04 ( 0.87 ) + joint & 4.36 ( 2.16 ) & 4.17 ( 1.85 ) & 4.68 ( 1.57 ) & 5.10 ( 1.34 ) & 5.21 ( 1.21 ) & 5.24 ( 1.01 ) + marginal & 3.84 ( 2.17 ) & 3.75 ( 1.87 ) & 4.17 ( 1.62 ) & 4.64 ( 1.41 ) & 4.75 ( 1.22 ) & 4.79 ( 1.09 ) +      + marginal & 10.89 ( 3.74 ) & 10.78 ( 3.60 ) & 10.18 ( 3.19 ) & 9.70 ( 2.67 ) & 9.64 ( 2.31 ) & 9.48 ( 2.14 ) +   + benchmark & 10.44 ( 3.48 ) & 10.50 ( 3.24 ) & 10.25 ( 2.81 ) & 10.51 ( 2.39 ) & 10.78 ( 2.05 ) & 10.04 ( 1.88 ) + joint & 10.68 ( 3.63 ) & 10.13 ( 3.35 ) & 9.57 ( 2.87 ) & 9.60 ( 2.36 ) & 9.31 ( 2.03 ) & 9.23 ( 1.82 ) + marginal & 10.89 ( 3.74 ) & 10.78 ( 3.60 ) & 10.18 ( 3.19 ) & 9.70 ( 2.67 ) & 9.64 ( 2.31 ) & 9.48 ( 2.15 ) +   + benchmark & 10.04 ( 3.21 ) & 10.07 ( 2.99 ) & 9.88 ( 2.58 ) & 9.59 ( 2.08 ) & 9.97 ( 1.87 ) & 9.97 ( 1.66 ) + joint & 10.58 ( 3.54 ) & 10.03 ( 3.19 ) & 9.29 ( 2.69 ) & 9.82 ( 2.22 ) & 9.93 ( 1.97 ) & 9.78 ( 1.73 ) + marginal & 10.94 ( 3.75 ) & 10.92 ( 3.61 ) & 10.13 ( 3.17 ) & 9.32 ( 2.60 ) & 9.30 ( 2.25 ) & 9.45 ( 1.96 ) +   + benchmark & 10.10 ( 3.55 ) & 9.88 ( 3.23 ) & 10.08 ( 2.74 ) & 9.98 ( 2.22 ) & 10.17 ( 1.95 ) & 10.06 ( 1.74 ) + joint & 10.10 ( 3.61 ) & 10.18 ( 3.39 ) & 9.44 ( 2.87 ) & 9.98 ( 2.33 ) & 9.85 ( 1.99 ) & 9.63 ( 1.75 ) + marginal & 10.61 ( 3.76 ) & 10.51 ( 3.59 ) & 10.11 ( 3.17 ) & 9.70 ( 2.66 ) & 9.45 ( 2.27 ) & 9.39 ( 1.98 ) +"
  ],
  "abstract_text": [
    "<S> in this paper , we model dependence between operational risks by allowing risk profiles to evolve stochastically in time and to be dependent . </S>",
    "<S> this allows for a flexible correlation structure where the dependence between frequencies of different risk categories and between severities of different risk categories as well as within risk categories can be modeled . the model is estimated using bayesian inference methodology , allowing for combination of internal data , external data and expert opinion in the estimation procedure . </S>",
    "<S> we use a specialized markov chain monte carlo simulation methodology known as slice sampling to obtain samples from the resulting posterior distribution and estimate the model parameters .    </S>",
    "<S> * keywords : * dependence modelling , copula , compound process , operational risk , bayesian inference , markov chain monte carlo , slice sampling .    </S>",
    "<S> * gareth w.  peters * +    * pavel v.  shevchenko ( _ corresponding author _ ) * +    * mario v.  wthrich * +    * first version : 31 october 2007 ; this version : 11 april 2009 + this is a preprint of an article published in + the journal of operational risk 4(2 ) , pp . </S>",
    "<S> 69 - 104 , 2009 www.journalofoperationalrisk.com * </S>"
  ]
}