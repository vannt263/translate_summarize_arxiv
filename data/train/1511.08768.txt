{
  "article_text": [
    "estimating the gradient of a given function ( with or without noise ) is often an important part of problems in reinforcement learning , optimization and manifold learning . in reinforcement learning ,",
    "policy - gradient methods are used to obtain an unbiased estimator for the gradient .",
    "the policy parameters are then updated with increments proportional to the estimated gradient @xcite .",
    "the objective is to learn a locally optimum policy .",
    "reinforce and pgpe methods ( policy gradients with parameter - based exploration ) are popular instances of this approach ( see @xcite for details and comparisons , @xcite for a survey on policy gradient methods in the context of actor - critic algorithms ) .",
    "in manifold learning , various finite difference methods have been explored for gradient estimation @xcite , @xcite .",
    "the idea is to use the estimated gradient to find the lower dimensional manifold where the given function actually lives .",
    "optimization , i.e. finding maximum or minimum of a function , is a ubiquitous problem that appears in many fields wherein one seeks zeroes of the gradient .",
    "but the gradient itself might be hard to compute .",
    "gradient estimation techniques prove particularly useful in such scenarios .",
    "a further theoretical justification is facilitated by the results of @xcite . in @xcite",
    ", it was shown that given a connected and locally connected metric probability space @xmath0 ( i.e. , @xmath1 is a compact metric space with metric @xmath2 and @xmath3 is a probability measure on the borel @xmath4-algebra of @xmath5 ) , under suitable conditions , any function @xmath6 is close ( in @xmath7 ) to a function @xmath8 on a lower dimensional space . as a special case , a similar fact can be proved for real - values @xmath9-lipschitz functions on @xmath10 with metric @xmath11 ( see theorem 1.2 in @xcite ) .",
    "this suggests that sparse gradients can be expected for functions on high dimensional spaces with adequate regularity conditions .    over the years",
    "gradient estimation has also become an interesting problem in its own right .",
    "one would expect that the efficiency of a given method for gradient estimation also depends on the properties of function @xmath12 .",
    "we consider one such class of problems in this paper .",
    "suppose we have a continuously differentiable function @xmath13 where @xmath14 is large , such that the gradient @xmath15 lives mostly in a lower dimensional subspace .",
    "this means that one can throw out most of the coordinates of @xmath15 in a suitable local basis without incurring too much error . in this case , computing @xmath16 is clearly a waste of means .",
    "dimensionality reduction techniques for optimization problems is an active area of research and several useful methods for this have been developed in the mathematical and engineering community @xcite , @xcite .",
    "if in addition the function evaluations are expensive , most gradient estimation methods become inefficient .",
    "such is the case , e.g. , if a single function evaluation is the output of a large time consuming simulation .",
    "this situation our specific focus .",
    "the problem of expensive function evaluations does not seem to have attracted much attention in machine learning literature .",
    "however , there has been quite a lot of work on optimization of functions with expensive evaluation .",
    "most methods , however , focus on learning a good surrogate for the original function ( see @xcite , @xcite , @xcite ) .    to handle the first issue , ideas from compressive sensing can be applied .",
    "compressive sensing theory tells us that an @xmath17-sparse vector can be reconstructed from @xmath18 measurements .",
    "this means that one does not need the information about @xmath15 in all @xmath14 directions , a much smaller number of measurements would suffice .",
    "these ideas are frequently used in signal as well as image processing ( see , e.g. , @xcite ) . to remedy the latter difficulty",
    ", we use an idea from simultaneous perturbation stochastic approximation ( spsa ) due to spall @xcite , viz . , the simultaneous perturbation ( sp ) .",
    "we begin by explaining the proposed method for gradient estimation .",
    "important ideas and results from compressive sensing and spsa that are relevant to this work are discussed in section  [ cs ] and section  [ ge ] respectively .",
    "we state the main result in section  [ main ] .",
    "section  [ app ] consists of applications to manifold learning and optimization with simulated examples .",
    "some notational preliminaries are as follows . by @xmath19",
    "we denote the usual euclidean norm in @xmath20 as well as the frobenius norm for matrices over @xmath21 . throughout , ` a.s . '",
    "stands for ` almost surely ' , i.e. , with probability one .",
    "as mentioned above , if function evaluations are expensive , sp works well to avoid the problem of computing function multiple times .",
    "however , if the gradient is sparse it makes sense to use the ideas of compressive sensing to our advantage .",
    "combining these two techniques helps us overcome the problem of too many function evaluations and also exploit the sparse structure of the gradient .",
    "the idea is to use sp to get sufficient number of observations to be able to recover the gradient via @xmath22-minimization .",
    "we describe the method in detail in the following sub - sections .",
    "assume that @xmath23 is an approximately sparse vector .",
    "the idea of compressive sensing is based on the fact that typically a sparse vector contains much less information or complexity than its apparent dimension",
    ". therefore one should be able to reconstruct @xmath15 with considerable accuracy with much less information than that of order @xmath14 .",
    "we will make these ideas more precise in the forthcoming discussion on compressive sensing .",
    "we state all the results for vectors in @xmath20 .",
    "all of these results also hold for vectors over @xmath24 .",
    "we start by defining what we mean by sparse vectors .",
    "the support of a vector @xmath25 is defined as : @xmath26 : x_j \\neq 0 \\}.\\ ] ] where @xmath27 = \\ { 1 , 2 , \\ldots , n \\}$ ] .",
    "the vector @xmath25 is called @xmath17-sparse if at most @xmath17 of its entries are nonzero , i.e. , if @xmath28    we assume that the observed data @xmath29 is related to the original vector @xmath25 via @xmath30 for some matrix @xmath31 , where @xmath32 . in other words , we have a linear measurement process for observing @xmath33 .",
    "the theory of compressive sensing tells us that if @xmath33 is sparse , then it can recovered from @xmath34 by solving a convex optimization problem . in particular , given a suitable matrix @xmath35 and appropriate @xmath36 , the following @xmath22-minimization problem recovers @xmath33 exactly . @xmath37 where @xmath38 are the @xmath36 observations .",
    "these ideas were introduced by e. candes and t. tao in their seminal paper on near - optimal signal reconstruction @xcite . in this paper",
    ", the authors proved that the matrices suitable for the recovery need to have what is called the restricted isometry property ( rip ) . a large class of random matrices satisfy the rip with quantifiable ` high probability ' and are therefore suitable for reconstruction via @xmath22-minimization .",
    "in particular , subgaussian matrices have been shown to have rip with high probability and are suitable for the aforementioned reconstruction scheme for @xmath18 .",
    "this gives the explicit relationship between the sparsity level @xmath17 , the dimension of the original vector @xmath14 and the dimension of the observed data @xmath36 . in recent times",
    "some work has been done to construct deterministic matrices with this restricted isometry property @xcite .",
    "the current known lower bound on @xmath36 for deterministic matrices is of the order of @xmath39 where @xmath17 is the sparsity .",
    "thus random matrices are a better choice for linear measurement for reconstruction via compressive sensing .    for the scope of this paper , we consider robust recovery options using gaussian random matrices , i.e. , matrices whose entries are realizations of independent standard normal random variables",
    ".    matrices with more structure like random partial fourier matrix or in general bounded orthonormal systems can also be used as meaurement matrices for compressive sensing techniques . given a random draw of such a matrix with associated constant @xmath40",
    ", a fixed @xmath17-sparse vector @xmath33 can be reconstructed via @xmath22-minimization with high probability provided @xmath41 . for more details on random sampling matrices in compressive sensing see chapter 12 of @xcite .",
    "the crucial point here is that it is enough that the given vector is sparse in some basis .",
    "a more detailed discussion on various aspects of compressive sensing can be found in @xcite . in real - life situations",
    "the measurements are almost always noisy .",
    "it may also happen that the original vector @xmath33 is not sparse but is close to a sparse vector .",
    "in other words , we would like the reconstruction scheme to be robust and stable . theorem 9.13 of @xcite gives explicit error bounds for stable and robust recovery where @xmath35 is a subgaussian matrix .",
    "the bound is expressed in terms of @xmath42 , the distance of @xmath33 from the nearest @xmath17-sparse vector , and the measurement error .",
    "see @xcite,@xcite and @xcite for more on robust and stable recovery via compressive sensing . + we assume that our observations @xmath43 are noisy .",
    "the following theorem gives an error bound on the reconstruction from noisy measurements using a gaussian matrix .",
    "[ robustgaussian ] let @xmath44 be a random draw of a gaussian random matrix and @xmath25 be a @xmath17-sparse vector .",
    "let @xmath45 be noisy measurements of @xmath33 such that @xmath46 .",
    "if for @xmath47 and some @xmath48 , @xmath49 then with probability at least @xmath50 every @xmath51 that minimizes @xmath52 subject to @xmath53 approximates @xmath33 with @xmath54-error @xmath55    see theorem 9.29 in @xcite for a statement for stable and robust recovery via gaussian matrices .",
    "as discussed above we have a fairly good reconstruction of a sparse gradient @xmath15 given a sufficient number of observations @xmath56 .",
    "however , as mentioned before , the problem often is the unavailability of these observations .",
    "even though observations for @xmath15 are not readily available , one may compute @xmath57 s using the available information , that is , noisy measurements of the function @xmath12 .",
    "note that we have , however , assumed that the function evaluations are computationally expensive .",
    "we will now address this issue of estimating @xmath15 with low computational overheads .",
    "+ let @xmath58 denote the @xmath59 coordinate direction for @xmath60 .",
    "we consider the finite difference approximation @xmath61 where @xmath62 and @xmath63 . by taylor s theorem ,",
    "the error of estimation is @xmath64 where @xmath65 denotes the hessian .",
    "this estimate requires @xmath66 function evaluations . replacing the ` two sided differences ' @xmath67 above by `",
    "one sided differences ' @xmath68 reduces this to @xmath69 , which is still large for large @xmath14 .",
    "given that we have assumed @xmath12 to be such that the function evaluations are computationally expensive , an alternative method is desirable .",
    "we use the method devised by spall @xcite in the context of stochastic gradient descent , known as simultaneous perturbation stochastic approximation ( spsa ) .",
    "+ recall the stochastic gradient descent scheme @xcite @xmath70 , \\label{sabasic}\\ ] ] where :    * @xmath71 is a square - integrable martingale difference sequence , viz .",
    ", a sequence of zero mean random variables with finite second moment satisfying @xmath72 = 0 \\ \\forall \\ k \\geq 0,\\ ] ] i.e. , it is uncorrelated with the past .",
    "we assume that it also satisfies @xmath73 <",
    "\\infty , \\label{mgbdd}\\ ] ] * @xmath74 are step - sizes satisfying @xmath75    the term in square bracket in ( [ sabasic ] ) stands for a noisy measurement of the gradient . under mild technical conditions , @xmath76 can be shown to converge a.s .  to a local minimum of @xmath12 @xcite .",
    "the idea is that the incremental adaptation due to the slowly decreasing step - size @xmath77 averages out the noise @xmath71 , rendering this a close approximation of the classical gradient descent with vanishing error @xcite . in practice",
    "the noisy gradient is often unavailable and one has to use an approximation @xmath78 thereof using noisy evaluations of @xmath12 , e.g. , the aforementioned finite difference approximations , which lead to the kiefer - wolfowitz scheme .",
    "that is where the sp scheme comes in .",
    "we describe this next .",
    "+ let @xmath79 be i.i.d .",
    "zero mean random variables such that    * @xmath80 is independent of @xmath81 . * @xmath82 .    then by taylor s theorem",
    ", we have that for @xmath83 : @xmath84 note that since @xmath85 s are i.i.d .",
    "zero mean random variables , we have for @xmath86 , @xmath87 = 0.\\ ] ] hence for the purpose of stochastic gradient descent , the second term in acts as a zero mean noise ( i.e. , martingale difference ) term that can be clubbed with @xmath88 as martingale difference noise and gets averaged out by the iteration .",
    "this serves our purpose , since the above scheme requires only two function evaluations per iterate given by @xmath89 + m_i(k+1).\\ ] ] our idea is to generate @xmath90 according to the scheme discussed above .",
    "+ it should be mentioned that spall also introduced another approximation based on a single function evaluation ( see , e.g. , @xcite , chapter 10 ) .",
    "but this suffers from numerical issues due to the ` small divisor ' problem , so we do not pursue it here .",
    "+      as mentioned in the introduction , the idea is to combine the sp and compressive sensing to obtain a sparse approximation of @xmath15 .",
    "note that while sp gives an estimate with zero - mean error , the final estimate of gradient obtained after compressive sensing may not be unbiased . to avoid the error from piling up we need to average out the error at sp stage .",
    "we propose the following algorithm for estimating gradient of @xmath12 .",
    "let @xmath91 denote the row vectors of @xmath35 .",
    "+    [ gea ]    * initialization : * + @xmath92 random gaussian matrix",
    ". +    @xmath93  for  @xmath94 .    @xmath95 repeat and average over @xmath57 s @xmath96 times to get @xmath97 .",
    "@xmath98 where @xmath99 denotes the error .",
    "@xmath95 solve the @xmath22-minimization problem to obtain @xmath90 : + @xmath100    * output : estimated gradient @xmath101 .",
    "*    the following theorem states that with high probability such an approximation is close \" to the actual gradient .",
    "[ mainthm ] let @xmath13 be a continuously differentiable function with bounded sparse gradient .",
    "then for @xmath102 such that it satisfies the bound in  , @xmath103 , and given @xmath63 ( as in ) and @xmath104 ( as in theorem  [ robustgaussian ] ) , @xmath15 can be estimated by a sparse vector @xmath90 such that with probability at least @xmath105 , @xmath106 where , @xmath107 .",
    "let @xmath31 be a gaussian matrix such that @xmath36 satisfies  .",
    "then , following the same idea as in  , we have : @xmath108 so we get @xmath109 where we quantify the ` error ' below .",
    "the above computation is carried out @xmath96 times independently , keeping the matrix @xmath35 fixed and choosing the random vector @xmath110 according to the distribution defined in section  [ ge ] .",
    "the reason for this additional averaging is as follows .",
    "the reconstruction in compressive sensing need not give an unbiased estimate , since it performs a nonlinear ( minimization ) operation .",
    "thus it is better to do some pre - processing of the sp estimate ( which is nearly , i.e. , modulo the @xmath111 term , unbiased ) to reduce its variance .",
    "we do so by repeating it @xmath96 times with independent perturbations and taking its arithmetic mean .",
    "this may seem to defeat our original objective of reducing function evaluations , but the @xmath96 required to get reasonable error bounds is not large as our analysis shows later , and the computational saving is still significant ( see ` remark [ rem ] ' below ) .",
    "+ denote by @xmath112 the measurement obtained at @xmath113 iteration of sp . the error for a single iteration",
    "is given by @xmath114 denote by @xmath115 .",
    "so , @xmath116 are zero - mean conditionally ( given past iterates ) independent random variables .",
    "the error vector after @xmath96 iterations is given by @xmath117    in order to apply the ideas from compressive sensing as in theorem  [ robustgaussian ] , we need to have a bound on the error @xmath118 .",
    "this is obtained as follows .",
    "let @xmath119 be a constant such that the @xmath111 term above is bounded in absolute value by @xmath120 .",
    "@xmath121 can , e.g. , be a bound on @xmath122 by the mean value theorem , where we use the frobenius norm .",
    "choose @xmath123 and @xmath124 .",
    "then , by hoeffding s inequality we have , @xmath125    choose the number of iterations , @xmath126 .",
    "then , @xmath127 we define @xmath90 to be the reconstruction of the gradient using @xmath36 measurements .",
    "that is , @xmath90 solves the following optimization problem : @xmath128 where @xmath34 is as in .",
    "our claim then follows from bound in and theorem  [ robustgaussian ] .",
    "[ rem ] note that the minimum number of iterations of sp required to obtain a good \" estimate of @xmath15 is given by @xmath129 for a suitable constant @xmath130 .",
    "the above @xmath90 can now be used as an effective gradient in various problems involving gradients of high - dimensional functions .",
    "two such applications are discussed in the next section .",
    "we consider the applications of our method to manifold learning and optimization problems . the gradient estimates obtained using our method",
    "can be used to estimate the gradient outer product matrix or can be plugged into an optimization scheme . in the former case , along with an example , we also provide error bounds on the estimated and actual gradient outer product matrix . for the latter case",
    ", we look at an example and provide suitable modifications to existing algorithms to achieve faster convergence .",
    "algorithm 1 below which is based on theorem  [ mainthm ] , is used for gradient estimation .",
    "there are various algorithms available for carrying out the @xmath22-minimization .",
    "a detailed discussion of these algorithms can be found in @xcite , chapter 15 of @xcite .",
    "here we use the homotopy method .",
    "[ gea ]    * initialization : * + @xmath92 random gaussian matrix .",
    "+    @xmath131 + error as obtained in equation  .",
    "@xmath132 minimization using @xmath133 .",
    "* output : estimated gradient @xmath101 . *    here @xmath134 denotes the @xmath22-recovery from observations @xmath34 and gaussian random matrix @xmath35 using the homotopy method .",
    "all the simulations were performed on matlab using the available toolbox for @xmath22-minimization ( berkeley database : http://www.eecs.berkeley.edu/  yang / software/ l1benchmark/ ) .    consider a function @xmath135 given by @xmath136 where , @xmath137 is @xmath138-dimensional matrix with @xmath139 non - zero elements per row .",
    "let @xmath35 be a random gaussian matrix that is used for measurement .",
    "we consider @xmath140 measurements .",
    "figure  [ fig : sim ] shows the performance of the proposed method with varying number of sp iterations .",
    "figure  [ fig : comp1 ] and [ fig : comp1blown ] show the comparison between our method and naive sp for estimating gradient with gradually increasing number of iterations for averaging over sp ( the quantity ` @xmath96 ' in ( [ etaerror ] ) ) . as mentioned earlier , since the gradient is assumed to be sparse , using naive sp to compute derivative in each direction seems wasteful . although the error diminishes as the number of iterations for sp increase , the proposed method combining compressive sensing with sp consistently performs better .",
    "figures  [ fig : comp2 ] and [ fig : comp2blown ] show that the proposed method works well with higher sparsity levels too .",
    "it also shows that with higher @xmath17 , performance of naive sp improves .",
    "this is expected .",
    "the extremely high error in the naive sp method ( especially for small @xmath17 ) is owing to the fact that the actual gradient is extremely sparse and in the beginning sp method ends up populating almost all the coordinates .",
    "that contributes to the high percentage of error as seen in the aforementioned figures .     :",
    "performance of the proposed algorithm vs. the sp method with varying number iterations @xmath96 at sp step . ]     : performance of the proposed algorithm vs. the sp method with varying number iterations @xmath96 at sp step . ]    : performance of the proposed algorithm vs. the sp method with varying number iterations @xmath96 at sp step . here",
    "@xmath141 and @xmath142 . ]    : performance of the proposed algorithm vs. the sp method with varying number iterations @xmath96 at sp step . here",
    "@xmath141 and @xmath142 . ]",
    "before we consider specific applications , we illustrate how the percentage error of estimated gradient with varying @xmath96 for different sparsity levels @xmath17 . for appropriately large @xmath36 , for small @xmath96",
    "the error is high ( this matches with the discussion in remark  [ rem ] ) .",
    "as @xmath96 increases the error is much less . as long as @xmath36 satisfies [ mbound ] , the compressive sensing results apply .",
    "figure  [ fig : s ] shows the behaviour of the proposed method with variation in the sparsity , but with constant number of observations .",
    "we consider @xmath143-dimensional vector with @xmath144 observations .",
    "as expected , for a fixed @xmath36 , as the sparsity increases , increasing @xmath96 no longer helps as the compressive sensing results do not apply and the error increases .",
    "@xmath145 was used as a test function for both the simulations .      consider the following semi - parametric model @xmath146 where @xmath147 is noise and @xmath12 is a smooth function @xmath148 of the form @xmath149 .",
    "define by @xmath150 the matrix @xmath151 .",
    "@xmath150 maps the data to a @xmath2-dimensional relevant subspace .",
    "this means that the function @xmath12 depends on a subspace of smaller dimension given by range@xmath152 ( note that this is essentially the local view in manifold learning : @xmath150 can vary with location . ) .",
    "the vectors or the directions given by the vectors @xmath153 are called the effective dimension reducing directions or e.d.r .",
    "the question is : how to find the matrix @xmath150 ?",
    "it turns out that if @xmath12 does nt vary in some direction @xmath154 , then @xmath155)$ ] where @xmath156 is the gradient outer product matrix defined as @xmath157 \\ ; \\ ; \\mbox{where } \\ ; g_{ij } = \\big \\langle \\frac{\\partial f}{\\partial x_i}(x ) , \\frac{\\partial f}{\\partial x_j}(x ) \\big \\rangle\\ ] ] and @xmath158 $ ] denotes the expectation over @xmath1 .",
    "lemma 1 from @xcite stated below implies that to find the e.d.r .",
    "directions it is enough to compute @xmath159 $ ] .",
    "consider the semi - parametric model @xmath160 where @xmath147 represents zero mean finite variance noise .",
    "then the espected gradient outer product ( egop ) matrix @xmath156 is of rank at most @xmath2 .",
    "furthermore , if @xmath161 are the eigenvectors associated to the nonzero eigenvalues of @xmath156 , following holds : @xmath162     + clearly , calculating @xmath163 $ ] is computationally heavy .",
    "we therefore try to estimate this matrix .",
    "several methods are known for estimating the egop and this has been a very popular problem in statistics for a while .",
    "the idea of using egop for obtaining e.d.r .  originated in @xcite . while there are other methods based on inverse regression etc .",
    ", most of the efforts have been directed towards getting an efficient way to estimate gradients in order to finally estimate egop ( see @xcite ) . in @xcite ,",
    "the authors use their method of gradient estimation for this purpose .",
    "the idea is to use sample observations @xmath164 for @xmath165 in a neighborhood of the given point @xmath33 and minimize over @xmath166 the error @xmath167 ^ 2,\\ ] ] where @xmath168 are weights ( ` kernel ' ) that favor locality @xmath169 and are typically gaussian , with regularization in a reproducing kernel hilbert space ( rkhs ) .",
    "the minimizer then is the desired estimate . in @xcite",
    "a rather simple rough estimator using directional derivative along each coordinate direction is provided .",
    "the authors demonstrate that for the purpose of finding e.d.r . , a rough estimate such as theirs suffices .",
    "we also propose a method via gradient estimation .",
    "take @xmath170 to be the matrix defined by @xmath171 in other words , @xmath172 , where @xmath90 denotes the estimate of @xmath15 obtained by algorithm 1 .",
    "we impose our previous restrictions on @xmath12 .",
    "that is , the function evaluations at any point are expensive and the gradient of @xmath12 is sparse . in this case",
    "we propose an estimate for @xmath159 $ ] by the mean of @xmath170 over a sample of @xmath173 points given by the set @xmath174 .",
    "by @xmath175 , we shall denote the empirical mean over the sample set @xmath176 .",
    "thus ,    @xmath177    and @xmath178    let @xmath179 from the semi - parametric model in   be a continuously differentiable function with bounded sparse gradient . then , for @xmath103 and some @xmath48 , with probability at least @xmath180 , @xmath181 - \\langle \\widehat{g}\\rangle \\right\\|_2",
    "< \\frac{6r^2}{\\sqrt{r } } \\left ( \\sqrt{\\ln n } + \\sqrt{\\ln \\frac{1}{\\epsilon } } \\right ) + \\frac{2t}{\\tau } \\left ( \\frac{2t}{\\tau } + 2r \\right)\\ ] ] where @xmath173 is the sample size , @xmath182 is such that @xmath183 , @xmath184 is as in theorem  [ mainthm ] and @xmath102 is such that it satisfies the bound in  .",
    "the proof closely follows the line of argument in @xcite .",
    "note that , @xmath185 - \\langle \\widehat{g}(x)\\rangle \\|_2 \\leq \\| e_x[g(x ) ] - \\langle g(x)\\rangle",
    "\\|_2 + \\| \\langle g(x)\\rangle - \\langle \\widehat{g}(x)\\rangle \\|_2.\\ ] ] the idea is to bound each term .",
    "we use concentration inequality for sum of random matrices ( see theorem 2.1 , lecture 23 @xcite and @xcite for more general results ) to claim that for @xmath186 , @xmath185 - \\langle g(x)\\rangle \\|_2 \\leq \\frac{6r^2}{\\sqrt{r } } \\left ( \\sqrt{\\ln",
    "n } + \\sqrt{\\ln \\frac{1}{\\epsilon } } \\right)\\ ] ] with probability @xmath187 . for the second term , it is enough to show that it is bounded for any single sample point @xmath33 .",
    "observe that for any two vectors @xmath154 and @xmath188 , @xmath189 using this we get , for a fixed @xmath33 , @xmath190 with probability @xmath105 , where the last inequality is obtained by applying the bound from theorem  [ mainthm ] .",
    "we now simulate an example to illustrate the decay of the error @xmath191 ( see figure  [ fig : g ] ) .",
    "consider a function @xmath192 given by : @xmath193 where , @xmath194 is a @xmath195-dimensional vector with @xmath139 non - zero elements and @xmath196 corresponds to the @xmath59 dimension of @xmath197 . a @xmath198 gaussian random matrix",
    "is used for the compressive sensing part of the algorithm .",
    "the plot of percentage in normed error between @xmath199 and @xmath200 , i.e. @xmath201 is shown below by varying number of samples @xmath202 to @xmath203 .",
    "remember that due to the bias at compressive sensing step , we need to average out the gradient estimation error at sp step .",
    "this is done in @xmath204 iterations .",
    "learning e.d.r . by estimating the gradient using the method proposed in this paper",
    "was compared with the sgl ( sparse gradient learning ) method proposed in @xcite using the same function as above and an exponential kernel ( see http://www2.stat.duke.edu/~sayan/soft.html for details ) . here ,",
    "@xmath205 and the measurement matrix is a @xmath206 gaussian matrix .",
    "the sp step is averaged over @xmath207 iterations .",
    "@xmath207 samples were considered for the sgl method with the neighborhood radius of of @xmath208 .",
    "sparsity of the gradient vector is @xmath209 .",
    "we consider next a typical problem of function minimization , but only consider a function with sparse gradient .",
    "in other words , we want to minimize @xmath197 where @xmath210 is a continuously differentiable real - valued lipschitz function such that function evaluation at a point in @xmath20 is typically expensive .",
    "we also assume that @xmath14 is large and that @xmath15 is sparse .",
    "in addition , we assume that the critical points of @xmath12 ( i.e. , the zeros of @xmath15 ) are isolated .",
    "( this is generically true unless there is overparametrization . )",
    "the idea is to use the stochastic gradient scheme ( [ sabasic ] ) with the standard assumptions ( [ mgbdd ] ) , ( [ steps ] ) .",
    "it follows from the theory of stochastic approximation ( see @xcite , chapter 2 ) that under above conditions , the solution of the random difference equation ( [ sabasic ] ) tracks with probability one the trajectory of the solution of a limiting o.d.e .  as long as the iterates remain bounded , which they do under mild additional conditions on @xmath12",
    ". following @xcite , chapter 2 , we use this so called ` o.d.e .",
    "approach ' which states that the algorithm will a.s .",
    "converge to the equilibria of the limiting o.d.e .",
    ", which is @xmath211 for this , @xmath12 itself serves as the lyapunov function , leading to the conclusion that the trajectories of ( [ ode ] ) and therefore a.s . , the iterates of ( [ sabasic ] ) will converge to one of its equilibria , viz . , the critical points of @xmath12 .",
    "in fact under additional conditions on the noise , it will converge to a ( possibly random ) stable equilibrium thereof ( _ ibid .",
    "_ , chapter 4 ) .",
    "the stochastic gradient scheme requires @xmath212 at each iteration .",
    "the problem , as noted , often is the unavailability of @xmath212 .",
    "it is therefore important to have a good method for estimating the gradient .",
    "typically one would obtain noisy measurements and hence the estimate will have a non - zero error @xmath99 .",
    "it is known that if the error remains small , the iterates converge a.s .  to a small neighbourhood of some point in the set of equilibria of .",
    "we analyze the resultant error below .",
    "also , the error obtained in sp is zero - mean modulo higher order terms , so one can even take an empirical average over a few separate estimates in order to reduce variance . for high dimensional problems ,",
    "the number of function evaluations remains still small as compared with , e.g. , the classical kiefer - wolfowitz scheme .",
    "we use theorem  [ mainthm ] to justify using sp ( simultaneous perturbation stochastic approximation ) combined with compressed sensing to obtain an approximation for the gradient and then use the above scheme to minimize @xmath12 .    consider the following stochastic approximation scheme : @xmath213\\ ] ] where @xmath214 is the additional error arising due to the error in gradient estimation .",
    "that is , @xmath215 .",
    "if @xmath216 for some small @xmath217 , then the iterates of converge to a small neighbourhood @xmath35 of some point @xmath218 in @xmath219 ( see @xcite and chapter 10 of @xcite ) .",
    "this is ensured by a lyapunov argument as follows .",
    "the limiting o.d.e .",
    "is of the form @xmath220 for some measurable @xmath221 with @xmath222 .",
    "then @xmath223 which is @xmath224 as long as @xmath225 .",
    "therefore @xmath226 will converge to the set @xmath227 assume that the hessian @xmath228 is positive definite , which is generically so for isolated local minima . then for @xmath35 small enough",
    ", the lowest eigenvalue @xmath229 of @xmath230 for @xmath231 is @xmath232 . by mean value theorem ,",
    "@xmath233 for some @xmath234 , so @xmath235 .",
    "thus there is convergence to a ball of radius @xmath236 around @xmath218 .",
    "( a statement to this result without the estimate on the radius of the ball is contained in theorem 1 of @xcite . ) thus we have : +    the stochastic gradient scheme @xmath237\\ ] ] a.s .",
    "converges to a ball of radius @xmath238 centered at some local minimum of @xmath12 , where @xmath90 is the reconstructed gradient as in theorem  [ mainthm ] and @xmath217 is a bound on @xmath239 .",
    "_ proof _ the claim is immediate from the above observations about the perturbed differential equation and theorem 6 , pp .",
    "58 - 59 , @xcite .",
    "@xmath240    observe that we have only discussed asymptotic convergence above .",
    "for real - life optimization problems , however , we must ensure that the scheme in converges to a neighbourhood of @xmath218 in finite time .",
    "this is indeed true and recent concentration - type results ( see @xcite , @xcite ) strengthen the theoretical basis for plugging @xmath101 in place of @xmath212 in stochastic gradient descent schemes .",
    "the results in @xcite involve estimates on lock - in probability , i.e. , the probability of convergence to a stable equilibrium given that the iterates visit its domain of attraction .",
    "an estimate on the number of steps needed to be within a prescribed neighborhood of the desired limit set with a prescribed probability is also obtained .",
    "specifically , the result states that if the @xmath241th iterate is in the domain of attraction of a stable equilibrium @xmath218 , then after a certain number of additional steps , the iterates remain in a small tube around the differential equation trajectory converging to @xmath218 with probability exceeding @xmath242 _ ipso facto _ implying an analogous claim for the probability of remaining in a small neighborhood of @xmath218 after a certain number of iterates .",
    "we refer the reader to @xcite for details . in @xcite ,",
    "an improvement on this estimate is proved under additional regularity conditions on @xmath15 ( twice continuous differentiability ) using alekseev s formula .",
    "we have omitted the details of both the cases as it needs much additional notation to replicate them here .",
    "these would , however , apply to the exact stochastic gradient descent .",
    "since we have an additional error due to approximate gradient as in the preceding theorem , we need to combine the results of _ ibid . _ with the above theorem to make a weaker claim regarding how small the neighborhood of @xmath218 in question can be . furthermore , these claims are about iterates which are in the domain of attraction of a stable equilibrium .",
    "this , however , is not a problem , as ` avoidance of traps ' results as in section 4.3 of @xcite ( see also @xcite , @xcite , @xcite ) ensure that if the noise is rich enough in a certain precise sense , unstable equilibria are avoided with probability one .",
    "note that the gradient descent is a stochastic approximation scheme which itself averages out the noise .",
    "so in principle the averaging over @xmath96 steps at the sp stage in the original algorithm can be skipped .",
    "this means that for a stochastic gradient descent scheme , we cut down the cost of function evaluation even further .",
    "the simulations in the next section confirm that good results are obtained without averaging over sp iterations .",
    "there is , however , a standard trade - off involved between per step computation / speed of convergence , and fluctuations ( equivalently , variance ) of the estimates : any additional averaging improves the latter at the expense of the former .",
    "we compare following three algorithms .    1 .",
    "_ actual gradient descent _ + this is the classical stochastic gradient descent with exact gradient .",
    "+ [ gda ] +   + @xmath243 + @xmath244 be a sequence that satisfies the properties of stepsize listed above . + * iteration : * _ repeat until convergence criteria is met at @xmath245 . at  @xmath246 iteration : _ + @xmath247 + @xmath248 @xmath249 $ ] + * output : @xmath250 * 2 .",
    "_ accelerated gradient method _ + accelerated gradient scheme was proposed by nesterov @xcite .",
    "while gradient descent algorithm has a rate of convergence of order @xmath251 after @xmath17 steps , nesterov s method achieves a rate of order @xmath252 .",
    "we implement the method here to achieve an improvement in the time complexity further .",
    "the idea is to replace the @xmath246 iteration above by the following .",
    "+ [ agda ] at @xmath253 iteration : + @xmath247 + @xmath248 @xmath254 @xmath255 + where , @xmath256 and @xmath257 are as follows : @xmath258 + this gives us faster convergence towards the minimum .",
    "adaptive method _ + another way to achieve a faster convergence rate is to perform the @xmath22-minimization adaptively with the gradient descent .",
    "the idea is to again use the homotopy method for @xmath22-minimization but this part of the algorithm is run for very few iterations .",
    "the intermediate approximation of @xmath15 is then used for performing the stochastic gradient descent .",
    "as expected , the errors are high in the beginning but the convergence is faster .",
    "+ we consider the following function to test our algorithms : @xmath259 where , function @xmath260 and @xmath261 are @xmath262 random matrices .",
    "this is to ensure sparsity of the gradient . here ,",
    "@xmath263 and number of non - zero entries in each column of @xmath264 and @xmath265 are @xmath266 .",
    "number of measurements , @xmath144 .",
    "@xmath35 is a @xmath267 random gaussian matrix .",
    "+ figure  [ fig : sgl ] and [ fig : sglsmalln ] show the comparisons between various algorithms described above for the same function .    [",
    "cols=\"^,^,^,^ \" , ]     as expected , adaptive method turns out to be faster compared to the non - adaptive method which in turn is much faster than the algorithm that computes actual gradients .",
    "incidentally , the classical scheme all but converges in under 400 iterations .",
    "even so it takes more time than the other two which take more iterations .",
    "this is because of the heavy per iterate computation for the classical scheme . from the above table",
    "it is clear that as the dimensionality of the problem increases , adaptive method proves more and more useful compared to the other two algorithms .",
    "we compared our method with the method proposed in @xcite .",
    "the function in is used for the comparison . here ,",
    "number of samples of sgl were @xmath269 , chosen with neighbourhood radius @xmath208 .",
    "in the following scaled down version @xmath270 and @xmath142 .    the time taken by the sgl method and our method was @xmath271 and @xmath272 seconds respectively .",
    "as mentioned earlier , the aim of this paper is to provide a good estimation of gradient when the function evaluations are expensive . in such cases ,",
    "our method would provide a significant gain in terms of function evaluations needed .",
    "while in this example we do see a significant improvement in time taken for the estimation , there is no a priori reason to always expect it",
    ". it will indeed be the case when the function evaluations are ` expensive ' in terms of the time they take .",
    "one expects this to be so when the ambient dimension is high .",
    "we have proposed an estimation scheme for gradient in high dimensions that combines ideas from spall s spsa with compressive sensing and thereby tries to economize on the number of function evaluations .",
    "this has theoretical justification by the results of @xcite .",
    "our method can be extremely useful when the function evaluation is very expensive , e.g. , when a single evaluation is the output of a long simulation .",
    "this situation does not seem to have been addressed much in literature . in very high dimensional problems with sparse gradient , computing estimates for partial derivatives in every direction",
    "is inefficient because of the large number of function evaluations needed .",
    "sp simplifies the problem of repeated function evaluation by concentrating on a single _ random _ direction at each step .",
    "when the gradient vectors in such cases live in a lower dimensional subspace , it also makes sense to exploit ideas from compressive sensing .",
    "we have computed the error bound in this case and have also shown theoretically that this kind of estimation of gradient works well with high probability for the gradient descent problems and in other high dimensional problems such as estimating egop in manifold learning where gradients are actually low - dimensional and gradient estimation is relevant .",
    "simulations show that our method works much better than pure sp .",
    "we thank gpu centre of excellence , iit bombay for providing us with the facility to carry out simulations and prof .",
    "chandra murthy of indian institute of science for helpful discussions regarding compressive sensing .",
    "e.  cands , m.  rudelson , t.  tao and r.  vershynin .",
    "error correction via linear programming . in",
    "_ proceedings of the 46th annual ieee symposium on foundations of computer science ( focs ) _ , pages 295 - 308 , 2005 .",
    "l.  w.  chan , k.  charan , d.  takhar , k.  f.  kelly , r.  g.  baraniuk , g.  richard and d.  m.  mittleman .",
    "a single - pixel terahertz imaging system based on compressed sensing . _ applied physics letters _ , 93(12 ) : 121105 - 121105 - 3 , 2008 .",
    "m.  f.  duarte , m.  a.  davenport , t.  dharmpal , j.  n.  laska , t.  sun , k.  f.  kelly and r.  g.  baraniuk .",
    "single - pixel imaging via compressive sampling .",
    "_ ieee signal processing magazine _ , 25(2 ) : 83 - 91 , march 2008 .      a.  c.  gilbert , m.  j.  strauss , j.  a.  tropp and r.  vershynin .",
    "one sketch for all : fast algorithms for compressed sensing .",
    "_ proceedings of the thirty - ninth annual acm symposium on theory of computing _ , 237 - 246 , 2007 .",
    "i.  grondman , l.  busoniu , g.  a.  d.  lopes and r.  babuska .",
    "a survey of actor - critic reinforcement learning : standard and natural policy gradients .",
    "ieee transactions on systems , man , and cybernetics , part c : applications and reviews , 42(6 ) : 1291 - 1307 , nov .",
    "2012 .",
    "s.  shan , g.  gary wang .",
    "survey of modeling and optimization strategies to solve high - dimensional design problems with computationally - expensive black - box functions .",
    "_ structural and multidisciplinary optimization _ , 41(2 ) , 219 , 2010 .",
    "r.  s.  sutton , d.  mcallester , s.  singh and y.  mansour .",
    "policy gradient methods for reinforcement learning with function approximation . _ advances in neural information processing systems _ , 12 : 1057 - 1063 , mit press , 2000 .",
    "s.  trivedi , j.  wang , s.  kpotufe and g.  shakhnarovich . a consistent estimator of the expected gradient outerproduct . _",
    "proceedings of the thirtieth conference on uncertainty in artificial intelligence _ : 819 - 828 , july 2014 .",
    "q.  wu , j.  guinney , m.  maggioni and s.  mukherjee .",
    "learning gradients : predictive models that infer geometry and statistical dependence .",
    "_ journal of machine learning research _ , 11(1922 ) : 2175 - 2198 , 2010 ."
  ],
  "abstract_text": [
    "<S> we propose a scheme for finding a good \" estimator for the gradient of a function on a high - dimensional space with few function evaluations . </S>",
    "<S> often such functions are not sensitive in all coordinates and the gradient of the function is almost sparse . we propose a method for gradient estimation that combines ideas from spall s simultaneous perturbation stochastic approximation with compressive sensing . </S>",
    "<S> applications to estimating gradient outer product matrix as well as standard optimization problems are illustrated via simulations .    </S>",
    "<S> _ keywords : _ gradient estimation ; compressive sensing ; sparsity ; gradient descent ; gradient outer product matrix . </S>"
  ]
}