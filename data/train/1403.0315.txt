{
  "article_text": [
    "video abstraction aims at providing concise representations of long videos .",
    "it has applications in browsing and retrieval of large volumes of videos  @xcite and also in improving the effectiveness and efficiency of video storage  @xcite .",
    "video abstraction can be categorised into two general groups : video summarisation and video skimming  @xcite .",
    "video summarisation , also known as still image abstraction , static storyboard or static video abstract , is a compilation of representative frames selected from the original video  @xcite .",
    "video skimming , also known as moving image abstraction or moving / dynamic storyboard , is a collection of short video clips  @xcite .",
    "both approaches should preserve the most important content from the video in order to present a comprehensible and understandable description for the end user .",
    "in general , video skimming provides a more coherent and visually attractive result .",
    "it often retains a high - level of linguistic meaning due to its capacity to combine audio and moving elements  @xcite .",
    "however , video summarisation is easier to generate and is not constrained in terms of timing and synchronisation  @xcite .",
    "video summarisation is an active area of research within the computer vision community and it has been applied in various video categories such as wildlife videos  @xcite , sports videos  @xcite , tv documentaries  @xcite , among others . in  @xcite",
    "the various approaches to video summarisation are divided into six techniques consisting of : feature selection , clustering algorithms , event detection methods , shot selection , trajectory analysis and the use of mosaics .",
    "often a combination of techniques is used , for example one of the most common approaches is to combine feature selection with a form of clustering  @xcite .    in",
    "@xcite a video summary is obtained by extracting a feature vector from each frame and then clustering the resulting set of feature vectors .",
    "the smallest clusters are then removed .",
    "a keyframe  a frame that forms part of the video summary  is selected for each cluster centroid by taking the frame whose feature vector is closest to the centroid .",
    "similar approaches are adopted in  @xcite where the major difference is in the choice of feature vector used to represent each frame .",
    "colour histograms are used in  @xcite , motion - based features are used in  @xcite , and saliency maps are used in  @xcite .",
    "each of the previously proposed feature vectors has its drawbacks .",
    "for instance , the colour histogram approach used in  @xcite retains only coarse information about the frame .",
    "motion - based features of  @xcite fail when the motion in the videos is too large .",
    "finally , the saliency maps used in  @xcite perform poorly for cluttered and textured backgrounds . to date , limited work has been done on incorporating texture information to perform video summarisation .",
    "* contributions .",
    "* in this paper we first propose the use of texture information to improve video summarisation .",
    "we propose the use of the computationally efficient and effective bag - of - textures approach ; we conjecture that this will improve video summarisation as it has been successfully applied to a range of image processing tasks , such as matching and classification of natural scenes and faces  @xcite .",
    "the bag - of - textures model divides an image into small patches , extracts appearance descriptors from each patch , quantises each descriptor into a discrete `` visual word '' , and then computes a compact histogram representation  @xcite , providing considerably different information than colour histograms . in addition",
    ", we propose a fusion based system for video summarisation , where both colour and texture information is exploited .",
    "this will allow us to overcome the shortcomings of either approach .",
    "similar approaches have been shown to be advantageous in object classification tasks  @xcite .",
    "we show that our system may be applied not only to short - term videos but also to long - term videos , helping in the detection of the existence of a rare species of fish .",
    "the layout of this paper is as follows . in section [ sec : vis ] we describe in detail our proposed video summarisation method that exploits the benefits of using texture histograms based on the bag - of - textures model . in section [ sec : fvis ]",
    "we present our improved video summarisation method that fuses the visual information provided by both the colour and texture histograms . in section [ sec : eva ] we describe how we evaluate the video summaries of short - term and long - term videos . in section [ sec : experiments ] , we present experiments which show that the proposed methods obtain higher performance than existing methods based on colour histograms .",
    "section [ sec : conclusion ] summarises the main findings .",
    "this section describes our proposed bag - of - textures ( bot ) approach .",
    "there are four main stages :    1 .",
    "pre - processing : the input video is sub - sampled after which each frame is filtered and rescaled .",
    "bot representation : a.   _ local texture features_. each frame is divided into small patches ( blocks ) and from each block we extract 2d - dct features , which is an effective and compact representation  @xcite .",
    "b.   _ dictionary training_. a generic visual dictionary is trained to describe the most commonly occurring textures in an independent training set . c.   _ generation of bot histogram_. each frame is represented by a histogram which is obtained by matching the feature vectors from each block to the dictionary .",
    "keyframe selection : similar frames are grouped into an automatically determined number of clusters .",
    "one keyframe is selected per cluster .",
    "post - processing : in this final stage , we eliminate possible repetitive frames and create the static video summary .",
    "each of these stages is elucidated in the following sections .",
    "the original input video is re - sampled to one frame per second in order to reduce the number of video frames to be examined .",
    "each frame is then converted into gray - scale and re - scaled to be a quarter of its original size , in order to reduce the computational cost of the following stages .",
    "there are often uninformative frames that appear at the beginning and/or the end of a segment that may affect the appearance of a video summary  @xcite .",
    "these frames are usually colour - homogeneous due to fade - in and fade - out effects , and have a small standard deviation of their pixel values",
    ". frames with a standard deviation below a threshold are eliminated .",
    "each frame is divided into @xmath0 overlapping blocks . to each block",
    "we apply the 2d discrete cosine transform ( 2d - dct ) to obtain a @xmath1-dimensional feature vector that represents the local texture information  @xcite .",
    "thus , the local texture feature for the @xmath2-th block of the @xmath3-th frame is @xmath4 .",
    "the dictionary is trained using the _ k_-means algorithm  @xcite by pooling the local texture features from a set of training frames .",
    "the resulting @xmath5 cluster centers @xmath6 represent the local textures ( codewords ) of the dictionary .      in the bot approach the @xmath3-th frame",
    "is represented by a histogram , @xmath7 .",
    "this @xmath5-dimensional histogram represents the relative frequency of the local texture features within the frame .",
    "the @xmath8-th dimension of @xmath7 is the relative frequency of the @xmath8-th local texture feature from the dictionary , similar to @xcite .",
    "the histogram is normalised to sum to one .",
    "thus , each local texture feature can be converted to a local histogram , @xmath9 , of dimension @xmath5 where each dimension @xmath8 is given by , @xmath10 these @xmath0 local histograms can then be summed and normalised to produce the final bot histogram , @xmath11      to obtain a set of keyframes we adopt an approach similar to that of  @xcite .",
    "a keyframe is a frame that forms part of the video summarisation .",
    "the _ k_-means algorithm is used to cluster similar frames into @xmath12 segments , and the resultant centroids are then used to select the keyframes .    initially , the frames are grouped consecutively , assuming that sequential frames share similar content . to automatically determine the number of clusters , @xmath12 , we calculate the euclidean distance between two consecutive frames",
    ". if the distance is greater than a threshold @xmath13 then @xmath12 is incremented . for each cluster centroid the frame whose bot histogram is closest is selected as a keyframe .",
    "a total of @xmath12 keyframes is then reached .",
    "having obtained the initial set of @xmath12 keyframes we then attempt to discard those keyframes which are too similar .",
    "this is achieved by comparing all keyframes against each other .",
    "if the euclidean distance between the bot histograms of the keyframes is smaller than a threshold @xmath13 then one of the two keyframes under consideration is discarded .",
    "this gives the final static video summary that consists of @xmath14 keyframes , where @xmath15 , with @xmath16 standing for utomatic ummary .",
    "lastly , the static video summary is obtained after organising the resulting keyframes in temporal order .",
    "in this section , we present a hybrid system that fuses colour histograms  @xcite and bot texture information , termed as cat ( for * * c**olour * * a**nd * * t**exture ) .",
    "the proposed cat approach to video summarisation has the same 4 stages as our proposed bot video summarisation approach , but with additions in order to obtain colour histograms .",
    "we describe these additions below .    1 .   pre - processing : the input video",
    "is processed in two independent ways .",
    "first , we obtain the bot histograms as described in section  [ sec : pre ] .",
    "second , to obtain the colour histograms we extract the hue component , from the hsv colour space , of the unscaled input frame similar to  @xcite . in both cases",
    "we remove uninformative frames by employing the noise filtering process described in section  [ sec : pre ] .",
    "texture and colour histogram : the bot histogram is the same as explained in section [ sec : fex ] .",
    "the colour histogram , @xmath17 , of the @xmath3-th frame is computed using only the hue component as in  @xcite .",
    "keyframe selection : the bot and colour histograms are clustered using @xmath18-means . this stage is similar to section  [ sec : kmeansbof ] .",
    "the difference lies in the distance measure used to compare all frames against each other .",
    "a.   to select the number of keyframes @xmath12 we combine the information from the bot and colour histograms . when calculating the distance between frame @xmath19 and @xmath20 we use the weighted summation of euclidean distances : @xmath21 + under the constraints @xmath22 , @xmath23 , @xmath24",
    ". b.   each keyframe is selected by finding the frame which is closest to each cluster centroid .",
    "for the cat approach the distance between a frame and a centroid is calculated as a weighted summation of the euclidean distances , as per  .",
    "post - processing : to eliminate similar frames we use the procedure described in section  [ sec : post ] but replace the euclidean distance with the weighted summation of the euclidean distances , as per  .",
    "to evaluate the performance of video summarisation we use two datasets consisting of short- and long - term video data .",
    "the short - term data is obtained from the open video project .",
    "the long - term data is a new dataset that consists of 14 hours of underwater video surveillance which monitors the behaviour of marine wildlife .",
    "we use the 50 videos from the open video project which contain ground truth  @xcite . each ground truth",
    "consists of the summary provided by @xmath25 users .",
    "the users provided the summaries under no restrictions upon length nor appearance of the summaries .    to evaluate the performance on the short - term video data we use the `` comparison of user summaries '' ( cus ) method  @xcite .",
    "this method compares the automatic video summarisation and ground truth by exhaustively calculating the distance between the frames from the automatic summarisation and the ground truth .",
    "two frames are similar if the distance between their respective feature vectors ( histograms ) is less than an evaluation threshold @xmath26 .",
    "if the frames match they are removed from the next iteration of the comparison process . for performance evaluation ,",
    "the distance measure used for the bot approach is the euclidean distance , however , to be consistent with prior work  @xcite , the distance measure for the colour histograms is the @xmath27-norm .",
    "therefore , the distance measure used for cat is the weighted summation of the euclidean distance for the bot histograms and the @xmath27-norm for the colour histograms : @xmath28    various evaluation metrics exist to measure the quality of an automatic video summary .",
    "we use three evaluation metrics so that we can compare our proposed approaches with two state - of - the - art methods  @xcite . to compare with  @xcite we use accuracy ( @xmath29 ) and error ( @xmath30 ) , and to compare with  @xcite we use the @xmath31-measure .    to calculate @xmath29 and @xmath30 , each frame in the automatic video summary",
    "is compared with all frames in the user summary and then the number of matching frames ( @xmath32 ) and non - matching frames ( @xmath33 ) are calculated : @xmath34 where @xmath14 and @xmath35 are the total number of frames from the automatic and user summary , respectively .",
    "the @xmath31-measure , defined as @xmath36 is used to to provide a single number that balances @xmath37 and @xmath38 .",
    "the evaluation metrics are presented as an average .",
    "first , we take the average from the @xmath39 users to obtain @xmath40 , @xmath41 , and @xmath42 ; for each video there are @xmath25 users .",
    "then we take the average across all of the videos to obtain @xmath43 , @xmath44 , and  @xmath45 . in terms of @xmath43",
    "it is desirable to have a high value as it measures the number of matching frames . in terms of @xmath44",
    "it is desirable to have a small value as it measures the number of non - matching frames . with regards",
    "to @xmath45 it is desirable to obtain a high value , which occurs when the @xmath46 and @xmath47 are large .",
    "the long - term videos consist of 14 hours of underwater footage from 33 videos which are on average 25 minutes in duration .",
    "this data was obtained from the nsw - dpi , courtesy of david harasti .",
    "example images are shown in figure  [ fig : bcod_images ] . in each video",
    "there is always at least one segment where a rare species of fish , the black cod , is within view .",
    "normally these videos would be inspected by a human expert to determine if there is an instance of the rare fish within .",
    "we propose that video summarisation can be used to reduce the amount of footage to be viewed in order to detect the existence of this rare species of fish .    using ground truth which provides time - stamps",
    "when this rare species is within view , we examine the effectiveness of video summarisation to provide at least one keyframe in each static video summary with the rare species of interest within view .",
    "this is useful as it presents a way to reduce the time and cost of manually viewing a large amount of video data .",
    "+     to calculate the performance of long - term videos we present results in terms of detection accuracy and the average compression ratio ( @xmath48 ) . detection accuracy refers to whether an instance of the rare species is among any of the chosen keyframes for a static video summary ; @xmath49 would mean that there is at least 1 keyframe of the rare species in @xmath49 of the static video summaries .    to calculate the average compression ratio we first note that because we have long - term videos then for each video there might be many hundreds of keyframes . to present all of these keyframes effectively to the user we re - encode them into a static video summary by presenting each keyframe for @xmath50 seconds .",
    "this gives the user time to effectively view the keyframe .",
    "thus the @xmath51-th long - term video @xmath52 is converted to a static video summary @xmath53 with a compression ratio given by :    @xmath54    where @xmath55 is the duration of a video and the factor of @xmath56 is introduced as there are @xmath56 keyframes per second of the shortened video .",
    "an important part of both the bot and cat approaches is the training of the dictionary to obtain the texture histograms .",
    "to train this dictionary we use 10 frames randomly selected from videos taken from the open video project that have no user summaries , ensuring they are independent of the evaluation dataset .",
    "in addition , the frames selected to train the dictionary look significantly different to the ground truth provided by the users .    to obtain the proposed local texture features we divide each frame into a set of overlapping blocks .",
    "similar to  @xcite we use a block size of @xmath57 with an overlap margin of 6 pixels , and represent each block as a @xmath58 dimensional feature vector containing 2d - dct coefficients .",
    "we extract the first 16 2d - dct coefficients , which represent low - frequency information  @xcite , and omit the first coefficient as it is the most sensitive to illumination changes . with regards to the colour histogram , we quantise the hue component into 16 bins as per  @xcite .",
    "these parameters are the same for all experiments .    the values for the threshold @xmath13",
    ", fusion weight @xmath59 and evaluation threshold @xmath26 were determined experimentally .",
    "for all of the experiments we search for the optimal fusion parameter @xmath60 .",
    "our proposed methods were implemented using the opencv  @xcite and armadillo  @xcite c++ libraries .",
    "we compare the performance against two baseline systems from literature , vsumm  @xcite and vison  @xcite .",
    "the two baseline systems both use colour information as their primary feature .",
    "vsumm uses colour information by retaining only the hue component of hsv and generating a histogram of 16 bins .",
    "vison is a state - of - the - art approach and consists of a histogram of the hsv representation of each frame .",
    "it combines the hsv information in a compressed form such that the hue component is treated with greater importance and results in a histogram of 256 bins .",
    "an initial set of experiments were performed to find the optimal number of components for the dictionary of our proposed texture features . using a fixed number of components @xmath61 and a fixed number of thresholds @xmath62",
    ", we found that using just @xmath63 components provided optimal performance .",
    "we kept the number of components constant for the remainder of our experiments .    in figure",
    "[ fig : comparing ] we present a summary of the average performance for 50 short - videos of our proposed systems , bot and cat , and the two baselines .",
    "two interesting results can be seen from this figure .",
    "first , it can be seen that the texture - only bot system performs better than either the vsumm or vison approaches which primarily use colour information .",
    "the bot system obtains an average @xmath31-measure of @xmath64 , which is a relative improvement of @xmath65 when compared to vison , @xmath66 .",
    "furthermore , the @xmath43 and @xmath44 of the bot system shows that it produces a more accurate summarisation than vsumm and also has the lowest @xmath44 of any system and @xmath44 were supplied for vison in  @xcite . ] .",
    "this suggests that texture information is either equally or more important than colour information for the task of video summarisation .",
    "second , the proposed cat system ( fusing colour histograms and the proposed texture histograms ) performs better than the two baseline systems and the proposed texture - only bot system .",
    "the cat system has an average of @xmath67 , which is a relative improvement of @xmath68 when compared to vison @xmath66 , the previous state - of - the - art approach .     as well as",
    "higher values of @xmath43 and @xmath45 are desired . ]",
    "\\(a )              \\(b )               \\(c )               \\(d )              figure  [ fig : videosummaries ] shows the qualitative results for the automatic summarisation provided by vsumm and vison as well as our proposed bot and cat systems .",
    "it can be seen that vsumm ( figure  [ fig : videosummaries]a ) with @xmath69 , vison ( figure  [ fig : videosummaries]b ) with @xmath70 , and our proposed bot ( figure  [ fig : videosummaries]c ) with @xmath71 contain some keyframes that may not be of interest and/or are repetitive .",
    "in contrast , the proposed cat system ( figure  [ fig : videosummaries]d ) provides the most consistent video summary with @xmath72 .      in this section",
    "we present results on 33 long - term videos which last on average for 25 minutes .",
    "we examine the applicability of video summarisation to long - term videos to efficiently detect a rare species of fish and measure performance in terms of detection accuracy and compression rate ( see section  [ sec : eval : long_term ] ) .",
    "the accuracy and average compression ratio of the algorithm for various thresholds , @xmath73 , is presented in figure  [ fig : exp2_both ] .",
    "it can be seen in figure  [ fig : exp2_both]a that the cat algorithm consistently outperforms the bot and vsumm algorithms .",
    "we attribute this to the fact that the background in these videos is relatively stable and so the colour histograms used in vsumm do not change as often compared to the short - term videos used in  @xcite . in figure",
    "[ fig : exp2_both]b it can be seen that while using the vsumm algorithm provides better average compression ratio than either the bot or cat approaches , it comes at the cost of accuracy . in general",
    "the proposed fusion approach provides the most consistent trade - off between accuracy and average compression ratio .",
    "we take the optimal system at the threshold @xmath74 as this provides a high degree of detection accuracy , @xmath75 , and a good average compression ratio of @xmath76 .",
    "this system will allow a user to see the fish of interest in @xmath75 of the summarised videos while reducing the amount of video data to view by @xmath76 times , more than an order of magnitude .",
    "such an approach would reduce the @xmath77 hours of video data to just @xmath78 minutes , thus enabling significantly more efficient reviewing of the data .",
    "for the 33 long - term videos using the cat , bot and vsumm approaches . ]     for the 33 long - term videos using the cat , bot and vsumm approaches . ]",
    "in this paper , we have proposed the novel use of textures to perform video summarisation .",
    "we proposed to use a visual - bag - of - textures ( bot ) in two ways .",
    "first , a bot system which uses only texture features is proposed and it is shown to outperform two state - of - the - art systems which use colour only , vsumm and vison .",
    "second , a fused system that combines colour and texture ( cat ) is proposed and it is shown to provide further improvements .    both of our proposed systems outperform two state - of - the - art approaches , vsumm and vison , which use colour features .",
    "experiments on 50 short - term videos , obtained from the open video project , show that our proposed texture - only system ( bot ) obtains an @xmath31-measure of @xmath79 , which is better than either vsumm or vison which obtain an average @xmath31-measure of @xmath80 and @xmath81 , respectively .",
    "furthermore , our fused system ( cat ) demonstrates that combining colour and texture features yields state - of - the - art performance with an average @xmath31-measure of @xmath82 .",
    "we have also shown that video summarisation can be applied effectively to long - term videos . using 33 long - term surveillance videos , in our case underwater surveillance footage",
    ", we have shown that video summarisation can be used to significantly reduce the amount of footage to view , by up to a factor of 27 , with only a minor degradation in the information content .",
    "future work should examine alternative features and application settings with a particular emphasis for long - term videos .",
    "for instance , emphasising the importance of foreground objects  @xcite should be explored , as well as explicit modelling of movement ( or actions ) of such objects  @xcite . moreover , the applicability of video summarisation to cctv surveillance footage should also be considered ."
  ],
  "abstract_text": [
    "<S> we present a novel approach to video summarisation that makes use of a bag - of - visual - textures ( bot ) approach . </S>",
    "<S> two systems are proposed , one based solely on the bot approach and another which exploits both colour information and bot features . on 50 short - term videos from the open video project we show that our bot and fusion systems both achieve state - of - the - art performance , obtaining an average f - measure of 0.83 and 0.86 respectively , a relative improvement of 9% and  13% when compared to the previous state - of - the - art . when applied to a new underwater surveillance dataset containing 33 long - term videos , </S>",
    "<S> the proposed system reduces the amount of footage by a factor of 27 , with only minor degradation in the information content . </S>",
    "<S> this order of magnitude reduction in video data represents significant savings in terms of time and potential labour cost when manually reviewing such footage . </S>"
  ]
}