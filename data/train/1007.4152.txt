{
  "article_text": [
    "this work is motivated by a generalization of the classical maximum coverage problem which arises in the study of optimal experimental designs .",
    "this problem may be formally defined as follows : given @xmath6 positive semidefinite matrices @xmath7 of the same size and an integer @xmath8 , solve : @xmath9 } & \\quad { \\operatorname{rank}}\\big ( \\sum_{i \\in i } m_i \\big ) \\label{maxrank_intro } \\tag{$p_0$}\\\\   \\operatorname{s.t . } & \\quad { \\operatorname{card}}(i ) \\leq n , \\nonumber\\end{aligned}\\ ] ] where we use the standard notation @xmath10:=\\{1,\\ldots , s\\}$ ] and @xmath11 denotes the cardinality of @xmath12 . when each @xmath13 is diagonal , it is easy to see that problem   is equivalent to a max - coverage instance , by defining the sets @xmath14 , so that the rank in the objective of problem   is equal to @xmath15 .    a more general class of problems arising in the study of optimal experimental designs is obtained by considering a _ deformation _ of the rank which is defined through a spectral function .",
    "given @xmath0 $ ] , solve : @xmath16 } n_i \\leq n , \\nonumber\\end{aligned}\\ ] ] where @xmath17 is the sum of the eigenvalues of @xmath18 } n_i m_i$ ] raised to the exponent @xmath1 : if the eigenvalues of the positive semidefinite matrix @xmath18 } n_i m_i$ ] are @xmath19 ( counted with multiplicities ) , @xmath17 is defined by @xmath20 } n_i m_i \\big)^p = \\sum_{k=1}^m   \\lambda_k^p.\\ ] ] we shall see that problem   is the limit of problem   as @xmath21 indeed . on the other hand , the limit of problem   as",
    "@xmath22 is a knapsack problem ( in fact , it is the trivial instance in which the @xmath23 item has weight @xmath3 and utility @xmath24 ) .",
    "note that a matrix @xmath13 may be chosen @xmath25 times in problem  , while choosing a matrix more than once in problem   can not increase the rank .",
    "therefore we also define the binary variant of problem  : @xmath26 } n_i \\leq n\\right\\ } \\tag{$p_p^\\textrm{bin}$ } \\label{ppbin}\\ ] ] we shall also consider the case in which the selection of the @xmath23 matrix costs @xmath27 , and a total budget @xmath28 is allowed .",
    "this is the budgeted version of the problem : @xmath29 } c_i n_i \\leq b\\right\\ } \\tag{$p_p^\\textrm{bdg}$ } \\label{ppbdg}\\ ] ]    throughout this article , we use the term _ design _ for the variable @xmath30 .",
    "we say that @xmath31 is a @xmath32__replicated design _ _ if it is feasible for problem  , a @xmath32__binary design _ _ if @xmath31 is feasible for problem  , and a @xmath33__budgeted design _ _ when it satisfies the constraints of  .",
    "the theory of _ optimal design of experiments _ plays a central role in statistics .",
    "it studies how to best select experiments in order to estimate a set of parameters . under classical assumptions ,",
    "the best linear unbiased estimator is given by least square theory , and lies within confidence ellipsoids which are described by a positive semidefinite matrix depending only on the selected experiments .",
    "the _ optimal design of experiments _ aims at selecting the experiments in order to make these confidence ellipsoids as small as possible , which leads to more accurate estimators .",
    "a common approach consists in minimizing a scalar function measuring these ellipsoids , where the function is taken from the class of @xmath34-information functions proposed by kiefer  @xcite .",
    "this leads to a combinatorial optimization problem ( decide how many times each experiment should be performed ) involving a spectral function which is applied to the information matrix of the experiments . for @xmath350,1]$ ] , the kiefer s @xmath34-optimal design problem is equivalent to problem   ( up to the exponent @xmath36 in the objective function ) .",
    "in fact , little attention has been given to the combinatorial aspects of problem   in the optimal experimental design literature .",
    "the reason is that there is a natural relaxation of the problem which is much more tractable and usually yields very good results : instead of determining the exact number of times @xmath25 that each experiment will be selected , the optimization is done over the fractions @xmath37 $ ] , which reduces the problem to the maximization of a concave function over a convex set ( this is the theory of _ approximate optimal designs _ ) . for the common case ,",
    "in which the number @xmath38 of experiments to perform is large and @xmath39 ( where @xmath6 is the number of available experiments ) , this approach is justified by a result of pukelsheim and rieder  @xcite , who give a rounding procedure to transform an optimal approximate design @xmath40 into an @xmath32replicated design @xmath41 which approximates the optimum of the kiefer s @xmath42optimal design problem within a factor @xmath43 .",
    "this problem describes an _ underinstrumented situation _ , in which a small number @xmath8 of experiments should be selected . in this case , the combinatorial aspects of problem   become crucial .",
    "a similar problem was studied by song , qiu and zhang  @xcite , who proposed to use a greedy algorithm to approximate the solution of problem  . in this paper",
    ", we give an approximation bound which justifies this approach .",
    "another question addressed in this manuscript is whether it is appropriate to take roundings of ( continuous ) approximate designs in the underinstrumented situation ( recall that this is the common approach when dealing with experimental design problems in the _ overinstrumented _ case , where the number @xmath38 of experiments is large when compared to @xmath6 ) .",
    "appendix  [ sec : problemstatement ] is devoted to the application to the theory of optimal experimental designs ; we explain how a statistical problem ( choose which experiments to conduct in order to estimate a set of parameters ) leads to the study of problem  , with a particular focus to the _ underinstrumented _ situation described above . for more details on the subject ,",
    "the reader is referred to the monographs of fedorov  @xcite and pukelsheim  @xcite .",
    "lll algorithm & approximation factor for problem   & reference + & & + greedy & @xmath44 ( or @xmath45 ) & [ coro:1m1se ] ( @xcite ) + any @xmath32replicated design @xmath31 & & + ( posterior bound ) & & +    .summary of the approximation bounds obtained in this paper , as well as the bound of pukelsheim and rieder  @xcite .",
    "the column `` reference '' indicates the number of the theorem , proposition or remark where the bound is proved ( a citation in parenthesis means a direct application of a result of the cited paper , which is possible thanks to the submodularity of @xmath46 proved in corollary  [ coro : submodexample ] ) . in the table ,",
    "_ posterior _ denotes a bound which depends on the continuous solution @xmath40 of the relaxed problem , while a _ prior bound",
    "_ depends only on the parameters of the problem .",
    "[ tab : factors ] [ cols= \" < \" , ]     for all @xmath47 $ ] we denote by @xmath48 the fractional part of @xmath49 , and we assume without loss of generality that these numbers are sorted , i.e.  , @xmath50 .",
    "we will prove the theorem through a simple ( suboptimal ) rounding @xmath51 , which we define as follows : @xmath52 } \\lfloor w_i^ * \\rfloor ; \\\\",
    "\\lfloor w_i^ * \\rfloor & \\quad \\textrm{otherwise . }",
    "\\end{array } \\right.\\ ] ] we know from proposition  [ prop : boundw_n ] and from the fact that algorithm  [ algo : greedyrounding ] solves problem   the integer vector @xmath31 satisfies @xmath53 we shall now bound from below the latter expression : if @xmath54 and @xmath55 , then @xmath56 note that inequality   also holds if @xmath57 .",
    "if @xmath58 , we write @xmath59 where the inequality is a consequence of the concavity of @xmath60 .",
    "combining inequalities   and   yields @xmath61 where we have set @xmath62 .",
    "since the vector @xmath63 $ ] sums to @xmath64 , we can apply the result of lemma  [ lemma : minsummax ] with condition @xmath65 , with @xmath66 , and we obtain @xmath67 } u + ( n - u ) \\left",
    "( \\frac{n - u}{s } \\right)^{1-p}.\\ ] ]    we will compute this lower bound in closed - form , which will provide the approximation bound of the theorem . to do this ,",
    "we define the function @xmath68 on @xmath69-\\infty , n]$ ] , and we observe ( by differentiating ) that @xmath70 is decreasing on @xmath69-\\infty , u^*]$ ] and increasing on @xmath71 , where @xmath72 hence , only two cases can appear : either @xmath73 , and the minimum of @xmath70 over @xmath74 $ ] is attained for @xmath75 ; or @xmath76 , and @xmath77}$ ] attains its minimum at @xmath78 . finally , the bound given in this theorem is either @xmath79 or @xmath80 , depending on the sign of @xmath81 .",
    "in particular , since the function @xmath82 is nonincreasing on the interval @xmath83 $ ] , with @xmath84 and @xmath85 , we have : @xmath86 , \\quad \\frac{n}{s}\\leq e^{-1 } \\longrightarrow u^ * \\leq 0 \\quad\\textrm { and }",
    "\\quad   \\frac{n}{s}\\geq \\frac{1}{2 } \\longrightarrow u^ * \\geq 0.\\ ] ]    the alternative rounding @xmath87 is very useful to obtain the formula of theorem  [ theo : factorf ] . however , since @xmath87 differs from the design @xmath31 returned by algorithm  [ algo : greedyrounding ] in general , the inequality @xmath88 is not tight .",
    "consider for example the situation where @xmath89 and @xmath90 , which is a trivial case for the rank optimization problem  : the incremental rounding algorithm always returns a design @xmath31 such that @xmath91 , and hence the problem is solve to optimality ( the design is of full rank ) .",
    "in contrast , theorem  [ theo : factorf ] only guarantees a factor @xmath92 for this class of instances .",
    "we point out that theorem  [ theo : factorf ] improves on the greedy approximation factor @xmath93 in many situations .",
    "the gray area of figure  [ fig : bettergreedy ] shows the values of @xmath94 $ ] for which the approximation guarantee is better with algorithm  [ algo : greedyrounding ] than with the greedy algorithm of section  [ sec : submod ] .",
    "$ ] such that the factor @xmath95 of theorem  [ theo : factorf ] is larger than @xmath93 .",
    "[ fig : bettergreedy],scaledwidth=60.0% ]    recall that the relevant criterion for the theory of optimal design is the _ positively homogeneous",
    "_ function @xmath96 ( cf .  equation  ) .",
    "hence , if a design is within a factor @xmath95 of the optimum with respect to @xmath46 , its @xmath42efficiency is @xmath97 . in the",
    "_ overinstrumented _ case @xmath39 , pukelsheim gives a rounding procedure with a @xmath42efficiency of @xmath43 ( chapter  12 in  @xcite ) .",
    "we have plotted in figure  [ fig : betterpuk ] the area of the domain @xmath98 ^ 2 $ ] where the approximation guarantee of theorem  [ theo : factorf ] is better .    ^2 $ ] such that the factor @xmath95 of theorem  [ theo : factorf ] is larger than @xmath99 .",
    "[ fig : betterpuk],scaledwidth=60.0% ]",
    "this paper gives bounds on the behavior of some classical heuristics used for combinatorial problems arising in optimal experimental design .",
    "our results can either justify or discard the use of such heuristics , depending on the settings of the instances considered .",
    "moreover , our results confirm some facts that had been observed in the literature , namely that rounding algorithms perform better if the density of measurements is high , and that the greedy algorithm always gives a quite good solution .",
    "we illustrate these observations with two examples :    in a sensor location problem , uciski and patan  @xcite noticed that the trimming of a branch and bound algorithm was better if they activated more sensors , although this led to a much larger search space .",
    "the authors claims that this surprising result can be explained by the fact that a higher density of sensors leads to a better continuous relaxation .",
    "this is confirmed by our result of approximability , which shows that the larger is the number of selected experiments , the better is the quality of the rounding .",
    "it is also known that the greedy algorithm generally gives very good results for the optimal design of experiments ( see e.g.  @xcite , where the authors explicitly chose not to implement a local search from the design greedily chosen , since the greedy algorithm already performs very well ) .",
    "our @xmath100approximability result guarantees that this algorithm always well behaves indeed .",
    "the author wishes to thank stphane gaubert for his useful comments and advises , for suggesting to work within the framework of matrix inequalities and submodularity , and of course for his warm support .",
    "he also wants to thank mustapha bouhtou for the stimulating discussions which are at the origin of this work .",
    "the author expresses his gratitude to an anonymous referee of the isco conference  where an announcement of some of the present results was made  @xcite , and to three referees of dam for precious comments and suggestions .",
    "ccpv07    c.l .",
    "sequences converging to d - optimal designs of experiments . , 1(2):342352 , 1973 .",
    "t.  ando and x.  zhan .",
    "norm inequalities related to operator monotone functions .",
    ", 315:771780 , 1999 .",
    "m.  bouhtou , s.  gaubert , and g.  sagnol .",
    "optimization of network traffic measurement : a semidefinite programming approach . in _ proceedings of the international conference on engineering optimization ( engopt )",
    "_ , rio de janeiro , brazil , 2008 . 978 - 85 - 7650 - 152 - 7 .",
    "m.  bouhtou , s.  gaubert , and g.  sagnol .",
    "submodularity and randomized rounding techniques for optimal experimental design .",
    ", 36:679  686 , march 2010 .",
    "isco 2010 - international symposium on combinatorial optimization .",
    "hammamet , tunisia .",
    "r.  bhatia . .",
    "springer verlag , 1997 .",
    "m.  conforti and g.  cornujols .",
    "submodular set functions , matroids and the greedy algorithm : tight worst - case bounds and some generalizations of the rado - edmonds theorem .",
    ", 7(3):251274 , 1984 .",
    "g.  calinescu , c.  chekuri , m.  pl , and j.  vondrk . maximizing a submodular set function subject to a matroid constraint . in _ proceedings of the 12th international conference on integer programming and combinatorial optimization , ipco _ ,",
    "volume 4513 , pages 182196 , 2007 .",
    "h.  dette , a.  pepelyshev , and a.  zhigljavsky .",
    "improving updating rules in multiplicative algorithms for computing d - optimal designs .",
    ", 53(2):312  320 , 2008 .",
    "fedorov . .",
    "new york : academic press , 1972 . translated and edited by w.",
    "j. studden and e. m. klimko .",
    "u.  feige . a threshold of @xmath101 for approximating set cover . ,",
    "45(4):634652 , july 1998 .",
    "f.  hansen and g.k .",
    "perturbation formulas for traces on c*-algebras . , 31:169178 , 1995 .",
    "r.  harman and m.  trnovsk .",
    "approximate d - optimal designs of experiments on the convex hull of a finite set of information matrices .",
    ", 59(5):693704 , december 2009 .",
    "t.  ibaraki and n.  katoh .",
    ". mit press , 1988 .",
    "e.  jorswieck and h.  boche . .",
    "now publishers inc .",
    ", 2006 .",
    "j.  kiefer .",
    "general equivalence theory for optimum designs ( approximate theory ) .",
    ", 2(5):849879 , 1974 .",
    "j.  kiefer .",
    "optimal design : variation in structure and performance under change of criterion .",
    ", 62(2):277288 , 1975 .",
    "t.  kosem .",
    "inequalities between @xmath102 and @xmath103 . , 418:153160 , 2006 .",
    "a.  kulik , h.  shachnai , and t.  tamir . maximizing submodular set functions subject to multiple linear constraints . in _",
    "soda 09 : proceedings of the nineteenth annual acm -siam symposium on discrete algorithms _ , pages 545554 , philadelphia , pa , usa , 2009 .    k.  lwner .",
    "ber monotone matrixfunktionen .",
    ", 38(1):177216 , 1934 .",
    "michel minoux . accelerated greedy algorithms for maximizing submodular set functions . in j.",
    "stoer , editor , _ optimization techniques _ ,",
    "volume  7 of _ lecture notes in control and information sciences _ , pages 234243 .",
    "springer berlin / heidelberg , 1978 .",
    "10.1007/bfb0006528 .    t.l .",
    "morin and r.e .",
    "an algorithm for nonlinear knapsack problems . ,",
    "pages 11471158 , 1976 .",
    "aw  marshall and i.  olkin . .",
    "academic press , 1979 .",
    "nemhauser , l.a .",
    "wolsey , and m.l . fisher .",
    "an analysis of approximations for maximizing submodular set functions .",
    ", 14:265294 , 1978 .",
    "f.  pukelsheim and s.  rieder .",
    "efficient rounding of approximate designs . ,",
    "pages 763770 , 1992 .",
    "f.  pukelsheim and g.p.h .",
    "convexity and monotonicity properties of dispersion matrices of estimators in linear models . , 10(2):145149 , 1983 .",
    "f.  pukelsheim . on linear regression designs which maximize information .",
    ", 4:339364 , 1980 .",
    "f.  pukelsheim . .",
    "wiley , 1993 .",
    "tg  robertazzi and sc  schwartz . .",
    ", 10:341 , 1989 .",
    "g.  sagnol .",
    "computing optimal designs of multiresponse experiments reduces to second - order cone programming .",
    ", 141(5):1684  1708 , 2011 .",
    "g.  sagnol , s.  gaubert , and m.  bouhtou .",
    "optimal monitoring on large networks by successive c - optimal designs . in _",
    "22nd international teletraffic congress ( itc22 ) , amsterdam , the netherlands _ ,",
    "september 2010 .",
    "song , l.  qiu , and y.  zhang .",
    "netquest : a flexible framework for largescale network measurement . in _",
    "acm sigmetrics06 _ , st malo , france , 2006 .",
    "m.  sviridenko . a note on maximizing a submodular set function subject to a knapsack constraint .",
    ", 32(1):4143 , 2004 .",
    "d.  uciski and m.  patan .",
    "d - optimal design of a monitoring network for parameter estimation of distributed systems . , 39(2):291322 , 2007 .",
    "j.  vondrk .",
    "optimal approximation for the submodular welfare problem in the value oracle model . in _",
    "acm symposium on theory of computing , stoc08 _ , pages 6774 , 2008 .",
    "wolsey . maximising real - valued submodular functions : primal and dual heuristics for location problems . , pages 410425 , 1982 .",
    "y.  yu . monotonic convergence of a general algorithm for computing optimal designs . , 38(3):15931606 , 2010 .",
    "x.  zhan . .",
    "springer , 2002 .",
    "* appendix *",
    "we denote vectors by bold - face lowercase letters and we make use of the classical notation @xmath10:=\\{1,\\ldots , s\\}$ ] ( and we define @xmath104:=\\emptyset$ ] ) .",
    "the set of nonnegative ( resp .",
    "positive ) real numbers is denoted by @xmath105 ( resp .",
    "@xmath106 ) , the set of @xmath107 symmetric ( resp .",
    "symmetric positive semidefinite , symmetric positive definite ) is denoted by @xmath108 ( resp .",
    "@xmath109 , @xmath110 ) .",
    "the expected value of a random variable @xmath111 is denoted by @xmath112 $ ] .",
    "we denote by @xmath113 the vector of the parameters that we want to estimate . in accordance with the classical linear model ,",
    "we assume that the experimenter has a collection of @xmath6 experiments at his disposal , each one providing a ( multidimensional ) observation which is a linear combination of the parameters , up to a noise on the measurement whose covariance matrix is known and positive definite . in other words , for each experiment @xmath114 $ ] , we have @xmath115={\\boldsymbol{0 } } , \\qquad { \\mathbb{e}}[{\\boldsymbol{\\epsilon_i}}{\\boldsymbol{\\epsilon_i}}^t]=\\sigma_i , \\label{meseq}\\ ] ] where @xmath116 is the vector of measurement of size @xmath117 , @xmath118 is a @xmath119matrix , and @xmath120 is a known covariance matrix .",
    "we will assume that the noises have unit variance for the sake of simplicity : @xmath121 .",
    "we may always reduce to this case by a left multiplication of the observation equation   by @xmath122 .",
    "the errors on the measurements are assumed to be mutually independent , i.e. @xmath123=0.\\ ] ]    as explained in the introduction , the aim of experimental design theory is to choose how many times each experiment will be performed so as to maximize the accuracy of the estimation of @xmath124 , with the constraint that @xmath38 experiments may be conducted .",
    "we therefore define the integer - valued _ design _ variable @xmath125 , where @xmath126 indicates how many times the experiment @xmath127 is performed .",
    "we denote by @xmath128 $ ] the index of the @xmath129 conducted experiment ( the order in which we consider the measurements has no importance ) , so that the aggregated vector of observation reads : @xmath130",
    "@xmath131,\\qquad   \\mathcal{a}= \\left [ \\begin{array}{c } a_{i_1 } \\\\   \\vdots \\\\",
    "a_{i_n } \\end{array } \\right],\\qquad   { \\mathbb{e}}[{\\boldsymbol{\\epsilon}}]={\\boldsymbol{0}},\\quad \\mathrm{and}\\quad { \\mathbb{e}}[{\\boldsymbol{\\epsilon } } { \\boldsymbol{\\epsilon}}^t]=i.\\ ] ]    now , assume that we have enough measurements , so that @xmath132 is of full rank .",
    "a common result in the field of statistics , known as the _ gauss - markov _ theorem , states that the best linear unbiased estimator of @xmath124 is given by a pseudo inverse formula .",
    "its variance is given below : @xmath133    we denote the inverse of the covariance matrix   by @xmath134 , because in the gaussian case it coincides with the fisher information matrix of the measurements .",
    "note that it can be decomposed as the sum of the information matrices of the selected experiments : @xmath135 the classical experimental design approach consists in choosing the design @xmath31 in order to make the variance of the estimator  ( [ bestestimator ] ) _ as small as possible_. the interpretation is straightforward : with the assumption that the noise @xmath136 is normally distributed , for every probability level @xmath137 , the estimator @xmath138 lies in the confidence ellipsoid centered at @xmath124 and defined by the following inequality : @xmath139 where @xmath140 depends on the specified probability level , and @xmath141 is the inverse of the covariance matrix @xmath142 .",
    "we would like to make these confidence ellipsoids _ as small as possible _ , in order to reduce the uncertainty on the estimation of @xmath124 .",
    "to this end , we can express the inclusion of ellipsoids in terms of matrix inequalities .",
    "the space of symmetric matrices is equipped with the _ lwner ordering _ , which is defined by @xmath143",
    "let @xmath31 and @xmath144 denote two designs such that the matrices @xmath134 and @xmath145 are invertible .",
    "one can readily check that for any value of the probability level @xmath137 , the confidence ellipsoid   corresponding to @xmath141 is included in the confidence ellipsoid corresponding to @xmath146 if and only if @xmath147 .",
    "hence , we will prefer the design @xmath31 to the design @xmath144 if the latter inequality is satisfied .      since the lwner ordering on symmetric matrices is only a partial ordering ,",
    "the problem consisting in maximizing @xmath134 is ill - posed .",
    "so we will rather maximize a scalar _ information function _ of the fisher matrix , i.e.  a function mapping @xmath148 onto the real line , and which satisfies natural properties , such as positive homogeneity , monotonicity with respect to lwner ordering , and concavity .",
    "for a more detailed description of the information functions , the reader is referred to the book of pukelsheim  @xcite , who makes use of the class of matrix means @xmath34 , as first proposed by kiefer  @xcite .",
    "these functions are defined like the @xmath149-norm of the vector of eigenvalues of the fisher information matrix , but for @xmath150 $ ] : for a symmetric positive definite matrix @xmath151 , @xmath34 is defined by @xmath152-\\infty,1],\\ p \\neq 0 $ ; }   \\\\   ( \\operatorname{det}(m))^{\\frac{1}{m } } & \\textrm{for $ p=0 $ , } \\end{array } \\right .",
    "\\label{phip}\\ ] ]    where we have used the extended definition of powers of matrices @xmath153 for arbitrary real parameters @xmath1 : if @xmath19 are the eigenvalues of @xmath154 counted with multiplicities , @xmath155 . for singular positive semi - definite matrices",
    "@xmath156 , @xmath34 is defined by continuity :    @xmath157 $ ; }   \\\\   ( \\frac{1}{m}\\ \\mathrm{trace}\\ m^p)^{\\frac{1}{p } } & \\textrm{for $ p \\in\\ ] 0,1]$. } \\end{array } \\right .",
    "\\label{phising}\\ ] ]    the class of functions @xmath34 includes as special cases the classical optimality criteria used in the experimental design literature , namely @xmath158optimality for @xmath159 ( smallest eigenvalue of @xmath134 ) , @xmath160optimality for @xmath161 ( determinant of the information matrix ) , @xmath162optimality for @xmath163 ( harmonic average of the eigenvalues ) , and @xmath164optimality for @xmath165 ( trace ) .",
    "the case @xmath89 ( d - optimal design ) admits a simple geometric interpretation : the volume of the confidence ellipsoid   is given by @xmath166 where @xmath167 is a constant depending only on the dimension .",
    "hence , maximizing @xmath168 is the same as minimizing the volume of every confidence ellipsoid .",
    "we can finally give a mathematical formulation to the problem of selecting @xmath38 experiments to conduct among the set @xmath10 $ ] :    @xmath169      we note that the problem of maximizing the information matrix @xmath134 with respect to the lwner ordering remains meaningful even when @xmath134 is not of full rank ( the interpretation of @xmath134 as _ the inverse of the covariance matrix of the best linear unbiased estimator _ vanishes , but @xmath134 is still the fisher information matrix of the experiments if the measurement errors are gaussian ) .",
    "this case does arise in _ underinstrumented situations _ , in which some constraints may not allow one to conduct a number of experiments which is sufficient to infer all the parameters .    an interesting and natural idea to find an optimal under - instrumented design is to choose the design which maximizes the rank of the observation matrix @xmath132 , or equivalently of @xmath170 .",
    "the _ rank maximization _ is a nice combinatorial problem , where we are looking for a subset of matrices whose sum is of maximal rank : @xmath171    when every feasible information matrix is singular , equation   indicates that the maximization of @xmath172 can be considered only for nonnegative values of @xmath1 .",
    "then , the next proposition shows that @xmath34 can be seen as a deformation of the rank criterion for @xmath1730,1]$ ] .",
    "first notice that when @xmath174 , the maximization of @xmath172 is equivalent to : @xmath175 if we set @xmath176 , we obtain the problems   and   which were presented in the first lines of this article .",
    "[ limp0 ] for all positive semidefinite matrix @xmath177 @xmath178    let @xmath179 denote the positive eigenvalues of @xmath154 , counted with multiplicities , so that @xmath180 is the rank of @xmath154 .",
    "we have the first order expansion as @xmath21 : @xmath181    consequently , @xmath182 will stand for @xmath183 in the sequel and the rank maximization problem   is the limit of problem   as @xmath21 .    [ corop0 ] if @xmath174 is small enough , then every design @xmath184 which is a solution of problem   maximizes the rank of @xmath134 . moreover , among the designs which maximize this rank , @xmath184 maximizes the product of nonzero eigenvalues of @xmath134 .",
    "since there is only a finite number of designs , it follows from   that for @xmath174 small enough , every design which maximizes @xmath46 must maximize in the lexicographical order first the rank of @xmath134 , and then the pseudo - determinant @xmath185 .",
    "the proof of proposition  [ prop : boundw ] relies on several lemmas on the directional derivative of a scalar function applied to a symmetric matrix , which we state next .",
    "first recall that if @xmath186 is differentiable on @xmath106 , then @xmath186 is frchet differentiable over @xmath110 , and for @xmath151 , @xmath187 , we denote by @xmath188 its directional derivative at @xmath154 in the direction of @xmath189 ( see equation  ) .",
    "let @xmath194 be an eigenvalue decomposition of @xmath154 .",
    "it is known ( see e.g.  @xcite ) that @xmath188 can be expressed as @xmath195}(d ) \\odot q^t h q ) q^t$ ] , where @xmath196}(d)$ ] is a symmetric matrix called the _ first divided difference _ of @xmath186 at @xmath197 and @xmath198 denotes the hadamard ( elementwise ) product of matrices . with little work ,",
    "the latter derivative may be rewritten as : @xmath199}_{ij } { \\boldsymbol{q_i}}{\\boldsymbol{q_i}}^t h { \\boldsymbol{q_j}}{\\boldsymbol{q_j}}^t,\\ ] ] where @xmath200 is the @xmath129 eigenvector of @xmath154 ( i.e. , the @xmath129 column of @xmath201 ) and @xmath196}_{ij}$ ] denotes the @xmath202element of @xmath196}(d)$ ] .",
    "we can now conclude : @xmath203}_{ij } { \\operatorname{trace } } ( a{\\boldsymbol{q_i}}{\\boldsymbol{q_i}}^t b { \\boldsymbol{q_j}}{\\boldsymbol{q_j}}^t ) \\\\    & = \\sum_{i , j } f^{[1]}_{ji } { \\operatorname{trace } } ( b{\\boldsymbol{q_j}}{\\boldsymbol{q_j}}^t h { \\boldsymbol{q_i}}{\\boldsymbol{q_i}}^t ) \\\\    & = { \\operatorname{trace}}(b\\ d\\!f(m)(a))\\end{aligned}\\ ] ]            since @xmath213 and @xmath154 commute , we can diagonalize them simultaneously : @xmath216 thus , it is clear from the definition of the directional derivative that @xmath217 by reasoning entry - wise on the diagonal matrices , we find : @xmath218 the equality of the lemma is finally obtained by writing : @xmath219 note that the matrix @xmath220 is indeed symmetric , because @xmath221 and @xmath213 commute .",
    "[ prop : kkt_cond ] let @xmath222 $ ] .",
    "a design @xmath40 is optimal for problem   if and only if : @xmath223,\\quad",
    "n { \\operatorname{trace}}(m_f({\\boldsymbol{w^*}})^{p-1 } m_i ) \\leq \\varphi_p\\big({\\boldsymbol{w^*}}\\big).\\ ] ] moreover , the latter inequalities become equalities for all @xmath224 such that @xmath225 .    for a proof of this result ,",
    "see  @xcite or paragraph  7.19 in  @xcite , where the problem is studied with the normalized constraint @xmath226 .",
    "in fact , the _ general equivalence theorem _ details the karush - kuhn - tucker conditions of optimality of problem  . to derive them , one can use the fact that when @xmath227 is invertible , @xmath2280,1],\\ ] ] and @xmath229 note that for @xmath230 , the proposition implicitly implies that @xmath231 is invertible .",
    "a proof of this fact can be found in paragraph  7.13 of  @xcite .",
    "let @xmath40 be an optimal solution to problem   and @xmath12 be a subset of  @xmath10 $ ] such that @xmath225 for all @xmath232 ( the case in which @xmath233 for some index @xmath234 will trivially follow if we adopt the convention @xmath235 ) .",
    "we know from proposition  [ prop : kkt_cond ] that @xmath236 for all @xmath224 in @xmath12 .",
    "if we combine these equalities by multiplying each expression by a factor proportional to @xmath237 , we obtain : @xmath238 we are going to show that for all @xmath239 such that @xmath227 is invertible , @xmath240 , where @xmath241 , which will complete the proof . to do this , we introduce the function @xmath186 defined on the open subset of @xmath242 such that @xmath227 is invertible by : @xmath243 note that @xmath186 satisfies the property @xmath244 for all positive scalar @xmath245 ; this explains why we do not have to work with normalized designs such that @xmath246 .",
    "now , let @xmath239 be such that @xmath247 and let @xmath127 be an index of @xmath12 such that @xmath248 .",
    "we are first going to show that @xmath249 . by the rule of differentiation of a product , @xmath250(m_f({\\boldsymbol{w}}))(m_k ) \\right ) \\nonumber\\\\   & = { \\operatorname{trace}}m_k \\left ( ( 1-p ) w_k^{-p } m_f({\\boldsymbol{w}})^{p-1 } +   d[x\\mapsto x^{p-1}]\\big(m_f({\\boldsymbol{w}})\\big)\\big(\\sum_{i\\in s } w_i^{1-p } m_i)\\big ) \\right ) , \\label{mkparent}\\end{aligned}\\ ] ] where the first equality is simply a rewriting of @xmath251 by using a directional derivative , and the second equality follows from lemma  [ lemma : permderiv ] applied to the function @xmath252 . by linearity of the frchet derivative , we have : @xmath253\\big(m_f({\\boldsymbol{w}})\\big)\\big(\\sum_{i\\in s } w_i^{1-p } m_i\\big)=   d[x\\mapsto x^{p-1}]\\big(m_f({\\boldsymbol{w}})\\big)\\big(\\sum_{i\\in s } w_i \\left(\\frac{w_k}{w_i}\\right)^{\\!\\!p } m_i\\big ) .\\ ] ] since @xmath254 for all @xmath234 , the following matrix inequality holds : @xmath255 by applying successively lemma  [ lemma : nonincderiv ] ( @xmath252 is antitone on @xmath190 ) and lemma  [ lemma : commderiv ] ( the matrix @xmath227 commutes with itself ) , we obtain : @xmath256\\big(m_f({\\boldsymbol{w}})\\big)\\big(\\sum_{i\\in s } w_i^{1-p } m_i\\big ) & \\succeq   d[x\\mapsto",
    "x^{p-1}]\\big(m_f({\\boldsymbol{w}})\\big)\\big(m_f({\\boldsymbol{w}})\\big ) \\\\ & = ( p-1 ) m_f({\\boldsymbol{w}})^{p-2 } m_f({\\boldsymbol{w}})\\\\ & = ( p-1 ) m_f({\\boldsymbol{w}})^{p-1}.\\end{aligned}\\ ] ] dividing the previous matrix inequality by @xmath257 , we find that the matrix that is inside the largest parenthesis of equation   is positive semidefinite , from which we can conclude : @xmath249 .",
    "thanks to this property , we next show that @xmath258 , where @xmath259 is defined by @xmath260 if @xmath232 and @xmath261 otherwise . assume without loss of generality ( after a reordering of the coordinates ) that @xmath262 $ ] , @xmath263 , and denote the vector of the remaining components of @xmath264 by @xmath265 ( i.e. , we have @xmath266 $ ] and @xmath267 $ ] ) .",
    "the following inequalities hold : @xmath268\\right ) \\leq f\\left(\\left [ \\begin{array}{c } w_2 \\\\w_2\\\\w_3\\\\ \\vdots\\\\ w_{s_0 } \\\\ { \\boldsymbol{\\bar{w } } } \\end{array } \\right]\\right ) \\leq f\\left(\\left [ \\begin{array}{c } w_3 \\\\w_3\\\\w_3\\\\ \\vdots\\\\ w_{s_0 } \\\\ { \\boldsymbol{\\bar{w } } } \\end{array } \\right]\\right ) \\leq \\ldots \\leq f\\left(\\left [ \\begin{array}{c } w_{s_0 } \\\\w_{s_0}\\\\w_{s_0}\\\\ \\vdots\\\\ w_{s_0 } \\\\ { \\boldsymbol{\\bar{w } } } \\end{array } \\right]\\right ) = f({\\boldsymbol{v}}).\\ ] ] the first inequality holds because @xmath269 as long as @xmath270 . to see that the second inequality holds",
    ", we apply the same reasoning on the function @xmath271 \\mapsto f([w_2,w_2,w_3,\\ldots])$ ] , i.e. , we consider a variant of the problem where the matrices @xmath272 and @xmath273 have been replaced by a single matrix @xmath274 .",
    "the following inequalities are obtained in a similar manner .",
    "we denote by @xmath279 the moore - penrose inverse of @xmath111 .",
    "it is known  @xcite that if @xmath280 , the function @xmath281 is nondecreasing with respect to the lwner ordering over the set of matrices @xmath111 whose range contains @xmath13 .",
    "hence , since @xmath282 is invertible , @xmath283 and @xmath284 finally , we have @xmath285 and the proof is complete ."
  ],
  "abstract_text": [
    "<S> we study a family of combinatorial optimization problems defined by a parameter @xmath0 $ ] , which involves spectral functions applied to positive semidefinite matrices , and has some application in the theory of optimal experimental design . </S>",
    "<S> this family of problems tends to a generalization of the classical maximum coverage problem as @xmath1 goes to @xmath2 , and to a trivial instance of the knapsack problem as @xmath1 goes to @xmath3 .    in this article </S>",
    "<S> , we establish a matrix inequality which shows that the objective function is submodular for all @xmath0 $ ] , from which it follows that the greedy approach , which has often been used for this problem , always gives a design within @xmath4 of the optimum . </S>",
    "<S> we next study the design found by rounding the solution of the continuous relaxed problem , an approach which has been applied by several authors . </S>",
    "<S> we prove an inequality which generalizes a classical result from the theory of optimal designs , and allows us to give a rounding procedure with an approximation factor which tends to @xmath3 as @xmath1 goes to @xmath3 .    </S>",
    "<S> [ [ keyword ] ] keyword + + + + + + +    maximum coverage , optimal design of experiments , kiefer s @xmath5criterion , polynomial - time approximability , rounding algorithms , submodularity , matrix inequalities . </S>"
  ]
}