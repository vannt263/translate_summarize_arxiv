{
  "article_text": [
    "consider a sample @xmath0 that is a realisation of a random sample ( univariate or multivariate ) from a finite mixture of @xmath1 distributions @xmath2 where the component weights @xmath3 are non - negative and sum to @xmath4 .",
    "the collection of the component - specific parameters is denoted @xmath5 and the collection of all parameters by @xmath6 .",
    "as is now standard @xcite each observation @xmath7 from the sample can be assumed to originate from a specific if unobserved component of @xmath8 , denoted @xmath9 , and the mixture inference problem can then be analysed as a missing data model , with discrete missing data @xmath10 , such that @xmath11 the conditional distribution of @xmath12 $ ] is then given by @xmath13    this interpretation of the mixture model leads to a natural clustering of the observations according to their label and the cluster associated with the mixture component @xmath14 provides information about @xmath15 and @xmath16 .",
    "in particular , when the full conditional distribution of the parameter @xmath17 is available in closed form , conditional simulation from @xmath18 becomes straightforward ( see @xcite ) .    in a bayesian mixture modelling setup ,",
    "the goal is to perform inference on the parameter @xmath17 and the posterior distribution @xmath19 is usually approximated via mcmc methods .",
    "the likelihood function @xmath20 is both available and invariant under permutations of the component indices .",
    "if an exchangeable prior is chosen on @xmath21 , the posterior density reproduces the likelihood invariance and component labels are not identifiable .",
    "this phenomenon is called _ label switching _ and is well - studied in the literature @xcite . from a simulation perspective , label switching induces multimodality in the target and while it is desirable that a simulated markov chain targeting the posterior explores all of the @xmath22 symmetric modes of the posterior distribution , most samplers fail to switch between modes @xcite . for instance , when using a data augmentation scheme , which is a form of gibbs sampler adapted to missing data problems @xcite , the markov chain very slowly if ever switches between the symmetric modes . therefore , since the chain only explores a certain region of the support of the multimodal posterior , estimates based on the simulation output are necessarily biased .",
    "when label switching is missing from the mcmc output , it can be simulated by modifying the mcmc sample ( see @xcite ) .",
    "a different perspective on the label switching phenomenon is inferential .",
    "indeed , point estimates of the component - wise parameters are harder to produce when the markov chain moves freely between modes . to achieve component - specific inference and give a meaning to each component , relabelling methods have been proposed in the literature ( see @xcite and others ) .",
    "an r - package , label.switching @xcite , incorporates some of those label switching removing methods .",
    "evaluating the number of components @xmath1 is a special case of model comparison , which can be conducted by comparing the _ posterior probabilities of the models_. those probabilities are in turn computed via the marginal likelihoods @xmath23 , also known as model evidences @xcite @xmath24 where @xmath25 is the selected prior for the @xmath1-component mixture .",
    "( we assume here that it is exchangeable wrt its components . )",
    "recall that the ratio of evidences is a bayes factor and is properly scaled to be readily compared to @xmath4 @xcite .",
    "when a large collection of values of @xmath1 is considered for model comparison , sophisticated mcmc methods have been developed to bypass computing evidences @xcite , even though those are estimated as a byproduct of the methods .",
    "alternatively , estimating the number of components can also proceed from a bayesian nonparametric ( bnp ) modelling , which assumes an infinite number of components and then evaluates the non - empty components implicitly through partitioning data , using for instance the chinese restaurant process @xcite .",
    "this however requires a modification of the prior modelling and we will not cover it in this paper , which reassesses monte carlo ways of approximating the evidence .",
    "the difficulty with approaches using @xmath23 is that the quantity often can not directly and reliably be derived from simulations from the posterior distribution @xmath26 @xcite .",
    "the quantity has been approximated using dedicated methods such harmonic means @xcite , importance sampling @xcite , bridge sampling @xcite , laplace approximation @xcite , stochastic substitution @xcite , nested sampling @xcite , savage - dickey representations @xcite and erroneous implementations of the carlin and chib algorithm @xcite .",
    "comparative studies of those methods are found in @xcite and @xcite .    in the specific case of mixtures ,",
    "the invariance of the posterior density under an arbitrary relabelling of the mixture components must be exhibited by simulations and approximations to achieve a valid estimate of @xmath23 as discussed in @xcite .",
    "this often leads to computationally intensive steps in approximation methods , especially when @xmath1 is large , and it is the purpose of this paper to provide a partial answer to this specific issue .",
    "we consider here two estimators of @xmath23 , both based on importance sampling ( is ) .",
    "one is a version of chib s estimator and the second one a novel representation called _ dual importance sampling_. our importance construction aims to better approximate the posterior distribution both around a specific local mode and at the corresponding @xmath27 symmetric modes of the posterior distribution .",
    "a particular mode is first approximated based on ( i ) a point estimate and ( ii ) rao  blackwellisation from a gibbs sequence .",
    "then , the corresponding local density approximate is permuted to reach all modes .",
    "we demonstrate here that dual importance sampling is comparable to our benchmark method , chib s approach . taking advantage of the symmetry in the posterior distribution ,",
    "we show how to reduce computational demands by approximating our importance function .",
    "the paper starts with recalling the approximation techniques of chib s method and bridge sampling in section 2 . in section 3 , importance sampling",
    "is considered , including our choices of importance functions .",
    "our importance function approximate approach is introduced in section 4 .",
    "simulation studies using both simulated and benchmark datasets , namely the galaxy and fishery datasets used in @xcite are reported in section 5 , and the paper concludes with a short discussion in section 6 .",
    "in this paper , the reference estimator of evidence is chibs(@xcite ) method and is derived from rewriting bayes theorem @xmath28 where @xmath29 is any plug - in value for @xmath17 . when @xmath30 is not available in closed form , the gibbs sampling decomposition allows a rao  blackwellised approximation @xcite @xmath31 where @xmath32 is a markov chain with stationary distribution @xmath33 .",
    "the appeal of this estimator , when available , is that it constitutes a non - parametric density estimator converging at a regular parametric rate .",
    "it is now an accepted fact that label switching is necessary for the above rao  blackwellised @xmath34 to converge .",
    "when @xmath35 only explores part of the modes of the posterior , this estimator is biased , generally missing the target quantity @xmath36 by a factor of order @xmath37 , with no simple correction factor @xcite .",
    "@xcite later suggested a generic correction by averaging @xmath38 over all possible permutations of the labels , hence forcing  perfect \" label switching . the resulting approximation is expressed as @xmath39 where @xmath40 denotes the set of the @xmath22 permutations of @xmath41 and @xmath42 is one of those permutations .",
    "note that the above correction can also be rewritten as @xmath43 using a notational shortcut @xmath44 meaning that the components of @xmath29 are then switched according to the permutation @xmath42 .",
    "this representation may induce computational gains since only @xmath22 versions of @xmath44 need to be stored .    while chib s representation has often been advocated as a highly stable solution for computing the evidence in mixture models , which is why we selected it as our reference , alternative solutions abound within the literature , including nested sampling @xcite , reversible jump mcmc @xcite , and particle filtering @xcite .",
    "@xcite proposed a sample  based method to compute a ratio of normalizing constants of two densities with common support .",
    "the method is well - suited to estimate the marginal likelihood @xcite and used as a point posterior estimate for chib s method @xcite . considering a normalised density @xmath45 and the unnormalized posterior distribution @xmath46 ,",
    "the bridge sampling identity is given by @xmath47}{\\mathbb{e}_{\\pi_k(\\theta|{\\mathbf{x}})}[\\alpha(\\theta)q(\\theta ) ] } \\,,\\ ] ] for an arbitrary function @xmath48 ( provided all expectations are well - defined , @xcite ) .",
    "the ( formally ) optimal choice for @xmath48 @xcite leads to the following iterative estimator @xmath49 where @xmath50 . here ,",
    "@xmath51 and @xmath52 are samples from @xmath53 and @xmath54 respectively .",
    "the convergence of bridge sampling ( with the above optimal @xmath48 ) is trivial when @xmath55 and @xmath53 share a sufficiently large portion of their supports .",
    "if the support intersection is too small , the resulting bridge sampling estimator may end up with an infinite variance @xcite .",
    "improvements of the algorithm , like path sampling @xcite , a simple location shift of the proposal distribution @xcite , and a warp bridge sampling @xcite , have been proposed .    in the specific case of the mixture posterior distribution ,",
    "the parameter @xmath17 can be split in @xmath56 and @xmath1 further blocks @xmath57 in the gibbs sampling steps .",
    "the output samples from the gibbs sampler are denoted by @xmath58 , where the @xmath59 s are the component allocation vectors associated with the observations @xmath60 . for bridge sampling",
    ", @xcite suggested using a rao  blackwellised function @xmath61 of the form @xmath62 assuming @xmath63 is well - mixed , followed by switching the labels of the simulations from the posterior distribution @xcite .",
    "@xcite demonstrated that the iterative bridge sampling estimator ( [ eq_bs ] ) , using ( [ eq_perms_q ] ) as @xmath64 , converges relatively quickly , in about @xmath65 iterations , even with different starting values .",
    "if @xmath53 is an importance function with support @xmath66 , generating a sample @xmath67 from @xmath53 leads to the evidence approximation @xmath68 to provide a good approximation , the support of @xmath53 must overlap the support of the posterior distribution , which is both symmetric under permutations and multimodal . in this sense ,",
    "a rao  blackwellised estimate like ( [ eq_perms_q ] ) is a natural solution for the choice of @xmath45 , despite the drawback that @xmath69 increases  factorially \" fast with @xmath1 due to the permutations over @xmath70 @xcite .    in the following sections , the parameter @xmath17 is decomposed into @xmath71 blocks @xmath72 .",
    "note that @xmath73 is a component - wise block , most often a vector .",
    "two types of importance functions , based on the product of marginal posterior distributions , will be considered . the usefulness and details of the product of block marginal posterior distributions are well summarised in @xcite .      using a very simple rao ",
    "blackwell argument inspired from chib s representation , a natural importance function is @xmath74 samples are generated from the posterior distribution conditional on a given completion vector @xmath75 , which is usually taken equal the map ( maximum a posteriori ) or the marginal map estimate of @xmath76 derived from mcmc simulations . taking the full permutation of component labels of @xmath75 and @xmath29 ( inspired by @xcite and @xcite ) , we thus propose a symmetrised version of a map proposal @xmath77 this proposal is equivalent to generating @xmath17 from @xmath78 and then operating a random permutation on the components of @xmath17 .",
    "the computational cost of producing @xmath79 in , hence @xmath80 , is then multiplied by @xmath22 under the provision that the support of ( [ eq_q_mle ] ) is sufficiently wide .",
    "if the tails of samples generated from ( [ eq_q_mle ] ) are deemed to be too narrow , as signalled by the effective sample size , additional selected ( and thinned ) simulations @xmath81 taken from the gibbs output can be included to make the proposal more robust .    while this estimator is theoretically valid , providing an unbiased estimator of @xmath80 , it may face difficulties in practice by missing wide regions of the parameter space when simulating from @xmath82 .",
    "this is indeed the practical version of simulating from an importance function with a support that is smaller than the support of the integrand a setting that leads to an erroneous approximation of the corresponding integral . in the current situation ,",
    "since @xmath82 is everywhere positive , this is not a theoretical issue .",
    "however , in practice , the conditional density is numerically equal to zero around the alternative modes .",
    "+      a dual exploitation of the above rao ",
    "blackwellisation argument produces an alternative importance sampling proposal , based on mcmc draws @xmath83 from the unconstrained posterior distribution .",
    "the new importance function is expressed as @xmath84    here , @xmath85 is a product of full conditional densities on each parameter in a gibbs sampler representation and @xmath86 is the original albeit not necessarily well - mixed simulation outcome .",
    "label switching is imposed upon those @xmath87 conditional densities through all @xmath22 permutations and conversely the average of @xmath87 well - selected conditional densities should approximate the posterior around any of the @xmath22 symmetric modes of this posterior .",
    "if we now assume that the component labels of the terms @xmath88 in ( [ eq_ds ] ) have not been permuted and that any label switching occurence has been removed from the simulations by a recentering method @xcite , we denote the resulting transforms by @xmath89 . they can be interpreted as hyperparameters of @xmath45 .",
    "the density ( [ eq_ds ] ) then satisfies @xmath90 where @xmath91 .",
    "each of the densities @xmath92 has a support  i.e .",
    ", a domain where it takes non - negligible values denoted by @xmath93 and @xmath94 .",
    "note that an estimator using ( [ eq_ds_1 ] ) is equivalent to an estimator using ( [ eq_ds ] ) .    from a computational perspective ,",
    "an artificial label switching step is necessary in computing @xmath53 but not in generating a proposal @xmath17 from @xmath45 . for arbitrary permutation representations @xmath95 acting on both @xmath17 and @xmath96 , the following holds for ( [ eq_ds ] ) @xmath97 where @xmath98 . the full permutation representation set is invariant over an additional permutation representation @xmath99 ( e.g. , @xmath100 ) , @xmath101 and @xmath102 are equal .",
    "thus the standard estimator using @xmath45 in ( [ eq_ds ] ) is equivalent ( from a computational viewpoint ) to an estimator such that ( i ) proposals are generated from a particular term @xmath103 of ( [ eq_ds_1 ] ) and ( ii ) importance weights are computed according to ( [ eq_ds_1 ] ) .",
    "this makes a proposal generating step easier by ignoring label switching even though all the @xmath104 s need be evaluated to compute @xmath53 .",
    "importance functions found in ( [ eq_perms_q ] ) and ( [ eq_ds_1 ] ) have the same underlying motivation of a better approximation of the joint posterior distribution and the resulting estimate of ( [ impsamp ] ) should therefore be more efficient .",
    "both are designed to cover the @xmath22 clusters , which are created by either symmetrizing the labels of hyperparameter set @xmath105 as in ( [ eq_ds_1 ] ) or by randomly permuting the label of each @xmath70 as in ( [ eq_perms_q ] ) .",
    "once @xmath22 clusters of hyperparameters are thus constructed , the corresponding conditional densities constitute clusters for @xmath45 .",
    "consider @xmath106 , a cluster index of @xmath45 . associating the cluster component function @xmath107 with a support @xmath108",
    ", the importance function @xmath45 is expressed as @xmath109 where @xmath110 is the proportion of those conditional densities that are associated with the cluster @xmath111 and @xmath112 .",
    "the importance function representation ( [ eq_ds_1 ] ) is thus a special case of with ( @xmath113 ) @xmath114 by contrast , the density ( [ eq_perms_q ] ) does not achieve perfect symmetry , which means @xmath111 is not uniformly distributed , although @xmath115 as @xmath116 .    a marginal likelihood estimate using @xmath53 as in ( [ eq_q_cluster ] ) follows by a standard importance sampling identity    @xmath117   \\label{eq_is1}\\end{aligned}\\ ] ]    leading to @xmath118 where @xmath119 , namely a weighted sum of integrals over the @xmath108 s ( @xmath113 ) .    due to the perfect symmetry in the clusters of ( [ eq_ds_1 ] ) , the integrals of @xmath120 with respect to @xmath17 over @xmath108 for @xmath121 are equal . given an arbitrary cluster , @xmath122 , the evidence is @xmath123   ~. \\label{eq_is2 } \\end{aligned}\\ ] ] note that the corresponding estimator ( monte carlo approximation based on @xmath124 draws ) for the above is exactly in the same form to the estimator for ( [ eq_is1 ] ) .",
    "both ( [ eq_is1 ] ) and ( [ eq_is2 ] ) are thus importance sampling estimators using ( [ eq_perms_q ] ) and ( [ eq_ds_1 ] ) respectively .",
    "hence standard convergence result hold : by the law of large numbers , both estimates a.s .",
    "converge to @xmath23 , and the central limit theorem also holds @xmath125 where @xmath126 and @xmath127 .",
    "the perfect symmetry in the clusters of does not guarantee a better efficiency in estimation and those variances are rather highly related to how well the importance functions approximate the joint posterior distribution .",
    "if @xmath128 and both importance functions provide a good approximation , @xmath129 is expected .",
    "both estimators ( [ eq_is1 ] ) and ( [ eq_is2 ] ) suffer from massive computational demands when @xmath1 is large . in this section ,",
    "we show how to approximate ( [ eq_ds ] ) and increase the computational efficiency ( i.e. , computing time ) as a result .",
    "it was shown in section 3.2 that @xmath45 as in ( [ eq_ds ] ) is invariant under a permutation of the labels of @xmath17 and that proposals can be generated from a particular term @xmath103 of ( [ eq_ds_1 ] ) without any loss of statistical efficiency . given @xmath130 , it is natural to consider whether or not all terms in @xmath131 are different from zero for @xmath132 . in the case",
    "some are not , it is obviously computationally relevant to determine which ones among them are likely to be insignificant ( i.e. , almost zero ) .",
    "this perspective motivates the following section .",
    "given a proposal @xmath17 generated from a particular @xmath103 , @xmath133 , the importance function at @xmath17 is an average of all @xmath134 s and the relative contribution of each term is @xmath135 if @xmath136 is close to zero , @xmath137 is negligible within @xmath53 and on the opposite @xmath138 indicates a high contribution of @xmath137 .",
    "the expected relative contribution of @xmath137 @xmath139 = \\displaystyle\\int_{s_{\\sigma_c } }   \\eta_{\\sigma_i}(\\theta ) h_{\\sigma_c}(\\theta)\\ , \\text{d}\\theta \\label{eq_e_eta}\\ ] ] is estimated by @xmath140 =   \\dfrac{1}{m}\\displaystyle\\sum^m_{l=1 } \\eta_{\\sigma_i}(\\theta^{(l ) } )   ~ , \\hspace{1 cm } \\theta^{(l)}\\sim h_{\\sigma_c}(\\theta ) ~. \\label{eq_eta}\\ ] ] after an appropriate permutation of the indices , we obtain that @xmath141\\geq \\cdots \\geq \\widehat{\\mathbb{e}}_{h_{\\sigma_c}}[\\eta_{\\sigma_{k!}}(\\theta)]$ ] , namely that the corresponding @xmath142 are in decreasing order of expected contributions .",
    "the importance function @xmath53 can then be approximated by using only the @xmath143 most important @xmath144 s ( @xmath145 ) , leading to the approximation @xmath146 and the mean absolute difference from @xmath53 is approximated by @xmath147    when this mean absolute difference is below a certain threshold , @xmath148 , @xmath149 is considered to be an appropriate approximation for @xmath45 .",
    "we define the corresponding approximate set @xmath150 to be made of @xmath151 , @xmath143 being defined as the smallest size that satisfies the condition @xmath152 . with this truncation ,",
    "the computational efficiency obviously improves .",
    "note that the set @xmath153 is determined under the assumption that all proposals ( @xmath154 ) are generated from @xmath155 since the quality of the approximation is only guaranteed under this assumption . due to the perfect symmetry of @xmath53 over the @xmath22 permutations ,",
    "the choice of @xmath156 is obviously irrelevant for the computational gains .",
    "the evidence estimate using such an approximation is detailed in the following algorithm : +    in step 1 . , we used the method by @xcite , even though alternatives implemented in the label.switching package of @xcite or in @xcite could be implemented as well . the total number of @xmath157 values that are computed is @xmath158 in the standard dual importance sampling scheme but decreases to @xmath159 when using @xmath160 .",
    "the relative gain in the total number of terms is thus @xmath161 the gain will thus depend on how small @xmath162 is , when compared with @xmath22 , hence ultimately on the acceptable mean absolute difference @xmath148 .",
    "two simulated mixture datasets and two real datasets are used to examine the performance of seven marginal likelihood estimators .",
    "the simulated datasets , @xmath163 and @xmath164 , are ;    * @xmath165 * @xmath166    where @xmath167 denotes a normal distribution with a mean of 5 and a standard deviation of 2 .",
    "two real datasets , called galaxy and fishery datasets respectively , are shown in figure [ hist_data ] .",
    "they have been frequently used in the literature as benchmarks ( see , e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "gaussian and dirichlet priors are used for the means @xmath168 and proportions @xmath56 , @xmath169 for the variance parameters @xmath170 , inverse gamma distributions with two sets of hyperparameters , @xmath171 and @xmath172 , are considered . with the second calibration , label switching naturally occurred in gibbs sequences in our simulation experiments . removing the first 5000 gibbs simulations as burn - ins ,",
    "@xmath173 gibbs simulations are used to approximate @xmath23 .",
    "firstly , a sensitivity analysis is conducted about the expected relative contribution of @xmath174 to @xmath53 with respect to @xmath175 .",
    "then we set the values for both @xmath175 and @xmath148 . in section [",
    "sub : simults ] , the performance of seven estimators for @xmath23 are compared through a large simulation study , which confirms that the asymptotic variance of @xmath80 based on ( [ eq_ds ] ) is smaller than when based on ( [ eq_perms_q ] ) .",
    "( 14,3.5 ) ( 0,0 ) ( 7,0 ) ( 3.5,-0.1)@xmath176-values ( 10.5,-0.1)@xmath176-values ( 5.9,3)(a ( 12.9,3)(b      the approximation set is constructed in two steps .",
    "first , we compute @xmath177 $ ] , @xmath178 , @xmath179 $ ] , based on reduced samples of size @xmath175 as in ( [ eq_eta ] ) .",
    "second , we derive which terms are negligible when compared with the threshold @xmath148 . in our experiments , we chose @xmath148 conservatively so that all zero terms are identified . in matlab , @xmath180 is rounded down to @xmath181 thus @xmath182 was chosen for the following simulation studies .",
    "the expected relative contribution measures for @xmath163 and @xmath164 are given in tables [ q_d1 ] and [ q_d2 ] , respectively .",
    "for @xmath183 initial gibbs simulations , significantly contributing clusters are easily identified by @xmath184\\}_{i=1}^{k!}$ ] and both @xmath162 and @xmath185 are relatively stable against @xmath175 . under a natural lack of label switching , @xmath53 seems to be well approximated using only @xmath186 , as seen in table [ q_d1 ] .",
    "even when some label switching occurs in a gibbs sequence corresponding to a gaussian mixture model with three components , only two terms , @xmath186 and @xmath187 , significantly contribute to @xmath53 , as seen in table [ q_d2 ] .",
    "for the subsequent analyses in this paper , we chose @xmath183 , @xmath188 and @xmath182 . +    [ ht!]@xmath189\\}_{i=1}^{k ! } & |\\mathfrak{a}(k)| & \\widehat{\\phi }   \\\\ \\hline 10 ^ 2 & [ 1 , 1.89\\times 10^{-102 } ] & 1 & 0 \\\\ 10 ^ 3 & [ 1 , 5.25 \\times 10^{-90 } ] & 1 & 0 \\\\ 10 ^ 4 & [ 1 , 4.62\\times 10^{-91 } ] & 1 & 0 \\\\    10 ^ 5 & [ 1 , 3.56\\times 10^{-80 } ] & 1 & 0 \\\\   \\hline \\end{array}\\ ] ]    [ ht!]@xmath190\\}_{i=1}^{k ! } & |\\mathfrak{a}(k)| & \\widehat{\\phi } \\\\",
    "\\hline   10 ^ 2 & [ 3.56\\times 10^{-16},9.53\\times 10^{-160 } , 5.05\\times 10^{-55 } , 8.27\\times 10^{-144},1.0 , 4.64\\times 10^{-65 } ] & 2 & 0 \\\\   10 ^ 3 & [ 1.22\\times 10^{-8 } , 1.11\\times 10^{-144 } , 3.01\\times 10^{-49 } , 3.08\\times 10^{-125 } , 1.0 , 2.27\\times 10^{-53 } ] & 2 & 0 \\\\   10 ^ 4 & [ 2.03\\times 10^{-8},8.31\\times 10^{-136 } , 1.76\\times 10^{-43 } , 2.61\\times 10^{-95 } , 1.0 , 4.87\\times 10^{-49 } ] & 2 & 0 \\\\    10 ^ 5 & [ 1.04\\times 10^{-5 } , 1.56\\times 10^{-122 } , 1.51 \\times 10^{-44 } , 4.30\\times 10^{-87 } , 1.0 , 2.27\\times 10^{-39 } ] & 2 & 0 \\\\ \\hline \\end{array}\\ ] ]      the following seven marginal likelihood estimators using an equal number of proposals are compared ;    @xmath191 : : :    chib s method ( [ eq_01 ] ) using @xmath192 samples and    multiplying by @xmath22 to compensate for a lack of label    switching ; @xmath193 : : :    chib s method with density estimate ( [ eq_01 ] ) , using    @xmath192 randomly permuted gibbs samples ; @xmath194 : : :    importance sampling using @xmath45 as in ( [ eq_q_mle ] ) , with a    maximum likelihood estimate for @xmath195 and    @xmath192 particles ; @xmath196 : : :    dual importance sampling using @xmath45 as in ( [ eq_ds ] ) , with    @xmath192 particles and @xmath197 gibbs samples    in @xmath53 ; @xmath198 : : :    dual importance sampling using an approximation as in ( [ eq_08 ] ) , with    @xmath192 particles , @xmath197 and    @xmath188 ; @xmath199 : : :    importance sampling using @xmath45 as in ( [ eq_perms_q ] ) , with    @xmath192 particles . when @xmath200 ,    @xmath201 and otherwise @xmath202 ; @xmath203 : : :    bridge sampling ( [ eq_bs ] ) , using @xmath204    samples and @xmath53 as in ( [ eq_perms_q ] ) via 10    iterations .",
    "for @xmath45 , it is set as @xmath205    and label switching is imposed in hyperparameters    @xmath206 .",
    "the marginal likelihood estimates ( in log scales ) and the effective sample size ( ess ) ratios , @xmath207 , are summarized in figures [ sim_1 ] and [ sim_2 ] by boxplots , based on 50 replicates .",
    "subscripts of @xmath208 and @xmath209 denote the estimating technique .",
    "note that a modified ess , provided by equation ( 35 ) in @xcite , is used here for numerical stability .",
    "all estimators are based on @xmath173 proposals , as in table [ table_computation ] , where summing up the second and third columns leads to a fixed total number of function evaluations . within our setup",
    ", @xmath210 is the least demanding in terms of computational workload while the remaining importance estimators require the same computing time , except for @xmath198 .",
    "@xmath211      mixture models of two and three components are fitted to @xmath163 and @xmath164 respectively .",
    "regardless of the presence or not of label switching in the resulting gibbs sequences , all estimates based on importance sampling except @xmath194 coincide with @xmath193 , albeit with smaller monte carlo variations as seen in figures [ sim_1 ] and [ sim_2 ] .",
    "when a suitable approximate for @xmath53 is used for the dual importance sampling , no significant difference in the estimates @xmath212 and in the effective sample sizes are observed .",
    "the mean sizes of @xmath153 in table [ table_data ] are always smaller than @xmath22 and it shows that @xmath23 can be estimated with a lesser computational workload . when posterior modes are very well separated ( no natural label switching ever present in gibb sequences ) , the number of evaluations in @xmath45 is reduced almost by the maximal factor of @xmath213 . in table",
    "[ table_time_data ] , the least computational demand is observed for the chib s methods while the bridge sampling costs more than 100 times . when @xmath214 , some reduction in cpu time for @xmath215 is observed due to ignoring zero function evaluation .",
    "disagreement in the values of @xmath194 versus @xmath193 shows that an importance function may fail to properly approximate @xmath216 , resulting in an unreliable estimate with large variation .",
    "significantly small effective sample sizes ( i.e. , very small values for @xmath217 ) back this observation . in our simulation experiments , we observed that @xmath218 is correctly calibrated for a large value of @xmath69 ( i.e. , a large number of conditional densities in @xmath45 ) .",
    "when label switching naturally occurs , as in the gibbs sequence under the variance prior @xmath172 , @xmath191 disagrees with the other estimates , see figure [ sim_2 ] .",
    "unsurprisingly , this indicates that the simplistic correction through a multiplication by @xmath22 is of no use , as reported in @xcite , @xcite and @xcite .",
    "( 14.5,6 ) ( 0,3 ) and _ ( bottom ) _",
    "@xmath164 , respectively .",
    "the prior for @xmath170 is @xmath171 and label switching did not occur in gibbs samples .",
    "one outlier of @xmath194 in the top - left panel is discarded .",
    ", title=\"fig:\",width=340,height=113 ] ( 0,0 ) and _ ( bottom ) _ @xmath164 , respectively .",
    "the prior for @xmath170 is @xmath171 and label switching did not occur in gibbs samples .",
    "one outlier of @xmath194 in the top - left panel is discarded .",
    ", title=\"fig:\",width=340,height=113 ]    ( 8.5,3 ) and _ ( bottom ) _",
    "@xmath164 , respectively .",
    "the prior for @xmath170 is @xmath171 and label switching did not occur in gibbs samples .",
    "one outlier of @xmath194 in the top - left panel is discarded .",
    ", title=\"fig:\",width=170,height=113 ] ( 8.5,0 ) and _ ( bottom ) _",
    "@xmath164 , respectively .",
    "the prior for @xmath170 is @xmath171 and label switching did not occur in gibbs samples .",
    "one outlier of @xmath194 in the top - left panel is discarded .",
    ", title=\"fig:\",width=170,height=113 ]    ( 1.7,0.1)@xmath194 ( 2.7,0.1)@xmath191 ( 3.8,0.1)@xmath193 ( 4.8,0.1)@xmath196 ( 5.8,0.1)@xmath198 ( 6.8,0.1)@xmath199 ( 7.8,0.1)@xmath218    ( 1.7,3.1)@xmath194 ( 2.7,3.1)@xmath191 ( 3.8,3.1)@xmath193 ( 4.8,3.1)@xmath196 ( 5.8,3.1)@xmath198 ( 6.8,3.1)@xmath199 ( 7.8,3.1)@xmath218    ( 9.6,0.1)@xmath217 ( 10.4,0.1)@xmath219 ( 11.4,0.1)@xmath220 ( 12.3,0.1)@xmath221    ( 9.6,3.1)@xmath217 ( 10.4,3.1)@xmath219 ( 11.4,3.1)@xmath220 ( 12.3,3.1)@xmath221    ( 14.5,6 ) ( 0,3 ) and _ ( bottom ) _",
    "@xmath164 , respectively .",
    "the prior for @xmath170 is @xmath172 and label switching naturally occurred in gibbs samples .",
    "two outliers for @xmath191 in the top - left panel are discarded.,title=\"fig:\",width=340,height=113 ] ( 0,0 ) and _ ( bottom ) _",
    "@xmath164 , respectively .",
    "the prior for @xmath170 is @xmath172 and label switching naturally occurred in gibbs samples .",
    "two outliers for @xmath191 in the top - left panel are discarded.,title=\"fig:\",width=340,height=113 ]    ( 8.5,3 ) and _ ( bottom ) _",
    "@xmath164 , respectively .",
    "the prior for @xmath170 is @xmath172 and label switching naturally occurred in gibbs samples .",
    "two outliers for @xmath191 in the top - left panel are discarded.,title=\"fig:\",width=170,height=113 ] ( 8.5,0 ) and _ ( bottom ) _",
    "@xmath164 , respectively .",
    "the prior for @xmath170 is @xmath172 and label switching naturally occurred in gibbs samples .",
    "two outliers for @xmath191 in the top - left panel are discarded.,title=\"fig:\",width=170,height=113 ]    ( 1.7,0.1)@xmath194 ( 2.7,0.1)@xmath191 ( 3.8,0.1)@xmath193 ( 4.8,0.1)@xmath196 ( 5.8,0.1)@xmath198 ( 6.8,0.1)@xmath199 ( 7.7,0.1)@xmath218    ( 1.7,3.1)@xmath194 ( 2.7,3.1)@xmath191 ( 3.8,3.1)@xmath193 ( 4.8,3.1)@xmath196 ( 5.8,3.1)@xmath198 ( 6.8,3.1)@xmath199 ( 7.7,3.1)@xmath218    ( 9.5,0.1)@xmath217 ( 10.4,0.1)@xmath219 ( 11.3,0.1)@xmath220 ( 12.1,0.1)@xmath221    ( 9.5,3.1)@xmath217 ( 10.4,3.1)@xmath219 ( 11.3,3.1)@xmath220 ( 12.1,3.1)@xmath221    @xmath222    @xmath223      the priors suggested by @xcite are used for our simulation study : @xmath224 where @xmath225 and @xmath226 are the median and the range of @xmath60 , respectively .",
    "normal mixture models are fitted to both datasets and estimates of @xmath227 and @xmath209 are summarized in figures [ sim_fishery ] and [ sim_galaxy ] . in general , a similar behaviour of @xmath212 and @xmath209 between the methods is observed .",
    "for all cases , the dual importance sampling schemes ( @xmath196 and @xmath228 ) and @xmath199 agree with chib s approach ( @xmath193 ) . unless modes of the joint posterior distributions are clearly separated ( e.g. , @xmath229 ) , @xmath230 is biased due to an improper permutation correction . when a poor @xmath53 is used for importance sampling , inaccurate approximations result and",
    "the range of @xmath194 estimates is much off from the other estimates .",
    "symptoms of the  curse of dimensionality \" can be observed . as @xmath1 increases , the effective sample size decreases exponentially fast and the variation in the estimates increases .",
    "given the complex shape of the posterior distribution , the support common to @xmath53 and @xmath231 gets progressively smaller and @xmath218 becomes less accurate , as shown in both figures .",
    "when @xmath232 , the variation in the values of @xmath193 is much larger than those of the estimate by dual importance sampling .",
    "when @xmath233 , @xmath45 does not provide a good approximation of the joint posterior and @xmath234 is then biased .",
    "due to a fast increase of @xmath22 , fast increasing in cpu times is seen for all estimators in table [ table_time_galaxyfishery ] .",
    "the reduction in the number of evaluated terms used to approximate @xmath80 varies case by case , as shown in table [ table_galaxyfishery ] . when @xmath235 and @xmath232 , components of the posterior distribution for the galaxy data tend to have long flat tails and thus have higher chance to overlap each other .",
    "consequently , the workload reduction is of lesser magnitude than for a model with a smaller number of components .",
    "provided that some functions are neglected for @xmath228 , there is some gain in computational efficiency as can be seen in table [ table_galaxyfishery ] .",
    "@xmath236    ( 14,6 ) ( 0,3 ) in the top - left panel are discarded.,title=\"fig:\",width=340,height=113 ] ( 0,0 ) in the top - left panel are discarded.,title=\"fig:\",width=340,height=113 ] ( 8.5,3 ) in the top - left panel are discarded.,title=\"fig:\",width=188,height=113 ] ( 8.5,0 ) in the top - left panel are discarded.,title=\"fig:\",width=188,height=113 ]    ( 1.7,0.1)@xmath194 ( 2.7,0.1)@xmath237 ( 3.7,0.1)@xmath218 ( 4.7,0.1)@xmath193 ( 5.8,0.1)@xmath196 ( 6.7,0.1)@xmath238 ( 7.8,0.1)@xmath199    ( 1.7,3.1)@xmath194 ( 2.7,3.1)@xmath237 ( 3.7,3.1)@xmath218 ( 4.7,3.1)@xmath193 ( 5.8,3.1)@xmath196 ( 6.7,3.1)@xmath238 ( 7.8,3.1)@xmath199    ( 9.7,0.1)@xmath217 ( 10.7,0.1)@xmath219 ( 11.7,0.1)@xmath220 ( 12.6,0.1)@xmath239    ( 9.7,3.1)@xmath217 ( 10.7,3.1)@xmath219 ( 11.7,3.1)@xmath220 ( 12.6,3.1)@xmath221    ( 14,9 ) ( 0,6 ) in the top - left panel is discarded . , title=\"fig:\",width=340,height=113 ] ( 0,3 ) in the top - left panel is discarded . , title=\"fig:\",width=340,height=113 ]",
    "( 0,0 ) in the top - left panel is discarded .",
    ", title=\"fig:\",width=340,height=113 ] ( 8.5,6 ) in the top - left panel is discarded .",
    ", title=\"fig:\",width=188,height=113 ] ( 8.5,3 ) in the top - left panel is discarded . , title=\"fig:\",width=188,height=113 ] ( 8.5,0 ) in the top - left panel is discarded . ,",
    "title=\"fig:\",width=188,height=113 ]    ( 1.7,0.1)@xmath194 ( 2.7,0.1)@xmath237 ( 3.7,0.1)@xmath218 ( 4.7,0.1)@xmath193 ( 5.8,0.1)@xmath196 ( 6.7,0.1)@xmath238 ( 7.8,0.1)@xmath199    ( 1.7,3.1)@xmath194 ( 2.7,3.1)@xmath237 ( 3.7,3.1)@xmath218 ( 4.7,3.1)@xmath193 ( 5.8,3.1)@xmath196 ( 6.7,3.1)@xmath238 ( 7.8,3.1)@xmath199    ( 1.7,6.1)@xmath194 ( 2.7,6.1)@xmath237 ( 3.7,6.1)@xmath218 ( 4.7,6.1)@xmath193 ( 5.8,6.1)@xmath196 ( 6.7,6.1)@xmath238 ( 7.8,6.1)@xmath199    ( 9.7,0.1)@xmath217 ( 10.7,0.1)@xmath219 ( 11.7,0.1)@xmath220 ( 12.6,0.1)@xmath221    ( 9.7,3.1)@xmath217 ( 10.7,3.1)@xmath219 ( 11.7,3.1)@xmath220 ( 12.6,3.1)@xmath221    ( 9.7,6.1)@xmath217 ( 10.7,6.1)@xmath219 ( 11.7,6.1)@xmath220 ( 12.6,6.1)@xmath221",
    "this paper considered evidence approximations by importance sampling for mixture models and re - evaluated some of the known challenges resulting from high multimodality in the posterior density .",
    "importance sampling requires that the support of an importance function encompasses the support of the posterior density to perform properly . in the specific case on mixture models ,",
    "missing some of the invariance under permutation function is likely to produce an unsuitable support hence , a poor estimate of the evidence .    in our study ,",
    "exchangeable priors are used , which implies that the posterior and marginal posterior densities exhibit @xmath22 symmetrical terms .",
    "two marginal likelihood estimators are proposed here and tested against other existing estimators .",
    "the first approach exploits the permutation invariance of @xmath240 with a pointwise mle , @xmath75 , to create an importance function .",
    "however , due to a poor resulting support , this approach performs quite poorly in our simulation studies .",
    "another poor estimate is derived from chib s method when the invariance by permutation is not reproduced in the sample @xcite .",
    "a second importance function is constructed by double rao  blackwellisation , hence the denomination of _ dual importance sampling_. we demonstrate both methodologically and practically that this solution fits the demands of mixture estimation .",
    "moreover , introducing a suitable and implementable approximation scheme , we show how to avoid the exponential increase in @xmath1 of the computational workload .",
    "the idea at the core of this approximation is to bypass negligible elements in the approximation thanks to the perfect symmetry of the posterior density . when posterior modes are well - separated , the gain is of a larger magnitude than when those modes strongly overlap .",
    "borrowing from the original approach in @xcite , dual importance sampling can be extended to cases when conditional gibbs sampling densities are not available in closed form .",
    "however , this solution suffers from the curse of dimensionality , just like any other importance sampling estimator .",
    "alternative evidence approximation techniques could be considered for this problem , as exemplified in @xcite .",
    "for instance , _ ensemble monte carlo _ samples from local ensembles that are extensions or compositions of the original , e.g. , using parallel tempering monte carlo methods .",
    "extending this idea , bayes factor approximations were proposed using annealed importance sampling @xcite and power posteriors @xcite .",
    "further investigation is needed to characterize the performances of those alternative solutions in the setting of mixture models and label switching .                                                               ( 2010 ) .",
    "`` importance sampling methods for bayesian discrimination between embedded models . '' in chen , m .- h . , dey , d. , mller , p. , sun , d. , and ye , k. ( eds . ) , _ frontiers of statistical decision making and bayesian analysis_. springer - verlag , new york .",
    "marin , j .-",
    "m . , mengersen , k. , and robert , c.  p. ( 2005 ) .",
    "`` bayesian modelling and inference on mixtures of distributions . '' in rao , c. and dey , d. ( eds . ) , _ handbook of statistics _ ,",
    "volume  25 .",
    "springer - verlag , new york .",
    "raftery , a. , newton , m. , satagopan , j. , and krivitsky , p. ( 2006 ) .",
    "`` estimating the integrated likelihood via posterior simulation using the harmonic mean identity . ''",
    "technical report 499 , university of washington , department of statistics .                 ( 1988 ) .",
    "`` using the sir algorithm to simulate posterior distributions . '' in bernardo , j.  m. , degroot , m.  h. , lindley , d.  v. , and smith , a. f.  m. ( eds . ) , _ bayesian statistics , 3 _ , 395402 .",
    "oxford university press .",
    "satagopan , j. , newton , m. , and raftery , a. ( 2000 ) . `` easy estimation of normalizing constants and bayes factors from posterior simulation : stabilizing the harmonic mean estimator . '' technical report 1028 , university of wisconsin - madison , department of statistics .",
    "we are most grateful to the editorial team of bayesian analysis for their helpful suggestions and their support towards this revision .",
    "financial support by ceremade , universit paris - dauphine and auckland university of technology for a visit of jeong eun lee is most appreciated .",
    "christian robert is partially supported by the agence nationale de la recherche ( anr , 212 , rue de bercy 75012 paris ) through the 20102015 anr-11-bs01 - 0010 grant `` calibration '' and by a 20102015 institut universitaire de france senior chair ."
  ],
  "abstract_text": [
    "<S> the marginal likelihood is a central tool for drawing bayesian inference about the number of components in mixture models . </S>",
    "<S> it is often approximated since the exact form is unavailable . </S>",
    "<S> a bias in the approximation may be due to an incomplete exploration by a simulated markov chain ( e.g. , a gibbs sequence ) of the collection of posterior modes , a phenomenon also known as lack of label switching , as all possible label permutations must be simulated by a chain in order to converge and hence overcome the bias . in an importance sampling approach , imposing label switching to the importance function results an exponential increase of the computational cost with the number of components . in this paper , </S>",
    "<S> two importance sampling schemes are proposed through choices for the importance function ; a mle proposal and a rao  blackwellised importance function . the second scheme is called dual importance sampling . </S>",
    "<S> we demonstrate that this dual importance sampling is a valid estimator of the evidence . to reduce the induced high demand in computation , the original importance function is approximated but a suitable approximation can produce an estimate with the same precision and with reduced computational workload .     and </S>"
  ]
}