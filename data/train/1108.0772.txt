{
  "article_text": [
    "this note presents a short proof of the duality formula  for @xmath0 divergences defined through differentiable convex functions @xmath1 in parametric models and discusses some unexpected phenomenon in the context of exponential families .",
    "first versions of this formula appear in @xcite p 33 , in @xcite in the context of the kullback - leibler divergence and in @xcite in a general form .",
    "the paper @xcite introduces this form in the context of minimal @xmath2 estimation ; a global approach to this formulation is presented in broniatowski and kziou ( 2006)@xcite . independently liese and vajda ( 2006)@xcite have obtained a similar expression based on a much simpler argument as presented in all the above mentioned papers ( formula ( 118 ) in their paper ) ; however the proof of their result is merely sketched and we have found it useful to present a complete treatment of this interesting result in the parametric setting , in contrast with the aforementioned approaches .",
    "the main interest of the resulting expression is that it leads to a wide variety of estimators , by a plug in method of the empirical measure evaluated on the current data set ; so , for any type of sampling its estimators and inference procedures , for any @xmath0divergence criterion . in the case of the simple i.i.d .",
    "sampling resulting properties of those estimators and subsequent inferential procedures are studied in @xcite .",
    "a striking fact is that all minimum divergence estimators defined through this dual formula coincide with the mle in exponential families .",
    "they henceforth enjoy strong optimality under the standard exponential models , leading to estimators different from the mle  under different models than the exponential one .",
    "also this result proves that mle  s of parameters of exponential families are strongly motivated by being generated by the whole continuum of @xmath0divergences .",
    "this note results from joint cooperation with late igor vajda .",
    "let @xmath3 an identifiable parametric model on @xmath4 where @xmath5 is a subset of @xmath6 all measures in @xmath7 will be assumed to be measure equivalent sharing therefore the same support .",
    "the parameter space @xmath5 need not be open in the present setting .",
    "it may even happen that the model includes measures which would not be probability distributions ; cases of interest cover models including mixtures of probability distributions ; see @xcite .",
    "let @xmath1 be a proper closed convex function from @xmath8-\\infty,+\\infty\\lbrack$ ] to @xmath9 $ ] with @xmath10 and such that its domain @xmath11 is an interval with endpoints @xmath12 ( which may be finite or infinite ) . for two measures @xmath13 and @xmath14 in @xmath7 the @xmath1-divergence between @xmath15 and @xmath16",
    "is defined by @xmath17 in a broader context , the @xmath1-divergences were introduced by @xcite as @xmath18-divergences .",
    "the basic property of @xmath0 divergences states that when @xmath1 is strictly convex on a neighborhood of @xmath19 , then @xmath20 we refer to @xcite chapter 1 for a complete study of those properties .",
    "let us simply quote that in general @xmath21 and @xmath22are not equal .",
    "hence , @xmath1-divergences usually are not distances , but they merely measure some difference between two measures . a main feature of divergences between distributions of random variables @xmath23 and @xmath24 is the invariance property with respect to common smooth change of variables .",
    "the kullback - leibler @xmath25 , modified kullback - leibler @xmath26 , @xmath27 , modified @xmath27 @xmath28 , hellinger @xmath29 , and @xmath30 divergences are respectively associated to the convex functions @xmath31 , @xmath32 , @xmath33 , @xmath34 , @xmath35 and @xmath36 .",
    "all these divergences except the @xmath30 one , belong to the class of the so called power divergences  introduced in @xcite ( see also @xcite chapter 2 ) , a class which takes its origin from rnyi @xcite .",
    "they are defined through the class of convex functions @xmath370,+\\infty\\lbrack\\mapsto\\varphi_{\\gamma}(x):=\\frac{x^{\\gamma}-\\gamma x+\\gamma-1}{\\gamma(\\gamma-1 ) } \\label{gamma convex functions}\\ ] ] if @xmath38 , @xmath39 and @xmath40 .",
    "so , the @xmath41-divergence is associated to @xmath42 , the @xmath43 to @xmath44 , the @xmath27 to @xmath45 , the @xmath46 to @xmath47 and the hellinger distance to @xmath48",
    ".    it may be convenient to extend the definition of the power divergences in such a way that @xmath21 may be defined ( possibly infinite ) even when @xmath13 or @xmath14 is not a probability measure .",
    "this is achieved setting @xmath37-\\infty,+\\infty\\lbrack\\mapsto\\left\\ { \\begin{array } [ c]{lll}\\varphi_{\\gamma}(x ) & \\text { if } & x\\in\\lbrack0,+\\infty\\lbrack,\\\\ + \\infty & \\text { if } & x\\in]-\\infty,0[. \\end{array } \\right .",
    "\\label{gamma convex functions sur r}\\ ] ] when dom@xmath49 note that for the @xmath27-divergence , the corresponding @xmath1 function @xmath50 is defined and convex on whole @xmath51 .",
    "we will only consider divergences defined through differentiable functions @xmath1 , which we assume to satisfy    ( * rc * )     [ c]lthere exists a positive @xmath52 such that for all @xmath53 in @xmath54   $ ] , + we can find numbers @xmath55 @xmath56 such that + @xmath57 , for all real @xmath58 .    condition ( * rc * ) holds for all power divergences including @xmath41 and @xmath43 divergences .",
    "let @xmath59 and @xmath60 be any parameters in @xmath61 we intend to provide a new expression for @xmath62    by strict convexity , for all @xmath63 and @xmath64 @xmath65the domain of @xmath1 it holds@xmath66 with equality if and only if @xmath67    denote @xmath68    for any @xmath69 in @xmath5 denote@xmath70 define@xmath71    inserting these values in ( [ convevity of phi ] ) and integrating with respect to @xmath72 yields@xmath73   dp_{\\theta_{t}}.\\ ] ] assume at present that this entails @xmath74 for suitable @xmath69 s in some set @xmath75 included in @xmath5 .",
    "when @xmath76 the inequality in ( [ ineg ] ) turns to equality , which yields @xmath77 denote @xmath78 from which @xmath79 furthermore by ( [ ineg ] ) , for all suitable @xmath69 @xmath80 and the function @xmath81 is non negative , due to ( [ convevity of phi ] ) .",
    "it follows that @xmath82 is zero only if @xmath83 @xmath72 a.e .",
    "therefore for any @xmath58 in the support of @xmath72 @xmath84   -\\varphi^{\\#}\\left (   \\frac { dp_{\\theta}}{dp_{\\alpha}}(x)\\right )   + \\varphi^{\\#}\\left (   \\frac{dp_{\\theta}}{dp_{\\theta_{t}}}\\left (   x\\right )   \\right )   = 0\\ ] ] which can not hold for all @xmath58 when the functions @xmath85 @xmath86 and @xmath87 are linearly independent , unless @xmath88 we have proved that @xmath60 is the unique optimizer in ( [ forme duale div ] ) .    we have skipped some sufficient conditions which ensure that ( [ ineg ] ) holds .",
    "assume that @xmath89 assume further that @xmath90 is finite .",
    "since @xmath91 we obtain@xmath92 which entails ( [ ineg ] ) . when @xmath93 then clearly , under ( [ p_teta / p_alfain l1(teta ) ] ) @xmath94 we have proved that ( [ forme duale div ] ) holds when @xmath69 satisfies ( [ p_teta / p_alfain l1(teta ) ] ) .",
    "sufficient and simple conditions encompassing ( [ p_teta / p_alfain l1(teta ) ] ) can be assessed under standard requirements for nearly all divergences .",
    "we state the following lemma ( see liese and vajda ( 1987)@xcite ) and broniatowski and kziou ( 2006 ) @xcite , lemma 3.2 ) .",
    "assume that * rc * holds and @xmath95 is finite .",
    "then ( [ p_teta / p_alfain l1(teta ) ] ) holds .",
    "summing up ,  we state    let @xmath59 belong to @xmath5 and let @xmath90 be finite .",
    "assume that * rc * holds.let @xmath75 be the subset of all @xmath69 s in @xmath5 such that @xmath95 is finite .",
    "then @xmath96 furthermore the sup is reached at @xmath60 and uniqueness holds .    for",
    "the cressie - read family of divergences with @xmath97 this representation writes@xmath98 the set @xmath75 may depend on the choice of the parameter @xmath59 .",
    "such is the case for the @xmath27 divergence i.e. @xmath99 when @xmath100 in most cases the difficulty of dealing with a specific set @xmath75 depending on @xmath59 can be encompassed when @xmath101 which for example holds in the above case for any @xmath102 this simplication deserves to be stated in the next result    when @xmath90 is finite and * rc * holds , then under condition ( a ) @xmath103 furthermore the sup is reached at @xmath60 and uniqueness holds .",
    "identifying @xmath75 might be cumbersome .",
    "this difficulty also appears in the classical mle case , a special case of the above statement with divergence function @xmath44 @xmath104for which it is assumed that @xmath105 for @xmath59 in a neighborhood of @xmath102    under the above notation and hypotheses define @xmath106 it then holds@xmath107 for all @xmath60 in @xmath5 .",
    "also let @xmath108 which also satisfies@xmath109 for all @xmath60 in @xmath5 .",
    "we thus state    when @xmath90 is finite for all @xmath59 in @xmath5 and * rc * holds , both functionals @xmath110 and @xmath111 are fisher consistent for all @xmath60 in @xmath61",
    "from ( [ forme dual div pour est ] ) simple estimators for @xmath60 can be defined , plugging any convergent empirical measure in place of @xmath72 and taking the infimum in @xmath59 in the resulting estimator of @xmath62    in the context of simple i.i.d .",
    "sampling , introducing the empirical measure @xmath112 where the @xmath113 s are i.i.d .",
    "r.vs with common unknown distribution @xmath72 in @xmath114 the natural estimator of @xmath115 is @xmath116    since @xmath117 the resulting estimator of @xmath118 is @xmath119 also the estimator of @xmath60 is obtained as    @xmath120@xmath121 when [ ( a ) ] holds then @xmath75 may be substituted by @xmath122 in the above definitions .    the resulting minimum dual divergence estimators ( [ def div estim ] ) and ( [ def emphid estimates ] ) do not require any smoothing or grouping , in contrast with the classical approach which involves quantization .",
    "the paper @xcite provides a complete study of those estimates and subsequent inference tools for the usual i.i.d .",
    "sample scheme@xmath123 for all divergences considered here , these estimators are asymptotically efficient in the sense that they achieve the cramer - rao bound asymptotically .",
    "the case when @xmath124 leads to @xmath125 defined as the celebrated maximum likelihood estimator ( mle ) , in the context of the simple sampling .",
    "in this section we prove the following result    for all divergence @xmath126 defined through a differentiable function @xmath1 satisfying condition ( * rc * ) , the minimum dual divergence estimator defined by ( [ def emphid estimates ] ) coincides with the mle on any full exponential families such that @xmath127 is finite for all @xmath59 and @xmath69 in @xmath61    let @xmath7 be an exponential family on @xmath128 with canonical parameter in @xmath4 @xmath129{c}p_{\\theta}\\text { such that } p_{\\theta}(x)=\\frac{dp_{\\theta}}{d\\lambda}(x)\\\\ = \\exp\\left [   t(x)^{\\prime}\\theta - c(\\theta)\\right ]   \\text { ; } \\theta\\in\\theta \\end{array } \\right\\}\\ ] ] where @xmath58 is in @xmath128 and @xmath5 is an open subset of @xmath4 , and @xmath130 is a dominating measure for @xmath131 we assume @xmath7 to be full , namely that the hessian matrix @xmath132 is definite positive for all @xmath59 in @xmath61      we will prove that @xmath137 whatever the function @xmath1 satisfying the claim . in ( [ infsupm_n=0 ] )",
    "@xmath59 and @xmath69 run in @xmath61 this result extends the maximum likelihood case for which @xmath138   = 0.$ ]"
  ],
  "abstract_text": [
    "<S> in this note we prove the dual representation formula of the divergence between two distributions in a parametric model . resulting estimators for the divergence as for the parameter </S>",
    "<S> are derived .  </S>",
    "<S> these estimators do not make use of any grouping nor smoothing . </S>",
    "<S> it is proved that all differentiable divergences induce the same estimator of the parameter on any regular exponential family , which is nothing else but the mle .    </S>",
    "<S> key words : statistical divergence ; minimum divergence estimator ; maximum likelihood ; exponential family </S>"
  ]
}