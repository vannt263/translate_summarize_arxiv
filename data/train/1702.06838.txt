{
  "article_text": [
    "this paper discusses a fundamental class of convex matrix optimization problems with low - rank solutions .",
    "we argue that the main obstacle that prevents us from solving these problems at scale is not arithmetic , but storage .",
    "we exhibit the first provably correct algorithm for these problems with optimal storage .      to explain the challenge , we consider the problem of low - rank matrix completion .",
    "let @xmath0 be an unknown matrix , but assume that a bound @xmath1 on the rank of @xmath2 is available , where @xmath3 .",
    "suppose that we record noisy observations of a subset @xmath4 of entries from the matrix : @xmath5 the variables @xmath6 model ( unknown ) noise .",
    "the goal is to approximate the full matrix @xmath2 .",
    "matrix completion arises in machine learning applications , such as recommendation systems  @xcite .",
    "we can frame the matrix completion problem as a rank - constrained optimization : @xmath7 in general , the formulation   is intractable .",
    "instead , we retrench to a tractable convex problem @xcite : @xmath8 the schatten @xmath9-norm @xmath10 returns the sum of the singular values of its argument ; it is an effective proxy for the rank  @xcite . adjusting the value of the parameter @xmath11 modulates the rank of a solution @xmath12 of  .",
    "if we have enough data and choose @xmath11 well , we expect that each solution @xmath12 approximates the target matrix @xmath2 .",
    "the convex problem   is often a good model for matrix completion when the number of observations @xmath13 , where @xmath14 suppresses log - like factors ; see  @xcite .",
    "we can write a rank-@xmath1 approximation to a solution @xmath12 using @xmath15 parameters .",
    "thus , we can express the problem and an approximate solution with @xmath16 storage .    nevertheless , we need fully @xmath17 numbers to express the decision variable @xmath18 for the optimization problem  .",
    "the cost of storing the decision variable prevents us from solving large - scale instances of  , even without worrying about arithmetic .",
    "this discrepancy raises a question : * is there an algorithm that computes an approximate solution to   using the optimal storage @xmath19 ? *      here is another instance of the same predicament .",
    "fix a vector @xmath20 .",
    "suppose that we acquire @xmath21 noisy quadratic measurements of @xmath22 with the form @xmath23 the @xmath24 are known measurement vectors , and the @xmath25 model measurement noise .",
    "given the data @xmath26 and the vectors @xmath27 , the phase retrieval problem asks us to reconstruct @xmath22 up to a global phase shift .",
    "phase retrieval problems are prevalent in imaging science because it is easier to measure the intensity of light than its phase . in practice ,",
    "the vectors @xmath27 are structured because they reflect the physics of the imaging system .",
    "let us outline a convex approach  @xcite to the phase retrieval problem .",
    "the data   satisfies @xmath28 thus , we can formulate phase retrieval as @xmath29 now , pass to the convex problem @xmath30 we can estimate the parameter @xmath31 from @xmath27 and @xmath26 ; see  ( * ? ? ?",
    "ii ) . to approximate the true vector @xmath32 , we compute a top eigenvector @xmath33 of a solution to  .",
    "this procedure is often an effective approach for phase retrieval when the number of measurements @xmath34 ; see ( * ? ? ?",
    "once again , we recognize a discrepancy .",
    "the problem data @xmath35 and the approximate solution @xmath36 use storage @xmath37 , but the matrix variable in   requires @xmath38 storage",
    ".    we may ask : * is there an algorithm that computes an approximate solution to   using the optimal storage @xmath37 ?",
    "*      matrix completion and phase retrieval are examples of _ convex low - rank matrix optimization _ ( clro ) problems .",
    "informally , this class contains convex optimization problems whose decision variable is a matrix and whose solutions are ( close to ) low rank .",
    "these problems often arise as convex relaxations of rank - constrained problems ; however , the convex formulations are important in their own right",
    ".    there has been extensive empirical and theoretical work to validate the use of clros in a spectrum of applications .",
    "for example , see  @xcite .    over the last 20 years",
    ", optimization researchers have developed a diverse collection of algorithms for clro problems .",
    "surprisingly , every extant method lacks guarantees on storage or convergence ( or both ) .",
    "convex optimization algorithms dominated the early literature on algorithms for clro .",
    "the initial efforts , such as  @xcite , focused on interior - point methods , whose storage and arithmetic costs are forbidding . to resolve this issue , researchers turned to first - order convex algorithms , including bundle methods  @xcite , ( accelerated ) proximal gradient methods  @xcite , and the conditional gradient method ( cgm ) @xcite .",
    "convex algorithms are guaranteed to solve a clro .",
    "they come with a complete theory , including rigorous stopping criteria and bounds on convergence rates .",
    "they enjoy robust performance in practice . on the other hand , convex algorithms from the literature",
    "do not scale well enough to solve large clro problems because they operate on and store full - size matrices .",
    "the cgm iteration is sometimes touted as a low - storage method for clro  @xcite .",
    "indeed , cgm is guaranteed to increase the rank of an iterate by at most one per iteration .",
    "nevertheless , the algorithm converges slowly , so intermediate iterates can have very high rank .",
    "cgm variants , such as  @xcite , that control the rank of iterates lack storage guarantees or may not converge to a global optimum .",
    "recently , many investigators have sought recourse in nonconvex heuristics for solving clros .",
    "this line of work depends on the factorization idea of burer & monteiro  @xcite , which rewrites the matrix variable as a product of two low - rank factors .",
    "there are many heuristic procedures , e.g. ,  @xcite , that use clever initialization and nonlinear programming schemes in an attempt to optimize the factors directly .",
    "the resulting algorithms can have optimal storage costs , and they may achieve a fast rate of local convergence .",
    "there has been an intensive effort to justify the application of nonconvex heuristics for clro .",
    "to do so , researchers often frame unverifiable statistical assumptions on the problem data .",
    "for example , in the matrix completion problem  , it is common to assume that the entries of the matrix are revealed according to some ideal probability distribution  @xcite .",
    "when these assumptions fail , nonconvex heuristics can converge to the wrong point , or they may even diverge .    * * this paper explains how to extend the convex optimization algorithm cgm to obtain an approximate solution to a class of clro problems using optimal storage .",
    "our algorithm operates much like cgm , but it never forms the matrix variable explicitly .",
    "instead , we maintain a small randomized sketch of the matrix variable over the course of the iteration by means of a bespoke sketching method  @xcite .",
    "after the optimization method converges , we extract an approximate solution from the sketch .",
    "this technique achieves optimal storage , yet it converges under the same conditions and with the same guarantees as cgm .    in summary",
    ", this paper presents a solution to the problems posed above : the first algorithm for convex low - rank matrix optimization problems that provably uses optimal storage to compute an approximate solution .",
    "we write @xmath39 for the euclidean norm , @xmath40 for the frobenius norm , and @xmath10 for the schatten 1-norm ( aka the _ trace norm _ or the _ nuclear norm _ ) . depending on context",
    ", @xmath41 refers to the euclidean or frobenius inner product .",
    "the symbol @xmath42 denotes the conjugate transpose of a vector or matrix , as well as the adjoint of a linear map .",
    "the dagger @xmath43 refers to the pseudoinverse .",
    "the symbol @xmath44_r$ ] stands for a best rank-@xmath1 frobenius - norm approximation of the matrix @xmath45 .",
    "the function @xmath46 returns the minimum frobenius - norm distance from @xmath45 to a set @xmath47 .",
    "the symbol @xmath48 denotes the semidefinite order .",
    "we use the computer science interpretation of the order notation @xmath49 .",
    "let us begin with a generalization of the convex matrix completion formulation  . in  [ sec",
    ": cgm - psd ] , we return to the psd setting of the phase retrieval problem  .",
    "we consider a convex program with a matrix variable : @xmath50 the linear operator @xmath51 and its adjoint @xmath52 take the form @xmath53 @xmath54 each coefficient matrix @xmath55 .",
    "we interpret @xmath56 as a set of linear measurements of the matrix @xmath18 .",
    "for example , in the matrix completion problem  , the image @xmath56 lists the entries of @xmath18 indexed by the set @xmath4 .",
    "the function @xmath57 is convex and continuously differentiable . in many situations ,",
    "it is natural to regard the objective function as a loss : @xmath58 for a vector @xmath59 of measured data .    by choosing the parameter @xmath31 to be sufficiently small",
    ", we can often ensure that each minimizer of   is low - rank or close to low - rank .",
    "our goal is to develop a practical algorithm that provably computes a low - rank approximation of a solution to the problem  .    to validate as a model for a given application",
    ", one must undertake a separate empirical or theoretical study .",
    "we do not engage this question in our work .",
    "suppose that we want to produce a low - rank approximation to a solution of a generic instance of the problem  .",
    "what kind of storage can we hope to achieve ?",
    "it is clear that we need @xmath15 numbers to express a rank-@xmath1 approximate solution to .",
    "we must also understand how much extra storage is incurred because of the specific problem instance @xmath60 .",
    "it is natural to instate a _ black - box model _ for the linear map @xmath61 , its adjoint @xmath62 , and the objective function @xmath63 . for arbitrary vectors @xmath64 and @xmath65 and @xmath66 , assume we have routines that can compute @xmath67 we also assume routines for evaluating the function @xmath63 and its gradient @xmath68 for any argument @xmath66",
    ". we may neglect the storage used to compute these primitives .",
    "every algorithm based on these primitives must use storage @xmath69 just to represent their outputs .",
    "thus , under the black - box model , any algorithm that produces a rank-@xmath1 solution to a generic instance of   must use storage @xmath70 .",
    "we say that an algorithm is _ storage optimal _ if it achieves this bound .",
    "the parameter @xmath21 often reflects the amount of data that we have acquired , and it is usually far smaller than the dimension @xmath17 of the matrix variable in  .",
    "the problems that concern us are data - limited ; that is , @xmath71 .",
    "this is the situation where a strong structural prior ( e.g. , low rank or small schatten 1-norm ) is essential for fitting the data .",
    "this challenge is common in machine learning problems ( e.g. , matrix completion for recommendation systems ) , as well as in scientific applications ( e.g. , phase retrieval ) .    to the best of our knowledge ,",
    "no extant algorithm for   is guaranteed to produce an approximation of an optimal point and also enjoys optimal storage cost .",
    "to develop our algorithm for the model problem  , we must first describe a standard algorithm called the  _ conditional gradient method _ ( cgm ) .",
    "classic and contemporary references include  @xcite .",
    "cgm is a variant of gradient descent that replaces the gradient by a certain approximation .",
    "form the best linear underestimate of the objective function @xmath72 at a fixed matrix @xmath18 : @xmath73 for @xmath74 .",
    "@xmath75 to improve an estimated solution @xmath18 of  , cgm finds an update direction @xmath76 by minimizing the right - hand side of   over @xmath74 .",
    "the algorithm takes a small step in the direction @xmath77 .",
    "we can interpret the matrix @xmath77 as an approximate negative gradient of @xmath72 at @xmath18 .      here is the cgm algorithm for  .",
    "start with a feasible point , such as @xmath78 at each iteration @xmath79 , compute an update direction @xmath80 using the formulas @xmath81 @xmath82 @xmath83 returns a left / right pair of maximum singular vectors .",
    "update the decision variable : @xmath84 where @xmath85 . @xmath86",
    "the convex combination   remains feasible for   because @xmath87 and @xmath80 are feasible .",
    "cgm is a valuable algorithm for   because we can efficiently find the rank - one update direction @xmath80 by means of the singular vector computation  .",
    "the weak point of cgm is that the rank of @xmath87 typically increases with @xmath88 , and the peak rank of an iterate @xmath87 is often much larger than the rank of the solution of  .",
    "the cgm algorithm admits a simple stopping criterion . given a suboptimality parameter @xmath89 , we halt the cgm iteration when the duality gap @xmath90 : @xmath91 let @xmath12 be an optimal point for  .",
    "it is not hard to show  ( * ? ? ?",
    "2 ) that @xmath92 thus , the condition   ensures that the objective value @xmath93 is @xmath94-suboptimal .",
    "the cgm iterates satisfy within @xmath95 iterations  ( * ? ? ?",
    "* thm .  1 ) .",
    "the cgm iteration   requires @xmath96 storage because it maintains the @xmath97 matrix decision variable @xmath87 .",
    "we develop a remarkable extension of cgm that provably computes a rank-@xmath1 approximate solution to   with working storage @xmath98 .",
    "our approach depends on two efficiencies :    * we use the low - dimensional `` dual '' variable @xmath99 to drive the iteration . * instead of storing @xmath87",
    ", we maintain a small randomized sketch with size @xmath100 .",
    "it is easy to express the cgm iteration in terms of the `` dual '' variable @xmath101 .",
    "we can obviously rewrite the formula   for computing the rank - one update direction @xmath80 in terms of @xmath102 .",
    "we obtain an update rule for @xmath102 by applying the linear map @xmath61 to  .",
    "likewise , the stopping criterion   can be evaluated using @xmath102 and @xmath80 . under the black - box model",
    ", the dual formulation of cgm has storage cost @xmath103 .",
    "yet the dual formulation has a flaw : it `` solves '' the problem  , but we do not know the solution !    indeed , we must also track the evolution   of the primal decision variable @xmath87 . in the next subsection ,",
    "we summarize a randomized sketching method  @xcite that allows us to compute an accurate rank-@xmath1 approximation of @xmath87 but operates with storage @xmath15 .",
    "suppose that @xmath104 is a matrix that is presented to us as a stream of linear updates , as in  .",
    "for a parameter @xmath3 , we wish to maintain a small sketch that allows us to compute a rank-@xmath1 approximation of the final matrix @xmath18 .",
    "let us summarize an approach from the recent paper  @xcite .",
    "let us summarize an approach developed in our paper  @xcite .",
    "draw and fix two independent standard normal matrices @xmath105 and @xmath106 where @xmath107 @xmath108 the sketch consists of two matrices @xmath109 and @xmath110 that capture the range and co - range of @xmath18 : @xmath111 we can efficiently update the sketch @xmath112 to reflect a rank - one linear update to @xmath18 of the form @xmath113 both the storage cost for the sketch and the arithmetic cost of an update are @xmath15 .      the following procedure yields a rank-@xmath1 approximation @xmath114 of the matrix @xmath18 stored in the sketch  . @xmath115_r.\\ ] ] @xmath116_r.\\ ] ]",
    "the matrix @xmath117 has orthonormal columns that span the range of @xmath109 .",
    "the extra storage costs of the reconstruction are negligible ; its arithmetic cost is @xmath118 .",
    "see  @xcite for the intuition behind this method .",
    "it achieves the following error bound .",
    "[ thm : sketch - err - body ] fix a target rank @xmath1 .",
    "let @xmath18 be a matrix , and let @xmath112 be a sketch of @xmath18 of the form  . the procedure   yields a rank-@xmath1 matrix @xmath114 with @xmath119_r \\right\\vert}_{\\mathrm{f}}}.\\ ] ] similar bounds hold with high probability .",
    "* * the sketch size parameters @xmath120 appearing in   are recommended to balance storage against reconstruction quality .",
    "see  @xcite and our follow - up work for more details and for other sketching methods .",
    "we are now prepared to present sketchycgm , a storage - optimal extension of the cgm algorithm for the convex problem  .",
    "this method delivers a provably accurate low - rank approximation to a solution of  .",
    "see algorithm  [ alg : sketch - cgm ] for complete pseudocode .",
    "fix the suboptimality @xmath94 and the rank @xmath1 . draw and fix standard normal matrices @xmath121 and @xmath122 as in  .",
    "initialize the iterate and the sketches : @xmath123 @xmath124 at each iteration @xmath79 , compute an update direction via lanczos or via randomized svd  @xcite : @xmath125 @xmath126 set the learning rate @xmath85 .",
    "update the iterate and the two sketches : @xmath127 the iteration continues until it triggers the stopping criterion : @xmath128 at any iteration @xmath88 , we can form a rank-@xmath1 approximate solution @xmath129 of the model problem   by invoking the procedure   with @xmath130 and @xmath131 .",
    "suppose that the cgm iteration   generates the sequence @xmath132 of decision variables and the sequence @xmath133 of update directions .",
    "it is easy to verify that the sketchycgm iteration   maintains the loop invariants @xmath134 @xmath135 in view of the inequality   and the invariant  , the stopping rule   ensures that @xmath87 is an @xmath94-suboptimal solution to   when the iteration halts .",
    "furthermore , theorem  [ thm : sketch - err - body ] ensures that the computed solution @xmath129 is a near - optimal rank-@xmath1 approximation of @xmath87 at each time @xmath88 .",
    "the total storage cost is @xmath136 for the dual variable @xmath102 , the random matrices @xmath137 , and the sketch @xmath112 .",
    "owing to the black - box assumption  , the algorithm completes the singular vector computations in   with @xmath138 working storage . at no point during the iteration",
    "do we instantiate an @xmath97 matrix !",
    "arithmetic costs are on the same order as the standard version of cgm .",
    "sketch.init@xmath139 @xmath140 @xmath141 @xmath142 * break for * @xmath143 @xmath144 @xmath145 @xmath146    *methods for * sketch * object *     *methods for * sketch * object *     @xmath147 and @xmath148 @xmath149 and @xmath150 @xmath151 and @xmath152    @xmath153 @xmath154    @xmath155 @xmath156 @xmath157      sketchycgm is a provably correct method for computing a low - rank approximation of a solution to  .",
    "[ thm : low - rank - equivalence ] suppose that the iterates @xmath87 from the cgm iteration   converge to a matrix @xmath158 .",
    "let @xmath129 be the rank-@xmath1 reconstruction of @xmath87 produced by sketchycgm",
    ". then @xmath159_r   \\right\\vert}_{\\mathrm{f}}}.\\ ] ] in particular , if @xmath160 , then @xmath161    sketchycgm always works in the fundamental case where each solution of   has low rank .",
    "[ thm : sketch - cgm - unique ] suppose that the solution set @xmath162 of the problem   contains only matrices with rank @xmath1 or less .",
    "then sketchycgm attains @xmath163 .",
    "suppose that the optimal point of   is stable in the sense that the value of the objective function increases as we depart from optimality .",
    "then the sketchycgm estimates converge at a controlled rate .",
    "[ thm : cgm - sketch - rate ] fix @xmath164 and @xmath165 .",
    "suppose the ( unique ) solution @xmath12 of   has  @xmath166 and @xmath167 for all feasible @xmath18 .",
    "then we have the error bound @xmath168 where @xmath169 is the curvature constant  ( * ? ? ?",
    "( 3 ) ) of the problem  .",
    "next , we present a generalization of the convex phase retrieval problem  . consider the convex template @xmath170 as before , @xmath171 is a linear map , and @xmath172 is a differentiable convex function .",
    "it is easy to adapt sketchycgm to handle   instead of  . to sketch the complex psd matrix variable",
    ", we follow the approach described in  ( * ? ? ?",
    "we also make small changes to the sketchycgm iteration .",
    "replace the computation   with @xmath173 ` mineig ` returns the minimum eigenvalue @xmath174 and an associated eigenvector @xmath175 of a conjugate symmetric matrix .",
    "this variant behaves the same as sketchycgm .",
    "in this section , we demonstrate that sketchycgm is a practical algorithm for convex low - rank matrix optimization .",
    "we focus on phase retrieval problems because they provide a dramatic illustration of the power of storage - optimal convex optimization .      to begin ,",
    "we show that our approach to solving the convex phase retrieval problem   has better memory scaling than other convex optimization methods .",
    "we compare five convex optimization algorithms : the classic proximal gradient method ( pgm )  @xcite ; the auslender ",
    "teboulle ( at ) accelerated method  @xcite ; the classic cgm algorithm  @xcite ; a storage - efficient cgm variant ( thincgm )  @xcite based on low - rank svd updating ; and the psd variant of sketchycgm from  [ sec : cgm - psd ] with rank parameter @xmath176 .",
    "all five methods solve   reliably .",
    "the proximal methods ( pgm and at ) perform a full eigenvalue decomposition of the iterate at each step , but they can be accelerated by adaptively choosing the number of eigenvectors to compute .",
    "the methods based on cgm only need the top eigenvector , so they perform less arithmetic per iteration .    to compare the storage costs of the five algorithms , let us consider a synthetic phase retrieval problem .",
    "we draw a vector @xmath177 from the complex standard normal distribution",
    ". then we acquire @xmath178 phaseless measurements  , corrupted with independent gaussian noise so that the snr is 20 db .",
    "the measurement vectors @xmath27 derive from a coded diffraction pattern ; we solve the convex problem   with @xmath11 equal to the average of the measurements @xmath179 ; see  ( * ? ? ?",
    "then we compute a top eigenvector @xmath33 of the solution .    figure  [ fig : spaceconv](a ) displays storage costs for each algorithm as the signal length @xmath180 increases .",
    "we approximate memory usage by reporting the total workspace allocated by matlab for the algorithm .",
    "pgm , at , and cgm have static allocations , but they use a matrix variable of size @xmath181 . thincgm attempts to maintain a low - rank approximation of the decision variable , but the rank increases steadily , so the algorithm fails after @xmath182 .",
    "in contrast , sketchycgm has a static memory allocation of @xmath37 .",
    "it already offers the best memory footprint for @xmath183 , and it still works when @xmath184 .",
    "in fact , sketchycgm can tackle even larger problems .",
    "we were able to reconstruct an image with @xmath185 pixels , treated as a vector @xmath177 , given @xmath186 noiseless coded diffraction measurements , as above .",
    "figure  [ fig : spaceconv](b ) plots the convergence of the relative error : @xmath187 , where @xmath188 is a top eigenvector of the sketchycgm iterate @xmath129 .",
    "after 150 iterations , the algorithm produced an image with relative error @xmath189 and with psnr 36.19 db . thus , we can solve   when the psd matrix variable @xmath18 has @xmath190 complex entries !    as compared to other convex optimization algorithms ,",
    "the main weakness of sketchycgm is the @xmath95 iteration count .",
    "some algorithms , such as at , can achieve @xmath191 iteration count , but they are limited to smaller problems . closing this gap is an important question for future work .",
    "0.6     0.35     [ fig : spaceconv ]    0.3    ( image ) at ( 0,0 ) ;    ( 0.50,0.75 ) rectangle ( 0.80,0.90 ) ; ( 0.70,0.14 ) rectangle ( 0.95,0.38 ) ;       0.3        0.3       up to now , it has not been possible to attack phase retrieval problems of a realistic size by solving the convex formulation  .",
    "as we have shown , current convex optimization algorithms can not achieve scale . instead",
    ", many researchers apply nonconvex heuristics to solve phase retrieval problems  @xcite .",
    "these heuristics can produce reasonable solutions , but they require extensive tuning and have limited effectiveness . in this section , we demonstrate that , without any modification , sketchycgm can solve a phase retrieval problem from an application in microscopy . furthermore , it produces an image that is superior to the results obtained using major nonconvex heuristics .",
    "we study a phase retrieval problem that arises from an imaging modality called fourier ptychography ( fp )  @xcite .",
    "the authors of  @xcite provided measurements of a slide containing human blood cells from a working fp system .",
    "we treat the sample as an image with @xmath192 pixels , which we represent as a vector @xmath177 .",
    "the goal is to reconstruct the _ phase _ of the image , which roughly corresponds with the thickness of the sample at a given location .",
    "the data consists of 29 illuminations , each containing @xmath193 pixels .",
    "the number of measurements @xmath194 .",
    "the measurement vectors @xmath27 are obtained from windowed discrete fourier transforms .",
    "we can formulate the problem of reconstructing the sample @xmath32 using the convex phase retrieval template   with the parameter @xmath195 . in this instance , the psd matrix variable @xmath196 has @xmath197 complex entries .    to solve",
    ", we run the sketchycgm variant from  [ sec : cgm - psd ] with the rank parameter @xmath176 for @xmath198 iterations .",
    "we factor the rank - one matrix output to obtain an approximation @xmath33 of the sample .",
    "figure  [ fig : ptych](a ) displays the phase of the reconstruction @xmath33 .",
    "figure  [ fig : ptych ] also includes comparisons with two nonconvex heuristics .",
    "the authors of  @xcite provided a reconstruction obtained via the burer ",
    "monteiro method  @xcite .",
    "we also applied wirtinger flow  @xcite with the recommended parameters .",
    "sketchycgm yields a smooth and detailed phase reconstruction .",
    "monteiro produces severe artifacts , which suggest an unphysical oscillation in the thickness of the sample .",
    "wirtinger flow fails completely .",
    "these results are consistent with  ( * ? ? ?",
    "* fig .  4 ) , which indicates 510 db improvement of convex optimization over heuristics .",
    "the quality of phase reconstruction can be essential for scientific purposes . in this particular example ,",
    "some of the blood cells are infected with malaria parasites ( figure  [ fig : ptych](a ) , red boxes ) .",
    "diagnosis is easier when the visual acuity of the reconstruction is high .",
    "we have shown that it is possible to construct a low - rank approximate solution to a large - scale matrix optimization problem by sketching the decision variable .",
    "let us contrast our approach against other low - storage techniques for large - scale optimization .",
    "* * to the best of our knowledge , there are no direct precedents for the idea and realization of an optimization algorithm that sketches the decision variable .",
    "this work does partake in a broader vision that randomization can be used to design numerical algorithms  @xcite .",
    "researchers have considered * sketching the problem data * as a way to reduce the size of a problem specification in exchange for additional error .",
    "this idea dates to the paper of sarls  @xcite ; see also  @xcite .",
    "there are also several papers  @xcite in which researchers try to improve the computational or storage footprint of convex optimization methods by * sketching internal variables * , such as hessians .",
    "none of these approaches address the core issue that concerns us : the decision variable may require much more storage than the solution .    *",
    "* we have already discussed a major trend in which researchers develop algorithms that attack * nonconvex reformulations * of a problem .",
    "for example , see  @xcite .",
    "the main advantage is to reduce the size of the decision variable ; some methods also have the ancillary benefit of rapid local convergence . on the other hand , these algorithms are provably correct only under strong statistical assumptions on the problem data .    * * our work shows that convex optimization need not have high storage complexity for problems with a compact specification and a simple solution . in particular , for low - rank matrix optimization , storage is no reason to drop convexity .",
    "it has not escaped our notice that the specific pairing of sketching and cgm that we have postulated immediately suggests a possible mechanism for solving other structured convex programs using optimal storage .",
    "jat and mu were supported in part by onr awards n00014 - 11 - 1 - 0025 and n00014 - 17 - 1 - 2146 and the gordon & betty moore foundation .",
    "vc and ay were supported in part by the european commission under grant erc future proof , snf 200021 - 146750 , and snf crsii2 - 147633 .",
    "the authors thank dr .",
    "roarke horstmeyer for sharing the blood cell data .",
    "in this appendix , we develop a basic convergence theory for the sketchycgm algorithm .",
    "we focus on situations where the low - rank estimates produced by sketchycgm converge to a low - rank solution of the model problem  .",
    "[ [ preliminaries . ] ] preliminaries .",
    "+ + + + + + + + + + + + + +    we rely on the following standard convergence result for cgm .",
    "[ fact : cgm - objective ] let @xmath12 be an arbitrary solution to",
    ". for each @xmath199 , the matrix @xmath87 given by the cgm iteration   satisfies @xmath200 the number @xmath169 is called the _ curvature constant _",
    "( 3 ) ) of the problem  .    [",
    "[ theoremthmlow - rank - equivalence - a - basic - convergence - result . ] ] theorem  [ thm : low - rank - equivalence ] : a basic convergence result .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    first , we study the case where the standard cgm iteration   converges . in this setting",
    ", we can show that sketchycgm produces iterates that tend toward a matrix close to the limiting value of cgm .    according to the triangle inequality ,",
    "@xmath201 the error bound for reconstruction from the sketch , theorem  [ thm : sketch - err - body ] , shows that @xmath202_r   \\right\\vert}_{\\mathrm{f } } } \\\\      & \\leq 3\\sqrt{2 } { { \\left\\vert   { \\bm{x}}_t - [ { \\bm{x}}_{\\rm cgm}]_r   \\right\\vert}_{\\mathrm{f}}}. \\end{aligned}\\ ] ] the second inequality holds because @xmath203 $ ] is a best rank-@xmath1 approximation of @xmath87 with respect to frobenius norm , whereas @xmath204_r$ ] is an undistinguished rank-@xmath1 matrix .",
    "combine the last two displays to obtain @xmath205_r   \\right\\vert}_{\\mathrm{f } } } \\\\      & = 3\\sqrt{2 } { { \\left\\vert   { \\bm{x}}_{\\rm cgm } - [ { \\bm{x}}_{\\rm cgm}]_r   \\right\\vert}_{\\mathrm{f}}}. \\end{aligned}\\ ] ] the second inequality and the last line follow from the limit @xmath206 .    if @xmath160 , then @xmath207_r$ ] .",
    "therefore , we may conclude that @xmath208 .",
    "[ [ theoremthmsketch - cgm - unique - when - all - solutions - are - low - rank . ] ] theorem  [ thm : sketch - cgm - unique ] : when all solutions are low rank .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    next , we examine the situation where all of the solutions to the model problem   have low rank . in this case , we can show that sketchycgm produces a sequence of approximations that approaches the solution set of the problem .",
    "this point does not follow formally from theorem  [ thm : low - rank - equivalence ] because cgm need not produce a convergent sequence of iterates .    as a consequence of the triangle inequality , @xmath209",
    "we claim that the second term @xmath210 .",
    "if so , then the first term converges to zero : @xmath211_r   \\right\\vert}_{\\mathrm{f } } } \\\\      & \\leq 3\\sqrt{2 } { \\operatorname{dist}}_{\\rm f}({\\bm{x}}_t , s_{\\star } )      \\to 0.\\end{aligned}\\ ] ] the first inequality is theorem  [ thm : sketch - err - body ] .",
    "the second bound holds because @xmath203_r$ ] is a best rank-@xmath1 approximation of @xmath87 , while @xmath162 is an unremarkable set of rank-@xmath1 matrices .",
    "these observations complete the proof .    let us turn to the claim .",
    "abbreviate the objective function of   as @xmath212 .",
    "the continuous function @xmath213 attains its minimal value @xmath214 on the compact feasible set of  .",
    "the standard convergence result for cgm , fact  [ fact : cgm - objective ] , shows that @xmath215 .",
    "now , fix a number @xmath216 . define @xmath217 if @xmath4 is empty , then @xmath218 .",
    "otherwise , the continuous function @xmath213 attains the value @xmath219 on the compact set @xmath4 . in either case ,",
    "@xmath220 because @xmath4 contains no optimal point of  .",
    "since @xmath215 , we must have @xmath221 for all sufficiently large @xmath88 .",
    "therefore , @xmath222 for large @xmath88 .",
    "we conclude that @xmath223 for large @xmath88 , as required .",
    "[ [ theoremthmcgm - sketch - rate - rate - of - convergence . ] ] theorem  [ thm : cgm - sketch - rate ] : rate of convergence .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    finally , we identify a setting where we can bound the rate at which the estimates produced by sketchycgm converge to an optimal point of  . to do so , we assume that the optimal point is stable in the sense that feasible perturbations away from optimality are reflected in increases in the value of the objective function .    since the cgm iterate @xmath87 is feasible for  , we can use the stability hypothesis   to calculate that @xmath224_r   \\right\\vert}_{\\mathrm{f}}}^{\\nu } \\\\      & \\geq   \\kappa \\big [ ( 3\\sqrt{2})^{-1 } { \\operatorname{\\mathbb{e}}}{{\\left\\vert   \\smash{{\\bm{x}}_t - \\hat{{\\bm{x}}}_t }   \\right\\vert}_{\\mathrm{f } } } \\big]^{\\nu}. \\end{aligned}\\ ] ] the second inequality holds because @xmath12 has rank @xmath1 , while @xmath203_r$ ] is a better rank-@xmath1 approximation of @xmath87 .",
    "the last inequality follows from theorem  [ thm : sketch - err - body ] .",
    "the latter display implies that @xmath225^{1/\\nu}.\\end{gathered}\\ ] ] to complete the proof , invoke the standard convergence result for cgm , fact  [ fact : cgm - objective ] , to bound the quantity @xmath226 .",
    "last , simplify the constant .",
    "this appendix elaborates on the phase retrieval experiments described in  [ sec : numerics ] .",
    "it also presents additional experiments on matrix completion and phase retrieval .",
    "our experiments involve a number of different models for data , so we require several elementary loss functions . each of these maps is an extended convex function @xmath227 .",
    "define @xmath228 the objectives correspond , respectively , to the negative log - likelihood of observing the data @xmath229 under a gaussian model , a gauss  laplace model , a bernoulli model , and a poisson model .",
    "one principal advantage of sketchycgm is its flexibility . without any modification",
    ", the algorithm can provably solve any convex optimization problem with a smooth objective and a schatten 1-norm constraint . to demonstrate this point numerically , we present the results of fitting the benchmark movielens 100k and 10 m datasets  @xcite with three different loss functions .    the movielens @xmath230 dataset contains ( about ) @xmath230 ratings that users of a website assigned to a collection of movies .",
    "suppose that there are @xmath231 users and @xmath180 movies .",
    "the data consists of triples @xmath232 where @xmath233 is a user , @xmath234 is a movie , and @xmath235 is the rating of movie @xmath236 by user @xmath237 .",
    "we preprocess the data minimally .",
    "we remove the movies that are not rated by any user , as well as the users that have not provided any ratings . to fit a logistic model",
    ", we also binarize the ratings by replacing all values above 3.5 with @xmath9 and the rest with @xmath238 .",
    "thus , the logistic model seeks a classifier that separates high ratings ( 4 and 5 ) from low ratings ( 1 , 2 , and 3 ) .",
    "we can use low - rank matrix completion to fit a model to the movielens data . to see why , introduce a target matrix @xmath239 that tabulates all ratings ( known and unknown ) of movies by users .",
    "one may imagine that @xmath2 has low rank because a lot of the variation in the ratings is explained by the quality of the movie , its genre , and a user s preference for that genre .",
    "we only observe a subset of the entries of @xmath2 , and our goal is to predict the remaining entries .",
    "we model this matrix completion problem using the formulation  .",
    "let @xmath4 be a training set of user  movie pairs , and let @xmath240 list the associated ratings .",
    "introduce the linear map @xmath241 where @xmath242 the objective function @xmath243 has the form @xmath244 where @xmath245 .",
    "suppose that @xmath246 is an estimate for the target matrix @xmath2 .",
    "let @xmath247 be the test set of user  movie pairs , with ratings listed in @xmath248 .",
    "define the test error @xmath249 once again , @xmath245 .    for the 100k dataset",
    ", we use the default ` ub ` partition of the data into training and test sets . for each loss function",
    ", we sweep @xmath11 from @xmath250 to @xmath198 in steps of @xmath251 .",
    "as expected , the rank of a solution @xmath12 of the problem   increases with @xmath11 .",
    "we select the value of @xmath11 that provides the best test error after @xmath198 iterations of cgm .      for each dataset and each loss function",
    ", we fit the model with the designated value of @xmath11 by applying sketchycgm .",
    "figures  [ f - movielens ] and [ f - movielens-10 m ] show how the test error for the sketchycgm reconstruction varies as we change the rank parameter @xmath1 in sketchycgm .",
    "for the 100k dataset , rank @xmath255 yields test error similar with the cgm solution .",
    "for the 10 m dataset , rank @xmath256 yields equivalent performance .",
    "the setup is the same as in ",
    "[ sec : phase - vignette ] .",
    "let @xmath177 be a vector , and suppose we acquire measurements @xmath257 we can modify the measurement vectors @xmath24 to obtain a range of problems .",
    "we can also adjust the distribution of the noise @xmath258 .    to model the measurement process",
    ", it is convenient to form the matrix @xmath259 whose rows are the measurement vectors @xmath260 .",
    "then define a linear map @xmath171 and its adjoint @xmath261 via @xmath262 the map @xmath263 extracts the diagonal of a matrix ; @xmath264 maps a vector into a diagonal matrix .",
    "when @xmath18 is psd , note that @xmath265 .    we instantiate the convex optimization template   with the linear map   and the objective function @xmath266 here , the parameter @xmath267 . following  ( * ? ? ?",
    "ii ) , we usually set @xmath268 .",
    "we approximate the true vector @xmath32 by means of a maximum eigenvector @xmath33 of a solution @xmath12 to  .            at & @xmath276 & @xmath277 & @xmath278 & @xmath279 &  &  + pgm & @xmath280 & @xmath281 & @xmath282 & @xmath283 &  &  + cgm & @xmath284 & @xmath285 & @xmath286 & @xmath287 &  &  + thincgm & @xmath284 & @xmath288 & @xmath289 & @xmath290 & @xmath291 &  + sketchycgm & @xmath292 & @xmath293 & @xmath294 & @xmath295 & @xmath296 & @xmath297 +              in  [ sec : synthetic - pr ] , we considered a measurement model based on random coded diffraction patterns .",
    "this is a synthetic setup inspired by an imaging system where one can modulate the image before diffraction occurs  @xcite .    for this example",
    ", the matrix @xmath259 appearing in   takes the form @xmath298 in this expression , @xmath299 is the discrete fourier transform ( dft ) matrix , and @xmath300 are diagonal matrices that describe modulating waveforms .",
    "the parameter @xmath301 represents the number of @xmath180-dimensional views of the target vector @xmath177 that we acquire . the total number of measurements @xmath302 .",
    "we generate each diagonal entry of each matrix @xmath303 independently at random .",
    "each one is the product of two independent random variables @xmath304 and @xmath305 , where @xmath304 is chosen uniformly from @xmath306 and @xmath305 is drawn from @xmath307 with probabilities @xmath308 and @xmath309 .",
    "for the scaling experiments in  [ sec : synthetic - pr ] , the measurements take the form   where the @xmath260 are the rows of .",
    "we solve   with the loss @xmath311 , the linear map   , and @xmath11 set to the average of the data @xmath179 .",
    "the remaining details appear in  [ sec : synthetic - pr ] .",
    "table  [ tab : spaceeff ] summarizes the storage costs for solving this type of synthetic phase retrieval problem with five different convex optimization algorithms .                                  in many imaging systems ,",
    "a poisson noise model is more appropriate than a gaussian noise model .",
    "let us demonstrate that the sketchycgm algorithm can solve synthetic phase retrieval problems with the loss @xmath312 .",
    "this work highlights the importance of adapting the loss function to the noise distribution .",
    "the setup is similar to   [ sec : synthetic - app ] .",
    "fix a small image @xmath177 with @xmath313 pixels .",
    "acquire @xmath314 measurements of the form   using the coded diffraction model  . each realization @xmath315 of the noise is drawn iid from a @xmath316 distribution , whose mean is chosen so that the snr of the measurements is 20 db .      to solve this problem via sketchycgm ,",
    "it is necessary to make some small modifications  @xcite .",
    "we initialize the algorithm with the dual vector @xmath317 and set the learning rate @xmath318 . the rank parameter @xmath176 , and the algorithm runs for 100 iterations .",
    "figure  [ fig : poisson ] displays the results of this computation .",
    "we also compare the output with a reconstruction obtained by applying the unmodified version of sketchycgm to solve   with the ( mismatched ) loss @xmath311 , the same linear map , and the same value of @xmath11 .",
    "as expected , the poisson formulation performs better .      in  [ sec : fourier - ptychography - problem ] , we discussed a real - world phase retrieval problem arising from fourier ptychography  @xcite .",
    "let us explain the mathematical model for this problem and present some additional numerical work .    for fourier ptychography",
    ", the matrix @xmath259 appearing in   has the following structure .",
    "@xmath319 in this expression , @xmath320 is the 2d discrete fourier transform , and @xmath321 is a low - dimensional 2d discrete fourier transform .",
    "the sparse matrices @xmath322 describe bandpass filters ; each column of @xmath303 has at most one nonzero entry . the number of measurements @xmath323 .",
    "see  @xcite for details about the physical setup and the mathematical model .",
    "section  [ sec : fourier - ptychography - problem ] describes a specific instance of fourier ptychography imaging , applied to a slide containing red blood cells . to perform phase retrieval",
    ", we use the optimization problem   with loss @xmath311 and with @xmath195 .",
    "the measurement vectors @xmath260 are the rows of .",
    "we apply several algorithms , including a moderate number of iterations of cgm , sketchycgm with rank parameter @xmath176 , the burer  monteiro method  @xcite , and wirtinger flow  @xcite .",
    "figure  [ fig : evolution - spectrum ] and  [ fig : fourier - ptychography - convergence](a ) provide information about the spectrum of the cgm iterates . for several values of @xmath94",
    ", we observe that the @xmath94-rank - rank of a matrix is the number of singular values that exceed @xmath324 , where @xmath325 is the @xmath236th largest singular value .",
    "] of the iterates becomes large before declining to the value five .",
    "this type of behavior is typical for cgm , and it scuttles cgm variants that try to control the rank of the iterates directly .",
    "ideally , the solution @xmath12 to the convex formulation   of a phase retrieval problem has rank one .",
    "but these computations suggest that , for the blood cell data , the solution actually has rank five .",
    "the increase in rank is due to nonidealities in the imaging system , such as the spatial incoherence of the light source .",
    "in essence , the measurements capture a superposition of several slightly different images .",
    "regardless , the convex model   is still effective , and a top eigenvector of the solution @xmath12 still provides a good approximation to the image  @xcite .",
    "figure  [ fig : fourier - ptychography - convergence](b ) charts the objective value @xmath326 attained by the sketchycgm iterates .",
    "we can implicitly compute the objective value @xmath93 for the cgm iterate @xmath87 using the loop invariant  .",
    "note that @xmath87 achieves a much smaller objective value than the rank - one approximation @xmath129 produced by sketchycgm .",
    "the discrepancy is due to the fact that the solution to the optimization problem has approximate rank five .",
    "figure  [ fig : reconstruction - by - iterations ] displays snapshots of the sketchycgm iterates as the algorithm proceeds .",
    "we see that sketchycgm already achieve a diagnostic quality image after @xmath327 iterations , but the algorithm continues to resolve the image as it runs .",
    "last , figure  [ fig : ptych - grad ] shows the phase gradient of the solution to   obtained with three different algorithms ; these plots provide an alternative view of figure  [ fig : ptych ] .",
    "roughly , the phase gradient indicates the change in the thickness of the sample at a given location .",
    "therefore , absolute changes in the value of the phase gradient are meaningful .",
    "note the unphysical oscillations in the reconstruction via burer ",
    "monteiro  @xcite .",
    "the reconstruction via wirtinger flow  @xcite contains no information at all ."
  ],
  "abstract_text": [
    "<S> this paper concerns a fundamental class of convex matrix optimization problems . </S>",
    "<S> it presents the first algorithm that uses optimal storage and provably computes a low - rank approximation of a solution . </S>",
    "<S> in particular , when all solutions have low rank , the algorithm converges to a solution . </S>",
    "<S> this algorithm , sketchycgm , modifies a standard convex optimization scheme , the conditional gradient method , to store only a small randomized sketch of the matrix variable . </S>",
    "<S> after the optimization terminates , the algorithm extracts a low - rank approximation of the solution from the sketch . </S>",
    "<S> in contrast to nonconvex heuristics , the guarantees for sketchycgm do not rely on statistical models for the problem data . </S>",
    "<S> numerical work demonstrates the benefits of sketchycgm over heuristics . </S>"
  ]
}